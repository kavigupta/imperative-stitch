{"fsspec/caching.py": "from __future__ import annotations\n\nimport collections\nimport functools\nimport logging\nimport math\nimport os\nimport threading\nimport warnings\nfrom concurrent.futures import Future, ThreadPoolExecutor\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Generic,\n    NamedTuple,\n    Optional,\n    OrderedDict,\n    TypeVar,\n)\n\nif TYPE_CHECKING:\n    import mmap\n\n    from typing_extensions import ParamSpec\n\n    P = ParamSpec(\"P\")\nelse:\n    P = TypeVar(\"P\")\n\nT = TypeVar(\"T\")\n\n\nlogger = logging.getLogger(\"fsspec\")\n\nFetcher = Callable[[int, int], bytes]  # Maps (start, end) to bytes\n\n\nclass BaseCache:\n    \"\"\"Pass-though cache: doesn't keep anything, calls every time\n\n    Acts as base class for other cachers\n\n    Parameters\n    ----------\n    blocksize: int\n        How far to read ahead in numbers of bytes\n    fetcher: func\n        Function of the form f(start, end) which gets bytes from remote as\n        specified\n    size: int\n        How big this file is\n    \"\"\"\n\n    name: ClassVar[str] = \"none\"\n\n    def __init__(self, blocksize: int, fetcher: Fetcher, size: int) -> None:\n        self.blocksize = blocksize\n        self.nblocks = 0\n        self.fetcher = fetcher\n        self.size = size\n        self.hit_count = 0\n        self.miss_count = 0\n        # the bytes that we actually requested\n        self.total_requested_bytes = 0\n\n    def _fetch(self, start: int | None, stop: int | None) -> bytes:\n        if start is None:\n            start = 0\n        if stop is None:\n            stop = self.size\n        if start >= self.size or start >= stop:\n            return b\"\"\n        return self.fetcher(start, stop)\n\n    def _reset_stats(self) -> None:\n        \"\"\"Reset hit and miss counts for a more ganular report e.g. by file.\"\"\"\n        self.hit_count = 0\n        self.miss_count = 0\n        self.total_requested_bytes = 0\n\n    def _log_stats(self) -> str:\n        \"\"\"Return a formatted string of the cache statistics.\"\"\"\n        if self.hit_count == 0 and self.miss_count == 0:\n            # a cache that does nothing, this is for logs only\n            return \"\"\n        return \" , %s: %d hits, %d misses, %d total requested bytes\" % (\n            self.name,\n            self.hit_count,\n            self.miss_count,\n            self.total_requested_bytes,\n        )\n\n    def __repr__(self) -> str:\n        # TODO: use rich for better formatting\n        return f\"\"\"\n        <{self.__class__.__name__}:\n            block size  :   {self.blocksize}\n            block count :   {self.nblocks}\n            file size   :   {self.size}\n            cache hits  :   {self.hit_count}\n            cache misses:   {self.miss_count}\n            total requested bytes: {self.total_requested_bytes}>\n        \"\"\"\n\n\nclass MMapCache(BaseCache):\n    \"\"\"memory-mapped sparse file cache\n\n    Opens temporary file, which is filled blocks-wise when data is requested.\n    Ensure there is enough disc space in the temporary location.\n\n    This cache method might only work on posix\n    \"\"\"\n\n    name = \"mmap\"\n\n    def __init__(\n        self,\n        blocksize: int,\n        fetcher: Fetcher,\n        size: int,\n        location: str | None = None,\n        blocks: set[int] | None = None,\n    ) -> None:\n        super().__init__(blocksize, fetcher, size)\n        self.blocks = set() if blocks is None else blocks\n        self.location = location\n        self.cache = self._makefile()\n\n    def _makefile(self) -> mmap.mmap | bytearray:\n        import mmap\n        import tempfile\n\n        if self.size == 0:\n            return bytearray()\n\n        # posix version\n        if self.location is None or not os.path.exists(self.location):\n            if self.location is None:\n                fd = tempfile.TemporaryFile()\n                self.blocks = set()\n            else:\n                fd = open(self.location, \"wb+\")\n            fd.seek(self.size - 1)\n            fd.write(b\"1\")\n            fd.flush()\n        else:\n            fd = open(self.location, \"r+b\")\n\n        return mmap.mmap(fd.fileno(), self.size)\n\n    def _fetch(self, start: int | None, end: int | None) -> bytes:\n        logger.debug(f\"MMap cache fetching {start}-{end}\")\n        if start is None:\n            start = 0\n        if end is None:\n            end = self.size\n        if start >= self.size or start >= end:\n            return b\"\"\n        start_block = start // self.blocksize\n        end_block = end // self.blocksize\n        need = [i for i in range(start_block, end_block + 1) if i not in self.blocks]\n        hits = [i for i in range(start_block, end_block + 1) if i in self.blocks]\n        self.miss_count += len(need)\n        self.hit_count += len(hits)\n        while need:\n            # TODO: not a for loop so we can consolidate blocks later to\n            # make fewer fetch calls; this could be parallel\n            i = need.pop(0)\n\n            sstart = i * self.blocksize\n            send = min(sstart + self.blocksize, self.size)\n            self.total_requested_bytes += send - sstart\n            logger.debug(f\"MMap get block #{i} ({sstart}-{send})\")\n            self.cache[sstart:send] = self.fetcher(sstart, send)\n            self.blocks.add(i)\n\n        return self.cache[start:end]\n\n    def __getstate__(self) -> dict[str, Any]:\n        state = self.__dict__.copy()\n        # Remove the unpicklable entries.\n        del state[\"cache\"]\n        return state\n\n    def __setstate__(self, state: dict[str, Any]) -> None:\n        # Restore instance attributes\n        self.__dict__.update(state)\n        self.cache = self._makefile()\n\n\nclass ReadAheadCache(BaseCache):\n    \"\"\"Cache which reads only when we get beyond a block of data\n\n    This is a much simpler version of BytesCache, and does not attempt to\n    fill holes in the cache or keep fragments alive. It is best suited to\n    many small reads in a sequential order (e.g., reading lines from a file).\n    \"\"\"\n\n    name = \"readahead\"\n\n    def __init__(self, blocksize: int, fetcher: Fetcher, size: int) -> None:\n        super().__init__(blocksize, fetcher, size)\n        self.cache = b\"\"\n        self.start = 0\n        self.end = 0\n\n    def _fetch(self, start: int | None, end: int | None) -> bytes:\n        if start is None:\n            start = 0\n        if end is None or end > self.size:\n            end = self.size\n        if start >= self.size or start >= end:\n            return b\"\"\n        l = end - start\n        if start >= self.start and end <= self.end:\n            # cache hit\n            self.hit_count += 1\n            return self.cache[start - self.start : end - self.start]\n        elif self.start <= start < self.end:\n            # partial hit\n            self.miss_count += 1\n            part = self.cache[start - self.start :]\n            l -= len(part)\n            start = self.end\n        else:\n            # miss\n            self.miss_count += 1\n            part = b\"\"\n        end = min(self.size, end + self.blocksize)\n        self.total_requested_bytes += end - start\n        self.cache = self.fetcher(start, end)  # new block replaces old\n        self.start = start\n        self.end = self.start + len(self.cache)\n        return part + self.cache[:l]\n\n\nclass FirstChunkCache(BaseCache):\n    \"\"\"Caches the first block of a file only\n\n    This may be useful for file types where the metadata is stored in the header,\n    but is randomly accessed.\n    \"\"\"\n\n    name = \"first\"\n\n    def __init__(self, blocksize: int, fetcher: Fetcher, size: int) -> None:\n        if blocksize > size:\n            # this will buffer the whole thing\n            blocksize = size\n        super().__init__(blocksize, fetcher, size)\n        self.cache: bytes | None = None\n\n    def _fetch(self, start: int | None, end: int | None) -> bytes:\n        start = start or 0\n        if start > self.size:\n            logger.debug(\"FirstChunkCache: requested start > file size\")\n            return b\"\"\n\n        end = min(end, self.size)\n\n        if start < self.blocksize:\n            if self.cache is None:\n                self.miss_count += 1\n                if end > self.blocksize:\n                    self.total_requested_bytes += end\n                    data = self.fetcher(0, end)\n                    self.cache = data[: self.blocksize]\n                    return data[start:]\n                self.cache = self.fetcher(0, self.blocksize)\n                self.total_requested_bytes += self.blocksize\n            part = self.cache[start:end]\n            if end > self.blocksize:\n                self.total_requested_bytes += end - self.blocksize\n                part += self.fetcher(self.blocksize, end)\n            self.hit_count += 1\n            return part\n        else:\n            self.miss_count += 1\n            self.total_requested_bytes += end - start\n            return self.fetcher(start, end)\n\n\nclass BlockCache(BaseCache):\n    \"\"\"\n    Cache holding memory as a set of blocks.\n\n    Requests are only ever made ``blocksize`` at a time, and are\n    stored in an LRU cache. The least recently accessed block is\n    discarded when more than ``maxblocks`` are stored.\n\n    Parameters\n    ----------\n    blocksize : int\n        The number of bytes to store in each block.\n        Requests are only ever made for ``blocksize``, so this\n        should balance the overhead of making a request against\n        the granularity of the blocks.\n    fetcher : Callable\n    size : int\n        The total size of the file being cached.\n    maxblocks : int\n        The maximum number of blocks to cache for. The maximum memory\n        use for this cache is then ``blocksize * maxblocks``.\n    \"\"\"\n\n    name = \"blockcache\"\n\n    def __init__(\n        self, blocksize: int, fetcher: Fetcher, size: int, maxblocks: int = 32\n    ) -> None:\n        super().__init__(blocksize, fetcher, size)\n        self.nblocks = math.ceil(size / blocksize)\n        self.maxblocks = maxblocks\n        self._fetch_block_cached = functools.lru_cache(maxblocks)(self._fetch_block)\n\n    def cache_info(self):\n        \"\"\"\n        The statistics on the block cache.\n\n        Returns\n        -------\n        NamedTuple\n            Returned directly from the LRU Cache used internally.\n        \"\"\"\n        return self._fetch_block_cached.cache_info()\n\n    def __getstate__(self) -> dict[str, Any]:\n        state = self.__dict__\n        del state[\"_fetch_block_cached\"]\n        return state\n\n    def __setstate__(self, state: dict[str, Any]) -> None:\n        self.__dict__.update(state)\n        self._fetch_block_cached = functools.lru_cache(state[\"maxblocks\"])(\n            self._fetch_block\n        )\n\n    def _fetch(self, start: int | None, end: int | None) -> bytes:\n        if start is None:\n            start = 0\n        if end is None:\n            end = self.size\n        if start >= self.size or start >= end:\n            return b\"\"\n\n        # byte position -> block numbers\n        start_block_number = start // self.blocksize\n        end_block_number = end // self.blocksize\n\n        # these are cached, so safe to do multiple calls for the same start and end.\n        for block_number in range(start_block_number, end_block_number + 1):\n            self._fetch_block_cached(block_number)\n\n        return self._read_cache(\n            start,\n            end,\n            start_block_number=start_block_number,\n            end_block_number=end_block_number,\n        )\n\n    def _fetch_block(self, block_number: int) -> bytes:\n        \"\"\"\n        Fetch the block of data for `block_number`.\n        \"\"\"\n        if block_number > self.nblocks:\n            raise ValueError(\n                f\"'block_number={block_number}' is greater than \"\n                f\"the number of blocks ({self.nblocks})\"\n            )\n\n        start = block_number * self.blocksize\n        end = start + self.blocksize\n        self.total_requested_bytes += end - start\n        self.miss_count += 1\n        logger.info(\"BlockCache fetching block %d\", block_number)\n        block_contents = super()._fetch(start, end)\n        return block_contents\n\n    def _read_cache(\n        self, start: int, end: int, start_block_number: int, end_block_number: int\n    ) -> bytes:\n        \"\"\"\n        Read from our block cache.\n\n        Parameters\n        ----------\n        start, end : int\n            The start and end byte positions.\n        start_block_number, end_block_number : int\n            The start and end block numbers.\n        \"\"\"\n        start_pos = start % self.blocksize\n        end_pos = end % self.blocksize\n\n        self.hit_count += 1\n        if start_block_number == end_block_number:\n            block: bytes = self._fetch_block_cached(start_block_number)\n            return block[start_pos:end_pos]\n\n        else:\n            # read from the initial\n            out = [self._fetch_block_cached(start_block_number)[start_pos:]]\n\n            # intermediate blocks\n            # Note: it'd be nice to combine these into one big request. However\n            # that doesn't play nicely with our LRU cache.\n            out.extend(\n                map(\n                    self._fetch_block_cached,\n                    range(start_block_number + 1, end_block_number),\n                )\n            )\n\n            # final block\n            out.append(self._fetch_block_cached(end_block_number)[:end_pos])\n\n            return b\"\".join(out)\n\n\nclass BytesCache(BaseCache):\n    \"\"\"Cache which holds data in a in-memory bytes object\n\n    Implements read-ahead by the block size, for semi-random reads progressing\n    through the file.\n\n    Parameters\n    ----------\n    trim: bool\n        As we read more data, whether to discard the start of the buffer when\n        we are more than a blocksize ahead of it.\n    \"\"\"\n\n    name: ClassVar[str] = \"bytes\"\n\n    def __init__(\n        self, blocksize: int, fetcher: Fetcher, size: int, trim: bool = True\n    ) -> None:\n        super().__init__(blocksize, fetcher, size)\n        self.cache = b\"\"\n        self.start: int | None = None\n        self.end: int | None = None\n        self.trim = trim\n\n    def _fetch(self, start: int | None, end: int | None) -> bytes:\n        # TODO: only set start/end after fetch, in case it fails?\n        # is this where retry logic might go?\n        if start is None:\n            start = 0\n        if end is None:\n            end = self.size\n        if start >= self.size or start >= end:\n            return b\"\"\n        if (\n            self.start is not None\n            and start >= self.start\n            and self.end is not None\n            and end < self.end\n        ):\n            # cache hit: we have all the required data\n            offset = start - self.start\n            self.hit_count += 1\n            return self.cache[offset : offset + end - start]\n\n        if self.blocksize:\n            bend = min(self.size, end + self.blocksize)\n        else:\n            bend = end\n\n        if bend == start or start > self.size:\n            return b\"\"\n\n        if (self.start is None or start < self.start) and (\n            self.end is None or end > self.end\n        ):\n            # First read, or extending both before and after\n            self.total_requested_bytes += bend - start\n            self.miss_count += 1\n            self.cache = self.fetcher(start, bend)\n            self.start = start\n        else:\n            assert self.start is not None\n            assert self.end is not None\n            self.miss_count += 1\n\n            if start < self.start:\n                if self.end is None or self.end - end > self.blocksize:\n                    self.total_requested_bytes += bend - start\n                    self.cache = self.fetcher(start, bend)\n                    self.start = start\n                else:\n                    self.total_requested_bytes += self.start - start\n                    new = self.fetcher(start, self.start)\n                    self.start = start\n                    self.cache = new + self.cache\n            elif self.end is not None and bend > self.end:\n                if self.end > self.size:\n                    pass\n                elif end - self.end > self.blocksize:\n                    self.total_requested_bytes += bend - start\n                    self.cache = self.fetcher(start, bend)\n                    self.start = start\n                else:\n                    self.total_requested_bytes += bend - self.end\n                    new = self.fetcher(self.end, bend)\n                    self.cache = self.cache + new\n\n        self.end = self.start + len(self.cache)\n        offset = start - self.start\n        out = self.cache[offset : offset + end - start]\n        if self.trim:\n            num = (self.end - self.start) // (self.blocksize + 1)\n            if num > 1:\n                self.start += self.blocksize * num\n                self.cache = self.cache[self.blocksize * num :]\n        return out\n\n    def __len__(self) -> int:\n        return len(self.cache)\n\n\nclass AllBytes(BaseCache):\n    \"\"\"Cache entire contents of the file\"\"\"\n\n    name: ClassVar[str] = \"all\"\n\n    def __init__(\n        self,\n        blocksize: int | None = None,\n        fetcher: Fetcher | None = None,\n        size: int | None = None,\n        data: bytes | None = None,\n    ) -> None:\n        super().__init__(blocksize, fetcher, size)  # type: ignore[arg-type]\n        if data is None:\n            self.miss_count += 1\n            self.total_requested_bytes += self.size\n            data = self.fetcher(0, self.size)\n        self.data = data\n\n    def _fetch(self, start: int | None, stop: int | None) -> bytes:\n        self.hit_count += 1\n        return self.data[start:stop]\n\n\nclass KnownPartsOfAFile(BaseCache):\n    \"\"\"\n    Cache holding known file parts.\n\n    Parameters\n    ----------\n    blocksize: int\n        How far to read ahead in numbers of bytes\n    fetcher: func\n        Function of the form f(start, end) which gets bytes from remote as\n        specified\n    size: int\n        How big this file is\n    data: dict\n        A dictionary mapping explicit `(start, stop)` file-offset tuples\n        with known bytes.\n    strict: bool, default True\n        Whether to fetch reads that go beyond a known byte-range boundary.\n        If `False`, any read that ends outside a known part will be zero\n        padded. Note that zero padding will not be used for reads that\n        begin outside a known byte-range.\n    \"\"\"\n\n    name: ClassVar[str] = \"parts\"\n\n    def __init__(\n        self,\n        blocksize: int,\n        fetcher: Fetcher,\n        size: int,\n        data: Optional[dict[tuple[int, int], bytes]] = None,\n        strict: bool = True,\n        **_: Any,\n    ):\n        super().__init__(blocksize, fetcher, size)\n        self.strict = strict\n\n        # simple consolidation of contiguous blocks\n        if data:\n            old_offsets = sorted(data.keys())\n            offsets = [old_offsets[0]]\n            blocks = [data.pop(old_offsets[0])]\n            for start, stop in old_offsets[1:]:\n                start0, stop0 = offsets[-1]\n                if start == stop0:\n                    offsets[-1] = (start0, stop)\n                    blocks[-1] += data.pop((start, stop))\n                else:\n                    offsets.append((start, stop))\n                    blocks.append(data.pop((start, stop)))\n\n            self.data = dict(zip(offsets, blocks))\n        else:\n            self.data = {}\n\n    def _fetch(self, start: int | None, stop: int | None) -> bytes:\n        if start is None:\n            start = 0\n        if stop is None:\n            stop = self.size\n\n        out = b\"\"\n        for (loc0, loc1), data in self.data.items():\n            # If self.strict=False, use zero-padded data\n            # for reads beyond the end of a \"known\" buffer\n            if loc0 <= start < loc1:\n                off = start - loc0\n                out = data[off : off + stop - start]\n                if not self.strict or loc0 <= stop <= loc1:\n                    # The request is within a known range, or\n                    # it begins within a known range, and we\n                    # are allowed to pad reads beyond the\n                    # buffer with zero\n                    out += b\"\\x00\" * (stop - start - len(out))\n                    self.hit_count += 1\n                    return out\n                else:\n                    # The request ends outside a known range,\n                    # and we are being \"strict\" about reads\n                    # beyond the buffer\n                    start = loc1\n                    break\n\n        # We only get here if there is a request outside the\n        # known parts of the file. In an ideal world, this\n        # should never happen\n        if self.fetcher is None:\n            # We cannot fetch the data, so raise an error\n            raise ValueError(f\"Read is outside the known file parts: {(start, stop)}. \")\n        # We can fetch the data, but should warn the user\n        # that this may be slow\n        warnings.warn(\n            f\"Read is outside the known file parts: {(start, stop)}. \"\n            f\"IO/caching performance may be poor!\"\n        )\n        logger.debug(f\"KnownPartsOfAFile cache fetching {start}-{stop}\")\n        self.total_requested_bytes += stop - start\n        self.miss_count += 1\n        return out + super()._fetch(start, stop)\n\n\nclass UpdatableLRU(Generic[P, T]):\n    \"\"\"\n    Custom implementation of LRU cache that allows updating keys\n\n    Used by BackgroudBlockCache\n    \"\"\"\n\n    class CacheInfo(NamedTuple):\n        hits: int\n        misses: int\n        maxsize: int\n        currsize: int\n\n    def __init__(self, func: Callable[P, T], max_size: int = 128) -> None:\n        self._cache: OrderedDict[Any, T] = collections.OrderedDict()\n        self._func = func\n        self._max_size = max_size\n        self._hits = 0\n        self._misses = 0\n        self._lock = threading.Lock()\n\n    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> T:\n        if kwargs:\n            raise TypeError(f\"Got unexpected keyword argument {kwargs.keys()}\")\n        with self._lock:\n            if args in self._cache:\n                self._cache.move_to_end(args)\n                self._hits += 1\n                return self._cache[args]\n\n        result = self._func(*args, **kwargs)\n\n        with self._lock:\n            self._cache[args] = result\n            self._misses += 1\n            if len(self._cache) > self._max_size:\n                self._cache.popitem(last=False)\n\n        return result\n\n    def is_key_cached(self, *args: Any) -> bool:\n        with self._lock:\n            return args in self._cache\n\n    def add_key(self, result: T, *args: Any) -> None:\n        with self._lock:\n            self._cache[args] = result\n            if len(self._cache) > self._max_size:\n                self._cache.popitem(last=False)\n\n    def cache_info(self) -> UpdatableLRU.CacheInfo:\n        with self._lock:\n            return self.CacheInfo(\n                maxsize=self._max_size,\n                currsize=len(self._cache),\n                hits=self._hits,\n                misses=self._misses,\n            )\n\n\nclass BackgroundBlockCache(BaseCache):\n    \"\"\"\n    Cache holding memory as a set of blocks with pre-loading of\n    the next block in the background.\n\n    Requests are only ever made ``blocksize`` at a time, and are\n    stored in an LRU cache. The least recently accessed block is\n    discarded when more than ``maxblocks`` are stored. If the\n    next block is not in cache, it is loaded in a separate thread\n    in non-blocking way.\n\n    Parameters\n    ----------\n    blocksize : int\n        The number of bytes to store in each block.\n        Requests are only ever made for ``blocksize``, so this\n        should balance the overhead of making a request against\n        the granularity of the blocks.\n    fetcher : Callable\n    size : int\n        The total size of the file being cached.\n    maxblocks : int\n        The maximum number of blocks to cache for. The maximum memory\n        use for this cache is then ``blocksize * maxblocks``.\n    \"\"\"\n\n    name: ClassVar[str] = \"background\"\n\n    def __init__(\n        self, blocksize: int, fetcher: Fetcher, size: int, maxblocks: int = 32\n    ) -> None:\n        super().__init__(blocksize, fetcher, size)\n        self.nblocks = math.ceil(size / blocksize)\n        self.maxblocks = maxblocks\n        self._fetch_block_cached = UpdatableLRU(self._fetch_block, maxblocks)\n\n        self._thread_executor = ThreadPoolExecutor(max_workers=1)\n        self._fetch_future_block_number: int | None = None\n        self._fetch_future: Future[bytes] | None = None\n        self._fetch_future_lock = threading.Lock()\n\n    def cache_info(self) -> UpdatableLRU.CacheInfo:\n        \"\"\"\n        The statistics on the block cache.\n\n        Returns\n        -------\n        NamedTuple\n            Returned directly from the LRU Cache used internally.\n        \"\"\"\n        return self._fetch_block_cached.cache_info()\n\n    def __getstate__(self) -> dict[str, Any]:\n        state = self.__dict__\n        del state[\"_fetch_block_cached\"]\n        del state[\"_thread_executor\"]\n        del state[\"_fetch_future_block_number\"]\n        del state[\"_fetch_future\"]\n        del state[\"_fetch_future_lock\"]\n        return state\n\n    def __setstate__(self, state) -> None:\n        self.__dict__.update(state)\n        self._fetch_block_cached = UpdatableLRU(self._fetch_block, state[\"maxblocks\"])\n        self._thread_executor = ThreadPoolExecutor(max_workers=1)\n        self._fetch_future_block_number = None\n        self._fetch_future = None\n        self._fetch_future_lock = threading.Lock()\n\n    def _fetch(self, start: int | None, end: int | None) -> bytes:\n        if start is None:\n            start = 0\n        if end is None:\n            end = self.size\n        if start >= self.size or start >= end:\n            return b\"\"\n\n        # byte position -> block numbers\n        start_block_number = start // self.blocksize\n        end_block_number = end // self.blocksize\n\n        fetch_future_block_number = None\n        fetch_future = None\n        with self._fetch_future_lock:\n            # Background thread is running. Check we we can or must join it.\n            if self._fetch_future is not None:\n                assert self._fetch_future_block_number is not None\n                if self._fetch_future.done():\n                    logger.info(\"BlockCache joined background fetch without waiting.\")\n                    self._fetch_block_cached.add_key(\n                        self._fetch_future.result(), self._fetch_future_block_number\n                    )\n                    # Cleanup the fetch variables. Done with fetching the block.\n                    self._fetch_future_block_number = None\n                    self._fetch_future = None\n                else:\n                    # Must join if we need the block for the current fetch\n                    must_join = bool(\n                        start_block_number\n                        <= self._fetch_future_block_number\n                        <= end_block_number\n                    )\n                    if must_join:\n                        # Copy to the local variables to release lock\n                        # before waiting for result\n                        fetch_future_block_number = self._fetch_future_block_number\n                        fetch_future = self._fetch_future\n\n                        # Cleanup the fetch variables. Have a local copy.\n                        self._fetch_future_block_number = None\n                        self._fetch_future = None\n\n        # Need to wait for the future for the current read\n        if fetch_future is not None:\n            logger.info(\"BlockCache waiting for background fetch.\")\n            # Wait until result and put it in cache\n            self._fetch_block_cached.add_key(\n                fetch_future.result(), fetch_future_block_number\n            )\n\n        # these are cached, so safe to do multiple calls for the same start and end.\n        for block_number in range(start_block_number, end_block_number + 1):\n            self._fetch_block_cached(block_number)\n\n        # fetch next block in the background if nothing is running in the background,\n        # the block is within file and it is not already cached\n        end_block_plus_1 = end_block_number + 1\n        with self._fetch_future_lock:\n            if (\n                self._fetch_future is None\n                and end_block_plus_1 <= self.nblocks\n                and not self._fetch_block_cached.is_key_cached(end_block_plus_1)\n            ):\n                self._fetch_future_block_number = end_block_plus_1\n                self._fetch_future = self._thread_executor.submit(\n                    self._fetch_block, end_block_plus_1, \"async\"\n                )\n\n        return self._read_cache(\n            start,\n            end,\n            start_block_number=start_block_number,\n            end_block_number=end_block_number,\n        )\n\n    def _fetch_block(self, block_number: int, log_info: str = \"sync\") -> bytes:\n        \"\"\"\n        Fetch the block of data for `block_number`.\n        \"\"\"\n        if block_number > self.nblocks:\n            raise ValueError(\n                f\"'block_number={block_number}' is greater than \"\n                f\"the number of blocks ({self.nblocks})\"\n            )\n\n        start = block_number * self.blocksize\n        end = start + self.blocksize\n        logger.info(\"BlockCache fetching block (%s) %d\", log_info, block_number)\n        self.total_requested_bytes += end - start\n        self.miss_count += 1\n        block_contents = super()._fetch(start, end)\n        return block_contents\n\n    def _read_cache(\n        self, start: int, end: int, start_block_number: int, end_block_number: int\n    ) -> bytes:\n        \"\"\"\n        Read from our block cache.\n\n        Parameters\n        ----------\n        start, end : int\n            The start and end byte positions.\n        start_block_number, end_block_number : int\n            The start and end block numbers.\n        \"\"\"\n        start_pos = start % self.blocksize\n        end_pos = end % self.blocksize\n\n        # kind of pointless to count this as a hit, but it is\n        self.hit_count += 1\n\n        if start_block_number == end_block_number:\n            block = self._fetch_block_cached(start_block_number)\n            return block[start_pos:end_pos]\n\n        else:\n            # read from the initial\n            out = [self._fetch_block_cached(start_block_number)[start_pos:]]\n\n            # intermediate blocks\n            # Note: it'd be nice to combine these into one big request. However\n            # that doesn't play nicely with our LRU cache.\n            out.extend(\n                map(\n                    self._fetch_block_cached,\n                    range(start_block_number + 1, end_block_number),\n                )\n            )\n\n            # final block\n            out.append(self._fetch_block_cached(end_block_number)[:end_pos])\n\n            return b\"\".join(out)\n\n\ncaches: dict[str | None, type[BaseCache]] = {\n    # one custom case\n    None: BaseCache,\n}\n\n\ndef register_cache(cls: type[BaseCache], clobber: bool = False) -> None:\n    \"\"\"'Register' cache implementation.\n\n    Parameters\n    ----------\n    clobber: bool, optional\n        If set to True (default is False) - allow to overwrite existing\n        entry.\n\n    Raises\n    ------\n    ValueError\n    \"\"\"\n    name = cls.name\n    if not clobber and name in caches:\n        raise ValueError(f\"Cache with name {name!r} is already known: {caches[name]}\")\n    caches[name] = cls\n\n\nfor c in (\n    BaseCache,\n    MMapCache,\n    BytesCache,\n    ReadAheadCache,\n    BlockCache,\n    FirstChunkCache,\n    AllBytes,\n    KnownPartsOfAFile,\n    BackgroundBlockCache,\n):\n    register_cache(c)\n", "fsspec/compression.py": "\"\"\"Helper functions for a standard streaming compression API\"\"\"\n\nfrom zipfile import ZipFile\n\nimport fsspec.utils\nfrom fsspec.spec import AbstractBufferedFile\n\n\ndef noop_file(file, mode, **kwargs):\n    return file\n\n\n# TODO: files should also be available as contexts\n# should be functions of the form func(infile, mode=, **kwargs) -> file-like\ncompr = {None: noop_file}\n\n\ndef register_compression(name, callback, extensions, force=False):\n    \"\"\"Register an \"inferable\" file compression type.\n\n    Registers transparent file compression type for use with fsspec.open.\n    Compression can be specified by name in open, or \"infer\"-ed for any files\n    ending with the given extensions.\n\n    Args:\n        name: (str) The compression type name. Eg. \"gzip\".\n        callback: A callable of form (infile, mode, **kwargs) -> file-like.\n            Accepts an input file-like object, the target mode and kwargs.\n            Returns a wrapped file-like object.\n        extensions: (str, Iterable[str]) A file extension, or list of file\n            extensions for which to infer this compression scheme. Eg. \"gz\".\n        force: (bool) Force re-registration of compression type or extensions.\n\n    Raises:\n        ValueError: If name or extensions already registered, and not force.\n\n    \"\"\"\n    if isinstance(extensions, str):\n        extensions = [extensions]\n\n    # Validate registration\n    if name in compr and not force:\n        raise ValueError(f\"Duplicate compression registration: {name}\")\n\n    for ext in extensions:\n        if ext in fsspec.utils.compressions and not force:\n            raise ValueError(f\"Duplicate compression file extension: {ext} ({name})\")\n\n    compr[name] = callback\n\n    for ext in extensions:\n        fsspec.utils.compressions[ext] = name\n\n\ndef unzip(infile, mode=\"rb\", filename=None, **kwargs):\n    if \"r\" not in mode:\n        filename = filename or \"file\"\n        z = ZipFile(infile, mode=\"w\", **kwargs)\n        fo = z.open(filename, mode=\"w\")\n        fo.close = lambda closer=fo.close: closer() or z.close()\n        return fo\n    z = ZipFile(infile)\n    if filename is None:\n        filename = z.namelist()[0]\n    return z.open(filename, mode=\"r\", **kwargs)\n\n\nregister_compression(\"zip\", unzip, \"zip\")\n\ntry:\n    from bz2 import BZ2File\nexcept ImportError:\n    pass\nelse:\n    register_compression(\"bz2\", BZ2File, \"bz2\")\n\ntry:  # pragma: no cover\n    from isal import igzip\n\n    def isal(infile, mode=\"rb\", **kwargs):\n        return igzip.IGzipFile(fileobj=infile, mode=mode, **kwargs)\n\n    register_compression(\"gzip\", isal, \"gz\")\nexcept ImportError:\n    from gzip import GzipFile\n\n    register_compression(\n        \"gzip\", lambda f, **kwargs: GzipFile(fileobj=f, **kwargs), \"gz\"\n    )\n\ntry:\n    from lzma import LZMAFile\n\n    register_compression(\"lzma\", LZMAFile, \"lzma\")\n    register_compression(\"xz\", LZMAFile, \"xz\")\nexcept ImportError:\n    pass\n\ntry:\n    import lzmaffi\n\n    register_compression(\"lzma\", lzmaffi.LZMAFile, \"lzma\", force=True)\n    register_compression(\"xz\", lzmaffi.LZMAFile, \"xz\", force=True)\nexcept ImportError:\n    pass\n\n\nclass SnappyFile(AbstractBufferedFile):\n    def __init__(self, infile, mode, **kwargs):\n        import snappy\n\n        super().__init__(\n            fs=None, path=\"snappy\", mode=mode.strip(\"b\") + \"b\", size=999999999, **kwargs\n        )\n        self.infile = infile\n        if \"r\" in mode:\n            self.codec = snappy.StreamDecompressor()\n        else:\n            self.codec = snappy.StreamCompressor()\n\n    def _upload_chunk(self, final=False):\n        self.buffer.seek(0)\n        out = self.codec.add_chunk(self.buffer.read())\n        self.infile.write(out)\n        return True\n\n    def seek(self, loc, whence=0):\n        raise NotImplementedError(\"SnappyFile is not seekable\")\n\n    def seekable(self):\n        return False\n\n    def _fetch_range(self, start, end):\n        \"\"\"Get the specified set of bytes from remote\"\"\"\n        data = self.infile.read(end - start)\n        return self.codec.decompress(data)\n\n\ntry:\n    import snappy\n\n    snappy.compress(b\"\")\n    # Snappy may use the .sz file extension, but this is not part of the\n    # standard implementation.\n    register_compression(\"snappy\", SnappyFile, [])\n\nexcept (ImportError, NameError, AttributeError):\n    pass\n\ntry:\n    import lz4.frame\n\n    register_compression(\"lz4\", lz4.frame.open, \"lz4\")\nexcept ImportError:\n    pass\n\ntry:\n    import zstandard as zstd\n\n    def zstandard_file(infile, mode=\"rb\"):\n        if \"r\" in mode:\n            cctx = zstd.ZstdDecompressor()\n            return cctx.stream_reader(infile)\n        else:\n            cctx = zstd.ZstdCompressor(level=10)\n            return cctx.stream_writer(infile)\n\n    register_compression(\"zstd\", zstandard_file, \"zst\")\nexcept ImportError:\n    pass\n\n\ndef available_compressions():\n    \"\"\"Return a list of the implemented compressions.\"\"\"\n    return list(compr)\n", "fsspec/config.py": "from __future__ import annotations\n\nimport configparser\nimport json\nimport os\nimport warnings\nfrom typing import Any\n\nconf: dict[str, dict[str, Any]] = {}\ndefault_conf_dir = os.path.join(os.path.expanduser(\"~\"), \".config/fsspec\")\nconf_dir = os.environ.get(\"FSSPEC_CONFIG_DIR\", default_conf_dir)\n\n\ndef set_conf_env(conf_dict, envdict=os.environ):\n    \"\"\"Set config values from environment variables\n\n    Looks for variables of the form ``FSSPEC_<protocol>`` and\n    ``FSSPEC_<protocol>_<kwarg>``. For ``FSSPEC_<protocol>`` the value is parsed\n    as a json dictionary and used to ``update`` the config of the\n    corresponding protocol. For ``FSSPEC_<protocol>_<kwarg>`` there is no\n    attempt to convert the string value, but the kwarg keys will be lower-cased.\n\n    The ``FSSPEC_<protocol>_<kwarg>`` variables are applied after the\n    ``FSSPEC_<protocol>`` ones.\n\n    Parameters\n    ----------\n    conf_dict : dict(str, dict)\n        This dict will be mutated\n    envdict : dict-like(str, str)\n        Source for the values - usually the real environment\n    \"\"\"\n    kwarg_keys = []\n    for key in envdict:\n        if key.startswith(\"FSSPEC_\") and len(key) > 7 and key[7] != \"_\":\n            if key.count(\"_\") > 1:\n                kwarg_keys.append(key)\n                continue\n            try:\n                value = json.loads(envdict[key])\n            except json.decoder.JSONDecodeError as ex:\n                warnings.warn(\n                    f\"Ignoring environment variable {key} due to a parse failure: {ex}\"\n                )\n            else:\n                if isinstance(value, dict):\n                    _, proto = key.split(\"_\", 1)\n                    conf_dict.setdefault(proto.lower(), {}).update(value)\n                else:\n                    warnings.warn(\n                        f\"Ignoring environment variable {key} due to not being a dict:\"\n                        f\" {type(value)}\"\n                    )\n        elif key.startswith(\"FSSPEC\"):\n            warnings.warn(\n                f\"Ignoring environment variable {key} due to having an unexpected name\"\n            )\n\n    for key in kwarg_keys:\n        _, proto, kwarg = key.split(\"_\", 2)\n        conf_dict.setdefault(proto.lower(), {})[kwarg.lower()] = envdict[key]\n\n\ndef set_conf_files(cdir, conf_dict):\n    \"\"\"Set config values from files\n\n    Scans for INI and JSON files in the given dictionary, and uses their\n    contents to set the config. In case of repeated values, later values\n    win.\n\n    In the case of INI files, all values are strings, and these will not\n    be converted.\n\n    Parameters\n    ----------\n    cdir : str\n        Directory to search\n    conf_dict : dict(str, dict)\n        This dict will be mutated\n    \"\"\"\n    if not os.path.isdir(cdir):\n        return\n    allfiles = sorted(os.listdir(cdir))\n    for fn in allfiles:\n        if fn.endswith(\".ini\"):\n            ini = configparser.ConfigParser()\n            ini.read(os.path.join(cdir, fn))\n            for key in ini:\n                if key == \"DEFAULT\":\n                    continue\n                conf_dict.setdefault(key, {}).update(dict(ini[key]))\n        if fn.endswith(\".json\"):\n            with open(os.path.join(cdir, fn)) as f:\n                js = json.load(f)\n            for key in js:\n                conf_dict.setdefault(key, {}).update(dict(js[key]))\n\n\ndef apply_config(cls, kwargs, conf_dict=None):\n    \"\"\"Supply default values for kwargs when instantiating class\n\n    Augments the passed kwargs, by finding entries in the config dict\n    which match the classes ``.protocol`` attribute (one or more str)\n\n    Parameters\n    ----------\n    cls : file system implementation\n    kwargs : dict\n    conf_dict : dict of dict\n        Typically this is the global configuration\n\n    Returns\n    -------\n    dict : the modified set of kwargs\n    \"\"\"\n    if conf_dict is None:\n        conf_dict = conf\n    protos = cls.protocol if isinstance(cls.protocol, (tuple, list)) else [cls.protocol]\n    kw = {}\n    for proto in protos:\n        # default kwargs from the current state of the config\n        if proto in conf_dict:\n            kw.update(conf_dict[proto])\n    # explicit kwargs always win\n    kw.update(**kwargs)\n    kwargs = kw\n    return kwargs\n\n\nset_conf_files(conf_dir, conf)\nset_conf_env(conf)\n", "fsspec/asyn.py": "import asyncio\nimport asyncio.events\nimport functools\nimport inspect\nimport io\nimport numbers\nimport os\nimport re\nimport threading\nfrom contextlib import contextmanager\nfrom glob import has_magic\nfrom typing import TYPE_CHECKING, Iterable\n\nfrom .callbacks import DEFAULT_CALLBACK\nfrom .exceptions import FSTimeoutError\nfrom .implementations.local import LocalFileSystem, make_path_posix, trailing_sep\nfrom .spec import AbstractBufferedFile, AbstractFileSystem\nfrom .utils import glob_translate, is_exception, other_paths\n\nprivate = re.compile(\"_[^_]\")\niothread = [None]  # dedicated fsspec IO thread\nloop = [None]  # global event loop for any non-async instance\n_lock = None  # global lock placeholder\nget_running_loop = asyncio.get_running_loop\n\n\ndef get_lock():\n    \"\"\"Allocate or return a threading lock.\n\n    The lock is allocated on first use to allow setting one lock per forked process.\n    \"\"\"\n    global _lock\n    if not _lock:\n        _lock = threading.Lock()\n    return _lock\n\n\ndef reset_lock():\n    \"\"\"Reset the global lock.\n\n    This should be called only on the init of a forked process to reset the lock to\n    None, enabling the new forked process to get a new lock.\n    \"\"\"\n    global _lock\n\n    iothread[0] = None\n    loop[0] = None\n    _lock = None\n\n\nasync def _runner(event, coro, result, timeout=None):\n    timeout = timeout if timeout else None  # convert 0 or 0.0 to None\n    if timeout is not None:\n        coro = asyncio.wait_for(coro, timeout=timeout)\n    try:\n        result[0] = await coro\n    except Exception as ex:\n        result[0] = ex\n    finally:\n        event.set()\n\n\ndef sync(loop, func, *args, timeout=None, **kwargs):\n    \"\"\"\n    Make loop run coroutine until it returns. Runs in other thread\n\n    Examples\n    --------\n    >>> fsspec.asyn.sync(fsspec.asyn.get_loop(), func, *args,\n                         timeout=timeout, **kwargs)\n    \"\"\"\n    timeout = timeout if timeout else None  # convert 0 or 0.0 to None\n    # NB: if the loop is not running *yet*, it is OK to submit work\n    # and we will wait for it\n    if loop is None or loop.is_closed():\n        raise RuntimeError(\"Loop is not running\")\n    try:\n        loop0 = asyncio.events.get_running_loop()\n        if loop0 is loop:\n            raise NotImplementedError(\"Calling sync() from within a running loop\")\n    except NotImplementedError:\n        raise\n    except RuntimeError:\n        pass\n    coro = func(*args, **kwargs)\n    result = [None]\n    event = threading.Event()\n    asyncio.run_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n    while True:\n        # this loops allows thread to get interrupted\n        if event.wait(1):\n            break\n        if timeout is not None:\n            timeout -= 1\n            if timeout < 0:\n                raise FSTimeoutError\n\n    return_result = result[0]\n    if isinstance(return_result, asyncio.TimeoutError):\n        # suppress asyncio.TimeoutError, raise FSTimeoutError\n        raise FSTimeoutError from return_result\n    elif isinstance(return_result, BaseException):\n        raise return_result\n    else:\n        return return_result\n\n\ndef sync_wrapper(func, obj=None):\n    \"\"\"Given a function, make so can be called in blocking contexts\n\n    Leave obj=None if defining within a class. Pass the instance if attaching\n    as an attribute of the instance.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        self = obj or args[0]\n        return sync(self.loop, func, *args, **kwargs)\n\n    return wrapper\n\n\n@contextmanager\ndef _selector_policy():\n    original_policy = asyncio.get_event_loop_policy()\n    try:\n        if os.name == \"nt\" and hasattr(asyncio, \"WindowsSelectorEventLoopPolicy\"):\n            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\n        yield\n    finally:\n        asyncio.set_event_loop_policy(original_policy)\n\n\ndef get_loop():\n    \"\"\"Create or return the default fsspec IO loop\n\n    The loop will be running on a separate thread.\n    \"\"\"\n    if loop[0] is None:\n        with get_lock():\n            # repeat the check just in case the loop got filled between the\n            # previous two calls from another thread\n            if loop[0] is None:\n                with _selector_policy():\n                    loop[0] = asyncio.new_event_loop()\n                th = threading.Thread(target=loop[0].run_forever, name=\"fsspecIO\")\n                th.daemon = True\n                th.start()\n                iothread[0] = th\n    return loop[0]\n\n\nif TYPE_CHECKING:\n    import resource\n\n    ResourceError = resource.error\nelse:\n    try:\n        import resource\n    except ImportError:\n        resource = None\n        ResourceError = OSError\n    else:\n        ResourceError = getattr(resource, \"error\", OSError)\n\n_DEFAULT_BATCH_SIZE = 128\n_NOFILES_DEFAULT_BATCH_SIZE = 1280\n\n\ndef _get_batch_size(nofiles=False):\n    from fsspec.config import conf\n\n    if nofiles:\n        if \"nofiles_gather_batch_size\" in conf:\n            return conf[\"nofiles_gather_batch_size\"]\n    else:\n        if \"gather_batch_size\" in conf:\n            return conf[\"gather_batch_size\"]\n    if nofiles:\n        return _NOFILES_DEFAULT_BATCH_SIZE\n    if resource is None:\n        return _DEFAULT_BATCH_SIZE\n\n    try:\n        soft_limit, _ = resource.getrlimit(resource.RLIMIT_NOFILE)\n    except (ImportError, ValueError, ResourceError):\n        return _DEFAULT_BATCH_SIZE\n\n    if soft_limit == resource.RLIM_INFINITY:\n        return -1\n    else:\n        return soft_limit // 8\n\n\ndef running_async() -> bool:\n    \"\"\"Being executed by an event loop?\"\"\"\n    try:\n        asyncio.get_running_loop()\n        return True\n    except RuntimeError:\n        return False\n\n\nasync def _run_coros_in_chunks(\n    coros,\n    batch_size=None,\n    callback=DEFAULT_CALLBACK,\n    timeout=None,\n    return_exceptions=False,\n    nofiles=False,\n):\n    \"\"\"Run the given coroutines in  chunks.\n\n    Parameters\n    ----------\n    coros: list of coroutines to run\n    batch_size: int or None\n        Number of coroutines to submit/wait on simultaneously.\n        If -1, then it will not be any throttling. If\n        None, it will be inferred from _get_batch_size()\n    callback: fsspec.callbacks.Callback instance\n        Gets a relative_update when each coroutine completes\n    timeout: number or None\n        If given, each coroutine times out after this time. Note that, since\n        there are multiple batches, the total run time of this function will in\n        general be longer\n    return_exceptions: bool\n        Same meaning as in asyncio.gather\n    nofiles: bool\n        If inferring the batch_size, does this operation involve local files?\n        If yes, you normally expect smaller batches.\n    \"\"\"\n\n    if batch_size is None:\n        batch_size = _get_batch_size(nofiles=nofiles)\n\n    if batch_size == -1:\n        batch_size = len(coros)\n\n    assert batch_size > 0\n\n    async def _run_coro(coro, i):\n        try:\n            return await asyncio.wait_for(coro, timeout=timeout), i\n        except Exception as e:\n            if not return_exceptions:\n                raise\n            return e, i\n        finally:\n            callback.relative_update(1)\n\n    i = 0\n    n = len(coros)\n    results = [None] * n\n    pending = set()\n\n    while pending or i < n:\n        while len(pending) < batch_size and i < n:\n            pending.add(asyncio.ensure_future(_run_coro(coros[i], i)))\n            i += 1\n\n        if not pending:\n            break\n\n        done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n        while done:\n            result, k = await done.pop()\n            results[k] = result\n\n    return results\n\n\n# these methods should be implemented as async by any async-able backend\nasync_methods = [\n    \"_ls\",\n    \"_cat_file\",\n    \"_get_file\",\n    \"_put_file\",\n    \"_rm_file\",\n    \"_cp_file\",\n    \"_pipe_file\",\n    \"_expand_path\",\n    \"_info\",\n    \"_isfile\",\n    \"_isdir\",\n    \"_exists\",\n    \"_walk\",\n    \"_glob\",\n    \"_find\",\n    \"_du\",\n    \"_size\",\n    \"_mkdir\",\n    \"_makedirs\",\n]\n\n\nclass AsyncFileSystem(AbstractFileSystem):\n    \"\"\"Async file operations, default implementations\n\n    Passes bulk operations to asyncio.gather for concurrent operation.\n\n    Implementations that have concurrent batch operations and/or async methods\n    should inherit from this class instead of AbstractFileSystem. Docstrings are\n    copied from the un-underscored method in AbstractFileSystem, if not given.\n    \"\"\"\n\n    # note that methods do not have docstring here; they will be copied\n    # for _* methods and inferred for overridden methods.\n\n    async_impl = True\n    mirror_sync_methods = True\n    disable_throttling = False\n\n    def __init__(self, *args, asynchronous=False, loop=None, batch_size=None, **kwargs):\n        self.asynchronous = asynchronous\n        self._pid = os.getpid()\n        if not asynchronous:\n            self._loop = loop or get_loop()\n        else:\n            self._loop = None\n        self.batch_size = batch_size\n        super().__init__(*args, **kwargs)\n\n    @property\n    def loop(self):\n        if self._pid != os.getpid():\n            raise RuntimeError(\"This class is not fork-safe\")\n        return self._loop\n\n    async def _rm_file(self, path, **kwargs):\n        raise NotImplementedError\n\n    async def _rm(self, path, recursive=False, batch_size=None, **kwargs):\n        # TODO: implement on_error\n        batch_size = batch_size or self.batch_size\n        path = await self._expand_path(path, recursive=recursive)\n        return await _run_coros_in_chunks(\n            [self._rm_file(p, **kwargs) for p in reversed(path)],\n            batch_size=batch_size,\n            nofiles=True,\n        )\n\n    async def _cp_file(self, path1, path2, **kwargs):\n        raise NotImplementedError\n\n    async def _copy(\n        self,\n        path1,\n        path2,\n        recursive=False,\n        on_error=None,\n        maxdepth=None,\n        batch_size=None,\n        **kwargs,\n    ):\n        if on_error is None and recursive:\n            on_error = \"ignore\"\n        elif on_error is None:\n            on_error = \"raise\"\n\n        if isinstance(path1, list) and isinstance(path2, list):\n            # No need to expand paths when both source and destination\n            # are provided as lists\n            paths1 = path1\n            paths2 = path2\n        else:\n            source_is_str = isinstance(path1, str)\n            paths1 = await self._expand_path(\n                path1, maxdepth=maxdepth, recursive=recursive\n            )\n            if source_is_str and (not recursive or maxdepth is not None):\n                # Non-recursive glob does not copy directories\n                paths1 = [\n                    p for p in paths1 if not (trailing_sep(p) or await self._isdir(p))\n                ]\n                if not paths1:\n                    return\n\n            source_is_file = len(paths1) == 1\n            dest_is_dir = isinstance(path2, str) and (\n                trailing_sep(path2) or await self._isdir(path2)\n            )\n\n            exists = source_is_str and (\n                (has_magic(path1) and source_is_file)\n                or (not has_magic(path1) and dest_is_dir and not trailing_sep(path1))\n            )\n            paths2 = other_paths(\n                paths1,\n                path2,\n                exists=exists,\n                flatten=not source_is_str,\n            )\n\n        batch_size = batch_size or self.batch_size\n        coros = [self._cp_file(p1, p2, **kwargs) for p1, p2 in zip(paths1, paths2)]\n        result = await _run_coros_in_chunks(\n            coros, batch_size=batch_size, return_exceptions=True, nofiles=True\n        )\n\n        for ex in filter(is_exception, result):\n            if on_error == \"ignore\" and isinstance(ex, FileNotFoundError):\n                continue\n            raise ex\n\n    async def _pipe_file(self, path, value, **kwargs):\n        raise NotImplementedError\n\n    async def _pipe(self, path, value=None, batch_size=None, **kwargs):\n        if isinstance(path, str):\n            path = {path: value}\n        batch_size = batch_size or self.batch_size\n        return await _run_coros_in_chunks(\n            [self._pipe_file(k, v, **kwargs) for k, v in path.items()],\n            batch_size=batch_size,\n            nofiles=True,\n        )\n\n    async def _process_limits(self, url, start, end):\n        \"\"\"Helper for \"Range\"-based _cat_file\"\"\"\n        size = None\n        suff = False\n        if start is not None and start < 0:\n            # if start is negative and end None, end is the \"suffix length\"\n            if end is None:\n                end = -start\n                start = \"\"\n                suff = True\n            else:\n                size = size or (await self._info(url))[\"size\"]\n                start = size + start\n        elif start is None:\n            start = 0\n        if not suff:\n            if end is not None and end < 0:\n                if start is not None:\n                    size = size or (await self._info(url))[\"size\"]\n                    end = size + end\n            elif end is None:\n                end = \"\"\n            if isinstance(end, numbers.Integral):\n                end -= 1  # bytes range is inclusive\n        return f\"bytes={start}-{end}\"\n\n    async def _cat_file(self, path, start=None, end=None, **kwargs):\n        raise NotImplementedError\n\n    async def _cat(\n        self, path, recursive=False, on_error=\"raise\", batch_size=None, **kwargs\n    ):\n        paths = await self._expand_path(path, recursive=recursive)\n        coros = [self._cat_file(path, **kwargs) for path in paths]\n        batch_size = batch_size or self.batch_size\n        out = await _run_coros_in_chunks(\n            coros, batch_size=batch_size, nofiles=True, return_exceptions=True\n        )\n        if on_error == \"raise\":\n            ex = next(filter(is_exception, out), False)\n            if ex:\n                raise ex\n        if (\n            len(paths) > 1\n            or isinstance(path, list)\n            or paths[0] != self._strip_protocol(path)\n        ):\n            return {\n                k: v\n                for k, v in zip(paths, out)\n                if on_error != \"omit\" or not is_exception(v)\n            }\n        else:\n            return out[0]\n\n    async def _cat_ranges(\n        self,\n        paths,\n        starts,\n        ends,\n        max_gap=None,\n        batch_size=None,\n        on_error=\"return\",\n        **kwargs,\n    ):\n        \"\"\"Get the contents of byte ranges from one or more files\n\n        Parameters\n        ----------\n        paths: list\n            A list of of filepaths on this filesystems\n        starts, ends: int or list\n            Bytes limits of the read. If using a single int, the same value will be\n            used to read all the specified files.\n        \"\"\"\n        # TODO: on_error\n        if max_gap is not None:\n            # use utils.merge_offset_ranges\n            raise NotImplementedError\n        if not isinstance(paths, list):\n            raise TypeError\n        if not isinstance(starts, Iterable):\n            starts = [starts] * len(paths)\n        if not isinstance(ends, Iterable):\n            ends = [ends] * len(paths)\n        if len(starts) != len(paths) or len(ends) != len(paths):\n            raise ValueError\n        coros = [\n            self._cat_file(p, start=s, end=e, **kwargs)\n            for p, s, e in zip(paths, starts, ends)\n        ]\n        batch_size = batch_size or self.batch_size\n        return await _run_coros_in_chunks(\n            coros, batch_size=batch_size, nofiles=True, return_exceptions=True\n        )\n\n    async def _put_file(self, lpath, rpath, **kwargs):\n        raise NotImplementedError\n\n    async def _put(\n        self,\n        lpath,\n        rpath,\n        recursive=False,\n        callback=DEFAULT_CALLBACK,\n        batch_size=None,\n        maxdepth=None,\n        **kwargs,\n    ):\n        \"\"\"Copy file(s) from local.\n\n        Copies a specific file or tree of files (if recursive=True). If rpath\n        ends with a \"/\", it will be assumed to be a directory, and target files\n        will go within.\n\n        The put_file method will be called concurrently on a batch of files. The\n        batch_size option can configure the amount of futures that can be executed\n        at the same time. If it is -1, then all the files will be uploaded concurrently.\n        The default can be set for this instance by passing \"batch_size\" in the\n        constructor, or for all instances by setting the \"gather_batch_size\" key\n        in ``fsspec.config.conf``, falling back to 1/8th of the system limit .\n        \"\"\"\n        if isinstance(lpath, list) and isinstance(rpath, list):\n            # No need to expand paths when both source and destination\n            # are provided as lists\n            rpaths = rpath\n            lpaths = lpath\n        else:\n            source_is_str = isinstance(lpath, str)\n            if source_is_str:\n                lpath = make_path_posix(lpath)\n            fs = LocalFileSystem()\n            lpaths = fs.expand_path(lpath, recursive=recursive, maxdepth=maxdepth)\n            if source_is_str and (not recursive or maxdepth is not None):\n                # Non-recursive glob does not copy directories\n                lpaths = [p for p in lpaths if not (trailing_sep(p) or fs.isdir(p))]\n                if not lpaths:\n                    return\n\n            source_is_file = len(lpaths) == 1\n            dest_is_dir = isinstance(rpath, str) and (\n                trailing_sep(rpath) or await self._isdir(rpath)\n            )\n\n            rpath = self._strip_protocol(rpath)\n            exists = source_is_str and (\n                (has_magic(lpath) and source_is_file)\n                or (not has_magic(lpath) and dest_is_dir and not trailing_sep(lpath))\n            )\n            rpaths = other_paths(\n                lpaths,\n                rpath,\n                exists=exists,\n                flatten=not source_is_str,\n            )\n\n        is_dir = {l: os.path.isdir(l) for l in lpaths}\n        rdirs = [r for l, r in zip(lpaths, rpaths) if is_dir[l]]\n        file_pairs = [(l, r) for l, r in zip(lpaths, rpaths) if not is_dir[l]]\n\n        await asyncio.gather(*[self._makedirs(d, exist_ok=True) for d in rdirs])\n        batch_size = batch_size or self.batch_size\n\n        coros = []\n        callback.set_size(len(file_pairs))\n        for lfile, rfile in file_pairs:\n            put_file = callback.branch_coro(self._put_file)\n            coros.append(put_file(lfile, rfile, **kwargs))\n\n        return await _run_coros_in_chunks(\n            coros, batch_size=batch_size, callback=callback\n        )\n\n    async def _get_file(self, rpath, lpath, **kwargs):\n        raise NotImplementedError\n\n    async def _get(\n        self,\n        rpath,\n        lpath,\n        recursive=False,\n        callback=DEFAULT_CALLBACK,\n        maxdepth=None,\n        **kwargs,\n    ):\n        \"\"\"Copy file(s) to local.\n\n        Copies a specific file or tree of files (if recursive=True). If lpath\n        ends with a \"/\", it will be assumed to be a directory, and target files\n        will go within. Can submit a list of paths, which may be glob-patterns\n        and will be expanded.\n\n        The get_file method will be called concurrently on a batch of files. The\n        batch_size option can configure the amount of futures that can be executed\n        at the same time. If it is -1, then all the files will be uploaded concurrently.\n        The default can be set for this instance by passing \"batch_size\" in the\n        constructor, or for all instances by setting the \"gather_batch_size\" key\n        in ``fsspec.config.conf``, falling back to 1/8th of the system limit .\n        \"\"\"\n        if isinstance(lpath, list) and isinstance(rpath, list):\n            # No need to expand paths when both source and destination\n            # are provided as lists\n            rpaths = rpath\n            lpaths = lpath\n        else:\n            source_is_str = isinstance(rpath, str)\n            # First check for rpath trailing slash as _strip_protocol removes it.\n            source_not_trailing_sep = source_is_str and not trailing_sep(rpath)\n            rpath = self._strip_protocol(rpath)\n            rpaths = await self._expand_path(\n                rpath, recursive=recursive, maxdepth=maxdepth\n            )\n            if source_is_str and (not recursive or maxdepth is not None):\n                # Non-recursive glob does not copy directories\n                rpaths = [\n                    p for p in rpaths if not (trailing_sep(p) or await self._isdir(p))\n                ]\n                if not rpaths:\n                    return\n\n            lpath = make_path_posix(lpath)\n            source_is_file = len(rpaths) == 1\n            dest_is_dir = isinstance(lpath, str) and (\n                trailing_sep(lpath) or LocalFileSystem().isdir(lpath)\n            )\n\n            exists = source_is_str and (\n                (has_magic(rpath) and source_is_file)\n                or (not has_magic(rpath) and dest_is_dir and source_not_trailing_sep)\n            )\n            lpaths = other_paths(\n                rpaths,\n                lpath,\n                exists=exists,\n                flatten=not source_is_str,\n            )\n\n        [os.makedirs(os.path.dirname(lp), exist_ok=True) for lp in lpaths]\n        batch_size = kwargs.pop(\"batch_size\", self.batch_size)\n\n        coros = []\n        callback.set_size(len(lpaths))\n        for lpath, rpath in zip(lpaths, rpaths):\n            get_file = callback.branch_coro(self._get_file)\n            coros.append(get_file(rpath, lpath, **kwargs))\n        return await _run_coros_in_chunks(\n            coros, batch_size=batch_size, callback=callback\n        )\n\n    async def _isfile(self, path):\n        try:\n            return (await self._info(path))[\"type\"] == \"file\"\n        except:  # noqa: E722\n            return False\n\n    async def _isdir(self, path):\n        try:\n            return (await self._info(path))[\"type\"] == \"directory\"\n        except OSError:\n            return False\n\n    async def _size(self, path):\n        return (await self._info(path)).get(\"size\", None)\n\n    async def _sizes(self, paths, batch_size=None):\n        batch_size = batch_size or self.batch_size\n        return await _run_coros_in_chunks(\n            [self._size(p) for p in paths], batch_size=batch_size\n        )\n\n    async def _exists(self, path, **kwargs):\n        try:\n            await self._info(path, **kwargs)\n            return True\n        except FileNotFoundError:\n            return False\n\n    async def _info(self, path, **kwargs):\n        raise NotImplementedError\n\n    async def _ls(self, path, detail=True, **kwargs):\n        raise NotImplementedError\n\n    async def _walk(self, path, maxdepth=None, on_error=\"omit\", **kwargs):\n        if maxdepth is not None and maxdepth < 1:\n            raise ValueError(\"maxdepth must be at least 1\")\n\n        path = self._strip_protocol(path)\n        full_dirs = {}\n        dirs = {}\n        files = {}\n\n        detail = kwargs.pop(\"detail\", False)\n        try:\n            listing = await self._ls(path, detail=True, **kwargs)\n        except (FileNotFoundError, OSError) as e:\n            if on_error == \"raise\":\n                raise\n            elif callable(on_error):\n                on_error(e)\n            if detail:\n                yield path, {}, {}\n            else:\n                yield path, [], []\n            return\n\n        for info in listing:\n            # each info name must be at least [path]/part , but here\n            # we check also for names like [path]/part/\n            pathname = info[\"name\"].rstrip(\"/\")\n            name = pathname.rsplit(\"/\", 1)[-1]\n            if info[\"type\"] == \"directory\" and pathname != path:\n                # do not include \"self\" path\n                full_dirs[name] = pathname\n                dirs[name] = info\n            elif pathname == path:\n                # file-like with same name as give path\n                files[\"\"] = info\n            else:\n                files[name] = info\n\n        if detail:\n            yield path, dirs, files\n        else:\n            yield path, list(dirs), list(files)\n\n        if maxdepth is not None:\n            maxdepth -= 1\n            if maxdepth < 1:\n                return\n\n        for d in dirs:\n            async for _ in self._walk(\n                full_dirs[d], maxdepth=maxdepth, detail=detail, **kwargs\n            ):\n                yield _\n\n    async def _glob(self, path, maxdepth=None, **kwargs):\n        if maxdepth is not None and maxdepth < 1:\n            raise ValueError(\"maxdepth must be at least 1\")\n\n        import re\n\n        seps = (os.path.sep, os.path.altsep) if os.path.altsep else (os.path.sep,)\n        ends_with_sep = path.endswith(seps)  # _strip_protocol strips trailing slash\n        path = self._strip_protocol(path)\n        append_slash_to_dirname = ends_with_sep or path.endswith(\n            tuple(sep + \"**\" for sep in seps)\n        )\n        idx_star = path.find(\"*\") if path.find(\"*\") >= 0 else len(path)\n        idx_qmark = path.find(\"?\") if path.find(\"?\") >= 0 else len(path)\n        idx_brace = path.find(\"[\") if path.find(\"[\") >= 0 else len(path)\n\n        min_idx = min(idx_star, idx_qmark, idx_brace)\n\n        detail = kwargs.pop(\"detail\", False)\n\n        if not has_magic(path):\n            if await self._exists(path, **kwargs):\n                if not detail:\n                    return [path]\n                else:\n                    return {path: await self._info(path, **kwargs)}\n            else:\n                if not detail:\n                    return []  # glob of non-existent returns empty\n                else:\n                    return {}\n        elif \"/\" in path[:min_idx]:\n            min_idx = path[:min_idx].rindex(\"/\")\n            root = path[: min_idx + 1]\n            depth = path[min_idx + 1 :].count(\"/\") + 1\n        else:\n            root = \"\"\n            depth = path[min_idx + 1 :].count(\"/\") + 1\n\n        if \"**\" in path:\n            if maxdepth is not None:\n                idx_double_stars = path.find(\"**\")\n                depth_double_stars = path[idx_double_stars:].count(\"/\") + 1\n                depth = depth - depth_double_stars + maxdepth\n            else:\n                depth = None\n\n        allpaths = await self._find(\n            root, maxdepth=depth, withdirs=True, detail=True, **kwargs\n        )\n\n        pattern = glob_translate(path + (\"/\" if ends_with_sep else \"\"))\n        pattern = re.compile(pattern)\n\n        out = {\n            p: info\n            for p, info in sorted(allpaths.items())\n            if pattern.match(\n                (\n                    p + \"/\"\n                    if append_slash_to_dirname and info[\"type\"] == \"directory\"\n                    else p\n                )\n            )\n        }\n\n        if detail:\n            return out\n        else:\n            return list(out)\n\n    async def _du(self, path, total=True, maxdepth=None, **kwargs):\n        sizes = {}\n        # async for?\n        for f in await self._find(path, maxdepth=maxdepth, **kwargs):\n            info = await self._info(f)\n            sizes[info[\"name\"]] = info[\"size\"]\n        if total:\n            return sum(sizes.values())\n        else:\n            return sizes\n\n    async def _find(self, path, maxdepth=None, withdirs=False, **kwargs):\n        path = self._strip_protocol(path)\n        out = {}\n        detail = kwargs.pop(\"detail\", False)\n\n        # Add the root directory if withdirs is requested\n        # This is needed for posix glob compliance\n        if withdirs and path != \"\" and await self._isdir(path):\n            out[path] = await self._info(path)\n\n        # async for?\n        async for _, dirs, files in self._walk(path, maxdepth, detail=True, **kwargs):\n            if withdirs:\n                files.update(dirs)\n            out.update({info[\"name\"]: info for name, info in files.items()})\n        if not out and (await self._isfile(path)):\n            # walk works on directories, but find should also return [path]\n            # when path happens to be a file\n            out[path] = {}\n        names = sorted(out)\n        if not detail:\n            return names\n        else:\n            return {name: out[name] for name in names}\n\n    async def _expand_path(self, path, recursive=False, maxdepth=None):\n        if maxdepth is not None and maxdepth < 1:\n            raise ValueError(\"maxdepth must be at least 1\")\n\n        if isinstance(path, str):\n            out = await self._expand_path([path], recursive, maxdepth)\n        else:\n            out = set()\n            path = [self._strip_protocol(p) for p in path]\n            for p in path:  # can gather here\n                if has_magic(p):\n                    bit = set(await self._glob(p, maxdepth=maxdepth))\n                    out |= bit\n                    if recursive:\n                        # glob call above expanded one depth so if maxdepth is defined\n                        # then decrement it in expand_path call below. If it is zero\n                        # after decrementing then avoid expand_path call.\n                        if maxdepth is not None and maxdepth <= 1:\n                            continue\n                        out |= set(\n                            await self._expand_path(\n                                list(bit),\n                                recursive=recursive,\n                                maxdepth=maxdepth - 1 if maxdepth is not None else None,\n                            )\n                        )\n                    continue\n                elif recursive:\n                    rec = set(await self._find(p, maxdepth=maxdepth, withdirs=True))\n                    out |= rec\n                if p not in out and (recursive is False or (await self._exists(p))):\n                    # should only check once, for the root\n                    out.add(p)\n        if not out:\n            raise FileNotFoundError(path)\n        return sorted(out)\n\n    async def _mkdir(self, path, create_parents=True, **kwargs):\n        pass  # not necessary to implement, may not have directories\n\n    async def _makedirs(self, path, exist_ok=False):\n        pass  # not necessary to implement, may not have directories\n\n    async def open_async(self, path, mode=\"rb\", **kwargs):\n        if \"b\" not in mode or kwargs.get(\"compression\"):\n            raise ValueError\n        raise NotImplementedError\n\n\ndef mirror_sync_methods(obj):\n    \"\"\"Populate sync and async methods for obj\n\n    For each method will create a sync version if the name refers to an async method\n    (coroutine) and there is no override in the child class; will create an async\n    method for the corresponding sync method if there is no implementation.\n\n    Uses the methods specified in\n    - async_methods: the set that an implementation is expected to provide\n    - default_async_methods: that can be derived from their sync version in\n      AbstractFileSystem\n    - AsyncFileSystem: async-specific default coroutines\n    \"\"\"\n    from fsspec import AbstractFileSystem\n\n    for method in async_methods + dir(AsyncFileSystem):\n        if not method.startswith(\"_\"):\n            continue\n        smethod = method[1:]\n        if private.match(method):\n            isco = inspect.iscoroutinefunction(getattr(obj, method, None))\n            unsync = getattr(getattr(obj, smethod, False), \"__func__\", None)\n            is_default = unsync is getattr(AbstractFileSystem, smethod, \"\")\n            if isco and is_default:\n                mth = sync_wrapper(getattr(obj, method), obj=obj)\n                setattr(obj, smethod, mth)\n                if not mth.__doc__:\n                    mth.__doc__ = getattr(\n                        getattr(AbstractFileSystem, smethod, None), \"__doc__\", \"\"\n                    )\n\n\nclass FSSpecCoroutineCancel(Exception):\n    pass\n\n\ndef _dump_running_tasks(\n    printout=True, cancel=True, exc=FSSpecCoroutineCancel, with_task=False\n):\n    import traceback\n\n    tasks = [t for t in asyncio.tasks.all_tasks(loop[0]) if not t.done()]\n    if printout:\n        [task.print_stack() for task in tasks]\n    out = [\n        {\n            \"locals\": task._coro.cr_frame.f_locals,\n            \"file\": task._coro.cr_frame.f_code.co_filename,\n            \"firstline\": task._coro.cr_frame.f_code.co_firstlineno,\n            \"linelo\": task._coro.cr_frame.f_lineno,\n            \"stack\": traceback.format_stack(task._coro.cr_frame),\n            \"task\": task if with_task else None,\n        }\n        for task in tasks\n    ]\n    if cancel:\n        for t in tasks:\n            cbs = t._callbacks\n            t.cancel()\n            asyncio.futures.Future.set_exception(t, exc)\n            asyncio.futures.Future.cancel(t)\n            [cb[0](t) for cb in cbs]  # cancels any dependent concurrent.futures\n            try:\n                t._coro.throw(exc)  # exits coro, unless explicitly handled\n            except exc:\n                pass\n    return out\n\n\nclass AbstractAsyncStreamedFile(AbstractBufferedFile):\n    # no read buffering, and always auto-commit\n    # TODO: readahead might still be useful here, but needs async version\n\n    async def read(self, length=-1):\n        \"\"\"\n        Return data from cache, or fetch pieces as necessary\n\n        Parameters\n        ----------\n        length: int (-1)\n            Number of bytes to read; if <0, all remaining bytes.\n        \"\"\"\n        length = -1 if length is None else int(length)\n        if self.mode != \"rb\":\n            raise ValueError(\"File not in read mode\")\n        if length < 0:\n            length = self.size - self.loc\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file.\")\n        if length == 0:\n            # don't even bother calling fetch\n            return b\"\"\n        out = await self._fetch_range(self.loc, self.loc + length)\n        self.loc += len(out)\n        return out\n\n    async def write(self, data):\n        \"\"\"\n        Write data to buffer.\n\n        Buffer only sent on flush() or if buffer is greater than\n        or equal to blocksize.\n\n        Parameters\n        ----------\n        data: bytes\n            Set of bytes to be written.\n        \"\"\"\n        if self.mode not in {\"wb\", \"ab\"}:\n            raise ValueError(\"File not in write mode\")\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file.\")\n        if self.forced:\n            raise ValueError(\"This file has been force-flushed, can only close\")\n        out = self.buffer.write(data)\n        self.loc += out\n        if self.buffer.tell() >= self.blocksize:\n            await self.flush()\n        return out\n\n    async def close(self):\n        \"\"\"Close file\n\n        Finalizes writes, discards cache\n        \"\"\"\n        if getattr(self, \"_unclosable\", False):\n            return\n        if self.closed:\n            return\n        if self.mode == \"rb\":\n            self.cache = None\n        else:\n            if not self.forced:\n                await self.flush(force=True)\n\n            if self.fs is not None:\n                self.fs.invalidate_cache(self.path)\n                self.fs.invalidate_cache(self.fs._parent(self.path))\n\n        self.closed = True\n\n    async def flush(self, force=False):\n        if self.closed:\n            raise ValueError(\"Flush on closed file\")\n        if force and self.forced:\n            raise ValueError(\"Force flush cannot be called more than once\")\n        if force:\n            self.forced = True\n\n        if self.mode not in {\"wb\", \"ab\"}:\n            # no-op to flush on read-mode\n            return\n\n        if not force and self.buffer.tell() < self.blocksize:\n            # Defer write on small block\n            return\n\n        if self.offset is None:\n            # Initialize a multipart upload\n            self.offset = 0\n            try:\n                await self._initiate_upload()\n            except:  # noqa: E722\n                self.closed = True\n                raise\n\n        if await self._upload_chunk(final=force) is not False:\n            self.offset += self.buffer.seek(0, 2)\n            self.buffer = io.BytesIO()\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close()\n\n    async def _fetch_range(self, start, end):\n        raise NotImplementedError\n\n    async def _initiate_upload(self):\n        pass\n\n    async def _upload_chunk(self, final=False):\n        raise NotImplementedError\n", "fsspec/core.py": "from __future__ import annotations\n\nimport io\nimport logging\nimport os\nimport re\nfrom glob import has_magic\nfrom pathlib import Path\n\n# for backwards compat, we export cache things from here too\nfrom fsspec.caching import (  # noqa: F401\n    BaseCache,\n    BlockCache,\n    BytesCache,\n    MMapCache,\n    ReadAheadCache,\n    caches,\n)\nfrom fsspec.compression import compr\nfrom fsspec.config import conf\nfrom fsspec.registry import filesystem, get_filesystem_class\nfrom fsspec.utils import (\n    _unstrip_protocol,\n    build_name_function,\n    infer_compression,\n    stringify_path,\n)\n\nlogger = logging.getLogger(\"fsspec\")\n\n\nclass OpenFile:\n    \"\"\"\n    File-like object to be used in a context\n\n    Can layer (buffered) text-mode and compression over any file-system, which\n    are typically binary-only.\n\n    These instances are safe to serialize, as the low-level file object\n    is not created until invoked using ``with``.\n\n    Parameters\n    ----------\n    fs: FileSystem\n        The file system to use for opening the file. Should be a subclass or duck-type\n        with ``fsspec.spec.AbstractFileSystem``\n    path: str\n        Location to open\n    mode: str like 'rb', optional\n        Mode of the opened file\n    compression: str or None, optional\n        Compression to apply\n    encoding: str or None, optional\n        The encoding to use if opened in text mode.\n    errors: str or None, optional\n        How to handle encoding errors if opened in text mode.\n    newline: None or str\n        Passed to TextIOWrapper in text mode, how to handle line endings.\n    autoopen: bool\n        If True, calls open() immediately. Mostly used by pickle\n    pos: int\n        If given and autoopen is True, seek to this location immediately\n    \"\"\"\n\n    def __init__(\n        self,\n        fs,\n        path,\n        mode=\"rb\",\n        compression=None,\n        encoding=None,\n        errors=None,\n        newline=None,\n    ):\n        self.fs = fs\n        self.path = path\n        self.mode = mode\n        self.compression = get_compression(path, compression)\n        self.encoding = encoding\n        self.errors = errors\n        self.newline = newline\n        self.fobjects = []\n\n    def __reduce__(self):\n        return (\n            OpenFile,\n            (\n                self.fs,\n                self.path,\n                self.mode,\n                self.compression,\n                self.encoding,\n                self.errors,\n                self.newline,\n            ),\n        )\n\n    def __repr__(self):\n        return f\"<OpenFile '{self.path}'>\"\n\n    def __enter__(self):\n        mode = self.mode.replace(\"t\", \"\").replace(\"b\", \"\") + \"b\"\n\n        try:\n            f = self.fs.open(self.path, mode=mode)\n        except FileNotFoundError as e:\n            if has_magic(self.path):\n                raise FileNotFoundError(\n                    \"%s not found. The URL contains glob characters: you maybe needed\\n\"\n                    \"to pass expand=True in fsspec.open() or the storage_options of \\n\"\n                    \"your library. You can also set the config value 'open_expand'\\n\"\n                    \"before import, or fsspec.core.DEFAULT_EXPAND at runtime, to True.\",\n                    self.path,\n                ) from e\n            raise\n\n        self.fobjects = [f]\n\n        if self.compression is not None:\n            compress = compr[self.compression]\n            f = compress(f, mode=mode[0])\n            self.fobjects.append(f)\n\n        if \"b\" not in self.mode:\n            # assume, for example, that 'r' is equivalent to 'rt' as in builtin\n            f = PickleableTextIOWrapper(\n                f, encoding=self.encoding, errors=self.errors, newline=self.newline\n            )\n            self.fobjects.append(f)\n\n        return self.fobjects[-1]\n\n    def __exit__(self, *args):\n        self.close()\n\n    @property\n    def full_name(self):\n        return _unstrip_protocol(self.path, self.fs)\n\n    def open(self):\n        \"\"\"Materialise this as a real open file without context\n\n        The OpenFile object should be explicitly closed to avoid enclosed file\n        instances persisting. You must, therefore, keep a reference to the OpenFile\n        during the life of the file-like it generates.\n        \"\"\"\n        return self.__enter__()\n\n    def close(self):\n        \"\"\"Close all encapsulated file objects\"\"\"\n        for f in reversed(self.fobjects):\n            if \"r\" not in self.mode and not f.closed:\n                f.flush()\n            f.close()\n        self.fobjects.clear()\n\n\nclass OpenFiles(list):\n    \"\"\"List of OpenFile instances\n\n    Can be used in a single context, which opens and closes all of the\n    contained files. Normal list access to get the elements works as\n    normal.\n\n    A special case is made for caching filesystems - the files will\n    be down/uploaded together at the start or end of the context, and\n    this may happen concurrently, if the target filesystem supports it.\n    \"\"\"\n\n    def __init__(self, *args, mode=\"rb\", fs=None):\n        self.mode = mode\n        self.fs = fs\n        self.files = []\n        super().__init__(*args)\n\n    def __enter__(self):\n        if self.fs is None:\n            raise ValueError(\"Context has already been used\")\n\n        fs = self.fs\n        while True:\n            if hasattr(fs, \"open_many\"):\n                # check for concurrent cache download; or set up for upload\n                self.files = fs.open_many(self)\n                return self.files\n            if hasattr(fs, \"fs\") and fs.fs is not None:\n                fs = fs.fs\n            else:\n                break\n        return [s.__enter__() for s in self]\n\n    def __exit__(self, *args):\n        fs = self.fs\n        [s.__exit__(*args) for s in self]\n        if \"r\" not in self.mode:\n            while True:\n                if hasattr(fs, \"open_many\"):\n                    # check for concurrent cache upload\n                    fs.commit_many(self.files)\n                    return\n                if hasattr(fs, \"fs\") and fs.fs is not None:\n                    fs = fs.fs\n                else:\n                    break\n\n    def __getitem__(self, item):\n        out = super().__getitem__(item)\n        if isinstance(item, slice):\n            return OpenFiles(out, mode=self.mode, fs=self.fs)\n        return out\n\n    def __repr__(self):\n        return f\"<List of {len(self)} OpenFile instances>\"\n\n\ndef open_files(\n    urlpath,\n    mode=\"rb\",\n    compression=None,\n    encoding=\"utf8\",\n    errors=None,\n    name_function=None,\n    num=1,\n    protocol=None,\n    newline=None,\n    auto_mkdir=True,\n    expand=True,\n    **kwargs,\n):\n    \"\"\"Given a path or paths, return a list of ``OpenFile`` objects.\n\n    For writing, a str path must contain the \"*\" character, which will be filled\n    in by increasing numbers, e.g., \"part*\" ->  \"part1\", \"part2\" if num=2.\n\n    For either reading or writing, can instead provide explicit list of paths.\n\n    Parameters\n    ----------\n    urlpath: string or list\n        Absolute or relative filepath(s). Prefix with a protocol like ``s3://``\n        to read from alternative filesystems. To read from multiple files you\n        can pass a globstring or a list of paths, with the caveat that they\n        must all have the same protocol.\n    mode: 'rb', 'wt', etc.\n    compression: string or None\n        If given, open file using compression codec. Can either be a compression\n        name (a key in ``fsspec.compression.compr``) or \"infer\" to guess the\n        compression from the filename suffix.\n    encoding: str\n        For text mode only\n    errors: None or str\n        Passed to TextIOWrapper in text mode\n    name_function: function or None\n        if opening a set of files for writing, those files do not yet exist,\n        so we need to generate their names by formatting the urlpath for\n        each sequence number\n    num: int [1]\n        if writing mode, number of files we expect to create (passed to\n        name+function)\n    protocol: str or None\n        If given, overrides the protocol found in the URL.\n    newline: bytes or None\n        Used for line terminator in text mode. If None, uses system default;\n        if blank, uses no translation.\n    auto_mkdir: bool (True)\n        If in write mode, this will ensure the target directory exists before\n        writing, by calling ``fs.mkdirs(exist_ok=True)``.\n    expand: bool\n    **kwargs: dict\n        Extra options that make sense to a particular storage connection, e.g.\n        host, port, username, password, etc.\n\n    Examples\n    --------\n    >>> files = open_files('2015-*-*.csv')  # doctest: +SKIP\n    >>> files = open_files(\n    ...     's3://bucket/2015-*-*.csv.gz', compression='gzip'\n    ... )  # doctest: +SKIP\n\n    Returns\n    -------\n    An ``OpenFiles`` instance, which is a list of ``OpenFile`` objects that can\n    be used as a single context\n\n    Notes\n    -----\n    For a full list of the available protocols and the implementations that\n    they map across to see the latest online documentation:\n\n    - For implementations built into ``fsspec`` see\n      https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations\n    - For implementations in separate packages see\n      https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations\n    \"\"\"\n    fs, fs_token, paths = get_fs_token_paths(\n        urlpath,\n        mode,\n        num=num,\n        name_function=name_function,\n        storage_options=kwargs,\n        protocol=protocol,\n        expand=expand,\n    )\n    if fs.protocol == \"file\":\n        fs.auto_mkdir = auto_mkdir\n    elif \"r\" not in mode and auto_mkdir:\n        parents = {fs._parent(path) for path in paths}\n        for parent in parents:\n            try:\n                fs.makedirs(parent, exist_ok=True)\n            except PermissionError:\n                pass\n    return OpenFiles(\n        [\n            OpenFile(\n                fs,\n                path,\n                mode=mode,\n                compression=compression,\n                encoding=encoding,\n                errors=errors,\n                newline=newline,\n            )\n            for path in paths\n        ],\n        mode=mode,\n        fs=fs,\n    )\n\n\ndef _un_chain(path, kwargs):\n    x = re.compile(\".*[^a-z]+.*\")  # test for non protocol-like single word\n    bits = (\n        [p if \"://\" in p or x.match(p) else p + \"://\" for p in path.split(\"::\")]\n        if \"::\" in path\n        else [path]\n    )\n    # [[url, protocol, kwargs], ...]\n    out = []\n    previous_bit = None\n    kwargs = kwargs.copy()\n    for bit in reversed(bits):\n        protocol = kwargs.pop(\"protocol\", None) or split_protocol(bit)[0] or \"file\"\n        cls = get_filesystem_class(protocol)\n        extra_kwargs = cls._get_kwargs_from_urls(bit)\n        kws = kwargs.pop(protocol, {})\n        if bit is bits[0]:\n            kws.update(kwargs)\n        kw = dict(**extra_kwargs, **kws)\n        bit = cls._strip_protocol(bit)\n        if (\n            protocol in {\"blockcache\", \"filecache\", \"simplecache\"}\n            and \"target_protocol\" not in kw\n        ):\n            bit = previous_bit\n        out.append((bit, protocol, kw))\n        previous_bit = bit\n    out.reverse()\n    return out\n\n\ndef url_to_fs(url, **kwargs):\n    \"\"\"\n    Turn fully-qualified and potentially chained URL into filesystem instance\n\n    Parameters\n    ----------\n    url : str\n        The fsspec-compatible URL\n    **kwargs: dict\n        Extra options that make sense to a particular storage connection, e.g.\n        host, port, username, password, etc.\n\n    Returns\n    -------\n    filesystem : FileSystem\n        The new filesystem discovered from ``url`` and created with\n        ``**kwargs``.\n    urlpath : str\n        The file-systems-specific URL for ``url``.\n    \"\"\"\n    url = stringify_path(url)\n    # non-FS arguments that appear in fsspec.open()\n    # inspect could keep this in sync with open()'s signature\n    known_kwargs = {\n        \"compression\",\n        \"encoding\",\n        \"errors\",\n        \"expand\",\n        \"mode\",\n        \"name_function\",\n        \"newline\",\n        \"num\",\n    }\n    kwargs = {k: v for k, v in kwargs.items() if k not in known_kwargs}\n    chain = _un_chain(url, kwargs)\n    inkwargs = {}\n    # Reverse iterate the chain, creating a nested target_* structure\n    for i, ch in enumerate(reversed(chain)):\n        urls, protocol, kw = ch\n        if i == len(chain) - 1:\n            inkwargs = dict(**kw, **inkwargs)\n            continue\n        inkwargs[\"target_options\"] = dict(**kw, **inkwargs)\n        inkwargs[\"target_protocol\"] = protocol\n        inkwargs[\"fo\"] = urls\n    urlpath, protocol, _ = chain[0]\n    fs = filesystem(protocol, **inkwargs)\n    return fs, urlpath\n\n\nDEFAULT_EXPAND = conf.get(\"open_expand\", False)\n\n\ndef open(\n    urlpath,\n    mode=\"rb\",\n    compression=None,\n    encoding=\"utf8\",\n    errors=None,\n    protocol=None,\n    newline=None,\n    expand=None,\n    **kwargs,\n):\n    \"\"\"Given a path or paths, return one ``OpenFile`` object.\n\n    Parameters\n    ----------\n    urlpath: string or list\n        Absolute or relative filepath. Prefix with a protocol like ``s3://``\n        to read from alternative filesystems. Should not include glob\n        character(s).\n    mode: 'rb', 'wt', etc.\n    compression: string or None\n        If given, open file using compression codec. Can either be a compression\n        name (a key in ``fsspec.compression.compr``) or \"infer\" to guess the\n        compression from the filename suffix.\n    encoding: str\n        For text mode only\n    errors: None or str\n        Passed to TextIOWrapper in text mode\n    protocol: str or None\n        If given, overrides the protocol found in the URL.\n    newline: bytes or None\n        Used for line terminator in text mode. If None, uses system default;\n        if blank, uses no translation.\n    expand: bool or Nonw\n        Whether to regard file paths containing special glob characters as needing\n        expansion (finding the first match) or absolute. Setting False allows using\n        paths which do embed such characters. If None (default), this argument\n        takes its value from the DEFAULT_EXPAND module variable, which takes\n        its initial value from the \"open_expand\" config value at startup, which will\n        be False if not set.\n    **kwargs: dict\n        Extra options that make sense to a particular storage connection, e.g.\n        host, port, username, password, etc.\n\n    Examples\n    --------\n    >>> openfile = open('2015-01-01.csv')  # doctest: +SKIP\n    >>> openfile = open(\n    ...     's3://bucket/2015-01-01.csv.gz', compression='gzip'\n    ... )  # doctest: +SKIP\n    >>> with openfile as f:\n    ...     df = pd.read_csv(f)  # doctest: +SKIP\n    ...\n\n    Returns\n    -------\n    ``OpenFile`` object.\n\n    Notes\n    -----\n    For a full list of the available protocols and the implementations that\n    they map across to see the latest online documentation:\n\n    - For implementations built into ``fsspec`` see\n      https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations\n    - For implementations in separate packages see\n      https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations\n    \"\"\"\n    expand = DEFAULT_EXPAND if expand is None else expand\n    out = open_files(\n        urlpath=[urlpath],\n        mode=mode,\n        compression=compression,\n        encoding=encoding,\n        errors=errors,\n        protocol=protocol,\n        newline=newline,\n        expand=expand,\n        **kwargs,\n    )\n    if not out:\n        raise FileNotFoundError(urlpath)\n    return out[0]\n\n\ndef open_local(\n    url: str | list[str] | Path | list[Path],\n    mode: str = \"rb\",\n    **storage_options: dict,\n) -> str | list[str]:\n    \"\"\"Open file(s) which can be resolved to local\n\n    For files which either are local, or get downloaded upon open\n    (e.g., by file caching)\n\n    Parameters\n    ----------\n    url: str or list(str)\n    mode: str\n        Must be read mode\n    storage_options:\n        passed on to FS for or used by open_files (e.g., compression)\n    \"\"\"\n    if \"r\" not in mode:\n        raise ValueError(\"Can only ensure local files when reading\")\n    of = open_files(url, mode=mode, **storage_options)\n    if not getattr(of[0].fs, \"local_file\", False):\n        raise ValueError(\n            \"open_local can only be used on a filesystem which\"\n            \" has attribute local_file=True\"\n        )\n    with of as files:\n        paths = [f.name for f in files]\n    if (isinstance(url, str) and not has_magic(url)) or isinstance(url, Path):\n        return paths[0]\n    return paths\n\n\ndef get_compression(urlpath, compression):\n    if compression == \"infer\":\n        compression = infer_compression(urlpath)\n    if compression is not None and compression not in compr:\n        raise ValueError(f\"Compression type {compression} not supported\")\n    return compression\n\n\ndef split_protocol(urlpath):\n    \"\"\"Return protocol, path pair\"\"\"\n    urlpath = stringify_path(urlpath)\n    if \"://\" in urlpath:\n        protocol, path = urlpath.split(\"://\", 1)\n        if len(protocol) > 1:\n            # excludes Windows paths\n            return protocol, path\n    if urlpath.startswith(\"data:\"):\n        return urlpath.split(\":\", 1)\n    return None, urlpath\n\n\ndef strip_protocol(urlpath):\n    \"\"\"Return only path part of full URL, according to appropriate backend\"\"\"\n    protocol, _ = split_protocol(urlpath)\n    cls = get_filesystem_class(protocol)\n    return cls._strip_protocol(urlpath)\n\n\ndef expand_paths_if_needed(paths, mode, num, fs, name_function):\n    \"\"\"Expand paths if they have a ``*`` in them (write mode) or any of ``*?[]``\n    in them (read mode).\n\n    :param paths: list of paths\n    mode: str\n        Mode in which to open files.\n    num: int\n        If opening in writing mode, number of files we expect to create.\n    fs: filesystem object\n    name_function: callable\n        If opening in writing mode, this callable is used to generate path\n        names. Names are generated for each partition by\n        ``urlpath.replace('*', name_function(partition_index))``.\n    :return: list of paths\n    \"\"\"\n    expanded_paths = []\n    paths = list(paths)\n\n    if \"w\" in mode:  # read mode\n        if sum([1 for p in paths if \"*\" in p]) > 1:\n            raise ValueError(\n                \"When writing data, only one filename mask can be specified.\"\n            )\n        num = max(num, len(paths))\n\n        for curr_path in paths:\n            if \"*\" in curr_path:\n                # expand using name_function\n                expanded_paths.extend(_expand_paths(curr_path, name_function, num))\n            else:\n                expanded_paths.append(curr_path)\n        # if we generated more paths that asked for, trim the list\n        if len(expanded_paths) > num:\n            expanded_paths = expanded_paths[:num]\n\n    else:  # read mode\n        for curr_path in paths:\n            if has_magic(curr_path):\n                # expand using glob\n                expanded_paths.extend(fs.glob(curr_path))\n            else:\n                expanded_paths.append(curr_path)\n\n    return expanded_paths\n\n\ndef get_fs_token_paths(\n    urlpath,\n    mode=\"rb\",\n    num=1,\n    name_function=None,\n    storage_options=None,\n    protocol=None,\n    expand=True,\n):\n    \"\"\"Filesystem, deterministic token, and paths from a urlpath and options.\n\n    Parameters\n    ----------\n    urlpath: string or iterable\n        Absolute or relative filepath, URL (may include protocols like\n        ``s3://``), or globstring pointing to data.\n    mode: str, optional\n        Mode in which to open files.\n    num: int, optional\n        If opening in writing mode, number of files we expect to create.\n    name_function: callable, optional\n        If opening in writing mode, this callable is used to generate path\n        names. Names are generated for each partition by\n        ``urlpath.replace('*', name_function(partition_index))``.\n    storage_options: dict, optional\n        Additional keywords to pass to the filesystem class.\n    protocol: str or None\n        To override the protocol specifier in the URL\n    expand: bool\n        Expand string paths for writing, assuming the path is a directory\n    \"\"\"\n    if isinstance(urlpath, (list, tuple, set)):\n        if not urlpath:\n            raise ValueError(\"empty urlpath sequence\")\n        urlpath0 = stringify_path(list(urlpath)[0])\n    else:\n        urlpath0 = stringify_path(urlpath)\n    storage_options = storage_options or {}\n    if protocol:\n        storage_options[\"protocol\"] = protocol\n    chain = _un_chain(urlpath0, storage_options or {})\n    inkwargs = {}\n    # Reverse iterate the chain, creating a nested target_* structure\n    for i, ch in enumerate(reversed(chain)):\n        urls, nested_protocol, kw = ch\n        if i == len(chain) - 1:\n            inkwargs = dict(**kw, **inkwargs)\n            continue\n        inkwargs[\"target_options\"] = dict(**kw, **inkwargs)\n        inkwargs[\"target_protocol\"] = nested_protocol\n        inkwargs[\"fo\"] = urls\n    paths, protocol, _ = chain[0]\n    fs = filesystem(protocol, **inkwargs)\n    if isinstance(urlpath, (list, tuple, set)):\n        pchains = [\n            _un_chain(stringify_path(u), storage_options or {})[0] for u in urlpath\n        ]\n        if len({pc[1] for pc in pchains}) > 1:\n            raise ValueError(\"Protocol mismatch getting fs from %s\", urlpath)\n        paths = [pc[0] for pc in pchains]\n    else:\n        paths = fs._strip_protocol(paths)\n    if isinstance(paths, (list, tuple, set)):\n        if expand:\n            paths = expand_paths_if_needed(paths, mode, num, fs, name_function)\n        elif not isinstance(paths, list):\n            paths = list(paths)\n    else:\n        if \"w\" in mode and expand:\n            paths = _expand_paths(paths, name_function, num)\n        elif \"x\" in mode and expand:\n            paths = _expand_paths(paths, name_function, num)\n        elif \"*\" in paths:\n            paths = [f for f in sorted(fs.glob(paths)) if not fs.isdir(f)]\n        else:\n            paths = [paths]\n\n    return fs, fs._fs_token, paths\n\n\ndef _expand_paths(path, name_function, num):\n    if isinstance(path, str):\n        if path.count(\"*\") > 1:\n            raise ValueError(\"Output path spec must contain exactly one '*'.\")\n        elif \"*\" not in path:\n            path = os.path.join(path, \"*.part\")\n\n        if name_function is None:\n            name_function = build_name_function(num - 1)\n\n        paths = [path.replace(\"*\", name_function(i)) for i in range(num)]\n        if paths != sorted(paths):\n            logger.warning(\n                \"In order to preserve order between partitions\"\n                \" paths created with ``name_function`` should \"\n                \"sort to partition order\"\n            )\n    elif isinstance(path, (tuple, list)):\n        assert len(path) == num\n        paths = list(path)\n    else:\n        raise ValueError(\n            \"Path should be either\\n\"\n            \"1. A list of paths: ['foo.json', 'bar.json', ...]\\n\"\n            \"2. A directory: 'foo/\\n\"\n            \"3. A path with a '*' in it: 'foo.*.json'\"\n        )\n    return paths\n\n\nclass PickleableTextIOWrapper(io.TextIOWrapper):\n    \"\"\"TextIOWrapper cannot be pickled. This solves it.\n\n    Requires that ``buffer`` be pickleable, which all instances of\n    AbstractBufferedFile are.\n    \"\"\"\n\n    def __init__(\n        self,\n        buffer,\n        encoding=None,\n        errors=None,\n        newline=None,\n        line_buffering=False,\n        write_through=False,\n    ):\n        self.args = buffer, encoding, errors, newline, line_buffering, write_through\n        super().__init__(*self.args)\n\n    def __reduce__(self):\n        return PickleableTextIOWrapper, self.args\n", "fsspec/generic.py": "from __future__ import annotations\n\nimport inspect\nimport logging\nimport os\nimport shutil\nimport uuid\nfrom typing import Optional\n\nfrom .asyn import AsyncFileSystem, _run_coros_in_chunks, sync_wrapper\nfrom .callbacks import DEFAULT_CALLBACK\nfrom .core import filesystem, get_filesystem_class, split_protocol, url_to_fs\n\n_generic_fs = {}\nlogger = logging.getLogger(\"fsspec.generic\")\n\n\ndef set_generic_fs(protocol, **storage_options):\n    _generic_fs[protocol] = filesystem(protocol, **storage_options)\n\n\ndefault_method = \"default\"\n\n\ndef _resolve_fs(url, method=None, protocol=None, storage_options=None):\n    \"\"\"Pick instance of backend FS\"\"\"\n    method = method or default_method\n    protocol = protocol or split_protocol(url)[0]\n    storage_options = storage_options or {}\n    if method == \"default\":\n        return filesystem(protocol)\n    if method == \"generic\":\n        return _generic_fs[protocol]\n    if method == \"current\":\n        cls = get_filesystem_class(protocol)\n        return cls.current()\n    if method == \"options\":\n        fs, _ = url_to_fs(url, **storage_options.get(protocol, {}))\n        return fs\n    raise ValueError(f\"Unknown FS resolution method: {method}\")\n\n\ndef rsync(\n    source,\n    destination,\n    delete_missing=False,\n    source_field=\"size\",\n    dest_field=\"size\",\n    update_cond=\"different\",\n    inst_kwargs=None,\n    fs=None,\n    **kwargs,\n):\n    \"\"\"Sync files between two directory trees\n\n    (experimental)\n\n    Parameters\n    ----------\n    source: str\n        Root of the directory tree to take files from. This must be a directory, but\n        do not include any terminating \"/\" character\n    destination: str\n        Root path to copy into. The contents of this location should be\n        identical to the contents of ``source`` when done. This will be made a\n        directory, and the terminal \"/\" should not be included.\n    delete_missing: bool\n        If there are paths in the destination that don't exist in the\n        source and this is True, delete them. Otherwise, leave them alone.\n    source_field: str | callable\n        If ``update_field`` is \"different\", this is the key in the info\n        of source files to consider for difference. Maybe a function of the\n        info dict.\n    dest_field: str | callable\n        If ``update_field`` is \"different\", this is the key in the info\n        of destination files to consider for difference. May be a function of\n        the info dict.\n    update_cond: \"different\"|\"always\"|\"never\"\n        If \"always\", every file is copied, regardless of whether it exists in\n        the destination. If \"never\", files that exist in the destination are\n        not copied again. If \"different\" (default), only copy if the info\n        fields given by ``source_field`` and ``dest_field`` (usually \"size\")\n        are different. Other comparisons may be added in the future.\n    inst_kwargs: dict|None\n        If ``fs`` is None, use this set of keyword arguments to make a\n        GenericFileSystem instance\n    fs: GenericFileSystem|None\n        Instance to use if explicitly given. The instance defines how to\n        to make downstream file system instances from paths.\n\n    Returns\n    -------\n    dict of the copy operations that were performed, {source: destination}\n    \"\"\"\n    fs = fs or GenericFileSystem(**(inst_kwargs or {}))\n    source = fs._strip_protocol(source)\n    destination = fs._strip_protocol(destination)\n    allfiles = fs.find(source, withdirs=True, detail=True)\n    if not fs.isdir(source):\n        raise ValueError(\"Can only rsync on a directory\")\n    otherfiles = fs.find(destination, withdirs=True, detail=True)\n    dirs = [\n        a\n        for a, v in allfiles.items()\n        if v[\"type\"] == \"directory\" and a.replace(source, destination) not in otherfiles\n    ]\n    logger.debug(f\"{len(dirs)} directories to create\")\n    if dirs:\n        fs.make_many_dirs(\n            [dirn.replace(source, destination) for dirn in dirs], exist_ok=True\n        )\n    allfiles = {a: v for a, v in allfiles.items() if v[\"type\"] == \"file\"}\n    logger.debug(f\"{len(allfiles)} files to consider for copy\")\n    to_delete = [\n        o\n        for o, v in otherfiles.items()\n        if o.replace(destination, source) not in allfiles and v[\"type\"] == \"file\"\n    ]\n    for k, v in allfiles.copy().items():\n        otherfile = k.replace(source, destination)\n        if otherfile in otherfiles:\n            if update_cond == \"always\":\n                allfiles[k] = otherfile\n            elif update_cond == \"different\":\n                inf1 = source_field(v) if callable(source_field) else v[source_field]\n                v2 = otherfiles[otherfile]\n                inf2 = dest_field(v2) if callable(dest_field) else v2[dest_field]\n                if inf1 != inf2:\n                    # details mismatch, make copy\n                    allfiles[k] = otherfile\n                else:\n                    # details match, don't copy\n                    allfiles.pop(k)\n        else:\n            # file not in target yet\n            allfiles[k] = otherfile\n    logger.debug(f\"{len(allfiles)} files to copy\")\n    if allfiles:\n        source_files, target_files = zip(*allfiles.items())\n        fs.cp(source_files, target_files, **kwargs)\n    logger.debug(f\"{len(to_delete)} files to delete\")\n    if delete_missing and to_delete:\n        fs.rm(to_delete)\n    return allfiles\n\n\nclass GenericFileSystem(AsyncFileSystem):\n    \"\"\"Wrapper over all other FS types\n\n    <experimental!>\n\n    This implementation is a single unified interface to be able to run FS operations\n    over generic URLs, and dispatch to the specific implementations using the URL\n    protocol prefix.\n\n    Note: instances of this FS are always async, even if you never use it with any async\n    backend.\n    \"\"\"\n\n    protocol = \"generic\"  # there is no real reason to ever use a protocol with this FS\n\n    def __init__(self, default_method=\"default\", **kwargs):\n        \"\"\"\n\n        Parameters\n        ----------\n        default_method: str (optional)\n            Defines how to configure backend FS instances. Options are:\n            - \"default\": instantiate like FSClass(), with no\n              extra arguments; this is the default instance of that FS, and can be\n              configured via the config system\n            - \"generic\": takes instances from the `_generic_fs` dict in this module,\n              which you must populate before use. Keys are by protocol\n            - \"current\": takes the most recently instantiated version of each FS\n        \"\"\"\n        self.method = default_method\n        super().__init__(**kwargs)\n\n    def _parent(self, path):\n        fs = _resolve_fs(path, self.method)\n        return fs.unstrip_protocol(fs._parent(path))\n\n    def _strip_protocol(self, path):\n        # normalization only\n        fs = _resolve_fs(path, self.method)\n        return fs.unstrip_protocol(fs._strip_protocol(path))\n\n    async def _find(self, path, maxdepth=None, withdirs=False, detail=False, **kwargs):\n        fs = _resolve_fs(path, self.method)\n        if fs.async_impl:\n            out = await fs._find(\n                path, maxdepth=maxdepth, withdirs=withdirs, detail=True, **kwargs\n            )\n        else:\n            out = fs.find(\n                path, maxdepth=maxdepth, withdirs=withdirs, detail=True, **kwargs\n            )\n        result = {}\n        for k, v in out.items():\n            v = v.copy()  # don't corrupt target FS dircache\n            name = fs.unstrip_protocol(k)\n            v[\"name\"] = name\n            result[name] = v\n        if detail:\n            return result\n        return list(result)\n\n    async def _info(self, url, **kwargs):\n        fs = _resolve_fs(url, self.method)\n        if fs.async_impl:\n            out = await fs._info(url, **kwargs)\n        else:\n            out = fs.info(url, **kwargs)\n        out = out.copy()  # don't edit originals\n        out[\"name\"] = fs.unstrip_protocol(out[\"name\"])\n        return out\n\n    async def _ls(\n        self,\n        url,\n        detail=True,\n        **kwargs,\n    ):\n        fs = _resolve_fs(url, self.method)\n        if fs.async_impl:\n            out = await fs._ls(url, detail=True, **kwargs)\n        else:\n            out = fs.ls(url, detail=True, **kwargs)\n        out = [o.copy() for o in out]  # don't edit originals\n        for o in out:\n            o[\"name\"] = fs.unstrip_protocol(o[\"name\"])\n        if detail:\n            return out\n        else:\n            return [o[\"name\"] for o in out]\n\n    async def _cat_file(\n        self,\n        url,\n        **kwargs,\n    ):\n        fs = _resolve_fs(url, self.method)\n        if fs.async_impl:\n            return await fs._cat_file(url, **kwargs)\n        else:\n            return fs.cat_file(url, **kwargs)\n\n    async def _pipe_file(\n        self,\n        path,\n        value,\n        **kwargs,\n    ):\n        fs = _resolve_fs(path, self.method)\n        if fs.async_impl:\n            return await fs._pipe_file(path, value, **kwargs)\n        else:\n            return fs.pipe_file(path, value, **kwargs)\n\n    async def _rm(self, url, **kwargs):\n        urls = url\n        if isinstance(urls, str):\n            urls = [urls]\n        fs = _resolve_fs(urls[0], self.method)\n        if fs.async_impl:\n            await fs._rm(urls, **kwargs)\n        else:\n            fs.rm(url, **kwargs)\n\n    async def _makedirs(self, path, exist_ok=False):\n        logger.debug(\"Make dir %s\", path)\n        fs = _resolve_fs(path, self.method)\n        if fs.async_impl:\n            await fs._makedirs(path, exist_ok=exist_ok)\n        else:\n            fs.makedirs(path, exist_ok=exist_ok)\n\n    def rsync(self, source, destination, **kwargs):\n        \"\"\"Sync files between two directory trees\n\n        See `func:rsync` for more details.\n        \"\"\"\n        rsync(source, destination, fs=self, **kwargs)\n\n    async def _cp_file(\n        self,\n        url,\n        url2,\n        blocksize=2**20,\n        callback=DEFAULT_CALLBACK,\n        **kwargs,\n    ):\n        fs = _resolve_fs(url, self.method)\n        fs2 = _resolve_fs(url2, self.method)\n        if fs is fs2:\n            # pure remote\n            if fs.async_impl:\n                return await fs._cp_file(url, url2, **kwargs)\n            else:\n                return fs.cp_file(url, url2, **kwargs)\n        kw = {\"blocksize\": 0, \"cache_type\": \"none\"}\n        try:\n            f1 = (\n                await fs.open_async(url, \"rb\")\n                if hasattr(fs, \"open_async\")\n                else fs.open(url, \"rb\", **kw)\n            )\n            callback.set_size(await maybe_await(f1.size))\n            f2 = (\n                await fs2.open_async(url2, \"wb\")\n                if hasattr(fs2, \"open_async\")\n                else fs2.open(url2, \"wb\", **kw)\n            )\n            while f1.size is None or f2.tell() < f1.size:\n                data = await maybe_await(f1.read(blocksize))\n                if f1.size is None and not data:\n                    break\n                await maybe_await(f2.write(data))\n                callback.absolute_update(f2.tell())\n        finally:\n            try:\n                await maybe_await(f2.close())\n                await maybe_await(f1.close())\n            except NameError:\n                # fail while opening f1 or f2\n                pass\n\n    async def _make_many_dirs(self, urls, exist_ok=True):\n        fs = _resolve_fs(urls[0], self.method)\n        if fs.async_impl:\n            coros = [fs._makedirs(u, exist_ok=exist_ok) for u in urls]\n            await _run_coros_in_chunks(coros)\n        else:\n            for u in urls:\n                fs.makedirs(u, exist_ok=exist_ok)\n\n    make_many_dirs = sync_wrapper(_make_many_dirs)\n\n    async def _copy(\n        self,\n        path1: list[str],\n        path2: list[str],\n        recursive: bool = False,\n        on_error: str = \"ignore\",\n        maxdepth: Optional[int] = None,\n        batch_size: Optional[int] = None,\n        tempdir: Optional[str] = None,\n        **kwargs,\n    ):\n        if recursive:\n            raise NotImplementedError\n        fs = _resolve_fs(path1[0], self.method)\n        fs2 = _resolve_fs(path2[0], self.method)\n        # not expanding paths atm., assume call is from rsync()\n        if fs is fs2:\n            # pure remote\n            if fs.async_impl:\n                return await fs._copy(path1, path2, **kwargs)\n            else:\n                return fs.copy(path1, path2, **kwargs)\n        await copy_file_op(\n            fs, path1, fs2, path2, tempdir, batch_size, on_error=on_error\n        )\n\n\nasync def copy_file_op(\n    fs1, url1, fs2, url2, tempdir=None, batch_size=20, on_error=\"ignore\"\n):\n    import tempfile\n\n    tempdir = tempdir or tempfile.mkdtemp()\n    try:\n        coros = [\n            _copy_file_op(\n                fs1,\n                u1,\n                fs2,\n                u2,\n                os.path.join(tempdir, uuid.uuid4().hex),\n                on_error=on_error,\n            )\n            for u1, u2 in zip(url1, url2)\n        ]\n        await _run_coros_in_chunks(coros, batch_size=batch_size)\n    finally:\n        shutil.rmtree(tempdir)\n\n\nasync def _copy_file_op(fs1, url1, fs2, url2, local, on_error=\"ignore\"):\n    ex = () if on_error == \"raise\" else Exception\n    logger.debug(\"Copy %s -> %s\", url1, url2)\n    try:\n        if fs1.async_impl:\n            await fs1._get_file(url1, local)\n        else:\n            fs1.get_file(url1, local)\n        if fs2.async_impl:\n            await fs2._put_file(local, url2)\n        else:\n            fs2.put_file(local, url2)\n        os.unlink(local)\n        logger.debug(\"Copy %s -> %s; done\", url1, url2)\n    except ex as e:\n        logger.debug(\"ignoring cp exception for %s: %s\", url1, e)\n\n\nasync def maybe_await(cor):\n    if inspect.iscoroutine(cor):\n        return await cor\n    else:\n        return cor\n", "fsspec/exceptions.py": "\"\"\"\nfsspec user-defined exception classes\n\"\"\"\n\nimport asyncio\n\n\nclass BlocksizeMismatchError(ValueError):\n    \"\"\"\n    Raised when a cached file is opened with a different blocksize than it was\n    written with\n    \"\"\"\n\n\nclass FSTimeoutError(asyncio.TimeoutError):\n    \"\"\"\n    Raised when a fsspec function timed out occurs\n    \"\"\"\n", "fsspec/spec.py": "from __future__ import annotations\n\nimport io\nimport json\nimport logging\nimport os\nimport threading\nimport warnings\nimport weakref\nfrom errno import ESPIPE\nfrom glob import has_magic\nfrom hashlib import sha256\nfrom typing import Any, ClassVar, Dict, Tuple\n\nfrom .callbacks import DEFAULT_CALLBACK\nfrom .config import apply_config, conf\nfrom .dircache import DirCache\nfrom .transaction import Transaction\nfrom .utils import (\n    _unstrip_protocol,\n    glob_translate,\n    isfilelike,\n    other_paths,\n    read_block,\n    stringify_path,\n    tokenize,\n)\n\nlogger = logging.getLogger(\"fsspec\")\n\n\ndef make_instance(cls, args, kwargs):\n    return cls(*args, **kwargs)\n\n\nclass _Cached(type):\n    \"\"\"\n    Metaclass for caching file system instances.\n\n    Notes\n    -----\n    Instances are cached according to\n\n    * The values of the class attributes listed in `_extra_tokenize_attributes`\n    * The arguments passed to ``__init__``.\n\n    This creates an additional reference to the filesystem, which prevents the\n    filesystem from being garbage collected when all *user* references go away.\n    A call to the :meth:`AbstractFileSystem.clear_instance_cache` must *also*\n    be made for a filesystem instance to be garbage collected.\n    \"\"\"\n\n    def __init__(cls, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Note: we intentionally create a reference here, to avoid garbage\n        # collecting instances when all other references are gone. To really\n        # delete a FileSystem, the cache must be cleared.\n        if conf.get(\"weakref_instance_cache\"):  # pragma: no cover\n            # debug option for analysing fork/spawn conditions\n            cls._cache = weakref.WeakValueDictionary()\n        else:\n            cls._cache = {}\n        cls._pid = os.getpid()\n\n    def __call__(cls, *args, **kwargs):\n        kwargs = apply_config(cls, kwargs)\n        extra_tokens = tuple(\n            getattr(cls, attr, None) for attr in cls._extra_tokenize_attributes\n        )\n        token = tokenize(\n            cls, cls._pid, threading.get_ident(), *args, *extra_tokens, **kwargs\n        )\n        skip = kwargs.pop(\"skip_instance_cache\", False)\n        if os.getpid() != cls._pid:\n            cls._cache.clear()\n            cls._pid = os.getpid()\n        if not skip and cls.cachable and token in cls._cache:\n            cls._latest = token\n            return cls._cache[token]\n        else:\n            obj = super().__call__(*args, **kwargs)\n            # Setting _fs_token here causes some static linters to complain.\n            obj._fs_token_ = token\n            obj.storage_args = args\n            obj.storage_options = kwargs\n            if obj.async_impl and obj.mirror_sync_methods:\n                from .asyn import mirror_sync_methods\n\n                mirror_sync_methods(obj)\n\n            if cls.cachable and not skip:\n                cls._latest = token\n                cls._cache[token] = obj\n            return obj\n\n\nclass AbstractFileSystem(metaclass=_Cached):\n    \"\"\"\n    An abstract super-class for pythonic file-systems\n\n    Implementations are expected to be compatible with or, better, subclass\n    from here.\n    \"\"\"\n\n    cachable = True  # this class can be cached, instances reused\n    _cached = False\n    blocksize = 2**22\n    sep = \"/\"\n    protocol: ClassVar[str | tuple[str, ...]] = \"abstract\"\n    _latest = None\n    async_impl = False\n    mirror_sync_methods = False\n    root_marker = \"\"  # For some FSs, may require leading '/' or other character\n    transaction_type = Transaction\n\n    #: Extra *class attributes* that should be considered when hashing.\n    _extra_tokenize_attributes = ()\n\n    # Set by _Cached metaclass\n    storage_args: Tuple[Any, ...]\n    storage_options: Dict[str, Any]\n\n    def __init__(self, *args, **storage_options):\n        \"\"\"Create and configure file-system instance\n\n        Instances may be cachable, so if similar enough arguments are seen\n        a new instance is not required. The token attribute exists to allow\n        implementations to cache instances if they wish.\n\n        A reasonable default should be provided if there are no arguments.\n\n        Subclasses should call this method.\n\n        Parameters\n        ----------\n        use_listings_cache, listings_expiry_time, max_paths:\n            passed to ``DirCache``, if the implementation supports\n            directory listing caching. Pass use_listings_cache=False\n            to disable such caching.\n        skip_instance_cache: bool\n            If this is a cachable implementation, pass True here to force\n            creating a new instance even if a matching instance exists, and prevent\n            storing this instance.\n        asynchronous: bool\n        loop: asyncio-compatible IOLoop or None\n        \"\"\"\n        if self._cached:\n            # reusing instance, don't change\n            return\n        self._cached = True\n        self._intrans = False\n        self._transaction = None\n        self._invalidated_caches_in_transaction = []\n        self.dircache = DirCache(**storage_options)\n\n        if storage_options.pop(\"add_docs\", None):\n            warnings.warn(\"add_docs is no longer supported.\", FutureWarning)\n\n        if storage_options.pop(\"add_aliases\", None):\n            warnings.warn(\"add_aliases has been removed.\", FutureWarning)\n        # This is set in _Cached\n        self._fs_token_ = None\n\n    @property\n    def fsid(self):\n        \"\"\"Persistent filesystem id that can be used to compare filesystems\n        across sessions.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def _fs_token(self):\n        return self._fs_token_\n\n    def __dask_tokenize__(self):\n        return self._fs_token\n\n    def __hash__(self):\n        return int(self._fs_token, 16)\n\n    def __eq__(self, other):\n        return isinstance(other, type(self)) and self._fs_token == other._fs_token\n\n    def __reduce__(self):\n        return make_instance, (type(self), self.storage_args, self.storage_options)\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        \"\"\"Turn path from fully-qualified to file-system-specific\n\n        May require FS-specific handling, e.g., for relative paths or links.\n        \"\"\"\n        if isinstance(path, list):\n            return [cls._strip_protocol(p) for p in path]\n        path = stringify_path(path)\n        protos = (cls.protocol,) if isinstance(cls.protocol, str) else cls.protocol\n        for protocol in protos:\n            if path.startswith(protocol + \"://\"):\n                path = path[len(protocol) + 3 :]\n            elif path.startswith(protocol + \"::\"):\n                path = path[len(protocol) + 2 :]\n        path = path.rstrip(\"/\")\n        # use of root_marker to make minimum required path, e.g., \"/\"\n        return path or cls.root_marker\n\n    def unstrip_protocol(self, name: str) -> str:\n        \"\"\"Format FS-specific path to generic, including protocol\"\"\"\n        protos = (self.protocol,) if isinstance(self.protocol, str) else self.protocol\n        for protocol in protos:\n            if name.startswith(f\"{protocol}://\"):\n                return name\n        return f\"{protos[0]}://{name}\"\n\n    @staticmethod\n    def _get_kwargs_from_urls(path):\n        \"\"\"If kwargs can be encoded in the paths, extract them here\n\n        This should happen before instantiation of the class; incoming paths\n        then should be amended to strip the options in methods.\n\n        Examples may look like an sftp path \"sftp://user@host:/my/path\", where\n        the user and host should become kwargs and later get stripped.\n        \"\"\"\n        # by default, nothing happens\n        return {}\n\n    @classmethod\n    def current(cls):\n        \"\"\"Return the most recently instantiated FileSystem\n\n        If no instance has been created, then create one with defaults\n        \"\"\"\n        if cls._latest in cls._cache:\n            return cls._cache[cls._latest]\n        return cls()\n\n    @property\n    def transaction(self):\n        \"\"\"A context within which files are committed together upon exit\n\n        Requires the file class to implement `.commit()` and `.discard()`\n        for the normal and exception cases.\n        \"\"\"\n        if self._transaction is None:\n            self._transaction = self.transaction_type(self)\n        return self._transaction\n\n    def start_transaction(self):\n        \"\"\"Begin write transaction for deferring files, non-context version\"\"\"\n        self._intrans = True\n        self._transaction = self.transaction_type(self)\n        return self.transaction\n\n    def end_transaction(self):\n        \"\"\"Finish write transaction, non-context version\"\"\"\n        self.transaction.complete()\n        self._transaction = None\n        # The invalid cache must be cleared after the transaction is completed.\n        for path in self._invalidated_caches_in_transaction:\n            self.invalidate_cache(path)\n        self._invalidated_caches_in_transaction.clear()\n\n    def invalidate_cache(self, path=None):\n        \"\"\"\n        Discard any cached directory information\n\n        Parameters\n        ----------\n        path: string or None\n            If None, clear all listings cached else listings at or under given\n            path.\n        \"\"\"\n        # Not necessary to implement invalidation mechanism, may have no cache.\n        # But if have, you should call this method of parent class from your\n        # subclass to ensure expiring caches after transacations correctly.\n        # See the implementation of FTPFileSystem in ftp.py\n        if self._intrans:\n            self._invalidated_caches_in_transaction.append(path)\n\n    def mkdir(self, path, create_parents=True, **kwargs):\n        \"\"\"\n        Create directory entry at path\n\n        For systems that don't have true directories, may create an for\n        this instance only and not touch the real filesystem\n\n        Parameters\n        ----------\n        path: str\n            location\n        create_parents: bool\n            if True, this is equivalent to ``makedirs``\n        kwargs:\n            may be permissions, etc.\n        \"\"\"\n        pass  # not necessary to implement, may not have directories\n\n    def makedirs(self, path, exist_ok=False):\n        \"\"\"Recursively make directories\n\n        Creates directory at path and any intervening required directories.\n        Raises exception if, for instance, the path already exists but is a\n        file.\n\n        Parameters\n        ----------\n        path: str\n            leaf directory name\n        exist_ok: bool (False)\n            If False, will error if the target already exists\n        \"\"\"\n        pass  # not necessary to implement, may not have directories\n\n    def rmdir(self, path):\n        \"\"\"Remove a directory, if empty\"\"\"\n        pass  # not necessary to implement, may not have directories\n\n    def ls(self, path, detail=True, **kwargs):\n        \"\"\"List objects at path.\n\n        This should include subdirectories and files at that location. The\n        difference between a file and a directory must be clear when details\n        are requested.\n\n        The specific keys, or perhaps a FileInfo class, or similar, is TBD,\n        but must be consistent across implementations.\n        Must include:\n\n        - full path to the entry (without protocol)\n        - size of the entry, in bytes. If the value cannot be determined, will\n          be ``None``.\n        - type of entry, \"file\", \"directory\" or other\n\n        Additional information\n        may be present, appropriate to the file-system, e.g., generation,\n        checksum, etc.\n\n        May use refresh=True|False to allow use of self._ls_from_cache to\n        check for a saved listing and avoid calling the backend. This would be\n        common where listing may be expensive.\n\n        Parameters\n        ----------\n        path: str\n        detail: bool\n            if True, gives a list of dictionaries, where each is the same as\n            the result of ``info(path)``. If False, gives a list of paths\n            (str).\n        kwargs: may have additional backend-specific options, such as version\n            information\n\n        Returns\n        -------\n        List of strings if detail is False, or list of directory information\n        dicts if detail is True.\n        \"\"\"\n        raise NotImplementedError\n\n    def _ls_from_cache(self, path):\n        \"\"\"Check cache for listing\n\n        Returns listing, if found (may be empty list for a directly that exists\n        but contains nothing), None if not in cache.\n        \"\"\"\n        parent = self._parent(path)\n        try:\n            return self.dircache[path.rstrip(\"/\")]\n        except KeyError:\n            pass\n        try:\n            files = [\n                f\n                for f in self.dircache[parent]\n                if f[\"name\"] == path\n                or (f[\"name\"] == path.rstrip(\"/\") and f[\"type\"] == \"directory\")\n            ]\n            if len(files) == 0:\n                # parent dir was listed but did not contain this file\n                raise FileNotFoundError(path)\n            return files\n        except KeyError:\n            pass\n\n    def walk(self, path, maxdepth=None, topdown=True, on_error=\"omit\", **kwargs):\n        \"\"\"Return all files belows path\n\n        List all files, recursing into subdirectories; output is iterator-style,\n        like ``os.walk()``. For a simple list of files, ``find()`` is available.\n\n        When topdown is True, the caller can modify the dirnames list in-place (perhaps\n        using del or slice assignment), and walk() will\n        only recurse into the subdirectories whose names remain in dirnames;\n        this can be used to prune the search, impose a specific order of visiting,\n        or even to inform walk() about directories the caller creates or renames before\n        it resumes walk() again.\n        Modifying dirnames when topdown is False has no effect. (see os.walk)\n\n        Note that the \"files\" outputted will include anything that is not\n        a directory, such as links.\n\n        Parameters\n        ----------\n        path: str\n            Root to recurse into\n        maxdepth: int\n            Maximum recursion depth. None means limitless, but not recommended\n            on link-based file-systems.\n        topdown: bool (True)\n            Whether to walk the directory tree from the top downwards or from\n            the bottom upwards.\n        on_error: \"omit\", \"raise\", a collable\n            if omit (default), path with exception will simply be empty;\n            If raise, an underlying exception will be raised;\n            if callable, it will be called with a single OSError instance as argument\n        kwargs: passed to ``ls``\n        \"\"\"\n        if maxdepth is not None and maxdepth < 1:\n            raise ValueError(\"maxdepth must be at least 1\")\n\n        path = self._strip_protocol(path)\n        full_dirs = {}\n        dirs = {}\n        files = {}\n\n        detail = kwargs.pop(\"detail\", False)\n        try:\n            listing = self.ls(path, detail=True, **kwargs)\n        except (FileNotFoundError, OSError) as e:\n            if on_error == \"raise\":\n                raise\n            elif callable(on_error):\n                on_error(e)\n            if detail:\n                return path, {}, {}\n            return path, [], []\n\n        for info in listing:\n            # each info name must be at least [path]/part , but here\n            # we check also for names like [path]/part/\n            pathname = info[\"name\"].rstrip(\"/\")\n            name = pathname.rsplit(\"/\", 1)[-1]\n            if info[\"type\"] == \"directory\" and pathname != path:\n                # do not include \"self\" path\n                full_dirs[name] = pathname\n                dirs[name] = info\n            elif pathname == path:\n                # file-like with same name as give path\n                files[\"\"] = info\n            else:\n                files[name] = info\n\n        if not detail:\n            dirs = list(dirs)\n            files = list(files)\n\n        if topdown:\n            # Yield before recursion if walking top down\n            yield path, dirs, files\n\n        if maxdepth is not None:\n            maxdepth -= 1\n            if maxdepth < 1:\n                if not topdown:\n                    yield path, dirs, files\n                return\n\n        for d in dirs:\n            yield from self.walk(\n                full_dirs[d],\n                maxdepth=maxdepth,\n                detail=detail,\n                topdown=topdown,\n                **kwargs,\n            )\n\n        if not topdown:\n            # Yield after recursion if walking bottom up\n            yield path, dirs, files\n\n    def find(self, path, maxdepth=None, withdirs=False, detail=False, **kwargs):\n        \"\"\"List all files below path.\n\n        Like posix ``find`` command without conditions\n\n        Parameters\n        ----------\n        path : str\n        maxdepth: int or None\n            If not None, the maximum number of levels to descend\n        withdirs: bool\n            Whether to include directory paths in the output. This is True\n            when used by glob, but users usually only want files.\n        kwargs are passed to ``ls``.\n        \"\"\"\n        # TODO: allow equivalent of -name parameter\n        path = self._strip_protocol(path)\n        out = {}\n\n        # Add the root directory if withdirs is requested\n        # This is needed for posix glob compliance\n        if withdirs and path != \"\" and self.isdir(path):\n            out[path] = self.info(path)\n\n        for _, dirs, files in self.walk(path, maxdepth, detail=True, **kwargs):\n            if withdirs:\n                files.update(dirs)\n            out.update({info[\"name\"]: info for name, info in files.items()})\n        if not out and self.isfile(path):\n            # walk works on directories, but find should also return [path]\n            # when path happens to be a file\n            out[path] = {}\n        names = sorted(out)\n        if not detail:\n            return names\n        else:\n            return {name: out[name] for name in names}\n\n    def du(self, path, total=True, maxdepth=None, withdirs=False, **kwargs):\n        \"\"\"Space used by files and optionally directories within a path\n\n        Directory size does not include the size of its contents.\n\n        Parameters\n        ----------\n        path: str\n        total: bool\n            Whether to sum all the file sizes\n        maxdepth: int or None\n            Maximum number of directory levels to descend, None for unlimited.\n        withdirs: bool\n            Whether to include directory paths in the output.\n        kwargs: passed to ``find``\n\n        Returns\n        -------\n        Dict of {path: size} if total=False, or int otherwise, where numbers\n        refer to bytes used.\n        \"\"\"\n        sizes = {}\n        if withdirs and self.isdir(path):\n            # Include top-level directory in output\n            info = self.info(path)\n            sizes[info[\"name\"]] = info[\"size\"]\n        for f in self.find(path, maxdepth=maxdepth, withdirs=withdirs, **kwargs):\n            info = self.info(f)\n            sizes[info[\"name\"]] = info[\"size\"]\n        if total:\n            return sum(sizes.values())\n        else:\n            return sizes\n\n    def glob(self, path, maxdepth=None, **kwargs):\n        \"\"\"\n        Find files by glob-matching.\n\n        If the path ends with '/', only folders are returned.\n\n        We support ``\"**\"``,\n        ``\"?\"`` and ``\"[..]\"``. We do not support ^ for pattern negation.\n\n        The `maxdepth` option is applied on the first `**` found in the path.\n\n        kwargs are passed to ``ls``.\n        \"\"\"\n        if maxdepth is not None and maxdepth < 1:\n            raise ValueError(\"maxdepth must be at least 1\")\n\n        import re\n\n        seps = (os.path.sep, os.path.altsep) if os.path.altsep else (os.path.sep,)\n        ends_with_sep = path.endswith(seps)  # _strip_protocol strips trailing slash\n        path = self._strip_protocol(path)\n        append_slash_to_dirname = ends_with_sep or path.endswith(\n            tuple(sep + \"**\" for sep in seps)\n        )\n        idx_star = path.find(\"*\") if path.find(\"*\") >= 0 else len(path)\n        idx_qmark = path.find(\"?\") if path.find(\"?\") >= 0 else len(path)\n        idx_brace = path.find(\"[\") if path.find(\"[\") >= 0 else len(path)\n\n        min_idx = min(idx_star, idx_qmark, idx_brace)\n\n        detail = kwargs.pop(\"detail\", False)\n\n        if not has_magic(path):\n            if self.exists(path, **kwargs):\n                if not detail:\n                    return [path]\n                else:\n                    return {path: self.info(path, **kwargs)}\n            else:\n                if not detail:\n                    return []  # glob of non-existent returns empty\n                else:\n                    return {}\n        elif \"/\" in path[:min_idx]:\n            min_idx = path[:min_idx].rindex(\"/\")\n            root = path[: min_idx + 1]\n            depth = path[min_idx + 1 :].count(\"/\") + 1\n        else:\n            root = \"\"\n            depth = path[min_idx + 1 :].count(\"/\") + 1\n\n        if \"**\" in path:\n            if maxdepth is not None:\n                idx_double_stars = path.find(\"**\")\n                depth_double_stars = path[idx_double_stars:].count(\"/\") + 1\n                depth = depth - depth_double_stars + maxdepth\n            else:\n                depth = None\n\n        allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\n\n        pattern = glob_translate(path + (\"/\" if ends_with_sep else \"\"))\n        pattern = re.compile(pattern)\n\n        out = {\n            p: info\n            for p, info in sorted(allpaths.items())\n            if pattern.match(\n                (\n                    p + \"/\"\n                    if append_slash_to_dirname and info[\"type\"] == \"directory\"\n                    else p\n                )\n            )\n        }\n\n        if detail:\n            return out\n        else:\n            return list(out)\n\n    def exists(self, path, **kwargs):\n        \"\"\"Is there a file at the given path\"\"\"\n        try:\n            self.info(path, **kwargs)\n            return True\n        except:  # noqa: E722\n            # any exception allowed bar FileNotFoundError?\n            return False\n\n    def lexists(self, path, **kwargs):\n        \"\"\"If there is a file at the given path (including\n        broken links)\"\"\"\n        return self.exists(path)\n\n    def info(self, path, **kwargs):\n        \"\"\"Give details of entry at path\n\n        Returns a single dictionary, with exactly the same information as ``ls``\n        would with ``detail=True``.\n\n        The default implementation should calls ls and could be overridden by a\n        shortcut. kwargs are passed on to ```ls()``.\n\n        Some file systems might not be able to measure the file's size, in\n        which case, the returned dict will include ``'size': None``.\n\n        Returns\n        -------\n        dict with keys: name (full path in the FS), size (in bytes), type (file,\n        directory, or something else) and other FS-specific keys.\n        \"\"\"\n        path = self._strip_protocol(path)\n        out = self.ls(self._parent(path), detail=True, **kwargs)\n        out = [o for o in out if o[\"name\"].rstrip(\"/\") == path]\n        if out:\n            return out[0]\n        out = self.ls(path, detail=True, **kwargs)\n        path = path.rstrip(\"/\")\n        out1 = [o for o in out if o[\"name\"].rstrip(\"/\") == path]\n        if len(out1) == 1:\n            if \"size\" not in out1[0]:\n                out1[0][\"size\"] = None\n            return out1[0]\n        elif len(out1) > 1 or out:\n            return {\"name\": path, \"size\": 0, \"type\": \"directory\"}\n        else:\n            raise FileNotFoundError(path)\n\n    def checksum(self, path):\n        \"\"\"Unique value for current version of file\n\n        If the checksum is the same from one moment to another, the contents\n        are guaranteed to be the same. If the checksum changes, the contents\n        *might* have changed.\n\n        This should normally be overridden; default will probably capture\n        creation/modification timestamp (which would be good) or maybe\n        access timestamp (which would be bad)\n        \"\"\"\n        return int(tokenize(self.info(path)), 16)\n\n    def size(self, path):\n        \"\"\"Size in bytes of file\"\"\"\n        return self.info(path).get(\"size\", None)\n\n    def sizes(self, paths):\n        \"\"\"Size in bytes of each file in a list of paths\"\"\"\n        return [self.size(p) for p in paths]\n\n    def isdir(self, path):\n        \"\"\"Is this entry directory-like?\"\"\"\n        try:\n            return self.info(path)[\"type\"] == \"directory\"\n        except OSError:\n            return False\n\n    def isfile(self, path):\n        \"\"\"Is this entry file-like?\"\"\"\n        try:\n            return self.info(path)[\"type\"] == \"file\"\n        except:  # noqa: E722\n            return False\n\n    def read_text(self, path, encoding=None, errors=None, newline=None, **kwargs):\n        \"\"\"Get the contents of the file as a string.\n\n        Parameters\n        ----------\n        path: str\n            URL of file on this filesystems\n        encoding, errors, newline: same as `open`.\n        \"\"\"\n        with self.open(\n            path,\n            mode=\"r\",\n            encoding=encoding,\n            errors=errors,\n            newline=newline,\n            **kwargs,\n        ) as f:\n            return f.read()\n\n    def write_text(\n        self, path, value, encoding=None, errors=None, newline=None, **kwargs\n    ):\n        \"\"\"Write the text to the given file.\n\n        An existing file will be overwritten.\n\n        Parameters\n        ----------\n        path: str\n            URL of file on this filesystems\n        value: str\n            Text to write.\n        encoding, errors, newline: same as `open`.\n        \"\"\"\n        with self.open(\n            path,\n            mode=\"w\",\n            encoding=encoding,\n            errors=errors,\n            newline=newline,\n            **kwargs,\n        ) as f:\n            return f.write(value)\n\n    def cat_file(self, path, start=None, end=None, **kwargs):\n        \"\"\"Get the content of a file\n\n        Parameters\n        ----------\n        path: URL of file on this filesystems\n        start, end: int\n            Bytes limits of the read. If negative, backwards from end,\n            like usual python slices. Either can be None for start or\n            end of file, respectively\n        kwargs: passed to ``open()``.\n        \"\"\"\n        # explicitly set buffering off?\n        with self.open(path, \"rb\", **kwargs) as f:\n            if start is not None:\n                if start >= 0:\n                    f.seek(start)\n                else:\n                    f.seek(max(0, f.size + start))\n            if end is not None:\n                if end < 0:\n                    end = f.size + end\n                return f.read(end - f.tell())\n            return f.read()\n\n    def pipe_file(self, path, value, **kwargs):\n        \"\"\"Set the bytes of given file\"\"\"\n        with self.open(path, \"wb\", **kwargs) as f:\n            f.write(value)\n\n    def pipe(self, path, value=None, **kwargs):\n        \"\"\"Put value into path\n\n        (counterpart to ``cat``)\n\n        Parameters\n        ----------\n        path: string or dict(str, bytes)\n            If a string, a single remote location to put ``value`` bytes; if a dict,\n            a mapping of {path: bytesvalue}.\n        value: bytes, optional\n            If using a single path, these are the bytes to put there. Ignored if\n            ``path`` is a dict\n        \"\"\"\n        if isinstance(path, str):\n            self.pipe_file(self._strip_protocol(path), value, **kwargs)\n        elif isinstance(path, dict):\n            for k, v in path.items():\n                self.pipe_file(self._strip_protocol(k), v, **kwargs)\n        else:\n            raise ValueError(\"path must be str or dict\")\n\n    def cat_ranges(\n        self, paths, starts, ends, max_gap=None, on_error=\"return\", **kwargs\n    ):\n        \"\"\"Get the contents of byte ranges from one or more files\n\n        Parameters\n        ----------\n        paths: list\n            A list of of filepaths on this filesystems\n        starts, ends: int or list\n            Bytes limits of the read. If using a single int, the same value will be\n            used to read all the specified files.\n        \"\"\"\n        if max_gap is not None:\n            raise NotImplementedError\n        if not isinstance(paths, list):\n            raise TypeError\n        if not isinstance(starts, list):\n            starts = [starts] * len(paths)\n        if not isinstance(ends, list):\n            ends = [ends] * len(paths)\n        if len(starts) != len(paths) or len(ends) != len(paths):\n            raise ValueError\n        out = []\n        for p, s, e in zip(paths, starts, ends):\n            try:\n                out.append(self.cat_file(p, s, e))\n            except Exception as e:\n                if on_error == \"return\":\n                    out.append(e)\n                else:\n                    raise\n        return out\n\n    def cat(self, path, recursive=False, on_error=\"raise\", **kwargs):\n        \"\"\"Fetch (potentially multiple) paths' contents\n\n        Parameters\n        ----------\n        recursive: bool\n            If True, assume the path(s) are directories, and get all the\n            contained files\n        on_error : \"raise\", \"omit\", \"return\"\n            If raise, an underlying exception will be raised (converted to KeyError\n            if the type is in self.missing_exceptions); if omit, keys with exception\n            will simply not be included in the output; if \"return\", all keys are\n            included in the output, but the value will be bytes or an exception\n            instance.\n        kwargs: passed to cat_file\n\n        Returns\n        -------\n        dict of {path: contents} if there are multiple paths\n        or the path has been otherwise expanded\n        \"\"\"\n        paths = self.expand_path(path, recursive=recursive)\n        if (\n            len(paths) > 1\n            or isinstance(path, list)\n            or paths[0] != self._strip_protocol(path)\n        ):\n            out = {}\n            for path in paths:\n                try:\n                    out[path] = self.cat_file(path, **kwargs)\n                except Exception as e:\n                    if on_error == \"raise\":\n                        raise\n                    if on_error == \"return\":\n                        out[path] = e\n            return out\n        else:\n            return self.cat_file(paths[0], **kwargs)\n\n    def get_file(self, rpath, lpath, callback=DEFAULT_CALLBACK, outfile=None, **kwargs):\n        \"\"\"Copy single remote file to local\"\"\"\n        from .implementations.local import LocalFileSystem\n\n        if isfilelike(lpath):\n            outfile = lpath\n        elif self.isdir(rpath):\n            os.makedirs(lpath, exist_ok=True)\n            return None\n\n        fs = LocalFileSystem(auto_mkdir=True)\n        fs.makedirs(fs._parent(lpath), exist_ok=True)\n\n        with self.open(rpath, \"rb\", **kwargs) as f1:\n            if outfile is None:\n                outfile = open(lpath, \"wb\")\n\n            try:\n                callback.set_size(getattr(f1, \"size\", None))\n                data = True\n                while data:\n                    data = f1.read(self.blocksize)\n                    segment_len = outfile.write(data)\n                    if segment_len is None:\n                        segment_len = len(data)\n                    callback.relative_update(segment_len)\n            finally:\n                if not isfilelike(lpath):\n                    outfile.close()\n\n    def get(\n        self,\n        rpath,\n        lpath,\n        recursive=False,\n        callback=DEFAULT_CALLBACK,\n        maxdepth=None,\n        **kwargs,\n    ):\n        \"\"\"Copy file(s) to local.\n\n        Copies a specific file or tree of files (if recursive=True). If lpath\n        ends with a \"/\", it will be assumed to be a directory, and target files\n        will go within. Can submit a list of paths, which may be glob-patterns\n        and will be expanded.\n\n        Calls get_file for each source.\n        \"\"\"\n        if isinstance(lpath, list) and isinstance(rpath, list):\n            # No need to expand paths when both source and destination\n            # are provided as lists\n            rpaths = rpath\n            lpaths = lpath\n        else:\n            from .implementations.local import (\n                LocalFileSystem,\n                make_path_posix,\n                trailing_sep,\n            )\n\n            source_is_str = isinstance(rpath, str)\n            rpaths = self.expand_path(rpath, recursive=recursive, maxdepth=maxdepth)\n            if source_is_str and (not recursive or maxdepth is not None):\n                # Non-recursive glob does not copy directories\n                rpaths = [p for p in rpaths if not (trailing_sep(p) or self.isdir(p))]\n                if not rpaths:\n                    return\n\n            if isinstance(lpath, str):\n                lpath = make_path_posix(lpath)\n\n            source_is_file = len(rpaths) == 1\n            dest_is_dir = isinstance(lpath, str) and (\n                trailing_sep(lpath) or LocalFileSystem().isdir(lpath)\n            )\n\n            exists = source_is_str and (\n                (has_magic(rpath) and source_is_file)\n                or (not has_magic(rpath) and dest_is_dir and not trailing_sep(rpath))\n            )\n            lpaths = other_paths(\n                rpaths,\n                lpath,\n                exists=exists,\n                flatten=not source_is_str,\n            )\n\n        callback.set_size(len(lpaths))\n        for lpath, rpath in callback.wrap(zip(lpaths, rpaths)):\n            with callback.branched(rpath, lpath) as child:\n                self.get_file(rpath, lpath, callback=child, **kwargs)\n\n    def put_file(self, lpath, rpath, callback=DEFAULT_CALLBACK, **kwargs):\n        \"\"\"Copy single file to remote\"\"\"\n        if os.path.isdir(lpath):\n            self.makedirs(rpath, exist_ok=True)\n            return None\n\n        with open(lpath, \"rb\") as f1:\n            size = f1.seek(0, 2)\n            callback.set_size(size)\n            f1.seek(0)\n\n            self.mkdirs(self._parent(os.fspath(rpath)), exist_ok=True)\n            with self.open(rpath, \"wb\", **kwargs) as f2:\n                while f1.tell() < size:\n                    data = f1.read(self.blocksize)\n                    segment_len = f2.write(data)\n                    if segment_len is None:\n                        segment_len = len(data)\n                    callback.relative_update(segment_len)\n\n    def put(\n        self,\n        lpath,\n        rpath,\n        recursive=False,\n        callback=DEFAULT_CALLBACK,\n        maxdepth=None,\n        **kwargs,\n    ):\n        \"\"\"Copy file(s) from local.\n\n        Copies a specific file or tree of files (if recursive=True). If rpath\n        ends with a \"/\", it will be assumed to be a directory, and target files\n        will go within.\n\n        Calls put_file for each source.\n        \"\"\"\n        if isinstance(lpath, list) and isinstance(rpath, list):\n            # No need to expand paths when both source and destination\n            # are provided as lists\n            rpaths = rpath\n            lpaths = lpath\n        else:\n            from .implementations.local import (\n                LocalFileSystem,\n                make_path_posix,\n                trailing_sep,\n            )\n\n            source_is_str = isinstance(lpath, str)\n            if source_is_str:\n                lpath = make_path_posix(lpath)\n            fs = LocalFileSystem()\n            lpaths = fs.expand_path(lpath, recursive=recursive, maxdepth=maxdepth)\n            if source_is_str and (not recursive or maxdepth is not None):\n                # Non-recursive glob does not copy directories\n                lpaths = [p for p in lpaths if not (trailing_sep(p) or fs.isdir(p))]\n                if not lpaths:\n                    return\n\n            source_is_file = len(lpaths) == 1\n            dest_is_dir = isinstance(rpath, str) and (\n                trailing_sep(rpath) or self.isdir(rpath)\n            )\n\n            rpath = (\n                self._strip_protocol(rpath)\n                if isinstance(rpath, str)\n                else [self._strip_protocol(p) for p in rpath]\n            )\n            exists = source_is_str and (\n                (has_magic(lpath) and source_is_file)\n                or (not has_magic(lpath) and dest_is_dir and not trailing_sep(lpath))\n            )\n            rpaths = other_paths(\n                lpaths,\n                rpath,\n                exists=exists,\n                flatten=not source_is_str,\n            )\n\n        callback.set_size(len(rpaths))\n        for lpath, rpath in callback.wrap(zip(lpaths, rpaths)):\n            with callback.branched(lpath, rpath) as child:\n                self.put_file(lpath, rpath, callback=child, **kwargs)\n\n    def head(self, path, size=1024):\n        \"\"\"Get the first ``size`` bytes from file\"\"\"\n        with self.open(path, \"rb\") as f:\n            return f.read(size)\n\n    def tail(self, path, size=1024):\n        \"\"\"Get the last ``size`` bytes from file\"\"\"\n        with self.open(path, \"rb\") as f:\n            f.seek(max(-size, -f.size), 2)\n            return f.read()\n\n    def cp_file(self, path1, path2, **kwargs):\n        raise NotImplementedError\n\n    def copy(\n        self, path1, path2, recursive=False, maxdepth=None, on_error=None, **kwargs\n    ):\n        \"\"\"Copy within two locations in the filesystem\n\n        on_error : \"raise\", \"ignore\"\n            If raise, any not-found exceptions will be raised; if ignore any\n            not-found exceptions will cause the path to be skipped; defaults to\n            raise unless recursive is true, where the default is ignore\n        \"\"\"\n        if on_error is None and recursive:\n            on_error = \"ignore\"\n        elif on_error is None:\n            on_error = \"raise\"\n\n        if isinstance(path1, list) and isinstance(path2, list):\n            # No need to expand paths when both source and destination\n            # are provided as lists\n            paths1 = path1\n            paths2 = path2\n        else:\n            from .implementations.local import trailing_sep\n\n            source_is_str = isinstance(path1, str)\n            paths1 = self.expand_path(path1, recursive=recursive, maxdepth=maxdepth)\n            if source_is_str and (not recursive or maxdepth is not None):\n                # Non-recursive glob does not copy directories\n                paths1 = [p for p in paths1 if not (trailing_sep(p) or self.isdir(p))]\n                if not paths1:\n                    return\n\n            source_is_file = len(paths1) == 1\n            dest_is_dir = isinstance(path2, str) and (\n                trailing_sep(path2) or self.isdir(path2)\n            )\n\n            exists = source_is_str and (\n                (has_magic(path1) and source_is_file)\n                or (not has_magic(path1) and dest_is_dir and not trailing_sep(path1))\n            )\n            paths2 = other_paths(\n                paths1,\n                path2,\n                exists=exists,\n                flatten=not source_is_str,\n            )\n\n        for p1, p2 in zip(paths1, paths2):\n            try:\n                self.cp_file(p1, p2, **kwargs)\n            except FileNotFoundError:\n                if on_error == \"raise\":\n                    raise\n\n    def expand_path(self, path, recursive=False, maxdepth=None, **kwargs):\n        \"\"\"Turn one or more globs or directories into a list of all matching paths\n        to files or directories.\n\n        kwargs are passed to ``glob`` or ``find``, which may in turn call ``ls``\n        \"\"\"\n\n        if maxdepth is not None and maxdepth < 1:\n            raise ValueError(\"maxdepth must be at least 1\")\n\n        if isinstance(path, (str, os.PathLike)):\n            out = self.expand_path([path], recursive, maxdepth)\n        else:\n            out = set()\n            path = [self._strip_protocol(p) for p in path]\n            for p in path:\n                if has_magic(p):\n                    bit = set(self.glob(p, maxdepth=maxdepth, **kwargs))\n                    out |= bit\n                    if recursive:\n                        # glob call above expanded one depth so if maxdepth is defined\n                        # then decrement it in expand_path call below. If it is zero\n                        # after decrementing then avoid expand_path call.\n                        if maxdepth is not None and maxdepth <= 1:\n                            continue\n                        out |= set(\n                            self.expand_path(\n                                list(bit),\n                                recursive=recursive,\n                                maxdepth=maxdepth - 1 if maxdepth is not None else None,\n                                **kwargs,\n                            )\n                        )\n                    continue\n                elif recursive:\n                    rec = set(\n                        self.find(\n                            p, maxdepth=maxdepth, withdirs=True, detail=False, **kwargs\n                        )\n                    )\n                    out |= rec\n                if p not in out and (recursive is False or self.exists(p)):\n                    # should only check once, for the root\n                    out.add(p)\n        if not out:\n            raise FileNotFoundError(path)\n        return sorted(out)\n\n    def mv(self, path1, path2, recursive=False, maxdepth=None, **kwargs):\n        \"\"\"Move file(s) from one location to another\"\"\"\n        if path1 == path2:\n            logger.debug(\"%s mv: The paths are the same, so no files were moved.\", self)\n        else:\n            # explicitly raise exception to prevent data corruption\n            self.copy(\n                path1, path2, recursive=recursive, maxdepth=maxdepth, onerror=\"raise\"\n            )\n            self.rm(path1, recursive=recursive)\n\n    def rm_file(self, path):\n        \"\"\"Delete a file\"\"\"\n        self._rm(path)\n\n    def _rm(self, path):\n        \"\"\"Delete one file\"\"\"\n        # this is the old name for the method, prefer rm_file\n        raise NotImplementedError\n\n    def rm(self, path, recursive=False, maxdepth=None):\n        \"\"\"Delete files.\n\n        Parameters\n        ----------\n        path: str or list of str\n            File(s) to delete.\n        recursive: bool\n            If file(s) are directories, recursively delete contents and then\n            also remove the directory\n        maxdepth: int or None\n            Depth to pass to walk for finding files to delete, if recursive.\n            If None, there will be no limit and infinite recursion may be\n            possible.\n        \"\"\"\n        path = self.expand_path(path, recursive=recursive, maxdepth=maxdepth)\n        for p in reversed(path):\n            self.rm_file(p)\n\n    @classmethod\n    def _parent(cls, path):\n        path = cls._strip_protocol(path)\n        if \"/\" in path:\n            parent = path.rsplit(\"/\", 1)[0].lstrip(cls.root_marker)\n            return cls.root_marker + parent\n        else:\n            return cls.root_marker\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        **kwargs,\n    ):\n        \"\"\"Return raw bytes-mode file-like from the file-system\"\"\"\n        return AbstractBufferedFile(\n            self,\n            path,\n            mode,\n            block_size,\n            autocommit,\n            cache_options=cache_options,\n            **kwargs,\n        )\n\n    def open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        cache_options=None,\n        compression=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Return a file-like object from the filesystem\n\n        The resultant instance must function correctly in a context ``with``\n        block.\n\n        Parameters\n        ----------\n        path: str\n            Target file\n        mode: str like 'rb', 'w'\n            See builtin ``open()``\n        block_size: int\n            Some indication of buffering - this is a value in bytes\n        cache_options : dict, optional\n            Extra arguments to pass through to the cache.\n        compression: string or None\n            If given, open file using compression codec. Can either be a compression\n            name (a key in ``fsspec.compression.compr``) or \"infer\" to guess the\n            compression from the filename suffix.\n        encoding, errors, newline: passed on to TextIOWrapper for text mode\n        \"\"\"\n        import io\n\n        path = self._strip_protocol(path)\n        if \"b\" not in mode:\n            mode = mode.replace(\"t\", \"\") + \"b\"\n\n            text_kwargs = {\n                k: kwargs.pop(k)\n                for k in [\"encoding\", \"errors\", \"newline\"]\n                if k in kwargs\n            }\n            return io.TextIOWrapper(\n                self.open(\n                    path,\n                    mode,\n                    block_size=block_size,\n                    cache_options=cache_options,\n                    compression=compression,\n                    **kwargs,\n                ),\n                **text_kwargs,\n            )\n        else:\n            ac = kwargs.pop(\"autocommit\", not self._intrans)\n            f = self._open(\n                path,\n                mode=mode,\n                block_size=block_size,\n                autocommit=ac,\n                cache_options=cache_options,\n                **kwargs,\n            )\n            if compression is not None:\n                from fsspec.compression import compr\n                from fsspec.core import get_compression\n\n                compression = get_compression(path, compression)\n                compress = compr[compression]\n                f = compress(f, mode=mode[0])\n\n            if not ac and \"r\" not in mode:\n                self.transaction.files.append(f)\n            return f\n\n    def touch(self, path, truncate=True, **kwargs):\n        \"\"\"Create empty file, or update timestamp\n\n        Parameters\n        ----------\n        path: str\n            file location\n        truncate: bool\n            If True, always set file size to 0; if False, update timestamp and\n            leave file unchanged, if backend allows this\n        \"\"\"\n        if truncate or not self.exists(path):\n            with self.open(path, \"wb\", **kwargs):\n                pass\n        else:\n            raise NotImplementedError  # update timestamp, if possible\n\n    def ukey(self, path):\n        \"\"\"Hash of file properties, to tell if it has changed\"\"\"\n        return sha256(str(self.info(path)).encode()).hexdigest()\n\n    def read_block(self, fn, offset, length, delimiter=None):\n        \"\"\"Read a block of bytes from\n\n        Starting at ``offset`` of the file, read ``length`` bytes.  If\n        ``delimiter`` is set then we ensure that the read starts and stops at\n        delimiter boundaries that follow the locations ``offset`` and ``offset\n        + length``.  If ``offset`` is zero then we start at zero.  The\n        bytestring returned WILL include the end delimiter string.\n\n        If offset+length is beyond the eof, reads to eof.\n\n        Parameters\n        ----------\n        fn: string\n            Path to filename\n        offset: int\n            Byte offset to start read\n        length: int\n            Number of bytes to read. If None, read to end.\n        delimiter: bytes (optional)\n            Ensure reading starts and stops at delimiter bytestring\n\n        Examples\n        --------\n        >>> fs.read_block('data/file.csv', 0, 13)  # doctest: +SKIP\n        b'Alice, 100\\\\nBo'\n        >>> fs.read_block('data/file.csv', 0, 13, delimiter=b'\\\\n')  # doctest: +SKIP\n        b'Alice, 100\\\\nBob, 200\\\\n'\n\n        Use ``length=None`` to read to the end of the file.\n        >>> fs.read_block('data/file.csv', 0, None, delimiter=b'\\\\n')  # doctest: +SKIP\n        b'Alice, 100\\\\nBob, 200\\\\nCharlie, 300'\n\n        See Also\n        --------\n        :func:`fsspec.utils.read_block`\n        \"\"\"\n        with self.open(fn, \"rb\") as f:\n            size = f.size\n            if length is None:\n                length = size\n            if size is not None and offset + length > size:\n                length = size - offset\n            return read_block(f, offset, length, delimiter)\n\n    def to_json(self, *, include_password: bool = True) -> str:\n        \"\"\"\n        JSON representation of this filesystem instance.\n\n        Parameters\n        ----------\n        include_password: bool, default True\n            Whether to include the password (if any) in the output.\n\n        Returns\n        -------\n        JSON string with keys ``cls`` (the python location of this class),\n        protocol (text name of this class's protocol, first one in case of\n        multiple), ``args`` (positional args, usually empty), and all other\n        keyword arguments as their own keys.\n\n        Warnings\n        --------\n        Serialized filesystems may contain sensitive information which have been\n        passed to the constructor, such as passwords and tokens. Make sure you\n        store and send them in a secure environment!\n        \"\"\"\n        from .json import FilesystemJSONEncoder\n\n        return json.dumps(\n            self,\n            cls=type(\n                \"_FilesystemJSONEncoder\",\n                (FilesystemJSONEncoder,),\n                {\"include_password\": include_password},\n            ),\n        )\n\n    @staticmethod\n    def from_json(blob: str) -> AbstractFileSystem:\n        \"\"\"\n        Recreate a filesystem instance from JSON representation.\n\n        See ``.to_json()`` for the expected structure of the input.\n\n        Parameters\n        ----------\n        blob: str\n\n        Returns\n        -------\n        file system instance, not necessarily of this particular class.\n\n        Warnings\n        --------\n        This can import arbitrary modules (as determined by the ``cls`` key).\n        Make sure you haven't installed any modules that may execute malicious code\n        at import time.\n        \"\"\"\n        from .json import FilesystemJSONDecoder\n\n        return json.loads(blob, cls=FilesystemJSONDecoder)\n\n    def to_dict(self, *, include_password: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        JSON-serializable dictionary representation of this filesystem instance.\n\n        Parameters\n        ----------\n        include_password: bool, default True\n            Whether to include the password (if any) in the output.\n\n        Returns\n        -------\n        Dictionary with keys ``cls`` (the python location of this class),\n        protocol (text name of this class's protocol, first one in case of\n        multiple), ``args`` (positional args, usually empty), and all other\n        keyword arguments as their own keys.\n\n        Warnings\n        --------\n        Serialized filesystems may contain sensitive information which have been\n        passed to the constructor, such as passwords and tokens. Make sure you\n        store and send them in a secure environment!\n        \"\"\"\n        cls = type(self)\n        proto = self.protocol\n\n        storage_options = dict(self.storage_options)\n        if not include_password:\n            storage_options.pop(\"password\", None)\n\n        return dict(\n            cls=f\"{cls.__module__}:{cls.__name__}\",\n            protocol=proto[0] if isinstance(proto, (tuple, list)) else proto,\n            args=self.storage_args,\n            **storage_options,\n        )\n\n    @staticmethod\n    def from_dict(dct: Dict[str, Any]) -> AbstractFileSystem:\n        \"\"\"\n        Recreate a filesystem instance from dictionary representation.\n\n        See ``.to_dict()`` for the expected structure of the input.\n\n        Parameters\n        ----------\n        dct: Dict[str, Any]\n\n        Returns\n        -------\n        file system instance, not necessarily of this particular class.\n\n        Warnings\n        --------\n        This can import arbitrary modules (as determined by the ``cls`` key).\n        Make sure you haven't installed any modules that may execute malicious code\n        at import time.\n        \"\"\"\n        from .json import FilesystemJSONDecoder\n\n        dct = dict(dct)  # Defensive copy\n\n        cls = FilesystemJSONDecoder.try_resolve_fs_cls(dct)\n        if cls is None:\n            raise ValueError(\"Not a serialized AbstractFileSystem\")\n\n        dct.pop(\"cls\", None)\n        dct.pop(\"protocol\", None)\n\n        return cls(*dct.pop(\"args\", ()), **dct)\n\n    def _get_pyarrow_filesystem(self):\n        \"\"\"\n        Make a version of the FS instance which will be acceptable to pyarrow\n        \"\"\"\n        # all instances already also derive from pyarrow\n        return self\n\n    def get_mapper(self, root=\"\", check=False, create=False, missing_exceptions=None):\n        \"\"\"Create key/value store based on this file-system\n\n        Makes a MutableMapping interface to the FS at the given root path.\n        See ``fsspec.mapping.FSMap`` for further details.\n        \"\"\"\n        from .mapping import FSMap\n\n        return FSMap(\n            root,\n            self,\n            check=check,\n            create=create,\n            missing_exceptions=missing_exceptions,\n        )\n\n    @classmethod\n    def clear_instance_cache(cls):\n        \"\"\"\n        Clear the cache of filesystem instances.\n\n        Notes\n        -----\n        Unless overridden by setting the ``cachable`` class attribute to False,\n        the filesystem class stores a reference to newly created instances. This\n        prevents Python's normal rules around garbage collection from working,\n        since the instances refcount will not drop to zero until\n        ``clear_instance_cache`` is called.\n        \"\"\"\n        cls._cache.clear()\n\n    def created(self, path):\n        \"\"\"Return the created timestamp of a file as a datetime.datetime\"\"\"\n        raise NotImplementedError\n\n    def modified(self, path):\n        \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\"\n        raise NotImplementedError\n\n    # ------------------------------------------------------------------------\n    # Aliases\n\n    def read_bytes(self, path, start=None, end=None, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.cat_file`.\"\"\"\n        return self.cat_file(path, start=start, end=end, **kwargs)\n\n    def write_bytes(self, path, value, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.pipe_file`.\"\"\"\n        self.pipe_file(path, value, **kwargs)\n\n    def makedir(self, path, create_parents=True, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.mkdir`.\"\"\"\n        return self.mkdir(path, create_parents=create_parents, **kwargs)\n\n    def mkdirs(self, path, exist_ok=False):\n        \"\"\"Alias of `AbstractFileSystem.makedirs`.\"\"\"\n        return self.makedirs(path, exist_ok=exist_ok)\n\n    def listdir(self, path, detail=True, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.ls`.\"\"\"\n        return self.ls(path, detail=detail, **kwargs)\n\n    def cp(self, path1, path2, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.copy`.\"\"\"\n        return self.copy(path1, path2, **kwargs)\n\n    def move(self, path1, path2, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.mv`.\"\"\"\n        return self.mv(path1, path2, **kwargs)\n\n    def stat(self, path, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.info`.\"\"\"\n        return self.info(path, **kwargs)\n\n    def disk_usage(self, path, total=True, maxdepth=None, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.du`.\"\"\"\n        return self.du(path, total=total, maxdepth=maxdepth, **kwargs)\n\n    def rename(self, path1, path2, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.mv`.\"\"\"\n        return self.mv(path1, path2, **kwargs)\n\n    def delete(self, path, recursive=False, maxdepth=None):\n        \"\"\"Alias of `AbstractFileSystem.rm`.\"\"\"\n        return self.rm(path, recursive=recursive, maxdepth=maxdepth)\n\n    def upload(self, lpath, rpath, recursive=False, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.put`.\"\"\"\n        return self.put(lpath, rpath, recursive=recursive, **kwargs)\n\n    def download(self, rpath, lpath, recursive=False, **kwargs):\n        \"\"\"Alias of `AbstractFileSystem.get`.\"\"\"\n        return self.get(rpath, lpath, recursive=recursive, **kwargs)\n\n    def sign(self, path, expiration=100, **kwargs):\n        \"\"\"Create a signed URL representing the given path\n\n        Some implementations allow temporary URLs to be generated, as a\n        way of delegating credentials.\n\n        Parameters\n        ----------\n        path : str\n             The path on the filesystem\n        expiration : int\n            Number of seconds to enable the URL for (if supported)\n\n        Returns\n        -------\n        URL : str\n            The signed URL\n\n        Raises\n        ------\n        NotImplementedError : if method is not implemented for a filesystem\n        \"\"\"\n        raise NotImplementedError(\"Sign is not implemented for this filesystem\")\n\n    def _isfilestore(self):\n        # Originally inherited from pyarrow DaskFileSystem. Keeping this\n        # here for backwards compatibility as long as pyarrow uses its\n        # legacy fsspec-compatible filesystems and thus accepts fsspec\n        # filesystems as well\n        return False\n\n\nclass AbstractBufferedFile(io.IOBase):\n    \"\"\"Convenient class to derive from to provide buffering\n\n    In the case that the backend does not provide a pythonic file-like object\n    already, this class contains much of the logic to build one. The only\n    methods that need to be overridden are ``_upload_chunk``,\n    ``_initiate_upload`` and ``_fetch_range``.\n    \"\"\"\n\n    DEFAULT_BLOCK_SIZE = 5 * 2**20\n    _details = None\n\n    def __init__(\n        self,\n        fs,\n        path,\n        mode=\"rb\",\n        block_size=\"default\",\n        autocommit=True,\n        cache_type=\"readahead\",\n        cache_options=None,\n        size=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Template for files with buffered reading and writing\n\n        Parameters\n        ----------\n        fs: instance of FileSystem\n        path: str\n            location in file-system\n        mode: str\n            Normal file modes. Currently only 'wb', 'ab' or 'rb'. Some file\n            systems may be read-only, and some may not support append.\n        block_size: int\n            Buffer size for reading or writing, 'default' for class default\n        autocommit: bool\n            Whether to write to final destination; may only impact what\n            happens when file is being closed.\n        cache_type: {\"readahead\", \"none\", \"mmap\", \"bytes\"}, default \"readahead\"\n            Caching policy in read mode. See the definitions in ``core``.\n        cache_options : dict\n            Additional options passed to the constructor for the cache specified\n            by `cache_type`.\n        size: int\n            If given and in read mode, suppressed having to look up the file size\n        kwargs:\n            Gets stored as self.kwargs\n        \"\"\"\n        from .core import caches\n\n        self.path = path\n        self.fs = fs\n        self.mode = mode\n        self.blocksize = (\n            self.DEFAULT_BLOCK_SIZE if block_size in [\"default\", None] else block_size\n        )\n        self.loc = 0\n        self.autocommit = autocommit\n        self.end = None\n        self.start = None\n        self.closed = False\n\n        if cache_options is None:\n            cache_options = {}\n\n        if \"trim\" in kwargs:\n            warnings.warn(\n                \"Passing 'trim' to control the cache behavior has been deprecated. \"\n                \"Specify it within the 'cache_options' argument instead.\",\n                FutureWarning,\n            )\n            cache_options[\"trim\"] = kwargs.pop(\"trim\")\n\n        self.kwargs = kwargs\n\n        if mode not in {\"ab\", \"rb\", \"wb\"}:\n            raise NotImplementedError(\"File mode not supported\")\n        if mode == \"rb\":\n            if size is not None:\n                self.size = size\n            else:\n                self.size = self.details[\"size\"]\n            self.cache = caches[cache_type](\n                self.blocksize, self._fetch_range, self.size, **cache_options\n            )\n        else:\n            self.buffer = io.BytesIO()\n            self.offset = None\n            self.forced = False\n            self.location = None\n\n    @property\n    def details(self):\n        if self._details is None:\n            self._details = self.fs.info(self.path)\n        return self._details\n\n    @details.setter\n    def details(self, value):\n        self._details = value\n        self.size = value[\"size\"]\n\n    @property\n    def full_name(self):\n        return _unstrip_protocol(self.path, self.fs)\n\n    @property\n    def closed(self):\n        # get around this attr being read-only in IOBase\n        # use getattr here, since this can be called during del\n        return getattr(self, \"_closed\", True)\n\n    @closed.setter\n    def closed(self, c):\n        self._closed = c\n\n    def __hash__(self):\n        if \"w\" in self.mode:\n            return id(self)\n        else:\n            return int(tokenize(self.details), 16)\n\n    def __eq__(self, other):\n        \"\"\"Files are equal if they have the same checksum, only in read mode\"\"\"\n        if self is other:\n            return True\n        return (\n            isinstance(other, type(self))\n            and self.mode == \"rb\"\n            and other.mode == \"rb\"\n            and hash(self) == hash(other)\n        )\n\n    def commit(self):\n        \"\"\"Move from temp to final destination\"\"\"\n\n    def discard(self):\n        \"\"\"Throw away temporary file\"\"\"\n\n    def info(self):\n        \"\"\"File information about this path\"\"\"\n        if \"r\" in self.mode:\n            return self.details\n        else:\n            raise ValueError(\"Info not available while writing\")\n\n    def tell(self):\n        \"\"\"Current file location\"\"\"\n        return self.loc\n\n    def seek(self, loc, whence=0):\n        \"\"\"Set current file location\n\n        Parameters\n        ----------\n        loc: int\n            byte location\n        whence: {0, 1, 2}\n            from start of file, current location or end of file, resp.\n        \"\"\"\n        loc = int(loc)\n        if not self.mode == \"rb\":\n            raise OSError(ESPIPE, \"Seek only available in read mode\")\n        if whence == 0:\n            nloc = loc\n        elif whence == 1:\n            nloc = self.loc + loc\n        elif whence == 2:\n            nloc = self.size + loc\n        else:\n            raise ValueError(f\"invalid whence ({whence}, should be 0, 1 or 2)\")\n        if nloc < 0:\n            raise ValueError(\"Seek before start of file\")\n        self.loc = nloc\n        return self.loc\n\n    def write(self, data):\n        \"\"\"\n        Write data to buffer.\n\n        Buffer only sent on flush() or if buffer is greater than\n        or equal to blocksize.\n\n        Parameters\n        ----------\n        data: bytes\n            Set of bytes to be written.\n        \"\"\"\n        if self.mode not in {\"wb\", \"ab\"}:\n            raise ValueError(\"File not in write mode\")\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file.\")\n        if self.forced:\n            raise ValueError(\"This file has been force-flushed, can only close\")\n        out = self.buffer.write(data)\n        self.loc += out\n        if self.buffer.tell() >= self.blocksize:\n            self.flush()\n        return out\n\n    def flush(self, force=False):\n        \"\"\"\n        Write buffered data to backend store.\n\n        Writes the current buffer, if it is larger than the block-size, or if\n        the file is being closed.\n\n        Parameters\n        ----------\n        force: bool\n            When closing, write the last block even if it is smaller than\n            blocks are allowed to be. Disallows further writing to this file.\n        \"\"\"\n\n        if self.closed:\n            raise ValueError(\"Flush on closed file\")\n        if force and self.forced:\n            raise ValueError(\"Force flush cannot be called more than once\")\n        if force:\n            self.forced = True\n\n        if self.mode not in {\"wb\", \"ab\"}:\n            # no-op to flush on read-mode\n            return\n\n        if not force and self.buffer.tell() < self.blocksize:\n            # Defer write on small block\n            return\n\n        if self.offset is None:\n            # Initialize a multipart upload\n            self.offset = 0\n            try:\n                self._initiate_upload()\n            except:  # noqa: E722\n                self.closed = True\n                raise\n\n        if self._upload_chunk(final=force) is not False:\n            self.offset += self.buffer.seek(0, 2)\n            self.buffer = io.BytesIO()\n\n    def _upload_chunk(self, final=False):\n        \"\"\"Write one part of a multi-block file upload\n\n        Parameters\n        ==========\n        final: bool\n            This is the last block, so should complete file, if\n            self.autocommit is True.\n        \"\"\"\n        # may not yet have been initialized, may need to call _initialize_upload\n\n    def _initiate_upload(self):\n        \"\"\"Create remote file/upload\"\"\"\n        pass\n\n    def _fetch_range(self, start, end):\n        \"\"\"Get the specified set of bytes from remote\"\"\"\n        raise NotImplementedError\n\n    def read(self, length=-1):\n        \"\"\"\n        Return data from cache, or fetch pieces as necessary\n\n        Parameters\n        ----------\n        length: int (-1)\n            Number of bytes to read; if <0, all remaining bytes.\n        \"\"\"\n        length = -1 if length is None else int(length)\n        if self.mode != \"rb\":\n            raise ValueError(\"File not in read mode\")\n        if length < 0:\n            length = self.size - self.loc\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file.\")\n        if length == 0:\n            # don't even bother calling fetch\n            return b\"\"\n        out = self.cache._fetch(self.loc, self.loc + length)\n\n        logger.debug(\n            \"%s read: %i - %i %s\",\n            self,\n            self.loc,\n            self.loc + length,\n            self.cache._log_stats(),\n        )\n        self.loc += len(out)\n        return out\n\n    def readinto(self, b):\n        \"\"\"mirrors builtin file's readinto method\n\n        https://docs.python.org/3/library/io.html#io.RawIOBase.readinto\n        \"\"\"\n        out = memoryview(b).cast(\"B\")\n        data = self.read(out.nbytes)\n        out[: len(data)] = data\n        return len(data)\n\n    def readuntil(self, char=b\"\\n\", blocks=None):\n        \"\"\"Return data between current position and first occurrence of char\n\n        char is included in the output, except if the end of the tile is\n        encountered first.\n\n        Parameters\n        ----------\n        char: bytes\n            Thing to find\n        blocks: None or int\n            How much to read in each go. Defaults to file blocksize - which may\n            mean a new read on every call.\n        \"\"\"\n        out = []\n        while True:\n            start = self.tell()\n            part = self.read(blocks or self.blocksize)\n            if len(part) == 0:\n                break\n            found = part.find(char)\n            if found > -1:\n                out.append(part[: found + len(char)])\n                self.seek(start + found + len(char))\n                break\n            out.append(part)\n        return b\"\".join(out)\n\n    def readline(self):\n        \"\"\"Read until first occurrence of newline character\n\n        Note that, because of character encoding, this is not necessarily a\n        true line ending.\n        \"\"\"\n        return self.readuntil(b\"\\n\")\n\n    def __next__(self):\n        out = self.readline()\n        if out:\n            return out\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    def readlines(self):\n        \"\"\"Return all data, split by the newline character\"\"\"\n        data = self.read()\n        lines = data.split(b\"\\n\")\n        out = [l + b\"\\n\" for l in lines[:-1]]\n        if data.endswith(b\"\\n\"):\n            return out\n        else:\n            return out + [lines[-1]]\n        # return list(self)  ???\n\n    def readinto1(self, b):\n        return self.readinto(b)\n\n    def close(self):\n        \"\"\"Close file\n\n        Finalizes writes, discards cache\n        \"\"\"\n        if getattr(self, \"_unclosable\", False):\n            return\n        if self.closed:\n            return\n        if self.mode == \"rb\":\n            self.cache = None\n        else:\n            if not self.forced:\n                self.flush(force=True)\n\n            if self.fs is not None:\n                self.fs.invalidate_cache(self.path)\n                self.fs.invalidate_cache(self.fs._parent(self.path))\n\n        self.closed = True\n\n    def readable(self):\n        \"\"\"Whether opened for reading\"\"\"\n        return self.mode == \"rb\" and not self.closed\n\n    def seekable(self):\n        \"\"\"Whether is seekable (only in read mode)\"\"\"\n        return self.readable()\n\n    def writable(self):\n        \"\"\"Whether opened for writing\"\"\"\n        return self.mode in {\"wb\", \"ab\"} and not self.closed\n\n    def __del__(self):\n        if not self.closed:\n            self.close()\n\n    def __str__(self):\n        return f\"<File-like object {type(self.fs).__name__}, {self.path}>\"\n\n    __repr__ = __str__\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n", "fsspec/utils.py": "from __future__ import annotations\n\nimport contextlib\nimport logging\nimport math\nimport os\nimport pathlib\nimport re\nimport sys\nimport tempfile\nfrom functools import partial\nfrom hashlib import md5\nfrom importlib.metadata import version\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Iterable,\n    Iterator,\n    Sequence,\n    TypeVar,\n)\nfrom urllib.parse import urlsplit\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeGuard\n\n    from fsspec.spec import AbstractFileSystem\n\n\nDEFAULT_BLOCK_SIZE = 5 * 2**20\n\nT = TypeVar(\"T\")\n\n\ndef infer_storage_options(\n    urlpath: str, inherit_storage_options: dict[str, Any] | None = None\n) -> dict[str, Any]:\n    \"\"\"Infer storage options from URL path and merge it with existing storage\n    options.\n\n    Parameters\n    ----------\n    urlpath: str or unicode\n        Either local absolute file path or URL (hdfs://namenode:8020/file.csv)\n    inherit_storage_options: dict (optional)\n        Its contents will get merged with the inferred information from the\n        given path\n\n    Returns\n    -------\n    Storage options dict.\n\n    Examples\n    --------\n    >>> infer_storage_options('/mnt/datasets/test.csv')  # doctest: +SKIP\n    {\"protocol\": \"file\", \"path\", \"/mnt/datasets/test.csv\"}\n    >>> infer_storage_options(\n    ...     'hdfs://username:pwd@node:123/mnt/datasets/test.csv?q=1',\n    ...     inherit_storage_options={'extra': 'value'},\n    ... )  # doctest: +SKIP\n    {\"protocol\": \"hdfs\", \"username\": \"username\", \"password\": \"pwd\",\n    \"host\": \"node\", \"port\": 123, \"path\": \"/mnt/datasets/test.csv\",\n    \"url_query\": \"q=1\", \"extra\": \"value\"}\n    \"\"\"\n    # Handle Windows paths including disk name in this special case\n    if (\n        re.match(r\"^[a-zA-Z]:[\\\\/]\", urlpath)\n        or re.match(r\"^[a-zA-Z0-9]+://\", urlpath) is None\n    ):\n        return {\"protocol\": \"file\", \"path\": urlpath}\n\n    parsed_path = urlsplit(urlpath)\n    protocol = parsed_path.scheme or \"file\"\n    if parsed_path.fragment:\n        path = \"#\".join([parsed_path.path, parsed_path.fragment])\n    else:\n        path = parsed_path.path\n    if protocol == \"file\":\n        # Special case parsing file protocol URL on Windows according to:\n        # https://msdn.microsoft.com/en-us/library/jj710207.aspx\n        windows_path = re.match(r\"^/([a-zA-Z])[:|]([\\\\/].*)$\", path)\n        if windows_path:\n            path = \"%s:%s\" % windows_path.groups()\n\n    if protocol in [\"http\", \"https\"]:\n        # for HTTP, we don't want to parse, as requests will anyway\n        return {\"protocol\": protocol, \"path\": urlpath}\n\n    options: dict[str, Any] = {\"protocol\": protocol, \"path\": path}\n\n    if parsed_path.netloc:\n        # Parse `hostname` from netloc manually because `parsed_path.hostname`\n        # lowercases the hostname which is not always desirable (e.g. in S3):\n        # https://github.com/dask/dask/issues/1417\n        options[\"host\"] = parsed_path.netloc.rsplit(\"@\", 1)[-1].rsplit(\":\", 1)[0]\n\n        if protocol in (\"s3\", \"s3a\", \"gcs\", \"gs\"):\n            options[\"path\"] = options[\"host\"] + options[\"path\"]\n        else:\n            options[\"host\"] = options[\"host\"]\n        if parsed_path.port:\n            options[\"port\"] = parsed_path.port\n        if parsed_path.username:\n            options[\"username\"] = parsed_path.username\n        if parsed_path.password:\n            options[\"password\"] = parsed_path.password\n\n    if parsed_path.query:\n        options[\"url_query\"] = parsed_path.query\n    if parsed_path.fragment:\n        options[\"url_fragment\"] = parsed_path.fragment\n\n    if inherit_storage_options:\n        update_storage_options(options, inherit_storage_options)\n\n    return options\n\n\ndef update_storage_options(\n    options: dict[str, Any], inherited: dict[str, Any] | None = None\n) -> None:\n    if not inherited:\n        inherited = {}\n    collisions = set(options) & set(inherited)\n    if collisions:\n        for collision in collisions:\n            if options.get(collision) != inherited.get(collision):\n                raise KeyError(\n                    f\"Collision between inferred and specified storage \"\n                    f\"option:\\n{collision}\"\n                )\n    options.update(inherited)\n\n\n# Compression extensions registered via fsspec.compression.register_compression\ncompressions: dict[str, str] = {}\n\n\ndef infer_compression(filename: str) -> str | None:\n    \"\"\"Infer compression, if available, from filename.\n\n    Infer a named compression type, if registered and available, from filename\n    extension. This includes builtin (gz, bz2, zip) compressions, as well as\n    optional compressions. See fsspec.compression.register_compression.\n    \"\"\"\n    extension = os.path.splitext(filename)[-1].strip(\".\").lower()\n    if extension in compressions:\n        return compressions[extension]\n    return None\n\n\ndef build_name_function(max_int: float) -> Callable[[int], str]:\n    \"\"\"Returns a function that receives a single integer\n    and returns it as a string padded by enough zero characters\n    to align with maximum possible integer\n\n    >>> name_f = build_name_function(57)\n\n    >>> name_f(7)\n    '07'\n    >>> name_f(31)\n    '31'\n    >>> build_name_function(1000)(42)\n    '0042'\n    >>> build_name_function(999)(42)\n    '042'\n    >>> build_name_function(0)(0)\n    '0'\n    \"\"\"\n    # handle corner cases max_int is 0 or exact power of 10\n    max_int += 1e-8\n\n    pad_length = int(math.ceil(math.log10(max_int)))\n\n    def name_function(i: int) -> str:\n        return str(i).zfill(pad_length)\n\n    return name_function\n\n\ndef seek_delimiter(file: IO[bytes], delimiter: bytes, blocksize: int) -> bool:\n    r\"\"\"Seek current file to file start, file end, or byte after delimiter seq.\n\n    Seeks file to next chunk delimiter, where chunks are defined on file start,\n    a delimiting sequence, and file end. Use file.tell() to see location afterwards.\n    Note that file start is a valid split, so must be at offset > 0 to seek for\n    delimiter.\n\n    Parameters\n    ----------\n    file: a file\n    delimiter: bytes\n        a delimiter like ``b'\\n'`` or message sentinel, matching file .read() type\n    blocksize: int\n        Number of bytes to read from the file at once.\n\n\n    Returns\n    -------\n    Returns True if a delimiter was found, False if at file start or end.\n\n    \"\"\"\n\n    if file.tell() == 0:\n        # beginning-of-file, return without seek\n        return False\n\n    # Interface is for binary IO, with delimiter as bytes, but initialize last\n    # with result of file.read to preserve compatibility with text IO.\n    last: bytes | None = None\n    while True:\n        current = file.read(blocksize)\n        if not current:\n            # end-of-file without delimiter\n            return False\n        full = last + current if last else current\n        try:\n            if delimiter in full:\n                i = full.index(delimiter)\n                file.seek(file.tell() - (len(full) - i) + len(delimiter))\n                return True\n            elif len(current) < blocksize:\n                # end-of-file without delimiter\n                return False\n        except (OSError, ValueError):\n            pass\n        last = full[-len(delimiter) :]\n\n\ndef read_block(\n    f: IO[bytes],\n    offset: int,\n    length: int | None,\n    delimiter: bytes | None = None,\n    split_before: bool = False,\n) -> bytes:\n    \"\"\"Read a block of bytes from a file\n\n    Parameters\n    ----------\n    f: File\n        Open file\n    offset: int\n        Byte offset to start read\n    length: int\n        Number of bytes to read, read through end of file if None\n    delimiter: bytes (optional)\n        Ensure reading starts and stops at delimiter bytestring\n    split_before: bool (optional)\n        Start/stop read *before* delimiter bytestring.\n\n\n    If using the ``delimiter=`` keyword argument we ensure that the read\n    starts and stops at delimiter boundaries that follow the locations\n    ``offset`` and ``offset + length``.  If ``offset`` is zero then we\n    start at zero, regardless of delimiter.  The bytestring returned WILL\n    include the terminating delimiter string.\n\n    Examples\n    --------\n\n    >>> from io import BytesIO  # doctest: +SKIP\n    >>> f = BytesIO(b'Alice, 100\\\\nBob, 200\\\\nCharlie, 300')  # doctest: +SKIP\n    >>> read_block(f, 0, 13)  # doctest: +SKIP\n    b'Alice, 100\\\\nBo'\n\n    >>> read_block(f, 0, 13, delimiter=b'\\\\n')  # doctest: +SKIP\n    b'Alice, 100\\\\nBob, 200\\\\n'\n\n    >>> read_block(f, 10, 10, delimiter=b'\\\\n')  # doctest: +SKIP\n    b'Bob, 200\\\\nCharlie, 300'\n    \"\"\"\n    if delimiter:\n        f.seek(offset)\n        found_start_delim = seek_delimiter(f, delimiter, 2**16)\n        if length is None:\n            return f.read()\n        start = f.tell()\n        length -= start - offset\n\n        f.seek(start + length)\n        found_end_delim = seek_delimiter(f, delimiter, 2**16)\n        end = f.tell()\n\n        # Adjust split location to before delimiter if seek found the\n        # delimiter sequence, not start or end of file.\n        if found_start_delim and split_before:\n            start -= len(delimiter)\n\n        if found_end_delim and split_before:\n            end -= len(delimiter)\n\n        offset = start\n        length = end - start\n\n    f.seek(offset)\n\n    # TODO: allow length to be None and read to the end of the file?\n    assert length is not None\n    b = f.read(length)\n    return b\n\n\ndef tokenize(*args: Any, **kwargs: Any) -> str:\n    \"\"\"Deterministic token\n\n    (modified from dask.base)\n\n    >>> tokenize([1, 2, '3'])\n    '9d71491b50023b06fc76928e6eddb952'\n\n    >>> tokenize('Hello') == tokenize('Hello')\n    True\n    \"\"\"\n    if kwargs:\n        args += (kwargs,)\n    try:\n        h = md5(str(args).encode())\n    except ValueError:\n        # FIPS systems: https://github.com/fsspec/filesystem_spec/issues/380\n        h = md5(str(args).encode(), usedforsecurity=False)\n    return h.hexdigest()\n\n\ndef stringify_path(filepath: str | os.PathLike[str] | pathlib.Path) -> str:\n    \"\"\"Attempt to convert a path-like object to a string.\n\n    Parameters\n    ----------\n    filepath: object to be converted\n\n    Returns\n    -------\n    filepath_str: maybe a string version of the object\n\n    Notes\n    -----\n    Objects supporting the fspath protocol are coerced according to its\n    __fspath__ method.\n\n    For backwards compatibility with older Python version, pathlib.Path\n    objects are specially coerced.\n\n    Any other object is passed through unchanged, which includes bytes,\n    strings, buffers, or anything else that's not even path-like.\n    \"\"\"\n    if isinstance(filepath, str):\n        return filepath\n    elif hasattr(filepath, \"__fspath__\"):\n        return filepath.__fspath__()\n    elif hasattr(filepath, \"path\"):\n        return filepath.path\n    else:\n        return filepath  # type: ignore[return-value]\n\n\ndef make_instance(\n    cls: Callable[..., T], args: Sequence[Any], kwargs: dict[str, Any]\n) -> T:\n    inst = cls(*args, **kwargs)\n    inst._determine_worker()  # type: ignore[attr-defined]\n    return inst\n\n\ndef common_prefix(paths: Iterable[str]) -> str:\n    \"\"\"For a list of paths, find the shortest prefix common to all\"\"\"\n    parts = [p.split(\"/\") for p in paths]\n    lmax = min(len(p) for p in parts)\n    end = 0\n    for i in range(lmax):\n        end = all(p[i] == parts[0][i] for p in parts)\n        if not end:\n            break\n    i += end\n    return \"/\".join(parts[0][:i])\n\n\ndef other_paths(\n    paths: list[str],\n    path2: str | list[str],\n    exists: bool = False,\n    flatten: bool = False,\n) -> list[str]:\n    \"\"\"In bulk file operations, construct a new file tree from a list of files\n\n    Parameters\n    ----------\n    paths: list of str\n        The input file tree\n    path2: str or list of str\n        Root to construct the new list in. If this is already a list of str, we just\n        assert it has the right number of elements.\n    exists: bool (optional)\n        For a str destination, it is already exists (and is a dir), files should\n        end up inside.\n    flatten: bool (optional)\n        Whether to flatten the input directory tree structure so that the output files\n        are in the same directory.\n\n    Returns\n    -------\n    list of str\n    \"\"\"\n\n    if isinstance(path2, str):\n        path2 = path2.rstrip(\"/\")\n\n        if flatten:\n            path2 = [\"/\".join((path2, p.split(\"/\")[-1])) for p in paths]\n        else:\n            cp = common_prefix(paths)\n            if exists:\n                cp = cp.rsplit(\"/\", 1)[0]\n            if not cp and all(not s.startswith(\"/\") for s in paths):\n                path2 = [\"/\".join([path2, p]) for p in paths]\n            else:\n                path2 = [p.replace(cp, path2, 1) for p in paths]\n    else:\n        assert len(paths) == len(path2)\n    return path2\n\n\ndef is_exception(obj: Any) -> bool:\n    return isinstance(obj, BaseException)\n\n\ndef isfilelike(f: Any) -> TypeGuard[IO[bytes]]:\n    for attr in [\"read\", \"close\", \"tell\"]:\n        if not hasattr(f, attr):\n            return False\n    return True\n\n\ndef get_protocol(url: str) -> str:\n    url = stringify_path(url)\n    parts = re.split(r\"(\\:\\:|\\://)\", url, maxsplit=1)\n    if len(parts) > 1:\n        return parts[0]\n    return \"file\"\n\n\ndef can_be_local(path: str) -> bool:\n    \"\"\"Can the given URL be used with open_local?\"\"\"\n    from fsspec import get_filesystem_class\n\n    try:\n        return getattr(get_filesystem_class(get_protocol(path)), \"local_file\", False)\n    except (ValueError, ImportError):\n        # not in registry or import failed\n        return False\n\n\ndef get_package_version_without_import(name: str) -> str | None:\n    \"\"\"For given package name, try to find the version without importing it\n\n    Import and package.__version__ is still the backup here, so an import\n    *might* happen.\n\n    Returns either the version string, or None if the package\n    or the version was not readily  found.\n    \"\"\"\n    if name in sys.modules:\n        mod = sys.modules[name]\n        if hasattr(mod, \"__version__\"):\n            return mod.__version__\n    try:\n        return version(name)\n    except:  # noqa: E722\n        pass\n    try:\n        import importlib\n\n        mod = importlib.import_module(name)\n        return mod.__version__\n    except (ImportError, AttributeError):\n        return None\n\n\ndef setup_logging(\n    logger: logging.Logger | None = None,\n    logger_name: str | None = None,\n    level: str = \"DEBUG\",\n    clear: bool = True,\n) -> logging.Logger:\n    if logger is None and logger_name is None:\n        raise ValueError(\"Provide either logger object or logger name\")\n    logger = logger or logging.getLogger(logger_name)\n    handle = logging.StreamHandler()\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(funcName)s -- %(message)s\"\n    )\n    handle.setFormatter(formatter)\n    if clear:\n        logger.handlers.clear()\n    logger.addHandler(handle)\n    logger.setLevel(level)\n    return logger\n\n\ndef _unstrip_protocol(name: str, fs: AbstractFileSystem) -> str:\n    return fs.unstrip_protocol(name)\n\n\ndef mirror_from(\n    origin_name: str, methods: Iterable[str]\n) -> Callable[[type[T]], type[T]]:\n    \"\"\"Mirror attributes and methods from the given\n    origin_name attribute of the instance to the\n    decorated class\"\"\"\n\n    def origin_getter(method: str, self: Any) -> Any:\n        origin = getattr(self, origin_name)\n        return getattr(origin, method)\n\n    def wrapper(cls: type[T]) -> type[T]:\n        for method in methods:\n            wrapped_method = partial(origin_getter, method)\n            setattr(cls, method, property(wrapped_method))\n        return cls\n\n    return wrapper\n\n\n@contextlib.contextmanager\ndef nullcontext(obj: T) -> Iterator[T]:\n    yield obj\n\n\ndef merge_offset_ranges(\n    paths: list[str],\n    starts: list[int] | int,\n    ends: list[int] | int,\n    max_gap: int = 0,\n    max_block: int | None = None,\n    sort: bool = True,\n) -> tuple[list[str], list[int], list[int]]:\n    \"\"\"Merge adjacent byte-offset ranges when the inter-range\n    gap is <= `max_gap`, and when the merged byte range does not\n    exceed `max_block` (if specified). By default, this function\n    will re-order the input paths and byte ranges to ensure sorted\n    order. If the user can guarantee that the inputs are already\n    sorted, passing `sort=False` will skip the re-ordering.\n    \"\"\"\n    # Check input\n    if not isinstance(paths, list):\n        raise TypeError\n    if not isinstance(starts, list):\n        starts = [starts] * len(paths)\n    if not isinstance(ends, list):\n        ends = [ends] * len(paths)\n    if len(starts) != len(paths) or len(ends) != len(paths):\n        raise ValueError\n\n    # Early Return\n    if len(starts) <= 1:\n        return paths, starts, ends\n\n    starts = [s or 0 for s in starts]\n    # Sort by paths and then ranges if `sort=True`\n    if sort:\n        paths, starts, ends = (\n            list(v)\n            for v in zip(\n                *sorted(\n                    zip(paths, starts, ends),\n                )\n            )\n        )\n\n    if paths:\n        # Loop through the coupled `paths`, `starts`, and\n        # `ends`, and merge adjacent blocks when appropriate\n        new_paths = paths[:1]\n        new_starts = starts[:1]\n        new_ends = ends[:1]\n        for i in range(1, len(paths)):\n            if paths[i] == paths[i - 1] and new_ends[-1] is None:\n                continue\n            elif (\n                paths[i] != paths[i - 1]\n                or ((starts[i] - new_ends[-1]) > max_gap)\n                or (max_block is not None and (ends[i] - new_starts[-1]) > max_block)\n            ):\n                # Cannot merge with previous block.\n                # Add new `paths`, `starts`, and `ends` elements\n                new_paths.append(paths[i])\n                new_starts.append(starts[i])\n                new_ends.append(ends[i])\n            else:\n                # Merge with previous block by updating the\n                # last element of `ends`\n                new_ends[-1] = ends[i]\n        return new_paths, new_starts, new_ends\n\n    # `paths` is empty. Just return input lists\n    return paths, starts, ends\n\n\ndef file_size(filelike: IO[bytes]) -> int:\n    \"\"\"Find length of any open read-mode file-like\"\"\"\n    pos = filelike.tell()\n    try:\n        return filelike.seek(0, 2)\n    finally:\n        filelike.seek(pos)\n\n\n@contextlib.contextmanager\ndef atomic_write(path: str, mode: str = \"wb\"):\n    \"\"\"\n    A context manager that opens a temporary file next to `path` and, on exit,\n    replaces `path` with the temporary file, thereby updating `path`\n    atomically.\n    \"\"\"\n    fd, fn = tempfile.mkstemp(\n        dir=os.path.dirname(path), prefix=os.path.basename(path) + \"-\"\n    )\n    try:\n        with open(fd, mode) as fp:\n            yield fp\n    except BaseException:\n        with contextlib.suppress(FileNotFoundError):\n            os.unlink(fn)\n        raise\n    else:\n        os.replace(fn, path)\n\n\ndef _translate(pat, STAR, QUESTION_MARK):\n    # Copied from: https://github.com/python/cpython/pull/106703.\n    res: list[str] = []\n    add = res.append\n    i, n = 0, len(pat)\n    while i < n:\n        c = pat[i]\n        i = i + 1\n        if c == \"*\":\n            # compress consecutive `*` into one\n            if (not res) or res[-1] is not STAR:\n                add(STAR)\n        elif c == \"?\":\n            add(QUESTION_MARK)\n        elif c == \"[\":\n            j = i\n            if j < n and pat[j] == \"!\":\n                j = j + 1\n            if j < n and pat[j] == \"]\":\n                j = j + 1\n            while j < n and pat[j] != \"]\":\n                j = j + 1\n            if j >= n:\n                add(\"\\\\[\")\n            else:\n                stuff = pat[i:j]\n                if \"-\" not in stuff:\n                    stuff = stuff.replace(\"\\\\\", r\"\\\\\")\n                else:\n                    chunks = []\n                    k = i + 2 if pat[i] == \"!\" else i + 1\n                    while True:\n                        k = pat.find(\"-\", k, j)\n                        if k < 0:\n                            break\n                        chunks.append(pat[i:k])\n                        i = k + 1\n                        k = k + 3\n                    chunk = pat[i:j]\n                    if chunk:\n                        chunks.append(chunk)\n                    else:\n                        chunks[-1] += \"-\"\n                    # Remove empty ranges -- invalid in RE.\n                    for k in range(len(chunks) - 1, 0, -1):\n                        if chunks[k - 1][-1] > chunks[k][0]:\n                            chunks[k - 1] = chunks[k - 1][:-1] + chunks[k][1:]\n                            del chunks[k]\n                    # Escape backslashes and hyphens for set difference (--).\n                    # Hyphens that create ranges shouldn't be escaped.\n                    stuff = \"-\".join(\n                        s.replace(\"\\\\\", r\"\\\\\").replace(\"-\", r\"\\-\") for s in chunks\n                    )\n                # Escape set operations (&&, ~~ and ||).\n                stuff = re.sub(r\"([&~|])\", r\"\\\\\\1\", stuff)\n                i = j + 1\n                if not stuff:\n                    # Empty range: never match.\n                    add(\"(?!)\")\n                elif stuff == \"!\":\n                    # Negated empty range: match any character.\n                    add(\".\")\n                else:\n                    if stuff[0] == \"!\":\n                        stuff = \"^\" + stuff[1:]\n                    elif stuff[0] in (\"^\", \"[\"):\n                        stuff = \"\\\\\" + stuff\n                    add(f\"[{stuff}]\")\n        else:\n            add(re.escape(c))\n    assert i == n\n    return res\n\n\ndef glob_translate(pat):\n    # Copied from: https://github.com/python/cpython/pull/106703.\n    # The keyword parameters' values are fixed to:\n    # recursive=True, include_hidden=True, seps=None\n    \"\"\"Translate a pathname with shell wildcards to a regular expression.\"\"\"\n    if os.path.altsep:\n        seps = os.path.sep + os.path.altsep\n    else:\n        seps = os.path.sep\n    escaped_seps = \"\".join(map(re.escape, seps))\n    any_sep = f\"[{escaped_seps}]\" if len(seps) > 1 else escaped_seps\n    not_sep = f\"[^{escaped_seps}]\"\n    one_last_segment = f\"{not_sep}+\"\n    one_segment = f\"{one_last_segment}{any_sep}\"\n    any_segments = f\"(?:.+{any_sep})?\"\n    any_last_segments = \".*\"\n    results = []\n    parts = re.split(any_sep, pat)\n    last_part_idx = len(parts) - 1\n    for idx, part in enumerate(parts):\n        if part == \"*\":\n            results.append(one_segment if idx < last_part_idx else one_last_segment)\n            continue\n        if part == \"**\":\n            results.append(any_segments if idx < last_part_idx else any_last_segments)\n            continue\n        elif \"**\" in part:\n            raise ValueError(\n                \"Invalid pattern: '**' can only be an entire path component\"\n            )\n        if part:\n            results.extend(_translate(part, f\"{not_sep}*\", not_sep))\n        if idx < last_part_idx:\n            results.append(any_sep)\n    res = \"\".join(results)\n    return rf\"(?s:{res})\\Z\"\n", "fsspec/parquet.py": "import io\nimport json\nimport warnings\n\nfrom .core import url_to_fs\nfrom .utils import merge_offset_ranges\n\n# Parquet-Specific Utilities for fsspec\n#\n# Most of the functions defined in this module are NOT\n# intended for public consumption. The only exception\n# to this is `open_parquet_file`, which should be used\n# place of `fs.open()` to open parquet-formatted files\n# on remote file systems.\n\n\ndef open_parquet_file(\n    path,\n    mode=\"rb\",\n    fs=None,\n    metadata=None,\n    columns=None,\n    row_groups=None,\n    storage_options=None,\n    strict=False,\n    engine=\"auto\",\n    max_gap=64_000,\n    max_block=256_000_000,\n    footer_sample_size=1_000_000,\n    **kwargs,\n):\n    \"\"\"\n    Return a file-like object for a single Parquet file.\n\n    The specified parquet `engine` will be used to parse the\n    footer metadata, and determine the required byte ranges\n    from the file. The target path will then be opened with\n    the \"parts\" (`KnownPartsOfAFile`) caching strategy.\n\n    Note that this method is intended for usage with remote\n    file systems, and is unlikely to improve parquet-read\n    performance on local file systems.\n\n    Parameters\n    ----------\n    path: str\n        Target file path.\n    mode: str, optional\n        Mode option to be passed through to `fs.open`. Default is \"rb\".\n    metadata: Any, optional\n        Parquet metadata object. Object type must be supported\n        by the backend parquet engine. For now, only the \"fastparquet\"\n        engine supports an explicit `ParquetFile` metadata object.\n        If a metadata object is supplied, the remote footer metadata\n        will not need to be transferred into local memory.\n    fs: AbstractFileSystem, optional\n        Filesystem object to use for opening the file. If nothing is\n        specified, an `AbstractFileSystem` object will be inferred.\n    engine : str, default \"auto\"\n        Parquet engine to use for metadata parsing. Allowed options\n        include \"fastparquet\", \"pyarrow\", and \"auto\". The specified\n        engine must be installed in the current environment. If\n        \"auto\" is specified, and both engines are installed,\n        \"fastparquet\" will take precedence over \"pyarrow\".\n    columns: list, optional\n        List of all column names that may be read from the file.\n    row_groups : list, optional\n        List of all row-groups that may be read from the file. This\n        may be a list of row-group indices (integers), or it may be\n        a list of `RowGroup` metadata objects (if the \"fastparquet\"\n        engine is used).\n    storage_options : dict, optional\n        Used to generate an `AbstractFileSystem` object if `fs` was\n        not specified.\n    strict : bool, optional\n        Whether the resulting `KnownPartsOfAFile` cache should\n        fetch reads that go beyond a known byte-range boundary.\n        If `False` (the default), any read that ends outside a\n        known part will be zero padded. Note that using\n        `strict=True` may be useful for debugging.\n    max_gap : int, optional\n        Neighboring byte ranges will only be merged when their\n        inter-range gap is <= `max_gap`. Default is 64KB.\n    max_block : int, optional\n        Neighboring byte ranges will only be merged when the size of\n        the aggregated range is <= `max_block`. Default is 256MB.\n    footer_sample_size : int, optional\n        Number of bytes to read from the end of the path to look\n        for the footer metadata. If the sampled bytes do not contain\n        the footer, a second read request will be required, and\n        performance will suffer. Default is 1MB.\n    **kwargs :\n        Optional key-word arguments to pass to `fs.open`\n    \"\"\"\n\n    # Make sure we have an `AbstractFileSystem` object\n    # to work with\n    if fs is None:\n        fs = url_to_fs(path, **(storage_options or {}))[0]\n\n    # For now, `columns == []` not supported. Just use\n    # default `open` command with `path` input\n    if columns is not None and len(columns) == 0:\n        return fs.open(path, mode=mode)\n\n    # Set the engine\n    engine = _set_engine(engine)\n\n    # Fetch the known byte ranges needed to read\n    # `columns` and/or `row_groups`\n    data = _get_parquet_byte_ranges(\n        [path],\n        fs,\n        metadata=metadata,\n        columns=columns,\n        row_groups=row_groups,\n        engine=engine,\n        max_gap=max_gap,\n        max_block=max_block,\n        footer_sample_size=footer_sample_size,\n    )\n\n    # Extract file name from `data`\n    fn = next(iter(data)) if data else path\n\n    # Call self.open with \"parts\" caching\n    options = kwargs.pop(\"cache_options\", {}).copy()\n    return fs.open(\n        fn,\n        mode=mode,\n        cache_type=\"parts\",\n        cache_options={\n            **options,\n            \"data\": data.get(fn, {}),\n            \"strict\": strict,\n        },\n        **kwargs,\n    )\n\n\ndef _get_parquet_byte_ranges(\n    paths,\n    fs,\n    metadata=None,\n    columns=None,\n    row_groups=None,\n    max_gap=64_000,\n    max_block=256_000_000,\n    footer_sample_size=1_000_000,\n    engine=\"auto\",\n):\n    \"\"\"Get a dictionary of the known byte ranges needed\n    to read a specific column/row-group selection from a\n    Parquet dataset. Each value in the output dictionary\n    is intended for use as the `data` argument for the\n    `KnownPartsOfAFile` caching strategy of a single path.\n    \"\"\"\n\n    # Set engine if necessary\n    if isinstance(engine, str):\n        engine = _set_engine(engine)\n\n    # Pass to specialized function if metadata is defined\n    if metadata is not None:\n        # Use the provided parquet metadata object\n        # to avoid transferring/parsing footer metadata\n        return _get_parquet_byte_ranges_from_metadata(\n            metadata,\n            fs,\n            engine,\n            columns=columns,\n            row_groups=row_groups,\n            max_gap=max_gap,\n            max_block=max_block,\n        )\n\n    # Get file sizes asynchronously\n    file_sizes = fs.sizes(paths)\n\n    # Populate global paths, starts, & ends\n    result = {}\n    data_paths = []\n    data_starts = []\n    data_ends = []\n    add_header_magic = True\n    if columns is None and row_groups is None:\n        # We are NOT selecting specific columns or row-groups.\n        #\n        # We can avoid sampling the footers, and just transfer\n        # all file data with cat_ranges\n        for i, path in enumerate(paths):\n            result[path] = {}\n            for b in range(0, file_sizes[i], max_block):\n                data_paths.append(path)\n                data_starts.append(b)\n                data_ends.append(min(b + max_block, file_sizes[i]))\n        add_header_magic = False  # \"Magic\" should already be included\n    else:\n        # We ARE selecting specific columns or row-groups.\n        #\n        # Gather file footers.\n        # We just take the last `footer_sample_size` bytes of each\n        # file (or the entire file if it is smaller than that)\n        footer_starts = []\n        footer_ends = []\n        for i, path in enumerate(paths):\n            footer_ends.append(file_sizes[i])\n            sample_size = max(0, file_sizes[i] - footer_sample_size)\n            footer_starts.append(sample_size)\n        footer_samples = fs.cat_ranges(paths, footer_starts, footer_ends)\n\n        # Check our footer samples and re-sample if necessary.\n        missing_footer_starts = footer_starts.copy()\n        large_footer = 0\n        for i, path in enumerate(paths):\n            footer_size = int.from_bytes(footer_samples[i][-8:-4], \"little\")\n            real_footer_start = file_sizes[i] - (footer_size + 8)\n            if real_footer_start < footer_starts[i]:\n                missing_footer_starts[i] = real_footer_start\n                large_footer = max(large_footer, (footer_size + 8))\n        if large_footer:\n            warnings.warn(\n                f\"Not enough data was used to sample the parquet footer. \"\n                f\"Try setting footer_sample_size >= {large_footer}.\"\n            )\n            for i, block in enumerate(\n                fs.cat_ranges(\n                    paths,\n                    missing_footer_starts,\n                    footer_starts,\n                )\n            ):\n                footer_samples[i] = block + footer_samples[i]\n                footer_starts[i] = missing_footer_starts[i]\n\n        # Calculate required byte ranges for each path\n        for i, path in enumerate(paths):\n            # Deal with small-file case.\n            # Just include all remaining bytes of the file\n            # in a single range.\n            if file_sizes[i] < max_block:\n                if footer_starts[i] > 0:\n                    # Only need to transfer the data if the\n                    # footer sample isn't already the whole file\n                    data_paths.append(path)\n                    data_starts.append(0)\n                    data_ends.append(footer_starts[i])\n                continue\n\n            # Use \"engine\" to collect data byte ranges\n            path_data_starts, path_data_ends = engine._parquet_byte_ranges(\n                columns,\n                row_groups=row_groups,\n                footer=footer_samples[i],\n                footer_start=footer_starts[i],\n            )\n\n            data_paths += [path] * len(path_data_starts)\n            data_starts += path_data_starts\n            data_ends += path_data_ends\n\n        # Merge adjacent offset ranges\n        data_paths, data_starts, data_ends = merge_offset_ranges(\n            data_paths,\n            data_starts,\n            data_ends,\n            max_gap=max_gap,\n            max_block=max_block,\n            sort=False,  # Should already be sorted\n        )\n\n        # Start by populating `result` with footer samples\n        for i, path in enumerate(paths):\n            result[path] = {(footer_starts[i], footer_ends[i]): footer_samples[i]}\n\n    # Transfer the data byte-ranges into local memory\n    _transfer_ranges(fs, result, data_paths, data_starts, data_ends)\n\n    # Add b\"PAR1\" to header if necessary\n    if add_header_magic:\n        _add_header_magic(result)\n\n    return result\n\n\ndef _get_parquet_byte_ranges_from_metadata(\n    metadata,\n    fs,\n    engine,\n    columns=None,\n    row_groups=None,\n    max_gap=64_000,\n    max_block=256_000_000,\n):\n    \"\"\"Simplified version of `_get_parquet_byte_ranges` for\n    the case that an engine-specific `metadata` object is\n    provided, and the remote footer metadata does not need to\n    be transferred before calculating the required byte ranges.\n    \"\"\"\n\n    # Use \"engine\" to collect data byte ranges\n    data_paths, data_starts, data_ends = engine._parquet_byte_ranges(\n        columns,\n        row_groups=row_groups,\n        metadata=metadata,\n    )\n\n    # Merge adjacent offset ranges\n    data_paths, data_starts, data_ends = merge_offset_ranges(\n        data_paths,\n        data_starts,\n        data_ends,\n        max_gap=max_gap,\n        max_block=max_block,\n        sort=False,  # Should be sorted\n    )\n\n    # Transfer the data byte-ranges into local memory\n    result = {fn: {} for fn in list(set(data_paths))}\n    _transfer_ranges(fs, result, data_paths, data_starts, data_ends)\n\n    # Add b\"PAR1\" to header\n    _add_header_magic(result)\n\n    return result\n\n\ndef _transfer_ranges(fs, blocks, paths, starts, ends):\n    # Use cat_ranges to gather the data byte_ranges\n    ranges = (paths, starts, ends)\n    for path, start, stop, data in zip(*ranges, fs.cat_ranges(*ranges)):\n        blocks[path][(start, stop)] = data\n\n\ndef _add_header_magic(data):\n    # Add b\"PAR1\" to file headers\n    for path in list(data.keys()):\n        add_magic = True\n        for k in data[path].keys():\n            if k[0] == 0 and k[1] >= 4:\n                add_magic = False\n                break\n        if add_magic:\n            data[path][(0, 4)] = b\"PAR1\"\n\n\ndef _set_engine(engine_str):\n    # Define a list of parquet engines to try\n    if engine_str == \"auto\":\n        try_engines = (\"fastparquet\", \"pyarrow\")\n    elif not isinstance(engine_str, str):\n        raise ValueError(\n            \"Failed to set parquet engine! \"\n            \"Please pass 'fastparquet', 'pyarrow', or 'auto'\"\n        )\n    elif engine_str not in (\"fastparquet\", \"pyarrow\"):\n        raise ValueError(f\"{engine_str} engine not supported by `fsspec.parquet`\")\n    else:\n        try_engines = [engine_str]\n\n    # Try importing the engines in `try_engines`,\n    # and choose the first one that succeeds\n    for engine in try_engines:\n        try:\n            if engine == \"fastparquet\":\n                return FastparquetEngine()\n            elif engine == \"pyarrow\":\n                return PyarrowEngine()\n        except ImportError:\n            pass\n\n    # Raise an error if a supported parquet engine\n    # was not found\n    raise ImportError(\n        f\"The following parquet engines are not installed \"\n        f\"in your python environment: {try_engines}.\"\n        f\"Please install 'fastparquert' or 'pyarrow' to \"\n        f\"utilize the `fsspec.parquet` module.\"\n    )\n\n\nclass FastparquetEngine:\n    # The purpose of the FastparquetEngine class is\n    # to check if fastparquet can be imported (on initialization)\n    # and to define a `_parquet_byte_ranges` method. In the\n    # future, this class may also be used to define other\n    # methods/logic that are specific to fastparquet.\n\n    def __init__(self):\n        import fastparquet as fp\n\n        self.fp = fp\n\n    def _row_group_filename(self, row_group, pf):\n        return pf.row_group_filename(row_group)\n\n    def _parquet_byte_ranges(\n        self,\n        columns,\n        row_groups=None,\n        metadata=None,\n        footer=None,\n        footer_start=None,\n    ):\n        # Initialize offset ranges and define ParqetFile metadata\n        pf = metadata\n        data_paths, data_starts, data_ends = [], [], []\n        if pf is None:\n            pf = self.fp.ParquetFile(io.BytesIO(footer))\n\n        # Convert columns to a set and add any index columns\n        # specified in the pandas metadata (just in case)\n        column_set = None if columns is None else set(columns)\n        if column_set is not None and hasattr(pf, \"pandas_metadata\"):\n            md_index = [\n                ind\n                for ind in pf.pandas_metadata.get(\"index_columns\", [])\n                # Ignore RangeIndex information\n                if not isinstance(ind, dict)\n            ]\n            column_set |= set(md_index)\n\n        # Check if row_groups is a list of integers\n        # or a list of row-group metadata\n        if row_groups and not isinstance(row_groups[0], int):\n            # Input row_groups contains row-group metadata\n            row_group_indices = None\n        else:\n            # Input row_groups contains row-group indices\n            row_group_indices = row_groups\n            row_groups = pf.row_groups\n\n        # Loop through column chunks to add required byte ranges\n        for r, row_group in enumerate(row_groups):\n            # Skip this row-group if we are targeting\n            # specific row-groups\n            if row_group_indices is None or r in row_group_indices:\n                # Find the target parquet-file path for `row_group`\n                fn = self._row_group_filename(row_group, pf)\n\n                for column in row_group.columns:\n                    name = column.meta_data.path_in_schema[0]\n                    # Skip this column if we are targeting a\n                    # specific columns\n                    if column_set is None or name in column_set:\n                        file_offset0 = column.meta_data.dictionary_page_offset\n                        if file_offset0 is None:\n                            file_offset0 = column.meta_data.data_page_offset\n                        num_bytes = column.meta_data.total_compressed_size\n                        if footer_start is None or file_offset0 < footer_start:\n                            data_paths.append(fn)\n                            data_starts.append(file_offset0)\n                            data_ends.append(\n                                min(\n                                    file_offset0 + num_bytes,\n                                    footer_start or (file_offset0 + num_bytes),\n                                )\n                            )\n\n        if metadata:\n            # The metadata in this call may map to multiple\n            # file paths. Need to include `data_paths`\n            return data_paths, data_starts, data_ends\n        return data_starts, data_ends\n\n\nclass PyarrowEngine:\n    # The purpose of the PyarrowEngine class is\n    # to check if pyarrow can be imported (on initialization)\n    # and to define a `_parquet_byte_ranges` method. In the\n    # future, this class may also be used to define other\n    # methods/logic that are specific to pyarrow.\n\n    def __init__(self):\n        import pyarrow.parquet as pq\n\n        self.pq = pq\n\n    def _row_group_filename(self, row_group, metadata):\n        raise NotImplementedError\n\n    def _parquet_byte_ranges(\n        self,\n        columns,\n        row_groups=None,\n        metadata=None,\n        footer=None,\n        footer_start=None,\n    ):\n        if metadata is not None:\n            raise ValueError(\"metadata input not supported for PyarrowEngine\")\n\n        data_starts, data_ends = [], []\n        md = self.pq.ParquetFile(io.BytesIO(footer)).metadata\n\n        # Convert columns to a set and add any index columns\n        # specified in the pandas metadata (just in case)\n        column_set = None if columns is None else set(columns)\n        if column_set is not None:\n            schema = md.schema.to_arrow_schema()\n            has_pandas_metadata = (\n                schema.metadata is not None and b\"pandas\" in schema.metadata\n            )\n            if has_pandas_metadata:\n                md_index = [\n                    ind\n                    for ind in json.loads(\n                        schema.metadata[b\"pandas\"].decode(\"utf8\")\n                    ).get(\"index_columns\", [])\n                    # Ignore RangeIndex information\n                    if not isinstance(ind, dict)\n                ]\n                column_set |= set(md_index)\n\n        # Loop through column chunks to add required byte ranges\n        for r in range(md.num_row_groups):\n            # Skip this row-group if we are targeting\n            # specific row-groups\n            if row_groups is None or r in row_groups:\n                row_group = md.row_group(r)\n                for c in range(row_group.num_columns):\n                    column = row_group.column(c)\n                    name = column.path_in_schema\n                    # Skip this column if we are targeting a\n                    # specific columns\n                    split_name = name.split(\".\")[0]\n                    if (\n                        column_set is None\n                        or name in column_set\n                        or split_name in column_set\n                    ):\n                        file_offset0 = column.dictionary_page_offset\n                        if file_offset0 is None:\n                            file_offset0 = column.data_page_offset\n                        num_bytes = column.total_compressed_size\n                        if file_offset0 < footer_start:\n                            data_starts.append(file_offset0)\n                            data_ends.append(\n                                min(file_offset0 + num_bytes, footer_start)\n                            )\n        return data_starts, data_ends\n", "fsspec/transaction.py": "from collections import deque\n\n\nclass Transaction:\n    \"\"\"Filesystem transaction write context\n\n    Gathers files for deferred commit or discard, so that several write\n    operations can be finalized semi-atomically. This works by having this\n    instance as the ``.transaction`` attribute of the given filesystem\n    \"\"\"\n\n    def __init__(self, fs, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        fs: FileSystem instance\n        \"\"\"\n        self.fs = fs\n        self.files = deque()\n\n    def __enter__(self):\n        self.start()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"End transaction and commit, if exit is not due to exception\"\"\"\n        # only commit if there was no exception\n        self.complete(commit=exc_type is None)\n        if self.fs:\n            self.fs._intrans = False\n            self.fs._transaction = None\n            self.fs = None\n\n    def start(self):\n        \"\"\"Start a transaction on this FileSystem\"\"\"\n        self.files = deque()  # clean up after previous failed completions\n        self.fs._intrans = True\n\n    def complete(self, commit=True):\n        \"\"\"Finish transaction: commit or discard all deferred files\"\"\"\n        while self.files:\n            f = self.files.popleft()\n            if commit:\n                f.commit()\n            else:\n                f.discard()\n        self.fs._intrans = False\n        self.fs._transaction = None\n        self.fs = None\n\n\nclass FileActor:\n    def __init__(self):\n        self.files = []\n\n    def commit(self):\n        for f in self.files:\n            f.commit()\n        self.files.clear()\n\n    def discard(self):\n        for f in self.files:\n            f.discard()\n        self.files.clear()\n\n    def append(self, f):\n        self.files.append(f)\n\n\nclass DaskTransaction(Transaction):\n    def __init__(self, fs):\n        \"\"\"\n        Parameters\n        ----------\n        fs: FileSystem instance\n        \"\"\"\n        import distributed\n\n        super().__init__(fs)\n        client = distributed.default_client()\n        self.files = client.submit(FileActor, actor=True).result()\n\n    def complete(self, commit=True):\n        \"\"\"Finish transaction: commit or discard all deferred files\"\"\"\n        if commit:\n            self.files.commit().result()\n        else:\n            self.files.discard().result()\n        self.fs._intrans = False\n        self.fs = None\n", "fsspec/callbacks.py": "from functools import wraps\n\n\nclass Callback:\n    \"\"\"\n    Base class and interface for callback mechanism\n\n    This class can be used directly for monitoring file transfers by\n    providing ``callback=Callback(hooks=...)`` (see the ``hooks`` argument,\n    below), or subclassed for more specialised behaviour.\n\n    Parameters\n    ----------\n    size: int (optional)\n        Nominal quantity for the value that corresponds to a complete\n        transfer, e.g., total number of tiles or total number of\n        bytes\n    value: int (0)\n        Starting internal counter value\n    hooks: dict or None\n        A dict of named functions to be called on each update. The signature\n        of these must be ``f(size, value, **kwargs)``\n    \"\"\"\n\n    def __init__(self, size=None, value=0, hooks=None, **kwargs):\n        self.size = size\n        self.value = value\n        self.hooks = hooks or {}\n        self.kw = kwargs\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc_args):\n        self.close()\n\n    def close(self):\n        \"\"\"Close callback.\"\"\"\n\n    def branched(self, path_1, path_2, **kwargs):\n        \"\"\"\n        Return callback for child transfers\n\n        If this callback is operating at a higher level, e.g., put, which may\n        trigger transfers that can also be monitored. The function returns a callback\n        that has to be passed to the child method, e.g., put_file,\n        as `callback=` argument.\n\n        The implementation uses `callback.branch` for compatibility.\n        When implementing callbacks, it is recommended to override this function instead\n        of `branch` and avoid calling `super().branched(...)`.\n\n        Prefer using this function over `branch`.\n\n        Parameters\n        ----------\n        path_1: str\n            Child's source path\n        path_2: str\n            Child's destination path\n        **kwargs:\n            Arbitrary keyword arguments\n\n        Returns\n        -------\n        callback: Callback\n            A callback instance to be passed to the child method\n        \"\"\"\n        self.branch(path_1, path_2, kwargs)\n        # mutate kwargs so that we can force the caller to pass \"callback=\" explicitly\n        return kwargs.pop(\"callback\", DEFAULT_CALLBACK)\n\n    def branch_coro(self, fn):\n        \"\"\"\n        Wraps a coroutine, and pass a new child callback to it.\n        \"\"\"\n\n        @wraps(fn)\n        async def func(path1, path2: str, **kwargs):\n            with self.branched(path1, path2, **kwargs) as child:\n                return await fn(path1, path2, callback=child, **kwargs)\n\n        return func\n\n    def set_size(self, size):\n        \"\"\"\n        Set the internal maximum size attribute\n\n        Usually called if not initially set at instantiation. Note that this\n        triggers a ``call()``.\n\n        Parameters\n        ----------\n        size: int\n        \"\"\"\n        self.size = size\n        self.call()\n\n    def absolute_update(self, value):\n        \"\"\"\n        Set the internal value state\n\n        Triggers ``call()``\n\n        Parameters\n        ----------\n        value: int\n        \"\"\"\n        self.value = value\n        self.call()\n\n    def relative_update(self, inc=1):\n        \"\"\"\n        Delta increment the internal counter\n\n        Triggers ``call()``\n\n        Parameters\n        ----------\n        inc: int\n        \"\"\"\n        self.value += inc\n        self.call()\n\n    def call(self, hook_name=None, **kwargs):\n        \"\"\"\n        Execute hook(s) with current state\n\n        Each function is passed the internal size and current value\n\n        Parameters\n        ----------\n        hook_name: str or None\n            If given, execute on this hook\n        kwargs: passed on to (all) hook(s)\n        \"\"\"\n        if not self.hooks:\n            return\n        kw = self.kw.copy()\n        kw.update(kwargs)\n        if hook_name:\n            if hook_name not in self.hooks:\n                return\n            return self.hooks[hook_name](self.size, self.value, **kw)\n        for hook in self.hooks.values() or []:\n            hook(self.size, self.value, **kw)\n\n    def wrap(self, iterable):\n        \"\"\"\n        Wrap an iterable to call ``relative_update`` on each iterations\n\n        Parameters\n        ----------\n        iterable: Iterable\n            The iterable that is being wrapped\n        \"\"\"\n        for item in iterable:\n            self.relative_update()\n            yield item\n\n    def branch(self, path_1, path_2, kwargs):\n        \"\"\"\n        Set callbacks for child transfers\n\n        If this callback is operating at a higher level, e.g., put, which may\n        trigger transfers that can also be monitored. The passed kwargs are\n        to be *mutated* to add ``callback=``, if this class supports branching\n        to children.\n\n        Parameters\n        ----------\n        path_1: str\n            Child's source path\n        path_2: str\n            Child's destination path\n        kwargs: dict\n            arguments passed to child method, e.g., put_file.\n\n        Returns\n        -------\n\n        \"\"\"\n        return None\n\n    def no_op(self, *_, **__):\n        pass\n\n    def __getattr__(self, item):\n        \"\"\"\n        If undefined methods are called on this class, nothing happens\n        \"\"\"\n        return self.no_op\n\n    @classmethod\n    def as_callback(cls, maybe_callback=None):\n        \"\"\"Transform callback=... into Callback instance\n\n        For the special value of ``None``, return the global instance of\n        ``NoOpCallback``. This is an alternative to including\n        ``callback=DEFAULT_CALLBACK`` directly in a method signature.\n        \"\"\"\n        if maybe_callback is None:\n            return DEFAULT_CALLBACK\n        return maybe_callback\n\n\nclass NoOpCallback(Callback):\n    \"\"\"\n    This implementation of Callback does exactly nothing\n    \"\"\"\n\n    def call(self, *args, **kwargs):\n        return None\n\n\nclass DotPrinterCallback(Callback):\n    \"\"\"\n    Simple example Callback implementation\n\n    Almost identical to Callback with a hook that prints a char; here we\n    demonstrate how the outer layer may print \"#\" and the inner layer \".\"\n    \"\"\"\n\n    def __init__(self, chr_to_print=\"#\", **kwargs):\n        self.chr = chr_to_print\n        super().__init__(**kwargs)\n\n    def branch(self, path_1, path_2, kwargs):\n        \"\"\"Mutate kwargs to add new instance with different print char\"\"\"\n        kwargs[\"callback\"] = DotPrinterCallback(\".\")\n\n    def call(self, **kwargs):\n        \"\"\"Just outputs a character\"\"\"\n        print(self.chr, end=\"\")\n\n\nclass TqdmCallback(Callback):\n    \"\"\"\n    A callback to display a progress bar using tqdm\n\n    Parameters\n    ----------\n    tqdm_kwargs : dict, (optional)\n        Any argument accepted by the tqdm constructor.\n        See the `tqdm doc <https://tqdm.github.io/docs/tqdm/#__init__>`_.\n        Will be forwarded to `tqdm_cls`.\n    tqdm_cls: (optional)\n        subclass of `tqdm.tqdm`. If not passed, it will default to `tqdm.tqdm`.\n\n    Examples\n    --------\n    >>> import fsspec\n    >>> from fsspec.callbacks import TqdmCallback\n    >>> fs = fsspec.filesystem(\"memory\")\n    >>> path2distant_data = \"/your-path\"\n    >>> fs.upload(\n            \".\",\n            path2distant_data,\n            recursive=True,\n            callback=TqdmCallback(),\n        )\n\n    You can forward args to tqdm using the ``tqdm_kwargs`` parameter.\n\n    >>> fs.upload(\n            \".\",\n            path2distant_data,\n            recursive=True,\n            callback=TqdmCallback(tqdm_kwargs={\"desc\": \"Your tqdm description\"}),\n        )\n\n    You can also customize the progress bar by passing a subclass of `tqdm`.\n\n    .. code-block:: python\n\n        class TqdmFormat(tqdm):\n            '''Provides a `total_time` format parameter'''\n            @property\n            def format_dict(self):\n                d = super().format_dict\n                total_time = d[\"elapsed\"] * (d[\"total\"] or 0) / max(d[\"n\"], 1)\n                d.update(total_time=self.format_interval(total_time) + \" in total\")\n                return d\n\n    >>> with TqdmCallback(\n            tqdm_kwargs={\n                \"desc\": \"desc\",\n                \"bar_format\": \"{total_time}: {percentage:.0f}%|{bar}{r_bar}\",\n            },\n            tqdm_cls=TqdmFormat,\n        ) as callback:\n            fs.upload(\".\", path2distant_data, recursive=True, callback=callback)\n    \"\"\"\n\n    def __init__(self, tqdm_kwargs=None, *args, **kwargs):\n        try:\n            from tqdm import tqdm\n\n        except ImportError as exce:\n            raise ImportError(\n                \"Using TqdmCallback requires tqdm to be installed\"\n            ) from exce\n\n        self._tqdm_cls = kwargs.pop(\"tqdm_cls\", tqdm)\n        self._tqdm_kwargs = tqdm_kwargs or {}\n        self.tqdm = None\n        super().__init__(*args, **kwargs)\n\n    def call(self, *args, **kwargs):\n        if self.tqdm is None:\n            self.tqdm = self._tqdm_cls(total=self.size, **self._tqdm_kwargs)\n        self.tqdm.total = self.size\n        self.tqdm.update(self.value - self.tqdm.n)\n\n    def close(self):\n        if self.tqdm is not None:\n            self.tqdm.close()\n            self.tqdm = None\n\n    def __del__(self):\n        return self.close()\n\n\nDEFAULT_CALLBACK = _DEFAULT_CALLBACK = NoOpCallback()\n", "fsspec/registry.py": "from __future__ import annotations\n\nimport importlib\nimport types\nimport warnings\n\n__all__ = [\"registry\", \"get_filesystem_class\", \"default\"]\n\n# internal, mutable\n_registry: dict[str, type] = {}\n\n# external, immutable\nregistry = types.MappingProxyType(_registry)\ndefault = \"file\"\n\n\ndef register_implementation(name, cls, clobber=False, errtxt=None):\n    \"\"\"Add implementation class to the registry\n\n    Parameters\n    ----------\n    name: str\n        Protocol name to associate with the class\n    cls: class or str\n        if a class: fsspec-compliant implementation class (normally inherits from\n        ``fsspec.AbstractFileSystem``, gets added straight to the registry. If a\n        str, the full path to an implementation class like package.module.class,\n        which gets added to known_implementations,\n        so the import is deferred until the filesystem is actually used.\n    clobber: bool (optional)\n        Whether to overwrite a protocol with the same name; if False, will raise\n        instead.\n    errtxt: str (optional)\n        If given, then a failure to import the given class will result in this\n        text being given.\n    \"\"\"\n    if isinstance(cls, str):\n        if name in known_implementations and clobber is False:\n            if cls != known_implementations[name][\"class\"]:\n                raise ValueError(\n                    f\"Name ({name}) already in the known_implementations and clobber \"\n                    f\"is False\"\n                )\n        else:\n            known_implementations[name] = {\n                \"class\": cls,\n                \"err\": errtxt or f\"{cls} import failed for protocol {name}\",\n            }\n\n    else:\n        if name in registry and clobber is False:\n            if _registry[name] is not cls:\n                raise ValueError(\n                    f\"Name ({name}) already in the registry and clobber is False\"\n                )\n        else:\n            _registry[name] = cls\n\n\n# protocols mapped to the class which implements them. This dict can be\n# updated with register_implementation\nknown_implementations = {\n    \"abfs\": {\n        \"class\": \"adlfs.AzureBlobFileSystem\",\n        \"err\": \"Install adlfs to access Azure Datalake Gen2 and Azure Blob Storage\",\n    },\n    \"adl\": {\n        \"class\": \"adlfs.AzureDatalakeFileSystem\",\n        \"err\": \"Install adlfs to access Azure Datalake Gen1\",\n    },\n    \"arrow_hdfs\": {\n        \"class\": \"fsspec.implementations.arrow.HadoopFileSystem\",\n        \"err\": \"pyarrow and local java libraries required for HDFS\",\n    },\n    \"asynclocal\": {\n        \"class\": \"morefs.asyn_local.AsyncLocalFileSystem\",\n        \"err\": \"Install 'morefs[asynclocalfs]' to use AsyncLocalFileSystem\",\n    },\n    \"az\": {\n        \"class\": \"adlfs.AzureBlobFileSystem\",\n        \"err\": \"Install adlfs to access Azure Datalake Gen2 and Azure Blob Storage\",\n    },\n    \"blockcache\": {\"class\": \"fsspec.implementations.cached.CachingFileSystem\"},\n    \"box\": {\n        \"class\": \"boxfs.BoxFileSystem\",\n        \"err\": \"Please install boxfs to access BoxFileSystem\",\n    },\n    \"cached\": {\"class\": \"fsspec.implementations.cached.CachingFileSystem\"},\n    \"dask\": {\n        \"class\": \"fsspec.implementations.dask.DaskWorkerFileSystem\",\n        \"err\": \"Install dask distributed to access worker file system\",\n    },\n    \"data\": {\"class\": \"fsspec.implementations.data.DataFileSystem\"},\n    \"dbfs\": {\n        \"class\": \"fsspec.implementations.dbfs.DatabricksFileSystem\",\n        \"err\": \"Install the requests package to use the DatabricksFileSystem\",\n    },\n    \"dir\": {\"class\": \"fsspec.implementations.dirfs.DirFileSystem\"},\n    \"dropbox\": {\n        \"class\": \"dropboxdrivefs.DropboxDriveFileSystem\",\n        \"err\": (\n            'DropboxFileSystem requires \"dropboxdrivefs\",\"requests\" and \"'\n            '\"dropbox\" to be installed'\n        ),\n    },\n    \"dvc\": {\n        \"class\": \"dvc.api.DVCFileSystem\",\n        \"err\": \"Install dvc to access DVCFileSystem\",\n    },\n    \"file\": {\"class\": \"fsspec.implementations.local.LocalFileSystem\"},\n    \"filecache\": {\"class\": \"fsspec.implementations.cached.WholeFileCacheFileSystem\"},\n    \"ftp\": {\"class\": \"fsspec.implementations.ftp.FTPFileSystem\"},\n    \"gcs\": {\n        \"class\": \"gcsfs.GCSFileSystem\",\n        \"err\": \"Please install gcsfs to access Google Storage\",\n    },\n    \"gdrive\": {\n        \"class\": \"gdrivefs.GoogleDriveFileSystem\",\n        \"err\": \"Please install gdrivefs for access to Google Drive\",\n    },\n    \"generic\": {\"class\": \"fsspec.generic.GenericFileSystem\"},\n    \"git\": {\n        \"class\": \"fsspec.implementations.git.GitFileSystem\",\n        \"err\": \"Install pygit2 to browse local git repos\",\n    },\n    \"github\": {\n        \"class\": \"fsspec.implementations.github.GithubFileSystem\",\n        \"err\": \"Install the requests package to use the github FS\",\n    },\n    \"gs\": {\n        \"class\": \"gcsfs.GCSFileSystem\",\n        \"err\": \"Please install gcsfs to access Google Storage\",\n    },\n    \"hdfs\": {\n        \"class\": \"fsspec.implementations.arrow.HadoopFileSystem\",\n        \"err\": \"pyarrow and local java libraries required for HDFS\",\n    },\n    \"hf\": {\n        \"class\": \"huggingface_hub.HfFileSystem\",\n        \"err\": \"Install huggingface_hub to access HfFileSystem\",\n    },\n    \"http\": {\n        \"class\": \"fsspec.implementations.http.HTTPFileSystem\",\n        \"err\": 'HTTPFileSystem requires \"requests\" and \"aiohttp\" to be installed',\n    },\n    \"https\": {\n        \"class\": \"fsspec.implementations.http.HTTPFileSystem\",\n        \"err\": 'HTTPFileSystem requires \"requests\" and \"aiohttp\" to be installed',\n    },\n    \"jlab\": {\n        \"class\": \"fsspec.implementations.jupyter.JupyterFileSystem\",\n        \"err\": \"Jupyter FS requires requests to be installed\",\n    },\n    \"jupyter\": {\n        \"class\": \"fsspec.implementations.jupyter.JupyterFileSystem\",\n        \"err\": \"Jupyter FS requires requests to be installed\",\n    },\n    \"lakefs\": {\n        \"class\": \"lakefs_spec.LakeFSFileSystem\",\n        \"err\": \"Please install lakefs-spec to access LakeFSFileSystem\",\n    },\n    \"libarchive\": {\n        \"class\": \"fsspec.implementations.libarchive.LibArchiveFileSystem\",\n        \"err\": \"LibArchive requires to be installed\",\n    },\n    \"local\": {\"class\": \"fsspec.implementations.local.LocalFileSystem\"},\n    \"memory\": {\"class\": \"fsspec.implementations.memory.MemoryFileSystem\"},\n    \"oci\": {\n        \"class\": \"ocifs.OCIFileSystem\",\n        \"err\": \"Install ocifs to access OCI Object Storage\",\n    },\n    \"ocilake\": {\n        \"class\": \"ocifs.OCIFileSystem\",\n        \"err\": \"Install ocifs to access OCI Data Lake\",\n    },\n    \"oss\": {\n        \"class\": \"ossfs.OSSFileSystem\",\n        \"err\": \"Install ossfs to access Alibaba Object Storage System\",\n    },\n    \"reference\": {\"class\": \"fsspec.implementations.reference.ReferenceFileSystem\"},\n    \"root\": {\n        \"class\": \"fsspec_xrootd.XRootDFileSystem\",\n        \"err\": (\n            \"Install fsspec-xrootd to access xrootd storage system. \"\n            \"Note: 'root' is the protocol name for xrootd storage systems, \"\n            \"not referring to root directories\"\n        ),\n    },\n    \"s3\": {\"class\": \"s3fs.S3FileSystem\", \"err\": \"Install s3fs to access S3\"},\n    \"s3a\": {\"class\": \"s3fs.S3FileSystem\", \"err\": \"Install s3fs to access S3\"},\n    \"sftp\": {\n        \"class\": \"fsspec.implementations.sftp.SFTPFileSystem\",\n        \"err\": 'SFTPFileSystem requires \"paramiko\" to be installed',\n    },\n    \"simplecache\": {\"class\": \"fsspec.implementations.cached.SimpleCacheFileSystem\"},\n    \"smb\": {\n        \"class\": \"fsspec.implementations.smb.SMBFileSystem\",\n        \"err\": 'SMB requires \"smbprotocol\" or \"smbprotocol[kerberos]\" installed',\n    },\n    \"ssh\": {\n        \"class\": \"fsspec.implementations.sftp.SFTPFileSystem\",\n        \"err\": 'SFTPFileSystem requires \"paramiko\" to be installed',\n    },\n    \"tar\": {\"class\": \"fsspec.implementations.tar.TarFileSystem\"},\n    \"wandb\": {\"class\": \"wandbfs.WandbFS\", \"err\": \"Install wandbfs to access wandb\"},\n    \"webdav\": {\n        \"class\": \"webdav4.fsspec.WebdavFileSystem\",\n        \"err\": \"Install webdav4 to access WebDAV\",\n    },\n    \"webhdfs\": {\n        \"class\": \"fsspec.implementations.webhdfs.WebHDFS\",\n        \"err\": 'webHDFS access requires \"requests\" to be installed',\n    },\n    \"zip\": {\"class\": \"fsspec.implementations.zip.ZipFileSystem\"},\n}\n\nassert list(known_implementations) == sorted(\n    known_implementations\n), \"Not in alphabetical order\"\n\n\ndef get_filesystem_class(protocol):\n    \"\"\"Fetch named protocol implementation from the registry\n\n    The dict ``known_implementations`` maps protocol names to the locations\n    of classes implementing the corresponding file-system. When used for the\n    first time, appropriate imports will happen and the class will be placed in\n    the registry. All subsequent calls will fetch directly from the registry.\n\n    Some protocol implementations require additional dependencies, and so the\n    import may fail. In this case, the string in the \"err\" field of the\n    ``known_implementations`` will be given as the error message.\n    \"\"\"\n    if not protocol:\n        protocol = default\n\n    if protocol not in registry:\n        if protocol not in known_implementations:\n            raise ValueError(f\"Protocol not known: {protocol}\")\n        bit = known_implementations[protocol]\n        try:\n            register_implementation(protocol, _import_class(bit[\"class\"]))\n        except ImportError as e:\n            raise ImportError(bit[\"err\"]) from e\n    cls = registry[protocol]\n    if getattr(cls, \"protocol\", None) in (\"abstract\", None):\n        cls.protocol = protocol\n\n    return cls\n\n\ns3_msg = \"\"\"Your installed version of s3fs is very old and known to cause\nsevere performance issues, see also https://github.com/dask/dask/issues/10276\n\nTo fix, you should specify a lower version bound on s3fs, or\nupdate the current installation.\n\"\"\"\n\n\ndef _import_class(fqp: str):\n    \"\"\"Take a fully-qualified path and return the imported class or identifier.\n\n    ``fqp`` is of the form \"package.module.klass\" or\n    \"package.module:subobject.klass\".\n\n    Warnings\n    --------\n    This can import arbitrary modules. Make sure you haven't installed any modules\n    that may execute malicious code at import time.\n    \"\"\"\n    if \":\" in fqp:\n        mod, name = fqp.rsplit(\":\", 1)\n    else:\n        mod, name = fqp.rsplit(\".\", 1)\n\n    is_s3 = mod == \"s3fs\"\n    mod = importlib.import_module(mod)\n    if is_s3 and mod.__version__.split(\".\") < [\"0\", \"5\"]:\n        warnings.warn(s3_msg)\n    for part in name.split(\".\"):\n        mod = getattr(mod, part)\n\n    if not isinstance(mod, type):\n        raise TypeError(f\"{fqp} is not a class\")\n\n    return mod\n\n\ndef filesystem(protocol, **storage_options):\n    \"\"\"Instantiate filesystems for given protocol and arguments\n\n    ``storage_options`` are specific to the protocol being chosen, and are\n    passed directly to the class.\n    \"\"\"\n    if protocol == \"arrow_hdfs\":\n        warnings.warn(\n            \"The 'arrow_hdfs' protocol has been deprecated and will be \"\n            \"removed in the future. Specify it as 'hdfs'.\",\n            DeprecationWarning,\n        )\n\n    cls = get_filesystem_class(protocol)\n    return cls(**storage_options)\n\n\ndef available_protocols():\n    \"\"\"Return a list of the implemented protocols.\n\n    Note that any given protocol may require extra packages to be importable.\n    \"\"\"\n    return list(known_implementations)\n", "fsspec/mapping.py": "import array\nimport logging\nimport posixpath\nimport warnings\nfrom collections.abc import MutableMapping\nfrom functools import cached_property\n\nfrom fsspec.core import url_to_fs\n\nlogger = logging.getLogger(\"fsspec.mapping\")\n\n\nclass FSMap(MutableMapping):\n    \"\"\"Wrap a FileSystem instance as a mutable wrapping.\n\n    The keys of the mapping become files under the given root, and the\n    values (which must be bytes) the contents of those files.\n\n    Parameters\n    ----------\n    root: string\n        prefix for all the files\n    fs: FileSystem instance\n    check: bool (=True)\n        performs a touch at the location, to check for write access.\n\n    Examples\n    --------\n    >>> fs = FileSystem(**parameters)  # doctest: +SKIP\n    >>> d = FSMap('my-data/path/', fs)  # doctest: +SKIP\n    or, more likely\n    >>> d = fs.get_mapper('my-data/path/')\n\n    >>> d['loc1'] = b'Hello World'  # doctest: +SKIP\n    >>> list(d.keys())  # doctest: +SKIP\n    ['loc1']\n    >>> d['loc1']  # doctest: +SKIP\n    b'Hello World'\n    \"\"\"\n\n    def __init__(self, root, fs, check=False, create=False, missing_exceptions=None):\n        self.fs = fs\n        self.root = fs._strip_protocol(root)\n        self._root_key_to_str = fs._strip_protocol(posixpath.join(root, \"x\"))[:-1]\n        if missing_exceptions is None:\n            missing_exceptions = (\n                FileNotFoundError,\n                IsADirectoryError,\n                NotADirectoryError,\n            )\n        self.missing_exceptions = missing_exceptions\n        self.check = check\n        self.create = create\n        if create:\n            if not self.fs.exists(root):\n                self.fs.mkdir(root)\n        if check:\n            if not self.fs.exists(root):\n                raise ValueError(\n                    f\"Path {root} does not exist. Create \"\n                    f\" with the ``create=True`` keyword\"\n                )\n            self.fs.touch(root + \"/a\")\n            self.fs.rm(root + \"/a\")\n\n    @cached_property\n    def dirfs(self):\n        \"\"\"dirfs instance that can be used with the same keys as the mapper\"\"\"\n        from .implementations.dirfs import DirFileSystem\n\n        return DirFileSystem(path=self._root_key_to_str, fs=self.fs)\n\n    def clear(self):\n        \"\"\"Remove all keys below root - empties out mapping\"\"\"\n        logger.info(\"Clear mapping at %s\", self.root)\n        try:\n            self.fs.rm(self.root, True)\n            self.fs.mkdir(self.root)\n        except:  # noqa: E722\n            pass\n\n    def getitems(self, keys, on_error=\"raise\"):\n        \"\"\"Fetch multiple items from the store\n\n        If the backend is async-able, this might proceed concurrently\n\n        Parameters\n        ----------\n        keys: list(str)\n            They keys to be fetched\n        on_error : \"raise\", \"omit\", \"return\"\n            If raise, an underlying exception will be raised (converted to KeyError\n            if the type is in self.missing_exceptions); if omit, keys with exception\n            will simply not be included in the output; if \"return\", all keys are\n            included in the output, but the value will be bytes or an exception\n            instance.\n\n        Returns\n        -------\n        dict(key, bytes|exception)\n        \"\"\"\n        keys2 = [self._key_to_str(k) for k in keys]\n        oe = on_error if on_error == \"raise\" else \"return\"\n        try:\n            out = self.fs.cat(keys2, on_error=oe)\n            if isinstance(out, bytes):\n                out = {keys2[0]: out}\n        except self.missing_exceptions as e:\n            raise KeyError from e\n        out = {\n            k: (KeyError() if isinstance(v, self.missing_exceptions) else v)\n            for k, v in out.items()\n        }\n        return {\n            key: out[k2]\n            for key, k2 in zip(keys, keys2)\n            if on_error == \"return\" or not isinstance(out[k2], BaseException)\n        }\n\n    def setitems(self, values_dict):\n        \"\"\"Set the values of multiple items in the store\n\n        Parameters\n        ----------\n        values_dict: dict(str, bytes)\n        \"\"\"\n        values = {self._key_to_str(k): maybe_convert(v) for k, v in values_dict.items()}\n        self.fs.pipe(values)\n\n    def delitems(self, keys):\n        \"\"\"Remove multiple keys from the store\"\"\"\n        self.fs.rm([self._key_to_str(k) for k in keys])\n\n    def _key_to_str(self, key):\n        \"\"\"Generate full path for the key\"\"\"\n        if not isinstance(key, str):\n            # raise TypeError(\"key must be of type `str`, got `{type(key).__name__}`\"\n            warnings.warn(\n                \"from fsspec 2023.5 onward FSMap non-str keys will raise TypeError\",\n                DeprecationWarning,\n            )\n            if isinstance(key, list):\n                key = tuple(key)\n            key = str(key)\n        return f\"{self._root_key_to_str}{key}\".rstrip(\"/\")\n\n    def _str_to_key(self, s):\n        \"\"\"Strip path of to leave key name\"\"\"\n        return s[len(self.root) :].lstrip(\"/\")\n\n    def __getitem__(self, key, default=None):\n        \"\"\"Retrieve data\"\"\"\n        k = self._key_to_str(key)\n        try:\n            result = self.fs.cat(k)\n        except self.missing_exceptions:\n            if default is not None:\n                return default\n            raise KeyError(key)\n        return result\n\n    def pop(self, key, default=None):\n        \"\"\"Pop data\"\"\"\n        result = self.__getitem__(key, default)\n        try:\n            del self[key]\n        except KeyError:\n            pass\n        return result\n\n    def __setitem__(self, key, value):\n        \"\"\"Store value in key\"\"\"\n        key = self._key_to_str(key)\n        self.fs.mkdirs(self.fs._parent(key), exist_ok=True)\n        self.fs.pipe_file(key, maybe_convert(value))\n\n    def __iter__(self):\n        return (self._str_to_key(x) for x in self.fs.find(self.root))\n\n    def __len__(self):\n        return len(self.fs.find(self.root))\n\n    def __delitem__(self, key):\n        \"\"\"Remove key\"\"\"\n        try:\n            self.fs.rm(self._key_to_str(key))\n        except:  # noqa: E722\n            raise KeyError\n\n    def __contains__(self, key):\n        \"\"\"Does key exist in mapping?\"\"\"\n        path = self._key_to_str(key)\n        return self.fs.isfile(path)\n\n    def __reduce__(self):\n        return FSMap, (self.root, self.fs, False, False, self.missing_exceptions)\n\n\ndef maybe_convert(value):\n    if isinstance(value, array.array) or hasattr(value, \"__array__\"):\n        # bytes-like things\n        if hasattr(value, \"dtype\") and value.dtype.kind in \"Mm\":\n            # The buffer interface doesn't support datetime64/timdelta64 numpy\n            # arrays\n            value = value.view(\"int64\")\n        value = bytes(memoryview(value))\n    return value\n\n\ndef get_mapper(\n    url=\"\",\n    check=False,\n    create=False,\n    missing_exceptions=None,\n    alternate_root=None,\n    **kwargs,\n):\n    \"\"\"Create key-value interface for given URL and options\n\n    The URL will be of the form \"protocol://location\" and point to the root\n    of the mapper required. All keys will be file-names below this location,\n    and their values the contents of each key.\n\n    Also accepts compound URLs like zip::s3://bucket/file.zip , see ``fsspec.open``.\n\n    Parameters\n    ----------\n    url: str\n        Root URL of mapping\n    check: bool\n        Whether to attempt to read from the location before instantiation, to\n        check that the mapping does exist\n    create: bool\n        Whether to make the directory corresponding to the root before\n        instantiating\n    missing_exceptions: None or tuple\n        If given, these exception types will be regarded as missing keys and\n        return KeyError when trying to read data. By default, you get\n        (FileNotFoundError, IsADirectoryError, NotADirectoryError)\n    alternate_root: None or str\n        In cases of complex URLs, the parser may fail to pick the correct part\n        for the mapper root, so this arg can override\n\n    Returns\n    -------\n    ``FSMap`` instance, the dict-like key-value store.\n    \"\"\"\n    # Removing protocol here - could defer to each open() on the backend\n    fs, urlpath = url_to_fs(url, **kwargs)\n    root = alternate_root if alternate_root is not None else urlpath\n    return FSMap(root, fs, check, create, missing_exceptions=missing_exceptions)\n", "fsspec/json.py": "import json\nfrom contextlib import suppress\nfrom pathlib import PurePath\nfrom typing import Any, Callable, ClassVar, Dict, List, Optional, Tuple\n\nfrom .registry import _import_class, get_filesystem_class\nfrom .spec import AbstractFileSystem\n\n\nclass FilesystemJSONEncoder(json.JSONEncoder):\n    include_password: ClassVar[bool] = True\n\n    def default(self, o: Any) -> Any:\n        if isinstance(o, AbstractFileSystem):\n            return o.to_dict(include_password=self.include_password)\n        if isinstance(o, PurePath):\n            cls = type(o)\n            return {\"cls\": f\"{cls.__module__}.{cls.__name__}\", \"str\": str(o)}\n\n        return super().default(o)\n\n\nclass FilesystemJSONDecoder(json.JSONDecoder):\n    def __init__(\n        self,\n        *,\n        object_hook: Optional[Callable[[Dict[str, Any]], Any]] = None,\n        parse_float: Optional[Callable[[str], Any]] = None,\n        parse_int: Optional[Callable[[str], Any]] = None,\n        parse_constant: Optional[Callable[[str], Any]] = None,\n        strict: bool = True,\n        object_pairs_hook: Optional[Callable[[List[Tuple[str, Any]]], Any]] = None,\n    ) -> None:\n        self.original_object_hook = object_hook\n\n        super().__init__(\n            object_hook=self.custom_object_hook,\n            parse_float=parse_float,\n            parse_int=parse_int,\n            parse_constant=parse_constant,\n            strict=strict,\n            object_pairs_hook=object_pairs_hook,\n        )\n\n    @classmethod\n    def try_resolve_path_cls(cls, dct: Dict[str, Any]):\n        with suppress(Exception):\n            fqp = dct[\"cls\"]\n\n            path_cls = _import_class(fqp)\n\n            if issubclass(path_cls, PurePath):\n                return path_cls\n\n        return None\n\n    @classmethod\n    def try_resolve_fs_cls(cls, dct: Dict[str, Any]):\n        with suppress(Exception):\n            if \"cls\" in dct:\n                try:\n                    fs_cls = _import_class(dct[\"cls\"])\n                    if issubclass(fs_cls, AbstractFileSystem):\n                        return fs_cls\n                except Exception:\n                    if \"protocol\" in dct:  # Fallback if cls cannot be imported\n                        return get_filesystem_class(dct[\"protocol\"])\n\n                    raise\n\n        return None\n\n    def custom_object_hook(self, dct: Dict[str, Any]):\n        if \"cls\" in dct:\n            if (obj_cls := self.try_resolve_fs_cls(dct)) is not None:\n                return AbstractFileSystem.from_dict(dct)\n            if (obj_cls := self.try_resolve_path_cls(dct)) is not None:\n                return obj_cls(dct[\"str\"])\n\n        if self.original_object_hook is not None:\n            return self.original_object_hook(dct)\n\n        return dct\n", "fsspec/conftest.py": "import os\nimport shutil\nimport subprocess\nimport sys\nimport time\n\nimport pytest\n\nimport fsspec\nfrom fsspec.implementations.cached import CachingFileSystem\n\n\n@pytest.fixture()\ndef m():\n    \"\"\"\n    Fixture providing a memory filesystem.\n    \"\"\"\n    m = fsspec.filesystem(\"memory\")\n    m.store.clear()\n    m.pseudo_dirs.clear()\n    m.pseudo_dirs.append(\"\")\n    try:\n        yield m\n    finally:\n        m.store.clear()\n        m.pseudo_dirs.clear()\n        m.pseudo_dirs.append(\"\")\n\n\n@pytest.fixture\ndef ftp_writable(tmpdir):\n    \"\"\"\n    Fixture providing a writable FTP filesystem.\n    \"\"\"\n    pytest.importorskip(\"pyftpdlib\")\n    from fsspec.implementations.ftp import FTPFileSystem\n\n    FTPFileSystem.clear_instance_cache()  # remove lingering connections\n    CachingFileSystem.clear_instance_cache()\n    d = str(tmpdir)\n    with open(os.path.join(d, \"out\"), \"wb\") as f:\n        f.write(b\"hello\" * 10000)\n    P = subprocess.Popen(\n        [sys.executable, \"-m\", \"pyftpdlib\", \"-d\", d, \"-u\", \"user\", \"-P\", \"pass\", \"-w\"]\n    )\n    try:\n        time.sleep(1)\n        yield \"localhost\", 2121, \"user\", \"pass\"\n    finally:\n        P.terminate()\n        P.wait()\n        try:\n            shutil.rmtree(tmpdir)\n        except Exception:\n            pass\n", "fsspec/archive.py": "from fsspec import AbstractFileSystem\nfrom fsspec.utils import tokenize\n\n\nclass AbstractArchiveFileSystem(AbstractFileSystem):\n    \"\"\"\n    A generic superclass for implementing Archive-based filesystems.\n\n    Currently, it is shared amongst\n    :class:`~fsspec.implementations.zip.ZipFileSystem`,\n    :class:`~fsspec.implementations.libarchive.LibArchiveFileSystem` and\n    :class:`~fsspec.implementations.tar.TarFileSystem`.\n    \"\"\"\n\n    def __str__(self):\n        return f\"<Archive-like object {type(self).__name__} at {id(self)}>\"\n\n    __repr__ = __str__\n\n    def ukey(self, path):\n        return tokenize(path, self.fo, self.protocol)\n\n    def _all_dirnames(self, paths):\n        \"\"\"Returns *all* directory names for each path in paths, including intermediate\n        ones.\n\n        Parameters\n        ----------\n        paths: Iterable of path strings\n        \"\"\"\n        if len(paths) == 0:\n            return set()\n\n        dirnames = {self._parent(path) for path in paths} - {self.root_marker}\n        return dirnames | self._all_dirnames(dirnames)\n\n    def info(self, path, **kwargs):\n        self._get_dirs()\n        path = self._strip_protocol(path)\n        if path in {\"\", \"/\"} and self.dir_cache:\n            return {\"name\": \"\", \"type\": \"directory\", \"size\": 0}\n        if path in self.dir_cache:\n            return self.dir_cache[path]\n        elif path + \"/\" in self.dir_cache:\n            return self.dir_cache[path + \"/\"]\n        else:\n            raise FileNotFoundError(path)\n\n    def ls(self, path, detail=True, **kwargs):\n        self._get_dirs()\n        paths = {}\n        for p, f in self.dir_cache.items():\n            p = p.rstrip(\"/\")\n            if \"/\" in p:\n                root = p.rsplit(\"/\", 1)[0]\n            else:\n                root = \"\"\n            if root == path.rstrip(\"/\"):\n                paths[p] = f\n            elif all(\n                (a == b)\n                for a, b in zip(path.split(\"/\"), [\"\"] + p.strip(\"/\").split(\"/\"))\n            ):\n                # root directory entry\n                ppath = p.rstrip(\"/\").split(\"/\", 1)[0]\n                if ppath not in paths:\n                    out = {\"name\": ppath, \"size\": 0, \"type\": \"directory\"}\n                    paths[ppath] = out\n        if detail:\n            out = sorted(paths.values(), key=lambda _: _[\"name\"])\n            return out\n        else:\n            return sorted(paths)\n", "fsspec/gui.py": "import ast\nimport contextlib\nimport logging\nimport os\nimport re\nfrom typing import ClassVar, Sequence\n\nimport panel as pn\n\nfrom .core import OpenFile, get_filesystem_class, split_protocol\nfrom .registry import known_implementations\n\npn.extension()\nlogger = logging.getLogger(\"fsspec.gui\")\n\n\nclass SigSlot:\n    \"\"\"Signal-slot mixin, for Panel event passing\n\n    Include this class in a widget manager's superclasses to be able to\n    register events and callbacks on Panel widgets managed by that class.\n\n    The method ``_register`` should be called as widgets are added, and external\n    code should call ``connect`` to associate callbacks.\n\n    By default, all signals emit a DEBUG logging statement.\n    \"\"\"\n\n    # names of signals that this class may emit each of which must be\n    # set by _register for any new instance\n    signals: ClassVar[Sequence[str]] = []\n    # names of actions that this class may respond to\n    slots: ClassVar[Sequence[str]] = []\n\n    # each of which must be a method name\n\n    def __init__(self):\n        self._ignoring_events = False\n        self._sigs = {}\n        self._map = {}\n        self._setup()\n\n    def _setup(self):\n        \"\"\"Create GUI elements and register signals\"\"\"\n        self.panel = pn.pane.PaneBase()\n        # no signals to set up in the base class\n\n    def _register(\n        self, widget, name, thing=\"value\", log_level=logging.DEBUG, auto=False\n    ):\n        \"\"\"Watch the given attribute of a widget and assign it a named event\n\n        This is normally called at the time a widget is instantiated, in the\n        class which owns it.\n\n        Parameters\n        ----------\n        widget : pn.layout.Panel or None\n            Widget to watch. If None, an anonymous signal not associated with\n            any widget.\n        name : str\n            Name of this event\n        thing : str\n            Attribute of the given widget to watch\n        log_level : int\n            When the signal is triggered, a logging event of the given level\n            will be fired in the dfviz logger.\n        auto : bool\n            If True, automatically connects with a method in this class of the\n            same name.\n        \"\"\"\n        if name not in self.signals:\n            raise ValueError(f\"Attempt to assign an undeclared signal: {name}\")\n        self._sigs[name] = {\n            \"widget\": widget,\n            \"callbacks\": [],\n            \"thing\": thing,\n            \"log\": log_level,\n        }\n        wn = \"-\".join(\n            [\n                getattr(widget, \"name\", str(widget)) if widget is not None else \"none\",\n                thing,\n            ]\n        )\n        self._map[wn] = name\n        if widget is not None:\n            widget.param.watch(self._signal, thing, onlychanged=True)\n        if auto and hasattr(self, name):\n            self.connect(name, getattr(self, name))\n\n    def _repr_mimebundle_(self, *args, **kwargs):\n        \"\"\"Display in a notebook or a server\"\"\"\n        try:\n            return self.panel._repr_mimebundle_(*args, **kwargs)\n        except (ValueError, AttributeError):\n            raise NotImplementedError(\"Panel does not seem to be set up properly\")\n\n    def connect(self, signal, slot):\n        \"\"\"Associate call back with given event\n\n        The callback must be a function which takes the \"new\" value of the\n        watched attribute as the only parameter. If the callback return False,\n        this cancels any further processing of the given event.\n\n        Alternatively, the callback can be a string, in which case it means\n        emitting the correspondingly-named event (i.e., connect to self)\n        \"\"\"\n        self._sigs[signal][\"callbacks\"].append(slot)\n\n    def _signal(self, event):\n        \"\"\"This is called by a an action on a widget\n\n        Within an self.ignore_events context, nothing happens.\n\n        Tests can execute this method by directly changing the values of\n        widget components.\n        \"\"\"\n        if not self._ignoring_events:\n            wn = \"-\".join([event.obj.name, event.name])\n            if wn in self._map and self._map[wn] in self._sigs:\n                self._emit(self._map[wn], event.new)\n\n    @contextlib.contextmanager\n    def ignore_events(self):\n        \"\"\"Temporarily turn off events processing in this instance\n\n        (does not propagate to children)\n        \"\"\"\n        self._ignoring_events = True\n        try:\n            yield\n        finally:\n            self._ignoring_events = False\n\n    def _emit(self, sig, value=None):\n        \"\"\"An event happened, call its callbacks\n\n        This method can be used in tests to simulate message passing without\n        directly changing visual elements.\n\n        Calling of callbacks will halt whenever one returns False.\n        \"\"\"\n        logger.log(self._sigs[sig][\"log\"], f\"{sig}: {value}\")\n        for callback in self._sigs[sig][\"callbacks\"]:\n            if isinstance(callback, str):\n                self._emit(callback)\n            else:\n                try:\n                    # running callbacks should not break the interface\n                    ret = callback(value)\n                    if ret is False:\n                        break\n                except Exception as e:\n                    logger.exception(\n                        \"Exception (%s) while executing callback for signal: %s\",\n                        e,\n                        sig,\n                    )\n\n    def show(self, threads=False):\n        \"\"\"Open a new browser tab and display this instance's interface\"\"\"\n        self.panel.show(threads=threads, verbose=False)\n        return self\n\n\nclass SingleSelect(SigSlot):\n    \"\"\"A multiselect which only allows you to select one item for an event\"\"\"\n\n    signals = [\"_selected\", \"selected\"]  # the first is internal\n    slots = [\"set_options\", \"set_selection\", \"add\", \"clear\", \"select\"]\n\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n        super().__init__()\n\n    def _setup(self):\n        self.panel = pn.widgets.MultiSelect(**self.kwargs)\n        self._register(self.panel, \"_selected\", \"value\")\n        self._register(None, \"selected\")\n        self.connect(\"_selected\", self.select_one)\n\n    def _signal(self, *args, **kwargs):\n        super()._signal(*args, **kwargs)\n\n    def select_one(self, *_):\n        with self.ignore_events():\n            val = [self.panel.value[-1]] if self.panel.value else []\n            self.panel.value = val\n        self._emit(\"selected\", self.panel.value)\n\n    def set_options(self, options):\n        self.panel.options = options\n\n    def clear(self):\n        self.panel.options = []\n\n    @property\n    def value(self):\n        return self.panel.value\n\n    def set_selection(self, selection):\n        self.panel.value = [selection]\n\n\nclass FileSelector(SigSlot):\n    \"\"\"Panel-based graphical file selector widget\n\n    Instances of this widget are interactive and can be displayed in jupyter by having\n    them as the output of a cell,  or in a separate browser tab using ``.show()``.\n    \"\"\"\n\n    signals = [\n        \"protocol_changed\",\n        \"selection_changed\",\n        \"directory_entered\",\n        \"home_clicked\",\n        \"up_clicked\",\n        \"go_clicked\",\n        \"filters_changed\",\n    ]\n    slots = [\"set_filters\", \"go_home\"]\n\n    def __init__(self, url=None, filters=None, ignore=None, kwargs=None):\n        \"\"\"\n\n        Parameters\n        ----------\n        url : str (optional)\n            Initial value of the URL to populate the dialog; should include protocol\n        filters : list(str) (optional)\n            File endings to include in the listings. If not included, all files are\n            allowed. Does not affect directories.\n            If given, the endings will appear as checkboxes in the interface\n        ignore : list(str) (optional)\n            Regex(s) of file basename patterns to ignore, e.g., \"\\\\.\" for typical\n            hidden files on posix\n        kwargs : dict (optional)\n            To pass to file system instance\n        \"\"\"\n        if url:\n            self.init_protocol, url = split_protocol(url)\n        else:\n            self.init_protocol, url = \"file\", os.getcwd()\n        self.init_url = url\n        self.init_kwargs = (kwargs if isinstance(kwargs, str) else str(kwargs)) or \"{}\"\n        self.filters = filters\n        self.ignore = [re.compile(i) for i in ignore or []]\n        self._fs = None\n        super().__init__()\n\n    def _setup(self):\n        self.url = pn.widgets.TextInput(\n            name=\"url\",\n            value=self.init_url,\n            align=\"end\",\n            sizing_mode=\"stretch_width\",\n            width_policy=\"max\",\n        )\n        self.protocol = pn.widgets.Select(\n            options=sorted(known_implementations),\n            value=self.init_protocol,\n            name=\"protocol\",\n            align=\"center\",\n        )\n        self.kwargs = pn.widgets.TextInput(\n            name=\"kwargs\", value=self.init_kwargs, align=\"center\"\n        )\n        self.go = pn.widgets.Button(name=\"\u21e8\", align=\"end\", width=45)\n        self.main = SingleSelect(size=10)\n        self.home = pn.widgets.Button(name=\"\ud83c\udfe0\", width=40, height=30, align=\"end\")\n        self.up = pn.widgets.Button(name=\"\u2039\", width=30, height=30, align=\"end\")\n\n        self._register(self.protocol, \"protocol_changed\", auto=True)\n        self._register(self.go, \"go_clicked\", \"clicks\", auto=True)\n        self._register(self.up, \"up_clicked\", \"clicks\", auto=True)\n        self._register(self.home, \"home_clicked\", \"clicks\", auto=True)\n        self._register(None, \"selection_changed\")\n        self.main.connect(\"selected\", self.selection_changed)\n        self._register(None, \"directory_entered\")\n        self.prev_protocol = self.protocol.value\n        self.prev_kwargs = self.storage_options\n\n        self.filter_sel = pn.widgets.CheckBoxGroup(\n            value=[], options=[], inline=False, align=\"end\", width_policy=\"min\"\n        )\n        self._register(self.filter_sel, \"filters_changed\", auto=True)\n\n        self.panel = pn.Column(\n            pn.Row(self.protocol, self.kwargs),\n            pn.Row(self.home, self.up, self.url, self.go, self.filter_sel),\n            self.main.panel,\n        )\n        self.set_filters(self.filters)\n        self.go_clicked()\n\n    def set_filters(self, filters=None):\n        self.filters = filters\n        if filters:\n            self.filter_sel.options = filters\n            self.filter_sel.value = filters\n        else:\n            self.filter_sel.options = []\n            self.filter_sel.value = []\n\n    @property\n    def storage_options(self):\n        \"\"\"Value of the kwargs box as a dictionary\"\"\"\n        return ast.literal_eval(self.kwargs.value) or {}\n\n    @property\n    def fs(self):\n        \"\"\"Current filesystem instance\"\"\"\n        if self._fs is None:\n            cls = get_filesystem_class(self.protocol.value)\n            self._fs = cls(**self.storage_options)\n        return self._fs\n\n    @property\n    def urlpath(self):\n        \"\"\"URL of currently selected item\"\"\"\n        return (\n            (f\"{self.protocol.value}://{self.main.value[0]}\")\n            if self.main.value\n            else None\n        )\n\n    def open_file(self, mode=\"rb\", compression=None, encoding=None):\n        \"\"\"Create OpenFile instance for the currently selected item\n\n        For example, in a notebook you might do something like\n\n        .. code-block::\n\n            [ ]: sel = FileSelector(); sel\n\n            # user selects their file\n\n            [ ]: with sel.open_file('rb') as f:\n            ...      out = f.read()\n\n        Parameters\n        ----------\n        mode: str (optional)\n            Open mode for the file.\n        compression: str (optional)\n            The interact with the file as compressed. Set to 'infer' to guess\n            compression from the file ending\n        encoding: str (optional)\n            If using text mode, use this encoding; defaults to UTF8.\n        \"\"\"\n        if self.urlpath is None:\n            raise ValueError(\"No file selected\")\n        return OpenFile(self.fs, self.urlpath, mode, compression, encoding)\n\n    def filters_changed(self, values):\n        self.filters = values\n        self.go_clicked()\n\n    def selection_changed(self, *_):\n        if self.urlpath is None:\n            return\n        if self.fs.isdir(self.urlpath):\n            self.url.value = self.fs._strip_protocol(self.urlpath)\n        self.go_clicked()\n\n    def go_clicked(self, *_):\n        if (\n            self.prev_protocol != self.protocol.value\n            or self.prev_kwargs != self.storage_options\n        ):\n            self._fs = None  # causes fs to be recreated\n            self.prev_protocol = self.protocol.value\n            self.prev_kwargs = self.storage_options\n        listing = sorted(\n            self.fs.ls(self.url.value, detail=True), key=lambda x: x[\"name\"]\n        )\n        listing = [\n            l\n            for l in listing\n            if not any(i.match(l[\"name\"].rsplit(\"/\", 1)[-1]) for i in self.ignore)\n        ]\n        folders = {\n            \"\ud83d\udcc1 \" + o[\"name\"].rsplit(\"/\", 1)[-1]: o[\"name\"]\n            for o in listing\n            if o[\"type\"] == \"directory\"\n        }\n        files = {\n            \"\ud83d\udcc4 \" + o[\"name\"].rsplit(\"/\", 1)[-1]: o[\"name\"]\n            for o in listing\n            if o[\"type\"] == \"file\"\n        }\n        if self.filters:\n            files = {\n                k: v\n                for k, v in files.items()\n                if any(v.endswith(ext) for ext in self.filters)\n            }\n        self.main.set_options(dict(**folders, **files))\n\n    def protocol_changed(self, *_):\n        self._fs = None\n        self.main.options = []\n        self.url.value = \"\"\n\n    def home_clicked(self, *_):\n        self.protocol.value = self.init_protocol\n        self.kwargs.value = self.init_kwargs\n        self.url.value = self.init_url\n        self.go_clicked()\n\n    def up_clicked(self, *_):\n        self.url.value = self.fs._parent(self.url.value)\n        self.go_clicked()\n", "fsspec/dircache.py": "import time\nfrom collections.abc import MutableMapping\nfrom functools import lru_cache\n\n\nclass DirCache(MutableMapping):\n    \"\"\"\n    Caching of directory listings, in a structure like::\n\n        {\"path0\": [\n            {\"name\": \"path0/file0\",\n             \"size\": 123,\n             \"type\": \"file\",\n             ...\n            },\n            {\"name\": \"path0/file1\",\n            },\n            ...\n            ],\n         \"path1\": [...]\n        }\n\n    Parameters to this class control listing expiry or indeed turn\n    caching off\n    \"\"\"\n\n    def __init__(\n        self,\n        use_listings_cache=True,\n        listings_expiry_time=None,\n        max_paths=None,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        use_listings_cache: bool\n            If False, this cache never returns items, but always reports KeyError,\n            and setting items has no effect\n        listings_expiry_time: int or float (optional)\n            Time in seconds that a listing is considered valid. If None,\n            listings do not expire.\n        max_paths: int (optional)\n            The number of most recent listings that are considered valid; 'recent'\n            refers to when the entry was set.\n        \"\"\"\n        self._cache = {}\n        self._times = {}\n        if max_paths:\n            self._q = lru_cache(max_paths + 1)(lambda key: self._cache.pop(key, None))\n        self.use_listings_cache = use_listings_cache\n        self.listings_expiry_time = listings_expiry_time\n        self.max_paths = max_paths\n\n    def __getitem__(self, item):\n        if self.listings_expiry_time is not None:\n            if self._times.get(item, 0) - time.time() < -self.listings_expiry_time:\n                del self._cache[item]\n        if self.max_paths:\n            self._q(item)\n        return self._cache[item]  # maybe raises KeyError\n\n    def clear(self):\n        self._cache.clear()\n\n    def __len__(self):\n        return len(self._cache)\n\n    def __contains__(self, item):\n        try:\n            self[item]\n            return True\n        except KeyError:\n            return False\n\n    def __setitem__(self, key, value):\n        if not self.use_listings_cache:\n            return\n        if self.max_paths:\n            self._q(key)\n        self._cache[key] = value\n        if self.listings_expiry_time is not None:\n            self._times[key] = time.time()\n\n    def __delitem__(self, key):\n        del self._cache[key]\n\n    def __iter__(self):\n        entries = list(self._cache)\n\n        return (k for k in entries if k in self)\n\n    def __reduce__(self):\n        return (\n            DirCache,\n            (self.use_listings_cache, self.listings_expiry_time, self.max_paths),\n        )\n", "fsspec/__init__.py": "from importlib.metadata import entry_points\n\nfrom . import caching\nfrom ._version import __version__  # noqa: F401\nfrom .callbacks import Callback\nfrom .compression import available_compressions\nfrom .core import get_fs_token_paths, open, open_files, open_local, url_to_fs\nfrom .exceptions import FSTimeoutError\nfrom .mapping import FSMap, get_mapper\nfrom .registry import (\n    available_protocols,\n    filesystem,\n    get_filesystem_class,\n    register_implementation,\n    registry,\n)\nfrom .spec import AbstractFileSystem\n\n__all__ = [\n    \"AbstractFileSystem\",\n    \"FSTimeoutError\",\n    \"FSMap\",\n    \"filesystem\",\n    \"register_implementation\",\n    \"get_filesystem_class\",\n    \"get_fs_token_paths\",\n    \"get_mapper\",\n    \"open\",\n    \"open_files\",\n    \"open_local\",\n    \"registry\",\n    \"caching\",\n    \"Callback\",\n    \"available_protocols\",\n    \"available_compressions\",\n    \"url_to_fs\",\n]\n\n\ndef process_entries():\n    if entry_points is not None:\n        try:\n            eps = entry_points()\n        except TypeError:\n            pass  # importlib-metadata < 0.8\n        else:\n            if hasattr(eps, \"select\"):  # Python 3.10+ / importlib_metadata >= 3.9.0\n                specs = eps.select(group=\"fsspec.specs\")\n            else:\n                specs = eps.get(\"fsspec.specs\", [])\n            registered_names = {}\n            for spec in specs:\n                err_msg = f\"Unable to load filesystem from {spec}\"\n                name = spec.name\n                if name in registered_names:\n                    continue\n                registered_names[name] = True\n                register_implementation(\n                    name,\n                    spec.value.replace(\":\", \".\"),\n                    errtxt=err_msg,\n                    # We take our implementations as the ones to overload with if\n                    # for some reason we encounter some, may be the same, already\n                    # registered\n                    clobber=True,\n                )\n\n\nprocess_entries()\n", "fsspec/fuse.py": "import argparse\nimport logging\nimport os\nimport stat\nimport threading\nimport time\nfrom errno import EIO, ENOENT\n\nfrom fuse import FUSE, FuseOSError, LoggingMixIn, Operations\n\nfrom fsspec import __version__\nfrom fsspec.core import url_to_fs\n\nlogger = logging.getLogger(\"fsspec.fuse\")\n\n\nclass FUSEr(Operations):\n    def __init__(self, fs, path, ready_file=False):\n        self.fs = fs\n        self.cache = {}\n        self.root = path.rstrip(\"/\") + \"/\"\n        self.counter = 0\n        logger.info(\"Starting FUSE at %s\", path)\n        self._ready_file = ready_file\n\n    def getattr(self, path, fh=None):\n        logger.debug(\"getattr %s\", path)\n        if self._ready_file and path in [\"/.fuse_ready\", \".fuse_ready\"]:\n            return {\"type\": \"file\", \"st_size\": 5}\n\n        path = \"\".join([self.root, path.lstrip(\"/\")]).rstrip(\"/\")\n        try:\n            info = self.fs.info(path)\n        except FileNotFoundError:\n            raise FuseOSError(ENOENT)\n\n        data = {\"st_uid\": info.get(\"uid\", 1000), \"st_gid\": info.get(\"gid\", 1000)}\n        perm = info.get(\"mode\", 0o777)\n\n        if info[\"type\"] != \"file\":\n            data[\"st_mode\"] = stat.S_IFDIR | perm\n            data[\"st_size\"] = 0\n            data[\"st_blksize\"] = 0\n        else:\n            data[\"st_mode\"] = stat.S_IFREG | perm\n            data[\"st_size\"] = info[\"size\"]\n            data[\"st_blksize\"] = 5 * 2**20\n            data[\"st_nlink\"] = 1\n        data[\"st_atime\"] = info[\"atime\"] if \"atime\" in info else time.time()\n        data[\"st_ctime\"] = info[\"ctime\"] if \"ctime\" in info else time.time()\n        data[\"st_mtime\"] = info[\"mtime\"] if \"mtime\" in info else time.time()\n        return data\n\n    def readdir(self, path, fh):\n        logger.debug(\"readdir %s\", path)\n        path = \"\".join([self.root, path.lstrip(\"/\")])\n        files = self.fs.ls(path, False)\n        files = [os.path.basename(f.rstrip(\"/\")) for f in files]\n        return [\".\", \"..\"] + files\n\n    def mkdir(self, path, mode):\n        path = \"\".join([self.root, path.lstrip(\"/\")])\n        self.fs.mkdir(path)\n        return 0\n\n    def rmdir(self, path):\n        path = \"\".join([self.root, path.lstrip(\"/\")])\n        self.fs.rmdir(path)\n        return 0\n\n    def read(self, path, size, offset, fh):\n        logger.debug(\"read %s\", (path, size, offset))\n        if self._ready_file and path in [\"/.fuse_ready\", \".fuse_ready\"]:\n            # status indicator\n            return b\"ready\"\n\n        f = self.cache[fh]\n        f.seek(offset)\n        out = f.read(size)\n        return out\n\n    def write(self, path, data, offset, fh):\n        logger.debug(\"write %s\", (path, offset))\n        f = self.cache[fh]\n        f.seek(offset)\n        f.write(data)\n        return len(data)\n\n    def create(self, path, flags, fi=None):\n        logger.debug(\"create %s\", (path, flags))\n        fn = \"\".join([self.root, path.lstrip(\"/\")])\n        self.fs.touch(fn)  # OS will want to get attributes immediately\n        f = self.fs.open(fn, \"wb\")\n        self.cache[self.counter] = f\n        self.counter += 1\n        return self.counter - 1\n\n    def open(self, path, flags):\n        logger.debug(\"open %s\", (path, flags))\n        fn = \"\".join([self.root, path.lstrip(\"/\")])\n        if flags % 2 == 0:\n            # read\n            mode = \"rb\"\n        else:\n            # write/create\n            mode = \"wb\"\n        self.cache[self.counter] = self.fs.open(fn, mode)\n        self.counter += 1\n        return self.counter - 1\n\n    def truncate(self, path, length, fh=None):\n        fn = \"\".join([self.root, path.lstrip(\"/\")])\n        if length != 0:\n            raise NotImplementedError\n        # maybe should be no-op since open with write sets size to zero anyway\n        self.fs.touch(fn)\n\n    def unlink(self, path):\n        fn = \"\".join([self.root, path.lstrip(\"/\")])\n        try:\n            self.fs.rm(fn, False)\n        except (OSError, FileNotFoundError):\n            raise FuseOSError(EIO)\n\n    def release(self, path, fh):\n        try:\n            if fh in self.cache:\n                f = self.cache[fh]\n                f.close()\n                self.cache.pop(fh)\n        except Exception as e:\n            print(e)\n        return 0\n\n    def chmod(self, path, mode):\n        if hasattr(self.fs, \"chmod\"):\n            path = \"\".join([self.root, path.lstrip(\"/\")])\n            return self.fs.chmod(path, mode)\n        raise NotImplementedError\n\n\ndef run(\n    fs,\n    path,\n    mount_point,\n    foreground=True,\n    threads=False,\n    ready_file=False,\n    ops_class=FUSEr,\n):\n    \"\"\"Mount stuff in a local directory\n\n    This uses fusepy to make it appear as if a given path on an fsspec\n    instance is in fact resident within the local file-system.\n\n    This requires that fusepy by installed, and that FUSE be available on\n    the system (typically requiring a package to be installed with\n    apt, yum, brew, etc.).\n\n    Parameters\n    ----------\n    fs: file-system instance\n        From one of the compatible implementations\n    path: str\n        Location on that file-system to regard as the root directory to\n        mount. Note that you typically should include the terminating \"/\"\n        character.\n    mount_point: str\n        An empty directory on the local file-system where the contents of\n        the remote path will appear.\n    foreground: bool\n        Whether or not calling this function will block. Operation will\n        typically be more stable if True.\n    threads: bool\n        Whether or not to create threads when responding to file operations\n        within the mounter directory. Operation will typically be more\n        stable if False.\n    ready_file: bool\n        Whether the FUSE process is ready. The ``.fuse_ready`` file will\n        exist in the ``mount_point`` directory if True. Debugging purpose.\n    ops_class: FUSEr or Subclass of FUSEr\n        To override the default behavior of FUSEr. For Example, logging\n        to file.\n\n    \"\"\"\n    func = lambda: FUSE(\n        ops_class(fs, path, ready_file=ready_file),\n        mount_point,\n        nothreads=not threads,\n        foreground=foreground,\n    )\n    if not foreground:\n        th = threading.Thread(target=func)\n        th.daemon = True\n        th.start()\n        return th\n    else:  # pragma: no cover\n        try:\n            func()\n        except KeyboardInterrupt:\n            pass\n\n\ndef main(args):\n    \"\"\"Mount filesystem from chained URL to MOUNT_POINT.\n\n    Examples:\n\n    python3 -m fsspec.fuse memory /usr/share /tmp/mem\n\n    python3 -m fsspec.fuse local /tmp/source /tmp/local \\\\\n            -l /tmp/fsspecfuse.log\n\n    You can also mount chained-URLs and use special settings:\n\n    python3 -m fsspec.fuse 'filecache::zip::file://data.zip' \\\\\n            / /tmp/zip \\\\\n            -o 'filecache-cache_storage=/tmp/simplecache'\n\n    You can specify the type of the setting by using `[int]` or `[bool]`,\n    (`true`, `yes`, `1` represents the Boolean value `True`):\n\n    python3 -m fsspec.fuse 'simplecache::ftp://ftp1.at.proftpd.org' \\\\\n            /historic/packages/RPMS /tmp/ftp \\\\\n            -o 'simplecache-cache_storage=/tmp/simplecache' \\\\\n            -o 'simplecache-check_files=false[bool]' \\\\\n            -o 'ftp-listings_expiry_time=60[int]' \\\\\n            -o 'ftp-username=anonymous' \\\\\n            -o 'ftp-password=xieyanbo'\n    \"\"\"\n\n    class RawDescriptionArgumentParser(argparse.ArgumentParser):\n        def format_help(self):\n            usage = super().format_help()\n            parts = usage.split(\"\\n\\n\")\n            parts[1] = self.description.rstrip()\n            return \"\\n\\n\".join(parts)\n\n    parser = RawDescriptionArgumentParser(prog=\"fsspec.fuse\", description=main.__doc__)\n    parser.add_argument(\"--version\", action=\"version\", version=__version__)\n    parser.add_argument(\"url\", type=str, help=\"fs url\")\n    parser.add_argument(\"source_path\", type=str, help=\"source directory in fs\")\n    parser.add_argument(\"mount_point\", type=str, help=\"local directory\")\n    parser.add_argument(\n        \"-o\",\n        \"--option\",\n        action=\"append\",\n        help=\"Any options of protocol included in the chained URL\",\n    )\n    parser.add_argument(\n        \"-l\", \"--log-file\", type=str, help=\"Logging FUSE debug info (Default: '')\"\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--foreground\",\n        action=\"store_false\",\n        help=\"Running in foreground or not (Default: False)\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--threads\",\n        action=\"store_false\",\n        help=\"Running with threads support (Default: False)\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--ready-file\",\n        action=\"store_false\",\n        help=\"The `.fuse_ready` file will exist after FUSE is ready. \"\n        \"(Debugging purpose, Default: False)\",\n    )\n    args = parser.parse_args(args)\n\n    kwargs = {}\n    for item in args.option or []:\n        key, sep, value = item.partition(\"=\")\n        if not sep:\n            parser.error(message=f\"Wrong option: {item!r}\")\n        val = value.lower()\n        if val.endswith(\"[int]\"):\n            value = int(value[: -len(\"[int]\")])\n        elif val.endswith(\"[bool]\"):\n            value = val[: -len(\"[bool]\")] in [\"1\", \"yes\", \"true\"]\n\n        if \"-\" in key:\n            fs_name, setting_name = key.split(\"-\", 1)\n            if fs_name in kwargs:\n                kwargs[fs_name][setting_name] = value\n            else:\n                kwargs[fs_name] = {setting_name: value}\n        else:\n            kwargs[key] = value\n\n    if args.log_file:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            filename=args.log_file,\n            format=\"%(asctime)s %(message)s\",\n        )\n\n        class LoggingFUSEr(FUSEr, LoggingMixIn):\n            pass\n\n        fuser = LoggingFUSEr\n    else:\n        fuser = FUSEr\n\n    fs, url_path = url_to_fs(args.url, **kwargs)\n    logger.debug(\"Mounting %s to %s\", url_path, str(args.mount_point))\n    run(\n        fs,\n        args.source_path,\n        args.mount_point,\n        foreground=args.foreground,\n        threads=args.threads,\n        ready_file=args.ready_file,\n        ops_class=fuser,\n    )\n\n\nif __name__ == \"__main__\":\n    import sys\n\n    main(sys.argv[1:])\n", "fsspec/tests/test_compression.py": "import pathlib\n\nimport pytest\n\nimport fsspec.core\nfrom fsspec.compression import compr, register_compression\nfrom fsspec.utils import compressions, infer_compression\n\n\ndef test_infer_custom_compression():\n    \"\"\"Inferred compression gets values from fsspec.compression.compr.\"\"\"\n    assert infer_compression(\"fn.zip\") == \"zip\"\n    assert infer_compression(\"fn.gz\") == \"gzip\"\n    assert infer_compression(\"fn.unknown\") is None\n    assert infer_compression(\"fn.test_custom\") is None\n    assert infer_compression(\"fn.tst\") is None\n\n    register_compression(\"test_custom\", lambda f, **kwargs: f, \"tst\")\n\n    try:\n        assert infer_compression(\"fn.zip\") == \"zip\"\n        assert infer_compression(\"fn.gz\") == \"gzip\"\n        assert infer_compression(\"fn.unknown\") is None\n        assert infer_compression(\"fn.test_custom\") is None\n        assert infer_compression(\"fn.tst\") == \"test_custom\"\n\n        # Duplicate registration in name or extension raises a value error.\n        with pytest.raises(ValueError):\n            register_compression(\"test_custom\", lambda f, **kwargs: f, \"tst\")\n\n        with pytest.raises(ValueError):\n            register_compression(\"test_conflicting\", lambda f, **kwargs: f, \"tst\")\n        assert \"test_conflicting\" not in compr\n\n        # ...but can be forced.\n        register_compression(\n            \"test_conflicting\", lambda f, **kwargs: f, \"tst\", force=True\n        )\n        assert infer_compression(\"fn.zip\") == \"zip\"\n        assert infer_compression(\"fn.gz\") == \"gzip\"\n        assert infer_compression(\"fn.unknown\") is None\n        assert infer_compression(\"fn.test_custom\") is None\n        assert infer_compression(\"fn.tst\") == \"test_conflicting\"\n\n    finally:\n        del compr[\"test_custom\"]\n        del compr[\"test_conflicting\"]\n        del compressions[\"tst\"]\n\n\ndef test_infer_uppercase_compression():\n    assert infer_compression(\"fn.ZIP\") == \"zip\"\n    assert infer_compression(\"fn.GZ\") == \"gzip\"\n    assert infer_compression(\"fn.UNKNOWN\") is None\n    assert infer_compression(\"fn.TEST_UPPERCASE\") is None\n    assert infer_compression(\"fn.TEST\") is None\n\n\ndef test_lzma_compression_name():\n    pytest.importorskip(\"lzma\")\n    assert infer_compression(\"fn.xz\") == \"xz\"\n    assert infer_compression(\"fn.lzma\") == \"lzma\"\n\n\ndef test_lz4_compression(tmpdir):\n    \"\"\"Infer lz4 compression for .lz4 files if lz4 is available.\"\"\"\n    tmp_path = pathlib.Path(str(tmpdir))\n\n    lz4 = pytest.importorskip(\"lz4\")\n\n    tmp_path.mkdir(exist_ok=True)\n\n    tdat = \"foobar\" * 100\n\n    with fsspec.core.open(\n        str(tmp_path / \"out.lz4\"), mode=\"wt\", compression=\"infer\"\n    ) as outfile:\n        outfile.write(tdat)\n\n    compressed = (tmp_path / \"out.lz4\").open(\"rb\").read()\n    assert lz4.frame.decompress(compressed).decode() == tdat\n\n    with fsspec.core.open(\n        str(tmp_path / \"out.lz4\"), mode=\"rt\", compression=\"infer\"\n    ) as infile:\n        assert infile.read() == tdat\n\n    with fsspec.core.open(\n        str(tmp_path / \"out.lz4\"), mode=\"rt\", compression=\"lz4\"\n    ) as infile:\n        assert infile.read() == tdat\n\n\ndef test_zstd_compression(tmpdir):\n    \"\"\"Infer zstd compression for .zst files if zstandard is available.\"\"\"\n    tmp_path = pathlib.Path(str(tmpdir))\n\n    zstd = pytest.importorskip(\"zstandard\")\n\n    tmp_path.mkdir(exist_ok=True)\n\n    tdat = \"foobar\" * 100\n\n    with fsspec.core.open(\n        str(tmp_path / \"out.zst\"), mode=\"wt\", compression=\"infer\"\n    ) as outfile:\n        outfile.write(tdat)\n\n    compressed = (tmp_path / \"out.zst\").open(\"rb\").read()\n    assert zstd.ZstdDecompressor().decompress(compressed, len(tdat)).decode() == tdat\n\n    with fsspec.core.open(\n        str(tmp_path / \"out.zst\"), mode=\"rt\", compression=\"infer\"\n    ) as infile:\n        assert infile.read() == tdat\n\n    with fsspec.core.open(\n        str(tmp_path / \"out.zst\"), mode=\"rt\", compression=\"zstd\"\n    ) as infile:\n        assert infile.read() == tdat\n\n    # fails in https://github.com/fsspec/filesystem_spec/issues/725\n    infile = fsspec.core.open(\n        str(tmp_path / \"out.zst\"), mode=\"rb\", compression=\"infer\"\n    ).open()\n\n    infile.close()\n\n\ndef test_snappy_compression(tmpdir):\n    \"\"\"No registered compression for snappy, but can be specified.\"\"\"\n    tmp_path = pathlib.Path(str(tmpdir))\n\n    snappy = pytest.importorskip(\"snappy\")\n\n    tmp_path.mkdir(exist_ok=True)\n\n    tdat = \"foobar\" * 100\n\n    # Snappy isn't inferred.\n    with fsspec.core.open(\n        str(tmp_path / \"out.snappy\"), mode=\"wt\", compression=\"infer\"\n    ) as outfile:\n        outfile.write(tdat)\n    assert (tmp_path / \"out.snappy\").open(\"rb\").read().decode() == tdat\n\n    # but can be specified.\n    with fsspec.core.open(\n        str(tmp_path / \"out.snappy\"), mode=\"wt\", compression=\"snappy\"\n    ) as outfile:\n        outfile.write(tdat)\n\n    compressed = (tmp_path / \"out.snappy\").open(\"rb\").read()\n    assert snappy.StreamDecompressor().decompress(compressed).decode() == tdat\n\n    with fsspec.core.open(\n        str(tmp_path / \"out.snappy\"), mode=\"rb\", compression=\"infer\"\n    ) as infile:\n        assert infile.read() == compressed\n\n    with fsspec.core.open(\n        str(tmp_path / \"out.snappy\"), mode=\"rt\", compression=\"snappy\"\n    ) as infile:\n        assert infile.read() == tdat\n", "fsspec/tests/test_file.py": "\"\"\"Tests abstract buffered file API, using FTP implementation\"\"\"\n\nimport pickle\n\nimport pytest\n\nfrom fsspec.implementations.tests.test_ftp import FTPFileSystem\n\ndata = b\"hello\" * 10000\n\n\ndef test_pickle(ftp_writable):\n    host, port, user, pw = ftp_writable\n    ftp = FTPFileSystem(host=host, port=port, username=user, password=pw)\n\n    f = ftp.open(\"/out\", \"rb\")\n\n    f2 = pickle.loads(pickle.dumps(f))\n    assert f == f2\n\n\ndef test_file_read_attributes(ftp_writable):\n    host, port, user, pw = ftp_writable\n    ftp = FTPFileSystem(host=host, port=port, username=user, password=pw)\n\n    f = ftp.open(\"/out\", \"rb\")\n    assert f.info()[\"size\"] == len(data)\n    assert f.tell() == 0\n    assert f.seekable()\n    assert f.readable()\n    assert not f.writable()\n    out = bytearray(len(data))\n\n    assert f.read() == data\n    assert f.read() == b\"\"\n    f.seek(0)\n    assert f.readuntil(b\"l\") == b\"hel\"\n    assert f.tell() == 3\n\n    f.readinto1(out)\n    assert out[:-3] == data[3:]\n    with pytest.raises(ValueError):\n        f.write(b\"\")\n    f.close()\n    with pytest.raises(ValueError):\n        f.read()(b\"\")\n\n\ndef test_seek(ftp_writable):\n    host, port, user, pw = ftp_writable\n    ftp = FTPFileSystem(host=host, port=port, username=user, password=pw)\n\n    f = ftp.open(\"/out\", \"rb\")\n\n    assert f.seek(-10, 2) == len(data) - 10\n    assert f.tell() == len(data) - 10\n    assert f.seek(-1, 1) == len(data) - 11\n    with pytest.raises(ValueError):\n        f.seek(-1)\n    with pytest.raises(ValueError):\n        f.seek(0, 7)\n\n\ndef test_file_idempotent(ftp_writable):\n    host, port, user, pw = ftp_writable\n    ftp = FTPFileSystem(host=host, port=port, username=user, password=pw)\n\n    f = ftp.open(\"/out\", \"rb\")\n    f2 = ftp.open(\"/out\", \"rb\")\n    assert hash(f) == hash(f2)\n    assert f == f2\n    ftp.touch(\"/out2\")\n    f2 = ftp.open(\"/out2\", \"rb\")\n    assert hash(f2) != hash(f)\n    assert f != f2\n    f2 = ftp.open(\"/out\", \"wb\")\n    assert hash(f2) != hash(f)\n\n\ndef test_file_text_attributes(ftp_writable):\n    host, port, user, pw = ftp_writable\n    ftp = FTPFileSystem(host=host, port=port, username=user, password=pw)\n\n    data = b\"hello\\n\" * 1000\n    with ftp.open(\"/out2\", \"wb\") as f:\n        f.write(data)\n\n    f = ftp.open(\"/out2\", \"rb\")\n    assert f.readline() == b\"hello\\n\"\n    f.seek(0)\n    assert list(f) == [d + b\"\\n\" for d in data.split()]\n    f.seek(0)\n    assert f.readlines() == [d + b\"\\n\" for d in data.split()]\n\n    f = ftp.open(\"/out2\", \"rt\")\n    assert f.readline() == \"hello\\n\"\n    assert f.encoding\n\n\ndef test_file_write_attributes(ftp_writable):\n    host, port, user, pw = ftp_writable\n    ftp = FTPFileSystem(host=host, port=port, username=user, password=pw)\n    f = ftp.open(\"/out2\", \"wb\")\n    with pytest.raises(ValueError):\n        f.info()\n    with pytest.raises(OSError):\n        f.seek(0)\n    with pytest.raises(ValueError):\n        f.read(0)\n    assert not f.readable()\n    assert f.writable()\n\n    f.flush()  # no-op\n\n    assert f.write(b\"hello\") == 5\n    assert f.write(b\"hello\") == 5\n    assert not f.closed\n    f.close()\n    assert f.closed\n    with pytest.raises(ValueError):\n        f.write(b\"\")\n    with pytest.raises(ValueError):\n        f.flush()\n\n\ndef test_midread_cache(ftp_writable):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host=host, port=port, username=user, password=pw)\n    fn = \"/myfile\"\n    with fs.open(fn, \"wb\") as f:\n        f.write(b\"a\" * 175627146)\n    with fs.open(fn, \"rb\") as f:\n        f.seek(175561610)\n        d1 = f.read(65536)\n        assert len(d1) == 65536\n\n        f.seek(4)\n        size = 17562198\n        d2 = f.read(size)\n        assert len(d2) == size\n\n        f.seek(17562288)\n        size = 17562187\n        d3 = f.read(size)\n        assert len(d3) == size\n\n\ndef test_read_block(ftp_writable):\n    # not the same as test_read_block in test_utils, this depends on the\n    # behaviour of the bytest caching\n    from fsspec.utils import read_block\n\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host=host, port=port, username=user, password=pw)\n    fn = \"/myfile\"\n    with fs.open(fn, \"wb\") as f:\n        f.write(b\"a,b\\n1,2\")\n    f = fs.open(fn, \"rb\", cache_type=\"bytes\")\n    assert read_block(f, 0, 6400, b\"\\n\") == b\"a,b\\n1,2\"\n\n\ndef test_with_gzip(ftp_writable):\n    import gzip\n\n    data = b\"some compressible stuff\"\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host=host, port=port, username=user, password=pw)\n    fn = \"/myfile\"\n    with fs.open(fn, \"wb\") as f:\n        gf = gzip.GzipFile(fileobj=f, mode=\"w\")\n        gf.write(data)\n        gf.close()\n    with fs.open(fn, \"rb\") as f:\n        gf = gzip.GzipFile(fileobj=f, mode=\"r\")\n        assert gf.read() == data\n\n\ndef test_auto_compression(m):\n    fs = m\n    with fs.open(\"myfile.gz\", mode=\"wt\", compression=\"infer\") as f:\n        f.write(\"text\")\n    with fs.open(\"myfile.gz\", mode=\"rt\", compression=\"infer\") as f:\n        assert f.read() == \"text\"\n\n\ndef test_with_zip(ftp_writable):\n    import zipfile\n\n    data = b\"hello zip\"\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host=host, port=port, username=user, password=pw)\n    fn = \"/myfile.zip\"\n    inner_file = \"test.txt\"\n    with fs.open(fn, \"wb\") as f:\n        zf = zipfile.ZipFile(f, mode=\"w\")\n        zf.writestr(inner_file, data)\n        zf.close()\n    with fs.open(fn, \"rb\") as f:\n        zf = zipfile.ZipFile(f, mode=\"r\")\n        assert zf.read(inner_file) == data\n", "fsspec/tests/test_caches.py": "import pickle\nimport string\n\nimport pytest\n\nfrom fsspec.caching import (\n    BlockCache,\n    FirstChunkCache,\n    ReadAheadCache,\n    caches,\n    register_cache,\n)\nfrom fsspec.implementations.cached import WholeFileCacheFileSystem\n\n\ndef test_cache_getitem(Cache_imp):\n    cacher = Cache_imp(4, letters_fetcher, len(string.ascii_letters))\n    assert cacher._fetch(0, 4) == b\"abcd\"\n    assert cacher._fetch(None, 4) == b\"abcd\"\n    assert cacher._fetch(2, 4) == b\"cd\"\n\n\ndef test_block_cache_lru():\n    # BlockCache is a cache that stores blocks of data and uses LRU to evict\n    block_size = 4\n    cache = BlockCache(\n        block_size, letters_fetcher, len(string.ascii_letters), maxblocks=2\n    )\n    # miss\n    cache._fetch(0, 2)\n    assert cache.cache_info().misses == 1\n    assert cache.cache_info().currsize == 1\n    assert cache.total_requested_bytes == block_size * cache.miss_count\n    assert cache.size == 52\n\n    # hit\n    cache._fetch(0, 2)\n    assert cache.cache_info().misses == 1\n    assert cache.cache_info().currsize == 1\n    assert cache.total_requested_bytes == block_size * cache.miss_count\n\n    # hit\n    cache._fetch(0, 2)\n    assert cache.cache_info().misses == 1\n    assert cache.cache_info().currsize == 1\n    # this works as a counter since all the reads are from the cache\n    assert cache.hit_count == 3\n    assert cache.miss_count == 1\n    # so far only 4 bytes have been read using range requests\n    assert cache.total_requested_bytes == block_size * cache.miss_count\n\n    # miss\n    cache._fetch(4, 6)\n    assert cache.cache_info().misses == 2\n    assert cache.cache_info().currsize == 2\n    assert cache.total_requested_bytes == block_size * cache.miss_count\n\n    # miss & evict\n    cache._fetch(12, 13)\n    assert cache.cache_info().misses == 3\n    assert cache.cache_info().currsize == 2\n    assert cache.hit_count == 5\n    assert cache.miss_count == 3\n    assert cache.total_requested_bytes == block_size * cache.miss_count\n\n\ndef test_first_cache():\n    \"\"\"\n    FirstChunkCache is a cache that only caches the first chunk of data\n    when some of that first block is requested.\n    \"\"\"\n    block_size = 5\n    cache = FirstChunkCache(block_size, letters_fetcher, len(string.ascii_letters))\n    assert cache.cache is None\n    assert cache._fetch(12, 15) == letters_fetcher(12, 15)\n    assert cache.miss_count == 1\n    assert cache.hit_count == 0\n    assert cache.cache is None\n    total_requested_bytes = 15 - 12\n    assert cache.total_requested_bytes == total_requested_bytes\n\n    # because we overlap with the cache range, it will be cached\n    assert cache._fetch(3, 10) == letters_fetcher(3, 10)\n    assert cache.miss_count == 2\n    assert cache.hit_count == 0\n    # we'll read the first 5 and then the rest\n    total_requested_bytes += block_size + 5\n    assert cache.total_requested_bytes == total_requested_bytes\n\n    # partial hit again\n    assert cache._fetch(3, 10) == letters_fetcher(3, 10)\n    assert cache.miss_count == 2\n    assert cache.hit_count == 1\n    # we have the first 5 bytes cached\n    total_requested_bytes += 10 - 5\n    assert cache.total_requested_bytes == total_requested_bytes\n\n    assert cache.cache == letters_fetcher(0, 5)\n    assert cache._fetch(0, 4) == letters_fetcher(0, 4)\n    assert cache.hit_count == 2\n    assert cache.miss_count == 2\n    assert cache.total_requested_bytes == 18\n\n\ndef test_readahead_cache():\n    \"\"\"\n    ReadAheadCache is a cache that reads ahead of the requested range.\n    If the access pattern is not sequential it will be very inefficient.\n    \"\"\"\n    block_size = 5\n    cache = ReadAheadCache(block_size, letters_fetcher, len(string.ascii_letters))\n    assert cache._fetch(12, 15) == letters_fetcher(12, 15)\n    assert cache.miss_count == 1\n    assert cache.hit_count == 0\n    total_requested_bytes = 15 - 12 + block_size\n    assert cache.total_requested_bytes == total_requested_bytes\n\n    assert cache._fetch(3, 10) == letters_fetcher(3, 10)\n    assert cache.miss_count == 2\n    assert cache.hit_count == 0\n    assert len(cache.cache) == 12\n    total_requested_bytes += (10 - 3) + block_size\n    assert cache.total_requested_bytes == total_requested_bytes\n\n    # caache hit again\n    assert cache._fetch(3, 10) == letters_fetcher(3, 10)\n    assert cache.miss_count == 2\n    assert cache.hit_count == 1\n    assert len(cache.cache) == 12\n    assert cache.total_requested_bytes == total_requested_bytes\n    assert cache.cache == letters_fetcher(3, 15)\n\n    # cache miss\n    assert cache._fetch(0, 4) == letters_fetcher(0, 4)\n    assert cache.hit_count == 1\n    assert cache.miss_count == 3\n    assert len(cache.cache) == 9\n    total_requested_bytes += (4 - 0) + block_size\n    assert cache.total_requested_bytes == total_requested_bytes\n\n\ndef _fetcher(start, end):\n    return b\"0\" * (end - start)\n\n\ndef letters_fetcher(start, end):\n    return string.ascii_letters[start:end].encode()\n\n\nnot_parts_caches = {k: v for k, v in caches.items() if k != \"parts\"}\n\n\n@pytest.fixture(params=not_parts_caches.values(), ids=list(not_parts_caches))\ndef Cache_imp(request):\n    return request.param\n\n\ndef test_cache_empty_file(Cache_imp):\n    blocksize = 5\n    size = 0\n    cache = Cache_imp(blocksize, _fetcher, size)\n    assert cache._fetch(0, 0) == b\"\"\n\n\ndef test_cache_pickleable(Cache_imp):\n    blocksize = 5\n    size = 100\n    cache = Cache_imp(blocksize, _fetcher, size)\n    cache._fetch(0, 5)  # fill in cache\n    unpickled = pickle.loads(pickle.dumps(cache))\n    assert isinstance(unpickled, Cache_imp)\n    assert unpickled.blocksize == blocksize\n    assert unpickled.size == size\n    assert unpickled._fetch(0, 10) == b\"0\" * 10\n\n\n@pytest.mark.parametrize(\n    \"size_requests\",\n    [[(0, 30), (0, 35), (51, 52)], [(0, 1), (1, 11), (1, 52)], [(0, 52), (11, 15)]],\n)\n@pytest.mark.parametrize(\"blocksize\", [1, 10, 52, 100])\ndef test_cache_basic(Cache_imp, blocksize, size_requests):\n    cache = Cache_imp(blocksize, letters_fetcher, len(string.ascii_letters))\n\n    for start, end in size_requests:\n        result = cache._fetch(start, end)\n        expected = string.ascii_letters[start:end].encode()\n        assert result == expected\n\n\n@pytest.mark.parametrize(\"strict\", [True, False])\n@pytest.mark.parametrize(\"sort\", [True, False])\ndef test_known(sort, strict):\n    parts = {(10, 20): b\"1\" * 10, (20, 30): b\"2\" * 10, (0, 10): b\"0\" * 10}\n    if sort:\n        parts = dict(sorted(parts.items()))\n    c = caches[\"parts\"](None, None, 100, parts, strict=strict)\n    assert (0, 30) in c.data  # got consolidated\n    assert c._fetch(5, 15) == b\"0\" * 5 + b\"1\" * 5\n    assert c._fetch(15, 25) == b\"1\" * 5 + b\"2\" * 5\n    if strict:\n        # Over-read will raise error\n        with pytest.raises(ValueError):\n            # tries to call None fetcher\n            c._fetch(25, 35)\n    else:\n        # Over-read will be zero-padded\n        assert c._fetch(25, 35) == b\"2\" * 5 + b\"\\x00\" * 5\n\n\ndef test_background(server, monkeypatch):\n    import threading\n    import time\n\n    import fsspec\n\n    head = {\"head_ok\": \"true\", \"head_give_length\": \"true\"}\n    urla = server + \"/index/realfile\"\n    h = fsspec.filesystem(\"http\", headers=head)\n    thread_ids = {threading.current_thread().ident}\n    f = h.open(urla, block_size=5, cache_type=\"background\")\n    orig = f.cache._fetch_block\n\n    def wrapped(*a, **kw):\n        thread_ids.add(threading.current_thread().ident)\n        return orig(*a, **kw)\n\n    f.cache._fetch_block = wrapped\n    assert len(thread_ids) == 1\n    f.read(1)\n    time.sleep(0.1)  # second block is loading\n    assert len(thread_ids) == 2\n\n\ndef test_register_cache():\n    # just test that we have them populated and fail to re-add again unless overload\n    with pytest.raises(ValueError):\n        register_cache(BlockCache)\n    register_cache(BlockCache, clobber=True)\n\n\ndef test_cache_kwargs(mocker):\n    # test that kwargs are passed to the underlying filesystem after cache commit\n\n    fs = WholeFileCacheFileSystem(target_protocol=\"memory\")\n    fs.touch(\"test\")\n    fs.fs.put = mocker.MagicMock()\n\n    with fs.open(\"test\", \"wb\", overwrite=True) as file_handle:\n        file_handle.write(b\"foo\")\n\n    # We don't care about the first parameter, just retrieve its expected value.\n    # It is a random location that cannot be predicted.\n    # The important thing is the 'overwrite' kwarg\n    fs.fs.put.assert_called_with(fs.fs.put.call_args[0][0], \"/test\", overwrite=True)\n", "fsspec/tests/test_config.py": "import os\nfrom warnings import catch_warnings\n\nimport pytest\n\nimport fsspec\nfrom fsspec.config import conf, set_conf_env, set_conf_files\n\n\n@pytest.fixture\ndef clean_conf():\n    \"\"\"Tests should start and end with clean config dict\"\"\"\n    conf.clear()\n    yield\n    conf.clear()\n\n\ndef test_from_env_ignored(clean_conf):\n    env = {\n        \"FSSPEC\": \"missing_protocol\",\n        \"FSSPEC_\": \"missing_protocol\",\n        \"FSSPEC__INVALID_KEY\": \"invalid_protocol\",\n        \"FSSPEC_INVALID1\": \"not_json_dict\",\n        \"FSSPEC_INVALID2\": '[\"not_json_dict\"]',\n    }\n    cd = {}\n    with catch_warnings(record=True) as w:\n        set_conf_env(conf_dict=cd, envdict=env)\n        assert len(w) == 5\n        assert \"unexpected name\" in str(w[0].message)\n        assert \"unexpected name\" in str(w[1].message)\n        assert \"unexpected name\" in str(w[2].message)\n        assert \"parse failure\" in str(w[3].message)\n        assert \"not being a dict\" in str(w[4].message)\n    assert cd == {}\n\n\ndef test_from_env_kwargs(clean_conf):\n    env = {\n        \"FSSPEC_PROTO_KEY\": \"value\",\n        \"FSSPEC_PROTO_LONG_KEY\": \"othervalue\",\n        \"FSSPEC_MALFORMED\": \"novalue\",\n    }\n    cd = {}\n    with catch_warnings(record=True) as w:\n        set_conf_env(conf_dict=cd, envdict=env)\n        assert len(w) == 1\n        assert \"parse failure\" in str(w[0].message)\n    assert cd == {\"proto\": {\"key\": \"value\", \"long_key\": \"othervalue\"}}\n\n\ndef test_from_env_protocol_dict(clean_conf):\n    env = {\n        \"FSSPEC_PROTO\": '{\"int\": 1, \"float\": 2.3, \"bool\": true, \"dict\": {\"key\": \"val\"}}'\n    }\n    cd = {}\n    set_conf_env(conf_dict=cd, envdict=env)\n    assert cd == {\n        \"proto\": {\"int\": 1, \"float\": 2.3, \"bool\": True, \"dict\": {\"key\": \"val\"}}\n    }\n\n\ndef test_from_env_kwargs_override_protocol_dict(clean_conf):\n    env = {\n        \"FSSPEC_PROTO_LONG_KEY\": \"override1\",\n        \"FSSPEC_PROTO\": '{\"key\": \"value1\", \"long_key\": \"value2\", \"otherkey\": \"value3\"}',\n        \"FSSPEC_PROTO_KEY\": \"override2\",\n    }\n    cd = {}\n    set_conf_env(conf_dict=cd, envdict=env)\n    assert cd == {\n        \"proto\": {\"key\": \"override2\", \"long_key\": \"override1\", \"otherkey\": \"value3\"}\n    }\n\n\ndef test_from_file_ini(clean_conf, tmpdir):\n    file1 = os.path.join(tmpdir, \"1.ini\")\n    file2 = os.path.join(tmpdir, \"2.ini\")\n    with open(file1, \"w\") as f:\n        f.write(\n            \"\"\"[proto]\nkey=value\nother_key:othervalue\noverwritten=dont_see\n        \"\"\"\n        )\n    with open(file2, \"w\") as f:\n        f.write(\n            \"\"\"[proto]\noverwritten=see\n        \"\"\"\n        )\n    cd = {}\n    set_conf_files(tmpdir, cd)\n    assert cd == {\n        \"proto\": {\"key\": \"value\", \"other_key\": \"othervalue\", \"overwritten\": \"see\"}\n    }\n\n\ndef test_from_file_json(clean_conf, tmpdir):\n    file1 = os.path.join(tmpdir, \"1.json\")\n    file2 = os.path.join(tmpdir, \"2.json\")\n    with open(file1, \"w\") as f:\n        f.write(\n            \"\"\"{\"proto\":\n{\"key\": \"value\",\n\"other_key\": \"othervalue\",\n\"overwritten\": false}}\n        \"\"\"\n        )\n    with open(file2, \"w\") as f:\n        f.write(\n            \"\"\"{\"proto\":\n{\"overwritten\": true}}\n        \"\"\"\n        )\n    cd = {}\n    set_conf_files(tmpdir, cd)\n    assert cd == {\n        \"proto\": {\"key\": \"value\", \"other_key\": \"othervalue\", \"overwritten\": True}\n    }\n\n\ndef test_apply(clean_conf):\n    conf[\"file\"] = {\"auto_mkdir\": \"test\"}\n    fs = fsspec.filesystem(\"file\")\n    assert fs.auto_mkdir == \"test\"\n    fs = fsspec.filesystem(\"file\", auto_mkdir=True)\n    assert fs.auto_mkdir is True\n", "fsspec/tests/test_spec.py": "import glob\nimport json\nimport os\nimport pickle\nimport subprocess\nimport sys\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\n\nimport fsspec\nfrom fsspec.implementations.ftp import FTPFileSystem\nfrom fsspec.implementations.http import HTTPFileSystem\nfrom fsspec.implementations.local import LocalFileSystem\nfrom fsspec.spec import AbstractBufferedFile, AbstractFileSystem\n\nPATHS_FOR_GLOB_TESTS = (\n    {\"name\": \"test0.json\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0.yaml\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0\", \"type\": \"directory\", \"size\": 0},\n    {\"name\": \"test0/test0.json\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test0.yaml\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test1\", \"type\": \"directory\", \"size\": 0},\n    {\"name\": \"test0/test1/test0.json\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test1/test0.yaml\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test1/test2\", \"type\": \"directory\", \"size\": 0},\n    {\"name\": \"test0/test1/test2/test0.json\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test1/test2/test0.yaml\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test2\", \"type\": \"directory\", \"size\": 0},\n    {\"name\": \"test0/test2/test0.json\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test2/test0.yaml\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test2/test1\", \"type\": \"directory\", \"size\": 0},\n    {\"name\": \"test0/test2/test1/test0.json\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test2/test1/test0.yaml\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test2/test1/test3\", \"type\": \"directory\", \"size\": 0},\n    {\"name\": \"test0/test2/test1/test3/test0.json\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test0/test2/test1/test3/test0.yaml\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test1.json\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test1.yaml\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test1\", \"type\": \"directory\", \"size\": 0},\n    {\"name\": \"test1/test0.json\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test1/test0.yaml\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test1/test0\", \"type\": \"directory\", \"size\": 0},\n    {\"name\": \"test1/test0/test0.json\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"test1/test0/test0.yaml\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"special_chars\", \"type\": \"directory\", \"size\": 0},\n    {\"name\": \"special_chars/f\\\\oo.txt\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"special_chars/f.oo.txt\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"special_chars/f+oo.txt\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"special_chars/f(oo.txt\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"special_chars/f)oo.txt\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"special_chars/f|oo.txt\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"special_chars/f^oo.txt\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"special_chars/f$oo.txt\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"special_chars/f{oo.txt\", \"type\": \"file\", \"size\": 100},\n    {\"name\": \"special_chars/f}oo.txt\", \"type\": \"file\", \"size\": 100},\n)\n\nGLOB_POSIX_TESTS = {\n    \"argnames\": (\"path\", \"expected\"),\n    \"argvalues\": [\n        (\"nonexistent\", []),\n        (\"test0.json\", [\"test0.json\"]),\n        (\"test0\", [\"test0\"]),\n        (\"test0/\", [\"test0\"]),\n        (\"test1/test0.yaml\", [\"test1/test0.yaml\"]),\n        (\"test0/test[1-2]\", [\"test0/test1\", \"test0/test2\"]),\n        (\"test0/test[1-2]/\", [\"test0/test1\", \"test0/test2\"]),\n        (\n            \"test0/test[1-2]/*\",\n            [\n                \"test0/test1/test0.json\",\n                \"test0/test1/test0.yaml\",\n                \"test0/test1/test2\",\n                \"test0/test2/test0.json\",\n                \"test0/test2/test0.yaml\",\n                \"test0/test2/test1\",\n            ],\n        ),\n        (\n            \"test0/test[1-2]/*.[j]*\",\n            [\"test0/test1/test0.json\", \"test0/test2/test0.json\"],\n        ),\n        (\"special_chars/f\\\\oo.*\", [\"special_chars/f\\\\oo.txt\"]),\n        (\"special_chars/f.oo.*\", [\"special_chars/f.oo.txt\"]),\n        (\"special_chars/f+oo.*\", [\"special_chars/f+oo.txt\"]),\n        (\"special_chars/f(oo.*\", [\"special_chars/f(oo.txt\"]),\n        (\"special_chars/f)oo.*\", [\"special_chars/f)oo.txt\"]),\n        (\"special_chars/f|oo.*\", [\"special_chars/f|oo.txt\"]),\n        (\"special_chars/f^oo.*\", [\"special_chars/f^oo.txt\"]),\n        (\"special_chars/f$oo.*\", [\"special_chars/f$oo.txt\"]),\n        (\"special_chars/f{oo.*\", [\"special_chars/f{oo.txt\"]),\n        (\"special_chars/f}oo.*\", [\"special_chars/f}oo.txt\"]),\n        (\n            \"*\",\n            [\n                \"special_chars\",\n                \"test0.json\",\n                \"test0.yaml\",\n                \"test0\",\n                \"test1.json\",\n                \"test1.yaml\",\n                \"test1\",\n            ],\n        ),\n        (\"*.yaml\", [\"test0.yaml\", \"test1.yaml\"]),\n        (\n            \"**\",\n            [\n                \"special_chars\",\n                \"special_chars/f$oo.txt\",\n                \"special_chars/f(oo.txt\",\n                \"special_chars/f)oo.txt\",\n                \"special_chars/f+oo.txt\",\n                \"special_chars/f.oo.txt\",\n                \"special_chars/f\\\\oo.txt\",\n                \"special_chars/f^oo.txt\",\n                \"special_chars/f{oo.txt\",\n                \"special_chars/f|oo.txt\",\n                \"special_chars/f}oo.txt\",\n                \"test0.json\",\n                \"test0.yaml\",\n                \"test0\",\n                \"test0/test0.json\",\n                \"test0/test0.yaml\",\n                \"test0/test1\",\n                \"test0/test1/test0.json\",\n                \"test0/test1/test0.yaml\",\n                \"test0/test1/test2\",\n                \"test0/test1/test2/test0.json\",\n                \"test0/test1/test2/test0.yaml\",\n                \"test0/test2\",\n                \"test0/test2/test0.json\",\n                \"test0/test2/test0.yaml\",\n                \"test0/test2/test1\",\n                \"test0/test2/test1/test0.json\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test0/test2/test1/test3\",\n                \"test0/test2/test1/test3/test0.json\",\n                \"test0/test2/test1/test3/test0.yaml\",\n                \"test1.json\",\n                \"test1.yaml\",\n                \"test1\",\n                \"test1/test0.json\",\n                \"test1/test0.yaml\",\n                \"test1/test0\",\n                \"test1/test0/test0.json\",\n                \"test1/test0/test0.yaml\",\n            ],\n        ),\n        (\"*/\", [\"special_chars\", \"test0\", \"test1\"]),\n        (\n            \"**/\",\n            [\n                \"special_chars\",\n                \"test0\",\n                \"test0/test1\",\n                \"test0/test1/test2\",\n                \"test0/test2\",\n                \"test0/test2/test1\",\n                \"test0/test2/test1/test3\",\n                \"test1\",\n                \"test1/test0\",\n            ],\n        ),\n        (\"*/*.yaml\", [\"test0/test0.yaml\", \"test1/test0.yaml\"]),\n        (\n            \"**/*.yaml\",\n            [\n                \"test0.yaml\",\n                \"test0/test0.yaml\",\n                \"test0/test1/test0.yaml\",\n                \"test0/test1/test2/test0.yaml\",\n                \"test0/test2/test0.yaml\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test0/test2/test1/test3/test0.yaml\",\n                \"test1.yaml\",\n                \"test1/test0.yaml\",\n                \"test1/test0/test0.yaml\",\n            ],\n        ),\n        (\n            \"*/test1/*\",\n            [\"test0/test1/test0.json\", \"test0/test1/test0.yaml\", \"test0/test1/test2\"],\n        ),\n        (\"*/test1/*.yaml\", [\"test0/test1/test0.yaml\"]),\n        (\n            \"**/test1/*\",\n            [\n                \"test0/test1/test0.json\",\n                \"test0/test1/test0.yaml\",\n                \"test0/test1/test2\",\n                \"test0/test2/test1/test0.json\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test0/test2/test1/test3\",\n                \"test1/test0.json\",\n                \"test1/test0.yaml\",\n                \"test1/test0\",\n            ],\n        ),\n        (\n            \"**/test1/*.yaml\",\n            [\n                \"test0/test1/test0.yaml\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test1/test0.yaml\",\n            ],\n        ),\n        (\"*/test1/*/\", [\"test0/test1/test2\"]),\n        (\n            \"**/test1/*/\",\n            [\"test0/test1/test2\", \"test0/test2/test1/test3\", \"test1/test0\"],\n        ),\n        (\n            \"*/test1/**\",\n            [\n                \"test0/test1\",\n                \"test0/test1/test0.json\",\n                \"test0/test1/test0.yaml\",\n                \"test0/test1/test2\",\n                \"test0/test1/test2/test0.json\",\n                \"test0/test1/test2/test0.yaml\",\n            ],\n        ),\n        (\n            \"**/test1/**\",\n            [\n                \"test0/test1\",\n                \"test0/test1/test0.json\",\n                \"test0/test1/test0.yaml\",\n                \"test0/test1/test2\",\n                \"test0/test1/test2/test0.json\",\n                \"test0/test1/test2/test0.yaml\",\n                \"test0/test2/test1\",\n                \"test0/test2/test1/test0.json\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test0/test2/test1/test3\",\n                \"test0/test2/test1/test3/test0.json\",\n                \"test0/test2/test1/test3/test0.yaml\",\n                \"test1\",\n                \"test1/test0.json\",\n                \"test1/test0.yaml\",\n                \"test1/test0\",\n                \"test1/test0/test0.json\",\n                \"test1/test0/test0.yaml\",\n            ],\n        ),\n        (\"*/test1/**/\", [\"test0/test1\", \"test0/test1/test2\"]),\n        (\n            \"**/test1/**/\",\n            [\n                \"test0/test1\",\n                \"test0/test1/test2\",\n                \"test0/test2/test1\",\n                \"test0/test2/test1/test3\",\n                \"test1\",\n                \"test1/test0\",\n            ],\n        ),\n        (\n            \"test0/*\",\n            [\"test0/test0.json\", \"test0/test0.yaml\", \"test0/test1\", \"test0/test2\"],\n        ),\n        (\"test0/*.yaml\", [\"test0/test0.yaml\"]),\n        (\n            \"test0/**\",\n            [\n                \"test0\",\n                \"test0/test0.json\",\n                \"test0/test0.yaml\",\n                \"test0/test1\",\n                \"test0/test1/test0.json\",\n                \"test0/test1/test0.yaml\",\n                \"test0/test1/test2\",\n                \"test0/test1/test2/test0.json\",\n                \"test0/test1/test2/test0.yaml\",\n                \"test0/test2\",\n                \"test0/test2/test0.json\",\n                \"test0/test2/test0.yaml\",\n                \"test0/test2/test1\",\n                \"test0/test2/test1/test0.json\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test0/test2/test1/test3\",\n                \"test0/test2/test1/test3/test0.json\",\n                \"test0/test2/test1/test3/test0.yaml\",\n            ],\n        ),\n        (\"test0/*/\", [\"test0/test1\", \"test0/test2\"]),\n        (\n            \"test0/**/\",\n            [\n                \"test0\",\n                \"test0/test1\",\n                \"test0/test1/test2\",\n                \"test0/test2\",\n                \"test0/test2/test1\",\n                \"test0/test2/test1/test3\",\n            ],\n        ),\n        (\"test0/*/*.yaml\", [\"test0/test1/test0.yaml\", \"test0/test2/test0.yaml\"]),\n        (\n            \"test0/**/*.yaml\",\n            [\n                \"test0/test0.yaml\",\n                \"test0/test1/test0.yaml\",\n                \"test0/test1/test2/test0.yaml\",\n                \"test0/test2/test0.yaml\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test0/test2/test1/test3/test0.yaml\",\n            ],\n        ),\n        (\n            \"test0/*/test1/*\",\n            [\n                \"test0/test2/test1/test0.json\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test0/test2/test1/test3\",\n            ],\n        ),\n        (\"test0/*/test1/*.yaml\", [\"test0/test2/test1/test0.yaml\"]),\n        (\n            \"test0/**/test1/*\",\n            [\n                \"test0/test1/test0.json\",\n                \"test0/test1/test0.yaml\",\n                \"test0/test1/test2\",\n                \"test0/test2/test1/test0.json\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test0/test2/test1/test3\",\n            ],\n        ),\n        (\n            \"test0/**/test1/*.yaml\",\n            [\"test0/test1/test0.yaml\", \"test0/test2/test1/test0.yaml\"],\n        ),\n        (\"test0/*/test1/*/\", [\"test0/test2/test1/test3\"]),\n        (\"test0/**/test1/*/\", [\"test0/test1/test2\", \"test0/test2/test1/test3\"]),\n        (\n            \"test0/*/test1/**\",\n            [\n                \"test0/test2/test1\",\n                \"test0/test2/test1/test0.json\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test0/test2/test1/test3\",\n                \"test0/test2/test1/test3/test0.json\",\n                \"test0/test2/test1/test3/test0.yaml\",\n            ],\n        ),\n        (\n            \"test0/**/test1/**\",\n            [\n                \"test0/test1\",\n                \"test0/test1/test0.json\",\n                \"test0/test1/test0.yaml\",\n                \"test0/test1/test2\",\n                \"test0/test1/test2/test0.json\",\n                \"test0/test1/test2/test0.yaml\",\n                \"test0/test2/test1\",\n                \"test0/test2/test1/test0.json\",\n                \"test0/test2/test1/test0.yaml\",\n                \"test0/test2/test1/test3\",\n                \"test0/test2/test1/test3/test0.json\",\n                \"test0/test2/test1/test3/test0.yaml\",\n            ],\n        ),\n        (\"test0/*/test1/**/\", [\"test0/test2/test1\", \"test0/test2/test1/test3\"]),\n        (\n            \"test0/**/test1/**/\",\n            [\n                \"test0/test1\",\n                \"test0/test1/test2\",\n                \"test0/test2/test1\",\n                \"test0/test2/test1/test3\",\n            ],\n        ),\n    ],\n}\n\n\nclass DummyTestFS(AbstractFileSystem):\n    protocol = \"mock\"\n    _file_class = AbstractBufferedFile\n    _fs_contents = (\n        {\"name\": \"top_level\", \"type\": \"directory\"},\n        {\"name\": \"top_level/second_level\", \"type\": \"directory\"},\n        {\"name\": \"top_level/second_level/date=2019-10-01\", \"type\": \"directory\"},\n        {\n            \"name\": \"top_level/second_level/date=2019-10-01/a.parquet\",\n            \"type\": \"file\",\n            \"size\": 100,\n        },\n        {\n            \"name\": \"top_level/second_level/date=2019-10-01/b.parquet\",\n            \"type\": \"file\",\n            \"size\": 100,\n        },\n        {\"name\": \"top_level/second_level/date=2019-10-02\", \"type\": \"directory\"},\n        {\n            \"name\": \"top_level/second_level/date=2019-10-02/a.parquet\",\n            \"type\": \"file\",\n            \"size\": 100,\n        },\n        {\"name\": \"top_level/second_level/date=2019-10-04\", \"type\": \"directory\"},\n        {\n            \"name\": \"top_level/second_level/date=2019-10-04/a.parquet\",\n            \"type\": \"file\",\n            \"size\": 100,\n        },\n        {\"name\": \"misc\", \"type\": \"directory\"},\n        {\"name\": \"misc/foo.txt\", \"type\": \"file\", \"size\": 100},\n    )\n\n    def __init__(self, fs_content=None, **kwargs):\n        if fs_content is not None:\n            self._fs_contents = fs_content\n        super().__init__(**kwargs)\n\n    def __getitem__(self, name):\n        for item in self._fs_contents:\n            if item[\"name\"] == name:\n                return item\n        raise IndexError(f\"{name} not found!\")\n\n    def ls(self, path, detail=True, refresh=True, **kwargs):\n        if kwargs.pop(\"strip_proto\", True):\n            path = self._strip_protocol(path)\n\n        files = not refresh and self._ls_from_cache(path)\n        if not files:\n            files = [\n                file for file in self._fs_contents if path == self._parent(file[\"name\"])\n            ]\n            files.sort(key=lambda file: file[\"name\"])\n            self.dircache[path.rstrip(\"/\")] = files\n\n        if detail:\n            return files\n        return [file[\"name\"] for file in files]\n\n    @classmethod\n    def get_test_paths(cls, start_with=\"\"):\n        \"\"\"Helper to return directory and file paths with no details\"\"\"\n        all = [\n            file[\"name\"]\n            for file in cls._fs_contents\n            if file[\"name\"].startswith(start_with)\n        ]\n        return all\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        **kwargs,\n    ):\n        return self._file_class(\n            self,\n            path,\n            mode,\n            block_size,\n            autocommit,\n            cache_options=cache_options,\n            **kwargs,\n        )\n\n\n@pytest.mark.parametrize(\n    [\"test_paths\", \"recursive\", \"maxdepth\", \"expected\"],\n    [\n        (\n            (\n                \"top_level/second_level\",\n                \"top_level/sec*\",\n                \"top_level/sec*vel\",\n                \"top_level/*\",\n            ),\n            True,\n            None,\n            [\n                \"top_level/second_level\",\n                \"top_level/second_level/date=2019-10-01\",\n                \"top_level/second_level/date=2019-10-01/a.parquet\",\n                \"top_level/second_level/date=2019-10-01/b.parquet\",\n                \"top_level/second_level/date=2019-10-02\",\n                \"top_level/second_level/date=2019-10-02/a.parquet\",\n                \"top_level/second_level/date=2019-10-04\",\n                \"top_level/second_level/date=2019-10-04/a.parquet\",\n            ],\n        ),\n        (\n            (\n                \"top_level/second_level\",\n                \"top_level/sec*\",\n                \"top_level/sec*vel\",\n                \"top_level/*\",\n            ),\n            False,\n            None,\n            [\n                \"top_level/second_level\",\n            ],\n        ),\n        (\n            (\"top_level/second_level\",),\n            True,\n            1,\n            [\n                \"top_level/second_level\",\n                \"top_level/second_level/date=2019-10-01\",\n                \"top_level/second_level/date=2019-10-02\",\n                \"top_level/second_level/date=2019-10-04\",\n            ],\n        ),\n        (\n            (\"top_level/second_level\",),\n            True,\n            2,\n            [\n                \"top_level/second_level\",\n                \"top_level/second_level/date=2019-10-01\",\n                \"top_level/second_level/date=2019-10-01/a.parquet\",\n                \"top_level/second_level/date=2019-10-01/b.parquet\",\n                \"top_level/second_level/date=2019-10-02\",\n                \"top_level/second_level/date=2019-10-02/a.parquet\",\n                \"top_level/second_level/date=2019-10-04\",\n                \"top_level/second_level/date=2019-10-04/a.parquet\",\n            ],\n        ),\n        (\n            (\"top_level/*\", \"top_level/sec*\", \"top_level/sec*vel\", \"top_level/*\"),\n            True,\n            1,\n            [\"top_level/second_level\"],\n        ),\n        (\n            (\"top_level/*\", \"top_level/sec*\", \"top_level/sec*vel\", \"top_level/*\"),\n            True,\n            2,\n            [\n                \"top_level/second_level\",\n                \"top_level/second_level/date=2019-10-01\",\n                \"top_level/second_level/date=2019-10-02\",\n                \"top_level/second_level/date=2019-10-04\",\n            ],\n        ),\n        (\n            (\"top_level/**\",),\n            False,\n            None,\n            [\n                \"top_level\",\n                \"top_level/second_level\",\n                \"top_level/second_level/date=2019-10-01\",\n                \"top_level/second_level/date=2019-10-01/a.parquet\",\n                \"top_level/second_level/date=2019-10-01/b.parquet\",\n                \"top_level/second_level/date=2019-10-02\",\n                \"top_level/second_level/date=2019-10-02/a.parquet\",\n                \"top_level/second_level/date=2019-10-04\",\n                \"top_level/second_level/date=2019-10-04/a.parquet\",\n            ],\n        ),\n        (\n            (\"top_level/**\",),\n            True,\n            None,\n            [\n                \"top_level\",\n                \"top_level/second_level\",\n                \"top_level/second_level/date=2019-10-01\",\n                \"top_level/second_level/date=2019-10-01/a.parquet\",\n                \"top_level/second_level/date=2019-10-01/b.parquet\",\n                \"top_level/second_level/date=2019-10-02\",\n                \"top_level/second_level/date=2019-10-02/a.parquet\",\n                \"top_level/second_level/date=2019-10-04\",\n                \"top_level/second_level/date=2019-10-04/a.parquet\",\n            ],\n        ),\n        ((\"top_level/**\",), True, 1, [\"top_level\", \"top_level/second_level\"]),\n        (\n            (\"top_level/**\",),\n            True,\n            2,\n            [\n                \"top_level\",\n                \"top_level/second_level\",\n                \"top_level/second_level/date=2019-10-01\",\n                \"top_level/second_level/date=2019-10-01/a.parquet\",\n                \"top_level/second_level/date=2019-10-01/b.parquet\",\n                \"top_level/second_level/date=2019-10-02\",\n                \"top_level/second_level/date=2019-10-02/a.parquet\",\n                \"top_level/second_level/date=2019-10-04\",\n                \"top_level/second_level/date=2019-10-04/a.parquet\",\n            ],\n        ),\n        (\n            (\"top_level/**/a.*\",),\n            False,\n            None,\n            [\n                \"top_level/second_level/date=2019-10-01/a.parquet\",\n                \"top_level/second_level/date=2019-10-02/a.parquet\",\n                \"top_level/second_level/date=2019-10-04/a.parquet\",\n            ],\n        ),\n        (\n            (\"top_level/**/a.*\",),\n            True,\n            None,\n            [\n                \"top_level/second_level/date=2019-10-01/a.parquet\",\n                \"top_level/second_level/date=2019-10-02/a.parquet\",\n                \"top_level/second_level/date=2019-10-04/a.parquet\",\n            ],\n        ),\n        (\n            (\"top_level/**/second_level/date=2019-10-02\",),\n            False,\n            2,\n            [\n                \"top_level/second_level/date=2019-10-02\",\n            ],\n        ),\n        (\n            (\"top_level/**/second_level/date=2019-10-02\",),\n            True,\n            2,\n            [\n                \"top_level/second_level/date=2019-10-02\",\n                \"top_level/second_level/date=2019-10-02/a.parquet\",\n            ],\n        ),\n        [(\"misc/foo.txt\", \"misc/*.txt\"), False, None, [\"misc/foo.txt\"]],\n        [(\"misc/foo.txt\", \"misc/*.txt\"), True, None, [\"misc/foo.txt\"]],\n        (\n            (\"\",),\n            False,\n            None,\n            [DummyTestFS.root_marker],\n        ),\n        (\n            (\"\",),\n            True,\n            None,\n            DummyTestFS.get_test_paths() + [DummyTestFS.root_marker],\n        ),\n        [\n            (Path(\"misc/foo.txt\"),),\n            False,\n            None,\n            [f\"misc{os.sep}foo.txt\"],\n        ],\n    ],\n)\ndef test_expand_path(test_paths, recursive, maxdepth, expected):\n    \"\"\"Test a number of paths and then their combination which should all yield\n    the same set of expanded paths\"\"\"\n    test_fs = DummyTestFS()\n\n    # test single query\n    for test_path in test_paths:\n        paths = test_fs.expand_path(test_path, recursive=recursive, maxdepth=maxdepth)\n        assert sorted(paths) == sorted(expected)\n\n    # test with all queries\n    paths = test_fs.expand_path(\n        list(test_paths), recursive=recursive, maxdepth=maxdepth\n    )\n    assert sorted(paths) == sorted(expected)\n\n\ndef test_expand_paths_with_wrong_args():\n    test_fs = DummyTestFS()\n\n    with pytest.raises(ValueError):\n        test_fs.expand_path(\"top_level\", recursive=True, maxdepth=0)\n    with pytest.raises(ValueError):\n        test_fs.expand_path(\"top_level\", maxdepth=0)\n    with pytest.raises(FileNotFoundError):\n        test_fs.expand_path(\"top_level/**/second_level/date=2019-10-02\", maxdepth=1)\n    with pytest.raises(FileNotFoundError):\n        test_fs.expand_path(\"nonexistent/*\")\n\n\n@pytest.mark.xfail\ndef test_find():\n    \"\"\"Test .find() method on debian server (ftp, https) with constant folder\"\"\"\n    filesystem, host, test_path = (\n        FTPFileSystem,\n        \"ftp.fau.de\",\n        \"ftp://ftp.fau.de/debian-cd/current/amd64/log/success\",\n    )\n    test_fs = filesystem(host)\n    filenames_ftp = test_fs.find(test_path)\n    assert filenames_ftp\n\n    filesystem, host, test_path = (\n        HTTPFileSystem,\n        \"https://ftp.fau.de\",\n        \"https://ftp.fau.de/debian-cd/current/amd64/log/success\",\n    )\n    test_fs = filesystem()\n    filenames_http = test_fs.find(test_path)\n    roots = [f.rsplit(\"/\", 1)[-1] for f in filenames_http]\n\n    assert all(f.rsplit(\"/\", 1)[-1] in roots for f in filenames_ftp)\n\n\ndef test_find_details():\n    test_fs = DummyTestFS()\n    filenames = test_fs.find(\"/\")\n    details = test_fs.find(\"/\", detail=True)\n    for filename in filenames:\n        assert details[filename] == test_fs.info(filename)\n\n\ndef test_find_file():\n    test_fs = DummyTestFS()\n\n    filename = \"misc/foo.txt\"\n    assert test_fs.find(filename) == [filename]\n    assert test_fs.find(filename, detail=True) == {filename: {}}\n\n\ndef test_cache():\n    fs = DummyTestFS()\n    fs2 = DummyTestFS()\n    assert fs is fs2\n\n    assert DummyTestFS.current() is fs\n    assert len(fs._cache) == 1\n    del fs2\n    assert len(fs._cache) == 1\n    del fs\n\n    # keeps and internal reference, doesn't get collected\n    assert len(DummyTestFS._cache) == 1\n\n    DummyTestFS.clear_instance_cache()\n    assert len(DummyTestFS._cache) == 0\n\n\ndef test_current():\n    fs = DummyTestFS()\n    fs2 = DummyTestFS(arg=1)\n\n    assert fs is not fs2\n    assert DummyTestFS.current() is fs2\n\n    DummyTestFS()\n    assert DummyTestFS.current() is fs\n\n\ndef test_alias():\n    with pytest.warns(FutureWarning, match=\"add_aliases\"):\n        DummyTestFS(add_aliases=True)\n\n\ndef test_add_docs_warns():\n    with pytest.warns(FutureWarning, match=\"add_docs\"):\n        AbstractFileSystem(add_docs=True)\n\n\ndef test_cache_options():\n    fs = DummyTestFS()\n    f = AbstractBufferedFile(fs, \"misc/foo.txt\", cache_type=\"bytes\")\n    assert f.cache.trim\n\n    # TODO: dummy buffered file\n    f = AbstractBufferedFile(\n        fs, \"misc/foo.txt\", cache_type=\"bytes\", cache_options={\"trim\": False}\n    )\n    assert f.cache.trim is False\n\n    f = fs.open(\"misc/foo.txt\", cache_type=\"bytes\", cache_options={\"trim\": False})\n    assert f.cache.trim is False\n\n\ndef test_trim_kwarg_warns():\n    fs = DummyTestFS()\n    with pytest.warns(FutureWarning, match=\"cache_options\"):\n        AbstractBufferedFile(fs, \"misc/foo.txt\", cache_type=\"bytes\", trim=False)\n\n\ndef tests_file_open_error(monkeypatch):\n    class InitiateError(ValueError): ...\n\n    class UploadError(ValueError): ...\n\n    class DummyBufferedFile(AbstractBufferedFile):\n        can_initiate = False\n\n        def _initiate_upload(self):\n            if not self.can_initiate:\n                raise InitiateError\n\n        def _upload_chunk(self, final=False):\n            raise UploadError\n\n    monkeypatch.setattr(DummyTestFS, \"_file_class\", DummyBufferedFile)\n\n    fs = DummyTestFS()\n    with pytest.raises(InitiateError):\n        with fs.open(\"misc/foo.txt\", \"wb\") as stream:\n            stream.write(b\"hello\" * stream.blocksize * 2)\n\n    with pytest.raises(UploadError):\n        with fs.open(\"misc/foo.txt\", \"wb\") as stream:\n            stream.can_initiate = True\n            stream.write(b\"hello\" * stream.blocksize * 2)\n\n\ndef test_eq():\n    fs = DummyTestFS()\n    result = fs == 1\n    assert result is False\n\n    f = AbstractBufferedFile(fs, \"misc/foo.txt\", cache_type=\"bytes\")\n    result = f == 1\n    assert result is False\n\n\ndef test_pickle_multiple():\n    a = DummyTestFS(1)\n    b = DummyTestFS(2, bar=1)\n\n    x = pickle.dumps(a)\n    y = pickle.dumps(b)\n\n    del a, b\n    DummyTestFS.clear_instance_cache()\n\n    result = pickle.loads(x)\n    assert result.storage_args == (1,)\n    assert result.storage_options == {}\n\n    result = pickle.loads(y)\n    assert result.storage_args == (2,)\n    assert result.storage_options == {\"bar\": 1}\n\n\ndef test_json():\n    a = DummyTestFS(1)\n    b = DummyTestFS(2, bar=1)\n\n    outa = a.to_json()\n    outb = b.to_json()\n\n    assert json.loads(outb)  # is valid JSON\n    assert a != b\n    assert \"bar\" in outb\n\n    assert DummyTestFS.from_json(outa) is a\n    assert DummyTestFS.from_json(outb) is b\n\n\ndef test_json_path_attr():\n    a = DummyTestFS(1)\n    b = DummyTestFS(2, bar=Path(\"baz\"))\n\n    outa = a.to_json()\n    outb = b.to_json()\n\n    assert json.loads(outb)  # is valid JSON\n    assert a != b\n    assert \"bar\" in outb\n\n    assert DummyTestFS.from_json(outa) is a\n    assert DummyTestFS.from_json(outb) is b\n\n\ndef test_json_fs_attr():\n    a = DummyTestFS(1)\n    b = DummyTestFS(2, bar=a)\n\n    outa = a.to_json()\n    outb = b.to_json()\n\n    assert json.loads(outb)  # is valid JSON\n    assert a != b\n    assert \"bar\" in outb\n\n    assert DummyTestFS.from_json(outa) is a\n    assert DummyTestFS.from_json(outb) is b\n\n\ndef test_dict():\n    a = DummyTestFS(1)\n    b = DummyTestFS(2, bar=1)\n\n    outa = a.to_dict()\n    outb = b.to_dict()\n\n    assert isinstance(outa, dict)\n    assert a != b\n    assert \"bar\" in outb\n\n    assert DummyTestFS.from_dict(outa) is a\n    assert DummyTestFS.from_dict(outb) is b\n\n\ndef test_dict_idempotent():\n    a = DummyTestFS(1)\n\n    outa = a.to_dict()\n\n    assert DummyTestFS.from_dict(outa) is a\n    assert DummyTestFS.from_dict(outa) is a\n\n\ndef test_serialize_no_password():\n    fs = DummyTestFS(1, password=\"admin\")\n\n    assert \"password\" not in fs.to_json(include_password=False)\n    assert \"password\" not in fs.to_dict(include_password=False)\n\n\ndef test_serialize_with_password():\n    fs = DummyTestFS(1, password=\"admin\")\n\n    assert \"password\" in fs.to_json(include_password=True)\n    assert \"password\" in fs.to_dict(include_password=True)\n\n\ndef test_from_dict_valid():\n    fs = DummyTestFS.from_dict({\"cls\": \"fsspec.tests.test_spec.DummyTestFS\"})\n    assert isinstance(fs, DummyTestFS)\n\n    fs = DummyTestFS.from_dict({\"cls\": \"fsspec.tests.test_spec.DummyTestFS\", \"bar\": 1})\n    assert fs.storage_options[\"bar\"] == 1\n\n    fs = DummyTestFS.from_dict({\"cls\": \"fsspec.implementations.local.LocalFileSystem\"})\n    assert isinstance(fs, LocalFileSystem)\n\n    fs = DummyTestFS.from_dict(\n        {\n            \"cls\": \"fsspec.implementations.local.LocalFileSystem\",\n            \"protocol\": \"local\",\n        }\n    )\n    assert isinstance(fs, LocalFileSystem)\n\n\ndef test_from_dict_invalid():\n    with pytest.raises(ValueError, match=\"Not a serialized AbstractFileSystem\"):\n        DummyTestFS.from_dict({})\n\n    with pytest.raises(ValueError, match=\"Not a serialized AbstractFileSystem\"):\n        DummyTestFS.from_dict({\"cls\": \"pathlib.Path\"})\n\n    with pytest.raises(ValueError, match=\"Not a serialized AbstractFileSystem\"):\n        DummyTestFS.from_dict({\"protocol\": \"local\"})  # cls must be present\n\n\ndef test_ls_from_cache():\n    fs = DummyTestFS()\n    uncached_results = fs.ls(\"top_level/second_level/\", refresh=True)\n\n    assert fs.ls(\"top_level/second_level/\", refresh=False) == uncached_results\n\n    # _strip_protocol removes everything by default though\n    # for the sake of testing the _ls_from_cache interface\n    # directly, we need run one time more without that call\n    # to actually verify that our stripping in the client\n    # function works.\n    assert (\n        fs.ls(\"top_level/second_level/\", refresh=False, strip_proto=False)\n        == uncached_results\n    )\n\n\n@pytest.mark.parametrize(\n    \"dt\",\n    [\n        np.int8,\n        np.int16,\n        np.int32,\n        np.int64,\n        np.uint8,\n        np.uint16,\n        np.uint32,\n        np.uint64,\n        np.float32,\n        np.float64,\n    ],\n)\ndef test_readinto_with_numpy(tmpdir, dt):\n    store_path = str(tmpdir / \"test_arr.npy\")\n    arr = np.arange(10, dtype=dt)\n    arr.tofile(store_path)\n\n    arr2 = np.empty_like(arr)\n    with fsspec.open(store_path, \"rb\") as f:\n        f.readinto(arr2)\n\n    assert np.array_equal(arr, arr2)\n\n\n@pytest.mark.parametrize(\n    \"dt\",\n    [\n        np.int8,\n        np.int16,\n        np.int32,\n        np.int64,\n        np.uint8,\n        np.uint16,\n        np.uint32,\n        np.uint64,\n        np.float32,\n        np.float64,\n    ],\n)\ndef test_readinto_with_multibyte(ftp_writable, tmpdir, dt):\n    host, port, user, pw = ftp_writable\n    ftp = FTPFileSystem(host=host, port=port, username=user, password=pw)\n\n    with ftp.open(\"/out\", \"wb\") as fp:\n        arr = np.arange(10, dtype=dt)\n        fp.write(arr.tobytes())\n\n    with ftp.open(\"/out\", \"rb\") as fp:\n        arr2 = np.empty_like(arr)\n        fp.readinto(arr2)\n\n    assert np.array_equal(arr, arr2)\n\n\nclass DummyOpenFS(DummyTestFS):\n    blocksize = 10\n\n    def _open(self, path, mode=\"rb\", **kwargs):\n        stream = open(path, mode)\n        stream.size = os.stat(path).st_size\n        return stream\n\n\nclass BasicCallback(fsspec.Callback):\n    def __init__(self, **kwargs):\n        self.events = []\n        super().__init__(**kwargs)\n\n    def set_size(self, size):\n        self.events.append((\"set_size\", size))\n\n    def relative_update(self, inc=1):\n        self.events.append((\"relative_update\", inc))\n\n\ndef imitate_transfer(size, chunk, *, file=True):\n    events = [(\"set_size\", size)]\n    events.extend((\"relative_update\", size // chunk) for _ in range(chunk))\n    if file:\n        # The reason that there is a relative_update(0) at the\n        # end is that, we don't have an early exit on the\n        # implementations of get_file/put_file so it needs to\n        # go through the callback to get catch by the while's\n        # condition and then it will stop the transfer.\n        events.append((\"relative_update\", 0))\n\n    return events\n\n\ndef get_files(tmpdir, amount=10):\n    src, dest, base = [], [], []\n    for index in range(amount):\n        src_path = tmpdir / f\"src_{index}.txt\"\n        src_path.write_text(\"x\" * 50, \"utf-8\")\n\n        src.append(str(src_path))\n        dest.append(str(tmpdir / f\"dst_{index}.txt\"))\n        base.append(str(tmpdir / f\"file_{index}.txt\"))\n    return src, dest, base\n\n\ndef test_dummy_callbacks_file(tmpdir):\n    fs = DummyOpenFS()\n    callback = BasicCallback()\n\n    file = tmpdir / \"file.txt\"\n    source = tmpdir / \"tmp.txt\"\n    destination = tmpdir / \"tmp2.txt\"\n\n    size = 100\n    source.write_text(\"x\" * 100, \"utf-8\")\n\n    fs.put_file(source, file, callback=callback)\n\n    # -1 here since put_file no longer has final zero-size put\n    assert callback.events == imitate_transfer(size, 10)[:-1]\n    callback.events.clear()\n\n    fs.get_file(file, destination, callback=callback)\n    assert callback.events == imitate_transfer(size, 10)\n    callback.events.clear()\n\n    assert destination.read_text(\"utf-8\") == \"x\" * 100\n\n\ndef test_dummy_callbacks_files(tmpdir):\n    fs = DummyOpenFS()\n    callback = BasicCallback()\n    src, dest, base = get_files(tmpdir)\n\n    fs.put(src, base, callback=callback)\n    assert callback.events == imitate_transfer(10, 10, file=False)\n    callback.events.clear()\n\n    fs.get(base, dest, callback=callback)\n    assert callback.events == imitate_transfer(10, 10, file=False)\n\n\nclass BranchableCallback(BasicCallback):\n    def __init__(self, source, dest=None, events=None, **kwargs):\n        super().__init__(**kwargs)\n        if dest:\n            self.key = source, dest\n        else:\n            self.key = (source,)\n        self.events = events or defaultdict(list)\n\n    def branch(self, path_1, path_2, kwargs):\n        from fsspec.implementations.local import make_path_posix\n\n        path_1 = make_path_posix(path_1)\n        path_2 = make_path_posix(path_2)\n        kwargs[\"callback\"] = BranchableCallback(path_1, path_2, events=self.events)\n\n    def set_size(self, size):\n        self.events[self.key].append((\"set_size\", size))\n\n    def relative_update(self, inc=1):\n        self.events[self.key].append((\"relative_update\", inc))\n\n\ndef test_dummy_callbacks_files_branched(tmpdir):\n    fs = DummyOpenFS()\n    src, dest, base = get_files(tmpdir)\n\n    callback = BranchableCallback(\"top-level\")\n\n    def check_events(lpaths, rpaths):\n        from fsspec.implementations.local import make_path_posix\n\n        base_keys = zip(make_path_posix(lpaths), make_path_posix(rpaths))\n        assert set(callback.events.keys()) == {(\"top-level\",), *base_keys}\n        assert callback.events[\"top-level\",] == imitate_transfer(10, 10, file=False)\n\n        for key in base_keys:\n            assert callback.events[key] == imitate_transfer(50, 5)\n\n    fs.put(src, base, callback=callback)\n    check_events(src, base)\n    callback.events.clear()\n\n    fs.get(base, dest, callback=callback)\n    check_events(base, dest)\n    callback.events.clear()\n\n\ndef _clean_paths(paths, prefix=\"\"):\n    \"\"\"\n    Helper to cleanup paths results by doing the following:\n      - remove the prefix provided from all paths\n      - remove the trailing slashes from all paths\n      - remove duplicates paths\n      - sort all paths\n    \"\"\"\n    paths_list = paths\n    if isinstance(paths, dict):\n        paths_list = list(paths)\n    paths_list = [p.replace(prefix, \"\").strip(\"/\") for p in sorted(set(paths_list))]\n    if isinstance(paths, dict):\n        return {p: paths[p] for p in paths_list}\n    return paths_list\n\n\n@pytest.fixture(scope=\"function\")\ndef glob_fs():\n    return DummyTestFS(fs_content=PATHS_FOR_GLOB_TESTS)\n\n\n@pytest.fixture(scope=\"function\")\ndef glob_files_folder(tmp_path):\n    local_fs = LocalFileSystem(auto_mkdir=True)\n    local_fake_dir = str(tmp_path)\n    for path_info in PATHS_FOR_GLOB_TESTS:\n        if path_info[\"type\"] == \"file\":\n            local_fs.touch(path=f\"{str(tmp_path)}/{path_info['name']}\")\n    return local_fake_dir\n\n\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\"),\n    reason=\"no need to run python glob posix tests on windows\",\n)\n@pytest.mark.parametrize(\n    GLOB_POSIX_TESTS[\"argnames\"],\n    GLOB_POSIX_TESTS[\"argvalues\"],\n)\ndef test_posix_tests_python_glob(path, expected, glob_files_folder):\n    \"\"\"\n    Tests against python glob to check if our posix tests are accurate.\n    \"\"\"\n    os.chdir(glob_files_folder)\n\n    python_output = glob.glob(pathname=path, recursive=True)\n    assert _clean_paths(python_output, glob_files_folder) == _clean_paths(expected)\n\n\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\"),\n    reason=\"no need to run bash stat posix tests on windows\",\n)\n@pytest.mark.parametrize(\n    GLOB_POSIX_TESTS[\"argnames\"],\n    GLOB_POSIX_TESTS[\"argvalues\"],\n)\ndef test_posix_tests_bash_stat(path, expected, glob_files_folder):\n    \"\"\"\n    Tests against bash stat to check if our posix tests are accurate.\n    \"\"\"\n    try:\n        subprocess.check_output([\"bash\", \"-c\", \"shopt -s globstar\"])\n    except FileNotFoundError:\n        pytest.skip(\"bash is not available\")\n    except subprocess.CalledProcessError:\n        pytest.skip(\"globstar option is not available\")\n\n    bash_path = (\n        path.replace(\"\\\\\", \"\\\\\\\\\")\n        .replace(\"$\", \"\\\\$\")\n        .replace(\"(\", \"\\\\(\")\n        .replace(\")\", \"\\\\)\")\n        .replace(\"|\", \"\\\\|\")\n    )\n    bash_output = subprocess.run(\n        [\n            \"bash\",\n            \"-c\",\n            f\"cd {glob_files_folder} && shopt -s globstar && stat -c %N {bash_path}\",\n        ],\n        capture_output=True,\n        check=False,\n    )\n    # Remove the last element always empty\n    bash_output = bash_output.stdout.decode(\"utf-8\").replace(\"'\", \"\").split(\"\\n\")[:-1]\n    assert _clean_paths(bash_output, glob_files_folder) == _clean_paths(expected)\n\n\n@pytest.mark.parametrize(\n    GLOB_POSIX_TESTS[\"argnames\"],\n    GLOB_POSIX_TESTS[\"argvalues\"],\n)\ndef test_glob_posix_rules(path, expected, glob_fs):\n    output = glob_fs.glob(path=f\"mock://{path}\")\n    assert _clean_paths(output) == _clean_paths(expected)\n\n    detailed_output = glob_fs.glob(path=f\"mock://{path}\", detail=True)\n    for name, info in _clean_paths(detailed_output).items():\n        assert info == glob_fs[name]\n", "fsspec/tests/test_downstream.py": "import pytest\n\npytest.importorskip(\"s3fs\")\npytest.importorskip(\"moto\")\n\ntry:\n    from s3fs.tests.test_s3fs import (  # noqa: E402,F401\n        endpoint_uri,\n        s3,\n        s3_base,\n        test_bucket_name,\n    )\nexcept ImportError:\n    pytest.skip(\"s3 tests not available.\")\n\nso = {\"anon\": False, \"client_kwargs\": {\"endpoint_url\": endpoint_uri}}\n\n\ndef test_pandas(s3):\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n    df.to_csv(f\"s3://{test_bucket_name}/a.csv\", storage_options=so)\n    df2 = pd.read_csv(f\"s3://{test_bucket_name}/a.csv\", storage_options=so)\n\n    assert df.a.equals(df2.a)\n\n\ndef test_xarray_zarr(s3):\n    xr = pytest.importorskip(\"xarray\")\n    pytest.importorskip(\"zarr\")\n    import numpy as np\n\n    x = np.arange(5)\n    xarr = xr.DataArray(x)\n    ds = xr.Dataset({\"x\": xarr})\n    ds.to_zarr(f\"s3://{test_bucket_name}/a.zarr\", storage_options=so)\n\n    ds2 = xr.open_zarr(f\"s3://{test_bucket_name}/a.zarr\", storage_options=so)\n\n    assert (ds.x == ds2.x).all()\n", "fsspec/tests/test_generic.py": "import pytest\n\nimport fsspec\nfrom fsspec.tests.conftest import data, server  # noqa: F401\n\n\ndef test_remote_async_ops(server):\n    fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true\"})\n    fs = fsspec.filesystem(\"generic\", default_method=\"current\")\n    out = fs.info(server + \"/index/realfile\")\n    assert out[\"size\"] == len(data)\n    assert out[\"type\"] == \"file\"\n    assert fs.isfile(server + \"/index/realfile\")  # this method from superclass\n\n\ndef test_touch_rm(m):\n    m.touch(\"afile\")\n    m.touch(\"dir/afile\")\n\n    fs = fsspec.filesystem(\"generic\", default_method=\"current\")\n    fs.rm(\"memory://afile\")\n    assert not m.exists(\"afile\")\n\n    fs.rm(\"memory://dir\", recursive=True)\n    assert not m.exists(\"dir/afile\")\n    assert not m.exists(\"dir\")\n\n\ndef test_cp_async_to_sync(server, m):\n    fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true\"})\n    fs = fsspec.filesystem(\"generic\", default_method=\"current\")\n    fs.cp([server + \"/index/realfile\"], [\"memory://realfile\"])\n    assert m.cat(\"realfile\") == data\n\n    fs.rm(\"memory://realfile\")\n    assert not m.exists(\"realfile\")\n\n\ndef test_pipe_cat_sync(m):\n    fs = fsspec.filesystem(\"generic\", default_method=\"current\")\n    fs.pipe(\"memory://afile\", b\"data\")\n    assert fs.cat(\"memory://afile\") == b\"data\"\n\n\ndef test_cat_async(server):\n    fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true\"})\n    fs = fsspec.filesystem(\"generic\", default_method=\"current\")\n    assert fs.cat(server + \"/index/realfile\") == data\n\n\ndef test_rsync(tmpdir, m):\n    from fsspec.generic import GenericFileSystem, rsync\n\n    fs = GenericFileSystem()\n    fs.pipe(\"memory:///deep/path/afile\", b\"data1\")\n    fs.pipe(\"memory:///deep/afile\", b\"data2\")\n\n    with pytest.raises(ValueError):\n        rsync(\"memory:///deep/afile\", f\"file://{tmpdir}\")\n    rsync(\"memory://\", f\"file://{tmpdir}\")\n\n    allfiles = fs.find(f\"file://{tmpdir}\", withdirs=True, detail=True)\n    pos_tmpdir = fsspec.implementations.local.make_path_posix(str(tmpdir))  # for WIN\n    assert set(allfiles) == {\n        f\"file://{pos_tmpdir}{_}\"\n        for _ in [\n            \"\",\n            \"/deep\",\n            \"/deep/path\",\n            \"/deep/path/afile\",\n            \"/deep/afile\",\n        ]\n    }\n    fs.rm(\"memory:///deep/afile\")\n    rsync(\"memory://\", f\"file://{tmpdir}\", delete_missing=True)\n    allfiles2 = fs.find(f\"file://{tmpdir}\", withdirs=True, detail=True)\n    assert set(allfiles2) == {\n        f\"file://{pos_tmpdir}{_}\"\n        for _ in [\n            \"\",\n            \"/deep\",\n            \"/deep/path\",\n            \"/deep/path/afile\",\n        ]\n    }\n    # the file was not updated, since size was correct\n    assert (\n        allfiles[f\"file://{pos_tmpdir}/deep/path/afile\"]\n        == allfiles2[f\"file://{pos_tmpdir}/deep/path/afile\"]\n    )\n", "fsspec/tests/test_fuse.py": "import os\nimport subprocess\nimport time\nfrom multiprocessing import Process\n\nimport pytest\n\ntry:\n    pytest.importorskip(\"fuse\")  # noqa: E402\nexcept OSError:\n    # can succeed in importing fuse, but fail to load so\n    pytest.importorskip(\"nonexistent\")  # noqa: E402\n\nfrom fsspec.fuse import main, run\nfrom fsspec.implementations.memory import MemoryFileSystem\n\n\ndef host_fuse(mountdir):\n    fs = MemoryFileSystem()\n    fs.touch(\"/mounted/testfile\")\n    run(fs, \"/mounted/\", mountdir)\n\n\ndef test_basic(tmpdir, capfd):\n    mountdir = str(tmpdir.mkdir(\"mount\"))\n\n    fuse_process = Process(target=host_fuse, args=(str(mountdir),))\n    fuse_process.start()\n\n    try:\n        timeout = 10\n        while True:\n            try:\n                # can fail with device not ready while waiting for fuse\n                if \"testfile\" in os.listdir(mountdir):\n                    break\n            except Exception:\n                pass\n            timeout -= 1\n            time.sleep(1)\n            if not timeout > 0:\n                import pdb\n\n                pdb.set_trace()\n                pytest.skip(msg=\"fuse didn't come live\")\n\n        fn = os.path.join(mountdir, \"test\")\n        with open(fn, \"wb\") as f:\n            f.write(b\"data\")\n\n        with open(fn) as f:\n            assert f.read() == \"data\"\n\n        os.remove(fn)\n\n        os.mkdir(fn)\n        assert os.listdir(fn) == []\n\n        os.mkdir(fn + \"/inner\")\n\n        with pytest.raises(OSError):\n            os.rmdir(fn)\n\n        captured = capfd.readouterr()\n        assert \"Traceback\" not in captured.out\n        assert \"Traceback\" not in captured.err\n\n        os.rmdir(fn + \"/inner\")\n        os.rmdir(fn)\n    finally:\n        fuse_process.terminate()\n        fuse_process.join(timeout=10)\n        if fuse_process.is_alive():\n            fuse_process.kill()\n            fuse_process.join()\n\n\ndef host_mount_local(source_dir, mount_dir, debug_log):\n    main([\"local\", source_dir, mount_dir, \"-l\", debug_log, \"--ready-file\"])\n\n\n@pytest.fixture()\ndef mount_local(tmpdir):\n    source_dir = tmpdir.mkdir(\"source\")\n    mount_dir = tmpdir.mkdir(\"local\")\n    debug_log = tmpdir / \"debug.log\"\n    fuse_process = Process(\n        target=host_mount_local, args=(str(source_dir), str(mount_dir), str(debug_log))\n    )\n    fuse_process.start()\n    ready_file = mount_dir / \".fuse_ready\"\n    for _ in range(20):\n        if ready_file.exists() and open(ready_file).read() == b\"ready\":\n            break\n        time.sleep(0.1)\n    try:\n        yield (source_dir, mount_dir)\n    finally:\n        fuse_process.terminate()\n        fuse_process.join(timeout=10)\n        if fuse_process.is_alive():\n            fuse_process.kill()\n            fuse_process.join()\n\n\ndef test_mount(mount_local):\n    source_dir, mount_dir = mount_local\n    assert os.listdir(mount_dir) == []\n    assert os.listdir(source_dir) == []\n\n    mount_dir.mkdir(\"a\")\n\n    assert os.listdir(mount_dir) == [\"a\"]\n    assert os.listdir(source_dir) == [\"a\"]\n\n\ndef test_chmod(mount_local):\n    source_dir, mount_dir = mount_local\n    open(mount_dir / \"text\", \"w\").write(\"test\")\n    assert os.listdir(source_dir) == [\"text\"]\n\n    cp = subprocess.run(\n        [\"cp\", str(mount_dir / \"text\"), str(mount_dir / \"new\")],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        check=False,\n    )\n\n    assert cp.stderr == b\"\"\n    assert cp.stdout == b\"\"\n    assert set(os.listdir(source_dir)) == {\"text\", \"new\"}\n    assert open(mount_dir / \"new\").read() == \"test\"\n\n\ndef test_seek_rw(mount_local):\n    source_dir, mount_dir = mount_local\n    fh = open(mount_dir / \"text\", \"w\")\n    fh.write(\"teST\")\n    fh.seek(2)\n    fh.write(\"st\")\n    fh.close()\n\n    fh = open(mount_dir / \"text\", \"r\")\n    assert fh.read() == \"test\"\n    fh.seek(2)\n    assert fh.read() == \"st\"\n    fh.close()\n", "fsspec/tests/test_async.py": "import asyncio\nimport inspect\nimport io\nimport os\nimport time\n\nimport pytest\n\nimport fsspec\nimport fsspec.asyn\nfrom fsspec.asyn import _run_coros_in_chunks\n\n\ndef test_sync_methods():\n    inst = fsspec.asyn.AsyncFileSystem()\n    assert inspect.iscoroutinefunction(inst._info)\n    assert hasattr(inst, \"info\")\n    assert inst.info.__qualname__ == \"AsyncFileSystem._info\"\n    assert not inspect.iscoroutinefunction(inst.info)\n\n\ndef test_when_sync_methods_are_disabled():\n    class TestFS(fsspec.asyn.AsyncFileSystem):\n        mirror_sync_methods = False\n\n    inst = TestFS()\n    assert inspect.iscoroutinefunction(inst._info)\n    assert not inspect.iscoroutinefunction(inst.info)\n    assert inst.info.__qualname__ == \"AbstractFileSystem.info\"\n\n\ndef test_interrupt():\n    loop = fsspec.asyn.get_loop()\n\n    async def f():\n        await asyncio.sleep(1000000)\n        return True\n\n    fut = asyncio.run_coroutine_threadsafe(f(), loop)\n    time.sleep(0.01)  # task launches\n    out = fsspec.asyn._dump_running_tasks(with_task=True)\n    task = out[0][\"task\"]\n    assert task.done() and fut.done()\n    assert isinstance(fut.exception(), fsspec.asyn.FSSpecCoroutineCancel)\n\n\nclass _DummyAsyncKlass:\n    def __init__(self):\n        self.loop = fsspec.asyn.get_loop()\n\n    async def _dummy_async_func(self):\n        # Sleep 1 second function to test timeout\n        await asyncio.sleep(1)\n        return True\n\n    async def _bad_multiple_sync(self):\n        fsspec.asyn.sync_wrapper(_DummyAsyncKlass._dummy_async_func)(self)\n        return True\n\n    dummy_func = fsspec.asyn.sync_wrapper(_dummy_async_func)\n    bad_multiple_sync_func = fsspec.asyn.sync_wrapper(_bad_multiple_sync)\n\n\ndef test_sync_wrapper_timeout_on_less_than_expected_wait_time_not_finish_function():\n    test_obj = _DummyAsyncKlass()\n    with pytest.raises(fsspec.FSTimeoutError):\n        test_obj.dummy_func(timeout=0.1)\n\n\ndef test_sync_wrapper_timeout_on_more_than_expected_wait_time_will_finish_function():\n    test_obj = _DummyAsyncKlass()\n    assert test_obj.dummy_func(timeout=5)\n\n\ndef test_sync_wrapper_timeout_none_will_wait_func_finished():\n    test_obj = _DummyAsyncKlass()\n    assert test_obj.dummy_func(timeout=None)\n\n\ndef test_sync_wrapper_treat_timeout_0_as_none():\n    test_obj = _DummyAsyncKlass()\n    assert test_obj.dummy_func(timeout=0)\n\n\ndef test_sync_wrapper_bad_multiple_sync():\n    test_obj = _DummyAsyncKlass()\n    with pytest.raises(NotImplementedError):\n        test_obj.bad_multiple_sync_func(timeout=5)\n\n\ndef test_run_coros_in_chunks(monkeypatch):\n    total_running = 0\n\n    async def runner():\n        nonlocal total_running\n\n        total_running += 1\n        await asyncio.sleep(0)\n        if total_running > 4:\n            raise ValueError(\"More than 4 coroutines are running together\")\n        total_running -= 1\n        return 1\n\n    async def main(**kwargs):\n        nonlocal total_running\n\n        total_running = 0\n        coros = [runner() for _ in range(32)]\n        results = await _run_coros_in_chunks(coros, **kwargs)\n        for result in results:\n            if isinstance(result, Exception):\n                raise result\n        return results\n\n    assert sum(asyncio.run(main(batch_size=4))) == 32\n\n    with pytest.raises(ValueError):\n        asyncio.run(main(batch_size=5))\n\n    with pytest.raises(ValueError):\n        asyncio.run(main(batch_size=-1))\n\n    assert sum(asyncio.run(main(batch_size=4))) == 32\n\n    monkeypatch.setitem(fsspec.config.conf, \"gather_batch_size\", 5)\n    with pytest.raises(ValueError):\n        asyncio.run(main())\n    assert sum(asyncio.run(main(batch_size=4))) == 32  # override\n\n    monkeypatch.setitem(fsspec.config.conf, \"gather_batch_size\", 4)\n    assert sum(asyncio.run(main())) == 32  # override\n\n\n@pytest.mark.skipif(os.name != \"nt\", reason=\"only for windows\")\ndef test_windows_policy():\n    from asyncio.windows_events import SelectorEventLoop\n\n    loop = fsspec.asyn.get_loop()\n    policy = asyncio.get_event_loop_policy()\n\n    # Ensure that the created loop always uses selector policy\n    assert isinstance(loop, SelectorEventLoop)\n\n    # Ensure that the global policy is not changed and it is\n    # set to the default one. This is important since the\n    # get_loop() method will temporarily override the policy\n    # with the one which uses selectors on windows, so this\n    # check ensures that we are restoring the old policy back\n    # after our change.\n    assert isinstance(policy, asyncio.DefaultEventLoopPolicy)\n\n\ndef test_running_async():\n    assert not fsspec.asyn.running_async()\n\n    async def go():\n        assert fsspec.asyn.running_async()\n\n    asyncio.run(go())\n\n\nclass DummyAsyncFS(fsspec.asyn.AsyncFileSystem):\n    _file_class = fsspec.asyn.AbstractAsyncStreamedFile\n\n    async def _info(self, path, **kwargs):\n        return {\"name\": \"misc/foo.txt\", \"type\": \"file\", \"size\": 100}\n\n    async def open_async(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        **kwargs,\n    ):\n        return DummyAsyncStreamedFile(\n            self,\n            path,\n            mode,\n            block_size,\n            autocommit,\n            cache_options=cache_options,\n            **kwargs,\n        )\n\n\nclass DummyAsyncStreamedFile(fsspec.asyn.AbstractAsyncStreamedFile):\n    def __init__(self, fs, path, mode, block_size, autocommit, **kwargs):\n        super().__init__(fs, path, mode, block_size, autocommit, **kwargs)\n        self.temp_buffer = io.BytesIO(b\"foo-bar\" * 20)\n\n    async def _fetch_range(self, start, end):\n        return self.temp_buffer.read(end - start)\n\n    async def _initiate_upload(self):\n        # Reinitialize for new uploads.\n        self.temp_buffer = io.BytesIO()\n\n    async def _upload_chunk(self, final=False):\n        self.temp_buffer.write(self.buffer.getbuffer())\n\n    async def get_data(self):\n        return self.temp_buffer.getbuffer().tobytes()\n\n    async def get_data(self):\n        return self.temp_buffer.getbuffer().tobytes()\n\n\n@pytest.mark.asyncio\nasync def test_async_streamed_file_write():\n    test_fs = DummyAsyncFS()\n    streamed_file = await test_fs.open_async(\"misc/foo.txt\", mode=\"wb\")\n    inp_data = \"foo-bar\".encode(\"utf8\") * streamed_file.blocksize * 2\n    await streamed_file.write(inp_data)\n    assert streamed_file.loc == len(inp_data)\n    await streamed_file.close()\n    out_data = await streamed_file.get_data()\n    assert out_data.count(b\"foo-bar\") == streamed_file.blocksize * 2\n\n\n@pytest.mark.asyncio\nasync def test_async_streamed_file_read():\n    test_fs = DummyAsyncFS()\n    streamed_file = await test_fs.open_async(\"misc/foo.txt\", mode=\"rb\")\n    assert (\n        await streamed_file.read(7 * 3) + await streamed_file.read(7 * 18)\n        == b\"foo-bar\" * 20\n    )\n    await streamed_file.close()\n", "fsspec/tests/test_gui.py": "import pytest\n\npanel = pytest.importorskip(\"panel\")\n\n\ndef test_basic():\n    import fsspec.gui\n\n    gui = fsspec.gui.FileSelector()\n    assert \"url\" in str(gui.panel)\n\n\ndef test_kwargs(tmpdir):\n    \"\"\"confirm kwargs are passed to the filesystem instance\"\"\"\n    import fsspec.gui\n\n    gui = fsspec.gui.FileSelector(f\"file://{tmpdir}\", kwargs=\"{'auto_mkdir': True}\")\n\n    assert gui.fs.auto_mkdir\n\n    gui = fsspec.gui.FileSelector(f\"file://{tmpdir}\", kwargs={\"auto_mkdir\": True})\n\n    assert gui.fs.auto_mkdir\n", "fsspec/tests/conftest.py": "import contextlib\nimport gzip\nimport json\nimport os\nimport threading\nfrom collections import ChainMap\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\n\nimport pytest\n\nrequests = pytest.importorskip(\"requests\")\nport = 9898\ndata = b\"\\n\".join([b\"some test data\"] * 1000)\nrealfile = f\"http://127.0.0.1:{port}/index/realfile\"\nindex = b'<a href=\"%s\">Link</a>' % realfile.encode()\nlisting = open(\n    os.path.join(os.path.dirname(__file__), \"data\", \"listing.html\"), \"rb\"\n).read()\nwin = os.name == \"nt\"\n\n\ndef _make_listing(*paths):\n    return \"\\n\".join(\n        f'<a href=\"http://127.0.0.1:{port}{f}\">Link_{i}</a>'\n        for i, f in enumerate(paths)\n    ).encode()\n\n\n@pytest.fixture\ndef reset_files():\n    yield\n\n    # Reset the newly added files after the\n    # test is completed.\n    HTTPTestHandler.dynamic_files.clear()\n\n\nclass HTTPTestHandler(BaseHTTPRequestHandler):\n    static_files = {\n        \"/index/realfile\": data,\n        \"/index/otherfile\": data,\n        \"/index\": index,\n        \"/data/20020401\": listing,\n        \"/simple/\": _make_listing(\"/simple/file\", \"/simple/dir/\"),\n        \"/simple/file\": data,\n        \"/simple/dir/\": _make_listing(\"/simple/dir/file\"),\n        \"/simple/dir/file\": data,\n    }\n    dynamic_files = {}\n\n    files = ChainMap(dynamic_files, static_files)\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def _respond(self, code=200, headers=None, data=b\"\"):\n        headers = headers or {}\n        headers.update({\"User-Agent\": \"test\"})\n        self.send_response(code)\n        for k, v in headers.items():\n            self.send_header(k, str(v))\n        self.end_headers()\n        if data:\n            self.wfile.write(data)\n\n    def do_GET(self):\n        file_path = self.path\n        if file_path.endswith(\"/\") and file_path.rstrip(\"/\") in self.files:\n            file_path = file_path.rstrip(\"/\")\n        file_data = self.files.get(file_path)\n        if \"give_path\" in self.headers:\n            return self._respond(200, data=json.dumps({\"path\": self.path}).encode())\n        if \"redirect\" in self.headers and file_path != \"/index/realfile\":\n            new_url = f\"http://127.0.0.1:{port}/index/realfile\"\n            return self._respond(301, {\"Location\": new_url})\n        if file_data is None:\n            return self._respond(404)\n\n        status = 200\n        content_range = f\"bytes 0-{len(file_data) - 1}/{len(file_data)}\"\n        if (\"Range\" in self.headers) and (\"ignore_range\" not in self.headers):\n            ran = self.headers[\"Range\"]\n            b, ran = ran.split(\"=\")\n            start, end = ran.split(\"-\")\n            if start:\n                content_range = f\"bytes {start}-{end}/{len(file_data)}\"\n                file_data = file_data[int(start) : (int(end) + 1) if end else None]\n            else:\n                # suffix only\n                l = len(file_data)\n                content_range = f\"bytes {l - int(end)}-{l - 1}/{l}\"\n                file_data = file_data[-int(end) :]\n            if \"use_206\" in self.headers:\n                status = 206\n        if \"give_length\" in self.headers:\n            if \"gzip_encoding\" in self.headers:\n                file_data = gzip.compress(file_data)\n                response_headers = {\n                    \"Content-Length\": len(file_data),\n                    \"Content-Encoding\": \"gzip\",\n                }\n            else:\n                response_headers = {\"Content-Length\": len(file_data)}\n            self._respond(status, response_headers, file_data)\n        elif \"give_range\" in self.headers:\n            self._respond(status, {\"Content-Range\": content_range}, file_data)\n        elif \"give_mimetype\" in self.headers:\n            self._respond(\n                status, {\"Content-Type\": \"text/html; charset=utf-8\"}, file_data\n            )\n        else:\n            self._respond(status, data=file_data)\n\n    def do_POST(self):\n        length = self.headers.get(\"Content-Length\")\n        file_path = self.path.rstrip(\"/\")\n        if length is None:\n            assert self.headers.get(\"Transfer-Encoding\") == \"chunked\"\n            self.files[file_path] = b\"\".join(self.read_chunks())\n        else:\n            self.files[file_path] = self.rfile.read(length)\n        self._respond(200)\n\n    do_PUT = do_POST\n\n    def read_chunks(self):\n        length = -1\n        while length != 0:\n            line = self.rfile.readline().strip()\n            if len(line) == 0:\n                length = 0\n            else:\n                length = int(line, 16)\n            yield self.rfile.read(length)\n            self.rfile.readline()\n\n    def do_HEAD(self):\n        if \"head_not_auth\" in self.headers:\n            return self._respond(\n                403, {\"Content-Length\": 123}, b\"not authorized for HEAD request\"\n            )\n        elif \"head_ok\" not in self.headers:\n            return self._respond(405)\n\n        file_path = self.path.rstrip(\"/\")\n        file_data = self.files.get(file_path)\n        if file_data is None:\n            return self._respond(404)\n\n        if (\"give_length\" in self.headers) or (\"head_give_length\" in self.headers):\n            response_headers = {\"Content-Length\": len(file_data)}\n            if \"zero_length\" in self.headers:\n                response_headers[\"Content-Length\"] = 0\n            elif \"gzip_encoding\" in self.headers:\n                file_data = gzip.compress(file_data)\n                response_headers[\"Content-Encoding\"] = \"gzip\"\n                response_headers[\"Content-Length\"] = len(file_data)\n\n            self._respond(200, response_headers)\n        elif \"give_range\" in self.headers:\n            self._respond(\n                200, {\"Content-Range\": f\"0-{len(file_data) - 1}/{len(file_data)}\"}\n            )\n        elif \"give_etag\" in self.headers:\n            self._respond(200, {\"ETag\": \"xxx\"})\n        else:\n            self._respond(200)  # OK response, but no useful info\n\n\n@contextlib.contextmanager\ndef serve():\n    server_address = (\"\", port)\n    httpd = HTTPServer(server_address, HTTPTestHandler)\n    th = threading.Thread(target=httpd.serve_forever)\n    th.daemon = True\n    th.start()\n    try:\n        yield f\"http://127.0.0.1:{port}\"\n    finally:\n        httpd.socket.close()\n        httpd.shutdown()\n        th.join()\n\n\n@pytest.fixture(scope=\"module\")\ndef server():\n    with serve() as s:\n        yield s\n", "fsspec/tests/test_registry.py": "import sys\nfrom importlib.metadata import EntryPoint\nfrom unittest.mock import create_autospec, patch\n\nimport pytest\n\nimport fsspec\nfrom fsspec.implementations.zip import ZipFileSystem\nfrom fsspec.registry import (\n    _registry,\n    filesystem,\n    get_filesystem_class,\n    known_implementations,\n    register_implementation,\n    registry,\n)\nfrom fsspec.spec import AbstractFileSystem\n\n\n@pytest.fixture()\ndef clear_registry():\n    try:\n        yield\n    finally:\n        _registry.clear()\n        known_implementations.pop(\"test\", None)\n\n\n@pytest.fixture()\ndef clean_imports():\n    try:\n        real_module = sys.modules[\"fsspec\"]\n        del sys.modules[\"fsspec\"]\n        yield\n    finally:\n        sys.modules[\"fsspec\"] = real_module\n\n\ndef test_registry_readonly():\n    get_filesystem_class(\"file\")\n    assert \"file\" in registry\n    assert \"file\" in list(registry)\n    with pytest.raises(TypeError):\n        del registry[\"file\"]\n    with pytest.raises(TypeError):\n        registry[\"file\"] = None\n    with pytest.raises(AttributeError):\n        registry.clear()\n\n\ndef test_register_cls(clear_registry):\n    with pytest.raises(ValueError):\n        get_filesystem_class(\"test\")\n    register_implementation(\"test\", AbstractFileSystem)\n    cls = get_filesystem_class(\"test\")\n    assert cls is AbstractFileSystem\n\n\ndef test_register_str(clear_registry):\n    with pytest.raises(ValueError):\n        get_filesystem_class(\"test\")\n    register_implementation(\"test\", \"fsspec.AbstractFileSystem\")\n    assert \"test\" not in registry\n    cls = get_filesystem_class(\"test\")\n    assert cls is AbstractFileSystem\n    assert \"test\" in registry\n\n\ndef test_register_fail(clear_registry):\n    register_implementation(\"test\", \"doesntexist.AbstractFileSystem\")\n    with pytest.raises(ImportError):\n        get_filesystem_class(\"test\")\n\n    # NOOP\n    register_implementation(\"test\", \"doesntexist.AbstractFileSystem\", clobber=False)\n    with pytest.raises(ValueError):\n        register_implementation(\n            \"test\", \"doesntexist.AbstractFileSystemm\", clobber=False\n        )\n\n    # by default we do not allow clobbering\n    with pytest.raises(ValueError):\n        register_implementation(\"test\", \"doesntexist.AbstractFileSystemm\")\n\n    register_implementation(\n        \"test\", \"doesntexist.AbstractFileSystem\", errtxt=\"hiho\", clobber=True\n    )\n    with pytest.raises(ImportError) as e:\n        get_filesystem_class(\"test\")\n    assert \"hiho\" in str(e.value)\n    register_implementation(\"test\", AbstractFileSystem)\n\n    # NOOP\n    register_implementation(\"test\", AbstractFileSystem)\n    with pytest.raises(ValueError):\n        register_implementation(\"test\", ZipFileSystem)\n    register_implementation(\"test\", AbstractFileSystem, clobber=True)\n    assert isinstance(fsspec.filesystem(\"test\"), AbstractFileSystem)\n\n\ndef test_entry_points_registered_on_import(clear_registry, clean_imports):\n    mock_ep = create_autospec(EntryPoint, module=\"fsspec.spec.AbstractFileSystem\")\n    mock_ep.name = \"test\"  # this can't be set in the constructor...\n    mock_ep.value = \"fsspec.spec.AbstractFileSystem\"\n    import_location = \"importlib.metadata.entry_points\"\n    with patch(import_location, return_value={\"fsspec.specs\": [mock_ep]}):\n        assert \"test\" not in registry\n        import fsspec  # noqa\n\n        get_filesystem_class(\"test\")\n        assert \"test\" in registry\n\n\ndef test_filesystem_warning_arrow_hdfs_deprecated(clear_registry, clean_imports):\n    mock_ep = create_autospec(EntryPoint, module=\"fsspec.spec.AbstractFileSystem\")\n    mock_ep.name = \"arrow_hdfs\"  # this can't be set in the constructor...\n    mock_ep.value = \"fsspec.spec.AbstractFileSystem\"\n    import_location = \"importlib.metadata.entry_points\"\n    with patch(import_location, return_value={\"fsspec.specs\": [mock_ep]}):\n        import fsspec  # noqa\n\n        with pytest.warns(DeprecationWarning):\n            filesystem(\"arrow_hdfs\")\n\n\ndef test_old_s3(monkeypatch):\n    from fsspec.registry import _import_class\n\n    s3fs = pytest.importorskip(\"s3fs\")\n    monkeypatch.setattr(s3fs, \"__version__\", \"0.4.2\")\n    with pytest.warns():\n        _import_class(\"s3fs:S3FileSystem\")\n    with pytest.warns():\n        _import_class(\"s3fs.S3FileSystem\")\n", "fsspec/tests/test_parquet.py": "import os\n\nimport pytest\n\ntry:\n    import fastparquet\nexcept ImportError:\n    fastparquet = None\ntry:\n    import pyarrow.parquet as pq\nexcept ImportError:\n    pq = None\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.parquet import _get_parquet_byte_ranges, open_parquet_file\n\n# Define `engine` fixture\nFASTPARQUET_MARK = pytest.mark.skipif(not fastparquet, reason=\"fastparquet not found\")\nPYARROW_MARK = pytest.mark.skipif(not pq, reason=\"pyarrow not found\")\nANY_ENGINE_MARK = pytest.mark.skipif(\n    not (fastparquet or pq),\n    reason=\"No parquet engine (fastparquet or pyarrow) found\",\n)\n\n\n@pytest.fixture(\n    params=[\n        pytest.param(\"fastparquet\", marks=FASTPARQUET_MARK),\n        pytest.param(\"pyarrow\", marks=PYARROW_MARK),\n        pytest.param(\"auto\", marks=ANY_ENGINE_MARK),\n    ]\n)\ndef engine(request):\n    return request.param\n\n\n@pytest.mark.parametrize(\"columns\", [None, [\"x\"], [\"x\", \"y\"], [\"z\"]])\n@pytest.mark.parametrize(\"max_gap\", [0, 64])\n@pytest.mark.parametrize(\"max_block\", [64, 256_000_000])\n@pytest.mark.parametrize(\"footer_sample_size\", [8, 1_000])\n@pytest.mark.parametrize(\"range_index\", [True, False])\ndef test_open_parquet_file(\n    tmpdir, engine, columns, max_gap, max_block, footer_sample_size, range_index\n):\n    # Pandas required for this test\n    pd = pytest.importorskip(\"pandas\")\n\n    # Write out a simple DataFrame\n    path = os.path.join(str(tmpdir), \"test.parquet\")\n    nrows = 40\n    df = pd.DataFrame(\n        {\n            \"x\": [i * 7 % 5 for i in range(nrows)],\n            \"y\": [[0, i] for i in range(nrows)],  # list\n            \"z\": [{\"a\": i, \"b\": \"cat\"} for i in range(nrows)],  # struct\n        },\n        index=pd.Index([10 * i for i in range(nrows)], name=\"myindex\"),\n    )\n    if range_index:\n        df = df.reset_index(drop=True)\n        df.index.name = \"myindex\"\n    df.to_parquet(path)\n\n    # \"Traditional read\" (without `open_parquet_file`)\n    expect = pd.read_parquet(path, columns=columns)\n\n    # Use `_get_parquet_byte_ranges` to re-write a\n    # place-holder file with all bytes NOT required\n    # to read `columns` set to b\"0\". The purpose of\n    # this step is to make sure the read will fail\n    # if the correct bytes have not been accurately\n    # selected by `_get_parquet_byte_ranges`. If this\n    # test were reading from remote storage, we would\n    # not need this logic to capture errors.\n    fs = url_to_fs(path)[0]\n    data = _get_parquet_byte_ranges(\n        [path],\n        fs,\n        columns=columns,\n        engine=engine,\n        max_gap=max_gap,\n        max_block=max_block,\n        footer_sample_size=footer_sample_size,\n    )[path]\n    file_size = fs.size(path)\n    with open(path, \"wb\") as f:\n        f.write(b\"0\" * file_size)\n\n        if footer_sample_size == 8:\n            # We know 8 bytes is too small to include\n            # the footer metadata, so there should NOT\n            # be a key for the last 8 bytes of the file\n            bad_key = (file_size - 8, file_size)\n            assert bad_key not in data.keys()\n\n        for (start, stop), byte_data in data.items():\n            f.seek(start)\n            f.write(byte_data)\n\n    # Read back the modified file with `open_parquet_file`\n    with open_parquet_file(\n        path,\n        columns=columns,\n        engine=engine,\n        max_gap=max_gap,\n        max_block=max_block,\n        footer_sample_size=footer_sample_size,\n    ) as f:\n        result = pd.read_parquet(f, columns=columns)\n\n    # Check that `result` matches `expect`\n    pd.testing.assert_frame_equal(expect, result)\n\n    # Try passing metadata\n    if engine == \"fastparquet\":\n        # Should work fine for \"fastparquet\"\n        pf = fastparquet.ParquetFile(path)\n        with open_parquet_file(\n            path,\n            metadata=pf,\n            columns=columns,\n            engine=engine,\n            max_gap=max_gap,\n            max_block=max_block,\n            footer_sample_size=footer_sample_size,\n        ) as f:\n            result = pd.read_parquet(f, columns=columns)\n        pd.testing.assert_frame_equal(expect, result)\n    elif engine == \"pyarrow\":\n        # Should raise ValueError for \"pyarrow\"\n        with pytest.raises(ValueError):\n            open_parquet_file(\n                path,\n                metadata=[\"Not-None\"],\n                columns=columns,\n                engine=engine,\n                max_gap=max_gap,\n                max_block=max_block,\n                footer_sample_size=footer_sample_size,\n            )\n", "fsspec/tests/test_core.py": "import os\nimport pickle\nimport tempfile\nimport zipfile\nfrom contextlib import contextmanager\nfrom pathlib import Path\n\nimport pytest\n\nimport fsspec\nfrom fsspec.core import (\n    OpenFile,\n    OpenFiles,\n    _expand_paths,\n    expand_paths_if_needed,\n    get_compression,\n    get_fs_token_paths,\n    open_files,\n    open_local,\n)\n\n\n@contextmanager\ndef tempzip(data=None):\n    data = data or {}\n    f = tempfile.mkstemp(suffix=\"zip\")[1]\n    with zipfile.ZipFile(f, mode=\"w\") as z:\n        for k, v in data.items():\n            z.writestr(k, v)\n    try:\n        yield f\n    finally:\n        try:\n            os.remove(f)\n        except OSError:\n            pass\n\n\n@pytest.mark.parametrize(\n    \"path, name_function, num, out\",\n    [\n        [[\"apath\"], None, 1, [\"apath\"]],\n        [\"apath.*.csv\", None, 1, [\"apath.0.csv\"]],\n        [\"apath.*.csv\", None, 2, [\"apath.0.csv\", \"apath.1.csv\"]],\n        [\"a*\", lambda x: \"abc\"[x], 2, [\"aa\", \"ab\"]],\n    ],\n)\ndef test_expand_paths(path, name_function, num, out):\n    assert _expand_paths(path, name_function, num) == out\n\n\n@pytest.mark.parametrize(\n    \"create_files, path, out\",\n    [\n        [[\"apath\"], \"apath\", [\"apath\"]],\n        [[\"apath1\"], \"apath*\", [\"apath1\"]],\n        [[\"apath1\", \"apath2\"], \"apath*\", [\"apath1\", \"apath2\"]],\n        [[\"apath1\", \"apath2\"], \"apath[1]\", [\"apath1\"]],\n        [[\"apath1\", \"apath11\"], \"apath?\", [\"apath1\"]],\n    ],\n)\ndef test_expand_paths_if_needed_in_read_mode(create_files, path, out):\n    d = str(tempfile.mkdtemp())\n    for f in create_files:\n        f = os.path.join(d, f)\n        open(f, \"w\").write(\"test\")\n\n    path = os.path.join(d, path)\n\n    fs = fsspec.filesystem(\"file\")\n    res = expand_paths_if_needed([path], \"r\", 0, fs, None)\n    assert [os.path.basename(p) for p in res] == out\n\n\ndef test_expand_error():\n    with pytest.raises(ValueError):\n        _expand_paths(\"*.*\", None, 1)\n\n\n@pytest.mark.parametrize(\"mode\", [\"w\", \"w+\", \"x\", \"x+\"])\ndef test_expand_fs_token_paths(mode):\n    assert len(get_fs_token_paths(\"path\", mode, num=2, expand=True)[-1]) == 2\n\n\ndef test_openfile_api(m):\n    m.open(\"somepath\", \"wb\").write(b\"data\")\n    of = OpenFile(m, \"somepath\")\n    assert str(of) == \"<OpenFile 'somepath'>\"\n    f = of.open()\n    assert f.read() == b\"data\"\n    f.close()\n    with OpenFile(m, \"somepath\", mode=\"rt\") as f:\n        assert f.read() == \"data\"\n\n\ndef test_openfile_open(m):\n    of = OpenFile(m, \"somepath\", mode=\"wt\")\n    f = of.open()\n    f.write(\"hello\")\n    assert m.size(\"somepath\") == 0  # no flush yet\n    of.close()\n    assert m.size(\"somepath\") == 5\n\n\ndef test_open_local_w_cache():\n    d1 = str(tempfile.mkdtemp())\n    f1 = os.path.join(d1, \"f1\")\n    open(f1, \"w\").write(\"test1\")\n    d2 = str(tempfile.mkdtemp())\n    fn = open_local(f\"simplecache://{f1}\", cache_storage=d2, target_protocol=\"file\")\n    assert isinstance(fn, str)\n    assert open(fn).read() == \"test1\"\n    assert d2 in fn\n\n\ndef test_open_local_w_magic():\n    d1 = str(tempfile.mkdtemp())\n    f1 = os.path.join(d1, \"f1\")\n    open(f1, \"w\").write(\"test1\")\n    fn = open_local(os.path.join(d1, \"f*\"))\n    assert len(fn) == 1\n    assert isinstance(fn, list)\n\n\ndef test_open_local_w_list_of_str():\n    d1 = str(tempfile.mkdtemp())\n    f1 = os.path.join(d1, \"f1\")\n    open(f1, \"w\").write(\"test1\")\n    fn = open_local([f1, f1])\n    assert len(fn) == 2\n    assert isinstance(fn, list)\n    assert all(isinstance(elem, str) for elem in fn)\n\n\ndef test_open_local_w_path():\n    d1 = str(tempfile.mkdtemp())\n    f1 = os.path.join(d1, \"f1\")\n    open(f1, \"w\").write(\"test1\")\n    p = Path(f1)\n    fn = open_local(p)\n    assert isinstance(fn, str)\n\n\ndef test_open_local_w_list_of_path():\n    d1 = str(tempfile.mkdtemp())\n    f1 = os.path.join(d1, \"f1\")\n    open(f1, \"w\").write(\"test1\")\n    p = Path(f1)\n    fn = open_local([p, p])\n    assert len(fn) == 2\n    assert isinstance(fn, list)\n    assert all(isinstance(elem, str) for elem in fn)\n\n\ndef test_xz_lzma_compressions():\n    pytest.importorskip(\"lzma\")\n    # Ensure that both 'xz' and 'lzma' compression names can be parsed\n    assert get_compression(\"some_file.xz\", \"infer\") == \"xz\"\n    assert get_compression(\"some_file.xz\", \"xz\") == \"xz\"\n    assert get_compression(\"some_file.xz\", \"lzma\") == \"lzma\"\n\n\ndef test_list():\n    here = os.path.abspath(os.path.dirname(__file__))\n    flist = os.listdir(here)\n    plist = [os.path.join(here, p).replace(\"\\\\\", \"/\") for p in flist]\n    of = open_files(plist)\n    assert len(of) == len(flist)\n    assert [f.path for f in of] == plist\n\n\ndef test_open_expand(m, monkeypatch):\n    m.pipe(\"/myfile\", b\"hello\")\n    with pytest.raises(FileNotFoundError, match=\"expand=True\"):\n        with fsspec.open(\"memory://my*\", expand=False):\n            pass\n    with fsspec.open(\"memory://my*\", expand=True) as f:\n        assert f.path == \"/myfile\"\n    monkeypatch.setattr(fsspec.core, \"DEFAULT_EXPAND\", True)\n    with fsspec.open(\"memory://my*\") as f:\n        assert f.path == \"/myfile\"\n\n\ndef test_pathobject(tmpdir):\n    import pathlib\n\n    tmpdir = str(tmpdir)\n    plist_str = [os.path.join(str(tmpdir), f).replace(\"\\\\\", \"/\") for f in [\"a\", \"b\"]]\n    open(plist_str[0], \"w\").write(\"first file\")\n    open(plist_str[1], \"w\").write(\"second file\")\n    plist = [pathlib.Path(p) for p in plist_str]\n    of = open_files(plist)\n    assert len(of) == 2\n    assert [f.path for f in of] == plist_str\n\n    of = open_files(plist[0])\n    assert len(of) == 1\n    assert of[0].path == plist_str[0]\n    with of[0] as f:\n        assert f.read() == open(plist_str[0], \"rb\").read()\n\n\ndef test_automkdir(tmpdir):\n    dir = os.path.join(str(tmpdir), \"a\")\n    of = fsspec.open(os.path.join(dir, \"afile\"), \"w\", auto_mkdir=False)\n    with pytest.raises(IOError):\n        with of:\n            pass\n\n    dir = os.path.join(str(tmpdir), \"b\")\n    of = fsspec.open(os.path.join(dir, \"bfile\"), \"w\", auto_mkdir=True)\n    with of:\n        pass\n\n    assert \"bfile\" in os.listdir(dir)\n\n    dir = os.path.join(str(tmpdir), \"c\")\n    with pytest.raises(FileNotFoundError):\n        of = fsspec.open(os.path.join(dir, \"bfile\"), \"w\", auto_mkdir=False)\n        with of:\n            pass\n\n\ndef test_automkdir_readonly(tmpdir):\n    dir = os.path.join(str(tmpdir), \"d\")\n    with pytest.raises(FileNotFoundError):\n        of = fsspec.open(os.path.join(dir, \"dfile\"), \"r\")\n        with of:\n            pass\n\n\ndef test_openfile_pickle_newline():\n    # GH#318\n    test = fsspec.open(__file__, newline=b\"\")\n\n    pickled = pickle.dumps(test)\n    restored = pickle.loads(pickled)\n\n    assert test.newline == restored.newline\n\n\ndef test_pickle_after_open_open():\n    of = fsspec.open(__file__, mode=\"rt\")\n    test = of.open()\n    of2 = pickle.loads(pickle.dumps(of))\n    test2 = of2.open()\n    test.close()\n\n    assert not test2.closed\n    of.close()\n    of2.close()\n\n\n# Define a list of special glob characters.\n# Note that we need to escape some characters and also consider file system limitations.\n# '*' and '?' are excluded because they are not valid for many file systems.\n# Similarly, we're careful with '{', '}', and '@' as their special meaning is\n# context-specific and might not be considered special for filenames.\n# Add tests for more file systems and for more glob magic later\nglob_magic_characters = [\"[\", \"]\", \"!\"]\nif os.name != \"nt\":\n    glob_magic_characters.extend((\"*\", \"?\"))  # not valid on Windows\n\n\n@pytest.mark.parametrize(\"char\", glob_magic_characters)\ndef test_open_file_read_with_special_characters(tmp_path, char):\n    # Create a filename incorporating the special character\n    file_name = f\"test{char}.txt\"\n    file_path = tmp_path / file_name\n    expected_content = \"Hello, world!\"\n\n    with open(file_path, \"w\") as f:\n        f.write(expected_content)\n\n    with fsspec.open(file_path, \"r\") as f:\n        actual_content = f.read()\n\n    assert actual_content == expected_content\n\n\n@pytest.mark.parametrize(\"char\", glob_magic_characters)\ndef test_open_files_read_with_special_characters(tmp_path, char):\n    # Create a filename incorporating the special character\n    file_name = f\"test{char}.txt\"\n    file_path = tmp_path / file_name\n    expected_content = \"Hello, world!\"\n\n    with open(file_path, \"w\") as f:\n        f.write(expected_content)\n\n    with fsspec.open_files(file_path, \"r\")[0] as f:\n        actual_content = f.read()\n\n    assert actual_content == expected_content\n\n\n@pytest.mark.parametrize(\"char\", glob_magic_characters)\ndef test_open_file_write_with_special_characters(tmp_path, char, monkeypatch):\n    # Create a filename incorporating the special character\n    file_name = f\"test{char}.txt\"\n    file_path = tmp_path / file_name\n    expected_content = \"Hello, world!\"\n\n    with fsspec.open(file_path, \"w\", expand=False) as f:\n        f.write(expected_content)\n\n    with open(file_path, \"r\") as f:\n        actual_content = f.read()\n\n    monkeypatch.setattr(fsspec.core, \"DEFAULT_EXPAND\", False)\n    with fsspec.open(file_path, \"w\") as f:\n        f.write(expected_content * 2)\n\n    with open(file_path, \"r\") as f:\n        assert f.read() == actual_content * 2\n\n    assert actual_content == expected_content\n\n\n@pytest.mark.parametrize(\"char\", glob_magic_characters)\ndef test_open_files_read_with_special_characters(tmp_path, char):\n    # Create a filename incorporating the special character\n    file_name = f\"test{char}.txt\"\n    file_path = tmp_path / file_name\n    expected_content = \"Hello, world!\"\n\n    with open(file_path, \"w\") as f:\n        f.write(expected_content)\n\n    with fsspec.open_files(\n        urlpath=[os.fspath(file_path)], mode=\"r\", auto_mkdir=False, expand=False\n    )[0] as f:\n        actual_content = f.read()\n\n    assert actual_content == expected_content\n\n\n@pytest.mark.parametrize(\"char\", glob_magic_characters)\ndef test_open_files_write_with_special_characters(tmp_path, char):\n    # Create a filename incorporating the special character\n    file_name = f\"test{char}.txt\"\n    file_path = tmp_path / file_name\n    expected_content = \"Hello, world!\"\n\n    with fsspec.open_files(\n        urlpath=[os.fspath(file_path)], mode=\"w\", auto_mkdir=False, expand=False\n    )[0] as f:\n        f.write(expected_content)\n\n    with open(file_path, \"r\") as f:\n        actual_content = f.read()\n\n    assert actual_content == expected_content\n\n\ndef test_mismatch():\n    pytest.importorskip(\"s3fs\")\n    with pytest.raises(ValueError):\n        open_files([\"s3://test/path.csv\", \"/other/path.csv\"])\n\n\ndef test_url_kwargs_chain(ftp_writable):\n    host, port, username, password = ftp_writable\n    data = b\"hello\"\n    with fsspec.open(\n        \"ftp:///afile\", \"wb\", host=host, port=port, username=username, password=password\n    ) as f:\n        f.write(data)\n\n    with fsspec.open(\n        f\"simplecache::ftp://{username}:{password}@{host}:{port}//afile\", \"rb\"\n    ) as f:\n        assert f.read() == data\n\n\ndef test_multi_context(tmpdir):\n    fns = [os.path.join(tmpdir, fn) for fn in [\"a\", \"b\"]]\n    files = open_files(fns, \"wb\")\n    assert isinstance(files, OpenFiles)\n    assert isinstance(files[0], OpenFile)\n    assert len(files) == 2\n    assert isinstance(files[:1], OpenFiles)\n    assert len(files[:1]) == 1\n    with files as of:\n        assert len(of) == 2\n        assert not of[0].closed\n        assert of[0].name.endswith(\"a\")\n    assert of[0].closed\n    assert repr(files) == \"<List of 2 OpenFile instances>\"\n\n\ndef test_not_local():\n    with pytest.raises(ValueError, match=\"attribute local_file=True\"):\n        open_local(\"memory://afile\")\n\n\ndef test_url_to_fs(ftp_writable):\n    host, port, username, password = ftp_writable\n    data = b\"hello\"\n    with fsspec.open(f\"ftp://{username}:{password}@{host}:{port}/afile\", \"wb\") as f:\n        f.write(data)\n    fs, url = fsspec.core.url_to_fs(\n        f\"simplecache::ftp://{username}:{password}@{host}:{port}/afile\"\n    )\n    assert url == \"/afile\"\n    fs, url = fsspec.core.url_to_fs(f\"ftp://{username}:{password}@{host}:{port}/afile\")\n    assert url == \"/afile\"\n\n    with fsspec.open(f\"ftp://{username}:{password}@{host}:{port}/afile.zip\", \"wb\") as f:\n        import zipfile\n\n        with zipfile.ZipFile(f, \"w\") as z:\n            with z.open(\"inner\", \"w\") as f2:\n                f2.write(b\"hello\")\n        f.write(data)\n\n    fs, url = fsspec.core.url_to_fs(\n        f\"zip://inner::ftp://{username}:{password}@{host}:{port}/afile.zip\"\n    )\n    assert url == \"inner\"\n    fs, url = fsspec.core.url_to_fs(\n        f\"simplecache::zip::ftp://{username}:{password}@{host}:{port}/afile.zip\"\n    )\n    assert url == \"\"\n\n\ndef test_target_protocol_options(ftp_writable):\n    host, port, username, password = ftp_writable\n    data = {\"afile\": b\"hello\"}\n    options = {\"host\": host, \"port\": port, \"username\": username, \"password\": password}\n    with tempzip(data) as lfile, fsspec.open(\n        \"ftp:///archive.zip\", \"wb\", **options\n    ) as f:\n        f.write(open(lfile, \"rb\").read())\n    with fsspec.open(\n        \"zip://afile\",\n        \"rb\",\n        target_protocol=\"ftp\",\n        target_options=options,\n        fo=\"archive.zip\",\n    ) as f:\n        assert f.read() == data[\"afile\"]\n\n\ndef test_chained_url(ftp_writable):\n    host, port, username, password = ftp_writable\n    data = {\"afile\": b\"hello\"}\n    cls = fsspec.get_filesystem_class(\"ftp\")\n    fs = cls(host=host, port=port, username=username, password=password)\n    with tempzip(data) as lfile:\n        fs.put_file(lfile, \"archive.zip\")\n\n    urls = [\n        \"zip://afile\",\n        \"zip://afile::simplecache\",\n        \"simplecache::zip://afile\",\n        \"simplecache::zip://afile::simplecache\",\n    ]\n    for url in urls:\n        url += f\"::ftp://{username}:{password}@{host}:{port}/archive.zip\"\n        with fsspec.open(url, \"rb\") as f:\n            assert f.read() == data[\"afile\"]\n\n\ndef test_automkdir_local():\n    fs, _ = fsspec.core.url_to_fs(\"file://\", auto_mkdir=True)\n    assert fs.auto_mkdir is True\n", "fsspec/tests/test_api.py": "\"\"\"Tests the spec, using memoryfs\"\"\"\n\nimport contextlib\nimport os\nimport pickle\nimport tempfile\nfrom unittest.mock import Mock\n\nimport pytest\n\nimport fsspec\nfrom fsspec.implementations.memory import MemoryFile, MemoryFileSystem\n\n\ndef test_idempotent():\n    MemoryFileSystem.clear_instance_cache()\n    fs = MemoryFileSystem()\n    fs2 = MemoryFileSystem()\n    assert fs is fs2\n    assert MemoryFileSystem.current() is fs2\n\n    MemoryFileSystem.clear_instance_cache()\n    assert not MemoryFileSystem._cache\n\n    fs2 = MemoryFileSystem().current()\n    assert fs == fs2\n\n\ndef test_pickle():\n    fs = MemoryFileSystem()\n    fs2 = pickle.loads(pickle.dumps(fs))\n    assert fs == fs2\n\n\ndef test_class_methods():\n    assert MemoryFileSystem._strip_protocol(\"memory://stuff\") == \"/stuff\"\n    assert MemoryFileSystem._strip_protocol(\"stuff\") == \"/stuff\"\n    assert MemoryFileSystem._strip_protocol(\"other://stuff\") == \"other://stuff\"\n\n    assert MemoryFileSystem._get_kwargs_from_urls(\"memory://user@thing\") == {}\n\n\ndef test_multi(m):\n    m.pipe(\"/afile\", b\"data\")\n    fs, token, paths = fsspec.core.get_fs_token_paths([\"/afile\", \"/afile\"])\n    assert len(paths) == 2\n\n\ndef test_get_put(tmpdir, m):\n    tmpdir = str(tmpdir)\n    fn = os.path.join(tmpdir, \"one\")\n    open(fn, \"wb\").write(b\"one\")\n    os.mkdir(os.path.join(tmpdir, \"dir\"))\n    fn2 = os.path.join(tmpdir, \"dir\", \"two\")\n    open(fn2, \"wb\").write(b\"two\")\n\n    fs = MemoryFileSystem()\n    fs.put(fn, \"/afile\")\n    assert fs.cat(\"/afile\") == b\"one\"\n\n    fs.store[\"/bfile\"] = MemoryFile(fs, \"/bfile\", b\"data\")\n    fn3 = os.path.join(tmpdir, \"three\")\n    fs.get(\"/bfile\", fn3)\n    assert open(fn3, \"rb\").read() == b\"data\"\n\n    fs.put(tmpdir, \"/more\", recursive=True)\n    assert fs.find(\"/more\") == [\"/more/dir/two\", \"/more/one\", \"/more/three\"]\n\n    @contextlib.contextmanager\n    def tmp_chdir(path):\n        curdir = os.getcwd()\n        os.chdir(path)\n        try:\n            yield\n        finally:\n            os.chdir(curdir)\n\n    with tmp_chdir(os.path.join(tmpdir, os.path.pardir)):\n        fs.put(os.path.basename(tmpdir), \"/moretwo\", recursive=True)\n        assert fs.find(\"/moretwo\") == [\n            \"/moretwo/dir/two\",\n            \"/moretwo/one\",\n            \"/moretwo/three\",\n        ]\n\n    with tmp_chdir(tmpdir):\n        fs.put(os.path.curdir, \"/morethree\", recursive=True)\n        assert fs.find(\"/morethree\") == [\n            \"/morethree/dir/two\",\n            \"/morethree/one\",\n            \"/morethree/three\",\n        ]\n\n    for f in [fn, fn2, fn3]:\n        os.remove(f)\n    os.rmdir(os.path.join(tmpdir, \"dir\"))\n\n    fs.get(\"/more/\", tmpdir + \"/\", recursive=True)\n    assert open(fn3, \"rb\").read() == b\"data\"\n    assert open(fn, \"rb\").read() == b\"one\"\n\n\ndef test_du(m):\n    fs = MemoryFileSystem()\n    fs.store.update(\n        {\n            \"/dir/afile\": MemoryFile(fs, \"/afile\", b\"a\"),\n            \"/dir/dirb/afile\": MemoryFile(fs, \"/afile\", b\"bb\"),\n            \"/dir/dirb/bfile\": MemoryFile(fs, \"/afile\", b\"ccc\"),\n        }\n    )\n    assert fs.du(\"/dir\") == 6\n    assert fs.du(\"/dir\", total=False) == {\n        \"/dir/afile\": 1,\n        \"/dir/dirb/afile\": 2,\n        \"/dir/dirb/bfile\": 3,\n    }\n    assert fs.du(\"/dir\", withdirs=True) == 6\n    assert fs.du(\"/dir\", total=False, withdirs=True) == {\n        \"/dir\": 0,\n        \"/dir/afile\": 1,\n        \"/dir/dirb\": 0,\n        \"/dir/dirb/afile\": 2,\n        \"/dir/dirb/bfile\": 3,\n    }\n    with pytest.raises(ValueError):\n        assert fs.du(\"/dir\", maxdepth=0) == 1\n    assert fs.du(\"/dir\", total=False, withdirs=True, maxdepth=1) == {\n        \"/dir\": 0,\n        \"/dir/afile\": 1,\n        \"/dir/dirb\": 0,\n    }\n\n    # Size of file only.\n    assert fs.du(\"/dir/afile\") == 1\n    assert fs.du(\"/dir/afile\", withdirs=True) == 1\n\n\ndef test_head_tail(m):\n    fs = MemoryFileSystem()\n    with fs.open(\"/myfile\", \"wb\") as f:\n        f.write(b\"I had a nice big cabbage\")\n    assert fs.head(\"/myfile\", 5) == b\"I had\"\n    assert fs.tail(\"/myfile\", 7) == b\"cabbage\"\n\n\ndef test_move(m):\n    fs = MemoryFileSystem()\n    with fs.open(\"/myfile\", \"wb\") as f:\n        f.write(b\"I had a nice big cabbage\")\n    fs.move(\"/myfile\", \"/otherfile\")\n    assert not fs.exists(\"/myfile\")\n    assert fs.info(\"/otherfile\")\n    assert isinstance(fs.ukey(\"/otherfile\"), str)\n\n\ndef test_recursive_get_put(tmpdir, m):\n    fs = MemoryFileSystem()\n    os.makedirs(f\"{tmpdir}/nest\")\n    for file in [\"one\", \"two\", \"nest/other\"]:\n        with open(f\"{tmpdir}/{file}\", \"wb\") as f:\n            f.write(b\"data\")\n\n    fs.put(str(tmpdir), \"test\", recursive=True)\n\n    # get to directory with slash\n    d = tempfile.mkdtemp()\n    fs.get(\"test/\", d, recursive=True)\n    for file in [\"one\", \"two\", \"nest/other\"]:\n        with open(f\"{d}/{file}\", \"rb\") as f:\n            assert f.read() == b\"data\"\n\n    # get to directory without slash\n    d = tempfile.mkdtemp()\n    fs.get(\"test\", d, recursive=True)\n    for file in [\"test/one\", \"test/two\", \"test/nest/other\"]:\n        with open(f\"{d}/{file}\", \"rb\") as f:\n            assert f.read() == b\"data\"\n\n\ndef test_pipe_cat(m):\n    fs = MemoryFileSystem()\n    fs.pipe(\"afile\", b\"contents\")\n    assert fs.cat(\"afile\") == b\"contents\"\n\n    data = {\"/bfile\": b\"more\", \"/cfile\": b\"stuff\"}\n    fs.pipe(data)\n    assert fs.cat(list(data)) == data\n\n\ndef test_read_block_delimiter(m):\n    fs = MemoryFileSystem()\n    with fs.open(\"/myfile\", \"wb\") as f:\n        f.write(b\"some\\nlines\\nof\\ntext\")\n    assert fs.read_block(\"/myfile\", 0, 2, b\"\\n\") == b\"some\\n\"\n    assert fs.read_block(\"/myfile\", 2, 6, b\"\\n\") == b\"lines\\n\"\n    assert fs.read_block(\"/myfile\", 6, 2, b\"\\n\") == b\"\"\n    assert fs.read_block(\"/myfile\", 2, 9, b\"\\n\") == b\"lines\\nof\\n\"\n    assert fs.read_block(\"/myfile\", 12, 6, b\"\\n\") == b\"text\"\n    assert fs.read_block(\"/myfile\", 0, None) == fs.cat(\"/myfile\")\n\n\ndef test_open_text(m):\n    fs = MemoryFileSystem()\n    with fs.open(\"/myfile\", \"wb\") as f:\n        f.write(b\"some\\nlines\\nof\\ntext\")\n    f = fs.open(\"/myfile\", \"r\", encoding=\"latin1\")\n    assert f.encoding == \"latin1\"\n\n\ndef test_read_text(m):\n    with m.open(\"/myfile\", \"w\", encoding=\"utf-8\") as f:\n        f.write(\"some\\nlines\\nof\\ntext\")\n    assert m.read_text(\"/myfile\", encoding=\"utf-8\") == \"some\\nlines\\nof\\ntext\"\n\n\ndef test_write_text(m):\n    m.write_text(\"/myfile\", \"some\\nlines\\nof\\ntext\", encoding=\"utf-8\")\n    assert m.read_text(\"/myfile\", encoding=\"utf-8\") == \"some\\nlines\\nof\\ntext\"\n\n\ndef test_chained_fs():\n    d1 = tempfile.mkdtemp()\n    d2 = tempfile.mkdtemp()\n    f1 = os.path.join(d1, \"f1\")\n    with open(f1, \"wb\") as f:\n        f.write(b\"test\")\n\n    of = fsspec.open(\n        f\"simplecache::file://{f1}\",\n        simplecache={\"cache_storage\": d2, \"same_names\": True},\n    )\n    with of as f:\n        assert f.read() == b\"test\"\n\n    assert os.listdir(d2) == [\"f1\"]\n\n\n@pytest.mark.xfail(reason=\"see issue #334\", strict=True)\ndef test_multilevel_chained_fs():\n    \"\"\"This test reproduces fsspec/filesystem_spec#334\"\"\"\n    import zipfile\n\n    d1 = tempfile.mkdtemp()\n    f1 = os.path.join(d1, \"f1.zip\")\n    with zipfile.ZipFile(f1, mode=\"w\") as z:\n        # filename, content\n        z.writestr(\"foo.txt\", \"foo.txt\")\n        z.writestr(\"bar.txt\", \"bar.txt\")\n\n    # We expected this to be the correct syntax\n    with pytest.raises(IsADirectoryError):\n        of = fsspec.open_files(f\"zip://*.txt::simplecache::file://{f1}\")\n        assert len(of) == 2\n\n    # But this is what is actually valid...\n    of = fsspec.open_files(f\"zip://*.txt::simplecache://{f1}::file://\")\n\n    assert len(of) == 2\n    for open_file in of:\n        with open_file as f:\n            assert f.read().decode(\"utf-8\") == f.name\n\n\ndef test_multilevel_chained_fs_zip_zip_file():\n    \"\"\"This test reproduces fsspec/filesystem_spec#334\"\"\"\n    import zipfile\n\n    d1 = tempfile.mkdtemp()\n    f1 = os.path.join(d1, \"f1.zip\")\n    f2 = os.path.join(d1, \"f2.zip\")\n    with zipfile.ZipFile(f1, mode=\"w\") as z:\n        # filename, content\n        z.writestr(\"foo.txt\", \"foo.txt\")\n        z.writestr(\"bar.txt\", \"bar.txt\")\n\n    with zipfile.ZipFile(f2, mode=\"w\") as z:\n        with open(f1, \"rb\") as f:\n            z.writestr(\"f1.zip\", f.read())\n\n    # We expected this to be the correct syntax\n    of = fsspec.open_files(f\"zip://*.txt::zip://f1.zip::file://{f2}\")\n\n    assert len(of) == 2\n    for open_file in of:\n        with open_file as f:\n            assert f.read().decode(\"utf-8\") == f.name\n\n\ndef test_chained_equivalent():\n    d1 = tempfile.mkdtemp()\n    d2 = tempfile.mkdtemp()\n    f1 = os.path.join(d1, \"f1\")\n    with open(f1, \"wb\") as f:\n        f.write(b\"test1\")\n\n    of = fsspec.open(\n        f\"simplecache::file://{f1}\",\n        simplecache={\"cache_storage\": d2, \"same_names\": True},\n    )\n    of2 = fsspec.open(\n        f\"simplecache://{f1}\",\n        cache_storage=d2,\n        same_names=True,\n        target_protocol=\"file\",\n        target_options={},\n    )\n    # the following line passes by fluke - they are not quite the same instance,\n    #  since the parameters don't quite match. Also, the url understood by the two\n    #  of s are not the same (path gets munged a bit differently)\n    assert of.fs == of2.fs\n    assert hash(of.fs) == hash(of2.fs)\n    assert of.open().read() == of2.open().read()\n\n\ndef test_chained_fs_multi():\n    d1 = tempfile.mkdtemp()\n    d2 = tempfile.mkdtemp()\n    f1 = os.path.join(d1, \"f1\")\n    f2 = os.path.join(d1, \"f2\")\n    with open(f1, \"wb\") as f:\n        f.write(b\"test1\")\n    with open(f2, \"wb\") as f:\n        f.write(b\"test2\")\n\n    of = fsspec.open_files(\n        f\"simplecache::file://{d1}/*\",\n        simplecache={\"cache_storage\": d2, \"same_names\": True},\n    )\n    with of[0] as f:\n        assert f.read() == b\"test1\"\n    with of[1] as f:\n        assert f.read() == b\"test2\"\n\n    assert sorted(os.listdir(d2)) == [\"f1\", \"f2\"]\n\n    d2 = tempfile.mkdtemp()\n\n    of = fsspec.open_files(\n        [f\"simplecache::file://{f1}\", f\"simplecache::file://{f2}\"],\n        simplecache={\"cache_storage\": d2, \"same_names\": True},\n    )\n    with of[0] as f:\n        assert f.read() == b\"test1\"\n    with of[1] as f:\n        assert f.read() == b\"test2\"\n\n    assert sorted(os.listdir(d2)) == [\"f1\", \"f2\"]\n\n\ndef test_chained_fo():\n    import zipfile\n\n    d1 = tempfile.mkdtemp()\n    f1 = os.path.join(d1, \"temp.zip\")\n    d3 = tempfile.mkdtemp()\n    with zipfile.ZipFile(f1, mode=\"w\") as z:\n        z.writestr(\"afile\", b\"test\")\n\n    of = fsspec.open(f\"zip://afile::file://{f1}\")\n    with of as f:\n        assert f.read() == b\"test\"\n\n    of = fsspec.open_files(f\"zip://*::file://{f1}\")\n    with of[0] as f:\n        assert f.read() == b\"test\"\n\n    of = fsspec.open_files(\n        f\"simplecache::zip://*::file://{f1}\",\n        simplecache={\"cache_storage\": d3, \"same_names\": True},\n    )\n    with of[0] as f:\n        assert f.read() == b\"test\"\n    assert \"afile\" in os.listdir(d3)\n\n\ndef test_url_to_fs():\n    url = \"memory://a.txt\"\n    fs, url2 = fsspec.core.url_to_fs(url)\n\n    assert isinstance(fs, MemoryFileSystem)\n    assert url2 == \"/a.txt\"\n\n\ndef test_walk(m):\n    # depth = 0\n    dir1 = \"/dir1\"\n    # depth = 1 (2 dirs, 1 file)\n    dir11 = dir1 + \"/dir11\"\n    dir12 = dir1 + \"/dir12\"\n    file11 = dir1 + \"/file11\"\n    # depth = 2\n    dir111 = dir11 + \"/dir111\"\n    file111 = dir11 + \"/file111\"\n    file121 = dir12 + \"/file121\"\n    # depth = 3\n    file1111 = dir111 + \"/file1111\"\n\n    m.mkdir(dir111)  # Creates parents too\n    m.mkdir(dir12)  # Creates parents too\n    m.touch(file11)\n    m.touch(file111)\n    m.touch(file121)\n    m.touch(file1111)\n\n    # No maxdepth\n    assert list(m.walk(dir1, topdown=True)) == [\n        (dir1, [\"dir11\", \"dir12\"], [\"file11\"]),\n        (dir11, [\"dir111\"], [\"file111\"]),\n        (dir111, [], [\"file1111\"]),\n        (dir12, [], [\"file121\"]),\n    ]\n    assert list(m.walk(dir1, topdown=False)) == [\n        (dir111, [], [\"file1111\"]),\n        (dir11, [\"dir111\"], [\"file111\"]),\n        (dir12, [], [\"file121\"]),\n        (dir1, [\"dir11\", \"dir12\"], [\"file11\"]),\n    ]\n\n    # maxdepth=2\n    assert list(m.walk(dir1, maxdepth=2, topdown=True)) == [\n        (dir1, [\"dir11\", \"dir12\"], [\"file11\"]),\n        (dir11, [\"dir111\"], [\"file111\"]),\n        (dir12, [], [\"file121\"]),\n    ]\n    assert list(m.walk(dir1, maxdepth=2, topdown=False)) == [\n        (dir11, [\"dir111\"], [\"file111\"]),\n        (dir12, [], [\"file121\"]),\n        (dir1, [\"dir11\", \"dir12\"], [\"file11\"]),\n    ]\n\n    # maxdepth=1\n    assert list(m.walk(dir1, maxdepth=1, topdown=True)) == [\n        (dir1, [\"dir11\", \"dir12\"], [\"file11\"]),\n    ]\n    assert list(m.walk(dir1, maxdepth=1, topdown=False)) == [\n        (dir1, [\"dir11\", \"dir12\"], [\"file11\"]),\n    ]\n\n    # maxdepth=0\n    with pytest.raises(ValueError):\n        list(m.walk(dir1, maxdepth=0, topdown=True))\n    with pytest.raises(ValueError):\n        list(m.walk(dir1, maxdepth=0, topdown=False))\n\n    # prune dir111\n    def _walk(*args, **kwargs):\n        for path, dirs, files in m.walk(*args, **kwargs):\n            yield (path, dirs.copy(), files)\n            if \"dir111\" in dirs:\n                dirs.remove(\"dir111\")\n\n    assert list(_walk(dir1, topdown=True)) == [\n        (dir1, [\"dir11\", \"dir12\"], [\"file11\"]),\n        (dir11, [\"dir111\"], [\"file111\"]),\n        (dir12, [], [\"file121\"]),\n    ]\n    assert list(_walk(dir1, topdown=False)) == [\n        (dir111, [], [\"file1111\"]),\n        (dir11, [\"dir111\"], [\"file111\"]),\n        (dir12, [], [\"file121\"]),\n        (dir1, [\"dir11\", \"dir12\"], [\"file11\"]),\n    ]\n\n    # reverse dirs order\n    def _walk(*args, **kwargs):\n        for path, dirs, files in m.walk(*args, **kwargs):\n            yield (path, dirs.copy(), files)\n            dirs.reverse()\n\n    assert list(_walk(dir1, topdown=True)) == [\n        (dir1, [\"dir11\", \"dir12\"], [\"file11\"]),\n        # Here dir12 comes before dir11\n        (dir12, [], [\"file121\"]),\n        (dir11, [\"dir111\"], [\"file111\"]),\n        (dir111, [], [\"file1111\"]),\n    ]\n    assert list(_walk(dir1, topdown=False)) == [\n        (dir111, [], [\"file1111\"]),\n        (dir11, [\"dir111\"], [\"file111\"]),\n        (dir12, [], [\"file121\"]),\n        (dir1, [\"dir11\", \"dir12\"], [\"file11\"]),\n    ]\n\n    # on_error omit by default\n    assert list(m.walk(\"do_not_exist\")) == []\n    # on_error omit\n    assert list(m.walk(\"do_not_exist\", on_error=\"omit\")) == []\n    # on_error raise\n    with pytest.raises(FileNotFoundError):\n        list(m.walk(\"do_not_exist\", on_error=\"raise\"))\n    # on_error callable function\n    mock = Mock()\n    assert list(m.walk(\"do_not_exist\", on_error=mock.onerror)) == []\n    mock.onerror.assert_called()\n    assert mock.onerror.call_args.kwargs == {}\n    assert len(mock.onerror.call_args.args) == 1\n    assert isinstance(mock.onerror.call_args.args[0], FileNotFoundError)\n", "fsspec/tests/__init__.py": "", "fsspec/tests/test_callbacks.py": "import pytest\n\nfrom fsspec.callbacks import Callback, TqdmCallback\n\n\ndef test_callbacks():\n    empty_callback = Callback()\n    assert empty_callback.call(\"something\", somearg=None) is None\n\n    hooks = {\"something\": lambda *_, arg=None: arg + 2}\n    simple_callback = Callback(hooks=hooks)\n    assert simple_callback.call(\"something\", arg=2) == 4\n\n    hooks = {\"something\": lambda *_, arg1=None, arg2=None: arg1 + arg2}\n    multi_arg_callback = Callback(hooks=hooks)\n    assert multi_arg_callback.call(\"something\", arg1=2, arg2=2) == 4\n\n\ndef test_callbacks_as_callback():\n    empty_callback = Callback.as_callback(None)\n    assert empty_callback.call(\"something\", arg=\"somearg\") is None\n    assert Callback.as_callback(None) is Callback.as_callback(None)\n\n    hooks = {\"something\": lambda *_, arg=None: arg + 2}\n    real_callback = Callback.as_callback(Callback(hooks=hooks))\n    assert real_callback.call(\"something\", arg=2) == 4\n\n\ndef test_callbacks_as_context_manager(mocker):\n    spy_close = mocker.spy(Callback, \"close\")\n\n    with Callback() as cb:\n        assert isinstance(cb, Callback)\n\n    spy_close.assert_called_once()\n\n\ndef test_callbacks_branched():\n    callback = Callback()\n\n    branch = callback.branched(\"path_1\", \"path_2\")\n\n    assert branch is not callback\n    assert isinstance(branch, Callback)\n\n\n@pytest.mark.asyncio\nasync def test_callbacks_branch_coro(mocker):\n    async_fn = mocker.AsyncMock(return_value=10)\n    callback = Callback()\n    wrapped_fn = callback.branch_coro(async_fn)\n    spy = mocker.spy(callback, \"branched\")\n\n    assert await wrapped_fn(\"path_1\", \"path_2\", key=\"value\") == 10\n\n    spy.assert_called_once_with(\"path_1\", \"path_2\", key=\"value\")\n    async_fn.assert_called_once_with(\n        \"path_1\", \"path_2\", callback=spy.spy_return, key=\"value\"\n    )\n\n\ndef test_callbacks_wrap():\n    events = []\n\n    class TestCallback(Callback):\n        def relative_update(self, inc=1):\n            events.append(inc)\n\n    callback = TestCallback()\n    for _ in callback.wrap(range(10)):\n        ...\n\n    assert events == [1] * 10\n\n\n@pytest.mark.parametrize(\"tqdm_kwargs\", [{}, {\"desc\": \"A custom desc\"}])\ndef test_tqdm_callback(tqdm_kwargs, mocker):\n    pytest.importorskip(\"tqdm\")\n    callback = TqdmCallback(tqdm_kwargs=tqdm_kwargs)\n    mocker.patch.object(callback, \"_tqdm_cls\")\n    callback.set_size(10)\n    for _ in callback.wrap(range(10)):\n        ...\n\n    assert callback.tqdm.update.call_count == 11\n    if not tqdm_kwargs:\n        callback._tqdm_cls.assert_called_with(total=10)\n    else:\n        callback._tqdm_cls.assert_called_with(total=10, **tqdm_kwargs)\n", "fsspec/tests/test_utils.py": "import io\nimport sys\nfrom pathlib import Path, PurePath\nfrom unittest.mock import Mock\n\nimport pytest\n\nimport fsspec.utils\nfrom fsspec.utils import (\n    can_be_local,\n    common_prefix,\n    get_protocol,\n    infer_storage_options,\n    merge_offset_ranges,\n    mirror_from,\n    other_paths,\n    read_block,\n    seek_delimiter,\n    setup_logging,\n)\n\nWIN = sys.platform.startswith(\"win\")\n\n\ndef test_read_block():\n    delimiter = b\"\\n\"\n    data = delimiter.join([b\"123\", b\"456\", b\"789\"])\n    f = io.BytesIO(data)\n\n    assert read_block(f, 1, 2) == b\"23\"\n    assert read_block(f, 0, 1, delimiter=b\"\\n\") == b\"123\\n\"\n    assert read_block(f, 0, 2, delimiter=b\"\\n\") == b\"123\\n\"\n    assert read_block(f, 0, 3, delimiter=b\"\\n\") == b\"123\\n\"\n    assert read_block(f, 0, 5, delimiter=b\"\\n\") == b\"123\\n456\\n\"\n    assert read_block(f, 0, 8, delimiter=b\"\\n\") == b\"123\\n456\\n789\"\n    assert read_block(f, 0, 100, delimiter=b\"\\n\") == b\"123\\n456\\n789\"\n    assert read_block(f, 1, 1, delimiter=b\"\\n\") == b\"\"\n    assert read_block(f, 1, 5, delimiter=b\"\\n\") == b\"456\\n\"\n    assert read_block(f, 1, 8, delimiter=b\"\\n\") == b\"456\\n789\"\n\n    for ols in [[(0, 3), (3, 3), (6, 3), (9, 2)], [(0, 4), (4, 4), (8, 4)]]:\n        out = [read_block(f, o, l, b\"\\n\") for o, l in ols]\n        assert b\"\".join(filter(None, out)) == data\n\n\ndef test_read_block_split_before():\n    \"\"\"Test start/middle/end cases of split_before.\"\"\"  # noqa: I\n    d = (\n        \"#header\" + \"\".join(\">foo{i}\\nFOOBAR{i}\\n\".format(i=i) for i in range(100000))\n    ).encode()\n\n    # Read single record at beginning.\n    # All reads include beginning of file and read through termination of\n    # delimited record.\n    assert read_block(io.BytesIO(d), 0, 10, delimiter=b\"\\n\") == b\"#header>foo0\\n\"\n    assert (\n        read_block(io.BytesIO(d), 0, 10, delimiter=b\"\\n\", split_before=True)\n        == b\"#header>foo0\"\n    )\n    assert (\n        read_block(io.BytesIO(d), 0, 10, delimiter=b\">\") == b\"#header>foo0\\nFOOBAR0\\n>\"\n    )\n    assert (\n        read_block(io.BytesIO(d), 0, 10, delimiter=b\">\", split_before=True)\n        == b\"#header>foo0\\nFOOBAR0\\n\"\n    )\n\n    # Read multiple records at beginning.\n    # All reads include beginning of file and read through termination of\n    # delimited record.\n    assert (\n        read_block(io.BytesIO(d), 0, 27, delimiter=b\"\\n\")\n        == b\"#header>foo0\\nFOOBAR0\\n>foo1\\nFOOBAR1\\n\"\n    )\n    assert (\n        read_block(io.BytesIO(d), 0, 27, delimiter=b\"\\n\", split_before=True)\n        == b\"#header>foo0\\nFOOBAR0\\n>foo1\\nFOOBAR1\"\n    )\n    assert (\n        read_block(io.BytesIO(d), 0, 27, delimiter=b\">\")\n        == b\"#header>foo0\\nFOOBAR0\\n>foo1\\nFOOBAR1\\n>\"\n    )\n    assert (\n        read_block(io.BytesIO(d), 0, 27, delimiter=b\">\", split_before=True)\n        == b\"#header>foo0\\nFOOBAR0\\n>foo1\\nFOOBAR1\\n\"\n    )\n\n    # Read with offset spanning into next record, splits on either side of delimiter.\n    # Read not spanning the full record returns nothing.\n    assert read_block(io.BytesIO(d), 10, 3, delimiter=b\"\\n\") == b\"FOOBAR0\\n\"\n    assert (\n        read_block(io.BytesIO(d), 10, 3, delimiter=b\"\\n\", split_before=True)\n        == b\"\\nFOOBAR0\"\n    )\n    assert read_block(io.BytesIO(d), 10, 3, delimiter=b\">\") == b\"\"\n    assert read_block(io.BytesIO(d), 10, 3, delimiter=b\">\", split_before=True) == b\"\"\n\n    # Read with offset spanning multiple records, splits on either side of delimiter\n    assert (\n        read_block(io.BytesIO(d), 10, 20, delimiter=b\"\\n\")\n        == b\"FOOBAR0\\n>foo1\\nFOOBAR1\\n\"\n    )\n    assert (\n        read_block(io.BytesIO(d), 10, 20, delimiter=b\"\\n\", split_before=True)\n        == b\"\\nFOOBAR0\\n>foo1\\nFOOBAR1\"\n    )\n    assert read_block(io.BytesIO(d), 10, 20, delimiter=b\">\") == b\"foo1\\nFOOBAR1\\n>\"\n    assert (\n        read_block(io.BytesIO(d), 10, 20, delimiter=b\">\", split_before=True)\n        == b\">foo1\\nFOOBAR1\\n\"\n    )\n\n    # Read record at end, all records read to end\n\n    tlen = len(d)\n\n    assert (\n        read_block(io.BytesIO(d), tlen - 30, 35, delimiter=b\"\\n\")\n        == b\">foo99999\\nFOOBAR99999\\n\"\n    )\n\n    assert (\n        read_block(io.BytesIO(d), tlen - 30, 35, delimiter=b\"\\n\", split_before=True)\n        == b\"\\n>foo99999\\nFOOBAR99999\\n\"\n    )\n\n    assert (\n        read_block(io.BytesIO(d), tlen - 30, 35, delimiter=b\">\")\n        == b\"foo99999\\nFOOBAR99999\\n\"\n    )\n\n    assert (\n        read_block(io.BytesIO(d), tlen - 30, 35, delimiter=b\">\", split_before=True)\n        == b\">foo99999\\nFOOBAR99999\\n\"\n    )\n\n\ndef test_seek_delimiter_endline():\n    f = io.BytesIO(b\"123\\n456\\n789\")\n\n    # if at zero, stay at zero\n    seek_delimiter(f, b\"\\n\", 5)\n    assert f.tell() == 0\n\n    # choose the first block\n    for bs in [1, 5, 100]:\n        f.seek(1)\n        seek_delimiter(f, b\"\\n\", blocksize=bs)\n        assert f.tell() == 4\n\n    # handle long delimiters well, even with short blocksizes\n    f = io.BytesIO(b\"123abc456abc789\")\n    for bs in [1, 2, 3, 4, 5, 6, 10]:\n        f.seek(1)\n        seek_delimiter(f, b\"abc\", blocksize=bs)\n        assert f.tell() == 6\n\n    # End at the end\n    f = io.BytesIO(b\"123\\n456\")\n    f.seek(5)\n    seek_delimiter(f, b\"\\n\", 5)\n    assert f.tell() == 7\n\n\ndef test_infer_options():\n    so = infer_storage_options(\"/mnt/datasets/test.csv\")\n    assert so.pop(\"protocol\") == \"file\"\n    assert so.pop(\"path\") == \"/mnt/datasets/test.csv\"\n    assert not so\n\n    assert infer_storage_options(\"./test.csv\")[\"path\"] == \"./test.csv\"\n    assert infer_storage_options(\"../test.csv\")[\"path\"] == \"../test.csv\"\n\n    so = infer_storage_options(\"C:\\\\test.csv\")\n    assert so.pop(\"protocol\") == \"file\"\n    assert so.pop(\"path\") == \"C:\\\\test.csv\"\n    assert not so\n\n    assert infer_storage_options(\"d:\\\\test.csv\")[\"path\"] == \"d:\\\\test.csv\"\n    assert infer_storage_options(\"\\\\test.csv\")[\"path\"] == \"\\\\test.csv\"\n    assert infer_storage_options(\".\\\\test.csv\")[\"path\"] == \".\\\\test.csv\"\n    assert infer_storage_options(\"test.csv\")[\"path\"] == \"test.csv\"\n\n    so = infer_storage_options(\n        \"hdfs://username:pwd@Node:123/mnt/datasets/test.csv?q=1#fragm\",\n        inherit_storage_options={\"extra\": \"value\"},\n    )\n    assert so.pop(\"protocol\") == \"hdfs\"\n    assert so.pop(\"username\") == \"username\"\n    assert so.pop(\"password\") == \"pwd\"\n    assert so.pop(\"host\") == \"Node\"\n    assert so.pop(\"port\") == 123\n    assert so.pop(\"path\") == \"/mnt/datasets/test.csv#fragm\"\n    assert so.pop(\"url_query\") == \"q=1\"\n    assert so.pop(\"url_fragment\") == \"fragm\"\n    assert so.pop(\"extra\") == \"value\"\n    assert not so\n\n    so = infer_storage_options(\"hdfs://User-name@Node-name.com/mnt/datasets/test.csv\")\n    assert so.pop(\"username\") == \"User-name\"\n    assert so.pop(\"host\") == \"Node-name.com\"\n\n    u = \"http://127.0.0.1:8080/test.csv\"\n    assert infer_storage_options(u) == {\"protocol\": \"http\", \"path\": u}\n\n    # For s3 and gcs the netloc is actually the bucket name, so we want to\n    # include it in the path. Test that:\n    # - Parsing doesn't lowercase the bucket\n    # - The bucket is included in path\n    for protocol in [\"s3\", \"s3a\", \"gcs\", \"gs\"]:\n        options = infer_storage_options(f\"{protocol}://Bucket-name.com/test.csv\")\n        assert options[\"path\"] == \"Bucket-name.com/test.csv\"\n\n    with pytest.raises(KeyError):\n        infer_storage_options(\"file:///bucket/file.csv\", {\"path\": \"collide\"})\n    with pytest.raises(KeyError):\n        infer_storage_options(\"hdfs:///bucket/file.csv\", {\"protocol\": \"collide\"})\n\n\ndef test_infer_simple():\n    out = infer_storage_options(\"//mnt/datasets/test.csv\")\n    assert out[\"protocol\"] == \"file\"\n    assert out[\"path\"] == \"//mnt/datasets/test.csv\"\n    assert out.get(\"host\", None) is None\n\n\n@pytest.mark.parametrize(\n    \"urlpath, expected_path\",\n    (\n        (r\"c:\\foo\\bar\", r\"c:\\foo\\bar\"),\n        (r\"C:\\\\foo\\bar\", r\"C:\\\\foo\\bar\"),\n        (r\"c:/foo/bar\", r\"c:/foo/bar\"),\n        (r\"file:///c|\\foo\\bar\", r\"c:\\foo\\bar\"),\n        (r\"file:///C|/foo/bar\", r\"C:/foo/bar\"),\n        (r\"file:///C:/foo/bar\", r\"C:/foo/bar\"),\n    ),\n)\ndef test_infer_storage_options_c(urlpath, expected_path):\n    so = infer_storage_options(urlpath)\n    assert so[\"protocol\"] == \"file\"\n    assert so[\"path\"] == expected_path\n\n\n@pytest.mark.parametrize(\n    \"paths, out\",\n    (\n        ([\"/more/dir/\", \"/more/dir/two\", \"/more/one\", \"/more/three\"], \"/more\"),\n        ([\"/\", \"\", \"/\"], \"\"),\n        ([\"/\", \"/\"], \"/\"),\n        ([\"/more/\", \"/\"], \"\"),\n        ([\"/more/\", \"/more\"], \"/more\"),\n        ([\"more/dir/\", \"more/dir/two\", \"more/one\", \"more/three\"], \"more\"),\n    ),\n)\ndef test_common_prefix(paths, out):\n    assert common_prefix(paths) == out\n\n\n@pytest.mark.parametrize(\n    \"paths, other, exists, expected\",\n    (\n        ([\"/path1\"], \"/path2\", False, [\"/path2\"]),\n        ([\"/path1\"], \"/path2\", True, [\"/path2/path1\"]),\n        ([\"/path1\"], \"/path2\", False, [\"/path2\"]),\n        ([\"/path1\"], \"/path2/\", True, [\"/path2/path1\"]),\n        ([\"/path1\"], [\"/path2\"], False, [\"/path2\"]),\n        ([\"/path1\"], [\"/path2\"], True, [\"/path2\"]),\n        ([\"/path1\", \"/path2\"], \"/path2\", False, [\"/path2/path1\", \"/path2/path2\"]),\n        ([\"/path1\", \"/path2\"], \"/path2\", True, [\"/path2/path1\", \"/path2/path2\"]),\n        (\n            [\"/more/path1\", \"/more/path2\"],\n            \"/path2\",\n            False,\n            [\"/path2/path1\", \"/path2/path2\"],\n        ),\n        (\n            [\"/more/path1\", \"/more/path2\"],\n            \"/path2\",\n            True,\n            [\"/path2/more/path1\", \"/path2/more/path2\"],\n        ),\n        (\n            [\"/more/path1\", \"/more/path2\"],\n            \"/path2\",\n            False,\n            [\"/path2/path1\", \"/path2/path2\"],\n        ),\n        (\n            [\"/more/path1\", \"/more/path2\"],\n            \"/path2\",\n            True,\n            [\"/path2/more/path1\", \"/path2/more/path2\"],\n        ),\n        (\n            [\"/more/path1\", \"/more/path2\"],\n            \"/path2/\",\n            False,\n            [\"/path2/path1\", \"/path2/path2\"],\n        ),\n        (\n            [\"/more/path1\", \"/more/path2\"],\n            \"/path2/\",\n            True,\n            [\"/path2/more/path1\", \"/path2/more/path2\"],\n        ),\n        (\n            [\"/more/path1\", \"/diff/path2\"],\n            \"/path2/\",\n            False,\n            [\"/path2/more/path1\", \"/path2/diff/path2\"],\n        ),\n        (\n            [\"/more/path1\", \"/diff/path2\"],\n            \"/path2/\",\n            True,\n            [\"/path2/more/path1\", \"/path2/diff/path2\"],\n        ),\n        ([\"a\", \"b/\", \"b/c\"], \"dest/\", False, [\"dest/a\", \"dest/b/\", \"dest/b/c\"]),\n        (\n            [\"/a\", \"/b/\", \"/b/c\"],\n            \"dest/\",\n            False,\n            [\"dest/a\", \"dest/b/\", \"dest/b/c\"],\n        ),\n    ),\n)\ndef test_other_paths(paths, other, exists, expected):\n    assert other_paths(paths, other, exists) == expected\n\n\ndef test_log():\n    import logging\n\n    logger = setup_logging(logger_name=\"fsspec.test\")\n    assert logger.level == logging.DEBUG\n\n\n@pytest.mark.parametrize(\n    \"par\",\n    [\n        (\"afile\", \"file\"),\n        (\"file://afile\", \"file\"),\n        (\"noproto://afile\", \"noproto\"),\n        (\"noproto::stuff\", \"noproto\"),\n        (\"simplecache::stuff\", \"simplecache\"),\n        (\"simplecache://stuff\", \"simplecache\"),\n        (\"s3://afile\", \"s3\"),\n        (Path(\"afile\"), \"file\"),\n    ],\n)\ndef test_get_protocol(par):\n    url, outcome = par\n    assert get_protocol(url) == outcome\n\n\n@pytest.mark.parametrize(\n    \"par\",\n    [\n        (\"afile\", True),\n        (\"file://afile\", True),\n        (\"noproto://afile\", False),\n        (\"noproto::stuff\", False),\n        (\"simplecache::stuff\", True),\n        (\"simplecache://stuff\", True),\n        (Path(\"afile\"), True),\n    ],\n)\ndef test_can_local(par):\n    url, outcome = par\n    assert can_be_local(url) == outcome\n\n\ndef test_mirror_from():\n    mock = Mock()\n    mock.attr = 1\n\n    @mirror_from(\"client\", [\"attr\", \"func_1\", \"func_2\"])\n    class Real:\n        @property\n        def client(self):\n            return mock\n\n        def func_2(self):\n            raise AssertionError(\"have to overwrite this\")\n\n        def func_3(self):\n            return \"should succeed\"\n\n    obj = Real()\n    assert obj.attr == mock.attr\n\n    obj.func_1()\n    mock.func_1.assert_called()\n\n    obj.func_2(1, 2)\n    mock.func_2.assert_called_with(1, 2)\n\n    assert obj.func_3() == \"should succeed\"\n    mock.func_3.assert_not_called()\n\n\n@pytest.mark.parametrize(\"max_gap\", [0, 32])\n@pytest.mark.parametrize(\"max_block\", [None, 128])\ndef test_merge_offset_ranges(max_gap, max_block):\n    # Input ranges\n    # (Using out-of-order ranges for full coverage)\n    paths = [\"foo\", \"bar\", \"bar\", \"bar\", \"foo\"]\n    starts = [0, 0, 512, 64, 32]\n    ends = [32, 32, 1024, 256, 64]\n\n    # Call merge_offset_ranges\n    (\n        result_paths,\n        result_starts,\n        result_ends,\n    ) = merge_offset_ranges(\n        paths,\n        starts,\n        ends,\n        max_gap=max_gap,\n        max_block=max_block,\n    )\n\n    # Check result\n    if max_block is None and max_gap == 32:\n        expect_paths = [\"bar\", \"bar\", \"foo\"]\n        expect_starts = [0, 512, 0]\n        expect_ends = [256, 1024, 64]\n    else:\n        expect_paths = [\"bar\", \"bar\", \"bar\", \"foo\"]\n        expect_starts = [0, 64, 512, 0]\n        expect_ends = [32, 256, 1024, 64]\n\n    assert expect_paths == result_paths\n    assert expect_starts == result_starts\n    assert expect_ends == result_ends\n\n\ndef test_size():\n    f = io.BytesIO(b\"hello\")\n    assert fsspec.utils.file_size(f) == 5\n    assert f.tell() == 0\n\n\nclass _HasFspath:\n    def __fspath__(self):\n        return \"foo\"\n\n\nclass _HasPathAttr:\n    def __init__(self):\n        self.path = \"foo\"\n\n\n@pytest.mark.parametrize(\n    \"path,expected\",\n    [\n        # coerce to string\n        (\"foo\", \"foo\"),\n        (Path(\"foo\"), \"foo\"),\n        (PurePath(\"foo\"), \"foo\"),\n        (_HasFspath(), \"foo\"),\n        (_HasPathAttr(), \"foo\"),\n        # passthrough\n        (b\"bytes\", b\"bytes\"),\n        (None, None),\n        (1, 1),\n        (True, True),\n        (o := object(), o),\n        ([], []),\n        ((), ()),\n        (set(), set()),\n    ],\n)\ndef test_stringify_path(path, expected):\n    path = fsspec.utils.stringify_path(path)\n\n    assert path == expected\n", "fsspec/tests/test_mapping.py": "import os\nimport pickle\nimport platform\nimport sys\nimport uuid\n\nimport pytest\n\nimport fsspec\nfrom fsspec.implementations.local import LocalFileSystem\nfrom fsspec.implementations.memory import MemoryFileSystem\n\n\ndef test_mapping_prefix(tmpdir):\n    tmpdir = str(tmpdir)\n    os.makedirs(os.path.join(tmpdir, \"afolder\"))\n    open(os.path.join(tmpdir, \"afile\"), \"w\").write(\"test\")\n    open(os.path.join(tmpdir, \"afolder\", \"anotherfile\"), \"w\").write(\"test2\")\n\n    m = fsspec.get_mapper(f\"file://{tmpdir}\")\n    assert \"afile\" in m\n    assert m[\"afolder/anotherfile\"] == b\"test2\"\n\n    fs = fsspec.filesystem(\"file\")\n    m2 = fs.get_mapper(tmpdir)\n    m3 = fs.get_mapper(f\"file://{tmpdir}\")\n\n    assert m == m2 == m3\n\n\ndef test_getitems_errors(tmpdir):\n    tmpdir = str(tmpdir)\n    os.makedirs(os.path.join(tmpdir, \"afolder\"))\n    open(os.path.join(tmpdir, \"afile\"), \"w\").write(\"test\")\n    open(os.path.join(tmpdir, \"afolder\", \"anotherfile\"), \"w\").write(\"test2\")\n    m = fsspec.get_mapper(f\"file://{tmpdir}\")\n    assert m.getitems([\"afile\", \"bfile\"], on_error=\"omit\") == {\"afile\": b\"test\"}\n    with pytest.raises(KeyError):\n        m.getitems([\"afile\", \"bfile\"])\n    out = m.getitems([\"afile\", \"bfile\"], on_error=\"return\")\n    assert isinstance(out[\"bfile\"], KeyError)\n    m = fsspec.get_mapper(f\"file://{tmpdir}\", missing_exceptions=())\n    assert m.getitems([\"afile\", \"bfile\"], on_error=\"omit\") == {\"afile\": b\"test\"}\n    with pytest.raises(FileNotFoundError):\n        m.getitems([\"afile\", \"bfile\"])\n\n\ndef test_ops():\n    MemoryFileSystem.store.clear()\n    m = fsspec.get_mapper(\"memory://\")\n    assert not m\n    assert list(m) == []\n\n    with pytest.raises(KeyError):\n        m[\"hi\"]\n\n    assert m.pop(\"key\", 0) == 0\n\n    m[\"key0\"] = b\"data\"\n    assert list(m) == [\"key0\"]\n    assert m[\"key0\"] == b\"data\"\n\n    m.clear()\n\n    assert list(m) == []\n\n\ndef test_pickle():\n    m = fsspec.get_mapper(\"memory://\")\n    assert isinstance(m.fs, MemoryFileSystem)\n    m[\"key\"] = b\"data\"\n    m2 = pickle.loads(pickle.dumps(m))\n    assert list(m) == list(m2)\n    assert m.missing_exceptions == m2.missing_exceptions\n\n\ndef test_keys_view():\n    # https://github.com/fsspec/filesystem_spec/issues/186\n    m = fsspec.get_mapper(\"memory://\")\n    m[\"key\"] = b\"data\"\n\n    keys = m.keys()\n    assert len(keys) == 1\n    # check that we don't consume the keys\n    assert len(keys) == 1\n    m.clear()\n\n\ndef test_multi():\n    m = fsspec.get_mapper(\"memory:///\")\n    data = {\"a\": b\"data1\", \"b\": b\"data2\"}\n    m.setitems(data)\n\n    assert m.getitems(list(data)) == data\n    m.delitems(list(data))\n    assert not list(m)\n\n\ndef test_setitem_types():\n    import array\n\n    m = fsspec.get_mapper(\"memory://\")\n    m[\"a\"] = array.array(\"i\", [1])\n    if sys.byteorder == \"little\":\n        assert m[\"a\"] == b\"\\x01\\x00\\x00\\x00\"\n    else:\n        assert m[\"a\"] == b\"\\x00\\x00\\x00\\x01\"\n    m[\"b\"] = bytearray(b\"123\")\n    assert m[\"b\"] == b\"123\"\n    m.setitems({\"c\": array.array(\"i\", [1]), \"d\": bytearray(b\"123\")})\n    if sys.byteorder == \"little\":\n        assert m[\"c\"] == b\"\\x01\\x00\\x00\\x00\"\n    else:\n        assert m[\"c\"] == b\"\\x00\\x00\\x00\\x01\"\n    assert m[\"d\"] == b\"123\"\n\n\ndef test_setitem_numpy():\n    m = fsspec.get_mapper(\"memory://\")\n    np = pytest.importorskip(\"numpy\")\n    m[\"c\"] = np.array(1, dtype=\"<i4\")  # scalar\n    assert m[\"c\"] == b\"\\x01\\x00\\x00\\x00\"\n    m[\"c\"] = np.array([1, 2], dtype=\"<i4\")  # array\n    assert m[\"c\"] == b\"\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\"\n    m[\"c\"] = np.array(\n        np.datetime64(\"2000-01-01T23:59:59.999999999\"), dtype=\"<M8[ns]\"\n    )  # datetime64 scalar\n    assert m[\"c\"] == b\"\\xff\\xff\\x91\\xe3c\\x9b#\\r\"\n    m[\"c\"] = np.array(\n        [\n            np.datetime64(\"1900-01-01T23:59:59.999999999\"),\n            np.datetime64(\"2000-01-01T23:59:59.999999999\"),\n        ],\n        dtype=\"<M8[ns]\",\n    )  # datetime64 array\n    assert m[\"c\"] == b\"\\xff\\xff}p\\xf8fX\\xe1\\xff\\xff\\x91\\xe3c\\x9b#\\r\"\n    m[\"c\"] = np.array(\n        np.timedelta64(3155673612345678901, \"ns\"), dtype=\"<m8[ns]\"\n    )  # timedelta64 scalar\n    assert m[\"c\"] == b\"5\\x1c\\xf0Rn4\\xcb+\"\n    m[\"c\"] = np.array(\n        [\n            np.timedelta64(450810516049382700, \"ns\"),\n            np.timedelta64(3155673612345678901, \"ns\"),\n        ],\n        dtype=\"<m8[ns]\",\n    )  # timedelta64 scalar\n    assert m[\"c\"] == b',M\"\\x9e\\xc6\\x99A\\x065\\x1c\\xf0Rn4\\xcb+'\n\n\ndef test_empty_url():\n    m = fsspec.get_mapper()\n    assert isinstance(m.fs, LocalFileSystem)\n\n\ndef test_fsmap_access_with_root_prefix(tmp_path):\n    # \"/a\" and \"a\" are the same for LocalFileSystem\n    tmp_path.joinpath(\"a\").write_bytes(b\"data\")\n    m = fsspec.get_mapper(f\"file://{tmp_path}\")\n    assert m[\"/a\"] == m[\"a\"] == b\"data\"\n\n    # \"/a\" and \"a\" differ for MemoryFileSystem\n    m = fsspec.get_mapper(f\"memory://{uuid.uuid4()}\")\n    m[\"/a\"] = b\"data\"\n\n    assert m[\"/a\"] == b\"data\"\n    with pytest.raises(KeyError):\n        _ = m[\"a\"]\n\n\n@pytest.mark.parametrize(\n    \"key\",\n    [\n        pytest.param(b\"k\", id=\"bytes\"),\n        pytest.param(1234, id=\"int\"),\n        pytest.param((1,), id=\"tuple\"),\n        pytest.param([\"\"], id=\"list\"),\n    ],\n)\ndef test_fsmap_non_str_keys(key):\n    m = fsspec.get_mapper()\n\n    # Once the deprecation period passes\n    # FSMap.__getitem__ should raise TypeError for non-str keys\n    #   with pytest.raises(TypeError):\n    #       _ = m[key]\n\n    with pytest.warns(DeprecationWarning):\n        with pytest.raises(KeyError):\n            _ = m[key]\n\n\ndef test_fsmap_error_on_protocol_keys():\n    root = uuid.uuid4()\n    m = fsspec.get_mapper(f\"memory://{root}\", create=True)\n    m[\"a\"] = b\"data\"\n\n    assert m[\"a\"] == b\"data\"\n    with pytest.raises(KeyError):\n        _ = m[f\"memory://{root}/a\"]\n\n\ndef test_fsmap_access_with_suffix(tmp_path):\n    tmp_path.joinpath(\"b\").mkdir()\n    tmp_path.joinpath(\"b\", \"a\").write_bytes(b\"data\")\n    if platform.system() == \"Windows\":\n        # on Windows opening a directory will raise PermissionError\n        # see: https://bugs.python.org/issue43095\n        missing_exceptions = (\n            FileNotFoundError,\n            IsADirectoryError,\n            NotADirectoryError,\n            PermissionError,\n        )\n    else:\n        missing_exceptions = None\n    m = fsspec.get_mapper(f\"file://{tmp_path}\", missing_exceptions=missing_exceptions)\n    with pytest.raises(KeyError):\n        _ = m[\"b/\"]\n    assert m[\"b/a/\"] == b\"data\"\n\n\ndef test_fsmap_dirfs():\n    m = fsspec.get_mapper(\"memory://\")\n\n    fs = m.dirfs\n    assert isinstance(fs, fsspec.implementations.dirfs.DirFileSystem)\n    assert fs.path == m.root\n", "fsspec/tests/abstract/common.py": "GLOB_EDGE_CASES_TESTS = {\n    \"argnames\": (\"path\", \"recursive\", \"maxdepth\", \"expected\"),\n    \"argvalues\": [\n        (\"fil?1\", False, None, [\"file1\"]),\n        (\"fil?1\", True, None, [\"file1\"]),\n        (\"file[1-2]\", False, None, [\"file1\", \"file2\"]),\n        (\"file[1-2]\", True, None, [\"file1\", \"file2\"]),\n        (\"*\", False, None, [\"file1\", \"file2\"]),\n        (\n            \"*\",\n            True,\n            None,\n            [\n                \"file1\",\n                \"file2\",\n                \"subdir0/subfile1\",\n                \"subdir0/subfile2\",\n                \"subdir0/nesteddir/nestedfile\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n                \"subdir1/nesteddir/nestedfile\",\n            ],\n        ),\n        (\"*\", True, 1, [\"file1\", \"file2\"]),\n        (\n            \"*\",\n            True,\n            2,\n            [\n                \"file1\",\n                \"file2\",\n                \"subdir0/subfile1\",\n                \"subdir0/subfile2\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n            ],\n        ),\n        (\"*1\", False, None, [\"file1\"]),\n        (\n            \"*1\",\n            True,\n            None,\n            [\n                \"file1\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n                \"subdir1/nesteddir/nestedfile\",\n            ],\n        ),\n        (\"*1\", True, 2, [\"file1\", \"subdir1/subfile1\", \"subdir1/subfile2\"]),\n        (\n            \"**\",\n            False,\n            None,\n            [\n                \"file1\",\n                \"file2\",\n                \"subdir0/subfile1\",\n                \"subdir0/subfile2\",\n                \"subdir0/nesteddir/nestedfile\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n                \"subdir1/nesteddir/nestedfile\",\n            ],\n        ),\n        (\n            \"**\",\n            True,\n            None,\n            [\n                \"file1\",\n                \"file2\",\n                \"subdir0/subfile1\",\n                \"subdir0/subfile2\",\n                \"subdir0/nesteddir/nestedfile\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n                \"subdir1/nesteddir/nestedfile\",\n            ],\n        ),\n        (\"**\", True, 1, [\"file1\", \"file2\"]),\n        (\n            \"**\",\n            True,\n            2,\n            [\n                \"file1\",\n                \"file2\",\n                \"subdir0/subfile1\",\n                \"subdir0/subfile2\",\n                \"subdir0/nesteddir/nestedfile\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n                \"subdir1/nesteddir/nestedfile\",\n            ],\n        ),\n        (\n            \"**\",\n            False,\n            2,\n            [\n                \"file1\",\n                \"file2\",\n                \"subdir0/subfile1\",\n                \"subdir0/subfile2\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n            ],\n        ),\n        (\"**/*1\", False, None, [\"file1\", \"subdir0/subfile1\", \"subdir1/subfile1\"]),\n        (\n            \"**/*1\",\n            True,\n            None,\n            [\n                \"file1\",\n                \"subdir0/subfile1\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n                \"subdir1/nesteddir/nestedfile\",\n            ],\n        ),\n        (\"**/*1\", True, 1, [\"file1\"]),\n        (\n            \"**/*1\",\n            True,\n            2,\n            [\"file1\", \"subdir0/subfile1\", \"subdir1/subfile1\", \"subdir1/subfile2\"],\n        ),\n        (\"**/*1\", False, 2, [\"file1\", \"subdir0/subfile1\", \"subdir1/subfile1\"]),\n        (\"**/subdir0\", False, None, []),\n        (\"**/subdir0\", True, None, [\"subfile1\", \"subfile2\", \"nesteddir/nestedfile\"]),\n        (\"**/subdir0/nested*\", False, 2, []),\n        (\"**/subdir0/nested*\", True, 2, [\"nestedfile\"]),\n        (\"subdir[1-2]\", False, None, []),\n        (\"subdir[1-2]\", True, None, [\"subfile1\", \"subfile2\", \"nesteddir/nestedfile\"]),\n        (\"subdir[1-2]\", True, 2, [\"subfile1\", \"subfile2\"]),\n        (\"subdir[0-1]\", False, None, []),\n        (\n            \"subdir[0-1]\",\n            True,\n            None,\n            [\n                \"subdir0/subfile1\",\n                \"subdir0/subfile2\",\n                \"subdir0/nesteddir/nestedfile\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n                \"subdir1/nesteddir/nestedfile\",\n            ],\n        ),\n        (\n            \"subdir[0-1]/*fil[e]*\",\n            False,\n            None,\n            [\n                \"subdir0/subfile1\",\n                \"subdir0/subfile2\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n            ],\n        ),\n        (\n            \"subdir[0-1]/*fil[e]*\",\n            True,\n            None,\n            [\n                \"subdir0/subfile1\",\n                \"subdir0/subfile2\",\n                \"subdir1/subfile1\",\n                \"subdir1/subfile2\",\n            ],\n        ),\n    ],\n}\n", "fsspec/tests/abstract/mv.py": "import os\n\nimport pytest\n\nimport fsspec\n\n\ndef test_move_raises_error_with_tmpdir(tmpdir):\n    # Create a file in the temporary directory\n    source = tmpdir.join(\"source_file.txt\")\n    source.write(\"content\")\n\n    # Define a destination that simulates a protected or invalid path\n    destination = tmpdir.join(\"non_existent_directory/destination_file.txt\")\n\n    # Instantiate the filesystem (assuming the local file system interface)\n    fs = fsspec.filesystem(\"file\")\n\n    # Use the actual file paths as string\n    with pytest.raises(FileNotFoundError):\n        fs.mv(str(source), str(destination))\n\n\n@pytest.mark.parametrize(\"recursive\", (True, False))\ndef test_move_raises_error_with_tmpdir_permission(recursive, tmpdir):\n    # Create a file in the temporary directory\n    source = tmpdir.join(\"source_file.txt\")\n    source.write(\"content\")\n\n    # Create a protected directory (non-writable)\n    protected_dir = tmpdir.mkdir(\"protected_directory\")\n    protected_path = str(protected_dir)\n\n    # Set the directory to read-only\n    if os.name == \"nt\":\n        os.system(f'icacls \"{protected_path}\" /deny Everyone:(W)')\n    else:\n        os.chmod(protected_path, 0o555)  # Sets the directory to read-only\n\n    # Define a destination inside the protected directory\n    destination = protected_dir.join(\"destination_file.txt\")\n\n    # Instantiate the filesystem (assuming the local file system interface)\n    fs = fsspec.filesystem(\"file\")\n\n    # Try to move the file to the read-only directory, expecting a permission error\n    with pytest.raises(PermissionError):\n        fs.mv(str(source), str(destination), recursive=recursive)\n\n    # Assert the file was not created in the destination\n    assert not os.path.exists(destination)\n\n    # Cleanup: Restore permissions so the directory can be cleaned up\n    if os.name == \"nt\":\n        os.system(f'icacls \"{protected_path}\" /remove:d Everyone')\n    else:\n        os.chmod(protected_path, 0o755)  # Restore write permission for cleanup\n", "fsspec/tests/abstract/get.py": "from hashlib import md5\nfrom itertools import product\n\nimport pytest\n\nfrom fsspec.implementations.local import make_path_posix\nfrom fsspec.tests.abstract.common import GLOB_EDGE_CASES_TESTS\n\n\nclass AbstractGetTests:\n    def test_get_file_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 1a\n        source = fs_bulk_operations_scenario_0\n\n        target = local_target\n        local_fs.mkdir(target)\n        assert local_fs.isdir(target)\n\n        target_file2 = local_join(target, \"file2\")\n        target_subfile1 = local_join(target, \"subfile1\")\n\n        # Copy from source directory\n        fs.get(fs_join(source, \"file2\"), target)\n        assert local_fs.isfile(target_file2)\n\n        # Copy from sub directory\n        fs.get(fs_join(source, \"subdir\", \"subfile1\"), target)\n        assert local_fs.isfile(target_subfile1)\n\n        # Remove copied files\n        local_fs.rm([target_file2, target_subfile1])\n        assert not local_fs.exists(target_file2)\n        assert not local_fs.exists(target_subfile1)\n\n        # Repeat with trailing slash on target\n        fs.get(fs_join(source, \"file2\"), target + \"/\")\n        assert local_fs.isdir(target)\n        assert local_fs.isfile(target_file2)\n\n        fs.get(fs_join(source, \"subdir\", \"subfile1\"), target + \"/\")\n        assert local_fs.isfile(target_subfile1)\n\n    def test_get_file_to_new_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 1b\n        source = fs_bulk_operations_scenario_0\n\n        target = local_target\n        local_fs.mkdir(target)\n\n        fs.get(\n            fs_join(source, \"subdir\", \"subfile1\"), local_join(target, \"newdir/\")\n        )  # Note trailing slash\n\n        assert local_fs.isdir(target)\n        assert local_fs.isdir(local_join(target, \"newdir\"))\n        assert local_fs.isfile(local_join(target, \"newdir\", \"subfile1\"))\n\n    def test_get_file_to_file_in_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 1c\n        source = fs_bulk_operations_scenario_0\n\n        target = local_target\n        local_fs.mkdir(target)\n\n        fs.get(fs_join(source, \"subdir\", \"subfile1\"), local_join(target, \"newfile\"))\n        assert local_fs.isfile(local_join(target, \"newfile\"))\n\n    def test_get_file_to_file_in_new_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 1d\n        source = fs_bulk_operations_scenario_0\n\n        target = local_target\n        local_fs.mkdir(target)\n\n        fs.get(\n            fs_join(source, \"subdir\", \"subfile1\"),\n            local_join(target, \"newdir\", \"newfile\"),\n        )\n        assert local_fs.isdir(local_join(target, \"newdir\"))\n        assert local_fs.isfile(local_join(target, \"newdir\", \"newfile\"))\n\n    def test_get_directory_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 1e\n        source = fs_bulk_operations_scenario_0\n\n        target = local_target\n        local_fs.mkdir(target)\n        assert local_fs.isdir(target)\n\n        for source_slash, target_slash in zip([False, True], [False, True]):\n            s = fs_join(source, \"subdir\")\n            if source_slash:\n                s += \"/\"\n            t = target + \"/\" if target_slash else target\n\n            # Without recursive does nothing\n            fs.get(s, t)\n            assert local_fs.ls(target) == []\n\n            # With recursive\n            fs.get(s, t, recursive=True)\n            if source_slash:\n                assert local_fs.isfile(local_join(target, \"subfile1\"))\n                assert local_fs.isfile(local_join(target, \"subfile2\"))\n                assert local_fs.isdir(local_join(target, \"nesteddir\"))\n                assert local_fs.isfile(local_join(target, \"nesteddir\", \"nestedfile\"))\n                assert not local_fs.exists(local_join(target, \"subdir\"))\n\n                local_fs.rm(\n                    [\n                        local_join(target, \"subfile1\"),\n                        local_join(target, \"subfile2\"),\n                        local_join(target, \"nesteddir\"),\n                    ],\n                    recursive=True,\n                )\n            else:\n                assert local_fs.isdir(local_join(target, \"subdir\"))\n                assert local_fs.isfile(local_join(target, \"subdir\", \"subfile1\"))\n                assert local_fs.isfile(local_join(target, \"subdir\", \"subfile2\"))\n                assert local_fs.isdir(local_join(target, \"subdir\", \"nesteddir\"))\n                assert local_fs.isfile(\n                    local_join(target, \"subdir\", \"nesteddir\", \"nestedfile\")\n                )\n\n                local_fs.rm(local_join(target, \"subdir\"), recursive=True)\n            assert local_fs.ls(target) == []\n\n            # Limit recursive by maxdepth\n            fs.get(s, t, recursive=True, maxdepth=1)\n            if source_slash:\n                assert local_fs.isfile(local_join(target, \"subfile1\"))\n                assert local_fs.isfile(local_join(target, \"subfile2\"))\n                assert not local_fs.exists(local_join(target, \"nesteddir\"))\n                assert not local_fs.exists(local_join(target, \"subdir\"))\n\n                local_fs.rm(\n                    [\n                        local_join(target, \"subfile1\"),\n                        local_join(target, \"subfile2\"),\n                    ],\n                    recursive=True,\n                )\n            else:\n                assert local_fs.isdir(local_join(target, \"subdir\"))\n                assert local_fs.isfile(local_join(target, \"subdir\", \"subfile1\"))\n                assert local_fs.isfile(local_join(target, \"subdir\", \"subfile2\"))\n                assert not local_fs.exists(local_join(target, \"subdir\", \"nesteddir\"))\n\n                local_fs.rm(local_join(target, \"subdir\"), recursive=True)\n            assert local_fs.ls(target) == []\n\n    def test_get_directory_to_new_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 1f\n        source = fs_bulk_operations_scenario_0\n\n        target = local_target\n        local_fs.mkdir(target)\n\n        for source_slash, target_slash in zip([False, True], [False, True]):\n            s = fs_join(source, \"subdir\")\n            if source_slash:\n                s += \"/\"\n            t = local_join(target, \"newdir\")\n            if target_slash:\n                t += \"/\"\n\n            # Without recursive does nothing\n            fs.get(s, t)\n            assert local_fs.ls(target) == []\n\n            # With recursive\n            fs.get(s, t, recursive=True)\n            assert local_fs.isdir(local_join(target, \"newdir\"))\n            assert local_fs.isfile(local_join(target, \"newdir\", \"subfile1\"))\n            assert local_fs.isfile(local_join(target, \"newdir\", \"subfile2\"))\n            assert local_fs.isdir(local_join(target, \"newdir\", \"nesteddir\"))\n            assert local_fs.isfile(\n                local_join(target, \"newdir\", \"nesteddir\", \"nestedfile\")\n            )\n            assert not local_fs.exists(local_join(target, \"subdir\"))\n\n            local_fs.rm(local_join(target, \"newdir\"), recursive=True)\n            assert local_fs.ls(target) == []\n\n            # Limit recursive by maxdepth\n            fs.get(s, t, recursive=True, maxdepth=1)\n            assert local_fs.isdir(local_join(target, \"newdir\"))\n            assert local_fs.isfile(local_join(target, \"newdir\", \"subfile1\"))\n            assert local_fs.isfile(local_join(target, \"newdir\", \"subfile2\"))\n            assert not local_fs.exists(local_join(target, \"newdir\", \"nesteddir\"))\n            assert not local_fs.exists(local_join(target, \"subdir\"))\n\n            local_fs.rm(local_join(target, \"newdir\"), recursive=True)\n            assert not local_fs.exists(local_join(target, \"newdir\"))\n\n    def test_get_glob_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 1g\n        source = fs_bulk_operations_scenario_0\n\n        target = local_target\n        local_fs.mkdir(target)\n\n        for target_slash in [False, True]:\n            t = target + \"/\" if target_slash else target\n\n            # Without recursive\n            fs.get(fs_join(source, \"subdir\", \"*\"), t)\n            assert local_fs.isfile(local_join(target, \"subfile1\"))\n            assert local_fs.isfile(local_join(target, \"subfile2\"))\n            assert not local_fs.isdir(local_join(target, \"nesteddir\"))\n            assert not local_fs.exists(local_join(target, \"nesteddir\", \"nestedfile\"))\n            assert not local_fs.exists(local_join(target, \"subdir\"))\n\n            local_fs.rm(\n                [\n                    local_join(target, \"subfile1\"),\n                    local_join(target, \"subfile2\"),\n                ],\n                recursive=True,\n            )\n            assert local_fs.ls(target) == []\n\n            # With recursive\n            for glob, recursive in zip([\"*\", \"**\"], [True, False]):\n                fs.get(fs_join(source, \"subdir\", glob), t, recursive=recursive)\n                assert local_fs.isfile(local_join(target, \"subfile1\"))\n                assert local_fs.isfile(local_join(target, \"subfile2\"))\n                assert local_fs.isdir(local_join(target, \"nesteddir\"))\n                assert local_fs.isfile(local_join(target, \"nesteddir\", \"nestedfile\"))\n                assert not local_fs.exists(local_join(target, \"subdir\"))\n\n                local_fs.rm(\n                    [\n                        local_join(target, \"subfile1\"),\n                        local_join(target, \"subfile2\"),\n                        local_join(target, \"nesteddir\"),\n                    ],\n                    recursive=True,\n                )\n                assert local_fs.ls(target) == []\n\n                # Limit recursive by maxdepth\n                fs.get(\n                    fs_join(source, \"subdir\", glob), t, recursive=recursive, maxdepth=1\n                )\n                assert local_fs.isfile(local_join(target, \"subfile1\"))\n                assert local_fs.isfile(local_join(target, \"subfile2\"))\n                assert not local_fs.exists(local_join(target, \"nesteddir\"))\n                assert not local_fs.exists(local_join(target, \"subdir\"))\n\n                local_fs.rm(\n                    [\n                        local_join(target, \"subfile1\"),\n                        local_join(target, \"subfile2\"),\n                    ],\n                    recursive=True,\n                )\n                assert local_fs.ls(target) == []\n\n    def test_get_glob_to_new_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 1h\n        source = fs_bulk_operations_scenario_0\n\n        target = local_target\n        local_fs.mkdir(target)\n\n        for target_slash in [False, True]:\n            t = fs_join(target, \"newdir\")\n            if target_slash:\n                t += \"/\"\n\n            # Without recursive\n            fs.get(fs_join(source, \"subdir\", \"*\"), t)\n            assert local_fs.isdir(local_join(target, \"newdir\"))\n            assert local_fs.isfile(local_join(target, \"newdir\", \"subfile1\"))\n            assert local_fs.isfile(local_join(target, \"newdir\", \"subfile2\"))\n            assert not local_fs.exists(local_join(target, \"newdir\", \"nesteddir\"))\n            assert not local_fs.exists(\n                local_join(target, \"newdir\", \"nesteddir\", \"nestedfile\")\n            )\n            assert not local_fs.exists(local_join(target, \"subdir\"))\n            assert not local_fs.exists(local_join(target, \"newdir\", \"subdir\"))\n\n            local_fs.rm(local_join(target, \"newdir\"), recursive=True)\n            assert local_fs.ls(target) == []\n\n            # With recursive\n            for glob, recursive in zip([\"*\", \"**\"], [True, False]):\n                fs.get(fs_join(source, \"subdir\", glob), t, recursive=recursive)\n                assert local_fs.isdir(local_join(target, \"newdir\"))\n                assert local_fs.isfile(local_join(target, \"newdir\", \"subfile1\"))\n                assert local_fs.isfile(local_join(target, \"newdir\", \"subfile2\"))\n                assert local_fs.isdir(local_join(target, \"newdir\", \"nesteddir\"))\n                assert local_fs.isfile(\n                    local_join(target, \"newdir\", \"nesteddir\", \"nestedfile\")\n                )\n                assert not local_fs.exists(local_join(target, \"subdir\"))\n                assert not local_fs.exists(local_join(target, \"newdir\", \"subdir\"))\n\n                local_fs.rm(local_join(target, \"newdir\"), recursive=True)\n                assert not local_fs.exists(local_join(target, \"newdir\"))\n\n                # Limit recursive by maxdepth\n                fs.get(\n                    fs_join(source, \"subdir\", glob), t, recursive=recursive, maxdepth=1\n                )\n                assert local_fs.isdir(local_join(target, \"newdir\"))\n                assert local_fs.isfile(local_join(target, \"newdir\", \"subfile1\"))\n                assert local_fs.isfile(local_join(target, \"newdir\", \"subfile2\"))\n                assert not local_fs.exists(local_join(target, \"newdir\", \"nesteddir\"))\n                assert not local_fs.exists(local_join(target, \"subdir\"))\n                assert not local_fs.exists(local_join(target, \"newdir\", \"subdir\"))\n\n                local_fs.rm(local_fs.ls(target, detail=False), recursive=True)\n                assert not local_fs.exists(local_join(target, \"newdir\"))\n\n    @pytest.mark.parametrize(\n        GLOB_EDGE_CASES_TESTS[\"argnames\"],\n        GLOB_EDGE_CASES_TESTS[\"argvalues\"],\n    )\n    def test_get_glob_edge_cases(\n        self,\n        path,\n        recursive,\n        maxdepth,\n        expected,\n        fs,\n        fs_join,\n        fs_glob_edge_cases_files,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 1g\n        source = fs_glob_edge_cases_files\n\n        target = local_target\n\n        for new_dir, target_slash in product([True, False], [True, False]):\n            local_fs.mkdir(target)\n\n            t = local_join(target, \"newdir\") if new_dir else target\n            t = t + \"/\" if target_slash else t\n\n            fs.get(fs_join(source, path), t, recursive=recursive, maxdepth=maxdepth)\n\n            output = local_fs.find(target)\n            if new_dir:\n                prefixed_expected = [\n                    make_path_posix(local_join(target, \"newdir\", p)) for p in expected\n                ]\n            else:\n                prefixed_expected = [\n                    make_path_posix(local_join(target, p)) for p in expected\n                ]\n            assert sorted(output) == sorted(prefixed_expected)\n\n            try:\n                local_fs.rm(target, recursive=True)\n            except FileNotFoundError:\n                pass\n\n    def test_get_list_of_files_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 2a\n        source = fs_bulk_operations_scenario_0\n\n        target = local_target\n        local_fs.mkdir(target)\n\n        source_files = [\n            fs_join(source, \"file1\"),\n            fs_join(source, \"file2\"),\n            fs_join(source, \"subdir\", \"subfile1\"),\n        ]\n\n        for target_slash in [False, True]:\n            t = target + \"/\" if target_slash else target\n\n            fs.get(source_files, t)\n            assert local_fs.isfile(local_join(target, \"file1\"))\n            assert local_fs.isfile(local_join(target, \"file2\"))\n            assert local_fs.isfile(local_join(target, \"subfile1\"))\n\n            local_fs.rm(\n                [\n                    local_join(target, \"file1\"),\n                    local_join(target, \"file2\"),\n                    local_join(target, \"subfile1\"),\n                ],\n                recursive=True,\n            )\n            assert local_fs.ls(target) == []\n\n    def test_get_list_of_files_to_new_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        local_fs,\n        local_join,\n        local_target,\n    ):\n        # Copy scenario 2b\n        source = fs_bulk_operations_scenario_0\n\n        target = local_target\n        local_fs.mkdir(target)\n\n        source_files = [\n            fs_join(source, \"file1\"),\n            fs_join(source, \"file2\"),\n            fs_join(source, \"subdir\", \"subfile1\"),\n        ]\n\n        fs.get(source_files, local_join(target, \"newdir\") + \"/\")  # Note trailing slash\n        assert local_fs.isdir(local_join(target, \"newdir\"))\n        assert local_fs.isfile(local_join(target, \"newdir\", \"file1\"))\n        assert local_fs.isfile(local_join(target, \"newdir\", \"file2\"))\n        assert local_fs.isfile(local_join(target, \"newdir\", \"subfile1\"))\n\n    def test_get_directory_recursive(\n        self, fs, fs_join, fs_path, local_fs, local_join, local_target\n    ):\n        # https://github.com/fsspec/filesystem_spec/issues/1062\n        # Recursive cp/get/put of source directory into non-existent target directory.\n        src = fs_join(fs_path, \"src\")\n        src_file = fs_join(src, \"file\")\n        fs.mkdir(src)\n        fs.touch(src_file)\n\n        target = local_target\n\n        # get without slash\n        assert not local_fs.exists(target)\n        for loop in range(2):\n            fs.get(src, target, recursive=True)\n            assert local_fs.isdir(target)\n\n            if loop == 0:\n                assert local_fs.isfile(local_join(target, \"file\"))\n                assert not local_fs.exists(local_join(target, \"src\"))\n            else:\n                assert local_fs.isfile(local_join(target, \"file\"))\n                assert local_fs.isdir(local_join(target, \"src\"))\n                assert local_fs.isfile(local_join(target, \"src\", \"file\"))\n\n        local_fs.rm(target, recursive=True)\n\n        # get with slash\n        assert not local_fs.exists(target)\n        for loop in range(2):\n            fs.get(src + \"/\", target, recursive=True)\n            assert local_fs.isdir(target)\n            assert local_fs.isfile(local_join(target, \"file\"))\n            assert not local_fs.exists(local_join(target, \"src\"))\n\n    def test_get_directory_without_files_with_same_name_prefix(\n        self,\n        fs,\n        fs_join,\n        local_fs,\n        local_join,\n        local_target,\n        fs_dir_and_file_with_same_name_prefix,\n    ):\n        # Create the test dirs\n        source = fs_dir_and_file_with_same_name_prefix\n        target = local_target\n\n        # Test without glob\n        fs.get(fs_join(source, \"subdir\"), target, recursive=True)\n\n        assert local_fs.isfile(local_join(target, \"subfile.txt\"))\n        assert not local_fs.isfile(local_join(target, \"subdir.txt\"))\n\n        local_fs.rm([local_join(target, \"subfile.txt\")])\n        assert local_fs.ls(target) == []\n\n        # Test with glob\n        fs.get(fs_join(source, \"subdir*\"), target, recursive=True)\n\n        assert local_fs.isdir(local_join(target, \"subdir\"))\n        assert local_fs.isfile(local_join(target, \"subdir\", \"subfile.txt\"))\n        assert local_fs.isfile(local_join(target, \"subdir.txt\"))\n\n    def test_get_with_source_and_destination_as_list(\n        self,\n        fs,\n        fs_join,\n        local_fs,\n        local_join,\n        local_target,\n        fs_10_files_with_hashed_names,\n    ):\n        # Create the test dir\n        source = fs_10_files_with_hashed_names\n        target = local_target\n\n        # Create list of files for source and destination\n        source_files = []\n        destination_files = []\n        for i in range(10):\n            hashed_i = md5(str(i).encode(\"utf-8\")).hexdigest()\n            source_files.append(fs_join(source, f\"{hashed_i}.txt\"))\n            destination_files.append(\n                make_path_posix(local_join(target, f\"{hashed_i}.txt\"))\n            )\n\n        # Copy and assert order was kept\n        fs.get(rpath=source_files, lpath=destination_files)\n\n        for i in range(10):\n            file_content = local_fs.cat(destination_files[i]).decode(\"utf-8\")\n            assert file_content == str(i)\n", "fsspec/tests/abstract/put.py": "from hashlib import md5\nfrom itertools import product\n\nimport pytest\n\nfrom fsspec.tests.abstract.common import GLOB_EDGE_CASES_TESTS\n\n\nclass AbstractPutTests:\n    def test_put_file_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_target,\n        local_join,\n        local_bulk_operations_scenario_0,\n        supports_empty_directories,\n    ):\n        # Copy scenario 1a\n        source = local_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n        if not supports_empty_directories:\n            # Force target directory to exist by adding a dummy file\n            fs.touch(fs_join(target, \"dummy\"))\n        assert fs.isdir(target)\n\n        target_file2 = fs_join(target, \"file2\")\n        target_subfile1 = fs_join(target, \"subfile1\")\n\n        # Copy from source directory\n        fs.put(local_join(source, \"file2\"), target)\n        assert fs.isfile(target_file2)\n\n        # Copy from sub directory\n        fs.put(local_join(source, \"subdir\", \"subfile1\"), target)\n        assert fs.isfile(target_subfile1)\n\n        # Remove copied files\n        fs.rm([target_file2, target_subfile1])\n        assert not fs.exists(target_file2)\n        assert not fs.exists(target_subfile1)\n\n        # Repeat with trailing slash on target\n        fs.put(local_join(source, \"file2\"), target + \"/\")\n        assert fs.isdir(target)\n        assert fs.isfile(target_file2)\n\n        fs.put(local_join(source, \"subdir\", \"subfile1\"), target + \"/\")\n        assert fs.isfile(target_subfile1)\n\n    def test_put_file_to_new_directory(\n        self, fs, fs_join, fs_target, local_join, local_bulk_operations_scenario_0\n    ):\n        # Copy scenario 1b\n        source = local_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n\n        fs.put(\n            local_join(source, \"subdir\", \"subfile1\"), fs_join(target, \"newdir/\")\n        )  # Note trailing slash\n        assert fs.isdir(target)\n        assert fs.isdir(fs_join(target, \"newdir\"))\n        assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n\n    def test_put_file_to_file_in_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_target,\n        local_join,\n        supports_empty_directories,\n        local_bulk_operations_scenario_0,\n    ):\n        # Copy scenario 1c\n        source = local_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n        if not supports_empty_directories:\n            # Force target directory to exist by adding a dummy file\n            fs.touch(fs_join(target, \"dummy\"))\n        assert fs.isdir(target)\n\n        fs.put(local_join(source, \"subdir\", \"subfile1\"), fs_join(target, \"newfile\"))\n        assert fs.isfile(fs_join(target, \"newfile\"))\n\n    def test_put_file_to_file_in_new_directory(\n        self, fs, fs_join, fs_target, local_join, local_bulk_operations_scenario_0\n    ):\n        # Copy scenario 1d\n        source = local_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n\n        fs.put(\n            local_join(source, \"subdir\", \"subfile1\"),\n            fs_join(target, \"newdir\", \"newfile\"),\n        )\n        assert fs.isdir(fs_join(target, \"newdir\"))\n        assert fs.isfile(fs_join(target, \"newdir\", \"newfile\"))\n\n    def test_put_directory_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_target,\n        local_bulk_operations_scenario_0,\n        supports_empty_directories,\n    ):\n        # Copy scenario 1e\n        source = local_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n        if not supports_empty_directories:\n            # Force target directory to exist by adding a dummy file\n            dummy = fs_join(target, \"dummy\")\n            fs.touch(dummy)\n        assert fs.isdir(target)\n\n        for source_slash, target_slash in zip([False, True], [False, True]):\n            s = fs_join(source, \"subdir\")\n            if source_slash:\n                s += \"/\"\n            t = target + \"/\" if target_slash else target\n\n            # Without recursive does nothing\n            fs.put(s, t)\n            assert fs.ls(target, detail=False) == (\n                [] if supports_empty_directories else [dummy]\n            )\n\n            # With recursive\n            fs.put(s, t, recursive=True)\n            if source_slash:\n                assert fs.isfile(fs_join(target, \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subfile2\"))\n                assert fs.isdir(fs_join(target, \"nesteddir\"))\n                assert fs.isfile(fs_join(target, \"nesteddir\", \"nestedfile\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n\n                fs.rm(\n                    [\n                        fs_join(target, \"subfile1\"),\n                        fs_join(target, \"subfile2\"),\n                        fs_join(target, \"nesteddir\"),\n                    ],\n                    recursive=True,\n                )\n            else:\n                assert fs.isdir(fs_join(target, \"subdir\"))\n                assert fs.isfile(fs_join(target, \"subdir\", \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subdir\", \"subfile2\"))\n                assert fs.isdir(fs_join(target, \"subdir\", \"nesteddir\"))\n                assert fs.isfile(fs_join(target, \"subdir\", \"nesteddir\", \"nestedfile\"))\n\n                fs.rm(fs_join(target, \"subdir\"), recursive=True)\n            assert fs.ls(target, detail=False) == (\n                [] if supports_empty_directories else [dummy]\n            )\n\n            # Limit recursive by maxdepth\n            fs.put(s, t, recursive=True, maxdepth=1)\n            if source_slash:\n                assert fs.isfile(fs_join(target, \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subfile2\"))\n                assert not fs.exists(fs_join(target, \"nesteddir\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n\n                fs.rm(\n                    [\n                        fs_join(target, \"subfile1\"),\n                        fs_join(target, \"subfile2\"),\n                    ],\n                    recursive=True,\n                )\n            else:\n                assert fs.isdir(fs_join(target, \"subdir\"))\n                assert fs.isfile(fs_join(target, \"subdir\", \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subdir\", \"subfile2\"))\n                assert not fs.exists(fs_join(target, \"subdir\", \"nesteddir\"))\n\n                fs.rm(fs_join(target, \"subdir\"), recursive=True)\n            assert fs.ls(target, detail=False) == (\n                [] if supports_empty_directories else [dummy]\n            )\n\n    def test_put_directory_to_new_directory(\n        self,\n        fs,\n        fs_join,\n        fs_target,\n        local_bulk_operations_scenario_0,\n        supports_empty_directories,\n    ):\n        # Copy scenario 1f\n        source = local_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n\n        for source_slash, target_slash in zip([False, True], [False, True]):\n            s = fs_join(source, \"subdir\")\n            if source_slash:\n                s += \"/\"\n            t = fs_join(target, \"newdir\")\n            if target_slash:\n                t += \"/\"\n\n            # Without recursive does nothing\n            fs.put(s, t)\n            if supports_empty_directories:\n                assert fs.ls(target) == []\n            else:\n                with pytest.raises(FileNotFoundError):\n                    fs.ls(target)\n\n            # With recursive\n            fs.put(s, t, recursive=True)\n            assert fs.isdir(fs_join(target, \"newdir\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile2\"))\n            assert fs.isdir(fs_join(target, \"newdir\", \"nesteddir\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"nesteddir\", \"nestedfile\"))\n            assert not fs.exists(fs_join(target, \"subdir\"))\n\n            fs.rm(fs_join(target, \"newdir\"), recursive=True)\n            assert not fs.exists(fs_join(target, \"newdir\"))\n\n            # Limit recursive by maxdepth\n            fs.put(s, t, recursive=True, maxdepth=1)\n            assert fs.isdir(fs_join(target, \"newdir\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile2\"))\n            assert not fs.exists(fs_join(target, \"newdir\", \"nesteddir\"))\n            assert not fs.exists(fs_join(target, \"subdir\"))\n\n            fs.rm(fs_join(target, \"newdir\"), recursive=True)\n            assert not fs.exists(fs_join(target, \"newdir\"))\n\n    def test_put_glob_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_target,\n        local_join,\n        supports_empty_directories,\n        local_bulk_operations_scenario_0,\n    ):\n        # Copy scenario 1g\n        source = local_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n        if not supports_empty_directories:\n            # Force target directory to exist by adding a dummy file\n            dummy = fs_join(target, \"dummy\")\n            fs.touch(dummy)\n        assert fs.isdir(target)\n\n        for target_slash in [False, True]:\n            t = target + \"/\" if target_slash else target\n\n            # Without recursive\n            fs.put(local_join(source, \"subdir\", \"*\"), t)\n            assert fs.isfile(fs_join(target, \"subfile1\"))\n            assert fs.isfile(fs_join(target, \"subfile2\"))\n            assert not fs.isdir(fs_join(target, \"nesteddir\"))\n            assert not fs.exists(fs_join(target, \"nesteddir\", \"nestedfile\"))\n            assert not fs.exists(fs_join(target, \"subdir\"))\n\n            fs.rm(\n                [\n                    fs_join(target, \"subfile1\"),\n                    fs_join(target, \"subfile2\"),\n                ],\n                recursive=True,\n            )\n            assert fs.ls(target, detail=False) == (\n                [] if supports_empty_directories else [dummy]\n            )\n\n            # With recursive\n            for glob, recursive in zip([\"*\", \"**\"], [True, False]):\n                fs.put(local_join(source, \"subdir\", glob), t, recursive=recursive)\n                assert fs.isfile(fs_join(target, \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subfile2\"))\n                assert fs.isdir(fs_join(target, \"nesteddir\"))\n                assert fs.isfile(fs_join(target, \"nesteddir\", \"nestedfile\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n\n                fs.rm(\n                    [\n                        fs_join(target, \"subfile1\"),\n                        fs_join(target, \"subfile2\"),\n                        fs_join(target, \"nesteddir\"),\n                    ],\n                    recursive=True,\n                )\n                assert fs.ls(target, detail=False) == (\n                    [] if supports_empty_directories else [dummy]\n                )\n\n                # Limit recursive by maxdepth\n                fs.put(\n                    local_join(source, \"subdir\", glob),\n                    t,\n                    recursive=recursive,\n                    maxdepth=1,\n                )\n                assert fs.isfile(fs_join(target, \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subfile2\"))\n                assert not fs.exists(fs_join(target, \"nesteddir\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n\n                fs.rm(\n                    [\n                        fs_join(target, \"subfile1\"),\n                        fs_join(target, \"subfile2\"),\n                    ],\n                    recursive=True,\n                )\n                assert fs.ls(target, detail=False) == (\n                    [] if supports_empty_directories else [dummy]\n                )\n\n    def test_put_glob_to_new_directory(\n        self, fs, fs_join, fs_target, local_join, local_bulk_operations_scenario_0\n    ):\n        # Copy scenario 1h\n        source = local_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n\n        for target_slash in [False, True]:\n            t = fs_join(target, \"newdir\")\n            if target_slash:\n                t += \"/\"\n\n            # Without recursive\n            fs.put(local_join(source, \"subdir\", \"*\"), t)\n            assert fs.isdir(fs_join(target, \"newdir\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile2\"))\n            assert not fs.exists(fs_join(target, \"newdir\", \"nesteddir\"))\n            assert not fs.exists(fs_join(target, \"newdir\", \"nesteddir\", \"nestedfile\"))\n            assert not fs.exists(fs_join(target, \"subdir\"))\n            assert not fs.exists(fs_join(target, \"newdir\", \"subdir\"))\n\n            fs.rm(fs_join(target, \"newdir\"), recursive=True)\n            assert not fs.exists(fs_join(target, \"newdir\"))\n\n            # With recursive\n            for glob, recursive in zip([\"*\", \"**\"], [True, False]):\n                fs.put(local_join(source, \"subdir\", glob), t, recursive=recursive)\n                assert fs.isdir(fs_join(target, \"newdir\"))\n                assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"newdir\", \"subfile2\"))\n                assert fs.isdir(fs_join(target, \"newdir\", \"nesteddir\"))\n                assert fs.isfile(fs_join(target, \"newdir\", \"nesteddir\", \"nestedfile\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n                assert not fs.exists(fs_join(target, \"newdir\", \"subdir\"))\n\n                fs.rm(fs_join(target, \"newdir\"), recursive=True)\n                assert not fs.exists(fs_join(target, \"newdir\"))\n\n                # Limit recursive by maxdepth\n                fs.put(\n                    local_join(source, \"subdir\", glob),\n                    t,\n                    recursive=recursive,\n                    maxdepth=1,\n                )\n                assert fs.isdir(fs_join(target, \"newdir\"))\n                assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"newdir\", \"subfile2\"))\n                assert not fs.exists(fs_join(target, \"newdir\", \"nesteddir\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n                assert not fs.exists(fs_join(target, \"newdir\", \"subdir\"))\n\n                fs.rm(fs_join(target, \"newdir\"), recursive=True)\n                assert not fs.exists(fs_join(target, \"newdir\"))\n\n    @pytest.mark.parametrize(\n        GLOB_EDGE_CASES_TESTS[\"argnames\"],\n        GLOB_EDGE_CASES_TESTS[\"argvalues\"],\n    )\n    def test_put_glob_edge_cases(\n        self,\n        path,\n        recursive,\n        maxdepth,\n        expected,\n        fs,\n        fs_join,\n        fs_target,\n        local_glob_edge_cases_files,\n        local_join,\n        fs_sanitize_path,\n    ):\n        # Copy scenario 1g\n        source = local_glob_edge_cases_files\n\n        target = fs_target\n\n        for new_dir, target_slash in product([True, False], [True, False]):\n            fs.mkdir(target)\n\n            t = fs_join(target, \"newdir\") if new_dir else target\n            t = t + \"/\" if target_slash else t\n\n            fs.put(local_join(source, path), t, recursive=recursive, maxdepth=maxdepth)\n\n            output = fs.find(target)\n            if new_dir:\n                prefixed_expected = [\n                    fs_sanitize_path(fs_join(target, \"newdir\", p)) for p in expected\n                ]\n            else:\n                prefixed_expected = [\n                    fs_sanitize_path(fs_join(target, p)) for p in expected\n                ]\n            assert sorted(output) == sorted(prefixed_expected)\n\n            try:\n                fs.rm(target, recursive=True)\n            except FileNotFoundError:\n                pass\n\n    def test_put_list_of_files_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_target,\n        local_join,\n        local_bulk_operations_scenario_0,\n        supports_empty_directories,\n    ):\n        # Copy scenario 2a\n        source = local_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n        if not supports_empty_directories:\n            # Force target directory to exist by adding a dummy file\n            dummy = fs_join(target, \"dummy\")\n            fs.touch(dummy)\n        assert fs.isdir(target)\n\n        source_files = [\n            local_join(source, \"file1\"),\n            local_join(source, \"file2\"),\n            local_join(source, \"subdir\", \"subfile1\"),\n        ]\n\n        for target_slash in [False, True]:\n            t = target + \"/\" if target_slash else target\n\n            fs.put(source_files, t)\n            assert fs.isfile(fs_join(target, \"file1\"))\n            assert fs.isfile(fs_join(target, \"file2\"))\n            assert fs.isfile(fs_join(target, \"subfile1\"))\n\n            fs.rm(\n                [\n                    fs_join(target, \"file1\"),\n                    fs_join(target, \"file2\"),\n                    fs_join(target, \"subfile1\"),\n                ],\n                recursive=True,\n            )\n            assert fs.ls(target, detail=False) == (\n                [] if supports_empty_directories else [dummy]\n            )\n\n    def test_put_list_of_files_to_new_directory(\n        self, fs, fs_join, fs_target, local_join, local_bulk_operations_scenario_0\n    ):\n        # Copy scenario 2b\n        source = local_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n\n        source_files = [\n            local_join(source, \"file1\"),\n            local_join(source, \"file2\"),\n            local_join(source, \"subdir\", \"subfile1\"),\n        ]\n\n        fs.put(source_files, fs_join(target, \"newdir\") + \"/\")  # Note trailing slash\n        assert fs.isdir(fs_join(target, \"newdir\"))\n        assert fs.isfile(fs_join(target, \"newdir\", \"file1\"))\n        assert fs.isfile(fs_join(target, \"newdir\", \"file2\"))\n        assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n\n    def test_put_directory_recursive(\n        self, fs, fs_join, fs_target, local_fs, local_join, local_path\n    ):\n        # https://github.com/fsspec/filesystem_spec/issues/1062\n        # Recursive cp/get/put of source directory into non-existent target directory.\n        src = local_join(local_path, \"src\")\n        src_file = local_join(src, \"file\")\n        local_fs.mkdir(src)\n        local_fs.touch(src_file)\n\n        target = fs_target\n\n        # put without slash\n        assert not fs.exists(target)\n        for loop in range(2):\n            fs.put(src, target, recursive=True)\n            assert fs.isdir(target)\n\n            if loop == 0:\n                assert fs.isfile(fs_join(target, \"file\"))\n                assert not fs.exists(fs_join(target, \"src\"))\n            else:\n                assert fs.isfile(fs_join(target, \"file\"))\n                assert fs.isdir(fs_join(target, \"src\"))\n                assert fs.isfile(fs_join(target, \"src\", \"file\"))\n\n        fs.rm(target, recursive=True)\n\n        # put with slash\n        assert not fs.exists(target)\n        for loop in range(2):\n            fs.put(src + \"/\", target, recursive=True)\n            assert fs.isdir(target)\n            assert fs.isfile(fs_join(target, \"file\"))\n            assert not fs.exists(fs_join(target, \"src\"))\n\n    def test_put_directory_without_files_with_same_name_prefix(\n        self,\n        fs,\n        fs_join,\n        fs_target,\n        local_join,\n        local_dir_and_file_with_same_name_prefix,\n        supports_empty_directories,\n    ):\n        # Create the test dirs\n        source = local_dir_and_file_with_same_name_prefix\n        target = fs_target\n\n        # Test without glob\n        fs.put(local_join(source, \"subdir\"), fs_target, recursive=True)\n\n        assert fs.isfile(fs_join(fs_target, \"subfile.txt\"))\n        assert not fs.isfile(fs_join(fs_target, \"subdir.txt\"))\n\n        fs.rm([fs_join(target, \"subfile.txt\")])\n        if supports_empty_directories:\n            assert fs.ls(target) == []\n        else:\n            assert not fs.exists(target)\n\n        # Test with glob\n        fs.put(local_join(source, \"subdir*\"), fs_target, recursive=True)\n\n        assert fs.isdir(fs_join(fs_target, \"subdir\"))\n        assert fs.isfile(fs_join(fs_target, \"subdir\", \"subfile.txt\"))\n        assert fs.isfile(fs_join(fs_target, \"subdir.txt\"))\n\n    def test_copy_with_source_and_destination_as_list(\n        self, fs, fs_target, fs_join, local_join, local_10_files_with_hashed_names\n    ):\n        # Create the test dir\n        source = local_10_files_with_hashed_names\n        target = fs_target\n\n        # Create list of files for source and destination\n        source_files = []\n        destination_files = []\n        for i in range(10):\n            hashed_i = md5(str(i).encode(\"utf-8\")).hexdigest()\n            source_files.append(local_join(source, f\"{hashed_i}.txt\"))\n            destination_files.append(fs_join(target, f\"{hashed_i}.txt\"))\n\n        # Copy and assert order was kept\n        fs.put(lpath=source_files, rpath=destination_files)\n\n        for i in range(10):\n            file_content = fs.cat(destination_files[i]).decode(\"utf-8\")\n            assert file_content == str(i)\n", "fsspec/tests/abstract/copy.py": "from hashlib import md5\nfrom itertools import product\n\nimport pytest\n\nfrom fsspec.tests.abstract.common import GLOB_EDGE_CASES_TESTS\n\n\nclass AbstractCopyTests:\n    def test_copy_file_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        fs_target,\n        supports_empty_directories,\n    ):\n        # Copy scenario 1a\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n        if not supports_empty_directories:\n            # Force target directory to exist by adding a dummy file\n            fs.touch(fs_join(target, \"dummy\"))\n        assert fs.isdir(target)\n\n        target_file2 = fs_join(target, \"file2\")\n        target_subfile1 = fs_join(target, \"subfile1\")\n\n        # Copy from source directory\n        fs.cp(fs_join(source, \"file2\"), target)\n        assert fs.isfile(target_file2)\n\n        # Copy from sub directory\n        fs.cp(fs_join(source, \"subdir\", \"subfile1\"), target)\n        assert fs.isfile(target_subfile1)\n\n        # Remove copied files\n        fs.rm([target_file2, target_subfile1])\n        assert not fs.exists(target_file2)\n        assert not fs.exists(target_subfile1)\n\n        # Repeat with trailing slash on target\n        fs.cp(fs_join(source, \"file2\"), target + \"/\")\n        assert fs.isdir(target)\n        assert fs.isfile(target_file2)\n\n        fs.cp(fs_join(source, \"subdir\", \"subfile1\"), target + \"/\")\n        assert fs.isfile(target_subfile1)\n\n    def test_copy_file_to_new_directory(\n        self, fs, fs_join, fs_bulk_operations_scenario_0, fs_target\n    ):\n        # Copy scenario 1b\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n\n        fs.cp(\n            fs_join(source, \"subdir\", \"subfile1\"), fs_join(target, \"newdir/\")\n        )  # Note trailing slash\n        assert fs.isdir(target)\n        assert fs.isdir(fs_join(target, \"newdir\"))\n        assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n\n    def test_copy_file_to_file_in_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        fs_target,\n        supports_empty_directories,\n    ):\n        # Copy scenario 1c\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n        if not supports_empty_directories:\n            # Force target directory to exist by adding a dummy file\n            fs.touch(fs_join(target, \"dummy\"))\n        assert fs.isdir(target)\n\n        fs.cp(fs_join(source, \"subdir\", \"subfile1\"), fs_join(target, \"newfile\"))\n        assert fs.isfile(fs_join(target, \"newfile\"))\n\n    def test_copy_file_to_file_in_new_directory(\n        self, fs, fs_join, fs_bulk_operations_scenario_0, fs_target\n    ):\n        # Copy scenario 1d\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n\n        fs.cp(\n            fs_join(source, \"subdir\", \"subfile1\"), fs_join(target, \"newdir\", \"newfile\")\n        )\n        assert fs.isdir(fs_join(target, \"newdir\"))\n        assert fs.isfile(fs_join(target, \"newdir\", \"newfile\"))\n\n    def test_copy_directory_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        fs_target,\n        supports_empty_directories,\n    ):\n        # Copy scenario 1e\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n        if not supports_empty_directories:\n            # Force target directory to exist by adding a dummy file\n            dummy = fs_join(target, \"dummy\")\n            fs.touch(dummy)\n        assert fs.isdir(target)\n\n        for source_slash, target_slash in zip([False, True], [False, True]):\n            s = fs_join(source, \"subdir\")\n            if source_slash:\n                s += \"/\"\n            t = target + \"/\" if target_slash else target\n\n            # Without recursive does nothing\n            fs.cp(s, t)\n            assert fs.ls(target, detail=False) == (\n                [] if supports_empty_directories else [dummy]\n            )\n\n            # With recursive\n            fs.cp(s, t, recursive=True)\n            if source_slash:\n                assert fs.isfile(fs_join(target, \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subfile2\"))\n                assert fs.isdir(fs_join(target, \"nesteddir\"))\n                assert fs.isfile(fs_join(target, \"nesteddir\", \"nestedfile\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n\n                fs.rm(\n                    [\n                        fs_join(target, \"subfile1\"),\n                        fs_join(target, \"subfile2\"),\n                        fs_join(target, \"nesteddir\"),\n                    ],\n                    recursive=True,\n                )\n            else:\n                assert fs.isdir(fs_join(target, \"subdir\"))\n                assert fs.isfile(fs_join(target, \"subdir\", \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subdir\", \"subfile2\"))\n                assert fs.isdir(fs_join(target, \"subdir\", \"nesteddir\"))\n                assert fs.isfile(fs_join(target, \"subdir\", \"nesteddir\", \"nestedfile\"))\n\n                fs.rm(fs_join(target, \"subdir\"), recursive=True)\n            assert fs.ls(target, detail=False) == (\n                [] if supports_empty_directories else [dummy]\n            )\n\n            # Limit recursive by maxdepth\n            fs.cp(s, t, recursive=True, maxdepth=1)\n            if source_slash:\n                assert fs.isfile(fs_join(target, \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subfile2\"))\n                assert not fs.exists(fs_join(target, \"nesteddir\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n\n                fs.rm(\n                    [\n                        fs_join(target, \"subfile1\"),\n                        fs_join(target, \"subfile2\"),\n                    ],\n                    recursive=True,\n                )\n            else:\n                assert fs.isdir(fs_join(target, \"subdir\"))\n                assert fs.isfile(fs_join(target, \"subdir\", \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subdir\", \"subfile2\"))\n                assert not fs.exists(fs_join(target, \"subdir\", \"nesteddir\"))\n\n                fs.rm(fs_join(target, \"subdir\"), recursive=True)\n            assert fs.ls(target, detail=False) == (\n                [] if supports_empty_directories else [dummy]\n            )\n\n    def test_copy_directory_to_new_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        fs_target,\n        supports_empty_directories,\n    ):\n        # Copy scenario 1f\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n\n        for source_slash, target_slash in zip([False, True], [False, True]):\n            s = fs_join(source, \"subdir\")\n            if source_slash:\n                s += \"/\"\n            t = fs_join(target, \"newdir\")\n            if target_slash:\n                t += \"/\"\n\n            # Without recursive does nothing\n            fs.cp(s, t)\n            if supports_empty_directories:\n                assert fs.ls(target) == []\n            else:\n                with pytest.raises(FileNotFoundError):\n                    fs.ls(target)\n\n            # With recursive\n            fs.cp(s, t, recursive=True)\n            assert fs.isdir(fs_join(target, \"newdir\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile2\"))\n            assert fs.isdir(fs_join(target, \"newdir\", \"nesteddir\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"nesteddir\", \"nestedfile\"))\n            assert not fs.exists(fs_join(target, \"subdir\"))\n\n            fs.rm(fs_join(target, \"newdir\"), recursive=True)\n            assert not fs.exists(fs_join(target, \"newdir\"))\n\n            # Limit recursive by maxdepth\n            fs.cp(s, t, recursive=True, maxdepth=1)\n            assert fs.isdir(fs_join(target, \"newdir\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile2\"))\n            assert not fs.exists(fs_join(target, \"newdir\", \"nesteddir\"))\n            assert not fs.exists(fs_join(target, \"subdir\"))\n\n            fs.rm(fs_join(target, \"newdir\"), recursive=True)\n            assert not fs.exists(fs_join(target, \"newdir\"))\n\n    def test_copy_glob_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        fs_target,\n        supports_empty_directories,\n    ):\n        # Copy scenario 1g\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n        if not supports_empty_directories:\n            # Force target directory to exist by adding a dummy file\n            dummy = fs_join(target, \"dummy\")\n            fs.touch(dummy)\n        assert fs.isdir(target)\n\n        for target_slash in [False, True]:\n            t = target + \"/\" if target_slash else target\n\n            # Without recursive\n            fs.cp(fs_join(source, \"subdir\", \"*\"), t)\n            assert fs.isfile(fs_join(target, \"subfile1\"))\n            assert fs.isfile(fs_join(target, \"subfile2\"))\n            assert not fs.isdir(fs_join(target, \"nesteddir\"))\n            assert not fs.exists(fs_join(target, \"nesteddir\", \"nestedfile\"))\n            assert not fs.exists(fs_join(target, \"subdir\"))\n\n            fs.rm(\n                [\n                    fs_join(target, \"subfile1\"),\n                    fs_join(target, \"subfile2\"),\n                ],\n                recursive=True,\n            )\n            assert fs.ls(target, detail=False) == (\n                [] if supports_empty_directories else [dummy]\n            )\n\n            # With recursive\n            for glob, recursive in zip([\"*\", \"**\"], [True, False]):\n                fs.cp(fs_join(source, \"subdir\", glob), t, recursive=recursive)\n                assert fs.isfile(fs_join(target, \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subfile2\"))\n                assert fs.isdir(fs_join(target, \"nesteddir\"))\n                assert fs.isfile(fs_join(target, \"nesteddir\", \"nestedfile\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n\n                fs.rm(\n                    [\n                        fs_join(target, \"subfile1\"),\n                        fs_join(target, \"subfile2\"),\n                        fs_join(target, \"nesteddir\"),\n                    ],\n                    recursive=True,\n                )\n                assert fs.ls(target, detail=False) == (\n                    [] if supports_empty_directories else [dummy]\n                )\n\n                # Limit recursive by maxdepth\n                fs.cp(\n                    fs_join(source, \"subdir\", glob), t, recursive=recursive, maxdepth=1\n                )\n                assert fs.isfile(fs_join(target, \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"subfile2\"))\n                assert not fs.exists(fs_join(target, \"nesteddir\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n\n                fs.rm(\n                    [\n                        fs_join(target, \"subfile1\"),\n                        fs_join(target, \"subfile2\"),\n                    ],\n                    recursive=True,\n                )\n                assert fs.ls(target, detail=False) == (\n                    [] if supports_empty_directories else [dummy]\n                )\n\n    def test_copy_glob_to_new_directory(\n        self, fs, fs_join, fs_bulk_operations_scenario_0, fs_target\n    ):\n        # Copy scenario 1h\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n\n        for target_slash in [False, True]:\n            t = fs_join(target, \"newdir\")\n            if target_slash:\n                t += \"/\"\n\n            # Without recursive\n            fs.cp(fs_join(source, \"subdir\", \"*\"), t)\n            assert fs.isdir(fs_join(target, \"newdir\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n            assert fs.isfile(fs_join(target, \"newdir\", \"subfile2\"))\n            assert not fs.exists(fs_join(target, \"newdir\", \"nesteddir\"))\n            assert not fs.exists(fs_join(target, \"newdir\", \"nesteddir\", \"nestedfile\"))\n            assert not fs.exists(fs_join(target, \"subdir\"))\n            assert not fs.exists(fs_join(target, \"newdir\", \"subdir\"))\n\n            fs.rm(fs_join(target, \"newdir\"), recursive=True)\n            assert not fs.exists(fs_join(target, \"newdir\"))\n\n            # With recursive\n            for glob, recursive in zip([\"*\", \"**\"], [True, False]):\n                fs.cp(fs_join(source, \"subdir\", glob), t, recursive=recursive)\n                assert fs.isdir(fs_join(target, \"newdir\"))\n                assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"newdir\", \"subfile2\"))\n                assert fs.isdir(fs_join(target, \"newdir\", \"nesteddir\"))\n                assert fs.isfile(fs_join(target, \"newdir\", \"nesteddir\", \"nestedfile\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n                assert not fs.exists(fs_join(target, \"newdir\", \"subdir\"))\n\n                fs.rm(fs_join(target, \"newdir\"), recursive=True)\n                assert not fs.exists(fs_join(target, \"newdir\"))\n\n                # Limit recursive by maxdepth\n                fs.cp(\n                    fs_join(source, \"subdir\", glob), t, recursive=recursive, maxdepth=1\n                )\n                assert fs.isdir(fs_join(target, \"newdir\"))\n                assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n                assert fs.isfile(fs_join(target, \"newdir\", \"subfile2\"))\n                assert not fs.exists(fs_join(target, \"newdir\", \"nesteddir\"))\n                assert not fs.exists(fs_join(target, \"subdir\"))\n                assert not fs.exists(fs_join(target, \"newdir\", \"subdir\"))\n\n                fs.rm(fs_join(target, \"newdir\"), recursive=True)\n                assert not fs.exists(fs_join(target, \"newdir\"))\n\n    @pytest.mark.parametrize(\n        GLOB_EDGE_CASES_TESTS[\"argnames\"],\n        GLOB_EDGE_CASES_TESTS[\"argvalues\"],\n    )\n    def test_copy_glob_edge_cases(\n        self,\n        path,\n        recursive,\n        maxdepth,\n        expected,\n        fs,\n        fs_join,\n        fs_glob_edge_cases_files,\n        fs_target,\n        fs_sanitize_path,\n    ):\n        # Copy scenario 1g\n        source = fs_glob_edge_cases_files\n\n        target = fs_target\n\n        for new_dir, target_slash in product([True, False], [True, False]):\n            fs.mkdir(target)\n\n            t = fs_join(target, \"newdir\") if new_dir else target\n            t = t + \"/\" if target_slash else t\n\n            fs.copy(fs_join(source, path), t, recursive=recursive, maxdepth=maxdepth)\n\n            output = fs.find(target)\n            if new_dir:\n                prefixed_expected = [\n                    fs_sanitize_path(fs_join(target, \"newdir\", p)) for p in expected\n                ]\n            else:\n                prefixed_expected = [\n                    fs_sanitize_path(fs_join(target, p)) for p in expected\n                ]\n            assert sorted(output) == sorted(prefixed_expected)\n\n            try:\n                fs.rm(target, recursive=True)\n            except FileNotFoundError:\n                pass\n\n    def test_copy_list_of_files_to_existing_directory(\n        self,\n        fs,\n        fs_join,\n        fs_bulk_operations_scenario_0,\n        fs_target,\n        supports_empty_directories,\n    ):\n        # Copy scenario 2a\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n        if not supports_empty_directories:\n            # Force target directory to exist by adding a dummy file\n            dummy = fs_join(target, \"dummy\")\n            fs.touch(dummy)\n        assert fs.isdir(target)\n\n        source_files = [\n            fs_join(source, \"file1\"),\n            fs_join(source, \"file2\"),\n            fs_join(source, \"subdir\", \"subfile1\"),\n        ]\n\n        for target_slash in [False, True]:\n            t = target + \"/\" if target_slash else target\n\n            fs.cp(source_files, t)\n            assert fs.isfile(fs_join(target, \"file1\"))\n            assert fs.isfile(fs_join(target, \"file2\"))\n            assert fs.isfile(fs_join(target, \"subfile1\"))\n\n            fs.rm(\n                [\n                    fs_join(target, \"file1\"),\n                    fs_join(target, \"file2\"),\n                    fs_join(target, \"subfile1\"),\n                ],\n                recursive=True,\n            )\n            assert fs.ls(target, detail=False) == (\n                [] if supports_empty_directories else [dummy]\n            )\n\n    def test_copy_list_of_files_to_new_directory(\n        self, fs, fs_join, fs_bulk_operations_scenario_0, fs_target\n    ):\n        # Copy scenario 2b\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        fs.mkdir(target)\n\n        source_files = [\n            fs_join(source, \"file1\"),\n            fs_join(source, \"file2\"),\n            fs_join(source, \"subdir\", \"subfile1\"),\n        ]\n\n        fs.cp(source_files, fs_join(target, \"newdir\") + \"/\")  # Note trailing slash\n        assert fs.isdir(fs_join(target, \"newdir\"))\n        assert fs.isfile(fs_join(target, \"newdir\", \"file1\"))\n        assert fs.isfile(fs_join(target, \"newdir\", \"file2\"))\n        assert fs.isfile(fs_join(target, \"newdir\", \"subfile1\"))\n\n    def test_copy_two_files_new_directory(\n        self, fs, fs_join, fs_bulk_operations_scenario_0, fs_target\n    ):\n        # This is a duplicate of test_copy_list_of_files_to_new_directory and\n        # can eventually be removed.\n        source = fs_bulk_operations_scenario_0\n\n        target = fs_target\n        assert not fs.exists(target)\n        fs.cp([fs_join(source, \"file1\"), fs_join(source, \"file2\")], target)\n\n        assert fs.isdir(target)\n        assert fs.isfile(fs_join(target, \"file1\"))\n        assert fs.isfile(fs_join(target, \"file2\"))\n\n    def test_copy_directory_without_files_with_same_name_prefix(\n        self,\n        fs,\n        fs_join,\n        fs_target,\n        fs_dir_and_file_with_same_name_prefix,\n        supports_empty_directories,\n    ):\n        # Create the test dirs\n        source = fs_dir_and_file_with_same_name_prefix\n        target = fs_target\n\n        # Test without glob\n        fs.cp(fs_join(source, \"subdir\"), target, recursive=True)\n\n        assert fs.isfile(fs_join(target, \"subfile.txt\"))\n        assert not fs.isfile(fs_join(target, \"subdir.txt\"))\n\n        fs.rm([fs_join(target, \"subfile.txt\")])\n        if supports_empty_directories:\n            assert fs.ls(target) == []\n        else:\n            assert not fs.exists(target)\n\n        # Test with glob\n        fs.cp(fs_join(source, \"subdir*\"), target, recursive=True)\n\n        assert fs.isdir(fs_join(target, \"subdir\"))\n        assert fs.isfile(fs_join(target, \"subdir\", \"subfile.txt\"))\n        assert fs.isfile(fs_join(target, \"subdir.txt\"))\n\n    def test_copy_with_source_and_destination_as_list(\n        self, fs, fs_target, fs_join, fs_10_files_with_hashed_names\n    ):\n        # Create the test dir\n        source = fs_10_files_with_hashed_names\n        target = fs_target\n\n        # Create list of files for source and destination\n        source_files = []\n        destination_files = []\n        for i in range(10):\n            hashed_i = md5(str(i).encode(\"utf-8\")).hexdigest()\n            source_files.append(fs_join(source, f\"{hashed_i}.txt\"))\n            destination_files.append(fs_join(target, f\"{hashed_i}.txt\"))\n\n        # Copy and assert order was kept\n        fs.copy(path1=source_files, path2=destination_files)\n\n        for i in range(10):\n            file_content = fs.cat(destination_files[i]).decode(\"utf-8\")\n            assert file_content == str(i)\n", "fsspec/tests/abstract/__init__.py": "import os\nfrom hashlib import md5\n\nimport pytest\n\nfrom fsspec.implementations.local import LocalFileSystem\nfrom fsspec.tests.abstract.copy import AbstractCopyTests  # noqa\nfrom fsspec.tests.abstract.get import AbstractGetTests  # noqa\nfrom fsspec.tests.abstract.put import AbstractPutTests  # noqa\n\n\nclass BaseAbstractFixtures:\n    \"\"\"\n    Abstract base class containing fixtures that are used by but never need to\n    be overridden in derived filesystem-specific classes to run the abstract\n    tests on such filesystems.\n    \"\"\"\n\n    @pytest.fixture\n    def fs_bulk_operations_scenario_0(self, fs, fs_join, fs_path):\n        \"\"\"\n        Scenario on remote filesystem that is used for many cp/get/put tests.\n\n        Cleans up at the end of each test it which it is used.\n        \"\"\"\n        source = self._bulk_operations_scenario_0(fs, fs_join, fs_path)\n        yield source\n        fs.rm(source, recursive=True)\n\n    @pytest.fixture\n    def fs_glob_edge_cases_files(self, fs, fs_join, fs_path):\n        \"\"\"\n        Scenario on remote filesystem that is used for glob edge cases cp/get/put tests.\n\n        Cleans up at the end of each test it which it is used.\n        \"\"\"\n        source = self._glob_edge_cases_files(fs, fs_join, fs_path)\n        yield source\n        fs.rm(source, recursive=True)\n\n    @pytest.fixture\n    def fs_dir_and_file_with_same_name_prefix(self, fs, fs_join, fs_path):\n        \"\"\"\n        Scenario on remote filesystem that is used to check cp/get/put on directory\n        and file with the same name prefixes.\n\n        Cleans up at the end of each test it which it is used.\n        \"\"\"\n        source = self._dir_and_file_with_same_name_prefix(fs, fs_join, fs_path)\n        yield source\n        fs.rm(source, recursive=True)\n\n    @pytest.fixture\n    def fs_10_files_with_hashed_names(self, fs, fs_join, fs_path):\n        \"\"\"\n        Scenario on remote filesystem that is used to check cp/get/put files order\n        when source and destination are lists.\n\n        Cleans up at the end of each test it which it is used.\n        \"\"\"\n        source = self._10_files_with_hashed_names(fs, fs_join, fs_path)\n        yield source\n        fs.rm(source, recursive=True)\n\n    @pytest.fixture\n    def fs_target(self, fs, fs_join, fs_path):\n        \"\"\"\n        Return name of remote directory that does not yet exist to copy into.\n\n        Cleans up at the end of each test it which it is used.\n        \"\"\"\n        target = fs_join(fs_path, \"target\")\n        yield target\n        if fs.exists(target):\n            fs.rm(target, recursive=True)\n\n    @pytest.fixture\n    def local_bulk_operations_scenario_0(self, local_fs, local_join, local_path):\n        \"\"\"\n        Scenario on local filesystem that is used for many cp/get/put tests.\n\n        Cleans up at the end of each test it which it is used.\n        \"\"\"\n        source = self._bulk_operations_scenario_0(local_fs, local_join, local_path)\n        yield source\n        local_fs.rm(source, recursive=True)\n\n    @pytest.fixture\n    def local_glob_edge_cases_files(self, local_fs, local_join, local_path):\n        \"\"\"\n        Scenario on local filesystem that is used for glob edge cases cp/get/put tests.\n\n        Cleans up at the end of each test it which it is used.\n        \"\"\"\n        source = self._glob_edge_cases_files(local_fs, local_join, local_path)\n        yield source\n        local_fs.rm(source, recursive=True)\n\n    @pytest.fixture\n    def local_dir_and_file_with_same_name_prefix(\n        self, local_fs, local_join, local_path\n    ):\n        \"\"\"\n        Scenario on local filesystem that is used to check cp/get/put on directory\n        and file with the same name prefixes.\n\n        Cleans up at the end of each test it which it is used.\n        \"\"\"\n        source = self._dir_and_file_with_same_name_prefix(\n            local_fs, local_join, local_path\n        )\n        yield source\n        local_fs.rm(source, recursive=True)\n\n    @pytest.fixture\n    def local_10_files_with_hashed_names(self, local_fs, local_join, local_path):\n        \"\"\"\n        Scenario on local filesystem that is used to check cp/get/put files order\n        when source and destination are lists.\n\n        Cleans up at the end of each test it which it is used.\n        \"\"\"\n        source = self._10_files_with_hashed_names(local_fs, local_join, local_path)\n        yield source\n        local_fs.rm(source, recursive=True)\n\n    @pytest.fixture\n    def local_target(self, local_fs, local_join, local_path):\n        \"\"\"\n        Return name of local directory that does not yet exist to copy into.\n\n        Cleans up at the end of each test it which it is used.\n        \"\"\"\n        target = local_join(local_path, \"target\")\n        yield target\n        if local_fs.exists(target):\n            local_fs.rm(target, recursive=True)\n\n    def _glob_edge_cases_files(self, some_fs, some_join, some_path):\n        \"\"\"\n        Scenario that is used for glob edge cases cp/get/put tests.\n        Creates the following directory and file structure:\n\n        \ud83d\udcc1 source\n        \u251c\u2500\u2500 \ud83d\udcc4 file1\n        \u251c\u2500\u2500 \ud83d\udcc4 file2\n        \u251c\u2500\u2500 \ud83d\udcc1 subdir0\n        \u2502   \u251c\u2500\u2500 \ud83d\udcc4 subfile1\n        \u2502   \u251c\u2500\u2500 \ud83d\udcc4 subfile2\n        \u2502   \u2514\u2500\u2500 \ud83d\udcc1 nesteddir\n        \u2502       \u2514\u2500\u2500 \ud83d\udcc4 nestedfile\n        \u2514\u2500\u2500 \ud83d\udcc1 subdir1\n            \u251c\u2500\u2500 \ud83d\udcc4 subfile1\n            \u251c\u2500\u2500 \ud83d\udcc4 subfile2\n            \u2514\u2500\u2500 \ud83d\udcc1 nesteddir\n                \u2514\u2500\u2500 \ud83d\udcc4 nestedfile\n        \"\"\"\n        source = some_join(some_path, \"source\")\n        some_fs.touch(some_join(source, \"file1\"))\n        some_fs.touch(some_join(source, \"file2\"))\n\n        for subdir_idx in range(2):\n            subdir = some_join(source, f\"subdir{subdir_idx}\")\n            nesteddir = some_join(subdir, \"nesteddir\")\n            some_fs.makedirs(nesteddir)\n            some_fs.touch(some_join(subdir, \"subfile1\"))\n            some_fs.touch(some_join(subdir, \"subfile2\"))\n            some_fs.touch(some_join(nesteddir, \"nestedfile\"))\n\n        return source\n\n    def _bulk_operations_scenario_0(self, some_fs, some_join, some_path):\n        \"\"\"\n        Scenario that is used for many cp/get/put tests. Creates the following\n        directory and file structure:\n\n        \ud83d\udcc1 source\n        \u251c\u2500\u2500 \ud83d\udcc4 file1\n        \u251c\u2500\u2500 \ud83d\udcc4 file2\n        \u2514\u2500\u2500 \ud83d\udcc1 subdir\n            \u251c\u2500\u2500 \ud83d\udcc4 subfile1\n            \u251c\u2500\u2500 \ud83d\udcc4 subfile2\n            \u2514\u2500\u2500 \ud83d\udcc1 nesteddir\n                \u2514\u2500\u2500 \ud83d\udcc4 nestedfile\n        \"\"\"\n        source = some_join(some_path, \"source\")\n        subdir = some_join(source, \"subdir\")\n        nesteddir = some_join(subdir, \"nesteddir\")\n        some_fs.makedirs(nesteddir)\n        some_fs.touch(some_join(source, \"file1\"))\n        some_fs.touch(some_join(source, \"file2\"))\n        some_fs.touch(some_join(subdir, \"subfile1\"))\n        some_fs.touch(some_join(subdir, \"subfile2\"))\n        some_fs.touch(some_join(nesteddir, \"nestedfile\"))\n        return source\n\n    def _dir_and_file_with_same_name_prefix(self, some_fs, some_join, some_path):\n        \"\"\"\n        Scenario that is used to check cp/get/put on directory and file with\n        the same name prefixes. Creates the following directory and file structure:\n\n        \ud83d\udcc1 source\n        \u251c\u2500\u2500 \ud83d\udcc4 subdir.txt\n        \u2514\u2500\u2500 \ud83d\udcc1 subdir\n            \u2514\u2500\u2500 \ud83d\udcc4 subfile.txt\n        \"\"\"\n        source = some_join(some_path, \"source\")\n        subdir = some_join(source, \"subdir\")\n        file = some_join(source, \"subdir.txt\")\n        subfile = some_join(subdir, \"subfile.txt\")\n        some_fs.makedirs(subdir)\n        some_fs.touch(file)\n        some_fs.touch(subfile)\n        return source\n\n    def _10_files_with_hashed_names(self, some_fs, some_join, some_path):\n        \"\"\"\n        Scenario that is used to check cp/get/put files order when source and\n        destination are lists. Creates the following directory and file structure:\n\n        \ud83d\udcc1 source\n        \u2514\u2500\u2500 \ud83d\udcc4 {hashed([0-9])}.txt\n        \"\"\"\n        source = some_join(some_path, \"source\")\n        for i in range(10):\n            hashed_i = md5(str(i).encode(\"utf-8\")).hexdigest()\n            path = some_join(source, f\"{hashed_i}.txt\")\n            some_fs.pipe(path=path, value=f\"{i}\".encode(\"utf-8\"))\n        return source\n\n\nclass AbstractFixtures(BaseAbstractFixtures):\n    \"\"\"\n    Abstract base class containing fixtures that may be overridden in derived\n    filesystem-specific classes to run the abstract tests on such filesystems.\n\n    For any particular filesystem some of these fixtures must be overridden,\n    such as ``fs`` and ``fs_path``, and others may be overridden if the\n    default functions here are not appropriate, such as ``fs_join``.\n    \"\"\"\n\n    @pytest.fixture\n    def fs(self):\n        raise NotImplementedError(\"This function must be overridden in derived classes\")\n\n    @pytest.fixture\n    def fs_join(self):\n        \"\"\"\n        Return a function that joins its arguments together into a path.\n\n        Most fsspec implementations join paths in a platform-dependent way,\n        but some will override this to always use a forward slash.\n        \"\"\"\n        return os.path.join\n\n    @pytest.fixture\n    def fs_path(self):\n        raise NotImplementedError(\"This function must be overridden in derived classes\")\n\n    @pytest.fixture(scope=\"class\")\n    def local_fs(self):\n        # Maybe need an option for auto_mkdir=False?  This is only relevant\n        # for certain implementations.\n        return LocalFileSystem(auto_mkdir=True)\n\n    @pytest.fixture\n    def local_join(self):\n        \"\"\"\n        Return a function that joins its arguments together into a path, on\n        the local filesystem.\n        \"\"\"\n        return os.path.join\n\n    @pytest.fixture\n    def local_path(self, tmpdir):\n        return tmpdir\n\n    @pytest.fixture\n    def supports_empty_directories(self):\n        \"\"\"\n        Return whether this implementation supports empty directories.\n        \"\"\"\n        return True\n\n    @pytest.fixture\n    def fs_sanitize_path(self):\n        return lambda x: x\n", "fsspec/implementations/webhdfs.py": "# https://hadoop.apache.org/docs/r1.0.4/webhdfs.html\n\nimport logging\nimport os\nimport secrets\nimport shutil\nimport tempfile\nimport uuid\nfrom contextlib import suppress\nfrom urllib.parse import quote\n\nimport requests\n\nfrom ..spec import AbstractBufferedFile, AbstractFileSystem\nfrom ..utils import infer_storage_options, tokenize\n\nlogger = logging.getLogger(\"webhdfs\")\n\n\nclass WebHDFS(AbstractFileSystem):\n    \"\"\"\n    Interface to HDFS over HTTP using the WebHDFS API. Supports also HttpFS gateways.\n\n    Four auth mechanisms are supported:\n\n    insecure: no auth is done, and the user is assumed to be whoever they\n        say they are (parameter ``user``), or a predefined value such as\n        \"dr.who\" if not given\n    spnego: when kerberos authentication is enabled, auth is negotiated by\n        requests_kerberos https://github.com/requests/requests-kerberos .\n        This establishes a session based on existing kinit login and/or\n        specified principal/password; parameters are passed with ``kerb_kwargs``\n    token: uses an existing Hadoop delegation token from another secured\n        service. Indeed, this client can also generate such tokens when\n        not insecure. Note that tokens expire, but can be renewed (by a\n        previously specified user) and may allow for proxying.\n    basic-auth: used when both parameter ``user`` and parameter ``password``\n        are provided.\n\n    \"\"\"\n\n    tempdir = str(tempfile.gettempdir())\n    protocol = \"webhdfs\", \"webHDFS\"\n\n    def __init__(\n        self,\n        host,\n        port=50070,\n        kerberos=False,\n        token=None,\n        user=None,\n        password=None,\n        proxy_to=None,\n        kerb_kwargs=None,\n        data_proxy=None,\n        use_https=False,\n        session_cert=None,\n        session_verify=True,\n        **kwargs,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        host: str\n            Name-node address\n        port: int\n            Port for webHDFS\n        kerberos: bool\n            Whether to authenticate with kerberos for this connection\n        token: str or None\n            If given, use this token on every call to authenticate. A user\n            and user-proxy may be encoded in the token and should not be also\n            given\n        user: str or None\n            If given, assert the user name to connect with\n        password: str or None\n            If given, assert the password to use for basic auth. If password\n            is provided, user must be provided also\n        proxy_to: str or None\n            If given, the user has the authority to proxy, and this value is\n            the user in who's name actions are taken\n        kerb_kwargs: dict\n            Any extra arguments for HTTPKerberosAuth, see\n            `<https://github.com/requests/requests-kerberos/blob/master/requests_kerberos/kerberos_.py>`_\n        data_proxy: dict, callable or None\n            If given, map data-node addresses. This can be necessary if the\n            HDFS cluster is behind a proxy, running on Docker or otherwise has\n            a mismatch between the host-names given by the name-node and the\n            address by which to refer to them from the client. If a dict,\n            maps host names ``host->data_proxy[host]``; if a callable, full\n            URLs are passed, and function must conform to\n            ``url->data_proxy(url)``.\n        use_https: bool\n            Whether to connect to the Name-node using HTTPS instead of HTTP\n        session_cert: str or Tuple[str, str] or None\n            Path to a certificate file, or tuple of (cert, key) files to use\n            for the requests.Session\n        session_verify: str, bool or None\n            Path to a certificate file to use for verifying the requests.Session.\n        kwargs\n        \"\"\"\n        if self._cached:\n            return\n        super().__init__(**kwargs)\n        self.url = f\"{'https' if use_https else 'http'}://{host}:{port}/webhdfs/v1\"  # noqa\n        self.kerb = kerberos\n        self.kerb_kwargs = kerb_kwargs or {}\n        self.pars = {}\n        self.proxy = data_proxy or {}\n        if token is not None:\n            if user is not None or proxy_to is not None:\n                raise ValueError(\n                    \"If passing a delegation token, must not set \"\n                    \"user or proxy_to, as these are encoded in the\"\n                    \" token\"\n                )\n            self.pars[\"delegation\"] = token\n        self.user = user\n        self.password = password\n\n        if password is not None:\n            if user is None:\n                raise ValueError(\n                    \"If passing a password, the user must also be\"\n                    \"set in order to set up the basic-auth\"\n                )\n        else:\n            if user is not None:\n                self.pars[\"user.name\"] = user\n\n        if proxy_to is not None:\n            self.pars[\"doas\"] = proxy_to\n        if kerberos and user is not None:\n            raise ValueError(\n                \"If using Kerberos auth, do not specify the \"\n                \"user, this is handled by kinit.\"\n            )\n\n        self.session_cert = session_cert\n        self.session_verify = session_verify\n\n        self._connect()\n\n        self._fsid = f\"webhdfs_{tokenize(host, port)}\"\n\n    @property\n    def fsid(self):\n        return self._fsid\n\n    def _connect(self):\n        self.session = requests.Session()\n\n        if self.session_cert:\n            self.session.cert = self.session_cert\n\n        self.session.verify = self.session_verify\n\n        if self.kerb:\n            from requests_kerberos import HTTPKerberosAuth\n\n            self.session.auth = HTTPKerberosAuth(**self.kerb_kwargs)\n\n        if self.user is not None and self.password is not None:\n            from requests.auth import HTTPBasicAuth\n\n            self.session.auth = HTTPBasicAuth(self.user, self.password)\n\n    def _call(self, op, method=\"get\", path=None, data=None, redirect=True, **kwargs):\n        url = self._apply_proxy(self.url + quote(path or \"\", safe=\"/=\"))\n        args = kwargs.copy()\n        args.update(self.pars)\n        args[\"op\"] = op.upper()\n        logger.debug(\"sending %s with %s\", url, method)\n        out = self.session.request(\n            method=method.upper(),\n            url=url,\n            params=args,\n            data=data,\n            allow_redirects=redirect,\n        )\n        if out.status_code in [400, 401, 403, 404, 500]:\n            try:\n                err = out.json()\n                msg = err[\"RemoteException\"][\"message\"]\n                exp = err[\"RemoteException\"][\"exception\"]\n            except (ValueError, KeyError):\n                pass\n            else:\n                if exp in [\"IllegalArgumentException\", \"UnsupportedOperationException\"]:\n                    raise ValueError(msg)\n                elif exp in [\"SecurityException\", \"AccessControlException\"]:\n                    raise PermissionError(msg)\n                elif exp in [\"FileNotFoundException\"]:\n                    raise FileNotFoundError(msg)\n                else:\n                    raise RuntimeError(msg)\n        out.raise_for_status()\n        return out\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        replication=None,\n        permissions=None,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        path: str\n            File location\n        mode: str\n            'rb', 'wb', etc.\n        block_size: int\n            Client buffer size for read-ahead or write buffer\n        autocommit: bool\n            If False, writes to temporary file that only gets put in final\n            location upon commit\n        replication: int\n            Number of copies of file on the cluster, write mode only\n        permissions: str or int\n            posix permissions, write mode only\n        kwargs\n\n        Returns\n        -------\n        WebHDFile instance\n        \"\"\"\n        block_size = block_size or self.blocksize\n        return WebHDFile(\n            self,\n            path,\n            mode=mode,\n            block_size=block_size,\n            tempdir=self.tempdir,\n            autocommit=autocommit,\n            replication=replication,\n            permissions=permissions,\n        )\n\n    @staticmethod\n    def _process_info(info):\n        info[\"type\"] = info[\"type\"].lower()\n        info[\"size\"] = info[\"length\"]\n        return info\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        return infer_storage_options(path)[\"path\"]\n\n    @staticmethod\n    def _get_kwargs_from_urls(urlpath):\n        out = infer_storage_options(urlpath)\n        out.pop(\"path\", None)\n        out.pop(\"protocol\", None)\n        if \"username\" in out:\n            out[\"user\"] = out.pop(\"username\")\n        return out\n\n    def info(self, path):\n        out = self._call(\"GETFILESTATUS\", path=path)\n        info = out.json()[\"FileStatus\"]\n        info[\"name\"] = path\n        return self._process_info(info)\n\n    def ls(self, path, detail=False):\n        out = self._call(\"LISTSTATUS\", path=path)\n        infos = out.json()[\"FileStatuses\"][\"FileStatus\"]\n        for info in infos:\n            self._process_info(info)\n            info[\"name\"] = path.rstrip(\"/\") + \"/\" + info[\"pathSuffix\"]\n        if detail:\n            return sorted(infos, key=lambda i: i[\"name\"])\n        else:\n            return sorted(info[\"name\"] for info in infos)\n\n    def content_summary(self, path):\n        \"\"\"Total numbers of files, directories and bytes under path\"\"\"\n        out = self._call(\"GETCONTENTSUMMARY\", path=path)\n        return out.json()[\"ContentSummary\"]\n\n    def ukey(self, path):\n        \"\"\"Checksum info of file, giving method and result\"\"\"\n        out = self._call(\"GETFILECHECKSUM\", path=path, redirect=False)\n        if \"Location\" in out.headers:\n            location = self._apply_proxy(out.headers[\"Location\"])\n            out2 = self.session.get(location)\n            out2.raise_for_status()\n            return out2.json()[\"FileChecksum\"]\n        else:\n            out.raise_for_status()\n            return out.json()[\"FileChecksum\"]\n\n    def home_directory(self):\n        \"\"\"Get user's home directory\"\"\"\n        out = self._call(\"GETHOMEDIRECTORY\")\n        return out.json()[\"Path\"]\n\n    def get_delegation_token(self, renewer=None):\n        \"\"\"Retrieve token which can give the same authority to other uses\n\n        Parameters\n        ----------\n        renewer: str or None\n            User who may use this token; if None, will be current user\n        \"\"\"\n        if renewer:\n            out = self._call(\"GETDELEGATIONTOKEN\", renewer=renewer)\n        else:\n            out = self._call(\"GETDELEGATIONTOKEN\")\n        t = out.json()[\"Token\"]\n        if t is None:\n            raise ValueError(\"No token available for this user/security context\")\n        return t[\"urlString\"]\n\n    def renew_delegation_token(self, token):\n        \"\"\"Make token live longer. Returns new expiry time\"\"\"\n        out = self._call(\"RENEWDELEGATIONTOKEN\", method=\"put\", token=token)\n        return out.json()[\"long\"]\n\n    def cancel_delegation_token(self, token):\n        \"\"\"Stop the token from being useful\"\"\"\n        self._call(\"CANCELDELEGATIONTOKEN\", method=\"put\", token=token)\n\n    def chmod(self, path, mod):\n        \"\"\"Set the permission at path\n\n        Parameters\n        ----------\n        path: str\n            location to set (file or directory)\n        mod: str or int\n            posix epresentation or permission, give as oct string, e.g, '777'\n            or 0o777\n        \"\"\"\n        self._call(\"SETPERMISSION\", method=\"put\", path=path, permission=mod)\n\n    def chown(self, path, owner=None, group=None):\n        \"\"\"Change owning user and/or group\"\"\"\n        kwargs = {}\n        if owner is not None:\n            kwargs[\"owner\"] = owner\n        if group is not None:\n            kwargs[\"group\"] = group\n        self._call(\"SETOWNER\", method=\"put\", path=path, **kwargs)\n\n    def set_replication(self, path, replication):\n        \"\"\"\n        Set file replication factor\n\n        Parameters\n        ----------\n        path: str\n            File location (not for directories)\n        replication: int\n            Number of copies of file on the cluster. Should be smaller than\n            number of data nodes; normally 3 on most systems.\n        \"\"\"\n        self._call(\"SETREPLICATION\", path=path, method=\"put\", replication=replication)\n\n    def mkdir(self, path, **kwargs):\n        self._call(\"MKDIRS\", method=\"put\", path=path)\n\n    def makedirs(self, path, exist_ok=False):\n        if exist_ok is False and self.exists(path):\n            raise FileExistsError(path)\n        self.mkdir(path)\n\n    def mv(self, path1, path2, **kwargs):\n        self._call(\"RENAME\", method=\"put\", path=path1, destination=path2)\n\n    def rm(self, path, recursive=False, **kwargs):\n        self._call(\n            \"DELETE\",\n            method=\"delete\",\n            path=path,\n            recursive=\"true\" if recursive else \"false\",\n        )\n\n    def rm_file(self, path, **kwargs):\n        self.rm(path)\n\n    def cp_file(self, lpath, rpath, **kwargs):\n        with self.open(lpath) as lstream:\n            tmp_fname = \"/\".join([self._parent(rpath), f\".tmp.{secrets.token_hex(16)}\"])\n            # Perform an atomic copy (stream to a temporary file and\n            # move it to the actual destination).\n            try:\n                with self.open(tmp_fname, \"wb\") as rstream:\n                    shutil.copyfileobj(lstream, rstream)\n                self.mv(tmp_fname, rpath)\n            except BaseException:  # noqa\n                with suppress(FileNotFoundError):\n                    self.rm(tmp_fname)\n                raise\n\n    def _apply_proxy(self, location):\n        if self.proxy and callable(self.proxy):\n            location = self.proxy(location)\n        elif self.proxy:\n            # as a dict\n            for k, v in self.proxy.items():\n                location = location.replace(k, v, 1)\n        return location\n\n\nclass WebHDFile(AbstractBufferedFile):\n    \"\"\"A file living in HDFS over webHDFS\"\"\"\n\n    def __init__(self, fs, path, **kwargs):\n        super().__init__(fs, path, **kwargs)\n        kwargs = kwargs.copy()\n        if kwargs.get(\"permissions\", None) is None:\n            kwargs.pop(\"permissions\", None)\n        if kwargs.get(\"replication\", None) is None:\n            kwargs.pop(\"replication\", None)\n        self.permissions = kwargs.pop(\"permissions\", 511)\n        tempdir = kwargs.pop(\"tempdir\")\n        if kwargs.pop(\"autocommit\", False) is False:\n            self.target = self.path\n            self.path = os.path.join(tempdir, str(uuid.uuid4()))\n\n    def _upload_chunk(self, final=False):\n        \"\"\"Write one part of a multi-block file upload\n\n        Parameters\n        ==========\n        final: bool\n            This is the last block, so should complete file, if\n            self.autocommit is True.\n        \"\"\"\n        out = self.fs.session.post(\n            self.location,\n            data=self.buffer.getvalue(),\n            headers={\"content-type\": \"application/octet-stream\"},\n        )\n        out.raise_for_status()\n        return True\n\n    def _initiate_upload(self):\n        \"\"\"Create remote file/upload\"\"\"\n        kwargs = self.kwargs.copy()\n        if \"a\" in self.mode:\n            op, method = \"APPEND\", \"POST\"\n        else:\n            op, method = \"CREATE\", \"PUT\"\n            kwargs[\"overwrite\"] = \"true\"\n        out = self.fs._call(op, method, self.path, redirect=False, **kwargs)\n        location = self.fs._apply_proxy(out.headers[\"Location\"])\n        if \"w\" in self.mode:\n            # create empty file to append to\n            out2 = self.fs.session.put(\n                location, headers={\"content-type\": \"application/octet-stream\"}\n            )\n            out2.raise_for_status()\n            # after creating empty file, change location to append to\n            out2 = self.fs._call(\"APPEND\", \"POST\", self.path, redirect=False, **kwargs)\n            self.location = self.fs._apply_proxy(out2.headers[\"Location\"])\n\n    def _fetch_range(self, start, end):\n        start = max(start, 0)\n        end = min(self.size, end)\n        if start >= end or start >= self.size:\n            return b\"\"\n        out = self.fs._call(\n            \"OPEN\", path=self.path, offset=start, length=end - start, redirect=False\n        )\n        out.raise_for_status()\n        if \"Location\" in out.headers:\n            location = out.headers[\"Location\"]\n            out2 = self.fs.session.get(self.fs._apply_proxy(location))\n            return out2.content\n        else:\n            return out.content\n\n    def commit(self):\n        self.fs.mv(self.path, self.target)\n\n    def discard(self):\n        self.fs.rm(self.path)\n", "fsspec/implementations/dbfs.py": "import base64\nimport urllib\n\nimport requests\nimport requests.exceptions\nfrom requests.adapters import HTTPAdapter, Retry\n\nfrom fsspec import AbstractFileSystem\nfrom fsspec.spec import AbstractBufferedFile\n\n\nclass DatabricksException(Exception):\n    \"\"\"\n    Helper class for exceptions raised in this module.\n    \"\"\"\n\n    def __init__(self, error_code, message):\n        \"\"\"Create a new DatabricksException\"\"\"\n        super().__init__(message)\n\n        self.error_code = error_code\n        self.message = message\n\n\nclass DatabricksFileSystem(AbstractFileSystem):\n    \"\"\"\n    Get access to the Databricks filesystem implementation over HTTP.\n    Can be used inside and outside of a databricks cluster.\n    \"\"\"\n\n    def __init__(self, instance, token, **kwargs):\n        \"\"\"\n        Create a new DatabricksFileSystem.\n\n        Parameters\n        ----------\n        instance: str\n            The instance URL of the databricks cluster.\n            For example for an Azure databricks cluster, this\n            has the form adb-<some-number>.<two digits>.azuredatabricks.net.\n        token: str\n            Your personal token. Find out more\n            here: https://docs.databricks.com/dev-tools/api/latest/authentication.html\n        \"\"\"\n        self.instance = instance\n        self.token = token\n        self.session = requests.Session()\n        self.retries = Retry(\n            total=10,\n            backoff_factor=0.05,\n            status_forcelist=[408, 429, 500, 502, 503, 504],\n        )\n\n        self.session.mount(\"https://\", HTTPAdapter(max_retries=self.retries))\n        self.session.headers.update({\"Authorization\": f\"Bearer {self.token}\"})\n\n        super().__init__(**kwargs)\n\n    def ls(self, path, detail=True, **kwargs):\n        \"\"\"\n        List the contents of the given path.\n\n        Parameters\n        ----------\n        path: str\n            Absolute path\n        detail: bool\n            Return not only the list of filenames,\n            but also additional information on file sizes\n            and types.\n        \"\"\"\n        out = self._ls_from_cache(path)\n        if not out:\n            try:\n                r = self._send_to_api(\n                    method=\"get\", endpoint=\"list\", json={\"path\": path}\n                )\n            except DatabricksException as e:\n                if e.error_code == \"RESOURCE_DOES_NOT_EXIST\":\n                    raise FileNotFoundError(e.message)\n\n                raise e\n            files = r[\"files\"]\n            out = [\n                {\n                    \"name\": o[\"path\"],\n                    \"type\": \"directory\" if o[\"is_dir\"] else \"file\",\n                    \"size\": o[\"file_size\"],\n                }\n                for o in files\n            ]\n            self.dircache[path] = out\n\n        if detail:\n            return out\n        return [o[\"name\"] for o in out]\n\n    def makedirs(self, path, exist_ok=True):\n        \"\"\"\n        Create a given absolute path and all of its parents.\n\n        Parameters\n        ----------\n        path: str\n            Absolute path to create\n        exist_ok: bool\n            If false, checks if the folder\n            exists before creating it (and raises an\n            Exception if this is the case)\n        \"\"\"\n        if not exist_ok:\n            try:\n                # If the following succeeds, the path is already present\n                self._send_to_api(\n                    method=\"get\", endpoint=\"get-status\", json={\"path\": path}\n                )\n                raise FileExistsError(f\"Path {path} already exists\")\n            except DatabricksException as e:\n                if e.error_code == \"RESOURCE_DOES_NOT_EXIST\":\n                    pass\n\n        try:\n            self._send_to_api(method=\"post\", endpoint=\"mkdirs\", json={\"path\": path})\n        except DatabricksException as e:\n            if e.error_code == \"RESOURCE_ALREADY_EXISTS\":\n                raise FileExistsError(e.message)\n\n            raise e\n        self.invalidate_cache(self._parent(path))\n\n    def mkdir(self, path, create_parents=True, **kwargs):\n        \"\"\"\n        Create a given absolute path and all of its parents.\n\n        Parameters\n        ----------\n        path: str\n            Absolute path to create\n        create_parents: bool\n            Whether to create all parents or not.\n            \"False\" is not implemented so far.\n        \"\"\"\n        if not create_parents:\n            raise NotImplementedError\n\n        self.mkdirs(path, **kwargs)\n\n    def rm(self, path, recursive=False, **kwargs):\n        \"\"\"\n        Remove the file or folder at the given absolute path.\n\n        Parameters\n        ----------\n        path: str\n            Absolute path what to remove\n        recursive: bool\n            Recursively delete all files in a folder.\n        \"\"\"\n        try:\n            self._send_to_api(\n                method=\"post\",\n                endpoint=\"delete\",\n                json={\"path\": path, \"recursive\": recursive},\n            )\n        except DatabricksException as e:\n            # This is not really an exception, it just means\n            # not everything was deleted so far\n            if e.error_code == \"PARTIAL_DELETE\":\n                self.rm(path=path, recursive=recursive)\n            elif e.error_code == \"IO_ERROR\":\n                # Using the same exception as the os module would use here\n                raise OSError(e.message)\n\n            raise e\n        self.invalidate_cache(self._parent(path))\n\n    def mv(\n        self, source_path, destination_path, recursive=False, maxdepth=None, **kwargs\n    ):\n        \"\"\"\n        Move a source to a destination path.\n\n        A note from the original [databricks API manual]\n        (https://docs.databricks.com/dev-tools/api/latest/dbfs.html#move).\n\n        When moving a large number of files the API call will time out after\n        approximately 60s, potentially resulting in partially moved data.\n        Therefore, for operations that move more than 10k files, we strongly\n        discourage using the DBFS REST API.\n\n        Parameters\n        ----------\n        source_path: str\n            From where to move (absolute path)\n        destination_path: str\n            To where to move (absolute path)\n        recursive: bool\n            Not implemented to far.\n        maxdepth:\n            Not implemented to far.\n        \"\"\"\n        if recursive:\n            raise NotImplementedError\n        if maxdepth:\n            raise NotImplementedError\n\n        try:\n            self._send_to_api(\n                method=\"post\",\n                endpoint=\"move\",\n                json={\"source_path\": source_path, \"destination_path\": destination_path},\n            )\n        except DatabricksException as e:\n            if e.error_code == \"RESOURCE_DOES_NOT_EXIST\":\n                raise FileNotFoundError(e.message)\n            elif e.error_code == \"RESOURCE_ALREADY_EXISTS\":\n                raise FileExistsError(e.message)\n\n            raise e\n        self.invalidate_cache(self._parent(source_path))\n        self.invalidate_cache(self._parent(destination_path))\n\n    def _open(self, path, mode=\"rb\", block_size=\"default\", **kwargs):\n        \"\"\"\n        Overwrite the base class method to make sure to create a DBFile.\n        All arguments are copied from the base method.\n\n        Only the default blocksize is allowed.\n        \"\"\"\n        return DatabricksFile(self, path, mode=mode, block_size=block_size, **kwargs)\n\n    def _send_to_api(self, method, endpoint, json):\n        \"\"\"\n        Send the given json to the DBFS API\n        using a get or post request (specified by the argument `method`).\n\n        Parameters\n        ----------\n        method: str\n            Which http method to use for communication; \"get\" or \"post\".\n        endpoint: str\n            Where to send the request to (last part of the API URL)\n        json: dict\n            Dictionary of information to send\n        \"\"\"\n        if method == \"post\":\n            session_call = self.session.post\n        elif method == \"get\":\n            session_call = self.session.get\n        else:\n            raise ValueError(f\"Do not understand method {method}\")\n\n        url = urllib.parse.urljoin(f\"https://{self.instance}/api/2.0/dbfs/\", endpoint)\n\n        r = session_call(url, json=json)\n\n        # The DBFS API will return a json, also in case of an exception.\n        # We want to preserve this information as good as possible.\n        try:\n            r.raise_for_status()\n        except requests.HTTPError as e:\n            # try to extract json error message\n            # if that fails, fall back to the original exception\n            try:\n                exception_json = e.response.json()\n            except Exception:\n                raise e\n\n            raise DatabricksException(**exception_json)\n\n        return r.json()\n\n    def _create_handle(self, path, overwrite=True):\n        \"\"\"\n        Internal function to create a handle, which can be used to\n        write blocks of a file to DBFS.\n        A handle has a unique identifier which needs to be passed\n        whenever written during this transaction.\n        The handle is active for 10 minutes - after that a new\n        write transaction needs to be created.\n        Make sure to close the handle after you are finished.\n\n        Parameters\n        ----------\n        path: str\n            Absolute path for this file.\n        overwrite: bool\n            If a file already exist at this location, either overwrite\n            it or raise an exception.\n        \"\"\"\n        try:\n            r = self._send_to_api(\n                method=\"post\",\n                endpoint=\"create\",\n                json={\"path\": path, \"overwrite\": overwrite},\n            )\n            return r[\"handle\"]\n        except DatabricksException as e:\n            if e.error_code == \"RESOURCE_ALREADY_EXISTS\":\n                raise FileExistsError(e.message)\n\n            raise e\n\n    def _close_handle(self, handle):\n        \"\"\"\n        Close a handle, which was opened by :func:`_create_handle`.\n\n        Parameters\n        ----------\n        handle: str\n            Which handle to close.\n        \"\"\"\n        try:\n            self._send_to_api(method=\"post\", endpoint=\"close\", json={\"handle\": handle})\n        except DatabricksException as e:\n            if e.error_code == \"RESOURCE_DOES_NOT_EXIST\":\n                raise FileNotFoundError(e.message)\n\n            raise e\n\n    def _add_data(self, handle, data):\n        \"\"\"\n        Upload data to an already opened file handle\n        (opened by :func:`_create_handle`).\n        The maximal allowed data size is 1MB after\n        conversion to base64.\n        Remember to close the handle when you are finished.\n\n        Parameters\n        ----------\n        handle: str\n            Which handle to upload data to.\n        data: bytes\n            Block of data to add to the handle.\n        \"\"\"\n        data = base64.b64encode(data).decode()\n        try:\n            self._send_to_api(\n                method=\"post\",\n                endpoint=\"add-block\",\n                json={\"handle\": handle, \"data\": data},\n            )\n        except DatabricksException as e:\n            if e.error_code == \"RESOURCE_DOES_NOT_EXIST\":\n                raise FileNotFoundError(e.message)\n            elif e.error_code == \"MAX_BLOCK_SIZE_EXCEEDED\":\n                raise ValueError(e.message)\n\n            raise e\n\n    def _get_data(self, path, start, end):\n        \"\"\"\n        Download data in bytes from a given absolute path in a block\n        from [start, start+length].\n        The maximum number of allowed bytes to read is 1MB.\n\n        Parameters\n        ----------\n        path: str\n            Absolute path to download data from\n        start: int\n            Start position of the block\n        end: int\n            End position of the block\n        \"\"\"\n        try:\n            r = self._send_to_api(\n                method=\"get\",\n                endpoint=\"read\",\n                json={\"path\": path, \"offset\": start, \"length\": end - start},\n            )\n            return base64.b64decode(r[\"data\"])\n        except DatabricksException as e:\n            if e.error_code == \"RESOURCE_DOES_NOT_EXIST\":\n                raise FileNotFoundError(e.message)\n            elif e.error_code in [\"INVALID_PARAMETER_VALUE\", \"MAX_READ_SIZE_EXCEEDED\"]:\n                raise ValueError(e.message)\n\n            raise e\n\n    def invalidate_cache(self, path=None):\n        if path is None:\n            self.dircache.clear()\n        else:\n            self.dircache.pop(path, None)\n        super().invalidate_cache(path)\n\n\nclass DatabricksFile(AbstractBufferedFile):\n    \"\"\"\n    Helper class for files referenced in the DatabricksFileSystem.\n    \"\"\"\n\n    DEFAULT_BLOCK_SIZE = 1 * 2**20  # only allowed block size\n\n    def __init__(\n        self,\n        fs,\n        path,\n        mode=\"rb\",\n        block_size=\"default\",\n        autocommit=True,\n        cache_type=\"readahead\",\n        cache_options=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Create a new instance of the DatabricksFile.\n\n        The blocksize needs to be the default one.\n        \"\"\"\n        if block_size is None or block_size == \"default\":\n            block_size = self.DEFAULT_BLOCK_SIZE\n\n        assert (\n            block_size == self.DEFAULT_BLOCK_SIZE\n        ), f\"Only the default block size is allowed, not {block_size}\"\n\n        super().__init__(\n            fs,\n            path,\n            mode=mode,\n            block_size=block_size,\n            autocommit=autocommit,\n            cache_type=cache_type,\n            cache_options=cache_options or {},\n            **kwargs,\n        )\n\n    def _initiate_upload(self):\n        \"\"\"Internal function to start a file upload\"\"\"\n        self.handle = self.fs._create_handle(self.path)\n\n    def _upload_chunk(self, final=False):\n        \"\"\"Internal function to add a chunk of data to a started upload\"\"\"\n        self.buffer.seek(0)\n        data = self.buffer.getvalue()\n\n        data_chunks = [\n            data[start:end] for start, end in self._to_sized_blocks(len(data))\n        ]\n\n        for data_chunk in data_chunks:\n            self.fs._add_data(handle=self.handle, data=data_chunk)\n\n        if final:\n            self.fs._close_handle(handle=self.handle)\n            return True\n\n    def _fetch_range(self, start, end):\n        \"\"\"Internal function to download a block of data\"\"\"\n        return_buffer = b\"\"\n        length = end - start\n        for chunk_start, chunk_end in self._to_sized_blocks(length, start):\n            return_buffer += self.fs._get_data(\n                path=self.path, start=chunk_start, end=chunk_end\n            )\n\n        return return_buffer\n\n    def _to_sized_blocks(self, length, start=0):\n        \"\"\"Helper function to split a range from 0 to total_length into bloksizes\"\"\"\n        end = start + length\n        for data_chunk in range(start, end, self.blocksize):\n            data_start = data_chunk\n            data_end = min(end, data_chunk + self.blocksize)\n            yield data_start, data_end\n", "fsspec/implementations/local.py": "import datetime\nimport io\nimport logging\nimport os\nimport os.path as osp\nimport shutil\nimport stat\nimport tempfile\n\nfrom fsspec import AbstractFileSystem\nfrom fsspec.compression import compr\nfrom fsspec.core import get_compression\nfrom fsspec.utils import isfilelike, stringify_path\n\nlogger = logging.getLogger(\"fsspec.local\")\n\n\nclass LocalFileSystem(AbstractFileSystem):\n    \"\"\"Interface to files on local storage\n\n    Parameters\n    ----------\n    auto_mkdir: bool\n        Whether, when opening a file, the directory containing it should\n        be created (if it doesn't already exist). This is assumed by pyarrow\n        code.\n    \"\"\"\n\n    root_marker = \"/\"\n    protocol = \"file\", \"local\"\n    local_file = True\n\n    def __init__(self, auto_mkdir=False, **kwargs):\n        super().__init__(**kwargs)\n        self.auto_mkdir = auto_mkdir\n\n    @property\n    def fsid(self):\n        return \"local\"\n\n    def mkdir(self, path, create_parents=True, **kwargs):\n        path = self._strip_protocol(path)\n        if self.exists(path):\n            raise FileExistsError(path)\n        if create_parents:\n            self.makedirs(path, exist_ok=True)\n        else:\n            os.mkdir(path, **kwargs)\n\n    def makedirs(self, path, exist_ok=False):\n        path = self._strip_protocol(path)\n        os.makedirs(path, exist_ok=exist_ok)\n\n    def rmdir(self, path):\n        path = self._strip_protocol(path)\n        os.rmdir(path)\n\n    def ls(self, path, detail=False, **kwargs):\n        path = self._strip_protocol(path)\n        info = self.info(path)\n        if info[\"type\"] == \"directory\":\n            with os.scandir(path) as it:\n                infos = [self.info(f) for f in it]\n        else:\n            infos = [info]\n\n        if not detail:\n            return [i[\"name\"] for i in infos]\n        return infos\n\n    def info(self, path, **kwargs):\n        if isinstance(path, os.DirEntry):\n            # scandir DirEntry\n            out = path.stat(follow_symlinks=False)\n            link = path.is_symlink()\n            if path.is_dir(follow_symlinks=False):\n                t = \"directory\"\n            elif path.is_file(follow_symlinks=False):\n                t = \"file\"\n            else:\n                t = \"other\"\n            path = self._strip_protocol(path.path)\n        else:\n            # str or path-like\n            path = self._strip_protocol(path)\n            out = os.stat(path, follow_symlinks=False)\n            link = stat.S_ISLNK(out.st_mode)\n            if link:\n                out = os.stat(path, follow_symlinks=True)\n            if stat.S_ISDIR(out.st_mode):\n                t = \"directory\"\n            elif stat.S_ISREG(out.st_mode):\n                t = \"file\"\n            else:\n                t = \"other\"\n        result = {\n            \"name\": path,\n            \"size\": out.st_size,\n            \"type\": t,\n            \"created\": out.st_ctime,\n            \"islink\": link,\n        }\n        for field in [\"mode\", \"uid\", \"gid\", \"mtime\", \"ino\", \"nlink\"]:\n            result[field] = getattr(out, f\"st_{field}\")\n        if result[\"islink\"]:\n            result[\"destination\"] = os.readlink(path)\n            try:\n                out2 = os.stat(path, follow_symlinks=True)\n                result[\"size\"] = out2.st_size\n            except OSError:\n                result[\"size\"] = 0\n        return result\n\n    def lexists(self, path, **kwargs):\n        return osp.lexists(path)\n\n    def cp_file(self, path1, path2, **kwargs):\n        path1 = self._strip_protocol(path1)\n        path2 = self._strip_protocol(path2)\n        if self.auto_mkdir:\n            self.makedirs(self._parent(path2), exist_ok=True)\n        if self.isfile(path1):\n            shutil.copyfile(path1, path2)\n        elif self.isdir(path1):\n            self.mkdirs(path2, exist_ok=True)\n        else:\n            raise FileNotFoundError(path1)\n\n    def isfile(self, path):\n        path = self._strip_protocol(path)\n        return os.path.isfile(path)\n\n    def isdir(self, path):\n        path = self._strip_protocol(path)\n        return os.path.isdir(path)\n\n    def get_file(self, path1, path2, callback=None, **kwargs):\n        if isfilelike(path2):\n            with open(path1, \"rb\") as f:\n                shutil.copyfileobj(f, path2)\n        else:\n            return self.cp_file(path1, path2, **kwargs)\n\n    def put_file(self, path1, path2, callback=None, **kwargs):\n        return self.cp_file(path1, path2, **kwargs)\n\n    def mv(self, path1, path2, **kwargs):\n        path1 = self._strip_protocol(path1)\n        path2 = self._strip_protocol(path2)\n        shutil.move(path1, path2)\n\n    def link(self, src, dst, **kwargs):\n        src = self._strip_protocol(src)\n        dst = self._strip_protocol(dst)\n        os.link(src, dst, **kwargs)\n\n    def symlink(self, src, dst, **kwargs):\n        src = self._strip_protocol(src)\n        dst = self._strip_protocol(dst)\n        os.symlink(src, dst, **kwargs)\n\n    def islink(self, path) -> bool:\n        return os.path.islink(self._strip_protocol(path))\n\n    def rm_file(self, path):\n        os.remove(self._strip_protocol(path))\n\n    def rm(self, path, recursive=False, maxdepth=None):\n        if not isinstance(path, list):\n            path = [path]\n\n        for p in path:\n            p = self._strip_protocol(p)\n            if self.isdir(p):\n                if not recursive:\n                    raise ValueError(\"Cannot delete directory, set recursive=True\")\n                if osp.abspath(p) == os.getcwd():\n                    raise ValueError(\"Cannot delete current working directory\")\n                shutil.rmtree(p)\n            else:\n                os.remove(p)\n\n    def unstrip_protocol(self, name):\n        name = self._strip_protocol(name)  # normalise for local/win/...\n        return f\"file://{name}\"\n\n    def _open(self, path, mode=\"rb\", block_size=None, **kwargs):\n        path = self._strip_protocol(path)\n        if self.auto_mkdir and \"w\" in mode:\n            self.makedirs(self._parent(path), exist_ok=True)\n        return LocalFileOpener(path, mode, fs=self, **kwargs)\n\n    def touch(self, path, truncate=True, **kwargs):\n        path = self._strip_protocol(path)\n        if self.auto_mkdir:\n            self.makedirs(self._parent(path), exist_ok=True)\n        if self.exists(path):\n            os.utime(path, None)\n        else:\n            open(path, \"a\").close()\n        if truncate:\n            os.truncate(path, 0)\n\n    def created(self, path):\n        info = self.info(path=path)\n        return datetime.datetime.fromtimestamp(\n            info[\"created\"], tz=datetime.timezone.utc\n        )\n\n    def modified(self, path):\n        info = self.info(path=path)\n        return datetime.datetime.fromtimestamp(info[\"mtime\"], tz=datetime.timezone.utc)\n\n    @classmethod\n    def _parent(cls, path):\n        path = cls._strip_protocol(path)\n        if os.sep == \"/\":\n            # posix native\n            return path.rsplit(\"/\", 1)[0] or \"/\"\n        else:\n            # NT\n            path_ = path.rsplit(\"/\", 1)[0]\n            if len(path_) <= 3:\n                if path_[1:2] == \":\":\n                    # nt root (something like c:/)\n                    return path_[0] + \":/\"\n            # More cases may be required here\n            return path_\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        path = stringify_path(path)\n        if path.startswith(\"file://\"):\n            path = path[7:]\n        elif path.startswith(\"file:\"):\n            path = path[5:]\n        elif path.startswith(\"local://\"):\n            path = path[8:]\n        elif path.startswith(\"local:\"):\n            path = path[6:]\n\n        path = make_path_posix(path)\n        if os.sep != \"/\":\n            # This code-path is a stripped down version of\n            # > drive, path = ntpath.splitdrive(path)\n            if path[1:2] == \":\":\n                # Absolute drive-letter path, e.g. X:\\Windows\n                # Relative path with drive, e.g. X:Windows\n                drive, path = path[:2], path[2:]\n            elif path[:2] == \"//\":\n                # UNC drives, e.g. \\\\server\\share or \\\\?\\UNC\\server\\share\n                # Device drives, e.g. \\\\.\\device or \\\\?\\device\n                if (index1 := path.find(\"/\", 2)) == -1 or (\n                    index2 := path.find(\"/\", index1 + 1)\n                ) == -1:\n                    drive, path = path, \"\"\n                else:\n                    drive, path = path[:index2], path[index2:]\n            else:\n                # Relative path, e.g. Windows\n                drive = \"\"\n\n            path = path.rstrip(\"/\") or cls.root_marker\n            return drive + path\n\n        else:\n            return path.rstrip(\"/\") or cls.root_marker\n\n    def _isfilestore(self):\n        # Inheriting from DaskFileSystem makes this False (S3, etc. were)\n        # the original motivation. But we are a posix-like file system.\n        # See https://github.com/dask/dask/issues/5526\n        return True\n\n    def chmod(self, path, mode):\n        path = stringify_path(path)\n        return os.chmod(path, mode)\n\n\ndef make_path_posix(path):\n    \"\"\"Make path generic and absolute for current OS\"\"\"\n    if not isinstance(path, str):\n        if isinstance(path, (list, set, tuple)):\n            return type(path)(make_path_posix(p) for p in path)\n        else:\n            path = stringify_path(path)\n            if not isinstance(path, str):\n                raise TypeError(f\"could not convert {path!r} to string\")\n    if os.sep == \"/\":\n        # Native posix\n        if path.startswith(\"/\"):\n            # most common fast case for posix\n            return path\n        elif path.startswith(\"~\"):\n            return osp.expanduser(path)\n        elif path.startswith(\"./\"):\n            path = path[2:]\n        elif path == \".\":\n            path = \"\"\n        return f\"{os.getcwd()}/{path}\"\n    else:\n        # NT handling\n        if path[0:1] == \"/\" and path[2:3] == \":\":\n            # path is like \"/c:/local/path\"\n            path = path[1:]\n        if path[1:2] == \":\":\n            # windows full path like \"C:\\\\local\\\\path\"\n            if len(path) <= 3:\n                # nt root (something like c:/)\n                return path[0] + \":/\"\n            path = path.replace(\"\\\\\", \"/\")\n            return path\n        elif path[0:1] == \"~\":\n            return make_path_posix(osp.expanduser(path))\n        elif path.startswith((\"\\\\\\\\\", \"//\")):\n            # windows UNC/DFS-style paths\n            return \"//\" + path[2:].replace(\"\\\\\", \"/\")\n        elif path.startswith((\"\\\\\", \"/\")):\n            # windows relative path with root\n            path = path.replace(\"\\\\\", \"/\")\n            return f\"{osp.splitdrive(os.getcwd())[0]}{path}\"\n        else:\n            path = path.replace(\"\\\\\", \"/\")\n            if path.startswith(\"./\"):\n                path = path[2:]\n            elif path == \".\":\n                path = \"\"\n            return f\"{make_path_posix(os.getcwd())}/{path}\"\n\n\ndef trailing_sep(path):\n    \"\"\"Return True if the path ends with a path separator.\n\n    A forward slash is always considered a path separator, even on Operating\n    Systems that normally use a backslash.\n    \"\"\"\n    # TODO: if all incoming paths were posix-compliant then separator would\n    # always be a forward slash, simplifying this function.\n    # See https://github.com/fsspec/filesystem_spec/pull/1250\n    return path.endswith(os.sep) or (os.altsep is not None and path.endswith(os.altsep))\n\n\nclass LocalFileOpener(io.IOBase):\n    def __init__(\n        self, path, mode, autocommit=True, fs=None, compression=None, **kwargs\n    ):\n        logger.debug(\"open file: %s\", path)\n        self.path = path\n        self.mode = mode\n        self.fs = fs\n        self.f = None\n        self.autocommit = autocommit\n        self.compression = get_compression(path, compression)\n        self.blocksize = io.DEFAULT_BUFFER_SIZE\n        self._open()\n\n    def _open(self):\n        if self.f is None or self.f.closed:\n            if self.autocommit or \"w\" not in self.mode:\n                self.f = open(self.path, mode=self.mode)\n                if self.compression:\n                    compress = compr[self.compression]\n                    self.f = compress(self.f, mode=self.mode)\n            else:\n                # TODO: check if path is writable?\n                i, name = tempfile.mkstemp()\n                os.close(i)  # we want normal open and normal buffered file\n                self.temp = name\n                self.f = open(name, mode=self.mode)\n            if \"w\" not in self.mode:\n                self.size = self.f.seek(0, 2)\n                self.f.seek(0)\n                self.f.size = self.size\n\n    def _fetch_range(self, start, end):\n        # probably only used by cached FS\n        if \"r\" not in self.mode:\n            raise ValueError\n        self._open()\n        self.f.seek(start)\n        return self.f.read(end - start)\n\n    def __setstate__(self, state):\n        self.f = None\n        loc = state.pop(\"loc\", None)\n        self.__dict__.update(state)\n        if \"r\" in state[\"mode\"]:\n            self.f = None\n            self._open()\n            self.f.seek(loc)\n\n    def __getstate__(self):\n        d = self.__dict__.copy()\n        d.pop(\"f\")\n        if \"r\" in self.mode:\n            d[\"loc\"] = self.f.tell()\n        else:\n            if not self.f.closed:\n                raise ValueError(\"Cannot serialise open write-mode local file\")\n        return d\n\n    def commit(self):\n        if self.autocommit:\n            raise RuntimeError(\"Can only commit if not already set to autocommit\")\n        shutil.move(self.temp, self.path)\n\n    def discard(self):\n        if self.autocommit:\n            raise RuntimeError(\"Cannot discard if set to autocommit\")\n        os.remove(self.temp)\n\n    def readable(self) -> bool:\n        return True\n\n    def writable(self) -> bool:\n        return \"r\" not in self.mode\n\n    def read(self, *args, **kwargs):\n        return self.f.read(*args, **kwargs)\n\n    def write(self, *args, **kwargs):\n        return self.f.write(*args, **kwargs)\n\n    def tell(self, *args, **kwargs):\n        return self.f.tell(*args, **kwargs)\n\n    def seek(self, *args, **kwargs):\n        return self.f.seek(*args, **kwargs)\n\n    def seekable(self, *args, **kwargs):\n        return self.f.seekable(*args, **kwargs)\n\n    def readline(self, *args, **kwargs):\n        return self.f.readline(*args, **kwargs)\n\n    def readlines(self, *args, **kwargs):\n        return self.f.readlines(*args, **kwargs)\n\n    def close(self):\n        return self.f.close()\n\n    def truncate(self, size=None) -> int:\n        return self.f.truncate(size)\n\n    @property\n    def closed(self):\n        return self.f.closed\n\n    def fileno(self):\n        return self.raw.fileno()\n\n    def flush(self) -> None:\n        self.f.flush()\n\n    def __iter__(self):\n        return self.f.__iter__()\n\n    def __getattr__(self, item):\n        return getattr(self.f, item)\n\n    def __enter__(self):\n        self._incontext = True\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._incontext = False\n        self.f.__exit__(exc_type, exc_value, traceback)\n", "fsspec/implementations/cache_metadata.py": "from __future__ import annotations\n\nimport os\nimport pickle\nimport time\nfrom typing import TYPE_CHECKING\n\nfrom fsspec.utils import atomic_write\n\ntry:\n    import ujson as json\nexcept ImportError:\n    if not TYPE_CHECKING:\n        import json\n\nif TYPE_CHECKING:\n    from typing import Any, Dict, Iterator, Literal\n\n    from typing_extensions import TypeAlias\n\n    from .cached import CachingFileSystem\n\n    Detail: TypeAlias = Dict[str, Any]\n\n\nclass CacheMetadata:\n    \"\"\"Cache metadata.\n\n    All reading and writing of cache metadata is performed by this class,\n    accessing the cached files and blocks is not.\n\n    Metadata is stored in a single file per storage directory in JSON format.\n    For backward compatibility, also reads metadata stored in pickle format\n    which is converted to JSON when next saved.\n    \"\"\"\n\n    def __init__(self, storage: list[str]):\n        \"\"\"\n\n        Parameters\n        ----------\n        storage: list[str]\n            Directories containing cached files, must be at least one. Metadata\n            is stored in the last of these directories by convention.\n        \"\"\"\n        if not storage:\n            raise ValueError(\"CacheMetadata expects at least one storage location\")\n\n        self._storage = storage\n        self.cached_files: list[Detail] = [{}]\n\n        # Private attribute to force saving of metadata in pickle format rather than\n        # JSON for use in tests to confirm can read both pickle and JSON formats.\n        self._force_save_pickle = False\n\n    def _load(self, fn: str) -> Detail:\n        \"\"\"Low-level function to load metadata from specific file\"\"\"\n        try:\n            with open(fn, \"r\") as f:\n                loaded = json.load(f)\n        except ValueError:\n            with open(fn, \"rb\") as f:\n                loaded = pickle.load(f)\n        for c in loaded.values():\n            if isinstance(c.get(\"blocks\"), list):\n                c[\"blocks\"] = set(c[\"blocks\"])\n        return loaded\n\n    def _save(self, metadata_to_save: Detail, fn: str) -> None:\n        \"\"\"Low-level function to save metadata to specific file\"\"\"\n        if self._force_save_pickle:\n            with atomic_write(fn) as f:\n                pickle.dump(metadata_to_save, f)\n        else:\n            with atomic_write(fn, mode=\"w\") as f:\n                json.dump(metadata_to_save, f)\n\n    def _scan_locations(\n        self, writable_only: bool = False\n    ) -> Iterator[tuple[str, str, bool]]:\n        \"\"\"Yield locations (filenames) where metadata is stored, and whether\n        writable or not.\n\n        Parameters\n        ----------\n        writable: bool\n            Set to True to only yield writable locations.\n\n        Returns\n        -------\n        Yields (str, str, bool)\n        \"\"\"\n        n = len(self._storage)\n        for i, storage in enumerate(self._storage):\n            writable = i == n - 1\n            if writable_only and not writable:\n                continue\n            yield os.path.join(storage, \"cache\"), storage, writable\n\n    def check_file(\n        self, path: str, cfs: CachingFileSystem | None\n    ) -> Literal[False] | tuple[Detail, str]:\n        \"\"\"If path is in cache return its details, otherwise return ``False``.\n\n        If the optional CachingFileSystem is specified then it is used to\n        perform extra checks to reject possible matches, such as if they are\n        too old.\n        \"\"\"\n        for (fn, base, _), cache in zip(self._scan_locations(), self.cached_files):\n            if path not in cache:\n                continue\n            detail = cache[path].copy()\n\n            if cfs is not None:\n                if cfs.check_files and detail[\"uid\"] != cfs.fs.ukey(path):\n                    # Wrong file as determined by hash of file properties\n                    continue\n                if cfs.expiry and time.time() - detail[\"time\"] > cfs.expiry:\n                    # Cached file has expired\n                    continue\n\n            fn = os.path.join(base, detail[\"fn\"])\n            if os.path.exists(fn):\n                return detail, fn\n        return False\n\n    def clear_expired(self, expiry_time: int) -> tuple[list[str], bool]:\n        \"\"\"Remove expired metadata from the cache.\n\n        Returns names of files corresponding to expired metadata and a boolean\n        flag indicating whether the writable cache is empty. Caller is\n        responsible for deleting the expired files.\n        \"\"\"\n        expired_files = []\n        for path, detail in self.cached_files[-1].copy().items():\n            if time.time() - detail[\"time\"] > expiry_time:\n                fn = detail.get(\"fn\", \"\")\n                if not fn:\n                    raise RuntimeError(\n                        f\"Cache metadata does not contain 'fn' for {path}\"\n                    )\n                fn = os.path.join(self._storage[-1], fn)\n                expired_files.append(fn)\n                self.cached_files[-1].pop(path)\n\n        if self.cached_files[-1]:\n            cache_path = os.path.join(self._storage[-1], \"cache\")\n            self._save(self.cached_files[-1], cache_path)\n\n        writable_cache_empty = not self.cached_files[-1]\n        return expired_files, writable_cache_empty\n\n    def load(self) -> None:\n        \"\"\"Load all metadata from disk and store in ``self.cached_files``\"\"\"\n        cached_files = []\n        for fn, _, _ in self._scan_locations():\n            if os.path.exists(fn):\n                # TODO: consolidate blocks here\n                cached_files.append(self._load(fn))\n            else:\n                cached_files.append({})\n        self.cached_files = cached_files or [{}]\n\n    def on_close_cached_file(self, f: Any, path: str) -> None:\n        \"\"\"Perform side-effect actions on closing a cached file.\n\n        The actual closing of the file is the responsibility of the caller.\n        \"\"\"\n        # File must be writeble, so in self.cached_files[-1]\n        c = self.cached_files[-1][path]\n        if c[\"blocks\"] is not True and len(c[\"blocks\"]) * f.blocksize >= f.size:\n            c[\"blocks\"] = True\n\n    def pop_file(self, path: str) -> str | None:\n        \"\"\"Remove metadata of cached file.\n\n        If path is in the cache, return the filename of the cached file,\n        otherwise return ``None``.  Caller is responsible for deleting the\n        cached file.\n        \"\"\"\n        details = self.check_file(path, None)\n        if not details:\n            return None\n        _, fn = details\n        if fn.startswith(self._storage[-1]):\n            self.cached_files[-1].pop(path)\n            self.save()\n        else:\n            raise PermissionError(\n                \"Can only delete cached file in last, writable cache location\"\n            )\n        return fn\n\n    def save(self) -> None:\n        \"\"\"Save metadata to disk\"\"\"\n        for (fn, _, writable), cache in zip(self._scan_locations(), self.cached_files):\n            if not writable:\n                continue\n\n            if os.path.exists(fn):\n                cached_files = self._load(fn)\n                for k, c in cached_files.items():\n                    if k in cache:\n                        if c[\"blocks\"] is True or cache[k][\"blocks\"] is True:\n                            c[\"blocks\"] = True\n                        else:\n                            # self.cached_files[*][*][\"blocks\"] must continue to\n                            # point to the same set object so that updates\n                            # performed by MMapCache are propagated back to\n                            # self.cached_files.\n                            blocks = cache[k][\"blocks\"]\n                            blocks.update(c[\"blocks\"])\n                            c[\"blocks\"] = blocks\n                        c[\"time\"] = max(c[\"time\"], cache[k][\"time\"])\n                        c[\"uid\"] = cache[k][\"uid\"]\n\n                # Files can be added to cache after it was written once\n                for k, c in cache.items():\n                    if k not in cached_files:\n                        cached_files[k] = c\n            else:\n                cached_files = cache\n            cache = {k: v.copy() for k, v in cached_files.items()}\n            for c in cache.values():\n                if isinstance(c[\"blocks\"], set):\n                    c[\"blocks\"] = list(c[\"blocks\"])\n            self._save(cache, fn)\n            self.cached_files[-1] = cached_files\n\n    def update_file(self, path: str, detail: Detail) -> None:\n        \"\"\"Update metadata for specific file in memory, do not save\"\"\"\n        self.cached_files[-1][path] = detail\n", "fsspec/implementations/sftp.py": "import datetime\nimport logging\nimport os\nimport types\nimport uuid\nfrom stat import S_ISDIR, S_ISLNK\n\nimport paramiko\n\nfrom .. import AbstractFileSystem\nfrom ..utils import infer_storage_options\n\nlogger = logging.getLogger(\"fsspec.sftp\")\n\n\nclass SFTPFileSystem(AbstractFileSystem):\n    \"\"\"Files over SFTP/SSH\n\n    Peer-to-peer filesystem over SSH using paramiko.\n\n    Note: if using this with the ``open`` or ``open_files``, with full URLs,\n    there is no way to tell if a path is relative, so all paths are assumed\n    to be absolute.\n    \"\"\"\n\n    protocol = \"sftp\", \"ssh\"\n\n    def __init__(self, host, **ssh_kwargs):\n        \"\"\"\n\n        Parameters\n        ----------\n        host: str\n            Hostname or IP as a string\n        temppath: str\n            Location on the server to put files, when within a transaction\n        ssh_kwargs: dict\n            Parameters passed on to connection. See details in\n            https://docs.paramiko.org/en/3.3/api/client.html#paramiko.client.SSHClient.connect\n            May include port, username, password...\n        \"\"\"\n        if self._cached:\n            return\n        super().__init__(**ssh_kwargs)\n        self.temppath = ssh_kwargs.pop(\"temppath\", \"/tmp\")  # remote temp directory\n        self.host = host\n        self.ssh_kwargs = ssh_kwargs\n        self._connect()\n\n    def _connect(self):\n        logger.debug(\"Connecting to SFTP server %s\", self.host)\n        self.client = paramiko.SSHClient()\n        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        self.client.connect(self.host, **self.ssh_kwargs)\n        self.ftp = self.client.open_sftp()\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        return infer_storage_options(path)[\"path\"]\n\n    @staticmethod\n    def _get_kwargs_from_urls(urlpath):\n        out = infer_storage_options(urlpath)\n        out.pop(\"path\", None)\n        out.pop(\"protocol\", None)\n        return out\n\n    def mkdir(self, path, create_parents=True, mode=511):\n        logger.debug(\"Creating folder %s\", path)\n        if self.exists(path):\n            raise FileExistsError(f\"File exists: {path}\")\n\n        if create_parents:\n            self.makedirs(path)\n        else:\n            self.ftp.mkdir(path, mode)\n\n    def makedirs(self, path, exist_ok=False, mode=511):\n        if self.exists(path) and not exist_ok:\n            raise FileExistsError(f\"File exists: {path}\")\n\n        parts = path.split(\"/\")\n        new_path = \"/\" if path[:1] == \"/\" else \"\"\n\n        for part in parts:\n            if part:\n                new_path = f\"{new_path}/{part}\" if new_path else part\n                if not self.exists(new_path):\n                    self.ftp.mkdir(new_path, mode)\n\n    def rmdir(self, path):\n        logger.debug(\"Removing folder %s\", path)\n        self.ftp.rmdir(path)\n\n    def info(self, path):\n        stat = self._decode_stat(self.ftp.stat(path))\n        stat[\"name\"] = path\n        return stat\n\n    @staticmethod\n    def _decode_stat(stat, parent_path=None):\n        if S_ISDIR(stat.st_mode):\n            t = \"directory\"\n        elif S_ISLNK(stat.st_mode):\n            t = \"link\"\n        else:\n            t = \"file\"\n        out = {\n            \"name\": \"\",\n            \"size\": stat.st_size,\n            \"type\": t,\n            \"uid\": stat.st_uid,\n            \"gid\": stat.st_gid,\n            \"time\": datetime.datetime.fromtimestamp(\n                stat.st_atime, tz=datetime.timezone.utc\n            ),\n            \"mtime\": datetime.datetime.fromtimestamp(\n                stat.st_mtime, tz=datetime.timezone.utc\n            ),\n        }\n        if parent_path:\n            out[\"name\"] = \"/\".join([parent_path.rstrip(\"/\"), stat.filename])\n        return out\n\n    def ls(self, path, detail=False):\n        logger.debug(\"Listing folder %s\", path)\n        stats = [self._decode_stat(stat, path) for stat in self.ftp.listdir_iter(path)]\n        if detail:\n            return stats\n        else:\n            paths = [stat[\"name\"] for stat in stats]\n            return sorted(paths)\n\n    def put(self, lpath, rpath, callback=None, **kwargs):\n        logger.debug(\"Put file %s into %s\", lpath, rpath)\n        self.ftp.put(lpath, rpath)\n\n    def get_file(self, rpath, lpath, **kwargs):\n        if self.isdir(rpath):\n            os.makedirs(lpath, exist_ok=True)\n        else:\n            self.ftp.get(self._strip_protocol(rpath), lpath)\n\n    def _open(self, path, mode=\"rb\", block_size=None, **kwargs):\n        \"\"\"\n        block_size: int or None\n            If 0, no buffering, if 1, line buffering, if >1, buffer that many\n            bytes, if None use default from paramiko.\n        \"\"\"\n        logger.debug(\"Opening file %s\", path)\n        if kwargs.get(\"autocommit\", True) is False:\n            # writes to temporary file, move on commit\n            path2 = \"/\".join([self.temppath, str(uuid.uuid4())])\n            f = self.ftp.open(path2, mode, bufsize=block_size if block_size else -1)\n            f.temppath = path2\n            f.targetpath = path\n            f.fs = self\n            f.commit = types.MethodType(commit_a_file, f)\n            f.discard = types.MethodType(discard_a_file, f)\n        else:\n            f = self.ftp.open(path, mode, bufsize=block_size if block_size else -1)\n        return f\n\n    def _rm(self, path):\n        if self.isdir(path):\n            self.ftp.rmdir(path)\n        else:\n            self.ftp.remove(path)\n\n    def mv(self, old, new):\n        logger.debug(\"Renaming %s into %s\", old, new)\n        self.ftp.posix_rename(old, new)\n\n\ndef commit_a_file(self):\n    self.fs.mv(self.temppath, self.targetpath)\n\n\ndef discard_a_file(self):\n    self.fs._rm(self.temppath)\n", "fsspec/implementations/http.py": "import asyncio\nimport io\nimport logging\nimport re\nimport weakref\nfrom copy import copy\nfrom urllib.parse import urlparse\n\nimport aiohttp\nimport yarl\n\nfrom fsspec.asyn import AbstractAsyncStreamedFile, AsyncFileSystem, sync, sync_wrapper\nfrom fsspec.callbacks import DEFAULT_CALLBACK\nfrom fsspec.exceptions import FSTimeoutError\nfrom fsspec.spec import AbstractBufferedFile\nfrom fsspec.utils import (\n    DEFAULT_BLOCK_SIZE,\n    glob_translate,\n    isfilelike,\n    nullcontext,\n    tokenize,\n)\n\nfrom ..caching import AllBytes\n\n# https://stackoverflow.com/a/15926317/3821154\nex = re.compile(r\"\"\"<(a|A)\\s+(?:[^>]*?\\s+)?(href|HREF)=[\"'](?P<url>[^\"']+)\"\"\")\nex2 = re.compile(r\"\"\"(?P<url>http[s]?://[-a-zA-Z0-9@:%_+.~#?&/=]+)\"\"\")\nlogger = logging.getLogger(\"fsspec.http\")\n\n\nasync def get_client(**kwargs):\n    return aiohttp.ClientSession(**kwargs)\n\n\nclass HTTPFileSystem(AsyncFileSystem):\n    \"\"\"\n    Simple File-System for fetching data via HTTP(S)\n\n    ``ls()`` is implemented by loading the parent page and doing a regex\n    match on the result. If simple_link=True, anything of the form\n    \"http(s)://server.com/stuff?thing=other\"; otherwise only links within\n    HTML href tags will be used.\n    \"\"\"\n\n    sep = \"/\"\n\n    def __init__(\n        self,\n        simple_links=True,\n        block_size=None,\n        same_scheme=True,\n        size_policy=None,\n        cache_type=\"bytes\",\n        cache_options=None,\n        asynchronous=False,\n        loop=None,\n        client_kwargs=None,\n        get_client=get_client,\n        encoded=False,\n        **storage_options,\n    ):\n        \"\"\"\n        NB: if this is called async, you must await set_client\n\n        Parameters\n        ----------\n        block_size: int\n            Blocks to read bytes; if 0, will default to raw requests file-like\n            objects instead of HTTPFile instances\n        simple_links: bool\n            If True, will consider both HTML <a> tags and anything that looks\n            like a URL; if False, will consider only the former.\n        same_scheme: True\n            When doing ls/glob, if this is True, only consider paths that have\n            http/https matching the input URLs.\n        size_policy: this argument is deprecated\n        client_kwargs: dict\n            Passed to aiohttp.ClientSession, see\n            https://docs.aiohttp.org/en/stable/client_reference.html\n            For example, ``{'auth': aiohttp.BasicAuth('user', 'pass')}``\n        get_client: Callable[..., aiohttp.ClientSession]\n            A callable which takes keyword arguments and constructs\n            an aiohttp.ClientSession. It's state will be managed by\n            the HTTPFileSystem class.\n        storage_options: key-value\n            Any other parameters passed on to requests\n        cache_type, cache_options: defaults used in open\n        \"\"\"\n        super().__init__(self, asynchronous=asynchronous, loop=loop, **storage_options)\n        self.block_size = block_size if block_size is not None else DEFAULT_BLOCK_SIZE\n        self.simple_links = simple_links\n        self.same_schema = same_scheme\n        self.cache_type = cache_type\n        self.cache_options = cache_options\n        self.client_kwargs = client_kwargs or {}\n        self.get_client = get_client\n        self.encoded = encoded\n        self.kwargs = storage_options\n        self._session = None\n\n        # Clean caching-related parameters from `storage_options`\n        # before propagating them as `request_options` through `self.kwargs`.\n        # TODO: Maybe rename `self.kwargs` to `self.request_options` to make\n        #       it clearer.\n        request_options = copy(storage_options)\n        self.use_listings_cache = request_options.pop(\"use_listings_cache\", False)\n        request_options.pop(\"listings_expiry_time\", None)\n        request_options.pop(\"max_paths\", None)\n        request_options.pop(\"skip_instance_cache\", None)\n        self.kwargs = request_options\n\n    @property\n    def fsid(self):\n        return \"http\"\n\n    def encode_url(self, url):\n        return yarl.URL(url, encoded=self.encoded)\n\n    @staticmethod\n    def close_session(loop, session):\n        if loop is not None and loop.is_running():\n            try:\n                sync(loop, session.close, timeout=0.1)\n                return\n            except (TimeoutError, FSTimeoutError, NotImplementedError):\n                pass\n        connector = getattr(session, \"_connector\", None)\n        if connector is not None:\n            # close after loop is dead\n            connector._close()\n\n    async def set_session(self):\n        if self._session is None:\n            self._session = await self.get_client(loop=self.loop, **self.client_kwargs)\n            if not self.asynchronous:\n                weakref.finalize(self, self.close_session, self.loop, self._session)\n        return self._session\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        \"\"\"For HTTP, we always want to keep the full URL\"\"\"\n        return path\n\n    @classmethod\n    def _parent(cls, path):\n        # override, since _strip_protocol is different for URLs\n        par = super()._parent(path)\n        if len(par) > 7:  # \"http://...\"\n            return par\n        return \"\"\n\n    async def _ls_real(self, url, detail=True, **kwargs):\n        # ignoring URL-encoded arguments\n        kw = self.kwargs.copy()\n        kw.update(kwargs)\n        logger.debug(url)\n        session = await self.set_session()\n        async with session.get(self.encode_url(url), **self.kwargs) as r:\n            self._raise_not_found_for_status(r, url)\n            try:\n                text = await r.text()\n                if self.simple_links:\n                    links = ex2.findall(text) + [u[2] for u in ex.findall(text)]\n                else:\n                    links = [u[2] for u in ex.findall(text)]\n            except UnicodeDecodeError:\n                links = []  # binary, not HTML\n        out = set()\n        parts = urlparse(url)\n        for l in links:\n            if isinstance(l, tuple):\n                l = l[1]\n            if l.startswith(\"/\") and len(l) > 1:\n                # absolute URL on this server\n                l = f\"{parts.scheme}://{parts.netloc}{l}\"\n            if l.startswith(\"http\"):\n                if self.same_schema and l.startswith(url.rstrip(\"/\") + \"/\"):\n                    out.add(l)\n                elif l.replace(\"https\", \"http\").startswith(\n                    url.replace(\"https\", \"http\").rstrip(\"/\") + \"/\"\n                ):\n                    # allowed to cross http <-> https\n                    out.add(l)\n            else:\n                if l not in [\"..\", \"../\"]:\n                    # Ignore FTP-like \"parent\"\n                    out.add(\"/\".join([url.rstrip(\"/\"), l.lstrip(\"/\")]))\n        if not out and url.endswith(\"/\"):\n            out = await self._ls_real(url.rstrip(\"/\"), detail=False)\n        if detail:\n            return [\n                {\n                    \"name\": u,\n                    \"size\": None,\n                    \"type\": \"directory\" if u.endswith(\"/\") else \"file\",\n                }\n                for u in out\n            ]\n        else:\n            return sorted(out)\n\n    async def _ls(self, url, detail=True, **kwargs):\n        if self.use_listings_cache and url in self.dircache:\n            out = self.dircache[url]\n        else:\n            out = await self._ls_real(url, detail=detail, **kwargs)\n            self.dircache[url] = out\n        return out\n\n    ls = sync_wrapper(_ls)\n\n    def _raise_not_found_for_status(self, response, url):\n        \"\"\"\n        Raises FileNotFoundError for 404s, otherwise uses raise_for_status.\n        \"\"\"\n        if response.status == 404:\n            raise FileNotFoundError(url)\n        response.raise_for_status()\n\n    async def _cat_file(self, url, start=None, end=None, **kwargs):\n        kw = self.kwargs.copy()\n        kw.update(kwargs)\n        logger.debug(url)\n\n        if start is not None or end is not None:\n            if start == end:\n                return b\"\"\n            headers = kw.pop(\"headers\", {}).copy()\n\n            headers[\"Range\"] = await self._process_limits(url, start, end)\n            kw[\"headers\"] = headers\n        session = await self.set_session()\n        async with session.get(self.encode_url(url), **kw) as r:\n            out = await r.read()\n            self._raise_not_found_for_status(r, url)\n        return out\n\n    async def _get_file(\n        self, rpath, lpath, chunk_size=5 * 2**20, callback=DEFAULT_CALLBACK, **kwargs\n    ):\n        kw = self.kwargs.copy()\n        kw.update(kwargs)\n        logger.debug(rpath)\n        session = await self.set_session()\n        async with session.get(self.encode_url(rpath), **kw) as r:\n            try:\n                size = int(r.headers[\"content-length\"])\n            except (ValueError, KeyError):\n                size = None\n\n            callback.set_size(size)\n            self._raise_not_found_for_status(r, rpath)\n            if isfilelike(lpath):\n                outfile = lpath\n            else:\n                outfile = open(lpath, \"wb\")  # noqa: ASYNC101\n\n            try:\n                chunk = True\n                while chunk:\n                    chunk = await r.content.read(chunk_size)\n                    outfile.write(chunk)\n                    callback.relative_update(len(chunk))\n            finally:\n                if not isfilelike(lpath):\n                    outfile.close()\n\n    async def _put_file(\n        self,\n        lpath,\n        rpath,\n        chunk_size=5 * 2**20,\n        callback=DEFAULT_CALLBACK,\n        method=\"post\",\n        **kwargs,\n    ):\n        async def gen_chunks():\n            # Support passing arbitrary file-like objects\n            # and use them instead of streams.\n            if isinstance(lpath, io.IOBase):\n                context = nullcontext(lpath)\n                use_seek = False  # might not support seeking\n            else:\n                context = open(lpath, \"rb\")  # noqa: ASYNC101\n                use_seek = True\n\n            with context as f:\n                if use_seek:\n                    callback.set_size(f.seek(0, 2))\n                    f.seek(0)\n                else:\n                    callback.set_size(getattr(f, \"size\", None))\n\n                chunk = f.read(chunk_size)\n                while chunk:\n                    yield chunk\n                    callback.relative_update(len(chunk))\n                    chunk = f.read(chunk_size)\n\n        kw = self.kwargs.copy()\n        kw.update(kwargs)\n        session = await self.set_session()\n\n        method = method.lower()\n        if method not in (\"post\", \"put\"):\n            raise ValueError(\n                f\"method has to be either 'post' or 'put', not: {method!r}\"\n            )\n\n        meth = getattr(session, method)\n        async with meth(self.encode_url(rpath), data=gen_chunks(), **kw) as resp:\n            self._raise_not_found_for_status(resp, rpath)\n\n    async def _exists(self, path, **kwargs):\n        kw = self.kwargs.copy()\n        kw.update(kwargs)\n        try:\n            logger.debug(path)\n            session = await self.set_session()\n            r = await session.get(self.encode_url(path), **kw)\n            async with r:\n                return r.status < 400\n        except aiohttp.ClientError:\n            return False\n\n    async def _isfile(self, path, **kwargs):\n        return await self._exists(path, **kwargs)\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=None,  # XXX: This differs from the base class.\n        cache_type=None,\n        cache_options=None,\n        size=None,\n        **kwargs,\n    ):\n        \"\"\"Make a file-like object\n\n        Parameters\n        ----------\n        path: str\n            Full URL with protocol\n        mode: string\n            must be \"rb\"\n        block_size: int or None\n            Bytes to download in one request; use instance value if None. If\n            zero, will return a streaming Requests file-like instance.\n        kwargs: key-value\n            Any other parameters, passed to requests calls\n        \"\"\"\n        if mode != \"rb\":\n            raise NotImplementedError\n        block_size = block_size if block_size is not None else self.block_size\n        kw = self.kwargs.copy()\n        kw[\"asynchronous\"] = self.asynchronous\n        kw.update(kwargs)\n        size = size or self.info(path, **kwargs)[\"size\"]\n        session = sync(self.loop, self.set_session)\n        if block_size and size:\n            return HTTPFile(\n                self,\n                path,\n                session=session,\n                block_size=block_size,\n                mode=mode,\n                size=size,\n                cache_type=cache_type or self.cache_type,\n                cache_options=cache_options or self.cache_options,\n                loop=self.loop,\n                **kw,\n            )\n        else:\n            return HTTPStreamFile(\n                self,\n                path,\n                mode=mode,\n                loop=self.loop,\n                session=session,\n                **kw,\n            )\n\n    async def open_async(self, path, mode=\"rb\", size=None, **kwargs):\n        session = await self.set_session()\n        if size is None:\n            try:\n                size = (await self._info(path, **kwargs))[\"size\"]\n            except FileNotFoundError:\n                pass\n        return AsyncStreamFile(\n            self,\n            path,\n            loop=self.loop,\n            session=session,\n            size=size,\n            **kwargs,\n        )\n\n    def ukey(self, url):\n        \"\"\"Unique identifier; assume HTTP files are static, unchanging\"\"\"\n        return tokenize(url, self.kwargs, self.protocol)\n\n    async def _info(self, url, **kwargs):\n        \"\"\"Get info of URL\n\n        Tries to access location via HEAD, and then GET methods, but does\n        not fetch the data.\n\n        It is possible that the server does not supply any size information, in\n        which case size will be given as None (and certain operations on the\n        corresponding file will not work).\n        \"\"\"\n        info = {}\n        session = await self.set_session()\n\n        for policy in [\"head\", \"get\"]:\n            try:\n                info.update(\n                    await _file_info(\n                        self.encode_url(url),\n                        size_policy=policy,\n                        session=session,\n                        **self.kwargs,\n                        **kwargs,\n                    )\n                )\n                if info.get(\"size\") is not None:\n                    break\n            except Exception as exc:\n                if policy == \"get\":\n                    # If get failed, then raise a FileNotFoundError\n                    raise FileNotFoundError(url) from exc\n                logger.debug(\"\", exc_info=exc)\n\n        return {\"name\": url, \"size\": None, **info, \"type\": \"file\"}\n\n    async def _glob(self, path, maxdepth=None, **kwargs):\n        \"\"\"\n        Find files by glob-matching.\n\n        This implementation is idntical to the one in AbstractFileSystem,\n        but \"?\" is not considered as a character for globbing, because it is\n        so common in URLs, often identifying the \"query\" part.\n        \"\"\"\n        if maxdepth is not None and maxdepth < 1:\n            raise ValueError(\"maxdepth must be at least 1\")\n        import re\n\n        ends_with_slash = path.endswith(\"/\")  # _strip_protocol strips trailing slash\n        path = self._strip_protocol(path)\n        append_slash_to_dirname = ends_with_slash or path.endswith((\"/**\", \"/*\"))\n        idx_star = path.find(\"*\") if path.find(\"*\") >= 0 else len(path)\n        idx_brace = path.find(\"[\") if path.find(\"[\") >= 0 else len(path)\n\n        min_idx = min(idx_star, idx_brace)\n\n        detail = kwargs.pop(\"detail\", False)\n\n        if not has_magic(path):\n            if await self._exists(path, **kwargs):\n                if not detail:\n                    return [path]\n                else:\n                    return {path: await self._info(path, **kwargs)}\n            else:\n                if not detail:\n                    return []  # glob of non-existent returns empty\n                else:\n                    return {}\n        elif \"/\" in path[:min_idx]:\n            min_idx = path[:min_idx].rindex(\"/\")\n            root = path[: min_idx + 1]\n            depth = path[min_idx + 1 :].count(\"/\") + 1\n        else:\n            root = \"\"\n            depth = path[min_idx + 1 :].count(\"/\") + 1\n\n        if \"**\" in path:\n            if maxdepth is not None:\n                idx_double_stars = path.find(\"**\")\n                depth_double_stars = path[idx_double_stars:].count(\"/\") + 1\n                depth = depth - depth_double_stars + maxdepth\n            else:\n                depth = None\n\n        allpaths = await self._find(\n            root, maxdepth=depth, withdirs=True, detail=True, **kwargs\n        )\n\n        pattern = glob_translate(path + (\"/\" if ends_with_slash else \"\"))\n        pattern = re.compile(pattern)\n\n        out = {\n            (\n                p.rstrip(\"/\")\n                if not append_slash_to_dirname\n                and info[\"type\"] == \"directory\"\n                and p.endswith(\"/\")\n                else p\n            ): info\n            for p, info in sorted(allpaths.items())\n            if pattern.match(p.rstrip(\"/\"))\n        }\n\n        if detail:\n            return out\n        else:\n            return list(out)\n\n    async def _isdir(self, path):\n        # override, since all URLs are (also) files\n        try:\n            return bool(await self._ls(path))\n        except (FileNotFoundError, ValueError):\n            return False\n\n\nclass HTTPFile(AbstractBufferedFile):\n    \"\"\"\n    A file-like object pointing to a remove HTTP(S) resource\n\n    Supports only reading, with read-ahead of a predermined block-size.\n\n    In the case that the server does not supply the filesize, only reading of\n    the complete file in one go is supported.\n\n    Parameters\n    ----------\n    url: str\n        Full URL of the remote resource, including the protocol\n    session: aiohttp.ClientSession or None\n        All calls will be made within this session, to avoid restarting\n        connections where the server allows this\n    block_size: int or None\n        The amount of read-ahead to do, in bytes. Default is 5MB, or the value\n        configured for the FileSystem creating this file\n    size: None or int\n        If given, this is the size of the file in bytes, and we don't attempt\n        to call the server to find the value.\n    kwargs: all other key-values are passed to requests calls.\n    \"\"\"\n\n    def __init__(\n        self,\n        fs,\n        url,\n        session=None,\n        block_size=None,\n        mode=\"rb\",\n        cache_type=\"bytes\",\n        cache_options=None,\n        size=None,\n        loop=None,\n        asynchronous=False,\n        **kwargs,\n    ):\n        if mode != \"rb\":\n            raise NotImplementedError(\"File mode not supported\")\n        self.asynchronous = asynchronous\n        self.loop = loop\n        self.url = url\n        self.session = session\n        self.details = {\"name\": url, \"size\": size, \"type\": \"file\"}\n        super().__init__(\n            fs=fs,\n            path=url,\n            mode=mode,\n            block_size=block_size,\n            cache_type=cache_type,\n            cache_options=cache_options,\n            **kwargs,\n        )\n\n    def read(self, length=-1):\n        \"\"\"Read bytes from file\n\n        Parameters\n        ----------\n        length: int\n            Read up to this many bytes. If negative, read all content to end of\n            file. If the server has not supplied the filesize, attempting to\n            read only part of the data will raise a ValueError.\n        \"\"\"\n        if (\n            (length < 0 and self.loc == 0)  # explicit read all\n            # but not when the size is known and fits into a block anyways\n            and not (self.size is not None and self.size <= self.blocksize)\n        ):\n            self._fetch_all()\n        if self.size is None:\n            if length < 0:\n                self._fetch_all()\n        else:\n            length = min(self.size - self.loc, length)\n        return super().read(length)\n\n    async def async_fetch_all(self):\n        \"\"\"Read whole file in one shot, without caching\n\n        This is only called when position is still at zero,\n        and read() is called without a byte-count.\n        \"\"\"\n        logger.debug(f\"Fetch all for {self}\")\n        if not isinstance(self.cache, AllBytes):\n            r = await self.session.get(self.fs.encode_url(self.url), **self.kwargs)\n            async with r:\n                r.raise_for_status()\n                out = await r.read()\n                self.cache = AllBytes(\n                    size=len(out), fetcher=None, blocksize=None, data=out\n                )\n                self.size = len(out)\n\n    _fetch_all = sync_wrapper(async_fetch_all)\n\n    def _parse_content_range(self, headers):\n        \"\"\"Parse the Content-Range header\"\"\"\n        s = headers.get(\"Content-Range\", \"\")\n        m = re.match(r\"bytes (\\d+-\\d+|\\*)/(\\d+|\\*)\", s)\n        if not m:\n            return None, None, None\n\n        if m[1] == \"*\":\n            start = end = None\n        else:\n            start, end = [int(x) for x in m[1].split(\"-\")]\n        total = None if m[2] == \"*\" else int(m[2])\n        return start, end, total\n\n    async def async_fetch_range(self, start, end):\n        \"\"\"Download a block of data\n\n        The expectation is that the server returns only the requested bytes,\n        with HTTP code 206. If this is not the case, we first check the headers,\n        and then stream the output - if the data size is bigger than we\n        requested, an exception is raised.\n        \"\"\"\n        logger.debug(f\"Fetch range for {self}: {start}-{end}\")\n        kwargs = self.kwargs.copy()\n        headers = kwargs.pop(\"headers\", {}).copy()\n        headers[\"Range\"] = f\"bytes={start}-{end - 1}\"\n        logger.debug(f\"{self.url} : {headers['Range']}\")\n        r = await self.session.get(\n            self.fs.encode_url(self.url), headers=headers, **kwargs\n        )\n        async with r:\n            if r.status == 416:\n                # range request outside file\n                return b\"\"\n            r.raise_for_status()\n\n            # If the server has handled the range request, it should reply\n            # with status 206 (partial content). But we'll guess that a suitable\n            # Content-Range header or a Content-Length no more than the\n            # requested range also mean we have got the desired range.\n            response_is_range = (\n                r.status == 206\n                or self._parse_content_range(r.headers)[0] == start\n                or int(r.headers.get(\"Content-Length\", end + 1)) <= end - start\n            )\n\n            if response_is_range:\n                # partial content, as expected\n                out = await r.read()\n            elif start > 0:\n                raise ValueError(\n                    \"The HTTP server doesn't appear to support range requests. \"\n                    \"Only reading this file from the beginning is supported. \"\n                    \"Open with block_size=0 for a streaming file interface.\"\n                )\n            else:\n                # Response is not a range, but we want the start of the file,\n                # so we can read the required amount anyway.\n                cl = 0\n                out = []\n                while True:\n                    chunk = await r.content.read(2**20)\n                    # data size unknown, let's read until we have enough\n                    if chunk:\n                        out.append(chunk)\n                        cl += len(chunk)\n                        if cl > end - start:\n                            break\n                    else:\n                        break\n                out = b\"\".join(out)[: end - start]\n            return out\n\n    _fetch_range = sync_wrapper(async_fetch_range)\n\n    def __reduce__(self):\n        return (\n            reopen,\n            (\n                self.fs,\n                self.url,\n                self.mode,\n                self.blocksize,\n                self.cache.name if self.cache else \"none\",\n                self.size,\n            ),\n        )\n\n\ndef reopen(fs, url, mode, blocksize, cache_type, size=None):\n    return fs.open(\n        url, mode=mode, block_size=blocksize, cache_type=cache_type, size=size\n    )\n\n\nmagic_check = re.compile(\"([*[])\")\n\n\ndef has_magic(s):\n    match = magic_check.search(s)\n    return match is not None\n\n\nclass HTTPStreamFile(AbstractBufferedFile):\n    def __init__(self, fs, url, mode=\"rb\", loop=None, session=None, **kwargs):\n        self.asynchronous = kwargs.pop(\"asynchronous\", False)\n        self.url = url\n        self.loop = loop\n        self.session = session\n        if mode != \"rb\":\n            raise ValueError\n        self.details = {\"name\": url, \"size\": None}\n        super().__init__(fs=fs, path=url, mode=mode, cache_type=\"none\", **kwargs)\n\n        async def cor():\n            r = await self.session.get(self.fs.encode_url(url), **kwargs).__aenter__()\n            self.fs._raise_not_found_for_status(r, url)\n            return r\n\n        self.r = sync(self.loop, cor)\n        self.loop = fs.loop\n\n    def seek(self, loc, whence=0):\n        if loc == 0 and whence == 1:\n            return\n        if loc == self.loc and whence == 0:\n            return\n        raise ValueError(\"Cannot seek streaming HTTP file\")\n\n    async def _read(self, num=-1):\n        out = await self.r.content.read(num)\n        self.loc += len(out)\n        return out\n\n    read = sync_wrapper(_read)\n\n    async def _close(self):\n        self.r.close()\n\n    def close(self):\n        asyncio.run_coroutine_threadsafe(self._close(), self.loop)\n        super().close()\n\n    def __reduce__(self):\n        return reopen, (self.fs, self.url, self.mode, self.blocksize, self.cache.name)\n\n\nclass AsyncStreamFile(AbstractAsyncStreamedFile):\n    def __init__(\n        self, fs, url, mode=\"rb\", loop=None, session=None, size=None, **kwargs\n    ):\n        self.url = url\n        self.session = session\n        self.r = None\n        if mode != \"rb\":\n            raise ValueError\n        self.details = {\"name\": url, \"size\": None}\n        self.kwargs = kwargs\n        super().__init__(fs=fs, path=url, mode=mode, cache_type=\"none\")\n        self.size = size\n\n    async def read(self, num=-1):\n        if self.r is None:\n            r = await self.session.get(\n                self.fs.encode_url(self.url), **self.kwargs\n            ).__aenter__()\n            self.fs._raise_not_found_for_status(r, self.url)\n            self.r = r\n        out = await self.r.content.read(num)\n        self.loc += len(out)\n        return out\n\n    async def close(self):\n        if self.r is not None:\n            self.r.close()\n            self.r = None\n        await super().close()\n\n\nasync def get_range(session, url, start, end, file=None, **kwargs):\n    # explicit get a range when we know it must be safe\n    kwargs = kwargs.copy()\n    headers = kwargs.pop(\"headers\", {}).copy()\n    headers[\"Range\"] = f\"bytes={start}-{end - 1}\"\n    r = await session.get(url, headers=headers, **kwargs)\n    r.raise_for_status()\n    async with r:\n        out = await r.read()\n    if file:\n        with open(file, \"r+b\") as f:  # noqa: ASYNC101\n            f.seek(start)\n            f.write(out)\n    else:\n        return out\n\n\nasync def _file_info(url, session, size_policy=\"head\", **kwargs):\n    \"\"\"Call HEAD on the server to get details about the file (size/checksum etc.)\n\n    Default operation is to explicitly allow redirects and use encoding\n    'identity' (no compression) to get the true size of the target.\n    \"\"\"\n    logger.debug(\"Retrieve file size for %s\", url)\n    kwargs = kwargs.copy()\n    ar = kwargs.pop(\"allow_redirects\", True)\n    head = kwargs.get(\"headers\", {}).copy()\n    head[\"Accept-Encoding\"] = \"identity\"\n    kwargs[\"headers\"] = head\n\n    info = {}\n    if size_policy == \"head\":\n        r = await session.head(url, allow_redirects=ar, **kwargs)\n    elif size_policy == \"get\":\n        r = await session.get(url, allow_redirects=ar, **kwargs)\n    else:\n        raise TypeError(f'size_policy must be \"head\" or \"get\", got {size_policy}')\n    async with r:\n        r.raise_for_status()\n\n        # TODO:\n        #  recognise lack of 'Accept-Ranges',\n        #                 or 'Accept-Ranges': 'none' (not 'bytes')\n        #  to mean streaming only, no random access => return None\n        if \"Content-Length\" in r.headers:\n            # Some servers may choose to ignore Accept-Encoding and return\n            # compressed content, in which case the returned size is unreliable.\n            if \"Content-Encoding\" not in r.headers or r.headers[\"Content-Encoding\"] in [\n                \"identity\",\n                \"\",\n            ]:\n                info[\"size\"] = int(r.headers[\"Content-Length\"])\n        elif \"Content-Range\" in r.headers:\n            info[\"size\"] = int(r.headers[\"Content-Range\"].split(\"/\")[1])\n\n        if \"Content-Type\" in r.headers:\n            info[\"mimetype\"] = r.headers[\"Content-Type\"].partition(\";\")[0]\n\n        info[\"url\"] = str(r.url)\n\n        for checksum_field in [\"ETag\", \"Content-MD5\", \"Digest\"]:\n            if r.headers.get(checksum_field):\n                info[checksum_field] = r.headers[checksum_field]\n\n    return info\n\n\nasync def _file_size(url, session=None, *args, **kwargs):\n    if session is None:\n        session = await get_client()\n    info = await _file_info(url, session=session, *args, **kwargs)\n    return info.get(\"size\")\n\n\nfile_size = sync_wrapper(_file_size)\n", "fsspec/implementations/arrow.py": "import errno\nimport io\nimport os\nimport secrets\nimport shutil\nfrom contextlib import suppress\nfrom functools import cached_property, wraps\nfrom urllib.parse import parse_qs\n\nfrom fsspec.spec import AbstractFileSystem\nfrom fsspec.utils import (\n    get_package_version_without_import,\n    infer_storage_options,\n    mirror_from,\n    tokenize,\n)\n\n\ndef wrap_exceptions(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except OSError as exception:\n            if not exception.args:\n                raise\n\n            message, *args = exception.args\n            if isinstance(message, str) and \"does not exist\" in message:\n                raise FileNotFoundError(errno.ENOENT, message) from exception\n            else:\n                raise\n\n    return wrapper\n\n\nPYARROW_VERSION = None\n\n\nclass ArrowFSWrapper(AbstractFileSystem):\n    \"\"\"FSSpec-compatible wrapper of pyarrow.fs.FileSystem.\n\n    Parameters\n    ----------\n    fs : pyarrow.fs.FileSystem\n\n    \"\"\"\n\n    root_marker = \"/\"\n\n    def __init__(self, fs, **kwargs):\n        global PYARROW_VERSION\n        PYARROW_VERSION = get_package_version_without_import(\"pyarrow\")\n        self.fs = fs\n        super().__init__(**kwargs)\n\n    @property\n    def protocol(self):\n        return self.fs.type_name\n\n    @cached_property\n    def fsid(self):\n        return \"hdfs_\" + tokenize(self.fs.host, self.fs.port)\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        ops = infer_storage_options(path)\n        path = ops[\"path\"]\n        if path.startswith(\"//\"):\n            # special case for \"hdfs://path\" (without the triple slash)\n            path = path[1:]\n        return path\n\n    def ls(self, path, detail=False, **kwargs):\n        path = self._strip_protocol(path)\n        from pyarrow.fs import FileSelector\n\n        entries = [\n            self._make_entry(entry)\n            for entry in self.fs.get_file_info(FileSelector(path))\n        ]\n        if detail:\n            return entries\n        else:\n            return [entry[\"name\"] for entry in entries]\n\n    def info(self, path, **kwargs):\n        path = self._strip_protocol(path)\n        [info] = self.fs.get_file_info([path])\n        return self._make_entry(info)\n\n    def exists(self, path):\n        path = self._strip_protocol(path)\n        try:\n            self.info(path)\n        except FileNotFoundError:\n            return False\n        else:\n            return True\n\n    def _make_entry(self, info):\n        from pyarrow.fs import FileType\n\n        if info.type is FileType.Directory:\n            kind = \"directory\"\n        elif info.type is FileType.File:\n            kind = \"file\"\n        elif info.type is FileType.NotFound:\n            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), info.path)\n        else:\n            kind = \"other\"\n\n        return {\n            \"name\": info.path,\n            \"size\": info.size,\n            \"type\": kind,\n            \"mtime\": info.mtime,\n        }\n\n    @wrap_exceptions\n    def cp_file(self, path1, path2, **kwargs):\n        path1 = self._strip_protocol(path1).rstrip(\"/\")\n        path2 = self._strip_protocol(path2).rstrip(\"/\")\n\n        with self._open(path1, \"rb\") as lstream:\n            tmp_fname = f\"{path2}.tmp.{secrets.token_hex(6)}\"\n            try:\n                with self.open(tmp_fname, \"wb\") as rstream:\n                    shutil.copyfileobj(lstream, rstream)\n                self.fs.move(tmp_fname, path2)\n            except BaseException:  # noqa\n                with suppress(FileNotFoundError):\n                    self.fs.delete_file(tmp_fname)\n                raise\n\n    @wrap_exceptions\n    def mv(self, path1, path2, **kwargs):\n        path1 = self._strip_protocol(path1).rstrip(\"/\")\n        path2 = self._strip_protocol(path2).rstrip(\"/\")\n        self.fs.move(path1, path2)\n\n    @wrap_exceptions\n    def rm_file(self, path):\n        path = self._strip_protocol(path)\n        self.fs.delete_file(path)\n\n    @wrap_exceptions\n    def rm(self, path, recursive=False, maxdepth=None):\n        path = self._strip_protocol(path).rstrip(\"/\")\n        if self.isdir(path):\n            if recursive:\n                self.fs.delete_dir(path)\n            else:\n                raise ValueError(\"Can't delete directories without recursive=False\")\n        else:\n            self.fs.delete_file(path)\n\n    @wrap_exceptions\n    def _open(self, path, mode=\"rb\", block_size=None, seekable=True, **kwargs):\n        if mode == \"rb\":\n            if seekable:\n                method = self.fs.open_input_file\n            else:\n                method = self.fs.open_input_stream\n        elif mode == \"wb\":\n            method = self.fs.open_output_stream\n        elif mode == \"ab\":\n            method = self.fs.open_append_stream\n        else:\n            raise ValueError(f\"unsupported mode for Arrow filesystem: {mode!r}\")\n\n        _kwargs = {}\n        if mode != \"rb\" or not seekable:\n            if int(PYARROW_VERSION.split(\".\")[0]) >= 4:\n                # disable compression auto-detection\n                _kwargs[\"compression\"] = None\n        stream = method(path, **_kwargs)\n\n        return ArrowFile(self, stream, path, mode, block_size, **kwargs)\n\n    @wrap_exceptions\n    def mkdir(self, path, create_parents=True, **kwargs):\n        path = self._strip_protocol(path)\n        if create_parents:\n            self.makedirs(path, exist_ok=True)\n        else:\n            self.fs.create_dir(path, recursive=False)\n\n    @wrap_exceptions\n    def makedirs(self, path, exist_ok=False):\n        path = self._strip_protocol(path)\n        self.fs.create_dir(path, recursive=True)\n\n    @wrap_exceptions\n    def rmdir(self, path):\n        path = self._strip_protocol(path)\n        self.fs.delete_dir(path)\n\n    @wrap_exceptions\n    def modified(self, path):\n        path = self._strip_protocol(path)\n        return self.fs.get_file_info(path).mtime\n\n    def cat_file(self, path, start=None, end=None, **kwargs):\n        kwargs[\"seekable\"] = start not in [None, 0]\n        return super().cat_file(path, start=None, end=None, **kwargs)\n\n    def get_file(self, rpath, lpath, **kwargs):\n        kwargs[\"seekable\"] = False\n        super().get_file(rpath, lpath, **kwargs)\n\n\n@mirror_from(\n    \"stream\",\n    [\n        \"read\",\n        \"seek\",\n        \"tell\",\n        \"write\",\n        \"readable\",\n        \"writable\",\n        \"close\",\n        \"size\",\n        \"seekable\",\n    ],\n)\nclass ArrowFile(io.IOBase):\n    def __init__(self, fs, stream, path, mode, block_size=None, **kwargs):\n        self.path = path\n        self.mode = mode\n\n        self.fs = fs\n        self.stream = stream\n\n        self.blocksize = self.block_size = block_size\n        self.kwargs = kwargs\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        return self.close()\n\n\nclass HadoopFileSystem(ArrowFSWrapper):\n    \"\"\"A wrapper on top of the pyarrow.fs.HadoopFileSystem\n    to connect it's interface with fsspec\"\"\"\n\n    protocol = \"hdfs\"\n\n    def __init__(\n        self,\n        host=\"default\",\n        port=0,\n        user=None,\n        kerb_ticket=None,\n        replication=3,\n        extra_conf=None,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        host: str\n            Hostname, IP or \"default\" to try to read from Hadoop config\n        port: int\n            Port to connect on, or default from Hadoop config if 0\n        user: str or None\n            If given, connect as this username\n        kerb_ticket: str or None\n            If given, use this ticket for authentication\n        replication: int\n            set replication factor of file for write operations. default value is 3.\n        extra_conf: None or dict\n            Passed on to HadoopFileSystem\n        \"\"\"\n        from pyarrow.fs import HadoopFileSystem\n\n        fs = HadoopFileSystem(\n            host=host,\n            port=port,\n            user=user,\n            kerb_ticket=kerb_ticket,\n            replication=replication,\n            extra_conf=extra_conf,\n        )\n        super().__init__(fs=fs, **kwargs)\n\n    @staticmethod\n    def _get_kwargs_from_urls(path):\n        ops = infer_storage_options(path)\n        out = {}\n        if ops.get(\"host\", None):\n            out[\"host\"] = ops[\"host\"]\n        if ops.get(\"username\", None):\n            out[\"user\"] = ops[\"username\"]\n        if ops.get(\"port\", None):\n            out[\"port\"] = ops[\"port\"]\n        if ops.get(\"url_query\", None):\n            queries = parse_qs(ops[\"url_query\"])\n            if queries.get(\"replication\", None):\n                out[\"replication\"] = int(queries[\"replication\"][0])\n        return out\n", "fsspec/implementations/smb.py": "\"\"\"\nThis module contains SMBFileSystem class responsible for handling access to\nWindows Samba network shares by using package smbprotocol\n\"\"\"\n\nimport datetime\nimport uuid\nfrom stat import S_ISDIR, S_ISLNK\n\nimport smbclient\n\nfrom .. import AbstractFileSystem\nfrom ..utils import infer_storage_options\n\n# ! pylint: disable=bad-continuation\n\n\nclass SMBFileSystem(AbstractFileSystem):\n    \"\"\"Allow reading and writing to Windows and Samba network shares.\n\n    When using `fsspec.open()` for getting a file-like object the URI\n    should be specified as this format:\n    ``smb://workgroup;user:password@server:port/share/folder/file.csv``.\n\n    Example::\n\n        >>> import fsspec\n        >>> with fsspec.open(\n        ...     'smb://myuser:mypassword@myserver.com/' 'share/folder/file.csv'\n        ... ) as smbfile:\n        ...     df = pd.read_csv(smbfile, sep='|', header=None)\n\n    Note that you need to pass in a valid hostname or IP address for the host\n    component of the URL. Do not use the Windows/NetBIOS machine name for the\n    host component.\n\n    The first component of the path in the URL points to the name of the shared\n    folder. Subsequent path components will point to the directory/folder/file.\n\n    The URL components ``workgroup`` , ``user``, ``password`` and ``port`` may be\n    optional.\n\n    .. note::\n\n        For working this source require `smbprotocol`_ to be installed, e.g.::\n\n            $ pip install smbprotocol\n            # or\n            # pip install smbprotocol[kerberos]\n\n    .. _smbprotocol: https://github.com/jborean93/smbprotocol#requirements\n\n    Note: if using this with the ``open`` or ``open_files``, with full URLs,\n    there is no way to tell if a path is relative, so all paths are assumed\n    to be absolute.\n    \"\"\"\n\n    protocol = \"smb\"\n\n    # pylint: disable=too-many-arguments\n    def __init__(\n        self,\n        host,\n        port=None,\n        username=None,\n        password=None,\n        timeout=60,\n        encrypt=None,\n        share_access=None,\n        register_session_retries=5,\n        auto_mkdir=False,\n        **kwargs,\n    ):\n        \"\"\"\n        You can use _get_kwargs_from_urls to get some kwargs from\n        a reasonable SMB url.\n\n        Authentication will be anonymous or integrated if username/password are not\n        given.\n\n        Parameters\n        ----------\n        host: str\n            The remote server name/ip to connect to\n        port: int or None\n            Port to connect with. Usually 445, sometimes 139.\n        username: str or None\n            Username to connect with. Required if Kerberos auth is not being used.\n        password: str or None\n            User's password on the server, if using username\n        timeout: int\n            Connection timeout in seconds\n        encrypt: bool\n            Whether to force encryption or not, once this has been set to True\n            the session cannot be changed back to False.\n        share_access: str or None\n            Specifies the default access applied to file open operations\n            performed with this file system object.\n            This affects whether other processes can concurrently open a handle\n            to the same file.\n\n            - None (the default): exclusively locks the file until closed.\n            - 'r': Allow other handles to be opened with read access.\n            - 'w': Allow other handles to be opened with write access.\n            - 'd': Allow other handles to be opened with delete access.\n        auto_mkdir: bool\n            Whether, when opening a file, the directory containing it should\n            be created (if it doesn't already exist). This is assumed by pyarrow\n            and zarr-python code.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.host = host\n        self.port = port\n        self.username = username\n        self.password = password\n        self.timeout = timeout\n        self.encrypt = encrypt\n        self.temppath = kwargs.pop(\"temppath\", \"\")\n        self.share_access = share_access\n        self.register_session_retries = register_session_retries\n        self.auto_mkdir = auto_mkdir\n        self._connect()\n\n    @property\n    def _port(self):\n        return 445 if self.port is None else self.port\n\n    def _connect(self):\n        import time\n\n        for _ in range(self.register_session_retries):\n            try:\n                smbclient.register_session(\n                    self.host,\n                    username=self.username,\n                    password=self.password,\n                    port=self._port,\n                    encrypt=self.encrypt,\n                    connection_timeout=self.timeout,\n                )\n                break\n            except Exception:\n                time.sleep(0.1)\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        return infer_storage_options(path)[\"path\"]\n\n    @staticmethod\n    def _get_kwargs_from_urls(path):\n        # smb://workgroup;user:password@host:port/share/folder/file.csv\n        out = infer_storage_options(path)\n        out.pop(\"path\", None)\n        out.pop(\"protocol\", None)\n        return out\n\n    def mkdir(self, path, create_parents=True, **kwargs):\n        wpath = _as_unc_path(self.host, path)\n        if create_parents:\n            smbclient.makedirs(wpath, exist_ok=False, port=self._port, **kwargs)\n        else:\n            smbclient.mkdir(wpath, port=self._port, **kwargs)\n\n    def makedirs(self, path, exist_ok=False):\n        if _share_has_path(path):\n            wpath = _as_unc_path(self.host, path)\n            smbclient.makedirs(wpath, exist_ok=exist_ok, port=self._port)\n\n    def rmdir(self, path):\n        if _share_has_path(path):\n            wpath = _as_unc_path(self.host, path)\n            smbclient.rmdir(wpath, port=self._port)\n\n    def info(self, path, **kwargs):\n        wpath = _as_unc_path(self.host, path)\n        stats = smbclient.stat(wpath, port=self._port, **kwargs)\n        if S_ISDIR(stats.st_mode):\n            stype = \"directory\"\n        elif S_ISLNK(stats.st_mode):\n            stype = \"link\"\n        else:\n            stype = \"file\"\n        res = {\n            \"name\": path + \"/\" if stype == \"directory\" else path,\n            \"size\": stats.st_size,\n            \"type\": stype,\n            \"uid\": stats.st_uid,\n            \"gid\": stats.st_gid,\n            \"time\": stats.st_atime,\n            \"mtime\": stats.st_mtime,\n        }\n        return res\n\n    def created(self, path):\n        \"\"\"Return the created timestamp of a file as a datetime.datetime\"\"\"\n        wpath = _as_unc_path(self.host, path)\n        stats = smbclient.stat(wpath, port=self._port)\n        return datetime.datetime.fromtimestamp(stats.st_ctime, tz=datetime.timezone.utc)\n\n    def modified(self, path):\n        \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\"\n        wpath = _as_unc_path(self.host, path)\n        stats = smbclient.stat(wpath, port=self._port)\n        return datetime.datetime.fromtimestamp(stats.st_mtime, tz=datetime.timezone.utc)\n\n    def ls(self, path, detail=True, **kwargs):\n        unc = _as_unc_path(self.host, path)\n        listed = smbclient.listdir(unc, port=self._port, **kwargs)\n        dirs = [\"/\".join([path.rstrip(\"/\"), p]) for p in listed]\n        if detail:\n            dirs = [self.info(d) for d in dirs]\n        return dirs\n\n    # pylint: disable=too-many-arguments\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=-1,\n        autocommit=True,\n        cache_options=None,\n        **kwargs,\n    ):\n        \"\"\"\n        block_size: int or None\n            If 0, no buffering, 1, line buffering, >1, buffer that many bytes\n\n        Notes\n        -----\n        By specifying 'share_access' in 'kwargs' it is possible to override the\n        default shared access setting applied in the constructor of this object.\n        \"\"\"\n        if self.auto_mkdir and \"w\" in mode:\n            self.makedirs(self._parent(path), exist_ok=True)\n        bls = block_size if block_size is not None and block_size >= 0 else -1\n        wpath = _as_unc_path(self.host, path)\n        share_access = kwargs.pop(\"share_access\", self.share_access)\n        if \"w\" in mode and autocommit is False:\n            temp = _as_temp_path(self.host, path, self.temppath)\n            return SMBFileOpener(\n                wpath, temp, mode, port=self._port, block_size=bls, **kwargs\n            )\n        return smbclient.open_file(\n            wpath,\n            mode,\n            buffering=bls,\n            share_access=share_access,\n            port=self._port,\n            **kwargs,\n        )\n\n    def copy(self, path1, path2, **kwargs):\n        \"\"\"Copy within two locations in the same filesystem\"\"\"\n        wpath1 = _as_unc_path(self.host, path1)\n        wpath2 = _as_unc_path(self.host, path2)\n        if self.auto_mkdir:\n            self.makedirs(self._parent(path2), exist_ok=True)\n        smbclient.copyfile(wpath1, wpath2, port=self._port, **kwargs)\n\n    def _rm(self, path):\n        if _share_has_path(path):\n            wpath = _as_unc_path(self.host, path)\n            stats = smbclient.stat(wpath, port=self._port)\n            if S_ISDIR(stats.st_mode):\n                smbclient.rmdir(wpath, port=self._port)\n            else:\n                smbclient.remove(wpath, port=self._port)\n\n    def mv(self, path1, path2, recursive=None, maxdepth=None, **kwargs):\n        wpath1 = _as_unc_path(self.host, path1)\n        wpath2 = _as_unc_path(self.host, path2)\n        smbclient.rename(wpath1, wpath2, port=self._port, **kwargs)\n\n\ndef _as_unc_path(host, path):\n    rpath = path.replace(\"/\", \"\\\\\")\n    unc = f\"\\\\\\\\{host}{rpath}\"\n    return unc\n\n\ndef _as_temp_path(host, path, temppath):\n    share = path.split(\"/\")[1]\n    temp_file = f\"/{share}{temppath}/{uuid.uuid4()}\"\n    unc = _as_unc_path(host, temp_file)\n    return unc\n\n\ndef _share_has_path(path):\n    parts = path.count(\"/\")\n    if path.endswith(\"/\"):\n        return parts > 2\n    return parts > 1\n\n\nclass SMBFileOpener:\n    \"\"\"writes to remote temporary file, move on commit\"\"\"\n\n    def __init__(self, path, temp, mode, port=445, block_size=-1, **kwargs):\n        self.path = path\n        self.temp = temp\n        self.mode = mode\n        self.block_size = block_size\n        self.kwargs = kwargs\n        self.smbfile = None\n        self._incontext = False\n        self.port = port\n        self._open()\n\n    def _open(self):\n        if self.smbfile is None or self.smbfile.closed:\n            self.smbfile = smbclient.open_file(\n                self.temp,\n                self.mode,\n                port=self.port,\n                buffering=self.block_size,\n                **self.kwargs,\n            )\n\n    def commit(self):\n        \"\"\"Move temp file to definitive on success.\"\"\"\n        # TODO: use transaction support in SMB protocol\n        smbclient.replace(self.temp, self.path, port=self.port)\n\n    def discard(self):\n        \"\"\"Remove the temp file on failure.\"\"\"\n        smbclient.remove(self.temp, port=self.port)\n\n    def __fspath__(self):\n        return self.path\n\n    def __iter__(self):\n        return self.smbfile.__iter__()\n\n    def __getattr__(self, item):\n        return getattr(self.smbfile, item)\n\n    def __enter__(self):\n        self._incontext = True\n        return self.smbfile.__enter__()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._incontext = False\n        self.smbfile.__exit__(exc_type, exc_value, traceback)\n", "fsspec/implementations/cached.py": "from __future__ import annotations\n\nimport inspect\nimport logging\nimport os\nimport tempfile\nimport time\nimport weakref\nfrom shutil import rmtree\nfrom typing import TYPE_CHECKING, Any, Callable, ClassVar\n\nfrom fsspec import AbstractFileSystem, filesystem\nfrom fsspec.callbacks import DEFAULT_CALLBACK\nfrom fsspec.compression import compr\nfrom fsspec.core import BaseCache, MMapCache\nfrom fsspec.exceptions import BlocksizeMismatchError\nfrom fsspec.implementations.cache_mapper import create_cache_mapper\nfrom fsspec.implementations.cache_metadata import CacheMetadata\nfrom fsspec.spec import AbstractBufferedFile\nfrom fsspec.transaction import Transaction\nfrom fsspec.utils import infer_compression\n\nif TYPE_CHECKING:\n    from fsspec.implementations.cache_mapper import AbstractCacheMapper\n\nlogger = logging.getLogger(\"fsspec.cached\")\n\n\nclass WriteCachedTransaction(Transaction):\n    def complete(self, commit=True):\n        rpaths = [f.path for f in self.files]\n        lpaths = [f.fn for f in self.files]\n        if commit:\n            self.fs.put(lpaths, rpaths)\n        self.files.clear()\n        self.fs._intrans = False\n        self.fs._transaction = None\n        self.fs = None  # break cycle\n\n\nclass CachingFileSystem(AbstractFileSystem):\n    \"\"\"Locally caching filesystem, layer over any other FS\n\n    This class implements chunk-wise local storage of remote files, for quick\n    access after the initial download. The files are stored in a given\n    directory with hashes of URLs for the filenames. If no directory is given,\n    a temporary one is used, which should be cleaned up by the OS after the\n    process ends. The files themselves are sparse (as implemented in\n    :class:`~fsspec.caching.MMapCache`), so only the data which is accessed\n    takes up space.\n\n    Restrictions:\n\n    - the block-size must be the same for each access of a given file, unless\n      all blocks of the file have already been read\n    - caching can only be applied to file-systems which produce files\n      derived from fsspec.spec.AbstractBufferedFile ; LocalFileSystem is also\n      allowed, for testing\n    \"\"\"\n\n    protocol: ClassVar[str | tuple[str, ...]] = (\"blockcache\", \"cached\")\n\n    def __init__(\n        self,\n        target_protocol=None,\n        cache_storage=\"TMP\",\n        cache_check=10,\n        check_files=False,\n        expiry_time=604800,\n        target_options=None,\n        fs=None,\n        same_names: bool | None = None,\n        compression=None,\n        cache_mapper: AbstractCacheMapper | None = None,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        target_protocol: str (optional)\n            Target filesystem protocol. Provide either this or ``fs``.\n        cache_storage: str or list(str)\n            Location to store files. If \"TMP\", this is a temporary directory,\n            and will be cleaned up by the OS when this process ends (or later).\n            If a list, each location will be tried in the order given, but\n            only the last will be considered writable.\n        cache_check: int\n            Number of seconds between reload of cache metadata\n        check_files: bool\n            Whether to explicitly see if the UID of the remote file matches\n            the stored one before using. Warning: some file systems such as\n            HTTP cannot reliably give a unique hash of the contents of some\n            path, so be sure to set this option to False.\n        expiry_time: int\n            The time in seconds after which a local copy is considered useless.\n            Set to falsy to prevent expiry. The default is equivalent to one\n            week.\n        target_options: dict or None\n            Passed to the instantiation of the FS, if fs is None.\n        fs: filesystem instance\n            The target filesystem to run against. Provide this or ``protocol``.\n        same_names: bool (optional)\n            By default, target URLs are hashed using a ``HashCacheMapper`` so\n            that files from different backends with the same basename do not\n            conflict. If this argument is ``true``, a ``BasenameCacheMapper``\n            is used instead. Other cache mapper options are available by using\n            the ``cache_mapper`` keyword argument. Only one of this and\n            ``cache_mapper`` should be specified.\n        compression: str (optional)\n            To decompress on download. Can be 'infer' (guess from the URL name),\n            one of the entries in ``fsspec.compression.compr``, or None for no\n            decompression.\n        cache_mapper: AbstractCacheMapper (optional)\n            The object use to map from original filenames to cached filenames.\n            Only one of this and ``same_names`` should be specified.\n        \"\"\"\n        super().__init__(**kwargs)\n        if fs is None and target_protocol is None:\n            raise ValueError(\n                \"Please provide filesystem instance(fs) or target_protocol\"\n            )\n        if not (fs is None) ^ (target_protocol is None):\n            raise ValueError(\n                \"Both filesystems (fs) and target_protocol may not be both given.\"\n            )\n        if cache_storage == \"TMP\":\n            tempdir = tempfile.mkdtemp()\n            storage = [tempdir]\n            weakref.finalize(self, self._remove_tempdir, tempdir)\n        else:\n            if isinstance(cache_storage, str):\n                storage = [cache_storage]\n            else:\n                storage = cache_storage\n        os.makedirs(storage[-1], exist_ok=True)\n        self.storage = storage\n        self.kwargs = target_options or {}\n        self.cache_check = cache_check\n        self.check_files = check_files\n        self.expiry = expiry_time\n        self.compression = compression\n\n        # Size of cache in bytes. If None then the size is unknown and will be\n        # recalculated the next time cache_size() is called. On writes to the\n        # cache this is reset to None.\n        self._cache_size = None\n\n        if same_names is not None and cache_mapper is not None:\n            raise ValueError(\n                \"Cannot specify both same_names and cache_mapper in \"\n                \"CachingFileSystem.__init__\"\n            )\n        if cache_mapper is not None:\n            self._mapper = cache_mapper\n        else:\n            self._mapper = create_cache_mapper(\n                same_names if same_names is not None else False\n            )\n\n        self.target_protocol = (\n            target_protocol\n            if isinstance(target_protocol, str)\n            else (fs.protocol if isinstance(fs.protocol, str) else fs.protocol[0])\n        )\n        self._metadata = CacheMetadata(self.storage)\n        self.load_cache()\n        self.fs = fs if fs is not None else filesystem(target_protocol, **self.kwargs)\n\n        def _strip_protocol(path):\n            # acts as a method, since each instance has a difference target\n            return self.fs._strip_protocol(type(self)._strip_protocol(path))\n\n        self._strip_protocol: Callable = _strip_protocol\n\n    @staticmethod\n    def _remove_tempdir(tempdir):\n        try:\n            rmtree(tempdir)\n        except Exception:\n            pass\n\n    def _mkcache(self):\n        os.makedirs(self.storage[-1], exist_ok=True)\n\n    def cache_size(self):\n        \"\"\"Return size of cache in bytes.\n\n        If more than one cache directory is in use, only the size of the last\n        one (the writable cache directory) is returned.\n        \"\"\"\n        if self._cache_size is None:\n            cache_dir = self.storage[-1]\n            self._cache_size = filesystem(\"file\").du(cache_dir, withdirs=True)\n        return self._cache_size\n\n    def load_cache(self):\n        \"\"\"Read set of stored blocks from file\"\"\"\n        self._metadata.load()\n        self._mkcache()\n        self.last_cache = time.time()\n\n    def save_cache(self):\n        \"\"\"Save set of stored blocks from file\"\"\"\n        self._mkcache()\n        self._metadata.save()\n        self.last_cache = time.time()\n        self._cache_size = None\n\n    def _check_cache(self):\n        \"\"\"Reload caches if time elapsed or any disappeared\"\"\"\n        self._mkcache()\n        if not self.cache_check:\n            # explicitly told not to bother checking\n            return\n        timecond = time.time() - self.last_cache > self.cache_check\n        existcond = all(os.path.exists(storage) for storage in self.storage)\n        if timecond or not existcond:\n            self.load_cache()\n\n    def _check_file(self, path):\n        \"\"\"Is path in cache and still valid\"\"\"\n        path = self._strip_protocol(path)\n        self._check_cache()\n        return self._metadata.check_file(path, self)\n\n    def clear_cache(self):\n        \"\"\"Remove all files and metadata from the cache\n\n        In the case of multiple cache locations, this clears only the last one,\n        which is assumed to be the read/write one.\n        \"\"\"\n        rmtree(self.storage[-1])\n        self.load_cache()\n        self._cache_size = None\n\n    def clear_expired_cache(self, expiry_time=None):\n        \"\"\"Remove all expired files and metadata from the cache\n\n        In the case of multiple cache locations, this clears only the last one,\n        which is assumed to be the read/write one.\n\n        Parameters\n        ----------\n        expiry_time: int\n            The time in seconds after which a local copy is considered useless.\n            If not defined the default is equivalent to the attribute from the\n            file caching instantiation.\n        \"\"\"\n\n        if not expiry_time:\n            expiry_time = self.expiry\n\n        self._check_cache()\n\n        expired_files, writable_cache_empty = self._metadata.clear_expired(expiry_time)\n        for fn in expired_files:\n            if os.path.exists(fn):\n                os.remove(fn)\n\n        if writable_cache_empty:\n            rmtree(self.storage[-1])\n            self.load_cache()\n\n        self._cache_size = None\n\n    def pop_from_cache(self, path):\n        \"\"\"Remove cached version of given file\n\n        Deletes local copy of the given (remote) path. If it is found in a cache\n        location which is not the last, it is assumed to be read-only, and\n        raises PermissionError\n        \"\"\"\n        path = self._strip_protocol(path)\n        fn = self._metadata.pop_file(path)\n        if fn is not None:\n            os.remove(fn)\n        self._cache_size = None\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        **kwargs,\n    ):\n        \"\"\"Wrap the target _open\n\n        If the whole file exists in the cache, just open it locally and\n        return that.\n\n        Otherwise, open the file on the target FS, and make it have a mmap\n        cache pointing to the location which we determine, in our cache.\n        The ``blocks`` instance is shared, so as the mmap cache instance\n        updates, so does the entry in our ``cached_files`` attribute.\n        We monkey-patch this file, so that when it closes, we call\n        ``close_and_update`` to save the state of the blocks.\n        \"\"\"\n        path = self._strip_protocol(path)\n\n        path = self.fs._strip_protocol(path)\n        if \"r\" not in mode:\n            return self.fs._open(\n                path,\n                mode=mode,\n                block_size=block_size,\n                autocommit=autocommit,\n                cache_options=cache_options,\n                **kwargs,\n            )\n        detail = self._check_file(path)\n        if detail:\n            # file is in cache\n            detail, fn = detail\n            hash, blocks = detail[\"fn\"], detail[\"blocks\"]\n            if blocks is True:\n                # stored file is complete\n                logger.debug(\"Opening local copy of %s\", path)\n                return open(fn, mode)\n            # TODO: action where partial file exists in read-only cache\n            logger.debug(\"Opening partially cached copy of %s\", path)\n        else:\n            hash = self._mapper(path)\n            fn = os.path.join(self.storage[-1], hash)\n            blocks = set()\n            detail = {\n                \"original\": path,\n                \"fn\": hash,\n                \"blocks\": blocks,\n                \"time\": time.time(),\n                \"uid\": self.fs.ukey(path),\n            }\n            self._metadata.update_file(path, detail)\n            logger.debug(\"Creating local sparse file for %s\", path)\n\n        # call target filesystems open\n        self._mkcache()\n        f = self.fs._open(\n            path,\n            mode=mode,\n            block_size=block_size,\n            autocommit=autocommit,\n            cache_options=cache_options,\n            cache_type=\"none\",\n            **kwargs,\n        )\n        if self.compression:\n            comp = (\n                infer_compression(path)\n                if self.compression == \"infer\"\n                else self.compression\n            )\n            f = compr[comp](f, mode=\"rb\")\n        if \"blocksize\" in detail:\n            if detail[\"blocksize\"] != f.blocksize:\n                raise BlocksizeMismatchError(\n                    f\"Cached file must be reopened with same block\"\n                    f\" size as original (old: {detail['blocksize']},\"\n                    f\" new {f.blocksize})\"\n                )\n        else:\n            detail[\"blocksize\"] = f.blocksize\n        f.cache = MMapCache(f.blocksize, f._fetch_range, f.size, fn, blocks)\n        close = f.close\n        f.close = lambda: self.close_and_update(f, close)\n        self.save_cache()\n        return f\n\n    def _parent(self, path):\n        return self.fs._parent(path)\n\n    def hash_name(self, path: str, *args: Any) -> str:\n        # Kept for backward compatibility with downstream libraries.\n        # Ignores extra arguments, previously same_name boolean.\n        return self._mapper(path)\n\n    def close_and_update(self, f, close):\n        \"\"\"Called when a file is closing, so store the set of blocks\"\"\"\n        if f.closed:\n            return\n        path = self._strip_protocol(f.path)\n        self._metadata.on_close_cached_file(f, path)\n        try:\n            logger.debug(\"going to save\")\n            self.save_cache()\n            logger.debug(\"saved\")\n        except OSError:\n            logger.debug(\"Cache saving failed while closing file\")\n        except NameError:\n            logger.debug(\"Cache save failed due to interpreter shutdown\")\n        close()\n        f.closed = True\n\n    def ls(self, path, detail=True):\n        return self.fs.ls(path, detail)\n\n    def __getattribute__(self, item):\n        if item in {\n            \"load_cache\",\n            \"_open\",\n            \"save_cache\",\n            \"close_and_update\",\n            \"__init__\",\n            \"__getattribute__\",\n            \"__reduce__\",\n            \"_make_local_details\",\n            \"open\",\n            \"cat\",\n            \"cat_file\",\n            \"cat_ranges\",\n            \"get\",\n            \"read_block\",\n            \"tail\",\n            \"head\",\n            \"info\",\n            \"ls\",\n            \"exists\",\n            \"isfile\",\n            \"isdir\",\n            \"_check_file\",\n            \"_check_cache\",\n            \"_mkcache\",\n            \"clear_cache\",\n            \"clear_expired_cache\",\n            \"pop_from_cache\",\n            \"local_file\",\n            \"_paths_from_path\",\n            \"get_mapper\",\n            \"open_many\",\n            \"commit_many\",\n            \"hash_name\",\n            \"__hash__\",\n            \"__eq__\",\n            \"to_json\",\n            \"to_dict\",\n            \"cache_size\",\n            \"pipe_file\",\n            \"pipe\",\n            \"start_transaction\",\n            \"end_transaction\",\n        }:\n            # all the methods defined in this class. Note `open` here, since\n            # it calls `_open`, but is actually in superclass\n            return lambda *args, **kw: getattr(type(self), item).__get__(self)(\n                *args, **kw\n            )\n        if item in [\"__reduce_ex__\"]:\n            raise AttributeError\n        if item in [\"transaction\"]:\n            # property\n            return type(self).transaction.__get__(self)\n        if item in [\"_cache\", \"transaction_type\"]:\n            # class attributes\n            return getattr(type(self), item)\n        if item == \"__class__\":\n            return type(self)\n        d = object.__getattribute__(self, \"__dict__\")\n        fs = d.get(\"fs\", None)  # fs is not immediately defined\n        if item in d:\n            return d[item]\n        elif fs is not None:\n            if item in fs.__dict__:\n                # attribute of instance\n                return fs.__dict__[item]\n            # attributed belonging to the target filesystem\n            cls = type(fs)\n            m = getattr(cls, item)\n            if (inspect.isfunction(m) or inspect.isdatadescriptor(m)) and (\n                not hasattr(m, \"__self__\") or m.__self__ is None\n            ):\n                # instance method\n                return m.__get__(fs, cls)\n            return m  # class method or attribute\n        else:\n            # attributes of the superclass, while target is being set up\n            return super().__getattribute__(item)\n\n    def __eq__(self, other):\n        \"\"\"Test for equality.\"\"\"\n        if self is other:\n            return True\n        if not isinstance(other, type(self)):\n            return False\n        return (\n            self.storage == other.storage\n            and self.kwargs == other.kwargs\n            and self.cache_check == other.cache_check\n            and self.check_files == other.check_files\n            and self.expiry == other.expiry\n            and self.compression == other.compression\n            and self._mapper == other._mapper\n            and self.target_protocol == other.target_protocol\n        )\n\n    def __hash__(self):\n        \"\"\"Calculate hash.\"\"\"\n        return (\n            hash(tuple(self.storage))\n            ^ hash(str(self.kwargs))\n            ^ hash(self.cache_check)\n            ^ hash(self.check_files)\n            ^ hash(self.expiry)\n            ^ hash(self.compression)\n            ^ hash(self._mapper)\n            ^ hash(self.target_protocol)\n        )\n\n\nclass WholeFileCacheFileSystem(CachingFileSystem):\n    \"\"\"Caches whole remote files on first access\n\n    This class is intended as a layer over any other file system, and\n    will make a local copy of each file accessed, so that all subsequent\n    reads are local. This is similar to ``CachingFileSystem``, but without\n    the block-wise functionality and so can work even when sparse files\n    are not allowed. See its docstring for definition of the init\n    arguments.\n\n    The class still needs access to the remote store for listing files,\n    and may refresh cached files.\n    \"\"\"\n\n    protocol = \"filecache\"\n    local_file = True\n\n    def open_many(self, open_files, **kwargs):\n        paths = [of.path for of in open_files]\n        if \"r\" in open_files.mode:\n            self._mkcache()\n        else:\n            return [\n                LocalTempFile(\n                    self.fs,\n                    path,\n                    mode=open_files.mode,\n                    fn=os.path.join(self.storage[-1], self._mapper(path)),\n                    **kwargs,\n                )\n                for path in paths\n            ]\n\n        if self.compression:\n            raise NotImplementedError\n        details = [self._check_file(sp) for sp in paths]\n        downpath = [p for p, d in zip(paths, details) if not d]\n        downfn0 = [\n            os.path.join(self.storage[-1], self._mapper(p))\n            for p, d in zip(paths, details)\n        ]  # keep these path names for opening later\n        downfn = [fn for fn, d in zip(downfn0, details) if not d]\n        if downpath:\n            # skip if all files are already cached and up to date\n            self.fs.get(downpath, downfn)\n\n            # update metadata - only happens when downloads are successful\n            newdetail = [\n                {\n                    \"original\": path,\n                    \"fn\": self._mapper(path),\n                    \"blocks\": True,\n                    \"time\": time.time(),\n                    \"uid\": self.fs.ukey(path),\n                }\n                for path in downpath\n            ]\n            for path, detail in zip(downpath, newdetail):\n                self._metadata.update_file(path, detail)\n            self.save_cache()\n\n        def firstpart(fn):\n            # helper to adapt both whole-file and simple-cache\n            return fn[1] if isinstance(fn, tuple) else fn\n\n        return [\n            open(firstpart(fn0) if fn0 else fn1, mode=open_files.mode)\n            for fn0, fn1 in zip(details, downfn0)\n        ]\n\n    def commit_many(self, open_files):\n        self.fs.put([f.fn for f in open_files], [f.path for f in open_files])\n        [f.close() for f in open_files]\n        for f in open_files:\n            # in case autocommit is off, and so close did not already delete\n            try:\n                os.remove(f.name)\n            except FileNotFoundError:\n                pass\n        self._cache_size = None\n\n    def _make_local_details(self, path):\n        hash = self._mapper(path)\n        fn = os.path.join(self.storage[-1], hash)\n        detail = {\n            \"original\": path,\n            \"fn\": hash,\n            \"blocks\": True,\n            \"time\": time.time(),\n            \"uid\": self.fs.ukey(path),\n        }\n        self._metadata.update_file(path, detail)\n        logger.debug(\"Copying %s to local cache\", path)\n        return fn\n\n    def cat(\n        self,\n        path,\n        recursive=False,\n        on_error=\"raise\",\n        callback=DEFAULT_CALLBACK,\n        **kwargs,\n    ):\n        paths = self.expand_path(\n            path, recursive=recursive, maxdepth=kwargs.get(\"maxdepth\", None)\n        )\n        getpaths = []\n        storepaths = []\n        fns = []\n        out = {}\n        for p in paths.copy():\n            try:\n                detail = self._check_file(p)\n                if not detail:\n                    fn = self._make_local_details(p)\n                    getpaths.append(p)\n                    storepaths.append(fn)\n                else:\n                    detail, fn = detail if isinstance(detail, tuple) else (None, detail)\n                fns.append(fn)\n            except Exception as e:\n                if on_error == \"raise\":\n                    raise\n                if on_error == \"return\":\n                    out[p] = e\n                paths.remove(p)\n\n        if getpaths:\n            self.fs.get(getpaths, storepaths)\n            self.save_cache()\n\n        callback.set_size(len(paths))\n        for p, fn in zip(paths, fns):\n            with open(fn, \"rb\") as f:\n                out[p] = f.read()\n            callback.relative_update(1)\n        if isinstance(path, str) and len(paths) == 1 and recursive is False:\n            out = out[paths[0]]\n        return out\n\n    def _open(self, path, mode=\"rb\", **kwargs):\n        path = self._strip_protocol(path)\n        if \"r\" not in mode:\n            hash = self._mapper(path)\n            fn = os.path.join(self.storage[-1], hash)\n            user_specified_kwargs = {\n                k: v\n                for k, v in kwargs.items()\n                # those kwargs were added by open(), we don't want them\n                if k not in [\"autocommit\", \"block_size\", \"cache_options\"]\n            }\n            return LocalTempFile(self, path, mode=mode, fn=fn, **user_specified_kwargs)\n        detail = self._check_file(path)\n        if detail:\n            detail, fn = detail\n            _, blocks = detail[\"fn\"], detail[\"blocks\"]\n            if blocks is True:\n                logger.debug(\"Opening local copy of %s\", path)\n\n                # In order to support downstream filesystems to be able to\n                # infer the compression from the original filename, like\n                # the `TarFileSystem`, let's extend the `io.BufferedReader`\n                # fileobject protocol by adding a dedicated attribute\n                # `original`.\n                f = open(fn, mode)\n                f.original = detail.get(\"original\")\n                return f\n            else:\n                raise ValueError(\n                    f\"Attempt to open partially cached file {path}\"\n                    f\" as a wholly cached file\"\n                )\n        else:\n            fn = self._make_local_details(path)\n        kwargs[\"mode\"] = mode\n\n        # call target filesystems open\n        self._mkcache()\n        if self.compression:\n            with self.fs._open(path, **kwargs) as f, open(fn, \"wb\") as f2:\n                if isinstance(f, AbstractBufferedFile):\n                    # want no type of caching if just downloading whole thing\n                    f.cache = BaseCache(0, f.cache.fetcher, f.size)\n                comp = (\n                    infer_compression(path)\n                    if self.compression == \"infer\"\n                    else self.compression\n                )\n                f = compr[comp](f, mode=\"rb\")\n                data = True\n                while data:\n                    block = getattr(f, \"blocksize\", 5 * 2**20)\n                    data = f.read(block)\n                    f2.write(data)\n        else:\n            self.fs.get_file(path, fn)\n        self.save_cache()\n        return self._open(path, mode)\n\n\nclass SimpleCacheFileSystem(WholeFileCacheFileSystem):\n    \"\"\"Caches whole remote files on first access\n\n    This class is intended as a layer over any other file system, and\n    will make a local copy of each file accessed, so that all subsequent\n    reads are local. This implementation only copies whole files, and\n    does not keep any metadata about the download time or file details.\n    It is therefore safer to use in multi-threaded/concurrent situations.\n\n    This is the only of the caching filesystems that supports write: you will\n    be given a real local open file, and upon close and commit, it will be\n    uploaded to the target filesystem; the writability or the target URL is\n    not checked until that time.\n\n    \"\"\"\n\n    protocol = \"simplecache\"\n    local_file = True\n    transaction_type = WriteCachedTransaction\n\n    def __init__(self, **kwargs):\n        kw = kwargs.copy()\n        for key in [\"cache_check\", \"expiry_time\", \"check_files\"]:\n            kw[key] = False\n        super().__init__(**kw)\n        for storage in self.storage:\n            if not os.path.exists(storage):\n                os.makedirs(storage, exist_ok=True)\n\n    def _check_file(self, path):\n        self._check_cache()\n        sha = self._mapper(path)\n        for storage in self.storage:\n            fn = os.path.join(storage, sha)\n            if os.path.exists(fn):\n                return fn\n\n    def save_cache(self):\n        pass\n\n    def load_cache(self):\n        pass\n\n    def pipe_file(self, path, value=None, **kwargs):\n        if self._intrans:\n            with self.open(path, \"wb\") as f:\n                f.write(value)\n        else:\n            super().pipe_file(path, value)\n\n    def ls(self, path, detail=True, **kwargs):\n        path = self._strip_protocol(path)\n        details = []\n        try:\n            details = self.fs.ls(\n                path, detail=True, **kwargs\n            ).copy()  # don't edit original!\n        except FileNotFoundError as e:\n            ex = e\n        else:\n            ex = None\n        if self._intrans:\n            path1 = path.rstrip(\"/\") + \"/\"\n            for f in self.transaction.files:\n                if f.path == path:\n                    details.append(\n                        {\"name\": path, \"size\": f.size or f.tell(), \"type\": \"file\"}\n                    )\n                elif f.path.startswith(path1):\n                    if f.path.count(\"/\") == path1.count(\"/\"):\n                        details.append(\n                            {\"name\": f.path, \"size\": f.size or f.tell(), \"type\": \"file\"}\n                        )\n                    else:\n                        dname = \"/\".join(f.path.split(\"/\")[: path1.count(\"/\") + 1])\n                        details.append({\"name\": dname, \"size\": 0, \"type\": \"directory\"})\n        if ex is not None and not details:\n            raise ex\n        if detail:\n            return details\n        return sorted(_[\"name\"] for _ in details)\n\n    def info(self, path, **kwargs):\n        path = self._strip_protocol(path)\n        if self._intrans:\n            f = [_ for _ in self.transaction.files if _.path == path]\n            if f:\n                size = os.path.getsize(f[0].fn) if f[0].closed else f[0].tell()\n                return {\"name\": path, \"size\": size, \"type\": \"file\"}\n            f = any(_.path.startswith(path + \"/\") for _ in self.transaction.files)\n            if f:\n                return {\"name\": path, \"size\": 0, \"type\": \"directory\"}\n        return self.fs.info(path, **kwargs)\n\n    def pipe(self, path, value=None, **kwargs):\n        if isinstance(path, str):\n            self.pipe_file(self._strip_protocol(path), value, **kwargs)\n        elif isinstance(path, dict):\n            for k, v in path.items():\n                self.pipe_file(self._strip_protocol(k), v, **kwargs)\n        else:\n            raise ValueError(\"path must be str or dict\")\n\n    def cat_ranges(\n        self, paths, starts, ends, max_gap=None, on_error=\"return\", **kwargs\n    ):\n        lpaths = [self._check_file(p) for p in paths]\n        rpaths = [p for l, p in zip(lpaths, paths) if l is False]\n        lpaths = [l for l, p in zip(lpaths, paths) if l is False]\n        self.fs.get(rpaths, lpaths)\n        return super().cat_ranges(\n            paths, starts, ends, max_gap=max_gap, on_error=on_error, **kwargs\n        )\n\n    def _open(self, path, mode=\"rb\", **kwargs):\n        path = self._strip_protocol(path)\n        sha = self._mapper(path)\n\n        if \"r\" not in mode:\n            fn = os.path.join(self.storage[-1], sha)\n            user_specified_kwargs = {\n                k: v\n                for k, v in kwargs.items()\n                if k not in [\"autocommit\", \"block_size\", \"cache_options\"]\n            }  # those were added by open()\n            return LocalTempFile(\n                self,\n                path,\n                mode=mode,\n                autocommit=not self._intrans,\n                fn=fn,\n                **user_specified_kwargs,\n            )\n        fn = self._check_file(path)\n        if fn:\n            return open(fn, mode)\n\n        fn = os.path.join(self.storage[-1], sha)\n        logger.debug(\"Copying %s to local cache\", path)\n        kwargs[\"mode\"] = mode\n\n        self._mkcache()\n        self._cache_size = None\n        if self.compression:\n            with self.fs._open(path, **kwargs) as f, open(fn, \"wb\") as f2:\n                if isinstance(f, AbstractBufferedFile):\n                    # want no type of caching if just downloading whole thing\n                    f.cache = BaseCache(0, f.cache.fetcher, f.size)\n                comp = (\n                    infer_compression(path)\n                    if self.compression == \"infer\"\n                    else self.compression\n                )\n                f = compr[comp](f, mode=\"rb\")\n                data = True\n                while data:\n                    block = getattr(f, \"blocksize\", 5 * 2**20)\n                    data = f.read(block)\n                    f2.write(data)\n        else:\n            self.fs.get_file(path, fn)\n        return self._open(path, mode)\n\n\nclass LocalTempFile:\n    \"\"\"A temporary local file, which will be uploaded on commit\"\"\"\n\n    def __init__(self, fs, path, fn, mode=\"wb\", autocommit=True, seek=0, **kwargs):\n        self.fn = fn\n        self.fh = open(fn, mode)\n        self.mode = mode\n        if seek:\n            self.fh.seek(seek)\n        self.path = path\n        self.size = None\n        self.fs = fs\n        self.closed = False\n        self.autocommit = autocommit\n        self.kwargs = kwargs\n\n    def __reduce__(self):\n        # always open in r+b to allow continuing writing at a location\n        return (\n            LocalTempFile,\n            (self.fs, self.path, self.fn, \"r+b\", self.autocommit, self.tell()),\n        )\n\n    def __enter__(self):\n        return self.fh\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        # self.size = self.fh.tell()\n        if self.closed:\n            return\n        self.fh.close()\n        self.closed = True\n        if self.autocommit:\n            self.commit()\n\n    def discard(self):\n        self.fh.close()\n        os.remove(self.fn)\n\n    def commit(self):\n        self.fs.put(self.fn, self.path, **self.kwargs)\n        # we do not delete local copy - it's still in the cache\n\n    @property\n    def name(self):\n        return self.fn\n\n    def __repr__(self) -> str:\n        return f\"LocalTempFile: {self.path}\"\n\n    def __getattr__(self, item):\n        return getattr(self.fh, item)\n", "fsspec/implementations/ftp.py": "import os\nimport sys\nimport uuid\nimport warnings\nfrom ftplib import FTP, Error, error_perm\nfrom typing import Any\n\nfrom ..spec import AbstractBufferedFile, AbstractFileSystem\nfrom ..utils import infer_storage_options, isfilelike\n\n\nclass FTPFileSystem(AbstractFileSystem):\n    \"\"\"A filesystem over classic FTP\"\"\"\n\n    root_marker = \"/\"\n    cachable = False\n    protocol = \"ftp\"\n\n    def __init__(\n        self,\n        host,\n        port=21,\n        username=None,\n        password=None,\n        acct=None,\n        block_size=None,\n        tempdir=None,\n        timeout=30,\n        encoding=\"utf-8\",\n        **kwargs,\n    ):\n        \"\"\"\n        You can use _get_kwargs_from_urls to get some kwargs from\n        a reasonable FTP url.\n\n        Authentication will be anonymous if username/password are not\n        given.\n\n        Parameters\n        ----------\n        host: str\n            The remote server name/ip to connect to\n        port: int\n            Port to connect with\n        username: str or None\n            If authenticating, the user's identifier\n        password: str of None\n            User's password on the server, if using\n        acct: str or None\n            Some servers also need an \"account\" string for auth\n        block_size: int or None\n            If given, the read-ahead or write buffer size.\n        tempdir: str\n            Directory on remote to put temporary files when in a transaction\n        timeout: int\n            Timeout of the ftp connection in seconds\n        encoding: str\n            Encoding to use for directories and filenames in FTP connection\n        \"\"\"\n        super().__init__(**kwargs)\n        self.host = host\n        self.port = port\n        self.tempdir = tempdir or \"/tmp\"\n        self.cred = username, password, acct\n        self.timeout = timeout\n        self.encoding = encoding\n        if block_size is not None:\n            self.blocksize = block_size\n        else:\n            self.blocksize = 2**16\n        self._connect()\n\n    def _connect(self):\n        if sys.version_info >= (3, 9):\n            self.ftp = FTP(timeout=self.timeout, encoding=self.encoding)\n        elif self.encoding:\n            warnings.warn(\"`encoding` not supported for python<3.9, ignoring\")\n            self.ftp = FTP(timeout=self.timeout)\n        else:\n            self.ftp = FTP(timeout=self.timeout)\n        self.ftp.connect(self.host, self.port)\n        self.ftp.login(*self.cred)\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        return \"/\" + infer_storage_options(path)[\"path\"].lstrip(\"/\").rstrip(\"/\")\n\n    @staticmethod\n    def _get_kwargs_from_urls(urlpath):\n        out = infer_storage_options(urlpath)\n        out.pop(\"path\", None)\n        out.pop(\"protocol\", None)\n        return out\n\n    def ls(self, path, detail=True, **kwargs):\n        path = self._strip_protocol(path)\n        out = []\n        if path not in self.dircache:\n            try:\n                try:\n                    out = [\n                        (fn, details)\n                        for (fn, details) in self.ftp.mlsd(path)\n                        if fn not in [\".\", \"..\"]\n                        and details[\"type\"] not in [\"pdir\", \"cdir\"]\n                    ]\n                except error_perm:\n                    out = _mlsd2(self.ftp, path)  # Not platform independent\n                for fn, details in out:\n                    if path == \"/\":\n                        path = \"\"  # just for forming the names, below\n                    details[\"name\"] = \"/\".join([path, fn.lstrip(\"/\")])\n                    if details[\"type\"] == \"file\":\n                        details[\"size\"] = int(details[\"size\"])\n                    else:\n                        details[\"size\"] = 0\n                    if details[\"type\"] == \"dir\":\n                        details[\"type\"] = \"directory\"\n                self.dircache[path] = out\n            except Error:\n                try:\n                    info = self.info(path)\n                    if info[\"type\"] == \"file\":\n                        out = [(path, info)]\n                except (Error, IndexError):\n                    raise FileNotFoundError(path)\n        files = self.dircache.get(path, out)\n        if not detail:\n            return sorted([fn for fn, details in files])\n        return [details for fn, details in files]\n\n    def info(self, path, **kwargs):\n        # implement with direct method\n        path = self._strip_protocol(path)\n        if path == \"/\":\n            # special case, since this dir has no real entry\n            return {\"name\": \"/\", \"size\": 0, \"type\": \"directory\"}\n        files = self.ls(self._parent(path).lstrip(\"/\"), True)\n        try:\n            out = [f for f in files if f[\"name\"] == path][0]\n        except IndexError:\n            raise FileNotFoundError(path)\n        return out\n\n    def get_file(self, rpath, lpath, **kwargs):\n        if self.isdir(rpath):\n            if not os.path.exists(lpath):\n                os.mkdir(lpath)\n            return\n        if isfilelike(lpath):\n            outfile = lpath\n        else:\n            outfile = open(lpath, \"wb\")\n\n        def cb(x):\n            outfile.write(x)\n\n        self.ftp.retrbinary(\n            f\"RETR {rpath}\",\n            blocksize=self.blocksize,\n            callback=cb,\n        )\n        if not isfilelike(lpath):\n            outfile.close()\n\n    def cat_file(self, path, start=None, end=None, **kwargs):\n        if end is not None:\n            return super().cat_file(path, start, end, **kwargs)\n        out = []\n\n        def cb(x):\n            out.append(x)\n\n        try:\n            self.ftp.retrbinary(\n                f\"RETR {path}\",\n                blocksize=self.blocksize,\n                rest=start,\n                callback=cb,\n            )\n        except (Error, error_perm) as orig_exc:\n            raise FileNotFoundError(path) from orig_exc\n        return b\"\".join(out)\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        cache_options=None,\n        autocommit=True,\n        **kwargs,\n    ):\n        path = self._strip_protocol(path)\n        block_size = block_size or self.blocksize\n        return FTPFile(\n            self,\n            path,\n            mode=mode,\n            block_size=block_size,\n            tempdir=self.tempdir,\n            autocommit=autocommit,\n            cache_options=cache_options,\n        )\n\n    def _rm(self, path):\n        path = self._strip_protocol(path)\n        self.ftp.delete(path)\n        self.invalidate_cache(self._parent(path))\n\n    def rm(self, path, recursive=False, maxdepth=None):\n        paths = self.expand_path(path, recursive=recursive, maxdepth=maxdepth)\n        for p in reversed(paths):\n            if self.isfile(p):\n                self.rm_file(p)\n            else:\n                self.rmdir(p)\n\n    def mkdir(self, path: str, create_parents: bool = True, **kwargs: Any) -> None:\n        path = self._strip_protocol(path)\n        parent = self._parent(path)\n        if parent != self.root_marker and not self.exists(parent) and create_parents:\n            self.mkdir(parent, create_parents=create_parents)\n\n        self.ftp.mkd(path)\n        self.invalidate_cache(self._parent(path))\n\n    def makedirs(self, path: str, exist_ok: bool = False) -> None:\n        path = self._strip_protocol(path)\n        if self.exists(path):\n            # NB: \"/\" does not \"exist\" as it has no directory entry\n            if not exist_ok:\n                raise FileExistsError(f\"{path} exists without `exist_ok`\")\n            # exists_ok=True -> no-op\n        else:\n            self.mkdir(path, create_parents=True)\n\n    def rmdir(self, path):\n        path = self._strip_protocol(path)\n        self.ftp.rmd(path)\n        self.invalidate_cache(self._parent(path))\n\n    def mv(self, path1, path2, **kwargs):\n        path1 = self._strip_protocol(path1)\n        path2 = self._strip_protocol(path2)\n        self.ftp.rename(path1, path2)\n        self.invalidate_cache(self._parent(path1))\n        self.invalidate_cache(self._parent(path2))\n\n    def __del__(self):\n        self.ftp.close()\n\n    def invalidate_cache(self, path=None):\n        if path is None:\n            self.dircache.clear()\n        else:\n            self.dircache.pop(path, None)\n        super().invalidate_cache(path)\n\n\nclass TransferDone(Exception):\n    \"\"\"Internal exception to break out of transfer\"\"\"\n\n    pass\n\n\nclass FTPFile(AbstractBufferedFile):\n    \"\"\"Interact with a remote FTP file with read/write buffering\"\"\"\n\n    def __init__(\n        self,\n        fs,\n        path,\n        mode=\"rb\",\n        block_size=\"default\",\n        autocommit=True,\n        cache_type=\"readahead\",\n        cache_options=None,\n        **kwargs,\n    ):\n        super().__init__(\n            fs,\n            path,\n            mode=mode,\n            block_size=block_size,\n            autocommit=autocommit,\n            cache_type=cache_type,\n            cache_options=cache_options,\n            **kwargs,\n        )\n        if not autocommit:\n            self.target = self.path\n            self.path = \"/\".join([kwargs[\"tempdir\"], str(uuid.uuid4())])\n\n    def commit(self):\n        self.fs.mv(self.path, self.target)\n\n    def discard(self):\n        self.fs.rm(self.path)\n\n    def _fetch_range(self, start, end):\n        \"\"\"Get bytes between given byte limits\n\n        Implemented by raising an exception in the fetch callback when the\n        number of bytes received reaches the requested amount.\n\n        Will fail if the server does not respect the REST command on\n        retrieve requests.\n        \"\"\"\n        out = []\n        total = [0]\n\n        def callback(x):\n            total[0] += len(x)\n            if total[0] > end - start:\n                out.append(x[: (end - start) - total[0]])\n                if end < self.size:\n                    raise TransferDone\n            else:\n                out.append(x)\n\n            if total[0] == end - start and end < self.size:\n                raise TransferDone\n\n        try:\n            self.fs.ftp.retrbinary(\n                f\"RETR {self.path}\",\n                blocksize=self.blocksize,\n                rest=start,\n                callback=callback,\n            )\n        except TransferDone:\n            try:\n                # stop transfer, we got enough bytes for this block\n                self.fs.ftp.abort()\n                self.fs.ftp.getmultiline()\n            except Error:\n                self.fs._connect()\n\n        return b\"\".join(out)\n\n    def _upload_chunk(self, final=False):\n        self.buffer.seek(0)\n        self.fs.ftp.storbinary(\n            f\"STOR {self.path}\", self.buffer, blocksize=self.blocksize, rest=self.offset\n        )\n        return True\n\n\ndef _mlsd2(ftp, path=\".\"):\n    \"\"\"\n    Fall back to using `dir` instead of `mlsd` if not supported.\n\n    This parses a Linux style `ls -l` response to `dir`, but the response may\n    be platform dependent.\n\n    Parameters\n    ----------\n    ftp: ftplib.FTP\n    path: str\n        Expects to be given path, but defaults to \".\".\n    \"\"\"\n    lines = []\n    minfo = []\n    ftp.dir(path, lines.append)\n    for line in lines:\n        split_line = line.split()\n        if len(split_line) < 9:\n            continue\n        this = (\n            split_line[-1],\n            {\n                \"modify\": \" \".join(split_line[5:8]),\n                \"unix.owner\": split_line[2],\n                \"unix.group\": split_line[3],\n                \"unix.mode\": split_line[0],\n                \"size\": split_line[4],\n            },\n        )\n        if \"d\" == this[1][\"unix.mode\"][0]:\n            this[1][\"type\"] = \"dir\"\n        else:\n            this[1][\"type\"] = \"file\"\n        minfo.append(this)\n    return minfo\n", "fsspec/implementations/dask.py": "import dask\nfrom distributed.client import Client, _get_global_client\nfrom distributed.worker import Worker\n\nfrom fsspec import filesystem\nfrom fsspec.spec import AbstractBufferedFile, AbstractFileSystem\nfrom fsspec.utils import infer_storage_options\n\n\ndef _get_client(client):\n    if client is None:\n        return _get_global_client()\n    elif isinstance(client, Client):\n        return client\n    else:\n        # e.g., connection string\n        return Client(client)\n\n\ndef _in_worker():\n    return bool(Worker._instances)\n\n\nclass DaskWorkerFileSystem(AbstractFileSystem):\n    \"\"\"View files accessible to a worker as any other remote file-system\n\n    When instances are run on the worker, uses the real filesystem. When\n    run on the client, they call the worker to provide information or data.\n\n    **Warning** this implementation is experimental, and read-only for now.\n    \"\"\"\n\n    def __init__(\n        self, target_protocol=None, target_options=None, fs=None, client=None, **kwargs\n    ):\n        super().__init__(**kwargs)\n        if not (fs is None) ^ (target_protocol is None):\n            raise ValueError(\n                \"Please provide one of filesystem instance (fs) or\"\n                \" target_protocol, not both\"\n            )\n        self.target_protocol = target_protocol\n        self.target_options = target_options\n        self.worker = None\n        self.client = client\n        self.fs = fs\n        self._determine_worker()\n\n    @staticmethod\n    def _get_kwargs_from_urls(path):\n        so = infer_storage_options(path)\n        if \"host\" in so and \"port\" in so:\n            return {\"client\": f\"{so['host']}:{so['port']}\"}\n        else:\n            return {}\n\n    def _determine_worker(self):\n        if _in_worker():\n            self.worker = True\n            if self.fs is None:\n                self.fs = filesystem(\n                    self.target_protocol, **(self.target_options or {})\n                )\n        else:\n            self.worker = False\n            self.client = _get_client(self.client)\n            self.rfs = dask.delayed(self)\n\n    def mkdir(self, *args, **kwargs):\n        if self.worker:\n            self.fs.mkdir(*args, **kwargs)\n        else:\n            self.rfs.mkdir(*args, **kwargs).compute()\n\n    def rm(self, *args, **kwargs):\n        if self.worker:\n            self.fs.rm(*args, **kwargs)\n        else:\n            self.rfs.rm(*args, **kwargs).compute()\n\n    def copy(self, *args, **kwargs):\n        if self.worker:\n            self.fs.copy(*args, **kwargs)\n        else:\n            self.rfs.copy(*args, **kwargs).compute()\n\n    def mv(self, *args, **kwargs):\n        if self.worker:\n            self.fs.mv(*args, **kwargs)\n        else:\n            self.rfs.mv(*args, **kwargs).compute()\n\n    def ls(self, *args, **kwargs):\n        if self.worker:\n            return self.fs.ls(*args, **kwargs)\n        else:\n            return self.rfs.ls(*args, **kwargs).compute()\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        **kwargs,\n    ):\n        if self.worker:\n            return self.fs._open(\n                path,\n                mode=mode,\n                block_size=block_size,\n                autocommit=autocommit,\n                cache_options=cache_options,\n                **kwargs,\n            )\n        else:\n            return DaskFile(\n                fs=self,\n                path=path,\n                mode=mode,\n                block_size=block_size,\n                autocommit=autocommit,\n                cache_options=cache_options,\n                **kwargs,\n            )\n\n    def fetch_range(self, path, mode, start, end):\n        if self.worker:\n            with self._open(path, mode) as f:\n                f.seek(start)\n                return f.read(end - start)\n        else:\n            return self.rfs.fetch_range(path, mode, start, end).compute()\n\n\nclass DaskFile(AbstractBufferedFile):\n    def __init__(self, mode=\"rb\", **kwargs):\n        if mode != \"rb\":\n            raise ValueError('Remote dask files can only be opened in \"rb\" mode')\n        super().__init__(**kwargs)\n\n    def _upload_chunk(self, final=False):\n        pass\n\n    def _initiate_upload(self):\n        \"\"\"Create remote file/upload\"\"\"\n        pass\n\n    def _fetch_range(self, start, end):\n        \"\"\"Get the specified set of bytes from remote\"\"\"\n        return self.fs.fetch_range(self.path, self.mode, start, end)\n", "fsspec/implementations/reference.py": "import base64\nimport collections\nimport io\nimport itertools\nimport logging\nimport math\nimport os\nfrom functools import lru_cache\nfrom typing import TYPE_CHECKING\n\nimport fsspec.core\n\ntry:\n    import ujson as json\nexcept ImportError:\n    if not TYPE_CHECKING:\n        import json\n\nfrom ..asyn import AsyncFileSystem\nfrom ..callbacks import DEFAULT_CALLBACK\nfrom ..core import filesystem, open, split_protocol\nfrom ..utils import isfilelike, merge_offset_ranges, other_paths\n\nlogger = logging.getLogger(\"fsspec.reference\")\n\n\nclass ReferenceNotReachable(RuntimeError):\n    def __init__(self, reference, target, *args):\n        super().__init__(*args)\n        self.reference = reference\n        self.target = target\n\n    def __str__(self):\n        return f'Reference \"{self.reference}\" failed to fetch target {self.target}'\n\n\ndef _first(d):\n    return list(d.values())[0]\n\n\ndef _prot_in_references(path, references):\n    ref = references.get(path)\n    if isinstance(ref, (list, tuple)):\n        return split_protocol(ref[0])[0] if ref[0] else ref[0]\n\n\ndef _protocol_groups(paths, references):\n    if isinstance(paths, str):\n        return {_prot_in_references(paths, references): [paths]}\n    out = {}\n    for path in paths:\n        protocol = _prot_in_references(path, references)\n        out.setdefault(protocol, []).append(path)\n    return out\n\n\nclass RefsValuesView(collections.abc.ValuesView):\n    def __iter__(self):\n        for val in self._mapping.zmetadata.values():\n            yield json.dumps(val).encode()\n        yield from self._mapping._items.values()\n        for field in self._mapping.listdir():\n            chunk_sizes = self._mapping._get_chunk_sizes(field)\n            if len(chunk_sizes) == 0:\n                yield self._mapping[field + \"/0\"]\n                continue\n            yield from self._mapping._generate_all_records(field)\n\n\nclass RefsItemsView(collections.abc.ItemsView):\n    def __iter__(self):\n        return zip(self._mapping.keys(), self._mapping.values())\n\n\ndef ravel_multi_index(idx, sizes):\n    val = 0\n    mult = 1\n    for i, s in zip(idx[::-1], sizes[::-1]):\n        val += i * mult\n        mult *= s\n    return val\n\n\nclass LazyReferenceMapper(collections.abc.MutableMapping):\n    \"\"\"This interface can be used to read/write references from Parquet stores.\n    It is not intended for other types of references.\n    It can be used with Kerchunk's MultiZarrToZarr method to combine\n    references into a parquet store.\n    Examples of this use-case can be found here:\n    https://fsspec.github.io/kerchunk/advanced.html?highlight=parquet#parquet-storage\"\"\"\n\n    # import is class level to prevent numpy dep requirement for fsspec\n    @property\n    def np(self):\n        import numpy as np\n\n        return np\n\n    @property\n    def pd(self):\n        import pandas as pd\n\n        return pd\n\n    def __init__(\n        self, root, fs=None, out_root=None, cache_size=128, categorical_threshold=10\n    ):\n        \"\"\"\n\n        This instance will be writable, storing changes in memory until full partitions\n        are accumulated or .flush() is called.\n\n        To create an empty lazy store, use .create()\n\n        Parameters\n        ----------\n        root : str\n            Root of parquet store\n        fs : fsspec.AbstractFileSystem\n            fsspec filesystem object, default is local filesystem.\n        cache_size : int, default=128\n            Maximum size of LRU cache, where cache_size*record_size denotes\n            the total number of references that can be loaded in memory at once.\n        categorical_threshold : int\n            Encode urls as pandas.Categorical to reduce memory footprint if the ratio\n            of the number of unique urls to total number of refs for each variable\n            is greater than or equal to this number. (default 10)\n        \"\"\"\n        self.root = root\n        self.chunk_sizes = {}\n        self.out_root = out_root or self.root\n        self.cat_thresh = categorical_threshold\n        self.cache_size = cache_size\n        self.dirs = None\n        self.url = self.root + \"/{field}/refs.{record}.parq\"\n        # TODO: derive fs from `root`\n        self.fs = fsspec.filesystem(\"file\") if fs is None else fs\n\n    def __getattr__(self, item):\n        if item in (\"_items\", \"record_size\", \"zmetadata\"):\n            self.setup()\n            # avoid possible recursion if setup fails somehow\n            return self.__dict__[item]\n        raise AttributeError(item)\n\n    def setup(self):\n        self._items = {}\n        self._items[\".zmetadata\"] = self.fs.cat_file(\n            \"/\".join([self.root, \".zmetadata\"])\n        )\n        met = json.loads(self._items[\".zmetadata\"])\n        self.record_size = met[\"record_size\"]\n        self.zmetadata = met[\"metadata\"]\n\n        # Define function to open and decompress refs\n        @lru_cache(maxsize=self.cache_size)\n        def open_refs(field, record):\n            \"\"\"cached parquet file loader\"\"\"\n            path = self.url.format(field=field, record=record)\n            data = io.BytesIO(self.fs.cat_file(path))\n            df = self.pd.read_parquet(data, engine=\"fastparquet\")\n            refs = {c: df[c].values for c in df.columns}\n            return refs\n\n        self.open_refs = open_refs\n\n    @staticmethod\n    def create(root, storage_options=None, fs=None, record_size=10000, **kwargs):\n        \"\"\"Make empty parquet reference set\n\n        First deletes the contents of the given directory, if it exists.\n\n        Parameters\n        ----------\n        root: str\n            Directory to contain the output; will be created\n        storage_options: dict | None\n            For making the filesystem to use for writing is fs is None\n        fs: FileSystem | None\n            Filesystem for writing\n        record_size: int\n            Number of references per parquet file\n        kwargs: passed to __init__\n\n        Returns\n        -------\n        LazyReferenceMapper instance\n        \"\"\"\n        met = {\"metadata\": {}, \"record_size\": record_size}\n        if fs is None:\n            fs, root = fsspec.core.url_to_fs(root, **(storage_options or {}))\n        if fs.exists(root):\n            fs.rm(root, recursive=True)\n        fs.makedirs(root, exist_ok=True)\n        fs.pipe(\"/\".join([root, \".zmetadata\"]), json.dumps(met).encode())\n        return LazyReferenceMapper(root, fs, **kwargs)\n\n    def listdir(self, basename=True):\n        \"\"\"List top-level directories\"\"\"\n        # cache me?\n        if self.dirs is None:\n            dirs = [p.split(\"/\", 1)[0] for p in self.zmetadata]\n            self.dirs = {p for p in dirs if p and not p.startswith(\".\")}\n        listing = self.dirs\n        if basename:\n            listing = [os.path.basename(path) for path in listing]\n        return listing\n\n    def ls(self, path=\"\", detail=True):\n        \"\"\"Shortcut file listings\"\"\"\n        if not path:\n            dirnames = self.listdir()\n            others = set(\n                [\".zmetadata\"]\n                + [name for name in self.zmetadata if \"/\" not in name]\n                + [name for name in self._items if \"/\" not in name]\n            )\n            if detail is False:\n                others.update(dirnames)\n                return sorted(others)\n            dirinfo = [\n                {\"name\": name, \"type\": \"directory\", \"size\": 0} for name in dirnames\n            ]\n            fileinfo = [\n                {\n                    \"name\": name,\n                    \"type\": \"file\",\n                    \"size\": len(\n                        json.dumps(self.zmetadata[name])\n                        if name in self.zmetadata\n                        else self._items[name]\n                    ),\n                }\n                for name in others\n            ]\n            return sorted(dirinfo + fileinfo, key=lambda s: s[\"name\"])\n        parts = path.split(\"/\", 1)\n        if len(parts) > 1:\n            raise FileNotFoundError(\"Cannot list within directories right now\")\n        field = parts[0]\n        others = set(\n            [name for name in self.zmetadata if name.startswith(f\"{path}/\")]\n            + [name for name in self._items if name.startswith(f\"{path}/\")]\n        )\n        fileinfo = [\n            {\n                \"name\": name,\n                \"type\": \"file\",\n                \"size\": len(\n                    json.dumps(self.zmetadata[name])\n                    if name in self.zmetadata\n                    else self._items[name]\n                ),\n            }\n            for name in others\n        ]\n        keys = self._keys_in_field(field)\n\n        if detail is False:\n            return list(others) + list(keys)\n        recs = self._generate_all_records(field)\n        recinfo = [\n            {\"name\": name, \"type\": \"file\", \"size\": rec[-1]}\n            for name, rec in zip(keys, recs)\n            if rec[0]  # filters out path==None, deleted/missing\n        ]\n        return fileinfo + recinfo\n\n    def _load_one_key(self, key):\n        \"\"\"Get the reference for one key\n\n        Returns bytes, one-element list or three-element list.\n        \"\"\"\n        if key in self._items:\n            return self._items[key]\n        elif key in self.zmetadata:\n            return json.dumps(self.zmetadata[key]).encode()\n        elif \"/\" not in key or self._is_meta(key):\n            raise KeyError(key)\n        field, _ = key.rsplit(\"/\", 1)\n        record, ri, chunk_size = self._key_to_record(key)\n        maybe = self._items.get((field, record), {}).get(ri, False)\n        if maybe is None:\n            # explicitly deleted\n            raise KeyError\n        elif maybe:\n            return maybe\n        elif chunk_size == 0:\n            return b\"\"\n\n        # Chunk keys can be loaded from row group and cached in LRU cache\n        try:\n            refs = self.open_refs(field, record)\n        except (ValueError, TypeError, FileNotFoundError):\n            raise KeyError(key)\n        columns = [\"path\", \"offset\", \"size\", \"raw\"]\n        selection = [refs[c][ri] if c in refs else None for c in columns]\n        raw = selection[-1]\n        if raw is not None:\n            return raw\n        if selection[0] is None:\n            raise KeyError(\"This reference does not exist or has been deleted\")\n        if selection[1:3] == [0, 0]:\n            # URL only\n            return selection[:1]\n        # URL, offset, size\n        return selection[:3]\n\n    @lru_cache(4096)\n    def _key_to_record(self, key):\n        \"\"\"Details needed to construct a reference for one key\"\"\"\n        field, chunk = key.rsplit(\"/\", 1)\n        chunk_sizes = self._get_chunk_sizes(field)\n        if len(chunk_sizes) == 0:\n            return 0, 0, 0\n        chunk_idx = [int(c) for c in chunk.split(\".\")]\n        chunk_number = ravel_multi_index(chunk_idx, chunk_sizes)\n        record = chunk_number // self.record_size\n        ri = chunk_number % self.record_size\n        return record, ri, len(chunk_sizes)\n\n    def _get_chunk_sizes(self, field):\n        \"\"\"The number of chunks along each axis for a given field\"\"\"\n        if field not in self.chunk_sizes:\n            zarray = self.zmetadata[f\"{field}/.zarray\"]\n            size_ratio = [\n                math.ceil(s / c) for s, c in zip(zarray[\"shape\"], zarray[\"chunks\"])\n            ]\n            self.chunk_sizes[field] = size_ratio or [1]\n        return self.chunk_sizes[field]\n\n    def _generate_record(self, field, record):\n        \"\"\"The references for a given parquet file of a given field\"\"\"\n        refs = self.open_refs(field, record)\n        it = iter(zip(*refs.values()))\n        if len(refs) == 3:\n            # All urls\n            return (list(t) for t in it)\n        elif len(refs) == 1:\n            # All raws\n            return refs[\"raw\"]\n        else:\n            # Mix of urls and raws\n            return (list(t[:3]) if not t[3] else t[3] for t in it)\n\n    def _generate_all_records(self, field):\n        \"\"\"Load all the references within a field by iterating over the parquet files\"\"\"\n        nrec = 1\n        for ch in self._get_chunk_sizes(field):\n            nrec *= ch\n        nrec = math.ceil(nrec / self.record_size)\n        for record in range(nrec):\n            yield from self._generate_record(field, record)\n\n    def values(self):\n        return RefsValuesView(self)\n\n    def items(self):\n        return RefsItemsView(self)\n\n    def __hash__(self):\n        return id(self)\n\n    def __getitem__(self, key):\n        return self._load_one_key(key)\n\n    def __setitem__(self, key, value):\n        if \"/\" in key and not self._is_meta(key):\n            field, chunk = key.rsplit(\"/\", 1)\n            record, i, _ = self._key_to_record(key)\n            subdict = self._items.setdefault((field, record), {})\n            subdict[i] = value\n            if len(subdict) == self.record_size:\n                self.write(field, record)\n        else:\n            # metadata or top-level\n            self._items[key] = value\n            new_value = json.loads(\n                value.decode() if isinstance(value, bytes) else value\n            )\n            self.zmetadata[key] = {**self.zmetadata.get(key, {}), **new_value}\n\n    @staticmethod\n    def _is_meta(key):\n        return key.startswith(\".z\") or \"/.z\" in key\n\n    def __delitem__(self, key):\n        if key in self._items:\n            del self._items[key]\n        elif key in self.zmetadata:\n            del self.zmetadata[key]\n        else:\n            if \"/\" in key and not self._is_meta(key):\n                field, _ = key.rsplit(\"/\", 1)\n                record, i, _ = self._key_to_record(key)\n                subdict = self._items.setdefault((field, record), {})\n                subdict[i] = None\n                if len(subdict) == self.record_size:\n                    self.write(field, record)\n            else:\n                # metadata or top-level\n                self._items[key] = None\n\n    def write(self, field, record, base_url=None, storage_options=None):\n        # extra requirements if writing\n        import kerchunk.df\n        import numpy as np\n        import pandas as pd\n\n        partition = self._items[(field, record)]\n        original = False\n        if len(partition) < self.record_size:\n            try:\n                original = self.open_refs(field, record)\n            except IOError:\n                pass\n\n        if original:\n            paths = original[\"path\"]\n            offsets = original[\"offset\"]\n            sizes = original[\"size\"]\n            raws = original[\"raw\"]\n        else:\n            paths = np.full(self.record_size, np.nan, dtype=\"O\")\n            offsets = np.zeros(self.record_size, dtype=\"int64\")\n            sizes = np.zeros(self.record_size, dtype=\"int64\")\n            raws = np.full(self.record_size, np.nan, dtype=\"O\")\n        for j, data in partition.items():\n            if isinstance(data, list):\n                if (\n                    str(paths.dtype) == \"category\"\n                    and data[0] not in paths.dtype.categories\n                ):\n                    paths = paths.add_categories(data[0])\n                paths[j] = data[0]\n                if len(data) > 1:\n                    offsets[j] = data[1]\n                    sizes[j] = data[2]\n            elif data is None:\n                # delete\n                paths[j] = None\n                offsets[j] = 0\n                sizes[j] = 0\n                raws[j] = None\n            else:\n                # this is the only call into kerchunk, could remove\n                raws[j] = kerchunk.df._proc_raw(data)\n        # TODO: only save needed columns\n        df = pd.DataFrame(\n            {\n                \"path\": paths,\n                \"offset\": offsets,\n                \"size\": sizes,\n                \"raw\": raws,\n            },\n            copy=False,\n        )\n        if df.path.count() / (df.path.nunique() or 1) > self.cat_thresh:\n            df[\"path\"] = df[\"path\"].astype(\"category\")\n        object_encoding = {\"raw\": \"bytes\", \"path\": \"utf8\"}\n        has_nulls = [\"path\", \"raw\"]\n\n        fn = f\"{base_url or self.out_root}/{field}/refs.{record}.parq\"\n        self.fs.mkdirs(f\"{base_url or self.out_root}/{field}\", exist_ok=True)\n        df.to_parquet(\n            fn,\n            engine=\"fastparquet\",\n            storage_options=storage_options\n            or getattr(self.fs, \"storage_options\", None),\n            compression=\"zstd\",\n            index=False,\n            stats=False,\n            object_encoding=object_encoding,\n            has_nulls=has_nulls,\n            # **kwargs,\n        )\n        partition.clear()\n        self._items.pop((field, record))\n\n    def flush(self, base_url=None, storage_options=None):\n        \"\"\"Output any modified or deleted keys\n\n        Parameters\n        ----------\n        base_url: str\n            Location of the output\n        \"\"\"\n        # write what we have so far and clear sub chunks\n        for thing in list(self._items):\n            if isinstance(thing, tuple):\n                field, record = thing\n                self.write(\n                    field,\n                    record,\n                    base_url=base_url,\n                    storage_options=storage_options,\n                )\n\n        # gather .zmetadata from self._items and write that too\n        for k in list(self._items):\n            if k != \".zmetadata\" and \".z\" in k:\n                self.zmetadata[k] = json.loads(self._items.pop(k))\n        met = {\"metadata\": self.zmetadata, \"record_size\": self.record_size}\n        self._items[\".zmetadata\"] = json.dumps(met).encode()\n        self.fs.pipe(\n            \"/\".join([base_url or self.out_root, \".zmetadata\"]),\n            self._items[\".zmetadata\"],\n        )\n\n        # TODO: only clear those that we wrote to?\n        self.open_refs.cache_clear()\n\n    def __len__(self):\n        # Caveat: This counts expected references, not actual - but is fast\n        count = 0\n        for field in self.listdir():\n            if field.startswith(\".\"):\n                count += 1\n            else:\n                count += math.prod(self._get_chunk_sizes(field))\n        count += len(self.zmetadata)  # all metadata keys\n        # any other files not in reference partitions\n        count += sum(1 for _ in self._items if not isinstance(_, tuple))\n        return count\n\n    def __iter__(self):\n        # Caveat: returns only existing keys, so the number of these does not\n        #  match len(self)\n        metas = set(self.zmetadata)\n        metas.update(self._items)\n        for bit in metas:\n            if isinstance(bit, str):\n                yield bit\n        for field in self.listdir():\n            for k in self._keys_in_field(field):\n                if k in self:\n                    yield k\n\n    def __contains__(self, item):\n        try:\n            self._load_one_key(item)\n            return True\n        except KeyError:\n            return False\n\n    def _keys_in_field(self, field):\n        \"\"\"List key names in given field\n\n        Produces strings like \"field/x.y\" appropriate from the chunking of the array\n        \"\"\"\n        chunk_sizes = self._get_chunk_sizes(field)\n        if len(chunk_sizes) == 0:\n            yield field + \"/0\"\n            return\n        inds = itertools.product(*(range(i) for i in chunk_sizes))\n        for ind in inds:\n            yield field + \"/\" + \".\".join([str(c) for c in ind])\n\n\nclass ReferenceFileSystem(AsyncFileSystem):\n    \"\"\"View byte ranges of some other file as a file system\n    Initial version: single file system target, which must support\n    async, and must allow start and end args in _cat_file. Later versions\n    may allow multiple arbitrary URLs for the targets.\n    This FileSystem is read-only. It is designed to be used with async\n    targets (for now). This FileSystem only allows whole-file access, no\n    ``open``. We do not get original file details from the target FS.\n    Configuration is by passing a dict of references at init, or a URL to\n    a JSON file containing the same; this dict\n    can also contain concrete data for some set of paths.\n    Reference dict format:\n    {path0: bytes_data, path1: (target_url, offset, size)}\n    https://github.com/fsspec/kerchunk/blob/main/README.md\n    \"\"\"\n\n    protocol = \"reference\"\n\n    def __init__(\n        self,\n        fo,\n        target=None,\n        ref_storage_args=None,\n        target_protocol=None,\n        target_options=None,\n        remote_protocol=None,\n        remote_options=None,\n        fs=None,\n        template_overrides=None,\n        simple_templates=True,\n        max_gap=64_000,\n        max_block=256_000_000,\n        cache_size=128,\n        **kwargs,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        fo : dict or str\n            The set of references to use for this instance, with a structure as above.\n            If str referencing a JSON file, will use fsspec.open, in conjunction\n            with target_options and target_protocol to open and parse JSON at this\n            location. If a directory, then assume references are a set of parquet\n            files to be loaded lazily.\n        target : str\n            For any references having target_url as None, this is the default file\n            target to use\n        ref_storage_args : dict\n            If references is a str, use these kwargs for loading the JSON file.\n            Deprecated: use target_options instead.\n        target_protocol : str\n            Used for loading the reference file, if it is a path. If None, protocol\n            will be derived from the given path\n        target_options : dict\n            Extra FS options for loading the reference file ``fo``, if given as a path\n        remote_protocol : str\n            The protocol of the filesystem on which the references will be evaluated\n            (unless fs is provided). If not given, will be derived from the first\n            URL that has a protocol in the templates or in the references, in that\n            order.\n        remote_options : dict\n            kwargs to go with remote_protocol\n        fs : AbstractFileSystem | dict(str, (AbstractFileSystem | dict))\n            Directly provide a file system(s):\n                - a single filesystem instance\n                - a dict of protocol:filesystem, where each value is either a filesystem\n                  instance, or a dict of kwargs that can be used to create in\n                  instance for the given protocol\n\n            If this is given, remote_options and remote_protocol are ignored.\n        template_overrides : dict\n            Swap out any templates in the references file with these - useful for\n            testing.\n        simple_templates: bool\n            Whether templates can be processed with simple replace (True) or if\n            jinja  is needed (False, much slower). All reference sets produced by\n            ``kerchunk`` are simple in this sense, but the spec allows for complex.\n        max_gap, max_block: int\n            For merging multiple concurrent requests to the same remote file.\n            Neighboring byte ranges will only be merged when their\n            inter-range gap is <= ``max_gap``. Default is 64KB. Set to 0\n            to only merge when it requires no extra bytes. Pass a negative\n            number to disable merging, appropriate for local target files.\n            Neighboring byte ranges will only be merged when the size of\n            the aggregated range is <= ``max_block``. Default is 256MB.\n        cache_size : int\n            Maximum size of LRU cache, where cache_size*record_size denotes\n            the total number of references that can be loaded in memory at once.\n            Only used for lazily loaded references.\n        kwargs : passed to parent class\n        \"\"\"\n        super().__init__(**kwargs)\n        self.target = target\n        self.template_overrides = template_overrides\n        self.simple_templates = simple_templates\n        self.templates = {}\n        self.fss = {}\n        self._dircache = {}\n        self.max_gap = max_gap\n        self.max_block = max_block\n        if isinstance(fo, str):\n            dic = dict(\n                **(ref_storage_args or target_options or {}), protocol=target_protocol\n            )\n            ref_fs, fo2 = fsspec.core.url_to_fs(fo, **dic)\n            if ref_fs.isfile(fo2):\n                # text JSON\n                with fsspec.open(fo, \"rb\", **dic) as f:\n                    logger.info(\"Read reference from URL %s\", fo)\n                    text = json.load(f)\n                self._process_references(text, template_overrides)\n            else:\n                # Lazy parquet refs\n                logger.info(\"Open lazy reference dict from URL %s\", fo)\n                self.references = LazyReferenceMapper(\n                    fo2,\n                    fs=ref_fs,\n                    cache_size=cache_size,\n                )\n        else:\n            # dictionaries\n            self._process_references(fo, template_overrides)\n        if isinstance(fs, dict):\n            self.fss = {\n                k: (\n                    fsspec.filesystem(k.split(\":\", 1)[0], **opts)\n                    if isinstance(opts, dict)\n                    else opts\n                )\n                for k, opts in fs.items()\n            }\n            if None not in self.fss:\n                self.fss[None] = filesystem(\"file\")\n            return\n        if fs is not None:\n            # single remote FS\n            remote_protocol = (\n                fs.protocol[0] if isinstance(fs.protocol, tuple) else fs.protocol\n            )\n            self.fss[remote_protocol] = fs\n\n        if remote_protocol is None:\n            # get single protocol from any templates\n            for ref in self.templates.values():\n                if callable(ref):\n                    ref = ref()\n                protocol, _ = fsspec.core.split_protocol(ref)\n                if protocol and protocol not in self.fss:\n                    fs = filesystem(protocol, **(remote_options or {}))\n                    self.fss[protocol] = fs\n        if remote_protocol is None:\n            # get single protocol from references\n            # TODO: warning here, since this can be very expensive?\n            for ref in self.references.values():\n                if callable(ref):\n                    ref = ref()\n                if isinstance(ref, list) and ref[0]:\n                    protocol, _ = fsspec.core.split_protocol(ref[0])\n                    if protocol not in self.fss:\n                        fs = filesystem(protocol, **(remote_options or {}))\n                        self.fss[protocol] = fs\n                        # only use first remote URL\n                        break\n\n        if remote_protocol and remote_protocol not in self.fss:\n            fs = filesystem(remote_protocol, **(remote_options or {}))\n            self.fss[remote_protocol] = fs\n\n        self.fss[None] = fs or filesystem(\"file\")  # default one\n\n    def _cat_common(self, path, start=None, end=None):\n        path = self._strip_protocol(path)\n        logger.debug(f\"cat: {path}\")\n        try:\n            part = self.references[path]\n        except KeyError:\n            raise FileNotFoundError(path)\n        if isinstance(part, str):\n            part = part.encode()\n        if isinstance(part, bytes):\n            logger.debug(f\"Reference: {path}, type bytes\")\n            if part.startswith(b\"base64:\"):\n                part = base64.b64decode(part[7:])\n            return part, None, None\n\n        if len(part) == 1:\n            logger.debug(f\"Reference: {path}, whole file => {part}\")\n            url = part[0]\n            start1, end1 = start, end\n        else:\n            url, start0, size = part\n            logger.debug(f\"Reference: {path} => {url}, offset {start0}, size {size}\")\n            end0 = start0 + size\n\n            if start is not None:\n                if start >= 0:\n                    start1 = start0 + start\n                else:\n                    start1 = end0 + start\n            else:\n                start1 = start0\n            if end is not None:\n                if end >= 0:\n                    end1 = start0 + end\n                else:\n                    end1 = end0 + end\n            else:\n                end1 = end0\n        if url is None:\n            url = self.target\n        return url, start1, end1\n\n    async def _cat_file(self, path, start=None, end=None, **kwargs):\n        part_or_url, start0, end0 = self._cat_common(path, start=start, end=end)\n        if isinstance(part_or_url, bytes):\n            return part_or_url[start:end]\n        protocol, _ = split_protocol(part_or_url)\n        try:\n            await self.fss[protocol]._cat_file(part_or_url, start=start, end=end)\n        except Exception as e:\n            raise ReferenceNotReachable(path, part_or_url) from e\n\n    def cat_file(self, path, start=None, end=None, **kwargs):\n        part_or_url, start0, end0 = self._cat_common(path, start=start, end=end)\n        if isinstance(part_or_url, bytes):\n            return part_or_url[start:end]\n        protocol, _ = split_protocol(part_or_url)\n        try:\n            return self.fss[protocol].cat_file(part_or_url, start=start0, end=end0)\n        except Exception as e:\n            raise ReferenceNotReachable(path, part_or_url) from e\n\n    def pipe_file(self, path, value, **_):\n        \"\"\"Temporarily add binary data or reference as a file\"\"\"\n        self.references[path] = value\n\n    async def _get_file(self, rpath, lpath, **kwargs):\n        if self.isdir(rpath):\n            return os.makedirs(lpath, exist_ok=True)\n        data = await self._cat_file(rpath)\n        with open(lpath, \"wb\") as f:\n            f.write(data)\n\n    def get_file(self, rpath, lpath, callback=DEFAULT_CALLBACK, **kwargs):\n        if self.isdir(rpath):\n            return os.makedirs(lpath, exist_ok=True)\n        data = self.cat_file(rpath, **kwargs)\n        callback.set_size(len(data))\n        if isfilelike(lpath):\n            lpath.write(data)\n        else:\n            with open(lpath, \"wb\") as f:\n                f.write(data)\n        callback.absolute_update(len(data))\n\n    def get(self, rpath, lpath, recursive=False, **kwargs):\n        if recursive:\n            # trigger directory build\n            self.ls(\"\")\n        rpath = self.expand_path(rpath, recursive=recursive)\n        fs = fsspec.filesystem(\"file\", auto_mkdir=True)\n        targets = other_paths(rpath, lpath)\n        if recursive:\n            data = self.cat([r for r in rpath if not self.isdir(r)])\n        else:\n            data = self.cat(rpath)\n        for remote, local in zip(rpath, targets):\n            if remote in data:\n                fs.pipe_file(local, data[remote])\n\n    def cat(self, path, recursive=False, on_error=\"raise\", **kwargs):\n        if isinstance(path, str) and recursive:\n            raise NotImplementedError\n        if isinstance(path, list) and (recursive or any(\"*\" in p for p in path)):\n            raise NotImplementedError\n        # TODO: if references is lazy, pre-fetch all paths in batch before access\n        proto_dict = _protocol_groups(path, self.references)\n        out = {}\n        for proto, paths in proto_dict.items():\n            fs = self.fss[proto]\n            urls, starts, ends, valid_paths = [], [], [], []\n            for p in paths:\n                # find references or label not-found. Early exit if any not\n                # found and on_error is \"raise\"\n                try:\n                    u, s, e = self._cat_common(p)\n                except FileNotFoundError as err:\n                    if on_error == \"raise\":\n                        raise\n                    if on_error != \"omit\":\n                        out[p] = err\n                else:\n                    urls.append(u)\n                    starts.append(s)\n                    ends.append(e)\n                    valid_paths.append(p)\n\n            # process references into form for merging\n            urls2 = []\n            starts2 = []\n            ends2 = []\n            paths2 = []\n            whole_files = set()\n            for u, s, e, p in zip(urls, starts, ends, valid_paths):\n                if isinstance(u, bytes):\n                    # data\n                    out[p] = u\n                elif s is None:\n                    # whole file - limits are None, None, but no further\n                    # entries take for this file\n                    whole_files.add(u)\n                    urls2.append(u)\n                    starts2.append(s)\n                    ends2.append(e)\n                    paths2.append(p)\n            for u, s, e, p in zip(urls, starts, ends, valid_paths):\n                # second run to account for files that are to be loaded whole\n                if s is not None and u not in whole_files:\n                    urls2.append(u)\n                    starts2.append(s)\n                    ends2.append(e)\n                    paths2.append(p)\n\n            # merge and fetch consolidated ranges\n            new_paths, new_starts, new_ends = merge_offset_ranges(\n                list(urls2),\n                list(starts2),\n                list(ends2),\n                sort=True,\n                max_gap=self.max_gap,\n                max_block=self.max_block,\n            )\n            bytes_out = fs.cat_ranges(new_paths, new_starts, new_ends)\n\n            # unbundle from merged bytes - simple approach\n            for u, s, e, p in zip(urls, starts, ends, valid_paths):\n                if p in out:\n                    continue  # was bytes, already handled\n                for np, ns, ne, b in zip(new_paths, new_starts, new_ends, bytes_out):\n                    if np == u and (ns is None or ne is None):\n                        if isinstance(b, Exception):\n                            out[p] = b\n                        else:\n                            out[p] = b[s:e]\n                    elif np == u and s >= ns and e <= ne:\n                        if isinstance(b, Exception):\n                            out[p] = b\n                        else:\n                            out[p] = b[s - ns : (e - ne) or None]\n\n        for k, v in out.copy().items():\n            # these were valid references, but fetch failed, so transform exc\n            if isinstance(v, Exception) and k in self.references:\n                ex = out[k]\n                new_ex = ReferenceNotReachable(k, self.references[k])\n                new_ex.__cause__ = ex\n                if on_error == \"raise\":\n                    raise new_ex\n                elif on_error != \"omit\":\n                    out[k] = new_ex\n\n        if len(out) == 1 and isinstance(path, str) and \"*\" not in path:\n            return _first(out)\n        return out\n\n    def _process_references(self, references, template_overrides=None):\n        vers = references.get(\"version\", None)\n        if vers is None:\n            self._process_references0(references)\n        elif vers == 1:\n            self._process_references1(references, template_overrides=template_overrides)\n        else:\n            raise ValueError(f\"Unknown reference spec version: {vers}\")\n        # TODO: we make dircache by iterating over all entries, but for Spec >= 1,\n        #  can replace with programmatic. Is it even needed for mapper interface?\n\n    def _process_references0(self, references):\n        \"\"\"Make reference dict for Spec Version 0\"\"\"\n        if isinstance(references, dict):\n            # do not do this for lazy/parquet backend, which will not make dicts,\n            # but must remain writable in the original object\n            references = {\n                key: json.dumps(val) if isinstance(val, dict) else val\n                for key, val in references.items()\n            }\n        self.references = references\n\n    def _process_references1(self, references, template_overrides=None):\n        if not self.simple_templates or self.templates:\n            import jinja2\n        self.references = {}\n        self._process_templates(references.get(\"templates\", {}))\n\n        @lru_cache(1000)\n        def _render_jinja(u):\n            return jinja2.Template(u).render(**self.templates)\n\n        for k, v in references.get(\"refs\", {}).items():\n            if isinstance(v, str):\n                if v.startswith(\"base64:\"):\n                    self.references[k] = base64.b64decode(v[7:])\n                self.references[k] = v\n            elif isinstance(v, dict):\n                self.references[k] = json.dumps(v)\n            elif self.templates:\n                u = v[0]\n                if \"{{\" in u:\n                    if self.simple_templates:\n                        u = (\n                            u.replace(\"{{\", \"{\")\n                            .replace(\"}}\", \"}\")\n                            .format(**self.templates)\n                        )\n                    else:\n                        u = _render_jinja(u)\n                self.references[k] = [u] if len(v) == 1 else [u, v[1], v[2]]\n            else:\n                self.references[k] = v\n        self.references.update(self._process_gen(references.get(\"gen\", [])))\n\n    def _process_templates(self, tmp):\n        self.templates = {}\n        if self.template_overrides is not None:\n            tmp.update(self.template_overrides)\n        for k, v in tmp.items():\n            if \"{{\" in v:\n                import jinja2\n\n                self.templates[k] = lambda temp=v, **kwargs: jinja2.Template(\n                    temp\n                ).render(**kwargs)\n            else:\n                self.templates[k] = v\n\n    def _process_gen(self, gens):\n        out = {}\n        for gen in gens:\n            dimension = {\n                k: v\n                if isinstance(v, list)\n                else range(v.get(\"start\", 0), v[\"stop\"], v.get(\"step\", 1))\n                for k, v in gen[\"dimensions\"].items()\n            }\n            products = (\n                dict(zip(dimension.keys(), values))\n                for values in itertools.product(*dimension.values())\n            )\n            for pr in products:\n                import jinja2\n\n                key = jinja2.Template(gen[\"key\"]).render(**pr, **self.templates)\n                url = jinja2.Template(gen[\"url\"]).render(**pr, **self.templates)\n                if (\"offset\" in gen) and (\"length\" in gen):\n                    offset = int(\n                        jinja2.Template(gen[\"offset\"]).render(**pr, **self.templates)\n                    )\n                    length = int(\n                        jinja2.Template(gen[\"length\"]).render(**pr, **self.templates)\n                    )\n                    out[key] = [url, offset, length]\n                elif (\"offset\" in gen) ^ (\"length\" in gen):\n                    raise ValueError(\n                        \"Both 'offset' and 'length' are required for a \"\n                        \"reference generator entry if either is provided.\"\n                    )\n                else:\n                    out[key] = [url]\n        return out\n\n    def _dircache_from_items(self):\n        self.dircache = {\"\": []}\n        it = self.references.items()\n        for path, part in it:\n            if isinstance(part, (bytes, str)):\n                size = len(part)\n            elif len(part) == 1:\n                size = None\n            else:\n                _, _, size = part\n            par = path.rsplit(\"/\", 1)[0] if \"/\" in path else \"\"\n            par0 = par\n            subdirs = [par0]\n            while par0 and par0 not in self.dircache:\n                # collect parent directories\n                par0 = self._parent(par0)\n                subdirs.append(par0)\n\n            subdirs.reverse()\n            for parent, child in zip(subdirs, subdirs[1:]):\n                # register newly discovered directories\n                assert child not in self.dircache\n                assert parent in self.dircache\n                self.dircache[parent].append(\n                    {\"name\": child, \"type\": \"directory\", \"size\": 0}\n                )\n                self.dircache[child] = []\n\n            self.dircache[par].append({\"name\": path, \"type\": \"file\", \"size\": size})\n\n    def _open(self, path, mode=\"rb\", block_size=None, cache_options=None, **kwargs):\n        data = self.cat_file(path)  # load whole chunk into memory\n        return io.BytesIO(data)\n\n    def ls(self, path, detail=True, **kwargs):\n        path = self._strip_protocol(path)\n        if isinstance(self.references, LazyReferenceMapper):\n            try:\n                return self.references.ls(path, detail)\n            except KeyError:\n                pass\n            raise FileNotFoundError(f\"'{path}' is not a known key\")\n        if not self.dircache:\n            self._dircache_from_items()\n        out = self._ls_from_cache(path)\n        if out is None:\n            raise FileNotFoundError(path)\n        if detail:\n            return out\n        return [o[\"name\"] for o in out]\n\n    def exists(self, path, **kwargs):  # overwrite auto-sync version\n        return self.isdir(path) or self.isfile(path)\n\n    def isdir(self, path):  # overwrite auto-sync version\n        if self.dircache:\n            return path in self.dircache\n        elif isinstance(self.references, LazyReferenceMapper):\n            return path in self.references.listdir(\"\")\n        else:\n            # this may be faster than building dircache for single calls, but\n            # by looping will be slow for many calls; could cache it?\n            return any(_.startswith(f\"{path}/\") for _ in self.references)\n\n    def isfile(self, path):  # overwrite auto-sync version\n        return path in self.references\n\n    async def _ls(self, path, detail=True, **kwargs):  # calls fast sync code\n        return self.ls(path, detail, **kwargs)\n\n    def find(self, path, maxdepth=None, withdirs=False, detail=False, **kwargs):\n        if withdirs:\n            return super().find(\n                path, maxdepth=maxdepth, withdirs=withdirs, detail=detail, **kwargs\n            )\n        if path:\n            path = self._strip_protocol(path)\n            r = sorted(k for k in self.references if k.startswith(path))\n        else:\n            r = sorted(self.references)\n        if detail:\n            if not self.dircache:\n                self._dircache_from_items()\n            return {k: self._ls_from_cache(k)[0] for k in r}\n        else:\n            return r\n\n    def info(self, path, **kwargs):\n        out = self.references.get(path)\n        if out is not None:\n            if isinstance(out, (str, bytes)):\n                # decode base64 here\n                return {\"name\": path, \"type\": \"file\", \"size\": len(out)}\n            elif len(out) > 1:\n                return {\"name\": path, \"type\": \"file\", \"size\": out[2]}\n            else:\n                out0 = [{\"name\": path, \"type\": \"file\", \"size\": None}]\n        else:\n            out = self.ls(path, True)\n            out0 = [o for o in out if o[\"name\"] == path]\n            if not out0:\n                return {\"name\": path, \"type\": \"directory\", \"size\": 0}\n        if out0[0][\"size\"] is None:\n            # if this is a whole remote file, update size using remote FS\n            prot, _ = split_protocol(self.references[path][0])\n            out0[0][\"size\"] = self.fss[prot].size(self.references[path][0])\n        return out0[0]\n\n    async def _info(self, path, **kwargs):  # calls fast sync code\n        return self.info(path)\n\n    async def _rm_file(self, path, **kwargs):\n        self.references.pop(\n            path, None\n        )  # ignores FileNotFound, just as well for directories\n        self.dircache.clear()  # this is a bit heavy handed\n\n    async def _pipe_file(self, path, data):\n        # can be str or bytes\n        self.references[path] = data\n        self.dircache.clear()  # this is a bit heavy handed\n\n    async def _put_file(self, lpath, rpath, **kwargs):\n        # puts binary\n        with open(lpath, \"rb\") as f:\n            self.references[rpath] = f.read()\n        self.dircache.clear()  # this is a bit heavy handed\n\n    def save_json(self, url, **storage_options):\n        \"\"\"Write modified references into new location\"\"\"\n        out = {}\n        for k, v in self.references.items():\n            if isinstance(v, bytes):\n                try:\n                    out[k] = v.decode(\"ascii\")\n                except UnicodeDecodeError:\n                    out[k] = (b\"base64:\" + base64.b64encode(v)).decode()\n            else:\n                out[k] = v\n        with fsspec.open(url, \"wb\", **storage_options) as f:\n            f.write(json.dumps({\"version\": 1, \"refs\": out}).encode())\n", "fsspec/implementations/git.py": "import os\n\nimport pygit2\n\nfrom fsspec.spec import AbstractFileSystem\n\nfrom .memory import MemoryFile\n\n\nclass GitFileSystem(AbstractFileSystem):\n    \"\"\"Browse the files of a local git repo at any hash/tag/branch\n\n    (experimental backend)\n    \"\"\"\n\n    root_marker = \"\"\n    cachable = True\n\n    def __init__(self, path=None, fo=None, ref=None, **kwargs):\n        \"\"\"\n\n        Parameters\n        ----------\n        path: str (optional)\n            Local location of the repo (uses current directory if not given).\n            May be deprecated in favour of ``fo``. When used with a higher\n            level function such as fsspec.open(), may be of the form\n            \"git://[path-to-repo[:]][ref@]path/to/file\" (but the actual\n            file path should not contain \"@\" or \":\").\n        fo: str (optional)\n            Same as ``path``, but passed as part of a chained URL. This one\n            takes precedence if both are given.\n        ref: str (optional)\n            Reference to work with, could be a hash, tag or branch name. Defaults\n            to current working tree. Note that ``ls`` and ``open`` also take hash,\n            so this becomes the default for those operations\n        kwargs\n        \"\"\"\n        super().__init__(**kwargs)\n        self.repo = pygit2.Repository(fo or path or os.getcwd())\n        self.ref = ref or \"master\"\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        path = super()._strip_protocol(path).lstrip(\"/\")\n        if \":\" in path:\n            path = path.split(\":\", 1)[1]\n        if \"@\" in path:\n            path = path.split(\"@\", 1)[1]\n        return path.lstrip(\"/\")\n\n    def _path_to_object(self, path, ref):\n        comm, ref = self.repo.resolve_refish(ref or self.ref)\n        parts = path.split(\"/\")\n        tree = comm.tree\n        for part in parts:\n            if part and isinstance(tree, pygit2.Tree):\n                tree = tree[part]\n        return tree\n\n    @staticmethod\n    def _get_kwargs_from_urls(path):\n        if path.startswith(\"git://\"):\n            path = path[6:]\n        out = {}\n        if \":\" in path:\n            out[\"path\"], path = path.split(\":\", 1)\n        if \"@\" in path:\n            out[\"ref\"], path = path.split(\"@\", 1)\n        return out\n\n    def ls(self, path, detail=True, ref=None, **kwargs):\n        path = self._strip_protocol(path)\n        tree = self._path_to_object(path, ref)\n        if isinstance(tree, pygit2.Tree):\n            out = []\n            for obj in tree:\n                if isinstance(obj, pygit2.Tree):\n                    out.append(\n                        {\n                            \"type\": \"directory\",\n                            \"name\": \"/\".join([path, obj.name]).lstrip(\"/\"),\n                            \"hex\": obj.hex,\n                            \"mode\": f\"{obj.filemode:o}\",\n                            \"size\": 0,\n                        }\n                    )\n                else:\n                    out.append(\n                        {\n                            \"type\": \"file\",\n                            \"name\": \"/\".join([path, obj.name]).lstrip(\"/\"),\n                            \"hex\": obj.hex,\n                            \"mode\": f\"{obj.filemode:o}\",\n                            \"size\": obj.size,\n                        }\n                    )\n        else:\n            obj = tree\n            out = [\n                {\n                    \"type\": \"file\",\n                    \"name\": obj.name,\n                    \"hex\": obj.hex,\n                    \"mode\": f\"{obj.filemode:o}\",\n                    \"size\": obj.size,\n                }\n            ]\n        if detail:\n            return out\n        return [o[\"name\"] for o in out]\n\n    def ukey(self, path, ref=None):\n        return self.info(path, ref=ref)[\"hex\"]\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        ref=None,\n        **kwargs,\n    ):\n        obj = self._path_to_object(path, ref or self.ref)\n        return MemoryFile(data=obj.data)\n", "fsspec/implementations/github.py": "import requests\n\nimport fsspec\n\nfrom ..spec import AbstractFileSystem\nfrom ..utils import infer_storage_options\nfrom .memory import MemoryFile\n\n# TODO: add GIST backend, would be very similar\n\n\nclass GithubFileSystem(AbstractFileSystem):\n    \"\"\"Interface to files in github\n\n    An instance of this class provides the files residing within a remote github\n    repository. You may specify a point in the repos history, by SHA, branch\n    or tag (default is current master).\n\n    Given that code files tend to be small, and that github does not support\n    retrieving partial content, we always fetch whole files.\n\n    When using fsspec.open, allows URIs of the form:\n\n    - \"github://path/file\", in which case you must specify org, repo and\n      may specify sha in the extra args\n    - 'github://org:repo@/precip/catalog.yml', where the org and repo are\n      part of the URI\n    - 'github://org:repo@sha/precip/catalog.yml', where the sha is also included\n\n    ``sha`` can be the full or abbreviated hex of the commit you want to fetch\n    from, or a branch or tag name (so long as it doesn't contain special characters\n    like \"/\", \"?\", which would have to be HTTP-encoded).\n\n    For authorised access, you must provide username and token, which can be made\n    at https://github.com/settings/tokens\n    \"\"\"\n\n    url = \"https://api.github.com/repos/{org}/{repo}/git/trees/{sha}\"\n    rurl = \"https://raw.githubusercontent.com/{org}/{repo}/{sha}/{path}\"\n    protocol = \"github\"\n    timeout = (60, 60)  # connect, read timeouts\n\n    def __init__(\n        self, org, repo, sha=None, username=None, token=None, timeout=None, **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.org = org\n        self.repo = repo\n        if (username is None) ^ (token is None):\n            raise ValueError(\"Auth required both username and token\")\n        self.username = username\n        self.token = token\n        if timeout is not None:\n            self.timeout = timeout\n        if sha is None:\n            # look up default branch (not necessarily \"master\")\n            u = \"https://api.github.com/repos/{org}/{repo}\"\n            r = requests.get(\n                u.format(org=org, repo=repo), timeout=self.timeout, **self.kw\n            )\n            r.raise_for_status()\n            sha = r.json()[\"default_branch\"]\n\n        self.root = sha\n        self.ls(\"\")\n\n    @property\n    def kw(self):\n        if self.username:\n            return {\"auth\": (self.username, self.token)}\n        return {}\n\n    @classmethod\n    def repos(cls, org_or_user, is_org=True):\n        \"\"\"List repo names for given org or user\n\n        This may become the top level of the FS\n\n        Parameters\n        ----------\n        org_or_user: str\n            Name of the github org or user to query\n        is_org: bool (default True)\n            Whether the name is an organisation (True) or user (False)\n\n        Returns\n        -------\n        List of string\n        \"\"\"\n        r = requests.get(\n            f\"https://api.github.com/{['users', 'orgs'][is_org]}/{org_or_user}/repos\",\n            timeout=cls.timeout,\n        )\n        r.raise_for_status()\n        return [repo[\"name\"] for repo in r.json()]\n\n    @property\n    def tags(self):\n        \"\"\"Names of tags in the repo\"\"\"\n        r = requests.get(\n            f\"https://api.github.com/repos/{self.org}/{self.repo}/tags\",\n            timeout=self.timeout,\n            **self.kw,\n        )\n        r.raise_for_status()\n        return [t[\"name\"] for t in r.json()]\n\n    @property\n    def branches(self):\n        \"\"\"Names of branches in the repo\"\"\"\n        r = requests.get(\n            f\"https://api.github.com/repos/{self.org}/{self.repo}/branches\",\n            timeout=self.timeout,\n            **self.kw,\n        )\n        r.raise_for_status()\n        return [t[\"name\"] for t in r.json()]\n\n    @property\n    def refs(self):\n        \"\"\"Named references, tags and branches\"\"\"\n        return {\"tags\": self.tags, \"branches\": self.branches}\n\n    def ls(self, path, detail=False, sha=None, _sha=None, **kwargs):\n        \"\"\"List files at given path\n\n        Parameters\n        ----------\n        path: str\n            Location to list, relative to repo root\n        detail: bool\n            If True, returns list of dicts, one per file; if False, returns\n            list of full filenames only\n        sha: str (optional)\n            List at the given point in the repo history, branch or tag name or commit\n            SHA\n        _sha: str (optional)\n            List this specific tree object (used internally to descend into trees)\n        \"\"\"\n        path = self._strip_protocol(path)\n        if path == \"\":\n            _sha = sha or self.root\n        if _sha is None:\n            parts = path.rstrip(\"/\").split(\"/\")\n            so_far = \"\"\n            _sha = sha or self.root\n            for part in parts:\n                out = self.ls(so_far, True, sha=sha, _sha=_sha)\n                so_far += \"/\" + part if so_far else part\n                out = [o for o in out if o[\"name\"] == so_far]\n                if not out:\n                    raise FileNotFoundError(path)\n                out = out[0]\n                if out[\"type\"] == \"file\":\n                    if detail:\n                        return [out]\n                    else:\n                        return path\n                _sha = out[\"sha\"]\n        if path not in self.dircache or sha not in [self.root, None]:\n            r = requests.get(\n                self.url.format(org=self.org, repo=self.repo, sha=_sha),\n                timeout=self.timeout,\n                **self.kw,\n            )\n            if r.status_code == 404:\n                raise FileNotFoundError(path)\n            r.raise_for_status()\n            types = {\"blob\": \"file\", \"tree\": \"directory\"}\n            out = [\n                {\n                    \"name\": path + \"/\" + f[\"path\"] if path else f[\"path\"],\n                    \"mode\": f[\"mode\"],\n                    \"type\": types[f[\"type\"]],\n                    \"size\": f.get(\"size\", 0),\n                    \"sha\": f[\"sha\"],\n                }\n                for f in r.json()[\"tree\"]\n                if f[\"type\"] in types\n            ]\n            if sha in [self.root, None]:\n                self.dircache[path] = out\n        else:\n            out = self.dircache[path]\n        if detail:\n            return out\n        else:\n            return sorted([f[\"name\"] for f in out])\n\n    def invalidate_cache(self, path=None):\n        self.dircache.clear()\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        opts = infer_storage_options(path)\n        if \"username\" not in opts:\n            return super()._strip_protocol(path)\n        return opts[\"path\"].lstrip(\"/\")\n\n    @staticmethod\n    def _get_kwargs_from_urls(path):\n        opts = infer_storage_options(path)\n        if \"username\" not in opts:\n            return {}\n        out = {\"org\": opts[\"username\"], \"repo\": opts[\"password\"]}\n        if opts[\"host\"]:\n            out[\"sha\"] = opts[\"host\"]\n        return out\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        sha=None,\n        **kwargs,\n    ):\n        if mode != \"rb\":\n            raise NotImplementedError\n        url = self.rurl.format(\n            org=self.org, repo=self.repo, path=path, sha=sha or self.root\n        )\n        r = requests.get(url, timeout=self.timeout, **self.kw)\n        if r.status_code == 404:\n            raise FileNotFoundError(path)\n        r.raise_for_status()\n        return MemoryFile(None, None, r.content)\n\n    def cat(self, path, recursive=False, on_error=\"raise\", **kwargs):\n        paths = self.expand_path(path, recursive=recursive)\n        urls = [\n            self.rurl.format(org=self.org, repo=self.repo, path=u, sha=self.root)\n            for u, sh in paths\n        ]\n        fs = fsspec.filesystem(\"http\")\n        data = fs.cat(urls, on_error=\"return\")\n        return {u: v for ((k, v), u) in zip(data.items(), urls)}\n", "fsspec/implementations/memory.py": "from __future__ import annotations\n\nimport logging\nfrom datetime import datetime, timezone\nfrom errno import ENOTEMPTY\nfrom io import BytesIO\nfrom pathlib import PurePath, PureWindowsPath\nfrom typing import Any, ClassVar\n\nfrom fsspec import AbstractFileSystem\nfrom fsspec.implementations.local import LocalFileSystem\nfrom fsspec.utils import stringify_path\n\nlogger = logging.getLogger(\"fsspec.memoryfs\")\n\n\nclass MemoryFileSystem(AbstractFileSystem):\n    \"\"\"A filesystem based on a dict of BytesIO objects\n\n    This is a global filesystem so instances of this class all point to the same\n    in memory filesystem.\n    \"\"\"\n\n    store: ClassVar[dict[str, Any]] = {}  # global, do not overwrite!\n    pseudo_dirs = [\"\"]  # global, do not overwrite!\n    protocol = \"memory\"\n    root_marker = \"/\"\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        if isinstance(path, PurePath):\n            if isinstance(path, PureWindowsPath):\n                return LocalFileSystem._strip_protocol(path)\n            else:\n                path = stringify_path(path)\n\n        if path.startswith(\"memory://\"):\n            path = path[len(\"memory://\") :]\n        if \"::\" in path or \"://\" in path:\n            return path.rstrip(\"/\")\n        path = path.lstrip(\"/\").rstrip(\"/\")\n        return \"/\" + path if path else \"\"\n\n    def ls(self, path, detail=True, **kwargs):\n        path = self._strip_protocol(path)\n        if path in self.store:\n            # there is a key with this exact name\n            if not detail:\n                return [path]\n            return [\n                {\n                    \"name\": path,\n                    \"size\": self.store[path].size,\n                    \"type\": \"file\",\n                    \"created\": self.store[path].created.timestamp(),\n                }\n            ]\n        paths = set()\n        starter = path + \"/\"\n        out = []\n        for p2 in tuple(self.store):\n            if p2.startswith(starter):\n                if \"/\" not in p2[len(starter) :]:\n                    # exact child\n                    out.append(\n                        {\n                            \"name\": p2,\n                            \"size\": self.store[p2].size,\n                            \"type\": \"file\",\n                            \"created\": self.store[p2].created.timestamp(),\n                        }\n                    )\n                elif len(p2) > len(starter):\n                    # implied child directory\n                    ppath = starter + p2[len(starter) :].split(\"/\", 1)[0]\n                    if ppath not in paths:\n                        out = out or []\n                        out.append(\n                            {\n                                \"name\": ppath,\n                                \"size\": 0,\n                                \"type\": \"directory\",\n                            }\n                        )\n                        paths.add(ppath)\n        for p2 in self.pseudo_dirs:\n            if p2.startswith(starter):\n                if \"/\" not in p2[len(starter) :]:\n                    # exact child pdir\n                    if p2 not in paths:\n                        out.append({\"name\": p2, \"size\": 0, \"type\": \"directory\"})\n                        paths.add(p2)\n                else:\n                    # directory implied by deeper pdir\n                    ppath = starter + p2[len(starter) :].split(\"/\", 1)[0]\n                    if ppath not in paths:\n                        out.append({\"name\": ppath, \"size\": 0, \"type\": \"directory\"})\n                        paths.add(ppath)\n        if not out:\n            if path in self.pseudo_dirs:\n                # empty dir\n                return []\n            raise FileNotFoundError(path)\n        if detail:\n            return out\n        return sorted([f[\"name\"] for f in out])\n\n    def mkdir(self, path, create_parents=True, **kwargs):\n        path = self._strip_protocol(path)\n        if path in self.store or path in self.pseudo_dirs:\n            raise FileExistsError(path)\n        if self._parent(path).strip(\"/\") and self.isfile(self._parent(path)):\n            raise NotADirectoryError(self._parent(path))\n        if create_parents and self._parent(path).strip(\"/\"):\n            try:\n                self.mkdir(self._parent(path), create_parents, **kwargs)\n            except FileExistsError:\n                pass\n        if path and path not in self.pseudo_dirs:\n            self.pseudo_dirs.append(path)\n\n    def makedirs(self, path, exist_ok=False):\n        try:\n            self.mkdir(path, create_parents=True)\n        except FileExistsError:\n            if not exist_ok:\n                raise\n\n    def pipe_file(self, path, value, **kwargs):\n        \"\"\"Set the bytes of given file\n\n        Avoids copies of the data if possible\n        \"\"\"\n        self.open(path, \"wb\", data=value)\n\n    def rmdir(self, path):\n        path = self._strip_protocol(path)\n        if path == \"\":\n            # silently avoid deleting FS root\n            return\n        if path in self.pseudo_dirs:\n            if not self.ls(path):\n                self.pseudo_dirs.remove(path)\n            else:\n                raise OSError(ENOTEMPTY, \"Directory not empty\", path)\n        else:\n            raise FileNotFoundError(path)\n\n    def info(self, path, **kwargs):\n        logger.debug(\"info: %s\", path)\n        path = self._strip_protocol(path)\n        if path in self.pseudo_dirs or any(\n            p.startswith(path + \"/\") for p in list(self.store) + self.pseudo_dirs\n        ):\n            return {\n                \"name\": path,\n                \"size\": 0,\n                \"type\": \"directory\",\n            }\n        elif path in self.store:\n            filelike = self.store[path]\n            return {\n                \"name\": path,\n                \"size\": filelike.size,\n                \"type\": \"file\",\n                \"created\": getattr(filelike, \"created\", None),\n            }\n        else:\n            raise FileNotFoundError(path)\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        **kwargs,\n    ):\n        path = self._strip_protocol(path)\n        if path in self.pseudo_dirs:\n            raise IsADirectoryError(path)\n        parent = path\n        while len(parent) > 1:\n            parent = self._parent(parent)\n            if self.isfile(parent):\n                raise FileExistsError(parent)\n        if mode in [\"rb\", \"ab\", \"r+b\"]:\n            if path in self.store:\n                f = self.store[path]\n                if mode == \"ab\":\n                    # position at the end of file\n                    f.seek(0, 2)\n                else:\n                    # position at the beginning of file\n                    f.seek(0)\n                return f\n            else:\n                raise FileNotFoundError(path)\n        elif mode == \"wb\":\n            m = MemoryFile(self, path, kwargs.get(\"data\"))\n            if not self._intrans:\n                m.commit()\n            return m\n        else:\n            name = self.__class__.__name__\n            raise ValueError(f\"unsupported file mode for {name}: {mode!r}\")\n\n    def cp_file(self, path1, path2, **kwargs):\n        path1 = self._strip_protocol(path1)\n        path2 = self._strip_protocol(path2)\n        if self.isfile(path1):\n            self.store[path2] = MemoryFile(\n                self, path2, self.store[path1].getvalue()\n            )  # implicit copy\n        elif self.isdir(path1):\n            if path2 not in self.pseudo_dirs:\n                self.pseudo_dirs.append(path2)\n        else:\n            raise FileNotFoundError(path1)\n\n    def cat_file(self, path, start=None, end=None, **kwargs):\n        logger.debug(\"cat: %s\", path)\n        path = self._strip_protocol(path)\n        try:\n            return bytes(self.store[path].getbuffer()[start:end])\n        except KeyError:\n            raise FileNotFoundError(path)\n\n    def _rm(self, path):\n        path = self._strip_protocol(path)\n        try:\n            del self.store[path]\n        except KeyError as e:\n            raise FileNotFoundError(path) from e\n\n    def modified(self, path):\n        path = self._strip_protocol(path)\n        try:\n            return self.store[path].modified\n        except KeyError:\n            raise FileNotFoundError(path)\n\n    def created(self, path):\n        path = self._strip_protocol(path)\n        try:\n            return self.store[path].created\n        except KeyError:\n            raise FileNotFoundError(path)\n\n    def rm(self, path, recursive=False, maxdepth=None):\n        if isinstance(path, str):\n            path = self._strip_protocol(path)\n        else:\n            path = [self._strip_protocol(p) for p in path]\n        paths = self.expand_path(path, recursive=recursive, maxdepth=maxdepth)\n        for p in reversed(paths):\n            # If the expanded path doesn't exist, it is only because the expanded\n            # path was a directory that does not exist in self.pseudo_dirs. This\n            # is possible if you directly create files without making the\n            # directories first.\n            if not self.exists(p):\n                continue\n            if self.isfile(p):\n                self.rm_file(p)\n            else:\n                self.rmdir(p)\n\n\nclass MemoryFile(BytesIO):\n    \"\"\"A BytesIO which can't close and works as a context manager\n\n    Can initialise with data. Each path should only be active once at any moment.\n\n    No need to provide fs, path if auto-committing (default)\n    \"\"\"\n\n    def __init__(self, fs=None, path=None, data=None):\n        logger.debug(\"open file %s\", path)\n        self.fs = fs\n        self.path = path\n        self.created = datetime.now(tz=timezone.utc)\n        self.modified = datetime.now(tz=timezone.utc)\n        if data:\n            super().__init__(data)\n            self.seek(0)\n\n    @property\n    def size(self):\n        return self.getbuffer().nbytes\n\n    def __enter__(self):\n        return self\n\n    def close(self):\n        pass\n\n    def discard(self):\n        pass\n\n    def commit(self):\n        self.fs.store[self.path] = self\n        self.modified = datetime.now(tz=timezone.utc)\n", "fsspec/implementations/cache_mapper.py": "from __future__ import annotations\n\nimport abc\nimport hashlib\n\nfrom fsspec.implementations.local import make_path_posix\n\n\nclass AbstractCacheMapper(abc.ABC):\n    \"\"\"Abstract super-class for mappers from remote URLs to local cached\n    basenames.\n    \"\"\"\n\n    @abc.abstractmethod\n    def __call__(self, path: str) -> str: ...\n\n    def __eq__(self, other: object) -> bool:\n        # Identity only depends on class. When derived classes have attributes\n        # they will need to be included.\n        return isinstance(other, type(self))\n\n    def __hash__(self) -> int:\n        # Identity only depends on class. When derived classes have attributes\n        # they will need to be included.\n        return hash(type(self))\n\n\nclass BasenameCacheMapper(AbstractCacheMapper):\n    \"\"\"Cache mapper that uses the basename of the remote URL and a fixed number\n    of directory levels above this.\n\n    The default is zero directory levels, meaning different paths with the same\n    basename will have the same cached basename.\n    \"\"\"\n\n    def __init__(self, directory_levels: int = 0):\n        if directory_levels < 0:\n            raise ValueError(\n                \"BasenameCacheMapper requires zero or positive directory_levels\"\n            )\n        self.directory_levels = directory_levels\n\n        # Separator for directories when encoded as strings.\n        self._separator = \"_@_\"\n\n    def __call__(self, path: str) -> str:\n        path = make_path_posix(path)\n        prefix, *bits = path.rsplit(\"/\", self.directory_levels + 1)\n        if bits:\n            return self._separator.join(bits)\n        else:\n            return prefix  # No separator found, simple filename\n\n    def __eq__(self, other: object) -> bool:\n        return super().__eq__(other) and self.directory_levels == other.directory_levels\n\n    def __hash__(self) -> int:\n        return super().__hash__() ^ hash(self.directory_levels)\n\n\nclass HashCacheMapper(AbstractCacheMapper):\n    \"\"\"Cache mapper that uses a hash of the remote URL.\"\"\"\n\n    def __call__(self, path: str) -> str:\n        return hashlib.sha256(path.encode()).hexdigest()\n\n\ndef create_cache_mapper(same_names: bool) -> AbstractCacheMapper:\n    \"\"\"Factory method to create cache mapper for backward compatibility with\n    ``CachingFileSystem`` constructor using ``same_names`` kwarg.\n    \"\"\"\n    if same_names:\n        return BasenameCacheMapper()\n    else:\n        return HashCacheMapper()\n", "fsspec/implementations/__init__.py": "", "fsspec/implementations/zip.py": "import zipfile\n\nimport fsspec\nfrom fsspec.archive import AbstractArchiveFileSystem\n\n\nclass ZipFileSystem(AbstractArchiveFileSystem):\n    \"\"\"Read/Write contents of ZIP archive as a file-system\n\n    Keeps file object open while instance lives.\n\n    This class is pickleable, but not necessarily thread-safe\n    \"\"\"\n\n    root_marker = \"\"\n    protocol = \"zip\"\n    cachable = False\n\n    def __init__(\n        self,\n        fo=\"\",\n        mode=\"r\",\n        target_protocol=None,\n        target_options=None,\n        compression=zipfile.ZIP_STORED,\n        allowZip64=True,\n        compresslevel=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        fo: str or file-like\n            Contains ZIP, and must exist. If a str, will fetch file using\n            :meth:`~fsspec.open_files`, which must return one file exactly.\n        mode: str\n            Accept: \"r\", \"w\", \"a\"\n        target_protocol: str (optional)\n            If ``fo`` is a string, this value can be used to override the\n            FS protocol inferred from a URL\n        target_options: dict (optional)\n            Kwargs passed when instantiating the target FS, if ``fo`` is\n            a string.\n        compression, allowZip64, compresslevel: passed to ZipFile\n            Only relevant when creating a ZIP\n        \"\"\"\n        super().__init__(self, **kwargs)\n        if mode not in set(\"rwa\"):\n            raise ValueError(f\"mode '{mode}' no understood\")\n        self.mode = mode\n        if isinstance(fo, str):\n            if mode == \"a\":\n                m = \"r+b\"\n            else:\n                m = mode + \"b\"\n            fo = fsspec.open(\n                fo, mode=m, protocol=target_protocol, **(target_options or {})\n            )\n        self.force_zip_64 = allowZip64\n        self.of = fo\n        self.fo = fo.__enter__()  # the whole instance is a context\n        self.zip = zipfile.ZipFile(\n            self.fo,\n            mode=mode,\n            compression=compression,\n            allowZip64=allowZip64,\n            compresslevel=compresslevel,\n        )\n        self.dir_cache = None\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        # zip file paths are always relative to the archive root\n        return super()._strip_protocol(path).lstrip(\"/\")\n\n    def __del__(self):\n        if hasattr(self, \"zip\"):\n            self.close()\n            del self.zip\n\n    def close(self):\n        \"\"\"Commits any write changes to the file. Done on ``del`` too.\"\"\"\n        self.zip.close()\n\n    def _get_dirs(self):\n        if self.dir_cache is None or self.mode in set(\"wa\"):\n            # when writing, dir_cache is always in the ZipFile's attributes,\n            # not read from the file.\n            files = self.zip.infolist()\n            self.dir_cache = {\n                dirname.rstrip(\"/\"): {\n                    \"name\": dirname.rstrip(\"/\"),\n                    \"size\": 0,\n                    \"type\": \"directory\",\n                }\n                for dirname in self._all_dirnames(self.zip.namelist())\n            }\n            for z in files:\n                f = {s: getattr(z, s, None) for s in zipfile.ZipInfo.__slots__}\n                f.update(\n                    {\n                        \"name\": z.filename.rstrip(\"/\"),\n                        \"size\": z.file_size,\n                        \"type\": (\"directory\" if z.is_dir() else \"file\"),\n                    }\n                )\n                self.dir_cache[f[\"name\"]] = f\n\n    def pipe_file(self, path, value, **kwargs):\n        # override upstream, because we know the exact file size in this case\n        self.zip.writestr(path, value, **kwargs)\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        **kwargs,\n    ):\n        path = self._strip_protocol(path)\n        if \"r\" in mode and self.mode in set(\"wa\"):\n            if self.exists(path):\n                raise OSError(\"ZipFS can only be open for reading or writing, not both\")\n            raise FileNotFoundError(path)\n        if \"r\" in self.mode and \"w\" in mode:\n            raise OSError(\"ZipFS can only be open for reading or writing, not both\")\n        out = self.zip.open(path, mode.strip(\"b\"), force_zip64=self.force_zip_64)\n        if \"r\" in mode:\n            info = self.info(path)\n            out.size = info[\"size\"]\n            out.name = info[\"name\"]\n        return out\n", "fsspec/implementations/jupyter.py": "import base64\nimport io\nimport re\n\nimport requests\n\nimport fsspec\n\n\nclass JupyterFileSystem(fsspec.AbstractFileSystem):\n    \"\"\"View of the files as seen by a Jupyter server (notebook or lab)\"\"\"\n\n    protocol = (\"jupyter\", \"jlab\")\n\n    def __init__(self, url, tok=None, **kwargs):\n        \"\"\"\n\n        Parameters\n        ----------\n        url : str\n            Base URL of the server, like \"http://127.0.0.1:8888\". May include\n            token in the string, which is given by the process when starting up\n        tok : str\n            If the token is obtained separately, can be given here\n        kwargs\n        \"\"\"\n        if \"?\" in url:\n            if tok is None:\n                try:\n                    tok = re.findall(\"token=([a-z0-9]+)\", url)[0]\n                except IndexError as e:\n                    raise ValueError(\"Could not determine token\") from e\n            url = url.split(\"?\", 1)[0]\n        self.url = url.rstrip(\"/\") + \"/api/contents\"\n        self.session = requests.Session()\n        if tok:\n            self.session.headers[\"Authorization\"] = f\"token {tok}\"\n\n        super().__init__(**kwargs)\n\n    def ls(self, path, detail=True, **kwargs):\n        path = self._strip_protocol(path)\n        r = self.session.get(f\"{self.url}/{path}\")\n        if r.status_code == 404:\n            return FileNotFoundError(path)\n        r.raise_for_status()\n        out = r.json()\n\n        if out[\"type\"] == \"directory\":\n            out = out[\"content\"]\n        else:\n            out = [out]\n        for o in out:\n            o[\"name\"] = o.pop(\"path\")\n            o.pop(\"content\")\n            if o[\"type\"] == \"notebook\":\n                o[\"type\"] = \"file\"\n        if detail:\n            return out\n        return [o[\"name\"] for o in out]\n\n    def cat_file(self, path, start=None, end=None, **kwargs):\n        path = self._strip_protocol(path)\n        r = self.session.get(f\"{self.url}/{path}\")\n        if r.status_code == 404:\n            return FileNotFoundError(path)\n        r.raise_for_status()\n        out = r.json()\n        if out[\"format\"] == \"text\":\n            # data should be binary\n            b = out[\"content\"].encode()\n        else:\n            b = base64.b64decode(out[\"content\"])\n        return b[start:end]\n\n    def pipe_file(self, path, value, **_):\n        path = self._strip_protocol(path)\n        json = {\n            \"name\": path.rsplit(\"/\", 1)[-1],\n            \"path\": path,\n            \"size\": len(value),\n            \"content\": base64.b64encode(value).decode(),\n            \"format\": \"base64\",\n            \"type\": \"file\",\n        }\n        self.session.put(f\"{self.url}/{path}\", json=json)\n\n    def mkdir(self, path, create_parents=True, **kwargs):\n        path = self._strip_protocol(path)\n        if create_parents and \"/\" in path:\n            self.mkdir(path.rsplit(\"/\", 1)[0], True)\n        json = {\n            \"name\": path.rsplit(\"/\", 1)[-1],\n            \"path\": path,\n            \"size\": None,\n            \"content\": None,\n            \"type\": \"directory\",\n        }\n        self.session.put(f\"{self.url}/{path}\", json=json)\n\n    def _rm(self, path):\n        path = self._strip_protocol(path)\n        self.session.delete(f\"{self.url}/{path}\")\n\n    def _open(self, path, mode=\"rb\", **kwargs):\n        path = self._strip_protocol(path)\n        if mode == \"rb\":\n            data = self.cat_file(path)\n            return io.BytesIO(data)\n        else:\n            return SimpleFileWriter(self, path, mode=\"wb\")\n\n\nclass SimpleFileWriter(fsspec.spec.AbstractBufferedFile):\n    def _upload_chunk(self, final=False):\n        \"\"\"Never uploads a chunk until file is done\n\n        Not suitable for large files\n        \"\"\"\n        if final is False:\n            return False\n        self.buffer.seek(0)\n        data = self.buffer.read()\n        self.fs.pipe_file(self.path, data)\n", "fsspec/implementations/libarchive.py": "from contextlib import contextmanager\nfrom ctypes import (\n    CFUNCTYPE,\n    POINTER,\n    c_int,\n    c_longlong,\n    c_void_p,\n    cast,\n    create_string_buffer,\n)\n\nimport libarchive\nimport libarchive.ffi as ffi\n\nfrom fsspec import open_files\nfrom fsspec.archive import AbstractArchiveFileSystem\nfrom fsspec.implementations.memory import MemoryFile\nfrom fsspec.utils import DEFAULT_BLOCK_SIZE\n\n# Libarchive requires seekable files or memory only for certain archive\n# types. However, since we read the directory first to cache the contents\n# and also allow random access to any file, the file-like object needs\n# to be seekable no matter what.\n\n# Seek call-backs (not provided in the libarchive python wrapper)\nSEEK_CALLBACK = CFUNCTYPE(c_longlong, c_int, c_void_p, c_longlong, c_int)\nread_set_seek_callback = ffi.ffi(\n    \"read_set_seek_callback\", [ffi.c_archive_p, SEEK_CALLBACK], c_int, ffi.check_int\n)\nnew_api = hasattr(ffi, \"NO_OPEN_CB\")\n\n\n@contextmanager\ndef custom_reader(file, format_name=\"all\", filter_name=\"all\", block_size=ffi.page_size):\n    \"\"\"Read an archive from a seekable file-like object.\n\n    The `file` object must support the standard `readinto` and 'seek' methods.\n    \"\"\"\n    buf = create_string_buffer(block_size)\n    buf_p = cast(buf, c_void_p)\n\n    def read_func(archive_p, context, ptrptr):\n        # readinto the buffer, returns number of bytes read\n        length = file.readinto(buf)\n        # write the address of the buffer into the pointer\n        ptrptr = cast(ptrptr, POINTER(c_void_p))\n        ptrptr[0] = buf_p\n        # tell libarchive how much data was written into the buffer\n        return length\n\n    def seek_func(archive_p, context, offset, whence):\n        file.seek(offset, whence)\n        # tell libarchvie the current position\n        return file.tell()\n\n    read_cb = ffi.READ_CALLBACK(read_func)\n    seek_cb = SEEK_CALLBACK(seek_func)\n\n    if new_api:\n        open_cb = ffi.NO_OPEN_CB\n        close_cb = ffi.NO_CLOSE_CB\n    else:\n        open_cb = libarchive.read.OPEN_CALLBACK(ffi.VOID_CB)\n        close_cb = libarchive.read.CLOSE_CALLBACK(ffi.VOID_CB)\n\n    with libarchive.read.new_archive_read(format_name, filter_name) as archive_p:\n        read_set_seek_callback(archive_p, seek_cb)\n        ffi.read_open(archive_p, None, open_cb, read_cb, close_cb)\n        yield libarchive.read.ArchiveRead(archive_p)\n\n\nclass LibArchiveFileSystem(AbstractArchiveFileSystem):\n    \"\"\"Compressed archives as a file-system (read-only)\n\n    Supports the following formats:\n    tar, pax , cpio, ISO9660, zip, mtree, shar, ar, raw, xar, lha/lzh, rar\n    Microsoft CAB, 7-Zip, WARC\n\n    See the libarchive documentation for further restrictions.\n    https://www.libarchive.org/\n\n    Keeps file object open while instance lives. It only works in seekable\n    file-like objects. In case the filesystem does not support this kind of\n    file object, it is recommended to cache locally.\n\n    This class is pickleable, but not necessarily thread-safe (depends on the\n    platform). See libarchive documentation for details.\n    \"\"\"\n\n    root_marker = \"\"\n    protocol = \"libarchive\"\n    cachable = False\n\n    def __init__(\n        self,\n        fo=\"\",\n        mode=\"r\",\n        target_protocol=None,\n        target_options=None,\n        block_size=DEFAULT_BLOCK_SIZE,\n        **kwargs,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        fo: str or file-like\n            Contains ZIP, and must exist. If a str, will fetch file using\n            :meth:`~fsspec.open_files`, which must return one file exactly.\n        mode: str\n            Currently, only 'r' accepted\n        target_protocol: str (optional)\n            If ``fo`` is a string, this value can be used to override the\n            FS protocol inferred from a URL\n        target_options: dict (optional)\n            Kwargs passed when instantiating the target FS, if ``fo`` is\n            a string.\n        \"\"\"\n        super().__init__(self, **kwargs)\n        if mode != \"r\":\n            raise ValueError(\"Only read from archive files accepted\")\n        if isinstance(fo, str):\n            files = open_files(fo, protocol=target_protocol, **(target_options or {}))\n            if len(files) != 1:\n                raise ValueError(\n                    f'Path \"{fo}\" did not resolve to exactly one file: \"{files}\"'\n                )\n            fo = files[0]\n        self.of = fo\n        self.fo = fo.__enter__()  # the whole instance is a context\n        self.block_size = block_size\n        self.dir_cache = None\n\n    @contextmanager\n    def _open_archive(self):\n        self.fo.seek(0)\n        with custom_reader(self.fo, block_size=self.block_size) as arc:\n            yield arc\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        # file paths are always relative to the archive root\n        return super()._strip_protocol(path).lstrip(\"/\")\n\n    def _get_dirs(self):\n        fields = {\n            \"name\": \"pathname\",\n            \"size\": \"size\",\n            \"created\": \"ctime\",\n            \"mode\": \"mode\",\n            \"uid\": \"uid\",\n            \"gid\": \"gid\",\n            \"mtime\": \"mtime\",\n        }\n\n        if self.dir_cache is not None:\n            return\n\n        self.dir_cache = {}\n        list_names = []\n        with self._open_archive() as arc:\n            for entry in arc:\n                if not entry.isdir and not entry.isfile:\n                    # Skip symbolic links, fifo entries, etc.\n                    continue\n                self.dir_cache.update(\n                    {\n                        dirname: {\"name\": dirname, \"size\": 0, \"type\": \"directory\"}\n                        for dirname in self._all_dirnames(set(entry.name))\n                    }\n                )\n                f = {key: getattr(entry, fields[key]) for key in fields}\n                f[\"type\"] = \"directory\" if entry.isdir else \"file\"\n                list_names.append(entry.name)\n\n                self.dir_cache[f[\"name\"]] = f\n        # libarchive does not seem to return an entry for the directories (at least\n        # not in all formats), so get the directories names from the files names\n        self.dir_cache.update(\n            {\n                dirname: {\"name\": dirname, \"size\": 0, \"type\": \"directory\"}\n                for dirname in self._all_dirnames(list_names)\n            }\n        )\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        **kwargs,\n    ):\n        path = self._strip_protocol(path)\n        if mode != \"rb\":\n            raise NotImplementedError\n\n        data = bytes()\n        with self._open_archive() as arc:\n            for entry in arc:\n                if entry.pathname != path:\n                    continue\n\n                if entry.size == 0:\n                    # empty file, so there are no blocks\n                    break\n\n                for block in entry.get_blocks(entry.size):\n                    data = block\n                    break\n                else:\n                    raise ValueError\n        return MemoryFile(fs=self, path=path, data=data)\n", "fsspec/implementations/dirfs.py": "from .. import filesystem\nfrom ..asyn import AsyncFileSystem\n\n\nclass DirFileSystem(AsyncFileSystem):\n    \"\"\"Directory prefix filesystem\n\n    The DirFileSystem is a filesystem-wrapper. It assumes every path it is dealing with\n    is relative to the `path`. After performing the necessary paths operation it\n    delegates everything to the wrapped filesystem.\n    \"\"\"\n\n    protocol = \"dir\"\n\n    def __init__(\n        self,\n        path=None,\n        fs=None,\n        fo=None,\n        target_protocol=None,\n        target_options=None,\n        **storage_options,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        path: str\n            Path to the directory.\n        fs: AbstractFileSystem\n            An instantiated filesystem to wrap.\n        target_protocol, target_options:\n            if fs is none, construct it from these\n        fo: str\n            Alternate for path; do not provide both\n        \"\"\"\n        super().__init__(**storage_options)\n        if fs is None:\n            fs = filesystem(protocol=target_protocol, **(target_options or {}))\n        if (path is not None) ^ (fo is not None) is False:\n            raise ValueError(\"Provide path or fo, not both\")\n        path = path or fo\n\n        if self.asynchronous and not fs.async_impl:\n            raise ValueError(\"can't use asynchronous with non-async fs\")\n\n        if fs.async_impl and self.asynchronous != fs.asynchronous:\n            raise ValueError(\"both dirfs and fs should be in the same sync/async mode\")\n\n        self.path = fs._strip_protocol(path)\n        self.fs = fs\n\n    def _join(self, path):\n        if isinstance(path, str):\n            if not self.path:\n                return path\n            if not path:\n                return self.path\n            return self.fs.sep.join((self.path, self._strip_protocol(path)))\n        if isinstance(path, dict):\n            return {self._join(_path): value for _path, value in path.items()}\n        return [self._join(_path) for _path in path]\n\n    def _relpath(self, path):\n        if isinstance(path, str):\n            if not self.path:\n                return path\n            if path == self.path:\n                return \"\"\n            prefix = self.path + self.fs.sep\n            assert path.startswith(prefix)\n            return path[len(prefix) :]\n        return [self._relpath(_path) for _path in path]\n\n    # Wrappers below\n\n    @property\n    def sep(self):\n        return self.fs.sep\n\n    async def set_session(self, *args, **kwargs):\n        return await self.fs.set_session(*args, **kwargs)\n\n    async def _rm_file(self, path, **kwargs):\n        return await self.fs._rm_file(self._join(path), **kwargs)\n\n    def rm_file(self, path, **kwargs):\n        return self.fs.rm_file(self._join(path), **kwargs)\n\n    async def _rm(self, path, *args, **kwargs):\n        return await self.fs._rm(self._join(path), *args, **kwargs)\n\n    def rm(self, path, *args, **kwargs):\n        return self.fs.rm(self._join(path), *args, **kwargs)\n\n    async def _cp_file(self, path1, path2, **kwargs):\n        return await self.fs._cp_file(self._join(path1), self._join(path2), **kwargs)\n\n    def cp_file(self, path1, path2, **kwargs):\n        return self.fs.cp_file(self._join(path1), self._join(path2), **kwargs)\n\n    async def _copy(\n        self,\n        path1,\n        path2,\n        *args,\n        **kwargs,\n    ):\n        return await self.fs._copy(\n            self._join(path1),\n            self._join(path2),\n            *args,\n            **kwargs,\n        )\n\n    def copy(self, path1, path2, *args, **kwargs):\n        return self.fs.copy(\n            self._join(path1),\n            self._join(path2),\n            *args,\n            **kwargs,\n        )\n\n    async def _pipe(self, path, *args, **kwargs):\n        return await self.fs._pipe(self._join(path), *args, **kwargs)\n\n    def pipe(self, path, *args, **kwargs):\n        return self.fs.pipe(self._join(path), *args, **kwargs)\n\n    async def _pipe_file(self, path, *args, **kwargs):\n        return await self.fs._pipe_file(self._join(path), *args, **kwargs)\n\n    def pipe_file(self, path, *args, **kwargs):\n        return self.fs.pipe_file(self._join(path), *args, **kwargs)\n\n    async def _cat_file(self, path, *args, **kwargs):\n        return await self.fs._cat_file(self._join(path), *args, **kwargs)\n\n    def cat_file(self, path, *args, **kwargs):\n        return self.fs.cat_file(self._join(path), *args, **kwargs)\n\n    async def _cat(self, path, *args, **kwargs):\n        ret = await self.fs._cat(\n            self._join(path),\n            *args,\n            **kwargs,\n        )\n\n        if isinstance(ret, dict):\n            return {self._relpath(key): value for key, value in ret.items()}\n\n        return ret\n\n    def cat(self, path, *args, **kwargs):\n        ret = self.fs.cat(\n            self._join(path),\n            *args,\n            **kwargs,\n        )\n\n        if isinstance(ret, dict):\n            return {self._relpath(key): value for key, value in ret.items()}\n\n        return ret\n\n    async def _put_file(self, lpath, rpath, **kwargs):\n        return await self.fs._put_file(lpath, self._join(rpath), **kwargs)\n\n    def put_file(self, lpath, rpath, **kwargs):\n        return self.fs.put_file(lpath, self._join(rpath), **kwargs)\n\n    async def _put(\n        self,\n        lpath,\n        rpath,\n        *args,\n        **kwargs,\n    ):\n        return await self.fs._put(\n            lpath,\n            self._join(rpath),\n            *args,\n            **kwargs,\n        )\n\n    def put(self, lpath, rpath, *args, **kwargs):\n        return self.fs.put(\n            lpath,\n            self._join(rpath),\n            *args,\n            **kwargs,\n        )\n\n    async def _get_file(self, rpath, lpath, **kwargs):\n        return await self.fs._get_file(self._join(rpath), lpath, **kwargs)\n\n    def get_file(self, rpath, lpath, **kwargs):\n        return self.fs.get_file(self._join(rpath), lpath, **kwargs)\n\n    async def _get(self, rpath, *args, **kwargs):\n        return await self.fs._get(self._join(rpath), *args, **kwargs)\n\n    def get(self, rpath, *args, **kwargs):\n        return self.fs.get(self._join(rpath), *args, **kwargs)\n\n    async def _isfile(self, path):\n        return await self.fs._isfile(self._join(path))\n\n    def isfile(self, path):\n        return self.fs.isfile(self._join(path))\n\n    async def _isdir(self, path):\n        return await self.fs._isdir(self._join(path))\n\n    def isdir(self, path):\n        return self.fs.isdir(self._join(path))\n\n    async def _size(self, path):\n        return await self.fs._size(self._join(path))\n\n    def size(self, path):\n        return self.fs.size(self._join(path))\n\n    async def _exists(self, path):\n        return await self.fs._exists(self._join(path))\n\n    def exists(self, path):\n        return self.fs.exists(self._join(path))\n\n    async def _info(self, path, **kwargs):\n        return await self.fs._info(self._join(path), **kwargs)\n\n    def info(self, path, **kwargs):\n        return self.fs.info(self._join(path), **kwargs)\n\n    async def _ls(self, path, detail=True, **kwargs):\n        ret = (await self.fs._ls(self._join(path), detail=detail, **kwargs)).copy()\n        if detail:\n            out = []\n            for entry in ret:\n                entry = entry.copy()\n                entry[\"name\"] = self._relpath(entry[\"name\"])\n                out.append(entry)\n            return out\n\n        return self._relpath(ret)\n\n    def ls(self, path, detail=True, **kwargs):\n        ret = self.fs.ls(self._join(path), detail=detail, **kwargs).copy()\n        if detail:\n            out = []\n            for entry in ret:\n                entry = entry.copy()\n                entry[\"name\"] = self._relpath(entry[\"name\"])\n                out.append(entry)\n            return out\n\n        return self._relpath(ret)\n\n    async def _walk(self, path, *args, **kwargs):\n        async for root, dirs, files in self.fs._walk(self._join(path), *args, **kwargs):\n            yield self._relpath(root), dirs, files\n\n    def walk(self, path, *args, **kwargs):\n        for root, dirs, files in self.fs.walk(self._join(path), *args, **kwargs):\n            yield self._relpath(root), dirs, files\n\n    async def _glob(self, path, **kwargs):\n        detail = kwargs.get(\"detail\", False)\n        ret = await self.fs._glob(self._join(path), **kwargs)\n        if detail:\n            return {self._relpath(path): info for path, info in ret.items()}\n        return self._relpath(ret)\n\n    def glob(self, path, **kwargs):\n        detail = kwargs.get(\"detail\", False)\n        ret = self.fs.glob(self._join(path), **kwargs)\n        if detail:\n            return {self._relpath(path): info for path, info in ret.items()}\n        return self._relpath(ret)\n\n    async def _du(self, path, *args, **kwargs):\n        total = kwargs.get(\"total\", True)\n        ret = await self.fs._du(self._join(path), *args, **kwargs)\n        if total:\n            return ret\n\n        return {self._relpath(path): size for path, size in ret.items()}\n\n    def du(self, path, *args, **kwargs):\n        total = kwargs.get(\"total\", True)\n        ret = self.fs.du(self._join(path), *args, **kwargs)\n        if total:\n            return ret\n\n        return {self._relpath(path): size for path, size in ret.items()}\n\n    async def _find(self, path, *args, **kwargs):\n        detail = kwargs.get(\"detail\", False)\n        ret = await self.fs._find(self._join(path), *args, **kwargs)\n        if detail:\n            return {self._relpath(path): info for path, info in ret.items()}\n        return self._relpath(ret)\n\n    def find(self, path, *args, **kwargs):\n        detail = kwargs.get(\"detail\", False)\n        ret = self.fs.find(self._join(path), *args, **kwargs)\n        if detail:\n            return {self._relpath(path): info for path, info in ret.items()}\n        return self._relpath(ret)\n\n    async def _expand_path(self, path, *args, **kwargs):\n        return self._relpath(\n            await self.fs._expand_path(self._join(path), *args, **kwargs)\n        )\n\n    def expand_path(self, path, *args, **kwargs):\n        return self._relpath(self.fs.expand_path(self._join(path), *args, **kwargs))\n\n    async def _mkdir(self, path, *args, **kwargs):\n        return await self.fs._mkdir(self._join(path), *args, **kwargs)\n\n    def mkdir(self, path, *args, **kwargs):\n        return self.fs.mkdir(self._join(path), *args, **kwargs)\n\n    async def _makedirs(self, path, *args, **kwargs):\n        return await self.fs._makedirs(self._join(path), *args, **kwargs)\n\n    def makedirs(self, path, *args, **kwargs):\n        return self.fs.makedirs(self._join(path), *args, **kwargs)\n\n    def rmdir(self, path):\n        return self.fs.rmdir(self._join(path))\n\n    def mv(self, path1, path2, **kwargs):\n        return self.fs.mv(\n            self._join(path1),\n            self._join(path2),\n            **kwargs,\n        )\n\n    def touch(self, path, **kwargs):\n        return self.fs.touch(self._join(path), **kwargs)\n\n    def created(self, path):\n        return self.fs.created(self._join(path))\n\n    def modified(self, path):\n        return self.fs.modified(self._join(path))\n\n    def sign(self, path, *args, **kwargs):\n        return self.fs.sign(self._join(path), *args, **kwargs)\n\n    def __repr__(self):\n        return f\"{self.__class__.__qualname__}(path='{self.path}', fs={self.fs})\"\n\n    def open(\n        self,\n        path,\n        *args,\n        **kwargs,\n    ):\n        return self.fs.open(\n            self._join(path),\n            *args,\n            **kwargs,\n        )\n", "fsspec/implementations/tar.py": "import logging\nimport tarfile\n\nimport fsspec\nfrom fsspec.archive import AbstractArchiveFileSystem\nfrom fsspec.compression import compr\nfrom fsspec.utils import infer_compression\n\ntypemap = {b\"0\": \"file\", b\"5\": \"directory\"}\n\nlogger = logging.getLogger(\"tar\")\n\n\nclass TarFileSystem(AbstractArchiveFileSystem):\n    \"\"\"Compressed Tar archives as a file-system (read-only)\n\n    Supports the following formats:\n    tar.gz, tar.bz2, tar.xz\n    \"\"\"\n\n    root_marker = \"\"\n    protocol = \"tar\"\n    cachable = False\n\n    def __init__(\n        self,\n        fo=\"\",\n        index_store=None,\n        target_options=None,\n        target_protocol=None,\n        compression=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        target_options = target_options or {}\n\n        if isinstance(fo, str):\n            self.of = fsspec.open(fo, protocol=target_protocol, **target_options)\n            fo = self.of.open()  # keep the reference\n\n        # Try to infer compression.\n        if compression is None:\n            name = None\n\n            # Try different ways to get hold of the filename. `fo` might either\n            # be a `fsspec.LocalFileOpener`, an `io.BufferedReader` or an\n            # `fsspec.AbstractFileSystem` instance.\n            try:\n                # Amended io.BufferedReader or similar.\n                # This uses a \"protocol extension\" where original filenames are\n                # propagated to archive-like filesystems in order to let them\n                # infer the right compression appropriately.\n                if hasattr(fo, \"original\"):\n                    name = fo.original\n\n                # fsspec.LocalFileOpener\n                elif hasattr(fo, \"path\"):\n                    name = fo.path\n\n                # io.BufferedReader\n                elif hasattr(fo, \"name\"):\n                    name = fo.name\n\n                # fsspec.AbstractFileSystem\n                elif hasattr(fo, \"info\"):\n                    name = fo.info()[\"name\"]\n\n            except Exception as ex:\n                logger.warning(\n                    f\"Unable to determine file name, not inferring compression: {ex}\"\n                )\n\n            if name is not None:\n                compression = infer_compression(name)\n                logger.info(f\"Inferred compression {compression} from file name {name}\")\n\n        if compression is not None:\n            # TODO: tarfile already implements compression with modes like \"'r:gz'\",\n            #  but then would seek to offset in the file work?\n            fo = compr[compression](fo)\n\n        self._fo_ref = fo\n        self.fo = fo  # the whole instance is a context\n        self.tar = tarfile.TarFile(fileobj=self.fo)\n        self.dir_cache = None\n\n        self.index_store = index_store\n        self.index = None\n        self._index()\n\n    def _index(self):\n        # TODO: load and set saved index, if exists\n        out = {}\n        for ti in self.tar:\n            info = ti.get_info()\n            info[\"type\"] = typemap.get(info[\"type\"], \"file\")\n            name = ti.get_info()[\"name\"].rstrip(\"/\")\n            out[name] = (info, ti.offset_data)\n\n        self.index = out\n        # TODO: save index to self.index_store here, if set\n\n    def _get_dirs(self):\n        if self.dir_cache is not None:\n            return\n\n        # This enables ls to get directories as children as well as files\n        self.dir_cache = {\n            dirname: {\"name\": dirname, \"size\": 0, \"type\": \"directory\"}\n            for dirname in self._all_dirnames(self.tar.getnames())\n        }\n        for member in self.tar.getmembers():\n            info = member.get_info()\n            info[\"name\"] = info[\"name\"].rstrip(\"/\")\n            info[\"type\"] = typemap.get(info[\"type\"], \"file\")\n            self.dir_cache[info[\"name\"]] = info\n\n    def _open(self, path, mode=\"rb\", **kwargs):\n        if mode != \"rb\":\n            raise ValueError(\"Read-only filesystem implementation\")\n        details, offset = self.index[path]\n        if details[\"type\"] != \"file\":\n            raise ValueError(\"Can only handle regular files\")\n        return self.tar.extractfile(path)\n", "fsspec/implementations/data.py": "import base64\nimport io\nfrom typing import Optional\nfrom urllib.parse import unquote\n\nfrom fsspec import AbstractFileSystem\n\n\nclass DataFileSystem(AbstractFileSystem):\n    \"\"\"A handy decoder for data-URLs\n\n    Example\n    -------\n    >>> with fsspec.open(\"data:,Hello%2C%20World%21\") as f:\n    ...     print(f.read())\n    b\"Hello, World!\"\n\n    See https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs\n    \"\"\"\n\n    protocol = \"data\"\n\n    def __init__(self, **kwargs):\n        \"\"\"No parameters for this filesystem\"\"\"\n        super().__init__(**kwargs)\n\n    def cat_file(self, path, start=None, end=None, **kwargs):\n        pref, data = path.split(\",\", 1)\n        if pref.endswith(\"base64\"):\n            return base64.b64decode(data)[start:end]\n        return unquote(data).encode()[start:end]\n\n    def info(self, path, **kwargs):\n        pref, name = path.split(\",\", 1)\n        data = self.cat_file(path)\n        mime = pref.split(\":\", 1)[1].split(\";\", 1)[0]\n        return {\"name\": name, \"size\": len(data), \"type\": \"file\", \"mimetype\": mime}\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        autocommit=True,\n        cache_options=None,\n        **kwargs,\n    ):\n        if \"r\" not in mode:\n            raise ValueError(\"Read only filesystem\")\n        return io.BytesIO(self.cat_file(path))\n\n    @staticmethod\n    def encode(data: bytes, mime: Optional[str] = None):\n        \"\"\"Format the given data into data-URL syntax\n\n        This version always base64 encodes, even when the data is ascii/url-safe.\n        \"\"\"\n        return f\"data:{mime or ''};base64,{base64.b64encode(data).decode()}\"\n", "fsspec/implementations/tests/test_ftp.py": "import os\nimport subprocess\nimport sys\nimport time\n\nimport pytest\n\nimport fsspec\nfrom fsspec import open_files\nfrom fsspec.implementations.ftp import FTPFileSystem\n\nftplib = pytest.importorskip(\"ftplib\")\nhere = os.path.dirname(os.path.abspath(__file__))\n\n\n@pytest.fixture()\ndef ftp():\n    pytest.importorskip(\"pyftpdlib\")\n    P = subprocess.Popen(\n        [sys.executable, \"-m\", \"pyftpdlib\", \"-d\", here],\n        stderr=subprocess.STDOUT,\n        stdout=subprocess.PIPE,\n    )\n    try:\n        time.sleep(1)\n        yield \"localhost\", 2121\n    finally:\n        P.terminate()\n        P.wait()\n\n\ndef test_basic(ftp):\n    host, port = ftp\n    fs = FTPFileSystem(host, port)\n    assert fs.ls(\"/\", detail=False) == sorted(os.listdir(here))\n    out = fs.cat(f\"/{os.path.basename(__file__)}\")\n    assert out == open(__file__, \"rb\").read()\n\n\ndef test_not_cached(ftp):\n    host, port = ftp\n    fs = FTPFileSystem(host, port)\n    fs2 = FTPFileSystem(host, port)\n    assert fs is not fs2\n\n\n@pytest.mark.parametrize(\"cache_type\", [\"bytes\", \"mmap\"])\ndef test_complex(ftp_writable, cache_type):\n    from fsspec.core import BytesCache\n\n    host, port, user, pw = ftp_writable\n    files = open_files(\n        \"ftp:///ou*\",\n        host=host,\n        port=port,\n        username=user,\n        password=pw,\n        block_size=10000,\n        cache_type=cache_type,\n    )\n    assert len(files) == 1\n    with files[0] as fo:\n        assert fo.read(10) == b\"hellohello\"\n        if isinstance(fo.cache, BytesCache):\n            assert len(fo.cache.cache) == 10010\n        assert fo.read(2) == b\"he\"\n        assert fo.tell() == 12\n\n\ndef test_write_small(ftp_writable):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    with fs.open(\"/out2\", \"wb\") as f:\n        f.write(b\"oi\")\n    assert fs.cat(\"/out2\") == b\"oi\"\n\n\ndef test_with_url(ftp_writable):\n    host, port, user, pw = ftp_writable\n    fo = fsspec.open(f\"ftp://{user}:{pw}@{host}:{port}/out\", \"wb\")\n    with fo as f:\n        f.write(b\"hello\")\n    fo = fsspec.open(f\"ftp://{user}:{pw}@{host}:{port}/out\", \"rb\")\n    with fo as f:\n        assert f.read() == b\"hello\"\n\n\n@pytest.mark.parametrize(\"cache_type\", [\"bytes\", \"mmap\"])\ndef test_write_big(ftp_writable, cache_type):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw, block_size=1000, cache_type=cache_type)\n    fn = \"/bigger\"\n    with fs.open(fn, \"wb\") as f:\n        f.write(b\"o\" * 500)\n        assert not fs.exists(fn)\n        f.write(b\"o\" * 1000)\n        fs.invalidate_cache()\n        assert fs.exists(fn)\n        f.write(b\"o\" * 200)\n        f.flush()\n\n    assert fs.info(fn)[\"size\"] == 1700\n    assert fs.cat(fn) == b\"o\" * 1700\n\n\ndef test_transaction(ftp_writable):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    fs.mkdir(\"/tmp\")\n    fn = \"/tr\"\n    with fs.transaction:\n        with fs.open(fn, \"wb\") as f:\n            f.write(b\"not\")\n        assert not fs.exists(fn)\n    assert fs.exists(fn)\n    assert fs.cat(fn) == b\"not\"\n\n    fs.rm(fn)\n    assert not fs.exists(fn)\n\n\ndef test_transaction_with_cache(ftp_writable, tmpdir):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    fs.mkdir(\"/tmp\")\n    fs.mkdir(\"/tmp/dir\")\n    assert \"dir\" in fs.ls(\"/tmp\", detail=False)\n\n    with fs.transaction:\n        fs.rmdir(\"/tmp/dir\")\n\n    assert \"dir\" not in fs.ls(\"/tmp\", detail=False)\n    assert not fs.exists(\"/tmp/dir\")\n\n\ndef test_cat_get(ftp_writable, tmpdir):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw, block_size=500)\n    fs.mkdir(\"/tmp\")\n    data = b\"hello\" * 500\n    fs.pipe(\"/tmp/myfile\", data)\n    assert fs.cat_file(\"/tmp/myfile\") == data\n\n    fn = os.path.join(tmpdir, \"lfile\")\n    fs.get_file(\"/tmp/myfile\", fn)\n    assert open(fn, \"rb\").read() == data\n\n\ndef test_mkdir(ftp_writable):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    with pytest.raises(ftplib.error_perm):\n        fs.mkdir(\"/tmp/not/exist\", create_parents=False)\n    fs.mkdir(\"/tmp/not/exist\")\n    assert fs.exists(\"/tmp/not/exist\")\n    fs.makedirs(\"/tmp/not/exist\", exist_ok=True)\n    with pytest.raises(FileExistsError):\n        fs.makedirs(\"/tmp/not/exist\", exist_ok=False)\n    fs.makedirs(\"/tmp/not/exist/inner/inner\")\n    assert fs.isdir(\"/tmp/not/exist/inner/inner\")\n\n\ndef test_rm_get_recursive(ftp_writable, tmpdir):\n    tmpdir = str(tmpdir)\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    fs.mkdir(\"/tmp/topdir\")\n    fs.mkdir(\"/tmp/topdir/underdir\")\n    fs.touch(\"/tmp/topdir/afile\")\n    fs.touch(\"/tmp/topdir/underdir/afile\")\n\n    fs.get(\"/tmp/topdir\", tmpdir, recursive=True)\n\n    with pytest.raises(ftplib.error_perm):\n        fs.rmdir(\"/tmp/topdir\")\n\n    fs.rm(\"/tmp/topdir\", recursive=True)\n    assert not fs.exists(\"/tmp/topdir\")\n", "fsspec/implementations/tests/test_cached.py": "import json\nimport os\nimport pickle\nimport shutil\nimport tempfile\n\nimport pytest\n\nimport fsspec\nfrom fsspec.compression import compr\nfrom fsspec.exceptions import BlocksizeMismatchError\nfrom fsspec.implementations.cache_mapper import (\n    BasenameCacheMapper,\n    HashCacheMapper,\n    create_cache_mapper,\n)\nfrom fsspec.implementations.cached import (\n    CachingFileSystem,\n    LocalTempFile,\n    WholeFileCacheFileSystem,\n)\nfrom fsspec.implementations.local import make_path_posix\nfrom fsspec.implementations.zip import ZipFileSystem\nfrom fsspec.tests.conftest import win\n\nfrom .test_ftp import FTPFileSystem\n\n\n@pytest.fixture\ndef local_filecache():\n    import tempfile\n\n    original_location = tempfile.mkdtemp()\n    cache_location = tempfile.mkdtemp()\n    original_file = os.path.join(original_location, \"afile\")\n    data = b\"test data\"\n    with open(original_file, \"wb\") as f:\n        f.write(data)\n\n    # we can access the file and read it\n    fs = fsspec.filesystem(\n        \"filecache\", target_protocol=\"file\", cache_storage=cache_location\n    )\n\n    return data, original_file, cache_location, fs\n\n\ndef test_mapper():\n    mapper0 = create_cache_mapper(True)\n    assert mapper0(\"somefile\") == \"somefile\"\n    assert mapper0(\"/somefile\") == \"somefile\"\n    assert mapper0(\"/somedir/somefile\") == \"somefile\"\n    assert mapper0(\"/otherdir/somefile\") == \"somefile\"\n\n    mapper1 = create_cache_mapper(False)\n    assert (\n        mapper1(\"somefile\")\n        == \"dd00b9487898b02555b6a2d90a070586d63f93e80c70aaa60c992fa9e81a72fe\"\n    )\n    assert (\n        mapper1(\"/somefile\")\n        == \"884c07bc2efe65c60fb9d280a620e7f180488718fb5d97736521b7f9cf5c8b37\"\n    )\n    assert (\n        mapper1(\"/somedir/somefile\")\n        == \"67a6956e5a5f95231263f03758c1fd9254fdb1c564d311674cec56b0372d2056\"\n    )\n    assert (\n        mapper1(\"/otherdir/somefile\")\n        == \"f043dee01ab9b752c7f2ecaeb1a5e1b2d872018e2d0a1a26c43835ebf34e7d3e\"\n    )\n\n    assert mapper0 != mapper1\n    assert create_cache_mapper(True) == mapper0\n    assert create_cache_mapper(False) == mapper1\n\n    assert hash(mapper0) != hash(mapper1)\n    assert hash(create_cache_mapper(True)) == hash(mapper0)\n    assert hash(create_cache_mapper(False)) == hash(mapper1)\n\n    with pytest.raises(\n        ValueError,\n        match=\"BasenameCacheMapper requires zero or positive directory_levels\",\n    ):\n        BasenameCacheMapper(-1)\n\n    mapper2 = BasenameCacheMapper(1)\n    assert mapper2(\"/somefile\") == \"somefile\"\n    assert mapper2(\"/somedir/somefile\") == \"somedir_@_somefile\"\n    assert mapper2(\"/otherdir/somefile\") == \"otherdir_@_somefile\"\n    assert mapper2(\"/dir1/dir2/dir3/somefile\") == \"dir3_@_somefile\"\n\n    assert mapper2 != mapper0\n    assert mapper2 != mapper1\n    assert BasenameCacheMapper(1) == mapper2\n\n    assert hash(mapper2) != hash(mapper0)\n    assert hash(mapper2) != hash(mapper1)\n    assert hash(BasenameCacheMapper(1)) == hash(mapper2)\n\n    mapper3 = BasenameCacheMapper(2)\n    assert mapper3(\"/somefile\") == \"somefile\"\n    assert mapper3(\"/somedir/somefile\") == \"somedir_@_somefile\"\n    assert mapper3(\"/otherdir/somefile\") == \"otherdir_@_somefile\"\n    assert mapper3(\"/dir1/dir2/dir3/somefile\") == \"dir2_@_dir3_@_somefile\"\n\n    assert mapper3 != mapper0\n    assert mapper3 != mapper1\n    assert mapper3 != mapper2\n    assert BasenameCacheMapper(2) == mapper3\n\n    assert hash(mapper3) != hash(mapper0)\n    assert hash(mapper3) != hash(mapper1)\n    assert hash(mapper3) != hash(mapper2)\n    assert hash(BasenameCacheMapper(2)) == hash(mapper3)\n\n\n@pytest.mark.parametrize(\n    \"cache_mapper\", [BasenameCacheMapper(), BasenameCacheMapper(1), HashCacheMapper()]\n)\n@pytest.mark.parametrize(\"force_save_pickle\", [True, False])\ndef test_metadata(tmpdir, cache_mapper, force_save_pickle):\n    source = os.path.join(tmpdir, \"source\")\n    afile = os.path.join(source, \"afile\")\n    os.mkdir(source)\n    open(afile, \"w\").write(\"test\")\n\n    fs = fsspec.filesystem(\n        \"filecache\",\n        target_protocol=\"file\",\n        cache_storage=os.path.join(tmpdir, \"cache\"),\n        cache_mapper=cache_mapper,\n    )\n    fs._metadata._force_save_pickle = force_save_pickle\n\n    with fs.open(afile, \"rb\") as f:\n        assert f.read(5) == b\"test\"\n\n    afile_posix = make_path_posix(afile)\n    detail = fs._metadata.cached_files[0][afile_posix]\n    assert sorted(detail.keys()) == [\"blocks\", \"fn\", \"original\", \"time\", \"uid\"]\n    assert isinstance(detail[\"blocks\"], bool)\n    assert isinstance(detail[\"fn\"], str)\n    assert isinstance(detail[\"time\"], float)\n    assert isinstance(detail[\"uid\"], str)\n\n    assert detail[\"original\"] == afile_posix\n    assert detail[\"fn\"] == fs._mapper(afile_posix)\n\n    if isinstance(cache_mapper, BasenameCacheMapper):\n        if cache_mapper.directory_levels == 0:\n            assert detail[\"fn\"] == \"afile\"\n        else:\n            assert detail[\"fn\"] == \"source_@_afile\"\n\n\ndef test_metadata_replace_pickle_with_json(tmpdir):\n    # For backward compatibility will allow reading of old pickled metadata.\n    # When the metadata is next saved, it is in json format.\n    source = os.path.join(tmpdir, \"source\")\n    afile = os.path.join(source, \"afile\")\n    os.mkdir(source)\n    open(afile, \"w\").write(\"test\")\n\n    # Save metadata in pickle format, to simulate old metadata\n    fs = fsspec.filesystem(\n        \"filecache\",\n        target_protocol=\"file\",\n        cache_storage=os.path.join(tmpdir, \"cache\"),\n    )\n    fs._metadata._force_save_pickle = True\n    with fs.open(afile, \"rb\") as f:\n        assert f.read(5) == b\"test\"\n\n    # Confirm metadata is in pickle format\n    cache_fn = os.path.join(fs.storage[-1], \"cache\")\n    with open(cache_fn, \"rb\") as f:\n        metadata = pickle.load(f)\n    assert list(metadata.keys()) == [make_path_posix(afile)]\n\n    # Force rewrite of metadata, now in json format\n    fs._metadata._force_save_pickle = False\n    fs.pop_from_cache(afile)\n    with fs.open(afile, \"rb\") as f:\n        assert f.read(5) == b\"test\"\n\n    # Confirm metadata is in json format\n    with open(cache_fn, \"r\") as f:\n        metadata = json.load(f)\n    assert list(metadata.keys()) == [make_path_posix(afile)]\n\n\ndef test_constructor_kwargs(tmpdir):\n    fs = fsspec.filesystem(\"filecache\", target_protocol=\"file\", same_names=True)\n    assert isinstance(fs._mapper, BasenameCacheMapper)\n\n    fs = fsspec.filesystem(\"filecache\", target_protocol=\"file\", same_names=False)\n    assert isinstance(fs._mapper, HashCacheMapper)\n\n    fs = fsspec.filesystem(\"filecache\", target_protocol=\"file\")\n    assert isinstance(fs._mapper, HashCacheMapper)\n\n    with pytest.raises(\n        ValueError, match=\"Cannot specify both same_names and cache_mapper\"\n    ):\n        fs = fsspec.filesystem(\n            \"filecache\",\n            target_protocol=\"file\",\n            cache_mapper=HashCacheMapper(),\n            same_names=True,\n        )\n\n\ndef test_idempotent():\n    fs = CachingFileSystem(\"file\")\n    fs2 = CachingFileSystem(\"file\")\n    assert fs2 is fs\n    fs3 = pickle.loads(pickle.dumps(fs))\n    assert fs3.storage == fs.storage\n\n\n@pytest.mark.parametrize(\"force_save_pickle\", [True, False])\ndef test_blockcache_workflow(ftp_writable, tmp_path, force_save_pickle):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    with fs.open(\"/out\", \"wb\") as f:\n        f.write(b\"test\\n\" * 4096)\n\n    fs_kwargs = {\n        \"skip_instance_cache\": True,\n        \"cache_storage\": str(tmp_path),\n        \"target_protocol\": \"ftp\",\n        \"target_options\": {\n            \"host\": host,\n            \"port\": port,\n            \"username\": user,\n            \"password\": pw,\n        },\n    }\n\n    # Open the blockcache and read a little bit of the data\n    fs = fsspec.filesystem(\"blockcache\", **fs_kwargs)\n    fs._metadata._force_save_pickle = force_save_pickle\n    with fs.open(\"/out\", \"rb\", block_size=5) as f:\n        assert f.read(5) == b\"test\\n\"\n\n    # Save the cache/close it\n    fs.save_cache()\n    del fs\n\n    # Check that cache file only has the first two blocks\n    if force_save_pickle:\n        with open(tmp_path / \"cache\", \"rb\") as f:\n            cache = pickle.load(f)\n    else:\n        with open(tmp_path / \"cache\", \"r\") as f:\n            cache = json.load(f)\n    assert \"/out\" in cache\n    assert cache[\"/out\"][\"blocks\"] == [0, 1]\n\n    # Reopen the same cache and read some more...\n    fs = fsspec.filesystem(\"blockcache\", **fs_kwargs)\n    fs._metadata._force_save_pickle = force_save_pickle\n    with fs.open(\"/out\", block_size=5) as f:\n        assert f.read(5) == b\"test\\n\"\n        f.seek(30)\n        assert f.read(5) == b\"test\\n\"\n\n\n@pytest.mark.parametrize(\"impl\", [\"filecache\", \"blockcache\"])\ndef test_workflow(ftp_writable, impl):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    with fs.open(\"/out\", \"wb\") as f:\n        f.write(b\"test\")\n    fs = fsspec.filesystem(\n        impl,\n        target_protocol=\"ftp\",\n        target_options={\"host\": host, \"port\": port, \"username\": user, \"password\": pw},\n    )\n    assert os.listdir(fs.storage[-1]) == []\n    with fs.open(\"/out\") as f:\n        assert os.listdir(fs.storage[-1])\n        assert f.read() == b\"test\"\n        assert fs._metadata.cached_files[-1][\"/out\"][\"blocks\"]\n    assert fs.cat(\"/out\") == b\"test\"\n    assert fs._metadata.cached_files[-1][\"/out\"][\"blocks\"] is True\n\n    with fs.open(\"/out\", \"wb\") as f:\n        f.write(b\"changed\")\n\n    if impl == \"filecache\":\n        assert (\n            fs.cat(\"/out\") == b\"changed\"\n        )  # new value, because we overwrote the cached location\n\n\n@pytest.mark.parametrize(\"impl\", [\"simplecache\", \"blockcache\"])\ndef test_glob(ftp_writable, impl):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    with fs.open(\"/out\", \"wb\") as f:\n        f.write(b\"test\")\n    with fs.open(\"/out2\", \"wb\") as f:\n        f.write(b\"test2\")\n    fs = fsspec.filesystem(\n        impl,\n        target_protocol=\"ftp\",\n        target_options={\"host\": host, \"port\": port, \"username\": user, \"password\": pw},\n    )\n    assert fs.glob(\"/wrong*\") == []\n    assert fs.glob(\"/ou*\") == [\"/out\", \"/out2\"]\n\n\ndef test_write():\n    tmp = str(tempfile.mkdtemp())\n    fn = tmp + \"afile\"\n    url = f\"simplecache::file://{fn}\"\n    with fsspec.open(url, \"wb\") as f:\n        f.write(b\"hello\")\n        assert fn not in f.name\n        assert not os.listdir(tmp)\n\n    assert open(fn, \"rb\").read() == b\"hello\"\n\n\ndef test_clear():\n    import tempfile\n\n    origin = tempfile.mkdtemp()\n    cache1 = tempfile.mkdtemp()\n    data = b\"test data\"\n    f1 = os.path.join(origin, \"afile\")\n    with open(f1, \"wb\") as f:\n        f.write(data)\n\n    # populates first cache\n    fs = fsspec.filesystem(\"filecache\", target_protocol=\"file\", cache_storage=cache1)\n    assert fs.cat(f1) == data\n\n    assert \"cache\" in os.listdir(cache1)\n    assert len(os.listdir(cache1)) == 2\n    assert fs._check_file(f1)\n\n    fs.clear_cache()\n    assert not fs._check_file(f1)\n    assert len(os.listdir(cache1)) < 2\n\n\n@pytest.mark.parametrize(\"force_save_pickle\", [True, False])\ndef test_clear_expired(tmp_path, force_save_pickle):\n    def __ager(cache_fn, fn, del_fn=False):\n        \"\"\"\n        Modify the cache file to virtually add time lag to selected files.\n\n        Parameters\n        ---------\n        cache_fn: str\n            cache path\n        fn: str\n            file name to be modified\n        del_fn: bool\n            whether or not to delete 'fn' from cache details\n        \"\"\"\n        import pathlib\n        import time\n\n        if os.path.exists(cache_fn):\n            if force_save_pickle:\n                with open(cache_fn, \"rb\") as f:\n                    cached_files = pickle.load(f)\n            else:\n                with open(cache_fn, \"r\") as f:\n                    cached_files = json.load(f)\n            fn_posix = pathlib.Path(fn).as_posix()\n            cached_files[fn_posix][\"time\"] = cached_files[fn_posix][\"time\"] - 691200\n            assert os.access(cache_fn, os.W_OK), \"Cache is not writable\"\n            if del_fn:\n                del cached_files[fn_posix][\"fn\"]\n            if force_save_pickle:\n                with open(cache_fn, \"wb\") as f:\n                    pickle.dump(cached_files, f)\n            else:\n                with open(cache_fn, \"w\") as f:\n                    json.dump(cached_files, f)\n            time.sleep(1)\n\n    origin = tmp_path.joinpath(\"origin\")\n    cache1 = tmp_path.joinpath(\"cache1\")\n    cache2 = tmp_path.joinpath(\"cache2\")\n    cache3 = tmp_path.joinpath(\"cache3\")\n\n    origin.mkdir()\n    cache1.mkdir()\n    cache2.mkdir()\n    cache3.mkdir()\n\n    data = b\"test data\"\n    f1 = origin.joinpath(\"afile\")\n    f2 = origin.joinpath(\"bfile\")\n    f3 = origin.joinpath(\"cfile\")\n    f4 = origin.joinpath(\"dfile\")\n\n    with open(f1, \"wb\") as f:\n        f.write(data)\n    with open(f2, \"wb\") as f:\n        f.write(data)\n    with open(f3, \"wb\") as f:\n        f.write(data)\n    with open(f4, \"wb\") as f:\n        f.write(data)\n\n    # populates first cache\n    fs = fsspec.filesystem(\n        \"filecache\", target_protocol=\"file\", cache_storage=str(cache1), cache_check=1\n    )\n    fs._metadata._force_save_pickle = force_save_pickle\n    assert fs.cat(str(f1)) == data\n\n    # populates \"last\" cache if file not found in first one\n    fs = fsspec.filesystem(\n        \"filecache\",\n        target_protocol=\"file\",\n        cache_storage=[str(cache1), str(cache2)],\n        cache_check=1,\n    )\n    fs._metadata._force_save_pickle = force_save_pickle\n    assert fs.cat(str(f2)) == data\n    assert fs.cat(str(f3)) == data\n    assert len(os.listdir(cache2)) == 3\n\n    # force the expiration\n    cache_fn = os.path.join(fs.storage[-1], \"cache\")\n    __ager(cache_fn, f2)\n\n    # remove from cache2 the expired files\n    fs.clear_expired_cache()\n    assert len(os.listdir(cache2)) == 2\n\n    # check complete cleanup\n    __ager(cache_fn, f3)\n\n    fs.clear_expired_cache()\n    assert not fs._check_file(f2)\n    assert not fs._check_file(f3)\n    assert len(os.listdir(cache2)) < 2\n\n    # check cache1 to be untouched after cleaning\n    assert len(os.listdir(cache1)) == 2\n\n    # check cleaning with 'same_name' option enabled\n    fs = fsspec.filesystem(\n        \"filecache\",\n        target_protocol=\"file\",\n        cache_storage=[str(cache1), str(cache2), str(cache3)],\n        same_names=True,\n        cache_check=1,\n    )\n    fs._metadata._force_save_pickle = force_save_pickle\n    assert fs.cat(str(f4)) == data\n\n    cache_fn = os.path.join(fs.storage[-1], \"cache\")\n    __ager(cache_fn, f4)\n\n    fs.clear_expired_cache()\n    assert not fs._check_file(str(f4))\n\n    # check cache metadata lacking 'fn' raises RuntimeError.\n    fs = fsspec.filesystem(\n        \"filecache\",\n        target_protocol=\"file\",\n        cache_storage=str(cache1),\n        same_names=True,\n        cache_check=1,\n    )\n    fs._metadata._force_save_pickle = force_save_pickle\n    assert fs.cat(str(f1)) == data\n\n    cache_fn = os.path.join(fs.storage[-1], \"cache\")\n    __ager(cache_fn, f1, del_fn=True)\n\n    with pytest.raises(RuntimeError, match=\"Cache metadata does not contain 'fn' for\"):\n        fs.clear_expired_cache()\n\n\ndef test_pop():\n    import tempfile\n\n    origin = tempfile.mkdtemp()\n    cache1 = tempfile.mkdtemp()\n    cache2 = tempfile.mkdtemp()\n    data = b\"test data\"\n    f1 = os.path.join(origin, \"afile\")\n    f2 = os.path.join(origin, \"bfile\")\n    with open(f1, \"wb\") as f:\n        f.write(data)\n    with open(f2, \"wb\") as f:\n        f.write(data)\n\n    # populates first cache\n    fs = fsspec.filesystem(\"filecache\", target_protocol=\"file\", cache_storage=cache1)\n    fs.cat(f1)\n\n    # populates last cache if file not found in first cache\n    fs = fsspec.filesystem(\n        \"filecache\", target_protocol=\"file\", cache_storage=[cache1, cache2]\n    )\n    assert fs.cat(f2) == data\n    assert len(os.listdir(cache2)) == 2\n    assert fs._check_file(f1)\n    with pytest.raises(PermissionError):\n        fs.pop_from_cache(f1)\n    fs.pop_from_cache(f2)\n    fs.pop_from_cache(os.path.join(origin, \"uncached-file\"))\n    assert len(os.listdir(cache2)) == 1\n    assert not fs._check_file(f2)\n    assert fs._check_file(f1)\n\n\ndef test_write_pickle_context():\n    tmp = str(tempfile.mkdtemp())\n    fn = tmp + \"afile\"\n    url = f\"simplecache::file://{fn}\"\n    with fsspec.open(url, \"wb\") as f:\n        pickle.loads(pickle.dumps(f))\n        f.write(b\"hello \")\n        pickle.dumps(f)\n\n    with pytest.raises(ValueError):\n        pickle.dumps(f)\n\n    assert open(fn, \"rb\").read() == b\"hello \"\n\n\ndef test_blocksize(ftp_writable):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    with fs.open(\"/out_block\", \"wb\") as f:\n        f.write(b\"test\" * 4000)\n\n    fs = fsspec.filesystem(\n        \"blockcache\",\n        target_protocol=\"ftp\",\n        target_options={\"host\": host, \"port\": port, \"username\": user, \"password\": pw},\n    )\n\n    with fs.open(\"/out_block\", block_size=20) as f:\n        assert f.read(1) == b\"t\"\n    with pytest.raises(BlocksizeMismatchError):\n        fs.open(\"/out_block\", block_size=30)\n\n\ndef test_blockcache_multiinstance(ftp_writable):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    with fs.open(\"/one\", \"wb\") as f:\n        f.write(b\"test\" * 40)\n    with fs.open(\"/two\", \"wb\") as f:\n        f.write(b\"test\" * 40)\n    fs = fsspec.filesystem(\n        \"blockcache\",\n        target_protocol=\"ftp\",\n        target_options={\"host\": host, \"port\": port, \"username\": user, \"password\": pw},\n    )\n\n    with fs.open(\"/one\", block_size=20) as f:\n        assert f.read(1) == b\"t\"\n    fs2 = fsspec.filesystem(\n        \"blockcache\",\n        target_protocol=\"ftp\",\n        target_options={\"host\": host, \"port\": port, \"username\": user, \"password\": pw},\n        skip_instance_cache=True,\n        cache_storage=fs.storage,\n    )\n    assert fs2._metadata.cached_files  # loaded from metadata for \"one\"\n    with fs2.open(\"/two\", block_size=20) as f:\n        assert f.read(1) == b\"t\"\n    assert \"/two\" in fs2._metadata.cached_files[-1]\n    fs.save_cache()\n    assert list(fs._metadata.cached_files[-1]) == [\"/one\", \"/two\"]\n    assert list(fs2._metadata.cached_files[-1]) == [\"/one\", \"/two\"]\n\n\ndef test_metadata_save_blocked(ftp_writable, caplog):\n    import logging\n\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    with fs.open(\"/one\", \"wb\") as f:\n        f.write(b\"test\" * 40)\n    fs = fsspec.filesystem(\n        \"blockcache\",\n        target_protocol=\"ftp\",\n        target_options={\"host\": host, \"port\": port, \"username\": user, \"password\": pw},\n    )\n\n    with fs.open(\"/one\", block_size=20) as f:\n        assert f.read(1) == b\"t\"\n    fn = os.path.join(fs.storage[-1], \"cache\")\n    with caplog.at_level(logging.DEBUG):\n        with fs.open(\"/one\", block_size=20) as f:\n            f.seek(21)\n            assert f.read(1)\n            os.remove(fn)\n            os.mkdir(fn)\n    assert \"Cache saving failed while closing file\" in caplog.text\n    os.rmdir(fn)\n\n    def open_raise(*_, **__):\n        raise NameError\n\n    try:\n        # To simulate an interpreter shutdown we temporarily set an open function in the\n        # cache_metadata module which is used on the next attempt to save metadata.\n        with caplog.at_level(logging.DEBUG):\n            with fs.open(\"/one\", block_size=20) as f:\n                fsspec.implementations.cache_metadata.open = open_raise\n                f.seek(21)\n                assert f.read(1)\n    finally:\n        fsspec.implementations.cache_metadata.__dict__.pop(\"open\", None)\n    assert \"Cache save failed due to interpreter shutdown\" in caplog.text\n\n\n@pytest.mark.parametrize(\"impl\", [\"filecache\", \"simplecache\", \"blockcache\"])\ndef test_local_filecache_creates_dir_if_needed(impl):\n    import tempfile\n\n    original_location = tempfile.mkdtemp()\n    cache_location = tempfile.mkdtemp()\n    os.rmdir(cache_location)\n    assert not os.path.exists(cache_location)\n\n    original_file = os.path.join(original_location, \"afile\")\n    data = b\"test data\"\n    with open(original_file, \"wb\") as f:\n        f.write(data)\n\n    # we can access the file and read it\n    fs = fsspec.filesystem(impl, target_protocol=\"file\", cache_storage=cache_location)\n\n    with fs.open(original_file, \"rb\") as f:\n        data_in_cache = f.read()\n\n    assert os.path.exists(cache_location)\n\n    assert data_in_cache == data\n\n\n@pytest.mark.parametrize(\"toplevel\", [True, False])\n@pytest.mark.parametrize(\"impl\", [\"filecache\", \"simplecache\", \"blockcache\"])\ndef test_get_mapper(impl, toplevel):\n    import tempfile\n\n    original_location = tempfile.mkdtemp()\n    cache_location = tempfile.mkdtemp()\n    os.rmdir(cache_location)\n    original_file = os.path.join(original_location, \"afile\")\n    data = b\"test data\"\n    with open(original_file, \"wb\") as f:\n        f.write(data)\n\n    if toplevel:\n        m = fsspec.get_mapper(\n            f\"{impl}::file://{original_location}\",\n            **{impl: {\"cache_storage\": cache_location}},\n        )\n    else:\n        fs = fsspec.filesystem(\n            impl, target_protocol=\"file\", cache_storage=cache_location\n        )\n        m = fs.get_mapper(original_location)\n\n    assert m[\"afile\"] == data\n    assert os.listdir(cache_location)\n    assert m[\"afile\"] == data\n\n\ndef test_local_filecache_basic(local_filecache):\n    data, original_file, cache_location, fs = local_filecache\n\n    # reading from the file contains the right data\n    with fs.open(original_file, \"rb\") as f:\n        assert f.read() == data\n    assert \"cache\" in os.listdir(cache_location)\n\n    # the file in the location contains the right data\n    fn = list(fs._metadata.cached_files[-1].values())[0][\"fn\"]  # this is a hash value\n    assert fn in os.listdir(cache_location)\n    with open(os.path.join(cache_location, fn), \"rb\") as f:\n        assert f.read() == data\n\n    # still there when original file is removed (check=False)\n    os.remove(original_file)\n    with fs.open(original_file, \"rb\") as f:\n        assert f.read() == data\n\n\ndef test_local_filecache_does_not_change_when_original_data_changed(local_filecache):\n    old_data, original_file, cache_location, fs = local_filecache\n    new_data = b\"abc\"\n\n    with fs.open(original_file, \"rb\") as f:\n        assert f.read() == old_data\n\n    with open(original_file, \"wb\") as f:\n        f.write(new_data)\n\n    with fs.open(original_file, \"rb\") as f:\n        assert f.read() == old_data\n\n\ndef test_local_filecache_gets_from_original_if_cache_deleted(local_filecache):\n    old_data, original_file, cache_location, fs = local_filecache\n    new_data = b\"abc\"\n\n    with fs.open(original_file, \"rb\") as f:\n        assert f.read() == old_data\n\n    with open(original_file, \"wb\") as f:\n        f.write(new_data)\n\n    shutil.rmtree(cache_location)\n    assert os.path.exists(original_file)\n\n    with open(original_file, \"rb\") as f:\n        assert f.read() == new_data\n\n    with fs.open(original_file, \"rb\") as f:\n        assert f.read() == new_data\n\n    # the file in the location contains the right data\n    fn = list(fs._metadata.cached_files[-1].values())[0][\"fn\"]  # this is a hash value\n    assert fn in os.listdir(cache_location)\n    with open(os.path.join(cache_location, fn), \"rb\") as f:\n        assert f.read() == new_data\n\n\ndef test_local_filecache_with_new_cache_location_makes_a_new_copy(local_filecache):\n    import tempfile\n\n    data, original_file, old_cache_location, old_fs = local_filecache\n    new_cache_location = tempfile.mkdtemp()\n\n    with old_fs.open(original_file, \"rb\") as f:\n        assert f.read() == data\n\n    new_fs = fsspec.filesystem(\n        \"filecache\", target_protocol=\"file\", cache_storage=new_cache_location\n    )\n\n    with new_fs.open(original_file, \"rb\") as f:\n        assert f.read() == data\n\n    # the file in the location contains the right data\n    fn = list(new_fs._metadata.cached_files[-1].values())[0][\n        \"fn\"\n    ]  # this is a hash value\n    assert fn in os.listdir(old_cache_location)\n    assert fn in os.listdir(new_cache_location)\n\n    with open(os.path.join(new_cache_location, fn), \"rb\") as f:\n        assert f.read() == data\n\n\ndef test_filecache_multicache():\n    import tempfile\n\n    origin = tempfile.mkdtemp()\n    cache1 = tempfile.mkdtemp()\n    cache2 = tempfile.mkdtemp()\n    data = b\"test data\"\n    f1 = os.path.join(origin, \"afile\")\n    f2 = os.path.join(origin, \"bfile\")\n    with open(f1, \"wb\") as f:\n        f.write(data)\n    with open(f2, \"wb\") as f:\n        f.write(data * 2)\n\n    # populates first cache\n    fs = fsspec.filesystem(\"filecache\", target_protocol=\"file\", cache_storage=cache1)\n    assert fs.cat(f1) == data\n\n    assert len(os.listdir(cache1)) == 2  # cache and hashed afile\n    assert len(os.listdir(cache2)) == 0  # hasn't been initialized yet\n\n    # populates last cache if file not found in first cache\n    fs = fsspec.filesystem(\n        \"filecache\", target_protocol=\"file\", cache_storage=[cache1, cache2]\n    )\n\n    assert fs.cat(f1) == data\n    assert fs.cat(f2) == data * 2\n\n    assert \"cache\" in os.listdir(cache1)\n    assert \"cache\" in os.listdir(cache2)\n\n    cache1_contents = [f for f in os.listdir(cache1) if f != \"cache\"]\n    assert len(cache1_contents) == 1\n\n    with open(os.path.join(cache1, cache1_contents[0]), \"rb\") as f:\n        assert f.read() == data\n\n    cache2_contents = [f for f in os.listdir(cache2) if f != \"cache\"]\n    assert len(cache2_contents) == 1\n\n    with open(os.path.join(cache2, cache2_contents[0]), \"rb\") as f:\n        assert f.read() == data * 2\n\n\n@pytest.mark.parametrize(\"impl\", [\"filecache\", \"simplecache\"])\ndef test_filecache_multicache_with_same_file_different_data_reads_from_first(impl):\n    import tempfile\n\n    origin = tempfile.mkdtemp()\n    cache1 = tempfile.mkdtemp()\n    cache2 = tempfile.mkdtemp()\n    data = b\"test data\"\n    f1 = os.path.join(origin, \"afile\")\n    with open(f1, \"wb\") as f:\n        f.write(data)\n\n    # populate first cache\n    fs1 = fsspec.filesystem(impl, target_protocol=\"file\", cache_storage=cache1)\n    assert fs1.cat(f1) == data\n\n    with open(f1, \"wb\") as f:\n        f.write(data * 2)\n\n    # populate second cache\n    fs2 = fsspec.filesystem(impl, target_protocol=\"file\", cache_storage=cache2)\n\n    assert fs2.cat(f1) == data * 2\n\n    # the filenames in each cache are the same, but the data is different\n    assert sorted(os.listdir(cache1)) == sorted(os.listdir(cache2))\n\n    fs = fsspec.filesystem(impl, target_protocol=\"file\", cache_storage=[cache1, cache2])\n\n    assert fs.cat(f1) == data\n\n\ndef test_filecache_with_checks():\n    import time\n\n    origin = tempfile.mkdtemp()\n    cache1 = tempfile.mkdtemp()\n    data = b\"test data\"\n    f1 = os.path.join(origin, \"afile\")\n    with open(f1, \"wb\") as f:\n        f.write(data)\n\n    # populate first cache\n    fs = fsspec.filesystem(\n        \"filecache\", target_protocol=\"file\", cache_storage=cache1, expiry_time=0.1\n    )\n    fs2 = fsspec.filesystem(\n        \"filecache\", target_protocol=\"file\", cache_storage=cache1, check_files=True\n    )\n    assert fs.cat(f1) == data\n    assert fs2.cat(f1) == data\n\n    with open(f1, \"wb\") as f:\n        f.write(data * 2)\n\n    assert fs.cat(f1) == data  # does not change\n    assert fs2.cat(f1) == data * 2  # changed, since origin changed\n    with fs2.open(f1) as f:\n        assert f.read() == data * 2  # read also sees new data\n    time.sleep(0.11)  # allow cache details to expire\n    assert fs.cat(f1) == data * 2  # changed, since origin changed\n\n\n@pytest.mark.parametrize(\"impl\", [\"filecache\", \"simplecache\", \"blockcache\"])\n@pytest.mark.parametrize(\"fs\", [\"local\", \"multi\"], indirect=[\"fs\"])\ndef test_filecache_takes_fs_instance(impl, fs):\n    origin = tempfile.mkdtemp()\n    data = b\"test data\"\n    f1 = os.path.join(origin, \"afile\")\n    with open(f1, \"wb\") as f:\n        f.write(data)\n\n    fs2 = fsspec.filesystem(impl, fs=fs)\n\n    assert fs2.cat(f1) == data\n\n\n@pytest.mark.parametrize(\"impl\", [\"filecache\", \"simplecache\", \"blockcache\"])\n@pytest.mark.parametrize(\"fs\", [\"local\", \"multi\"], indirect=[\"fs\"])\ndef test_filecache_serialization(impl, fs):\n    fs1 = fsspec.filesystem(impl, fs=fs)\n    json1 = fs1.to_json()\n\n    assert fs1 is fsspec.AbstractFileSystem.from_json(json1)\n\n\ndef test_add_file_to_cache_after_save(local_filecache):\n    (data, original_file, cache_location, fs) = local_filecache\n\n    fs.save_cache()\n\n    fs.cat(original_file)\n    assert len(fs._metadata.cached_files[-1]) == 1\n\n    fs.save_cache()\n\n    fs2 = fsspec.filesystem(\n        \"filecache\",\n        target_protocol=\"file\",\n        cache_storage=cache_location,\n        do_not_use_cache_for_this_instance=True,  # cache is masking the issue\n    )\n    assert len(fs2._metadata.cached_files[-1]) == 1\n\n\ndef test_cached_open_close_read(ftp_writable):\n    # Regression test for <https://github.com/fsspec/filesystem_spec/issues/799>\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    with fs.open(\"/out_block\", \"wb\") as f:\n        f.write(b\"test\" * 4000)\n    fs = fsspec.filesystem(\n        \"cached\",\n        target_protocol=\"ftp\",\n        target_options={\"host\": host, \"port\": port, \"username\": user, \"password\": pw},\n    )\n    with fs.open(\"/out_block\", block_size=1024) as f:\n        pass\n    with fs.open(\"/out_block\", block_size=1024) as f:\n        assert f.read(1) == b\"t\"\n    # Regression test for <https://github.com/fsspec/filesystem_spec/issues/845>\n    assert fs._metadata.cached_files[-1][\"/out_block\"][\"blocks\"] == {0}\n\n\n@pytest.mark.parametrize(\"impl\", [\"filecache\", \"simplecache\"])\n@pytest.mark.parametrize(\"compression\", [\"gzip\", \"bz2\"])\ndef test_with_compression(impl, compression):\n    data = b\"123456789\"\n    tempdir = tempfile.mkdtemp()\n    cachedir = tempfile.mkdtemp()\n    fn = os.path.join(tempdir, \"data\")\n    f = compr[compression](open(fn, mode=\"wb\"), mode=\"w\")\n    f.write(data)\n    f.close()\n\n    with fsspec.open(\n        f\"{impl}::{fn}\",\n        \"rb\",\n        compression=compression,\n        **{impl: {\"same_names\": True, \"cache_storage\": cachedir}},\n    ) as f:\n        # stores original compressed file, uncompress on read\n        assert f.read() == data\n        assert \"data\" in os.listdir(cachedir)\n        assert open(os.path.join(cachedir, \"data\"), \"rb\").read() != data\n\n    cachedir = tempfile.mkdtemp()\n\n    with fsspec.open(\n        f\"{impl}::{fn}\",\n        \"rb\",\n        **{\n            impl: {\n                \"same_names\": True,\n                \"compression\": compression,\n                \"cache_storage\": cachedir,\n            }\n        },\n    ) as f:\n        # stores uncompressed data\n        assert f.read() == data\n        assert \"data\" in os.listdir(cachedir)\n        assert open(os.path.join(cachedir, \"data\"), \"rb\").read() == data\n\n\n@pytest.mark.parametrize(\"protocol\", [\"simplecache\", \"filecache\"])\ndef test_again(protocol):\n    fn = \"memory://afile\"\n    with fsspec.open(fn, \"wb\") as f:\n        f.write(b\"hello\")\n    d2 = tempfile.mkdtemp()\n    lurl = fsspec.open_local(f\"{protocol}::{fn}\", **{protocol: {\"cache_storage\": d2}})\n    assert os.path.exists(lurl)\n    assert d2 in lurl\n    assert open(lurl, \"rb\").read() == b\"hello\"\n\n    # remove cache dir\n    shutil.rmtree(d2)\n    assert not os.path.exists(lurl)\n\n    # gets recreated\n    lurl = fsspec.open_local(f\"{protocol}::{fn}\", **{protocol: {\"cache_storage\": d2}})\n    assert open(lurl, \"rb\").read() == b\"hello\"\n\n\n@pytest.mark.parametrize(\"protocol\", [\"simplecache\", \"filecache\"])\ndef test_multi_cache(protocol):\n    with fsspec.open_files(\"memory://file*\", \"wb\", num=2) as files:\n        for f in files:\n            f.write(b\"hello\")\n\n    d2 = tempfile.mkdtemp()\n    lurl = fsspec.open_local(\n        f\"{protocol}::memory://file*\",\n        mode=\"rb\",\n        **{protocol: {\"cache_storage\": d2, \"same_names\": True}},\n    )\n    assert all(d2 in u for u in lurl)\n    assert all(os.path.basename(f) in [\"file0\", \"file1\"] for f in lurl)\n    assert all(open(u, \"rb\").read() == b\"hello\" for u in lurl)\n\n    d2 = tempfile.mkdtemp()\n    lurl = fsspec.open_files(\n        f\"{protocol}::memory://file*\",\n        mode=\"rb\",\n        **{protocol: {\"cache_storage\": d2, \"same_names\": True}},\n    )\n    with lurl as files:\n        for f in files:\n            assert os.path.basename(f.name) in [\"file0\", \"file1\"]\n            assert f.read() == b\"hello\"\n    fs = fsspec.filesystem(\"memory\")\n    fs.store.clear()\n    with lurl as files:\n        for f in files:\n            assert os.path.basename(f.name) in [\"file0\", \"file1\"]\n            assert f.read() == b\"hello\"\n\n\n@pytest.mark.parametrize(\"protocol\", [\"simplecache\", \"filecache\", \"blockcache\"])\ndef test_multi_cat(protocol, ftp_writable):\n    host, port, user, pw = ftp_writable\n    fs = FTPFileSystem(host, port, user, pw)\n    for fn in (\"/file0\", \"/file1\"):\n        with fs.open(fn, \"wb\") as f:\n            f.write(b\"hello\")\n\n    d2 = tempfile.mkdtemp()\n    fs = fsspec.filesystem(protocol, storage=d2, fs=fs)\n    assert fs.cat(\"file*\") == {\"/file0\": b\"hello\", \"/file1\": b\"hello\"}\n\n\n@pytest.mark.parametrize(\"protocol\", [\"simplecache\", \"filecache\"])\ndef test_multi_cache_chain(protocol):\n    import zipfile\n\n    d = tempfile.mkdtemp()\n    fn = os.path.join(d, \"test.zip\")\n    zipfile.ZipFile(fn, mode=\"w\").open(\"test\", \"w\").write(b\"hello\")\n\n    with fsspec.open_files(f\"zip://test::{protocol}::file://{fn}\") as files:\n        assert d not in files[0]._fileobj._file.name\n        assert files[0].read() == b\"hello\"\n\n    # special test contains \"file:\" string\n    fn = os.path.join(d, \"file.zip\")\n    zipfile.ZipFile(fn, mode=\"w\").open(\"file\", \"w\").write(b\"hello\")\n    with fsspec.open_files(f\"zip://file::{protocol}::file://{fn}\") as files:\n        assert d not in files[0]._fileobj._file.name\n        assert files[0].read() == b\"hello\"\n\n\n@pytest.mark.parametrize(\"protocol\", [\"blockcache\", \"simplecache\", \"filecache\"])\ndef test_strip(protocol):\n    fs = fsspec.filesystem(protocol, target_protocol=\"memory\")\n    url1 = \"memory://afile\"\n    assert fs._strip_protocol(url1) == \"/afile\"\n    assert fs._strip_protocol(protocol + \"://afile\") == \"/afile\"\n    assert fs._strip_protocol(protocol + \"::memory://afile\") == \"/afile\"\n\n\n@pytest.mark.parametrize(\"protocol\", [\"simplecache\", \"filecache\"])\ndef test_cached_write(protocol):\n    d = tempfile.mkdtemp()\n    ofs = fsspec.open_files(f\"{protocol}::file://{d}/*.out\", mode=\"wb\", num=2)\n    with ofs as files:\n        for f in files:\n            assert isinstance(f, LocalTempFile)\n            f.write(b\"data\")\n            fn = f.name\n\n    assert sorted(os.listdir(d)) == [\"0.out\", \"1.out\"]\n    assert not os.path.exists(fn)\n\n\ndef test_expiry():\n    import time\n\n    d = tempfile.mkdtemp()\n    fs = fsspec.filesystem(\"memory\")\n    fn = \"/afile\"\n    fn0 = \"memory://afile\"\n    data = b\"hello\"\n    with fs.open(fn0, \"wb\") as f:\n        f.write(data)\n\n    fs = fsspec.filesystem(\n        \"filecache\",\n        fs=fs,\n        cache_storage=d,\n        check_files=False,\n        expiry_time=0.1,\n        same_names=True,\n    )\n\n    # get file\n    assert fs._check_file(fn0) is False\n    assert fs.open(fn0, mode=\"rb\").read() == data\n    start_time = fs._metadata.cached_files[-1][fn][\"time\"]\n\n    # cache time..\n    assert fs.last_cache - start_time < 0.19\n\n    # cache should have refreshed\n    time.sleep(0.01)\n\n    # file should still be valid... re-read\n    assert fs.open(fn0, mode=\"rb\").read() == data\n    detail, _ = fs._check_file(fn0)\n    assert detail[\"time\"] == start_time\n\n    time.sleep(0.11)\n    # file should still be invalid... re-read\n    assert fs._check_file(fn0) is False\n    assert fs.open(fn0, mode=\"rb\").read() == data\n    detail, _ = fs._check_file(fn0)\n    assert detail[\"time\"] - start_time > 0.09\n\n\ndef test_equality(tmpdir):\n    \"\"\"Test sane behaviour for equality and hashing.\n\n    Make sure that different CachingFileSystem only test equal to each other\n    when they should, and do not test equal to the filesystem they rely upon.\n    Similarly, make sure their hashes differ when they should and are equal\n    when they should not.\n\n    Related: GitHub#577, GitHub#578\n    \"\"\"\n    from fsspec.implementations.local import LocalFileSystem\n\n    lfs = LocalFileSystem()\n    dir1 = f\"{tmpdir}/raspberry\"\n    dir2 = f\"{tmpdir}/banana\"\n    cfs1 = CachingFileSystem(fs=lfs, cache_storage=dir1)\n    cfs2 = CachingFileSystem(fs=lfs, cache_storage=dir2)\n    cfs3 = CachingFileSystem(fs=lfs, cache_storage=dir2)\n    assert cfs1 == cfs1\n    assert cfs1 != cfs2\n    assert cfs1 != cfs3\n    assert cfs2 == cfs3\n    assert cfs1 != lfs\n    assert cfs2 != lfs\n    assert cfs3 != lfs\n    assert hash(lfs) != hash(cfs1)\n    assert hash(lfs) != hash(cfs2)\n    assert hash(lfs) != hash(cfs3)\n    assert hash(cfs1) != hash(cfs2)\n    assert hash(cfs1) != hash(cfs2)\n    assert hash(cfs2) == hash(cfs3)\n\n\ndef test_str():\n    \"\"\"Test that the str representation refers to correct class.\"\"\"\n    from fsspec.implementations.local import LocalFileSystem\n\n    lfs = LocalFileSystem()\n    cfs = CachingFileSystem(fs=lfs)\n    assert \"CachingFileSystem\" in str(cfs)\n\n\ndef test_getitems_errors(tmpdir):\n    tmpdir = str(tmpdir)\n    os.makedirs(os.path.join(tmpdir, \"afolder\"))\n    open(os.path.join(tmpdir, \"afile\"), \"w\").write(\"test\")\n    open(os.path.join(tmpdir, \"afolder\", \"anotherfile\"), \"w\").write(\"test2\")\n    m = fsspec.get_mapper(f\"file://{tmpdir}\")\n    assert m.getitems([\"afile\", \"bfile\"], on_error=\"omit\") == {\"afile\": b\"test\"}\n\n    # my code\n    m2 = fsspec.get_mapper(f\"simplecache::file://{tmpdir}\")\n    assert m2.getitems([\"afile\"], on_error=\"omit\") == {\"afile\": b\"test\"}  # works\n    assert m2.getitems([\"afile\", \"bfile\"], on_error=\"omit\") == {\n        \"afile\": b\"test\"\n    }  # throws KeyError\n\n    with pytest.raises(KeyError):\n        m.getitems([\"afile\", \"bfile\"])\n    out = m.getitems([\"afile\", \"bfile\"], on_error=\"return\")\n    assert isinstance(out[\"bfile\"], KeyError)\n    m = fsspec.get_mapper(f\"file://{tmpdir}\", missing_exceptions=())\n    assert m.getitems([\"afile\", \"bfile\"], on_error=\"omit\") == {\"afile\": b\"test\"}\n    with pytest.raises(FileNotFoundError):\n        m.getitems([\"afile\", \"bfile\"])\n\n\n@pytest.mark.parametrize(\"temp_cache\", [False, True])\ndef test_cache_dir_auto_deleted(temp_cache, tmpdir):\n    import gc\n\n    source = os.path.join(tmpdir, \"source\")\n    afile = os.path.join(source, \"afile\")\n    os.mkdir(source)\n    open(afile, \"w\").write(\"test\")\n\n    fs = fsspec.filesystem(\n        \"filecache\",\n        target_protocol=\"file\",\n        cache_storage=\"TMP\" if temp_cache else os.path.join(tmpdir, \"cache\"),\n        skip_instance_cache=True,  # Important to avoid fs itself being cached\n    )\n\n    cache_dir = fs.storage[-1]\n\n    # Force cache to be created\n    with fs.open(afile, \"rb\") as f:\n        assert f.read(5) == b\"test\"\n\n    # Confirm cache exists\n    local = fsspec.filesystem(\"file\")\n    assert local.exists(cache_dir)\n\n    # Delete file system\n    del fs\n    gc.collect()\n\n    # Ensure cache has been deleted, if it is temporary\n    if temp_cache:\n        assert not local.exists(cache_dir)\n    else:\n        assert local.exists(cache_dir)\n\n\n@pytest.mark.parametrize(\"protocol\", [\"filecache\", \"blockcache\", \"simplecache\"])\ndef test_cache_size(tmpdir, protocol):\n    if win and protocol == \"blockcache\":\n        pytest.skip(\"Windows file locking affects blockcache size tests\")\n\n    source = os.path.join(tmpdir, \"source\")\n    afile = os.path.join(source, \"afile\")\n    os.mkdir(source)\n    open(afile, \"w\").write(\"test\")\n\n    fs = fsspec.filesystem(protocol, target_protocol=\"file\")\n    empty_cache_size = fs.cache_size()\n\n    # Create cache\n    with fs.open(afile, \"rb\") as f:\n        assert f.read(5) == b\"test\"\n    single_file_cache_size = fs.cache_size()\n    assert single_file_cache_size > empty_cache_size\n\n    # Remove cached file but leave cache metadata file\n    fs.pop_from_cache(afile)\n    if win and protocol == \"filecache\":\n        assert empty_cache_size < fs.cache_size()\n    elif protocol != \"simplecache\":\n        assert empty_cache_size < fs.cache_size() < single_file_cache_size\n    else:\n        # simplecache never stores metadata\n        assert fs.cache_size() == single_file_cache_size\n\n    # Completely remove cache\n    fs.clear_cache()\n    if protocol != \"simplecache\":\n        assert fs.cache_size() == empty_cache_size\n    else:\n        # Whole cache directory has been deleted\n        assert fs.cache_size() == 0\n\n\ndef test_spurious_directory_issue1410(tmpdir):\n    import zipfile\n\n    os.chdir(tmpdir)\n    zipfile.ZipFile(\"dir.zip\", mode=\"w\").open(\"file.txt\", \"w\").write(b\"hello\")\n    fs = WholeFileCacheFileSystem(fs=ZipFileSystem(\"dir.zip\"))\n\n    assert len(os.listdir()) == 1\n    with fs.open(\"/file.txt\", \"rb\"):\n        pass\n\n    # There was a bug reported in issue #1410 in which a directory\n    # would be created and the next assertion would fail.\n    assert len(os.listdir()) == 1\n    assert fs._parent(\"/any/path\") == \"any\"  # correct for ZIP, which has no leading /\n\n\ndef test_write_transaction(tmpdir, m, monkeypatch):\n    called = [0]\n    orig = m.put\n\n    def patched_put(*args, **kwargs):\n        called[0] += 1\n        orig(*args, **kwargs)\n\n    monkeypatch.setattr(m, \"put\", patched_put)\n    tmpdir = str(tmpdir)\n    fs, _ = fsspec.core.url_to_fs(\"simplecache::memory://\", cache_storage=tmpdir)\n    with fs.transaction:\n        fs.pipe(\"myfile\", b\"1\")\n        fs.pipe(\"otherfile\", b\"2\")\n        fs.pipe(\"deep/dir/otherfile\", b\"3\")\n        with fs.open(\"blarh\", \"wb\") as f:\n            f.write(b\"ff\")\n        assert not m.find(\"\")\n\n        assert fs.info(\"otherfile\")[\"size\"] == 1\n        assert fs.info(\"deep\")[\"type\"] == \"directory\"\n        assert fs.isdir(\"deep\")\n        assert fs.ls(\"deep\", detail=False) == [\"/deep/dir\"]\n\n    assert m.cat(\"myfile\") == b\"1\"\n    assert m.cat(\"otherfile\") == b\"2\"\n    assert called[0] == 1  # copy was done in one go\n\n\ndef test_filecache_write(tmpdir, m):\n    fs = fsspec.filesystem(\n        \"filecache\", target_protocol=\"memory\", cache_storage=str(tmpdir)\n    )\n    fn = \"sample_file_in_mem.txt\"\n    data = \"hello world from memory\"\n    with fs.open(fn, \"w\") as f:\n        assert not m.exists(fn)\n        f.write(data)\n\n    assert m.cat(fn) == data.encode()\n    assert fs.cat(fn) == data.encode()\n", "fsspec/implementations/tests/test_git.py": "import os\nimport shutil\nimport subprocess\nimport tempfile\n\nimport pytest\n\nimport fsspec\nfrom fsspec.implementations.local import make_path_posix\n\npygit2 = pytest.importorskip(\"pygit2\")\n\n\n@pytest.fixture()\ndef repo():\n    orig_dir = os.getcwd()\n    d = tempfile.mkdtemp()\n    try:\n        os.chdir(d)\n        subprocess.call(\"git init -b master\", shell=True, cwd=d)\n        subprocess.call(\"git init -b master\", shell=True, cwd=d)\n        subprocess.call('git config user.email \"you@example.com\"', shell=True, cwd=d)\n        subprocess.call('git config user.name \"Your Name\"', shell=True, cwd=d)\n        open(os.path.join(d, \"file1\"), \"wb\").write(b\"data0\")\n        subprocess.call(\"git add file1\", shell=True, cwd=d)\n        subprocess.call('git commit -m \"init\"', shell=True, cwd=d)\n        sha = open(os.path.join(d, \".git/refs/heads/master\"), \"r\").read().strip()\n        open(os.path.join(d, \"file1\"), \"wb\").write(b\"data00\")\n        subprocess.check_output('git commit -a -m \"tagger\"', shell=True, cwd=d)\n        subprocess.call('git tag -a thetag -m \"make tag\"', shell=True, cwd=d)\n        open(os.path.join(d, \"file2\"), \"wb\").write(b\"data000\")\n        subprocess.call(\"git add file2\", shell=True)\n        subprocess.call('git commit -m \"master tip\"', shell=True, cwd=d)\n        subprocess.call(\"git checkout -b abranch\", shell=True, cwd=d)\n        os.mkdir(\"inner\")\n        open(os.path.join(d, \"inner\", \"file1\"), \"wb\").write(b\"data3\")\n        subprocess.call(\"git add inner/file1\", shell=True, cwd=d)\n        subprocess.call('git commit -m \"branch tip\"', shell=True, cwd=d)\n        os.chdir(orig_dir)\n        yield d, sha\n    finally:\n        os.chdir(orig_dir)\n        shutil.rmtree(d)\n\n\ndef test_refs(repo):\n    d, sha = repo\n    with fsspec.open(\"git://file1\", path=d, ref=sha) as f:\n        assert f.read() == b\"data0\"\n\n    with fsspec.open(\"git://file1\", path=d, ref=\"thetag\") as f:\n        assert f.read() == b\"data00\"\n\n    with fsspec.open(\"git://file2\", path=d, ref=\"master\") as f:\n        assert f.read() == b\"data000\"\n\n    with fsspec.open(\"git://file2\", path=d, ref=None) as f:\n        assert f.read() == b\"data000\"\n\n    with fsspec.open(\"git://inner/file1\", path=d, ref=\"abranch\") as f:\n        assert f.read() == b\"data3\"\n\n\ndef test_url(repo):\n    d, sha = repo\n    fs, _, paths = fsspec.core.get_fs_token_paths(f\"git://file1::file://{d}\")\n    assert make_path_posix(d) in make_path_posix(fs.repo.path)\n    assert paths == [\"file1\"]\n    with fsspec.open(f\"git://file1::file://{d}\") as f:\n        assert f.read() == b\"data00\"\n\n    fs, _, paths = fsspec.core.get_fs_token_paths(f\"git://{d}:master@file1\")\n    assert make_path_posix(d) in make_path_posix(fs.repo.path)\n    assert paths == [\"file1\"]\n    with fsspec.open(f\"git://{d}:master@file1\") as f:\n        assert f.read() == b\"data00\"\n", "fsspec/implementations/tests/test_memory.py": "import os\nfrom pathlib import PurePosixPath, PureWindowsPath\n\nimport pytest\n\nfrom fsspec.implementations.local import LocalFileSystem, make_path_posix\n\n\ndef test_1(m):\n    m.touch(\"/somefile\")  # NB: is found with or without initial /\n    m.touch(\"afiles/and/another\")\n    files = m.find(\"\")\n    assert files == [\"/afiles/and/another\", \"/somefile\"]\n\n    files = sorted(m.get_mapper())\n    assert files == [\"afiles/and/another\", \"somefile\"]\n\n\ndef test_strip(m):\n    assert m._strip_protocol(\"\") == \"\"\n    assert m._strip_protocol(\"memory://\") == \"\"\n    assert m._strip_protocol(\"afile\") == \"/afile\"\n    assert m._strip_protocol(\"/b/c\") == \"/b/c\"\n    assert m._strip_protocol(\"/b/c/\") == \"/b/c\"\n\n\ndef test_ls(m):\n    m.mkdir(\"/dir\")\n    m.mkdir(\"/dir/dir1\")\n\n    m.touch(\"/dir/afile\")\n    m.touch(\"/dir/dir1/bfile\")\n    m.touch(\"/dir/dir1/cfile\")\n\n    assert m.ls(\"/\", False) == [\"/dir\"]\n    assert m.ls(\"/dir\", False) == [\"/dir/afile\", \"/dir/dir1\"]\n    assert m.ls(\"/dir\", True)[0][\"type\"] == \"file\"\n    assert m.ls(\"/dir\", True)[1][\"type\"] == \"directory\"\n    assert m.ls(\"/dir/afile\", False) == [\"/dir/afile\"]\n    assert m.ls(\"/dir/afile\", True)[0][\"type\"] == \"file\"\n\n    assert len(m.ls(\"/dir/dir1\")) == 2\n    assert len(m.ls(\"/dir/afile\")) == 1\n\n\ndef test_directories(m):\n    m.mkdir(\"outer/inner\")\n    assert m.info(\"outer/inner\")[\"type\"] == \"directory\"\n\n    assert m.ls(\"outer\")\n    assert m.ls(\"outer/inner\") == []\n\n    with pytest.raises(OSError):\n        m.rmdir(\"outer\")\n\n    m.rmdir(\"outer/inner\")\n    m.rmdir(\"outer\")\n\n    assert not m.store\n\n\ndef test_exists_isdir_isfile(m):\n    m.mkdir(\"/root\")\n    m.touch(\"/root/a\")\n\n    assert m.exists(\"/root\")\n    assert m.isdir(\"/root\")\n    assert not m.isfile(\"/root\")\n\n    assert m.exists(\"/root/a\")\n    assert m.isfile(\"/root/a\")\n    assert not m.isdir(\"/root/a\")\n\n    assert not m.exists(\"/root/not-exists\")\n    assert not m.isfile(\"/root/not-exists\")\n    assert not m.isdir(\"/root/not-exists\")\n\n    m.rm(\"/root/a\")\n    m.rmdir(\"/root\")\n\n    assert not m.exists(\"/root\")\n\n    m.touch(\"/a/b\")\n    assert m.isfile(\"/a/b\")\n\n    assert m.exists(\"/a\")\n    assert m.isdir(\"/a\")\n    assert not m.isfile(\"/a\")\n\n\ndef test_touch(m):\n    m.touch(\"/root/a\")\n    with pytest.raises(FileExistsError):\n        m.touch(\"/root/a/b\")\n    with pytest.raises(FileExistsError):\n        m.touch(\"/root/a/b/c\")\n    assert not m.exists(\"/root/a/b/\")\n\n\ndef test_mv_recursive(m):\n    m.mkdir(\"src\")\n    m.touch(\"src/file.txt\")\n    m.mv(\"src\", \"dest\", recursive=True)\n    assert m.exists(\"dest/file.txt\")\n    assert not m.exists(\"src\")\n\n\ndef test_mv_same_paths(m):\n    m.mkdir(\"src\")\n    m.touch(\"src/file.txt\")\n    m.mv(\"src\", \"src\", recursive=True)\n    assert m.exists(\"src/file.txt\")\n\n\ndef test_rm_no_pseudo_dir(m):\n    m.touch(\"/dir1/dir2/file\")\n    m.rm(\"/dir1\", recursive=True)\n    assert not m.exists(\"/dir1/dir2/file\")\n    assert not m.exists(\"/dir1/dir2\")\n    assert not m.exists(\"/dir1\")\n\n    with pytest.raises(FileNotFoundError):\n        m.rm(\"/dir1\", recursive=True)\n\n\ndef test_rewind(m):\n    # https://github.com/fsspec/filesystem_spec/issues/349\n    with m.open(\"src/file.txt\", \"w\") as f:\n        f.write(\"content\")\n    with m.open(\"src/file.txt\") as f:\n        assert f.tell() == 0\n\n\ndef test_empty_raises(m):\n    with pytest.raises(FileNotFoundError):\n        m.ls(\"nonexistent\")\n\n    with pytest.raises(FileNotFoundError):\n        m.info(\"nonexistent\")\n\n\ndef test_dir_errors(m):\n    m.mkdir(\"/first\")\n\n    with pytest.raises(FileExistsError):\n        m.mkdir(\"/first\")\n    with pytest.raises(FileExistsError):\n        m.makedirs(\"/first\", exist_ok=False)\n    m.makedirs(\"/first\", exist_ok=True)\n    m.makedirs(\"/first/second/third\")\n    assert \"/first/second\" in m.pseudo_dirs\n\n    m.touch(\"/afile\")\n    with pytest.raises(NotADirectoryError):\n        m.mkdir(\"/afile/nodir\")\n\n\ndef test_no_rewind_append_mode(m):\n    # https://github.com/fsspec/filesystem_spec/issues/349\n    with m.open(\"src/file.txt\", \"w\") as f:\n        f.write(\"content\")\n    with m.open(\"src/file.txt\", \"a\") as f:\n        assert f.tell() == 7\n\n\ndef test_moves(m):\n    m.touch(\"source.txt\")\n    m.mv(\"source.txt\", \"target.txt\")\n\n    m.touch(\"source2.txt\")\n    m.mv(\"source2.txt\", \"target2.txt\", recursive=True)\n    assert m.find(\"\") == [\"/target.txt\", \"/target2.txt\"]\n\n\ndef test_rm_reursive_empty_subdir(m):\n    # https://github.com/fsspec/filesystem_spec/issues/500\n    m.mkdir(\"recdir\")\n    m.mkdir(\"recdir/subdir2\")\n    m.rm(\"recdir/\", recursive=True)\n    assert not m.exists(\"dir\")\n\n\ndef test_seekable(m):\n    fn0 = \"foo.txt\"\n    with m.open(fn0, \"wb\") as f:\n        f.write(b\"data\")\n\n    f = m.open(fn0, \"rt\")\n    assert f.seekable(), \"file is not seekable\"\n    f.seek(1)\n    assert f.read(1) == \"a\"\n    assert f.tell() == 2\n\n\n# https://github.com/fsspec/filesystem_spec/issues/1425\n@pytest.mark.parametrize(\"mode\", [\"r\", \"rb\", \"w\", \"wb\", \"ab\", \"r+b\"])\ndef test_open_mode(m, mode):\n    filename = \"mode.txt\"\n    m.touch(filename)\n    with m.open(filename, mode=mode) as _:\n        pass\n\n\ndef test_remove_all(m):\n    m.touch(\"afile\")\n    m.rm(\"/\", recursive=True)\n    assert not m.ls(\"/\")\n\n\ndef test_cp_directory_recursive(m):\n    # https://github.com/fsspec/filesystem_spec/issues/1062\n    # Recursive cp/get/put of source directory into non-existent target directory.\n    src = \"/src\"\n    src_file = src + \"/file\"\n    m.mkdir(src)\n    m.touch(src_file)\n\n    target = \"/target\"\n\n    # cp without slash\n    assert not m.exists(target)\n    for loop in range(2):\n        m.cp(src, target, recursive=True)\n        assert m.isdir(target)\n\n        if loop == 0:\n            correct = [target + \"/file\"]\n            assert m.find(target) == correct\n        else:\n            correct = [target + \"/file\", target + \"/src/file\"]\n            assert sorted(m.find(target)) == correct\n\n    m.rm(target, recursive=True)\n\n    # cp with slash\n    assert not m.exists(target)\n    for loop in range(2):\n        m.cp(src + \"/\", target, recursive=True)\n        assert m.isdir(target)\n        correct = [target + \"/file\"]\n        assert m.find(target) == correct\n\n\ndef test_get_directory_recursive(m, tmpdir):\n    # https://github.com/fsspec/filesystem_spec/issues/1062\n    # Recursive cp/get/put of source directory into non-existent target directory.\n    src = \"/src\"\n    src_file = src + \"/file\"\n    m.mkdir(src)\n    m.touch(src_file)\n\n    target = os.path.join(tmpdir, \"target\")\n    target_fs = LocalFileSystem()\n\n    # get without slash\n    assert not target_fs.exists(target)\n    for loop in range(2):\n        m.get(src, target, recursive=True)\n        assert target_fs.isdir(target)\n\n        if loop == 0:\n            correct = [make_path_posix(os.path.join(target, \"file\"))]\n            assert target_fs.find(target) == correct\n        else:\n            correct = [\n                make_path_posix(os.path.join(target, \"file\")),\n                make_path_posix(os.path.join(target, \"src\", \"file\")),\n            ]\n            assert sorted(target_fs.find(target)) == correct\n\n    target_fs.rm(target, recursive=True)\n\n    # get with slash\n    assert not target_fs.exists(target)\n    for loop in range(2):\n        m.get(src + \"/\", target, recursive=True)\n        assert target_fs.isdir(target)\n        correct = [make_path_posix(os.path.join(target, \"file\"))]\n        assert target_fs.find(target) == correct\n\n\ndef test_put_directory_recursive(m, tmpdir):\n    # https://github.com/fsspec/filesystem_spec/issues/1062\n    # Recursive cp/get/put of source directory into non-existent target directory.\n    src = os.path.join(tmpdir, \"src\")\n    src_file = os.path.join(src, \"file\")\n    source_fs = LocalFileSystem()\n    source_fs.mkdir(src)\n    source_fs.touch(src_file)\n\n    target = \"/target\"\n\n    # put without slash\n    assert not m.exists(target)\n    for loop in range(2):\n        m.put(src, target, recursive=True)\n        assert m.isdir(target)\n\n        if loop == 0:\n            correct = [target + \"/file\"]\n            assert m.find(target) == correct\n        else:\n            correct = [target + \"/file\", target + \"/src/file\"]\n            assert sorted(m.find(target)) == correct\n\n    m.rm(target, recursive=True)\n\n    # put with slash\n    assert not m.exists(target)\n    for loop in range(2):\n        m.put(src + \"/\", target, recursive=True)\n        assert m.isdir(target)\n        correct = [target + \"/file\"]\n        assert m.find(target) == correct\n\n\ndef test_cp_empty_directory(m):\n    # https://github.com/fsspec/filesystem_spec/issues/1198\n    # cp/get/put of empty directory.\n    empty = \"/src/empty\"\n    m.mkdir(empty)\n\n    target = \"/target\"\n    m.mkdir(target)\n\n    # cp without slash, target directory exists\n    assert m.isdir(target)\n    m.cp(empty, target)\n    assert m.find(target, withdirs=True) == [target]\n\n    # cp with slash, target directory exists\n    assert m.isdir(target)\n    m.cp(empty + \"/\", target)\n    assert m.find(target, withdirs=True) == [target]\n\n    m.rmdir(target)\n\n    # cp without slash, target directory doesn't exist\n    assert not m.isdir(target)\n    m.cp(empty, target)\n    assert not m.isdir(target)\n\n    # cp with slash, target directory doesn't exist\n    assert not m.isdir(target)\n    m.cp(empty + \"/\", target)\n    assert not m.isdir(target)\n\n\ndef test_cp_two_files(m):\n    src = \"/src\"\n    file0 = src + \"/file0\"\n    file1 = src + \"/file1\"\n    m.mkdir(src)\n    m.touch(file0)\n    m.touch(file1)\n\n    target = \"/target\"\n    assert not m.exists(target)\n\n    m.cp([file0, file1], target)\n\n    assert m.isdir(target)\n    assert sorted(m.find(target)) == [\n        \"/target/file0\",\n        \"/target/file1\",\n    ]\n\n\ndef test_open_path_posix(m):\n    path = PurePosixPath(\"/myfile/foo/bar\")\n    with m.open(path, \"wb\") as f:\n        f.write(b\"some\\nlines\\nof\\ntext\")\n\n    assert m.read_text(path) == \"some\\nlines\\nof\\ntext\"\n\n\ndef test_open_path_windows(m):\n    path = PureWindowsPath(\"C:\\\\myfile\\\\foo\\\\bar\")\n    with m.open(path, \"wb\") as f:\n        f.write(b\"some\\nlines\\nof\\ntext\")\n\n    assert m.read_text(path) == \"some\\nlines\\nof\\ntext\"\n", "fsspec/implementations/tests/test_data.py": "import fsspec\n\n\ndef test_1():\n    with fsspec.open(\"data:text/plain;base64,SGVsbG8sIFdvcmxkIQ==\") as f:\n        assert f.read() == b\"Hello, World!\"\n\n    with fsspec.open(\"data:,Hello%2C%20World%21\") as f:\n        assert f.read() == b\"Hello, World!\"\n\n\ndef test_info():\n    fs = fsspec.filesystem(\"data\")\n    info = fs.info(\"data:text/html,%3Ch1%3EHello%2C%20World%21%3C%2Fh1%3E\")\n    assert info == {\n        \"name\": \"%3Ch1%3EHello%2C%20World%21%3C%2Fh1%3E\",\n        \"size\": 22,\n        \"type\": \"file\",\n        \"mimetype\": \"text/html\",\n    }\n", "fsspec/implementations/tests/test_reference.py": "import json\nimport os\n\nimport pytest\n\nimport fsspec\nfrom fsspec.implementations.local import LocalFileSystem\nfrom fsspec.implementations.reference import (\n    LazyReferenceMapper,\n    ReferenceFileSystem,\n    ReferenceNotReachable,\n)\nfrom fsspec.tests.conftest import data, realfile, reset_files, server, win  # noqa: F401\n\n\ndef test_simple(server):  # noqa: F811\n    # The dictionary in refs may be dumped with a different separator\n    # depending on whether json or ujson is imported\n    from fsspec.implementations.reference import json as json_impl\n\n    refs = {\n        \"a\": b\"data\",\n        \"b\": (realfile, 0, 5),\n        \"c\": (realfile, 1, 5),\n        \"d\": b\"base64:aGVsbG8=\",\n        \"e\": {\"key\": \"value\"},\n    }\n    h = fsspec.filesystem(\"http\")\n    fs = fsspec.filesystem(\"reference\", fo=refs, fs=h)\n\n    assert fs.cat(\"a\") == b\"data\"\n    assert fs.cat(\"b\") == data[:5]\n    assert fs.cat(\"c\") == data[1 : 1 + 5]\n    assert fs.cat(\"d\") == b\"hello\"\n    assert fs.cat(\"e\") == json_impl.dumps(refs[\"e\"]).encode(\"utf-8\")\n    with fs.open(\"d\", \"rt\") as f:\n        assert f.read(2) == \"he\"\n\n\ndef test_simple_ver1(server):  # noqa: F811\n    # The dictionary in refs may be dumped with a different separator\n    # depending on whether json or ujson is imported\n    from fsspec.implementations.reference import json as json_impl\n\n    in_data = {\n        \"version\": 1,\n        \"refs\": {\n            \"a\": b\"data\",\n            \"b\": (realfile, 0, 5),\n            \"c\": (realfile, 1, 5),\n            \"d\": b\"base64:aGVsbG8=\",\n            \"e\": {\"key\": \"value\"},\n        },\n    }\n    h = fsspec.filesystem(\"http\")\n    fs = fsspec.filesystem(\"reference\", fo=in_data, fs=h)\n\n    assert fs.cat(\"a\") == b\"data\"\n    assert fs.cat(\"b\") == data[:5]\n    assert fs.cat(\"c\") == data[1 : 1 + 5]\n    assert fs.cat(\"d\") == b\"hello\"\n    assert fs.cat(\"e\") == json_impl.dumps(in_data[\"refs\"][\"e\"]).encode(\"utf-8\")\n    with fs.open(\"d\", \"rt\") as f:\n        assert f.read(2) == \"he\"\n\n\ndef test_target_options(m):\n    m.pipe(\"data/0\", b\"hello\")\n    refs = {\"a\": [\"memory://data/0\"]}\n    fn = \"memory://refs.json.gz\"\n    with fsspec.open(fn, \"wt\", compression=\"gzip\") as f:\n        json.dump(refs, f)\n\n    fs = fsspec.filesystem(\"reference\", fo=fn, target_options={\"compression\": \"gzip\"})\n    assert fs.cat(\"a\") == b\"hello\"\n\n\ndef test_ls(server):  # noqa: F811\n    refs = {\"a\": b\"data\", \"b\": (realfile, 0, 5), \"c/d\": (realfile, 1, 6)}\n    h = fsspec.filesystem(\"http\")\n    fs = fsspec.filesystem(\"reference\", fo=refs, fs=h)\n\n    assert fs.ls(\"\", detail=False) == [\"a\", \"b\", \"c\"]\n    assert {\"name\": \"c\", \"type\": \"directory\", \"size\": 0} in fs.ls(\"\", detail=True)\n    assert fs.find(\"\") == [\"a\", \"b\", \"c/d\"]\n    assert fs.find(\"\", withdirs=True) == [\"a\", \"b\", \"c\", \"c/d\"]\n    assert fs.find(\"c\", detail=True) == {\n        \"c/d\": {\"name\": \"c/d\", \"size\": 6, \"type\": \"file\"}\n    }\n\n\ndef test_nested_dirs_ls():\n    # issue #1430\n    refs = {\"a\": \"A\", \"B/C/b\": \"B\", \"B/C/d\": \"d\", \"B/_\": \"_\"}\n    fs = fsspec.filesystem(\"reference\", fo=refs)\n    assert len(fs.ls(\"\")) == 2\n    assert {e[\"name\"] for e in fs.ls(\"\")} == {\"a\", \"B\"}\n    assert len(fs.ls(\"B\")) == 2\n    assert {e[\"name\"] for e in fs.ls(\"B\")} == {\"B/C\", \"B/_\"}\n\n\ndef test_info(server):  # noqa: F811\n    refs = {\n        \"a\": b\"data\",\n        \"b\": (realfile, 0, 5),\n        \"c/d\": (realfile, 1, 6),\n        \"e\": (realfile,),\n    }\n    h = fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true\"})\n    fs = fsspec.filesystem(\"reference\", fo=refs, fs=h)\n    assert fs.size(\"a\") == 4\n    assert fs.size(\"b\") == 5\n    assert fs.size(\"c/d\") == 6\n    assert fs.info(\"e\")[\"size\"] == len(data)\n\n\ndef test_mutable(server, m):\n    refs = {\n        \"a\": b\"data\",\n        \"b\": (realfile, 0, 5),\n        \"c/d\": (realfile, 1, 6),\n        \"e\": (realfile,),\n    }\n    h = fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true\"})\n    fs = fsspec.filesystem(\"reference\", fo=refs, fs=h)\n    fs.rm(\"a\")\n    assert not fs.exists(\"a\")\n\n    bin_data = b\"bin data\"\n    fs.pipe(\"aa\", bin_data)\n    assert fs.cat(\"aa\") == bin_data\n\n    fs.save_json(\"memory://refs.json\")\n    assert m.exists(\"refs.json\")\n\n    fs = fsspec.filesystem(\"reference\", fo=\"memory://refs.json\", remote_protocol=\"http\")\n    assert not fs.exists(\"a\")\n    assert fs.cat(\"aa\") == bin_data\n\n\ndef test_put_get(tmpdir):\n    d1 = f\"{tmpdir}/d1\"\n    os.mkdir(d1)\n    with open(f\"{d1}/a\", \"wb\") as f:\n        f.write(b\"1\")\n    with open(f\"{d1}/b\", \"wb\") as f:\n        f.write(b\"2\")\n    d2 = f\"{tmpdir}/d2\"\n\n    fs = fsspec.filesystem(\"reference\", fo={}, remote_protocol=\"file\")\n    fs.put(d1, \"out\", recursive=True)\n\n    fs.get(\"out\", d2, recursive=True)\n    assert open(f\"{d2}/a\", \"rb\").read() == b\"1\"\n    assert open(f\"{d2}/b\", \"rb\").read() == b\"2\"\n\n\ndef test_put_get_single(tmpdir):\n    d1 = f\"{tmpdir}/f1\"\n    d2 = f\"{tmpdir}/f2\"\n    with open(d1, \"wb\") as f:\n        f.write(b\"1\")\n\n    # skip instance cache since this is the same kwargs as previous test\n    fs = fsspec.filesystem(\n        \"reference\", fo={}, remote_protocol=\"file\", skip_instance_cache=True\n    )\n    fs.put_file(d1, \"out\")\n\n    fs.get_file(\"out\", d2)\n    assert open(d2, \"rb\").read() == b\"1\"\n    fs.pipe({\"hi\": b\"data\"})\n    assert fs.cat(\"hi\") == b\"data\"\n\n\ndef test_defaults(server):  # noqa: F811\n    refs = {\"a\": b\"data\", \"b\": (None, 0, 5)}\n    fs = fsspec.filesystem(\n        \"reference\",\n        fo=refs,\n        target_protocol=\"http\",\n        target=realfile,\n        remote_protocol=\"http\",\n    )\n\n    assert fs.cat(\"a\") == b\"data\"\n    assert fs.cat(\"b\") == data[:5]\n\n\njdata = \"\"\"{\n    \"metadata\": {\n        \".zattrs\": {\n            \"Conventions\": \"UGRID-0.9.0\"\n        },\n        \".zgroup\": {\n            \"zarr_format\": 2\n        },\n        \"adcirc_mesh/.zarray\": {\n            \"chunks\": [\n                1\n            ],\n            \"dtype\": \"<i4\",\n            \"shape\": [\n                1\n            ],\n            \"zarr_format\": 2\n        },\n        \"adcirc_mesh/.zattrs\": {\n            \"_ARRAY_DIMENSIONS\": [\n                \"mesh\"\n            ],\n            \"cf_role\": \"mesh_topology\"\n        },\n        \"adcirc_mesh/.zchunkstore\": {\n            \"adcirc_mesh/0\": {\n                \"offset\": 8928,\n                \"size\": 4\n            },\n            \"source\": {\n                \"array_name\": \"/adcirc_mesh\",\n                \"uri\": \"https://url\"\n            }\n        }\n    },\n    \"zarr_consolidated_format\": 1\n}\n\"\"\"\n\n\ndef test_spec1_expand():\n    pytest.importorskip(\"jinja2\")\n    from fsspec.implementations.reference import json as json_impl\n\n    in_data = {\n        \"version\": 1,\n        \"templates\": {\"u\": \"server.domain/path\", \"f\": \"{{c}}\"},\n        \"gen\": [\n            {\n                \"key\": \"gen_key{{i}}\",\n                \"url\": \"http://{{u}}_{{i}}\",\n                \"offset\": \"{{(i + 1) * 1000}}\",\n                \"length\": \"1000\",\n                \"dimensions\": {\"i\": {\"stop\": 5}},\n            },\n            {\n                \"key\": \"gen_key{{i}}\",\n                \"url\": \"http://{{u}}_{{i}}\",\n                \"dimensions\": {\"i\": {\"start\": 5, \"stop\": 7}},\n            },\n        ],\n        \"refs\": {\n            \"key0\": \"data\",\n            \"key1\": [\"http://target_url\", 10000, 100],\n            \"key2\": [\"http://{{u}}\", 10000, 100],\n            \"key3\": [\"http://{{f(c='text')}}\", 10000, 100],\n            \"key4\": [\"http://target_url\"],\n            \"key5\": {\"key\": \"value\"},\n        },\n    }\n    fs = fsspec.filesystem(\n        \"reference\", fo=in_data, target_protocol=\"http\", simple_templates=False\n    )\n    assert fs.references == {\n        \"key0\": \"data\",\n        \"key1\": [\"http://target_url\", 10000, 100],\n        \"key2\": [\"http://server.domain/path\", 10000, 100],\n        \"key3\": [\"http://text\", 10000, 100],\n        \"key4\": [\"http://target_url\"],\n        \"key5\": json_impl.dumps(in_data[\"refs\"][\"key5\"]),\n        \"gen_key0\": [\"http://server.domain/path_0\", 1000, 1000],\n        \"gen_key1\": [\"http://server.domain/path_1\", 2000, 1000],\n        \"gen_key2\": [\"http://server.domain/path_2\", 3000, 1000],\n        \"gen_key3\": [\"http://server.domain/path_3\", 4000, 1000],\n        \"gen_key4\": [\"http://server.domain/path_4\", 5000, 1000],\n        \"gen_key5\": [\"http://server.domain/path_5\"],\n        \"gen_key6\": [\"http://server.domain/path_6\"],\n    }\n\n\ndef test_spec1_expand_simple():\n    pytest.importorskip(\"jinja2\")\n    from fsspec.implementations.reference import json as json_impl\n\n    in_data = {\n        \"version\": 1,\n        \"templates\": {\"u\": \"server.domain/path\"},\n        \"refs\": {\n            \"key0\": \"base64:ZGF0YQ==\",\n            \"key2\": [\"http://{{u}}\", 10000, 100],\n            \"key4\": [\"http://target_url\"],\n            \"key5\": {\"key\": \"value\"},\n        },\n    }\n    fs = fsspec.filesystem(\"reference\", fo=in_data, target_protocol=\"http\")\n    assert fs.references[\"key2\"] == [\"http://server.domain/path\", 10000, 100]\n    fs = fsspec.filesystem(\n        \"reference\",\n        fo=in_data,\n        target_protocol=\"http\",\n        template_overrides={\"u\": \"not.org/p\"},\n    )\n    assert fs.references[\"key2\"] == [\"http://not.org/p\", 10000, 100]\n    assert fs.cat(\"key0\") == b\"data\"\n    assert fs.cat(\"key5\") == json_impl.dumps(in_data[\"refs\"][\"key5\"]).encode(\"utf-8\")\n\n\ndef test_spec1_gen_variants():\n    pytest.importorskip(\"jinja2\")\n    with pytest.raises(ValueError):\n        missing_length_spec = {\n            \"version\": 1,\n            \"templates\": {\"u\": \"server.domain/path\"},\n            \"gen\": [\n                {\n                    \"key\": \"gen_key{{i}}\",\n                    \"url\": \"http://{{u}}_{{i}}\",\n                    \"offset\": \"{{(i + 1) * 1000}}\",\n                    \"dimensions\": {\"i\": {\"stop\": 2}},\n                },\n            ],\n        }\n        fsspec.filesystem(\"reference\", fo=missing_length_spec, target_protocol=\"http\")\n\n    with pytest.raises(ValueError):\n        missing_offset_spec = {\n            \"version\": 1,\n            \"templates\": {\"u\": \"server.domain/path\"},\n            \"gen\": [\n                {\n                    \"key\": \"gen_key{{i}}\",\n                    \"url\": \"http://{{u}}_{{i}}\",\n                    \"length\": \"1000\",\n                    \"dimensions\": {\"i\": {\"stop\": 2}},\n                },\n            ],\n        }\n        fsspec.filesystem(\"reference\", fo=missing_offset_spec, target_protocol=\"http\")\n\n    url_only_gen_spec = {\n        \"version\": 1,\n        \"templates\": {\"u\": \"server.domain/path\"},\n        \"gen\": [\n            {\n                \"key\": \"gen_key{{i}}\",\n                \"url\": \"http://{{u}}_{{i}}\",\n                \"dimensions\": {\"i\": {\"stop\": 2}},\n            },\n        ],\n    }\n\n    fs = fsspec.filesystem(\"reference\", fo=url_only_gen_spec, target_protocol=\"http\")\n    assert fs.references == {\n        \"gen_key0\": [\"http://server.domain/path_0\"],\n        \"gen_key1\": [\"http://server.domain/path_1\"],\n    }\n\n\ndef test_empty():\n    pytest.importorskip(\"jinja2\")\n    fs = fsspec.filesystem(\"reference\", fo={\"version\": 1}, target_protocol=\"http\")\n    assert fs.references == {}\n\n\ndef test_get_sync(tmpdir):\n    localfs = LocalFileSystem()\n\n    real = tmpdir / \"file\"\n    real.write_binary(b\"0123456789\")\n\n    refs = {\"a\": b\"data\", \"b\": (str(real), 0, 5), \"c/d\": (str(real), 1, 6)}\n    fs = fsspec.filesystem(\"reference\", fo=refs, fs=localfs)\n\n    fs.get(\"a\", str(tmpdir / \"a\"))\n    assert (tmpdir / \"a\").read_binary() == b\"data\"\n    fs.get(\"b\", str(tmpdir / \"b\"))\n    assert (tmpdir / \"b\").read_binary() == b\"01234\"\n    fs.get(\"c/d\", str(tmpdir / \"d\"))\n    assert (tmpdir / \"d\").read_binary() == b\"123456\"\n    fs.get(\"c\", str(tmpdir / \"c\"), recursive=True)\n    assert (tmpdir / \"c\").isdir()\n    assert (tmpdir / \"c\" / \"d\").read_binary() == b\"123456\"\n\n\ndef test_multi_fs_provided(m, tmpdir):\n    localfs = LocalFileSystem()\n\n    real = tmpdir / \"file\"\n    real.write_binary(b\"0123456789\")\n\n    m.pipe(\"afile\", b\"hello\")\n\n    # local URLs are file:// by default\n    refs = {\n        \"a\": b\"data\",\n        \"b\": (f\"file://{real}\", 0, 5),\n        \"c/d\": (f\"file://{real}\", 1, 6),\n        \"c/e\": [\"memory://afile\"],\n    }\n\n    fs = fsspec.filesystem(\"reference\", fo=refs, fs={\"file\": localfs, \"memory\": m})\n    assert fs.cat(\"c/e\") == b\"hello\"\n    assert fs.cat([\"c/e\", \"a\", \"b\"]) == {\n        \"a\": b\"data\",\n        \"b\": b\"01234\",\n        \"c/e\": b\"hello\",\n    }\n\n\ndef test_multi_fs_created(m, tmpdir):\n    real = tmpdir / \"file\"\n    real.write_binary(b\"0123456789\")\n\n    m.pipe(\"afile\", b\"hello\")\n\n    # local URLs are file:// by default\n    refs = {\n        \"a\": b\"data\",\n        \"b\": (f\"file://{real}\", 0, 5),\n        \"c/d\": (f\"file://{real}\", 1, 6),\n        \"c/e\": [\"memory://afile\"],\n    }\n\n    fs = fsspec.filesystem(\"reference\", fo=refs, fs={\"file\": {}, \"memory\": {}})\n    assert fs.cat(\"c/e\") == b\"hello\"\n    assert fs.cat([\"c/e\", \"a\", \"b\"]) == {\n        \"a\": b\"data\",\n        \"b\": b\"01234\",\n        \"c/e\": b\"hello\",\n    }\n\n\ndef test_missing_nonasync(m):\n    zarr = pytest.importorskip(\"zarr\")\n    zarray = {\n        \"chunks\": [1],\n        \"compressor\": None,\n        \"dtype\": \"<f8\",\n        \"fill_value\": \"NaN\",\n        \"filters\": [],\n        \"order\": \"C\",\n        \"shape\": [10],\n        \"zarr_format\": 2,\n    }\n    refs = {\".zarray\": json.dumps(zarray)}\n\n    m = fsspec.get_mapper(\"reference://\", fo=refs, remote_protocol=\"memory\")\n\n    a = zarr.open_array(m)\n    assert str(a[0]) == \"nan\"\n\n\ndef test_fss_has_defaults(m):\n    fs = fsspec.filesystem(\"reference\", fo={})\n    assert None in fs.fss\n\n    fs = fsspec.filesystem(\"reference\", fo={}, remote_protocol=\"memory\")\n    assert fs.fss[None].protocol == \"memory\"\n    assert fs.fss[\"memory\"].protocol == \"memory\"\n\n    fs = fsspec.filesystem(\"reference\", fs=m, fo={})\n    assert fs.fss[None] is m\n\n    fs = fsspec.filesystem(\"reference\", fs={\"memory\": m}, fo={})\n    assert fs.fss[\"memory\"] is m\n    assert fs.fss[None].protocol == (\"file\", \"local\")\n\n    fs = fsspec.filesystem(\"reference\", fs={None: m}, fo={})\n    assert fs.fss[None] is m\n\n    fs = fsspec.filesystem(\"reference\", fo={\"key\": [\"memory://a\"]})\n    assert fs.fss[None] is fs.fss[\"memory\"]\n\n    fs = fsspec.filesystem(\"reference\", fo={\"key\": [\"memory://a\"], \"blah\": [\"path\"]})\n    assert fs.fss[None] is fs.fss[\"memory\"]\n\n\ndef test_merging(m):\n    m.pipe(\"/a\", b\"test data\")\n    other = b\"other test data\"\n    m.pipe(\"/b\", other)\n    fs = fsspec.filesystem(\n        \"reference\",\n        fo={\n            \"a\": [\"memory://a\", 1, 1],\n            \"b\": [\"memory://a\", 2, 1],\n            \"c\": [\"memory://b\"],\n            \"d\": [\"memory://b\", 4, 6],\n        },\n    )\n    out = fs.cat([\"a\", \"b\", \"c\", \"d\"])\n    assert out == {\"a\": b\"e\", \"b\": b\"s\", \"c\": other, \"d\": other[4:10]}\n\n\ndef test_cat_file_ranges(m):\n    other = b\"other test data\"\n    m.pipe(\"/b\", other)\n\n    fs = fsspec.filesystem(\n        \"reference\",\n        fo={\n            \"c\": [\"memory://b\"],\n            \"d\": [\"memory://b\", 4, 6],\n        },\n    )\n    assert fs.cat_file(\"c\") == other\n    assert fs.cat_file(\"c\", start=1) == other[1:]\n    assert fs.cat_file(\"c\", start=-5) == other[-5:]\n    assert fs.cat_file(\"c\", 1, -5) == other[1:-5]\n\n    assert fs.cat_file(\"d\") == other[4:10]\n    assert fs.cat_file(\"d\", start=1) == other[4:10][1:]\n    assert fs.cat_file(\"d\", start=-5) == other[4:10][-5:]\n    assert fs.cat_file(\"d\", 1, -3) == other[4:10][1:-3]\n\n\n@pytest.mark.parametrize(\n    \"fo\",\n    [\n        {\n            \"c\": [\"memory://b\"],\n            \"d\": [\"memory://unknown\", 4, 6],\n        },\n        {\n            \"c\": [\"memory://b\"],\n            \"d\": [\"//unknown\", 4, 6],\n        },\n    ],\n    ids=[\"memory protocol\", \"mixed protocols: memory and unspecified\"],\n)\ndef test_cat_missing(m, fo):\n    other = b\"other test data\"\n    m.pipe(\"/b\", other)\n    fs = fsspec.filesystem(\n        \"reference\",\n        fo=fo,\n    )\n    with pytest.raises(FileNotFoundError):\n        fs.cat(\"notafile\")\n\n    with pytest.raises(FileNotFoundError):\n        fs.cat([\"notone\", \"nottwo\"])\n\n    mapper = fs.get_mapper(\"\")\n\n    with pytest.raises(KeyError):\n        mapper[\"notakey\"]\n\n    with pytest.raises(KeyError):\n        mapper.getitems([\"notone\", \"nottwo\"])\n\n    with pytest.raises(ReferenceNotReachable) as ex:\n        fs.cat(\"d\")\n    assert ex.value.__cause__\n    out = fs.cat(\"d\", on_error=\"return\")\n    assert isinstance(out, ReferenceNotReachable)\n\n    with pytest.raises(ReferenceNotReachable) as e:\n        mapper[\"d\"]\n    assert '\"d\"' in str(e.value)\n    assert \"//unknown\" in str(e.value)\n\n    with pytest.raises(ReferenceNotReachable):\n        mapper.getitems([\"c\", \"d\"])\n\n    out = mapper.getitems([\"c\", \"d\"], on_error=\"return\")\n    assert isinstance(out[\"d\"], ReferenceNotReachable)\n\n    out = fs.cat([\"notone\", \"c\", \"d\"], on_error=\"return\")\n    assert isinstance(out[\"notone\"], FileNotFoundError)\n    assert out[\"c\"] == other\n    assert isinstance(out[\"d\"], ReferenceNotReachable)\n\n    out = mapper.getitems([\"c\", \"d\"], on_error=\"omit\")\n    assert list(out) == [\"c\"]\n\n\ndef test_df_single(m):\n    pd = pytest.importorskip(\"pandas\")\n    pytest.importorskip(\"fastparquet\")\n    data = b\"data0data1data2\"\n    m.pipe({\"data\": data})\n    df = pd.DataFrame(\n        {\n            \"path\": [None, \"memory://data\", \"memory://data\"],\n            \"offset\": [0, 0, 4],\n            \"size\": [0, 0, 4],\n            \"raw\": [b\"raw\", None, None],\n        }\n    )\n    df.to_parquet(\"memory://stuff/refs.0.parq\")\n    m.pipe(\n        \".zmetadata\",\n        b\"\"\"{\n    \"metadata\": {\n        \".zgroup\": {\n            \"zarr_format\": 2\n        },\n        \"stuff/.zarray\": {\n            \"chunks\": [1],\n            \"compressor\": null,\n            \"dtype\": \"i8\",\n            \"filters\": null,\n            \"shape\": [3],\n            \"zarr_format\": 2\n        }\n    },\n    \"zarr_consolidated_format\": 1,\n    \"record_size\": 10\n    }\n    \"\"\",\n    )\n    fs = ReferenceFileSystem(fo=\"memory:///\", remote_protocol=\"memory\")\n    allfiles = fs.find(\"\")\n    assert \".zmetadata\" in allfiles\n    assert \".zgroup\" in allfiles\n    assert \"stuff/2\" in allfiles\n\n    assert fs.cat(\"stuff/0\") == b\"raw\"\n    assert fs.cat(\"stuff/1\") == data\n    assert fs.cat(\"stuff/2\") == data[4:8]\n\n\ndef test_df_multi(m):\n    pd = pytest.importorskip(\"pandas\")\n    pytest.importorskip(\"fastparquet\")\n    data = b\"data0data1data2\"\n    m.pipe({\"data\": data})\n    df0 = pd.DataFrame(\n        {\n            \"path\": [None, \"memory://data\", \"memory://data\"],\n            \"offset\": [0, 0, 4],\n            \"size\": [0, 0, 4],\n            \"raw\": [b\"raw1\", None, None],\n        }\n    )\n    df0.to_parquet(\"memory://stuff/refs.0.parq\")\n    df1 = pd.DataFrame(\n        {\n            \"path\": [None, \"memory://data\", \"memory://data\"],\n            \"offset\": [0, 0, 2],\n            \"size\": [0, 0, 2],\n            \"raw\": [b\"raw2\", None, None],\n        }\n    )\n    df1.to_parquet(\"memory://stuff/refs.1.parq\")\n    m.pipe(\n        \".zmetadata\",\n        b\"\"\"{\n    \"metadata\": {\n        \".zgroup\": {\n            \"zarr_format\": 2\n        },\n        \"stuff/.zarray\": {\n            \"chunks\": [1],\n            \"compressor\": null,\n            \"dtype\": \"i8\",\n            \"filters\": null,\n            \"shape\": [6],\n            \"zarr_format\": 2\n        }\n    },\n    \"zarr_consolidated_format\": 1,\n    \"record_size\": 3\n    }\n    \"\"\",\n    )\n    fs = ReferenceFileSystem(\n        fo=\"memory:///\", remote_protocol=\"memory\", skip_instance_cache=True\n    )\n    allfiles = fs.find(\"\")\n    assert \".zmetadata\" in allfiles\n    assert \".zgroup\" in allfiles\n    assert \"stuff/2\" in allfiles\n    assert \"stuff/4\" in allfiles\n\n    assert fs.cat(\"stuff/0\") == b\"raw1\"\n    assert fs.cat(\"stuff/1\") == data\n    assert fs.cat(\"stuff/2\") == data[4:8]\n    assert fs.cat(\"stuff/3\") == b\"raw2\"\n    assert fs.cat(\"stuff/4\") == data\n    assert fs.cat(\"stuff/5\") == data[2:4]\n\n\ndef test_mapping_getitems(m):\n    m.pipe({\"a\": b\"A\", \"b\": b\"B\"})\n\n    refs = {\n        \"a\": [\"a\"],\n        \"b\": [\"b\"],\n    }\n    h = fsspec.filesystem(\"memory\")\n    fs = fsspec.filesystem(\"reference\", fo=refs, fs=h)\n    mapping = fs.get_mapper(\"\")\n    assert mapping.getitems([\"b\", \"a\"]) == {\"a\": b\"A\", \"b\": b\"B\"}\n\n\ndef test_cached(m, tmpdir):\n    fn = f\"{tmpdir}/ref.json\"\n\n    m.pipe({\"a\": b\"A\", \"b\": b\"B\"})\n    m.pipe(\"ref.json\", b\"\"\"{\"a\": [\"a\"], \"b\": [\"b\"]}\"\"\")\n\n    fs = fsspec.filesystem(\n        \"reference\",\n        fo=\"simplecache::memory://ref.json\",\n        fs=m,\n        target_options={\"cache_storage\": str(tmpdir), \"same_names\": True},\n    )\n    assert fs.cat(\"a\") == b\"A\"\n    assert os.path.exists(fn)\n\n    # truncate original file to show we are loading from the cached version\n    m.pipe(\"ref.json\", b\"\")\n    fs = fsspec.filesystem(\n        \"reference\",\n        fo=\"simplecache::memory://ref.json\",\n        fs=m,\n        target_options={\"cache_storage\": str(tmpdir), \"same_names\": True},\n        skip_instance_cache=True,\n    )\n    assert fs.cat(\"a\") == b\"A\"\n\n\n@pytest.fixture()\ndef lazy_refs(m):\n    zarr = pytest.importorskip(\"zarr\")\n    l = LazyReferenceMapper.create(\"memory://refs\", fs=m)\n    g = zarr.open(l, mode=\"w\")\n    g.create_dataset(name=\"data\", shape=(100,), chunks=(10,), dtype=\"int64\")\n    return l\n\n\ndef test_append_parquet(lazy_refs, m):\n    pytest.importorskip(\"kerchunk\")\n    with pytest.raises(KeyError):\n        lazy_refs[\"data/0\"]\n    lazy_refs[\"data/0\"] = b\"data\"\n    assert lazy_refs[\"data/0\"] == b\"data\"\n    lazy_refs.flush()\n\n    lazy2 = LazyReferenceMapper(\"memory://refs\", fs=m)\n    assert lazy2[\"data/0\"] == b\"data\"\n    with pytest.raises(KeyError):\n        lazy_refs[\"data/1\"]\n    lazy2[\"data/1\"] = b\"Bdata\"\n    assert lazy2[\"data/1\"] == b\"Bdata\"\n    lazy2.flush()\n\n    lazy2 = LazyReferenceMapper(\"memory://refs\", fs=m)\n    assert lazy2[\"data/0\"] == b\"data\"\n    assert lazy2[\"data/1\"] == b\"Bdata\"\n    lazy2[\"data/1\"] = b\"Adata\"\n    del lazy2[\"data/0\"]\n    assert lazy2[\"data/1\"] == b\"Adata\"\n    assert \"data/0\" not in lazy2\n    lazy2.flush()\n\n    lazy2 = LazyReferenceMapper(\"memory://refs\", fs=m)\n    with pytest.raises(KeyError):\n        lazy2[\"data/0\"]\n    assert lazy2[\"data/1\"] == b\"Adata\"\n", "fsspec/implementations/tests/test_sftp.py": "import os\nimport shlex\nimport subprocess\nimport time\nfrom tarfile import TarFile\n\nimport pytest\n\nimport fsspec\n\npytest.importorskip(\"paramiko\")\n\n\ndef stop_docker(name):\n    cmd = shlex.split(f'docker ps -a -q --filter \"name={name}\"')\n    cid = subprocess.check_output(cmd).strip().decode()\n    if cid:\n        subprocess.call([\"docker\", \"rm\", \"-f\", cid])\n\n\n@pytest.fixture(scope=\"module\")\ndef ssh():\n    try:\n        pchk = [\"docker\", \"run\", \"--name\", \"fsspec_test_sftp\", \"hello-world\"]\n        subprocess.check_call(pchk)\n        stop_docker(\"fsspec_test_sftp\")\n    except (subprocess.CalledProcessError, FileNotFoundError):\n        pytest.skip(\"docker run not available\")\n        return\n\n    # requires docker\n    cmds = [\n        r\"apt-get update\",\n        r\"apt-get install -y openssh-server\",\n        r\"mkdir /var/run/sshd\",\n        \"bash -c \\\"echo 'root:pass' | chpasswd\\\"\",\n        (\n            r\"sed -i 's/PermitRootLogin prohibit-password/PermitRootLogin yes/' \"\n            r\"/etc/ssh/sshd_config\"\n        ),\n        (\n            r\"sed 's@session\\s*required\\s*pam_loginuid.so@session optional \"\n            r\"pam_loginuid.so@g' -i /etc/pam.d/sshd\"\n        ),\n        r'bash -c \"echo \\\"export VISIBLE=now\\\" >> /etc/profile\"',\n        r\"/usr/sbin/sshd\",\n    ]\n    name = \"fsspec_sftp\"\n    stop_docker(name)\n    cmd = f\"docker run -d -p 9200:22 --name {name} ubuntu:16.04 sleep 9000\"\n    try:\n        cid = subprocess.check_output(shlex.split(cmd)).strip().decode()\n        for cmd in cmds:\n            subprocess.call([\"docker\", \"exec\", cid] + shlex.split(cmd))\n        time.sleep(1)\n        yield {\n            \"host\": \"localhost\",\n            \"port\": 9200,\n            \"username\": \"root\",\n            \"password\": \"pass\",\n        }\n    finally:\n        stop_docker(name)\n\n\n@pytest.fixture(scope=\"module\")\ndef root_path():\n    return \"/home/someuser/\"\n\n\ndef test_simple(ssh, root_path):\n    f = fsspec.get_filesystem_class(\"sftp\")(**ssh)\n    f.mkdirs(root_path + \"deeper\")\n    try:\n        f.touch(root_path + \"deeper/afile\")\n        assert f.find(root_path) == [root_path + \"deeper/afile\"]\n        assert f.ls(root_path + \"deeper/\") == [root_path + \"deeper/afile\"]\n        assert f.info(root_path + \"deeper/afile\")[\"type\"] == \"file\"\n        assert f.info(root_path + \"deeper/afile\")[\"size\"] == 0\n        assert f.exists(root_path)\n    finally:\n        f.rm(root_path, recursive=True)\n        assert not f.exists(root_path)\n\n\n@pytest.mark.parametrize(\"protocol\", [\"sftp\", \"ssh\"])\ndef test_with_url(protocol, ssh):\n    fo = fsspec.open(\n        protocol\n        + \"://{username}:{password}@{host}:{port}/home/someuserout\".format(**ssh),\n        \"wb\",\n    )\n    with fo as f:\n        f.write(b\"hello\")\n    fo = fsspec.open(\n        protocol\n        + \"://{username}:{password}@{host}:{port}/home/someuserout\".format(**ssh),\n        \"rb\",\n    )\n    with fo as f:\n        assert f.read() == b\"hello\"\n\n\n@pytest.mark.parametrize(\"protocol\", [\"sftp\", \"ssh\"])\ndef test_get_dir(protocol, ssh, root_path, tmpdir):\n    path = str(tmpdir)\n    f = fsspec.filesystem(protocol, **ssh)\n    f.mkdirs(root_path + \"deeper\", exist_ok=True)\n    f.touch(root_path + \"deeper/afile\")\n    f.get(root_path, path, recursive=True)\n\n    assert os.path.isdir(f\"{path}/deeper\")\n    assert os.path.isfile(f\"{path}/deeper/afile\")\n\n    f.get(\n        protocol\n        + \"://{username}:{password}@{host}:{port}{root_path}\".format(\n            root_path=root_path, **ssh\n        ),\n        f\"{path}/test2\",\n        recursive=True,\n    )\n\n    assert os.path.isdir(f\"{path}/test2/deeper\")\n    assert os.path.isfile(f\"{path}/test2/deeper/afile\")\n\n\n@pytest.fixture(scope=\"module\")\ndef netloc(ssh):\n    username = ssh.get(\"username\")\n    password = ssh.get(\"password\")\n    host = ssh.get(\"host\")\n    port = ssh.get(\"port\")\n    userpass = (\n        f\"{username}:{password if password is not None else ''}@\"\n        if username is not None\n        else \"\"\n    )\n    netloc = f\"{host}:{port if port is not None else ''}\"\n    return userpass + netloc\n\n\ndef test_put_file(ssh, tmp_path, root_path):\n    tmp_file = tmp_path / \"a.txt\"\n    with open(tmp_file, mode=\"w\") as fd:\n        fd.write(\"blabla\")\n\n    f = fsspec.get_filesystem_class(\"sftp\")(**ssh)\n    f.put_file(lpath=tmp_file, rpath=root_path + \"a.txt\")\n\n\ndef test_simple_with_tar(ssh, netloc, tmp_path, root_path):\n    files_to_pack = [\"a.txt\", \"b.txt\"]\n\n    tar_filename = make_tarfile(files_to_pack, tmp_path)\n\n    f = fsspec.get_filesystem_class(\"sftp\")(**ssh)\n    f.mkdirs(f\"{root_path}deeper\", exist_ok=True)\n    try:\n        remote_tar_filename = f\"{root_path}deeper/somefile.tar\"\n        with f.open(remote_tar_filename, mode=\"wb\") as wfd:\n            with open(tar_filename, mode=\"rb\") as rfd:\n                wfd.write(rfd.read())\n        fs = fsspec.open(f\"tar::ssh://{netloc}{remote_tar_filename}\").fs\n        files = fs.find(\"/\")\n        assert files == files_to_pack\n    finally:\n        f.rm(root_path, recursive=True)\n\n\ndef make_tarfile(files_to_pack, tmp_path):\n    \"\"\"Create a tarfile with some files.\"\"\"\n    tar_filename = tmp_path / \"sometarfile.tar\"\n    for filename in files_to_pack:\n        with open(tmp_path / filename, mode=\"w\") as fd:\n            fd.write(\"\")\n    with TarFile(tar_filename, mode=\"w\") as tf:\n        for filename in files_to_pack:\n            tf.add(tmp_path / filename, arcname=filename)\n    return tar_filename\n\n\ndef test_transaction(ssh, root_path):\n    f = fsspec.get_filesystem_class(\"sftp\")(**ssh)\n    f.mkdirs(root_path + \"deeper\", exist_ok=True)\n    try:\n        f.start_transaction()\n        f.touch(root_path + \"deeper/afile\")\n        assert f.find(root_path) == []\n        f.end_transaction()\n        assert f.find(root_path) == [root_path + \"deeper/afile\"]\n\n        with f.transaction:\n            assert f._intrans\n            f.touch(root_path + \"deeper/afile2\")\n            assert f.find(root_path) == [root_path + \"deeper/afile\"]\n        assert f.find(root_path) == [\n            root_path + \"deeper/afile\",\n            root_path + \"deeper/afile2\",\n        ]\n    finally:\n        f.rm(root_path, recursive=True)\n\n\n@pytest.mark.parametrize(\"path\", [\"/a/b/c\", \"a/b/c\"])\ndef test_mkdir_create_parent(ssh, path):\n    f = fsspec.get_filesystem_class(\"sftp\")(**ssh)\n\n    with pytest.raises(FileNotFoundError):\n        f.mkdir(path, create_parents=False)\n\n    f.mkdir(path)\n    assert f.exists(path)\n\n    with pytest.raises(FileExistsError, match=path):\n        f.mkdir(path)\n\n    f.rm(path, recursive=True)\n    assert not f.exists(path)\n\n\n@pytest.mark.parametrize(\"path\", [\"/a/b/c\", \"a/b/c\"])\ndef test_makedirs_exist_ok(ssh, path):\n    f = fsspec.get_filesystem_class(\"sftp\")(**ssh)\n\n    f.makedirs(path, exist_ok=False)\n\n    with pytest.raises(FileExistsError, match=path):\n        f.makedirs(path, exist_ok=False)\n\n    f.makedirs(path, exist_ok=True)\n    f.rm(path, recursive=True)\n    assert not f.exists(path)\n", "fsspec/implementations/tests/test_archive.py": "import bz2\nimport gzip\nimport lzma\nimport os\nimport pickle\nimport tarfile\nimport tempfile\nimport zipfile\nfrom contextlib import contextmanager\nfrom io import BytesIO\n\nimport pytest\n\nimport fsspec\n\n# The blueprint to create synthesized archive files from.\narchive_data = {\"a\": b\"\", \"b\": b\"hello\", \"deeply/nested/path\": b\"stuff\"}\n\n\n@contextmanager\ndef tempzip(data=None):\n    \"\"\"\n    Provide test cases with temporary synthesized Zip archives.\n    \"\"\"\n    data = data or {}\n    f = tempfile.mkstemp(suffix=\".zip\")[1]\n    with zipfile.ZipFile(f, mode=\"w\") as z:\n        for k, v in data.items():\n            z.writestr(k, v)\n    try:\n        yield f\n    finally:\n        try:\n            os.remove(f)\n        except OSError:\n            pass\n\n\n@contextmanager\ndef temparchive(data=None):\n    \"\"\"\n    Provide test cases with temporary synthesized 7-Zip archives.\n    \"\"\"\n    data = data or {}\n    libarchive = pytest.importorskip(\"libarchive\")\n    f = tempfile.mkstemp(suffix=\".7z\")[1]\n    with libarchive.file_writer(f, \"7zip\") as archive:\n        for k, v in data.items():\n            archive.add_file_from_memory(entry_path=k, entry_size=len(v), entry_data=v)\n    try:\n        yield f\n    finally:\n        try:\n            os.remove(f)\n        except OSError:\n            pass\n\n\n@contextmanager\ndef temptar(data=None, mode=\"w\", suffix=\".tar\"):\n    \"\"\"\n    Provide test cases with temporary synthesized .tar archives.\n    \"\"\"\n    data = data or {}\n    fn = tempfile.mkstemp(suffix=suffix)[1]\n    with tarfile.TarFile.open(fn, mode=mode) as t:\n        touched = {}\n        for name, value in data.items():\n            # Create directory hierarchy.\n            # https://bugs.python.org/issue22208#msg225558\n            if \"/\" in name and name not in touched:\n                parts = os.path.dirname(name).split(\"/\")\n                for index in range(1, len(parts) + 1):\n                    info = tarfile.TarInfo(\"/\".join(parts[:index]))\n                    info.type = tarfile.DIRTYPE\n                    t.addfile(info)\n                touched[name] = True\n\n            # Add file content.\n            info = tarfile.TarInfo(name=name)\n            info.size = len(value)\n            t.addfile(info, BytesIO(value))\n\n    try:\n        yield fn\n    finally:\n        try:\n            os.remove(fn)\n        except OSError:\n            pass\n\n\n@contextmanager\ndef temptargz(data=None, mode=\"w\", suffix=\".tar.gz\"):\n    \"\"\"\n    Provide test cases with temporary synthesized .tar.gz archives.\n    \"\"\"\n\n    with temptar(data=data, mode=mode) as tarname:\n        fn = tempfile.mkstemp(suffix=suffix)[1]\n        with open(tarname, \"rb\") as tar:\n            cf = gzip.GzipFile(filename=fn, mode=mode)\n            cf.write(tar.read())\n            cf.close()\n\n        try:\n            yield fn\n        finally:\n            try:\n                os.remove(fn)\n            except OSError:\n                pass\n\n\n@contextmanager\ndef temptarbz2(data=None, mode=\"w\", suffix=\".tar.bz2\"):\n    \"\"\"\n    Provide test cases with temporary synthesized .tar.bz2 archives.\n    \"\"\"\n\n    with temptar(data=data, mode=mode) as tarname:\n        fn = tempfile.mkstemp(suffix=suffix)[1]\n        with open(tarname, \"rb\") as tar:\n            cf = bz2.BZ2File(filename=fn, mode=mode)\n            cf.write(tar.read())\n            cf.close()\n\n        try:\n            yield fn\n        finally:\n            try:\n                os.remove(fn)\n            except OSError:\n                pass\n\n\n@contextmanager\ndef temptarxz(data=None, mode=\"w\", suffix=\".tar.xz\"):\n    \"\"\"\n    Provide test cases with temporary synthesized .tar.xz archives.\n    \"\"\"\n\n    with temptar(data=data, mode=mode) as tarname:\n        fn = tempfile.mkstemp(suffix=suffix)[1]\n        with open(tarname, \"rb\") as tar:\n            cf = lzma.open(filename=fn, mode=mode, format=lzma.FORMAT_XZ)\n            cf.write(tar.read())\n            cf.close()\n\n        try:\n            yield fn\n        finally:\n            try:\n                os.remove(fn)\n            except OSError:\n                pass\n\n\nclass ArchiveTestScenario:\n    \"\"\"\n    Describe a test scenario for any type of archive.\n    \"\"\"\n\n    def __init__(self, protocol=None, provider=None, variant=None):\n        # The filesystem protocol identifier. Any of \"zip\", \"tar\" or \"libarchive\".\n        self.protocol = protocol\n        # A contextmanager function to provide temporary synthesized archives.\n        self.provider = provider\n        # The filesystem protocol variant identifier. Any of \"gz\", \"bz2\" or \"xz\".\n        self.variant = variant\n\n\ndef pytest_generate_tests(metafunc):\n    \"\"\"\n    Generate test scenario parametrization arguments with appropriate labels (idlist).\n\n    On the one hand, this yields an appropriate output like::\n\n        fsspec/implementations/tests/test_archive.py::TestArchive::test_empty[zip] PASSED  # noqa\n\n    On the other hand, it will support perfect test discovery, like::\n\n        pytest fsspec -vvv -k \"zip or tar or libarchive\"\n\n    https://docs.pytest.org/en/latest/example/parametrize.html#a-quick-port-of-testscenarios\n    \"\"\"\n    idlist = []\n    argnames = [\"scenario\"]\n    argvalues = []\n    for scenario in metafunc.cls.scenarios:\n        scenario: ArchiveTestScenario = scenario\n        label = scenario.protocol\n        if scenario.variant:\n            label += \"-\" + scenario.variant\n        idlist.append(label)\n        argvalues.append([scenario])\n    metafunc.parametrize(argnames, argvalues, ids=idlist, scope=\"class\")\n\n\n# Define test scenarios.\nscenario_zip = ArchiveTestScenario(protocol=\"zip\", provider=tempzip)\nscenario_tar = ArchiveTestScenario(protocol=\"tar\", provider=temptar)\nscenario_targz = ArchiveTestScenario(protocol=\"tar\", provider=temptargz, variant=\"gz\")\nscenario_tarbz2 = ArchiveTestScenario(\n    protocol=\"tar\", provider=temptarbz2, variant=\"bz2\"\n)\nscenario_tarxz = ArchiveTestScenario(protocol=\"tar\", provider=temptarxz, variant=\"xz\")\nscenario_libarchive = ArchiveTestScenario(protocol=\"libarchive\", provider=temparchive)\n\n\nclass TestAnyArchive:\n    \"\"\"\n    Validate that all filesystem adapter implementations for archive files\n    will adhere to the same specification.\n    \"\"\"\n\n    scenarios = [\n        scenario_zip,\n        scenario_tar,\n        scenario_targz,\n        scenario_tarbz2,\n        scenario_tarxz,\n        scenario_libarchive,\n    ]\n\n    def test_repr(self, scenario: ArchiveTestScenario):\n        with scenario.provider() as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n            assert repr(fs).startswith(\"<Archive-like object\")\n\n    def test_empty(self, scenario: ArchiveTestScenario):\n        with scenario.provider() as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n            assert fs.find(\"\") == []\n            assert fs.find(\"\", withdirs=True) == []\n            with pytest.raises(FileNotFoundError):\n                fs.info(\"\")\n            assert fs.ls(\"\") == []\n\n    def test_glob(self, scenario: ArchiveTestScenario):\n        with scenario.provider(archive_data) as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n            assert fs.glob(\"*/*/*th\") == [\"deeply/nested/path\"]\n\n    def test_mapping(self, scenario: ArchiveTestScenario):\n        with scenario.provider(archive_data) as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n            m = fs.get_mapper()\n            assert list(m) == [\"a\", \"b\", \"deeply/nested/path\"]\n            assert m[\"b\"] == archive_data[\"b\"]\n\n    def test_pickle(self, scenario: ArchiveTestScenario):\n        with scenario.provider(archive_data) as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n            fs2 = pickle.loads(pickle.dumps(fs))\n            assert fs2.cat(\"b\") == b\"hello\"\n\n    def test_all_dirnames(self, scenario: ArchiveTestScenario):\n        with scenario.provider(archive_data) as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n\n            # fx are files, dx are a directories\n            assert fs._all_dirnames([]) == set()\n            assert fs._all_dirnames([\"f1\"]) == set()\n            assert fs._all_dirnames([\"f1\", \"f2\"]) == set()\n            assert fs._all_dirnames([\"f1\", \"f2\", \"d1/f1\"]) == {\"d1\"}\n            assert fs._all_dirnames([\"f1\", \"d1/f1\", \"d1/f2\"]) == {\"d1\"}\n            assert fs._all_dirnames([\"f1\", \"d1/f1\", \"d2/f1\"]) == {\"d1\", \"d2\"}\n            assert fs._all_dirnames([\"d1/d1/d1/f1\"]) == {\"d1\", \"d1/d1\", \"d1/d1/d1\"}\n\n    def test_ls(self, scenario: ArchiveTestScenario):\n        with scenario.provider(archive_data) as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n\n            assert fs.ls(\"\", detail=False) == [\"a\", \"b\", \"deeply\"]\n            assert fs.ls(\"/\") == fs.ls(\"\")\n\n            assert fs.ls(\"deeply\", detail=False) == [\"deeply/nested\"]\n            assert fs.ls(\"deeply/\") == fs.ls(\"deeply\")\n\n            assert fs.ls(\"deeply/nested\", detail=False) == [\"deeply/nested/path\"]\n            assert fs.ls(\"deeply/nested/\") == fs.ls(\"deeply/nested\")\n\n    def test_find(self, scenario: ArchiveTestScenario):\n        with scenario.provider(archive_data) as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n\n            assert fs.find(\"\") == [\"a\", \"b\", \"deeply/nested/path\"]\n            assert fs.find(\"\", withdirs=True) == [\n                \"a\",\n                \"b\",\n                \"deeply\",\n                \"deeply/nested\",\n                \"deeply/nested/path\",\n            ]\n\n            assert fs.find(\"deeply\") == [\"deeply/nested/path\"]\n            assert fs.find(\"deeply/\") == fs.find(\"deeply\")\n\n    @pytest.mark.parametrize(\"topdown\", [True, False])\n    @pytest.mark.parametrize(\"prune_nested\", [True, False])\n    def test_walk(self, scenario: ArchiveTestScenario, topdown, prune_nested):\n        with scenario.provider(archive_data) as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n            expected = [\n                # (dirname, list of subdirs, list of files)\n                (\"\", [\"deeply\"], [\"a\", \"b\"]),\n                (\"deeply\", [\"nested\"], []),\n            ]\n            if not topdown or not prune_nested:\n                expected.append((\"deeply/nested\", [], [\"path\"]))\n            if not topdown:\n                expected.reverse()\n\n            result = []\n            for path, dirs, files in fs.walk(\"\", topdown=topdown):\n                result.append((path, dirs.copy(), files))\n                # Bypass the \"nested\" dir\n                if prune_nested and \"nested\" in dirs:\n                    dirs.remove(\"nested\")\n\n            # prior py3.10 zip() does not support strict=True, we need\n            # a manual len check here\n            assert len(result) == len(expected)\n            for lhs, rhs in zip(result, expected):\n                assert lhs[0] == rhs[0]\n                assert sorted(lhs[1]) == sorted(rhs[1])\n                assert sorted(lhs[2]) == sorted(rhs[2])\n\n    def test_info(self, scenario: ArchiveTestScenario):\n        # https://github.com/Suor/funcy/blob/1.15/funcy/colls.py#L243-L245\n        def project(mapping, keys):\n            \"\"\"Leaves only given keys in mapping.\"\"\"\n            return {k: mapping[k] for k in keys if k in mapping}\n\n        with scenario.provider(archive_data) as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n\n            with pytest.raises(FileNotFoundError):\n                fs.info(\"i-do-not-exist\")\n\n            # Iterate over all directories.\n            for d in fs._all_dirnames(archive_data.keys()):\n                lhs = project(fs.info(d), [\"name\", \"size\", \"type\"])\n                expected = {\"name\": f\"{d}\", \"size\": 0, \"type\": \"directory\"}\n                assert lhs == expected\n\n            # Iterate over all files.\n            for f, v in archive_data.items():\n                lhs = fs.info(f)\n                assert lhs[\"name\"] == f\n                assert lhs[\"size\"] == len(v)\n                assert lhs[\"type\"] == \"file\"\n\n    @pytest.mark.parametrize(\"scale\", [128, 512, 4096])\n    def test_isdir_isfile(self, scenario: ArchiveTestScenario, scale: int):\n        def make_nested_dir(i):\n            x = f\"{i}\"\n            table = x.maketrans(\"0123456789\", \"ABCDEFGHIJ\")\n            return \"/\".join(x.translate(table))\n\n        scaled_data = {f\"{make_nested_dir(i)}/{i}\": b\"\" for i in range(1, scale + 1)}\n        with scenario.provider(scaled_data) as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n\n            lhs_dirs, lhs_files = (\n                fs._all_dirnames(scaled_data.keys()),\n                scaled_data.keys(),\n            )\n\n            # Warm-up the Cache, this is done in both cases anyways...\n            fs._get_dirs()\n\n            entries = lhs_files | lhs_dirs\n\n            assert lhs_dirs == {e for e in entries if fs.isdir(e)}\n            assert lhs_files == {e for e in entries if fs.isfile(e)}\n\n    def test_read_empty_file(self, scenario: ArchiveTestScenario):\n        with scenario.provider(archive_data) as archive:\n            fs = fsspec.filesystem(scenario.protocol, fo=archive)\n            assert fs.open(\"a\").read() == b\"\"\n", "fsspec/implementations/tests/test_libarchive.py": "# this test case checks that the libarchive can be used from a seekable source (any fs\n# with a block cache active)\nimport fsspec\nfrom fsspec.implementations.tests.test_archive import archive_data, temparchive\n\n\ndef test_cache(ftp_writable):\n    host, port, username, password = \"localhost\", 2121, \"user\", \"pass\"\n\n    with temparchive(archive_data) as archive_file:\n        with fsspec.open(\n            \"ftp:///archive.7z\",\n            \"wb\",\n            host=host,\n            port=port,\n            username=username,\n            password=password,\n        ) as f:\n            f.write(open(archive_file, \"rb\").read())\n        of = fsspec.open(\n            \"libarchive://deeply/nested/path::ftp:///archive.7z\",\n            ftp={\n                \"host\": host,\n                \"port\": port,\n                \"username\": username,\n                \"password\": password,\n            },\n        )\n\n        with of as f:\n            readdata = f.read()\n\n        assert readdata == archive_data[\"deeply/nested/path\"]\n", "fsspec/implementations/tests/test_smb.py": "\"\"\"\nTest SMBFileSystem class using a docker container\n\"\"\"\n\nimport logging\nimport os\nimport shlex\nimport subprocess\nimport time\n\nimport pytest\n\nimport fsspec\n\npytest.importorskip(\"smbprotocol\")\n\n# ruff: noqa: F821\n\nif os.environ.get(\"WSL_INTEROP\"):\n    # Running on WSL (Windows)\n    port_test = [9999]\n\nelse:\n    # ! pylint: disable=redefined-outer-name,missing-function-docstring\n\n    # Test standard and non-standard ports\n    default_port = 445\n    port_test = [None, default_port, 9999]\n\n\ndef stop_docker(container):\n    cmd = shlex.split('docker ps -a -q --filter \"name=%s\"' % container)\n    cid = subprocess.check_output(cmd).strip().decode()\n    if cid:\n        subprocess.call([\"docker\", \"rm\", \"-f\", \"-v\", cid])\n\n\n@pytest.fixture(scope=\"module\", params=port_test)\ndef smb_params(request):\n    try:\n        pchk = [\"docker\", \"run\", \"--name\", \"fsspec_test_smb\", \"hello-world\"]\n        subprocess.check_call(pchk)\n        stop_docker(\"fsspec_test_smb\")\n    except (subprocess.CalledProcessError, FileNotFoundError):\n        pytest.skip(\"docker run not available\")\n\n    # requires docker\n    container = \"fsspec_smb\"\n    stop_docker(container)\n    cfg = \"-p -u 'testuser;testpass' -s 'home;/share;no;no;no;testuser'\"\n    port = request.param if request.param is not None else default_port\n    img = (\n        f\"docker run --name {container} --detach -p 139:139 -p {port}:445 dperson/samba\"  # noqa: E231 E501\n    )\n    cmd = f\"{img} {cfg}\"\n    try:\n        cid = subprocess.check_output(shlex.split(cmd)).strip().decode()\n        logger = logging.getLogger(\"fsspec\")\n        logger.debug(\"Container: %s\", cid)\n        time.sleep(1)\n        yield {\n            \"host\": \"localhost\",\n            \"port\": request.param,\n            \"username\": \"testuser\",\n            \"password\": \"testpass\",\n            \"register_session_retries\": 100,  # max ~= 10 seconds\n        }\n    finally:\n        import smbclient  # pylint: disable=import-outside-toplevel\n\n        smbclient.reset_connection_cache()\n        stop_docker(container)\n\n\n@pytest.mark.flaky(reruns=2, reruns_delay=2)\ndef test_simple(smb_params):\n    adir = \"/home/adir\"\n    adir2 = \"/home/adir/otherdir/\"\n    afile = \"/home/adir/otherdir/afile\"\n    fsmb = fsspec.get_filesystem_class(\"smb\")(**smb_params)\n    fsmb.mkdirs(adir2)\n    fsmb.touch(afile)\n    assert fsmb.find(adir) == [afile]\n    assert fsmb.ls(adir2, detail=False) == [afile]\n    assert fsmb.info(afile)[\"type\"] == \"file\"\n    assert fsmb.info(afile)[\"size\"] == 0\n    assert fsmb.exists(adir)\n    fsmb.rm(adir, recursive=True)\n    assert not fsmb.exists(adir)\n\n\n@pytest.mark.flaky(reruns=2, reruns_delay=2)\ndef test_auto_mkdir(smb_params):\n    adir = \"/home/adir\"\n    adir2 = \"/home/adir/otherdir/\"\n    afile = \"/home/adir/otherdir/afile\"\n    fsmb = fsspec.get_filesystem_class(\"smb\")(**smb_params, auto_mkdir=True)\n    fsmb.touch(afile)\n    assert fsmb.exists(adir)\n    assert fsmb.exists(adir2)\n    assert fsmb.exists(afile)\n    assert fsmb.info(afile)[\"type\"] == \"file\"\n\n    another_dir = \"/home/another_dir\"\n    another_dir2 = \"/home/another_dir/another_nested_dir/\"\n    another_file = \"/home/another_dir/another_nested_dir/another_file\"\n    fsmb.copy(afile, another_file)\n    assert fsmb.exists(another_dir)\n    assert fsmb.exists(another_dir2)\n    assert fsmb.exists(another_file)\n    assert fsmb.info(another_file)[\"type\"] == \"file\"\n\n    fsmb.rm(adir, recursive=True)\n    fsmb.rm(another_dir, recursive=True)\n    assert not fsmb.exists(adir)\n    assert not fsmb.exists(another_dir)\n\n\n@pytest.mark.flaky(reruns=2, reruns_delay=2)\ndef test_with_url(smb_params):\n    if smb_params[\"port\"] is None:\n        smb_url = \"smb://{username}:{password}@{host}/home/someuser.txt\"\n    else:\n        smb_url = \"smb://{username}:{password}@{host}:{port}/home/someuser.txt\"\n    fwo = fsspec.open(smb_url.format(**smb_params), \"wb\")\n    with fwo as fwr:\n        fwr.write(b\"hello\")\n    fro = fsspec.open(smb_url.format(**smb_params), \"rb\")\n    with fro as frd:\n        read_result = frd.read()\n        assert read_result == b\"hello\"\n\n\n@pytest.mark.flaky(reruns=2, reruns_delay=2)\ndef test_transaction(smb_params):\n    afile = \"/home/afolder/otherdir/afile\"\n    afile2 = \"/home/afolder/otherdir/afile2\"\n    adir = \"/home/afolder\"\n    adir2 = \"/home/afolder/otherdir\"\n    fsmb = fsspec.get_filesystem_class(\"smb\")(**smb_params)\n    fsmb.mkdirs(adir2)\n    fsmb.start_transaction()\n    fsmb.touch(afile)\n    assert fsmb.find(adir) == []\n    fsmb.end_transaction()\n    assert fsmb.find(adir) == [afile]\n\n    with fsmb.transaction:\n        assert fsmb._intrans\n        fsmb.touch(afile2)\n        assert fsmb.find(adir) == [afile]\n    assert fsmb.find(adir) == [afile, afile2]\n\n\n@pytest.mark.flaky(reruns=2, reruns_delay=2)\ndef test_makedirs_exist_ok(smb_params):\n    fsmb = fsspec.get_filesystem_class(\"smb\")(**smb_params)\n    fsmb.makedirs(\"/home/a/b/c\")\n    fsmb.makedirs(\"/home/a/b/c\", exist_ok=True)\n\n\n@pytest.mark.flaky(reruns=2, reruns_delay=2)\ndef test_rename_from_upath(smb_params):\n    fsmb = fsspec.get_filesystem_class(\"smb\")(**smb_params)\n    fsmb.makedirs(\"/home/a/b/c\", exist_ok=True)\n    fsmb.mv(\"/home/a/b/c\", \"/home/a/b/d\", recursive=False, maxdepth=None)\n", "fsspec/implementations/tests/test_jupyter.py": "import os\nimport shlex\nimport subprocess\nimport time\n\nimport pytest\n\nimport fsspec\n\npytest.importorskip(\"notebook\")\nrequests = pytest.importorskip(\"requests\")\n\n\n@pytest.fixture()\ndef jupyter(tmpdir):\n    tmpdir = str(tmpdir)\n    os.environ[\"JUPYTER_TOKEN\"] = \"blah\"\n    try:\n        cmd = f'jupyter notebook --notebook-dir=\"{tmpdir}\" --no-browser --port=5566'\n        P = subprocess.Popen(shlex.split(cmd))\n    except FileNotFoundError:\n        pytest.skip(\"notebook not installed correctly\")\n    try:\n        timeout = 15\n        while True:\n            try:\n                r = requests.get(\"http://localhost:5566/?token=blah\")\n                r.raise_for_status()\n                break\n            except (requests.exceptions.BaseHTTPError, OSError):\n                time.sleep(0.1)\n                timeout -= 0.1\n                if timeout < 0:\n                    pytest.xfail(\"Timed out for jupyter\")\n        yield \"http://localhost:5566/?token=blah\", tmpdir\n    finally:\n        P.terminate()\n\n\ndef test_simple(jupyter):\n    url, d = jupyter\n    fs = fsspec.filesystem(\"jupyter\", url=url)\n    assert fs.ls(\"\") == []\n\n    fs.pipe(\"afile\", b\"data\")\n    assert fs.cat(\"afile\") == b\"data\"\n    assert \"afile\" in os.listdir(d)\n\n    with fs.open(\"bfile\", \"wb\") as f:\n        f.write(b\"more\")\n    with fs.open(\"bfile\", \"rb\") as f:\n        assert f.read() == b\"more\"\n\n    assert fs.info(\"bfile\")[\"size\"] == 4\n    fs.rm(\"afile\")\n\n    assert \"afile\" not in os.listdir(d)\n", "fsspec/implementations/tests/test_zip.py": "import collections.abc\nimport os.path\n\nimport pytest\n\nimport fsspec\nfrom fsspec.implementations.tests.test_archive import archive_data, tempzip\n\n\ndef test_info():\n    with tempzip(archive_data) as z:\n        fs = fsspec.filesystem(\"zip\", fo=z)\n\n        # Iterate over all files.\n        for f in archive_data:\n            lhs = fs.info(f)\n\n            # Probe some specific fields of Zip archives.\n            assert \"CRC\" in lhs\n            assert \"compress_size\" in lhs\n\n\ndef test_fsspec_get_mapper():\n    \"\"\"Added for #788\"\"\"\n\n    with tempzip(archive_data) as z:\n        mapping = fsspec.get_mapper(f\"zip::{z}\")\n\n        assert isinstance(mapping, collections.abc.Mapping)\n        keys = sorted(mapping.keys())\n        assert keys == [\"a\", \"b\", \"deeply/nested/path\"]\n\n        # mapping.getitems() will call FSMap.fs.cat()\n        # which was not accurately implemented for zip.\n        assert isinstance(mapping, fsspec.mapping.FSMap)\n        items = dict(mapping.getitems(keys))\n        assert items == {\"a\": b\"\", \"b\": b\"hello\", \"deeply/nested/path\": b\"stuff\"}\n\n\ndef test_not_cached():\n    with tempzip(archive_data) as z:\n        fs = fsspec.filesystem(\"zip\", fo=z)\n        fs2 = fsspec.filesystem(\"zip\", fo=z)\n        assert fs is not fs2\n\n\ndef test_root_info():\n    with tempzip(archive_data) as z:\n        fs = fsspec.filesystem(\"zip\", fo=z)\n        assert fs.info(\"/\") == {\"name\": \"\", \"type\": \"directory\", \"size\": 0}\n        assert fs.info(\"\") == {\"name\": \"\", \"type\": \"directory\", \"size\": 0}\n\n\ndef test_write_seek(m):\n    with m.open(\"afile.zip\", \"wb\") as f:\n        fs = fsspec.filesystem(\"zip\", fo=f, mode=\"w\")\n        fs.pipe(\"another\", b\"hi\")\n        fs.zip.close()\n\n    with m.open(\"afile.zip\", \"rb\") as f:\n        fs = fsspec.filesystem(\"zip\", fo=f)\n        assert fs.cat(\"another\") == b\"hi\"\n\n\ndef test_rw(m):\n    # extra arg to zip means \"create archive\"\n    with fsspec.open(\n        \"zip://afile::memory://out.zip\", mode=\"wb\", zip={\"mode\": \"w\"}\n    ) as f:\n        f.write(b\"data\")\n\n    with fsspec.open(\"zip://afile::memory://out.zip\", mode=\"rb\") as f:\n        assert f.read() == b\"data\"\n\n\ndef test_mapper(m):\n    # extra arg to zip means \"create archive\"\n    mapper = fsspec.get_mapper(\"zip::memory://out.zip\", zip={\"mode\": \"w\"})\n    with pytest.raises(KeyError):\n        mapper[\"a\"]\n\n    mapper[\"a\"] = b\"data\"\n    with pytest.raises(OSError):\n        # fails because this is write mode and we cannot also read\n        mapper[\"a\"]\n    assert \"a\" in mapper  # but be can list\n\n\ndef test_zip_glob_star(m):\n    with fsspec.open(\n        \"zip://adir/afile::memory://out.zip\", mode=\"wb\", zip={\"mode\": \"w\"}\n    ) as f:\n        f.write(b\"data\")\n\n    fs, _ = fsspec.core.url_to_fs(\"zip::memory://out.zip\")\n    outfiles = fs.glob(\"*\")\n    assert len(outfiles) == 1\n\n    fs = fsspec.filesystem(\"zip\", fo=\"memory://out.zip\", mode=\"w\")\n    fs.mkdir(\"adir\")\n    fs.pipe(\"adir/afile\", b\"data\")\n    outfiles = fs.glob(\"*\")\n    assert len(outfiles) == 1\n\n    fn = f\"{os.path.dirname(os.path.abspath((__file__)))}/out.zip\"\n    fs = fsspec.filesystem(\"zip\", fo=fn, mode=\"r\")\n    outfiles = fs.glob(\"*\")\n    assert len(outfiles) == 1\n\n\ndef test_append(m, tmpdir):\n    fs = fsspec.filesystem(\"zip\", fo=\"memory://out.zip\", mode=\"w\")\n    with fs.open(\"afile\", \"wb\") as f:\n        f.write(b\"data\")\n    fs.close()\n\n    fs = fsspec.filesystem(\"zip\", fo=\"memory://out.zip\", mode=\"a\")\n    with fs.open(\"bfile\", \"wb\") as f:\n        f.write(b\"data\")\n    fs.close()\n\n    assert len(fsspec.open_files(\"zip://*::memory://out.zip\")) == 2\n\n    fs = fsspec.filesystem(\"zip\", fo=f\"{tmpdir}/out.zip\", mode=\"w\")\n    with fs.open(\"afile\", \"wb\") as f:\n        f.write(b\"data\")\n    fs.close()\n\n    fs = fsspec.filesystem(\"zip\", fo=f\"{tmpdir}/out.zip\", mode=\"a\")\n    with fs.open(\"bfile\", \"wb\") as f:\n        f.write(b\"data\")\n    fs.close()\n\n    assert len(fsspec.open_files(\"zip://*::memory://out.zip\")) == 2\n", "fsspec/implementations/tests/test_dask.py": "import pytest\n\nimport fsspec\n\npytest.importorskip(\"distributed\")\n\n\n@pytest.fixture()\ndef cli(tmpdir):\n    import dask.distributed\n\n    client = dask.distributed.Client(n_workers=1)\n\n    def setup():\n        m = fsspec.filesystem(\"memory\")\n        with m.open(\"afile\", \"wb\") as f:\n            f.write(b\"data\")\n\n    client.run(setup)\n    try:\n        yield client\n    finally:\n        client.shutdown()\n\n\ndef test_basic(cli):\n    fs = fsspec.filesystem(\"dask\", target_protocol=\"memory\")\n    assert fs.ls(\"\", detail=False) == [\"/afile\"]\n    assert fs.cat(\"/afile\") == b\"data\"\n", "fsspec/implementations/tests/test_arrow.py": "import secrets\n\nimport pytest\n\npyarrow_fs = pytest.importorskip(\"pyarrow.fs\")\nFileSystem = pyarrow_fs.FileSystem\n\nfrom fsspec.implementations.arrow import ArrowFSWrapper, HadoopFileSystem  # noqa\n\n\n@pytest.fixture(scope=\"function\")\ndef fs():\n    fs, _ = FileSystem.from_uri(\"mock://\")\n    return ArrowFSWrapper(fs)\n\n\n@pytest.fixture(scope=\"function\", params=[False, True])\ndef remote_dir(fs, request):\n    directory = secrets.token_hex(16)\n    fs.makedirs(directory)\n    yield (\"hdfs://\" if request.param else \"/\") + directory\n    fs.rm(directory, recursive=True)\n\n\ndef test_protocol():\n    fs, _ = FileSystem.from_uri(\"mock://\")\n    fss = ArrowFSWrapper(fs)\n    assert fss.protocol == \"mock\"\n\n\ndef strip_keys(original_entry):\n    entry = original_entry.copy()\n    entry.pop(\"mtime\")\n    return entry\n\n\ndef test_strip(fs):\n    assert fs._strip_protocol(\"/a/file\") == \"/a/file\"\n    assert fs._strip_protocol(\"hdfs:///a/file\") == \"/a/file\"\n    assert fs._strip_protocol(\"hdfs://1.1.1.1/a/file\") == \"/a/file\"\n    assert fs._strip_protocol(\"hdfs://1.1.1.1:8888/a/file\") == \"/a/file\"\n\n\ndef test_info(fs, remote_dir):\n    fs.touch(remote_dir + \"/a.txt\")\n    remote_dir_strip_protocol = fs._strip_protocol(remote_dir)\n    details = fs.info(remote_dir + \"/a.txt\")\n    assert details[\"type\"] == \"file\"\n    assert details[\"name\"] == remote_dir_strip_protocol + \"/a.txt\"\n    assert details[\"size\"] == 0\n\n    fs.mkdir(remote_dir + \"/dir\")\n    details = fs.info(remote_dir + \"/dir\")\n    assert details[\"type\"] == \"directory\"\n    assert details[\"name\"] == remote_dir_strip_protocol + \"/dir\"\n\n    details = fs.info(remote_dir + \"/dir/\")\n    assert details[\"name\"] == remote_dir_strip_protocol + \"/dir/\"\n\n\ndef test_move(fs, remote_dir):\n    fs.touch(remote_dir + \"/a.txt\")\n    initial_info = fs.info(remote_dir + \"/a.txt\")\n\n    fs.move(remote_dir + \"/a.txt\", remote_dir + \"/b.txt\")\n    secondary_info = fs.info(remote_dir + \"/b.txt\")\n\n    assert not fs.exists(remote_dir + \"/a.txt\")\n    assert fs.exists(remote_dir + \"/b.txt\")\n\n    initial_info.pop(\"name\")\n    secondary_info.pop(\"name\")\n    assert initial_info == secondary_info\n\n\ndef test_move_recursive(fs, remote_dir):\n    src = remote_dir + \"/src\"\n    dest = remote_dir + \"/dest\"\n\n    assert fs.isdir(src) is False\n    fs.mkdir(src)\n    assert fs.isdir(src)\n\n    fs.touch(src + \"/a.txt\")\n    fs.mkdir(src + \"/b\")\n    fs.touch(src + \"/b/c.txt\")\n    fs.move(src, dest, recursive=True)\n\n    assert fs.isdir(src) is False\n    assert not fs.exists(src)\n\n    assert fs.isdir(dest)\n    assert fs.exists(dest)\n    assert fs.cat(dest + \"/b/c.txt\") == fs.cat(dest + \"/a.txt\") == b\"\"\n\n\ndef test_copy(fs, remote_dir):\n    fs.touch(remote_dir + \"/a.txt\")\n    initial_info = fs.info(remote_dir + \"/a.txt\")\n\n    fs.copy(remote_dir + \"/a.txt\", remote_dir + \"/b.txt\")\n    secondary_info = fs.info(remote_dir + \"/b.txt\")\n\n    assert fs.exists(remote_dir + \"/a.txt\")\n    assert fs.exists(remote_dir + \"/b.txt\")\n\n    initial_info.pop(\"name\")\n    secondary_info.pop(\"name\")\n    assert strip_keys(initial_info) == strip_keys(secondary_info)\n\n\ndef test_rm(fs, remote_dir):\n    fs.touch(remote_dir + \"/a.txt\")\n    fs.rm(remote_dir + \"/a.txt\", recursive=True)\n    assert not fs.exists(remote_dir + \"/a.txt\")\n\n    fs.mkdir(remote_dir + \"/dir\")\n    fs.rm(remote_dir + \"/dir\", recursive=True)\n    assert not fs.exists(remote_dir + \"/dir\")\n\n    fs.mkdir(remote_dir + \"/dir\")\n    fs.touch(remote_dir + \"/dir/a\")\n    fs.touch(remote_dir + \"/dir/b\")\n    fs.mkdir(remote_dir + \"/dir/c/\")\n    fs.touch(remote_dir + \"/dir/c/a\")\n    fs.rm(remote_dir + \"/dir\", recursive=True)\n    assert not fs.exists(remote_dir + \"/dir\")\n\n\ndef test_ls(fs, remote_dir):\n    if remote_dir != \"/\":\n        remote_dir = remote_dir + \"/\"\n    remote_dir_strip_protocol = fs._strip_protocol(remote_dir)\n    fs.mkdir(remote_dir + \"dir/\")\n    files = set()\n    for no in range(8):\n        file = remote_dir + f\"dir/test_{no}\"\n        # we also want to make sure `fs.touch` works with protocol\n        fs.touch(file)\n        files.add(remote_dir_strip_protocol + f\"dir/test_{no}\")\n\n    assert set(fs.ls(remote_dir + \"dir/\")) == files\n\n    dirs = fs.ls(remote_dir + \"dir/\", detail=True)\n    expected = [fs.info(file) for file in files]\n\n    by_name = lambda details: details[\"name\"]\n    dirs.sort(key=by_name)\n    expected.sort(key=by_name)\n\n    assert dirs == expected\n\n\ndef test_mkdir(fs, remote_dir):\n    if remote_dir != \"/\":\n        remote_dir = remote_dir + \"/\"\n    fs.mkdir(remote_dir + \"dir/\")\n    assert fs.isdir(remote_dir + \"dir/\")\n    assert len(fs.ls(remote_dir + \"dir/\")) == 0\n\n    fs.mkdir(remote_dir + \"dir/sub\", create_parents=False)\n    assert fs.isdir(remote_dir + \"dir/sub\")\n\n\ndef test_makedirs(fs, remote_dir):\n    fs.makedirs(remote_dir + \"dir/a/b/c/\")\n    assert fs.isdir(remote_dir + \"dir/a/b/c/\")\n    assert fs.isdir(remote_dir + \"dir/a/b/\")\n    assert fs.isdir(remote_dir + \"dir/a/\")\n\n    fs.makedirs(remote_dir + \"dir/a/b/c/\", exist_ok=True)\n\n\ndef test_exceptions(fs, remote_dir):\n    with pytest.raises(FileNotFoundError):\n        with fs.open(remote_dir + \"/a.txt\"):\n            ...\n\n    with pytest.raises(FileNotFoundError):\n        fs.copy(remote_dir + \"/u.txt\", remote_dir + \"/y.txt\")\n\n\ndef test_open_rw(fs, remote_dir):\n    data = b\"dvc.org\"\n\n    with fs.open(remote_dir + \"/a.txt\", \"wb\") as stream:\n        stream.write(data)\n\n    with fs.open(remote_dir + \"/a.txt\") as stream:\n        assert stream.read() == data\n\n\ndef test_open_rw_flush(fs, remote_dir):\n    data = b\"dvc.org\"\n\n    with fs.open(remote_dir + \"/b.txt\", \"wb\") as stream:\n        for _ in range(200):\n            stream.write(data)\n            stream.write(data)\n            stream.flush()\n\n    with fs.open(remote_dir + \"/b.txt\", \"rb\") as stream:\n        assert stream.read() == data * 400\n\n\ndef test_open_append(fs, remote_dir):\n    data = b\"dvc.org\"\n\n    with fs.open(remote_dir + \"/a.txt\", \"wb\") as stream:\n        stream.write(data)\n\n    with fs.open(remote_dir + \"/a.txt\", \"ab\") as stream:\n        stream.write(data)\n\n    with fs.open(remote_dir + \"/a.txt\") as stream:\n        assert stream.read() == 2 * data\n\n\ndef test_open_seekable(fs, remote_dir):\n    data = b\"dvc.org\"\n\n    with fs.open(remote_dir + \"/a.txt\", \"wb\") as stream:\n        stream.write(data)\n\n    with fs.open(remote_dir + \"/a.txt\", \"rb\", seekable=True) as file:\n        file.seek(2)\n        assert file.read() == data[2:]\n\n\ndef test_seekable(fs, remote_dir):\n    data = b\"dvc.org\"\n\n    with fs.open(remote_dir + \"/a.txt\", \"wb\") as stream:\n        stream.write(data)\n\n    for seekable in [True, False]:\n        with fs.open(remote_dir + \"/a.txt\", \"rb\", seekable=seekable) as file:\n            assert file.seekable() == seekable\n            assert file.read() == data\n\n    with fs.open(remote_dir + \"/a.txt\", \"rb\", seekable=False) as file:\n        with pytest.raises(OSError):\n            file.seek(5)\n\n\ndef test_get_kwargs_from_urls_hadoop_fs():\n    kwargs = HadoopFileSystem._get_kwargs_from_urls(\n        \"hdfs://user@localhost:8020/?replication=2\"\n    )\n    assert kwargs[\"user\"] == \"user\"\n    assert kwargs[\"host\"] == \"localhost\"\n    assert kwargs[\"port\"] == 8020\n    assert kwargs[\"replication\"] == 2\n\n    kwargs = HadoopFileSystem._get_kwargs_from_urls(\"hdfs://user@localhost:8020/\")\n    assert kwargs[\"user\"] == \"user\"\n    assert kwargs[\"host\"] == \"localhost\"\n    assert kwargs[\"port\"] == 8020\n    assert \"replication\" not in kwargs\n", "fsspec/implementations/tests/test_dirfs.py": "import pytest\n\nfrom fsspec.asyn import AsyncFileSystem\nfrom fsspec.implementations.dirfs import DirFileSystem\nfrom fsspec.spec import AbstractFileSystem\n\nPATH = \"path/to/dir\"\nARGS = [\"foo\", \"bar\"]\nKWARGS = {\"baz\": \"baz\", \"qux\": \"qux\"}\n\n\n@pytest.fixture\ndef make_fs(mocker):\n    def _make_fs(async_impl=False, asynchronous=False):\n        attrs = {\n            \"sep\": \"/\",\n            \"async_impl\": async_impl,\n            \"_strip_protocol\": lambda path: path,\n        }\n\n        if async_impl:\n            attrs[\"asynchronous\"] = asynchronous\n            cls = AsyncFileSystem\n        else:\n            cls = AbstractFileSystem\n\n        fs = mocker.MagicMock(spec=cls, **attrs)\n\n        return fs\n\n    return _make_fs\n\n\n@pytest.fixture(\n    params=[\n        pytest.param(False, id=\"sync\"),\n        pytest.param(True, id=\"async\"),\n    ]\n)\ndef fs(make_fs, request):\n    return make_fs(async_impl=request.param)\n\n\n@pytest.fixture\ndef asyncfs(make_fs):\n    return make_fs(async_impl=True, asynchronous=True)\n\n\n@pytest.fixture\ndef make_dirfs():\n    def _make_dirfs(fs, asynchronous=False):\n        return DirFileSystem(PATH, fs, asynchronous=asynchronous)\n\n    return _make_dirfs\n\n\n@pytest.fixture\ndef dirfs(make_dirfs, fs):\n    return make_dirfs(fs)\n\n\n@pytest.fixture\ndef adirfs(make_dirfs, asyncfs):\n    return make_dirfs(asyncfs, asynchronous=True)\n\n\ndef test_dirfs(fs, asyncfs):\n    DirFileSystem(\"path\", fs)\n    DirFileSystem(\"path\", asyncfs, asynchronous=True)\n\n    with pytest.raises(ValueError):\n        DirFileSystem(\"path\", asyncfs)\n\n    with pytest.raises(ValueError):\n        DirFileSystem(\"path\", fs, asynchronous=True)\n\n\n@pytest.mark.parametrize(\n    \"root, rel, full\",\n    [\n        (\"\", \"\", \"\"),\n        (\"\", \"foo\", \"foo\"),\n        (\"root\", \"\", \"root\"),\n        (\"root\", \"foo\", \"root/foo\"),\n    ],\n)\ndef test_path(fs, root, rel, full):\n    dirfs = DirFileSystem(root, fs)\n    assert dirfs._join(rel) == full\n    assert dirfs._relpath(full) == rel\n\n\ndef test_sep(mocker, dirfs):\n    sep = mocker.Mock()\n    dirfs.fs.sep = sep\n    assert dirfs.sep == sep\n\n\n@pytest.mark.asyncio\nasync def test_set_session(mocker, adirfs):\n    adirfs.fs.set_session = mocker.AsyncMock()\n    assert (\n        await adirfs.set_session(*ARGS, **KWARGS) == adirfs.fs.set_session.return_value\n    )\n    adirfs.fs.set_session.assert_called_once_with(*ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_rm_file(adirfs):\n    await adirfs._rm_file(\"file\", **KWARGS)\n    adirfs.fs._rm_file.assert_called_once_with(f\"{PATH}/file\", **KWARGS)\n\n\ndef test_rm_file(dirfs):\n    dirfs.rm_file(\"file\", **KWARGS)\n    dirfs.fs.rm_file.assert_called_once_with(\"path/to/dir/file\", **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_rm(adirfs):\n    await adirfs._rm(\"file\", *ARGS, **KWARGS)\n    adirfs.fs._rm.assert_called_once_with(\"path/to/dir/file\", *ARGS, **KWARGS)\n\n\ndef test_rm(dirfs):\n    dirfs.rm(\"file\", *ARGS, **KWARGS)\n    dirfs.fs.rm.assert_called_once_with(\"path/to/dir/file\", *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_cp_file(adirfs):\n    await adirfs._cp_file(\"one\", \"two\", **KWARGS)\n    adirfs.fs._cp_file.assert_called_once_with(f\"{PATH}/one\", f\"{PATH}/two\", **KWARGS)\n\n\ndef test_cp_file(dirfs):\n    dirfs.cp_file(\"one\", \"two\", **KWARGS)\n    dirfs.fs.cp_file.assert_called_once_with(f\"{PATH}/one\", f\"{PATH}/two\", **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_copy(adirfs):\n    await adirfs._copy(\"one\", \"two\", *ARGS, **KWARGS)\n    adirfs.fs._copy.assert_called_once_with(\n        f\"{PATH}/one\", f\"{PATH}/two\", *ARGS, **KWARGS\n    )\n\n\ndef test_copy(dirfs):\n    dirfs.copy(\"one\", \"two\", *ARGS, **KWARGS)\n    dirfs.fs.copy.assert_called_once_with(f\"{PATH}/one\", f\"{PATH}/two\", *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_pipe(adirfs):\n    await adirfs._pipe(\"file\", *ARGS, **KWARGS)\n    adirfs.fs._pipe.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\ndef test_pipe(dirfs):\n    dirfs.pipe(\"file\", *ARGS, **KWARGS)\n    dirfs.fs.pipe.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\ndef test_pipe_dict(dirfs):\n    dirfs.pipe({\"file\": b\"foo\"}, *ARGS, **KWARGS)\n    dirfs.fs.pipe.assert_called_once_with({f\"{PATH}/file\": b\"foo\"}, *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_pipe_file(adirfs):\n    await adirfs._pipe_file(\"file\", *ARGS, **KWARGS)\n    adirfs.fs._pipe_file.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\ndef test_pipe_file(dirfs):\n    dirfs.pipe_file(\"file\", *ARGS, **KWARGS)\n    dirfs.fs.pipe_file.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_cat_file(adirfs):\n    assert (\n        await adirfs._cat_file(\"file\", *ARGS, **KWARGS)\n        == adirfs.fs._cat_file.return_value\n    )\n    adirfs.fs._cat_file.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\ndef test_cat_file(dirfs):\n    assert dirfs.cat_file(\"file\", *ARGS, **KWARGS) == dirfs.fs.cat_file.return_value\n    dirfs.fs.cat_file.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_cat(adirfs):\n    assert await adirfs._cat(\"file\", *ARGS, **KWARGS) == adirfs.fs._cat.return_value\n    adirfs.fs._cat.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\ndef test_cat(dirfs):\n    assert dirfs.cat(\"file\", *ARGS, **KWARGS) == dirfs.fs.cat.return_value\n    dirfs.fs.cat.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_cat_list(adirfs):\n    adirfs.fs._cat.return_value = {f\"{PATH}/one\": \"foo\", f\"{PATH}/two\": \"bar\"}\n    assert await adirfs._cat([\"one\", \"two\"], *ARGS, **KWARGS) == {\n        \"one\": \"foo\",\n        \"two\": \"bar\",\n    }\n    adirfs.fs._cat.assert_called_once_with(\n        [f\"{PATH}/one\", f\"{PATH}/two\"], *ARGS, **KWARGS\n    )\n\n\ndef test_cat_list(dirfs):\n    dirfs.fs.cat.return_value = {f\"{PATH}/one\": \"foo\", f\"{PATH}/two\": \"bar\"}\n    assert dirfs.cat([\"one\", \"two\"], *ARGS, **KWARGS) == {\"one\": \"foo\", \"two\": \"bar\"}\n    dirfs.fs.cat.assert_called_once_with(\n        [f\"{PATH}/one\", f\"{PATH}/two\"], *ARGS, **KWARGS\n    )\n\n\n@pytest.mark.asyncio\nasync def test_async_put_file(adirfs):\n    await adirfs._put_file(\"local\", \"file\", **KWARGS)\n    adirfs.fs._put_file.assert_called_once_with(\"local\", f\"{PATH}/file\", **KWARGS)\n\n\ndef test_put_file(dirfs):\n    dirfs.put_file(\"local\", \"file\", **KWARGS)\n    dirfs.fs.put_file.assert_called_once_with(\"local\", f\"{PATH}/file\", **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_put(adirfs):\n    await adirfs._put(\"local\", \"file\", **KWARGS)\n    adirfs.fs._put.assert_called_once_with(\"local\", f\"{PATH}/file\", **KWARGS)\n\n\ndef test_put(dirfs):\n    dirfs.put(\"local\", \"file\", **KWARGS)\n    dirfs.fs.put.assert_called_once_with(\"local\", f\"{PATH}/file\", **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_get_file(adirfs):\n    await adirfs._get_file(\"file\", \"local\", **KWARGS)\n    adirfs.fs._get_file.assert_called_once_with(f\"{PATH}/file\", \"local\", **KWARGS)\n\n\ndef test_get_file(dirfs):\n    dirfs.get_file(\"file\", \"local\", **KWARGS)\n    dirfs.fs.get_file.assert_called_once_with(f\"{PATH}/file\", \"local\", **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_get(adirfs):\n    await adirfs._get(\"file\", \"local\", **KWARGS)\n    adirfs.fs._get.assert_called_once_with(f\"{PATH}/file\", \"local\", **KWARGS)\n\n\ndef test_get(dirfs):\n    dirfs.get(\"file\", \"local\", **KWARGS)\n    dirfs.fs.get.assert_called_once_with(f\"{PATH}/file\", \"local\", **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_isfile(adirfs):\n    assert await adirfs._isfile(\"file\") == adirfs.fs._isfile.return_value\n    adirfs.fs._isfile.assert_called_once_with(f\"{PATH}/file\")\n\n\ndef test_isfile(dirfs):\n    assert dirfs.isfile(\"file\") == dirfs.fs.isfile.return_value\n    dirfs.fs.isfile.assert_called_once_with(f\"{PATH}/file\")\n\n\n@pytest.mark.asyncio\nasync def test_async_isdir(adirfs):\n    assert await adirfs._isdir(\"file\") == adirfs.fs._isdir.return_value\n    adirfs.fs._isdir.assert_called_once_with(f\"{PATH}/file\")\n\n\ndef test_isdir(dirfs):\n    assert dirfs.isdir(\"file\") == dirfs.fs.isdir.return_value\n    dirfs.fs.isdir.assert_called_once_with(f\"{PATH}/file\")\n\n\n@pytest.mark.asyncio\nasync def test_async_size(adirfs):\n    assert await adirfs._size(\"file\") == adirfs.fs._size.return_value\n    adirfs.fs._size.assert_called_once_with(f\"{PATH}/file\")\n\n\ndef test_size(dirfs):\n    assert dirfs.size(\"file\") == dirfs.fs.size.return_value\n    dirfs.fs.size.assert_called_once_with(f\"{PATH}/file\")\n\n\n@pytest.mark.asyncio\nasync def test_async_exists(adirfs):\n    assert await adirfs._exists(\"file\") == adirfs.fs._exists.return_value\n    adirfs.fs._exists.assert_called_once_with(f\"{PATH}/file\")\n\n\ndef test_exists(dirfs):\n    assert dirfs.exists(\"file\") == dirfs.fs.exists.return_value\n    dirfs.fs.exists.assert_called_once_with(f\"{PATH}/file\")\n\n\n@pytest.mark.asyncio\nasync def test_async_info(adirfs):\n    assert await adirfs._info(\"file\", **KWARGS) == adirfs.fs._info.return_value\n    adirfs.fs._info.assert_called_once_with(f\"{PATH}/file\", **KWARGS)\n\n\ndef test_info(dirfs):\n    assert dirfs.info(\"file\", **KWARGS) == dirfs.fs.info.return_value\n    dirfs.fs.info.assert_called_once_with(f\"{PATH}/file\", **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_ls(adirfs):\n    adirfs.fs._ls.return_value = [f\"{PATH}/file\"]\n    assert await adirfs._ls(\"file\", detail=False, **KWARGS) == [\"file\"]\n    adirfs.fs._ls.assert_called_once_with(f\"{PATH}/file\", detail=False, **KWARGS)\n\n\ndef test_ls(dirfs):\n    dirfs.fs.ls.return_value = [f\"{PATH}/file\"]\n    assert dirfs.ls(\"file\", detail=False, **KWARGS) == [\"file\"]\n    dirfs.fs.ls.assert_called_once_with(f\"{PATH}/file\", detail=False, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_ls_detail(adirfs):\n    adirfs.fs._ls.return_value = [{\"name\": f\"{PATH}/file\", \"foo\": \"bar\"}]\n    assert await adirfs._ls(\"file\", detail=True, **KWARGS) == [\n        {\"name\": \"file\", \"foo\": \"bar\"}\n    ]\n    adirfs.fs._ls.assert_called_once_with(f\"{PATH}/file\", detail=True, **KWARGS)\n\n\ndef test_ls_detail(dirfs):\n    dirfs.fs.ls.return_value = [{\"name\": f\"{PATH}/file\", \"foo\": \"bar\"}]\n    assert dirfs.ls(\"file\", detail=True, **KWARGS) == [{\"name\": \"file\", \"foo\": \"bar\"}]\n    dirfs.fs.ls.assert_called_once_with(f\"{PATH}/file\", detail=True, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_walk(adirfs, mocker):\n    async def _walk(path, *args, **kwargs):\n        yield (f\"{PATH}/root\", [\"foo\", \"bar\"], [\"baz\", \"qux\"])\n\n    adirfs.fs._walk = mocker.MagicMock()\n    adirfs.fs._walk.side_effect = _walk\n\n    actual = [entry async for entry in adirfs._walk(\"root\", *ARGS, **KWARGS)]\n    assert actual == [(\"root\", [\"foo\", \"bar\"], [\"baz\", \"qux\"])]\n    adirfs.fs._walk.assert_called_once_with(f\"{PATH}/root\", *ARGS, **KWARGS)\n\n\ndef test_walk(dirfs):\n    dirfs.fs.walk.return_value = iter(\n        [(f\"{PATH}/root\", [\"foo\", \"bar\"], [\"baz\", \"qux\"])]\n    )\n    assert list(dirfs.walk(\"root\", *ARGS, **KWARGS)) == [\n        (\"root\", [\"foo\", \"bar\"], [\"baz\", \"qux\"])\n    ]\n    dirfs.fs.walk.assert_called_once_with(f\"{PATH}/root\", *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_glob(adirfs):\n    adirfs.fs._glob.return_value = [f\"{PATH}/one\", f\"{PATH}/two\"]\n    assert await adirfs._glob(\"*\", **KWARGS) == [\"one\", \"two\"]\n    adirfs.fs._glob.assert_called_once_with(f\"{PATH}/*\", **KWARGS)\n\n\ndef test_glob(dirfs):\n    dirfs.fs.glob.return_value = [f\"{PATH}/one\", f\"{PATH}/two\"]\n    assert dirfs.glob(\"*\", **KWARGS) == [\"one\", \"two\"]\n    dirfs.fs.glob.assert_called_once_with(f\"{PATH}/*\", **KWARGS)\n\n\ndef test_glob_with_protocol(dirfs):\n    dirfs.fs.glob.return_value = [f\"{PATH}/one\", f\"{PATH}/two\"]\n    assert dirfs.glob(\"dir://*\", **KWARGS) == [\"one\", \"two\"]\n    dirfs.fs.glob.assert_called_once_with(f\"{PATH}/*\", **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_glob_detail(adirfs):\n    adirfs.fs._glob.return_value = {\n        f\"{PATH}/one\": {\"foo\": \"bar\"},\n        f\"{PATH}/two\": {\"baz\": \"qux\"},\n    }\n    assert await adirfs._glob(\"*\", detail=True, **KWARGS) == {\n        \"one\": {\"foo\": \"bar\"},\n        \"two\": {\"baz\": \"qux\"},\n    }\n    adirfs.fs._glob.assert_called_once_with(f\"{PATH}/*\", detail=True, **KWARGS)\n\n\ndef test_glob_detail(dirfs):\n    dirfs.fs.glob.return_value = {\n        f\"{PATH}/one\": {\"foo\": \"bar\"},\n        f\"{PATH}/two\": {\"baz\": \"qux\"},\n    }\n    assert dirfs.glob(\"*\", detail=True, **KWARGS) == {\n        \"one\": {\"foo\": \"bar\"},\n        \"two\": {\"baz\": \"qux\"},\n    }\n    dirfs.fs.glob.assert_called_once_with(f\"{PATH}/*\", detail=True, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_du(adirfs):\n    adirfs.fs._du.return_value = 1234\n    assert await adirfs._du(\"file\", *ARGS, **KWARGS) == 1234\n    adirfs.fs._du.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\ndef test_du(dirfs):\n    dirfs.fs.du.return_value = 1234\n    assert dirfs.du(\"file\", *ARGS, **KWARGS) == 1234\n    dirfs.fs.du.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_du_granular(adirfs):\n    adirfs.fs._du.return_value = {f\"{PATH}/dir/one\": 1, f\"{PATH}/dir/two\": 2}\n    assert await adirfs._du(\"dir\", *ARGS, total=False, **KWARGS) == {\n        \"dir/one\": 1,\n        \"dir/two\": 2,\n    }\n    adirfs.fs._du.assert_called_once_with(f\"{PATH}/dir\", *ARGS, total=False, **KWARGS)\n\n\ndef test_du_granular(dirfs):\n    dirfs.fs.du.return_value = {f\"{PATH}/dir/one\": 1, f\"{PATH}/dir/two\": 2}\n    assert dirfs.du(\"dir\", *ARGS, total=False, **KWARGS) == {\"dir/one\": 1, \"dir/two\": 2}\n    dirfs.fs.du.assert_called_once_with(f\"{PATH}/dir\", *ARGS, total=False, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_find(adirfs):\n    adirfs.fs._find.return_value = [f\"{PATH}/dir/one\", f\"{PATH}/dir/two\"]\n    assert await adirfs._find(\"dir\", *ARGS, **KWARGS) == [\"dir/one\", \"dir/two\"]\n    adirfs.fs._find.assert_called_once_with(f\"{PATH}/dir\", *ARGS, **KWARGS)\n\n\ndef test_find(dirfs):\n    dirfs.fs.find.return_value = [f\"{PATH}/dir/one\", f\"{PATH}/dir/two\"]\n    assert dirfs.find(\"dir\", *ARGS, **KWARGS) == [\"dir/one\", \"dir/two\"]\n    dirfs.fs.find.assert_called_once_with(f\"{PATH}/dir\", *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_find_detail(adirfs):\n    adirfs.fs._find.return_value = {\n        f\"{PATH}/dir/one\": {\"foo\": \"bar\"},\n        f\"{PATH}/dir/two\": {\"baz\": \"qux\"},\n    }\n    assert await adirfs._find(\"dir\", *ARGS, detail=True, **KWARGS) == {\n        \"dir/one\": {\"foo\": \"bar\"},\n        \"dir/two\": {\"baz\": \"qux\"},\n    }\n    adirfs.fs._find.assert_called_once_with(f\"{PATH}/dir\", *ARGS, detail=True, **KWARGS)\n\n\ndef test_find_detail(dirfs):\n    dirfs.fs.find.return_value = {\n        f\"{PATH}/dir/one\": {\"foo\": \"bar\"},\n        f\"{PATH}/dir/two\": {\"baz\": \"qux\"},\n    }\n    assert dirfs.find(\"dir\", *ARGS, detail=True, **KWARGS) == {\n        \"dir/one\": {\"foo\": \"bar\"},\n        \"dir/two\": {\"baz\": \"qux\"},\n    }\n    dirfs.fs.find.assert_called_once_with(f\"{PATH}/dir\", *ARGS, detail=True, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_expand_path(adirfs):\n    adirfs.fs._expand_path.return_value = [f\"{PATH}/file\"]\n    assert await adirfs._expand_path(\"*\", *ARGS, **KWARGS) == [\"file\"]\n    adirfs.fs._expand_path.assert_called_once_with(f\"{PATH}/*\", *ARGS, **KWARGS)\n\n\ndef test_expand_path(dirfs):\n    dirfs.fs.expand_path.return_value = [f\"{PATH}/file\"]\n    assert dirfs.expand_path(\"*\", *ARGS, **KWARGS) == [\"file\"]\n    dirfs.fs.expand_path.assert_called_once_with(f\"{PATH}/*\", *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_expand_path_list(adirfs):\n    adirfs.fs._expand_path.return_value = [f\"{PATH}/1file\", f\"{PATH}/2file\"]\n    assert await adirfs._expand_path([\"1*\", \"2*\"], *ARGS, **KWARGS) == [\n        \"1file\",\n        \"2file\",\n    ]\n    adirfs.fs._expand_path.assert_called_once_with(\n        [f\"{PATH}/1*\", f\"{PATH}/2*\"], *ARGS, **KWARGS\n    )\n\n\ndef test_expand_path_list(dirfs):\n    dirfs.fs.expand_path.return_value = [f\"{PATH}/1file\", f\"{PATH}/2file\"]\n    assert dirfs.expand_path([\"1*\", \"2*\"], *ARGS, **KWARGS) == [\"1file\", \"2file\"]\n    dirfs.fs.expand_path.assert_called_once_with(\n        [f\"{PATH}/1*\", f\"{PATH}/2*\"], *ARGS, **KWARGS\n    )\n\n\n@pytest.mark.asyncio\nasync def test_async_mkdir(adirfs):\n    await adirfs._mkdir(\"dir\", *ARGS, **KWARGS)\n    adirfs.fs._mkdir.assert_called_once_with(f\"{PATH}/dir\", *ARGS, **KWARGS)\n\n\ndef test_mkdir(dirfs):\n    dirfs.mkdir(\"dir\", *ARGS, **KWARGS)\n    dirfs.fs.mkdir.assert_called_once_with(f\"{PATH}/dir\", *ARGS, **KWARGS)\n\n\n@pytest.mark.asyncio\nasync def test_async_makedirs(adirfs):\n    await adirfs._makedirs(\"dir\", *ARGS, **KWARGS)\n    adirfs.fs._makedirs.assert_called_once_with(f\"{PATH}/dir\", *ARGS, **KWARGS)\n\n\ndef test_makedirs(dirfs):\n    dirfs.makedirs(\"dir\", *ARGS, **KWARGS)\n    dirfs.fs.makedirs.assert_called_once_with(f\"{PATH}/dir\", *ARGS, **KWARGS)\n\n\ndef test_rmdir(mocker, dirfs):\n    dirfs.fs.rmdir = mocker.Mock()\n    dirfs.rmdir(\"dir\")\n    dirfs.fs.rmdir.assert_called_once_with(f\"{PATH}/dir\")\n\n\ndef test_mv(mocker, dirfs):\n    dirfs.fs.mv = mocker.Mock()\n    dirfs.mv(\"one\", \"two\", **KWARGS)\n    dirfs.fs.mv.assert_called_once_with(f\"{PATH}/one\", f\"{PATH}/two\", **KWARGS)\n\n\ndef test_touch(mocker, dirfs):\n    dirfs.fs.touch = mocker.Mock()\n    dirfs.touch(\"file\", **KWARGS)\n    dirfs.fs.touch.assert_called_once_with(f\"{PATH}/file\", **KWARGS)\n\n\ndef test_created(mocker, dirfs):\n    dirfs.fs.created = mocker.Mock(return_value=\"date\")\n    assert dirfs.created(\"file\") == \"date\"\n    dirfs.fs.created.assert_called_once_with(f\"{PATH}/file\")\n\n\ndef test_modified(mocker, dirfs):\n    dirfs.fs.modified = mocker.Mock(return_value=\"date\")\n    assert dirfs.modified(\"file\") == \"date\"\n    dirfs.fs.modified.assert_called_once_with(f\"{PATH}/file\")\n\n\ndef test_sign(mocker, dirfs):\n    dirfs.fs.sign = mocker.Mock(return_value=\"url\")\n    assert dirfs.sign(\"file\", *ARGS, **KWARGS) == \"url\"\n    dirfs.fs.sign.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\ndef test_open(mocker, dirfs):\n    dirfs.fs.open = mocker.Mock()\n    assert dirfs.open(\"file\", *ARGS, **KWARGS) == dirfs.fs.open.return_value\n    dirfs.fs.open.assert_called_once_with(f\"{PATH}/file\", *ARGS, **KWARGS)\n\n\ndef test_from_url(m):\n    from fsspec.core import url_to_fs\n\n    m.pipe(\"inner/file\", b\"data\")\n    fs, _ = url_to_fs(\"dir::memory://inner\")\n    assert fs.ls(\"\", False) == [\"file\"]\n    assert fs.ls(\"\", True)[0][\"name\"] == \"file\"\n    assert fs.cat(\"file\") == b\"data\"\n", "fsspec/implementations/tests/test_common.py": "import datetime\nimport time\n\nimport pytest\n\nfrom fsspec import AbstractFileSystem\nfrom fsspec.implementations.tests.conftest import READ_ONLY_FILESYSTEMS\n\n\n@pytest.mark.parametrize(\"fs\", [\"local\"], indirect=[\"fs\"])\ndef test_created(fs: AbstractFileSystem, temp_file):\n    try:\n        fs.touch(temp_file)\n        created = fs.created(path=temp_file)\n        assert isinstance(created, datetime.datetime)\n    finally:\n        if not isinstance(fs, tuple(READ_ONLY_FILESYSTEMS)):\n            fs.rm(temp_file)\n\n\n@pytest.mark.parametrize(\"fs\", [\"local\", \"memory\", \"arrow\"], indirect=[\"fs\"])\ndef test_modified(fs: AbstractFileSystem, temp_file):\n    try:\n        fs.touch(temp_file)\n        # created = fs.created(path=temp_file)\n        created = datetime.datetime.now(\n            tz=datetime.timezone.utc\n        )  # pyarrow only have modified\n        time.sleep(0.05)\n        fs.touch(temp_file)\n        modified = fs.modified(path=temp_file)\n        assert isinstance(modified, datetime.datetime)\n        assert modified > created\n    finally:\n        fs.rm(temp_file)\n", "fsspec/implementations/tests/test_tar.py": "from __future__ import annotations\n\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport pytest\n\nimport fsspec\nfrom fsspec.core import OpenFile\nfrom fsspec.implementations.cached import WholeFileCacheFileSystem\nfrom fsspec.implementations.tar import TarFileSystem\nfrom fsspec.implementations.tests.test_archive import archive_data, temptar\n\n\ndef test_info():\n    with temptar(archive_data) as t:\n        fs = fsspec.filesystem(\"tar\", fo=t)\n\n        # Iterate over all directories.\n        # Probe specific fields of Tar archives.\n        for d in fs._all_dirnames(archive_data.keys()):\n            lhs = fs.info(d)\n            del lhs[\"chksum\"]\n            expected = {\n                \"name\": f\"{d}\",\n                \"size\": 0,\n                \"type\": \"directory\",\n                \"devmajor\": 0,\n                \"devminor\": 0,\n                \"gname\": \"\",\n                \"linkname\": \"\",\n                \"uid\": 0,\n                \"gid\": 0,\n                \"mode\": 420,\n                \"mtime\": 0,\n                \"uname\": \"\",\n            }\n            assert lhs == expected\n\n        # Iterate over all files.\n        for f in archive_data:\n            lhs = fs.info(f)\n\n            # Probe some specific fields of Tar archives.\n            assert \"mode\" in lhs\n            assert \"uid\" in lhs\n            assert \"gid\" in lhs\n            assert \"mtime\" in lhs\n            assert \"chksum\" in lhs\n\n\n@pytest.mark.parametrize(\n    \"recipe\",\n    [\n        {\"mode\": \"w\", \"suffix\": \".tar\", \"magic\": b\"a\\x00\\x00\\x00\\x00\"},\n        {\"mode\": \"w:gz\", \"suffix\": \".tar.gz\", \"magic\": b\"\\x1f\\x8b\\x08\\x08\"},\n        {\"mode\": \"w:bz2\", \"suffix\": \".tar.bz2\", \"magic\": b\"BZh91AY\"},\n        {\"mode\": \"w:xz\", \"suffix\": \".tar.xz\", \"magic\": b\"\\xfd7zXZ\\x00\\x00\"},\n    ],\n    ids=[\"tar\", \"tar-gz\", \"tar-bz2\", \"tar-xz\"],\n)\ndef test_compressions(recipe):\n    \"\"\"\n    Run tests on all available tar file compression variants.\n    \"\"\"\n    with temptar(archive_data, mode=recipe[\"mode\"], suffix=recipe[\"suffix\"]) as t:\n        fs = fsspec.filesystem(\"tar\", fo=t)\n\n        # Verify that the tar archive has the correct compression.\n        with open(t, \"rb\") as raw:\n            assert raw.read()[:10].startswith(recipe[\"magic\"])\n\n        # Verify content of a sample file.\n        assert fs.cat(\"b\") == b\"hello\"\n\n\n@pytest.mark.parametrize(\n    \"recipe\",\n    [\n        {\"mode\": \"w\", \"suffix\": \".tar\", \"magic\": b\"a\\x00\\x00\\x00\\x00\"},\n        {\"mode\": \"w:gz\", \"suffix\": \".tar.gz\", \"magic\": b\"\\x1f\\x8b\\x08\\x08\"},\n        {\"mode\": \"w:bz2\", \"suffix\": \".tar.bz2\", \"magic\": b\"BZh91AY\"},\n        {\"mode\": \"w:xz\", \"suffix\": \".tar.xz\", \"magic\": b\"\\xfd7zXZ\\x00\\x00\"},\n    ],\n    ids=[\"tar\", \"tar-gz\", \"tar-bz2\", \"tar-xz\"],\n)\ndef test_filesystem_direct(recipe, tmpdir):\n    \"\"\"\n    Run tests through a real fsspec filesystem implementation.\n    Here: `LocalFileSystem`.\n    \"\"\"\n\n    filename = os.path.join(tmpdir, f'temp{recipe[\"suffix\"]}')\n\n    fs = fsspec.filesystem(\"file\")\n    f = OpenFile(fs, filename, mode=\"wb\")\n\n    with temptar(archive_data, mode=recipe[\"mode\"], suffix=recipe[\"suffix\"]) as tf:\n        with f as fo:\n            fo.write(open(tf, \"rb\").read())\n\n    # Verify that the tar archive has the correct compression.\n    with open(filename, \"rb\") as raw:\n        assert raw.read()[:10].startswith(recipe[\"magic\"])\n\n    # Verify content of a sample file.\n    with fs.open(filename) as resource:\n        tarfs = fsspec.filesystem(\"tar\", fo=resource)\n        assert tarfs.cat(\"b\") == b\"hello\"\n\n\n@pytest.mark.parametrize(\n    \"recipe\",\n    [\n        {\"mode\": \"w\", \"suffix\": \".tar\", \"magic\": b\"a\\x00\\x00\\x00\\x00\"},\n        {\"mode\": \"w:gz\", \"suffix\": \".tar.gz\", \"magic\": b\"\\x1f\\x8b\\x08\\x08\"},\n        {\"mode\": \"w:bz2\", \"suffix\": \".tar.bz2\", \"magic\": b\"BZh91AY\"},\n        {\"mode\": \"w:xz\", \"suffix\": \".tar.xz\", \"magic\": b\"\\xfd7zXZ\\x00\\x00\"},\n    ],\n    ids=[\"tar\", \"tar-gz\", \"tar-bz2\", \"tar-xz\"],\n)\ndef test_filesystem_cached(recipe, tmpdir):\n    \"\"\"\n    Run tests through a real, cached, fsspec filesystem implementation.\n    Here: `TarFileSystem` over `WholeFileCacheFileSystem` over `LocalFileSystem`.\n    \"\"\"\n\n    filename = os.path.join(tmpdir, f'temp{recipe[\"suffix\"]}')\n\n    # Create a filesystem from test fixture.\n    fs = fsspec.filesystem(\"file\")\n    f = OpenFile(fs, filename, mode=\"wb\")\n\n    with temptar(archive_data, mode=recipe[\"mode\"], suffix=recipe[\"suffix\"]) as tf:\n        with f as fo:\n            fo.write(open(tf, \"rb\").read())\n\n    # Verify that the tar archive has the correct compression.\n    with open(filename, \"rb\") as raw:\n        assert raw.read()[:10].startswith(recipe[\"magic\"])\n\n    # Access cached filesystem.\n    cachedir = tempfile.mkdtemp()\n    filesystem = WholeFileCacheFileSystem(fs=fs, cache_storage=cachedir)\n\n    # Verify the cache is empty beforehand.\n    assert os.listdir(cachedir) == []\n\n    # Verify content of a sample file.\n    with filesystem.open(filename) as resource:\n        tarfs = fsspec.filesystem(\"tar\", fo=resource)\n        assert tarfs.cat(\"b\") == b\"hello\"\n\n    # Verify the cache is populated afterwards.\n    assert len(os.listdir(cachedir)) == 2\n\n    # Verify that the cache is empty after clearing it.\n    filesystem.clear_cache()\n    assert os.listdir(cachedir) == []\n\n    filesystem.clear_cache()\n    shutil.rmtree(cachedir)\n\n\n@pytest.mark.parametrize(\n    \"recipe\",\n    [\n        {\"mode\": \"w\", \"suffix\": \".tar\", \"magic\": b\"a\\x00\\x00\\x00\\x00\"},\n        {\"mode\": \"w:gz\", \"suffix\": \".tar.gz\", \"magic\": b\"\\x1f\\x8b\\x08\\x08\"},\n        {\"mode\": \"w:bz2\", \"suffix\": \".tar.bz2\", \"magic\": b\"BZh91AY\"},\n        {\"mode\": \"w:xz\", \"suffix\": \".tar.xz\", \"magic\": b\"\\xfd7zXZ\\x00\\x00\"},\n    ],\n    ids=[\"tar\", \"tar-gz\", \"tar-bz2\", \"tar-xz\"],\n)\ndef test_url_to_fs_direct(recipe, tmpdir):\n    with temptar(archive_data, mode=recipe[\"mode\"], suffix=recipe[\"suffix\"]) as tf:\n        url = f\"tar://inner::file://{tf}\"\n        fs, url = fsspec.core.url_to_fs(url=url)\n        assert fs.cat(\"b\") == b\"hello\"\n\n\n@pytest.mark.parametrize(\n    \"recipe\",\n    [\n        {\"mode\": \"w\", \"suffix\": \".tar\"},\n        {\"mode\": \"w:gz\", \"suffix\": \".tar.gz\"},\n        {\"mode\": \"w:bz2\", \"suffix\": \".tar.bz2\"},\n        {\"mode\": \"w:xz\", \"suffix\": \".tar.xz\"},\n    ],\n    ids=[\"tar\", \"tar-gz\", \"tar-bz2\", \"tar-xz\"],\n)\ndef test_url_to_fs_cached(recipe, tmpdir):\n    with temptar(archive_data, mode=recipe[\"mode\"], suffix=recipe[\"suffix\"]) as tf:\n        url = f\"tar://inner::simplecache::file://{tf}\"\n        # requires same_names in order to be able to guess compression from\n        # filename\n        fs, url = fsspec.core.url_to_fs(url=url, simplecache={\"same_names\": True})\n        assert fs.cat(\"b\") == b\"hello\"\n\n\n@pytest.mark.parametrize(\n    \"compression\", [\"\", \"gz\", \"bz2\", \"xz\"], ids=[\"tar\", \"tar-gz\", \"tar-bz2\", \"tar-xz\"]\n)\ndef test_ls_with_folders(compression: str, tmp_path: Path):\n    \"\"\"\n    Create a tar file that doesn't include the intermediate folder structure,\n    but make sure that the reading filesystem is still able to resolve the\n    intermediate folders, like the ZipFileSystem.\n    \"\"\"\n    tar_data: dict[str, bytes] = {\n        \"a.pdf\": b\"Hello A!\",\n        \"b/c.pdf\": b\"Hello C!\",\n        \"d/e/f.pdf\": b\"Hello F!\",\n        \"d/g.pdf\": b\"Hello G!\",\n    }\n    if compression:\n        temp_archive_file = tmp_path / f\"test_tar_file.tar.{compression}\"\n    else:\n        temp_archive_file = tmp_path / \"test_tar_file.tar\"\n    with open(temp_archive_file, \"wb\") as fd:\n        # We need to manually write the tarfile here, because temptar\n        # creates intermediate directories which is not how tars are always created\n        with tarfile.open(fileobj=fd, mode=f\"w:{compression}\") as tf:\n            for tar_file_path, data in tar_data.items():\n                content = data\n                info = tarfile.TarInfo(name=tar_file_path)\n                info.size = len(content)\n                tf.addfile(info, BytesIO(content))\n    with open(temp_archive_file, \"rb\") as fd:\n        fs = TarFileSystem(fd)\n        assert fs.find(\"/\", withdirs=True) == [\n            \"a.pdf\",\n            \"b\",\n            \"b/c.pdf\",\n            \"d\",\n            \"d/e\",\n            \"d/e/f.pdf\",\n            \"d/g.pdf\",\n        ]\n", "fsspec/implementations/tests/test_dbfs.py": "\"\"\"\nTest-Cases for the DataBricks Filesystem.\nThis test case is somewhat special, as there is no \"mock\" databricks\nAPI available. We use the [vcr(https://github.com/kevin1024/vcrpy)\npackage to record the requests and responses to the real databricks API and\nreplay them on tests.\n\nThis however means, that when you change the tests (or when the API\nitself changes, which is very unlikely to occur as it is versioned),\nyou need to re-record the answers. This can be done as follows:\n\n1. Delete all casettes files in the \"./cassettes/test_dbfs\" folder\n2. Spin up a databricks cluster. For example,\n   you can use an Azure Databricks instance for this.\n3. Take note of the instance details (the instance URL. For example for an Azure\n   databricks cluster, this has the form\n   adb-<some-number>.<two digits>.azuredatabricks.net)\n   and your personal token (Find out more here:\n   https://docs.databricks.com/dev-tools/api/latest/authentication.html)\n4. Set the two environment variables `DBFS_INSTANCE` and `DBFS_TOKEN`\n5. Now execute the tests as normal. The results of the API calls will be recorded.\n6. Unset the environment variables and replay the tests.\n\"\"\"\n\nimport os\nimport sys\nfrom urllib.parse import urlparse\n\nimport numpy\nimport pytest\n\nimport fsspec\n\nif sys.version_info >= (3, 10):\n    pytest.skip(\"These tests need to be re-recorded.\", allow_module_level=True)\n\nDUMMY_INSTANCE = \"my_instance.com\"\nINSTANCE = os.getenv(\"DBFS_INSTANCE\", DUMMY_INSTANCE)\nTOKEN = os.getenv(\"DBFS_TOKEN\", \"\")\n\n\n@pytest.fixture(scope=\"module\")\ndef vcr_config():\n    \"\"\"\n    To not record information in the instance and token details\n    (which are sensitive), we delete them from both the\n    request and the response before storing it.\n    We also delete the date as it is likely to change\n    (and will make git diffs harder).\n    If the DBFS_TOKEN env variable is set, we record with VCR.\n    If not, we only replay (to not accidentally record with a wrong URL).\n    \"\"\"\n\n    def before_record_response(response):\n        try:\n            del response[\"headers\"][\"x-databricks-org-id\"]\n            del response[\"headers\"][\"date\"]\n        except KeyError:\n            pass\n        return response\n\n    def before_record_request(request):\n        # Replace the instance URL\n        uri = urlparse(request.uri)\n        uri = uri._replace(netloc=DUMMY_INSTANCE)\n        request.uri = uri.geturl()\n\n        return request\n\n    if TOKEN:\n        return {\n            \"record_mode\": \"once\",\n            \"filter_headers\": [(\"authorization\", \"DUMMY\")],\n            \"before_record_response\": before_record_response,\n            \"before_record_request\": before_record_request,\n        }\n    else:\n        return {\n            \"record_mode\": \"none\",\n        }\n\n\n@pytest.fixture\ndef dbfsFS():\n    fs = fsspec.filesystem(\"dbfs\", instance=INSTANCE, token=TOKEN)\n\n    return fs\n\n\n@pytest.fixture\ndef make_mock_diabetes_ds():\n    pa = pytest.importorskip(\"pyarrow\")\n\n    names = [\n        \"Pregnancies\",\n        \"Glucose\",\n        \"BloodPressure\",\n        \"SkinThickness\",\n        \"Insulin\",\n        \"BMI\",\n        \"DiabetesPedigreeFunction\",\n        \"Age\",\n        \"Outcome\",\n    ]\n    pregnancies = pa.array(numpy.random.randint(low=0, high=17, size=25))\n    glucose = pa.array(numpy.random.randint(low=0, high=199, size=25))\n    blood_pressure = pa.array(numpy.random.randint(low=0, high=122, size=25))\n    skin_thickness = pa.array(numpy.random.randint(low=0, high=99, size=25))\n    insulin = pa.array(numpy.random.randint(low=0, high=846, size=25))\n    bmi = pa.array(numpy.random.uniform(0.0, 67.1, size=25))\n    diabetes_pedigree_function = pa.array(numpy.random.uniform(0.08, 2.42, size=25))\n    age = pa.array(numpy.random.randint(low=21, high=81, size=25))\n    outcome = pa.array(numpy.random.randint(low=0, high=1, size=25))\n\n    return pa.Table.from_arrays(\n        arrays=[\n            pregnancies,\n            glucose,\n            blood_pressure,\n            skin_thickness,\n            insulin,\n            bmi,\n            diabetes_pedigree_function,\n            age,\n            outcome,\n        ],\n        names=names,\n    )\n\n\n@pytest.mark.vcr()\ndef test_dbfs_file_listing(dbfsFS):\n    assert \"/FileStore\" in dbfsFS.ls(\"/\", detail=False)\n    assert {\"name\": \"/FileStore\", \"size\": 0, \"type\": \"directory\"} in dbfsFS.ls(\n        \"/\", detail=True\n    )\n\n\n@pytest.mark.vcr()\ndef test_dbfs_mkdir(dbfsFS):\n    dbfsFS.rm(\"/FileStore/my\", recursive=True)\n    assert \"/FileStore/my\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n\n    dbfsFS.mkdir(\"/FileStore/my/dir\", create_parents=True)\n\n    assert \"/FileStore/my\" in dbfsFS.ls(\"/FileStore/\", detail=False)\n    assert \"/FileStore/my/dir\" in dbfsFS.ls(\"/FileStore/my/\", detail=False)\n\n    with pytest.raises(FileExistsError):\n        dbfsFS.mkdir(\"/FileStore/my/dir\", create_parents=True, exist_ok=False)\n\n    with pytest.raises(OSError):\n        dbfsFS.rm(\"/FileStore/my\", recursive=False)\n\n    assert \"/FileStore/my\" in dbfsFS.ls(\"/FileStore/\", detail=False)\n\n    dbfsFS.rm(\"/FileStore/my\", recursive=True)\n    assert \"/FileStore/my\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n\n\n@pytest.mark.vcr()\ndef test_dbfs_write_and_read(dbfsFS):\n    dbfsFS.rm(\"/FileStore/file.csv\")\n    assert \"/FileStore/file.csv\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n\n    content = b\"This is a test\\n\" * 100000 + b\"For this is the end\\n\"\n\n    with dbfsFS.open(\"/FileStore/file.csv\", \"wb\") as f:\n        f.write(content)\n\n    assert \"/FileStore/file.csv\" in dbfsFS.ls(\"/FileStore\", detail=False)\n\n    with dbfsFS.open(\"/FileStore/file.csv\", \"rb\") as f:\n        data = f.read()\n        assert data == content\n    dbfsFS.rm(\"/FileStore/file.csv\")\n    assert \"/FileStore/file.csv\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n\n\n@pytest.mark.vcr()\ndef test_dbfs_read_range(dbfsFS):\n    dbfsFS.rm(\"/FileStore/file.txt\")\n    assert \"/FileStore/file.txt\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n    content = b\"This is a test\\n\"\n    with dbfsFS.open(\"/FileStore/file.txt\", \"wb\") as f:\n        f.write(content)\n    assert \"/FileStore/file.txt\" in dbfsFS.ls(\"/FileStore\", detail=False)\n    assert dbfsFS.cat_file(\"/FileStore/file.txt\", start=8, end=14) == content[8:14]\n    dbfsFS.rm(\"/FileStore/file.txt\")\n    assert \"/FileStore/file.txt\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n\n\n@pytest.mark.vcr()\ndef test_dbfs_read_range_chunked(dbfsFS):\n    dbfsFS.rm(\"/FileStore/large_file.txt\")\n    assert \"/FileStore/large_file.txt\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n    content = b\"This is a test\\n\" * (1 * 2**18) + b\"For this is the end\\n\"\n    with dbfsFS.open(\"/FileStore/large_file.txt\", \"wb\") as f:\n        f.write(content)\n    assert \"/FileStore/large_file.txt\" in dbfsFS.ls(\"/FileStore\", detail=False)\n    assert dbfsFS.cat_file(\"/FileStore/large_file.txt\", start=8) == content[8:]\n    dbfsFS.rm(\"/FileStore/large_file.txt\")\n    assert \"/FileStore/large_file.txt\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n\n\n@pytest.mark.vcr()\ndef test_dbfs_write_pyarrow_non_partitioned(dbfsFS, make_mock_diabetes_ds):\n    pytest.importorskip(\"pyarrow.dataset\")\n    pq = pytest.importorskip(\"pyarrow.parquet\")\n\n    dbfsFS.rm(\"/FileStore/pyarrow\", recursive=True)\n    assert \"/FileStore/pyarrow\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n\n    pq.write_to_dataset(\n        make_mock_diabetes_ds,\n        filesystem=dbfsFS,\n        compression=\"none\",\n        existing_data_behavior=\"error\",\n        root_path=\"/FileStore/pyarrow/diabetes\",\n        use_threads=False,\n    )\n\n    assert len(dbfsFS.ls(\"/FileStore/pyarrow/diabetes\", detail=False)) == 1\n    assert (\n        \"/FileStore/pyarrow/diabetes\"\n        in dbfsFS.ls(\"/FileStore/pyarrow/diabetes\", detail=False)[0]\n        and \".parquet\" in dbfsFS.ls(\"/FileStore/pyarrow/diabetes\", detail=False)[0]\n    )\n\n    dbfsFS.rm(\"/FileStore/pyarrow\", recursive=True)\n    assert \"/FileStore/pyarrow\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n\n\n@pytest.mark.vcr()\ndef test_dbfs_read_pyarrow_non_partitioned(dbfsFS, make_mock_diabetes_ds):\n    ds = pytest.importorskip(\"pyarrow.dataset\")\n    pq = pytest.importorskip(\"pyarrow.parquet\")\n\n    dbfsFS.rm(\"/FileStore/pyarrow\", recursive=True)\n    assert \"/FileStore/pyarrow\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n\n    pq.write_to_dataset(\n        make_mock_diabetes_ds,\n        filesystem=dbfsFS,\n        compression=\"none\",\n        existing_data_behavior=\"error\",\n        root_path=\"/FileStore/pyarrow/diabetes\",\n        use_threads=False,\n    )\n\n    assert len(dbfsFS.ls(\"/FileStore/pyarrow/diabetes\", detail=False)) == 1\n    assert (\n        \"/FileStore/pyarrow/diabetes\"\n        in dbfsFS.ls(\"/FileStore/pyarrow/diabetes\", detail=False)[0]\n        and \".parquet\" in dbfsFS.ls(\"/FileStore/pyarrow/diabetes\", detail=False)[0]\n    )\n\n    arr_res = ds.dataset(\n        source=\"/FileStore/pyarrow/diabetes\",\n        filesystem=dbfsFS,\n    ).to_table()\n\n    assert arr_res.num_rows == make_mock_diabetes_ds.num_rows\n    assert arr_res.num_columns == make_mock_diabetes_ds.num_columns\n    assert set(arr_res.schema).difference(set(make_mock_diabetes_ds.schema)) == set()\n\n    dbfsFS.rm(\"/FileStore/pyarrow\", recursive=True)\n    assert \"/FileStore/pyarrow\" not in dbfsFS.ls(\"/FileStore/\", detail=False)\n", "fsspec/implementations/tests/test_webhdfs.py": "import pickle\nimport shlex\nimport subprocess\nimport time\n\nimport pytest\n\nimport fsspec\n\nrequests = pytest.importorskip(\"requests\")\n\nfrom fsspec.implementations.webhdfs import WebHDFS  # noqa: E402\n\n\n@pytest.fixture(scope=\"module\")\ndef hdfs_cluster():\n    cmd0 = shlex.split(\"htcluster shutdown\")\n    try:\n        subprocess.check_output(cmd0, stderr=subprocess.STDOUT)\n    except FileNotFoundError:\n        pytest.skip(\"htcluster not found\")\n    except subprocess.CalledProcessError as ex:\n        pytest.skip(f\"htcluster failed: {ex.output.decode()}\")\n    cmd1 = shlex.split(\"htcluster startup --image base\")\n    subprocess.check_output(cmd1)\n    try:\n        while True:\n            t = 90\n            try:\n                requests.get(\"http://localhost:50070/webhdfs/v1/?op=LISTSTATUS\")\n            except:  # noqa: E722\n                t -= 1\n                assert t > 0, \"Timeout waiting for HDFS\"\n                time.sleep(1)\n                continue\n            break\n        time.sleep(7)\n        yield \"localhost\"\n    finally:\n        subprocess.check_output(cmd0)\n\n\ndef test_pickle(hdfs_cluster):\n    w = WebHDFS(hdfs_cluster, user=\"testuser\")\n    w2 = pickle.loads(pickle.dumps(w))\n    assert w == w2\n\n\ndef test_simple(hdfs_cluster):\n    w = WebHDFS(hdfs_cluster, user=\"testuser\")\n    home = w.home_directory()\n    assert home == \"/user/testuser\"\n    with pytest.raises(PermissionError):\n        w.mkdir(\"/root\")\n\n\ndef test_url(hdfs_cluster):\n    url = \"webhdfs://testuser@localhost:50070/user/testuser/myfile\"\n    fo = fsspec.open(url, \"wb\", data_proxy={\"worker.example.com\": \"localhost\"})\n    with fo as f:\n        f.write(b\"hello\")\n    fo = fsspec.open(url, \"rb\", data_proxy={\"worker.example.com\": \"localhost\"})\n    with fo as f:\n        assert f.read() == b\"hello\"\n\n\ndef test_workflow(hdfs_cluster):\n    w = WebHDFS(\n        hdfs_cluster, user=\"testuser\", data_proxy={\"worker.example.com\": \"localhost\"}\n    )\n    fn = \"/user/testuser/testrun/afile\"\n    w.mkdir(\"/user/testuser/testrun\")\n    with w.open(fn, \"wb\") as f:\n        f.write(b\"hello\")\n    assert w.exists(fn)\n    info = w.info(fn)\n    assert info[\"size\"] == 5\n    assert w.isfile(fn)\n    assert w.cat(fn) == b\"hello\"\n    w.rm(\"/user/testuser/testrun\", recursive=True)\n    assert not w.exists(fn)\n\n\ndef test_with_gzip(hdfs_cluster):\n    from gzip import GzipFile\n\n    w = WebHDFS(\n        hdfs_cluster, user=\"testuser\", data_proxy={\"worker.example.com\": \"localhost\"}\n    )\n    fn = \"/user/testuser/gzfile\"\n    with w.open(fn, \"wb\") as f:\n        gf = GzipFile(fileobj=f, mode=\"w\")\n        gf.write(b\"hello\")\n        gf.close()\n    with w.open(fn, \"rb\") as f:\n        gf = GzipFile(fileobj=f, mode=\"r\")\n        assert gf.read() == b\"hello\"\n\n\ndef test_workflow_transaction(hdfs_cluster):\n    w = WebHDFS(\n        hdfs_cluster, user=\"testuser\", data_proxy={\"worker.example.com\": \"localhost\"}\n    )\n    fn = \"/user/testuser/testrun/afile\"\n    w.mkdirs(\"/user/testuser/testrun\")\n    with w.transaction:\n        with w.open(fn, \"wb\") as f:\n            f.write(b\"hello\")\n        assert not w.exists(fn)\n    assert w.exists(fn)\n    assert w.ukey(fn)\n    files = w.ls(\"/user/testuser/testrun\", True)\n    summ = w.content_summary(\"/user/testuser/testrun\")\n    assert summ[\"length\"] == files[0][\"size\"]\n    assert summ[\"fileCount\"] == 1\n\n    w.rm(\"/user/testuser/testrun\", recursive=True)\n    assert not w.exists(fn)\n\n\ndef test_webhdfs_cp_file(hdfs_cluster):\n    fs = WebHDFS(\n        hdfs_cluster, user=\"testuser\", data_proxy={\"worker.example.com\": \"localhost\"}\n    )\n\n    src, dst = \"/user/testuser/testrun/f1\", \"/user/testuser/testrun/f2\"\n\n    fs.mkdir(\"/user/testuser/testrun\")\n\n    with fs.open(src, \"wb\") as f:\n        f.write(b\"hello\")\n\n    fs.cp_file(src, dst)\n\n    assert fs.exists(src)\n    assert fs.exists(dst)\n    assert fs.cat(src) == fs.cat(dst)\n\n\ndef test_path_with_equals(hdfs_cluster):\n    fs = WebHDFS(\n        hdfs_cluster, user=\"testuser\", data_proxy={\"worker.example.com\": \"localhost\"}\n    )\n    path_with_equals = \"/user/testuser/some_table/datestamp=2023-11-11\"\n\n    fs.mkdir(path_with_equals)\n\n    result = fs.ls(path_with_equals)\n    assert result is not None\n    assert fs.exists(path_with_equals)\n\n\ndef test_error_handling_with_equals_in_path(hdfs_cluster):\n    fs = WebHDFS(hdfs_cluster, user=\"testuser\")\n    invalid_path_with_equals = (\n        \"/user/testuser/some_table/invalid_path=datestamp=2023-11-11\"\n    )\n\n    with pytest.raises(FileNotFoundError):\n        fs.ls(invalid_path_with_equals)\n\n\ndef test_create_and_touch_file_with_equals(hdfs_cluster):\n    fs = WebHDFS(\n        hdfs_cluster,\n        user=\"testuser\",\n        data_proxy={\"worker.example.com\": \"localhost\"},\n    )\n    base_path = \"/user/testuser/some_table/datestamp=2023-11-11\"\n    file_path = f\"{base_path}/testfile.txt\"\n\n    fs.mkdir(base_path)\n    fs.touch(file_path, \"wb\")\n    assert fs.exists(file_path)\n\n\ndef test_write_read_verify_file_with_equals(hdfs_cluster):\n    fs = WebHDFS(\n        hdfs_cluster,\n        user=\"testuser\",\n        data_proxy={\"worker.example.com\": \"localhost\"},\n    )\n    base_path = \"/user/testuser/some_table/datestamp=2023-11-11\"\n    file_path = f\"{base_path}/testfile.txt\"\n    content = b\"This is some content!\"\n\n    fs.mkdir(base_path)\n    with fs.open(file_path, \"wb\") as f:\n        f.write(content)\n\n    with fs.open(file_path, \"rb\") as f:\n        assert f.read() == content\n\n    file_info = fs.ls(base_path, detail=True)\n    assert len(file_info) == 1\n    assert file_info[0][\"name\"] == file_path\n    assert file_info[0][\"size\"] == len(content)\n", "fsspec/implementations/tests/conftest.py": "import tempfile\n\nimport pytest\n\nfrom fsspec.implementations.arrow import ArrowFSWrapper\nfrom fsspec.implementations.local import LocalFileSystem\nfrom fsspec.implementations.memory import MemoryFileSystem\n\n# A dummy filesystem that has a list of protocols\n\n\nclass MultiProtocolFileSystem(LocalFileSystem):\n    protocol = [\"file\", \"other\"]\n\n\nFILESYSTEMS = {\n    \"local\": LocalFileSystem,\n    \"multi\": MultiProtocolFileSystem,\n    \"memory\": MemoryFileSystem,\n}\n\nREAD_ONLY_FILESYSTEMS = []\n\n\n@pytest.fixture(scope=\"function\")\ndef fs(request):\n    pyarrow_fs = pytest.importorskip(\"pyarrow.fs\")\n    FileSystem = pyarrow_fs.FileSystem\n    if request.param == \"arrow\":\n        fs = ArrowFSWrapper(FileSystem.from_uri(\"file:///\")[0])\n        return fs\n    cls = FILESYSTEMS[request.param]\n    return cls()\n\n\n@pytest.fixture(scope=\"function\")\ndef temp_file():\n    with tempfile.TemporaryDirectory() as temp_dir:\n        return temp_dir + \"test-file\"\n", "fsspec/implementations/tests/__init__.py": "", "fsspec/implementations/tests/test_http.py": "import asyncio\nimport io\nimport json\nimport os\nimport sys\nimport time\n\nimport aiohttp\nimport pytest\n\nimport fsspec.asyn\nimport fsspec.utils\nfrom fsspec.implementations.http import HTTPStreamFile\nfrom fsspec.tests.conftest import data, reset_files, server, win  # noqa: F401\n\n\ndef test_list(server):\n    h = fsspec.filesystem(\"http\")\n    out = h.glob(server + \"/index/*\")\n    assert out == [server + \"/index/realfile\"]\n\n\ndef test_list_invalid_args(server):\n    with pytest.raises(TypeError):\n        h = fsspec.filesystem(\"http\", use_foobar=True)\n        h.glob(server + \"/index/*\")\n\n\ndef test_list_cache(server):\n    h = fsspec.filesystem(\"http\", use_listings_cache=True)\n    out = h.glob(server + \"/index/*\")\n    assert out == [server + \"/index/realfile\"]\n\n\ndef test_list_cache_with_expiry_time_cached(server):\n    h = fsspec.filesystem(\"http\", use_listings_cache=True, listings_expiry_time=30)\n\n    # First, the directory cache is not initialized.\n    assert not h.dircache\n\n    # By querying the filesystem with \"use_listings_cache=True\",\n    # the cache will automatically get populated.\n    out = h.glob(server + \"/index/*\")\n    assert out == [server + \"/index/realfile\"]\n\n    # Verify cache content.\n    assert len(h.dircache) == 1\n\n    out = h.glob(server + \"/index/*\")\n    assert out == [server + \"/index/realfile\"]\n\n\ndef test_list_cache_with_expiry_time_purged(server):\n    h = fsspec.filesystem(\"http\", use_listings_cache=True, listings_expiry_time=0.3)\n\n    # First, the directory cache is not initialized.\n    assert not h.dircache\n\n    # By querying the filesystem with \"use_listings_cache=True\",\n    # the cache will automatically get populated.\n    out = h.glob(server + \"/index/*\")\n    assert out == [server + \"/index/realfile\"]\n    assert len(h.dircache) == 1\n\n    # Verify cache content.\n    assert server + \"/index/\" in h.dircache\n    assert len(h.dircache.get(server + \"/index/\")) == 1\n\n    # Wait beyond the TTL / cache expiry time.\n    time.sleep(0.31)\n\n    # Verify that the cache item should have been purged.\n    cached_items = h.dircache.get(server + \"/index/\")\n    assert cached_items is None\n\n    # Verify that after clearing the item from the cache,\n    # it can get populated again.\n    out = h.glob(server + \"/index/*\")\n    assert out == [server + \"/index/realfile\"]\n    cached_items = h.dircache.get(server + \"/index/\")\n    assert len(cached_items) == 1\n\n\ndef test_list_cache_reuse(server):\n    h = fsspec.filesystem(\"http\", use_listings_cache=True, listings_expiry_time=5)\n\n    # First, the directory cache is not initialized.\n    assert not h.dircache\n\n    # By querying the filesystem with \"use_listings_cache=True\",\n    # the cache will automatically get populated.\n    out = h.glob(server + \"/index/*\")\n    assert out == [server + \"/index/realfile\"]\n\n    # Verify cache content.\n    assert len(h.dircache) == 1\n\n    # Verify another instance without caching enabled does not have cache content.\n    h = fsspec.filesystem(\"http\", use_listings_cache=False)\n    assert not h.dircache\n\n    # Verify that yet another new instance, with caching enabled,\n    # will see the same cache content again.\n    h = fsspec.filesystem(\"http\", use_listings_cache=True, listings_expiry_time=5)\n    assert len(h.dircache) == 1\n\n    # However, yet another instance with a different expiry time will also not have\n    # any valid cache content.\n    h = fsspec.filesystem(\"http\", use_listings_cache=True, listings_expiry_time=666)\n    assert len(h.dircache) == 0\n\n\ndef test_ls_raises_filenotfound(server):\n    h = fsspec.filesystem(\"http\")\n\n    with pytest.raises(FileNotFoundError):\n        h.ls(server + \"/not-a-key\")\n\n\ndef test_list_cache_with_max_paths(server):\n    h = fsspec.filesystem(\"http\", use_listings_cache=True, max_paths=5)\n    out = h.glob(server + \"/index/*\")\n    assert out == [server + \"/index/realfile\"]\n\n\ndef test_list_cache_with_skip_instance_cache(server):\n    h = fsspec.filesystem(\"http\", use_listings_cache=True, skip_instance_cache=True)\n    out = h.glob(server + \"/index/*\")\n    assert out == [server + \"/index/realfile\"]\n\n\ndef test_glob_return_subfolders(server):\n    h = fsspec.filesystem(\"http\")\n    out = h.glob(server + \"/simple/*\")\n    assert set(out) == {\n        server + \"/simple/dir/\",\n        server + \"/simple/file\",\n    }\n\n\ndef test_isdir(server):\n    h = fsspec.filesystem(\"http\")\n    assert h.isdir(server + \"/index/\")\n    assert not h.isdir(server + \"/index/realfile\")\n    assert not h.isdir(server + \"doesnotevenexist\")\n\n\ndef test_policy_arg(server):\n    h = fsspec.filesystem(\"http\", size_policy=\"get\")\n    out = h.glob(server + \"/index/*\")\n    assert out == [server + \"/index/realfile\"]\n\n\ndef test_exists(server):\n    h = fsspec.filesystem(\"http\")\n    assert not h.exists(server + \"/notafile\")\n    with pytest.raises(FileNotFoundError):\n        h.cat(server + \"/notafile\")\n\n\ndef test_read(server):\n    h = fsspec.filesystem(\"http\")\n    out = server + \"/index/realfile\"\n    with h.open(out, \"rb\") as f:\n        assert f.read() == data\n    with h.open(out, \"rb\", block_size=0) as f:\n        assert f.read() == data\n    with h.open(out, \"rb\") as f:\n        assert f.read(100) + f.read() == data\n\n\ndef test_file_pickle(server):\n    import pickle\n\n    # via HTTPFile\n    h = fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true\"})\n    out = server + \"/index/realfile\"\n\n    with fsspec.open(out, headers={\"give_length\": \"true\", \"head_ok\": \"true\"}) as f:\n        pic = pickle.loads(pickle.dumps(f))\n        assert pic.read() == data\n\n    with h.open(out, \"rb\") as f:\n        pic = pickle.dumps(f)\n        assert f.read() == data\n    with pickle.loads(pic) as f:\n        assert f.read() == data\n\n    # via HTTPStreamFile\n    h = fsspec.filesystem(\"http\")\n    out = server + \"/index/realfile\"\n    with h.open(out, \"rb\") as f:\n        out = pickle.dumps(f)\n        assert f.read() == data\n    with pickle.loads(out) as f:\n        assert f.read() == data\n\n\ndef test_methods(server):\n    h = fsspec.filesystem(\"http\")\n    url = server + \"/index/realfile\"\n    assert h.exists(url)\n    assert h.cat(url) == data\n\n\n@pytest.mark.parametrize(\n    \"headers\",\n    [\n        {},\n        {\"give_length\": \"true\"},\n        {\"give_length\": \"true\", \"head_ok\": \"true\"},\n        {\"give_range\": \"true\"},\n        {\"give_length\": \"true\", \"head_not_auth\": \"true\"},\n        {\"give_range\": \"true\", \"head_not_auth\": \"true\"},\n        {\"use_206\": \"true\", \"head_ok\": \"true\", \"head_give_length\": \"true\"},\n        {\"use_206\": \"true\", \"give_length\": \"true\"},\n        {\"use_206\": \"true\", \"give_range\": \"true\"},\n    ],\n)\ndef test_random_access(server, headers):\n    h = fsspec.filesystem(\"http\", headers=headers)\n    url = server + \"/index/realfile\"\n    with h.open(url, \"rb\") as f:\n        if headers:\n            assert f.size == len(data)\n        assert f.read(5) == data[:5]\n\n        if headers:\n            f.seek(5, 1)\n            assert f.read(5) == data[10:15]\n        else:\n            with pytest.raises(ValueError):\n                f.seek(5, 1)\n    assert f.closed\n\n\n@pytest.mark.parametrize(\n    \"headers\",\n    [\n        {\"ignore_range\": \"true\", \"head_ok\": \"true\", \"head_give_length\": \"true\"},\n        {\"ignore_range\": \"true\", \"give_length\": \"true\"},\n        {\"ignore_range\": \"true\", \"give_range\": \"true\"},\n    ],\n)\ndef test_no_range_support(server, headers):\n    h = fsspec.filesystem(\"http\", headers=headers)\n    url = server + \"/index/realfile\"\n    with h.open(url, \"rb\") as f:\n        # Random access is not possible if the server doesn't respect Range\n        f.seek(5)\n        with pytest.raises(ValueError):\n            f.read(10)\n\n        # Reading from the beginning should still work\n        f.seek(0)\n        assert f.read(10) == data[:10]\n\n\ndef test_stream_seek(server):\n    h = fsspec.filesystem(\"http\")\n    url = server + \"/index/realfile\"\n    with h.open(url, \"rb\") as f:\n        f.seek(0)  # is OK\n        data1 = f.read(5)\n        assert len(data1) == 5\n        f.seek(5)\n        f.seek(0, 1)\n        data2 = f.read()\n        assert data1 + data2 == data\n\n\ndef test_mapper_url(server):\n    h = fsspec.filesystem(\"http\")\n    mapper = h.get_mapper(server + \"/index/\")\n    assert mapper.root.startswith(\"http:\")\n    assert list(mapper)\n\n    mapper2 = fsspec.get_mapper(server + \"/index/\")\n    assert mapper2.root.startswith(\"http:\")\n    assert list(mapper) == list(mapper2)\n\n\ndef test_content_length_zero(server):\n    h = fsspec.filesystem(\n        \"http\", headers={\"give_length\": \"true\", \"zero_length\": \"true\"}\n    )\n    url = server + \"/index/realfile\"\n\n    with h.open(url, \"rb\") as f:\n        assert f.read() == data\n\n\ndef test_content_encoding_gzip(server):\n    h = fsspec.filesystem(\n        \"http\", headers={\"give_length\": \"true\", \"gzip_encoding\": \"true\"}\n    )\n    url = server + \"/index/realfile\"\n\n    with h.open(url, \"rb\") as f:\n        assert isinstance(f, HTTPStreamFile)\n        assert f.size is None\n        assert f.read() == data\n\n\ndef test_download(server, tmpdir):\n    h = fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true \"})\n    url = server + \"/index/realfile\"\n    fn = os.path.join(tmpdir, \"afile\")\n    h.get(url, fn)\n    assert open(fn, \"rb\").read() == data\n\n\ndef test_multi_download(server, tmpdir):\n    h = fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true \"})\n    urla = server + \"/index/realfile\"\n    urlb = server + \"/index/otherfile\"\n    fna = os.path.join(tmpdir, \"afile\")\n    fnb = os.path.join(tmpdir, \"bfile\")\n    h.get([urla, urlb], [fna, fnb])\n    assert open(fna, \"rb\").read() == data\n    assert open(fnb, \"rb\").read() == data\n\n\ndef test_ls(server):\n    h = fsspec.filesystem(\"http\")\n    l = h.ls(server + \"/data/20020401/\", detail=False)\n    nc = server + \"/data/20020401/GRACEDADM_CLSM0125US_7D.A20020401.030.nc4\"\n    assert nc in l\n    assert len(l) == 11\n    assert all(u[\"type\"] == \"file\" for u in h.ls(server + \"/data/20020401/\"))\n    assert h.glob(server + \"/data/20020401/*.nc4\") == [nc]\n\n\ndef test_mcat(server):\n    h = fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true \"})\n    urla = server + \"/index/realfile\"\n    urlb = server + \"/index/otherfile\"\n    out = h.cat([urla, urlb])\n    assert out == {urla: data, urlb: data}\n\n\ndef test_cat_file_range(server):\n    h = fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true \"})\n    urla = server + \"/index/realfile\"\n    assert h.cat(urla, start=1, end=10) == data[1:10]\n    assert h.cat(urla, start=1) == data[1:]\n\n    assert h.cat(urla, start=-10) == data[-10:]\n    assert h.cat(urla, start=-10, end=-2) == data[-10:-2]\n\n    assert h.cat(urla, end=-10) == data[:-10]\n\n\ndef test_cat_file_range_numpy(server):\n    np = pytest.importorskip(\"numpy\")\n    h = fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true \"})\n    urla = server + \"/index/realfile\"\n    assert h.cat(urla, start=np.int8(1), end=np.int8(10)) == data[1:10]\n    out = h.cat_ranges([urla, urla], starts=np.array([1, 5]), ends=np.array([10, 15]))\n    assert out == [data[1:10], data[5:15]]\n\n\ndef test_mcat_cache(server):\n    urla = server + \"/index/realfile\"\n    urlb = server + \"/index/otherfile\"\n    fs = fsspec.filesystem(\"simplecache\", target_protocol=\"http\")\n    assert fs.cat([urla, urlb]) == {urla: data, urlb: data}\n\n\ndef test_mcat_expand(server):\n    h = fsspec.filesystem(\"http\", headers={\"give_length\": \"true\", \"head_ok\": \"true \"})\n    out = h.cat(server + \"/index/*\")\n    assert out == {server + \"/index/realfile\": data}\n\n\ndef test_info(server):\n    fs = fsspec.filesystem(\"http\", headers={\"give_etag\": \"true\", \"head_ok\": \"true\"})\n    info = fs.info(server + \"/index/realfile\")\n    assert info[\"ETag\"] == \"xxx\"\n\n    fs = fsspec.filesystem(\"http\", headers={\"give_mimetype\": \"true\"})\n    info = fs.info(server + \"/index/realfile\")\n    assert info[\"mimetype\"] == \"text/html\"\n\n    fs = fsspec.filesystem(\"http\", headers={\"redirect\": \"true\"})\n    info = fs.info(server + \"/redirectme\")\n    assert info[\"url\"] == server + \"/index/realfile\"\n\n\n@pytest.mark.parametrize(\"method\", [\"POST\", \"PUT\"])\ndef test_put_file(server, tmp_path, method, reset_files):\n    src_file = tmp_path / \"file_1\"\n    src_file.write_bytes(data)\n\n    dwl_file = tmp_path / \"down_1\"\n\n    fs = fsspec.filesystem(\"http\", headers={\"head_ok\": \"true\", \"give_length\": \"true\"})\n    with pytest.raises(FileNotFoundError):\n        fs.info(server + \"/hey\")\n\n    fs.put_file(src_file, server + \"/hey\", method=method)\n    assert fs.info(server + \"/hey\")[\"size\"] == len(data)\n\n    fs.get_file(server + \"/hey\", dwl_file)\n    assert dwl_file.read_bytes() == data\n\n    src_file.write_bytes(b\"xxx\")\n    with open(src_file, \"rb\") as stream:\n        fs.put_file(stream, server + \"/hey_2\", method=method)\n    assert fs.cat(server + \"/hey_2\") == b\"xxx\"\n\n    fs.put_file(io.BytesIO(b\"yyy\"), server + \"/hey_3\", method=method)\n    assert fs.cat(server + \"/hey_3\") == b\"yyy\"\n\n\nasync def get_aiohttp():\n    from aiohttp import ClientSession\n\n    return ClientSession()\n\n\nasync def get_proxy():\n    class ProxyClient:\n        pass\n\n    return ProxyClient()\n\n\n@pytest.mark.xfail(\n    condition=sys.flags.optimize > 1, reason=\"no docstrings when optimised\"\n)\ndef test_docstring():\n    h = fsspec.filesystem(\"http\")\n    # most methods have empty docstrings and draw from base class, but this one\n    # is generated\n    assert h.pipe.__doc__\n\n\ndef test_async_other_thread(server):\n    import threading\n\n    loop = asyncio.get_event_loop()\n    th = threading.Thread(target=loop.run_forever)\n\n    th.daemon = True\n    th.start()\n    fs = fsspec.filesystem(\"http\", asynchronous=True, loop=loop)\n    asyncio.run_coroutine_threadsafe(fs.set_session(), loop=loop).result()\n    url = server + \"/index/realfile\"\n    cor = fs._cat([url])\n    fut = asyncio.run_coroutine_threadsafe(cor, loop=loop)\n    assert fut.result() == {url: data}\n    loop.call_soon_threadsafe(loop.stop)\n\n\ndef test_async_this_thread(server):\n    async def _():\n        fs = fsspec.filesystem(\"http\", asynchronous=True)\n\n        session = await fs.set_session()  # creates client\n\n        url = server + \"/index/realfile\"\n        with pytest.raises((NotImplementedError, RuntimeError)):\n            fs.cat([url])\n        out = await fs._cat([url])\n        del fs\n        assert out == {url: data}\n        await session.close()\n\n    asyncio.run(_())\n\n\ndef _inner_pass(fs, q, fn):\n    # pass the FS instance, but don't use it; in new process, the instance\n    # cache should be skipped to make a new instance\n    import traceback\n\n    try:\n        fs = fsspec.filesystem(\"http\")\n        q.put(fs.cat(fn))\n    except Exception:\n        q.put(traceback.format_exc())\n\n\n@pytest.mark.parametrize(\"method\", [\"spawn\", \"forkserver\"])\ndef test_processes(server, method):\n    import multiprocessing as mp\n\n    if win and method != \"spawn\":\n        pytest.skip(\"Windows can only spawn\")\n    ctx = mp.get_context(method)\n    fn = server + \"/index/realfile\"\n    fs = fsspec.filesystem(\"http\")\n\n    q = ctx.Queue()\n    p = ctx.Process(target=_inner_pass, args=(fs, q, fn))\n    p.start()\n    out = q.get()\n    assert out == fs.cat(fn)\n    p.join()\n\n\n@pytest.mark.parametrize(\"get_client\", [get_aiohttp, get_proxy])\ndef test_close(get_client):\n    fs = fsspec.filesystem(\"http\", skip_instance_cache=True)\n    fs.close_session(None, asyncio.run(get_client()))\n\n\n@pytest.mark.asyncio\nasync def test_async_file(server):\n    fs = fsspec.filesystem(\"http\", asynchronous=True, skip_instance_cache=True)\n    fn = server + \"/index/realfile\"\n    of = await fs.open_async(fn)\n    async with of as f:\n        out1 = await f.read(10)\n        assert data.startswith(out1)\n        out2 = await f.read()\n        assert data == out1 + out2\n    await fs._session.close()\n\n\ndef test_encoded(server):\n    fs = fsspec.filesystem(\"http\", encoded=True)\n    out = fs.cat(server + \"/Hello%3A%20G%C3%BCnter\", headers={\"give_path\": \"true\"})\n    assert json.loads(out)[\"path\"] == \"/Hello%3A%20G%C3%BCnter\"\n    with pytest.raises(aiohttp.client_exceptions.ClientError):\n        fs.cat(server + \"/Hello: G\u00fcnter\", headers={\"give_path\": \"true\"})\n\n    fs = fsspec.filesystem(\"http\", encoded=False)\n    out = fs.cat(server + \"/Hello: G\u00fcnter\", headers={\"give_path\": \"true\"})\n    assert json.loads(out)[\"path\"] == \"/Hello:%20G%C3%BCnter\"\n\n\ndef test_with_cache(server):\n    fs = fsspec.filesystem(\"http\", headers={\"head_ok\": \"true\", \"give_length\": \"true\"})\n    fn = server + \"/index/realfile\"\n    fs1 = fsspec.filesystem(\"blockcache\", fs=fs)\n    with fs1.open(fn, \"rb\") as f:\n        out = f.read()\n    assert out == fs1.cat(fn)\n\n\n@pytest.mark.asyncio\nasync def test_async_expand_path(server):\n    fs = fsspec.filesystem(\"http\", asynchronous=True, skip_instance_cache=True)\n\n    # maxdepth=1\n    assert await fs._expand_path(server + \"/index\", recursive=True, maxdepth=1) == [\n        server + \"/index\",\n        server + \"/index/realfile\",\n    ]\n\n    # maxdepth=0\n    with pytest.raises(ValueError):\n        await fs._expand_path(server + \"/index\", maxdepth=0)\n    with pytest.raises(ValueError):\n        await fs._expand_path(server + \"/index\", recursive=True, maxdepth=0)\n\n    await fs._session.close()\n\n\n@pytest.mark.asyncio\nasync def test_async_walk(server):\n    fs = fsspec.filesystem(\"http\", asynchronous=True, skip_instance_cache=True)\n\n    # No maxdepth\n    res = [a async for a in fs._walk(server + \"/index\")]\n    assert res == [(server + \"/index\", [], [\"realfile\"])]\n\n    # maxdepth=0\n    with pytest.raises(ValueError):\n        async for a in fs._walk(server + \"/index\", maxdepth=0):\n            pass\n\n    await fs._session.close()\n", "fsspec/implementations/tests/test_local.py": "import bz2\nimport gzip\nimport os\nimport os.path\nimport pickle\nimport posixpath\nimport sys\nimport tempfile\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport pytest\n\nimport fsspec\nfrom fsspec import compression\nfrom fsspec.core import OpenFile, get_fs_token_paths, open_files\nfrom fsspec.implementations.local import LocalFileSystem, make_path_posix\nfrom fsspec.tests.test_utils import WIN\n\nfiles = {\n    \".test.accounts.1.json\": (\n        b'{\"amount\": 100, \"name\": \"Alice\"}\\n'\n        b'{\"amount\": 200, \"name\": \"Bob\"}\\n'\n        b'{\"amount\": 300, \"name\": \"Charlie\"}\\n'\n        b'{\"amount\": 400, \"name\": \"Dennis\"}\\n'\n    ),\n    \".test.accounts.2.json\": (\n        b'{\"amount\": 500, \"name\": \"Alice\"}\\n'\n        b'{\"amount\": 600, \"name\": \"Bob\"}\\n'\n        b'{\"amount\": 700, \"name\": \"Charlie\"}\\n'\n        b'{\"amount\": 800, \"name\": \"Dennis\"}\\n'\n    ),\n}\n\ncsv_files = {\n    \".test.fakedata.1.csv\": (b\"a,b\\n1,2\\n\"),\n    \".test.fakedata.2.csv\": (b\"a,b\\n3,4\\n\"),\n}\nodir = os.getcwd()\n\n\n@pytest.fixture()\ndef cwd():\n    pth = os.getcwd().replace(\"\\\\\", \"/\")\n    assert not pth.endswith(\"/\")\n    yield pth\n\n\n@pytest.fixture()\ndef current_drive(cwd):\n    drive = os.path.splitdrive(cwd)[0]\n    assert not drive or (len(drive) == 2 and drive.endswith(\":\"))\n    yield drive\n\n\n@pytest.fixture()\ndef user_home():\n    pth = os.path.expanduser(\"~\").replace(\"\\\\\", \"/\")\n    assert not pth.endswith(\"/\")\n    yield pth\n\n\ndef winonly(*args):\n    return pytest.param(*args, marks=pytest.mark.skipif(not WIN, reason=\"Windows only\"))\n\n\ndef posixonly(*args):\n    return pytest.param(*args, marks=pytest.mark.skipif(WIN, reason=\"Posix only\"))\n\n\n@contextmanager\ndef filetexts(d, open=open, mode=\"t\"):\n    \"\"\"Dumps a number of textfiles to disk\n\n    d - dict\n        a mapping from filename to text like {'a.csv': '1,1\\n2,2'}\n\n    Since this is meant for use in tests, this context manager will\n    automatically switch to a temporary current directory, to avoid\n    race conditions when running tests in parallel.\n    \"\"\"\n    dirname = tempfile.mkdtemp()\n    try:\n        os.chdir(dirname)\n        for filename, text in d.items():\n            if dirname := os.path.dirname(filename):\n                os.makedirs(dirname, exist_ok=True)\n            f = open(filename, f\"w{mode}\")\n            try:\n                f.write(text)\n            finally:\n                try:\n                    f.close()\n                except AttributeError:\n                    pass\n\n        yield list(d)\n\n        for filename in d:\n            if os.path.exists(filename):\n                try:\n                    os.remove(filename)\n                except OSError:\n                    pass\n    finally:\n        os.chdir(odir)\n\n\ndef test_urlpath_inference_strips_protocol(tmpdir):\n    tmpdir = make_path_posix(str(tmpdir))\n    paths = [\"/\".join([tmpdir, f\"test.{i:02d}.csv\"]) for i in range(20)]\n\n    for path in paths:\n        with open(path, \"wb\") as f:\n            f.write(b\"1,2,3\\n\" * 10)\n\n    # globstring\n    protocol = \"file:///\" if sys.platform == \"win32\" else \"file://\"\n    urlpath = protocol + os.path.join(tmpdir, \"test.*.csv\")\n    _, _, paths2 = get_fs_token_paths(urlpath)\n    assert paths2 == paths\n\n    # list of paths\n    _, _, paths2 = get_fs_token_paths([protocol + p for p in paths])\n    assert paths2 == paths\n\n\ndef test_urlpath_inference_errors():\n    # Empty list\n    with pytest.raises(ValueError) as err:\n        get_fs_token_paths([])\n    assert \"empty\" in str(err.value)\n\n    pytest.importorskip(\"s3fs\")\n    # Protocols differ\n    with pytest.raises(ValueError) as err:\n        get_fs_token_paths([\"s3://test/path.csv\", \"/other/path.csv\"])\n    assert \"Protocol\" in str(err.value)\n\n\ndef test_urlpath_expand_read():\n    \"\"\"Make sure * is expanded in file paths when reading.\"\"\"\n    # when reading, globs should be expanded to read files by mask\n    with filetexts(csv_files, mode=\"b\"):\n        _, _, paths = get_fs_token_paths(\"./.*.csv\")\n        assert len(paths) == 2\n        _, _, paths = get_fs_token_paths([\"./.*.csv\"])\n        assert len(paths) == 2\n\n\ndef test_cats():\n    with filetexts(csv_files, mode=\"b\"):\n        fs = fsspec.filesystem(\"file\")\n        assert fs.cat(\".test.fakedata.1.csv\") == b\"a,b\\n1,2\\n\"\n        out = set(fs.cat([\".test.fakedata.1.csv\", \".test.fakedata.2.csv\"]).values())\n        assert out == {b\"a,b\\n1,2\\n\", b\"a,b\\n3,4\\n\"}\n        assert fs.cat(\".test.fakedata.1.csv\", None, None) == b\"a,b\\n1,2\\n\"\n        assert fs.cat(\".test.fakedata.1.csv\", start=1, end=6) == b\"a,b\\n1,2\\n\"[1:6]\n        assert fs.cat(\".test.fakedata.1.csv\", start=-1) == b\"a,b\\n1,2\\n\"[-1:]\n        assert fs.cat(\".test.fakedata.1.csv\", start=1, end=-2) == b\"a,b\\n1,2\\n\"[1:-2]\n        out = set(\n            fs.cat(\n                [\".test.fakedata.1.csv\", \".test.fakedata.2.csv\"], start=1, end=-1\n            ).values()\n        )\n        assert out == {b\"a,b\\n1,2\\n\"[1:-1], b\"a,b\\n3,4\\n\"[1:-1]}\n\n\ndef test_urlpath_expand_write():\n    \"\"\"Make sure * is expanded in file paths when writing.\"\"\"\n    _, _, paths = get_fs_token_paths(\"prefix-*.csv\", mode=\"wb\", num=2)\n    assert all(\n        p.endswith(pa) for p, pa in zip(paths, [\"/prefix-0.csv\", \"/prefix-1.csv\"])\n    )\n    _, _, paths = get_fs_token_paths([\"prefix-*.csv\"], mode=\"wb\", num=2)\n    assert all(\n        p.endswith(pa) for p, pa in zip(paths, [\"/prefix-0.csv\", \"/prefix-1.csv\"])\n    )\n    # we can read with multiple masks, but not write\n    with pytest.raises(ValueError):\n        _, _, paths = get_fs_token_paths(\n            [\"prefix1-*.csv\", \"prefix2-*.csv\"], mode=\"wb\", num=2\n        )\n\n\ndef test_open_files():\n    with filetexts(files, mode=\"b\"):\n        myfiles = open_files(\"./.test.accounts.*\")\n        assert len(myfiles) == len(files)\n        for lazy_file, data_file in zip(myfiles, sorted(files)):\n            with lazy_file as f:\n                x = f.read()\n                assert x == files[data_file]\n\n\n@pytest.mark.parametrize(\"encoding\", [\"utf-8\", \"ascii\"])\ndef test_open_files_text_mode(encoding):\n    with filetexts(files, mode=\"b\"):\n        myfiles = open_files(\"./.test.accounts.*\", mode=\"rt\", encoding=encoding)\n        assert len(myfiles) == len(files)\n        data = []\n        for file in myfiles:\n            with file as f:\n                data.append(f.read())\n        assert list(data) == [files[k].decode(encoding) for k in sorted(files)]\n\n\n@pytest.mark.parametrize(\"mode\", [\"rt\", \"rb\"])\n@pytest.mark.parametrize(\"fmt\", list(compression.compr))\ndef test_compressions(fmt, mode, tmpdir):\n    tmpdir = str(tmpdir)\n    fn = os.path.join(tmpdir, \".tmp.getsize\")\n    fs = LocalFileSystem()\n    f = OpenFile(fs, fn, compression=fmt, mode=\"wb\")\n    data = b\"Long line of readily compressible text\"\n    with f as fo:\n        fo.write(data)\n    if fmt is None:\n        assert fs.size(fn) == len(data)\n    else:\n        assert fs.size(fn) != len(data)\n\n    f = OpenFile(fs, fn, compression=fmt, mode=mode)\n    with f as fo:\n        if mode == \"rb\":\n            assert fo.read() == data\n        else:\n            assert fo.read() == data.decode()\n\n\ndef test_bad_compression():\n    with filetexts(files, mode=\"b\"):\n        for func in [open_files]:\n            with pytest.raises(ValueError):\n                func(\"./.test.accounts.*\", compression=\"not-found\")\n\n\ndef test_not_found():\n    fn = \"not-a-file\"\n    fs = LocalFileSystem()\n    with pytest.raises((FileNotFoundError, OSError)):\n        with OpenFile(fs, fn, mode=\"rb\"):\n            pass\n\n\ndef test_isfile():\n    fs = LocalFileSystem()\n    with filetexts(files, mode=\"b\"):\n        for f in files.keys():\n            assert fs.isfile(f)\n            assert fs.isfile(f\"file://{f}\")\n        assert not fs.isfile(\"not-a-file\")\n        assert not fs.isfile(\"file://not-a-file\")\n\n\ndef test_isdir():\n    fs = LocalFileSystem()\n    with filetexts(files, mode=\"b\"):\n        for f in files.keys():\n            assert fs.isdir(os.path.dirname(os.path.abspath(f)))\n            assert not fs.isdir(f)\n        assert not fs.isdir(\"not-a-dir\")\n\n\n@pytest.mark.parametrize(\"compression_opener\", [(None, open), (\"gzip\", gzip.open)])\ndef test_open_files_write(tmpdir, compression_opener):\n    tmpdir = str(tmpdir)\n    compression, opener = compression_opener\n    fn = str(tmpdir) + \"/*.part\"\n    files = open_files(fn, num=2, mode=\"wb\", compression=compression)\n    assert len(files) == 2\n    assert {f.mode for f in files} == {\"wb\"}\n    for fil in files:\n        with fil as f:\n            f.write(b\"000\")\n    files = sorted(os.listdir(tmpdir))\n    assert files == [\"0.part\", \"1.part\"]\n\n    with opener(os.path.join(tmpdir, files[0]), \"rb\") as f:\n        d = f.read()\n    assert d == b\"000\"\n\n\ndef test_pickability_of_lazy_files(tmpdir):\n    tmpdir = str(tmpdir)\n    cloudpickle = pytest.importorskip(\"cloudpickle\")\n\n    with filetexts(files, mode=\"b\"):\n        myfiles = open_files(\"./.test.accounts.*\")\n        myfiles2 = cloudpickle.loads(cloudpickle.dumps(myfiles))\n\n        for f, f2 in zip(myfiles, myfiles2):\n            assert f.path == f2.path\n            assert isinstance(f.fs, type(f2.fs))\n            with f as f_open, f2 as f2_open:\n                assert f_open.read() == f2_open.read()\n\n\ndef test_abs_paths(tmpdir):\n    tmpdir = str(tmpdir)\n    here = os.getcwd()\n    os.chdir(tmpdir)\n    with open(\"tmp\", \"w\") as f:\n        f.write(\"hi\")\n    out = LocalFileSystem().glob(\"./*\")\n    assert len(out) == 1\n    assert \"/\" in out[0]\n    assert \"tmp\" in out[0]\n\n    # I don't know what this was testing - but should avoid local paths anyway\n    # fs = LocalFileSystem()\n    os.chdir(here)\n    # with fs.open('tmp', 'r') as f:\n    #     res = f.read()\n    # assert res == 'hi'\n\n\n@pytest.mark.parametrize(\"sep\", [\"/\", \"\\\\\"])\n@pytest.mark.parametrize(\"chars\", [\"+\", \"++\", \"(\", \")\", \"|\", \"\\\\\"])\ndef test_glob_weird_characters(tmpdir, sep, chars):\n    tmpdir = make_path_posix(str(tmpdir))\n\n    subdir = f\"{tmpdir}{sep}test{chars}x\"\n    try:\n        os.makedirs(subdir, exist_ok=True)\n    except OSError as e:\n        if WIN and \"label syntax\" in str(e):\n            pytest.xfail(\"Illegal windows directory name\")\n        else:\n            raise\n    with open(subdir + sep + \"tmp\", \"w\") as f:\n        f.write(\"hi\")\n\n    out = LocalFileSystem().glob(subdir + sep + \"*\")\n    assert len(out) == 1\n    assert \"/\" in out[0]\n    assert \"tmp\" in out[0]\n\n\ndef test_globfind_dirs(tmpdir):\n    tmpdir = make_path_posix(str(tmpdir))\n    fs = fsspec.filesystem(\"file\")\n    fs.mkdir(tmpdir + \"/dir\")\n    fs.touch(tmpdir + \"/dir/afile\")\n    assert [tmpdir + \"/dir\"] == fs.glob(tmpdir + \"/*\")\n    assert fs.glob(tmpdir + \"/*\", detail=True)[tmpdir + \"/dir\"][\"type\"] == \"directory\"\n    assert (\n        fs.glob(tmpdir + \"/dir/*\", detail=True)[tmpdir + \"/dir/afile\"][\"type\"] == \"file\"\n    )\n    assert [tmpdir + \"/dir/afile\"] == fs.find(tmpdir)\n    assert [tmpdir, tmpdir + \"/dir\", tmpdir + \"/dir/afile\"] == fs.find(\n        tmpdir, withdirs=True\n    )\n\n\ndef test_touch(tmpdir):\n    import time\n\n    fn = str(tmpdir + \"/in/file\")\n    fs = fsspec.filesystem(\"file\", auto_mkdir=False)\n    with pytest.raises(OSError):\n        fs.touch(fn)\n    fs = fsspec.filesystem(\"file\", auto_mkdir=True)\n    fs.touch(fn)\n    info = fs.info(fn)\n    time.sleep(0.2)\n    fs.touch(fn)\n    info2 = fs.info(fn)\n    if not WIN:\n        assert info2[\"mtime\"] > info[\"mtime\"]\n\n\ndef test_touch_truncate(tmpdir):\n    fn = str(tmpdir + \"/tfile\")\n    fs = fsspec.filesystem(\"file\")\n    fs.touch(fn, truncate=True)\n    fs.pipe(fn, b\"a\")\n    fs.touch(fn, truncate=True)\n    assert fs.cat(fn) == b\"\"\n    fs.pipe(fn, b\"a\")\n    fs.touch(fn, truncate=False)\n    assert fs.cat(fn) == b\"a\"\n\n\ndef test_directories(tmpdir):\n    tmpdir = make_path_posix(str(tmpdir))\n    fs = LocalFileSystem()\n    fs.mkdir(tmpdir + \"/dir\")\n    assert tmpdir + \"/dir\" in fs.ls(tmpdir)\n    assert fs.ls(tmpdir, True)[0][\"type\"] == \"directory\"\n    fs.rmdir(tmpdir + \"/dir\")\n    assert not fs.ls(tmpdir)\n    assert fs.ls(fs.root_marker)\n\n\ndef test_ls_on_file(tmpdir):\n    tmpdir = make_path_posix(str(tmpdir))\n    fs = LocalFileSystem()\n    resource = tmpdir + \"/a.json\"\n    fs.touch(resource)\n    assert fs.exists(resource)\n    assert fs.ls(tmpdir) == fs.ls(resource)\n    assert fs.ls(resource, detail=True)[0] == fs.info(resource)\n\n\n@pytest.mark.parametrize(\"file_protocol\", [\"\", \"file://\"])\ndef test_file_ops(tmpdir, file_protocol):\n    tmpdir = make_path_posix(str(tmpdir))\n    tmpdir_with_protocol = file_protocol + tmpdir\n    fs = LocalFileSystem(auto_mkdir=True)\n    with pytest.raises(FileNotFoundError):\n        fs.info(tmpdir_with_protocol + \"/nofile\")\n    fs.touch(tmpdir_with_protocol + \"/afile\")\n    i1 = fs.ukey(tmpdir_with_protocol + \"/afile\")\n\n    assert tmpdir + \"/afile\" in fs.ls(tmpdir_with_protocol)\n\n    with fs.open(tmpdir_with_protocol + \"/afile\", \"wb\") as f:\n        f.write(b\"data\")\n    i2 = fs.ukey(tmpdir_with_protocol + \"/afile\")\n    assert i1 != i2  # because file changed\n\n    fs.copy(tmpdir_with_protocol + \"/afile\", tmpdir_with_protocol + \"/afile2\")\n    assert tmpdir + \"/afile2\" in fs.ls(tmpdir_with_protocol)\n\n    fs.move(tmpdir_with_protocol + \"/afile\", tmpdir_with_protocol + \"/afile3\")\n    assert not fs.exists(tmpdir_with_protocol + \"/afile\")\n\n    fs.cp(\n        tmpdir_with_protocol + \"/afile3\", tmpdir_with_protocol + \"/deeply/nested/file\"\n    )\n    assert fs.exists(tmpdir_with_protocol + \"/deeply/nested/file\")\n\n    fs.rm(tmpdir_with_protocol + \"/afile3\", recursive=True)\n    assert not fs.exists(tmpdir_with_protocol + \"/afile3\")\n\n    files = [tmpdir_with_protocol + \"/afile4\", tmpdir_with_protocol + \"/afile5\"]\n    [fs.touch(f) for f in files]\n\n    with pytest.raises(AttributeError):\n        fs.rm_file(files)\n    fs.rm(files)\n    assert all(not fs.exists(f) for f in files)\n\n    fs.touch(tmpdir_with_protocol + \"/afile6\")\n    fs.rm_file(tmpdir_with_protocol + \"/afile6\")\n    assert not fs.exists(tmpdir_with_protocol + \"/afile6\")\n\n    # IsADirectoryError raised on Linux, PermissionError on Windows\n    with pytest.raises((IsADirectoryError, PermissionError)):\n        fs.rm_file(tmpdir_with_protocol)\n\n    fs.rm(tmpdir_with_protocol, recursive=True)\n    assert not fs.exists(tmpdir_with_protocol)\n\n\ndef test_recursive_get_put(tmpdir):\n    tmpdir = make_path_posix(str(tmpdir))\n    fs = LocalFileSystem(auto_mkdir=True)\n\n    fs.mkdir(tmpdir + \"/a1/a2/a3\")\n    fs.touch(tmpdir + \"/a1/a2/a3/afile\")\n    fs.touch(tmpdir + \"/a1/afile\")\n\n    fs.get(f\"file://{tmpdir}/a1\", tmpdir + \"/b1\", recursive=True)\n    assert fs.isfile(tmpdir + \"/b1/afile\")\n    assert fs.isfile(tmpdir + \"/b1/a2/a3/afile\")\n\n    fs.put(tmpdir + \"/b1\", f\"file://{tmpdir}/c1\", recursive=True)\n    assert fs.isfile(tmpdir + \"/c1/afile\")\n    assert fs.isfile(tmpdir + \"/c1/a2/a3/afile\")\n\n\ndef test_commit_discard(tmpdir):\n    tmpdir = str(tmpdir)\n    fs = LocalFileSystem()\n    with fs.transaction:\n        with fs.open(tmpdir + \"/afile\", \"wb\") as f:\n            assert not fs.exists(tmpdir + \"/afile\")\n            f.write(b\"data\")\n        assert not fs.exists(tmpdir + \"/afile\")\n\n    assert fs._transaction is None\n    assert fs.cat(tmpdir + \"/afile\") == b\"data\"\n\n    try:\n        with fs.transaction:\n            with fs.open(tmpdir + \"/bfile\", \"wb\") as f:\n                f.write(b\"data\")\n            raise KeyboardInterrupt\n    except KeyboardInterrupt:\n        assert not fs.exists(tmpdir + \"/bfile\")\n\n\ndef test_make_path_posix():\n    cwd = os.getcwd()\n    if WIN:\n        drive = cwd[0]\n        assert make_path_posix(\"/a/posix/path\") == f\"{drive}:/a/posix/path\"\n        assert make_path_posix(\"/posix\") == f\"{drive}:/posix\"\n        # Windows drive requires trailing slash\n        assert make_path_posix(\"C:\\\\\") == \"C:/\"\n    else:\n        assert make_path_posix(\"/a/posix/path\") == \"/a/posix/path\"\n        assert make_path_posix(\"/posix\") == \"/posix\"\n    assert make_path_posix(\"relpath\") == posixpath.join(make_path_posix(cwd), \"relpath\")\n    assert make_path_posix(\"rel/path\") == posixpath.join(\n        make_path_posix(cwd), \"rel/path\"\n    )\n    # NT style\n    if WIN:\n        assert make_path_posix(\"C:\\\\path\") == \"C:/path\"\n        assert (\n            make_path_posix(\n                \"\\\\\\\\windows-server\\\\someshare\\\\path\\\\more\\\\path\\\\dir\\\\foo.parquet\",\n            )\n            == \"//windows-server/someshare/path/more/path/dir/foo.parquet\"\n        )\n        assert (\n            make_path_posix(\n                \"\\\\\\\\SERVER\\\\UserHomeFolder$\\\\me\\\\My Documents\\\\proj\\\\data\\\\fname.csv\",\n            )\n            == \"//SERVER/UserHomeFolder$/me/My Documents/proj/data/fname.csv\"\n        )\n    assert \"/\" in make_path_posix(\"rel\\\\path\")\n    # Relative\n    pp = make_path_posix(\"./path\")\n    cd = make_path_posix(cwd)\n    assert pp == cd + \"/path\"\n    # Userpath\n    userpath = make_path_posix(\"~/path\")\n    assert userpath.endswith(\"/path\")\n\n\n@pytest.mark.parametrize(\n    \"path\",\n    [\n        \"/abc/def\",\n        \"abc/def\",\n        \"\",\n        \".\",\n        \"//server/share/\",\n        \"\\\\\\\\server\\\\share\\\\\",\n        \"C:\\\\\",\n        \"d:/abc/def\",\n        \"e:\",\n        pytest.param(\n            \"\\\\\\\\server\\\\share\",\n            marks=[\n                pytest.mark.xfail(\n                    WIN and sys.version_info < (3, 11),\n                    reason=\"requires py3.11+ see: python/cpython#96290\",\n                )\n            ],\n        ),\n        pytest.param(\n            \"f:foo\",\n            marks=[pytest.mark.xfail(WIN, reason=\"unsupported\")],\n            id=\"relative-path-with-drive\",\n        ),\n    ],\n)\ndef test_make_path_posix_returns_absolute_paths(path):\n    posix_pth = make_path_posix(path)\n    assert os.path.isabs(posix_pth)\n\n\n@pytest.mark.parametrize(\"container_cls\", [list, set, tuple])\ndef test_make_path_posix_set_list_tuple(container_cls):\n    paths = container_cls(\n        [\n            \"/foo/bar\",\n            \"bar/foo\",\n        ]\n    )\n    posix_paths = make_path_posix(paths)\n    assert isinstance(posix_paths, container_cls)\n    assert posix_paths == container_cls(\n        [\n            make_path_posix(\"/foo/bar\"),\n            make_path_posix(\"bar/foo\"),\n        ]\n    )\n\n\n@pytest.mark.parametrize(\n    \"obj\",\n    [\n        1,\n        True,\n        None,\n        object(),\n    ],\n)\ndef test_make_path_posix_wrong_type(obj):\n    with pytest.raises(TypeError):\n        make_path_posix(obj)\n\n\ndef test_parent():\n    if WIN:\n        assert LocalFileSystem._parent(\"C:\\\\file or folder\") == \"C:/\"\n        assert LocalFileSystem._parent(\"C:\\\\\") == \"C:/\"\n    else:\n        assert LocalFileSystem._parent(\"/file or folder\") == \"/\"\n        assert LocalFileSystem._parent(\"/\") == \"/\"\n\n\n@pytest.mark.parametrize(\n    \"path,parent\",\n    [\n        (\"C:\\\\\", \"C:/\"),\n        (\"C:\\\\.\", \"C:/\"),\n        (\"C:\\\\.\\\\\", \"C:/\"),\n        (\"file:C:/\", \"C:/\"),\n        (\"file://C:/\", \"C:/\"),\n        (\"local:C:/\", \"C:/\"),\n        (\"local://C:/\", \"C:/\"),\n        (\"\\\\\\\\server\\\\share\", \"//server/share\"),\n        (\"\\\\\\\\server\\\\share\\\\\", \"//server/share\"),\n        (\"\\\\\\\\server\\\\share\\\\path\", \"//server/share\"),\n        (\"//server/share\", \"//server/share\"),\n        (\"//server/share/\", \"//server/share\"),\n        (\"//server/share/path\", \"//server/share\"),\n        (\"C:\\\\file or folder\", \"C:/\"),\n        (\"C:\\\\file or folder\\\\\", \"C:/\"),\n        (\"file:///\", \"{current_drive}/\"),\n        (\"file:///path\", \"{current_drive}/\"),\n    ]\n    if WIN\n    else [\n        (\"/\", \"/\"),\n        (\"/.\", \"/\"),\n        (\"/./\", \"/\"),\n        (\"file:/\", \"/\"),\n        (\"file:///\", \"/\"),\n        (\"local:/\", \"/\"),\n        (\"local:///\", \"/\"),\n        (\"/file or folder\", \"/\"),\n        (\"/file or folder/\", \"/\"),\n        (\"file:///path\", \"/\"),\n        (\"file://c/\", \"{cwd}\"),\n    ],\n)\ndef test_parent_edge_cases(path, parent, cwd, current_drive):\n    parent = parent.format(cwd=cwd, current_drive=current_drive)\n\n    assert LocalFileSystem._parent(path) == parent\n\n\ndef test_linked_files(tmpdir):\n    tmpdir = str(tmpdir)\n    fn0 = os.path.join(tmpdir, \"target\")\n    fn1 = os.path.join(tmpdir, \"link1\")\n    fn2 = os.path.join(tmpdir, \"link2\")\n    data = b\"my target data\"\n    with open(fn0, \"wb\") as f:\n        f.write(data)\n    try:\n        os.symlink(fn0, fn1)\n        os.symlink(fn0, fn2)\n    except OSError:\n        if WIN:\n            pytest.xfail(\"Ran on win without admin permissions\")\n        else:\n            raise\n\n    fs = LocalFileSystem()\n    assert fs.info(fn0)[\"type\"] == \"file\"\n    assert fs.info(fn1)[\"type\"] == \"file\"\n    assert fs.info(fn2)[\"type\"] == \"file\"\n\n    assert not fs.info(fn0)[\"islink\"]\n    assert fs.info(fn1)[\"islink\"]\n    assert fs.info(fn2)[\"islink\"]\n\n    assert fs.info(fn0)[\"size\"] == len(data)\n    assert fs.info(fn1)[\"size\"] == len(data)\n    assert fs.info(fn2)[\"size\"] == len(data)\n\n    of = fsspec.open(fn1, \"rb\")\n    with of as f:\n        assert f.read() == data\n\n    of = fsspec.open(fn2, \"rb\")\n    with of as f:\n        assert f.read() == data\n\n\ndef test_linked_files_exists(tmpdir):\n    origin = tmpdir / \"original\"\n    copy_file = tmpdir / \"copy\"\n\n    fs = LocalFileSystem()\n    fs.touch(origin)\n\n    try:\n        os.symlink(origin, copy_file)\n    except OSError:\n        if WIN:\n            pytest.xfail(\"Ran on win without admin permissions\")\n        else:\n            raise\n\n    assert fs.exists(copy_file)\n    assert fs.lexists(copy_file)\n\n    os.unlink(origin)\n\n    assert not fs.exists(copy_file)\n    assert fs.lexists(copy_file)\n\n    os.unlink(copy_file)\n\n    assert not fs.exists(copy_file)\n    assert not fs.lexists(copy_file)\n\n\ndef test_linked_directories(tmpdir):\n    tmpdir = str(tmpdir)\n\n    subdir0 = os.path.join(tmpdir, \"target\")\n    subdir1 = os.path.join(tmpdir, \"link1\")\n    subdir2 = os.path.join(tmpdir, \"link2\")\n\n    os.makedirs(subdir0)\n\n    try:\n        os.symlink(subdir0, subdir1)\n        os.symlink(subdir0, subdir2)\n    except OSError:\n        if WIN:\n            pytest.xfail(\"Ran on win without admin permissions\")\n        else:\n            raise\n\n    fs = LocalFileSystem()\n    assert fs.info(subdir0)[\"type\"] == \"directory\"\n    assert fs.info(subdir1)[\"type\"] == \"directory\"\n    assert fs.info(subdir2)[\"type\"] == \"directory\"\n\n    assert not fs.info(subdir0)[\"islink\"]\n    assert fs.info(subdir1)[\"islink\"]\n    assert fs.info(subdir2)[\"islink\"]\n\n\ndef test_isfilestore():\n    fs = LocalFileSystem(auto_mkdir=False)\n    assert fs._isfilestore()\n\n\ndef test_pickle(tmpdir):\n    fs = LocalFileSystem()\n    tmpdir = str(tmpdir)\n    fn0 = os.path.join(tmpdir, \"target\")\n\n    with open(fn0, \"wb\") as f:\n        f.write(b\"data\")\n\n    f = fs.open(fn0, \"rb\")\n    f.seek(1)\n    f2 = pickle.loads(pickle.dumps(f))\n    assert f2.read() == f.read()\n\n    f = fs.open(fn0, \"wb\")\n    with pytest.raises(ValueError):\n        pickle.dumps(f)\n\n    # with context\n    with fs.open(fn0, \"rb\") as f:\n        f.seek(1)\n        f2 = pickle.loads(pickle.dumps(f))\n        assert f2.tell() == 1\n        assert f2.read() == f.read()\n\n    # with fsspec.open https://github.com/fsspec/filesystem_spec/issues/579\n    with fsspec.open(fn0, \"rb\") as f:\n        f.seek(1)\n        f2 = pickle.loads(pickle.dumps(f))\n        assert f2.tell() == 1\n        assert f2.read() == f.read()\n\n\n@pytest.mark.parametrize(\n    \"uri, expected\",\n    [\n        (\"file://~/foo/bar\", \"{user_home}/foo/bar\"),\n        (\"~/foo/bar\", \"{user_home}/foo/bar\"),\n        winonly(\"~\\\\foo\\\\bar\", \"{user_home}/foo/bar\"),\n        winonly(\"file://~\\\\foo\\\\bar\", \"{user_home}/foo/bar\"),\n    ],\n)\ndef test_strip_protocol_expanduser(uri, expected, user_home):\n    expected = expected.format(user_home=user_home)\n\n    stripped = LocalFileSystem._strip_protocol(uri)\n    assert expected == stripped\n\n\n@pytest.mark.parametrize(\n    \"uri, expected\",\n    [\n        (\"file://\", \"{cwd}\"),\n        (\"file://.\", \"{cwd}\"),\n        (\"file://./\", \"{cwd}\"),\n        (\"./\", \"{cwd}\"),\n        (\"file:path\", \"{cwd}/path\"),\n        (\"file://path\", \"{cwd}/path\"),\n        (\"path\", \"{cwd}/path\"),\n        (\"./path\", \"{cwd}/path\"),\n        winonly(\".\\\\\", \"{cwd}\"),\n        winonly(\"file://.\\\\path\", \"{cwd}/path\"),\n    ],\n)\ndef test_strip_protocol_relative_paths(uri, expected, cwd):\n    expected = expected.format(cwd=cwd)\n\n    stripped = LocalFileSystem._strip_protocol(uri)\n    assert expected == stripped\n\n\n@pytest.mark.parametrize(\n    \"uri, expected\",\n    [\n        posixonly(\"file:/foo/bar\", \"/foo/bar\"),\n        winonly(\"file:/foo/bar\", \"{current_drive}/foo/bar\"),\n        winonly(\"file:\\\\foo\\\\bar\", \"{current_drive}/foo/bar\"),\n        winonly(\"file:D:\\\\path\\\\file\", \"D:/path/file\"),\n        winonly(\"file:/D:\\\\path\\\\file\", \"D:/path/file\"),\n        winonly(\"file://D:\\\\path\\\\file\", \"D:/path/file\"),\n    ],\n)\ndef test_strip_protocol_no_authority(uri, expected, cwd, current_drive):\n    expected = expected.format(cwd=cwd, current_drive=current_drive)\n\n    stripped = LocalFileSystem._strip_protocol(uri)\n    assert expected == stripped\n\n\n@pytest.mark.parametrize(\n    \"uri, expected\",\n    [\n        (\"file:/path\", \"/path\"),\n        (\"file:///path\", \"/path\"),\n        (\"file:////path\", \"//path\"),\n        (\"local:/path\", \"/path\"),\n        (\"s3://bucket/key\", \"{cwd}/s3://bucket/key\"),\n        (\"/path\", \"/path\"),\n        (\"file:///\", \"/\"),\n    ]\n    if not WIN\n    else [\n        (\"file:c:/path\", \"c:/path\"),\n        (\"file:/c:/path\", \"c:/path\"),\n        (\"file:/C:/path\", \"C:/path\"),\n        (\"file://c:/path\", \"c:/path\"),\n        (\"file:///c:/path\", \"c:/path\"),\n        (\"local:/path\", \"{current_drive}/path\"),\n        (\"s3://bucket/key\", \"{cwd}/s3://bucket/key\"),\n        (\"c:/path\", \"c:/path\"),\n        (\"c:\\\\path\", \"c:/path\"),\n        (\"file:///\", \"{current_drive}/\"),\n        pytest.param(\n            \"file://localhost/c:/path\",\n            \"c:/path\",\n            marks=pytest.mark.xfail(\n                reason=\"rfc8089 section3 'localhost uri' not supported\"\n            ),\n        ),\n    ],\n)\ndef test_strip_protocol_absolute_paths(uri, expected, current_drive, cwd):\n    expected = expected.format(current_drive=current_drive, cwd=cwd)\n\n    stripped = LocalFileSystem._strip_protocol(uri)\n    assert expected == stripped\n\n\n@pytest.mark.parametrize(\n    \"uri, expected\",\n    [\n        (\"file:c|/path\", \"c:/path\"),\n        (\"file:/D|/path\", \"D:/path\"),\n        (\"file:///C|/path\", \"C:/path\"),\n    ],\n)\n@pytest.mark.skipif(not WIN, reason=\"Windows only\")\n@pytest.mark.xfail(WIN, reason=\"legacy dos uris not supported\")\ndef test_strip_protocol_legacy_dos_uris(uri, expected):\n    stripped = LocalFileSystem._strip_protocol(uri)\n    assert expected == stripped\n\n\n@pytest.mark.parametrize(\n    \"uri, stripped\",\n    [\n        (\"file://remote/share/pth\", \"{cwd}/remote/share/pth\"),\n        (\"file:////remote/share/pth\", \"//remote/share/pth\"),\n        (\"file://///remote/share/pth\", \"///remote/share/pth\"),\n        (\"//remote/share/pth\", \"//remote/share/pth\"),\n        winonly(\"\\\\\\\\remote\\\\share\\\\pth\", \"//remote/share/pth\"),\n    ],\n)\ndef test_strip_protocol_windows_remote_shares(uri, stripped, cwd):\n    stripped = stripped.format(cwd=cwd)\n\n    assert LocalFileSystem._strip_protocol(uri) == stripped\n\n\ndef test_mkdir_twice_faile(tmpdir):\n    fn = os.path.join(tmpdir, \"test\")\n    fs = fsspec.filesystem(\"file\")\n    fs.mkdir(fn)\n    with pytest.raises(FileExistsError):\n        fs.mkdir(fn)\n\n\ndef test_iterable(tmpdir):\n    data = b\"a\\nhello\\noi\"\n    fn = os.path.join(tmpdir, \"test\")\n    with open(fn, \"wb\") as f:\n        f.write(data)\n    of = fsspec.open(f\"file://{fn}\", \"rb\")\n    with of as f:\n        out = list(f)\n    assert b\"\".join(out) == data\n\n\ndef test_mv_empty(tmpdir):\n    localfs = fsspec.filesystem(\"file\")\n    src = os.path.join(str(tmpdir), \"src\")\n    dest = os.path.join(str(tmpdir), \"dest\")\n    assert localfs.isdir(src) is False\n    localfs.mkdir(src)\n    assert localfs.isdir(src)\n    localfs.move(src, dest, recursive=True)\n    assert localfs.isdir(src) is False\n    assert localfs.isdir(dest)\n    assert localfs.info(dest)\n\n\ndef test_mv_recursive(tmpdir):\n    localfs = fsspec.filesystem(\"file\")\n    src = os.path.join(str(tmpdir), \"src\")\n    dest = os.path.join(str(tmpdir), \"dest\")\n    assert localfs.isdir(src) is False\n    localfs.mkdir(src)\n    assert localfs.isdir(src)\n    localfs.touch(os.path.join(src, \"afile\"))\n    localfs.move(src, dest, recursive=True)\n    assert localfs.isdir(src) is False\n    assert localfs.isdir(dest)\n    assert localfs.info(os.path.join(dest, \"afile\"))\n\n\n@pytest.mark.xfail(WIN, reason=\"windows expand path to be revisited\")\ndef test_copy_errors(tmpdir):\n    localfs = fsspec.filesystem(\"file\", auto_mkdir=True)\n\n    dest1 = os.path.join(str(tmpdir), \"dest1\")\n    dest2 = os.path.join(str(tmpdir), \"dest2\")\n\n    src = os.path.join(str(tmpdir), \"src\")\n    file1 = os.path.join(src, \"afile1\")\n    file2 = os.path.join(src, \"afile2\")\n    dne = os.path.join(str(tmpdir), \"src\", \"notafile\")\n\n    localfs.mkdir(src)\n    localfs.mkdir(dest1)\n    localfs.mkdir(dest2)\n    localfs.touch(file1)\n    localfs.touch(file2)\n\n    # Non recursive should raise an error unless we specify ignore\n    with pytest.raises(FileNotFoundError):\n        localfs.copy([file1, file2, dne], dest1)\n\n    localfs.copy([file1, file2, dne], dest1, on_error=\"ignore\")\n\n    assert sorted(localfs.ls(dest1)) == [\n        make_path_posix(os.path.join(dest1, \"afile1\")),\n        make_path_posix(os.path.join(dest1, \"afile2\")),\n    ]\n\n    # Recursive should raise an error only if we specify raise\n    # the patch simulates the filesystem finding a file that does not\n    # exist in the directory\n    current_files = localfs.expand_path(src, recursive=True)\n    with patch.object(localfs, \"expand_path\", return_value=current_files + [dne]):\n        with pytest.raises(FileNotFoundError):\n            localfs.copy(src + \"/\", dest2, recursive=True, on_error=\"raise\")\n\n        localfs.copy(src + \"/\", dest2, recursive=True)\n        assert sorted(localfs.ls(dest2)) == [\n            make_path_posix(os.path.join(dest2, \"afile1\")),\n            make_path_posix(os.path.join(dest2, \"afile2\")),\n        ]\n\n\ndef test_transaction(tmpdir):\n    file = str(tmpdir / \"test.txt\")\n    fs = LocalFileSystem()\n\n    with fs.transaction:\n        content = \"hello world\"\n        with fs.open(file, \"w\") as fp:\n            fp.write(content)\n\n    with fs.open(file, \"r\") as fp:\n        read_content = fp.read()\n\n    assert content == read_content\n\n\ndef test_delete_cwd(tmpdir):\n    cwd = os.getcwd()\n    fs = LocalFileSystem()\n    try:\n        os.chdir(tmpdir)\n        with pytest.raises(ValueError):\n            fs.rm(\".\", recursive=True)\n    finally:\n        os.chdir(cwd)\n\n\ndef test_delete_non_recursive_dir_fails(tmpdir):\n    fs = LocalFileSystem()\n    subdir = os.path.join(tmpdir, \"testdir\")\n    fs.mkdir(subdir)\n    with pytest.raises(ValueError):\n        fs.rm(subdir)\n    fs.rm(subdir, recursive=True)\n\n\n@pytest.mark.parametrize(\n    \"opener, ext\", [(bz2.open, \".bz2\"), (gzip.open, \".gz\"), (open, \"\")]\n)\ndef test_infer_compression(tmpdir, opener, ext):\n    filename = str(tmpdir / f\"test{ext}\")\n    content = b\"hello world\"\n    with opener(filename, \"wb\") as fp:\n        fp.write(content)\n\n    fs = LocalFileSystem()\n    with fs.open(f\"file://{filename}\", \"rb\", compression=\"infer\") as fp:\n        read_content = fp.read()\n\n    assert content == read_content\n\n\ndef test_info_path_like(tmpdir):\n    path = Path(tmpdir / \"test_info\")\n    path.write_text(\"fsspec\")\n\n    fs = LocalFileSystem()\n    assert fs.exists(path)\n\n\ndef test_seekable(tmpdir):\n    fs = LocalFileSystem()\n    tmpdir = str(tmpdir)\n    fn0 = os.path.join(tmpdir, \"target\")\n\n    with open(fn0, \"wb\") as f:\n        f.write(b\"data\")\n\n    f = fs.open(fn0, \"rt\")\n    assert f.seekable(), \"file is not seekable\"\n    f.seek(1)\n    assert f.read(1) == \"a\"\n    assert f.tell() == 2\n\n\ndef test_numpy_fromfile(tmpdir):\n    # Regression test for #1005.\n    np = pytest.importorskip(\"numpy\")\n    fn = str(tmpdir / \"test_arr.npy\")\n    dt = np.int64\n    arr = np.arange(10, dtype=dt)\n    arr.tofile(fn)\n    assert np.array_equal(np.fromfile(fn, dtype=dt), arr)\n\n\ndef test_link(tmpdir):\n    target = os.path.join(tmpdir, \"target\")\n    link = os.path.join(tmpdir, \"link\")\n\n    fs = LocalFileSystem()\n    fs.touch(target)\n\n    fs.link(target, link)\n    assert fs.info(link)[\"nlink\"] > 1\n\n\ndef test_symlink(tmpdir):\n    target = os.path.join(tmpdir, \"target\")\n    link = os.path.join(tmpdir, \"link\")\n\n    fs = LocalFileSystem()\n    fs.touch(target)\n    try:\n        fs.symlink(target, link)\n    except OSError as e:\n        if \"[WinError 1314]\" in str(e):\n            # Windows requires developer mode to be enabled to use symbolic links\n            return\n        raise\n    assert fs.islink(link)\n\n\n# https://github.com/fsspec/filesystem_spec/issues/967\ndef test_put_file_to_dir(tmpdir):\n    src_file = os.path.join(str(tmpdir), \"src\")\n    target_dir = os.path.join(str(tmpdir), \"target\")\n    target_file = os.path.join(target_dir, \"src\")\n\n    fs = LocalFileSystem()\n    fs.touch(src_file)\n    fs.mkdir(target_dir)\n    fs.put(src_file, target_dir)\n\n    assert fs.isfile(target_file)\n\n\ndef test_du(tmpdir):\n    file = tmpdir / \"file\"\n    subdir = tmpdir / \"subdir\"\n    subfile = subdir / \"subfile\"\n\n    fs = LocalFileSystem()\n    with open(file, \"wb\") as f:\n        f.write(b\"4444\")\n    fs.mkdir(subdir)\n    with open(subfile, \"wb\") as f:\n        f.write(b\"7777777\")\n\n    # Switch to posix paths for comparisons\n    tmpdir_posix = Path(tmpdir).as_posix()\n    file_posix = Path(file).as_posix()\n    subdir_posix = Path(subdir).as_posix()\n    subfile_posix = Path(subfile).as_posix()\n\n    assert fs.du(tmpdir) == 11\n    assert fs.du(tmpdir, total=False) == {file_posix: 4, subfile_posix: 7}\n    # Note directory size is OS-specific, but must be >= 0\n    assert fs.du(tmpdir, withdirs=True) >= 11\n\n    d = fs.du(tmpdir, total=False, withdirs=True)\n    assert len(d) == 4\n    assert d[file_posix] == 4\n    assert d[subfile_posix] == 7\n    assert d[tmpdir_posix] >= 0\n    assert d[subdir_posix] >= 0\n\n    assert fs.du(tmpdir, maxdepth=2) == 11\n    assert fs.du(tmpdir, maxdepth=1) == 4\n    with pytest.raises(ValueError):\n        fs.du(tmpdir, maxdepth=0)\n\n    # Size of file only.\n    assert fs.du(file) == 4\n    assert fs.du(file, withdirs=True) == 4\n\n\n@pytest.mark.parametrize(\"funcname\", [\"cp\", \"get\", \"put\"])\ndef test_cp_get_put_directory_recursive(tmpdir, funcname):\n    # https://github.com/fsspec/filesystem_spec/issues/1062\n    # Recursive cp/get/put of source directory into non-existent target directory.\n    fs = LocalFileSystem()\n    src = os.path.join(str(tmpdir), \"src\")\n    fs.mkdir(src)\n    fs.touch(os.path.join(src, \"file\"))\n\n    target = os.path.join(str(tmpdir), \"target\")\n\n    if funcname == \"cp\":\n        func = fs.cp\n    elif funcname == \"get\":\n        func = fs.get\n    elif funcname == \"put\":\n        func = fs.put\n\n    # cp/get/put without slash\n    assert not fs.exists(target)\n    for loop in range(2):\n        func(src, target, recursive=True)\n        assert fs.isdir(target)\n\n        if loop == 0:\n            assert fs.find(target) == [make_path_posix(os.path.join(target, \"file\"))]\n        else:\n            assert sorted(fs.find(target)) == [\n                make_path_posix(os.path.join(target, \"file\")),\n                make_path_posix(os.path.join(target, \"src\", \"file\")),\n            ]\n\n    fs.rm(target, recursive=True)\n\n    # cp/get/put with slash\n    assert not fs.exists(target)\n    for loop in range(2):\n        func(src + \"/\", target, recursive=True)\n        assert fs.isdir(target)\n        assert fs.find(target) == [make_path_posix(os.path.join(target, \"file\"))]\n\n\n@pytest.mark.parametrize(\"funcname\", [\"cp\", \"get\", \"put\"])\ndef test_cp_get_put_empty_directory(tmpdir, funcname):\n    # https://github.com/fsspec/filesystem_spec/issues/1198\n    # cp/get/put of empty directory.\n    fs = LocalFileSystem(auto_mkdir=True)\n    empty = os.path.join(str(tmpdir), \"empty\")\n    fs.mkdir(empty)\n\n    target = os.path.join(str(tmpdir), \"target\")\n    fs.mkdir(target)\n\n    if funcname == \"cp\":\n        func = fs.cp\n    elif funcname == \"get\":\n        func = fs.get\n    elif funcname == \"put\":\n        func = fs.put\n\n    # cp/get/put without slash, target directory exists\n    assert fs.isdir(target)\n    func(empty, target)\n    assert fs.find(target, withdirs=True) == [make_path_posix(target)]\n\n    # cp/get/put with slash, target directory exists\n    assert fs.isdir(target)\n    func(empty + \"/\", target)\n    assert fs.find(target, withdirs=True) == [make_path_posix(target)]\n\n    fs.rmdir(target)\n\n    # cp/get/put without slash, target directory doesn't exist\n    assert not fs.isdir(target)\n    func(empty, target)\n    assert not fs.isdir(target)\n\n    # cp/get/put with slash, target directory doesn't exist\n    assert not fs.isdir(target)\n    func(empty + \"/\", target)\n    assert not fs.isdir(target)\n\n\ndef test_cp_two_files(tmpdir):\n    fs = LocalFileSystem(auto_mkdir=True)\n    src = os.path.join(str(tmpdir), \"src\")\n    file0 = os.path.join(src, \"file0\")\n    file1 = os.path.join(src, \"file1\")\n    fs.mkdir(src)\n    fs.touch(file0)\n    fs.touch(file1)\n\n    target = os.path.join(str(tmpdir), \"target\")\n    assert not fs.exists(target)\n\n    fs.cp([file0, file1], target)\n\n    assert fs.isdir(target)\n    assert sorted(fs.find(target)) == [\n        make_path_posix(os.path.join(target, \"file0\")),\n        make_path_posix(os.path.join(target, \"file1\")),\n    ]\n\n\n@pytest.mark.skipif(WIN, reason=\"Windows does not support colons in filenames\")\ndef test_issue_1447():\n    files_with_colons = {\n        \".local:file:with:colons.txt\": b\"content1\",\n        \".colons-after-extension.txt:after\": b\"content2\",\n        \".colons-after-extension/file:colon.txt:before/after\": b\"content3\",\n    }\n    with filetexts(files_with_colons, mode=\"b\"):\n        for file, contents in files_with_colons.items():\n            with fsspec.filesystem(\"file\").open(file, \"rb\") as f:\n                assert f.read() == contents\n\n            fs, urlpath = fsspec.core.url_to_fs(file)\n            assert isinstance(fs, fsspec.implementations.local.LocalFileSystem)\n            with fs.open(urlpath, \"rb\") as f:\n                assert f.read() == contents\n", "fsspec/implementations/tests/memory/memory_fixtures.py": "import pytest\n\nfrom fsspec import filesystem\nfrom fsspec.tests.abstract import AbstractFixtures\n\n\nclass MemoryFixtures(AbstractFixtures):\n    @pytest.fixture(scope=\"class\")\n    def fs(self):\n        m = filesystem(\"memory\")\n        m.store.clear()\n        m.pseudo_dirs.clear()\n        m.pseudo_dirs.append(\"\")\n        try:\n            yield m\n        finally:\n            m.store.clear()\n            m.pseudo_dirs.clear()\n            m.pseudo_dirs.append(\"\")\n\n    @pytest.fixture\n    def fs_join(self):\n        return lambda *args: \"/\".join(args)\n\n    @pytest.fixture\n    def fs_path(self):\n        return \"\"\n", "fsspec/implementations/tests/memory/memory_test.py": "import fsspec.tests.abstract as abstract\nfrom fsspec.implementations.tests.memory.memory_fixtures import MemoryFixtures\n\n\nclass TestMemoryCopy(abstract.AbstractCopyTests, MemoryFixtures):\n    pass\n\n\nclass TestMemoryGet(abstract.AbstractGetTests, MemoryFixtures):\n    pass\n\n\nclass TestMemoryPut(abstract.AbstractPutTests, MemoryFixtures):\n    pass\n", "fsspec/implementations/tests/memory/__init__.py": "", "fsspec/implementations/tests/local/local_test.py": "import fsspec.tests.abstract as abstract\nfrom fsspec.implementations.tests.local.local_fixtures import LocalFixtures\n\n\nclass TestLocalCopy(abstract.AbstractCopyTests, LocalFixtures):\n    pass\n\n\nclass TestLocalGet(abstract.AbstractGetTests, LocalFixtures):\n    pass\n\n\nclass TestLocalPut(abstract.AbstractPutTests, LocalFixtures):\n    pass\n", "fsspec/implementations/tests/local/__init__.py": "", "fsspec/implementations/tests/local/local_fixtures.py": "import pytest\n\nfrom fsspec.implementations.local import LocalFileSystem, make_path_posix\nfrom fsspec.tests.abstract import AbstractFixtures\n\n\nclass LocalFixtures(AbstractFixtures):\n    @pytest.fixture(scope=\"class\")\n    def fs(self):\n        return LocalFileSystem(auto_mkdir=True)\n\n    @pytest.fixture\n    def fs_path(self, tmpdir):\n        return str(tmpdir)\n\n    @pytest.fixture\n    def fs_sanitize_path(self):\n        return make_path_posix\n", "docs/source/conf.py": "# fsspec documentation build configuration file, created by\n# sphinx-quickstart on Mon Jan 15 18:11:02 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\"../..\"))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.extlinks\",\n    \"numpydoc\",\n    \"sphinx_design\",\n]\n\nnumpydoc_show_class_members = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = \".rst\"\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# General information about the project.\nproject = \"fsspec\"\ncopyright = \"2018, Martin Durant\"\nauthor = \"Martin Durant\"\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\n\nimport fsspec\n\nversion = fsspec.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = fsspec.__version__\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\ndefault_role = \"py:obj\"\nautodoc_mock_imports = [\n    \"aiohttp\",\n    \"dask\",\n    \"distributed\",\n    \"fuse\",\n    \"libarchive\",\n    \"panel\",\n    \"paramiko\",\n    \"pyarrow\",\n    \"pygit2\",\n    \"requests\",\n    \"smbprotocol\",\n    \"smbclient\",\n]\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: https://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \"**\": [\n        \"relations.html\",  # needs 'show_related': True theme option to display\n        \"searchbox.html\",\n    ]\n}\n\n# Custom CSS file to override read the docs default CSS.\n# Contains workaround for issue #790.\nhtml_css_files = [\"custom.css\"]\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"fsspecdoc\"\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \"fsspec.tex\", \"fsspec Documentation\", \"Joseph Crail\", \"manual\")\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \"fsspec\", \"fsspec Documentation\", [author], 1)]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        \"fsspec\",\n        \"fsspec Documentation\",\n        author,\n        \"fsspec\",\n        \"One line description of project.\",\n        \"Miscellaneous\",\n    )\n]\n\nextlinks = {\n    \"issue\": (\"https://github.com/fsspec/filesystem_spec/issues/%s\", \"GH#%s\"),\n    \"pr\": (\"https://github.com/fsspec/filesystem_spec/pull/%s\", \"GH#%s\"),\n}\n"}