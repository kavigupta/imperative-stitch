{"setup.py": "import setuptools\n\nwith open(\"readme.md\", \"r\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name='labml-nn',\n    version='0.4.136',\n    author=\"Varuna Jayasiri, Nipun Wijerathne\",\n    author_email=\"vpjayasiri@gmail.com, hnipun@gmail.com\",\n    description=\"\ud83e\uddd1\u200d\ud83c\udfeb Implementations/tutorials of deep learning papers with side-by-side notes \ud83d\udcdd; including transformers (original, xl, switch, feedback, vit), optimizers (adam, radam, adabelief), gans(dcgan, cyclegan, stylegan2), \ud83c\udfae reinforcement learning (ppo, dqn), capsnet, distillation, diffusion, etc. \ud83e\udde0\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/labmlai/annotated_deep_learning_paper_implementations\",\n    project_urls={\n        'Documentation': 'https://nn.labml.ai'\n    },\n    packages=setuptools.find_packages(exclude=('labml', 'labml.*',\n                                               'labml_samples', 'labml_samples.*',\n                                               'labml_helpers', 'labml_helpers.*',\n                                               'test',\n                                               'test.*')),\n    install_requires=['labml==0.4.168',\n                      'labml-helpers==0.4.89',\n                      'torch',\n                      'torchtext',\n                      'torchvision',\n                      'einops',\n                      'numpy',\n                      'fairscale'],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        'Intended Audience :: Developers',\n        'Intended Audience :: Science/Research',\n        'Topic :: Scientific/Engineering',\n        'Topic :: Scientific/Engineering :: Mathematics',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Topic :: Software Development',\n        'Topic :: Software Development :: Libraries',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n    ],\n    keywords='machine learning',\n)\n", "utils/papers_list.py": "import json\nimport re\nfrom pathlib import Path\n\nfrom labml import logger\nfrom labml.logger import Text\n\nHOME = Path('./labml_nn')\n\nREGEX = re.compile(r\"\"\"\n \\(\n https://papers\\.labml\\.ai/paper/  # Start of a numeric entity reference\n (?P<id>[0-9\\.]+)  # Paper ID\n \\)\n\"\"\", re.VERBOSE)\n\nIGNORE = {\n    'neox/model.html',\n    'transformers/index.html',\n    'transformers/configs.html',\n    'optimizers/noam.html',\n    'transformers/basic/autoregressive_experiment.html',\n    'transformers/xl/relative_mha.html',\n    'capsule_networks/mnist.html',\n    'transformers/rope/value_pe/index.html',\n}\n\nIGNORE_PAPERS = {\n    '2002.04745',  # On Layer Normalization in the Transformer Architecture\n    '1606.08415',  # Gaussian Error Linear Units (GELUs)\n    '1710.10196',  # Progressive Growing of GANs for Improved Quality, Stability, and Variation\n    '1904.11486',  # Making Convolutional Networks Shift-Invariant Again\n    '1801.04406',  # Which Training Methods for GANs do actually Converge?\n    '1812.04948',  # A Style-Based Generator Architecture for Generative Adversarial Networks\n    '1705.10528',  # Constrained Policy Optimization\n}\n\n\ndef collect(path: Path):\n    if path.is_file():\n        html = path.relative_to(HOME)\n        if html.suffix not in {'.py'}:\n            return []\n\n        if html.stem == '__init__':\n            html = html.parent / 'index.html'\n        else:\n            html = html.parent / f'{html.stem}.html'\n\n        if str(html) in IGNORE:\n            return []\n\n        with open(str(path), 'r') as f:\n            contents = f.read()\n            papers = set()\n            for m in REGEX.finditer(contents):\n                if m.group('id') in IGNORE_PAPERS:\n                    continue\n                papers.add(m.group('id'))\n\n            if len(papers) > 1:\n                logger.log([(str(html), Text.key), ': ', str(papers)])\n            return [{'url': str(html), 'arxiv_id': p} for p in papers]\n\n    urls = []\n    for f in path.iterdir():\n        urls += collect(f)\n\n    return urls\n\n\ndef main():\n    papers = []\n    for f in HOME.iterdir():\n        papers += collect(f)\n\n    papers.sort(key=lambda p: p['arxiv_id'])\n\n    by_id = {}\n    for p in papers:\n        if p['arxiv_id'] not in by_id:\n            by_id[p['arxiv_id']] = []\n        by_id[p['arxiv_id']].append(f'''https://nn.labml.ai/{p['url']}''')\n\n    logger.log([('Papers', Text.key), ': ', f'{len(by_id) :,}'])\n\n    with open(str(HOME.parent / 'docs' / 'papers.json'), 'w') as f:\n        f.write(json.dumps(by_id, indent=1))\n\n\nif __name__ == '__main__':\n    main()\n", "utils/sitemap.py": "from pathlib import Path\n\nimport git\n\nHOME = Path('./labml_nn')\nREPO = git.Repo('.')\n\n\ndef collect(path: Path):\n    if path.is_file():\n        try:\n            commit = next(iter(REPO.iter_commits(paths=path)))\n        except StopIteration:\n            return []\n\n        html = path.relative_to(HOME)\n        if html.suffix not in {'.py'}:\n            return []\n\n        if html.stem == '__init__':\n            html = html.parent / 'index.html'\n        else:\n            html = html.parent / f'{html.stem}.html'\n\n        return [{'path': str(html), 'date': str(commit.committed_datetime.date())}]\n\n    urls = []\n    for f in path.iterdir():\n        urls += collect(f)\n\n    return urls\n\n\ndef main():\n    urls = []\n    for f in HOME.iterdir():\n        urls += collect(f)\n\n    urls = [f'''\n    <url>\n      <loc>https://nn.labml.ai/{u['path']}</loc>\n      <lastmod>{u['date']}T16:30:00+00:00</lastmod>\n      <priority>1.00</priority>\n    </url>\n    ''' for u in urls]\n\n    urls = '\\n'.join(urls)\n    xml = f'''\n    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <urlset\n      xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n      xsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9\n            http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\">\n      {urls}\n    </urlset>\n    '''\n\n    with open(str(HOME.parent / 'docs' / 'sitemap.xml'), 'w') as f:\n        f.write(xml)\n\n\nif __name__ == '__main__':\n    main()\n", "utils/diagrams.py": "import shutil\nfrom pathlib import Path\nfrom typing import List\nfrom xml.dom import minidom\nimport os\n\nfrom labml import monit\n\nHOME = Path('.').absolute()\n\nSTYLES = \"\"\"\n.black-stroke {\n    stroke: #aaa;\n}\n\nrect.black-stroke {\n    stroke: #444;\n}\n\n.black-fill {\n    fill: #ddd;\n}\n\n.white-fill {\n    fill: #333;\n}\n\n.blue-stroke {\n    stroke: #5b8fab;\n}\n\n.blue-fill {\n    fill: #356782;\n}\n\n.yellow-stroke {\n    stroke: #bbab52;\n}\n\n.yellow-fill {\n    fill: #a7942b;\n}\n\n.grey-stroke {\n    stroke: #484d5a;\n}\n\n.grey-fill {\n    fill: #2e323c;\n}\n\n.red-stroke {\n    stroke: #bb3232;\n}\n\n.red-fill {\n    fill: #901c1c;\n}\n\n.orange-stroke {\n    stroke: #a5753f;\n}\n\n.orange-fill {\n    fill: #82531e;\n}\n\n.purple-stroke {\n    stroke: #a556a5;\n}\n\n.purple-fill {\n    fill: #8a308a;\n}\n\n.green-stroke {\n    stroke: #80cc92;\n}\n\n.green-fill {\n    fill: #499e5d;\n}\n\nswitch foreignObject div div div {\n    color: #ddd !important;\n}\n\nswitch foreignObject div div div span {\n    color: #ddd !important;\n}\n\n.has-background {\n    background-color: #1d2127 !important;\n}\n\"\"\"\n\nSTROKES = {\n    '#000000': 'black',\n    '#6c8ebf': 'blue',\n    '#d6b656': 'yellow',\n    '#666666': 'grey',\n    '#b85450': 'red',\n    '#d79b00': 'orange',\n    '#9673a6': 'purple',\n    '#82b366': 'green',\n}\n\nFILLS = {\n    '#000000': 'black',\n    '#ffffff': 'white',\n    '#dae8fc': 'blue',\n    '#fff2cc': 'yellow',\n    '#f5f5f5': 'grey',\n    '#f8cecc': 'red',\n    '#ffe6cc': 'orange',\n    '#e1d5e7': 'purple',\n    '#d5e8d4': 'green',\n}\n\n\ndef clear_switches(doc: minidom.Document):\n    switches = doc.getElementsByTagName('switch')\n    for s in switches:\n        children = s.childNodes\n        assert len(children) == 2\n        if children[0].tagName == 'g' and 'requiredFeatures' in children[0].attributes:\n            s.parentNode.removeChild(s)\n            s.unlink()\n            continue\n        assert children[0].tagName == 'foreignObject'\n        assert children[1].tagName == 'text'\n        c = children[1]\n        s.removeChild(c)\n        s.parentNode.insertBefore(c, s)\n        s.parentNode.removeChild(s)\n\n\ndef add_class(node: minidom.Node, class_name: str):\n    if 'class' not in node.attributes:\n        node.attributes['class'] = class_name\n        return\n\n    node.attributes['class'] = node.attributes['class'].value + f' {class_name}'\n\n\ndef add_bg_classes(nodes: List[minidom.Node]):\n    for node in nodes:\n        if 'style' in node.attributes:\n            s = node.attributes['style'].value\n            if s.count('background-color'):\n                add_class(node, 'has-background')\n\n\ndef add_stroke_classes(nodes: List[minidom.Node]):\n    for node in nodes:\n        if 'stroke' in node.attributes:\n            stroke = node.attributes['stroke'].value\n            if stroke not in STROKES:\n                continue\n\n            node.removeAttribute('stroke')\n            add_class(node, f'{STROKES[stroke]}-stroke')\n\n\ndef add_fill_classes(nodes: List[minidom.Node]):\n    for node in nodes:\n        if 'fill' in node.attributes:\n            fill = node.attributes['fill'].value\n            if fill not in FILLS:\n                continue\n\n            node.removeAttribute('fill')\n            add_class(node, f'{FILLS[fill]}-fill')\n\n\ndef add_classes(doc: minidom.Document):\n    paths = doc.getElementsByTagName('path')\n    add_stroke_classes(paths)\n    add_fill_classes(paths)\n\n    rects = doc.getElementsByTagName('rect')\n    add_stroke_classes(rects)\n    add_fill_classes(rects)\n\n    ellipse = doc.getElementsByTagName('ellipse')\n    add_stroke_classes(ellipse)\n    add_fill_classes(ellipse)\n\n    text = doc.getElementsByTagName('text')\n    add_fill_classes(text)\n\n    div = doc.getElementsByTagName('div')\n    add_bg_classes(div)\n\n    span = doc.getElementsByTagName('span')\n    add_bg_classes(span)\n\n\ndef parse(source: Path, dest: Path):\n    doc: minidom.Document = minidom.parse(str(source))\n\n    svg = doc.getElementsByTagName('svg')\n\n    assert len(svg) == 1\n    svg = svg[0]\n\n    if 'content' in svg.attributes:\n        svg.removeAttribute('content')\n    # svg.attributes['height'] = str(int(svg.attributes['height'].value[:-2]) + 30) + 'px'\n    # svg.attributes['width'] = str(int(svg.attributes['width'].value[:-2]) + 30) + 'px'\n\n    view_box = svg.attributes['viewBox'].value.split(' ')\n    view_box = [float(v) for v in view_box]\n    view_box[0] -= 10\n    view_box[1] -= 10\n    view_box[2] += 20\n    view_box[3] += 20\n    svg.attributes['viewBox'] = ' '.join([str(v) for v in view_box])\n\n    svg.attributes['style'] = 'background: #1d2127;'  # padding: 10px;'\n\n    # clear_switches(doc)\n\n    style = doc.createElement('style')\n    style.appendChild(doc.createTextNode(STYLES))\n    svg.insertBefore(style, svg.childNodes[0])\n    add_classes(doc)\n\n    with open(str(dest), 'w') as f:\n        doc.writexml(f)\n\n\ndef recurse(path: Path):\n    files = []\n    if path.is_file():\n        files.append(path)\n        return files\n\n    for f in path.iterdir():\n        files += recurse(f)\n\n    return files\n\n\ndef main():\n    diagrams_path = HOME / 'diagrams'\n    docs_path = HOME / 'docs'\n\n    # For first invocation\n    os.makedirs(diagrams_path, exist_ok=True)\n\n    for p in recurse(diagrams_path):\n        source_path = p\n        p = p.relative_to(diagrams_path)\n        dest_path = docs_path / p\n        if not dest_path.parent.exists():\n            dest_path.parent.mkdir(parents=True)\n\n        with monit.section(str(p)):\n            if source_path.suffix == '.svg':\n                parse(source_path, dest_path)\n            else:\n                shutil.copy(str(source_path), str(dest_path))\n\n\nif __name__ == '__main__':\n    main()\n", "utils/__init__.py": "", "labml_nn/__init__.py": "\"\"\"\n# [Annotated Research Paper Implementations: Transformers, StyleGAN, Stable Diffusion, DDPM/DDIM, LayerNorm, Nucleus Sampling and more](index.html)\n\nThis is a collection of simple PyTorch implementations of\nneural networks and related algorithms.\n[These implementations](https://github.com/labmlai/annotated_deep_learning_paper_implementations) are documented with explanations,\nand the [website](index.html)\nrenders these as side-by-side formatted notes.\nWe believe these would help you understand these algorithms better.\n\n![Screenshot](dqn-light.png)\n\nWe are actively maintaining this repo and adding new\nimplementations.\n[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai) for updates.\n\n## Translations\n\n### **[English (original)](https://nn.labml.ai)**\n### **[Chinese (translated)](https://nn.labml.ai/zh/)**\n### **[Japanese (translated)](https://nn.labml.ai/ja/)**\n\n## Paper Implementations\n\n#### \u2728 [Transformers](transformers/index.html)\n\n* [Multi-headed attention](transformers/mha.html)\n* [Transformer building blocks](transformers/models.html)\n* [Transformer XL](transformers/xl/index.html)\n    * [Relative multi-headed attention](transformers/xl/relative_mha.html)\n* [Rotary Positional Embeddings (RoPE)](transformers/rope/index.html)\n* [Attention with Linear Biases (ALiBi)](transformers/alibi/index.html)\n* [RETRO](transformers/retro/index.html)\n* [Compressive Transformer](transformers/compressive/index.html)\n* [GPT Architecture](transformers/gpt/index.html)\n* [GLU Variants](transformers/glu_variants/simple.html)\n* [kNN-LM: Generalization through Memorization](transformers/knn/index.html)\n* [Feedback Transformer](transformers/feedback/index.html)\n* [Switch Transformer](transformers/switch/index.html)\n* [Fast Weights Transformer](transformers/fast_weights/index.html)\n* [FNet](transformers/fnet/index.html)\n* [Attention Free Transformer](transformers/aft/index.html)\n* [Masked Language Model](transformers/mlm/index.html)\n* [MLP-Mixer: An all-MLP Architecture for Vision](transformers/mlp_mixer/index.html)\n* [Pay Attention to MLPs (gMLP)](transformers/gmlp/index.html)\n* [Vision Transformer (ViT)](transformers/vit/index.html)\n* [Primer EZ](transformers/primer_ez/index.html)\n* [Hourglass](transformers/hour_glass/index.html)\n\n#### \u2728 [Eleuther GPT-NeoX](neox/index.html)\n* [Generate on a 48GB GPU](neox/samples/generate.html)\n* [Finetune on two 48GB GPUs](neox/samples/finetune.html)\n* [LLM.int8()](neox/utils/llm_int8.html)\n\n#### \u2728 [Diffusion models](diffusion/index.html)\n\n* [Denoising Diffusion Probabilistic Models (DDPM)](diffusion/ddpm/index.html)\n* [Denoising Diffusion Implicit Models (DDIM)](diffusion/stable_diffusion/sampler/ddim.html)\n* [Latent Diffusion Models](diffusion/stable_diffusion/latent_diffusion.html)\n* [Stable Diffusion](diffusion/stable_diffusion/index.html)\n\n#### \u2728 [Generative Adversarial Networks](gan/index.html)\n* [Original GAN](gan/original/index.html)\n* [GAN with deep convolutional network](gan/dcgan/index.html)\n* [Cycle GAN](gan/cycle_gan/index.html)\n* [Wasserstein GAN](gan/wasserstein/index.html)\n* [Wasserstein GAN with Gradient Penalty](gan/wasserstein/gradient_penalty/index.html)\n* [StyleGAN 2](gan/stylegan/index.html)\n\n#### \u2728 [Recurrent Highway Networks](recurrent_highway_networks/index.html)\n\n#### \u2728 [LSTM](lstm/index.html)\n\n#### \u2728 [HyperNetworks - HyperLSTM](hypernetworks/hyper_lstm.html)\n\n#### \u2728 [ResNet](resnet/index.html)\n\n#### \u2728 [ConvMixer](conv_mixer/index.html)\n\n#### \u2728 [Capsule Networks](capsule_networks/index.html)\n\n#### \u2728 [U-Net](unet/index.html)\n\n#### \u2728 [Sketch RNN](sketch_rnn/index.html)\n\n#### \u2728 Graph Neural Networks\n\n* [Graph Attention Networks (GAT)](graphs/gat/index.html)\n* [Graph Attention Networks v2 (GATv2)](graphs/gatv2/index.html)\n\n#### \u2728 [Reinforcement Learning](rl/index.html)\n* [Proximal Policy Optimization](rl/ppo/index.html) with\n [Generalized Advantage Estimation](rl/ppo/gae.html)\n* [Deep Q Networks](rl/dqn/index.html) with\n with [Dueling Network](rl/dqn/model.html),\n [Prioritized Replay](rl/dqn/replay_buffer.html)\n and Double Q Network.\n\n#### \u2728 [Counterfactual Regret Minimization (CFR)](cfr/index.html)\n\nSolving games with incomplete information such as poker with CFR.\n\n* [Kuhn Poker](cfr/kuhn/index.html)\n\n#### \u2728 [Optimizers](optimizers/index.html)\n* [Adam](optimizers/adam.html)\n* [AMSGrad](optimizers/amsgrad.html)\n* [Adam Optimizer with warmup](optimizers/adam_warmup.html)\n* [Noam Optimizer](optimizers/noam.html)\n* [Rectified Adam Optimizer](optimizers/radam.html)\n* [AdaBelief Optimizer](optimizers/ada_belief.html)\n* [Sophia-G Optimizer](optimizers/sophia.html)\n\n#### \u2728 [Normalization Layers](normalization/index.html)\n* [Batch Normalization](normalization/batch_norm/index.html)\n* [Layer Normalization](normalization/layer_norm/index.html)\n* [Instance Normalization](normalization/instance_norm/index.html)\n* [Group Normalization](normalization/group_norm/index.html)\n* [Weight Standardization](normalization/weight_standardization/index.html)\n* [Batch-Channel Normalization](normalization/batch_channel_norm/index.html)\n* [DeepNorm](normalization/deep_norm/index.html)\n\n#### \u2728 [Distillation](distillation/index.html)\n\n#### \u2728 [Adaptive Computation](adaptive_computation/index.html)\n\n* [PonderNet](adaptive_computation/ponder_net/index.html)\n\n#### \u2728 [Uncertainty](uncertainty/index.html)\n\n* [Evidential Deep Learning to Quantify Classification Uncertainty](uncertainty/evidence/index.html)\n\n#### \u2728 [Activations](activations/index.html)\n\n* [Fuzzy Tiling Activations](activations/fta/index.html)\n\n#### \u2728 [Language Model Sampling Techniques](sampling/index.html)\n* [Greedy Sampling](sampling/greedy.html)\n* [Temperature Sampling](sampling/temperature.html)\n* [Top-k Sampling](sampling/top_k.html)\n* [Nucleus Sampling](sampling/nucleus.html)\n\n#### \u2728 [Scalable Training/Inference](scaling/index.html)\n* [Zero3 memory optimizations](scaling/zero3/index.html)\n\n### Installation\n\n```bash\npip install labml-nn\n```\n\"\"\"\n", "labml_nn/normalization/__init__.py": "\"\"\"\n---\ntitle: Normalization Layers\nsummary: >\n A set of PyTorch implementations/tutorials of normalization layers.\n---\n\n# Normalization Layers\n\n* [Batch Normalization](batch_norm/index.html)\n* [Layer Normalization](layer_norm/index.html)\n* [Instance Normalization](instance_norm/index.html)\n* [Group Normalization](group_norm/index.html)\n* [Weight Standardization](weight_standardization/index.html)\n* [Batch-Channel Normalization](batch_channel_norm/index.html)\n* [DeepNorm](deep_norm/index.html)\n\"\"\"\n", "labml_nn/normalization/layer_norm/__init__.py": "\"\"\"\n---\ntitle: Layer Normalization\nsummary: >\n A PyTorch implementation/tutorial of layer normalization.\n---\n\n# Layer Normalization\n\nThis is a [PyTorch](https://pytorch.org) implementation of\n[Layer Normalization](https://arxiv.org/abs/1607.06450).\n\n### Limitations of [Batch Normalization](../batch_norm/index.html)\n\n* You need to maintain running means.\n* Tricky for RNNs. Do you need different normalizations for each step?\n* Doesn't work with small batch sizes;\nlarge NLP models are usually trained with small batch sizes.\n* Need to compute means and variances across devices in distributed training.\n\n## Layer Normalization\n\nLayer normalization is a simpler normalization method that works\non a wider range of settings.\nLayer normalization transforms the inputs to have zero mean and unit variance\nacross the features.\n*Note that batch normalization fixes the zero mean and unit variance for each element.*\nLayer normalization does it for each batch across all elements.\n\nLayer normalization is generally used for NLP tasks.\n\nWe have used layer normalization in most of the\n[transformer implementations](../../transformers/gpt/index.html).\n\"\"\"\nfrom typing import Union, List\n\nimport torch\nfrom torch import nn, Size\n\nfrom labml_helpers.module import Module\n\n\nclass LayerNorm(Module):\n    r\"\"\"\n    ## Layer Normalization\n\n    Layer normalization $\\text{LN}$ normalizes the input $X$ as follows:\n\n    When input $X \\in \\mathbb{R}^{B \\times C}$ is a batch of embeddings,\n    where $B$ is the batch size and $C$ is the number of features.\n    $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$.\n    $$\\text{LN}(X) = \\gamma\n    \\frac{X - \\underset{C}{\\mathbb{E}}[X]}{\\sqrt{\\underset{C}{Var}[X] + \\epsilon}}\n    + \\beta$$\n\n    When input $X \\in \\mathbb{R}^{L \\times B \\times C}$ is a batch of a sequence of embeddings,\n    where $B$ is the batch size, $C$ is the number of channels, $L$ is the length of the sequence.\n    $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$.\n    $$\\text{LN}(X) = \\gamma\n    \\frac{X - \\underset{C}{\\mathbb{E}}[X]}{\\sqrt{\\underset{C}{Var}[X] + \\epsilon}}\n    + \\beta$$\n\n    When input $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ is a batch of image representations,\n    where $B$ is the batch size, $C$ is the number of channels, $H$ is the height and $W$ is the width.\n    This is not a widely used scenario.\n    $\\gamma \\in \\mathbb{R}^{C \\times H \\times W}$ and $\\beta \\in \\mathbb{R}^{C \\times H \\times W}$.\n    $$\\text{LN}(X) = \\gamma\n    \\frac{X - \\underset{C, H, W}{\\mathbb{E}}[X]}{\\sqrt{\\underset{C, H, W}{Var}[X] + \\epsilon}}\n    + \\beta$$\n    \"\"\"\n\n    def __init__(self, normalized_shape: Union[int, List[int], Size], *,\n                 eps: float = 1e-5,\n                 elementwise_affine: bool = True):\n        \"\"\"\n        * `normalized_shape` $S$ is the shape of the elements (except the batch).\n         The input should then be\n         $X \\in \\mathbb{R}^{* \\times S[0] \\times S[1] \\times ... \\times S[n]}$\n        * `eps` is $\\epsilon$, used in $\\sqrt{Var[X] + \\epsilon}$ for numerical stability\n        * `elementwise_affine` is whether to scale and shift the normalized value\n\n        We've tried to use the same names for arguments as PyTorch `LayerNorm` implementation.\n        \"\"\"\n        super().__init__()\n\n        # Convert `normalized_shape` to `torch.Size`\n        if isinstance(normalized_shape, int):\n            normalized_shape = torch.Size([normalized_shape])\n        elif isinstance(normalized_shape, list):\n            normalized_shape = torch.Size(normalized_shape)\n        assert isinstance(normalized_shape, torch.Size)\n\n        #\n        self.normalized_shape = normalized_shape\n        self.eps = eps\n        self.elementwise_affine = elementwise_affine\n        # Create parameters for $\\gamma$ and $\\beta$ for gain and bias\n        if self.elementwise_affine:\n            self.gain = nn.Parameter(torch.ones(normalized_shape))\n            self.bias = nn.Parameter(torch.zeros(normalized_shape))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        `x` is a tensor of shape `[*, S[0], S[1], ..., S[n]]`.\n        `*` could be any number of dimensions.\n         For example, in an NLP task this will be\n        `[seq_len, batch_size, features]`\n        \"\"\"\n        # Sanity check to make sure the shapes match\n        assert self.normalized_shape == x.shape[-len(self.normalized_shape):]\n\n        # The dimensions to calculate the mean and variance on\n        dims = [-(i + 1) for i in range(len(self.normalized_shape))]\n\n        # Calculate the mean of all elements;\n        # i.e. the means for each element $\\mathbb{E}[X]$\n        mean = x.mean(dim=dims, keepdim=True)\n        # Calculate the squared mean of all elements;\n        # i.e. the means for each element $\\mathbb{E}[X^2]$\n        mean_x2 = (x ** 2).mean(dim=dims, keepdim=True)\n        # Variance of all element $Var[X] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$\n        var = mean_x2 - mean ** 2\n\n        # Normalize $$\\hat{X} = \\frac{X - \\mathbb{E}[X]}{\\sqrt{Var[X] + \\epsilon}}$$\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        # Scale and shift $$\\text{LN}(x) = \\gamma \\hat{X} + \\beta$$\n        if self.elementwise_affine:\n            x_norm = self.gain * x_norm + self.bias\n\n        #\n        return x_norm\n\n\ndef _test():\n    \"\"\"\n    Simple test\n    \"\"\"\n    from labml.logger import inspect\n\n    x = torch.zeros([2, 3, 2, 4])\n    inspect(x.shape)\n    ln = LayerNorm(x.shape[2:])\n\n    x = ln(x)\n    inspect(x.shape)\n    inspect(ln.gain.shape)\n\n\n#\nif __name__ == '__main__':\n    _test()\n", "labml_nn/normalization/batch_norm/cifar10.py": "\"\"\"\n---\ntitle: CIFAR10 Experiment to try Group Normalization\nsummary: >\n  This trains is a simple convolutional neural network that uses group normalization\n  to classify CIFAR10 images.\n---\n\n# CIFAR10 Experiment for Group Normalization\n\"\"\"\n\nimport torch.nn as nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.experiments.cifar10 import CIFAR10Configs, CIFAR10VGGModel\nfrom labml_nn.normalization.batch_norm import BatchNorm\n\n\nclass Model(CIFAR10VGGModel):\n    \"\"\"\n    ### VGG model for CIFAR-10 classification\n\n    This derives from the [generic VGG style architecture](../../experiments/cifar10.html).\n    \"\"\"\n\n    def conv_block(self, in_channels, out_channels) -> nn.Module:\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            BatchNorm(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def __init__(self):\n        super().__init__([[64, 64], [128, 128], [256, 256, 256], [512, 512, 512], [512, 512, 512]])\n\n\n@option(CIFAR10Configs.model)\ndef model(c: CIFAR10Configs):\n    \"\"\"\n    ### Create model\n    \"\"\"\n    return Model().to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='cifar10', comment='batch norm')\n    # Create configurations\n    conf = CIFAR10Configs()\n    # Load configurations\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n        'train_batch_size': 64,\n    })\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/normalization/batch_norm/mnist.py": "\"\"\"\n---\ntitle: MNIST Experiment to try Batch Normalization\nsummary: >\n  This trains is a simple convolutional neural network that uses batch normalization\n  to classify MNIST digits.\n---\n\n# MNIST Experiment for Batch Normalization\n\"\"\"\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_nn.experiments.mnist import MNISTConfigs\nfrom labml_nn.normalization.batch_norm import BatchNorm\n\n\nclass Model(Module):\n    \"\"\"\n    ### Model definition\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Note that we omit the bias parameter\n        self.conv1 = nn.Conv2d(1, 20, 5, 1, bias=False)\n        # Batch normalization with 20 channels (output of convolution layer).\n        # The input to this layer will have shape `[batch_size, 20, height(24), width(24)]`\n        self.bn1 = BatchNorm(20)\n        #\n        self.conv2 = nn.Conv2d(20, 50, 5, 1, bias=False)\n        # Batch normalization with 50 channels.\n        # The input to this layer will have shape `[batch_size, 50, height(8), width(8)]`\n        self.bn2 = BatchNorm(50)\n        #\n        self.fc1 = nn.Linear(4 * 4 * 50, 500, bias=False)\n        # Batch normalization with 500 channels (output of fully connected layer).\n        # The input to this layer will have shape `[batch_size, 500]`\n        self.bn3 = BatchNorm(500)\n        #\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x: torch.Tensor):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.bn3(self.fc1(x)))\n        return self.fc2(x)\n\n\n@option(MNISTConfigs.model)\ndef model(c: MNISTConfigs):\n    \"\"\"\n    ### Create model\n\n    We use [`MNISTConfigs`](../../experiments/mnist.html#MNISTConfigs) configurations\n    and set a new function to calculate the model.\n    \"\"\"\n    return Model().to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='mnist_batch_norm')\n    # Create configurations\n    conf = MNISTConfigs()\n    # Load configurations\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 0.001,\n    })\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/normalization/batch_norm/__init__.py": "\"\"\"\n---\ntitle: Batch Normalization\nsummary: >\n A PyTorch implementation/tutorial of batch normalization.\n---\n\n# Batch Normalization\n\nThis is a [PyTorch](https://pytorch.org) implementation of Batch Normalization from paper\n [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167).\n\n### Internal Covariate Shift\n\nThe paper defines *Internal Covariate Shift* as the change in the\ndistribution of network activations due to the change in\nnetwork parameters during training.\nFor example, let's say there are two layers $l_1$ and $l_2$.\nDuring the beginning of the training $l_1$ outputs (inputs to $l_2$)\ncould be in distribution $\\mathcal{N}(0.5, 1)$.\nThen, after some training steps, it could move to $\\mathcal{N}(0.6, 1.5)$.\nThis is *internal covariate shift*.\n\nInternal covariate shift will adversely affect training speed because the later layers\n($l_2$ in the above example) have to adapt to this shifted distribution.\n\nBy stabilizing the distribution, batch normalization minimizes the internal covariate shift.\n\n## Normalization\n\nIt is known that whitening improves training speed and convergence.\n*Whitening* is linearly transforming inputs to have zero mean, unit variance,\nand be uncorrelated.\n\n### Normalizing outside gradient computation doesn't work\n\nNormalizing outside the gradient computation using pre-computed (detached)\nmeans and variances doesn't work. For instance. (ignoring variance), let\n$$\\hat{x} = x - \\mathbb{E}[x]$$\nwhere $x = u + b$ and $b$ is a trained bias\nand $\\mathbb{E}[x]$ is an outside gradient computation (pre-computed constant).\n\nNote that $\\hat{x}$ has no effect on $b$.\nTherefore,\n$b$ will increase or decrease based\n$\\frac{\\partial{\\mathcal{L}}}{\\partial x}$,\nand keep on growing indefinitely in each training update.\nThe paper notes that similar explosions happen with variances.\n\n### Batch Normalization\n\nWhitening is computationally expensive because you need to de-correlate and\nthe gradients must flow through the full whitening calculation.\n\nThe paper introduces a simplified version which they call *Batch Normalization*.\nFirst simplification is that it normalizes each feature independently to have\nzero mean and unit variance:\n$$\\hat{x}^{(k)} = \\frac{x^{(k)} - \\mathbb{E}[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$$\nwhere $x = (x^{(1)} ... x^{(d)})$ is the $d$-dimensional input.\n\nThe second simplification is to use estimates of mean $\\mathbb{E}[x^{(k)}]$\nand variance $Var[x^{(k)}]$ from the mini-batch\nfor normalization; instead of calculating the mean and variance across the whole dataset.\n\nNormalizing each feature to zero mean and unit variance could affect what the layer\ncan represent.\nAs an example paper illustrates that, if the inputs to a sigmoid are normalized\nmost of it will be within $[-1, 1]$ range where the sigmoid is linear.\nTo overcome this each feature is scaled and shifted by two trained parameters\n$\\gamma^{(k)}$ and $\\beta^{(k)}$.\n$$y^{(k)} =\\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}$$\nwhere $y^{(k)}$ is the output of the batch normalization layer.\n\nNote that when applying batch normalization after a linear transform\nlike $Wu + b$ the bias parameter $b$ gets cancelled due to normalization.\nSo you can and should omit bias parameter in linear transforms right before the\nbatch normalization.\n\nBatch normalization also makes the back propagation invariant to the scale of the weights\nand empirically it improves generalization, so it has regularization effects too.\n\n## Inference\n\nWe need to know $\\mathbb{E}[x^{(k)}]$ and $Var[x^{(k)}]$ in order to\nperform the normalization.\nSo during inference, you either need to go through the whole (or part of) dataset\nand find the mean and variance, or you can use an estimate calculated during training.\nThe usual practice is to calculate an exponential moving average of\nmean and variance during the training phase and use that for inference.\n\nHere's [the training code](mnist.html) and a notebook for training\na CNN classifier that uses batch normalization for MNIST dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/batch_norm/mnist.ipynb)\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass BatchNorm(Module):\n    r\"\"\"\n    ## Batch Normalization Layer\n\n    Batch normalization layer $\\text{BN}$ normalizes the input $X$ as follows:\n\n    When input $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ is a batch of image representations,\n    where $B$ is the batch size, $C$ is the number of channels, $H$ is the height and $W$ is the width.\n    $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$.\n    $$\\text{BN}(X) = \\gamma\n    \\frac{X - \\underset{B, H, W}{\\mathbb{E}}[X]}{\\sqrt{\\underset{B, H, W}{Var}[X] + \\epsilon}}\n    + \\beta$$\n\n    When input $X \\in \\mathbb{R}^{B \\times C}$ is a batch of embeddings,\n    where $B$ is the batch size and $C$ is the number of features.\n    $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$.\n    $$\\text{BN}(X) = \\gamma\n    \\frac{X - \\underset{B}{\\mathbb{E}}[X]}{\\sqrt{\\underset{B}{Var}[X] + \\epsilon}}\n    + \\beta$$\n\n    When input $X \\in \\mathbb{R}^{B \\times C \\times L}$ is a batch of a sequence embeddings,\n    where $B$ is the batch size, $C$ is the number of features, and $L$ is the length of the sequence.\n    $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$.\n    $$\\text{BN}(X) = \\gamma\n    \\frac{X - \\underset{B, L}{\\mathbb{E}}[X]}{\\sqrt{\\underset{B, L}{Var}[X] + \\epsilon}}\n    + \\beta$$\n    \"\"\"\n\n    def __init__(self, channels: int, *,\n                 eps: float = 1e-5, momentum: float = 0.1,\n                 affine: bool = True, track_running_stats: bool = True):\n        \"\"\"\n        * `channels` is the number of features in the input\n        * `eps` is $\\epsilon$, used in $\\sqrt{Var[x^{(k)}] + \\epsilon}$ for numerical stability\n        * `momentum` is the momentum in taking the exponential moving average\n        * `affine` is whether to scale and shift the normalized value\n        * `track_running_stats` is whether to calculate the moving averages or mean and variance\n\n        We've tried to use the same names for arguments as PyTorch `BatchNorm` implementation.\n        \"\"\"\n        super().__init__()\n\n        self.channels = channels\n\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        # Create parameters for $\\gamma$ and $\\beta$ for scale and shift\n        if self.affine:\n            self.scale = nn.Parameter(torch.ones(channels))\n            self.shift = nn.Parameter(torch.zeros(channels))\n        # Create buffers to store exponential moving averages of\n        # mean $\\mathbb{E}[x^{(k)}]$ and variance $Var[x^{(k)}]$\n        if self.track_running_stats:\n            self.register_buffer('exp_mean', torch.zeros(channels))\n            self.register_buffer('exp_var', torch.ones(channels))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        `x` is a tensor of shape `[batch_size, channels, *]`.\n        `*` denotes any number of (possibly 0) dimensions.\n         For example, in an image (2D) convolution this will be\n        `[batch_size, channels, height, width]`\n        \"\"\"\n        # Keep the original shape\n        x_shape = x.shape\n        # Get the batch size\n        batch_size = x_shape[0]\n        # Sanity check to make sure the number of features is the same\n        assert self.channels == x.shape[1]\n\n        # Reshape into `[batch_size, channels, n]`\n        x = x.view(batch_size, self.channels, -1)\n\n        # We will calculate the mini-batch mean and variance\n        # if we are in training mode or if we have not tracked exponential moving averages\n        if self.training or not self.track_running_stats:\n            # Calculate the mean across first and last dimension;\n            # i.e. the means for each feature $\\mathbb{E}[x^{(k)}]$\n            mean = x.mean(dim=[0, 2])\n            # Calculate the squared mean across first and last dimension;\n            # i.e. the means for each feature $\\mathbb{E}[(x^{(k)})^2]$\n            mean_x2 = (x ** 2).mean(dim=[0, 2])\n            # Variance for each feature $Var[x^{(k)}] = \\mathbb{E}[(x^{(k)})^2] - \\mathbb{E}[x^{(k)}]^2$\n            var = mean_x2 - mean ** 2\n\n            # Update exponential moving averages\n            if self.training and self.track_running_stats:\n                self.exp_mean = (1 - self.momentum) * self.exp_mean + self.momentum * mean\n                self.exp_var = (1 - self.momentum) * self.exp_var + self.momentum * var\n        # Use exponential moving averages as estimates\n        else:\n            mean = self.exp_mean\n            var = self.exp_var\n\n        # Normalize $$\\hat{x}^{(k)} = \\frac{x^{(k)} - \\mathbb{E}[x^{(k)}]}{\\sqrt{Var[x^{(k)}] + \\epsilon}}$$\n        x_norm = (x - mean.view(1, -1, 1)) / torch.sqrt(var + self.eps).view(1, -1, 1)\n        # Scale and shift $$y^{(k)} =\\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}$$\n        if self.affine:\n            x_norm = self.scale.view(1, -1, 1) * x_norm + self.shift.view(1, -1, 1)\n\n        # Reshape to original and return\n        return x_norm.view(x_shape)\n\n\ndef _test():\n    \"\"\"\n    Simple test\n    \"\"\"\n    from labml.logger import inspect\n\n    x = torch.zeros([2, 3, 2, 4])\n    inspect(x.shape)\n    bn = BatchNorm(3)\n\n    x = bn(x)\n    inspect(x.shape)\n    inspect(bn.exp_var.shape)\n\n\n#\nif __name__ == '__main__':\n    _test()\n", "labml_nn/normalization/instance_norm/experiment.py": "\"\"\"\n---\ntitle: CIFAR10 Experiment to try Instance Normalization\nsummary: >\n  This trains is a simple convolutional neural network that uses instance normalization\n  to classify CIFAR10 images.\n---\n\n# CIFAR10 Experiment for Instance Normalization\n\nThis demonstrates the use of an instance normalization layer in a convolutional\nneural network for classification. Not that instance normalization was designed for\nstyle transfer and this is only a demo.\n\"\"\"\n\nimport torch.nn as nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.experiments.cifar10 import CIFAR10Configs, CIFAR10VGGModel\nfrom labml_nn.normalization.instance_norm import InstanceNorm\n\n\nclass Model(CIFAR10VGGModel):\n    \"\"\"\n    ### VGG model for CIFAR-10 classification\n\n    This derives from the [generic VGG style architecture](../../experiments/cifar10.html).\n    \"\"\"\n\n    def conv_block(self, in_channels, out_channels) -> nn.Module:\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            InstanceNorm(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def __init__(self):\n        super().__init__([[64, 64], [128, 128], [256, 256, 256], [512, 512, 512], [512, 512, 512]])\n\n\n@option(CIFAR10Configs.model)\ndef _model(c: CIFAR10Configs):\n    \"\"\"\n    ### Create model\n    \"\"\"\n    return Model().to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='cifar10', comment='instance norm')\n    # Create configurations\n    conf = CIFAR10Configs()\n    # Load configurations\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n    })\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/normalization/instance_norm/__init__.py": "\"\"\"\n---\ntitle: Instance Normalization\nsummary: >\n A PyTorch implementation/tutorial of instance normalization.\n---\n\n# Instance Normalization\n\nThis is a [PyTorch](https://pytorch.org) implementation of\n[Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022).\n\nInstance normalization was introduced to improve [style transfer](https://paperswithcode.com/task/style-transfer).\nIt is based on the observation that stylization should not depend on the contrast of the content image.\nThe \"contrast normalization\" is\n\n$$y_{t,i,j,k} = \\frac{x_{t,i,j,k}}{\\sum_{l=1}^H \\sum_{m=1}^W x_{t,i,l,m}}$$\n\nwhere $x$ is a batch of images with dimensions image index $t$,\nfeature channel $i$, and\nspatial position $j, k$.\n\nSince it's hard for a convolutional network to learn \"contrast normalization\", this paper\nintroduces instance normalization which does that.\n\nHere's a [CIFAR 10 classification model](experiment.html) that uses instance normalization.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass InstanceNorm(Module):\n    r\"\"\"\n    ## Instance Normalization Layer\n\n    Instance normalization layer $\\text{IN}$ normalizes the input $X$ as follows:\n\n    When input $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ is a batch of image representations,\n    where $B$ is the batch size, $C$ is the number of channels, $H$ is the height and $W$ is the width.\n    $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$. The affine transformation with $gamma$ and\n    $beta$ are optional.\n\n    $$\\text{IN}(X) = \\gamma\n    \\frac{X - \\underset{H, W}{\\mathbb{E}}[X]}{\\sqrt{\\underset{H, W}{Var}[X] + \\epsilon}}\n    + \\beta$$\n    \"\"\"\n\n    def __init__(self, channels: int, *,\n                 eps: float = 1e-5, affine: bool = True):\n        \"\"\"\n        * `channels` is the number of features in the input\n        * `eps` is $\\epsilon$, used in $\\sqrt{Var[X] + \\epsilon}$ for numerical stability\n        * `affine` is whether to scale and shift the normalized value\n        \"\"\"\n        super().__init__()\n\n        self.channels = channels\n\n        self.eps = eps\n        self.affine = affine\n        # Create parameters for $\\gamma$ and $\\beta$ for scale and shift\n        if self.affine:\n            self.scale = nn.Parameter(torch.ones(channels))\n            self.shift = nn.Parameter(torch.zeros(channels))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        `x` is a tensor of shape `[batch_size, channels, *]`.\n        `*` denotes any number of (possibly 0) dimensions.\n         For example, in an image (2D) convolution this will be\n        `[batch_size, channels, height, width]`\n        \"\"\"\n        # Keep the original shape\n        x_shape = x.shape\n        # Get the batch size\n        batch_size = x_shape[0]\n        # Sanity check to make sure the number of features is the same\n        assert self.channels == x.shape[1]\n\n        # Reshape into `[batch_size, channels, n]`\n        x = x.view(batch_size, self.channels, -1)\n\n        # Calculate the mean across last dimension\n        # i.e. the means for each feature  $\\mathbb{E}[x_{t,i}]$\n        mean = x.mean(dim=[-1], keepdim=True)\n        # Calculate the squared mean across first and last dimension;\n        # i.e. the means for each feature $\\mathbb{E}[(x_{t,i}^2]$\n        mean_x2 = (x ** 2).mean(dim=[-1], keepdim=True)\n        # Variance for each feature $Var[x_{t,i}] = \\mathbb{E}[x_{t,i}^2] - \\mathbb{E}[x_{t,i}]^2$\n        var = mean_x2 - mean ** 2\n\n        # Normalize $$\\hat{x}_{t,i} = \\frac{x_{t,i} - \\mathbb{E}[x_{t,i}]}{\\sqrt{Var[x_{t,i}] + \\epsilon}}$$\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        x_norm = x_norm.view(batch_size, self.channels, -1)\n\n        # Scale and shift $$y_{t,i} =\\gamma_i \\hat{x}_{t,i} + \\beta_i$$\n        if self.affine:\n            x_norm = self.scale.view(1, -1, 1) * x_norm + self.shift.view(1, -1, 1)\n\n        # Reshape to original and return\n        return x_norm.view(x_shape)\n\n\ndef _test():\n    \"\"\"\n    Simple test\n    \"\"\"\n    from labml.logger import inspect\n\n    x = torch.zeros([2, 6, 2, 4])\n    inspect(x.shape)\n    bn = InstanceNorm(6)\n\n    x = bn(x)\n    inspect(x.shape)\n\n\n#\nif __name__ == '__main__':\n    _test()\n", "labml_nn/normalization/deep_norm/experiment.py": "\"\"\"\n---\ntitle: DeepNorm Experiment\nsummary: >\n Training a DeepNorm transformer on Tiny Shakespeare.\n---\n\n# [DeepNorm](index.html) Experiment\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/deep_norm/experiment.ipynb)\n\"\"\"\n\nimport copy\n\nimport torch\nimport torch.nn as nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.normalization.deep_norm import DeepNormTransformerLayer\nfrom labml_nn.transformers import MultiHeadAttention\nfrom labml_nn.transformers.feed_forward import FeedForward\n\n\nclass AutoregressiveTransformer(Module):\n    \"\"\"\n    ## Auto-Regressive model\n\n    This is a autoregressive transformer model that uses DeepNorm.\n    \"\"\"\n\n    def __init__(self, n_tokens: int, d_model: int, n_layers: int, layer: DeepNormTransformerLayer):\n        \"\"\"\n        :param n_tokens: is the number of tokens in the vocabulary\n        :param d_model: is the embedding size\n        :param n_layers: is the number of transformer layers\n        :param layer: is the layer. We use `n_layers` copies of this for the tranformer.\n        \"\"\"\n        super().__init__()\n        # Transformer with `n_layers` layers\n        self.transformer = nn.Sequential(*[copy.deepcopy(layer) for _ in range(n_layers)])\n\n        # Token embedding layer\n        self.emb = nn.Embedding(n_tokens, d_model)\n        # Readout layer\n        self.readout = nn.Linear(d_model, n_tokens)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: are the input tokens of shape `[seq_len, batch_size]`\n        \"\"\"\n        # Get the token embeddings\n        x = self.emb(x)\n        # Transformer encoder\n        x = self.transformer(x)\n        # Get logits\n        x = self.readout(x)\n\n        # Return results\n        return x, None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`NLPAutoRegressionConfigs`](../../experiments/nlp_autoregression.html#NLPAutoRegressionConfigs)\n    \"\"\"\n\n    # Model\n    model: AutoregressiveTransformer\n\n    # Number of layers\n    n_layers: int = 32\n\n    # $\\alpha$ and $\\beta$ for DeepNorm\n    deep_norm_alpha: float\n    deep_norm_beta: float\n\n    # Number of heads in the attention\n    n_heads: int = 4\n    # Embedding size\n    d_model: int = 64\n    # Size of each attention head\n    d_k: int = 16\n\n\n@option(Configs.deep_norm_alpha)\ndef _deep_norm_alpha(c: Configs):\n    \"\"\"\n    #### Calculate $\\alpha$\n\n    $\\alpha = (2M)^{\\frac{1}{4}}$\n    \"\"\"\n    return (2. * c.n_layers) ** (1. / 4.)\n\n\n@option(Configs.deep_norm_beta)\ndef _deep_norm_beta(c: Configs):\n    \"\"\"\n    #### Calculate $\\beta$\n\n    $\\beta = (8M)^{-\\frac{1}{4}}$\n    \"\"\"\n    return (8. * c.n_layers) ** -(1. / 4.)\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    #### Initialize the model\n    \"\"\"\n    m = AutoregressiveTransformer(c.n_tokens, c.d_model, c.n_layers,\n                                  DeepNormTransformerLayer(d_model=c.d_model,\n                                                           deep_norm_alpha=c.deep_norm_alpha,\n                                                           deep_norm_beta=c.deep_norm_beta,\n                                                           feed_forward=FeedForward(d_model=c.d_model,\n                                                                                    d_ff=c.d_model * 4),\n                                                           self_attn=MultiHeadAttention(c.n_heads, c.d_model,\n                                                                                        dropout_prob=0.0)))\n\n    return m.to(c.device)\n\n\ndef main():\n    \"\"\"\n    #### Create and run the experiment\n    \"\"\"\n    # Create experiment\n    experiment.create(name=\"deep_norm\", writers={'screen', 'web_api'})\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 256,\n        # Train for 32 epochs\n        'epochs': 32,\n        # Batch size $16$\n        'batch_size': 16,\n        # Switch between training and validation for $10$ times per epoch\n        'inner_iterations': 10,\n\n        # Number of layers\n        'n_layers': 50,\n\n\n        # Adam optimizer with no warmup\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 1.25e-4,\n    })\n\n    # Set model(s) for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/normalization/deep_norm/__init__.py": "\"\"\"\n---\ntitle: DeepNorm\nsummary: >\n A PyTorch implementation/tutorial of DeepNorm from paper DeepNet: Scaling Transformers to 1,000 Layers.\n---\n\n# DeepNorm\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/deep_norm/experiment.ipynb)\n\nThis is a [PyTorch](https://pytorch.org) implementation of\nthe DeepNorm from the paper\n[DeepNet: Scaling Transformers to 1,000 Layers](https://arxiv.org/abs/2203.00555).\n\nThe paper proposes a method to stabilize extremely deep transformers through a new normalizing function\nto replace LayerNorm and a weight initialization scheme.\nThis combines the performance of Post-LayerNorm and the stability of Pre-LayerNorm.\nTransformers with DeepNorms are supposed to be stable even without a learning rate warm-up.\n\nThe paper first shows that the changes to layer outputs (for the same input)\n change gradually during stable training;\nwhen unstable it changes rapidly during the initial training steps.\nThis happens with initializing weights to small values, and learning rate warm-ups where the\ntraining is stable.\nThey use the idea of keeping the changes to layer outputs small to derive the new\n normalization and weight initialization mechanism.\n\n## Weight Initializations\n\nUsually, the weights are initialized with Xavier or Kaiming initializations.\nThis paper scales (sets the gain) the weights by a constant $\\beta$ depending on the size of the\n transformer.\n\nDeepNorm suggests scaling the weights of the two linear transforms in the\n[Feed-Forward Network](../../transformers/feed_forward.html),\nthe value projection transform, and the output projection transform of the\nattention layer.\nWeights of these transforms are scaled by (has a gain equal to) $\\beta$.\n\nThe scaling is implemented in the\n\n## Normalization Function\n\n$$x_{l + 1} = \\mathop{LN}\\Big( \\alpha x_l + \\mathop{G}_l \\big(x_l, \\theta_l \\big)\\Big)$$\n\nwhere $\\alpha$ is a constant that depends on the depth of the transformer,\n $\\mathop{LN}$ is [Layer Normalization](../layer_norm/index.html), and\n $\\mathop{G}_l (x_l, \\theta_l)$ is the function of the $l$-th transformer sub-layer (FFN or attention).\n\nThis function is used to replace Post-LayerNorm.\n\n## $\\alpha$ and $\\beta$ constants\n\n\\begin{align}\n\\begin{array} {c|cc|cc}\n\\text{Type} & \\text{Enc-} \\alpha & \\text{Enc-} \\beta &  \\text{Dec-} \\alpha & \\text{Dec-} \\beta \\\\\n\\hline \\\\\n\\text{Encoder only} & (2N)^{\\frac{1}{4}} & (8N)^{-\\frac{1}{4}} & - & - \\\\\n\\text{Decoder only} & - & - & (2M)^{\\frac{1}{4}} & (8M)^{-\\frac{1}{4}} \\\\\n\\text{Enc-Dec} & 0.81 (N^4M)^{\\frac{1}{16}} & 0.87 (N^4 M)^{-\\frac{1}{16}} &\n (3M)^{\\frac{1}{4}} & (12M)^{-\\frac{1}{4}} \\\\\n\\end{array}\n\\end{align}\n\nWhere $N$ is the number of layers in the encoder and $M$ is the number of layers in the decoder.\n\nRefer to [the paper](https://arxiv.org/abs/2203.00555) for derivation.\n\n[Here is an experiment implementation](experiment.html) that uses DeepNorm.\n\"\"\"\n\nfrom typing import Union, List\n\nimport torch\nfrom torch import nn, Size\n\nfrom labml_nn.normalization.layer_norm import LayerNorm\nfrom labml_nn.transformers import MultiHeadAttention\nfrom labml_nn.transformers.feed_forward import FeedForward\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass DeepNorm(nn.Module):\n    \"\"\"\n    ## DeepNorm Normalization\n\n    $$x_{l + 1} = \\mathop{LN}\\Big( \\alpha x_l + \\mathop{G}_l \\big(x_l, \\theta_l \\big)\\Big)$$\n    \"\"\"\n\n    def __init__(self, alpha: float, normalized_shape: Union[int, List[int], Size], *,\n                 eps: float = 1e-5,\n                 elementwise_affine: bool = True):\n        \"\"\"\n        :param alpha: is $\\alpha$\n        :param normalized_shape: is the shape for LayerNorm $\\mathop{LN}$\n        :param eps: is $\\epsilon$ for LayerNorm\n        :param elementwise_affine: is a flag indicating whether to do an elementwise transformation in LayerNorm\n        \"\"\"\n        super().__init__()\n\n        self.alpha = alpha\n        # Initialize $\\mathop{LN}$\n        self.layer_norm = LayerNorm(normalized_shape, eps=eps, elementwise_affine=elementwise_affine)\n\n    def forward(self, x: torch.Tensor, gx: torch.Tensor):\n        \"\"\"\n        :param x: is the output from the previous layer $x_l$\n        :param gx: is the output of the current sub-layer $\\mathop{G}_l (x_l, \\theta_l)$\n        \"\"\"\n        # $$x_{l + 1} = \\mathop{LN}\\Big( \\alpha x_l + \\mathop{G}_l \\big(x_l, \\theta_l \\big)\\Big)$$\n        return self.layer_norm(x + self.alpha * gx)\n\n\nclass DeepNormTransformerLayer(nn.Module):\n    \"\"\"\n    ## Transformer Decoder Layer with DeepNorm\n\n    This implements a transformer decoder layer with DeepNorm.\n    Encoder layers will have a similar form.\n    \"\"\"\n    def __init__(self, *,\n                 d_model: int,\n                 self_attn: MultiHeadAttention,\n                 feed_forward: FeedForward,\n                 deep_norm_alpha: float,\n                 deep_norm_beta: float,\n                 ):\n        \"\"\"\n        :param d_model: is the token embedding size\n        :param self_attn: is the self attention module\n        :param feed_forward: is the feed forward module\n        :param deep_norm_alpha: is $\\alpha$ coefficient in DeepNorm\n        :param deep_norm_beta: is $\\beta$ constant for scaling weights initialization\n        \"\"\"\n        super().__init__()\n\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        # DeepNorms after attention and feed forward network\n        self.self_attn_norm = DeepNorm(deep_norm_alpha, [d_model])\n        self.feed_forward_norm = DeepNorm(deep_norm_alpha, [d_model])\n\n        # Scale weights after initialization\n        with torch.no_grad():\n            # Feed forward network linear transformations\n            feed_forward.layer1.weight *= deep_norm_beta\n            feed_forward.layer2.weight *= deep_norm_beta\n\n            # Attention value projection\n            self_attn.value.linear.weight *= deep_norm_beta\n            # Attention output project\n            self_attn.output.weight *= deep_norm_beta\n\n        # The mask will be initialized on the first call\n        self.mask = None\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: are the embeddings of shape `[seq_len, batch_size, d_model]`\n        \"\"\"\n        # Create causal mask\n        if self.mask is None or self.mask.size(0) != len(x):\n            # Subsequent mask, will mask out tokens from seeing future tokens\n            self.mask = subsequent_mask(len(x)).to(x.device)\n\n        # Run through self attention, i.e. keys and values are from self\n        x = self.self_attn_norm(x, self.self_attn(query=x, key=x, value=x, mask=self.mask))\n        # Pass through the feed-forward network\n        x = self.feed_forward_norm(x, self.feed_forward(x))\n\n        #\n        return x\n", "labml_nn/normalization/group_norm/experiment.py": "\"\"\"\n---\ntitle: CIFAR10 Experiment to try Group Normalization\nsummary: >\n  This trains is a simple convolutional neural network that uses group normalization\n  to classify CIFAR10 images.\n---\n\n# CIFAR10 Experiment for Group Normalization\n\"\"\"\n\nimport torch.nn as nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_nn.experiments.cifar10 import CIFAR10Configs, CIFAR10VGGModel\nfrom labml_nn.normalization.group_norm import GroupNorm\n\n\nclass Model(CIFAR10VGGModel):\n    \"\"\"\n    ### VGG model for CIFAR-10 classification\n\n    This derives from the [generic VGG style architecture](../../experiments/cifar10.html).\n    \"\"\"\n\n    def conv_block(self, in_channels, out_channels) -> nn.Module:\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            fnorm.GroupNorm(self.groups, out_channels),#new\n            nn.ReLU(inplace=True),\n        )\n\n    def __init__(self, groups: int = 32):\n        self.groups = groups#input param:groups to conv_block\n        super().__init__([[64, 64], [128, 128], [256, 256, 256], [512, 512, 512], [512, 512, 512]])\n\n\nclass Configs(CIFAR10Configs):\n    # Number of groups\n    groups: int = 16\n\n\n@option(Configs.model)\ndef model(c: Configs):\n    \"\"\"\n    ### Create model\n    \"\"\"\n    return Model(c.groups).to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='cifar10', comment='group norm')\n    # Create configurations\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n    })\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/normalization/group_norm/__init__.py": "\"\"\"\n---\ntitle: Group Normalization\nsummary: >\n A PyTorch implementation/tutorial of group normalization.\n---\n\n# Group Normalization\n\nThis is a [PyTorch](https://pytorch.org) implementation of\nthe [Group Normalization](https://arxiv.org/abs/1803.08494) paper.\n\n[Batch Normalization](../batch_norm/index.html) works well for large enough batch sizes\nbut not well for small batch sizes, because it normalizes over the batch.\nTraining large models with large batch sizes is not possible due to the memory capacity of the\ndevices.\n\nThis paper introduces Group Normalization, which normalizes a set of features together as a group.\nThis is based on the observation that classical features such as\n[SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform) and\n[HOG](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients) are group-wise features.\nThe paper proposes dividing feature channels into groups and then separately normalizing\nall channels within each group.\n\n## Formulation\n\nAll normalization layers can be defined by the following computation.\n\n$$\\hat{x}_i = \\frac{1}{\\sigma_i} (x_i - \\mu_i)$$\n\nwhere $x$ is the tensor representing the batch,\nand $i$ is the index of a single value.\nFor instance, when it's 2D images\n$i = (i_N, i_C, i_H, i_W)$ is a 4-d vector for indexing\nimage within batch, feature channel, vertical coordinate and horizontal coordinate.\n$\\mu_i$ and $\\sigma_i$ are mean and standard deviation.\n\n\\begin{align}\n\\mu_i &= \\frac{1}{m} \\sum_{k \\in \\mathcal{S}_i} x_k \\\\\n\\sigma_i  &= \\sqrt{\\frac{1}{m} \\sum_{k \\in \\mathcal{S}_i} (x_k - \\mu_i)^2 + \\epsilon}\n\\end{align}\n\n$\\mathcal{S}_i$ is the set of indexes across which the mean and standard deviation\nare calculated for index $i$.\n$m$ is the size of the set $\\mathcal{S}_i$ which is the same for all $i$.\n\nThe definition of $\\mathcal{S}_i$ is different for\n[Batch normalization](../batch_norm/index.html),\n[Layer normalization](../layer_norm/index.html), and\n[Instance normalization](../instance_norm/index.html).\n\n### [Batch Normalization](../batch_norm/index.html)\n\n$$\\mathcal{S}_i = \\{k | k_C = i_C\\}$$\n\nThe values that share the same feature channel are normalized together.\n\n### [Layer Normalization](../layer_norm/index.html)\n\n$$\\mathcal{S}_i = \\{k | k_N = i_N\\}$$\n\nThe values from the same sample in the batch are normalized together.\n\n### [Instance Normalization](../instance_norm/index.html)\n\n$$\\mathcal{S}_i = \\{k | k_N = i_N, k_C = i_C\\}$$\n\nThe values from the same sample and same feature channel are normalized together.\n\n### Group Normalization\n\n$$\\mathcal{S}_i = \\{k | k_N = i_N,\n \\bigg \\lfloor \\frac{k_C}{C/G} \\bigg \\rfloor = \\bigg \\lfloor \\frac{i_C}{C/G} \\bigg \\rfloor\\}$$\n\nwhere $G$ is the number of groups and $C$ is the number of channels.\n\nGroup normalization normalizes values of the same sample and the same group of channels together.\n\nHere's a [CIFAR 10 classification model](experiment.html) that uses instance normalization.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/group_norm/experiment.ipynb)\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass GroupNorm(Module):\n    r\"\"\"\n    ## Group Normalization Layer\n    \"\"\"\n\n    def __init__(self, groups: int, channels: int, *,\n                 eps: float = 1e-5, affine: bool = True):\n        \"\"\"\n        * `groups` is the number of groups the features are divided into\n        * `channels` is the number of features in the input\n        * `eps` is $\\epsilon$, used in $\\sqrt{Var[x^{(k)}] + \\epsilon}$ for numerical stability\n        * `affine` is whether to scale and shift the normalized value\n        \"\"\"\n        super().__init__()\n\n        assert channels % groups == 0, \"Number of channels should be evenly divisible by the number of groups\"\n        self.groups = groups\n        self.channels = channels\n\n        self.eps = eps\n        self.affine = affine\n        # Create parameters for $\\gamma$ and $\\beta$ for scale and shift\n        if self.affine:\n            self.scale = nn.Parameter(torch.ones(channels))\n            self.shift = nn.Parameter(torch.zeros(channels))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        `x` is a tensor of shape `[batch_size, channels, *]`.\n        `*` denotes any number of (possibly 0) dimensions.\n         For example, in an image (2D) convolution this will be\n        `[batch_size, channels, height, width]`\n        \"\"\"\n        # Keep the original shape\n        x_shape = x.shape\n        # Get the batch size\n        batch_size = x_shape[0]\n        # Sanity check to make sure the number of features is the same\n        assert self.channels == x.shape[1]\n\n        # Reshape into `[batch_size, groups, n]`\n        x = x.view(batch_size, self.groups, -1)\n\n        # Calculate the mean across last dimension;\n        # i.e. the means for each sample and channel group $\\mathbb{E}[x_{(i_N, i_G)}]$\n        mean = x.mean(dim=[-1], keepdim=True)\n        # Calculate the squared mean across last dimension;\n        # i.e. the means for each sample and channel group $\\mathbb{E}[x^2_{(i_N, i_G)}]$\n        mean_x2 = (x ** 2).mean(dim=[-1], keepdim=True)\n        # Variance for each sample and feature group\n        # $Var[x_{(i_N, i_G)}] = \\mathbb{E}[x^2_{(i_N, i_G)}] - \\mathbb{E}[x_{(i_N, i_G)}]^2$\n        var = mean_x2 - mean ** 2\n\n        # Normalize\n        # $$\\hat{x}_{(i_N, i_G)} =\n        # \\frac{x_{(i_N, i_G)} - \\mathbb{E}[x_{(i_N, i_G)}]}{\\sqrt{Var[x_{(i_N, i_G)}] + \\epsilon}}$$\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n\n        # Scale and shift channel-wise\n        # $$y_{i_C} =\\gamma_{i_C} \\hat{x}_{i_C} + \\beta_{i_C}$$\n        if self.affine:\n            x_norm = x_norm.view(batch_size, self.channels, -1)\n            x_norm = self.scale.view(1, -1, 1) * x_norm + self.shift.view(1, -1, 1)\n\n        # Reshape to original and return\n        return x_norm.view(x_shape)\n\n\ndef _test():\n    \"\"\"\n    Simple test\n    \"\"\"\n    from labml.logger import inspect\n\n    x = torch.zeros([2, 6, 2, 4])\n    inspect(x.shape)\n    bn = GroupNorm(2, 6)\n\n    x = bn(x)\n    inspect(x.shape)\n\n\n#\nif __name__ == '__main__':\n    _test()\n", "labml_nn/normalization/batch_channel_norm/__init__.py": "\"\"\"\n---\ntitle: Batch-Channel Normalization\nsummary: >\n A PyTorch implementation/tutorial of Batch-Channel Normalization.\n---\n\n# Batch-Channel Normalization\n\nThis is a [PyTorch](https://pytorch.org) implementation of Batch-Channel Normalization from the paper\n [Micro-Batch Training with Batch-Channel Normalization and Weight Standardization](https://arxiv.org/abs/1903.10520).\nWe also have an [annotated implementation of Weight Standardization](../weight_standardization/index.html).\n\nBatch-Channel Normalization performs batch normalization followed\nby a channel normalization (similar to a [Group Normalization](../group_norm/index.html).\nWhen the batch size is small a running mean and variance is used for\nbatch normalization.\n\nHere is [the training code](../weight_standardization/experiment.html) for training\na VGG network that uses weight standardization to classify CIFAR-10 data.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/weight_standardization/experiment.ipynb)\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.normalization.batch_norm import BatchNorm\n\n\nclass BatchChannelNorm(Module):\n    \"\"\"\n    ## Batch-Channel Normalization\n\n    This first performs a batch normalization - either [normal batch norm](../batch_norm/index.html)\n    or a batch norm with\n    estimated mean and variance (exponential mean/variance over multiple batches).\n    Then a channel normalization performed.\n    \"\"\"\n\n    def __init__(self, channels: int, groups: int,\n                 eps: float = 1e-5, momentum: float = 0.1, estimate: bool = True):\n        \"\"\"\n        * `channels` is the number of features in the input\n        * `groups` is the number of groups the features are divided into\n        * `eps` is $\\epsilon$, used in $\\sqrt{Var[x^{(k)}] + \\epsilon}$ for numerical stability\n        * `momentum` is the momentum in taking the exponential moving average\n        * `estimate` is whether to use running mean and variance for batch norm\n        \"\"\"\n        super().__init__()\n\n        # Use estimated batch norm or normal batch norm.\n        if estimate:\n            self.batch_norm = EstimatedBatchNorm(channels,\n                                                 eps=eps, momentum=momentum)\n        else:\n            self.batch_norm = BatchNorm(channels,\n                                        eps=eps, momentum=momentum)\n\n        # Channel normalization\n        self.channel_norm = ChannelNorm(channels, groups, eps)\n\n    def forward(self, x):\n        x = self.batch_norm(x)\n        return self.channel_norm(x)\n\n\nclass EstimatedBatchNorm(Module):\n    \"\"\"\n    ## Estimated Batch Normalization\n\n    When input $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ is a batch of image representations,\n    where $B$ is the batch size, $C$ is the number of channels, $H$ is the height and $W$ is the width.\n    $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$.\n\n    $$\\dot{X}_{\\cdot, C, \\cdot, \\cdot} = \\gamma_C\n    \\frac{X_{\\cdot, C, \\cdot, \\cdot} - \\hat{\\mu}_C}{\\hat{\\sigma}_C}\n    + \\beta_C$$\n\n    where,\n\n    \\begin{align}\n    \\hat{\\mu}_C &\\longleftarrow (1 - r)\\hat{\\mu}_C + r \\frac{1}{B H W} \\sum_{b,h,w} X_{b,c,h,w} \\\\\n    \\hat{\\sigma}^2_C &\\longleftarrow (1 - r)\\hat{\\sigma}^2_C + r \\frac{1}{B H W} \\sum_{b,h,w} \\big(X_{b,c,h,w} - \\hat{\\mu}_C \\big)^2\n    \\end{align}\n\n    are the running mean and variances. $r$ is the momentum for calculating the exponential mean.\n    \"\"\"\n    def __init__(self, channels: int,\n                 eps: float = 1e-5, momentum: float = 0.1, affine: bool = True):\n        \"\"\"\n        * `channels` is the number of features in the input\n        * `eps` is $\\epsilon$, used in $\\sqrt{Var[x^{(k)}] + \\epsilon}$ for numerical stability\n        * `momentum` is the momentum in taking the exponential moving average\n        * `estimate` is whether to use running mean and variance for batch norm\n        \"\"\"\n        super().__init__()\n\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.channels = channels\n\n        # Channel wise transformation parameters\n        if self.affine:\n            self.scale = nn.Parameter(torch.ones(channels))\n            self.shift = nn.Parameter(torch.zeros(channels))\n\n        # Tensors for $\\hat{\\mu}_C$ and $\\hat{\\sigma}^2_C$\n        self.register_buffer('exp_mean', torch.zeros(channels))\n        self.register_buffer('exp_var', torch.ones(channels))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        `x` is a tensor of shape `[batch_size, channels, *]`.\n        `*` denotes any number of (possibly 0) dimensions.\n         For example, in an image (2D) convolution this will be\n        `[batch_size, channels, height, width]`\n        \"\"\"\n        # Keep old shape\n        x_shape = x.shape\n        # Get the batch size\n        batch_size = x_shape[0]\n\n        # Sanity check to make sure the number of features is correct\n        assert self.channels == x.shape[1]\n\n        # Reshape into `[batch_size, channels, n]`\n        x = x.view(batch_size, self.channels, -1)\n\n        # Update $\\hat{\\mu}_C$ and $\\hat{\\sigma}^2_C$ in training mode only\n        if self.training:\n            # No backpropagation through $\\hat{\\mu}_C$ and $\\hat{\\sigma}^2_C$\n            with torch.no_grad():\n                # Calculate the mean across first and last dimensions;\n                # $$\\frac{1}{B H W} \\sum_{b,h,w} X_{b,c,h,w}$$\n                mean = x.mean(dim=[0, 2])\n                # Calculate the squared mean across first and last dimensions;\n                # $$\\frac{1}{B H W} \\sum_{b,h,w} X^2_{b,c,h,w}$$\n                mean_x2 = (x ** 2).mean(dim=[0, 2])\n                # Variance for each feature\n                # $$\\frac{1}{B H W} \\sum_{b,h,w} \\big(X_{b,c,h,w} - \\hat{\\mu}_C \\big)^2$$\n                var = mean_x2 - mean ** 2\n\n                # Update exponential moving averages\n                #\n                # \\begin{align}\n                # \\hat{\\mu}_C &\\longleftarrow (1 - r)\\hat{\\mu}_C + r \\frac{1}{B H W} \\sum_{b,h,w} X_{b,c,h,w} \\\\\n                # \\hat{\\sigma}^2_C &\\longleftarrow (1 - r)\\hat{\\sigma}^2_C + r \\frac{1}{B H W} \\sum_{b,h,w} \\big(X_{b,c,h,w} - \\hat{\\mu}_C \\big)^2\n                # \\end{align}\n                self.exp_mean = (1 - self.momentum) * self.exp_mean + self.momentum * mean\n                self.exp_var = (1 - self.momentum) * self.exp_var + self.momentum * var\n\n        # Normalize\n        # $$\\frac{X_{\\cdot, C, \\cdot, \\cdot} - \\hat{\\mu}_C}{\\hat{\\sigma}_C}$$\n        x_norm = (x - self.exp_mean.view(1, -1, 1)) / torch.sqrt(self.exp_var + self.eps).view(1, -1, 1)\n        # Scale and shift\n        # $$ \\gamma_C\n        #     \\frac{X_{\\cdot, C, \\cdot, \\cdot} - \\hat{\\mu}_C}{\\hat{\\sigma}_C}\n        #     + \\beta_C$$\n        if self.affine:\n            x_norm = self.scale.view(1, -1, 1) * x_norm + self.shift.view(1, -1, 1)\n\n        # Reshape to original and return\n        return x_norm.view(x_shape)\n\n\nclass ChannelNorm(Module):\n    \"\"\"\n    ## Channel Normalization\n\n    This is similar to [Group Normalization](../group_norm/index.html) but affine transform is done group wise.\n    \"\"\"\n\n    def __init__(self, channels, groups,\n                 eps: float = 1e-5, affine: bool = True):\n        \"\"\"\n        * `groups` is the number of groups the features are divided into\n        * `channels` is the number of features in the input\n        * `eps` is $\\epsilon$, used in $\\sqrt{Var[x^{(k)}] + \\epsilon}$ for numerical stability\n        * `affine` is whether to scale and shift the normalized value\n        \"\"\"\n        super().__init__()\n        self.channels = channels\n        self.groups = groups\n        self.eps = eps\n        self.affine = affine\n        # Parameters for affine transformation.\n        #\n        # *Note that these transforms are per group, unlike in group norm where\n        # they are transformed channel-wise.*\n        if self.affine:\n            self.scale = nn.Parameter(torch.ones(groups))\n            self.shift = nn.Parameter(torch.zeros(groups))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        `x` is a tensor of shape `[batch_size, channels, *]`.\n        `*` denotes any number of (possibly 0) dimensions.\n         For example, in an image (2D) convolution this will be\n        `[batch_size, channels, height, width]`\n        \"\"\"\n\n        # Keep the original shape\n        x_shape = x.shape\n        # Get the batch size\n        batch_size = x_shape[0]\n        # Sanity check to make sure the number of features is the same\n        assert self.channels == x.shape[1]\n\n        # Reshape into `[batch_size, groups, n]`\n        x = x.view(batch_size, self.groups, -1)\n\n        # Calculate the mean across last dimension;\n        # i.e. the means for each sample and channel group $\\mathbb{E}[x_{(i_N, i_G)}]$\n        mean = x.mean(dim=[-1], keepdim=True)\n        # Calculate the squared mean across last dimension;\n        # i.e. the means for each sample and channel group $\\mathbb{E}[x^2_{(i_N, i_G)}]$\n        mean_x2 = (x ** 2).mean(dim=[-1], keepdim=True)\n        # Variance for each sample and feature group\n        # $Var[x_{(i_N, i_G)}] = \\mathbb{E}[x^2_{(i_N, i_G)}] - \\mathbb{E}[x_{(i_N, i_G)}]^2$\n        var = mean_x2 - mean ** 2\n\n        # Normalize\n        # $$\\hat{x}_{(i_N, i_G)} =\n        # \\frac{x_{(i_N, i_G)} - \\mathbb{E}[x_{(i_N, i_G)}]}{\\sqrt{Var[x_{(i_N, i_G)}] + \\epsilon}}$$\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n\n        # Scale and shift group-wise\n        # $$y_{i_G} =\\gamma_{i_G} \\hat{x}_{i_G} + \\beta_{i_G}$$\n        if self.affine:\n            x_norm = self.scale.view(1, -1, 1) * x_norm + self.shift.view(1, -1, 1)\n\n        # Reshape to original and return\n        return x_norm.view(x_shape)\n", "labml_nn/normalization/weight_standardization/experiment.py": "\"\"\"\n---\ntitle: CIFAR10 Experiment to try Weight Standardization and Batch-Channel Normalization\nsummary: >\n  This trains is a VGG net that uses weight standardization  and batch-channel normalization\n  to classify CIFAR10 images.\n---\n\n# CIFAR10 Experiment to try Weight Standardization and Batch-Channel Normalization\n\"\"\"\n\nimport torch.nn as nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.experiments.cifar10 import CIFAR10Configs, CIFAR10VGGModel\nfrom labml_nn.normalization.batch_channel_norm import BatchChannelNorm\nfrom labml_nn.normalization.weight_standardization.conv2d import Conv2d\n\n\nclass Model(CIFAR10VGGModel):\n    \"\"\"\n    ### VGG model for CIFAR-10 classification\n\n    This derives from the [generic VGG style architecture](../../experiments/cifar10.html).\n    \"\"\"\n\n    def conv_block(self, in_channels, out_channels) -> nn.Module:\n        return nn.Sequential(\n            Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            BatchChannelNorm(out_channels, 32),\n            nn.ReLU(inplace=True),\n        )\n\n    def __init__(self):\n        super().__init__([[64, 64], [128, 128], [256, 256, 256], [512, 512, 512], [512, 512, 512]])\n\n\n@option(CIFAR10Configs.model)\ndef _model(c: CIFAR10Configs):\n    \"\"\"\n    ### Create model\n    \"\"\"\n    return Model().to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='cifar10', comment='weight standardization')\n    # Create configurations\n    conf = CIFAR10Configs()\n    # Load configurations\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n        'train_batch_size': 64,\n    })\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/normalization/weight_standardization/__init__.py": "\"\"\"\n---\ntitle: Weight Standardization\nsummary: >\n A PyTorch implementation/tutorial of Weight Standardization.\n---\n\n# Weight Standardization\n\nThis is a [PyTorch](https://pytorch.org) implementation of Weight Standardization from the paper\n [Micro-Batch Training with Batch-Channel Normalization and Weight Standardization](https://arxiv.org/abs/1903.10520).\nWe also have an [annotated implementation of Batch-Channel Normalization](../batch_channel_norm/index.html).\n\nBatch normalization **gives a smooth loss landscape** and\n**avoids elimination singularities**.\nElimination singularities are nodes of the network that become\nuseless (e.g. a ReLU that gives 0 all the time).\n\nHowever, batch normalization doesn't work well when the batch size is too small,\nwhich happens when training large networks because of device memory limitations.\nThe paper introduces Weight Standardization with Batch-Channel Normalization as\na better alternative.\n\nWeight Standardization:\n1. Normalizes the gradients\n2. Smoothes the landscape (reduced Lipschitz constant)\n3. Avoids elimination singularities\n\nThe Lipschitz constant is the maximum slope a function has between two points.\nThat is, $L$ is the Lipschitz constant where $L$ is the smallest value that satisfies,\n$\\forall a,b \\in A: \\lVert f(a) - f(b) \\rVert \\le L \\lVert a - b \\rVert$\nwhere $f: A \\rightarrow \\mathbb{R}^m, A \\in \\mathbb{R}^n$.\n\nElimination singularities are avoided because it keeps the statistics of the outputs similar to the\ninputs. So as long as the inputs are normally distributed the outputs remain close to normal.\nThis avoids outputs of nodes from always falling beyond the active range of the activation function\n(e.g. always negative input for a ReLU).\n\n*[Refer to the paper for proofs](https://arxiv.org/abs/1903.10520)*.\n\nHere is [the training code](experiment.html) for training\na VGG network that uses weight standardization to classify CIFAR-10 data.\nThis uses a [2D-Convolution Layer with Weight Standardization](conv2d.html).\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/weight_standardization/experiment.ipynb)\n\"\"\"\n\nimport torch\n\n\ndef weight_standardization(weight: torch.Tensor, eps: float):\n    r\"\"\"\n    ## Weight Standardization\n\n    $$\\hat{W}_{i,j} = \\frac{W_{i,j} - \\mu_{W_{i,\\cdot}}} {\\sigma_{W_{i,\\cdot}}}$$\n\n    where,\n\n    \\begin{align}\n    W &\\in \\mathbb{R}^{O \\times I} \\\\\n    \\mu_{W_{i,\\cdot}} &= \\frac{1}{I} \\sum_{j=1}^I W_{i,j} \\\\\n    \\sigma_{W_{i,\\cdot}} &= \\sqrt{\\frac{1}{I} \\sum_{j=1}^I W^2_{i,j} - \\mu^2_{W_{i,\\cdot}} + \\epsilon} \\\\\n    \\end{align}\n\n    for a 2D-convolution layer $O$ is the number of output channels ($O = C_{out}$)\n    and $I$ is the number of input channels times the kernel size ($I = C_{in} \\times k_H \\times k_W$)\n    \"\"\"\n\n    # Get $C_{out}$, $C_{in}$ and kernel shape\n    c_out, c_in, *kernel_shape = weight.shape\n    # Reshape $W$ to $O \\times I$\n    weight = weight.view(c_out, -1)\n    # Calculate\n    #\n    # \\begin{align}\n    # \\mu_{W_{i,\\cdot}} &= \\frac{1}{I} \\sum_{j=1}^I W_{i,j} \\\\\n    # \\sigma^2_{W_{i,\\cdot}} &= \\frac{1}{I} \\sum_{j=1}^I W^2_{i,j} - \\mu^2_{W_{i,\\cdot}}\n    # \\end{align}\n    var, mean = torch.var_mean(weight, dim=1, keepdim=True)\n    # Normalize\n    # $$\\hat{W}_{i,j} = \\frac{W_{i,j} - \\mu_{W_{i,\\cdot}}} {\\sigma_{W_{i,\\cdot}}}$$\n    weight = (weight - mean) / (torch.sqrt(var + eps))\n    # Change back to original shape and return\n    return weight.view(c_out, c_in, *kernel_shape)\n", "labml_nn/normalization/weight_standardization/conv2d.py": "\"\"\"\n---\ntitle: 2D Convolution Layer with Weight Standardization\nsummary: >\n A PyTorch implementation/tutorial of a 2D Convolution Layer with Weight Standardization.\n---\n\n# 2D Convolution Layer with Weight Standardization\n\nThis is an implementation of a 2 dimensional convolution layer with [Weight Standardization](./index.html)\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom labml_nn.normalization.weight_standardization import weight_standardization\n\n\nclass Conv2d(nn.Conv2d):\n    \"\"\"\n    ## 2D Convolution Layer\n\n    This extends the standard 2D Convolution layer and standardize the weights before the convolution step.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups: int = 1,\n                 bias: bool = True,\n                 padding_mode: str = 'zeros',\n                 eps: float = 1e-5):\n        super(Conv2d, self).__init__(in_channels, out_channels, kernel_size,\n                                     stride=stride,\n                                     padding=padding,\n                                     dilation=dilation,\n                                     groups=groups,\n                                     bias=bias,\n                                     padding_mode=padding_mode)\n        self.eps = eps\n\n    def forward(self, x: torch.Tensor):\n        return F.conv2d(x, weight_standardization(self.weight, self.eps), self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n\ndef _test():\n    \"\"\"\n    A simple test to verify the tensor sizes\n    \"\"\"\n    conv2d = Conv2d(10, 20, 5)\n    from labml.logger import inspect\n    inspect(conv2d.weight)\n    import torch\n    inspect(conv2d(torch.zeros(10, 10, 100, 100)))\n\n\nif __name__ == '__main__':\n    _test()\n", "labml_nn/diffusion/__init__.py": "\"\"\"\n---\ntitle: Diffusion models\nsummary: >\n A set of PyTorch implementations/tutorials of diffusion models.\n---\n\n# Diffusion models\n\n* [Denoising Diffusion Probabilistic Models (DDPM)](ddpm/index.html)\n* [Stable Diffusion](stable_diffusion/index.html)\n* [Latent Diffusion Model](stable_diffusion/latent_diffusion.html)\n* [Denoising Diffusion Implicit Models (DDIM) Sampling](stable_diffusion/sampler/ddim.html)\n\"\"\"\n", "labml_nn/diffusion/stable_diffusion/util.py": "\"\"\"\n---\ntitle: Utility functions for stable diffusion\nsummary: >\n Utility functions for stable diffusion\n---\n\n# Utility functions for [stable diffusion](index.html)\n\"\"\"\n\nimport os\nimport random\nfrom pathlib import Path\n\nimport PIL\nimport numpy as np\nimport torch\nfrom PIL import Image\n\nfrom labml import monit\nfrom labml.logger import inspect\nfrom labml_nn.diffusion.stable_diffusion.latent_diffusion import LatentDiffusion\nfrom labml_nn.diffusion.stable_diffusion.model.autoencoder import Encoder, Decoder, Autoencoder\nfrom labml_nn.diffusion.stable_diffusion.model.clip_embedder import CLIPTextEmbedder\nfrom labml_nn.diffusion.stable_diffusion.model.unet import UNetModel\n\n\ndef set_seed(seed: int):\n    \"\"\"\n    ### Set random seeds\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef load_model(path: Path = None) -> LatentDiffusion:\n    \"\"\"\n    ### Load [`LatentDiffusion` model](latent_diffusion.html)\n    \"\"\"\n\n    # Initialize the autoencoder\n    with monit.section('Initialize autoencoder'):\n        encoder = Encoder(z_channels=4,\n                          in_channels=3,\n                          channels=128,\n                          channel_multipliers=[1, 2, 4, 4],\n                          n_resnet_blocks=2)\n\n        decoder = Decoder(out_channels=3,\n                          z_channels=4,\n                          channels=128,\n                          channel_multipliers=[1, 2, 4, 4],\n                          n_resnet_blocks=2)\n\n        autoencoder = Autoencoder(emb_channels=4,\n                                  encoder=encoder,\n                                  decoder=decoder,\n                                  z_channels=4)\n\n    # Initialize the CLIP text embedder\n    with monit.section('Initialize CLIP Embedder'):\n        clip_text_embedder = CLIPTextEmbedder()\n\n    # Initialize the U-Net\n    with monit.section('Initialize U-Net'):\n        unet_model = UNetModel(in_channels=4,\n                               out_channels=4,\n                               channels=320,\n                               attention_levels=[0, 1, 2],\n                               n_res_blocks=2,\n                               channel_multipliers=[1, 2, 4, 4],\n                               n_heads=8,\n                               tf_layers=1,\n                               d_cond=768)\n\n    # Initialize the Latent Diffusion model\n    with monit.section('Initialize Latent Diffusion model'):\n        model = LatentDiffusion(linear_start=0.00085,\n                                linear_end=0.0120,\n                                n_steps=1000,\n                                latent_scaling_factor=0.18215,\n\n                                autoencoder=autoencoder,\n                                clip_embedder=clip_text_embedder,\n                                unet_model=unet_model)\n\n    # Load the checkpoint\n    with monit.section(f\"Loading model from {path}\"):\n        checkpoint = torch.load(path, map_location=\"cpu\")\n\n    # Set model state\n    with monit.section('Load state'):\n        missing_keys, extra_keys = model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n\n    # Debugging output\n    inspect(global_step=checkpoint.get('global_step', -1), missing_keys=missing_keys, extra_keys=extra_keys,\n            _expand=True)\n\n    #\n    model.eval()\n    return model\n\n\ndef load_img(path: str):\n    \"\"\"\n    ### Load an image\n\n    This loads an image from a file and returns a PyTorch tensor.\n\n    :param path: is the path of the image\n    \"\"\"\n    # Open Image\n    image = Image.open(path).convert(\"RGB\")\n    # Get image size\n    w, h = image.size\n    # Resize to a multiple of 32\n    w = w - w % 32\n    h = h - h % 32\n    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n    # Convert to numpy and map to `[-1, 1]` for `[0, 255]`\n    image = np.array(image).astype(np.float32) * (2. / 255.0) - 1\n    # Transpose to shape `[batch_size, channels, height, width]`\n    image = image[None].transpose(0, 3, 1, 2)\n    # Convert to torch\n    return torch.from_numpy(image)\n\n\ndef save_images(images: torch.Tensor, dest_path: str, prefix: str = '', img_format: str = 'jpeg'):\n    \"\"\"\n    ### Save a images\n\n    :param images: is the tensor with images of shape `[batch_size, channels, height, width]`\n    :param dest_path: is the folder to save images in\n    :param prefix: is the prefix to add to file names\n    :param img_format: is the image format\n    \"\"\"\n\n    # Create the destination folder\n    os.makedirs(dest_path, exist_ok=True)\n\n    # Map images to `[0, 1]` space and clip\n    images = torch.clamp((images + 1.0) / 2.0, min=0.0, max=1.0)\n    # Transpose to `[batch_size, height, width, channels]` and convert to numpy\n    images = images.cpu().permute(0, 2, 3, 1).numpy()\n\n    # Save images\n    for i, img in enumerate(images):\n        img = Image.fromarray((255. * img).astype(np.uint8))\n        img.save(os.path.join(dest_path, f\"{prefix}{i:05}.{img_format}\"), format=img_format)\n", "labml_nn/diffusion/stable_diffusion/latent_diffusion.py": "\"\"\"\n---\ntitle: Latent Diffusion Models\nsummary: >\n Annotated PyTorch implementation/tutorial of latent diffusion models from paper\n High-Resolution Image Synthesis with Latent Diffusion Models\n---\n\n# Latent Diffusion Models\n\nLatent diffusion models use an auto-encoder to map between image space and\nlatent space. The diffusion model works on the latent space, which makes it\na lot easier to train.\nIt is based on paper\n[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).\n\nThey use a pre-trained auto-encoder and train the diffusion U-Net on the latent\nspace of the pre-trained auto-encoder.\n\nFor a simpler diffusion implementation refer to our [DDPM implementation](../ddpm/index.html).\nWe use same notations for $\\alpha_t$, $\\beta_t$ schedules, etc.\n\"\"\"\n\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\n\nfrom labml_nn.diffusion.stable_diffusion.model.autoencoder import Autoencoder\nfrom labml_nn.diffusion.stable_diffusion.model.clip_embedder import CLIPTextEmbedder\nfrom labml_nn.diffusion.stable_diffusion.model.unet import UNetModel\n\n\nclass DiffusionWrapper(nn.Module):\n    \"\"\"\n    *This is an empty wrapper class around the [U-Net](model/unet.html).\n    We keep this to have the same model structure as\n    [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)\n    so that we do not have to map the checkpoint weights explicitly*.\n    \"\"\"\n\n    def __init__(self, diffusion_model: UNetModel):\n        super().__init__()\n        self.diffusion_model = diffusion_model\n\n    def forward(self, x: torch.Tensor, time_steps: torch.Tensor, context: torch.Tensor):\n        return self.diffusion_model(x, time_steps, context)\n\n\nclass LatentDiffusion(nn.Module):\n    \"\"\"\n    ## Latent diffusion model\n\n    This contains following components:\n\n    * [AutoEncoder](model/autoencoder.html)\n    * [U-Net](model/unet.html) with [attention](model/unet_attention.html)\n    * [CLIP embeddings generator](model/clip_embedder.html)\n    \"\"\"\n    model: DiffusionWrapper\n    first_stage_model: Autoencoder\n    cond_stage_model: CLIPTextEmbedder\n\n    def __init__(self,\n                 unet_model: UNetModel,\n                 autoencoder: Autoencoder,\n                 clip_embedder: CLIPTextEmbedder,\n                 latent_scaling_factor: float,\n                 n_steps: int,\n                 linear_start: float,\n                 linear_end: float,\n                 ):\n        \"\"\"\n        :param unet_model: is the [U-Net](model/unet.html) that predicts noise\n         $\\epsilon_\\text{cond}(x_t, c)$, in latent space\n        :param autoencoder: is the [AutoEncoder](model/autoencoder.html)\n        :param clip_embedder: is the [CLIP embeddings generator](model/clip_embedder.html)\n        :param latent_scaling_factor: is the scaling factor for the latent space. The encodings of\n         the autoencoder are scaled by this before feeding into the U-Net.\n        :param n_steps: is the number of diffusion steps $T$.\n        :param linear_start: is the start of the $\\beta$ schedule.\n        :param linear_end: is the end of the $\\beta$ schedule.\n        \"\"\"\n        super().__init__()\n        # Wrap the [U-Net](model/unet.html) to keep the same model structure as\n        # [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion).\n        self.model = DiffusionWrapper(unet_model)\n        # Auto-encoder and scaling factor\n        self.first_stage_model = autoencoder\n        self.latent_scaling_factor = latent_scaling_factor\n        # [CLIP embeddings generator](model/clip_embedder.html)\n        self.cond_stage_model = clip_embedder\n\n        # Number of steps $T$\n        self.n_steps = n_steps\n\n        # $\\beta$ schedule\n        beta = torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_steps, dtype=torch.float64) ** 2\n        self.beta = nn.Parameter(beta.to(torch.float32), requires_grad=False)\n        # $\\alpha_t = 1 - \\beta_t$\n        alpha = 1. - beta\n        # $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$\n        alpha_bar = torch.cumprod(alpha, dim=0)\n        self.alpha_bar = nn.Parameter(alpha_bar.to(torch.float32), requires_grad=False)\n\n    @property\n    def device(self):\n        \"\"\"\n        ### Get model device\n        \"\"\"\n        return next(iter(self.model.parameters())).device\n\n    def get_text_conditioning(self, prompts: List[str]):\n        \"\"\"\n        ### Get [CLIP embeddings](model/clip_embedder.html) for a list of text prompts\n        \"\"\"\n        return self.cond_stage_model(prompts)\n\n    def autoencoder_encode(self, image: torch.Tensor):\n        \"\"\"\n        ### Get scaled latent space representation of the image\n\n        The encoder output is a distribution.\n        We sample from that and multiply by the scaling factor.\n        \"\"\"\n        return self.latent_scaling_factor * self.first_stage_model.encode(image).sample()\n\n    def autoencoder_decode(self, z: torch.Tensor):\n        \"\"\"\n        ### Get image from the latent representation\n\n        We scale down by the scaling factor and then decode.\n        \"\"\"\n        return self.first_stage_model.decode(z / self.latent_scaling_factor)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor, context: torch.Tensor):\n        \"\"\"\n        ### Predict noise\n\n        Predict noise given the latent representation $x_t$, time step $t$, and the\n        conditioning context $c$.\n\n        $$\\epsilon_\\text{cond}(x_t, c)$$\n        \"\"\"\n        return self.model(x, t, context)\n", "labml_nn/diffusion/stable_diffusion/__init__.py": "\"\"\"\n---\ntitle: Stable Diffusion\nsummary: >\n Annotated PyTorch implementation/tutorial of stable diffusion.\n---\n\n# Stable Diffusion\n\nThis is based on official stable diffusion repository\n [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion).\nWe have kept the model structure same so that open sourced weights could be directly loaded.\nOur implementation does not contain training code.\n\n### [PromptArt](https://promptart.labml.ai)\n\n![PromptArt](https://labml.ai/images/promptart-feed.webp)\n\nWe have deployed a stable diffusion based image generation service\nat [promptart.labml.ai](https://promptart.labml.ai)\n\n### [Latent Diffusion Model](latent_diffusion.html)\n\nThe core is the [Latent Diffusion Model](latent_diffusion.html).\nIt consists of:\n\n* [AutoEncoder](model/autoencoder.html)\n* [U-Net](model/unet.html) with [attention](model/unet_attention.html)\n\nWe have also (optionally) integrated [Flash Attention](https://github.com/HazyResearch/flash-attention)\ninto our [U-Net attention](model/unet_attention.html) which lets you speed up\nthe performance by close to 50% on an RTX A6000 GPU.\n\nThe diffusion is conditioned based on [CLIP embeddings](model/clip_embedder.html).\n\n### [Sampling Algorithms](sampler/index.html)\n\nWe have implemented the following [sampling algorithms](sampler/index.html):\n\n* [Denoising Diffusion Probabilistic Models (DDPM) Sampling](sampler/ddpm.html)\n* [Denoising Diffusion Implicit Models (DDIM) Sampling](sampler/ddim.html)\n\n### [Example Scripts](scripts/index.html)\n\nHere are the image generation scripts:\n\n* [Generate images from text prompts](scripts/text_to_image.html)\n* [Generate images based on a given image, guided by a prompt](scripts/image_to_image.html)\n* [Modify parts of a given image based on a text prompt](scripts/in_paint.html)\n\n#### [Utilities](util.html)\n\n[`util.py`](util.html) defines the utility functions.\n\"\"\"\n", "labml_nn/diffusion/stable_diffusion/model/clip_embedder.py": "\"\"\"\n---\ntitle: CLIP Text Embedder\nsummary: >\n CLIP embedder to get prompt embeddings for stable diffusion\n---\n\n# CLIP Text Embedder\n\nThis is used to get prompt embeddings for [stable diffusion](../index.html).\nIt uses HuggingFace Transformers CLIP model.\n\"\"\"\n\nfrom typing import List\n\nfrom torch import nn\nfrom transformers import CLIPTokenizer, CLIPTextModel\n\n\nclass CLIPTextEmbedder(nn.Module):\n    \"\"\"\n    ## CLIP Text Embedder\n    \"\"\"\n\n    def __init__(self, version: str = \"openai/clip-vit-large-patch14\", device=\"cuda:0\", max_length: int = 77):\n        \"\"\"\n        :param version: is the model version\n        :param device: is the device\n        :param max_length: is the max length of the tokenized prompt\n        \"\"\"\n        super().__init__()\n        # Load the tokenizer\n        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n        # Load the CLIP transformer\n        self.transformer = CLIPTextModel.from_pretrained(version).eval()\n\n        self.device = device\n        self.max_length = max_length\n\n    def forward(self, prompts: List[str]):\n        \"\"\"\n        :param prompts: are the list of prompts to embed\n        \"\"\"\n        # Tokenize the prompts\n        batch_encoding = self.tokenizer(prompts, truncation=True, max_length=self.max_length, return_length=True,\n                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n        # Get token ids\n        tokens = batch_encoding[\"input_ids\"].to(self.device)\n        # Get CLIP embeddings\n        return self.transformer(input_ids=tokens).last_hidden_state\n", "labml_nn/diffusion/stable_diffusion/model/unet_attention.py": "\"\"\"\n---\ntitle: Transformer for Stable Diffusion U-Net\nsummary: >\n Annotated PyTorch implementation/tutorial of the transformer\n for U-Net in stable diffusion.\n---\n\n# Transformer for Stable Diffusion [U-Net](unet.html)\n\nThis implements the transformer module used in [U-Net](unet.html) that\n gives $\\epsilon_\\text{cond}(x_t, c)$\n\nWe have kept to the model definition and naming unchanged from\n[CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)\nso that we can load the checkpoints directly.\n\"\"\"\n\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass SpatialTransformer(nn.Module):\n    \"\"\"\n    ## Spatial Transformer\n    \"\"\"\n\n    def __init__(self, channels: int, n_heads: int, n_layers: int, d_cond: int):\n        \"\"\"\n        :param channels: is the number of channels in the feature map\n        :param n_heads: is the number of attention heads\n        :param n_layers: is the number of transformer layers\n        :param d_cond: is the size of the conditional embedding\n        \"\"\"\n        super().__init__()\n        # Initial group normalization\n        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6, affine=True)\n        # Initial $1 \\times 1$ convolution\n        self.proj_in = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n\n        # Transformer layers\n        self.transformer_blocks = nn.ModuleList(\n            [BasicTransformerBlock(channels, n_heads, channels // n_heads, d_cond=d_cond) for _ in range(n_layers)]\n        )\n\n        # Final $1 \\times 1$ convolution\n        self.proj_out = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n        \"\"\"\n        :param x: is the feature map of shape `[batch_size, channels, height, width]`\n        :param cond: is the conditional embeddings of shape `[batch_size,  n_cond, d_cond]`\n        \"\"\"\n        # Get shape `[batch_size, channels, height, width]`\n        b, c, h, w = x.shape\n        # For residual connection\n        x_in = x\n        # Normalize\n        x = self.norm(x)\n        # Initial $1 \\times 1$ convolution\n        x = self.proj_in(x)\n        # Transpose and reshape from `[batch_size, channels, height, width]`\n        # to `[batch_size, height * width, channels]`\n        x = x.permute(0, 2, 3, 1).view(b, h * w, c)\n        # Apply the transformer layers\n        for block in self.transformer_blocks:\n            x = block(x, cond)\n        # Reshape and transpose from `[batch_size, height * width, channels]`\n        # to `[batch_size, channels, height, width]`\n        x = x.view(b, h, w, c).permute(0, 3, 1, 2)\n        # Final $1 \\times 1$ convolution\n        x = self.proj_out(x)\n        # Add residual\n        return x + x_in\n\n\nclass BasicTransformerBlock(nn.Module):\n    \"\"\"\n    ### Transformer Layer\n    \"\"\"\n\n    def __init__(self, d_model: int, n_heads: int, d_head: int, d_cond: int):\n        \"\"\"\n        :param d_model: is the input embedding size\n        :param n_heads: is the number of attention heads\n        :param d_head: is the size of a attention head\n        :param d_cond: is the size of the conditional embeddings\n        \"\"\"\n        super().__init__()\n        # Self-attention layer and pre-norm layer\n        self.attn1 = CrossAttention(d_model, d_model, n_heads, d_head)\n        self.norm1 = nn.LayerNorm(d_model)\n        # Cross attention layer and pre-norm layer\n        self.attn2 = CrossAttention(d_model, d_cond, n_heads, d_head)\n        self.norm2 = nn.LayerNorm(d_model)\n        # Feed-forward network and pre-norm layer\n        self.ff = FeedForward(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n\n    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n        \"\"\"\n        :param x: are the input embeddings of shape `[batch_size, height * width, d_model]`\n        :param cond: is the conditional embeddings of shape `[batch_size,  n_cond, d_cond]`\n        \"\"\"\n        # Self attention\n        x = self.attn1(self.norm1(x)) + x\n        # Cross-attention with conditioning\n        x = self.attn2(self.norm2(x), cond=cond) + x\n        # Feed-forward network\n        x = self.ff(self.norm3(x)) + x\n        #\n        return x\n\n\nclass CrossAttention(nn.Module):\n    \"\"\"\n    ### Cross Attention Layer\n\n    This falls-back to self-attention when conditional embeddings are not specified.\n    \"\"\"\n\n    use_flash_attention: bool = False\n\n    def __init__(self, d_model: int, d_cond: int, n_heads: int, d_head: int, is_inplace: bool = True):\n        \"\"\"\n        :param d_model: is the input embedding size\n        :param n_heads: is the number of attention heads\n        :param d_head: is the size of a attention head\n        :param d_cond: is the size of the conditional embeddings\n        :param is_inplace: specifies whether to perform the attention softmax computation inplace to\n            save memory\n        \"\"\"\n        super().__init__()\n\n        self.is_inplace = is_inplace\n        self.n_heads = n_heads\n        self.d_head = d_head\n\n        # Attention scaling factor\n        self.scale = d_head ** -0.5\n\n        # Query, key and value mappings\n        d_attn = d_head * n_heads\n        self.to_q = nn.Linear(d_model, d_attn, bias=False)\n        self.to_k = nn.Linear(d_cond, d_attn, bias=False)\n        self.to_v = nn.Linear(d_cond, d_attn, bias=False)\n\n        # Final linear layer\n        self.to_out = nn.Sequential(nn.Linear(d_attn, d_model))\n\n        # Setup [flash attention](https://github.com/HazyResearch/flash-attention).\n        # Flash attention is only used if it's installed\n        # and `CrossAttention.use_flash_attention` is set to `True`.\n        try:\n            # You can install flash attention by cloning their Github repo,\n            # [https://github.com/HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention)\n            # and then running `python setup.py install`\n            from flash_attn.flash_attention import FlashAttention\n            self.flash = FlashAttention()\n            # Set the scale for scaled dot-product attention.\n            self.flash.softmax_scale = self.scale\n        # Set to `None` if it's not installed\n        except ImportError:\n            self.flash = None\n\n    def forward(self, x: torch.Tensor, cond: Optional[torch.Tensor] = None):\n        \"\"\"\n        :param x: are the input embeddings of shape `[batch_size, height * width, d_model]`\n        :param cond: is the conditional embeddings of shape `[batch_size, n_cond, d_cond]`\n        \"\"\"\n\n        # If `cond` is `None` we perform self attention\n        has_cond = cond is not None\n        if not has_cond:\n            cond = x\n\n        # Get query, key and value vectors\n        q = self.to_q(x)\n        k = self.to_k(cond)\n        v = self.to_v(cond)\n\n        # Use flash attention if it's available and the head size is less than or equal to `128`\n        if CrossAttention.use_flash_attention and self.flash is not None and not has_cond and self.d_head <= 128:\n            return self.flash_attention(q, k, v)\n        # Otherwise, fallback to normal attention\n        else:\n            return self.normal_attention(q, k, v)\n\n    def flash_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n        \"\"\"\n        #### Flash Attention\n\n        :param q: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n        :param k: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n        :param v: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n        \"\"\"\n\n        # Get batch size and number of elements along sequence axis (`width * height`)\n        batch_size, seq_len, _ = q.shape\n\n        # Stack `q`, `k`, `v` vectors for flash attention, to get a single tensor of\n        # shape `[batch_size, seq_len, 3, n_heads * d_head]`\n        qkv = torch.stack((q, k, v), dim=2)\n        # Split the heads\n        qkv = qkv.view(batch_size, seq_len, 3, self.n_heads, self.d_head)\n\n        # Flash attention works for head sizes `32`, `64` and `128`, so we have to pad the heads to\n        # fit this size.\n        if self.d_head <= 32:\n            pad = 32 - self.d_head\n        elif self.d_head <= 64:\n            pad = 64 - self.d_head\n        elif self.d_head <= 128:\n            pad = 128 - self.d_head\n        else:\n            raise ValueError(f'Head size ${self.d_head} too large for Flash Attention')\n\n        # Pad the heads\n        if pad:\n            qkv = torch.cat((qkv, qkv.new_zeros(batch_size, seq_len, 3, self.n_heads, pad)), dim=-1)\n\n        # Compute attention\n        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_{key}}}\\Bigg)V$$\n        # This gives a tensor of shape `[batch_size, seq_len, n_heads, d_padded]`\n        out, _ = self.flash(qkv)\n        # Truncate the extra head size\n        out = out[:, :, :, :self.d_head]\n        # Reshape to `[batch_size, seq_len, n_heads * d_head]`\n        out = out.reshape(batch_size, seq_len, self.n_heads * self.d_head)\n\n        # Map to `[batch_size, height * width, d_model]` with a linear layer\n        return self.to_out(out)\n\n    def normal_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n        \"\"\"\n        #### Normal Attention\n        \n        :param q: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n        :param k: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n        :param v: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n        \"\"\"\n\n        # Split them to heads of shape `[batch_size, seq_len, n_heads, d_head]`\n        q = q.view(*q.shape[:2], self.n_heads, -1)\n        k = k.view(*k.shape[:2], self.n_heads, -1)\n        v = v.view(*v.shape[:2], self.n_heads, -1)\n\n        # Calculate attention $\\frac{Q K^\\top}{\\sqrt{d_{key}}}$\n        attn = torch.einsum('bihd,bjhd->bhij', q, k) * self.scale\n\n        # Compute softmax\n        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_{key}}}\\Bigg)$$\n        if self.is_inplace:\n            half = attn.shape[0] // 2\n            attn[half:] = attn[half:].softmax(dim=-1)\n            attn[:half] = attn[:half].softmax(dim=-1)\n        else:\n            attn = attn.softmax(dim=-1)\n\n        # Compute attention output\n        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_{key}}}\\Bigg)V$$\n        out = torch.einsum('bhij,bjhd->bihd', attn, v)\n        # Reshape to `[batch_size, height * width, n_heads * d_head]`\n        out = out.reshape(*out.shape[:2], -1)\n        # Map to `[batch_size, height * width, d_model]` with a linear layer\n        return self.to_out(out)\n\n\nclass FeedForward(nn.Module):\n    \"\"\"\n    ### Feed-Forward Network\n    \"\"\"\n\n    def __init__(self, d_model: int, d_mult: int = 4):\n        \"\"\"\n        :param d_model: is the input embedding size\n        :param d_mult: is multiplicative factor for the hidden layer size\n        \"\"\"\n        super().__init__()\n        self.net = nn.Sequential(\n            GeGLU(d_model, d_model * d_mult),\n            nn.Dropout(0.),\n            nn.Linear(d_model * d_mult, d_model)\n        )\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x)\n\n\nclass GeGLU(nn.Module):\n    \"\"\"\n    ### GeGLU Activation\n\n    $$\\text{GeGLU}(x) = (xW + b) * \\text{GELU}(xV + c)$$\n    \"\"\"\n\n    def __init__(self, d_in: int, d_out: int):\n        super().__init__()\n        # Combined linear projections $xW + b$ and $xV + c$\n        self.proj = nn.Linear(d_in, d_out * 2)\n\n    def forward(self, x: torch.Tensor):\n        # Get $xW + b$ and $xV + c$\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        # $\\text{GeGLU}(x) = (xW + b) * \\text{GELU}(xV + c)$\n        return x * F.gelu(gate)\n", "labml_nn/diffusion/stable_diffusion/model/__init__.py": "\"\"\"\n---\ntitle: Modules used in stable diffusion\nsummary: >\n Models and components for stable diffusion.\n---\n\n# [Stable Diffusion](../index.html) Models\n\n* [AutoEncoder](autoencoder.html)\n* [U-Net](unet.html) with [attention](unet_attention.html)\n* [CLIP embedder](clip_embedder.html).\n\"\"\"\n", "labml_nn/diffusion/stable_diffusion/model/unet.py": "\"\"\"\n---\ntitle: U-Net for Stable Diffusion\nsummary: >\n Annotated PyTorch implementation/tutorial of the U-Net in stable diffusion.\n---\n\n#  U-Net for [Stable Diffusion](../index.html)\n\nThis implements the U-Net that\n gives $\\epsilon_\\text{cond}(x_t, c)$\n\nWe have kept to the model definition and naming unchanged from\n[CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)\nso that we can load the checkpoints directly.\n\"\"\"\n\nimport math\nfrom typing import List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom labml_nn.diffusion.stable_diffusion.model.unet_attention import SpatialTransformer\n\n\nclass UNetModel(nn.Module):\n    \"\"\"\n    ## U-Net model\n    \"\"\"\n\n    def __init__(\n            self, *,\n            in_channels: int,\n            out_channels: int,\n            channels: int,\n            n_res_blocks: int,\n            attention_levels: List[int],\n            channel_multipliers: List[int],\n            n_heads: int,\n            tf_layers: int = 1,\n            d_cond: int = 768):\n        \"\"\"\n        :param in_channels: is the number of channels in the input feature map\n        :param out_channels: is the number of channels in the output feature map\n        :param channels: is the base channel count for the model\n        :param n_res_blocks: number of residual blocks at each level\n        :param attention_levels: are the levels at which attention should be performed\n        :param channel_multipliers: are the multiplicative factors for number of channels for each level\n        :param n_heads: is the number of attention heads in the transformers\n        :param tf_layers: is the number of transformer layers in the transformers\n        :param d_cond: is the size of the conditional embedding in the transformers\n        \"\"\"\n        super().__init__()\n        self.channels = channels\n\n        # Number of levels\n        levels = len(channel_multipliers)\n        # Size time embeddings\n        d_time_emb = channels * 4\n        self.time_embed = nn.Sequential(\n            nn.Linear(channels, d_time_emb),\n            nn.SiLU(),\n            nn.Linear(d_time_emb, d_time_emb),\n        )\n\n        # Input half of the U-Net\n        self.input_blocks = nn.ModuleList()\n        # Initial $3 \\times 3$ convolution that maps the input to `channels`.\n        # The blocks are wrapped in `TimestepEmbedSequential` module because\n        # different modules have different forward function signatures;\n        # for example, convolution only accepts the feature map and\n        # residual blocks accept the feature map and time embedding.\n        # `TimestepEmbedSequential` calls them accordingly.\n        self.input_blocks.append(TimestepEmbedSequential(\n            nn.Conv2d(in_channels, channels, 3, padding=1)))\n        # Number of channels at each block in the input half of U-Net\n        input_block_channels = [channels]\n        # Number of channels at each level\n        channels_list = [channels * m for m in channel_multipliers]\n        # Prepare levels\n        for i in range(levels):\n            # Add the residual blocks and attentions\n            for _ in range(n_res_blocks):\n                # Residual block maps from previous number of channels to the number of\n                # channels in the current level\n                layers = [ResBlock(channels, d_time_emb, out_channels=channels_list[i])]\n                channels = channels_list[i]\n                # Add transformer\n                if i in attention_levels:\n                    layers.append(SpatialTransformer(channels, n_heads, tf_layers, d_cond))\n                # Add them to the input half of the U-Net and keep track of the number of channels of\n                # its output\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                input_block_channels.append(channels)\n            # Down sample at all levels except last\n            if i != levels - 1:\n                self.input_blocks.append(TimestepEmbedSequential(DownSample(channels)))\n                input_block_channels.append(channels)\n\n        # The middle of the U-Net\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(channels, d_time_emb),\n            SpatialTransformer(channels, n_heads, tf_layers, d_cond),\n            ResBlock(channels, d_time_emb),\n        )\n\n        # Second half of the U-Net\n        self.output_blocks = nn.ModuleList([])\n        # Prepare levels in reverse order\n        for i in reversed(range(levels)):\n            # Add the residual blocks and attentions\n            for j in range(n_res_blocks + 1):\n                # Residual block maps from previous number of channels plus the\n                # skip connections from the input half of U-Net to the number of\n                # channels in the current level.\n                layers = [ResBlock(channels + input_block_channels.pop(), d_time_emb, out_channels=channels_list[i])]\n                channels = channels_list[i]\n                # Add transformer\n                if i in attention_levels:\n                    layers.append(SpatialTransformer(channels, n_heads, tf_layers, d_cond))\n                # Up-sample at every level after last residual block\n                # except the last one.\n                # Note that we are iterating in reverse; i.e. `i == 0` is the last.\n                if i != 0 and j == n_res_blocks:\n                    layers.append(UpSample(channels))\n                # Add to the output half of the U-Net\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n\n        # Final normalization and $3 \\times 3$ convolution\n        self.out = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            nn.Conv2d(channels, out_channels, 3, padding=1),\n        )\n\n    def time_step_embedding(self, time_steps: torch.Tensor, max_period: int = 10000):\n        \"\"\"\n        ## Create sinusoidal time step embeddings\n\n        :param time_steps: are the time steps of shape `[batch_size]`\n        :param max_period: controls the minimum frequency of the embeddings.\n        \"\"\"\n        # $\\frac{c}{2}$; half the channels are sin and the other half is cos,\n        half = self.channels // 2\n        # $\\frac{1}{10000^{\\frac{2i}{c}}}$\n        frequencies = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n        ).to(device=time_steps.device)\n        # $\\frac{t}{10000^{\\frac{2i}{c}}}$\n        args = time_steps[:, None].float() * frequencies[None]\n        # $\\cos\\Bigg(\\frac{t}{10000^{\\frac{2i}{c}}}\\Bigg)$ and $\\sin\\Bigg(\\frac{t}{10000^{\\frac{2i}{c}}}\\Bigg)$\n        return torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n\n    def forward(self, x: torch.Tensor, time_steps: torch.Tensor, cond: torch.Tensor):\n        \"\"\"\n        :param x: is the input feature map of shape `[batch_size, channels, width, height]`\n        :param time_steps: are the time steps of shape `[batch_size]`\n        :param cond: conditioning of shape `[batch_size, n_cond, d_cond]`\n        \"\"\"\n        # To store the input half outputs for skip connections\n        x_input_block = []\n\n        # Get time step embeddings\n        t_emb = self.time_step_embedding(time_steps)\n        t_emb = self.time_embed(t_emb)\n\n        # Input half of the U-Net\n        for module in self.input_blocks:\n            x = module(x, t_emb, cond)\n            x_input_block.append(x)\n        # Middle of the U-Net\n        x = self.middle_block(x, t_emb, cond)\n        # Output half of the U-Net\n        for module in self.output_blocks:\n            x = torch.cat([x, x_input_block.pop()], dim=1)\n            x = module(x, t_emb, cond)\n\n        # Final normalization and $3 \\times 3$ convolution\n        return self.out(x)\n\n\nclass TimestepEmbedSequential(nn.Sequential):\n    \"\"\"\n    ### Sequential block for modules with different inputs\n\n    This sequential module can compose of different modules such as `ResBlock`,\n    `nn.Conv` and `SpatialTransformer` and calls them with the matching signatures\n    \"\"\"\n\n    def forward(self, x, t_emb, cond=None):\n        for layer in self:\n            if isinstance(layer, ResBlock):\n                x = layer(x, t_emb)\n            elif isinstance(layer, SpatialTransformer):\n                x = layer(x, cond)\n            else:\n                x = layer(x)\n        return x\n\n\nclass UpSample(nn.Module):\n    \"\"\"\n    ### Up-sampling layer\n    \"\"\"\n\n    def __init__(self, channels: int):\n        \"\"\"\n        :param channels: is the number of channels\n        \"\"\"\n        super().__init__()\n        # $3 \\times 3$ convolution mapping\n        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Up-sample by a factor of $2$\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        # Apply convolution\n        return self.conv(x)\n\n\nclass DownSample(nn.Module):\n    \"\"\"\n    ## Down-sampling layer\n    \"\"\"\n\n    def __init__(self, channels: int):\n        \"\"\"\n        :param channels: is the number of channels\n        \"\"\"\n        super().__init__()\n        # $3 \\times 3$ convolution with stride length of $2$ to down-sample by a factor of $2$\n        self.op = nn.Conv2d(channels, channels, 3, stride=2, padding=1)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Apply convolution\n        return self.op(x)\n\n\nclass ResBlock(nn.Module):\n    \"\"\"\n    ## ResNet Block\n    \"\"\"\n\n    def __init__(self, channels: int, d_t_emb: int, *, out_channels=None):\n        \"\"\"\n        :param channels: the number of input channels\n        :param d_t_emb: the size of timestep embeddings\n        :param out_channels: is the number of out channels. defaults to `channels.\n        \"\"\"\n        super().__init__()\n        # `out_channels` not specified\n        if out_channels is None:\n            out_channels = channels\n\n        # First normalization and convolution\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            nn.Conv2d(channels, out_channels, 3, padding=1),\n        )\n\n        # Time step embeddings\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(d_t_emb, out_channels),\n        )\n        # Final convolution layer\n        self.out_layers = nn.Sequential(\n            normalization(out_channels),\n            nn.SiLU(),\n            nn.Dropout(0.),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        )\n\n        # `channels` to `out_channels` mapping layer for residual connection\n        if out_channels == channels:\n            self.skip_connection = nn.Identity()\n        else:\n            self.skip_connection = nn.Conv2d(channels, out_channels, 1)\n\n    def forward(self, x: torch.Tensor, t_emb: torch.Tensor):\n        \"\"\"\n        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n        :param t_emb: is the time step embeddings of shape `[batch_size, d_t_emb]`\n        \"\"\"\n        # Initial convolution\n        h = self.in_layers(x)\n        # Time step embeddings\n        t_emb = self.emb_layers(t_emb).type(h.dtype)\n        # Add time step embeddings\n        h = h + t_emb[:, :, None, None]\n        # Final convolution\n        h = self.out_layers(h)\n        # Add skip connection\n        return self.skip_connection(x) + h\n\n\nclass GroupNorm32(nn.GroupNorm):\n    \"\"\"\n    ### Group normalization with float32 casting\n    \"\"\"\n\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\ndef normalization(channels):\n    \"\"\"\n    ### Group normalization\n\n    This is a helper function, with fixed number of groups..\n    \"\"\"\n    return GroupNorm32(32, channels)\n\n\ndef _test_time_embeddings():\n    \"\"\"\n    Test sinusoidal time step embeddings\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(15, 5))\n    m = UNetModel(in_channels=1, out_channels=1, channels=320, n_res_blocks=1, attention_levels=[],\n                  channel_multipliers=[],\n                  n_heads=1, tf_layers=1, d_cond=1)\n    te = m.time_step_embedding(torch.arange(0, 1000))\n    plt.plot(np.arange(1000), te[:, [50, 100, 190, 260]].numpy())\n    plt.legend([\"dim %d\" % p for p in [50, 100, 190, 260]])\n    plt.title(\"Time embeddings\")\n    plt.show()\n\n\n#\nif __name__ == '__main__':\n    _test_time_embeddings()\n", "labml_nn/diffusion/stable_diffusion/model/autoencoder.py": "\"\"\"\n---\ntitle: Autoencoder for Stable Diffusion\nsummary: >\n Annotated PyTorch implementation/tutorial of the autoencoder\n for stable diffusion.\n---\n\n# Autoencoder for [Stable Diffusion](../index.html)\n\nThis implements the auto-encoder model used to map between image space and latent space.\n\nWe have kept to the model definition and naming unchanged from\n[CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)\nso that we can load the checkpoints directly.\n\"\"\"\n\nfrom typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass Autoencoder(nn.Module):\n    \"\"\"\n    ## Autoencoder\n\n    This consists of the encoder and decoder modules.\n    \"\"\"\n\n    def __init__(self, encoder: 'Encoder', decoder: 'Decoder', emb_channels: int, z_channels: int):\n        \"\"\"\n        :param encoder: is the encoder\n        :param decoder: is the decoder\n        :param emb_channels: is the number of dimensions in the quantized embedding space\n        :param z_channels: is the number of channels in the embedding space\n        \"\"\"\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        # Convolution to map from embedding space to\n        # quantized embedding space moments (mean and log variance)\n        self.quant_conv = nn.Conv2d(2 * z_channels, 2 * emb_channels, 1)\n        # Convolution to map from quantized embedding space back to\n        # embedding space\n        self.post_quant_conv = nn.Conv2d(emb_channels, z_channels, 1)\n\n    def encode(self, img: torch.Tensor) -> 'GaussianDistribution':\n        \"\"\"\n        ### Encode images to latent representation\n\n        :param img: is the image tensor with shape `[batch_size, img_channels, img_height, img_width]`\n        \"\"\"\n        # Get embeddings with shape `[batch_size, z_channels * 2, z_height, z_height]`\n        z = self.encoder(img)\n        # Get the moments in the quantized embedding space\n        moments = self.quant_conv(z)\n        # Return the distribution\n        return GaussianDistribution(moments)\n\n    def decode(self, z: torch.Tensor):\n        \"\"\"\n        ### Decode images from latent representation\n\n        :param z: is the latent representation with shape `[batch_size, emb_channels, z_height, z_height]`\n        \"\"\"\n        # Map to embedding space from the quantized representation\n        z = self.post_quant_conv(z)\n        # Decode the image of shape `[batch_size, channels, height, width]`\n        return self.decoder(z)\n\n\nclass Encoder(nn.Module):\n    \"\"\"\n    ## Encoder module\n    \"\"\"\n\n    def __init__(self, *, channels: int, channel_multipliers: List[int], n_resnet_blocks: int,\n                 in_channels: int, z_channels: int):\n        \"\"\"\n        :param channels: is the number of channels in the first convolution layer\n        :param channel_multipliers: are the multiplicative factors for the number of channels in the\n            subsequent blocks\n        :param n_resnet_blocks: is the number of resnet layers at each resolution\n        :param in_channels: is the number of channels in the image\n        :param z_channels: is the number of channels in the embedding space\n        \"\"\"\n        super().__init__()\n\n        # Number of blocks of different resolutions.\n        # The resolution is halved at the end each top level block\n        n_resolutions = len(channel_multipliers)\n\n        # Initial $3 \\times 3$ convolution layer that maps the image to `channels`\n        self.conv_in = nn.Conv2d(in_channels, channels, 3, stride=1, padding=1)\n\n        # Number of channels in each top level block\n        channels_list = [m * channels for m in [1] + channel_multipliers]\n\n        # List of top-level blocks\n        self.down = nn.ModuleList()\n        # Create top-level blocks\n        for i in range(n_resolutions):\n            # Each top level block consists of multiple ResNet Blocks and down-sampling\n            resnet_blocks = nn.ModuleList()\n            # Add ResNet Blocks\n            for _ in range(n_resnet_blocks):\n                resnet_blocks.append(ResnetBlock(channels, channels_list[i + 1]))\n                channels = channels_list[i + 1]\n            # Top-level block\n            down = nn.Module()\n            down.block = resnet_blocks\n            # Down-sampling at the end of each top level block except the last\n            if i != n_resolutions - 1:\n                down.downsample = DownSample(channels)\n            else:\n                down.downsample = nn.Identity()\n            #\n            self.down.append(down)\n\n        # Final ResNet blocks with attention\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(channels, channels)\n        self.mid.attn_1 = AttnBlock(channels)\n        self.mid.block_2 = ResnetBlock(channels, channels)\n\n        # Map to embedding space with a $3 \\times 3$ convolution\n        self.norm_out = normalization(channels)\n        self.conv_out = nn.Conv2d(channels, 2 * z_channels, 3, stride=1, padding=1)\n\n    def forward(self, img: torch.Tensor):\n        \"\"\"\n        :param img: is the image tensor with shape `[batch_size, img_channels, img_height, img_width]`\n        \"\"\"\n\n        # Map to `channels` with the initial convolution\n        x = self.conv_in(img)\n\n        # Top-level blocks\n        for down in self.down:\n            # ResNet Blocks\n            for block in down.block:\n                x = block(x)\n            # Down-sampling\n            x = down.downsample(x)\n\n        # Final ResNet blocks with attention\n        x = self.mid.block_1(x)\n        x = self.mid.attn_1(x)\n        x = self.mid.block_2(x)\n\n        # Normalize and map to embedding space\n        x = self.norm_out(x)\n        x = swish(x)\n        x = self.conv_out(x)\n\n        #\n        return x\n\n\nclass Decoder(nn.Module):\n    \"\"\"\n    ## Decoder module\n    \"\"\"\n\n    def __init__(self, *, channels: int, channel_multipliers: List[int], n_resnet_blocks: int,\n                 out_channels: int, z_channels: int):\n        \"\"\"\n        :param channels: is the number of channels in the final convolution layer\n        :param channel_multipliers: are the multiplicative factors for the number of channels in the\n            previous blocks, in reverse order\n        :param n_resnet_blocks: is the number of resnet layers at each resolution\n        :param out_channels: is the number of channels in the image\n        :param z_channels: is the number of channels in the embedding space\n        \"\"\"\n        super().__init__()\n\n        # Number of blocks of different resolutions.\n        # The resolution is halved at the end each top level block\n        num_resolutions = len(channel_multipliers)\n\n        # Number of channels in each top level block, in the reverse order\n        channels_list = [m * channels for m in channel_multipliers]\n\n        # Number of channels in the  top-level block\n        channels = channels_list[-1]\n\n        # Initial $3 \\times 3$ convolution layer that maps the embedding space to `channels`\n        self.conv_in = nn.Conv2d(z_channels, channels, 3, stride=1, padding=1)\n\n        # ResNet blocks with attention\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(channels, channels)\n        self.mid.attn_1 = AttnBlock(channels)\n        self.mid.block_2 = ResnetBlock(channels, channels)\n\n        # List of top-level blocks\n        self.up = nn.ModuleList()\n        # Create top-level blocks\n        for i in reversed(range(num_resolutions)):\n            # Each top level block consists of multiple ResNet Blocks and up-sampling\n            resnet_blocks = nn.ModuleList()\n            # Add ResNet Blocks\n            for _ in range(n_resnet_blocks + 1):\n                resnet_blocks.append(ResnetBlock(channels, channels_list[i]))\n                channels = channels_list[i]\n            # Top-level block\n            up = nn.Module()\n            up.block = resnet_blocks\n            # Up-sampling at the end of each top level block except the first\n            if i != 0:\n                up.upsample = UpSample(channels)\n            else:\n                up.upsample = nn.Identity()\n            # Prepend to be consistent with the checkpoint\n            self.up.insert(0, up)\n\n        # Map to image space with a $3 \\times 3$ convolution\n        self.norm_out = normalization(channels)\n        self.conv_out = nn.Conv2d(channels, out_channels, 3, stride=1, padding=1)\n\n    def forward(self, z: torch.Tensor):\n        \"\"\"\n        :param z: is the embedding tensor with shape `[batch_size, z_channels, z_height, z_height]`\n        \"\"\"\n\n        # Map to `channels` with the initial convolution\n        h = self.conv_in(z)\n\n        # ResNet blocks with attention\n        h = self.mid.block_1(h)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h)\n\n        # Top-level blocks\n        for up in reversed(self.up):\n            # ResNet Blocks\n            for block in up.block:\n                h = block(h)\n            # Up-sampling\n            h = up.upsample(h)\n\n        # Normalize and map to image space\n        h = self.norm_out(h)\n        h = swish(h)\n        img = self.conv_out(h)\n\n        #\n        return img\n\n\nclass GaussianDistribution:\n    \"\"\"\n    ## Gaussian Distribution\n    \"\"\"\n\n    def __init__(self, parameters: torch.Tensor):\n        \"\"\"\n        :param parameters: are the means and log of variances of the embedding of shape\n            `[batch_size, z_channels * 2, z_height, z_height]`\n        \"\"\"\n        # Split mean and log of variance\n        self.mean, log_var = torch.chunk(parameters, 2, dim=1)\n        # Clamp the log of variances\n        self.log_var = torch.clamp(log_var, -30.0, 20.0)\n        # Calculate standard deviation\n        self.std = torch.exp(0.5 * self.log_var)\n\n    def sample(self):\n        # Sample from the distribution\n        return self.mean + self.std * torch.randn_like(self.std)\n\n\nclass AttnBlock(nn.Module):\n    \"\"\"\n    ## Attention block\n    \"\"\"\n\n    def __init__(self, channels: int):\n        \"\"\"\n        :param channels: is the number of channels\n        \"\"\"\n        super().__init__()\n        # Group normalization\n        self.norm = normalization(channels)\n        # Query, key and value mappings\n        self.q = nn.Conv2d(channels, channels, 1)\n        self.k = nn.Conv2d(channels, channels, 1)\n        self.v = nn.Conv2d(channels, channels, 1)\n        # Final $1 \\times 1$ convolution layer\n        self.proj_out = nn.Conv2d(channels, channels, 1)\n        # Attention scaling factor\n        self.scale = channels ** -0.5\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: is the tensor of shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Normalize `x`\n        x_norm = self.norm(x)\n        # Get query, key and vector embeddings\n        q = self.q(x_norm)\n        k = self.k(x_norm)\n        v = self.v(x_norm)\n\n        # Reshape to query, key and vector embeedings from\n        # `[batch_size, channels, height, width]` to\n        # `[batch_size, channels, height * width]`\n        b, c, h, w = q.shape\n        q = q.view(b, c, h * w)\n        k = k.view(b, c, h * w)\n        v = v.view(b, c, h * w)\n\n        # Compute $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_{key}}}\\Bigg)$\n        attn = torch.einsum('bci,bcj->bij', q, k) * self.scale\n        attn = F.softmax(attn, dim=2)\n\n        # Compute $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_{key}}}\\Bigg)V$\n        out = torch.einsum('bij,bcj->bci', attn, v)\n\n        # Reshape back to `[batch_size, channels, height, width]`\n        out = out.view(b, c, h, w)\n        # Final $1 \\times 1$ convolution layer\n        out = self.proj_out(out)\n\n        # Add residual connection\n        return x + out\n\n\nclass UpSample(nn.Module):\n    \"\"\"\n    ## Up-sampling layer\n    \"\"\"\n    def __init__(self, channels: int):\n        \"\"\"\n        :param channels: is the number of channels\n        \"\"\"\n        super().__init__()\n        # $3 \\times 3$ convolution mapping\n        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Up-sample by a factor of $2$\n        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        # Apply convolution\n        return self.conv(x)\n\n\nclass DownSample(nn.Module):\n    \"\"\"\n    ## Down-sampling layer\n    \"\"\"\n    def __init__(self, channels: int):\n        \"\"\"\n        :param channels: is the number of channels\n        \"\"\"\n        super().__init__()\n        # $3 \\times 3$ convolution with stride length of $2$ to down-sample by a factor of $2$\n        self.conv = nn.Conv2d(channels, channels, 3, stride=2, padding=0)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Add padding\n        x = F.pad(x, (0, 1, 0, 1), mode=\"constant\", value=0)\n        # Apply convolution\n        return self.conv(x)\n\n\nclass ResnetBlock(nn.Module):\n    \"\"\"\n    ## ResNet Block\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int):\n        \"\"\"\n        :param in_channels: is the number of channels in the input\n        :param out_channels: is the number of channels in the output\n        \"\"\"\n        super().__init__()\n        # First normalization and convolution layer\n        self.norm1 = normalization(in_channels)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1)\n        # Second normalization and convolution layer\n        self.norm2 = normalization(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)\n        # `in_channels` to `out_channels` mapping layer for residual connection\n        if in_channels != out_channels:\n            self.nin_shortcut = nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0)\n        else:\n            self.nin_shortcut = nn.Identity()\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n        \"\"\"\n\n        h = x\n\n        # First normalization and convolution layer\n        h = self.norm1(h)\n        h = swish(h)\n        h = self.conv1(h)\n\n        # Second normalization and convolution layer\n        h = self.norm2(h)\n        h = swish(h)\n        h = self.conv2(h)\n\n        # Map and add residual\n        return self.nin_shortcut(x) + h\n\n\ndef swish(x: torch.Tensor):\n    \"\"\"\n    ### Swish activation\n\n    $$x \\cdot \\sigma(x)$$\n    \"\"\"\n    return x * torch.sigmoid(x)\n\n\ndef normalization(channels: int):\n    \"\"\"\n    ### Group normalization\n\n    This is a helper function, with fixed number of groups and `eps`.\n    \"\"\"\n    return nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)\n", "labml_nn/diffusion/stable_diffusion/sampler/ddim.py": "\"\"\"\n---\ntitle: Denoising Diffusion Implicit Models (DDIM) Sampling\nsummary: >\n Annotated PyTorch implementation/tutorial of\n Denoising Diffusion Implicit Models (DDIM) Sampling\n for stable diffusion model.\n---\n\n# Denoising Diffusion Implicit Models (DDIM) Sampling\n\nThis implements DDIM sampling from the paper\n[Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)\n\"\"\"\n\nfrom typing import Optional, List\n\nimport numpy as np\nimport torch\n\nfrom labml import monit\nfrom labml_nn.diffusion.stable_diffusion.latent_diffusion import LatentDiffusion\nfrom labml_nn.diffusion.stable_diffusion.sampler import DiffusionSampler\n\n\nclass DDIMSampler(DiffusionSampler):\n    \"\"\"\n    ## DDIM Sampler\n\n    This extends the [`DiffusionSampler` base class](index.html).\n\n    DDIM samples images by repeatedly removing noise by sampling step by step using,\n\n    \\begin{align}\n    x_{\\tau_{i-1}} &= \\sqrt{\\alpha_{\\tau_{i-1}}}\\Bigg(\n            \\frac{x_{\\tau_i} - \\sqrt{1 - \\alpha_{\\tau_i}}\\epsilon_\\theta(x_{\\tau_i})}{\\sqrt{\\alpha_{\\tau_i}}}\n            \\Bigg) \\\\\n            &+ \\sqrt{1 - \\alpha_{\\tau_{i- 1}} - \\sigma_{\\tau_i}^2} \\cdot \\epsilon_\\theta(x_{\\tau_i}) \\\\\n            &+ \\sigma_{\\tau_i} \\epsilon_{\\tau_i}\n    \\end{align}\n\n    where $\\epsilon_{\\tau_i}$ is random noise,\n    $\\tau$ is a subsequence of $[1,2,\\dots,T]$ of length $S$,\n    and\n    $$\\sigma_{\\tau_i} =\n    \\eta \\sqrt{\\frac{1 - \\alpha_{\\tau_{i-1}}}{1 - \\alpha_{\\tau_i}}}\n    \\sqrt{1 - \\frac{\\alpha_{\\tau_i}}{\\alpha_{\\tau_{i-1}}}}$$\n\n    Note that, $\\alpha_t$ in DDIM paper refers to ${\\color{lightgreen}\\bar\\alpha_t}$ from [DDPM](ddpm.html).\n    \"\"\"\n\n    model: LatentDiffusion\n\n    def __init__(self, model: LatentDiffusion, n_steps: int, ddim_discretize: str = \"uniform\", ddim_eta: float = 0.):\n        \"\"\"\n        :param model: is the model to predict noise $\\epsilon_\\text{cond}(x_t, c)$\n        :param n_steps: is the number of DDIM sampling steps, $S$\n        :param ddim_discretize: specifies how to extract $\\tau$ from $[1,2,\\dots,T]$.\n            It can be either `uniform` or `quad`.\n        :param ddim_eta: is $\\eta$ used to calculate $\\sigma_{\\tau_i}$. $\\eta = 0$ makes the\n            sampling process deterministic.\n        \"\"\"\n        super().__init__(model)\n        # Number of steps, $T$\n        self.n_steps = model.n_steps\n\n        # Calculate $\\tau$ to be uniformly distributed across $[1,2,\\dots,T]$\n        if ddim_discretize == 'uniform':\n            c = self.n_steps // n_steps\n            self.time_steps = np.asarray(list(range(0, self.n_steps, c))) + 1\n        # Calculate $\\tau$ to be quadratically distributed across $[1,2,\\dots,T]$\n        elif ddim_discretize == 'quad':\n            self.time_steps = ((np.linspace(0, np.sqrt(self.n_steps * .8), n_steps)) ** 2).astype(int) + 1\n        else:\n            raise NotImplementedError(ddim_discretize)\n\n        with torch.no_grad():\n            # Get ${\\color{lightgreen}\\bar\\alpha_t}$\n            alpha_bar = self.model.alpha_bar\n\n            # $\\alpha_{\\tau_i}$\n            self.ddim_alpha = alpha_bar[self.time_steps].clone().to(torch.float32)\n            # $\\sqrt{\\alpha_{\\tau_i}}$\n            self.ddim_alpha_sqrt = torch.sqrt(self.ddim_alpha)\n            # $\\alpha_{\\tau_{i-1}}$\n            self.ddim_alpha_prev = torch.cat([alpha_bar[0:1], alpha_bar[self.time_steps[:-1]]])\n\n            # $$\\sigma_{\\tau_i} =\n            # \\eta \\sqrt{\\frac{1 - \\alpha_{\\tau_{i-1}}}{1 - \\alpha_{\\tau_i}}}\n            # \\sqrt{1 - \\frac{\\alpha_{\\tau_i}}{\\alpha_{\\tau_{i-1}}}}$$\n            self.ddim_sigma = (ddim_eta *\n                               ((1 - self.ddim_alpha_prev) / (1 - self.ddim_alpha) *\n                                (1 - self.ddim_alpha / self.ddim_alpha_prev)) ** .5)\n\n            # $\\sqrt{1 - \\alpha_{\\tau_i}}$\n            self.ddim_sqrt_one_minus_alpha = (1. - self.ddim_alpha) ** .5\n\n    @torch.no_grad()\n    def sample(self,\n               shape: List[int],\n               cond: torch.Tensor,\n               repeat_noise: bool = False,\n               temperature: float = 1.,\n               x_last: Optional[torch.Tensor] = None,\n               uncond_scale: float = 1.,\n               uncond_cond: Optional[torch.Tensor] = None,\n               skip_steps: int = 0,\n               ):\n        \"\"\"\n        ### Sampling Loop\n\n        :param shape: is the shape of the generated images in the\n            form `[batch_size, channels, height, width]`\n        :param cond: is the conditional embeddings $c$\n        :param temperature: is the noise temperature (random noise gets multiplied by this)\n        :param x_last: is $x_{\\tau_S}$. If not provided random noise will be used.\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n        :param skip_steps: is the number of time steps to skip $i'$. We start sampling from $S - i'$.\n            And `x_last` is then $x_{\\tau_{S - i'}}$.\n        \"\"\"\n\n        # Get device and batch size\n        device = self.model.device\n        bs = shape[0]\n\n        # Get $x_{\\tau_S}$\n        x = x_last if x_last is not None else torch.randn(shape, device=device)\n\n        # Time steps to sample at $\\tau_{S - i'}, \\tau_{S - i' - 1}, \\dots, \\tau_1$\n        time_steps = np.flip(self.time_steps)[skip_steps:]\n\n        for i, step in monit.enum('Sample', time_steps):\n            # Index $i$ in the list $[\\tau_1, \\tau_2, \\dots, \\tau_S]$\n            index = len(time_steps) - i - 1\n            # Time step $\\tau_i$\n            ts = x.new_full((bs,), step, dtype=torch.long)\n\n            # Sample $x_{\\tau_{i-1}}$\n            x, pred_x0, e_t = self.p_sample(x, cond, ts, step, index=index,\n                                            repeat_noise=repeat_noise,\n                                            temperature=temperature,\n                                            uncond_scale=uncond_scale,\n                                            uncond_cond=uncond_cond)\n\n        # Return $x_0$\n        return x\n\n    @torch.no_grad()\n    def p_sample(self, x: torch.Tensor, c: torch.Tensor, t: torch.Tensor, step: int, index: int, *,\n                 repeat_noise: bool = False,\n                 temperature: float = 1.,\n                 uncond_scale: float = 1.,\n                 uncond_cond: Optional[torch.Tensor] = None):\n        \"\"\"\n        ### Sample $x_{\\tau_{i-1}}$\n\n        :param x: is $x_{\\tau_i}$ of shape `[batch_size, channels, height, width]`\n        :param c: is the conditional embeddings $c$ of shape `[batch_size, emb_size]`\n        :param t: is $\\tau_i$ of shape `[batch_size]`\n        :param step: is the step $\\tau_i$ as an integer\n        :param index: is index $i$ in the list $[\\tau_1, \\tau_2, \\dots, \\tau_S]$\n        :param repeat_noise: specified whether the noise should be same for all samples in the batch\n        :param temperature: is the noise temperature (random noise gets multiplied by this)\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n        \"\"\"\n\n        # Get $\\epsilon_\\theta(x_{\\tau_i})$\n        e_t = self.get_eps(x, t, c,\n                           uncond_scale=uncond_scale,\n                           uncond_cond=uncond_cond)\n\n        # Calculate $x_{\\tau_{i - 1}}$ and predicted $x_0$\n        x_prev, pred_x0 = self.get_x_prev_and_pred_x0(e_t, index, x,\n                                                      temperature=temperature,\n                                                      repeat_noise=repeat_noise)\n\n        #\n        return x_prev, pred_x0, e_t\n\n    def get_x_prev_and_pred_x0(self, e_t: torch.Tensor, index: int, x: torch.Tensor, *,\n                               temperature: float,\n                               repeat_noise: bool):\n        \"\"\"\n        ### Sample $x_{\\tau_{i-1}}$ given $\\epsilon_\\theta(x_{\\tau_i})$\n        \"\"\"\n\n        # $\\alpha_{\\tau_i}$\n        alpha = self.ddim_alpha[index]\n        # $\\alpha_{\\tau_{i-1}}$\n        alpha_prev = self.ddim_alpha_prev[index]\n        # $\\sigma_{\\tau_i}$\n        sigma = self.ddim_sigma[index]\n        # $\\sqrt{1 - \\alpha_{\\tau_i}}$\n        sqrt_one_minus_alpha = self.ddim_sqrt_one_minus_alpha[index]\n\n        # Current prediction for $x_0$,\n        # $$\\frac{x_{\\tau_i} - \\sqrt{1 - \\alpha_{\\tau_i}}\\epsilon_\\theta(x_{\\tau_i})}{\\sqrt{\\alpha_{\\tau_i}}}$$\n        pred_x0 = (x - sqrt_one_minus_alpha * e_t) / (alpha ** 0.5)\n        # Direction pointing to $x_t$\n        # $$\\sqrt{1 - \\alpha_{\\tau_{i- 1}} - \\sigma_{\\tau_i}^2} \\cdot \\epsilon_\\theta(x_{\\tau_i})$$\n        dir_xt = (1. - alpha_prev - sigma ** 2).sqrt() * e_t\n\n        # No noise is added, when $\\eta = 0$\n        if sigma == 0.:\n            noise = 0.\n        # If same noise is used for all samples in the batch\n        elif repeat_noise:\n            noise = torch.randn((1, *x.shape[1:]), device=x.device)\n            # Different noise for each sample\n        else:\n            noise = torch.randn(x.shape, device=x.device)\n\n        # Multiply noise by the temperature\n        noise = noise * temperature\n\n        #  \\begin{align}\n        #     x_{\\tau_{i-1}} &= \\sqrt{\\alpha_{\\tau_{i-1}}}\\Bigg(\n        #             \\frac{x_{\\tau_i} - \\sqrt{1 - \\alpha_{\\tau_i}}\\epsilon_\\theta(x_{\\tau_i})}{\\sqrt{\\alpha_{\\tau_i}}}\n        #             \\Bigg) \\\\\n        #             &+ \\sqrt{1 - \\alpha_{\\tau_{i- 1}} - \\sigma_{\\tau_i}^2} \\cdot \\epsilon_\\theta(x_{\\tau_i}) \\\\\n        #             &+ \\sigma_{\\tau_i} \\epsilon_{\\tau_i}\n        #  \\end{align}\n        x_prev = (alpha_prev ** 0.5) * pred_x0 + dir_xt + sigma * noise\n\n        #\n        return x_prev, pred_x0\n\n    @torch.no_grad()\n    def q_sample(self, x0: torch.Tensor, index: int, noise: Optional[torch.Tensor] = None):\n        \"\"\"\n        ### Sample from $q_{\\sigma,\\tau}(x_{\\tau_i}|x_0)$\n\n        $$q_{\\sigma,\\tau}(x_t|x_0) =\n         \\mathcal{N} \\Big(x_t; \\sqrt{\\alpha_{\\tau_i}} x_0, (1-\\alpha_{\\tau_i}) \\mathbf{I} \\Big)$$\n\n        :param x0: is $x_0$ of shape `[batch_size, channels, height, width]`\n        :param index: is the time step $\\tau_i$ index $i$\n        :param noise: is the noise, $\\epsilon$\n        \"\"\"\n\n        # Random noise, if noise is not specified\n        if noise is None:\n            noise = torch.randn_like(x0)\n\n        # Sample from\n        #  $$q_{\\sigma,\\tau}(x_t|x_0) =\n        #          \\mathcal{N} \\Big(x_t; \\sqrt{\\alpha_{\\tau_i}} x_0, (1-\\alpha_{\\tau_i}) \\mathbf{I} \\Big)$$\n        return self.ddim_alpha_sqrt[index] * x0 + self.ddim_sqrt_one_minus_alpha[index] * noise\n\n    @torch.no_grad()\n    def paint(self, x: torch.Tensor, cond: torch.Tensor, t_start: int, *,\n              orig: Optional[torch.Tensor] = None,\n              mask: Optional[torch.Tensor] = None, orig_noise: Optional[torch.Tensor] = None,\n              uncond_scale: float = 1.,\n              uncond_cond: Optional[torch.Tensor] = None,\n              ):\n        \"\"\"\n        ### Painting Loop\n\n        :param x: is $x_{S'}$ of shape `[batch_size, channels, height, width]`\n        :param cond: is the conditional embeddings $c$\n        :param t_start: is the sampling step to start from, $S'$\n        :param orig: is the original image in latent page which we are in paining.\n            If this is not provided, it'll be an image to image transformation.\n        :param mask: is the mask to keep the original image.\n        :param orig_noise: is fixed noise to be added to the original image.\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n        \"\"\"\n        # Get  batch size\n        bs = x.shape[0]\n\n        # Time steps to sample at $\\tau_{S`}, \\tau_{S' - 1}, \\dots, \\tau_1$\n        time_steps = np.flip(self.time_steps[:t_start])\n\n        for i, step in monit.enum('Paint', time_steps):\n            # Index $i$ in the list $[\\tau_1, \\tau_2, \\dots, \\tau_S]$\n            index = len(time_steps) - i - 1\n            # Time step $\\tau_i$\n            ts = x.new_full((bs,), step, dtype=torch.long)\n\n            # Sample $x_{\\tau_{i-1}}$\n            x, _, _ = self.p_sample(x, cond, ts, step, index=index,\n                                    uncond_scale=uncond_scale,\n                                    uncond_cond=uncond_cond)\n\n            # Replace the masked area with original image\n            if orig is not None:\n                # Get the $q_{\\sigma,\\tau}(x_{\\tau_i}|x_0)$ for original image in latent space\n                orig_t = self.q_sample(orig, index, noise=orig_noise)\n                # Replace the masked area\n                x = orig_t * mask + x * (1 - mask)\n\n        #\n        return x\n", "labml_nn/diffusion/stable_diffusion/sampler/__init__.py": "\"\"\"\n---\ntitle: Sampling algorithms for stable diffusion\nsummary: >\n Annotated PyTorch implementation/tutorial of\n sampling algorithms\n for stable diffusion model.\n---\n\n# Sampling algorithms for [stable diffusion](../index.html)\n\nWe have implemented the following [sampling algorithms](sampler/index.html):\n\n* [Denoising Diffusion Probabilistic Models (DDPM) Sampling](ddpm.html)\n* [Denoising Diffusion Implicit Models (DDIM) Sampling](ddim.html)\n\"\"\"\n\nfrom typing import Optional, List\n\nimport torch\n\nfrom labml_nn.diffusion.stable_diffusion.latent_diffusion import LatentDiffusion\n\n\nclass DiffusionSampler:\n    \"\"\"\n    ## Base class for sampling algorithms\n    \"\"\"\n    model: LatentDiffusion\n\n    def __init__(self, model: LatentDiffusion):\n        \"\"\"\n        :param model: is the model to predict noise $\\epsilon_\\text{cond}(x_t, c)$\n        \"\"\"\n        super().__init__()\n        # Set the model $\\epsilon_\\text{cond}(x_t, c)$\n        self.model = model\n        # Get number of steps the model was trained with $T$\n        self.n_steps = model.n_steps\n\n    def get_eps(self, x: torch.Tensor, t: torch.Tensor, c: torch.Tensor, *,\n                uncond_scale: float, uncond_cond: Optional[torch.Tensor]):\n        \"\"\"\n        ## Get $\\epsilon(x_t, c)$\n\n        :param x: is $x_t$ of shape `[batch_size, channels, height, width]`\n        :param t: is $t$ of shape `[batch_size]`\n        :param c: is the conditional embeddings $c$ of shape `[batch_size, emb_size]`\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n        \"\"\"\n        # When the scale $s = 1$\n        # $$\\epsilon_\\theta(x_t, c) = \\epsilon_\\text{cond}(x_t, c)$$\n        if uncond_cond is None or uncond_scale == 1.:\n            return self.model(x, t, c)\n\n        # Duplicate $x_t$ and $t$\n        x_in = torch.cat([x] * 2)\n        t_in = torch.cat([t] * 2)\n        # Concatenated $c$ and $c_u$\n        c_in = torch.cat([uncond_cond, c])\n        # Get $\\epsilon_\\text{cond}(x_t, c)$ and $\\epsilon_\\text{cond}(x_t, c_u)$\n        e_t_uncond, e_t_cond = self.model(x_in, t_in, c_in).chunk(2)\n        # Calculate\n        # $$\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$$\n        e_t = e_t_uncond + uncond_scale * (e_t_cond - e_t_uncond)\n\n        #\n        return e_t\n\n    def sample(self,\n               shape: List[int],\n               cond: torch.Tensor,\n               repeat_noise: bool = False,\n               temperature: float = 1.,\n               x_last: Optional[torch.Tensor] = None,\n               uncond_scale: float = 1.,\n               uncond_cond: Optional[torch.Tensor] = None,\n               skip_steps: int = 0,\n               ):\n        \"\"\"\n        ### Sampling Loop\n\n        :param shape: is the shape of the generated images in the\n            form `[batch_size, channels, height, width]`\n        :param cond: is the conditional embeddings $c$\n        :param temperature: is the noise temperature (random noise gets multiplied by this)\n        :param x_last: is $x_T$. If not provided random noise will be used.\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n        :param skip_steps: is the number of time steps to skip.\n        \"\"\"\n        raise NotImplementedError()\n\n    def paint(self, x: torch.Tensor, cond: torch.Tensor, t_start: int, *,\n              orig: Optional[torch.Tensor] = None,\n              mask: Optional[torch.Tensor] = None, orig_noise: Optional[torch.Tensor] = None,\n              uncond_scale: float = 1.,\n              uncond_cond: Optional[torch.Tensor] = None,\n              ):\n        \"\"\"\n        ### Painting Loop\n\n        :param x: is $x_{T'}$ of shape `[batch_size, channels, height, width]`\n        :param cond: is the conditional embeddings $c$\n        :param t_start: is the sampling step to start from, $T'$\n        :param orig: is the original image in latent page which we are in paining.\n        :param mask: is the mask to keep the original image.\n        :param orig_noise: is fixed noise to be added to the original image.\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n        \"\"\"\n        raise NotImplementedError()\n\n    def q_sample(self, x0: torch.Tensor, index: int, noise: Optional[torch.Tensor] = None):\n        \"\"\"\n        ### Sample from $q(x_t|x_0)$\n\n        :param x0: is $x_0$ of shape `[batch_size, channels, height, width]`\n        :param index: is the time step $t$ index\n        :param noise: is the noise, $\\epsilon$\n        \"\"\"\n        raise NotImplementedError()\n", "labml_nn/diffusion/stable_diffusion/sampler/ddpm.py": "\"\"\"\n---\ntitle: Denoising Diffusion Probabilistic Models (DDPM) Sampling\nsummary: >\n Annotated PyTorch implementation/tutorial of\n Denoising Diffusion Probabilistic Models (DDPM) Sampling\n for stable diffusion model.\n---\n\n# Denoising Diffusion Probabilistic Models (DDPM) Sampling\n\nFor a simpler DDPM implementation refer to our [DDPM implementation](../../ddpm/index.html).\nWe use same notations for $\\alpha_t$, $\\beta_t$ schedules, etc.\n\"\"\"\n\nfrom typing import Optional, List\n\nimport numpy as np\nimport torch\n\nfrom labml import monit\nfrom labml_nn.diffusion.stable_diffusion.latent_diffusion import LatentDiffusion\nfrom labml_nn.diffusion.stable_diffusion.sampler import DiffusionSampler\n\n\nclass DDPMSampler(DiffusionSampler):\n    \"\"\"\n    ## DDPM Sampler\n\n    This extends the [`DiffusionSampler` base class](index.html).\n\n    DDPM samples images by repeatedly removing noise by sampling step by step from\n    $p_\\theta(x_{t-1} | x_t)$,\n\n    \\begin{align}\n\n    p_\\theta(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1}; \\mu_\\theta(x_t, t), \\tilde\\beta_t \\mathbf{I} \\big) \\\\\n\n    \\mu_t(x_t, t) &= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}x_0\n                         + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t \\\\\n\n    \\tilde\\beta_t &= \\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t \\\\\n\n    x_0 &= \\frac{1}{\\sqrt{\\bar\\alpha_t}} x_t -  \\Big(\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}\\Big)\\epsilon_\\theta \\\\\n\n    \\end{align}\n    \"\"\"\n\n    model: LatentDiffusion\n\n    def __init__(self, model: LatentDiffusion):\n        \"\"\"\n        :param model: is the model to predict noise $\\epsilon_\\text{cond}(x_t, c)$\n        \"\"\"\n        super().__init__(model)\n\n        # Sampling steps $1, 2, \\dots, T$\n        self.time_steps = np.asarray(list(range(self.n_steps)))\n\n        with torch.no_grad():\n            # $\\bar\\alpha_t$\n            alpha_bar = self.model.alpha_bar\n            # $\\beta_t$ schedule\n            beta = self.model.beta\n            #  $\\bar\\alpha_{t-1}$\n            alpha_bar_prev = torch.cat([alpha_bar.new_tensor([1.]), alpha_bar[:-1]])\n\n            # $\\sqrt{\\bar\\alpha}$\n            self.sqrt_alpha_bar = alpha_bar ** .5\n            # $\\sqrt{1 - \\bar\\alpha}$\n            self.sqrt_1m_alpha_bar = (1. - alpha_bar) ** .5\n            # $\\frac{1}{\\sqrt{\\bar\\alpha_t}}$\n            self.sqrt_recip_alpha_bar = alpha_bar ** -.5\n            # $\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}$\n            self.sqrt_recip_m1_alpha_bar = (1 / alpha_bar - 1) ** .5\n\n            # $\\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t$\n            variance = beta * (1. - alpha_bar_prev) / (1. - alpha_bar)\n            # Clamped log of $\\tilde\\beta_t$\n            self.log_var = torch.log(torch.clamp(variance, min=1e-20))\n            # $\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}$\n            self.mean_x0_coef = beta * (alpha_bar_prev ** .5) / (1. - alpha_bar)\n            # $\\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}$\n            self.mean_xt_coef = (1. - alpha_bar_prev) * ((1 - beta) ** 0.5) / (1. - alpha_bar)\n\n    @torch.no_grad()\n    def sample(self,\n               shape: List[int],\n               cond: torch.Tensor,\n               repeat_noise: bool = False,\n               temperature: float = 1.,\n               x_last: Optional[torch.Tensor] = None,\n               uncond_scale: float = 1.,\n               uncond_cond: Optional[torch.Tensor] = None,\n               skip_steps: int = 0,\n               ):\n        \"\"\"\n        ### Sampling Loop\n\n        :param shape: is the shape of the generated images in the\n            form `[batch_size, channels, height, width]`\n        :param cond: is the conditional embeddings $c$\n        :param temperature: is the noise temperature (random noise gets multiplied by this)\n        :param x_last: is $x_T$. If not provided random noise will be used.\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n        :param skip_steps: is the number of time steps to skip $t'$. We start sampling from $T - t'$.\n            And `x_last` is then $x_{T - t'}$.\n        \"\"\"\n\n        # Get device and batch size\n        device = self.model.device\n        bs = shape[0]\n\n        # Get $x_T$\n        x = x_last if x_last is not None else torch.randn(shape, device=device)\n\n        # Time steps to sample at $T - t', T - t' - 1, \\dots, 1$\n        time_steps = np.flip(self.time_steps)[skip_steps:]\n\n        # Sampling loop\n        for step in monit.iterate('Sample', time_steps):\n            # Time step $t$\n            ts = x.new_full((bs,), step, dtype=torch.long)\n\n            # Sample $x_{t-1}$\n            x, pred_x0, e_t = self.p_sample(x, cond, ts, step,\n                                            repeat_noise=repeat_noise,\n                                            temperature=temperature,\n                                            uncond_scale=uncond_scale,\n                                            uncond_cond=uncond_cond)\n\n        # Return $x_0$\n        return x\n\n    @torch.no_grad()\n    def p_sample(self, x: torch.Tensor, c: torch.Tensor, t: torch.Tensor, step: int,\n                 repeat_noise: bool = False,\n                 temperature: float = 1.,\n                 uncond_scale: float = 1., uncond_cond: Optional[torch.Tensor] = None):\n        \"\"\"\n        ### Sample $x_{t-1}$ from $p_\\theta(x_{t-1} | x_t)$\n\n        :param x: is $x_t$ of shape `[batch_size, channels, height, width]`\n        :param c: is the conditional embeddings $c$ of shape `[batch_size, emb_size]`\n        :param t: is $t$ of shape `[batch_size]`\n        :param step: is the step $t$ as an integer\n        :repeat_noise: specified whether the noise should be same for all samples in the batch\n        :param temperature: is the noise temperature (random noise gets multiplied by this)\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n        \"\"\"\n\n        # Get $\\epsilon_\\theta$\n        e_t = self.get_eps(x, t, c,\n                           uncond_scale=uncond_scale,\n                           uncond_cond=uncond_cond)\n\n        # Get batch size\n        bs = x.shape[0]\n\n        # $\\frac{1}{\\sqrt{\\bar\\alpha_t}}$\n        sqrt_recip_alpha_bar = x.new_full((bs, 1, 1, 1), self.sqrt_recip_alpha_bar[step])\n        # $\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}$\n        sqrt_recip_m1_alpha_bar = x.new_full((bs, 1, 1, 1), self.sqrt_recip_m1_alpha_bar[step])\n\n        # Calculate $x_0$ with current $\\epsilon_\\theta$\n        #\n        # $$x_0 = \\frac{1}{\\sqrt{\\bar\\alpha_t}} x_t -  \\Big(\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}\\Big)\\epsilon_\\theta$$\n        x0 = sqrt_recip_alpha_bar * x - sqrt_recip_m1_alpha_bar * e_t\n\n        # $\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}$\n        mean_x0_coef = x.new_full((bs, 1, 1, 1), self.mean_x0_coef[step])\n        # $\\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}$\n        mean_xt_coef = x.new_full((bs, 1, 1, 1), self.mean_xt_coef[step])\n\n        # Calculate $\\mu_t(x_t, t)$\n        #\n        # $$\\mu_t(x_t, t) = \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}x_0\n        #    + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t$$\n        mean = mean_x0_coef * x0 + mean_xt_coef * x\n        # $\\log \\tilde\\beta_t$\n        log_var = x.new_full((bs, 1, 1, 1), self.log_var[step])\n\n        # Do not add noise when $t = 1$ (final step sampling process).\n        # Note that `step` is `0` when $t = 1$)\n        if step == 0:\n            noise = 0\n        # If same noise is used for all samples in the batch\n        elif repeat_noise:\n            noise = torch.randn((1, *x.shape[1:]))\n        # Different noise for each sample\n        else:\n            noise = torch.randn(x.shape)\n\n        # Multiply noise by the temperature\n        noise = noise * temperature\n\n        # Sample from,\n        #\n        # $$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}\\big(x_{t-1}; \\mu_\\theta(x_t, t), \\tilde\\beta_t \\mathbf{I} \\big)$$\n        x_prev = mean + (0.5 * log_var).exp() * noise\n\n        #\n        return x_prev, x0, e_t\n\n    @torch.no_grad()\n    def q_sample(self, x0: torch.Tensor, index: int, noise: Optional[torch.Tensor] = None):\n        \"\"\"\n        ### Sample from $q(x_t|x_0)$\n\n        $$q(x_t|x_0) = \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)$$\n\n        :param x0: is $x_0$ of shape `[batch_size, channels, height, width]`\n        :param index: is the time step $t$ index\n        :param noise: is the noise, $\\epsilon$\n        \"\"\"\n\n        # Random noise, if noise is not specified\n        if noise is None:\n            noise = torch.randn_like(x0)\n\n        # Sample from $\\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)$\n        return self.sqrt_alpha_bar[index] * x0 + self.sqrt_1m_alpha_bar[index] * noise\n", "labml_nn/diffusion/stable_diffusion/scripts/text_to_image.py": "\"\"\"\n---\ntitle: Generate images using stable diffusion with a prompt\nsummary: >\n Generate images using stable diffusion with a prompt\n---\n\n# Generate images using [stable diffusion](../index.html) with a prompt\n\"\"\"\n\nimport argparse\nimport os\nfrom pathlib import Path\n\nimport torch\n\nfrom labml import lab, monit\nfrom labml_nn.diffusion.stable_diffusion.latent_diffusion import LatentDiffusion\nfrom labml_nn.diffusion.stable_diffusion.sampler.ddim import DDIMSampler\nfrom labml_nn.diffusion.stable_diffusion.sampler.ddpm import DDPMSampler\nfrom labml_nn.diffusion.stable_diffusion.util import load_model, save_images, set_seed\n\n\nclass Txt2Img:\n    \"\"\"\n    ### Text to image class\n    \"\"\"\n    model: LatentDiffusion\n\n    def __init__(self, *,\n                 checkpoint_path: Path,\n                 sampler_name: str,\n                 n_steps: int = 50,\n                 ddim_eta: float = 0.0,\n                 ):\n        \"\"\"\n        :param checkpoint_path: is the path of the checkpoint\n        :param sampler_name: is the name of the [sampler](../sampler/index.html)\n        :param n_steps: is the number of sampling steps\n        :param ddim_eta: is the [DDIM sampling](../sampler/ddim.html) $\\eta$ constant\n        \"\"\"\n        # Load [latent diffusion model](../latent_diffusion.html)\n        self.model = load_model(checkpoint_path)\n        # Get device\n        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        # Move the model to device\n        self.model.to(self.device)\n\n        # Initialize [sampler](../sampler/index.html)\n        if sampler_name == 'ddim':\n            self.sampler = DDIMSampler(self.model,\n                                       n_steps=n_steps,\n                                       ddim_eta=ddim_eta)\n        elif sampler_name == 'ddpm':\n            self.sampler = DDPMSampler(self.model)\n\n    @torch.no_grad()\n    def __call__(self, *,\n                 dest_path: str,\n                 batch_size: int = 3,\n                 prompt: str,\n                 h: int = 512, w: int = 512,\n                 uncond_scale: float = 7.5,\n                 ):\n        \"\"\"\n        :param dest_path: is the path to store the generated images\n        :param batch_size: is the number of images to generate in a batch\n        :param prompt: is the prompt to generate images with\n        :param h: is the height of the image\n        :param w: is the width of the image\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        \"\"\"\n        # Number of channels in the image\n        c = 4\n        # Image to latent space resolution reduction\n        f = 8\n\n        # Make a batch of prompts\n        prompts = batch_size * [prompt]\n\n        # AMP auto casting\n        with torch.cuda.amp.autocast():\n            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n            if uncond_scale != 1.0:\n                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n            else:\n                un_cond = None\n            # Get the prompt embeddings\n            cond = self.model.get_text_conditioning(prompts)\n            # [Sample in the latent space](../sampler/index.html).\n            # `x` will be of shape `[batch_size, c, h / f, w / f]`\n            x = self.sampler.sample(cond=cond,\n                                    shape=[batch_size, c, h // f, w // f],\n                                    uncond_scale=uncond_scale,\n                                    uncond_cond=un_cond)\n            # Decode the image from the [autoencoder](../model/autoencoder.html)\n            images = self.model.autoencoder_decode(x)\n\n        # Save images\n        save_images(images, dest_path, 'txt_')\n\n\ndef main():\n    \"\"\"\n    ### CLI\n    \"\"\"\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        nargs=\"?\",\n        default=\"a painting of a virus monster playing guitar\",\n        help=\"the prompt to render\"\n    )\n\n    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size\")\n\n    parser.add_argument(\n        '--sampler',\n        dest='sampler_name',\n        choices=['ddim', 'ddpm'],\n        default='ddim',\n        help=f'Set the sampler.',\n    )\n\n    parser.add_argument(\"--flash\", action='store_true', help=\"whether to use flash attention\")\n\n    parser.add_argument(\"--steps\", type=int, default=50, help=\"number of sampling steps\")\n\n    parser.add_argument(\"--scale\", type=float, default=7.5,\n                        help=\"unconditional guidance scale: \"\n                             \"eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\")\n\n    opt = parser.parse_args()\n\n    set_seed(42)\n\n    # Set flash attention\n    from labml_nn.diffusion.stable_diffusion.model.unet_attention import CrossAttention\n    CrossAttention.use_flash_attention = opt.flash\n\n    #\n    txt2img = Txt2Img(checkpoint_path=lab.get_data_path() / 'stable-diffusion' / 'sd-v1-4.ckpt',\n                      sampler_name=opt.sampler_name,\n                      n_steps=opt.steps)\n\n    with monit.section('Generate'):\n        txt2img(dest_path='outputs',\n                batch_size=opt.batch_size,\n                prompt=opt.prompt,\n                uncond_scale=opt.scale)\n\n\n#\nif __name__ == \"__main__\":\n    main()\n", "labml_nn/diffusion/stable_diffusion/scripts/in_paint.py": "\"\"\"\n---\ntitle: In-paint images using stable diffusion with a prompt\nsummary: >\n In-paint images using stable diffusion with a prompt\n---\n\n# In-paint images using [stable diffusion](../index.html) with a prompt\n\"\"\"\n\nimport argparse\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch\n\nfrom labml import lab, monit\nfrom labml_nn.diffusion.stable_diffusion.latent_diffusion import LatentDiffusion\nfrom labml_nn.diffusion.stable_diffusion.sampler import DiffusionSampler\nfrom labml_nn.diffusion.stable_diffusion.sampler.ddim import DDIMSampler\nfrom labml_nn.diffusion.stable_diffusion.util import load_model, save_images, load_img, set_seed\n\n\nclass InPaint:\n    \"\"\"\n    ### Image in-painting class\n    \"\"\"\n    model: LatentDiffusion\n    sampler: DiffusionSampler\n\n    def __init__(self, *, checkpoint_path: Path,\n                 ddim_steps: int = 50,\n                 ddim_eta: float = 0.0):\n        \"\"\"\n        :param checkpoint_path: is the path of the checkpoint\n        :param ddim_steps: is the number of sampling steps\n        :param ddim_eta: is the [DDIM sampling](../sampler/ddim.html) $\\eta$ constant\n        \"\"\"\n        self.ddim_steps = ddim_steps\n\n        # Load [latent diffusion model](../latent_diffusion.html)\n        self.model = load_model(checkpoint_path)\n        # Get device\n        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        # Move the model to device\n        self.model.to(self.device)\n\n        # Initialize [DDIM sampler](../sampler/ddim.html)\n        self.sampler = DDIMSampler(self.model,\n                                   n_steps=ddim_steps,\n                                   ddim_eta=ddim_eta)\n\n    @torch.no_grad()\n    def __call__(self, *,\n                 dest_path: str,\n                 orig_img: str,\n                 strength: float,\n                 batch_size: int = 3,\n                 prompt: str,\n                 uncond_scale: float = 5.0,\n                 mask: Optional[torch.Tensor] = None,\n                 ):\n        \"\"\"\n        :param dest_path: is the path to store the generated images\n        :param orig_img: is the image to transform\n        :param strength: specifies how much of the original image should not be preserved\n        :param batch_size: is the number of images to generate in a batch\n        :param prompt: is the prompt to generate images with\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        \"\"\"\n        # Make a batch of prompts\n        prompts = batch_size * [prompt]\n        # Load image\n        orig_image = load_img(orig_img).to(self.device)\n        # Encode the image in the latent space and make `batch_size` copies of it\n        orig = self.model.autoencoder_encode(orig_image).repeat(batch_size, 1, 1, 1)\n        # If `mask` is not provided,\n        # we set a sample mask to preserve the bottom half of the image\n        if mask is None:\n            mask = torch.zeros_like(orig, device=self.device)\n            mask[:, :, mask.shape[2] // 2:, :] = 1.\n        else:\n            mask = mask.to(self.device)\n        # Noise diffuse the original image\n        orig_noise = torch.randn(orig.shape, device=self.device)\n\n        # Get the number of steps to diffuse the original\n        assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n        t_index = int(strength * self.ddim_steps)\n\n        # AMP auto casting\n        with torch.cuda.amp.autocast():\n            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n            if uncond_scale != 1.0:\n                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n            else:\n                un_cond = None\n            # Get the prompt embeddings\n            cond = self.model.get_text_conditioning(prompts)\n            # Add noise to the original image\n            x = self.sampler.q_sample(orig, t_index, noise=orig_noise)\n            # Reconstruct from the noisy image, while preserving the masked area\n            x = self.sampler.paint(x, cond, t_index,\n                                   orig=orig,\n                                   mask=mask,\n                                   orig_noise=orig_noise,\n                                   uncond_scale=uncond_scale,\n                                   uncond_cond=un_cond)\n            # Decode the image from the [autoencoder](../model/autoencoder.html)\n            images = self.model.autoencoder_decode(x)\n\n        # Save images\n        save_images(images, dest_path, 'paint_')\n\n\ndef main():\n    \"\"\"\n    ### CLI\n    \"\"\"\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        nargs=\"?\",\n        default=\"a painting of a cute monkey playing guitar\",\n        help=\"the prompt to render\"\n    )\n\n    parser.add_argument(\n        \"--orig-img\",\n        type=str,\n        nargs=\"?\",\n        help=\"path to the input image\"\n    )\n\n    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size\", )\n    parser.add_argument(\"--steps\", type=int, default=50, help=\"number of sampling steps\")\n\n    parser.add_argument(\"--scale\", type=float, default=5.0,\n                        help=\"unconditional guidance scale: \"\n                             \"eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\")\n\n    parser.add_argument(\"--strength\", type=float, default=0.75,\n                        help=\"strength for noise: \"\n                             \" 1.0 corresponds to full destruction of information in init image\")\n\n    opt = parser.parse_args()\n    set_seed(42)\n\n    in_paint = InPaint(checkpoint_path=lab.get_data_path() / 'stable-diffusion' / 'sd-v1-4.ckpt',\n                       ddim_steps=opt.steps)\n\n    with monit.section('Generate'):\n        in_paint(dest_path='outputs',\n                 orig_img=opt.orig_img,\n                 strength=opt.strength,\n                 batch_size=opt.batch_size,\n                 prompt=opt.prompt,\n                 uncond_scale=opt.scale)\n\n\n#\nif __name__ == \"__main__\":\n    main()\n", "labml_nn/diffusion/stable_diffusion/scripts/__init__.py": "\"\"\"\n---\ntitle: Scripts to show example usages stable diffusion\nsummary: >\n Annotated PyTorch implementation/tutorial of example usages of stable diffusion\n---\n\n# Scripts to show example usages [stable diffusion](../index.html)\n\n* [Prompt to image diffusion](text_to_image.html)\n* [Image to image diffusion](image_to_image.html)\n* [In-painting](in_paint.html)\n\"\"\"\n", "labml_nn/diffusion/stable_diffusion/scripts/image_to_image.py": "\"\"\"\n---\ntitle: Generate images using stable diffusion with a prompt from a given image\nsummary: >\n Generate images using stable diffusion with a prompt from a given image\n---\n\n# Generate images using [stable diffusion](../index.html) with a prompt from a given image\n\"\"\"\n\nimport argparse\nfrom pathlib import Path\n\nimport torch\n\nfrom labml import lab, monit\nfrom labml_nn.diffusion.stable_diffusion.sampler.ddim import DDIMSampler\nfrom labml_nn.diffusion.stable_diffusion.util import load_model, load_img, save_images, set_seed\n\n\nclass Img2Img:\n    \"\"\"\n    ### Image to image class\n    \"\"\"\n\n    def __init__(self, *, checkpoint_path: Path,\n                 ddim_steps: int = 50,\n                 ddim_eta: float = 0.0):\n        \"\"\"\n        :param checkpoint_path: is the path of the checkpoint\n        :param ddim_steps: is the number of sampling steps\n        :param ddim_eta: is the [DDIM sampling](../sampler/ddim.html) $\\eta$ constant\n        \"\"\"\n        self.ddim_steps = ddim_steps\n\n        # Load [latent diffusion model](../latent_diffusion.html)\n        self.model = load_model(checkpoint_path)\n        # Get device\n        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        # Move the model to device\n        self.model.to(self.device)\n\n        # Initialize [DDIM sampler](../sampler/ddim.html)\n        self.sampler = DDIMSampler(self.model,\n                                   n_steps=ddim_steps,\n                                   ddim_eta=ddim_eta)\n\n    @torch.no_grad()\n    def __call__(self, *,\n                 dest_path: str,\n                 orig_img: str,\n                 strength: float,\n                 batch_size: int = 3,\n                 prompt: str,\n                 uncond_scale: float = 5.0,\n                 ):\n        \"\"\"\n        :param dest_path: is the path to store the generated images\n        :param orig_img: is the image to transform\n        :param strength: specifies how much of the original image should not be preserved\n        :param batch_size: is the number of images to generate in a batch\n        :param prompt: is the prompt to generate images with\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        \"\"\"\n        # Make a batch of prompts\n        prompts = batch_size * [prompt]\n        # Load image\n        orig_image = load_img(orig_img).to(self.device)\n        # Encode the image in the latent space and make `batch_size` copies of it\n        orig = self.model.autoencoder_encode(orig_image).repeat(batch_size, 1, 1, 1)\n\n        # Get the number of steps to diffuse the original\n        assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n        t_index = int(strength * self.ddim_steps)\n\n        # AMP auto casting\n        with torch.cuda.amp.autocast():\n            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n            if uncond_scale != 1.0:\n                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n            else:\n                un_cond = None\n            # Get the prompt embeddings\n            cond = self.model.get_text_conditioning(prompts)\n            # Add noise to the original image\n            x = self.sampler.q_sample(orig, t_index)\n            # Reconstruct from the noisy image\n            x = self.sampler.paint(x, cond, t_index,\n                                   uncond_scale=uncond_scale,\n                                   uncond_cond=un_cond)\n            # Decode the image from the [autoencoder](../model/autoencoder.html)\n            images = self.model.autoencoder_decode(x)\n\n        # Save images\n        save_images(images, dest_path, 'img_')\n\n\ndef main():\n    \"\"\"\n    ### CLI\n    \"\"\"\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        nargs=\"?\",\n        default=\"a painting of a cute monkey playing guitar\",\n        help=\"the prompt to render\"\n    )\n\n    parser.add_argument(\n        \"--orig-img\",\n        type=str,\n        nargs=\"?\",\n        help=\"path to the input image\"\n    )\n\n    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size\", )\n    parser.add_argument(\"--steps\", type=int, default=50, help=\"number of ddim sampling steps\")\n\n    parser.add_argument(\"--scale\", type=float, default=5.0,\n                        help=\"unconditional guidance scale: \"\n                             \"eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\")\n\n    parser.add_argument(\"--strength\", type=float, default=0.75,\n                        help=\"strength for noise: \"\n                             \" 1.0 corresponds to full destruction of information in init image\")\n\n    opt = parser.parse_args()\n    set_seed(42)\n\n    img2img = Img2Img(checkpoint_path=lab.get_data_path() / 'stable-diffusion' / 'sd-v1-4.ckpt',\n                      ddim_steps=opt.steps)\n\n    with monit.section('Generate'):\n        img2img(\n            dest_path='outputs',\n            orig_img=opt.orig_img,\n            strength=opt.strength,\n            batch_size=opt.batch_size,\n            prompt=opt.prompt,\n            uncond_scale=opt.scale)\n\n\n#\nif __name__ == \"__main__\":\n    main()\n", "labml_nn/diffusion/ddpm/evaluate.py": "\"\"\"\n---\ntitle: Denoising Diffusion Probabilistic Models (DDPM) evaluation/sampling\nsummary: >\n  Code to generate samples from a trained\n  Denoising Diffusion Probabilistic Model.\n---\n\n# [Denoising Diffusion Probabilistic Models (DDPM)](index.html) evaluation/sampling\n\nThis is the code to generate images and create interpolations between given images.\n\"\"\"\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torchvision.transforms.functional import to_pil_image, resize\n\nfrom labml import experiment, monit\nfrom labml_nn.diffusion.ddpm import DenoiseDiffusion, gather\nfrom labml_nn.diffusion.ddpm.experiment import Configs\n\n\nclass Sampler:\n    \"\"\"\n    ## Sampler class\n    \"\"\"\n\n    def __init__(self, diffusion: DenoiseDiffusion, image_channels: int, image_size: int, device: torch.device):\n        \"\"\"\n        * `diffusion` is the `DenoiseDiffusion` instance\n        * `image_channels` is the number of channels in the image\n        * `image_size` is the image size\n        * `device` is the device of the model\n        \"\"\"\n        self.device = device\n        self.image_size = image_size\n        self.image_channels = image_channels\n        self.diffusion = diffusion\n\n        # $T$\n        self.n_steps = diffusion.n_steps\n        # $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n        self.eps_model = diffusion.eps_model\n        # $\\beta_t$\n        self.beta = diffusion.beta\n        # $\\alpha_t$\n        self.alpha = diffusion.alpha\n        # $\\bar\\alpha_t$\n        self.alpha_bar = diffusion.alpha_bar\n        # $\\bar\\alpha_{t-1}$\n        alpha_bar_tm1 = torch.cat([self.alpha_bar.new_ones((1,)), self.alpha_bar[:-1]])\n\n        # To calculate\n        #\n        # \\begin{align}\n        # q(x_{t-1}|x_t, x_0) &= \\mathcal{N} \\Big(x_{t-1}; \\tilde\\mu_t(x_t, x_0), \\tilde\\beta_t \\mathbf{I} \\Big) \\\\\n        # \\tilde\\mu_t(x_t, x_0) &= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}x_0\n        #                          + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t \\\\\n        # \\tilde\\beta_t &= \\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t\n        # \\end{align}\n\n        # $$\\tilde\\beta_t = \\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t$$\n        self.beta_tilde = self.beta * (1 - alpha_bar_tm1) / (1 - self.alpha_bar)\n        # $$\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}$$\n        self.mu_tilde_coef1 = self.beta * (alpha_bar_tm1 ** 0.5) / (1 - self.alpha_bar)\n        # $$\\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}$$\n        self.mu_tilde_coef2 = (self.alpha ** 0.5) * (1 - alpha_bar_tm1) / (1 - self.alpha_bar)\n        # $\\sigma^2 = \\beta$\n        self.sigma2 = self.beta\n\n    def show_image(self, img, title=\"\"):\n        \"\"\"Helper function to display an image\"\"\"\n        img = img.clip(0, 1)\n        img = img.cpu().numpy()\n        plt.imshow(img.transpose(1, 2, 0))\n        plt.title(title)\n        plt.show()\n\n    def make_video(self, frames, path=\"video.mp4\"):\n        \"\"\"Helper function to create a video\"\"\"\n        import imageio\n        # 20 second video\n        writer = imageio.get_writer(path, fps=len(frames) // 20)\n        # Add each image\n        for f in frames:\n            f = f.clip(0, 1)\n            f = to_pil_image(resize(f, [368, 368]))\n            writer.append_data(np.array(f))\n        #\n        writer.close()\n\n    def sample_animation(self, n_frames: int = 1000, create_video: bool = True):\n        \"\"\"\n        #### Sample an image step-by-step using $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n\n        We sample an image step-by-step using $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$ and at each step\n        show the estimate\n        $$x_0 \\approx \\hat{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha}}\n         \\Big( x_t - \\sqrt{1 - \\bar\\alpha_t} \\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n        \"\"\"\n\n        # $x_T \\sim p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n        xt = torch.randn([1, self.image_channels, self.image_size, self.image_size], device=self.device)\n\n        # Interval to log $\\hat{x}_0$\n        interval = self.n_steps // n_frames\n        # Frames for video\n        frames = []\n        # Sample $T$ steps\n        for t_inv in monit.iterate('Denoise', self.n_steps):\n            # $t$\n            t_ = self.n_steps - t_inv - 1\n            # $t$ in a tensor\n            t = xt.new_full((1,), t_, dtype=torch.long)\n            # $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n            eps_theta = self.eps_model(xt, t)\n            if t_ % interval == 0:\n                # Get $\\hat{x}_0$ and add to frames\n                x0 = self.p_x0(xt, t, eps_theta)\n                frames.append(x0[0])\n                if not create_video:\n                    self.show_image(x0[0], f\"{t_}\")\n            # Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n            xt = self.p_sample(xt, t, eps_theta)\n\n        # Make video\n        if create_video:\n            self.make_video(frames)\n\n    def interpolate(self, x1: torch.Tensor, x2: torch.Tensor, lambda_: float, t_: int = 100):\n        \"\"\"\n        #### Interpolate two images $x_0$ and $x'_0$\n\n        We get $x_t \\sim q(x_t|x_0)$ and $x'_t \\sim q(x'_t|x_0)$.\n\n        Then interpolate to\n         $$\\bar{x}_t = (1 - \\lambda)x_t + \\lambda x'_0$$\n\n        Then get\n         $$\\bar{x}_0 \\sim \\textcolor{lightgreen}{p_\\theta}(x_0|\\bar{x}_t)$$\n\n        * `x1` is $x_0$\n        * `x2` is $x'_0$\n        * `lambda_` is $\\lambda$\n        * `t_` is $t$\n        \"\"\"\n\n        # Number of samples\n        n_samples = x1.shape[0]\n        # $t$ tensor\n        t = torch.full((n_samples,), t_, device=self.device)\n        # $$\\bar{x}_t = (1 - \\lambda)x_t + \\lambda x'_0$$\n        xt = (1 - lambda_) * self.diffusion.q_sample(x1, t) + lambda_ * self.diffusion.q_sample(x2, t)\n\n        # $$\\bar{x}_0 \\sim \\textcolor{lightgreen}{p_\\theta}(x_0|\\bar{x}_t)$$\n        return self._sample_x0(xt, t_)\n\n    def interpolate_animate(self, x1: torch.Tensor, x2: torch.Tensor, n_frames: int = 100, t_: int = 100,\n                            create_video=True):\n        \"\"\"\n        #### Interpolate two images $x_0$ and $x'_0$ and make a video\n\n        * `x1` is $x_0$\n        * `x2` is $x'_0$\n        * `n_frames` is the number of frames for the image\n        * `t_` is $t$\n        * `create_video` specifies whether to make a video or to show each frame\n        \"\"\"\n\n        # Show original images\n        self.show_image(x1, \"x1\")\n        self.show_image(x2, \"x2\")\n        # Add batch dimension\n        x1 = x1[None, :, :, :]\n        x2 = x2[None, :, :, :]\n        # $t$ tensor\n        t = torch.full((1,), t_, device=self.device)\n        # $x_t \\sim q(x_t|x_0)$\n        x1t = self.diffusion.q_sample(x1, t)\n        # $x'_t \\sim q(x'_t|x_0)$\n        x2t = self.diffusion.q_sample(x2, t)\n\n        frames = []\n        # Get frames with different $\\lambda$\n        for i in monit.iterate('Interpolate', n_frames + 1, is_children_silent=True):\n            # $\\lambda$\n            lambda_ = i / n_frames\n            # $$\\bar{x}_t = (1 - \\lambda)x_t + \\lambda x'_0$$\n            xt = (1 - lambda_) * x1t + lambda_ * x2t\n            # $$\\bar{x}_0 \\sim \\textcolor{lightgreen}{p_\\theta}(x_0|\\bar{x}_t)$$\n            x0 = self._sample_x0(xt, t_)\n            # Add to frames\n            frames.append(x0[0])\n            # Show frame\n            if not create_video:\n                self.show_image(x0[0], f\"{lambda_ :.2f}\")\n\n        # Make video\n        if create_video:\n            self.make_video(frames)\n\n    def _sample_x0(self, xt: torch.Tensor, n_steps: int):\n        \"\"\"\n        #### Sample an image using $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n\n        * `xt` is $x_t$\n        * `n_steps` is $t$\n        \"\"\"\n\n        # Number of sampels\n        n_samples = xt.shape[0]\n        # Iterate until $t$ steps\n        for t_ in monit.iterate('Denoise', n_steps):\n            t = n_steps - t_ - 1\n            # Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n            xt = self.diffusion.p_sample(xt, xt.new_full((n_samples,), t, dtype=torch.long))\n\n        # Return $x_0$\n        return xt\n\n    def sample(self, n_samples: int = 16):\n        \"\"\"\n        #### Generate images\n        \"\"\"\n        # $x_T \\sim p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n        xt = torch.randn([n_samples, self.image_channels, self.image_size, self.image_size], device=self.device)\n\n        # $$x_0 \\sim \\textcolor{lightgreen}{p_\\theta}(x_0|x_t)$$\n        x0 = self._sample_x0(xt, self.n_steps)\n\n        # Show images\n        for i in range(n_samples):\n            self.show_image(x0[i])\n\n    def p_sample(self, xt: torch.Tensor, t: torch.Tensor, eps_theta: torch.Tensor):\n        \"\"\"\n        #### Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n\n        \\begin{align}\n        \\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\sigma_t^2 \\mathbf{I} \\big) \\\\\n        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t)\n          &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n            \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)\n        \\end{align}\n        \"\"\"\n        # [gather](utils.html) $\\bar\\alpha_t$\n        alpha_bar = gather(self.alpha_bar, t)\n        # $\\alpha_t$\n        alpha = gather(self.alpha, t)\n        # $\\frac{\\beta}{\\sqrt{1-\\bar\\alpha_t}}$\n        eps_coef = (1 - alpha) / (1 - alpha_bar) ** .5\n        # $$\\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n        #      \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n        mean = 1 / (alpha ** 0.5) * (xt - eps_coef * eps_theta)\n        # $\\sigma^2$\n        var = gather(self.sigma2, t)\n\n        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n        eps = torch.randn(xt.shape, device=xt.device)\n        # Sample\n        return mean + (var ** .5) * eps\n\n    def p_x0(self, xt: torch.Tensor, t: torch.Tensor, eps: torch.Tensor):\n        \"\"\"\n        #### Estimate $x_0$\n\n        $$x_0 \\approx \\hat{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha}}\n         \\Big( x_t - \\sqrt{1 - \\bar\\alpha_t} \\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n        \"\"\"\n        # [gather](utils.html) $\\bar\\alpha_t$\n        alpha_bar = gather(self.alpha_bar, t)\n\n        # $$x_0 \\approx \\hat{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha}}\n        #  \\Big( x_t - \\sqrt{1 - \\bar\\alpha_t} \\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n        return (xt - (1 - alpha_bar) ** 0.5 * eps) / (alpha_bar ** 0.5)\n\n\ndef main():\n    \"\"\"Generate samples\"\"\"\n\n    # Training experiment run UUID\n    run_uuid = \"a44333ea251411ec8007d1a1762ed686\"\n\n    # Start an evaluation\n    experiment.evaluate()\n\n    # Create configs\n    configs = Configs()\n    # Load custom configuration of the training run\n    configs_dict = experiment.load_configs(run_uuid)\n    # Set configurations\n    experiment.configs(configs, configs_dict)\n\n    # Initialize\n    configs.init()\n\n    # Set PyTorch modules for saving and loading\n    experiment.add_pytorch_models({'eps_model': configs.eps_model})\n\n    # Load training experiment\n    experiment.load(run_uuid)\n\n    # Create sampler\n    sampler = Sampler(diffusion=configs.diffusion,\n                      image_channels=configs.image_channels,\n                      image_size=configs.image_size,\n                      device=configs.device)\n\n    # Start evaluation\n    with experiment.start():\n        # No gradients\n        with torch.no_grad():\n            # Sample an image with an denoising animation\n            sampler.sample_animation()\n\n            if False:\n                # Get some images fro data\n                data = next(iter(configs.data_loader)).to(configs.device)\n\n                # Create an interpolation animation\n                sampler.interpolate_animate(data[0], data[1])\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/diffusion/ddpm/experiment.py": "\"\"\"\n---\ntitle: Denoising Diffusion Probabilistic Models (DDPM) training\nsummary: >\n  Training code for\n  Denoising Diffusion Probabilistic Model.\n---\n\n# [Denoising Diffusion Probabilistic Models (DDPM)](index.html) training\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/ddpm/experiment.ipynb)\n\nThis trains a DDPM based model on CelebA HQ dataset. You can find the download instruction in this\n[discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\nSave the images inside [`data/celebA` folder](#dataset_path).\n\nThe paper had used a exponential moving average of the model with a decay of $0.9999$. We have skipped this for\nsimplicity.\n\"\"\"\nfrom typing import List\n\nimport torch\nimport torch.utils.data\nimport torchvision\nfrom PIL import Image\n\nfrom labml import lab, tracker, experiment, monit\nfrom labml.configs import BaseConfigs, option\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_nn.diffusion.ddpm import DenoiseDiffusion\nfrom labml_nn.diffusion.ddpm.unet import UNet\n\n\nclass Configs(BaseConfigs):\n    \"\"\"\n    ## Configurations\n    \"\"\"\n    # Device to train the model on.\n    # [`DeviceConfigs`](https://docs.labml.ai/api/helpers.html#labml_helpers.device.DeviceConfigs)\n    #  picks up an available CUDA device or defaults to CPU.\n    device: torch.device = DeviceConfigs()\n\n    # U-Net model for $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n    eps_model: UNet\n    # [DDPM algorithm](index.html)\n    diffusion: DenoiseDiffusion\n\n    # Number of channels in the image. $3$ for RGB.\n    image_channels: int = 3\n    # Image size\n    image_size: int = 32\n    # Number of channels in the initial feature map\n    n_channels: int = 64\n    # The list of channel numbers at each resolution.\n    # The number of channels is `channel_multipliers[i] * n_channels`\n    channel_multipliers: List[int] = [1, 2, 2, 4]\n    # The list of booleans that indicate whether to use attention at each resolution\n    is_attention: List[int] = [False, False, False, True]\n\n    # Number of time steps $T$\n    n_steps: int = 1_000\n    # Batch size\n    batch_size: int = 64\n    # Number of samples to generate\n    n_samples: int = 16\n    # Learning rate\n    learning_rate: float = 2e-5\n\n    # Number of training epochs\n    epochs: int = 1_000\n\n    # Dataset\n    dataset: torch.utils.data.Dataset\n    # Dataloader\n    data_loader: torch.utils.data.DataLoader\n\n    # Adam optimizer\n    optimizer: torch.optim.Adam\n\n    def init(self):\n        # Create $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$ model\n        self.eps_model = UNet(\n            image_channels=self.image_channels,\n            n_channels=self.n_channels,\n            ch_mults=self.channel_multipliers,\n            is_attn=self.is_attention,\n        ).to(self.device)\n\n        # Create [DDPM class](index.html)\n        self.diffusion = DenoiseDiffusion(\n            eps_model=self.eps_model,\n            n_steps=self.n_steps,\n            device=self.device,\n        )\n\n        # Create dataloader\n        self.data_loader = torch.utils.data.DataLoader(self.dataset, self.batch_size, shuffle=True, pin_memory=True)\n        # Create optimizer\n        self.optimizer = torch.optim.Adam(self.eps_model.parameters(), lr=self.learning_rate)\n\n        # Image logging\n        tracker.set_image(\"sample\", True)\n\n    def sample(self):\n        \"\"\"\n        ### Sample images\n        \"\"\"\n        with torch.no_grad():\n            # $x_T \\sim p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n            x = torch.randn([self.n_samples, self.image_channels, self.image_size, self.image_size],\n                            device=self.device)\n\n            # Remove noise for $T$ steps\n            for t_ in monit.iterate('Sample', self.n_steps):\n                # $t$\n                t = self.n_steps - t_ - 1\n                # Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n                x = self.diffusion.p_sample(x, x.new_full((self.n_samples,), t, dtype=torch.long))\n\n            # Log samples\n            tracker.save('sample', x)\n\n    def train(self):\n        \"\"\"\n        ### Train\n        \"\"\"\n\n        # Iterate through the dataset\n        for data in monit.iterate('Train', self.data_loader):\n            # Increment global step\n            tracker.add_global_step()\n            # Move data to device\n            data = data.to(self.device)\n\n            # Make the gradients zero\n            self.optimizer.zero_grad()\n            # Calculate loss\n            loss = self.diffusion.loss(data)\n            # Compute gradients\n            loss.backward()\n            # Take an optimization step\n            self.optimizer.step()\n            # Track the loss\n            tracker.save('loss', loss)\n\n    def run(self):\n        \"\"\"\n        ### Training loop\n        \"\"\"\n        for _ in monit.loop(self.epochs):\n            # Train the model\n            self.train()\n            # Sample some images\n            self.sample()\n            # New line in the console\n            tracker.new_line()\n            # Save the model\n            experiment.save_checkpoint()\n\n\nclass CelebADataset(torch.utils.data.Dataset):\n    \"\"\"\n    ### CelebA HQ dataset\n    \"\"\"\n\n    def __init__(self, image_size: int):\n        super().__init__()\n\n        # CelebA images folder\n        folder = lab.get_data_path() / 'celebA'\n        # List of files\n        self._files = [p for p in folder.glob(f'**/*.jpg')]\n\n        # Transformations to resize the image and convert to tensor\n        self._transform = torchvision.transforms.Compose([\n            torchvision.transforms.Resize(image_size),\n            torchvision.transforms.ToTensor(),\n        ])\n\n    def __len__(self):\n        \"\"\"\n        Size of the dataset\n        \"\"\"\n        return len(self._files)\n\n    def __getitem__(self, index: int):\n        \"\"\"\n        Get an image\n        \"\"\"\n        img = Image.open(self._files[index])\n        return self._transform(img)\n\n\n@option(Configs.dataset, 'CelebA')\ndef celeb_dataset(c: Configs):\n    \"\"\"\n    Create CelebA dataset\n    \"\"\"\n    return CelebADataset(c.image_size)\n\n\nclass MNISTDataset(torchvision.datasets.MNIST):\n    \"\"\"\n    ### MNIST dataset\n    \"\"\"\n\n    def __init__(self, image_size):\n        transform = torchvision.transforms.Compose([\n            torchvision.transforms.Resize(image_size),\n            torchvision.transforms.ToTensor(),\n        ])\n\n        super().__init__(str(lab.get_data_path()), train=True, download=True, transform=transform)\n\n    def __getitem__(self, item):\n        return super().__getitem__(item)[0]\n\n\n@option(Configs.dataset, 'MNIST')\ndef mnist_dataset(c: Configs):\n    \"\"\"\n    Create MNIST dataset\n    \"\"\"\n    return MNISTDataset(c.image_size)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='diffuse', writers={'screen', 'labml'})\n\n    # Create configurations\n    configs = Configs()\n\n    # Set configurations. You can override the defaults by passing the values in the dictionary.\n    experiment.configs(configs, {\n        'dataset': 'CelebA',  # 'MNIST'\n        'image_channels': 3,  # 1,\n        'epochs': 100,  # 5,\n    })\n\n    # Initialize\n    configs.init()\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'eps_model': configs.eps_model})\n\n    # Start and run the training loop\n    with experiment.start():\n        configs.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/diffusion/ddpm/utils.py": "\"\"\"\n---\ntitle: Utility functions for DDPM experiment\nsummary: >\n  Utility functions for DDPM experiment\n---\n\n# Utility functions for [DDPM](index.html) experiemnt\n\"\"\"\nimport torch.utils.data\n\n\ndef gather(consts: torch.Tensor, t: torch.Tensor):\n    \"\"\"Gather consts for $t$ and reshape to feature map shape\"\"\"\n    c = consts.gather(-1, t)\n    return c.reshape(-1, 1, 1, 1)\n", "labml_nn/diffusion/ddpm/__init__.py": "\"\"\"\n---\ntitle: Denoising Diffusion Probabilistic Models (DDPM)\nsummary: >\n  PyTorch implementation and tutorial of the paper\n  Denoising Diffusion Probabilistic Models (DDPM).\n---\n\n# Denoising Diffusion Probabilistic Models (DDPM)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/ddpm/experiment.ipynb)\n\nThis is a [PyTorch](https://pytorch.org) implementation/tutorial of the paper\n[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239).\n\nIn simple terms, we get an image from data and add noise step by step.\nThen We train a model to predict that noise at each step and use the model to\ngenerate images.\n\nThe following definitions and derivations show how this works.\nFor details please refer to [the paper](https://arxiv.org/abs/2006.11239).\n\n## Forward Process\n\nThe forward process adds noise to the data $x_0 \\sim q(x_0)$, for $T$ timesteps.\n\n\\begin{align}\nq(x_t | x_{t-1}) = \\mathcal{N}\\big(x_t; \\sqrt{1-  \\beta_t} x_{t-1}, \\beta_t \\mathbf{I}\\big) \\\\\nq(x_{1:T} | x_0) = \\prod_{t = 1}^{T} q(x_t | x_{t-1})\n\\end{align}\n\nwhere $\\beta_1, \\dots, \\beta_T$ is the variance schedule.\n\nWe can sample $x_t$ at any timestep $t$ with,\n\n\\begin{align}\nq(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n\\end{align}\n\nwhere $\\alpha_t = 1 - \\beta_t$ and $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$\n\n## Reverse Process\n\nThe reverse process removes noise starting at $p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\nfor $T$ time steps.\n\n\\begin{align}\n\\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\textcolor{lightgreen}{\\Sigma_\\theta}(x_t, t)\\big) \\\\\n\\textcolor{lightgreen}{p_\\theta}(x_{0:T}) &= \\textcolor{lightgreen}{p_\\theta}(x_T) \\prod_{t = 1}^{T} \\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) \\\\\n\\textcolor{lightgreen}{p_\\theta}(x_0) &= \\int \\textcolor{lightgreen}{p_\\theta}(x_{0:T}) dx_{1:T}\n\\end{align}\n\n$\\textcolor{lightgreen}\\theta$ are the parameters we train.\n\n## Loss\n\nWe optimize the ELBO (from Jenson's inequality) on the negative log likelihood.\n\n\\begin{align}\n\\mathbb{E}[-\\log \\textcolor{lightgreen}{p_\\theta}(x_0)]\n &\\le \\mathbb{E}_q [ -\\log \\frac{\\textcolor{lightgreen}{p_\\theta}(x_{0:T})}{q(x_{1:T}|x_0)} ] \\\\\n &=L\n\\end{align}\n\nThe loss can be rewritten as  follows.\n\n\\begin{align}\nL\n &= \\mathbb{E}_q [ -\\log \\frac{\\textcolor{lightgreen}{p_\\theta}(x_{0:T})}{q(x_{1:T}|x_0)} ] \\\\\n &= \\mathbb{E}_q [ -\\log p(x_T) - \\sum_{t=1}^T \\log \\frac{\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1})} ] \\\\\n &= \\mathbb{E}_q [\n  -\\log \\frac{p(x_T)}{q(x_T|x_0)}\n  -\\sum_{t=2}^T \\log \\frac{\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)}{q(x_{t-1}|x_t,x_0)}\n  -\\log \\textcolor{lightgreen}{p_\\theta}(x_0|x_1)] \\\\\n &= \\mathbb{E}_q [\n   D_{KL}(q(x_T|x_0) \\Vert p(x_T))\n  +\\sum_{t=2}^T D_{KL}(q(x_{t-1}|x_t,x_0) \\Vert \\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t))\n  -\\log \\textcolor{lightgreen}{p_\\theta}(x_0|x_1)]\n\\end{align}\n\n$D_{KL}(q(x_T|x_0) \\Vert p(x_T))$ is constant since we keep $\\beta_1, \\dots, \\beta_T$ constant.\n\n### Computing $L_{t-1} = D_{KL}(q(x_{t-1}|x_t,x_0) \\Vert \\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t))$\n\nThe forward process posterior conditioned by $x_0$ is,\n\n\\begin{align}\nq(x_{t-1}|x_t, x_0) &= \\mathcal{N} \\Big(x_{t-1}; \\tilde\\mu_t(x_t, x_0), \\tilde\\beta_t \\mathbf{I} \\Big) \\\\\n\\tilde\\mu_t(x_t, x_0) &= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}x_0\n                         + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t \\\\\n\\tilde\\beta_t &= \\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t\n\\end{align}\n\nThe paper sets $\\textcolor{lightgreen}{\\Sigma_\\theta}(x_t, t) = \\sigma_t^2 \\mathbf{I}$ where $\\sigma_t^2$ is set to constants\n$\\beta_t$ or $\\tilde\\beta_t$.\n\nThen,\n$$\\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) = \\mathcal{N}\\big(x_{t-1}; \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\sigma_t^2 \\mathbf{I} \\big)$$\n\nFor given noise $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ using $q(x_t|x_0)$\n\n\\begin{align}\nx_t(x_0, \\epsilon) &= \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon \\\\\nx_0 &= \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\Big(x_t(x_0, \\epsilon) -  \\sqrt{1-\\bar\\alpha_t}\\epsilon\\Big)\n\\end{align}\n\nThis gives,\n\n\\begin{align}\nL_{t-1}\n &= D_{KL}(q(x_{t-1}|x_t,x_0) \\Vert \\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)) \\\\\n &= \\mathbb{E}_q \\Bigg[ \\frac{1}{2\\sigma_t^2}\n \\Big \\Vert \\tilde\\mu(x_t, x_0) - \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t) \\Big \\Vert^2 \\Bigg] \\\\\n &= \\mathbb{E}_{x_0, \\epsilon} \\Bigg[ \\frac{1}{2\\sigma_t^2}\n  \\bigg\\Vert \\frac{1}{\\sqrt{\\alpha_t}} \\Big(\n  x_t(x_0, \\epsilon) - \\frac{\\beta_t}{\\sqrt{1 - \\bar\\alpha_t}} \\epsilon\n  \\Big) - \\textcolor{lightgreen}{\\mu_\\theta}(x_t(x_0, \\epsilon), t) \\bigg\\Vert^2 \\Bigg] \\\\\n\\end{align}\n\nRe-parameterizing with a model to predict noise\n\n\\begin{align}\n\\textcolor{lightgreen}{\\mu_\\theta}(x_t, t) &= \\tilde\\mu \\bigg(x_t,\n  \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\Big(x_t -\n   \\sqrt{1-\\bar\\alpha_t}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big) \\bigg) \\\\\n  &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n  \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)\n\\end{align}\n\nwhere $\\epsilon_\\theta$ is a learned function that predicts $\\epsilon$ given $(x_t, t)$.\n\nThis gives,\n\n\\begin{align}\nL_{t-1}\n&= \\mathbb{E}_{x_0, \\epsilon} \\Bigg[ \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t (1 - \\bar\\alpha_t)}\n  \\Big\\Vert\n  \\epsilon - \\textcolor{lightgreen}{\\epsilon_\\theta}(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)\n  \\Big\\Vert^2 \\Bigg]\n\\end{align}\n\nThat is, we are training to predict the noise.\n\n### Simplified loss\n\n$$L_{\\text{simple}}(\\theta) = \\mathbb{E}_{t,x_0, \\epsilon} \\Bigg[ \\bigg\\Vert\n\\epsilon - \\textcolor{lightgreen}{\\epsilon_\\theta}(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)\n\\bigg\\Vert^2 \\Bigg]$$\n\nThis minimizes $-\\log \\textcolor{lightgreen}{p_\\theta}(x_0|x_1)$ when $t=1$ and $L_{t-1}$ for $t\\gt1$ discarding the\nweighting in $L_{t-1}$. Discarding the weights $\\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t (1 - \\bar\\alpha_t)}$\nincrease the weight given to higher $t$ (which have higher noise levels), therefore increasing the sample quality.\n\nThis file implements the loss calculation and a basic sampling method that we use to generate images during\ntraining.\n\nHere is the [UNet model](unet.html) that gives $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$ and\n[training code](experiment.html).\n[This file](evaluate.html) can generate samples and interpolations from a trained model.\n\"\"\"\nfrom typing import Tuple, Optional\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom torch import nn\n\nfrom labml_nn.diffusion.ddpm.utils import gather\n\n\nclass DenoiseDiffusion:\n    \"\"\"\n    ## Denoise Diffusion\n    \"\"\"\n\n    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n        \"\"\"\n        * `eps_model` is $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$ model\n        * `n_steps` is $t$\n        * `device` is the device to place constants on\n        \"\"\"\n        super().__init__()\n        self.eps_model = eps_model\n\n        # Create $\\beta_1, \\dots, \\beta_T$ linearly increasing variance schedule\n        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(device)\n\n        # $\\alpha_t = 1 - \\beta_t$\n        self.alpha = 1. - self.beta\n        # $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$\n        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n        # $T$\n        self.n_steps = n_steps\n        # $\\sigma^2 = \\beta$\n        self.sigma2 = self.beta\n\n    def q_xt_x0(self, x0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        #### Get $q(x_t|x_0)$ distribution\n\n        \\begin{align}\n        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n        \\end{align}\n        \"\"\"\n\n        # [gather](utils.html) $\\alpha_t$ and compute $\\sqrt{\\bar\\alpha_t} x_0$\n        mean = gather(self.alpha_bar, t) ** 0.5 * x0\n        # $(1-\\bar\\alpha_t) \\mathbf{I}$\n        var = 1 - gather(self.alpha_bar, t)\n        #\n        return mean, var\n\n    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor] = None):\n        \"\"\"\n        #### Sample from $q(x_t|x_0)$\n\n        \\begin{align}\n        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n        \\end{align}\n        \"\"\"\n\n        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n        if eps is None:\n            eps = torch.randn_like(x0)\n\n        # get $q(x_t|x_0)$\n        mean, var = self.q_xt_x0(x0, t)\n        # Sample from $q(x_t|x_0)$\n        return mean + (var ** 0.5) * eps\n\n    def p_sample(self, xt: torch.Tensor, t: torch.Tensor):\n        \"\"\"\n        #### Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n\n        \\begin{align}\n        \\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\sigma_t^2 \\mathbf{I} \\big) \\\\\n        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t)\n          &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n            \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)\n        \\end{align}\n        \"\"\"\n\n        # $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n        eps_theta = self.eps_model(xt, t)\n        # [gather](utils.html) $\\bar\\alpha_t$\n        alpha_bar = gather(self.alpha_bar, t)\n        # $\\alpha_t$\n        alpha = gather(self.alpha, t)\n        # $\\frac{\\beta}{\\sqrt{1-\\bar\\alpha_t}}$\n        eps_coef = (1 - alpha) / (1 - alpha_bar) ** .5\n        # $$\\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n        #      \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n        mean = 1 / (alpha ** 0.5) * (xt - eps_coef * eps_theta)\n        # $\\sigma^2$\n        var = gather(self.sigma2, t)\n\n        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n        eps = torch.randn(xt.shape, device=xt.device)\n        # Sample\n        return mean + (var ** .5) * eps\n\n    def loss(self, x0: torch.Tensor, noise: Optional[torch.Tensor] = None):\n        \"\"\"\n        #### Simplified Loss\n\n        $$L_{\\text{simple}}(\\theta) = \\mathbb{E}_{t,x_0, \\epsilon} \\Bigg[ \\bigg\\Vert\n        \\epsilon - \\textcolor{lightgreen}{\\epsilon_\\theta}(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)\n        \\bigg\\Vert^2 \\Bigg]$$\n        \"\"\"\n        # Get batch size\n        batch_size = x0.shape[0]\n        # Get random $t$ for each sample in the batch\n        t = torch.randint(0, self.n_steps, (batch_size,), device=x0.device, dtype=torch.long)\n\n        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n        if noise is None:\n            noise = torch.randn_like(x0)\n\n        # Sample $x_t$ for $q(x_t|x_0)$\n        xt = self.q_sample(x0, t, eps=noise)\n        # Get $\\textcolor{lightgreen}{\\epsilon_\\theta}(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)$\n        eps_theta = self.eps_model(xt, t)\n\n        # MSE loss\n        return F.mse_loss(noise, eps_theta)\n", "labml_nn/diffusion/ddpm/unet.py": "\"\"\"\n---\ntitle: U-Net model for Denoising Diffusion Probabilistic Models (DDPM)\nsummary: >\n  UNet model for Denoising Diffusion Probabilistic Models (DDPM)\n---\n\n# U-Net model for [Denoising Diffusion Probabilistic Models (DDPM)](index.html)\n\nThis is a [U-Net](../../unet/index.html) based model to predict noise\n$\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$.\n\nU-Net is a gets it's name from the U shape in the model diagram.\nIt processes a given image by progressively lowering (halving) the feature map resolution and then\nincreasing the resolution.\nThere are pass-through connection at each resolution.\n\n![U-Net diagram from paper](../../unet/unet.png)\n\nThis implementation contains a bunch of modifications to original U-Net (residual blocks, multi-head attention)\n and also adds time-step embeddings $t$.\n\"\"\"\n\nimport math\nfrom typing import Optional, Tuple, Union, List\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass Swish(Module):\n    \"\"\"\n    ### Swish activation function\n\n    $$x \\cdot \\sigma(x)$$\n    \"\"\"\n\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\nclass TimeEmbedding(nn.Module):\n    \"\"\"\n    ### Embeddings for $t$\n    \"\"\"\n\n    def __init__(self, n_channels: int):\n        \"\"\"\n        * `n_channels` is the number of dimensions in the embedding\n        \"\"\"\n        super().__init__()\n        self.n_channels = n_channels\n        # First linear layer\n        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n        # Activation\n        self.act = Swish()\n        # Second linear layer\n        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n\n    def forward(self, t: torch.Tensor):\n        # Create sinusoidal position embeddings\n        # [same as those from the transformer](../../transformers/positional_encoding.html)\n        #\n        # \\begin{align}\n        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n        # \\end{align}\n        #\n        # where $d$ is `half_dim`\n        half_dim = self.n_channels // 8\n        emb = math.log(10_000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n        emb = t[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n\n        # Transform with the MLP\n        emb = self.act(self.lin1(emb))\n        emb = self.lin2(emb)\n\n        #\n        return emb\n\n\nclass ResidualBlock(Module):\n    \"\"\"\n    ### Residual block\n\n    A residual block has two convolution layers with group normalization.\n    Each resolution is processed with two residual blocks.\n    \"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, time_channels: int,\n                 n_groups: int = 32, dropout: float = 0.1):\n        \"\"\"\n        * `in_channels` is the number of input channels\n        * `out_channels` is the number of input channels\n        * `time_channels` is the number channels in the time step ($t$) embeddings\n        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n        * `dropout` is the dropout rate\n        \"\"\"\n        super().__init__()\n        # Group normalization and the first convolution layer\n        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n        self.act1 = Swish()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n\n        # Group normalization and the second convolution layer\n        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n        self.act2 = Swish()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n\n        # If the number of input channels is not equal to the number of output channels we have to\n        # project the shortcut connection\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n        else:\n            self.shortcut = nn.Identity()\n\n        # Linear layer for time embeddings\n        self.time_emb = nn.Linear(time_channels, out_channels)\n        self.time_act = Swish()\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        \"\"\"\n        * `x` has shape `[batch_size, in_channels, height, width]`\n        * `t` has shape `[batch_size, time_channels]`\n        \"\"\"\n        # First convolution layer\n        h = self.conv1(self.act1(self.norm1(x)))\n        # Add time embeddings\n        h += self.time_emb(self.time_act(t))[:, :, None, None]\n        # Second convolution layer\n        h = self.conv2(self.dropout(self.act2(self.norm2(h))))\n\n        # Add the shortcut connection and return\n        return h + self.shortcut(x)\n\n\nclass AttentionBlock(Module):\n    \"\"\"\n    ### Attention block\n\n    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n    \"\"\"\n\n    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n        \"\"\"\n        * `n_channels` is the number of channels in the input\n        * `n_heads` is the number of heads in multi-head attention\n        * `d_k` is the number of dimensions in each head\n        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n        \"\"\"\n        super().__init__()\n\n        # Default `d_k`\n        if d_k is None:\n            d_k = n_channels\n        # Normalization layer\n        self.norm = nn.GroupNorm(n_groups, n_channels)\n        # Projections for query, key and values\n        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n        # Linear layer for final transformation\n        self.output = nn.Linear(n_heads * d_k, n_channels)\n        # Scale for dot-product attention\n        self.scale = d_k ** -0.5\n        #\n        self.n_heads = n_heads\n        self.d_k = d_k\n\n    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n        \"\"\"\n        * `x` has shape `[batch_size, in_channels, height, width]`\n        * `t` has shape `[batch_size, time_channels]`\n        \"\"\"\n        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n        # to match with `ResidualBlock`.\n        _ = t\n        # Get shape\n        batch_size, n_channels, height, width = x.shape\n        # Change `x` to shape `[batch_size, seq, n_channels]`\n        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n        q, k, v = torch.chunk(qkv, 3, dim=-1)\n        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n        attn = attn.softmax(dim=2)\n        # Multiply by values\n        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n        # Reshape to `[batch_size, seq, n_heads * d_k]`\n        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n        # Transform to `[batch_size, seq, n_channels]`\n        res = self.output(res)\n\n        # Add skip connection\n        res += x\n\n        # Change to shape `[batch_size, in_channels, height, width]`\n        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n\n        #\n        return res\n\n\nclass DownBlock(Module):\n    \"\"\"\n    ### Down block\n\n    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n    \"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n        super().__init__()\n        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.res(x, t)\n        x = self.attn(x)\n        return x\n\n\nclass UpBlock(Module):\n    \"\"\"\n    ### Up block\n\n    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n    \"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n        super().__init__()\n        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n        # from the first half of the U-Net\n        self.res = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.res(x, t)\n        x = self.attn(x)\n        return x\n\n\nclass MiddleBlock(Module):\n    \"\"\"\n    ### Middle block\n\n    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n    This block is applied at the lowest resolution of the U-Net.\n    \"\"\"\n\n    def __init__(self, n_channels: int, time_channels: int):\n        super().__init__()\n        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n        self.attn = AttentionBlock(n_channels)\n        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.res1(x, t)\n        x = self.attn(x)\n        x = self.res2(x, t)\n        return x\n\n\nclass Upsample(nn.Module):\n    \"\"\"\n    ### Scale up the feature map by $2 \\times$\n    \"\"\"\n\n    def __init__(self, n_channels):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n        # to match with `ResidualBlock`.\n        _ = t\n        return self.conv(x)\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    ### Scale down the feature map by $\\frac{1}{2} \\times$\n    \"\"\"\n\n    def __init__(self, n_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n        # to match with `ResidualBlock`.\n        _ = t\n        return self.conv(x)\n\n\nclass UNet(Module):\n    \"\"\"\n    ## U-Net\n    \"\"\"\n\n    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n                 is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, True, True),\n                 n_blocks: int = 2):\n        \"\"\"\n        * `image_channels` is the number of channels in the image. $3$ for RGB.\n        * `n_channels` is number of channels in the initial feature map that we transform the image into\n        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n        \"\"\"\n        super().__init__()\n\n        # Number of resolutions\n        n_resolutions = len(ch_mults)\n\n        # Project image into feature map\n        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n\n        # Time embedding layer. Time embedding has `n_channels * 4` channels\n        self.time_emb = TimeEmbedding(n_channels * 4)\n\n        # #### First half of U-Net - decreasing resolution\n        down = []\n        # Number of channels\n        out_channels = in_channels = n_channels\n        # For each resolution\n        for i in range(n_resolutions):\n            # Number of output channels at this resolution\n            out_channels = in_channels * ch_mults[i]\n            # Add `n_blocks`\n            for _ in range(n_blocks):\n                down.append(DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n                in_channels = out_channels\n            # Down sample at all resolutions except the last\n            if i < n_resolutions - 1:\n                down.append(Downsample(in_channels))\n\n        # Combine the set of modules\n        self.down = nn.ModuleList(down)\n\n        # Middle block\n        self.middle = MiddleBlock(out_channels, n_channels * 4, )\n\n        # #### Second half of U-Net - increasing resolution\n        up = []\n        # Number of channels\n        in_channels = out_channels\n        # For each resolution\n        for i in reversed(range(n_resolutions)):\n            # `n_blocks` at the same resolution\n            out_channels = in_channels\n            for _ in range(n_blocks):\n                up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n            # Final block to reduce the number of channels\n            out_channels = in_channels // ch_mults[i]\n            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n            in_channels = out_channels\n            # Up sample at all resolutions except last\n            if i > 0:\n                up.append(Upsample(in_channels))\n\n        # Combine the set of modules\n        self.up = nn.ModuleList(up)\n\n        # Final normalization and convolution layer\n        self.norm = nn.GroupNorm(8, n_channels)\n        self.act = Swish()\n        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        \"\"\"\n        * `x` has shape `[batch_size, in_channels, height, width]`\n        * `t` has shape `[batch_size]`\n        \"\"\"\n\n        # Get time-step embeddings\n        t = self.time_emb(t)\n\n        # Get image projection\n        x = self.image_proj(x)\n\n        # `h` will store outputs at each resolution for skip connection\n        h = [x]\n        # First half of U-Net\n        for m in self.down:\n            x = m(x, t)\n            h.append(x)\n\n        # Middle (bottom)\n        x = self.middle(x, t)\n\n        # Second half of U-Net\n        for m in self.up:\n            if isinstance(m, Upsample):\n                x = m(x, t)\n            else:\n                # Get the skip connection from first half of U-Net and concatenate\n                s = h.pop()\n                x = torch.cat((x, s), dim=1)\n                #\n                x = m(x, t)\n\n        # Final normalization and convolution\n        return self.final(self.act(self.norm(x)))\n", "labml_nn/conv_mixer/experiment.py": "\"\"\"\n---\ntitle: Train ConvMixer on CIFAR 10\nsummary: >\n  Train ConvMixer on CIFAR 10\n---\n\n#  Train a [ConvMixer](index.html) on CIFAR 10\n\nThis script trains a ConvMixer on CIFAR 10 dataset.\n\nThis is not an attempt to reproduce the results of the paper.\nThe paper uses  image augmentations\npresent in [PyTorch Image Models (timm)](https://github.com/rwightman/pytorch-image-models)\nfor training. We haven't done this for simplicity - which causes our validation accuracy to drop.\n\"\"\"\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.experiments.cifar10 import CIFAR10Configs\n\n\nclass Configs(CIFAR10Configs):\n    \"\"\"\n    ## Configurations\n\n    We use [`CIFAR10Configs`](../experiments/cifar10.html) which defines all the\n    dataset related configurations, optimizer, and a training loop.\n    \"\"\"\n\n    # Size of a patch, $p$\n    patch_size: int = 2\n    # Number of channels in patch embeddings, $h$\n    d_model: int = 256\n    # Number of [ConvMixer layers](#ConvMixerLayer) or depth, $d$\n    n_layers: int = 8\n    # Kernel size of the depth-wise convolution, $k$\n    kernel_size: int = 7\n    # Number of classes in the task\n    n_classes: int = 10\n\n\n@option(Configs.model)\ndef _conv_mixer(c: Configs):\n    \"\"\"\n    ### Create model\n    \"\"\"\n    from labml_nn.conv_mixer import ConvMixerLayer, ConvMixer, ClassificationHead, PatchEmbeddings\n\n    # Create ConvMixer\n    return ConvMixer(ConvMixerLayer(c.d_model, c.kernel_size), c.n_layers,\n                     PatchEmbeddings(c.d_model, c.patch_size, 3),\n                     ClassificationHead(c.d_model, c.n_classes)).to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='ConvMixer', comment='cifar10')\n    # Create configurations\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf, {\n        # Optimizer\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n\n        # Training epochs and batch size\n        'epochs': 150,\n        'train_batch_size': 64,\n\n        # Simple image augmentations\n        'train_dataset': 'cifar10_train_augmented',\n        # Do not augment images for validation\n        'valid_dataset': 'cifar10_valid_no_augment',\n    })\n    # Set model for saving/loading\n    experiment.add_pytorch_models({'model': conf.model})\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/conv_mixer/__init__.py": "\"\"\"\n---\ntitle: Patches Are All You Need? (ConvMixer)\nsummary: >\n A PyTorch implementation/tutorial of the paper\n \"Patches Are All You Need?\"\n---\n\n#  Patches Are All You Need? (ConvMixer)\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[Patches Are All You Need?](https://arxiv.org/abs/2201.09792).\n\n![ConvMixer diagram from the paper](conv_mixer.png)\n\nConvMixer is Similar to [MLP-Mixer](../transformers/mlp_mixer/index.html).\nMLP-Mixer separates mixing of spatial and channel dimensions, by applying an MLP across spatial dimension\nand then an MLP across the channel dimension\n(spatial MLP replaces the [ViT](../transformers/vit/index.html) attention\nand channel MLP is the [FFN](../transformers/feed_forward.html) of ViT).\n\nConvMixer uses a $1 \\times 1$ convolution for channel mixing and a\ndepth-wise convolution for spatial mixing.\nSince it's a convolution instead of a full MLP across the space, it mixes only the nearby batches in\ncontrast to ViT or MLP-Mixer.\nAlso, the MLP-mixer uses MLPs of two layers for each mixing and ConvMixer uses a single layer for each mixing.\n\nThe paper recommends removing the residual connection across the channel mixing (point-wise convolution)\nand having only a residual connection over the spatial mixing (depth-wise convolution).\nThey also use [Batch normalization](../normalization/batch_norm/index.html) instead\nof [Layer normalization](../normalization/layer_norm/index.html).\n\nHere's [an experiment](experiment.html) that trains ConvMixer on CIFAR-10.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.utils import clone_module_list\n\n\nclass ConvMixerLayer(Module):\n    \"\"\"\n    <a id=\"ConvMixerLayer\"></a>\n\n    ## ConvMixer layer\n\n    This is a single ConvMixer layer. The model will have a series of these.\n    \"\"\"\n\n    def __init__(self, d_model: int, kernel_size: int):\n        \"\"\"\n        * `d_model` is the number of channels in patch embeddings, $h$\n        * `kernel_size` is the size of the kernel of spatial convolution, $k$\n        \"\"\"\n        super().__init__()\n        # Depth-wise convolution is separate convolution for each channel.\n        # We do this with a convolution layer with the number of groups equal to the number of channels.\n        # So that each channel is it's own group.\n        self.depth_wise_conv = nn.Conv2d(d_model, d_model,\n                                         kernel_size=kernel_size,\n                                         groups=d_model,\n                                         padding=(kernel_size - 1) // 2)\n        # Activation after depth-wise convolution\n        self.act1 = nn.GELU()\n        # Normalization after depth-wise convolution\n        self.norm1 = nn.BatchNorm2d(d_model)\n\n        # Point-wise convolution is a $1 \\times 1$ convolution.\n        # i.e. a linear transformation of patch embeddings\n        self.point_wise_conv = nn.Conv2d(d_model, d_model, kernel_size=1)\n        # Activation after point-wise convolution\n        self.act2 = nn.GELU()\n        # Normalization after point-wise convolution\n        self.norm2 = nn.BatchNorm2d(d_model)\n\n    def forward(self, x: torch.Tensor):\n        # For the residual connection around the depth-wise convolution\n        residual = x\n\n        # Depth-wise convolution, activation and normalization\n        x = self.depth_wise_conv(x)\n        x = self.act1(x)\n        x = self.norm1(x)\n\n        # Add residual connection\n        x += residual\n\n        # Point-wise convolution, activation and normalization\n        x = self.point_wise_conv(x)\n        x = self.act2(x)\n        x = self.norm2(x)\n\n        #\n        return x\n\n\nclass PatchEmbeddings(Module):\n    \"\"\"\n    <a id=\"PatchEmbeddings\"></a>\n\n    ## Get patch embeddings\n\n    This splits the image into patches of size $p \\times p$ and gives an embedding for each patch.\n    \"\"\"\n\n    def __init__(self, d_model: int, patch_size: int, in_channels: int):\n        \"\"\"\n        * `d_model` is the number of channels in patch embeddings $h$\n        * `patch_size` is the size of the patch, $p$\n        * `in_channels` is the number of channels in the input image (3 for rgb)\n        \"\"\"\n        super().__init__()\n\n        # We create a convolution layer with a kernel size and and stride length equal to patch size.\n        # This is equivalent to splitting the image into patches and doing a linear\n        # transformation on each patch.\n        self.conv = nn.Conv2d(in_channels, d_model, kernel_size=patch_size, stride=patch_size)\n        # Activation function\n        self.act = nn.GELU()\n        # Batch normalization\n        self.norm = nn.BatchNorm2d(d_model)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input image of shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Apply convolution layer\n        x = self.conv(x)\n        # Activation and normalization\n        x = self.act(x)\n        x = self.norm(x)\n\n        #\n        return x\n\n\nclass ClassificationHead(Module):\n    \"\"\"\n    <a id=\"ClassificationHead\"></a>\n\n    ## Classification Head\n\n    They do average pooling (taking the mean of all patch embeddings) and a final linear transformation\n    to predict the log-probabilities of the image classes.\n    \"\"\"\n\n    def __init__(self, d_model: int, n_classes: int):\n        \"\"\"\n        * `d_model` is the number of channels in patch embeddings, $h$\n        * `n_classes` is the number of classes in the classification task\n        \"\"\"\n        super().__init__()\n        # Average Pool\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        # Linear layer\n        self.linear = nn.Linear(d_model, n_classes)\n\n    def forward(self, x: torch.Tensor):\n        # Average pooling\n        x = self.pool(x)\n        # Get the embedding, `x` will have shape `[batch_size, d_model, 1, 1]`\n        x = x[:, :, 0, 0]\n        # Linear layer\n        x = self.linear(x)\n\n        #\n        return x\n\n\nclass ConvMixer(Module):\n    \"\"\"\n    ## ConvMixer\n\n    This combines the patch embeddings block, a number of ConvMixer layers and a classification head.\n    \"\"\"\n\n    def __init__(self, conv_mixer_layer: ConvMixerLayer, n_layers: int,\n                 patch_emb: PatchEmbeddings,\n                 classification: ClassificationHead):\n        \"\"\"\n        * `conv_mixer_layer` is a copy of a single [ConvMixer layer](#ConvMixerLayer).\n         We make copies of it to make ConvMixer with `n_layers`.\n        * `n_layers` is the number of ConvMixer layers (or depth), $d$.\n        * `patch_emb` is the [patch embeddings layer](#PatchEmbeddings).\n        * `classification` is the [classification head](#ClassificationHead).\n        \"\"\"\n        super().__init__()\n        # Patch embeddings\n        self.patch_emb = patch_emb\n        # Classification head\n        self.classification = classification\n        # Make copies of the [ConvMixer layer](#ConvMixerLayer)\n        self.conv_mixer_layers = clone_module_list(conv_mixer_layer, n_layers)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input image of shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Get patch embeddings. This gives a tensor of shape `[batch_size, d_model, height / patch_size, width / patch_size]`.\n        x = self.patch_emb(x)\n\n        # Pass through [ConvMixer layers](#ConvMixerLayer)\n        for layer in self.conv_mixer_layers:\n            x = layer(x)\n\n        # Classification head, to get logits\n        x = self.classification(x)\n\n        #\n        return x\n", "labml_nn/scaling/__init__.py": "\"\"\"\n---\ntitle: Large scale model training\nsummary: >\n    Large scale model training/inference implementations.\n---\n\n# Large scale model training\n\n* [Zero-DP optimizer](zero3/index.html)\n\"\"\"\n", "labml_nn/scaling/zero3/finetune_neox.py": "\"\"\"\n---\ntitle: Finetune GPT-NeoX with Zero3 memory optimizer\nsummary: >\n    This script trains the bias parameters of the GPT-NeoX on multiple devices with Zero-DP Memory Optimization.\n---\n\n#  Finetune [GPT-NeoX](../../neox/index.html) with [Zero3 memory optimizer](index.html)\n\nThis script trains the bias parameters of the [GPT-NeoX model](../../neox/model.html)\n on multiple devices with Zero-DP Memory Optimization.\n\"\"\"\n\nimport datetime\n\nimport torch\nimport torch.distributed\n\nfrom labml import experiment, monit, tracker\nfrom labml.configs import option\nfrom labml.logger import inspect\nfrom labml_nn.neox.samples.finetune import PipelineParallelTrainerConf\n\n\n# Use the [Pipeline Parallel Trainer configurations](../../neox/samples/finetune.html) and adapt it for\n# Zero3 memory optimizer.\nclass Configs(PipelineParallelTrainerConf):\n    rank: int\n    world_size: int\n\n\n@option(Configs.optimizer, 'Zero3Adam')\ndef _optimizer(c: Configs):\n    \"\"\"\n    #### Set the optimizers for the model\n\n    Note that we pass the sharded parameters from `get_trainable_chunk`.\n    \"\"\"\n    from labml_nn.optimizers.adam_fp16 import AdamFP16\n    return AdamFP16(c.model.get_trainable_chunk(), lr=c.learning_rate)\n\n\n@option(Configs.model, 'Zero3')\ndef _model(c: Configs):\n    \"\"\"\n    #### Create the model with Zero3 memory optimizer\n    \"\"\"\n    from labml_nn.scaling.zero3 import Zero3Layer, Zero3Sequential\n\n    # To make sure the fine tuner sets the trainable parameters\n    _ = c.fine_tuner\n\n    # Wrap the layers with `Zero3Layer`\n    modules = []\n    for m in monit.iterate('Zero3', c.layers):\n        modules.append(Zero3Layer(m.to(c.device),\n                                  c.rank, c.world_size, c.device, c.dtype))\n\n    # Create a sequential model\n    model = Zero3Sequential(modules)\n\n    #\n    return model\n\n\ndef main(rank: int, world_size: int, init_method: str = 'tcp://localhost:23456'):\n    \"\"\"\n    #### Run the training on the node with rank `rank`.\n    \"\"\"\n    # Initialize PyTorch distributed process group\n    with monit.section('Distributed'):\n        torch.distributed.init_process_group('nccl',\n                                             timeout=datetime.timedelta(seconds=30),\n                                             init_method=init_method,\n                                             rank=rank,\n                                             world_size=world_size)\n\n    # Set current device\n    device = torch.device(f'cuda:{rank}')\n    torch.cuda.set_device(device)\n\n    # Create the experiment\n    experiment.create(name='zero3_neox', writers={'screen', 'labml'},\n                      distributed_world_size=world_size,\n                      distributed_rank=rank)\n\n    # Create configurations\n    conf = Configs()\n\n    # Load configurations\n    experiment.configs(conf, {\n        'model': 'Zero3',\n        'optimizer': 'Zero3Adam',\n\n        'device': device,\n        'rank': rank,\n        'world_size': world_size,\n\n        'learning_rate': 3e-4,\n        'max_seq_len': 128,\n        'batch_size': 16,\n    })\n\n    # Start the experiment\n    with experiment.start():\n        # Initialize the model. Do this before the loop for cleaner logs.\n        _ = conf.model\n\n        # Train the model\n        for epoch in monit.loop(conf.epochs):\n            conf.train_epoch()\n            tracker.new_line()\n\n\n#\nif __name__ == '__main__':\n    # Log the machine configurations\n    inspect([torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])\n    inspect(\n        n_gpus=torch.cuda.device_count(),\n        mpi=torch.distributed.is_mpi_available(),\n        nccl=torch.distributed.is_nccl_available(),\n    )\n\n    n_gpu = torch.cuda.device_count()\n\n    # Start a process for each GPU. You will need a separate launcher if you are using multiple computers.\n    torch.multiprocessing.spawn(main, args=(n_gpu,), nprocs=n_gpu, join=True)\n", "labml_nn/scaling/zero3/__init__.py": "\"\"\"\n---\ntitle: Zero-DP Memory Optimization\nsummary: >\n    This is an implementation of Zero-DP Memory Optimization written in PyTorch.\n---\n\n# Zero-DP Memory Optimization\n\nThis is an implementation of Zero-DP introduced in the paper\n[ZeRO: Memory Optimization Towards Training A Trillion Parameter Models](https://arxiv.org/abs/1910.02054),\n\nIt keeps shards of the optimizer state, gradients and parameters into multiple devices/nodes.\nIt reduces the memory consumption to $\\frac{(2 + 2 + K)\\Psi}{N_d}$ of the original model,\nwhere $\\Psi$ is the number of parameters, $N_d$ is the number of shards,\n and $K$ is number of optimizer bytes per parameter.\n$2 + 2$ are the parameter and gradient memory assuming 16-bit precision; i.e. 2 bytes per parameter and gradient.\n$K = 12$ for Adam optimizer because it maintains a copy of parameters, and two moments per parameter in fp32.\n\nThe communication volume of Zero-DP is $\\mathcal{O}(3\\Psi)$. For comparison data-parallel training\nhas a communication volume of $\\mathcal{O}(2\\Psi)$.\n\nAlthough this is named `Zero3`, we have only implemented the Zero-DP part of it and not the\n Zero-R memory optimizations which target residual memory consumption.\nOut implementation supports training only a subset of parameters.\n\nThis implementation is inspired by [Fairscale FSDP](https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html).\n\n[Here's a script to fine-tune](finetune_neox.html) GPT NeoX using Zero-DP memory optimization.\n\"\"\"\n\nimport functools\nfrom typing import List, Optional, Tuple\n\nimport torch\nimport torch.distributed as dist\nfrom torch import nn\n\n\nclass Zero3Layer(nn.Module):\n    \"\"\"\n    ## Zero3 Layer\n\n    Each layer of the model (or a combination of a few consecutive layers) should be wrapped in\n    this module.\n    \"\"\"\n    # Each shard keeps parameters in `chunk` list.\n    # The `chunk[0]` is for trainable parameters and `chunk[1]` is for fixed parameters.\n    chunk: List[nn.Parameter]\n    # This is the sizes of the chunks in `chunk` list.\n    chunk_size: List[int]\n    # The first chunk is for trainable parameters.\n    TRAINING_PARAMS_IDX = 0\n\n    # This is the list of parameters split into lists as trainable and fixed parameters.\n    param_refs: List[List[nn.Parameter]]\n\n    # CUDA stream to featch parameters\n    fetch_stream: Optional[torch.cuda.Stream]\n    # CUDA stream to backup/accumulate gradients\n    backup_stream: Optional[torch.cuda.Stream]\n    # List of layers right before this layer\n    prev_layer: List['Zero3Layer']\n    # List of layers right after this layer\n    next_layer: List['Zero3Layer']\n    # The position of the current layer; used this for debugging logs\n    layer_idx: int\n\n    # Whether parameters have been fetched\n    is_fetched: bool\n\n    # Device of the layer\n    device: torch.device\n    # Data type of the layer\n    dtype: torch.dtype\n    # The module to be wrapped\n    module: nn.Module\n    # Number of nodes/devices the data is sharded across\n    world_size: int\n\n    def __init__(self, module: nn.Module, rank: int, world_size: int, device: torch.device, dtype: torch.dtype):\n        \"\"\"\n        :param module: The module to be wrapped.\n        :param rank: The rank of the current node.\n        :param world_size: The number of nodes/devices the data is sharded across.\n        :param device: The device of the layer.\n        :param dtype: The data type of the layer.\n        \"\"\"\n        super().__init__()\n\n        # Initialize the properties\n        self.device = device\n        self.dtype = dtype\n        self.module = module\n        self.prev_layer = []\n        self.next_layer = []\n        self.is_fetched = False\n        self.world_size = world_size\n        self.layer_idx = -1\n        self.fetch_stream = None\n        self.backup_stream = None\n\n        with torch.no_grad():\n            # Collect all the parameters of the layer\n            all_param_refs = [p for p in self.parameters()]\n\n            # Store the shape of the parameters because we need it later to reconstruct them\n            for p in all_param_refs:\n                p._orig_shape = p.shape\n\n            # All parameters should have the same type\n            for p in all_param_refs:\n                assert p.dtype == dtype, \"All parameters should have same dtype\"\n\n            # Separate parameters as trainable and fixed\n            self.param_refs = [[p for p in all_param_refs if p.requires_grad],\n                               [p for p in all_param_refs if not p.requires_grad]]\n            del all_param_refs\n\n            # The `rank = 0` node will calculate the size each device/node should store, and\n            # distribute the parameters accordingly.\n            if rank == 0:\n                # Merge and pad trainable (`merged_params[0]`) and fixed (`merged_params[1]`) parameters\n                merged_params = [self._merge_and_pad_params(ps) for ps in self.param_refs]\n                # Calculate the chunk sizes of trainable and fixed params\n                self.chunk_size = [(len(p) // world_size if p is not None else 0) for p in merged_params]\n                # Broadcast the sizes\n                dist.broadcast(torch.tensor(self.chunk_size, device=device), src=0)\n            else:\n                # Create an empty tensor to receive the sizes\n                chunk_size = torch.tensor([0, 0], device=device)\n                # Receive the sizes\n                dist.broadcast(chunk_size, src=0)\n                self.chunk_size = chunk_size.tolist()\n\n            # Create parameters for trainable (`self.chunk[0]`) and fixed (`self.chunk[1]`)\n            # parameters to be stored in current device/node\n            self.chunk = [nn.Parameter(self._empty((s,)), requires_grad=i == self.TRAINING_PARAMS_IDX)\n                          for i, s in enumerate(self.chunk_size)]\n\n            # An empty tensor to receive the trainable and fixed parameters combined\n            chunk = self._empty((sum(self.chunk_size),))\n\n            if rank == 0:\n                # Concatenate both trainable and fixed params\n                all_params = torch.cat([p.view(world_size, -1) for p in merged_params], dim=-1).view(-1)\n                del merged_params\n\n                # Scatter them to all the nodes/devices\n                dist.scatter(chunk, list(all_params.split(sum(self.chunk_size))))\n                del all_params\n            else:\n                # Receive the parameters\n                dist.scatter(chunk)\n\n            # Collect the chunk data\n            chunk = chunk.split(self.chunk_size)\n            for i, c in enumerate(chunk):\n                self.chunk[i].data[:] = c\n            del chunk\n\n            # Cleanup the normal parameters\n            self._cleanup_params()\n\n            # Add a backward hook. This gets called when the gradients relative to the module are computed.\n            self._backward_hook_ref = self.register_full_backward_hook(self._backward_hook)  # type: ignore\n\n    def _merge_and_pad_params(self, params: List[nn.Parameter]) -> torch.Tensor:\n        \"\"\"\n        #### Merge all the parameters and pad it so that it's divisible by `world_size`.\n        \"\"\"\n        # Total number of parameters\n        size = sum(p.shape.numel() for p in params)\n\n        # If it is not divisible by `world_size`, pad it\n        if size % self.world_size != 0:\n            padding_fixed = self.world_size - (size % self.world_size)\n        # Otherwise, no need to pad\n        else:\n            padding_fixed = 0\n        # Create an empty padding tensor\n        padding = self._empty((padding_fixed,))\n        # Concatenate all the parameters and pad it\n        return torch.cat([p.view(-1) for p in params] + [padding], dim=0)\n\n    def get_trainable_chunk(self) -> List[nn.Parameter]:\n        \"\"\"\n        ### Get trainable chunk/shard of the parameters.\n\n        This is what we pass on to the optimizer on the current node.\n        \"\"\"\n        # Return and empty list if there are no trainable parameters\n        if len(self.chunk[self.TRAINING_PARAMS_IDX]) == 0:\n            return []\n\n        # Return the trainable chunk as a list\n        return [self.chunk[self.TRAINING_PARAMS_IDX]]\n\n    def _empty(self, shape: Tuple[int, ...]) -> torch.Tensor:\n        \"\"\"\n        #### Create an empty tensor of the given shape.\n        \"\"\"\n        return torch.empty(shape, device=self.device, dtype=self.dtype)\n\n    @torch.no_grad()\n    def _cleanup_params(self):\n        \"\"\"\n        #### Cleanup the parameter data\n\n        This will release all the memory used by the layer parameters.\n        \"\"\"\n\n        # Set the flag to indicate that the parameters are not fetched\n        self.is_fetched = False\n\n        # Iterate through all parameters\n        for ps in self.param_refs:\n            for p in ps:\n                # Wait for operations on the parameters to complete before any new operations\n                p.data.record_stream(torch.cuda.current_stream())\n                # Check to make sure the parameter is not sharing storage with anything else\n                assert p.data.storage_offset() == 0, \"The tensor is not the sole occupant of the storage.\"\n                # Resize the storage to $0$. This will release the memory used by the parameter.\n                #\n                # **Setting `p.data` will not release the memory, since the autograd graph keeps a reference to it.**\n                p.data.storage().resize_(0)  # This is what actually clears the memory\n                # Make sure the parameter has no gradient data\n                assert p.grad is None, 'Gradients should be None'\n\n    @torch.no_grad()\n    def fetch_params(self):\n        \"\"\"\n        ### Fetch the parameters from all shards\n\n        This will fetch all the parameter data from all the nodes and rebuild the parameters on each node.\n        \"\"\"\n\n        # Skip is already fetched\n        if self.is_fetched:\n            return\n\n        # Set the flag\n        self.is_fetched = True\n\n        # Skip if there's nothing to fetch or share.\n        if sum(self.chunk_size) == 0:\n            return\n\n        # Use `fetch_stream` to fetch the parameters from all the shards\n        with torch.cuda.stream(self.fetch_stream):\n            # Create an empty tensor to receive the parameters\n            buffer = self._empty((self.world_size * sum(self.chunk_size),))\n            # Split the continuous buffer into the number of nodes. These splits are views of `buffer'.\n            buffers = list(buffer.split(sum(self.chunk_size)))\n\n            # Concatenate both trainable and fixed chunks\n            chunk = torch.cat(self.chunk, dim=0)\n\n            # Gather the parameters from all the nodes/devices\n            dist.all_gather(buffers, chunk)\n\n            # Split the gathered parameters into the trainable and fixed chunks\n            params = buffer.view(-1, sum(self.chunk_size)).split(self.chunk_size, dim=1)\n            # Wait for the gather operation to complete and then clear the references to the buffers\n            buffer.record_stream(self.fetch_stream)\n            for b in buffers:\n                b.record_stream(self.fetch_stream)\n            buffer.record_stream(self.fetch_stream)\n            del buffer\n            del buffers\n\n            # Reshape the trainable and fixed parameters to continuous tensors\n            params = [p.reshape(-1) for p in params]\n\n            # Collect the individual parameter tensors\n            for cont, ps in zip(params, self.param_refs):\n                # If there are no parameters, skip\n                if not ps:\n                    continue\n\n                # Offset of the continuous tensor\n                offset = 0\n                # Iterate through model parameters and assign the values from the continuous tensor\n                for p in ps:\n                    # Original parameter shape\n                    shape = p._orig_shape  # type: ignore[attr-defined]\n                    # Change the storage size of the parameter. This was set to $0$ when we cleaned up the parameters.\n                    p.data.storage().resize_(shape.numel())\n                    # Assign the values from the continuous tensor\n                    p.data[:] = cont[offset: offset + shape.numel()].reshape(shape)\n                    # Wait for the operations to complete before other operations can be performed\n                    p.data.record_stream(self.fetch_stream)\n                    # Update the offset\n                    offset += shape.numel()\n\n                # Wait for the operation to complete before other operations can be performed\n                cont.record_stream(self.fetch_stream)\n\n            #\n            del params\n\n    def forward(self, *args, **kwargs):\n        \"\"\"\n        ### Forward pass\n        \"\"\"\n\n        # Fetch all the parameters of the current node.\n        # This gets called by the previous layer so this call is just to make sure parameters are fetched.\n        self.fetch_params()\n\n        # Wait for parameter fetching to complete.\n        torch.cuda.current_stream().wait_stream(self.fetch_stream)\n\n        # Start fetching parameters of the proceeding layers, so that they will fetch them which the current layer\n        # does its computations.\n        for layer in self.next_layer:\n            layer.fetch_params()\n\n        # Add backward hooks to the parameters of the current layer if autograd is enabled.\n        if torch.is_grad_enabled():\n            self._add_backward_hooks()\n\n        # Compute the outputs of the current layer\n        res = self.module(*args, **kwargs)\n\n        # Cleanup the parameters of the layer.\n        #\n        # *Skip cleaning up if autograd is enabled and this is the last layer in the network,\n        # because we will need to fetch the parameters again for the backward pass.*\n        if not torch.is_grad_enabled() or self.next_layer:\n            self._cleanup_params()\n\n        return res\n\n    def _add_backward_hooks(self):\n        \"\"\"\n        #### Add backward hooks to the parameters of the current layer.\n        \"\"\"\n\n        # Number of backward hooks added\n        self._backward_hook_handles = 0\n\n        # Loop through trainable parameters of the current layer\n        for p in self.param_refs[self.TRAINING_PARAMS_IDX]:\n            # Make sure a hook hasn't already been added\n            assert not hasattr(p, \"_hook_handle\"), 'Parameter has already been hooked'\n            # Use `expand_as` to create an autograd step which we can intercept\n            p_tmp = p.expand_as(p)\n            # Get a handle to add the backward hook.\n            # [This blog discusses about `grad_acc`](https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00).\n            grad_acc = p_tmp.grad_fn.next_functions[0][0]\n            # Add the backward hook\n            handle = grad_acc.register_hook(\n                functools.partial(self._post_backward_hook, p))\n            # Keep a reference to the handle\n            p._hook_handle = handle\n            # Increment the number of hooks added\n            self._backward_hook_handles += 1\n\n    def _backward_event(self):\n        \"\"\"\n        #### Handle a backward event\n\n        This gets called by parameter backward hooks and the module backward hook.\n        \"\"\"\n\n        # Decrement the hooks counter\n        self._backward_hook_handles -= 1\n\n        # If all the hooks (including the module hook) have been called,\n        # then we can back up gradients and clean up the parameters.\n        if self._backward_hook_handles == -1:\n            self._backup_grads()\n            self._cleanup_params()\n\n        # Start fetch parameters of the previous layer, because autograd will next process the gradients of it.\n        for layer in self.prev_layer:\n            layer.fetch_params()\n\n    def _post_backward_hook(self, p: nn.Parameter, *args):\n        \"\"\"\n        #### Parameter backward hook\n        \"\"\"\n        # Remove the handle from the parameter\n        p._hook_handle.remove()  # type: ignore[attr-defined]\n        delattr(p, \"_hook_handle\")\n\n        # Handle a backward event\n        self._backward_event()\n\n    def _backward_hook(self, *args, **kwargs):\n        \"\"\"\n        #### Module backward hook\n        \"\"\"\n        # Handle a backward event\n        self._backward_event()\n\n        # The previous layer will start computing gradients. We need to make sure it has finished fetching params.\n        torch.cuda.current_stream().wait_stream(self.fetch_stream)\n\n        #\n        return None\n\n    @torch.no_grad()\n    def _backup_grads(self):\n        \"\"\"\n        ### Backup the gradients of the current layer\n        \"\"\"\n        # Skip if there are no trainable parameters\n        if self.chunk_size[self.TRAINING_PARAMS_IDX] == 0:\n            return\n\n        # Use the backup stream to backup the gradients\n        with torch.cuda.stream(self.backup_stream):\n            # Buffer to store the gradients\n            buffer = self._empty((self.world_size * self.chunk_size[self.TRAINING_PARAMS_IDX],))\n            # Split the continuous buffer into number of nodes. These splits are views of `buffer'.\n            buffers = list(buffer.split(self.chunk_size[self.TRAINING_PARAMS_IDX]))\n\n            # Offset of the continuous buffer\n            offset = 0\n            # Iterate through trainable parameters\n            for p in self.param_refs[self.TRAINING_PARAMS_IDX]:\n                # Collect gradients\n                shape = p._orig_shape  # type: ignore[attr-defined]\n                buffer[offset: offset + shape.numel()] = p.grad.view(-1)\n                # Update the offset\n                offset += shape.numel()\n                # Clean the gradients\n                p.grad = None\n\n            # Empty tensor to accumulate the gradients of the current shard\n            grad = self._empty((self.chunk_size[self.TRAINING_PARAMS_IDX],))\n            # Accumulate the gradients of each shard. It scatters the buffers across the nodes,\n            # and each node accumulates (reduces) the tensors it receives.\n            dist.reduce_scatter(grad, buffers)\n\n            # Wait for the operation to complete and then clear the references to the buffers\n            for b in buffers:\n                b.record_stream(self.fetch_stream)\n            buffer.record_stream(self.fetch_stream)\n            del buffer\n            del buffers\n\n            # Set the chunk gradients. This is what the optimizer sees.\n            self.chunk[self.TRAINING_PARAMS_IDX].grad = grad\n            del grad\n\n\nclass Zero3Sequential(nn.Module):\n    \"\"\"\n    ## Sequential module for `Zero3Layer` layers\n    \"\"\"\n    def __init__(self, modules: List[Zero3Layer]):\n        \"\"\"\n        :param modules: List of `Zero3Layer` layers\n        \"\"\"\n        super().__init__()\n\n        # CUDA stream to fetch parameters\n        self.fetch_stream = torch.cuda.Stream()\n        # CUDA stream to back up (accumulate) gradients\n        self.backup_stream = torch.cuda.Stream()\n\n        # Set the streams and preceding and proceeding layers for each `Zero3Layer` layer\n        for i in range(len(modules)):\n            # Set layer index\n            modules[i].layer_idx = i\n            # Set streams\n            modules[i].fetch_stream = self.fetch_stream\n            modules[i].backup_stream = self.backup_stream\n            # Set proceeding layers\n            if i + 1 < len(modules):\n                modules[i].next_layer.append(modules[i + 1])\n            # Set preceding layers\n            if i - 1 >= 0:\n                modules[i].prev_layer.append(modules[i - 1])\n\n        # Store list of modules\n        self.module_list = nn.ModuleList(modules)\n\n    def get_trainable_chunk(self):\n        # Return the list of trainable chunks from each layer\n        return sum([m.get_trainable_chunk() for m in self.module_list], [])\n\n    def forward(self, x: torch.Tensor):\n        # Make sure gradient back up is complete\n        torch.cuda.current_stream().wait_stream(self.backup_stream)\n\n        # Forward pass\n        for m in self.module_list:\n            x = m(x)\n\n        #\n        return x\n", "labml_nn/graphs/__init__.py": "\"\"\"\n---\ntitle: Graph Neural Networks\nsummary: >\n A set of PyTorch implementations/tutorials related to graph neural networks\n---\n\n# Graph Neural Networks\n\n* [Graph Attention Networks (GAT)](gat/index.html)\n* [Graph Attention Networks v2 (GATv2)](gatv2/index.html)\n\"\"\"\n", "labml_nn/graphs/gat/experiment.py": "\"\"\"\n---\ntitle: Train a Graph Attention Network (GAT) on Cora dataset\nsummary: >\n  This trains is a  Graph Attention Network (GAT) on Cora dataset\n---\n\n# Train a Graph Attention Network (GAT) on Cora dataset\n\"\"\"\n\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom labml import lab, monit, tracker, experiment\nfrom labml.configs import BaseConfigs, option, calculate\nfrom labml.utils import download\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_helpers.module import Module\nfrom labml_nn.graphs.gat import GraphAttentionLayer\nfrom labml_nn.optimizers.configs import OptimizerConfigs\n\n\nclass CoraDataset:\n    \"\"\"\n    ## [Cora Dataset](https://linqs.soe.ucsc.edu/data)\n\n    Cora dataset is a dataset of research papers.\n    For each paper we are given a binary feature vector that indicates the presence of words.\n    Each paper is classified into one of 7 classes.\n    The dataset also has the citation network.\n\n    The papers are the nodes of the graph and the edges are the citations.\n\n    The task is to classify the nodes to the 7 classes with feature vectors and\n    citation network as input.\n    \"\"\"\n    # Labels for each node\n    labels: torch.Tensor\n    # Set of class names and an unique integer index\n    classes: Dict[str, int]\n    # Feature vectors for all nodes\n    features: torch.Tensor\n    # Adjacency matrix with the edge information.\n    # `adj_mat[i][j]` is `True` if there is an edge from `i` to `j`.\n    adj_mat: torch.Tensor\n\n    @staticmethod\n    def _download():\n        \"\"\"\n        Download the dataset\n        \"\"\"\n        if not (lab.get_data_path() / 'cora').exists():\n            download.download_file('https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz',\n                                   lab.get_data_path() / 'cora.tgz')\n            download.extract_tar(lab.get_data_path() / 'cora.tgz', lab.get_data_path())\n\n    def __init__(self, include_edges: bool = True):\n        \"\"\"\n        Load the dataset\n        \"\"\"\n\n        # Whether to include edges.\n        # This is test how much accuracy is lost if we ignore the citation network.\n        self.include_edges = include_edges\n\n        # Download dataset\n        self._download()\n\n        # Read the paper ids, feature vectors, and labels\n        with monit.section('Read content file'):\n            content = np.genfromtxt(str(lab.get_data_path() / 'cora/cora.content'), dtype=np.dtype(str))\n        # Load the citations, it's a list of pairs of integers.\n        with monit.section('Read citations file'):\n            citations = np.genfromtxt(str(lab.get_data_path() / 'cora/cora.cites'), dtype=np.int32)\n\n        # Get the feature vectors\n        features = torch.tensor(np.array(content[:, 1:-1], dtype=np.float32))\n        # Normalize the feature vectors\n        self.features = features / features.sum(dim=1, keepdim=True)\n\n        # Get the class names and assign an unique integer to each of them\n        self.classes = {s: i for i, s in enumerate(set(content[:, -1]))}\n        # Get the labels as those integers\n        self.labels = torch.tensor([self.classes[i] for i in content[:, -1]], dtype=torch.long)\n\n        # Get the paper ids\n        paper_ids = np.array(content[:, 0], dtype=np.int32)\n        # Map of paper id to index\n        ids_to_idx = {id_: i for i, id_ in enumerate(paper_ids)}\n\n        # Empty adjacency matrix - an identity matrix\n        self.adj_mat = torch.eye(len(self.labels), dtype=torch.bool)\n\n        # Mark the citations in the adjacency matrix\n        if self.include_edges:\n            for e in citations:\n                # The pair of paper indexes\n                e1, e2 = ids_to_idx[e[0]], ids_to_idx[e[1]]\n                # We build a symmetrical graph, where if paper $i$ referenced\n                # paper $j$ we place an adge from $i$ to $j$ as well as an edge\n                # from $j$ to $i$.\n                self.adj_mat[e1][e2] = True\n                self.adj_mat[e2][e1] = True\n\n\nclass GAT(Module):\n    \"\"\"\n    ## Graph Attention Network (GAT)\n\n    This graph attention network has two [graph attention layers](index.html).\n    \"\"\"\n\n    def __init__(self, in_features: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float):\n        \"\"\"\n        * `in_features` is the number of features per node\n        * `n_hidden` is the number of features in the first graph attention layer\n        * `n_classes` is the number of classes\n        * `n_heads` is the number of heads in the graph attention layers\n        * `dropout` is the dropout probability\n        \"\"\"\n        super().__init__()\n\n        # First graph attention layer where we concatenate the heads\n        self.layer1 = GraphAttentionLayer(in_features, n_hidden, n_heads, is_concat=True, dropout=dropout)\n        # Activation function after first graph attention layer\n        self.activation = nn.ELU()\n        # Final graph attention layer where we average the heads\n        self.output = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout)\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n        \"\"\"\n        * `x` is the features vectors of shape `[n_nodes, in_features]`\n        * `adj_mat` is the adjacency matrix of the form\n         `[n_nodes, n_nodes, n_heads]` or `[n_nodes, n_nodes, 1]`\n        \"\"\"\n        # Apply dropout to the input\n        x = self.dropout(x)\n        # First graph attention layer\n        x = self.layer1(x, adj_mat)\n        # Activation function\n        x = self.activation(x)\n        # Dropout\n        x = self.dropout(x)\n        # Output layer (without activation) for logits\n        return self.output(x, adj_mat)\n\n\ndef accuracy(output: torch.Tensor, labels: torch.Tensor):\n    \"\"\"\n    A simple function to calculate the accuracy\n    \"\"\"\n    return output.argmax(dim=-1).eq(labels).sum().item() / len(labels)\n\n\nclass Configs(BaseConfigs):\n    \"\"\"\n    ## Configurations\n    \"\"\"\n\n    # Model\n    model: GAT\n    # Number of nodes to train on\n    training_samples: int = 500\n    # Number of features per node in the input\n    in_features: int\n    # Number of features in the first graph attention layer\n    n_hidden: int = 64\n    # Number of heads\n    n_heads: int = 8\n    # Number of classes for classification\n    n_classes: int\n    # Dropout probability\n    dropout: float = 0.6\n    # Whether to include the citation network\n    include_edges: bool = True\n    # Dataset\n    dataset: CoraDataset\n    # Number of training iterations\n    epochs: int = 1_000\n    # Loss function\n    loss_func = nn.CrossEntropyLoss()\n    # Device to train on\n    #\n    # This creates configs for device, so that\n    # we can change the device by passing a config value\n    device: torch.device = DeviceConfigs()\n    # Optimizer\n    optimizer: torch.optim.Adam\n\n    def run(self):\n        \"\"\"\n        ### Training loop\n\n        We do full batch training since the dataset is small.\n        If we were to sample and train we will have to sample a set of\n        nodes for each training step along with the edges that span\n        across those selected nodes.\n        \"\"\"\n        # Move the feature vectors to the device\n        features = self.dataset.features.to(self.device)\n        # Move the labels to the device\n        labels = self.dataset.labels.to(self.device)\n        # Move the adjacency matrix to the device\n        edges_adj = self.dataset.adj_mat.to(self.device)\n        # Add an empty third dimension for the heads\n        edges_adj = edges_adj.unsqueeze(-1)\n\n        # Random indexes\n        idx_rand = torch.randperm(len(labels))\n        # Nodes for training\n        idx_train = idx_rand[:self.training_samples]\n        # Nodes for validation\n        idx_valid = idx_rand[self.training_samples:]\n\n        # Training loop\n        for epoch in monit.loop(self.epochs):\n            # Set the model to training mode\n            self.model.train()\n            # Make all the gradients zero\n            self.optimizer.zero_grad()\n            # Evaluate the model\n            output = self.model(features, edges_adj)\n            # Get the loss for training nodes\n            loss = self.loss_func(output[idx_train], labels[idx_train])\n            # Calculate gradients\n            loss.backward()\n            # Take optimization step\n            self.optimizer.step()\n            # Log the loss\n            tracker.add('loss.train', loss)\n            # Log the accuracy\n            tracker.add('accuracy.train', accuracy(output[idx_train], labels[idx_train]))\n\n            # Set mode to evaluation mode for validation\n            self.model.eval()\n\n            # No need to compute gradients\n            with torch.no_grad():\n                # Evaluate the model again\n                output = self.model(features, edges_adj)\n                # Calculate the loss for validation nodes\n                loss = self.loss_func(output[idx_valid], labels[idx_valid])\n                # Log the loss\n                tracker.add('loss.valid', loss)\n                # Log the accuracy\n                tracker.add('accuracy.valid', accuracy(output[idx_valid], labels[idx_valid]))\n\n            # Save logs\n            tracker.save()\n\n\n@option(Configs.dataset)\ndef cora_dataset(c: Configs):\n    \"\"\"\n    Create Cora dataset\n    \"\"\"\n    return CoraDataset(c.include_edges)\n\n\n# Get the number of classes\ncalculate(Configs.n_classes, lambda c: len(c.dataset.classes))\n# Number of features in the input\ncalculate(Configs.in_features, lambda c: c.dataset.features.shape[1])\n\n\n@option(Configs.model)\ndef gat_model(c: Configs):\n    \"\"\"\n    Create GAT model\n    \"\"\"\n    return GAT(c.in_features, c.n_hidden, c.n_classes, c.n_heads, c.dropout).to(c.device)\n\n\n@option(Configs.optimizer)\ndef _optimizer(c: Configs):\n    \"\"\"\n    Create configurable optimizer\n    \"\"\"\n    opt_conf = OptimizerConfigs()\n    opt_conf.parameters = c.model.parameters()\n    return opt_conf\n\n\ndef main():\n    # Create configurations\n    conf = Configs()\n    # Create an experiment\n    experiment.create(name='gat')\n    # Calculate configurations.\n    experiment.configs(conf, {\n        # Adam optimizer\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 5e-3,\n        'optimizer.weight_decay': 5e-4,\n    })\n\n    # Start and watch the experiment\n    with experiment.start():\n        # Run the training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/graphs/gat/__init__.py": "\"\"\"\n---\ntitle: Graph Attention Networks (GAT)\nsummary: >\n A PyTorch implementation/tutorial of Graph Attention Networks.\n---\n\n# Graph Attention Networks (GAT)\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[Graph Attention Networks](https://arxiv.org/abs/1710.10903).\n\nGATs work on graph data.\nA graph consists of nodes and edges connecting nodes.\nFor example, in Cora dataset the nodes are research papers and the edges are citations that\nconnect the papers.\n\nGAT uses masked self-attention, kind of similar to [transformers](../../transformers/mha.html).\nGAT consists of graph attention layers stacked on top of each other.\nEach graph attention layer gets node embeddings as inputs and outputs transformed embeddings.\nThe node embeddings pay attention to the embeddings of other nodes it's connected to.\nThe details of graph attention layers are included alongside the implementation.\n\nHere is [the training code](experiment.html) for training\na two-layer GAT on Cora dataset.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass GraphAttentionLayer(Module):\n    \"\"\"\n    ## Graph attention layer\n\n    This is a single graph attention layer.\n    A GAT is made up of multiple such layers.\n\n    It takes\n    $$\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$$,\n    where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input\n    and outputs\n    $$\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$$,\n    where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, n_heads: int,\n                 is_concat: bool = True,\n                 dropout: float = 0.6,\n                 leaky_relu_negative_slope: float = 0.2):\n        \"\"\"\n        * `in_features`, $F$, is the number of input features per node\n        * `out_features`, $F'$, is the number of output features per node\n        * `n_heads`, $K$, is the number of attention heads\n        * `is_concat` whether the multi-head results should be concatenated or averaged\n        * `dropout` is the dropout probability\n        * `leaky_relu_negative_slope` is the negative slope for leaky relu activation\n        \"\"\"\n        super().__init__()\n\n        self.is_concat = is_concat\n        self.n_heads = n_heads\n\n        # Calculate the number of dimensions per head\n        if is_concat:\n            assert out_features % n_heads == 0\n            # If we are concatenating the multiple heads\n            self.n_hidden = out_features // n_heads\n        else:\n            # If we are averaging the multiple heads\n            self.n_hidden = out_features\n\n        # Linear layer for initial transformation;\n        # i.e. to transform the node embeddings before self-attention\n        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n        # Linear layer to compute attention score $e_{ij}$\n        self.attn = nn.Linear(self.n_hidden * 2, 1, bias=False)\n        # The activation for attention score $e_{ij}$\n        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n        # Softmax to compute attention $\\alpha_{ij}$\n        self.softmax = nn.Softmax(dim=1)\n        # Dropout layer to be applied for attention\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n        \"\"\"\n        * `h`, $\\mathbf{h}$ is the input node embeddings of shape `[n_nodes, in_features]`.\n        * `adj_mat` is the adjacency matrix of shape `[n_nodes, n_nodes, n_heads]`.\n        We use shape `[n_nodes, n_nodes, 1]` since the adjacency is the same for each head.\n\n        Adjacency matrix represent the edges (or connections) among nodes.\n        `adj_mat[i][j]` is `True` if there is an edge from node `i` to node `j`.\n        \"\"\"\n\n        # Number of nodes\n        n_nodes = h.shape[0]\n        # The initial transformation,\n        # $$\\overrightarrow{g^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$$\n        # for each head.\n        # We do single linear transformation and then split it up for each head.\n        g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n\n        # #### Calculate attention score\n        #\n        # We calculate these for each head $k$. *We have omitted $\\cdot^k$ for simplicity*.\n        #\n        # $$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =\n        # a(\\overrightarrow{g_i}, \\overrightarrow{g_j})$$\n        #\n        # $e_{ij}$ is the attention score (importance) from node $j$ to node $i$.\n        # We calculate this for each head.\n        #\n        # $a$ is the attention mechanism, that calculates the attention score.\n        # The paper concatenates\n        # $\\overrightarrow{g_i}$, $\\overrightarrow{g_j}$\n        # and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$\n        # followed by a $\\text{LeakyReLU}$.\n        #\n        # $$e_{ij} = \\text{LeakyReLU} \\Big(\n        # \\mathbf{a}^\\top \\Big[\n        # \\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}\n        # \\Big] \\Big)$$\n\n        # First we calculate\n        # $\\Big[\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j} \\Big]$\n        # for all pairs of $i, j$.\n        #\n        # `g_repeat` gets\n        # $$\\{\\overrightarrow{g_1}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_N},\n        # \\overrightarrow{g_1}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_N}, ...\\}$$\n        # where each node embedding is repeated `n_nodes` times.\n        g_repeat = g.repeat(n_nodes, 1, 1)\n        # `g_repeat_interleave` gets\n        # $$\\{\\overrightarrow{g_1}, \\overrightarrow{g_1}, \\dots, \\overrightarrow{g_1},\n        # \\overrightarrow{g_2}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_2}, ...\\}$$\n        # where each node embedding is repeated `n_nodes` times.\n        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n        # Now we concatenate to get\n        # $$\\{\\overrightarrow{g_1} \\Vert \\overrightarrow{g_1},\n        # \\overrightarrow{g_1} \\Vert \\overrightarrow{g_2},\n        # \\dots, \\overrightarrow{g_1}  \\Vert \\overrightarrow{g_N},\n        # \\overrightarrow{g_2} \\Vert \\overrightarrow{g_1},\n        # \\overrightarrow{g_2} \\Vert \\overrightarrow{g_2},\n        # \\dots, \\overrightarrow{g_2}  \\Vert \\overrightarrow{g_N}, ...\\}$$\n        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim=-1)\n        # Reshape so that `g_concat[i, j]` is $\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}$\n        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n\n        # Calculate\n        # $$e_{ij} = \\text{LeakyReLU} \\Big(\n        # \\mathbf{a}^\\top \\Big[\n        # \\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}\n        # \\Big] \\Big)$$\n        # `e` is of shape `[n_nodes, n_nodes, n_heads, 1]`\n        e = self.activation(self.attn(g_concat))\n        # Remove the last dimension of size `1`\n        e = e.squeeze(-1)\n\n        # The adjacency matrix should have shape\n        # `[n_nodes, n_nodes, n_heads]` or`[n_nodes, n_nodes, 1]`\n        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n        # Mask $e_{ij}$ based on adjacency matrix.\n        # $e_{ij}$ is set to $- \\infty$ if there is no edge from $i$ to $j$.\n        e = e.masked_fill(adj_mat == 0, float('-inf'))\n\n        # We then normalize attention scores (or coefficients)\n        # $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =\n        # \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n        #\n        # where $\\mathcal{N}_i$ is the set of nodes connected to $i$.\n        #\n        # We do this by setting unconnected $e_{ij}$ to $- \\infty$ which\n        # makes $\\exp(e_{ij}) \\sim 0$ for unconnected pairs.\n        a = self.softmax(e)\n\n        # Apply dropout regularization\n        a = self.dropout(a)\n\n        # Calculate final output for each head\n        # $$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{g^k_j}$$\n        #\n        # *Note:* The paper includes the final activation $\\sigma$ in $\\overrightarrow{h_i}$\n        # We have omitted this from the Graph Attention Layer implementation\n        # and use it on the GAT model to match with how other PyTorch modules are defined -\n        # activation as a separate layer.\n        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n\n        # Concatenate the heads\n        if self.is_concat:\n            # $$\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n        # Take the mean of the heads\n        else:\n            # $$\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n            return attn_res.mean(dim=1)\n", "labml_nn/graphs/gatv2/experiment.py": "\"\"\"\n---\ntitle: Train a Graph Attention Network v2 (GATv2) on Cora dataset\nsummary: >\n  This trains is a  Graph Attention Network v2 (GATv2) on Cora dataset\n---\n\n# Train a Graph Attention Network v2 (GATv2) on Cora dataset\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_nn.graphs.gat.experiment import Configs as GATConfigs\nfrom labml_nn.graphs.gatv2 import GraphAttentionV2Layer\n\n\nclass GATv2(Module):\n    \"\"\"\n    ## Graph Attention Network v2 (GATv2)\n\n    This graph attention network has two [graph attention layers](index.html).\n    \"\"\"\n\n    def __init__(self, in_features: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float,\n                 share_weights: bool = True):\n        \"\"\"\n        * `in_features` is the number of features per node\n        * `n_hidden` is the number of features in the first graph attention layer\n        * `n_classes` is the number of classes\n        * `n_heads` is the number of heads in the graph attention layers\n        * `dropout` is the dropout probability\n        * `share_weights` if set to True, the same matrix will be applied to the source and the target node of every edge\n        \"\"\"\n        super().__init__()\n\n        # First graph attention layer where we concatenate the heads\n        self.layer1 = GraphAttentionV2Layer(in_features, n_hidden, n_heads,\n                                            is_concat=True, dropout=dropout, share_weights=share_weights)\n        # Activation function after first graph attention layer\n        self.activation = nn.ELU()\n        # Final graph attention layer where we average the heads\n        self.output = GraphAttentionV2Layer(n_hidden, n_classes, 1,\n                                            is_concat=False, dropout=dropout, share_weights=share_weights)\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n        \"\"\"\n        * `x` is the features vectors of shape `[n_nodes, in_features]`\n        * `adj_mat` is the adjacency matrix of the form\n         `[n_nodes, n_nodes, n_heads]` or `[n_nodes, n_nodes, 1]`\n        \"\"\"\n        # Apply dropout to the input\n        x = self.dropout(x)\n        # First graph attention layer\n        x = self.layer1(x, adj_mat)\n        # Activation function\n        x = self.activation(x)\n        # Dropout\n        x = self.dropout(x)\n        # Output layer (without activation) for logits\n        return self.output(x, adj_mat)\n\n\nclass Configs(GATConfigs):\n    \"\"\"\n    ## Configurations\n\n    Since the experiment is same as [GAT experiment](../gat/experiment.html) but with\n    [GATv2 model](index.html) we extend the same configs and change the model.\n    \"\"\"\n\n    # Whether to share weights for source and target nodes of edges\n    share_weights: bool = False\n    # Set the model\n    model: GATv2 = 'gat_v2_model'\n\n\n@option(Configs.model)\ndef gat_v2_model(c: Configs):\n    \"\"\"\n    Create GATv2 model\n    \"\"\"\n    return GATv2(c.in_features, c.n_hidden, c.n_classes, c.n_heads, c.dropout, c.share_weights).to(c.device)\n\n\ndef main():\n    # Create configurations\n    conf = Configs()\n    # Create an experiment\n    experiment.create(name='gatv2')\n    # Calculate configurations.\n    experiment.configs(conf, {\n        # Adam optimizer\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 5e-3,\n        'optimizer.weight_decay': 5e-4,\n\n        'dropout': 0.7,\n    })\n\n    # Start and watch the experiment\n    with experiment.start():\n        # Run the training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/graphs/gatv2/__init__.py": "\"\"\"\n---\ntitle: Graph Attention Networks v2 (GATv2)\nsummary: >\n A PyTorch implementation/tutorial of Graph Attention Networks v2.\n---\n# Graph Attention Networks v2 (GATv2)\nThis is a [PyTorch](https://pytorch.org) implementation of the GATv2 operator from the paper\n[How Attentive are Graph Attention Networks?](https://arxiv.org/abs/2105.14491).\n\nGATv2s work on graph data similar to [GAT](../gat/index.html).\nA graph consists of nodes and edges connecting nodes.\nFor example, in Cora dataset the nodes are research papers and the edges are citations that\nconnect the papers.\n\nThe GATv2 operator fixes the static attention problem of the standard [GAT](../gat/index.html).\nStatic attention is when the attention to the key nodes has the same rank (order) for any query node.\n[GAT](../gat/index.html) computes attention from query node $i$ to key node $j$ as,\n\n\\begin{align}\ne_{ij} &= \\text{LeakyReLU} \\Big(\\mathbf{a}^\\top \\Big[\n \\mathbf{W} \\overrightarrow{h_i} \\Vert  \\mathbf{W} \\overrightarrow{h_j}\n\\Big] \\Big) \\\\\n&=\n\\text{LeakyReLU} \\Big(\\mathbf{a}_1^\\top  \\mathbf{W} \\overrightarrow{h_i} +\n \\mathbf{a}_2^\\top  \\mathbf{W} \\overrightarrow{h_j}\n\\Big)\n\\end{align}\n\nNote that for any query node $i$, the attention rank ($argsort$) of keys depends only\non $\\mathbf{a}_2^\\top  \\mathbf{W} \\overrightarrow{h_j}$.\nTherefore the attention rank of keys remains the same (*static*) for all queries.\n\nGATv2 allows dynamic attention by changing the attention mechanism,\n\n\\begin{align}\ne_{ij} &= \\mathbf{a}^\\top \\text{LeakyReLU} \\Big( \\mathbf{W} \\Big[\n \\overrightarrow{h_i} \\Vert  \\overrightarrow{h_j}\n\\Big] \\Big) \\\\\n&= \\mathbf{a}^\\top \\text{LeakyReLU} \\Big(\n\\mathbf{W}_l \\overrightarrow{h_i} +  \\mathbf{W}_r \\overrightarrow{h_j}\n\\Big)\n\\end{align}\n\nThe paper shows that GATs static attention mechanism fails on some graph problems\nwith a synthetic dictionary lookup dataset.\nIt's a fully connected bipartite graph where one set of nodes (query nodes)\nhave a key associated with it\nand the other set of nodes have both a key and a value associated with it.\nThe goal is to predict the values of query nodes.\nGAT fails on this task because of its limited static attention.\n\nHere is [the training code](experiment.html) for training\na two-layer GATv2 on Cora dataset.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass GraphAttentionV2Layer(Module):\n    \"\"\"\n    ## Graph attention v2 layer\n    This is a single graph attention v2 layer.\n    A GATv2 is made up of multiple such layers.\n    It takes\n    $$\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$$,\n    where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input\n    and outputs\n    $$\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$$,\n    where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n    \"\"\"\n\n    def __init__(self, in_features: int, out_features: int, n_heads: int,\n                 is_concat: bool = True,\n                 dropout: float = 0.6,\n                 leaky_relu_negative_slope: float = 0.2,\n                 share_weights: bool = False):\n        \"\"\"\n        * `in_features`, $F$, is the number of input features per node\n        * `out_features`, $F'$, is the number of output features per node\n        * `n_heads`, $K$, is the number of attention heads\n        * `is_concat` whether the multi-head results should be concatenated or averaged\n        * `dropout` is the dropout probability\n        * `leaky_relu_negative_slope` is the negative slope for leaky relu activation\n        * `share_weights` if set to `True`, the same matrix will be applied to the source and the target node of every edge\n        \"\"\"\n        super().__init__()\n\n        self.is_concat = is_concat\n        self.n_heads = n_heads\n        self.share_weights = share_weights\n\n        # Calculate the number of dimensions per head\n        if is_concat:\n            assert out_features % n_heads == 0\n            # If we are concatenating the multiple heads\n            self.n_hidden = out_features // n_heads\n        else:\n            # If we are averaging the multiple heads\n            self.n_hidden = out_features\n\n        # Linear layer for initial source transformation;\n        # i.e. to transform the source node embeddings before self-attention\n        self.linear_l = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n        # If `share_weights` is `True` the same linear layer is used for the target nodes\n        if share_weights:\n            self.linear_r = self.linear_l\n        else:\n            self.linear_r = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n        # Linear layer to compute attention score $e_{ij}$\n        self.attn = nn.Linear(self.n_hidden, 1, bias=False)\n        # The activation for attention score $e_{ij}$\n        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n        # Softmax to compute attention $\\alpha_{ij}$\n        self.softmax = nn.Softmax(dim=1)\n        # Dropout layer to be applied for attention\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n        \"\"\"\n        * `h`, $\\mathbf{h}$ is the input node embeddings of shape `[n_nodes, in_features]`.\n        * `adj_mat` is the adjacency matrix of shape `[n_nodes, n_nodes, n_heads]`.\n        We use shape `[n_nodes, n_nodes, 1]` since the adjacency is the same for each head.\n        Adjacency matrix represent the edges (or connections) among nodes.\n        `adj_mat[i][j]` is `True` if there is an edge from node `i` to node `j`.\n        \"\"\"\n\n        # Number of nodes\n        n_nodes = h.shape[0]\n        # The initial transformations,\n        # $$\\overrightarrow{{g_l}^k_i} = \\mathbf{W_l}^k \\overrightarrow{h_i}$$\n        # $$\\overrightarrow{{g_r}^k_i} = \\mathbf{W_r}^k \\overrightarrow{h_i}$$\n        # for each head.\n        # We do two linear transformations and then split it up for each head.\n        g_l = self.linear_l(h).view(n_nodes, self.n_heads, self.n_hidden)\n        g_r = self.linear_r(h).view(n_nodes, self.n_heads, self.n_hidden)\n\n        # #### Calculate attention score\n        #\n        # We calculate these for each head $k$. *We have omitted $\\cdot^k$ for simplicity*.\n        #\n        # $$e_{ij} = a(\\mathbf{W_l} \\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j}) =\n        # a(\\overrightarrow{{g_l}_i}, \\overrightarrow{{g_r}_j})$$\n        #\n        # $e_{ij}$ is the attention score (importance) from node $j$ to node $i$.\n        # We calculate this for each head.\n        #\n        # $a$ is the attention mechanism, that calculates the attention score.\n        # The paper sums\n        # $\\overrightarrow{{g_l}_i}$, $\\overrightarrow{{g_r}_j}$\n        # followed by a $\\text{LeakyReLU}$ \n        # and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{F'}$\n        # \n        #\n        # $$e_{ij} = \\mathbf{a}^\\top \\text{LeakyReLU} \\Big(\n        # \\Big[\n        # \\overrightarrow{{g_l}_i} + \\overrightarrow{{g_r}_j}\n        # \\Big] \\Big)$$\n        # Note: The paper desrcibes $e_{ij}$ as         \n        # $$e_{ij} = \\mathbf{a}^\\top \\text{LeakyReLU} \\Big( \\mathbf{W}\n        # \\Big[\n        # \\overrightarrow{h_i} \\Vert \\overrightarrow{h_j}\n        # \\Big] \\Big)$$\n        # which is equivalent to the definition we use here.\n\n        # First we calculate\n        # $\\Big[\\overrightarrow{{g_l}_i} + \\overrightarrow{{g_r}_j} \\Big]$\n        # for all pairs of $i, j$.\n        #\n        # `g_l_repeat` gets\n        # $$\\{\\overrightarrow{{g_l}_1}, \\overrightarrow{{g_l}_2}, \\dots, \\overrightarrow{{g_l}_N},\n        # \\overrightarrow{{g_l}_1}, \\overrightarrow{{g_l}_2}, \\dots, \\overrightarrow{{g_l}_N}, ...\\}$$\n        # where each node embedding is repeated `n_nodes` times.\n        g_l_repeat = g_l.repeat(n_nodes, 1, 1)\n        # `g_r_repeat_interleave` gets\n        # $$\\{\\overrightarrow{{g_r}_1}, \\overrightarrow{{g_r}_1}, \\dots, \\overrightarrow{{g_r}_1},\n        # \\overrightarrow{{g_r}_2}, \\overrightarrow{{g_r}_2}, \\dots, \\overrightarrow{{g_r}_2}, ...\\}$$\n        # where each node embedding is repeated `n_nodes` times.\n        g_r_repeat_interleave = g_r.repeat_interleave(n_nodes, dim=0)\n        # Now we add the two tensors to get\n        # $$\\{\\overrightarrow{{g_l}_1} + \\overrightarrow{{g_r}_1},\n        # \\overrightarrow{{g_l}_1} + \\overrightarrow{{g_r}_2},\n        # \\dots, \\overrightarrow{{g_l}_1}  +\\overrightarrow{{g_r}_N},\n        # \\overrightarrow{{g_l}_2} + \\overrightarrow{{g_r}_1},\n        # \\overrightarrow{{g_l}_2} + \\overrightarrow{{g_r}_2},\n        # \\dots, \\overrightarrow{{g_l}_2}  + \\overrightarrow{{g_r}_N}, ...\\}$$\n        g_sum = g_l_repeat + g_r_repeat_interleave\n        # Reshape so that `g_sum[i, j]` is $\\overrightarrow{{g_l}_i} + \\overrightarrow{{g_r}_j}$\n        g_sum = g_sum.view(n_nodes, n_nodes, self.n_heads, self.n_hidden)\n\n        # Calculate\n        # $$e_{ij} = \\mathbf{a}^\\top \\text{LeakyReLU} \\Big(\n        # \\Big[\n        # \\overrightarrow{{g_l}_i} + \\overrightarrow{{g_r}_j}\n        # \\Big] \\Big)$$\n        # `e` is of shape `[n_nodes, n_nodes, n_heads, 1]`\n        e = self.attn(self.activation(g_sum))\n        # Remove the last dimension of size `1`\n        e = e.squeeze(-1)\n\n        # The adjacency matrix should have shape\n        # `[n_nodes, n_nodes, n_heads]` or`[n_nodes, n_nodes, 1]`\n        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n        # Mask $e_{ij}$ based on adjacency matrix.\n        # $e_{ij}$ is set to $- \\infty$ if there is no edge from $i$ to $j$.\n        e = e.masked_fill(adj_mat == 0, float('-inf'))\n\n        # We then normalize attention scores (or coefficients)\n        # $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =\n        # \\frac{\\exp(e_{ij})}{\\sum_{j' \\in \\mathcal{N}_i} \\exp(e_{ij'})}$$\n        #\n        # where $\\mathcal{N}_i$ is the set of nodes connected to $i$.\n        #\n        # We do this by setting unconnected $e_{ij}$ to $- \\infty$ which\n        # makes $\\exp(e_{ij}) \\sim 0$ for unconnected pairs.\n        a = self.softmax(e)\n\n        # Apply dropout regularization\n        a = self.dropout(a)\n\n        # Calculate final output for each head\n        # $$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{{g_r}_{j,k}}$$\n        attn_res = torch.einsum('ijh,jhf->ihf', a, g_r)\n\n        # Concatenate the heads\n        if self.is_concat:\n            # $$\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n        # Take the mean of the heads\n        else:\n            # $$\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n            return attn_res.mean(dim=1)\n", "labml_nn/utils/tokenizer.py": "from typing import Callable\n\nfrom labml.configs import BaseConfigs, option\n\n\nclass TokenizerConfigs(BaseConfigs):\n    \"\"\"\n    <a id=\"TokenizerConfigs\"></a>\n\n    ## Tokenizer Configurations\n    \"\"\"\n\n    tokenizer: Callable = 'character'\n\n    def __init__(self):\n        super().__init__(_primary='tokenizer')\n\n\n@option(TokenizerConfigs.tokenizer)\ndef basic_english():\n    \"\"\"\n    ### Basic  english tokenizer\n\n    We use character level tokenizer in this experiment.\n    You can switch by setting,\n\n    ```\n    'tokenizer': 'basic_english'\n    ```\n\n    in the configurations dictionary when starting the experiment.\n\n    \"\"\"\n    from torchtext.data import get_tokenizer\n    return get_tokenizer('basic_english')\n\n\ndef character_tokenizer(x: str):\n    \"\"\"\n    ### Character level tokenizer\n    \"\"\"\n    return list(x)\n\n\n@option(TokenizerConfigs.tokenizer)\ndef character():\n    \"\"\"\n    Character level tokenizer configuration\n    \"\"\"\n    return character_tokenizer\n", "labml_nn/utils/__init__.py": "\"\"\"\n---\ntitle: Utilities\nsummary: A bunch of utility functions and classes\n---\n\n# Utilities\n\"\"\"\n\nimport copy\n\nfrom torch.utils.data import Dataset, IterableDataset\n\nfrom labml_helpers.module import M, TypedModuleList\n\n\ndef clone_module_list(module: M, n: int) -> TypedModuleList[M]:\n    \"\"\"\n    ## Clone Module\n\n    Make a `nn.ModuleList` with clones of a given module\n    \"\"\"\n    return TypedModuleList([copy.deepcopy(module) for _ in range(n)])\n\n\ndef cycle_dataloader(data_loader):\n    \"\"\"\n    <a id=\"cycle_dataloader\"></a>\n\n    ## Cycle Data Loader\n\n    Infinite loader that recycles the data loader after each epoch\n    \"\"\"\n    while True:\n        for batch in data_loader:\n            yield batch\n\n\nclass MapStyleDataset(Dataset):\n    \"\"\"\n    <a id=\"map_style_dataset\"></a>\n\n    ## Map Style Dataset\n\n    This converts an [`IterableDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)\n    to a [map-style dataset](https://pytorch.org/docs/stable/data.html#map-style-datasets)\n    so that we can shuffle the dataset.\n\n    *This only works when the dataset size is small and can be held in memory.*\n    \"\"\"\n\n    def __init__(self, dataset: IterableDataset):\n        # Load the data to memory\n        self.data = [d for d in dataset]\n\n    def __getitem__(self, idx: int):\n        \"\"\"Get a sample by index\"\"\"\n        return self.data[idx]\n\n    def __iter__(self):\n        \"\"\"Create an iterator\"\"\"\n        return iter(self.data)\n\n    def __len__(self):\n        \"\"\"Size of the dataset\"\"\"\n        return len(self.data)\n", "labml_nn/recurrent_highway_networks/__init__.py": "\"\"\"\n---\ntitle: Recurrent Highway Networks\nsummary: A simple PyTorch implementation/tutorial of Recurrent Highway Networks.\n---\n\n# Recurrent Highway Networks\n\nThis is a [PyTorch](https://pytorch.org) implementation of [Recurrent Highway Networks](https://arxiv.org/abs/1607.03474).\n\"\"\"\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass RHNCell(Module):\n    \"\"\"\n    ## Recurrent Highway Network Cell\n\n    This implements equations $(6) - (9)$.\n\n    $s_d^t = h_d^t \\odot g_d^t + s_{d - 1}^t \\odot c_d^t$\n\n    where\n\n    \\begin{align}\n    h_0^t &= \\tanh(lin_{hx}(x) + lin_{hs}(s_D^{t-1})) \\\\\n    g_0^t &= \\sigma(lin_{gx}(x) + lin_{gs}^1(s_D^{t-1})) \\\\\n    c_0^t &= \\sigma(lin_{cx}(x) + lin_{cs}^1(s_D^{t-1}))\n    \\end{align}\n\n    and for $0 < d < D$\n\n    \\begin{align}\n    h_d^t &= \\tanh(lin_{hs}^d(s_d^t)) \\\\\n    g_d^t &= \\sigma(lin_{gs}^d(s_d^t)) \\\\\n    c_d^t &= \\sigma(lin_{cs}^d(s_d^t))\n    \\end{align}\n\n    $\\odot$ stands for element-wise multiplication.\n\n    Here we have made a couple of changes to notations from the paper.\n    To avoid confusion with time, gate is represented with $g$,\n    which was $t$ in the paper.\n    To avoid confusion with multiple layers we use $d$ for depth and $D$ for\n    total depth instead of $l$ and $L$ from the paper.\n\n    We have also replaced the weight matrices and bias vectors from the equations with\n    linear transforms, because that's how the implementation is going to look like.\n\n    We implement weight tying, as described in paper, $c_d^t = 1 - g_d^t$.\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, depth: int):\n        \"\"\"\n        `input_size` is the feature length of the input and `hidden_size` is\n        the feature length of the cell.\n        `depth` is $D$.\n        \"\"\"\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.depth = depth\n        # We combine $lin_{hs}$ and $lin_{gs}$, with a single linear layer.\n        # We can then split the results to get the $lin_{hs}$ and $lin_{gs}$ components.\n        # This is the $lin_{hs}^d$ and $lin_{gs}^d$ for $0 \\leq d < D$.\n        self.hidden_lin = nn.ModuleList([nn.Linear(hidden_size, 2 * hidden_size) for _ in range(depth)])\n\n        # Similarly we combine $lin_{hx}$ and $lin_{gx}$.\n        self.input_lin = nn.Linear(input_size, 2 * hidden_size, bias=False)\n\n    def forward(self, x: torch.Tensor, s: torch.Tensor):\n        \"\"\"\n        `x` has shape `[batch_size, input_size]` and\n        `s` has shape `[batch_size, hidden_size]`.\n        \"\"\"\n\n        # Iterate $0 \\leq d < D$\n        for d in range(self.depth):\n            # We calculate the concatenation of linear transforms for $h$ and $g$\n            if d == 0:\n                # The input is used only when $d$ is $0$.\n                hg = self.input_lin(x) + self.hidden_lin[d](s)\n            else:\n                hg = self.hidden_lin[d](s)\n\n            # Use the first half of `hg` to get $h_d^t$\n            #\n            # \\begin{align}\n            # h_0^t &= \\tanh(lin_{hx}(x) + lin_{hs}(s_D^{t-1})) \\\\\n            # h_d^t &= \\tanh(lin_{hs}^d(s_d^t))\n            # \\end{align}\n            h = torch.tanh(hg[:, :self.hidden_size])\n            # Use the second half of `hg` to get $g_d^t$\n            #\n            # \\begin{align}\n            # g_0^t &= \\sigma(lin_{gx}(x) + lin_{gs}^1(s_D^{t-1})) \\\\\n            # g_d^t &= \\sigma(lin_{gs}^d(s_d^t))\n            # \\end{align}\n            g = torch.sigmoid(hg[:, self.hidden_size:])\n\n            s = h * g + s * (1 - g)\n\n        return s\n\n\nclass RHN(Module):\n    \"\"\"\n    ## Multilayer Recurrent Highway Network\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, depth: int, n_layers: int):\n        \"\"\"\n        Create a network of `n_layers` of recurrent highway network layers, each with depth `depth`, $D$.\n        \"\"\"\n\n        super().__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        # Create cells for each layer. Note that only the first layer gets the input directly.\n        # Rest of the layers get the input from the layer below\n        self.cells = nn.ModuleList([RHNCell(input_size, hidden_size, depth)] +\n                                   [RHNCell(hidden_size, hidden_size, depth) for _ in range(n_layers - 1)])\n\n    def forward(self, x: torch.Tensor, state: Optional[torch.Tensor] = None):\n        \"\"\"\n        `x` has shape `[seq_len, batch_size, input_size]` and\n        `state` has shape `[batch_size, hidden_size]`.\n        \"\"\"\n        time_steps, batch_size = x.shape[:2]\n\n        # Initialize the state if `None`\n        if state is None:\n            s = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n        else:\n            # Reverse stack the state to get the state of each layer\n            #\n            # \ud83d\udcdd You can just work with the tensor itself but this is easier to debug\n            s = torch.unbind(state)\n\n        # Array to collect the outputs of the final layer at each time step.\n        out = []\n\n        # Run through the network for each time step\n        for t in range(time_steps):\n            # Input to the first layer is the input itself\n            inp = x[t]\n            # Loop through the layers\n            for layer in range(self.n_layers):\n                # Get the state of the layer\n                s[layer] = self.cells[layer](inp, s[layer])\n                # Input to the next layer is the state of this layer\n                inp = s[layer]\n            # Collect the output of the final layer\n            out.append(s[-1])\n\n        # Stack the outputs and states\n        out = torch.stack(out)\n        s = torch.stack(s)\n\n        return out, s\n", "labml_nn/hypernetworks/experiment.py": "import torch\nimport torch.nn as nn\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml.utils.pytorch import get_modules\nfrom labml_helpers.module import Module\n\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.hypernetworks.hyper_lstm import HyperLSTM\nfrom labml_nn.lstm import LSTM\n\n\nclass AutoregressiveModel(Module):\n    \"\"\"\n    ## Auto regressive model\n    \"\"\"\n\n    def __init__(self, n_vocab: int, d_model: int, rnn_model: Module):\n        super().__init__()\n        # Token embedding module\n        self.src_embed = nn.Embedding(n_vocab, d_model)\n        self.lstm = rnn_model\n        self.generator = nn.Linear(d_model, n_vocab)\n\n    def forward(self, x: torch.Tensor):\n        x = self.src_embed(x)\n        # Embed the tokens (`src`) and run it through the the transformer\n        res, state = self.lstm(x)\n        # Generate logits of the next token\n        return self.generator(res), state\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    The default configs can and will be over-ridden when we start the experiment\n    \"\"\"\n\n    model: AutoregressiveModel\n    rnn_model: Module\n\n    d_model: int = 512\n    n_rhn: int = 16\n    n_z: int = 16\n\n\n@option(Configs.model)\ndef autoregressive_model(c: Configs):\n    \"\"\"\n    Initialize the auto-regressive model\n    \"\"\"\n    m = AutoregressiveModel(c.n_tokens, c.d_model, c.rnn_model)\n    return m.to(c.device)\n\n\n@option(Configs.rnn_model)\ndef hyper_lstm(c: Configs):\n    return HyperLSTM(c.d_model, c.d_model, c.n_rhn, c.n_z, 1)\n\n\n@option(Configs.rnn_model)\ndef lstm(c: Configs):\n    return LSTM(c.d_model, c.d_model, 1)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"hyper_lstm\", comment='')\n    # Create configs\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf,\n                       # A dictionary of configurations to override\n                       {'tokenizer': 'character',\n                        'text': 'tiny_shakespeare',\n                        'optimizer.learning_rate': 2.5e-4,\n                        'optimizer.optimizer': 'Adam',\n                        'prompt': 'It is',\n                        'prompt_separator': '',\n\n                        'rnn_model': 'hyper_lstm',\n\n                        'train_loader': 'shuffled_train_loader',\n                        'valid_loader': 'shuffled_valid_loader',\n\n                        'seq_len': 512,\n                        'epochs': 128,\n                        'batch_size': 2,\n                        'inner_iterations': 25})\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models(get_modules(conf))\n\n    # Start the experiment\n    with experiment.start():\n        # `TrainValidConfigs.run`\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/hypernetworks/__init__.py": "\"\"\"\n---\ntitle: HyperNetworks\nsummary: A PyTorch implementation/tutorial of HyperLSTM introduced in paper HyperNetworks.\n---\n\n## [HyperLSTM](hyper_lstm.html)\n\"\"\"", "labml_nn/hypernetworks/hyper_lstm.py": "\"\"\"\n---\ntitle: HyperNetworks - HyperLSTM\nsummary: A PyTorch implementation/tutorial of HyperLSTM introduced in paper HyperNetworks.\n---\n\n# HyperNetworks - HyperLSTM\n\nWe have implemented HyperLSTM introduced in paper\n[HyperNetworks](https://arxiv.org/abs/1609.09106), with annotations\nusing [PyTorch](https://pytorch.org).\n[This blog post](https://blog.otoro.net/2016/09/28/hyper-networks/)\nby David Ha gives a good explanation of HyperNetworks.\n\nWe have an experiment that trains a HyperLSTM to predict text on Shakespeare dataset.\nHere's the link to code: [`experiment.py`](experiment.html)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/hypernetworks/experiment.ipynb)\n\nHyperNetworks use a smaller network to generate weights of a larger network.\nThere are two variants: static hyper-networks and dynamic hyper-networks.\nStatic HyperNetworks have smaller networks that generate weights (kernels)\nof a convolutional network. Dynamic HyperNetworks generate parameters of a\nrecurrent neural network\nfor each step. This is an implementation of the latter.\n\n## Dynamic HyperNetworks\nIn a RNN the parameters stay constant for each step.\nDynamic HyperNetworks generate different parameters for each step.\nHyperLSTM has the structure of a LSTM but the parameters of\neach step are changed by a smaller LSTM network.\n\nIn the basic form, a Dynamic HyperNetwork has a smaller recurrent network that generates\na feature vector corresponding to each parameter tensor of the larger recurrent network.\nLet's say the larger network has some parameter $\\textcolor{cyan}{W_h}$ the smaller network generates a feature\nvector $z_h$ and we dynamically compute $\\textcolor{cyan}{W_h}$ as a linear transformation of $z_h$.\nFor instance $\\textcolor{cyan}{W_h} =  \\langle W_{hz}, z_h \\rangle$ where\n$W_{hz}$ is a 3-d tensor parameter and $\\langle . \\rangle$ is a tensor-vector multiplication.\n$z_h$ is usually a linear transformation of the output of the smaller recurrent network.\n\n### Weight scaling instead of computing\n\nLarge recurrent networks have large dynamically computed parameters.\nThese are calculated using linear transformation of feature vector $z$.\nAnd this transformation requires an even larger weight tensor.\nThat is, when $\\textcolor{cyan}{W_h}$ has shape $N_h \\times N_h$,\n$W_{hz}$ will be $N_h \\times N_h \\times N_z$.\n\nTo overcome this, we compute the weight parameters of the recurrent network by\ndynamically scaling each row of a matrix of same size.\n\n\\begin{align}\nd(z) = W_{hz} z_h \\\\\n\\\\\n\\textcolor{cyan}{W_h} =\n\\begin{pmatrix}\nd_0(z) W_{hd_0} \\\\\nd_1(z) W_{hd_1} \\\\\n... \\\\\nd_{N_h}(z) W_{hd_{N_h}} \\\\\n\\end{pmatrix}\n\\end{align}\n\nwhere $W_{hd}$ is a $N_h \\times N_h$ parameter matrix.\n\nWe can further optimize this when we compute $\\textcolor{cyan}{W_h} h$,\nas\n$$\\textcolor{lightgreen}{d(z) \\odot (W_{hd} h)}$$\nwhere $\\odot$ stands for element-wise multiplication.\n\"\"\"\n\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.lstm import LSTMCell\n\n\nclass HyperLSTMCell(Module):\n    \"\"\"\n    ## HyperLSTM Cell\n\n    For HyperLSTM the smaller network and the larger network both have the LSTM structure.\n    This is defined in Appendix A.2.2 in the paper.\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, hyper_size: int, n_z: int):\n        \"\"\"\n        `input_size` is the size of the input $x_t$,\n        `hidden_size` is the size of the LSTM, and\n        `hyper_size` is the size of the smaller LSTM that alters the weights of the larger outer LSTM.\n        `n_z` is the size of the feature vectors used to alter the LSTM weights.\n\n        We use the output of the smaller LSTM to compute $z_h^{i,f,g,o}$, $z_x^{i,f,g,o}$ and\n        $z_b^{i,f,g,o}$ using linear transformations.\n        We calculate $d_h^{i,f,g,o}(z_h^{i,f,g,o})$, $d_x^{i,f,g,o}(z_x^{i,f,g,o})$, and\n        $d_b^{i,f,g,o}(z_b^{i,f,g,o})$ from these, using linear transformations again.\n        These are then used to scale the rows of weight and bias tensors of the main LSTM.\n\n        \ud83d\udcdd Since the computation of $z$ and $d$ are two sequential linear transformations\n        these can be combined into a single linear transformation.\n        However we've implemented this separately so that it matches with the description\n        in the paper.\n        \"\"\"\n        super().__init__()\n\n        # The input to the hyperLSTM is\n        # $$\n        # \\hat{x}_t = \\begin{pmatrix}\n        # h_{t-1} \\\\\n        # x_t\n        # \\end{pmatrix}\n        # $$\n        # where $x_t$ is the input and $h_{t-1}$ is the output of the outer LSTM at previous step.\n        # So the input size is `hidden_size + input_size`.\n        #\n        # The output of hyperLSTM is $\\hat{h}_t$ and $\\hat{c}_t$.\n        self.hyper = LSTMCell(hidden_size + input_size, hyper_size, layer_norm=True)\n\n        # $$z_h^{i,f,g,o} = lin_{h}^{i,f,g,o}(\\hat{h}_t)$$\n        # \ud83e\udd14 In the paper it was specified as\n        # $$z_h^{i,f,g,o} = lin_{h}^{i,f,g,o}(\\hat{h}_{\\textcolor{red}{t-1}})$$\n        # I feel that it's a typo.\n        self.z_h = nn.Linear(hyper_size, 4 * n_z)\n        # $$z_x^{i,f,g,o} = lin_x^{i,f,g,o}(\\hat{h}_t)$$\n        self.z_x = nn.Linear(hyper_size, 4 * n_z)\n        # $$z_b^{i,f,g,o} = lin_b^{i,f,g,o}(\\hat{h}_t)$$\n        self.z_b = nn.Linear(hyper_size, 4 * n_z, bias=False)\n\n        # $$d_h^{i,f,g,o}(z_h^{i,f,g,o}) = lin_{dh}^{i,f,g,o}(z_h^{i,f,g,o})$$\n        d_h = [nn.Linear(n_z, hidden_size, bias=False) for _ in range(4)]\n        self.d_h = nn.ModuleList(d_h)\n        # $$d_x^{i,f,g,o}(z_x^{i,f,g,o}) = lin_{dx}^{i,f,g,o}(z_x^{i,f,g,o})$$\n        d_x = [nn.Linear(n_z, hidden_size, bias=False) for _ in range(4)]\n        self.d_x = nn.ModuleList(d_x)\n        # $$d_b^{i,f,g,o}(z_b^{i,f,g,o}) = lin_{db}^{i,f,g,o}(z_b^{i,f,g,o})$$\n        d_b = [nn.Linear(n_z, hidden_size) for _ in range(4)]\n        self.d_b = nn.ModuleList(d_b)\n\n        # The weight matrices $W_h^{i,f,g,o}$\n        self.w_h = nn.ParameterList([nn.Parameter(torch.zeros(hidden_size, hidden_size)) for _ in range(4)])\n        # The weight matrices $W_x^{i,f,g,o}$\n        self.w_x = nn.ParameterList([nn.Parameter(torch.zeros(hidden_size, input_size)) for _ in range(4)])\n\n        # Layer normalization\n        self.layer_norm = nn.ModuleList([nn.LayerNorm(hidden_size) for _ in range(4)])\n        self.layer_norm_c = nn.LayerNorm(hidden_size)\n\n    def forward(self, x: torch.Tensor,\n                h: torch.Tensor, c: torch.Tensor,\n                h_hat: torch.Tensor, c_hat: torch.Tensor):\n        # $$\n        # \\hat{x}_t = \\begin{pmatrix}\n        # h_{t-1} \\\\\n        # x_t\n        # \\end{pmatrix}\n        # $$\n        x_hat = torch.cat((h, x), dim=-1)\n        # $$\\hat{h}_t, \\hat{c}_t = lstm(\\hat{x}_t, \\hat{h}_{t-1}, \\hat{c}_{t-1})$$\n        h_hat, c_hat = self.hyper(x_hat, h_hat, c_hat)\n\n        # $$z_h^{i,f,g,o} = lin_{h}^{i,f,g,o}(\\hat{h}_t)$$\n        z_h = self.z_h(h_hat).chunk(4, dim=-1)\n        # $$z_x^{i,f,g,o} = lin_x^{i,f,g,o}(\\hat{h}_t)$$\n        z_x = self.z_x(h_hat).chunk(4, dim=-1)\n        # $$z_b^{i,f,g,o} = lin_b^{i,f,g,o}(\\hat{h}_t)$$\n        z_b = self.z_b(h_hat).chunk(4, dim=-1)\n\n        # We calculate $i$, $f$, $g$ and $o$ in a loop\n        ifgo = []\n        for i in range(4):\n            # $$d_h^{i,f,g,o}(z_h^{i,f,g,o}) = lin_{dh}^{i,f,g,o}(z_h^{i,f,g,o})$$\n            d_h = self.d_h[i](z_h[i])\n            # $$d_x^{i,f,g,o}(z_x^{i,f,g,o}) = lin_{dx}^{i,f,g,o}(z_x^{i,f,g,o})$$\n            d_x = self.d_x[i](z_x[i])\n\n            # \\begin{align}\n            # {i,f,g,o} = LN(&\\textcolor{lightgreen}{d_h^{i,f,g,o}(z_h) \\odot (W_h^{i,f,g,o} h_{t-1})} \\\\\n            #              + &\\textcolor{lightgreen}{d_x^{i,f,g,o}(z_x) \\odot (W_h^{i,f,g,o} x_t)} \\\\\n            #              + &d_b^{i,f,g,o}(z_b))\n            # \\end{align}\n            y = d_h * torch.einsum('ij,bj->bi', self.w_h[i], h) + \\\n                d_x * torch.einsum('ij,bj->bi', self.w_x[i], x) + \\\n                self.d_b[i](z_b[i])\n\n            ifgo.append(self.layer_norm[i](y))\n\n        # $$i_t, f_t, g_t, o_t$$\n        i, f, g, o = ifgo\n\n        # $$c_t = \\sigma(f_t) \\odot c_{t-1} + \\sigma(i_t) \\odot \\tanh(g_t) $$\n        c_next = torch.sigmoid(f) * c + torch.sigmoid(i) * torch.tanh(g)\n\n        # $$h_t = \\sigma(o_t) \\odot \\tanh(LN(c_t))$$\n        h_next = torch.sigmoid(o) * torch.tanh(self.layer_norm_c(c_next))\n\n        return h_next, c_next, h_hat, c_hat\n\n\nclass HyperLSTM(Module):\n    \"\"\"\n    # HyperLSTM module\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, hyper_size: int, n_z: int, n_layers: int):\n        \"\"\"\n        Create a network of `n_layers` of HyperLSTM.\n        \"\"\"\n\n        super().__init__()\n\n        # Store sizes to initialize state\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.hyper_size = hyper_size\n\n        # Create cells for each layer. Note that only the first layer gets the input directly.\n        # Rest of the layers get the input from the layer below\n        self.cells = nn.ModuleList([HyperLSTMCell(input_size, hidden_size, hyper_size, n_z)] +\n                                   [HyperLSTMCell(hidden_size, hidden_size, hyper_size, n_z) for _ in\n                                    range(n_layers - 1)])\n\n    def forward(self, x: torch.Tensor,\n                state: Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]] = None):\n        \"\"\"\n        * `x` has shape `[n_steps, batch_size, input_size]` and\n        * `state` is a tuple of $h, c, \\hat{h}, \\hat{c}$.\n         $h, c$ have shape `[batch_size, hidden_size]` and\n         $\\hat{h}, \\hat{c}$ have shape `[batch_size, hyper_size]`.\n        \"\"\"\n        n_steps, batch_size = x.shape[:2]\n\n        # Initialize the state with zeros if `None`\n        if state is None:\n            h = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n            c = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n            h_hat = [x.new_zeros(batch_size, self.hyper_size) for _ in range(self.n_layers)]\n            c_hat = [x.new_zeros(batch_size, self.hyper_size) for _ in range(self.n_layers)]\n        #\n        else:\n            (h, c, h_hat, c_hat) = state\n            # Reverse stack the tensors to get the states of each layer\n            #\n            # \ud83d\udcdd You can just work with the tensor itself but this is easier to debug\n            h, c = list(torch.unbind(h)), list(torch.unbind(c))\n            h_hat, c_hat = list(torch.unbind(h_hat)), list(torch.unbind(c_hat))\n\n        # Collect the outputs of the final layer at each step\n        out = []\n        for t in range(n_steps):\n            # Input to the first layer is the input itself\n            inp = x[t]\n            # Loop through the layers\n            for layer in range(self.n_layers):\n                # Get the state of the layer\n                h[layer], c[layer], h_hat[layer], c_hat[layer] = \\\n                    self.cells[layer](inp, h[layer], c[layer], h_hat[layer], c_hat[layer])\n                # Input to the next layer is the state of this layer\n                inp = h[layer]\n            # Collect the output $h$ of the final layer\n            out.append(h[-1])\n\n        # Stack the outputs and states\n        out = torch.stack(out)\n        h = torch.stack(h)\n        c = torch.stack(c)\n        h_hat = torch.stack(h_hat)\n        c_hat = torch.stack(c_hat)\n\n        #\n        return out, (h, c, h_hat, c_hat)\n", "labml_nn/activations/__init__.py": "\"\"\"\n---\ntitle: Neural Network Activation Functions\nsummary: >\n A set of PyTorch implementations/tutorials related to neural network activations\n---\n\n# Neural Networks Activations\n\n* [Fuzzy Tiling Activations](fta/index.html)\n* \ud83d\udea7 [Swish](swish/index.html)\n\"\"\"\n\nfrom .swish import Swish\n", "labml_nn/activations/swish.py": "import torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass Swish(Module):\n    def __init__(self):\n        super().__init__()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return x * self.sigmoid(x)\n", "labml_nn/activations/fta/experiment.py": "\"\"\"\n---\ntitle: Fuzzy Tiling Activation Experiment\nsummary: >\n Training a transformer with FTA in FFN on Tiny Shakespeare.\n---\n\n# [Fuzzy Tiling Activation](index.html) Experiment\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/activations/fta/experiment.ipynb)\n\nHere we train a transformer that uses [Fuzzy Tiling Activation](index.html) in the\n[Feed-Forward Network](../../transformers/feed_forward.html).\nWe use it for a language model and train it on Tiny Shakespeare dataset\nfor demonstration.\n\nHowever, this is probably not the ideal task for FTA, and we\nbelieve FTA is more suitable for modeling data with continuous variables.\n\"\"\"\n\nimport copy\n\nimport torch\nimport torch.nn as nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_nn.activations.fta import FTA\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers import MultiHeadAttention, TransformerLayer\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass FeedForwardFTA(nn.Module):\n    \"\"\"\n    ## FFN module with [FTA](index.html) activation\n    \"\"\"\n\n    def __init__(self, d_model: int, d_ff: int,\n                 activation: FTA,\n                 dropout: float = 0.1):\n        \"\"\"\n        * `d_model` is the number of features in a token embedding\n        * `d_ff` is the number of features in the hidden layer of the FFN\n        * `activation` is FTA activation module\n        * `dropout` is dropout probability for the hidden layer\n        \"\"\"\n        super().__init__()\n        # Layer one parameterized by weight $W_1$ and bias $b_1$\n        self.layer1 = nn.Linear(d_model, d_ff)\n        # Layer two parameterized by weight $W_1$ and bias $b_1$\n        self.layer2 = nn.Linear(d_ff * activation.expansion_factor, d_model)\n        # Hidden layer dropout\n        self.dropout = nn.Dropout(dropout)\n        # Activation function $f$\n        self.activation = activation\n\n    def forward(self, x: torch.Tensor):\n        # $f(x W_1 + b_1)$\n        x = self.activation(self.layer1(x))\n        # Apply dropout\n        x = self.dropout(x)\n        #\n        return self.layer2(x)\n\n\nclass AutoregressiveTransformer(Module):\n    \"\"\"\n    ## Auto-Regressive model\n\n    This is an autoregressive transformer model that uses Feed-Forward Networks with\n     (Fuzzy Tiling Activations)(index.html).\n    \"\"\"\n\n    def __init__(self, n_tokens: int, d_model: int, n_layers: int, layer: TransformerLayer):\n        \"\"\"\n        :param n_tokens: is the number of tokens in the vocabulary\n        :param d_model: is the embedding size\n        :param n_layers: is the number of transformer layers\n        :param layer: is the layer. We use `n_layers` copies of this for the transformer.\n        \"\"\"\n        super().__init__()\n        # Transformer with `n_layers` layers\n        self.transformer_layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layers)])\n\n        # Token embedding layer\n        self.emb = nn.Embedding(n_tokens, d_model)\n        # Readout layer\n        self.readout = nn.Linear(d_model, n_tokens)\n\n        # The mask will be initialized on the first call\n        self.mask = None\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: are the input tokens of shape `[seq_len, batch_size]`\n        \"\"\"\n        # Create auto-regressive mask\n        if self.mask is None or self.mask.size(0) != len(x):\n            # Subsequent mask, will mask out tokens from seeing future tokens\n            self.mask = subsequent_mask(len(x)).to(x.device)\n\n        # Get the token embeddings\n        x = self.emb(x)\n        # Transformer encoder\n        for layer in self.transformer_layers:\n            x = layer(x=x, mask=self.mask)\n        # Get logits\n        x = self.readout(x)\n\n        # Return results\n        return x, None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`NLPAutoRegressionConfigs`](../../experiments/nlp_autoregression.html#NLPAutoRegressionConfigs)\n    \"\"\"\n\n    # Model\n    model: AutoregressiveTransformer\n\n    # Number of layers\n    n_layers: int = 4\n\n    # $\\alpha$ and $\\beta$ for DeepNorm\n    deep_norm_alpha: float\n    deep_norm_beta: float\n\n    # Number of heads in the attention\n    n_heads: int = 4\n    # Embedding size\n    d_model: int = 256\n    # Size of each attention head\n    d_k: int = 16\n    # Feed forward layer size\n    d_ff: int = 256\n\n    # FTA\n    fta_lower_limit: float = -1.\n    fta_upper_limit: float = +1.\n    fta_delta: float = 0.2\n    fta_eta: float = 0.05\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    #### Initialize the model\n    \"\"\"\n\n    # Create FTA activation module\n    fta = FTA(c.fta_lower_limit, c.fta_upper_limit, c.fta_delta, c.fta_eta)\n    # Create the transformer.\n    # We re-use [`TransformerLayer`](../../transformers/models.html#TransformerLayer) and\n    # [`MultiHeadAttention`](../../transformers/mha.html) implementations.\n    m = AutoregressiveTransformer(c.n_tokens, c.d_model, c.n_layers,\n                                  TransformerLayer(d_model=c.d_model,\n                                                   feed_forward=FeedForwardFTA(d_model=c.d_model,\n                                                                               d_ff=c.d_ff,\n                                                                               activation=fta,\n                                                                               dropout=0.1),\n                                                   self_attn=MultiHeadAttention(c.n_heads, c.d_model,\n                                                                                dropout_prob=0.0),\n                                                   dropout_prob=0.0))\n\n    # Move to the device\n    return m.to(c.device)\n\n\ndef main():\n    \"\"\"\n    #### Create and run the experiment\n    \"\"\"\n    # Create experiment\n    experiment.create(name=\"fta\", writers={'screen', 'labml'})\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 256,\n        # Train for 32 epochs\n        'epochs': 32,\n        # Batch size $16$\n        'batch_size': 16,\n        # Switch between training and validation for $10$ times per epoch\n        'inner_iterations': 10,\n\n        # Adam optimizer with no warmup\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 3e-4,\n    })\n\n    # Set model(s) for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/activations/fta/__init__.py": "\"\"\"\n---\ntitle: Fuzzy Tiling Activations\nsummary: >\n  PyTorch implementation and tutorial of Fuzzy Tiling Activations from the\n  paper Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online.\n---\n\n# Fuzzy Tiling Activations (FTA)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/activations/fta/experiment.ipynb)\n\nThis is a [PyTorch](https://pytorch.org) implementation/tutorial of\n[Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online](https://arxiv.org/abs/1911.08068).\n\nFuzzy tiling activations are a form of sparse activations based on binning.\n\nBinning is classification of a scalar value into a bin based on intervals.\nOne problem with binning is that it gives zero gradients for most values (except at the boundary of bins).\nThe other is that binning loses precision if the bin intervals are large.\n\nFTA overcomes these disadvantages.\nInstead of hard boundaries like in Tiling Activations, FTA uses soft boundaries\nbetween bins.\nThis gives non-zero gradients for all or a wide range of values.\nAnd also doesn't lose precision since it's captured in partial values.\n\n#### Tiling Activations\n\n$\\mathbf{c}$ is the tiling vector,\n\n$$\\mathbf{c} = (l, l + \\delta, l + 2 \\delta, \\dots, u - 2 \\delta, u - \\delta)$$\n\nwhere $[l, u]$ is the input range, $\\delta$ is the bin size, and $u - l$ is divisible by $\\delta$.\n\nTiling activation is,\n\n$$\\phi(z) = 1 - I_+ \\big( \\max(\\mathbf{c} - z, 0) + \\max(z - \\delta - \\mathbf{c}) \\big)$$\n\nwhere $I_+(\\cdot)$ is the indicator function which gives $1$ if the input is positive and $0$ otherwise.\n\nNote that tiling activation gives zero gradients because it has hard boundaries.\n\n#### Fuzzy Tiling Activations\n\nThe fuzzy indicator function,\n\n$$I_{\\eta,+}(x) = I_+(\\eta - x) x + I_+ (x - \\eta)$$\n\nwhich increases linearly from $0$ to $1$ when $0 \\le x \\lt \\eta$\nand is equal to $1$ for $\\eta \\le x$.\n$\\eta$ is a hyper-parameter.\n\nFTA uses this to create soft boundaries between bins.\n\n$$\\phi_\\eta(z) = 1 - I_{\\eta,+} \\big( \\max(\\mathbf{c} - z, 0) + \\max(z - \\delta - \\mathbf{c}, 0) \\big)$$\n\n[Here's a simple experiment](experiment.html) that uses FTA in a transformer.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\n\nclass FTA(nn.Module):\n    \"\"\"\n    ### Fuzzy Tiling Activations (FTA)\n    \"\"\"\n\n    def __init__(self, lower_limit: float, upper_limit: float, delta: float, eta: float):\n        \"\"\"\n        :param lower_limit: is the lower limit $l$\n        :param upper_limit: is the upper limit $u$\n        :param delta: is the bin size $\\delta$\n        :param eta: is the parameter $\\eta$ that detemines the softness of the boundaries.\n        \"\"\"\n        super().__init__()\n        # Initialize tiling vector\n        # $$\\mathbf{c} = (l, l + \\delta, l + 2 \\delta, \\dots, u - 2 \\delta, u - \\delta)$$\n        self.c = nn.Parameter(torch.arange(lower_limit, upper_limit, delta), requires_grad=False)\n        # The input vector expands by a factor equal to the number of bins $\\frac{u - l}{\\delta}$\n        self.expansion_factor = len(self.c)\n        # $\\delta$\n        self.delta = delta\n        # $\\eta$\n        self.eta = eta\n\n    def fuzzy_i_plus(self, x: torch.Tensor):\n        \"\"\"\n        #### Fuzzy indicator function\n\n        $$I_{\\eta,+}(x) = I_+(\\eta - x) x + I_+ (x - \\eta)$$\n        \"\"\"\n        return (x <= self.eta) * x + (x > self.eta)\n\n    def forward(self, z: torch.Tensor):\n        # Add another dimension of size $1$.\n        # We will expand this into bins.\n        z = z.view(*z.shape, 1)\n\n        # $$\\phi_\\eta(z) = 1 - I_{\\eta,+} \\big( \\max(\\mathbf{c} - z, 0) + \\max(z - \\delta - \\mathbf{c}, 0) \\big)$$\n        z = 1. - self.fuzzy_i_plus(torch.clip(self.c - z, min=0.) + torch.clip(z - self.delta - self.c, min=0.))\n\n        # Reshape back to original number of dimensions.\n        # The last dimension size gets expanded by the number of bins, $\\frac{u - l}{\\delta}$.\n        return z.view(*z.shape[:-2], -1)\n\n\ndef _test():\n    \"\"\"\n    #### Code to test the FTA module\n    \"\"\"\n    from labml.logger import inspect\n\n    # Initialize\n    a = FTA(-10, 10, 2., 0.5)\n    # Print $\\mathbf{c}$\n    inspect(a.c)\n    # Print number of bins $\\frac{u - l}{\\delta}$\n    inspect(a.expansion_factor)\n\n    # Input $z$\n    z = torch.tensor([1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9., 10., 11.])\n    # Print $z$\n    inspect(z)\n    # Print $\\phi_\\eta(z)$\n    inspect(a(z))\n\n\nif __name__ == '__main__':\n    _test()\n", "labml_nn/lstm/__init__.py": "\"\"\"\n---\ntitle: Long Short-Term Memory (LSTM)\nsummary: A simple PyTorch implementation/tutorial of Long Short-Term Memory (LSTM) modules.\n---\n\n# Long Short-Term Memory (LSTM)\n\nThis is a [PyTorch](https://pytorch.org) implementation of Long Short-Term Memory.\n\"\"\"\n\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass LSTMCell(Module):\n    \"\"\"\n    ## Long Short-Term Memory Cell\n\n    LSTM Cell computes $c$, and $h$. $c$ is like the long-term memory,\n    and $h$ is like the short term memory.\n    We use the input $x$ and $h$ to update the long term memory.\n    In the update, some features of $c$ are cleared with a forget gate $f$,\n    and some features $i$ are added through a gate $g$.\n\n    The new short term memory is the $\\tanh$ of the long-term memory\n    multiplied by the output gate $o$.\n\n    Note that the cell doesn't look at long term memory $c$ when doing the update. It only modifies it.\n    Also $c$ never goes through a linear transformation.\n    This is what solves vanishing and exploding gradients.\n\n    Here's the update rule.\n\n    \\begin{align}\n    c_t &= \\sigma(f_t) \\odot c_{t-1} + \\sigma(i_t) \\odot \\tanh(g_t) \\\\\n    h_t &= \\sigma(o_t) \\odot \\tanh(c_t)\n    \\end{align}\n\n    $\\odot$ stands for element-wise multiplication.\n\n    Intermediate values and gates are computed as linear transformations of the hidden\n    state and input.\n\n    \\begin{align}\n    i_t &= lin_x^i(x_t) + lin_h^i(h_{t-1}) \\\\\n    f_t &= lin_x^f(x_t) + lin_h^f(h_{t-1}) \\\\\n    g_t &= lin_x^g(x_t) + lin_h^g(h_{t-1}) \\\\\n    o_t &= lin_x^o(x_t) + lin_h^o(h_{t-1})\n    \\end{align}\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, layer_norm: bool = False):\n        super().__init__()\n\n        # These are the linear layer to transform the `input` and `hidden` vectors.\n        # One of them doesn't need a bias since we add the transformations.\n\n        # This combines $lin_x^i$, $lin_x^f$, $lin_x^g$, and $lin_x^o$ transformations.\n        self.hidden_lin = nn.Linear(hidden_size, 4 * hidden_size)\n        # This combines $lin_h^i$, $lin_h^f$, $lin_h^g$, and $lin_h^o$ transformations.\n        self.input_lin = nn.Linear(input_size, 4 * hidden_size, bias=False)\n\n        # Whether to apply layer normalizations.\n        #\n        # Applying layer normalization gives better results.\n        # $i$, $f$, $g$ and $o$ embeddings are normalized and $c_t$ is normalized in\n        # $h_t = o_t \\odot \\tanh(\\mathop{LN}(c_t))$\n        if layer_norm:\n            self.layer_norm = nn.ModuleList([nn.LayerNorm(hidden_size) for _ in range(4)])\n            self.layer_norm_c = nn.LayerNorm(hidden_size)\n        else:\n            self.layer_norm = nn.ModuleList([nn.Identity() for _ in range(4)])\n            self.layer_norm_c = nn.Identity()\n\n    def forward(self, x: torch.Tensor, h: torch.Tensor, c: torch.Tensor):\n        # We compute the linear transformations for $i_t$, $f_t$, $g_t$ and $o_t$\n        # using the same linear layers.\n        ifgo = self.hidden_lin(h) + self.input_lin(x)\n        # Each layer produces an output of 4 times the `hidden_size` and we split them\n        ifgo = ifgo.chunk(4, dim=-1)\n\n        # Apply layer normalization (not in original paper, but gives better results)\n        ifgo = [self.layer_norm[i](ifgo[i]) for i in range(4)]\n\n        # $$i_t, f_t, g_t, o_t$$\n        i, f, g, o = ifgo\n\n        # $$c_t = \\sigma(f_t) \\odot c_{t-1} + \\sigma(i_t) \\odot \\tanh(g_t) $$\n        c_next = torch.sigmoid(f) * c + torch.sigmoid(i) * torch.tanh(g)\n\n        # $$h_t = \\sigma(o_t) \\odot \\tanh(c_t)$$\n        # Optionally, apply layer norm to $c_t$\n        h_next = torch.sigmoid(o) * torch.tanh(self.layer_norm_c(c_next))\n\n        return h_next, c_next\n\n\nclass LSTM(Module):\n    \"\"\"\n    ## Multilayer LSTM\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, n_layers: int):\n        \"\"\"\n        Create a network of `n_layers` of LSTM.\n        \"\"\"\n\n        super().__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        # Create cells for each layer. Note that only the first layer gets the input directly.\n        # Rest of the layers get the input from the layer below\n        self.cells = nn.ModuleList([LSTMCell(input_size, hidden_size)] +\n                                   [LSTMCell(hidden_size, hidden_size) for _ in range(n_layers - 1)])\n\n    def forward(self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n        \"\"\"\n        `x` has shape `[n_steps, batch_size, input_size]` and\n        `state` is a tuple of $h$ and $c$, each with a shape of `[batch_size, hidden_size]`.\n        \"\"\"\n        n_steps, batch_size = x.shape[:2]\n\n        # Initialize the state if `None`\n        if state is None:\n            h = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n            c = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n        else:\n            (h, c) = state\n            # Reverse stack the tensors to get the states of each layer\n            #\n            # \ud83d\udcdd You can just work with the tensor itself but this is easier to debug\n            h, c = list(torch.unbind(h)), list(torch.unbind(c))\n\n        # Array to collect the outputs of the final layer at each time step.\n        out = []\n        for t in range(n_steps):\n            # Input to the first layer is the input itself\n            inp = x[t]\n            # Loop through the layers\n            for layer in range(self.n_layers):\n                # Get the state of the layer\n                h[layer], c[layer] = self.cells[layer](inp, h[layer], c[layer])\n                # Input to the next layer is the state of this layer\n                inp = h[layer]\n            # Collect the output $h$ of the final layer\n            out.append(h[-1])\n\n        # Stack the outputs and states\n        out = torch.stack(out)\n        h = torch.stack(h)\n        c = torch.stack(c)\n\n        return out, (h, c)\n", "labml_nn/rl/game.py": "\"\"\"\n---\ntitle: Atari wrapper with multi-processing\nsummary: This implements the Atari games with multi-processing.\n---\n\n# Atari wrapper with multi-processing\n\"\"\"\nimport multiprocessing\nimport multiprocessing.connection\n\nimport cv2\nimport gym\nimport numpy as np\n\n\nclass Game:\n    \"\"\"\n    <a id=\"GameEnvironment\"></a>\n\n    ## Game environment\n\n    This is a wrapper for OpenAI gym game environment.\n    We do a few things here:\n\n    1. Apply the same action on four frames and get the last frame\n    2. Convert observation frames to gray and scale it to (84, 84)\n    3. Stack four frames of the last four actions\n    4. Add episode information (total reward for the entire episode) for monitoring\n    5. Restrict an episode to a single life (game has 5 lives, we reset after every single life)\n\n    #### Observation format\n    Observation is tensor of size (4, 84, 84). It is four frames\n    (images of the game screen) stacked on first axis.\n    i.e, each channel is a frame.\n    \"\"\"\n\n    def __init__(self, seed: int):\n        # create environment\n        self.env = gym.make('BreakoutNoFrameskip-v4')\n        self.env.seed(seed)\n\n        # tensor for a stack of 4 frames\n        self.obs_4 = np.zeros((4, 84, 84))\n\n        # buffer to keep the maximum of last 2 frames\n        self.obs_2_max = np.zeros((2, 84, 84))\n\n        # keep track of the episode rewards\n        self.rewards = []\n        # and number of lives left\n        self.lives = 0\n\n    def step(self, action):\n        \"\"\"\n        ### Step\n        Executes `action` for 4 time steps and\n         returns a tuple of (observation, reward, done, episode_info).\n\n        * `observation`: stacked 4 frames (this frame and frames for last 3 actions)\n        * `reward`: total reward while the action was executed\n        * `done`: whether the episode finished (a life lost)\n        * `episode_info`: episode information if completed\n        \"\"\"\n\n        reward = 0.\n        done = None\n\n        # run for 4 steps\n        for i in range(4):\n            # execute the action in the OpenAI Gym environment\n            obs, r, done, info = self.env.step(action)\n\n            if i >= 2:\n                self.obs_2_max[i % 2] = self._process_obs(obs)\n\n            reward += r\n\n            # get number of lives left\n            lives = self.env.unwrapped.ale.lives()\n            # reset if a life is lost\n            if lives < self.lives:\n                done = True\n                break\n\n        # maintain rewards for each step\n        self.rewards.append(reward)\n\n        if done:\n            # if finished, set episode information if episode is over, and reset\n            episode_info = {\"reward\": sum(self.rewards), \"length\": len(self.rewards)}\n            self.reset()\n        else:\n            episode_info = None\n\n            # get the max of last two frames\n            obs = self.obs_2_max.max(axis=0)\n\n            # push it to the stack of 4 frames\n            self.obs_4 = np.roll(self.obs_4, shift=-1, axis=0)\n            self.obs_4[-1] = obs\n\n        return self.obs_4, reward, done, episode_info\n\n    def reset(self):\n        \"\"\"\n        ### Reset environment\n        Clean up episode info and 4 frame stack\n        \"\"\"\n\n        # reset OpenAI Gym environment\n        obs = self.env.reset()\n\n        # reset caches\n        obs = self._process_obs(obs)\n        for i in range(4):\n            self.obs_4[i] = obs\n        self.rewards = []\n\n        self.lives = self.env.unwrapped.ale.lives()\n\n        return self.obs_4\n\n    @staticmethod\n    def _process_obs(obs):\n        \"\"\"\n        #### Process game frames\n        Convert game frames to gray and rescale to 84x84\n        \"\"\"\n        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)\n        return obs\n\n\ndef worker_process(remote: multiprocessing.connection.Connection, seed: int):\n    \"\"\"\n    ##Worker Process\n\n    Each worker process runs this method\n    \"\"\"\n\n    # create game\n    game = Game(seed)\n\n    # wait for instructions from the connection and execute them\n    while True:\n        cmd, data = remote.recv()\n        if cmd == \"step\":\n            remote.send(game.step(data))\n        elif cmd == \"reset\":\n            remote.send(game.reset())\n        elif cmd == \"close\":\n            remote.close()\n            break\n        else:\n            raise NotImplementedError\n\n\nclass Worker:\n    \"\"\"\n    Creates a new worker and runs it in a separate process.\n    \"\"\"\n\n    def __init__(self, seed):\n        self.child, parent = multiprocessing.Pipe()\n        self.process = multiprocessing.Process(target=worker_process, args=(parent, seed))\n        self.process.start()\n\n\n", "labml_nn/rl/__init__.py": "\"\"\"\n---\ntitle: Reinforcement Learning Algorithms\nsummary: >\n  This is a collection of PyTorch implementations/tutorials of reinforcement learning algorithms.\n  It currently includes Proximal Policy Optimization, Generalized Advantage Estimation, and\n  Deep Q Networks.\n---\n\n# Reinforcement Learning Algorithms\n\n* [Proximal Policy Optimization](ppo)\n    * [This is an experiment](ppo/experiment.html) that runs a PPO agent on Atari Breakout.\n    * [Generalized advantage estimation](ppo/gae.html)\n* [Deep Q Networks](dqn)\n    * [This is an experiment](dqn/experiment.html) that runs a DQN agent on Atari Breakout.\n    * [Model](dqn/model.html) with dueling network\n    * [Prioritized Experience Replay Buffer](dqn/replay_buffer.html)\n\n[This is the implementation for OpenAI game wrapper](game.html) using `multiprocessing`.\n\"\"\"", "labml_nn/rl/dqn/experiment.py": "\"\"\"\n---\ntitle: DQN Experiment with Atari Breakout\nsummary: Implementation of DQN experiment with Atari Breakout\n---\n\n# DQN Experiment with Atari Breakout\n\nThis experiment trains a Deep Q Network (DQN) to play Atari Breakout game on OpenAI Gym.\nIt runs the [game environments on multiple processes](../game.html) to sample efficiently.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/experiment.ipynb)\n\"\"\"\n\nimport numpy as np\nimport torch\n\nfrom labml import tracker, experiment, logger, monit\nfrom labml.internal.configs.dynamic_hyperparam import FloatDynamicHyperParam\nfrom labml_helpers.schedule import Piecewise\nfrom labml_nn.rl.dqn import QFuncLoss\nfrom labml_nn.rl.dqn.model import Model\nfrom labml_nn.rl.dqn.replay_buffer import ReplayBuffer\nfrom labml_nn.rl.game import Worker\n\n# Select device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\nelse:\n    device = torch.device(\"cpu\")\n\n\ndef obs_to_torch(obs: np.ndarray) -> torch.Tensor:\n    \"\"\"Scale observations from `[0, 255]` to `[0, 1]`\"\"\"\n    return torch.tensor(obs, dtype=torch.float32, device=device) / 255.\n\n\nclass Trainer:\n    \"\"\"\n    ## Trainer\n    \"\"\"\n\n    def __init__(self, *,\n                 updates: int, epochs: int,\n                 n_workers: int, worker_steps: int, mini_batch_size: int,\n                 update_target_model: int,\n                 learning_rate: FloatDynamicHyperParam,\n                 ):\n        # number of workers\n        self.n_workers = n_workers\n        # steps sampled on each update\n        self.worker_steps = worker_steps\n        # number of training iterations\n        self.train_epochs = epochs\n\n        # number of updates\n        self.updates = updates\n        # size of mini batch for training\n        self.mini_batch_size = mini_batch_size\n\n        # update target network every 250 update\n        self.update_target_model = update_target_model\n\n        # learning rate\n        self.learning_rate = learning_rate\n\n        # exploration as a function of updates\n        self.exploration_coefficient = Piecewise(\n            [\n                (0, 1.0),\n                (25_000, 0.1),\n                (self.updates / 2, 0.01)\n            ], outside_value=0.01)\n\n        # $\\beta$ for replay buffer as a function of updates\n        self.prioritized_replay_beta = Piecewise(\n            [\n                (0, 0.4),\n                (self.updates, 1)\n            ], outside_value=1)\n\n        # Replay buffer with $\\alpha = 0.6$. Capacity of the replay buffer must be a power of 2.\n        self.replay_buffer = ReplayBuffer(2 ** 14, 0.6)\n\n        # Model for sampling and training\n        self.model = Model().to(device)\n        # target model to get $\\textcolor{orange}Q(s';\\textcolor{orange}{\\theta_i^{-}})$\n        self.target_model = Model().to(device)\n\n        # create workers\n        self.workers = [Worker(47 + i) for i in range(self.n_workers)]\n\n        # initialize tensors for observations\n        self.obs = np.zeros((self.n_workers, 4, 84, 84), dtype=np.uint8)\n\n        # reset the workers\n        for worker in self.workers:\n            worker.child.send((\"reset\", None))\n\n        # get the initial observations\n        for i, worker in enumerate(self.workers):\n            self.obs[i] = worker.child.recv()\n\n        # loss function\n        self.loss_func = QFuncLoss(0.99)\n        # optimizer\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=2.5e-4)\n\n    def _sample_action(self, q_value: torch.Tensor, exploration_coefficient: float):\n        \"\"\"\n        #### $\\epsilon$-greedy Sampling\n        When sampling actions we use a $\\epsilon$-greedy strategy, where we\n        take a greedy action with probabiliy $1 - \\epsilon$ and\n        take a random action with probability $\\epsilon$.\n        We refer to $\\epsilon$ as `exploration_coefficient`.\n        \"\"\"\n\n        # Sampling doesn't need gradients\n        with torch.no_grad():\n            # Sample the action with highest Q-value. This is the greedy action.\n            greedy_action = torch.argmax(q_value, dim=-1)\n            # Uniformly sample and action\n            random_action = torch.randint(q_value.shape[-1], greedy_action.shape, device=q_value.device)\n            # Whether to chose greedy action or the random action\n            is_choose_rand = torch.rand(greedy_action.shape, device=q_value.device) < exploration_coefficient\n            # Pick the action based on `is_choose_rand`\n            return torch.where(is_choose_rand, random_action, greedy_action).cpu().numpy()\n\n    def sample(self, exploration_coefficient: float):\n        \"\"\"### Sample data\"\"\"\n\n        # This doesn't need gradients\n        with torch.no_grad():\n            # Sample `worker_steps`\n            for t in range(self.worker_steps):\n                # Get Q_values for the current observation\n                q_value = self.model(obs_to_torch(self.obs))\n                # Sample actions\n                actions = self._sample_action(q_value, exploration_coefficient)\n\n                # Run sampled actions on each worker\n                for w, worker in enumerate(self.workers):\n                    worker.child.send((\"step\", actions[w]))\n\n                # Collect information from each worker\n                for w, worker in enumerate(self.workers):\n                    # Get results after executing the actions\n                    next_obs, reward, done, info = worker.child.recv()\n\n                    # Add transition to replay buffer\n                    self.replay_buffer.add(self.obs[w], actions[w], reward, next_obs, done)\n\n                    # update episode information. \n                    # collect episode info, which is available if an episode finished;\n                    #  this includes total reward and length of the episode -\n                    #  look at `Game` to see how it works.\n                    if info:\n                        tracker.add('reward', info['reward'])\n                        tracker.add('length', info['length'])\n\n                    # update current observation\n                    self.obs[w] = next_obs\n\n    def train(self, beta: float):\n        \"\"\"\n        ### Train the model\n        \"\"\"\n        for _ in range(self.train_epochs):\n            # Sample from priority replay buffer\n            samples = self.replay_buffer.sample(self.mini_batch_size, beta)\n            # Get the predicted Q-value\n            q_value = self.model(obs_to_torch(samples['obs']))\n\n            # Get the Q-values of the next state for [Double Q-learning](index.html).\n            # Gradients shouldn't propagate for these\n            with torch.no_grad():\n                # Get $\\textcolor{cyan}Q(s';\\textcolor{cyan}{\\theta_i})$\n                double_q_value = self.model(obs_to_torch(samples['next_obs']))\n                # Get $\\textcolor{orange}Q(s';\\textcolor{orange}{\\theta_i^{-}})$\n                target_q_value = self.target_model(obs_to_torch(samples['next_obs']))\n\n            # Compute Temporal Difference (TD) errors, $\\delta$, and the loss, $\\mathcal{L}(\\theta)$.\n            td_errors, loss = self.loss_func(q_value,\n                                             q_value.new_tensor(samples['action']),\n                                             double_q_value, target_q_value,\n                                             q_value.new_tensor(samples['done']),\n                                             q_value.new_tensor(samples['reward']),\n                                             q_value.new_tensor(samples['weights']))\n\n            # Calculate priorities for replay buffer $p_i = |\\delta_i| + \\epsilon$\n            new_priorities = np.abs(td_errors.cpu().numpy()) + 1e-6\n            # Update replay buffer priorities\n            self.replay_buffer.update_priorities(samples['indexes'], new_priorities)\n\n            # Set learning rate\n            for pg in self.optimizer.param_groups:\n                pg['lr'] = self.learning_rate()\n            # Zero out the previously calculated gradients\n            self.optimizer.zero_grad()\n            # Calculate gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n            # Update parameters based on gradients\n            self.optimizer.step()\n\n    def run_training_loop(self):\n        \"\"\"\n        ### Run training loop\n        \"\"\"\n\n        # Last 100 episode information\n        tracker.set_queue('reward', 100, True)\n        tracker.set_queue('length', 100, True)\n\n        # Copy to target network initially\n        self.target_model.load_state_dict(self.model.state_dict())\n\n        for update in monit.loop(self.updates):\n            # $\\epsilon$, exploration fraction\n            exploration = self.exploration_coefficient(update)\n            tracker.add('exploration', exploration)\n            # $\\beta$ for prioritized replay\n            beta = self.prioritized_replay_beta(update)\n            tracker.add('beta', beta)\n\n            # Sample with current policy\n            self.sample(exploration)\n\n            # Start training after the buffer is full\n            if self.replay_buffer.is_full():\n                # Train the model\n                self.train(beta)\n\n                # Periodically update target network\n                if update % self.update_target_model == 0:\n                    self.target_model.load_state_dict(self.model.state_dict())\n\n            # Save tracked indicators.\n            tracker.save()\n            # Add a new line to the screen periodically\n            if (update + 1) % 1_000 == 0:\n                logger.log()\n\n    def destroy(self):\n        \"\"\"\n        ### Destroy\n        Stop the workers\n        \"\"\"\n        for worker in self.workers:\n            worker.child.send((\"close\", None))\n\n\ndef main():\n    # Create the experiment\n    experiment.create(name='dqn')\n\n    # Configurations\n    configs = {\n        # Number of updates\n        'updates': 1_000_000,\n        # Number of epochs to train the model with sampled data.\n        'epochs': 8,\n        # Number of worker processes\n        'n_workers': 8,\n        # Number of steps to run on each process for a single update\n        'worker_steps': 4,\n        # Mini batch size\n        'mini_batch_size': 32,\n        # Target model updating interval\n        'update_target_model': 250,\n        # Learning rate.\n        'learning_rate': FloatDynamicHyperParam(1e-4, (0, 1e-3)),\n    }\n\n    # Configurations\n    experiment.configs(configs)\n\n    # Initialize the trainer\n    m = Trainer(**configs)\n    # Run and monitor the experiment\n    with experiment.start():\n        m.run_training_loop()\n    # Stop the workers\n    m.destroy()\n\n\n# ## Run it\nif __name__ == \"__main__\":\n    main()\n", "labml_nn/rl/dqn/model.py": "\"\"\"\n---\ntitle: Deep Q Network (DQN) Model\nsummary: Implementation of neural network model for Deep Q Network (DQN).\n---\n\n# Deep Q Network (DQN) Model\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/experiment.ipynb)\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass Model(Module):\n    \"\"\"\n    ## Dueling Network \u2694\ufe0f Model for $Q$ Values\n\n    We are using a [dueling network](https://arxiv.org/abs/1511.06581)\n     to calculate Q-values.\n    Intuition behind dueling network architecture is that in most states\n     the action doesn't matter,\n    and in some states the action is significant. Dueling network allows\n     this to be represented very well.\n\n    \\begin{align}\n        Q^\\pi(s,a) &= V^\\pi(s) + A^\\pi(s, a)\n        \\\\\n        \\mathop{\\mathbb{E}}_{a \\sim \\pi(s)}\n         \\Big[\n          A^\\pi(s, a)\n         \\Big]\n        &= 0\n    \\end{align}\n\n    So we create two networks for $V$ and $A$ and get $Q$ from them.\n    $$\n        Q(s, a) = V(s) +\n        \\Big(\n            A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a' \\in \\mathcal{A}} A(s, a')\n        \\Big)\n    $$\n    We share the initial layers of the $V$ and $A$ networks.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            # The first convolution layer takes a\n            # $84\\times84$ frame and produces a $20\\times20$ frame\n            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n            nn.ReLU(),\n\n            # The second convolution layer takes a\n            # $20\\times20$ frame and produces a $9\\times9$ frame\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n            nn.ReLU(),\n\n            # The third convolution layer takes a\n            # $9\\times9$ frame and produces a $7\\times7$ frame\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n            nn.ReLU(),\n        )\n\n        # A fully connected layer takes the flattened\n        # frame from third convolution layer, and outputs\n        # $512$ features\n        self.lin = nn.Linear(in_features=7 * 7 * 64, out_features=512)\n        self.activation = nn.ReLU()\n\n        # This head gives the state value $V$\n        self.state_value = nn.Sequential(\n            nn.Linear(in_features=512, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=1),\n        )\n        # This head gives the action value $A$\n        self.action_value = nn.Sequential(\n            nn.Linear(in_features=512, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=4),\n        )\n\n    def forward(self, obs: torch.Tensor):\n        # Convolution\n        h = self.conv(obs)\n        # Reshape for linear layers\n        h = h.reshape((-1, 7 * 7 * 64))\n\n        # Linear layer\n        h = self.activation(self.lin(h))\n\n        # $A$\n        action_value = self.action_value(h)\n        # $V$\n        state_value = self.state_value(h)\n\n        # $A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a' \\in \\mathcal{A}} A(s, a')$\n        action_score_centered = action_value - action_value.mean(dim=-1, keepdim=True)\n        # $Q(s, a) =V(s) + \\Big(A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a' \\in \\mathcal{A}} A(s, a')\\Big)$\n        q = state_value + action_score_centered\n\n        return q\n", "labml_nn/rl/dqn/replay_buffer.py": "\"\"\"\n---\ntitle: Prioritized Experience Replay Buffer\nsummary: Annotated implementation of prioritized experience replay using a binary segment tree.\n---\n\n# Prioritized Experience Replay Buffer\n\nThis implements paper [Prioritized experience replay](https://arxiv.org/abs/1511.05952),\nusing a binary segment tree.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/experiment.ipynb)\n\"\"\"\n\nimport random\n\nimport numpy as np\n\n\nclass ReplayBuffer:\n    \"\"\"\n    ## Buffer for Prioritized Experience Replay\n\n    [Prioritized experience replay](https://arxiv.org/abs/1511.05952)\n     samples important transitions more frequently.\n    The transitions are prioritized by the Temporal Difference error (td error), $\\delta$.\n\n    We sample transition $i$ with probability,\n    $$P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}$$\n    where $\\alpha$ is a hyper-parameter that determines how much\n    prioritization is used, with $\\alpha = 0$ corresponding to uniform case.\n    $p_i$ is the priority.\n\n    We use proportional prioritization $p_i = |\\delta_i| + \\epsilon$ where\n    $\\delta_i$ is the temporal difference for transition $i$.\n\n    We correct the bias introduced by prioritized replay using\n     importance-sampling (IS) weights\n    $$w_i = \\bigg(\\frac{1}{N} \\frac{1}{P(i)}\\bigg)^\\beta$$ in the loss function.\n    This fully compensates when $\\beta = 1$.\n    We normalize weights by $\\frac{1}{\\max_i w_i}$ for stability.\n    Unbiased nature is most important towards the convergence at end of training.\n    Therefore we increase $\\beta$ towards end of training.\n\n    ### Binary Segment Tree\n    We use a binary segment tree to efficiently calculate\n    $\\sum_k^i p_k^\\alpha$, the cumulative probability,\n    which is needed to sample.\n    We also use a binary segment tree to find $\\min p_i^\\alpha$,\n    which is needed for $\\frac{1}{\\max_i w_i}$.\n    We can also use a min-heap for this.\n    Binary Segment Tree lets us calculate these in $\\mathcal{O}(\\log n)$\n    time, which is way more efficient that the naive $\\mathcal{O}(n)$\n    approach.\n\n    This is how a binary segment tree works for sum;\n    it is similar for minimum.\n    Let $x_i$ be the list of $N$ values we want to represent.\n    Let $b_{i,j}$ be the $j^{\\mathop{th}}$ node of the $i^{\\mathop{th}}$ row\n     in the binary tree.\n    That is two children of node $b_{i,j}$ are $b_{i+1,2j}$ and $b_{i+1,2j + 1}$.\n\n    The leaf nodes on row $D = \\left\\lceil {1 + \\log_2 N} \\right\\rceil$\n     will have values of $x$.\n    Every node keeps the sum of the two child nodes.\n    That is, the root node keeps the sum of the entire array of values.\n    The left and right children of the root node keep\n     the sum of the first half of the array and\n     the sum of the second half of the array, respectively.\n    And so on...\n\n    $$b_{i,j} = \\sum_{k = (j -1) * 2^{D - i} + 1}^{j * 2^{D - i}} x_k$$\n\n    Number of nodes in row $i$,\n    $$N_i = \\left\\lceil{\\frac{N}{D - i + 1}} \\right\\rceil$$\n    This is equal to the sum of nodes in all rows above $i$.\n    So we can use a single array $a$ to store the tree, where,\n    $$b_{i,j} \\rightarrow a_{N_i + j}$$\n\n    Then child nodes of $a_i$ are $a_{2i}$ and $a_{2i + 1}$.\n    That is,\n    $$a_i = a_{2i} + a_{2i + 1}$$\n\n    This way of maintaining binary trees is very easy to program.\n    *Note that we are indexing starting from 1*.\n\n    We use the same structure to compute the minimum.\n    \"\"\"\n\n    def __init__(self, capacity, alpha):\n        \"\"\"\n        ### Initialize\n        \"\"\"\n        # We use a power of $2$ for capacity because it simplifies the code and debugging\n        self.capacity = capacity\n        # $\\alpha$\n        self.alpha = alpha\n\n        # Maintain segment binary trees to take sum and find minimum over a range\n        self.priority_sum = [0 for _ in range(2 * self.capacity)]\n        self.priority_min = [float('inf') for _ in range(2 * self.capacity)]\n\n        # Current max priority, $p$, to be assigned to new transitions\n        self.max_priority = 1.\n\n        # Arrays for buffer\n        self.data = {\n            'obs': np.zeros(shape=(capacity, 4, 84, 84), dtype=np.uint8),\n            'action': np.zeros(shape=capacity, dtype=np.int32),\n            'reward': np.zeros(shape=capacity, dtype=np.float32),\n            'next_obs': np.zeros(shape=(capacity, 4, 84, 84), dtype=np.uint8),\n            'done': np.zeros(shape=capacity, dtype=np.bool)\n        }\n        # We use cyclic buffers to store data, and `next_idx` keeps the index of the next empty\n        # slot\n        self.next_idx = 0\n\n        # Size of the buffer\n        self.size = 0\n\n    def add(self, obs, action, reward, next_obs, done):\n        \"\"\"\n        ### Add sample to queue\n        \"\"\"\n\n        # Get next available slot\n        idx = self.next_idx\n\n        # store in the queue\n        self.data['obs'][idx] = obs\n        self.data['action'][idx] = action\n        self.data['reward'][idx] = reward\n        self.data['next_obs'][idx] = next_obs\n        self.data['done'][idx] = done\n\n        # Increment next available slot\n        self.next_idx = (idx + 1) % self.capacity\n        # Calculate the size\n        self.size = min(self.capacity, self.size + 1)\n\n        # $p_i^\\alpha$, new samples get `max_priority`\n        priority_alpha = self.max_priority ** self.alpha\n        # Update the two segment trees for sum and minimum\n        self._set_priority_min(idx, priority_alpha)\n        self._set_priority_sum(idx, priority_alpha)\n\n    def _set_priority_min(self, idx, priority_alpha):\n        \"\"\"\n        #### Set priority in binary segment tree for minimum\n        \"\"\"\n\n        # Leaf of the binary tree\n        idx += self.capacity\n        self.priority_min[idx] = priority_alpha\n\n        # Update tree, by traversing along ancestors.\n        # Continue until the root of the tree.\n        while idx >= 2:\n            # Get the index of the parent node\n            idx //= 2\n            # Value of the parent node is the minimum of it's two children\n            self.priority_min[idx] = min(self.priority_min[2 * idx], self.priority_min[2 * idx + 1])\n\n    def _set_priority_sum(self, idx, priority):\n        \"\"\"\n        #### Set priority in binary segment tree for sum\n        \"\"\"\n\n        # Leaf of the binary tree\n        idx += self.capacity\n        # Set the priority at the leaf\n        self.priority_sum[idx] = priority\n\n        # Update tree, by traversing along ancestors.\n        # Continue until the root of the tree.\n        while idx >= 2:\n            # Get the index of the parent node\n            idx //= 2\n            # Value of the parent node is the sum of it's two children\n            self.priority_sum[idx] = self.priority_sum[2 * idx] + self.priority_sum[2 * idx + 1]\n\n    def _sum(self):\n        \"\"\"\n        #### $\\sum_k p_k^\\alpha$\n        \"\"\"\n\n        # The root node keeps the sum of all values\n        return self.priority_sum[1]\n\n    def _min(self):\n        \"\"\"\n        #### $\\min_k p_k^\\alpha$\n        \"\"\"\n\n        # The root node keeps the minimum of all values\n        return self.priority_min[1]\n\n    def find_prefix_sum_idx(self, prefix_sum):\n        \"\"\"\n        #### Find largest $i$ such that $\\sum_{k=1}^{i} p_k^\\alpha  \\le P$\n        \"\"\"\n\n        # Start from the root\n        idx = 1\n        while idx < self.capacity:\n            # If the sum of the left branch is higher than required sum\n            if self.priority_sum[idx * 2] > prefix_sum:\n                # Go to left branch of the tree\n                idx = 2 * idx\n            else:\n                # Otherwise go to right branch and reduce the sum of left\n                #  branch from required sum\n                prefix_sum -= self.priority_sum[idx * 2]\n                idx = 2 * idx + 1\n\n        # We are at the leaf node. Subtract the capacity by the index in the tree\n        # to get the index of actual value\n        return idx - self.capacity\n\n    def sample(self, batch_size, beta):\n        \"\"\"\n        ### Sample from buffer\n        \"\"\"\n\n        # Initialize samples\n        samples = {\n            'weights': np.zeros(shape=batch_size, dtype=np.float32),\n            'indexes': np.zeros(shape=batch_size, dtype=np.int32)\n        }\n\n        # Get sample indexes\n        for i in range(batch_size):\n            p = random.random() * self._sum()\n            idx = self.find_prefix_sum_idx(p)\n            samples['indexes'][i] = idx\n\n        # $\\min_i P(i) = \\frac{\\min_i p_i^\\alpha}{\\sum_k p_k^\\alpha}$\n        prob_min = self._min() / self._sum()\n        # $\\max_i w_i = \\bigg(\\frac{1}{N} \\frac{1}{\\min_i P(i)}\\bigg)^\\beta$\n        max_weight = (prob_min * self.size) ** (-beta)\n\n        for i in range(batch_size):\n            idx = samples['indexes'][i]\n            # $P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}$\n            prob = self.priority_sum[idx + self.capacity] / self._sum()\n            # $w_i = \\bigg(\\frac{1}{N} \\frac{1}{P(i)}\\bigg)^\\beta$\n            weight = (prob * self.size) ** (-beta)\n            # Normalize by $\\frac{1}{\\max_i w_i}$,\n            #  which also cancels off the $\\frac{1}{N}$ term\n            samples['weights'][i] = weight / max_weight\n\n        # Get samples data\n        for k, v in self.data.items():\n            samples[k] = v[samples['indexes']]\n\n        return samples\n\n    def update_priorities(self, indexes, priorities):\n        \"\"\"\n        ### Update priorities\n        \"\"\"\n\n        for idx, priority in zip(indexes, priorities):\n            # Set current max priority\n            self.max_priority = max(self.max_priority, priority)\n\n            # Calculate $p_i^\\alpha$\n            priority_alpha = priority ** self.alpha\n            # Update the trees\n            self._set_priority_min(idx, priority_alpha)\n            self._set_priority_sum(idx, priority_alpha)\n\n    def is_full(self):\n        \"\"\"\n        ### Whether the buffer is full\n        \"\"\"\n        return self.capacity == self.size\n", "labml_nn/rl/dqn/__init__.py": "\"\"\"\n---\ntitle: Deep Q Networks (DQN)\nsummary: >\n  This is a PyTorch implementation/tutorial of Deep Q Networks (DQN) from paper\n  Playing Atari with Deep Reinforcement Learning.\n  This includes dueling network architecture, a prioritized replay buffer and\n  double-Q-network training.\n---\n\n\n# Deep Q Networks (DQN)\n\nThis is a [PyTorch](https://pytorch.org) implementation of paper\n [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)\n along with [Dueling Network](model.html), [Prioritized Replay](replay_buffer.html)\n and Double Q Network.\n\nHere is the [experiment](experiment.html) and [model](model.html) implementation.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/experiment.ipynb)\n\"\"\"\n\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn\n\nfrom labml import tracker\nfrom labml_helpers.module import Module\nfrom labml_nn.rl.dqn.replay_buffer import ReplayBuffer\n\n\nclass QFuncLoss(Module):\n    \"\"\"\n    ## Train the model\n\n    We want to find optimal action-value function.\n\n    \\begin{align}\n        Q^*(s,a) &= \\max_\\pi \\mathbb{E} \\Big[\n            r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + ... | s_t = s, a_t = a, \\pi\n        \\Big]\n    \\\\\n        Q^*(s,a) &= \\mathop{\\mathbb{E}}_{s' \\sim \\large{\\varepsilon}} \\Big[\n            r + \\gamma \\max_{a'} Q^* (s', a') | s, a\n        \\Big]\n    \\end{align}\n\n    ### Target network \ud83c\udfaf\n    In order to improve stability we use experience replay that randomly sample\n    from previous experience $U(D)$. We also use a Q network\n    with a separate set of parameters $\\textcolor{orange}{\\theta_i^{-}}$ to calculate the target.\n    $\\textcolor{orange}{\\theta_i^{-}}$ is updated periodically.\n    This is according to paper\n    [Human Level Control Through Deep Reinforcement Learning](https://deepmind.com/research/dqn/).\n\n    So the loss function is,\n    $$\n    \\mathcal{L}_i(\\theta_i) = \\mathop{\\mathbb{E}}_{(s,a,r,s') \\sim U(D)}\n    \\bigg[\n        \\Big(\n            r + \\gamma \\max_{a'} Q(s', a'; \\textcolor{orange}{\\theta_i^{-}}) - Q(s,a;\\theta_i)\n        \\Big) ^ 2\n    \\bigg]\n    $$\n\n    ### Double $Q$-Learning\n    The max operator in the above calculation uses same network for both\n    selecting the best action and for evaluating the value.\n    That is,\n    $$\n    \\max_{a'} Q(s', a'; \\theta) = \\textcolor{cyan}{Q}\n    \\Big(\n        s', \\mathop{\\operatorname{argmax}}_{a'}\n        \\textcolor{cyan}{Q}(s', a'; \\textcolor{cyan}{\\theta}); \\textcolor{cyan}{\\theta}\n    \\Big)\n    $$\n    We use [double Q-learning](https://arxiv.org/abs/1509.06461), where\n    the $\\operatorname{argmax}$ is taken from $\\textcolor{cyan}{\\theta_i}$ and\n    the value is taken from $\\textcolor{orange}{\\theta_i^{-}}$.\n\n    And the loss function becomes,\n\n    \\begin{align}\n        \\mathcal{L}_i(\\theta_i) = \\mathop{\\mathbb{E}}_{(s,a,r,s') \\sim U(D)}\n        \\Bigg[\n            \\bigg(\n                &r + \\gamma \\textcolor{orange}{Q}\n                \\Big(\n                    s',\n                    \\mathop{\\operatorname{argmax}}_{a'}\n                        \\textcolor{cyan}{Q}(s', a'; \\textcolor{cyan}{\\theta_i}); \\textcolor{orange}{\\theta_i^{-}}\n                \\Big)\n                \\\\\n                - &Q(s,a;\\theta_i)\n            \\bigg) ^ 2\n        \\Bigg]\n    \\end{align}\n    \"\"\"\n\n    def __init__(self, gamma: float):\n        super().__init__()\n        self.gamma = gamma\n        self.huber_loss = nn.SmoothL1Loss(reduction='none')\n\n    def forward(self, q: torch.Tensor, action: torch.Tensor, double_q: torch.Tensor,\n                target_q: torch.Tensor, done: torch.Tensor, reward: torch.Tensor,\n                weights: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        * `q` - $Q(s;\\theta_i)$\n        * `action` - $a$\n        * `double_q` - $\\textcolor{cyan}Q(s';\\textcolor{cyan}{\\theta_i})$\n        * `target_q` - $\\textcolor{orange}Q(s';\\textcolor{orange}{\\theta_i^{-}})$\n        * `done` - whether the game ended after taking the action\n        * `reward` - $r$\n        * `weights` - weights of the samples from prioritized experienced replay\n        \"\"\"\n\n        # $Q(s,a;\\theta_i)$\n        q_sampled_action = q.gather(-1, action.to(torch.long).unsqueeze(-1)).squeeze(-1)\n        tracker.add('q_sampled_action', q_sampled_action)\n\n        # Gradients shouldn't propagate gradients\n        # $$r + \\gamma \\textcolor{orange}{Q}\n        #                 \\Big(s',\n        #                     \\mathop{\\operatorname{argmax}}_{a'}\n        #                         \\textcolor{cyan}{Q}(s', a'; \\textcolor{cyan}{\\theta_i}); \\textcolor{orange}{\\theta_i^{-}}\n        #                 \\Big)$$\n        with torch.no_grad():\n            # Get the best action at state $s'$\n            # $$\\mathop{\\operatorname{argmax}}_{a'}\n            #  \\textcolor{cyan}{Q}(s', a'; \\textcolor{cyan}{\\theta_i})$$\n            best_next_action = torch.argmax(double_q, -1)\n            # Get the q value from the target network for the best action at state $s'$\n            # $$\\textcolor{orange}{Q}\n            # \\Big(s',\\mathop{\\operatorname{argmax}}_{a'}\n            # \\textcolor{cyan}{Q}(s', a'; \\textcolor{cyan}{\\theta_i}); \\textcolor{orange}{\\theta_i^{-}}\n            # \\Big)$$\n            best_next_q_value = target_q.gather(-1, best_next_action.unsqueeze(-1)).squeeze(-1)\n\n            # Calculate the desired Q value.\n            # We multiply by `(1 - done)` to zero out\n            # the next state Q values if the game ended.\n            #\n            # $$r + \\gamma \\textcolor{orange}{Q}\n            #                 \\Big(s',\n            #                     \\mathop{\\operatorname{argmax}}_{a'}\n            #                         \\textcolor{cyan}{Q}(s', a'; \\textcolor{cyan}{\\theta_i}); \\textcolor{orange}{\\theta_i^{-}}\n            #                 \\Big)$$\n            q_update = reward + self.gamma * best_next_q_value * (1 - done)\n            tracker.add('q_update', q_update)\n\n            # Temporal difference error $\\delta$ is used to weigh samples in replay buffer\n            td_error = q_sampled_action - q_update\n            tracker.add('td_error', td_error)\n\n        # We take [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) instead of\n        # mean squared error loss because it is less sensitive to outliers\n        losses = self.huber_loss(q_sampled_action, q_update)\n        # Get weighted means\n        loss = torch.mean(weights * losses)\n        tracker.add('loss', loss)\n\n        return td_error, loss\n", "labml_nn/rl/ppo/experiment.py": "\"\"\"\n---\ntitle: PPO Experiment with Atari Breakout\nsummary: Annotated implementation to train a PPO agent on Atari Breakout game.\n---\n\n# PPO Experiment with Atari Breakout\n\nThis experiment trains Proximal Policy Optimization (PPO) agent  Atari Breakout game on OpenAI Gym.\nIt runs the [game environments on multiple processes](../game.html) to sample efficiently.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/ppo/experiment.ipynb)\n\"\"\"\n\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.distributions import Categorical\n\nfrom labml import monit, tracker, logger, experiment\nfrom labml.configs import FloatDynamicHyperParam, IntDynamicHyperParam\nfrom labml_helpers.module import Module\nfrom labml_nn.rl.game import Worker\nfrom labml_nn.rl.ppo import ClippedPPOLoss, ClippedValueFunctionLoss\nfrom labml_nn.rl.ppo.gae import GAE\n\n# Select device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\nelse:\n    device = torch.device(\"cpu\")\n\n\nclass Model(Module):\n    \"\"\"\n    ## Model\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        # The first convolution layer takes a\n        # 84x84 frame and produces a 20x20 frame\n        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n\n        # The second convolution layer takes a\n        # 20x20 frame and produces a 9x9 frame\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n\n        # The third convolution layer takes a\n        # 9x9 frame and produces a 7x7 frame\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n\n        # A fully connected layer takes the flattened\n        # frame from third convolution layer, and outputs\n        # 512 features\n        self.lin = nn.Linear(in_features=7 * 7 * 64, out_features=512)\n\n        # A fully connected layer to get logits for $\\pi$\n        self.pi_logits = nn.Linear(in_features=512, out_features=4)\n\n        # A fully connected layer to get value function\n        self.value = nn.Linear(in_features=512, out_features=1)\n\n        #\n        self.activation = nn.ReLU()\n\n    def forward(self, obs: torch.Tensor):\n        h = self.activation(self.conv1(obs))\n        h = self.activation(self.conv2(h))\n        h = self.activation(self.conv3(h))\n        h = h.reshape((-1, 7 * 7 * 64))\n\n        h = self.activation(self.lin(h))\n\n        pi = Categorical(logits=self.pi_logits(h))\n        value = self.value(h).reshape(-1)\n\n        return pi, value\n\n\ndef obs_to_torch(obs: np.ndarray) -> torch.Tensor:\n    \"\"\"Scale observations from `[0, 255]` to `[0, 1]`\"\"\"\n    return torch.tensor(obs, dtype=torch.float32, device=device) / 255.\n\n\nclass Trainer:\n    \"\"\"\n    ## Trainer\n    \"\"\"\n\n    def __init__(self, *,\n                 updates: int, epochs: IntDynamicHyperParam,\n                 n_workers: int, worker_steps: int, batches: int,\n                 value_loss_coef: FloatDynamicHyperParam,\n                 entropy_bonus_coef: FloatDynamicHyperParam,\n                 clip_range: FloatDynamicHyperParam,\n                 learning_rate: FloatDynamicHyperParam,\n                 ):\n        # #### Configurations\n\n        # number of updates\n        self.updates = updates\n        # number of epochs to train the model with sampled data\n        self.epochs = epochs\n        # number of worker processes\n        self.n_workers = n_workers\n        # number of steps to run on each process for a single update\n        self.worker_steps = worker_steps\n        # number of mini batches\n        self.batches = batches\n        # total number of samples for a single update\n        self.batch_size = self.n_workers * self.worker_steps\n        # size of a mini batch\n        self.mini_batch_size = self.batch_size // self.batches\n        assert (self.batch_size % self.batches == 0)\n\n        # Value loss coefficient\n        self.value_loss_coef = value_loss_coef\n        # Entropy bonus coefficient\n        self.entropy_bonus_coef = entropy_bonus_coef\n\n        # Clipping range\n        self.clip_range = clip_range\n        # Learning rate\n        self.learning_rate = learning_rate\n\n        # #### Initialize\n\n        # create workers\n        self.workers = [Worker(47 + i) for i in range(self.n_workers)]\n\n        # initialize tensors for observations\n        self.obs = np.zeros((self.n_workers, 4, 84, 84), dtype=np.uint8)\n        for worker in self.workers:\n            worker.child.send((\"reset\", None))\n        for i, worker in enumerate(self.workers):\n            self.obs[i] = worker.child.recv()\n\n        # model\n        self.model = Model().to(device)\n\n        # optimizer\n        self.optimizer = optim.Adam(self.model.parameters(), lr=2.5e-4)\n\n        # GAE with $\\gamma = 0.99$ and $\\lambda = 0.95$\n        self.gae = GAE(self.n_workers, self.worker_steps, 0.99, 0.95)\n\n        # PPO Loss\n        self.ppo_loss = ClippedPPOLoss()\n\n        # Value Loss\n        self.value_loss = ClippedValueFunctionLoss()\n\n    def sample(self) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        ### Sample data with current policy\n        \"\"\"\n\n        rewards = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)\n        actions = np.zeros((self.n_workers, self.worker_steps), dtype=np.int32)\n        done = np.zeros((self.n_workers, self.worker_steps), dtype=np.bool)\n        obs = np.zeros((self.n_workers, self.worker_steps, 4, 84, 84), dtype=np.uint8)\n        log_pis = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)\n        values = np.zeros((self.n_workers, self.worker_steps + 1), dtype=np.float32)\n\n        with torch.no_grad():\n            # sample `worker_steps` from each worker\n            for t in range(self.worker_steps):\n                # `self.obs` keeps track of the last observation from each worker,\n                #  which is the input for the model to sample the next action\n                obs[:, t] = self.obs\n                # sample actions from $\\pi_{\\theta_{OLD}}$ for each worker;\n                #  this returns arrays of size `n_workers`\n                pi, v = self.model(obs_to_torch(self.obs))\n                values[:, t] = v.cpu().numpy()\n                a = pi.sample()\n                actions[:, t] = a.cpu().numpy()\n                log_pis[:, t] = pi.log_prob(a).cpu().numpy()\n\n                # run sampled actions on each worker\n                for w, worker in enumerate(self.workers):\n                    worker.child.send((\"step\", actions[w, t]))\n\n                for w, worker in enumerate(self.workers):\n                    # get results after executing the actions\n                    self.obs[w], rewards[w, t], done[w, t], info = worker.child.recv()\n\n                    # collect episode info, which is available if an episode finished;\n                    #  this includes total reward and length of the episode -\n                    #  look at `Game` to see how it works.\n                    if info:\n                        tracker.add('reward', info['reward'])\n                        tracker.add('length', info['length'])\n\n            # Get value of after the final step\n            _, v = self.model(obs_to_torch(self.obs))\n            values[:, self.worker_steps] = v.cpu().numpy()\n\n        # calculate advantages\n        advantages = self.gae(done, rewards, values)\n\n        #\n        samples = {\n            'obs': obs,\n            'actions': actions,\n            'values': values[:, :-1],\n            'log_pis': log_pis,\n            'advantages': advantages\n        }\n\n        # samples are currently in `[workers, time_step]` table,\n        # we should flatten it for training\n        samples_flat = {}\n        for k, v in samples.items():\n            v = v.reshape(v.shape[0] * v.shape[1], *v.shape[2:])\n            if k == 'obs':\n                samples_flat[k] = obs_to_torch(v)\n            else:\n                samples_flat[k] = torch.tensor(v, device=device)\n\n        return samples_flat\n\n    def train(self, samples: Dict[str, torch.Tensor]):\n        \"\"\"\n        ### Train the model based on samples\n        \"\"\"\n\n        # It learns faster with a higher number of epochs,\n        #  but becomes a little unstable; that is,\n        #  the average episode reward does not monotonically increase\n        #  over time.\n        # May be reducing the clipping range might solve it.\n        for _ in range(self.epochs()):\n            # shuffle for each epoch\n            indexes = torch.randperm(self.batch_size)\n\n            # for each mini batch\n            for start in range(0, self.batch_size, self.mini_batch_size):\n                # get mini batch\n                end = start + self.mini_batch_size\n                mini_batch_indexes = indexes[start: end]\n                mini_batch = {}\n                for k, v in samples.items():\n                    mini_batch[k] = v[mini_batch_indexes]\n\n                # train\n                loss = self._calc_loss(mini_batch)\n\n                # Set learning rate\n                for pg in self.optimizer.param_groups:\n                    pg['lr'] = self.learning_rate()\n                # Zero out the previously calculated gradients\n                self.optimizer.zero_grad()\n                # Calculate gradients\n                loss.backward()\n                # Clip gradients\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n                # Update parameters based on gradients\n                self.optimizer.step()\n\n    @staticmethod\n    def _normalize(adv: torch.Tensor):\n        \"\"\"#### Normalize advantage function\"\"\"\n        return (adv - adv.mean()) / (adv.std() + 1e-8)\n\n    def _calc_loss(self, samples: Dict[str, torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        ### Calculate total loss\n        \"\"\"\n\n        # $R_t$ returns sampled from $\\pi_{\\theta_{OLD}}$\n        sampled_return = samples['values'] + samples['advantages']\n\n        # $\\bar{A_t} = \\frac{\\hat{A_t} - \\mu(\\hat{A_t})}{\\sigma(\\hat{A_t})}$,\n        # where $\\hat{A_t}$ is advantages sampled from $\\pi_{\\theta_{OLD}}$.\n        # Refer to sampling function in [Main class](#main) below\n        #  for the calculation of $\\hat{A}_t$.\n        sampled_normalized_advantage = self._normalize(samples['advantages'])\n\n        # Sampled observations are fed into the model to get $\\pi_\\theta(a_t|s_t)$ and $V^{\\pi_\\theta}(s_t)$;\n        #  we are treating observations as state\n        pi, value = self.model(samples['obs'])\n\n        # $-\\log \\pi_\\theta (a_t|s_t)$, $a_t$ are actions sampled from $\\pi_{\\theta_{OLD}}$\n        log_pi = pi.log_prob(samples['actions'])\n\n        # Calculate policy loss\n        policy_loss = self.ppo_loss(log_pi, samples['log_pis'], sampled_normalized_advantage, self.clip_range())\n\n        # Calculate Entropy Bonus\n        #\n        # $\\mathcal{L}^{EB}(\\theta) =\n        #  \\mathbb{E}\\Bigl[ S\\bigl[\\pi_\\theta\\bigr] (s_t) \\Bigr]$\n        entropy_bonus = pi.entropy()\n        entropy_bonus = entropy_bonus.mean()\n\n        # Calculate value function loss\n        value_loss = self.value_loss(value, samples['values'], sampled_return, self.clip_range())\n\n        # $\\mathcal{L}^{CLIP+VF+EB} (\\theta) =\n        #  \\mathcal{L}^{CLIP} (\\theta) +\n        #  c_1 \\mathcal{L}^{VF} (\\theta) - c_2 \\mathcal{L}^{EB}(\\theta)$\n        loss = (policy_loss\n                + self.value_loss_coef() * value_loss\n                - self.entropy_bonus_coef() * entropy_bonus)\n\n        # for monitoring\n        approx_kl_divergence = .5 * ((samples['log_pis'] - log_pi) ** 2).mean()\n\n        # Add to tracker\n        tracker.add({'policy_reward': -policy_loss,\n                     'value_loss': value_loss,\n                     'entropy_bonus': entropy_bonus,\n                     'kl_div': approx_kl_divergence,\n                     'clip_fraction': self.ppo_loss.clip_fraction})\n\n        return loss\n\n    def run_training_loop(self):\n        \"\"\"\n        ### Run training loop\n        \"\"\"\n\n        # last 100 episode information\n        tracker.set_queue('reward', 100, True)\n        tracker.set_queue('length', 100, True)\n\n        for update in monit.loop(self.updates):\n            # sample with current policy\n            samples = self.sample()\n\n            # train the model\n            self.train(samples)\n\n            # Save tracked indicators.\n            tracker.save()\n            # Add a new line to the screen periodically\n            if (update + 1) % 1_000 == 0:\n                logger.log()\n\n    def destroy(self):\n        \"\"\"\n        ### Destroy\n        Stop the workers\n        \"\"\"\n        for worker in self.workers:\n            worker.child.send((\"close\", None))\n\n\ndef main():\n    # Create the experiment\n    experiment.create(name='ppo')\n    # Configurations\n    configs = {\n        # Number of updates\n        'updates': 10000,\n        # \u2699\ufe0f Number of epochs to train the model with sampled data.\n        # You can change this while the experiment is running.\n        'epochs': IntDynamicHyperParam(8),\n        # Number of worker processes\n        'n_workers': 8,\n        # Number of steps to run on each process for a single update\n        'worker_steps': 128,\n        # Number of mini batches\n        'batches': 4,\n        # \u2699\ufe0f Value loss coefficient.\n        # You can change this while the experiment is running.\n        'value_loss_coef': FloatDynamicHyperParam(0.5),\n        # \u2699\ufe0f Entropy bonus coefficient.\n        # You can change this while the experiment is running.\n        'entropy_bonus_coef': FloatDynamicHyperParam(0.01),\n        # \u2699\ufe0f Clip range.\n        'clip_range': FloatDynamicHyperParam(0.1),\n        # You can change this while the experiment is running.\n        # \u2699\ufe0f Learning rate.\n        'learning_rate': FloatDynamicHyperParam(1e-3, (0, 1e-3)),\n    }\n\n    experiment.configs(configs)\n\n    # Initialize the trainer\n    m = Trainer(**configs)\n\n    # Run and monitor the experiment\n    with experiment.start():\n        m.run_training_loop()\n    # Stop the workers\n    m.destroy()\n\n\n# ## Run it\nif __name__ == \"__main__\":\n    main()\n", "labml_nn/rl/ppo/__init__.py": "\"\"\"\n---\ntitle: Proximal Policy Optimization - PPO\nsummary: >\n An annotated implementation of Proximal Policy Optimization - PPO algorithm in PyTorch.\n---\n\n# Proximal Policy Optimization - PPO\n\nThis is a [PyTorch](https://pytorch.org) implementation of\n[Proximal Policy Optimization - PPO](https://arxiv.org/abs/1707.06347).\n\nPPO is a policy gradient method for reinforcement learning.\nSimple policy gradient methods do a single gradient update per sample (or a set of samples).\nDoing multiple gradient steps for a single sample causes problems\nbecause the policy deviates too much, producing a bad policy.\nPPO lets us do multiple gradient updates per sample by trying to keep the\npolicy close to the policy that was used to sample data.\nIt does so by clipping gradient flow if the updated policy\nis not close to the policy used to sample the data.\n\nYou can find an experiment that uses it [here](experiment.html).\nThe experiment uses [Generalized Advantage Estimation](gae.html).\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/ppo/experiment.ipynb)\n\"\"\"\n\nimport torch\n\nfrom labml_helpers.module import Module\nfrom labml_nn.rl.ppo.gae import GAE\n\n\nclass ClippedPPOLoss(Module):\n    \"\"\"\n    ## PPO Loss\n\n    Here's how the PPO update rule is derived.\n\n    We want to maximize policy reward\n     $$\\max_\\theta J(\\pi_\\theta) =\n       \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_\\theta}\\Biggl[\\sum_{t=0}^\\infty \\gamma^t r_t \\Biggr]$$\n     where $r$ is the reward, $\\pi$ is the policy, $\\tau$ is a trajectory sampled from policy,\n     and $\\gamma$ is the discount factor between $[0, 1]$.\n\n    \\begin{align}\n    \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n     \\sum_{t=0}^\\infty \\gamma^t A^{\\pi_{OLD}}(s_t, a_t)\n    \\Biggr] &=\n    \\\\\n    \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n      \\sum_{t=0}^\\infty \\gamma^t \\Bigl(\n       Q^{\\pi_{OLD}}(s_t, a_t) - V^{\\pi_{OLD}}(s_t)\n      \\Bigr)\n     \\Biggr] &=\n    \\\\\n    \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n      \\sum_{t=0}^\\infty \\gamma^t \\Bigl(\n       r_t + V^{\\pi_{OLD}}(s_{t+1}) - V^{\\pi_{OLD}}(s_t)\n      \\Bigr)\n     \\Biggr] &=\n    \\\\\n    \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n      \\sum_{t=0}^\\infty \\gamma^t \\Bigl(\n       r_t\n      \\Bigr)\n     \\Biggr]\n     - \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n        \\Biggl[V^{\\pi_{OLD}}(s_0)\\Biggr] &=\n    J(\\pi_\\theta) - J(\\pi_{\\theta_{OLD}})\n    \\end{align}\n\n    So,\n     $$\\max_\\theta J(\\pi_\\theta) =\n       \\max_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n          \\sum_{t=0}^\\infty \\gamma^t A^{\\pi_{OLD}}(s_t, a_t)\n       \\Biggr]$$\n\n    Define discounted-future state distribution,\n     $$d^\\pi(s) = (1 - \\gamma) \\sum_{t=0}^\\infty \\gamma^t P(s_t = s | \\pi)$$\n\n    Then,\n\n    \\begin{align}\n    J(\\pi_\\theta) - J(\\pi_{\\theta_{OLD}})\n    &= \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\Biggl[\n     \\sum_{t=0}^\\infty \\gamma^t A^{\\pi_{OLD}}(s_t, a_t)\n    \\Biggr]\n    \\\\\n    &= \\frac{1}{1 - \\gamma}\n     \\mathbb{E}_{s \\sim d^{\\pi_\\theta}, a \\sim \\pi_\\theta} \\Bigl[\n      A^{\\pi_{OLD}}(s, a)\n     \\Bigr]\n    \\end{align}\n\n    Importance sampling $a$ from $\\pi_{\\theta_{OLD}}$,\n\n    \\begin{align}\n    J(\\pi_\\theta) - J(\\pi_{\\theta_{OLD}})\n    &= \\frac{1}{1 - \\gamma}\n     \\mathbb{E}_{s \\sim d^{\\pi_\\theta}, a \\sim \\pi_\\theta} \\Bigl[\n      A^{\\pi_{OLD}}(s, a)\n     \\Bigr]\n    \\\\\n    &= \\frac{1}{1 - \\gamma}\n     \\mathbb{E}_{s \\sim d^{\\pi_\\theta}, a \\sim \\pi_{\\theta_{OLD}}} \\Biggl[\n      \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{OLD}}(a|s)} A^{\\pi_{OLD}}(s, a)\n     \\Biggr]\n    \\end{align}\n\n    Then we assume $d^\\pi_\\theta(s)$ and  $d^\\pi_{\\theta_{OLD}}(s)$ are similar.\n    The error we introduce to $J(\\pi_\\theta) - J(\\pi_{\\theta_{OLD}})$\n     by this assumption is bound by the KL divergence between\n     $\\pi_\\theta$ and $\\pi_{\\theta_{OLD}}$.\n    [Constrained Policy Optimization](https://arxiv.org/abs/1705.10528)\n     shows the proof of this. I haven't read it.\n\n\n    \\begin{align}\n    J(\\pi_\\theta) - J(\\pi_{\\theta_{OLD}})\n    &= \\frac{1}{1 - \\gamma}\n     \\mathop{\\mathbb{E}}_{s \\sim d^{\\pi_\\theta} \\atop a \\sim \\pi_{\\theta_{OLD}}} \\Biggl[\n      \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{OLD}}(a|s)} A^{\\pi_{OLD}}(s, a)\n     \\Biggr]\n    \\\\\n    &\\approx \\frac{1}{1 - \\gamma}\n     \\mathop{\\mathbb{E}}_{\\textcolor{orange}{s \\sim d^{\\pi_{\\theta_{OLD}}}}\n     \\atop a \\sim \\pi_{\\theta_{OLD}}} \\Biggl[\n      \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{OLD}}(a|s)} A^{\\pi_{OLD}}(s, a)\n     \\Biggr]\n    \\\\\n    &= \\frac{1}{1 - \\gamma} \\mathcal{L}^{CPI}\n    \\end{align}\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, log_pi: torch.Tensor, sampled_log_pi: torch.Tensor,\n                advantage: torch.Tensor, clip: float) -> torch.Tensor:\n        # ratio $r_t(\\theta) = \\frac{\\pi_\\theta (a_t|s_t)}{\\pi_{\\theta_{OLD}} (a_t|s_t)}$;\n        # *this is different from rewards* $r_t$.\n        ratio = torch.exp(log_pi - sampled_log_pi)\n\n        # ### Cliping the policy ratio\n        #\n        # \\begin{align}\n        # \\mathcal{L}^{CLIP}(\\theta) =\n        #  \\mathbb{E}_{a_t, s_t \\sim \\pi_{\\theta{OLD}}} \\biggl[\n        #    min \\Bigl(r_t(\\theta) \\bar{A_t},\n        #              clip \\bigl(\n        #               r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon\n        #              \\bigr) \\bar{A_t}\n        #    \\Bigr)\n        #  \\biggr]\n        # \\end{align}\n        #\n        # The ratio is clipped to be close to 1.\n        # We take the minimum so that the gradient will only pull\n        # $\\pi_\\theta$ towards $\\pi_{\\theta_{OLD}}$ if the ratio is\n        # not between $1 - \\epsilon$ and $1 + \\epsilon$.\n        # This keeps the KL divergence between $\\pi_\\theta$\n        #  and $\\pi_{\\theta_{OLD}}$ constrained.\n        # Large deviation can cause performance collapse;\n        #  where the policy performance drops and doesn't recover because\n        #  we are sampling from a bad policy.\n        #\n        # Using the normalized advantage\n        #  $\\bar{A_t} = \\frac{\\hat{A_t} - \\mu(\\hat{A_t})}{\\sigma(\\hat{A_t})}$\n        #  introduces a bias to the policy gradient estimator,\n        #  but it reduces variance a lot.\n        clipped_ratio = ratio.clamp(min=1.0 - clip,\n                                    max=1.0 + clip)\n        policy_reward = torch.min(ratio * advantage,\n                                  clipped_ratio * advantage)\n\n        self.clip_fraction = (abs((ratio - 1.0)) > clip).to(torch.float).mean()\n\n        return -policy_reward.mean()\n\n\nclass ClippedValueFunctionLoss(Module):\n    \"\"\"\n    ## Clipped Value Function Loss\n\n    Similarly we clip the value function update also.\n\n    \\begin{align}\n    V^{\\pi_\\theta}_{CLIP}(s_t)\n     &= clip\\Bigl(V^{\\pi_\\theta}(s_t) - \\hat{V_t}, -\\epsilon, +\\epsilon\\Bigr)\n    \\\\\n    \\mathcal{L}^{VF}(\\theta)\n     &= \\frac{1}{2} \\mathbb{E} \\biggl[\n      max\\Bigl(\\bigl(V^{\\pi_\\theta}(s_t) - R_t\\bigr)^2,\n          \\bigl(V^{\\pi_\\theta}_{CLIP}(s_t) - R_t\\bigr)^2\\Bigr)\n     \\biggr]\n    \\end{align}\n\n    Clipping makes sure the value function $V_\\theta$ doesn't deviate\n     significantly from $V_{\\theta_{OLD}}$.\n\n    \"\"\"\n\n    def forward(self, value: torch.Tensor, sampled_value: torch.Tensor, sampled_return: torch.Tensor, clip: float):\n        clipped_value = sampled_value + (value - sampled_value).clamp(min=-clip, max=clip)\n        vf_loss = torch.max((value - sampled_return) ** 2, (clipped_value - sampled_return) ** 2)\n        return 0.5 * vf_loss.mean()\n", "labml_nn/rl/ppo/gae.py": "\"\"\"\n---\ntitle: Generalized Advantage Estimation (GAE)\nsummary: A PyTorch implementation/tutorial of Generalized Advantage Estimation (GAE).\n---\n\n# Generalized Advantage Estimation (GAE)\n\nThis is a [PyTorch](https://pytorch.org) implementation of paper\n[Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438).\n\nYou can find an experiment that uses it [here](experiment.html).\n\"\"\"\n\nimport numpy as np\n\n\nclass GAE:\n    def __init__(self, n_workers: int, worker_steps: int, gamma: float, lambda_: float):\n        self.lambda_ = lambda_\n        self.gamma = gamma\n        self.worker_steps = worker_steps\n        self.n_workers = n_workers\n\n    def __call__(self, done: np.ndarray, rewards: np.ndarray, values: np.ndarray) -> np.ndarray:\n        \"\"\"\n        ### Calculate advantages\n\n        \\begin{align}\n        \\hat{A_t^{(1)}} &= r_t + \\gamma V(s_{t+1}) - V(s)\n        \\\\\n        \\hat{A_t^{(2)}} &= r_t + \\gamma r_{t+1} +\\gamma^2 V(s_{t+2}) - V(s)\n        \\\\\n        ...\n        \\\\\n        \\hat{A_t^{(\\infty)}} &= r_t + \\gamma r_{t+1} +\\gamma^2 r_{t+2} + ... - V(s)\n        \\end{align}\n\n        $\\hat{A_t^{(1)}}$ is high bias, low variance, whilst\n        $\\hat{A_t^{(\\infty)}}$ is unbiased, high variance.\n\n        We take a weighted average of $\\hat{A_t^{(k)}}$ to balance bias and variance.\n        This is called Generalized Advantage Estimation.\n        $$\\hat{A_t} = \\hat{A_t^{GAE}} = \\frac{\\sum_k w_k \\hat{A_t^{(k)}}}{\\sum_k w_k}$$\n        We set $w_k = \\lambda^{k-1}$, this gives clean calculation for\n        $\\hat{A_t}$\n\n        \\begin{align}\n        \\delta_t &= r_t + \\gamma V(s_{t+1}) - V(s_t)\n        \\\\\n        \\hat{A_t} &= \\delta_t + \\gamma \\lambda \\delta_{t+1} + ... +\n                             (\\gamma \\lambda)^{T - t + 1} \\delta_{T - 1}\n        \\\\\n        &= \\delta_t + \\gamma \\lambda \\hat{A_{t+1}}\n        \\end{align}\n        \"\"\"\n\n        # advantages table\n        advantages = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)\n        last_advantage = 0\n\n        # $V(s_{t+1})$\n        last_value = values[:, -1]\n\n        for t in reversed(range(self.worker_steps)):\n            # mask if episode completed after step $t$\n            mask = 1.0 - done[:, t]\n            last_value = last_value * mask\n            last_advantage = last_advantage * mask\n            # $\\delta_t$\n            delta = rewards[:, t] + self.gamma * last_value - values[:, t]\n\n            # $\\hat{A_t} = \\delta_t + \\gamma \\lambda \\hat{A_{t+1}}$\n            last_advantage = delta + self.gamma * self.lambda_ * last_advantage\n\n            #\n            advantages[:, t] = last_advantage\n\n            last_value = values[:, t]\n\n        # $\\hat{A_t}$\n        return advantages\n", "labml_nn/optimizers/radam.py": "\"\"\"\n---\ntitle: Rectified Adam (RAdam) optimizer\nsummary: A simple PyTorch implementation/tutorial of RAdam optimizer.\n---\n\n# Rectified Adam (RAdam) optimizer\n\nThis implementation is based on\n[the official implementation](https://github.com/LiyuanLucasLiu/RAdam)\nof the paper\n[On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265).\n\nWe have implemented it in [PyTorch](https://pytorch.org)\nas an extension to [our AMSGrad implementation](amsgrad.html)\nthus requiring only the modifications to be implemented.\n\nAdam optimizer sometimes converges to a bad local optima during the initial stages of the training;\nespecially when training transformers.\nResearches use warmups to counter this; for the the initial training steps (warm-up stage)\nthey use a low learning rate.\nThis paper identifies the problem to be the high variance of adaptive learning rate\nduring initial stages of training, and counters it using a new rectification term to\nreduce variance.\n\nThe paper also evaluates two variance reduction mechanisms:\n* **Adam-2k**: Only compute the adaptive learning rate ($v_t$ in [Adam](adam.html)) during the first 2k steps,\nwithout changing parameters or calculating momentum ($m_t$).\n* **Adam-eps**: Adam with large $\\epsilon \\approx 10^{-4}$.\n\n## Rectified Adam\n\nLet $\\sigma(g_1, ..., g_t)$ and $\\psi(g_1, ..., g_t)$ be the functions to calculate\nmomentum and adaptive learning rate.\nFor Adam, they are\n\n\\begin{align}\n\\sigma(g_1, ..., g_t) &=  \\frac{(1 - \\beta_1)\\sum_{i=1}^t \\beta_1^{t-i} g_i}{1 - \\beta_1^t} \\\\\n\\psi(g_1, ..., g_t) &=  \\sqrt \\frac{1 - \\beta_2^t}{(1 - \\beta_2)\\sum_{i=1}^t \\beta_2^{t-i} g_i^2}\n\\end{align}\n\n### Exponential moving average as simple moving average\n\nThe distribution of exponential moving average can be approximated as a simple moving average.\n\n\\begin{align}\np\\Bigg(\\frac{(1-\\beta_2) \\sum_{i=1}^t \\beta_2^{t-i} g_i^2}{1 - \\beta_2^t} \\Bigg) \\approx\np\\Bigg(\\frac{\\sum_{i=1}^{f(t,\\beta_2)} g_{t+1-i}^2}{f(t,\\beta_2)} \\Bigg)\n\\end{align}\n\nHere we are taking the simple moving average of the last $f(t,\\beta_2)$ gradients.\n$f(t,\\beta_2)$ satisfies the following,\n\n\\begin{align}\n\\frac{(1-\\beta_2) \\sum_{i=1}^t \\beta_2^{t-i} \\cdot i}{1 - \\beta_2^t} =\n\\frac{\\sum_{i=1}^{f(t,\\beta_2)} (t+1-i)}{f(t,\\beta_2)}\n\\end{align}\n\nwhich gives,\n$$f(t,\\beta_2) = \\frac{2}{1-\\beta_2} - 1 - \\frac{2 t \\beta_2^t}{1 - \\beta_2^t}$$\n\n### Scaled inverse chi-squared\n\nFrom above we have\n$$\np\\Big( \\psi^2(g_1, ..., g_t) \\Big) \\approx\np\\Bigg(\\frac{\\sum_{i=1}^{f(t,\\beta_2)} g_{t+1-i}^2}{f(t,\\beta_2)} \\Bigg)\n$$\nwhere $g_i \\sim \\mathcal{N}(0, \\sigma^2)$.\nNote that $sigma$ here is the standard deviation and different from $\\sigma(.)$ for momentum.\n\n[Scaled inverse chi-squared](https://en.wikipedia.org/wiki/Scaled_inverse_chi-squared_distribution)\nis the distribution of squared inverse of mean of $p$ normal distributions.\n$$\np\\Bigg(\\frac{\\sum_{i=1}^{f(t,\\beta_2)} g_{t+1-i}^2}{f(t,\\beta_2)} \\Bigg)\n\\sim\n\\text{Scale-inv} \\mathcal{X}^2(\\rho,\\frac{1}{\\sigma^2})\n$$\nwhere $\\rho = f(t,\\beta_2)$.\n\n### Rectification\n\nThey prove that variance of $\\psi(.)$ decreases with $\\rho$ when\n$\\psi^2(.) \\sim \\text{Scale-inv} \\mathcal{X}^2(\\rho,\\frac{1}{\\sigma^2})$.\n\nTherefore the variance is minimized at maximal $\\rho$ which is\n$\\rho_{\\infty} = \\frac{2}{1-\\beta_2} - 1$. Let the minimum variance be $C_{\\text{var}}$\n\nIn order to ensure that the adaptive learning\nrate $\\psi(.)$ has consistent variance, we rectify the variance with $r$\n\n\\begin{align}\nr = \\sqrt{\\frac{C_{\\text{var}}}{Var\\big[\\psi(.)\\big]}}\n\\end{align}\n\n### Approximating $Var[\\psi(.)]$\n\nThey estimate $Var[\\psi(.)] \\approx \\frac{Var[\\psi^2(.)]}{4 \\mathbb{E}[\\psi^2(.)}$\nbased on first order expansion of $\\sqrt{\\psi^2(.)}$\n\ud83e\udd2a I didn't get how it was derived.\n\nFrom $\\text{Scale-inv} \\mathcal{X}^2$ distribution we have,\n\n\\begin{align}\n\\mathbb{E}\\big[\\psi^2(.)\\big] &= \\frac{\\rho / \\sigma^2}{\\rho-2} \\\\\nVar\\big[\\psi^2(.)\\big] &= \\frac{2 \\rho / \\sigma^4}{(\\rho-2)^2 (\\rho - 2)}\n\\end{align}\n\nwhich gives,\n$$\nVar[\\psi(.)] \\approx \\frac{\\rho}{2(\\rho-2)(\\rho-4)\\sigma^2}\n$$\n\n### Rectification term\n\nWe have\n\n\\begin{align}\nr &= \\sqrt{\\frac{C_{\\text{var}}}{Var\\big[\\psi(.)\\big]}} \\\\\nVar[\\psi(.)] &\\approx \\frac{\\rho}{2(\\rho-2)(\\rho-4)\\sigma^2}\n\\end{align}\n\nwhere $C_{\\text{var}}$ is $Var\\big[\\psi(.)\\big]$ for $\\rho_\\infty$.\nLt $\\rho$ and step $t$ be $\\rho_t$, and $r_t$ be the rectification term\nat step $t$.\n\n\\begin{align}\nC_{\\text{var}} &\\approx \\frac{\\rho_\\infty}{2(\\rho_\\infty-2)(\\rho_\\infty-4)\\sigma^2} \\\\\nVar[\\psi(g_1,...,g_t)] &\\approx \\frac{\\rho_t}{2(\\rho_t-2)(\\rho_t-4)\\sigma^2}\n\\end{align}\n\nThis gives,\n\n\\begin{align}\nr_t &= \\sqrt{\\frac{(\\rho_t-2)(\\rho_t-4)\\rho_\\infty}{(\\rho_\\infty-2)(\\rho_\\infty-4)\\rho_t}}\n\\end{align}\n\"\"\"\n\nimport math\nfrom typing import Dict, Optional\n\nimport torch\n\nfrom labml_nn.optimizers import WeightDecay\nfrom labml_nn.optimizers.amsgrad import AMSGrad\n\n\nclass RAdam(AMSGrad):\n    \"\"\"\n    ## Rectified Adam Optimizer\n\n    This class extends from AMSAdam optimizer defined in [`amsadam.py`](amsadam.html).\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay: WeightDecay = WeightDecay(),\n                 optimized_update: bool = True,\n                 amsgrad=False,\n                 degenerated_to_sgd=True, defaults=None):\n        \"\"\"\n        ### Initialize the optimizer\n\n        * `params` is the list of parameters\n        * `lr` is the learning rate $\\alpha$\n        * `betas` is a tuple of ($\\beta_1$, $\\beta_2$)\n        * `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update`\n        * `weight_decay` is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)\n        * `optimized_update` is a flag whether to optimize the bias correction of the second moment\n          by doing it after adding $\\epsilon$\n        * `amsgrad` is a flag indicating whether to use AMSGrad or fallback to plain Adam\n        * `degenerate_to_sgd` whether to use sgd when the rectification term $r_t$ is intractable.\n        * `defaults` is a dictionary of default for group values.\n         This is useful when you want to extend the class `RAdam`.\n        \"\"\"\n        self.degenerated_to_sgd = degenerated_to_sgd\n        super().__init__(params, lr, betas, eps, weight_decay, optimized_update, amsgrad, defaults)\n\n    def step_param(self, state: Dict[str, any], group: Dict[str, any], grad: torch.Tensor, param: torch.nn.Parameter):\n        \"\"\"\n        ### Take an update step for a given parameter tensor\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `grad` is the current gradient tensor  $g_t$ for the parameter $\\theta_{t-1}$\n        * `param` is the parameter tensor $\\theta_{t-1}$\n        \"\"\"\n\n        # Calculate weight decay\n        grad = self.weight_decay(param, grad, group)\n\n        # Get $m_t$ and $v_t$; i.e. $\\sigma(.)$ and $\\psi(.)$ without bias correction\n        m, v = self.get_mv(state, group, grad)\n\n        # Calculate $t$ the number of optimizer steps\n        state['step'] += 1\n\n        # Perform *RAdam* update\n        self.r_adam_update(state, group, param, m, v)\n\n    @staticmethod\n    def calc_rectification_term(beta2: float, step: int) -> Optional[float]:\n        \"\"\"\n        ### Calculate rectification term $r_t$\n        \"\"\"\n\n        # $\\beta_2^t$\n        beta2_t = beta2 ** step\n        # $$\\rho_\\infty = \\frac{2}{1 - \\beta_2} - 1$$\n        rho_inf = 2 / (1 - beta2) - 1\n        # $$\\rho_t = \\frac{2}{1-\\beta_2} - 1 - \\frac{2 t \\beta_2^t}{1-\\beta_2^t}$$\n        rho = rho_inf - 2 * step * beta2_t / (1 - beta2_t)\n\n        # $r_t$ is tractable when $\\rho_t >= 4$.\n        # We are being a little more conservative since it's an approximated value\n        if rho >= 5:\n            # $$r_t = \\sqrt{\\frac{(\\rho_t-2)(\\rho_t-4)\\rho_\\infty}{(\\rho_\\infty-2)(\\rho_\\infty-4)\\rho_t}}$$\n            r2 = (rho - 4) / (rho_inf - 4) * (rho - 2) / rho * rho_inf / (rho_inf - 2)\n            return math.sqrt(r2)\n        else:\n            return None\n\n    def r_adam_update(self, state: Dict[str, any], group: Dict[str, any], param: torch.nn.Parameter,\n                      m: torch.Tensor, v: torch.Tensor):\n        \"\"\"\n        ### Do the *RAdam* parameter update\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `param` is the parameter tensor $\\theta_{t-1}$\n        * `m` and `v` are the uncorrected first and second moments $m_t$ and $v_t$;\n          i.e. $\\sigma(.)$ and $\\psi(.)$ without bias correction\n        \"\"\"\n\n        # Get $\\beta_1$ and $\\beta_2$\n        beta1, beta2 = group['betas']\n        # Bias correction term for $\\hat{m}_t$, $1 - \\beta_1^t$\n        bias_correction1 = 1 - beta1 ** state['step']\n        # Bias correction term for $\\hat{v}_t$, $1 - \\beta_2^t$\n        bias_correction2 = 1 - beta2 ** state['step']\n\n        r = self.calc_rectification_term(beta2, state['step'])\n\n        # Get learning rate\n        lr = self.get_lr(state, group)\n\n        # If $r_t$ is intractable\n        if r is not None:\n            # Whether to optimize the computation by combining scalar computations\n            if self.optimized_update:\n                # Denominator $\\sqrt{v_t} + \\hat{\\epsilon}$\n                denominator = v.sqrt().add_(group['eps'])\n                # Step size $\\alpha \\sqrt{r_t} * \\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t}$\n                step_size = lr * math.sqrt(bias_correction2) * r / bias_correction1\n                # Update parameters $\\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\sqrt{r_t} \\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t} \\cdot\n                #  \\frac{m_t}{\\sqrt{v_t} + \\hat{\\epsilon}}$\n                param.data.addcdiv_(m, denominator, value=-step_size)\n            # Computation without optimization\n            else:\n                # Denominator  $\\frac{\\sqrt{v_t}}{\\sqrt{1-\\beta_2^t}} + \\epsilon$\n                denominator = (v.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n                # Step size $\\frac{\\alpha \\sqrt{r_t}}{1-\\beta_1^t}$\n                step_size = lr * r / bias_correction1\n                # Update parameters $\\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\sqrt{r_t} \\cdot\n                # \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n                param.data.addcdiv_(m, denominator, value=-step_size)\n\n        # If $r_t$ is intractable do a SGD with momentum\n        elif self.degenerated_to_sgd:\n            # Step size $\\frac{\\alpha}{1-\\beta_1^t}$\n            step_size = lr / bias_correction1\n            # Update parameters\n            # $\\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\cdot \\hat{m}_t$\n            param.data.add_(m, alpha=-step_size)\n\n\ndef _test_rectification_term():\n    \"\"\"\n    ### Plot $r_t$ against $t$ for various $\\beta_2$\n\n    ![Plot of r_t](radam_r_t.png)\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    beta2 = [0.9999, 0.999, 0.99, 0.9, 0.8, 0.6, 0.5]\n    plt.plot(np.arange(1, 5_000), [[RAdam.calc_rectification_term(b, i) for b in beta2] for i in range(1, 5_000)])\n    plt.legend(beta2)\n    plt.title(\"Optimizer\")\n    plt.show()\n\n\nif __name__ == '__main__':\n    _test_rectification_term()\n", "labml_nn/optimizers/noam.py": "\"\"\"\n---\ntitle: Noam optimizer from Attention is All You Need paper\nsummary: >\n  This is a tutorial/implementation of Noam optimizer.\n  Noam optimizer has a warm-up period and then an exponentially decaying learning rate.\n---\n\n# Noam Optimizer\n\nThis is the [PyTorch](https://pytorch.org) implementation of optimizer introduced in the paper\n[Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n\"\"\"\nfrom typing import Dict\n\nfrom labml_nn.optimizers import WeightDecay\nfrom labml_nn.optimizers.amsgrad import AMSGrad\n\n\nclass Noam(AMSGrad):\n    \"\"\"\n    ## Noam Optimizer\n\n    This class extends from Adam optimizer defined in [`adam.py`](adam.html).\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-16,\n                 weight_decay: WeightDecay = WeightDecay(),\n                 optimized_update: bool = True,\n                 amsgrad=False,\n                 warmup=0, d_model=512, defaults=None):\n        \"\"\"\n        ### Initialize the optimizer\n\n        * `params` is the list of parameters\n        * `lr` is the learning rate $\\alpha$\n        * `betas` is a tuple of ($\\beta_1$, $\\beta_2$)\n        * `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update`\n        * `weight_decay` is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)\n        * 'optimized_update' is a flag whether to optimize the bias correction of the second moment\n          by doing it after adding $\\epsilon$\n        * `amsgrad` is a flag indicating whether to use AMSGrad or fallback to plain Adam\n        * `warmup` number of warmup steps\n        * `d_model` model size; i.e. number of dimensions in the transformer\n        * `defaults` is a dictionary of default for group values.\n         This is useful when you want to extend the class `AdamWarmup`.\n        \"\"\"\n\n        defaults = {} if defaults is None else defaults\n        defaults.update(dict(warmup=warmup))\n        super().__init__(params, lr, betas, eps, weight_decay, optimized_update, amsgrad, defaults)\n        self.d_model = d_model\n\n    def get_lr(self, state: Dict[str, any], group: Dict[str, any]):\n        \"\"\"\n        ### Get learning-rate\n\n        $$\\alpha \\frac{1}{\\sqrt{d_{model}}} \\min \\bigg(\\frac{1}{\\sqrt{t}}, \\frac{t}{w^{3/2}}\\bigg)$$\n        where $w$ is the number of warmup steps.\n        \"\"\"\n        # $$\\min \\bigg(\\frac{1}{\\sqrt{t}}, \\frac{t}{w^{3/2}}\\bigg)$$\n        factor = min(state['step'] ** (-0.5), state['step'] * group['warmup'] ** (-1.5))\n        # $$\\alpha \\frac{1}{\\sqrt{d_{model}}} \\min \\bigg(\\frac{1}{\\sqrt{t}}, \\frac{t}{w^{3/2}}\\bigg)$$\n        return group['lr'] * self.d_model ** (-0.5) * factor\n\n\ndef _test_noam_lr():\n    \"\"\"\n    ### Plot learning rate for different warmups and model sizes\n\n    ![Plot of learning rate](noam_lr.png)\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from torch import nn\n\n    model = nn.Linear(10, 10)\n    opts = [Noam(model.parameters(), d_model=512, warmup=4000, lr=1),\n            Noam(model.parameters(), d_model=512, warmup=8000, lr=1),\n            Noam(model.parameters(), d_model=2048, warmup=2000, lr=1)]\n    plt.plot(np.arange(1, 20000), [[opt.get_lr({'step': i}, opt.defaults) for opt in opts] for i in range(1, 20000)])\n    plt.legend([\"512:4000\", \"512:8000\", \"2048:2000\"])\n    plt.title(\"Learning Rate\")\n    plt.show()\n\n\nif __name__ == '__main__':\n    _test_noam_lr()\n", "labml_nn/optimizers/ada_belief.py": "\"\"\"\n---\ntitle: AdaBelief optimizer\nsummary: A simple PyTorch implementation/tutorial of AdaBelief optimizer.\n---\n\n# AdaBelief Optimizer\n\nThis is based from AdaBelief\n[official implementation](https://github.com/juntang-zhuang/Adabelief-Optimizer)\nof the paper\n[AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients](https://arxiv.org/abs/2010.07468).\n\nThis is implemented in [PyTorch](https://pytorch.org) as an extension to [RAdam](radam.html).\n\nThe main difference between Adam optimizer and AdaBelief is that,\nhow it calculates the adaptive learning rate;\ninstead of dividing by the exponential moving average of square of the gradients,\nAdaBelief divides by the exponential mean of variance.\n\n\\begin{align}\nm_t &\\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g_t \\\\\n\\textcolor{cyan}{s_t} &\\textcolor{cyan}{\\leftarrow} \\textcolor{cyan}{\\beta_2 s_{t-1} + (1 - \\beta_2) \\cdot (g_t - m_t)^2} \\\\\n\\hat{m}_t &\\leftarrow \\frac{m_t}{1-\\beta_1^t} \\\\\n\\textcolor{cyan}{\\hat{s}_t} &\\textcolor{cyan}{\\leftarrow} \\frac{\\textcolor{cyan}{s_t} + \\textcolor{red}{\\epsilon}}{\\textcolor{cyan}{1-\\beta_2^t}} \\\\\n\\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\textcolor{cyan}{\\hat{s}_t}} + \\epsilon}\n\\end{align}\n\n\ud83e\udd14 The paper calculates variance as $(g_t - m_t)^2$,\nbut I feel it should use the bias corrected momentum\n$(g_t - \\textcolor{orange}{\\hat{m}_t})^2$.\nI guess this doesn't affect things much because\nbias correction is $\\approx 1$ after the initial training steps.\n\"\"\"\n\nfrom typing import Dict, Any\n\nimport torch\nfrom torch import nn\n\nfrom labml_nn.optimizers import WeightDecay\nfrom labml_nn.optimizers.radam import RAdam\n\n\nclass AdaBelief(RAdam):\n    \"\"\"\n    ## AdaBelief Optimizer\n\n    This class extends from RAdam optimizer defined in [`radam.py`](radam.html).\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-16,\n                 weight_decay: WeightDecay = WeightDecay(), amsgrad=False,\n                 degenerate_to_sgd=True,\n                 rectify=True, defaults=None):\n        \"\"\"\n        ### Initialize the optimizer\n\n        * `params` is the list of parameters\n        * `lr` is the learning rate $\\alpha$\n        * `betas` is a tuple of ($\\beta_1$, $\\beta_2$)\n        * `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update`\n        * `weight_decay` is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)\n        * `optimized_update` is a flag whether to optimize the bias correction of the second moment\n          by doing it after adding $\\epsilon$\n        * `amsgrad` is a flag indicating whether to use AMSGrad or fallback to plain Adam\n        * `degenerate_to_sgd` whether to use sgd when the rectification term $r_t$ is intractable\n        * `rectify` is whether to use RAdam update\n        * `defaults` is a dictionary of default for group values.\n         This is useful when you want to extend the class `AdaBelief`.\n        \"\"\"\n\n        defaults = {} if defaults is None else defaults\n        super().__init__(params, lr, betas, eps, weight_decay, amsgrad, degenerate_to_sgd, defaults)\n        self.rectify = rectify\n\n    def init_state(self, state: Dict[str, any], group: Dict[str, any], param: nn.Parameter):\n        \"\"\"\n        ### Initialize a parameter state\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `param` is the parameter tensor $\\theta_{t-1}$\n        \"\"\"\n        state['step'] = 0\n        # Exponential moving average of gradient values\n        state['exp_avg'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n        # Exponential moving average of variance\n        state['exp_avg_var'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n\n        # If `amsgrad` flag is `True` for this parameter group, we maintain the maximum of\n        # exponential moving average of variance\n        if group['amsgrad']:\n            # Maintains max of all exp. moving avg. of sq. grad. values\n            state['max_exp_avg_var'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n\n    def get_ms(self, state: Dict[str, Any], group: Dict[str, Any], grad: torch.Tensor):\n        \"\"\"\n        ### Calculate $m_t$ and $s_t$ or $\\max(s_1, s_2, ..., s_{t-1}, s_t)$\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `grad` is the current gradient tensor $g_t$ for the parameter $\\theta_{t-1}$\n        \"\"\"\n\n        # Get $\\beta_1$ and $\\beta_2$\n        beta1, beta2 = group['betas']\n\n        # Get $m_{t-1}$ and $s_{t-1}$\n        m, s = state['exp_avg'], state['exp_avg_var']\n\n        # In-place calculation of $m_t$\n        # $$m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\n        m.mul_(beta1).add_(grad, alpha=1 - beta1)\n        # Difference between gradient and momentum\n        grad_residual = grad - m\n        # In-place calculation of $s_t$\n        # $$s_t \\leftarrow \\beta_2 s_{t-1} + (1 - \\beta_2) \\cdot (g_t - m_t)^2$$\n        s.mul_(beta2).addcmul_(grad_residual, grad_residual, value=1 - beta2)\n\n        # If this parameter group is using `amsgrad`\n        if group['amsgrad']:\n            # Get $\\max(s_1, s_2, ..., s_{t-1})$.\n            s_max = state['max_exp_avg_var']\n            # Calculate $\\max(s_1, s_2, ..., s_{t-1}, s_t)$.\n            torch.maximum(s_max, s, out=s_max)\n\n            return m, s_max\n        else:\n            # $m_t$ and $s_t$ otherwise\n            return m, s\n\n    def step_param(self, state: Dict[str, any], group: Dict[str, any], grad: torch.Tensor, param: torch.nn.Parameter):\n        \"\"\"\n        ### Take an update step for a given parameter tensor\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `grad` is the current gradient tensor  $g_t$ for the parameter $\\theta_{t-1}$\n        * `param` is the parameter tensor $\\theta_{t-1}$\n        \"\"\"\n\n        # Calculate weight decay\n        grad = self.weight_decay(param, grad, group)\n\n        # Get $m_t$ and $v_t$\n        m, s = self.get_ms(state, group, grad)\n\n        # Increment $t$ the number of optimizer steps\n        state['step'] += 1\n\n        if not self.rectify:\n            # Perform *Adam* update, defined in [`adam.py`](adam.html), with\n            # $\\textcolor{cyan}{s_t} + \\textcolor{red}{\\epsilon}$ in place of $v_t$.\n            self.adam_update(state, group, param, m, s + group['eps'])\n        else:\n            # Perform *Rectified Adam* update defined in [`radam.py`](radam.html), with\n            # $\\textcolor{cyan}{s_t} + \\textcolor{red}{\\epsilon}$ in place of $v_t$.\n            self.r_adam_update(state, group, param, m, s + group['eps'])\n", "labml_nn/optimizers/mnist_experiment.py": "\"\"\"\n---\ntitle: MNIST example to test the optimizers\nsummary: This is a simple MNIST example with a CNN model to test the optimizers.\n---\n\n# MNIST example to test the optimizers\n\"\"\"\nimport torch.nn as nn\nimport torch.utils.data\nfrom labml_helpers.module import Module\n\nfrom labml import experiment, tracker\nfrom labml.configs import option\nfrom labml_helpers.datasets.mnist import MNISTConfigs\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_helpers.metrics.accuracy import Accuracy\nfrom labml_helpers.seed import SeedConfigs\nfrom labml_helpers.train_valid import TrainValidConfigs, BatchIndex, hook_model_outputs\nfrom labml_nn.optimizers.configs import OptimizerConfigs\n\n\nclass Model(Module):\n    \"\"\"\n    ## The model\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.pool1 = nn.MaxPool2d(2)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.pool2 = nn.MaxPool2d(2)\n        self.fc1 = nn.Linear(16 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        x = self.activation(self.conv1(x))\n        x = self.pool1(x)\n        x = self.activation(self.conv2(x))\n        x = self.pool2(x)\n        x = self.activation(self.fc1(x.view(-1, 16 * 50)))\n        return self.fc2(x)\n\n\nclass Configs(MNISTConfigs, TrainValidConfigs):\n    \"\"\"\n    ## Configurable Experiment Definition\n    \"\"\"\n    optimizer: torch.optim.Adam\n    model: nn.Module\n    set_seed = SeedConfigs()\n    device: torch.device = DeviceConfigs()\n    epochs: int = 10\n\n    is_save_models = True\n    model: nn.Module\n    inner_iterations = 10\n\n    accuracy_func = Accuracy()\n    loss_func = nn.CrossEntropyLoss()\n\n    def init(self):\n        tracker.set_queue(\"loss.*\", 20, True)\n        tracker.set_scalar(\"accuracy.*\", True)\n        hook_model_outputs(self.mode, self.model, 'model')\n        self.state_modules = [self.accuracy_func]\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        # Get the batch\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Add global step if we are in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Run the model and specify whether to log the activations\n        with self.mode.update(is_log_activations=batch_idx.is_last):\n            output = self.model(data)\n\n        # Calculate the loss\n        loss = self.loss_func(output, target)\n        # Calculate the accuracy\n        self.accuracy_func(output, target)\n        # Log the loss\n        tracker.add(\"loss.\", loss)\n\n        # Optimize if we are in training mode\n        if self.mode.is_train:\n            # Calculate the gradients\n            loss.backward()\n\n            # Take optimizer step\n            self.optimizer.step()\n            # Log the parameter and gradient L2 norms once per epoch\n            if batch_idx.is_last:\n                tracker.add('model', self.model)\n                tracker.add('optimizer', (self.optimizer, {'model': self.model}))\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save logs\n        tracker.save()\n\n\n@option(Configs.model)\ndef model(c: Configs):\n    return Model().to(c.device)\n\n\n@option(Configs.optimizer)\ndef _optimizer(c: Configs):\n    \"\"\"\n    Create a configurable optimizer.\n    We can change the optimizer type and hyper-parameters using configurations.\n    \"\"\"\n    opt_conf = OptimizerConfigs()\n    opt_conf.parameters = c.model.parameters()\n    return opt_conf\n\n\ndef main():\n    conf = Configs()\n    conf.inner_iterations = 10\n    experiment.create(name='mnist_ada_belief')\n    experiment.configs(conf, {'inner_iterations': 10,\n                              # Specify the optimizer\n                              'optimizer.optimizer': 'Adam',\n                              'optimizer.learning_rate': 1.5e-4})\n    conf.set_seed.set()\n    experiment.add_pytorch_models(dict(model=conf.model))\n    with experiment.start():\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/optimizers/adam_fp16.py": "\"\"\"\n---\ntitle: Adam Optimizer for Half Precision Training\nsummary: A simple PyTorch implementation/tutorial of Adam optimizer\n---\n\n# Adam Optimizer for Half Precision Training\n\"\"\"\n\nfrom typing import Dict, Tuple, Optional, Any\n\nimport torch\nfrom torch import nn\nfrom torch.optim import Optimizer\nfrom torch.cuda.amp import grad_scaler\nfrom collections import defaultdict, abc\n\nfrom labml_nn.optimizers import WeightDecay\nfrom labml_nn.optimizers.adam import Adam\n\n\nclass AdamFP16(Adam):\n    \"\"\"\n    ## Adam Optimizer for Half Precision Training\n\n    We extend [Adam Optimizer](adam.html) but use FP32 to store gradients and moments.\n    \"\"\"\n\n    def __init__(self, params, lr: float = 1e-3, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-16,\n                 weight_decay: WeightDecay = WeightDecay(), optimized_update: bool = True,\n                 defaults: Optional[Dict[str, Any]] = None):\n        # Parameter to store 32 bit gradients. This get populated by the `GradScaler` defined below.\n        self.grad_fp32 = {}\n        # Call the [Adam Optimizer](adam.html) initializer\n        super().__init__(params, lr, betas, eps, weight_decay, optimized_update, defaults)\n\n    def init_state(self, state: Dict[str, any], group: Dict[str, any], param: nn.Parameter):\n        \"\"\"\n        ### Initialize a parameter state\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `param` is the parameter tensor $\\theta_{t-1}$\n\n        All the state tensors use FP32.\n        \"\"\"\n\n        # This is the number of optimizer steps taken on the parameter, $t$\n        state['step'] = 0\n        # Exponential moving average of gradients, $m_t$\n        state['exp_avg'] = torch.zeros_like(param, memory_format=torch.preserve_format, dtype=torch.float)\n        # Exponential moving average of squared gradient values, $v_t$\n        state['exp_avg_sq'] = torch.zeros_like(param, memory_format=torch.preserve_format, dtype=torch.float)\n        # Maintain a FP32 copy of the parameters\n        state['fp32_copy'] = param.to(torch.float)\n\n    def step_param(self, state: Dict[str, any], group: Dict[str, any], grad: torch.Tensor, param: torch.nn.Parameter):\n        \"\"\"\n        ### Take an update step for a given parameter tensor\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `grad` is the current gradient tensor  $g_t$ for the parameter $\\theta_{t-1}$\n        * `param` is the parameter tensor $\\theta_{t-1}$\n        \"\"\"\n\n        # Get the FP32 parameters\n        param_fp32 = state['fp32_copy']\n        # Get the FP32 gradients if available\n        grad_fp32 = self.grad_fp32.get(param, None)\n        if grad_fp32 is not None:\n            del self.grad_fp32[param]\n            grad = grad_fp32\n        else:\n            # Otherwise, convert the gradients to FP32\n            grad = grad.to(torch.float)\n\n        # Calculate weight decay\n        grad = self.weight_decay(param_fp32, grad, group)\n\n        # Get $m_t$ and $v_t$\n        m, v = self.get_mv(state, group, grad)\n\n        # Increment $t$ the number of optimizer steps\n        state['step'] += 1\n\n        # Perform *Adam* update\n        self.adam_update(state, group, param_fp32, m, v)\n\n        # Set the parameters\n        param.data = param_fp32.to(param.dtype)\n\n\nclass GradScalerFP16(grad_scaler.GradScaler):\n    \"\"\"\n    ## Gradient Scaler with half precision gradients\n\n    We extend PyTorch gradient scaler to use FP32 gradients.\n    \"\"\"\n\n    def _unscale_grads_(self, optimizer: Optimizer, inv_scale: torch.Tensor, found_inf: torch.Tensor,\n                        allow_fp16: bool) -> Dict[torch.device, torch.Tensor]:\n        per_device_inv_scale = grad_scaler._MultiDeviceReplicator(inv_scale)\n        per_device_found_inf = grad_scaler._MultiDeviceReplicator(found_inf)\n\n        per_device_and_dtype_grads = defaultdict(lambda: defaultdict(list))  # type: ignore[var-annotated]\n\n        with torch.no_grad():\n            # Loop through parameters\n            for group in optimizer.param_groups:\n                for param in group[\"params\"]:\n                    # Skip non-trainable parameters\n                    if param.grad is None:\n                        continue\n                    # Not implemented for sparse tensors\n                    if param.grad.is_sparse:\n                        raise NotImplementedError\n\n                    # If we are using the `AdamFP16` optimizer set `optimizer.grad_fp32[param]` to the FP32 gradients\n                    if isinstance(optimizer, AdamFP16):\n                        grad = param.grad.to(torch.float)\n                        optimizer.grad_fp32[param] = grad\n                    # Otherwise, do not convert the gradients to FP32\n                    else:\n                        grad = param.grad\n\n                    per_device_and_dtype_grads[grad.device][grad.dtype].append(grad)\n\n            # Unscale all the gradients\n            for device, per_dtype_grads in per_device_and_dtype_grads.items():\n                for grads in per_dtype_grads.values():\n                    torch._amp_foreach_non_finite_check_and_unscale_(grads,\n                                                                     per_device_found_inf.get(device),\n                                                                     per_device_inv_scale.get(device))\n        #\n        return per_device_found_inf._per_device_tensors\n", "labml_nn/optimizers/amsgrad.py": "\"\"\"\n---\ntitle: AMSGrad Optimizer\nsummary: A simple PyTorch implementation/tutorial of AMSGrad optimizer.\n---\n\n# AMSGrad\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[On the Convergence of Adam and Beyond](https://arxiv.org/abs/1904.09237).\n\nWe implement this as an extension to our [Adam optimizer implementation](adam.html).\nThe implementation it self is really small since it's very similar to Adam.\n\nWe also have an implementation of the synthetic example described in the paper where Adam fails to converge.\n\"\"\"\n\nfrom typing import Dict\n\nimport torch\nfrom torch import nn\n\nfrom labml_nn.optimizers import WeightDecay\nfrom labml_nn.optimizers.adam import Adam\n\n\nclass AMSGrad(Adam):\n    \"\"\"\n    ## AMSGrad Optimizer\n\n    This class extends from Adam optimizer defined in [`adam.py`](adam.html).\n    Adam optimizer is extending the class `GenericAdaptiveOptimizer`\n    defined in [`__init__.py`](index.html).\n    \"\"\"\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-16,\n                 weight_decay: WeightDecay = WeightDecay(),\n                 optimized_update: bool = True,\n                 amsgrad=True, defaults=None):\n        \"\"\"\n        ### Initialize the optimizer\n\n        * `params` is the list of parameters\n        * `lr` is the learning rate $\\alpha$\n        * `betas` is a tuple of ($\\beta_1$, $\\beta_2$)\n        * `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update`\n        * `weight_decay` is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)\n        * 'optimized_update' is a flag whether to optimize the bias correction of the second moment\n          by doing it after adding $\\epsilon$\n        * `amsgrad` is a flag indicating whether to use AMSGrad or fallback to plain Adam\n        * `defaults` is a dictionary of default for group values.\n         This is useful when you want to extend the class `Adam`.\n        \"\"\"\n        defaults = {} if defaults is None else defaults\n        defaults.update(dict(amsgrad=amsgrad))\n\n        super().__init__(params, lr, betas, eps, weight_decay, optimized_update, defaults)\n\n    def init_state(self, state: Dict[str, any], group: Dict[str, any], param: nn.Parameter):\n        \"\"\"\n        ### Initialize a parameter state\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `param` is the parameter tensor $\\theta_{t-1}$\n        \"\"\"\n\n        # Call `init_state` of Adam optimizer which we are extending\n        super().init_state(state, group, param)\n\n        # If `amsgrad` flag is `True` for this parameter group, we maintain the maximum of\n        # exponential moving average of squared gradient\n        if group['amsgrad']:\n            state['max_exp_avg_sq'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n\n    def get_mv(self, state: Dict[str, any], group: Dict[str, any], grad: torch.Tensor):\n        \"\"\"\n        ### Calculate $m_t$ and and $v_t$ or $\\max(v_1, v_2, ..., v_{t-1}, v_t)$\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `grad` is the current gradient tensor $g_t$ for the parameter $\\theta_{t-1}$\n        \"\"\"\n\n        # Get $m_t$ and $v_t$ from *Adam*\n        m, v = super().get_mv(state, group, grad)\n\n        # If this parameter group is using `amsgrad`\n        if group['amsgrad']:\n            # Get $\\max(v_1, v_2, ..., v_{t-1})$.\n            #\n            # \ud83d\uddd2 The paper uses the notation $\\hat{v}_t$ for this, which we don't use\n            # that here because it confuses with the Adam's usage of the same notation\n            # for bias corrected exponential moving average.\n            v_max = state['max_exp_avg_sq']\n            # Calculate $\\max(v_1, v_2, ..., v_{t-1}, v_t)$.\n            #\n            # \ud83e\udd14 I feel you should be taking / maintaining the max of the bias corrected\n            # second exponential average of squared gradient.\n            # But this is how it's\n            # [implemented in PyTorch also](https://github.com/pytorch/pytorch/blob/19f4c5110e8bcad5e7e75375194262fca0a6293a/torch/optim/functional.py#L90).\n            # I guess it doesn't really matter since bias correction only increases the value\n            # and it only makes an actual difference during the early few steps of the training.\n            torch.maximum(v_max, v, out=v_max)\n\n            return m, v_max\n        else:\n            # Fall back to *Adam* if the parameter group is not using `amsgrad`\n            return m, v\n\n\ndef _synthetic_experiment(is_adam: bool):\n    \"\"\"\n    ## Synthetic Experiment\n\n    This is the synthetic experiment described in the paper,\n    that shows a scenario where *Adam* fails.\n\n    The paper (and Adam) formulates the problem of optimizing as\n    minimizing the expected value of a function, $\\mathbb{E}[f(\\theta)]$\n    with respect to the parameters $\\theta$.\n    In the stochastic training setting we do not get hold of the function $f$\n    it self; that is,\n    when you are optimizing a NN $f$ would be the function on  entire\n    batch of data.\n    What we actually evaluate is a mini-batch so the actual function is\n    realization of the stochastic $f$.\n    This is why we are talking about an expected value.\n    So let the function realizations be $f_1, f_2, ..., f_T$ for each time step\n    of training.\n\n    We measure the performance of the optimizer as the regret,\n    $$R(T) = \\sum_{t=1}^T \\big[ f_t(\\theta_t) - f_t(\\theta^*) \\big]$$\n    where $\\theta_t$ is the parameters at time step $t$, and  $\\theta^*$ is the\n    optimal parameters that minimize $\\mathbb{E}[f(\\theta)]$.\n\n    Now lets define the synthetic problem,\n\n    \\begin{align}\n    f_t(x) =\n    \\begin{cases}\n    1010 x,  & \\text{for } t \\mod 101 = 1 \\\\\n    -10  x, & \\text{otherwise}\n    \\end{cases}\n    \\end{align}\n\n    where $-1 \\le x \\le +1$.\n    The optimal solution is $x = -1$.\n\n    This code will try running *Adam* and *AMSGrad* on this problem.\n    \"\"\"\n\n    # Define $x$ parameter\n    x = nn.Parameter(torch.tensor([.0]))\n    # Optimal, $x^* = -1$\n    x_star = nn.Parameter(torch.tensor([-1]), requires_grad=False)\n\n    def func(t: int, x_: nn.Parameter):\n        \"\"\"\n        ### $f_t(x)$\n        \"\"\"\n        if t % 101 == 1:\n            return (1010 * x_).sum()\n        else:\n            return (-10 * x_).sum()\n\n    # Initialize the relevant optimizer\n    if is_adam:\n        optimizer = Adam([x], lr=1e-2, betas=(0.9, 0.99))\n    else:\n        optimizer = AMSGrad([x], lr=1e-2, betas=(0.9, 0.99))\n    # $R(T)$\n    total_regret = 0\n\n    from labml import monit, tracker, experiment\n\n    # Create experiment to record results\n    with experiment.record(name='synthetic', comment='Adam' if is_adam else 'AMSGrad'):\n        # Run for $10^7$ steps\n        for step in monit.loop(10_000_000):\n            # $f_t(\\theta_t) - f_t(\\theta^*)$\n            regret = func(step, x) - func(step, x_star)\n            # $R(T) = \\sum_{t=1}^T \\big[ f_t(\\theta_t) - f_t(\\theta^*) \\big]$\n            total_regret += regret.item()\n            # Track results every 1,000 steps\n            if (step + 1) % 1000 == 0:\n                tracker.save(loss=regret, x=x, regret=total_regret / (step + 1))\n            # Calculate gradients\n            regret.backward()\n            # Optimize\n            optimizer.step()\n            # Clear gradients\n            optimizer.zero_grad()\n\n            # Make sure $-1 \\le x \\le +1$\n            x.data.clamp_(-1., +1.)\n\n\nif __name__ == '__main__':\n    # Run the synthetic experiment is *Adam*.\n    # You can see that Adam converges at $x = +1$\n    _synthetic_experiment(True)\n    # Run the synthetic experiment is *AMSGrad*\n    # You can see that AMSGrad converges to true optimal $x = -1$\n    _synthetic_experiment(False)\n", "labml_nn/optimizers/adam_warmup.py": "\"\"\"\n---\ntitle: Adam optimizer with warm-up\nsummary: A simple PyTorch implementation/tutorial of Adam optimizer with warm-up.\n---\n\n# Adam Optimizer with Warmup\n\nThis extends [AMSGrad optimizer](amsgrad.html) and adds a warmup stage.\n\"\"\"\n\nfrom typing import Dict\n\nfrom labml_nn.optimizers import WeightDecay\nfrom labml_nn.optimizers.amsgrad import AMSGrad\n\n\nclass AdamWarmup(AMSGrad):\n    \"\"\"\n    ## Adam Optimizer with Warmup\n\n    This class extends from AMSGrad optimizer defined in [`amsgrad.py`](amsgrad.html).\n    \"\"\"\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-16,\n                 weight_decay: WeightDecay = WeightDecay(),\n                 optimized_update: bool = True,\n                 amsgrad=False, warmup=0, defaults=None):\n        \"\"\"\n        ### Initialize the optimizer\n\n        * `params` is the list of parameters\n        * `lr` is the learning rate $\\alpha$\n        * `betas` is a tuple of ($\\beta_1$, $\\beta_2$)\n        * `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update`\n        * `weight_decay` is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)\n        * 'optimized_update' is a flag whether to optimize the bias correction of the second moment\n          by doing it after adding $\\epsilon$\n        * `amsgrad` is a flag indicating whether to use AMSGrad or fallback to plain Adam\n        * `warmup` number of warmup steps\n        * `defaults` is a dictionary of default for group values.\n         This is useful when you want to extend the class `AdamWarmup`.\n        \"\"\"\n\n        defaults = {} if defaults is None else defaults\n        defaults.update(dict(warmup=warmup))\n        super().__init__(params, lr, betas, eps, weight_decay, optimized_update, amsgrad, defaults)\n\n    def get_lr(self, state: Dict[str, any], group: Dict[str, any]):\n        \"\"\"\n        ### Get learning-rate\n\n        $$\\alpha \\min \\bigg(1, \\frac{t}{w}\\bigg)$$\n        where $w$ is the number of warmup steps.\n        \"\"\"\n        # If we are in warmup stage\n        if group['warmup'] > state['step']:\n            # A linearly increasing learning rate from $0$ to $\\alpha$\n            return 1e-8 + state['step'] * group['lr'] / group['warmup']\n        else:\n            # Constant learning rate $\\alpha$\n            return group['lr']\n", "labml_nn/optimizers/configs.py": "\"\"\"\n---\ntitle: Configurable optimizer module\nsummary: This implements a configurable module for optimizers.\n---\n\n# Configurable Optimizer\n\"\"\"\n\nfrom typing import Tuple\n\nimport torch\n\nfrom labml.configs import BaseConfigs, option, meta_config\nfrom labml_nn.optimizers import WeightDecay\n\n\nclass OptimizerConfigs(BaseConfigs):\n    \"\"\"\n    <a id=\"OptimizerConfigs\"></a>\n\n    ## Optimizer Configurations\n    \"\"\"\n\n    # Optimizer\n    optimizer: torch.optim.Adam\n\n    # Weight decay\n    weight_decay_obj: WeightDecay\n    # Whether weight decay is decoupled;\n    # i.e. weight decay is not added to gradients\n    weight_decouple: bool = True\n    # Weight decay\n    weight_decay: float = 0.0\n    # Whether weight decay is absolute or should be multiplied by learning rate\n    weight_decay_absolute: bool = False\n\n    # Whether the adam update is optimized (different epsilon)\n    optimized_adam_update: bool = True\n\n    # Parameters to be optimized\n    parameters: any\n\n    # Learning rate $\\alpha$\n    learning_rate: float = 0.01\n    # Beta values $(\\beta_1, \\beta_2)$ for Adam\n    betas: Tuple[float, float] = (0.9, 0.999)\n    # Epsilon $\\epsilon$ for adam\n    eps: float = 1e-08\n\n    # Momentum for SGD\n    momentum: float = 0.5\n    # Whether to use AMSGrad\n    amsgrad: bool = False\n\n    # Number of warmup optimizer steps\n    warmup: int = 2_000\n    # Total number of optimizer steps (for cosine decay)\n    total_steps: int = int(1e10)\n\n    # Whether to degenerate to SGD in AdaBelief\n    degenerate_to_sgd: bool = True\n\n    # Whether to use Rectified Adam in AdaBelief\n    rectify: bool = True\n\n    # Model embedding size for Noam optimizer\n    d_model: int\n\n    rho: float\n\n    def __init__(self):\n        super().__init__(_primary='optimizer')\n\n\nmeta_config(OptimizerConfigs.parameters)\n\n\n@option(OptimizerConfigs.weight_decay_obj, 'L2')\ndef _weight_decay(c: OptimizerConfigs):\n    return WeightDecay(c.weight_decay, c.weight_decouple, c.weight_decay_absolute)\n\n\n@option(OptimizerConfigs.optimizer, 'SGD')\ndef _sgd_optimizer(c: OptimizerConfigs):\n    return torch.optim.SGD(c.parameters, c.learning_rate, c.momentum,\n                           weight_decay=c.weight_decay)\n\n\n@option(OptimizerConfigs.optimizer, 'Adam')\ndef _adam_optimizer(c: OptimizerConfigs):\n    if c.amsgrad:\n        from labml_nn.optimizers.amsgrad import AMSGrad\n        return AMSGrad(c.parameters,\n                       lr=c.learning_rate, betas=c.betas, eps=c.eps,\n                       optimized_update=c.optimized_adam_update,\n                       weight_decay=c.weight_decay_obj, amsgrad=c.amsgrad)\n    else:\n        from labml_nn.optimizers.adam import Adam\n        return Adam(c.parameters,\n                    lr=c.learning_rate, betas=c.betas, eps=c.eps,\n                    optimized_update=c.optimized_adam_update,\n                    weight_decay=c.weight_decay_obj)\n\n\n@option(OptimizerConfigs.optimizer, 'AdamW')\ndef _adam_warmup_optimizer(c: OptimizerConfigs):\n    from labml_nn.optimizers.adam_warmup import AdamWarmup\n    return AdamWarmup(c.parameters,\n                      lr=c.learning_rate, betas=c.betas, eps=c.eps,\n                      weight_decay=c.weight_decay_obj, amsgrad=c.amsgrad, warmup=c.warmup)\n\n\n@option(OptimizerConfigs.optimizer, 'RAdam')\ndef _radam_optimizer(c: OptimizerConfigs):\n    from labml_nn.optimizers.radam import RAdam\n    return RAdam(c.parameters,\n                 lr=c.learning_rate, betas=c.betas, eps=c.eps,\n                 weight_decay=c.weight_decay_obj, amsgrad=c.amsgrad,\n                 degenerated_to_sgd=c.degenerate_to_sgd)\n\n\n@option(OptimizerConfigs.optimizer, 'AdaBelief')\ndef _ada_belief_optimizer(c: OptimizerConfigs):\n    from labml_nn.optimizers.ada_belief import AdaBelief\n    return AdaBelief(c.parameters,\n                     lr=c.learning_rate, betas=c.betas, eps=c.eps,\n                     weight_decay=c.weight_decay_obj, amsgrad=c.amsgrad,\n                     degenerate_to_sgd=c.degenerate_to_sgd,\n                     rectify=c.rectify)\n\n\n@option(OptimizerConfigs.optimizer, 'Noam')\ndef _noam_optimizer(c: OptimizerConfigs):\n    from labml_nn.optimizers.noam import Noam\n    return Noam(c.parameters,\n                lr=c.learning_rate, betas=c.betas, eps=c.eps,\n                weight_decay=c.weight_decay_obj, amsgrad=c.amsgrad, warmup=c.warmup,\n                d_model=c.d_model)\n\n\n@option(OptimizerConfigs.optimizer, 'Sophia')\ndef _sophia_optimizer(c: OptimizerConfigs):\n    from labml_nn.optimizers.sophia import Sophia\n    return Sophia(c.parameters,\n                  lr=c.learning_rate, betas=c.betas, eps=c.eps,\n                  weight_decay=c.weight_decay_obj, rho=c.rho)\n\n\n@option(OptimizerConfigs.optimizer, 'AdamWarmupCosineDecay')\ndef _noam_optimizer(c: OptimizerConfigs):\n    from labml_nn.optimizers.adam_warmup_cosine_decay import AdamWarmupCosineDecay\n    return AdamWarmupCosineDecay(c.parameters,\n                                 lr=c.learning_rate, betas=c.betas, eps=c.eps,\n                                 weight_decay=c.weight_decay_obj, amsgrad=c.amsgrad,\n                                 warmup=c.warmup, total_steps=c.total_steps)\n", "labml_nn/optimizers/adam_warmup_cosine_decay.py": "\"\"\"\n---\ntitle: Adam optimizer with warm-up and cosine decay\nsummary: A PyTorch implementation/tutorial of Adam optimizer with warm-up and cosine decay for GPT.\n---\n\n# Adam Optimizer with Warmup and Cosine Decay\n\nThis extends [AMSGrad optimizer](adam.html) and adds a warmup stage.\n\"\"\"\nimport math\nfrom typing import Dict\n\nfrom labml_nn.optimizers import WeightDecay\nfrom labml_nn.optimizers.amsgrad import AMSGrad\n\n\nclass AdamWarmupCosineDecay(AMSGrad):\n    \"\"\"\n    <a id=\"EmbeddingsWithPositionalEncoding\"></a>\n\n    ## Adam Optimizer with Warmup and Cosine Decay\n\n    This class extends from AMSGrad optimizer defined in [`amsgrad.py`](amsgrad.html).\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-16,\n                 weight_decay: WeightDecay = WeightDecay(),\n                 optimized_update: bool = True,\n                 amsgrad=False, warmup=0, total_steps=1e10, defaults=None):\n        \"\"\"\n        ### Initialize the optimizer\n\n        * `params` is the list of parameters\n        * `lr` is the learning rate $\\alpha$\n        * `betas` is a tuple of ($\\beta_1$, $\\beta_2$)\n        * `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update`\n        * `weight_decay` is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)\n        * 'optimized_update' is a flag whether to optimize the bias correction of the second moment\n          by doing it after adding $\\epsilon$\n        * `amsgrad` is a flag indicating whether to use AMSGrad or fallback to plain Adam\n        * `warmup` number of warmup steps\n        * `total_steps` total number of steps. Cosine decay reaches 0 at this,\n        but stays at 10% of `lr` because we take $\\alpha * \\max(0.1, decay)$\n        * `defaults` is a dictionary of default for group values.\n         This is useful when you want to extend the class `AdamWarmup`.\n        \"\"\"\n\n        defaults = {} if defaults is None else defaults\n        defaults.update(dict(warmup=warmup, total_steps=total_steps))\n        super().__init__(params, lr, betas, eps, weight_decay, optimized_update, amsgrad, defaults)\n\n    def get_lr(self, state: Dict[str, any], group: Dict[str, any]):\n        \"\"\"\n        ### Get learning-rate\n\n        $$\\alpha \\min \\bigg(1, \\frac{t}{w}\\bigg)$$\n        where $w$ is the number of warmup steps.\n        \"\"\"\n        # If we are in warmup stage\n        if group['warmup'] > state['step']:\n            # A linearly increasing learning rate from $0$ to $\\alpha$\n            return 1e-8 + state['step'] * group['lr'] / group['warmup']\n        else:\n            # Constant learning rate $\\alpha$\n            progress = (state['step'] - group['warmup']) / max(1, group['total_steps'] - group['warmup'])\n            return group['lr'] * max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n\n\ndef _test_lr():\n    \"\"\"\n    ### Plot learning rate for different warmups and model sizes\n\n    ![Plot of learning rate](noam_lr.png)\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from torch import nn\n\n    model = nn.Linear(10, 10)\n    opt = AdamWarmupCosineDecay(model.parameters(), warmup=5000, lr=1e-4, total_steps=4e6)\n    steps = 20_000\n    plt.plot(np.arange(1, steps), [opt.get_lr({'step': i}, opt.defaults) for i in range(1, steps)])\n    plt.legend([\"5000:4e6\", \"5000:2e6\", \"5000:1e6\"])\n    plt.title(\"Learning Rate\")\n    plt.show()\n\n    steps = int(6e6)\n    step_size = 1000\n    plt.plot(np.arange(1, steps, step_size), [opt.get_lr({'step': i}, opt.defaults) for i in range(1, steps, step_size)])\n    plt.legend([\"5000:4e6\", \"5000:2e6\", \"5000:1e6\"])\n    plt.title(\"Learning Rate\")\n    plt.show()\n\n\nif __name__ == '__main__':\n    _test_lr()\n", "labml_nn/optimizers/__init__.py": "\"\"\"\n---\ntitle: Optimizers\nsummary: >\n A set of PyTorch implementations/tutorials of popular gradient descent based optimizers.\n Currently includes Adam, AMSGrad and RAdam optimizers.\n---\n\n# Optimizers\n\n## Optimizer Implementations\n* [Adam Optimizer](adam.html)\n* [AMSGrad Optimizer](amsgrad.html)\n* [Adam Optimizer with warmup](adam_warmup.html)\n* [Noam Optimizer](noam.html)\n* [Rectified Adam Optimizer](radam.html)\n* [AdaBelief Optimizer](ada_belief.html)\n* [Sophia-G Optimizer](sophia.html)\n\nThis [MNIST example](mnist_experiment.html) uses these optimizers.\n\n## Generic Adaptive Optimizer Base class and Weight Decay\nThis file defines a common base class for *Adam* and extensions of it.\nThe base class helps use implement other optimizers with minimal code\nbecause of re-usability.\n\nWe also define a special class for L2 weight decay, so that we don't\nhave to implement it inside each of the optimizers,\nand can easily extend to other weight decays like L1 without\nchanging the optimizers.\n\nHere are some concepts on PyTorch optimizers:\n\n### Parameter groups\nPyTorch optimizers group parameters into sets called groups.\nEach group can have its own hyper-parameters like learning rates.\n\nIn most common cases there will be only one group.\nThis is when you initialize your optimizer with,\n\n```python\nOptimizer(model.parameters())\n```\n\nYou can define multiple parameter groups when initializing the optimizer:\n\n```python\nOptimizer([{'params': model1.parameters()}, {'params': model2.parameters(), 'lr': 2}])\n```\n\nHere we pass a list of groups. Each group is a dictionary with its parameters under the key 'params'.\nYou specify any hyper-parameters as well. If the hyper parameters are not defined they will default\nto the optimizer level defaults.\n\nYou can access (and even change) these groups, and their hyper-parameters with `optimizer.param_groups`.\nMost learning rate schedule implementations I've come across do access this and change 'lr'.\n\n### States\nOptimizer maintains states (a dictionary) for each parameter (a tensor), in a dictionary `optimizer.state`.\nThis is where the optimizer maintains things like exponential averages.\n\"\"\"\n\nfrom typing import Dict, Tuple, Any\n\nimport torch\nfrom torch import nn\nfrom torch.optim.optimizer import Optimizer\n\n\nclass GenericAdaptiveOptimizer(Optimizer):\n    \"\"\"\n    ## Base class for *Adam* and extensions\n    \"\"\"\n\n    def __init__(self, params, defaults: Dict[str, Any], lr: float, betas: Tuple[float, float], eps: float):\n        \"\"\"\n        ### Initialize\n\n        * `params` is the collection of parameters or set of parameter groups.\n        * `defaults` a dictionary of default hyper-parameters\n        * `lr` is the learning rate, $\\alpha$\n        * `betas` is the tuple $(\\beta_1, \\beta_2)$\n        * `eps` is $\\epsilon$\n        \"\"\"\n\n        # Check the hyper-parameters\n        if not 0.0 <= lr:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if not 0.0 <= eps:\n            raise ValueError(f\"Invalid epsilon value: {eps}\")\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n\n        # Add the hyper-parameters to the defaults\n        defaults.update(dict(lr=lr, betas=betas, eps=eps))\n        # Initialize the PyTorch optimizer.\n        # This will create parameter groups with the default hyper-parameters\n        super().__init__(params, defaults)\n\n    def init_state(self, state: Dict[str, any], group: Dict[str, any], param: nn.Parameter):\n        \"\"\"\n        ### Initialize state for a given parameter tensor\n\n        This should be overridden with code to initialize `state` for parameters `param`.\n        `group` is the parameter group dictionary to which `param` belongs.\n        \"\"\"\n        pass\n\n    def step_param(self, state: Dict[str, any], group: Dict[str, any], grad: torch.Tensor, param: torch.Tensor):\n        \"\"\"\n        ### Take optimizer step on a parameter tensor\n\n        This should be overridden and take the optimization step on `param` tensor $\\theta$,\n        where `grad` is the gradient for that parameter, $g_t$,\n        `state` is the optimizer state dictionary for that parameter, and\n        `group` is the parameter group dictionary `param` belongs to.\n        \"\"\"\n        pass\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"\n        ### Optimizer step\n\n        We have created a template method that does the common stuff every *Adam* based optimizer needs.\n        \"\"\"\n        # Calculate loss.\n        #\n        # \ud83e\udd14 I'm not sure when you need this. I guess it's if you define a function that\n        # calculates the loss, does `loss.backward` and return the loss, instead of calling\n        # it on your own you could pass it to `optimizer.step`. \ud83e\udd37\u200d\u2642\ufe0f\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        # Iterate through the parameter groups\n        for group in self.param_groups:\n            # Iterate through the parameters in the parameter group\n            for param in group['params']:\n                # Skip if the parameter has no gradient\n                if param.grad is None:\n                    continue\n                # Get the gradient tensor\n                grad = param.grad.data\n                # We don't handle sparse gradients\n                if grad.is_sparse:\n                    raise RuntimeError('GenericAdaptiveOptimizer does not support sparse gradients,'\n                                       ' please consider SparseAdam instead')\n\n                # Get the state for the parameter\n                state = self.state[param]\n\n                # Initialize the state if state is uninitialized\n                if len(state) == 0:\n                    self.init_state(state, group, param)\n\n                # Take the optimization step on the parameter\n                self.step_param(state, group, grad, param)\n\n        # Return the loss, calculated from closure\n        return loss\n\n\nclass WeightDecay:\n    \"\"\"\n    ## L2 Weight decay\n    \"\"\"\n\n    def __init__(self, weight_decay: float = 0., weight_decouple: bool = True, absolute: bool = False):\n        \"\"\"\n        ### Initialize weight decay\n\n        * `weight_decay` is the decay coefficient\n        * `weight_decouple` is a flag indicating whether to add the weight decay to the gradient or directly\n        decay from the parameter. If added to the  gradient it will go through the normal optimizer update.\n        * `absolute` this flag indicates whether the weight decay coefficient is absolute. This is applicable\n        when the decay is performed directly on the parameter. If this is false the actual decay is\n        `weight_decay`\n        * `learning_rate`.\n        \"\"\"\n        # Check hyper-parameters\n        if not 0.0 <= weight_decay:\n            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n\n        self.absolute = absolute\n        self.weight_decouple = weight_decouple\n        self.weight_decay = weight_decay\n\n    def defaults(self):\n        \"\"\"\n        Return defaults for parameter groups\n        \"\"\"\n        return dict(weight_decay=self.weight_decay)\n\n    def __call__(self, param: torch.nn.Parameter, grad: torch.Tensor, group: Dict[str, any]):\n        \"\"\"\n        ### Perform weight decay and return the gradient\n        \"\"\"\n\n        # If we are doing the decay on the parameter directly\n        if self.weight_decouple:\n            # If the weight decay coefficient is absolute\n            if self.absolute:\n                param.data.mul_(1.0 - group['weight_decay'])\n            # Otherwise,\n            else:\n                param.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n            # Return the unmodified gradient\n            return grad\n        else:\n            if group['weight_decay'] != 0:\n                # Add the weight decay to the gradient and return the modified gradient\n                return grad.add(param.data, alpha=group['weight_decay'])\n            else:\n                return grad\n", "labml_nn/optimizers/adam.py": "\"\"\"\n---\ntitle: Adam Optimizer\nsummary: A simple PyTorch implementation/tutorial of Adam optimizer\n---\n\n# Adam Optimizer\n\nThis is a [PyTorch](https://pytorch.org) implementation of popular optimizer *Adam* from paper\n [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980).\n\n*Adam* update is,\n\n\\begin{align}\nm_t &\\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g_t \\\\\nv_t &\\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\\\\n\\hat{m}_t &\\leftarrow \\frac{m_t}{1-\\beta_1^t} \\\\\n\\hat{v}_t &\\leftarrow \\frac{v_t}{1-\\beta_2^t} \\\\\n\\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n\\end{align}\n\nwhere $\\alpha$, $\\beta_1$, $\\beta_2$ and $\\epsilon$ are scalar hyper parameters.\n$m_t$ and $v_t$ are first and second order moments.\n$\\hat{m}_t$  and $\\hat{v}_t$ are biased corrected moments.\n$\\epsilon$ is used as a fix for division by zero error, but also acts as a form of a hyper-parameter\nthat acts against variance in gradients.\n\nEffective step taken assuming $\\epsilon = 0$ is,\n$$\\Delta t = \\alpha \\cdot \\frac{\\hat{m}_t}{\\hat{v}_t}$$\nThis is bounded by,\n$$\\vert \\Delta t \\vert \\le \\alpha \\cdot \\frac{1 - \\beta_1}{\\sqrt{1-\\beta_2}}$$\nwhen $1-\\beta_1 \\gt \\sqrt{1-\\beta_2}$\nand\n$$\\vert \\Delta t\\vert  \\le \\alpha$$\notherwise.\nAnd in most common scenarios,\n$$\\vert \\Delta t \\vert \\approx \\alpha$$\n\"\"\"\n\nimport math\nfrom typing import Dict, Any, Tuple, Optional\n\nimport torch\nfrom labml import tracker\nfrom torch import nn\n\nfrom labml_nn.optimizers import GenericAdaptiveOptimizer, WeightDecay\n\n\nclass Adam(GenericAdaptiveOptimizer):\n    \"\"\"\n    ## Adam Optimizer\n\n    We extend the class `GenericAdaptiveOptimizer` defined in [`__init__.py`](index.html)\n    to implement the Adam optimizer.\n    \"\"\"\n\n    def __init__(self, params,\n                 lr: float = 1e-3, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-16,\n                 weight_decay: WeightDecay = WeightDecay(),\n                 optimized_update: bool = True,\n                 defaults: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        ### Initialize the optimizer\n\n        * `params` is the list of parameters\n        * `lr` is the learning rate $\\alpha$\n        * `betas` is a tuple of ($\\beta_1$, $\\beta_2$)\n        * `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update`\n        * `weight_decay` is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)\n        * `optimized_update` is a flag whether to optimize the bias correction of the second moment\n          by doing it after adding $\\epsilon$\n        * `defaults` is a dictionary of default for group values.\n         This is useful when you want to extend the class `Adam`.\n        \"\"\"\n        defaults = {} if defaults is None else defaults\n        defaults.update(weight_decay.defaults())\n        super().__init__(params, defaults, lr, betas, eps)\n\n        self.weight_decay = weight_decay\n        self.optimized_update = optimized_update\n\n    def init_state(self, state: Dict[str, any], group: Dict[str, any], param: nn.Parameter):\n        \"\"\"\n        ### Initialize a parameter state\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `param` is the parameter tensor $\\theta_{t-1}$\n        \"\"\"\n\n        # This is the number of optimizer steps taken on the parameter, $t$\n        state['step'] = 0\n        # Exponential moving average of gradients, $m_t$\n        state['exp_avg'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n        # Exponential moving average of squared gradient values, $v_t$\n        state['exp_avg_sq'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n\n    def get_mv(self, state: Dict[str, Any], group: Dict[str, Any], grad: torch.Tensor):\n        \"\"\"\n        ### Calculate $m_t$ and and $v_t$\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `grad` is the current gradient tensor $g_t$ for the parameter $\\theta_{t-1}$\n        \"\"\"\n\n        # Get $\\beta_1$ and $\\beta_2$\n        beta1, beta2 = group['betas']\n\n        # Get $m_{t-1}$ and $v_{t-1}$\n        m, v = state['exp_avg'], state['exp_avg_sq']\n\n        # In-place calculation of $m_t$\n        # $$m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\n        m.mul_(beta1).add_(grad, alpha=1 - beta1)\n        # In-place calculation of $v_t$\n        # $$v_t \\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$$\n        v.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n        return m, v\n\n    def get_lr(self, state: Dict[str, any], group: Dict[str, any]):\n        \"\"\"\n        ### Get learning-rate\n\n        This returns the modified learning rate based on the state.\n        For *Adam* this is just the specified learning rate for the parameter group,\n        $\\alpha$.\n        \"\"\"\n        return group['lr']\n\n    def adam_update(self, state: Dict[str, any], group: Dict[str, any], param: torch.nn.Parameter,\n                    m: torch.Tensor, v: torch.Tensor):\n        \"\"\"\n        ### Do the *Adam* parameter update\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `param` is the parameter tensor $\\theta_{t-1}$\n        * `m` and `v` are the uncorrected first and second moments $m_t$ and $v_t$.\n\n        This computes the following\n\n        \\begin{align}\n        \\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n        \\end{align}\n\n        Since $\\alpha$, $\\beta_1$, $\\beta_2$ and $\\epsilon$ are scalars and others are tensors\n        we modify this calculation to optimize the computation.\n\n        \\begin{align}\n        \\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\\\\n        \\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\cdot\n                \\frac{m_t / (1-\\beta_1^t)}{\\sqrt{v_t/(1-\\beta_2^t)} + \\epsilon} \\\\\n        \\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t} \\cdot\n                \\frac{m_t}{\\sqrt{v_t} + \\hat{\\epsilon}} \\\\\n        \\end{align}\n\n        where\n        $$\\hat{\\epsilon} = (1-\\beta_2^t) \\epsilon$$\n        is what we should specify as the hyper-parameter.\n        \"\"\"\n\n        # Get $\\beta_1$ and $\\beta_2$\n        beta1, beta2 = group['betas']\n        # Bias correction term for $\\hat{m}_t$, $1 - \\beta_1^t$\n        bias_correction1 = 1 - beta1 ** state['step']\n        # Bias correction term for $\\hat{v}_t$, $1 - \\beta_2^t$\n        bias_correction2 = 1 - beta2 ** state['step']\n\n        # Get learning rate\n        lr = self.get_lr(state, group)\n\n        # Whether to optimize the computation\n        if self.optimized_update:\n            # $\\sqrt{v_t} + \\hat{\\epsilon}$\n            denominator = v.sqrt().add_(group['eps'])\n            # $\\alpha \\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t}$\n            step_size = lr * math.sqrt(bias_correction2) / bias_correction1\n            # $\\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\frac{\\sqrt{1-\\beta_2^t}}{1-\\beta_1^t} \\cdot\n            #  \\frac{m_t}{\\sqrt{v_t} + \\hat{\\epsilon}}$\n            param.data.addcdiv_(m, denominator, value=-step_size)\n        # Computation without optimization\n        else:\n            # $\\frac{\\sqrt{v_t}}{\\sqrt{1-\\beta_2^t}} + \\epsilon$\n            denominator = (v.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n            # $\\frac{\\alpha}{1-\\beta_1^t}$\n            step_size = lr / bias_correction1\n            # $\\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\cdot\n            # \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n            param.data.addcdiv_(m, denominator, value=-step_size)\n\n    def step_param(self, state: Dict[str, any], group: Dict[str, any], grad: torch.Tensor, param: torch.nn.Parameter):\n        \"\"\"\n        ### Take an update step for a given parameter tensor\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `grad` is the current gradient tensor  $g_t$ for the parameter $\\theta_{t-1}$\n        * `param` is the parameter tensor $\\theta_{t-1}$\n        \"\"\"\n\n        # Calculate weight decay\n        grad = self.weight_decay(param, grad, group)\n\n        # Get $m_t$ and $v_t$\n        m, v = self.get_mv(state, group, grad)\n\n        # Increment $t$ the number of optimizer steps\n        state['step'] += 1\n\n        # Perform *Adam* update\n        self.adam_update(state, group, param, m, v)\n", "labml_nn/optimizers/sophia.py": "\"\"\"\n---\ntitle: Sophia Optimizer\nsummary: A simple PyTorch implementation/tutorial of Sophia optimizer\n---\n\n# Sophia Optimizer\n\nThis is a [PyTorch](https://pytorch.org) implementation of *Sophia-G* from paper\n [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training](https://arxiv.org/abs/2305.14342).\nOfficial implementation is available at [Liuhong99/Sophia](https://github.com/Liuhong99/Sophia).\n\nSophia is more adaptive to heterogeneous curvatures than Adam, more resistant\nto non-convexity and rapid change of Hessian than Newton\u2019s method, and also uses a low-cost\npre-conditioner.\n\nSophia keeps diagonal Hessian estimates with EMA across iterations.\nThe diagonal Hessian $\\hat{h}_t$ is calculated every $k$ steps.\n\n\\begin{align}\nh_t = \\beta_2 h_{t-k} + (1 - \\beta_2) \\hat{h}_t \\ \\ \\ \\ \\text{ if } t \\text{ mod } k = 1; \\text{ else }  h_t = h_{t-1}\n\\end{align}\n\nSophia uses EMA of gradients $m_t$, only considers positive entries of\n the diagonal Hessian and does per-coordinate clipping to the update.\n\n\\begin{align}\nm_t &\\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1)g_t \\\\\n\\theta_{t + 1} &\\leftarrow \\theta_t - \\eta \\cdot \\operatorname{clip} \\bigg(\\frac{m_t}{ \\max \\{h_t, \\epsilon \\} }, \\rho \\bigg)\n\\end{align}\n\nwhere $\\epsilon$ is a very small value to prevent division by $0$.\n\n### Gauss-Newton-Bartlett (GNB) estimator\n\n\\begin{align}\n\\hat{L}(\\theta) &= \\frac{1}{B} \\sum^{B}_{b=1} \\ell_{CE} \\big( f(\\theta, x_b), \\hat{y}_b \\big) \\\\\n\\hat{h}_t &= B \\cdot \\nabla_\\theta \\hat{L} (\\theta) \\odot \\nabla_\\theta \\hat{L} (\\theta)\n\\end{align}\n\nwhere $x_b$ are the inputs,\n$B$ is the batch size (number of inputs/tokens),\n$\\ell_{CE}$ is cross entropy loss, and\n$\\hat{y}_b$ are sampled from the logits $f(\\theta, x_b)$.\n\nNote that this hessian estimate is always positive and therefore we\ncan replace $\\max \\{h_t, \\epsilon \\}$ with $h_t + \\epsilon$.\n\nSophia with Gauss-Newton-Bartlett (GNB) estimator is **Sophia-G**\n\nHere is an [experiment](../transformers/basic/with_sophia.html) that uses Sophia-G to train a transformer.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple, Optional\n\nimport torch\nfrom torch import nn\n\nfrom labml_nn.optimizers import GenericAdaptiveOptimizer, WeightDecay\n\n\nclass Sophia(GenericAdaptiveOptimizer):\n    \"\"\"\n    ## Sophia-G Optimizer\n\n    We extend the class `GenericAdaptiveOptimizer` defined in [`__init__.py`](index.html)\n    to implement the Sophia optimizer.\n    \"\"\"\n\n    def __init__(self, params,\n                 lr: float = 1e-4, betas: Tuple[float, float] = (0.9, 0.95), eps: float = 1e-12,\n                 rho: float = 0.03,\n                 weight_decay: WeightDecay = WeightDecay(),\n                 defaults: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        ### Initialize the optimizer\n\n        * `params` is the list of parameters\n        * `lr` is the maximum learning rate $\\eta \\rho$\n        * `betas` is a tuple of ($\\beta_1$, $\\beta_2$)\n        * `eps` is $\\epsilon$\n        * `pho` is $\\rho$\n        * `weight_decay` is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)\n        * `defaults` is a dictionary of default for group values.\n         This is useful when you want to extend the class `Adam`.\n        \"\"\"\n        defaults = {} if defaults is None else defaults\n        defaults.update(weight_decay.defaults())\n        defaults.update(dict(rho=rho))\n        super().__init__(params, defaults, lr, betas, eps)\n\n        self.weight_decay = weight_decay\n\n    def init_state(self, state: Dict[str, any], group: Dict[str, any], param: nn.Parameter):\n        \"\"\"\n        ### Initialize a parameter state\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `param` is the parameter tensor $\\theta_{t-1}$\n        \"\"\"\n\n        # This is the number of optimizer steps taken on the parameter, $t$\n        state['step'] = 0\n        # Exponential moving average of gradients, $m_t$\n        state['exp_avg'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n        # Exponential moving average of Hessian diagonal, $h_t$\n        state['hessian'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n\n    def update_hessian(self, n_tokens_training_batch):\n        \"\"\"\n        ### Update the EMA of Hessian diagonal $h_t$\n\n        * `n_tokens_training_batch` is the number of tokens/inputs in the batch $B$\n\n        \\begin{align}\n        \\hat{h}_t &= B \\cdot \\nabla_\\theta \\hat{L} (\\theta) \\odot \\nabla_\\theta \\hat{L} (\\theta) \\\\\n        h_t &= \\beta_2 h_{t-k} + (1 - \\beta_2) \\hat{h}_t\n        \\end{align}\n        \"\"\"\n\n        # Iterate through parameter groups\n        for group in self.param_groups:\n            # $\\beta_2$\n            _, beta2 = group['betas']\n            # Iterate through parameters\n            for p in group['params']:\n                # Skip parameters without gradients\n                if p.grad is None:\n                    continue\n\n                # Get optimizer state\n                state = self.state[p]\n\n                # Initialize state if empty\n                if len(state) == 0:\n                    self.init_state(state, group, p)\n\n                # Update EMA Hessian diagonal\n                #\n                # \\begin{align}\n                # \\hat{h}_t &= B \\cdot \\nabla_\\theta \\hat{L} (\\theta) \\odot \\nabla_\\theta \\hat{L} (\\theta) \\\\\n                # h_t &= \\beta_2 h_{t-k} + (1 - \\beta_2) \\hat{h}_t\n                # \\end{align}\n                state['hessian'].mul_(beta2).addcmul_(p.grad, p.grad, value=(1 - beta2) * n_tokens_training_batch)\n\n    def step_param(self, state: Dict[str, any], group: Dict[str, any], grad: torch.Tensor, param: torch.nn.Parameter):\n        \"\"\"\n        ### Take an update step for a given parameter tensor\n\n        * `state` is the optimizer state of the parameter (tensor)\n        * `group` stores optimizer attributes of the parameter group\n        * `grad` is the current gradient tensor  $g_t$ for the parameter $\\theta_{t-1}$\n        * `param` is the parameter tensor $\\theta_{t-1}$\n\n        We do the following parameter update,\n\n        \\begin{align}\n        \\theta_{t + 1} &\\leftarrow \\theta_t - \\eta \\cdot \\operatorname{clip} \\bigg(\\frac{m_t}{h_t + \\epsilon}, \\rho \\bigg)\n        \\end{align}\n        \"\"\"\n\n        # Calculate weight decay\n        grad = self.weight_decay(param, grad, group)\n\n        # Get $\\beta_1$ and $\\beta_2$\n        beta1, beta2 = group['betas']\n        # Get $\\rho$\n        rho = group['rho']\n\n        # Get $m_{t-1}$ and $h_{t}$\n        m, hessian = state['exp_avg'], state['hessian']\n\n        # In-place calculation of $m_t$\n        # $$m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\n        m.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n        # Increment $t$ the number of optimizer steps\n        state['step'] += 1\n\n        # Get maximum learning rate $\\eta \\rho$\n        lr = group['lr']\n\n        # $\\eta$\n        eta = lr / rho\n\n        # $$\\operatorname{clip} \\bigg(\\frac{m_t}{h_t + \\epsilon}, \\rho \\bigg)$$\n        ratio = (m / (hessian + group['eps'])).clamp(-rho, rho)\n\n        # $$\\theta_{t + 1} \\leftarrow \\theta_t - \\eta \\cdot \\operatorname{clip} \\bigg(\\frac{m_t}{h_t + \\epsilon}, \\rho \\bigg)$$\n        param.data.add_(ratio, alpha=-eta)\n", "labml_nn/sketch_rnn/__init__.py": "\"\"\"\n---\ntitle: Sketch RNN\nsummary: >\n  This is an annotated PyTorch implementation of the Sketch RNN from paper A Neural Representation of Sketch Drawings.\n  Sketch RNN is a sequence-to-sequence model that generates sketches of objects such as bicycles, cats, etc.\n---\n\n# Sketch RNN\n\nThis is an annotated [PyTorch](https://pytorch.org) implementation of the paper\n[A Neural Representation of Sketch Drawings](https://arxiv.org/abs/1704.03477).\n\nSketch RNN is a sequence-to-sequence variational auto-encoder.\nBoth encoder and decoder are recurrent neural network models.\nIt learns to reconstruct stroke based simple drawings, by predicting\na series of strokes.\nDecoder predicts each stroke as a mixture of Gaussian's.\n\n### Getting data\nDownload data from [Quick, Draw! Dataset](https://github.com/googlecreativelab/quickdraw-dataset).\nThere is a link to download `npz` files in *Sketch-RNN QuickDraw Dataset* section of the readme.\nPlace the downloaded `npz` file(s) in `data/sketch` folder.\nThis code is configured to use `bicycle` dataset.\nYou can change this in configurations.\n\n### Acknowledgements\nTook help from [PyTorch Sketch RNN](https://github.com/alexis-jacq/Pytorch-Sketch-RNN) project by\n[Alexis David Jacq](https://github.com/alexis-jacq)\n\"\"\"\n\nimport math\nfrom typing import Optional, Tuple, Any\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom matplotlib import pyplot as plt\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport einops\nfrom labml import lab, experiment, tracker, monit\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_helpers.module import Module\nfrom labml_helpers.optimizer import OptimizerConfigs\nfrom labml_helpers.train_valid import TrainValidConfigs, hook_model_outputs, BatchIndex\n\n\nclass StrokesDataset(Dataset):\n    \"\"\"\n    ## Dataset\n\n    This class loads and pre-processes the data.\n    \"\"\"\n\n    def __init__(self, dataset: np.array, max_seq_length: int, scale: Optional[float] = None):\n        \"\"\"\n        `dataset` is a list of numpy arrays of shape [seq_len, 3].\n        It is a sequence of strokes, and each stroke is represented by\n        3 integers.\n        First two are the displacements along x and y ($\\Delta x$, $\\Delta y$)\n        and the last integer represents the state of the pen, $1$ if it's touching\n        the paper and $0$ otherwise.\n        \"\"\"\n\n        data = []\n        # We iterate through each of the sequences and filter\n        for seq in dataset:\n            # Filter if the length of the sequence of strokes is within our range\n            if 10 < len(seq) <= max_seq_length:\n                # Clamp $\\Delta x$, $\\Delta y$ to $[-1000, 1000]$\n                seq = np.minimum(seq, 1000)\n                seq = np.maximum(seq, -1000)\n                # Convert to a floating point array and add to `data`\n                seq = np.array(seq, dtype=np.float32)\n                data.append(seq)\n\n        # We then calculate the scaling factor which is the\n        # standard deviation of ($\\Delta x$, $\\Delta y$) combined.\n        # Paper notes that the mean is not adjusted for simplicity,\n        # since the mean is anyway close to $0$.\n        if scale is None:\n            scale = np.std(np.concatenate([np.ravel(s[:, 0:2]) for s in data]))\n        self.scale = scale\n\n        # Get the longest sequence length among all sequences\n        longest_seq_len = max([len(seq) for seq in data])\n\n        # We initialize PyTorch data array with two extra steps for start-of-sequence (sos)\n        # and end-of-sequence (eos).\n        # Each step is a vector $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n        # Only one of $p_1, p_2, p_3$ is $1$ and the others are $0$.\n        # They represent *pen down*, *pen up* and *end-of-sequence* in that order.\n        # $p_1$ is $1$ if the pen touches the paper in the next step.\n        # $p_2$ is $1$ if the pen doesn't touch the paper in the next step.\n        # $p_3$ is $1$ if it is the end of the drawing.\n        self.data = torch.zeros(len(data), longest_seq_len + 2, 5, dtype=torch.float)\n        # The mask array needs only one extra-step since it is for the outputs of the\n        # decoder, which takes in `data[:-1]` and predicts next step.\n        self.mask = torch.zeros(len(data), longest_seq_len + 1)\n\n        for i, seq in enumerate(data):\n            seq = torch.from_numpy(seq)\n            len_seq = len(seq)\n            # Scale and set $\\Delta x, \\Delta y$\n            self.data[i, 1:len_seq + 1, :2] = seq[:, :2] / scale\n            # $p_1$\n            self.data[i, 1:len_seq + 1, 2] = 1 - seq[:, 2]\n            # $p_2$\n            self.data[i, 1:len_seq + 1, 3] = seq[:, 2]\n            # $p_3$\n            self.data[i, len_seq + 1:, 4] = 1\n            # Mask is on until end of sequence\n            self.mask[i, :len_seq + 1] = 1\n\n        # Start-of-sequence is $(0, 0, 1, 0, 0)$\n        self.data[:, 0, 2] = 1\n\n    def __len__(self):\n        \"\"\"Size of the dataset\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx: int):\n        \"\"\"Get a sample\"\"\"\n        return self.data[idx], self.mask[idx]\n\n\nclass BivariateGaussianMixture:\n    \"\"\"\n    ## Bi-variate Gaussian mixture\n\n    The mixture is represented by $\\Pi$ and\n    $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n    This class adjusts temperatures and creates the categorical and Gaussian\n    distributions from the parameters.\n    \"\"\"\n\n    def __init__(self, pi_logits: torch.Tensor, mu_x: torch.Tensor, mu_y: torch.Tensor,\n                 sigma_x: torch.Tensor, sigma_y: torch.Tensor, rho_xy: torch.Tensor):\n        self.pi_logits = pi_logits\n        self.mu_x = mu_x\n        self.mu_y = mu_y\n        self.sigma_x = sigma_x\n        self.sigma_y = sigma_y\n        self.rho_xy = rho_xy\n\n    @property\n    def n_distributions(self):\n        \"\"\"Number of distributions in the mixture, $M$\"\"\"\n        return self.pi_logits.shape[-1]\n\n    def set_temperature(self, temperature: float):\n        \"\"\"\n        Adjust by temperature $\\tau$\n        \"\"\"\n        # $$\\hat{\\Pi_k} \\leftarrow \\frac{\\hat{\\Pi_k}}{\\tau}$$\n        self.pi_logits /= temperature\n        # $$\\sigma^2_x \\leftarrow \\sigma^2_x \\tau$$\n        self.sigma_x *= math.sqrt(temperature)\n        # $$\\sigma^2_y \\leftarrow \\sigma^2_y \\tau$$\n        self.sigma_y *= math.sqrt(temperature)\n\n    def get_distribution(self):\n        # Clamp $\\sigma_x$, $\\sigma_y$ and $\\rho_{xy}$ to avoid getting `NaN`s\n        sigma_x = torch.clamp_min(self.sigma_x, 1e-5)\n        sigma_y = torch.clamp_min(self.sigma_y, 1e-5)\n        rho_xy = torch.clamp(self.rho_xy, -1 + 1e-5, 1 - 1e-5)\n\n        # Get means\n        mean = torch.stack([self.mu_x, self.mu_y], -1)\n        # Get covariance matrix\n        cov = torch.stack([\n            sigma_x * sigma_x, rho_xy * sigma_x * sigma_y,\n            rho_xy * sigma_x * sigma_y, sigma_y * sigma_y\n        ], -1)\n        cov = cov.view(*sigma_y.shape, 2, 2)\n\n        # Create bi-variate normal distribution.\n        #\n        # \ud83d\udcdd It would be efficient to `scale_tril` matrix as `[[a, 0], [b, c]]`\n        # where\n        # $$a = \\sigma_x, b = \\rho_{xy} \\sigma_y, c = \\sigma_y \\sqrt{1 - \\rho^2_{xy}}$$.\n        # But for simplicity we use co-variance matrix.\n        # [This is a good resource](https://www2.stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec22.pdf)\n        # if you want to read up more about bi-variate distributions, their co-variance matrix,\n        # and probability density function.\n        multi_dist = torch.distributions.MultivariateNormal(mean, covariance_matrix=cov)\n\n        # Create categorical distribution $\\Pi$ from logits\n        cat_dist = torch.distributions.Categorical(logits=self.pi_logits)\n\n        #\n        return cat_dist, multi_dist\n\n\nclass EncoderRNN(Module):\n    \"\"\"\n    ## Encoder module\n\n    This consists of a bidirectional LSTM\n    \"\"\"\n\n    def __init__(self, d_z: int, enc_hidden_size: int):\n        super().__init__()\n        # Create a bidirectional LSTM taking a sequence of\n        # $(\\Delta x, \\Delta y, p_1, p_2, p_3)$ as input.\n        self.lstm = nn.LSTM(5, enc_hidden_size, bidirectional=True)\n        # Head to get $\\mu$\n        self.mu_head = nn.Linear(2 * enc_hidden_size, d_z)\n        # Head to get $\\hat{\\sigma}$\n        self.sigma_head = nn.Linear(2 * enc_hidden_size, d_z)\n\n    def forward(self, inputs: torch.Tensor, state=None):\n        # The hidden state of the bidirectional LSTM is the concatenation of the\n        # output of the last token in the forward direction and\n        # first token in the reverse direction, which is what we want.\n        # $$h_{\\rightarrow} = encode_{\\rightarrow}(S),\n        # h_{\\leftarrow} = encode\u2190_{\\leftarrow}(S_{reverse}),\n        # h = [h_{\\rightarrow}; h_{\\leftarrow}]$$\n        _, (hidden, cell) = self.lstm(inputs.float(), state)\n        # The state has shape `[2, batch_size, hidden_size]`,\n        # where the first dimension is the direction.\n        # We rearrange it to get $h = [h_{\\rightarrow}; h_{\\leftarrow}]$\n        hidden = einops.rearrange(hidden, 'fb b h -> b (fb h)')\n\n        # $\\mu$\n        mu = self.mu_head(hidden)\n        # $\\hat{\\sigma}$\n        sigma_hat = self.sigma_head(hidden)\n        # $\\sigma = \\exp(\\frac{\\hat{\\sigma}}{2})$\n        sigma = torch.exp(sigma_hat / 2.)\n\n        # Sample $z = \\mu + \\sigma \\cdot \\mathcal{N}(0, I)$\n        z = mu + sigma * torch.normal(mu.new_zeros(mu.shape), mu.new_ones(mu.shape))\n\n        #\n        return z, mu, sigma_hat\n\n\nclass DecoderRNN(Module):\n    \"\"\"\n    ## Decoder module\n\n    This consists of a LSTM\n    \"\"\"\n\n    def __init__(self, d_z: int, dec_hidden_size: int, n_distributions: int):\n        super().__init__()\n        # LSTM takes $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ as input\n        self.lstm = nn.LSTM(d_z + 5, dec_hidden_size)\n\n        # Initial state of the LSTM is $[h_0; c_0] = \\tanh(W_{z}z + b_z)$.\n        # `init_state` is the linear transformation for this\n        self.init_state = nn.Linear(d_z, 2 * dec_hidden_size)\n\n        # This layer produces outputs for each of the `n_distributions`.\n        # Each distribution needs six parameters\n        # $(\\hat{\\Pi_i}, \\mu_{x_i}, \\mu_{y_i}, \\hat{\\sigma_{x_i}}, \\hat{\\sigma_{y_i}} \\hat{\\rho_{xy_i}})$\n        self.mixtures = nn.Linear(dec_hidden_size, 6 * n_distributions)\n\n        # This head is for the logits $(\\hat{q_1}, \\hat{q_2}, \\hat{q_3})$\n        self.q_head = nn.Linear(dec_hidden_size, 3)\n        # This is to calculate $\\log(q_k)$ where\n        # $$q_k = \\operatorname{softmax}(\\hat{q})_k = \\frac{\\exp(\\hat{q_k})}{\\sum_{j = 1}^3 \\exp(\\hat{q_j})}$$\n        self.q_log_softmax = nn.LogSoftmax(-1)\n\n        # These parameters are stored for future reference\n        self.n_distributions = n_distributions\n        self.dec_hidden_size = dec_hidden_size\n\n    def forward(self, x: torch.Tensor, z: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]]):\n        # Calculate the initial state\n        if state is None:\n            # $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n            h, c = torch.split(torch.tanh(self.init_state(z)), self.dec_hidden_size, 1)\n            # `h` and `c` have shapes `[batch_size, lstm_size]`. We want to shape them\n            # to `[1, batch_size, lstm_size]` because that's the shape used in LSTM.\n            state = (h.unsqueeze(0).contiguous(), c.unsqueeze(0).contiguous())\n\n        # Run the LSTM\n        outputs, state = self.lstm(x, state)\n\n        # Get $\\log(q)$\n        q_logits = self.q_log_softmax(self.q_head(outputs))\n\n        # Get $(\\hat{\\Pi_i}, \\mu_{x,i}, \\mu_{y,i}, \\hat{\\sigma_{x,i}},\n        # \\hat{\\sigma_{y,i}} \\hat{\\rho_{xy,i}})$.\n        # `torch.split` splits the output into 6 tensors of size `self.n_distribution`\n        # across dimension `2`.\n        pi_logits, mu_x, mu_y, sigma_x, sigma_y, rho_xy = \\\n            torch.split(self.mixtures(outputs), self.n_distributions, 2)\n\n        # Create a bi-variate Gaussian mixture\n        # $\\Pi$ and \n        # $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n        # where\n        # $$\\sigma_{x,i} = \\exp(\\hat{\\sigma_{x,i}}), \\sigma_{y,i} = \\exp(\\hat{\\sigma_{y,i}}),\n        # \\rho_{xy,i} = \\tanh(\\hat{\\rho_{xy,i}})$$\n        # and\n        # $$\\Pi_i = \\operatorname{softmax}(\\hat{\\Pi})_i = \\frac{\\exp(\\hat{\\Pi_i})}{\\sum_{j = 1}^3 \\exp(\\hat{\\Pi_j})}$$\n        #\n        # $\\Pi$ is the categorical probabilities of choosing the distribution out of the mixture\n        # $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n        dist = BivariateGaussianMixture(pi_logits, mu_x, mu_y,\n                                        torch.exp(sigma_x), torch.exp(sigma_y), torch.tanh(rho_xy))\n\n        #\n        return dist, q_logits, state\n\n\nclass ReconstructionLoss(Module):\n    \"\"\"\n    ## Reconstruction Loss\n    \"\"\"\n\n    def forward(self, mask: torch.Tensor, target: torch.Tensor,\n                 dist: 'BivariateGaussianMixture', q_logits: torch.Tensor):\n        # Get $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n        pi, mix = dist.get_distribution()\n        # `target` has shape `[seq_len, batch_size, 5]` where the last dimension is the features\n        # $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n        # We want to get $\\Delta x, \\Delta$ y and get the probabilities from each of the distributions\n        # in the mixture $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n        #\n        # `xy` will have shape `[seq_len, batch_size, n_distributions, 2]`\n        xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, dist.n_distributions, -1)\n        # Calculate the probabilities\n        # $$p(\\Delta x, \\Delta y) =\n        # \\sum_{j=1}^M \\Pi_j \\mathcal{N} \\big( \\Delta x, \\Delta y \\vert\n        # \\mu_{x,j}, \\mu_{y,j}, \\sigma_{x,j}, \\sigma_{y,j}, \\rho_{xy,j}\n        # \\big)$$\n        probs = torch.sum(pi.probs * torch.exp(mix.log_prob(xy)), 2)\n\n        # $$L_s = - \\frac{1}{N_{max}} \\sum_{i=1}^{N_s} \\log \\big (p(\\Delta x, \\Delta y) \\big)$$\n        # Although `probs` has $N_{max}$ (`longest_seq_len`) elements, the sum is only taken\n        # upto $N_s$ because the rest is masked out.\n        #\n        # It might feel like we should be taking the sum and dividing by $N_s$ and not $N_{max}$,\n        # but this will give higher weight for individual predictions in shorter sequences.\n        # We give equal weight to each prediction $p(\\Delta x, \\Delta y)$ when we divide by $N_{max}$\n        loss_stroke = -torch.mean(mask * torch.log(1e-5 + probs))\n\n        # $$L_p = - \\frac{1}{N_{max}} \\sum_{i=1}^{N_{max}} \\sum_{k=1}^{3} p_{k,i} \\log(q_{k,i})$$\n        loss_pen = -torch.mean(target[:, :, 2:] * q_logits)\n\n        # $$L_R = L_s + L_p$$\n        return loss_stroke + loss_pen\n\n\nclass KLDivLoss(Module):\n    \"\"\"\n    ## KL-Divergence loss\n\n    This calculates the KL divergence between a given normal distribution and $\\mathcal{N}(0, 1)$\n    \"\"\"\n\n    def forward(self, sigma_hat: torch.Tensor, mu: torch.Tensor):\n        # $$L_{KL} = - \\frac{1}{2 N_z} \\bigg( 1 + \\hat{\\sigma} - \\mu^2 - \\exp(\\hat{\\sigma}) \\bigg)$$\n        return -0.5 * torch.mean(1 + sigma_hat - mu ** 2 - torch.exp(sigma_hat))\n\n\nclass Sampler:\n    \"\"\"\n    ## Sampler\n\n    This samples a sketch from the decoder and plots it\n    \"\"\"\n\n    def __init__(self, encoder: EncoderRNN, decoder: DecoderRNN):\n        self.decoder = decoder\n        self.encoder = encoder\n\n    def sample(self, data: torch.Tensor, temperature: float):\n        # $N_{max}$\n        longest_seq_len = len(data)\n\n        # Get $z$ from the encoder\n        z, _, _ = self.encoder(data)\n\n        # Start-of-sequence stroke is $(0, 0, 1, 0, 0)$\n        s = data.new_tensor([0, 0, 1, 0, 0])\n        seq = [s]\n        # Initial decoder is `None`.\n        # The decoder will initialize it to $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n        state = None\n\n        # We don't need gradients\n        with torch.no_grad():\n            # Sample $N_{max}$ strokes\n            for i in range(longest_seq_len):\n                # $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ is the input to the decoder\n                data = torch.cat([s.view(1, 1, -1), z.unsqueeze(0)], 2)\n                # Get $\\Pi$, $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$,\n                # $q$ and the next state from the decoder\n                dist, q_logits, state = self.decoder(data, z, state)\n                # Sample a stroke\n                s = self._sample_step(dist, q_logits, temperature)\n                # Add the new stroke to the sequence of strokes\n                seq.append(s)\n                # Stop sampling if $p_3 = 1$. This indicates that sketching has stopped\n                if s[4] == 1:\n                    break\n\n        # Create a PyTorch tensor of the sequence of strokes\n        seq = torch.stack(seq)\n\n        # Plot the sequence of strokes\n        self.plot(seq)\n\n    @staticmethod\n    def _sample_step(dist: 'BivariateGaussianMixture', q_logits: torch.Tensor, temperature: float):\n        # Set temperature $\\tau$ for sampling. This is implemented in class `BivariateGaussianMixture`.\n        dist.set_temperature(temperature)\n        # Get temperature adjusted $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n        pi, mix = dist.get_distribution()\n        # Sample from $\\Pi$ the index of the distribution to use from the mixture\n        idx = pi.sample()[0, 0]\n\n        # Create categorical distribution $q$ with log-probabilities `q_logits` or $\\hat{q}$\n        q = torch.distributions.Categorical(logits=q_logits / temperature)\n        # Sample from $q$\n        q_idx = q.sample()[0, 0]\n\n        # Sample from the normal distributions in the mixture and pick the one indexed by `idx`\n        xy = mix.sample()[0, 0, idx]\n\n        # Create an empty stroke $(\\Delta x, \\Delta y, q_1, q_2, q_3)$\n        stroke = q_logits.new_zeros(5)\n        # Set $\\Delta x, \\Delta y$\n        stroke[:2] = xy\n        # Set $q_1, q_2, q_3$\n        stroke[q_idx + 2] = 1\n        #\n        return stroke\n\n    @staticmethod\n    def plot(seq: torch.Tensor):\n        # Take the cumulative sums of $(\\Delta x, \\Delta y)$ to get $(x, y)$\n        seq[:, 0:2] = torch.cumsum(seq[:, 0:2], dim=0)\n        # Create a new numpy array of the form $(x, y, q_2)$\n        seq[:, 2] = seq[:, 3]\n        seq = seq[:, 0:3].detach().cpu().numpy()\n\n        # Split the array at points where $q_2$ is $1$.\n        # i.e. split the array of strokes at the points where the pen is lifted from the paper.\n        # This gives a list of sequence of strokes.\n        strokes = np.split(seq, np.where(seq[:, 2] > 0)[0] + 1)\n        # Plot each sequence of strokes\n        for s in strokes:\n            plt.plot(s[:, 0], -s[:, 1])\n        # Don't show axes\n        plt.axis('off')\n        # Show the plot\n        plt.show()\n\n\nclass Configs(TrainValidConfigs):\n    \"\"\"\n    ## Configurations\n\n    These are default configurations which can later be adjusted by passing a `dict`.\n    \"\"\"\n\n    # Device configurations to pick the device to run the experiment\n    device: torch.device = DeviceConfigs()\n    #\n    encoder: EncoderRNN\n    decoder: DecoderRNN\n    optimizer: optim.Adam\n    sampler: Sampler\n\n    dataset_name: str\n    train_loader: DataLoader\n    valid_loader: DataLoader\n    train_dataset: StrokesDataset\n    valid_dataset: StrokesDataset\n\n    # Encoder and decoder sizes\n    enc_hidden_size = 256\n    dec_hidden_size = 512\n\n    # Batch size\n    batch_size = 100\n\n    # Number of features in $z$\n    d_z = 128\n    # Number of distributions in the mixture, $M$\n    n_distributions = 20\n\n    # Weight of KL divergence loss, $w_{KL}$\n    kl_div_loss_weight = 0.5\n    # Gradient clipping\n    grad_clip = 1.\n    # Temperature $\\tau$ for sampling\n    temperature = 0.4\n\n    # Filter out stroke sequences longer than $200$\n    max_seq_length = 200\n\n    epochs = 100\n\n    kl_div_loss = KLDivLoss()\n    reconstruction_loss = ReconstructionLoss()\n\n    def init(self):\n        # Initialize encoder & decoder\n        self.encoder = EncoderRNN(self.d_z, self.enc_hidden_size).to(self.device)\n        self.decoder = DecoderRNN(self.d_z, self.dec_hidden_size, self.n_distributions).to(self.device)\n\n        # Set optimizer. Things like type of optimizer and learning rate are configurable\n        optimizer = OptimizerConfigs()\n        optimizer.parameters = list(self.encoder.parameters()) + list(self.decoder.parameters())\n        self.optimizer = optimizer\n\n        # Create sampler\n        self.sampler = Sampler(self.encoder, self.decoder)\n\n        # `npz` file path is `data/sketch/[DATASET NAME].npz`\n        path = lab.get_data_path() / 'sketch' / f'{self.dataset_name}.npz'\n        # Load the numpy file\n        dataset = np.load(str(path), encoding='latin1', allow_pickle=True)\n\n        # Create training dataset\n        self.train_dataset = StrokesDataset(dataset['train'], self.max_seq_length)\n        # Create validation dataset\n        self.valid_dataset = StrokesDataset(dataset['valid'], self.max_seq_length, self.train_dataset.scale)\n\n        # Create training data loader\n        self.train_loader = DataLoader(self.train_dataset, self.batch_size, shuffle=True)\n        # Create validation data loader\n        self.valid_loader = DataLoader(self.valid_dataset, self.batch_size)\n\n        # Add hooks to monitor layer outputs on Tensorboard\n        hook_model_outputs(self.mode, self.encoder, 'encoder')\n        hook_model_outputs(self.mode, self.decoder, 'decoder')\n\n        # Configure the tracker to print the total train/validation loss\n        tracker.set_scalar(\"loss.total.*\", True)\n\n        self.state_modules = []\n\n    def step(self, batch: Any, batch_idx: BatchIndex):\n        self.encoder.train(self.mode.is_train)\n        self.decoder.train(self.mode.is_train)\n\n        # Move `data` and `mask` to device and swap the sequence and batch dimensions.\n        # `data` will have shape `[seq_len, batch_size, 5]` and\n        # `mask` will have shape `[seq_len, batch_size]`.\n        data = batch[0].to(self.device).transpose(0, 1)\n        mask = batch[1].to(self.device).transpose(0, 1)\n\n        # Increment step in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Encode the sequence of strokes\n        with monit.section(\"encoder\"):\n            # Get $z$, $\\mu$, and $\\hat{\\sigma}$\n            z, mu, sigma_hat = self.encoder(data)\n\n        # Decode the mixture of distributions and $\\hat{q}$\n        with monit.section(\"decoder\"):\n            # Concatenate $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$\n            z_stack = z.unsqueeze(0).expand(data.shape[0] - 1, -1, -1)\n            inputs = torch.cat([data[:-1], z_stack], 2)\n            # Get mixture of distributions and $\\hat{q}$\n            dist, q_logits, _ = self.decoder(inputs, z, None)\n\n        # Compute the loss\n        with monit.section('loss'):\n            # $L_{KL}$\n            kl_loss = self.kl_div_loss(sigma_hat, mu)\n            # $L_R$\n            reconstruction_loss = self.reconstruction_loss(mask, data[1:], dist, q_logits)\n            # $Loss = L_R + w_{KL} L_{KL}$\n            loss = reconstruction_loss + self.kl_div_loss_weight * kl_loss\n\n            # Track losses\n            tracker.add(\"loss.kl.\", kl_loss)\n            tracker.add(\"loss.reconstruction.\", reconstruction_loss)\n            tracker.add(\"loss.total.\", loss)\n\n        # Only if we are in training state\n        if self.mode.is_train:\n            # Run optimizer\n            with monit.section('optimize'):\n                # Set `grad` to zero\n                self.optimizer.zero_grad()\n                # Compute gradients\n                loss.backward()\n                # Log model parameters and gradients\n                if batch_idx.is_last:\n                    tracker.add(encoder=self.encoder, decoder=self.decoder)\n                # Clip gradients\n                nn.utils.clip_grad_norm_(self.encoder.parameters(), self.grad_clip)\n                nn.utils.clip_grad_norm_(self.decoder.parameters(), self.grad_clip)\n                # Optimize\n                self.optimizer.step()\n\n        tracker.save()\n\n    def sample(self):\n        # Randomly pick a sample from validation dataset to encoder\n        data, *_ = self.valid_dataset[np.random.choice(len(self.valid_dataset))]\n        # Add batch dimension and move it to device\n        data = data.unsqueeze(1).to(self.device)\n        # Sample\n        self.sampler.sample(data, self.temperature)\n\n\ndef main():\n    configs = Configs()\n    experiment.create(name=\"sketch_rnn\")\n\n    # Pass a dictionary of configurations\n    experiment.configs(configs, {\n        'optimizer.optimizer': 'Adam',\n        # We use a learning rate of `1e-3` because we can see results faster.\n        # Paper had suggested `1e-4`.\n        'optimizer.learning_rate': 1e-3,\n        # Name of the dataset\n        'dataset_name': 'bicycle',\n        # Number of inner iterations within an epoch to switch between training, validation and sampling.\n        'inner_iterations': 10\n    })\n\n    with experiment.start():\n        # Run the experiment\n        configs.run()\n\n\nif __name__ == \"__main__\":\n    main()\n", "labml_nn/gan/__init__.py": "\"\"\"\n---\ntitle: Generative Adversarial Networks\nsummary: >\n A set of PyTorch implementations/tutorials of GANs.\n---\n\n# Generative Adversarial Networks\n\n* [Original GAN](original/index.html)\n* [GAN with deep convolutional network](dcgan/index.html)\n* [Cycle GAN](cycle_gan/index.html)\n* [Wasserstein GAN](wasserstein/index.html)\n* [Wasserstein GAN with Gradient Penalty](wasserstein/gradient_penalty/index.html)\n* [StyleGAN 2](stylegan/index.html)\n\"\"\"", "labml_nn/gan/original/experiment.py": "\"\"\"\n---\ntitle: Generative Adversarial Networks experiment with MNIST\nsummary: This experiment generates MNIST images using multi-layer perceptron.\n---\n\n# Generative Adversarial Networks experiment with MNIST\n\"\"\"\n\nfrom typing import Any\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torchvision import transforms\n\nfrom labml import tracker, monit, experiment\nfrom labml.configs import option, calculate\nfrom labml_helpers.datasets.mnist import MNISTConfigs\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_helpers.module import Module\nfrom labml_helpers.optimizer import OptimizerConfigs\nfrom labml_helpers.train_valid import TrainValidConfigs, hook_model_outputs, BatchIndex\nfrom labml_nn.gan.original import DiscriminatorLogitsLoss, GeneratorLogitsLoss\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n\nclass Generator(Module):\n    \"\"\"\n    ### Simple MLP Generator\n\n    This has three linear layers of increasing size with `LeakyReLU` activations.\n    The final layer has a $tanh$ activation.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        layer_sizes = [256, 512, 1024]\n        layers = []\n        d_prev = 100\n        for size in layer_sizes:\n            layers = layers + [nn.Linear(d_prev, size), nn.LeakyReLU(0.2)]\n            d_prev = size\n\n        self.layers = nn.Sequential(*layers, nn.Linear(d_prev, 28 * 28), nn.Tanh())\n\n        self.apply(weights_init)\n\n    def forward(self, x):\n        return self.layers(x).view(x.shape[0], 1, 28, 28)\n\n\nclass Discriminator(Module):\n    \"\"\"\n    ### Simple MLP Discriminator\n\n    This has three  linear layers of decreasing size with `LeakyReLU` activations.\n    The final layer has a single output that gives the logit of whether input\n    is real or fake. You can get the probability by calculating the sigmoid of it.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        layer_sizes = [1024, 512, 256]\n        layers = []\n        d_prev = 28 * 28\n        for size in layer_sizes:\n            layers = layers + [nn.Linear(d_prev, size), nn.LeakyReLU(0.2)]\n            d_prev = size\n\n        self.layers = nn.Sequential(*layers, nn.Linear(d_prev, 1))\n        self.apply(weights_init)\n\n    def forward(self, x):\n        return self.layers(x.view(x.shape[0], -1))\n\n\nclass Configs(MNISTConfigs, TrainValidConfigs):\n    \"\"\"\n    ## Configurations\n\n    This extends MNIST configurations to get the data loaders and Training and validation loop\n    configurations to simplify our implementation.\n    \"\"\"\n\n    device: torch.device = DeviceConfigs()\n    dataset_transforms = 'mnist_gan_transforms'\n    epochs: int = 10\n\n    is_save_models = True\n    discriminator: Module = 'mlp'\n    generator: Module = 'mlp'\n    generator_optimizer: torch.optim.Adam\n    discriminator_optimizer: torch.optim.Adam\n    generator_loss: GeneratorLogitsLoss = 'original'\n    discriminator_loss: DiscriminatorLogitsLoss = 'original'\n    label_smoothing: float = 0.2\n    discriminator_k: int = 1\n\n    def init(self):\n        \"\"\"\n        Initializations\n        \"\"\"\n        self.state_modules = []\n\n        hook_model_outputs(self.mode, self.generator, 'generator')\n        hook_model_outputs(self.mode, self.discriminator, 'discriminator')\n        tracker.set_scalar(\"loss.generator.*\", True)\n        tracker.set_scalar(\"loss.discriminator.*\", True)\n        tracker.set_image(\"generated\", True, 1 / 100)\n\n    def sample_z(self, batch_size: int):\n        \"\"\"\n        $$z \\sim p(z)$$\n        \"\"\"\n        return torch.randn(batch_size, 100, device=self.device)\n\n    def step(self, batch: Any, batch_idx: BatchIndex):\n        \"\"\"\n        Take a training step\n        \"\"\"\n\n        # Set model states\n        self.generator.train(self.mode.is_train)\n        self.discriminator.train(self.mode.is_train)\n\n        # Get MNIST images\n        data = batch[0].to(self.device)\n\n        # Increment step in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Train the discriminator\n        with monit.section(\"discriminator\"):\n            # Get discriminator loss\n            loss = self.calc_discriminator_loss(data)\n\n            # Train\n            if self.mode.is_train:\n                self.discriminator_optimizer.zero_grad()\n                loss.backward()\n                if batch_idx.is_last:\n                    tracker.add('discriminator', self.discriminator)\n                self.discriminator_optimizer.step()\n\n        # Train the generator once in every `discriminator_k`\n        if batch_idx.is_interval(self.discriminator_k):\n            with monit.section(\"generator\"):\n                loss = self.calc_generator_loss(data.shape[0])\n\n                # Train\n                if self.mode.is_train:\n                    self.generator_optimizer.zero_grad()\n                    loss.backward()\n                    if batch_idx.is_last:\n                        tracker.add('generator', self.generator)\n                    self.generator_optimizer.step()\n\n        tracker.save()\n\n    def calc_discriminator_loss(self, data):\n        \"\"\"\n        Calculate discriminator loss\n        \"\"\"\n        latent = self.sample_z(data.shape[0])\n        logits_true = self.discriminator(data)\n        logits_false = self.discriminator(self.generator(latent).detach())\n        loss_true, loss_false = self.discriminator_loss(logits_true, logits_false)\n        loss = loss_true + loss_false\n\n        # Log stuff\n        tracker.add(\"loss.discriminator.true.\", loss_true)\n        tracker.add(\"loss.discriminator.false.\", loss_false)\n        tracker.add(\"loss.discriminator.\", loss)\n\n        return loss\n\n    def calc_generator_loss(self, batch_size: int):\n        \"\"\"\n        Calculate generator loss\n        \"\"\"\n        latent =  self.sample_z(batch_size)\n        generated_images = self.generator(latent)\n        logits = self.discriminator(generated_images)\n        loss = self.generator_loss(logits)\n\n        # Log stuff\n        tracker.add('generated', generated_images[0:6])\n        tracker.add(\"loss.generator.\", loss)\n\n        return loss\n\n\n\n\n@option(Configs.dataset_transforms)\ndef mnist_gan_transforms():\n    return transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,))\n    ])\n\n\n@option(Configs.discriminator_optimizer)\ndef _discriminator_optimizer(c: Configs):\n    opt_conf = OptimizerConfigs()\n    opt_conf.optimizer = 'Adam'\n    opt_conf.parameters = c.discriminator.parameters()\n    opt_conf.learning_rate = 2.5e-4\n    # Setting exponent decay rate for first moment of gradient,\n    # $\\beta_1$ to `0.5` is important.\n    # Default of `0.9` fails.\n    opt_conf.betas = (0.5, 0.999)\n    return opt_conf\n\n\n@option(Configs.generator_optimizer)\ndef _generator_optimizer(c: Configs):\n    opt_conf = OptimizerConfigs()\n    opt_conf.optimizer = 'Adam'\n    opt_conf.parameters = c.generator.parameters()\n    opt_conf.learning_rate = 2.5e-4\n    # Setting exponent decay rate for first moment of gradient,\n    # $\\beta_1$ to `0.5` is important.\n    # Default of `0.9` fails.\n    opt_conf.betas = (0.5, 0.999)\n    return opt_conf\n\n\ncalculate(Configs.generator, 'mlp', lambda c: Generator().to(c.device))\ncalculate(Configs.discriminator, 'mlp', lambda c: Discriminator().to(c.device))\ncalculate(Configs.generator_loss, 'original', lambda c: GeneratorLogitsLoss(c.label_smoothing).to(c.device))\ncalculate(Configs.discriminator_loss, 'original', lambda c: DiscriminatorLogitsLoss(c.label_smoothing).to(c.device))\n\n\ndef main():\n    conf = Configs()\n    experiment.create(name='mnist_gan', comment='test')\n    experiment.configs(conf,\n                       {'label_smoothing': 0.01})\n    with experiment.start():\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/gan/original/__init__.py": "\"\"\"\n---\ntitle: Generative Adversarial Networks (GAN)\nsummary: A simple PyTorch implementation/tutorial of Generative Adversarial Networks (GAN) loss functions.\n---\n\n# Generative Adversarial Networks (GAN)\n\nThis is an implementation of\n[Generative Adversarial Networks](https://arxiv.org/abs/1406.2661).\n\nThe generator, $G(\\pmb{z}; \\theta_g)$ generates samples that match the\ndistribution of data, while the discriminator, $D(\\pmb{x}; \\theta_g)$\ngives the probability that $\\pmb{x}$ came from data rather than $G$.\n\nWe train $D$ and $G$ simultaneously on a two-player min-max game with value\nfunction $V(G, D)$.\n\n$$\\min_G \\max_D V(D, G) =\n    \\mathop{\\mathbb{E}}_{\\pmb{x} \\sim p_{data}(\\pmb{x})}\n        \\big[\\log D(\\pmb{x})\\big] +\n    \\mathop{\\mathbb{E}}_{\\pmb{z} \\sim p_{\\pmb{z}}(\\pmb{z})}\n        \\big[\\log (1 - D(G(\\pmb{z}))\\big]\n$$\n\n$p_{data}(\\pmb{x})$ is the probability distribution over data,\nwhilst $p_{\\pmb{z}}(\\pmb{z})$ probability distribution of $\\pmb{z}$, which is set to\ngaussian noise.\n\nThis file defines the loss functions. [Here](experiment.html) is an MNIST example\nwith two multilayer perceptron for the generator and discriminator.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.utils.data\n\nfrom labml_helpers.module import Module\n\n\nclass DiscriminatorLogitsLoss(Module):\n    \"\"\"\n    ## Discriminator Loss\n\n    Discriminator should **ascend** on the gradient,\n\n    $$\\nabla_{\\theta_d} \\frac{1}{m} \\sum_{i=1}^m \\Bigg[\n        \\log D\\Big(\\pmb{x}^{(i)}\\Big) +\n        \\log \\Big(1 - D\\Big(G\\Big(\\pmb{z}^{(i)}\\Big)\\Big)\\Big)\n    \\Bigg]$$\n\n    $m$ is the mini-batch size and $(i)$ is used to index samples in the mini-batch.\n    $\\pmb{x}$ are samples from $p_{data}$ and $\\pmb{z}$ are samples from $p_z$.\n    \"\"\"\n\n    def __init__(self, smoothing: float = 0.2):\n        super().__init__()\n        # We use PyTorch Binary Cross Entropy Loss, which is\n        # $-\\sum\\Big[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})\\Big]$,\n        # where $y$ are the labels and $\\hat{y}$ are the predictions.\n        # *Note the negative sign*.\n        # We use labels equal to $1$ for $\\pmb{x}$ from $p_{data}$\n        # and labels equal to $0$ for $\\pmb{x}$ from $p_{G}.$\n        # Then descending on the sum of these is the same as ascending on\n        # the above gradient.\n        #\n        # `BCEWithLogitsLoss` combines softmax and binary cross entropy loss.\n        self.loss_true = nn.BCEWithLogitsLoss()\n        self.loss_false = nn.BCEWithLogitsLoss()\n\n        # We use label smoothing because it seems to work better in some cases\n        self.smoothing = smoothing\n\n        # Labels are registered as buffered and persistence is set to `False`.\n        self.register_buffer('labels_true', _create_labels(256, 1.0 - smoothing, 1.0), False)\n        self.register_buffer('labels_false', _create_labels(256, 0.0, smoothing), False)\n\n    def forward(self, logits_true: torch.Tensor, logits_false: torch.Tensor):\n        \"\"\"\n        `logits_true` are logits from $D(\\pmb{x}^{(i)})$ and\n        `logits_false` are logits from $D(G(\\pmb{z}^{(i)}))$\n        \"\"\"\n        if len(logits_true) > len(self.labels_true):\n            self.register_buffer(\"labels_true\",\n                                 _create_labels(len(logits_true), 1.0 - self.smoothing, 1.0, logits_true.device), False)\n        if len(logits_false) > len(self.labels_false):\n            self.register_buffer(\"labels_false\",\n                                 _create_labels(len(logits_false), 0.0, self.smoothing, logits_false.device), False)\n\n        return (self.loss_true(logits_true, self.labels_true[:len(logits_true)]),\n                self.loss_false(logits_false, self.labels_false[:len(logits_false)]))\n\n\nclass GeneratorLogitsLoss(Module):\n    \"\"\"\n    ## Generator Loss\n\n    Generator should **descend** on the gradient,\n\n    $$\\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^m \\Bigg[\n        \\log \\Big(1 - D\\Big(G\\Big(\\pmb{z}^{(i)}\\Big)\\Big)\\Big)\n    \\Bigg]$$\n    \"\"\"\n    def __init__(self, smoothing: float = 0.2):\n        super().__init__()\n        self.loss_true = nn.BCEWithLogitsLoss()\n        self.smoothing = smoothing\n        # We use labels equal to $1$ for $\\pmb{x}$ from $p_{G}.$\n        # Then descending on this loss is the same as descending on\n        # the above gradient.\n        self.register_buffer('fake_labels', _create_labels(256, 1.0 - smoothing, 1.0), False)\n\n    def forward(self, logits: torch.Tensor):\n        if len(logits) > len(self.fake_labels):\n            self.register_buffer(\"fake_labels\",\n                                 _create_labels(len(logits), 1.0 - self.smoothing, 1.0, logits.device), False)\n\n        return self.loss_true(logits, self.fake_labels[:len(logits)])\n\n\ndef _create_labels(n: int, r1: float, r2: float, device: torch.device = None):\n    \"\"\"\n    Create smoothed labels\n    \"\"\"\n    return torch.empty(n, 1, requires_grad=False, device=device).uniform_(r1, r2)\n", "labml_nn/gan/wasserstein/experiment.py": "\"\"\"\n---\ntitle: WGAN experiment with MNIST\nsummary: This experiment generates MNIST images using convolutional neural network.\n---\n\n# WGAN experiment with MNIST\n\"\"\"\nfrom labml import experiment\n\nfrom labml.configs import calculate\n# Import configurations from [DCGAN experiment](../dcgan/index.html)\nfrom labml_nn.gan.dcgan import Configs\n\n# Import [Wasserstein GAN losses](./index.html)\nfrom labml_nn.gan.wasserstein import GeneratorLoss, DiscriminatorLoss\n\n# Set configurations options for Wasserstein GAN losses\ncalculate(Configs.generator_loss, 'wasserstein', lambda c: GeneratorLoss())\ncalculate(Configs.discriminator_loss, 'wasserstein', lambda c: DiscriminatorLoss())\n\n\ndef main():\n    # Create configs object\n    conf = Configs()\n    # Create experiment\n    experiment.create(name='mnist_wassertein_dcgan', comment='test')\n    # Override configurations\n    experiment.configs(conf,\n                       {\n                           'discriminator': 'cnn',\n                           'generator': 'cnn',\n                           'label_smoothing': 0.01,\n                           'generator_loss': 'wasserstein',\n                           'discriminator_loss': 'wasserstein',\n                       })\n\n    # Start the experiment and run training loop\n    with experiment.start():\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/gan/wasserstein/__init__.py": "r\"\"\"\n---\ntitle: Wasserstein GAN (WGAN)\nsummary: A simple PyTorch implementation/tutorial of Wasserstein Generative Adversarial Networks (WGAN) loss functions.\n---\n\n# Wasserstein GAN (WGAN)\n\nThis is an implementation of\n[Wasserstein GAN](https://arxiv.org/abs/1701.07875).\n\nThe original GAN loss is based on Jensen-Shannon (JS) divergence\nbetween the real distribution $\\mathbb{P}_r$ and generated distribution $\\mathbb{P}_g$.\nThe Wasserstein GAN is based on Earth Mover distance between these distributions.\n\n$$\nW(\\mathbb{P}_r, \\mathbb{P}_g) =\n \\underset{\\gamma \\in \\Pi(\\mathbb{P}_r, \\mathbb{P}_g)} {\\mathrm{inf}}\n \\mathbb{E}_{(x,y) \\sim \\gamma}\n \\Vert x - y \\Vert\n$$\n\n$\\Pi(\\mathbb{P}_r, \\mathbb{P}_g)$ is the set of all joint distributions, whose\nmarginal probabilities are $\\gamma(x, y)$.\n\n$\\mathbb{E}_{(x,y) \\sim \\gamma} \\Vert x - y \\Vert$ is the earth mover distance for\na given joint distribution ($x$ and $y$ are probabilities).\n\nSo $W(\\mathbb{P}_r, \\mathbb{P}g)$ is equal to the least earth mover distance for\nany joint distribution between the real distribution $\\mathbb{P}_r$ and generated distribution $\\mathbb{P}_g$.\n\nThe paper shows that Jensen-Shannon (JS) divergence and other measures for the difference between two probability\ndistributions are not smooth. And therefore if we are doing gradient descent on one of the probability\ndistributions (parameterized) it will not converge.\n\nBased on Kantorovich-Rubinstein duality,\n$$\nW(\\mathbb{P}_r, \\mathbb{P}_g) =\n \\underset{\\Vert f \\Vert_L \\le 1} {\\mathrm{sup}}\n \\mathbb{E}_{x \\sim \\mathbb{P}_r} [f(x)]- \\mathbb{E}_{x \\sim \\mathbb{P}_g} [f(x)]\n$$\n\nwhere $\\Vert f \\Vert_L \\le 1$ are all 1-Lipschitz functions.\n\nThat is, it is equal to the greatest difference\n$$\\mathbb{E}_{x \\sim \\mathbb{P}_r} [f(x)] - \\mathbb{E}_{x \\sim \\mathbb{P}_g} [f(x)]$$\namong all 1-Lipschitz functions.\n\nFor $K$-Lipschitz functions,\n$$\nW(\\mathbb{P}_r, \\mathbb{P}_g) =\n \\underset{\\Vert f \\Vert_L \\le K} {\\mathrm{sup}}\n \\mathbb{E}_{x \\sim \\mathbb{P}_r} \\Bigg[\\frac{1}{K} f(x) \\Bigg]\n  - \\mathbb{E}_{x \\sim \\mathbb{P}_g} \\Bigg[\\frac{1}{K} f(x) \\Bigg]\n$$\n\nIf all $K$-Lipschitz functions can be represented as $f_w$ where $f$ is parameterized by\n$w \\in \\mathcal{W}$,\n\n$$\nK \\cdot W(\\mathbb{P}_r, \\mathbb{P}_g) =\n \\max_{w \\in \\mathcal{W}}\n \\mathbb{E}_{x \\sim \\mathbb{P}_r} [f_w(x)]- \\mathbb{E}_{x \\sim \\mathbb{P}_g} [f_w(x)]\n$$\n\nIf $(\\mathbb{P}_{g})$ is represented by a generator $$g_\\theta (z)$$ and $z$ is from a known\ndistribution $z \\sim p(z)$,\n\n$$\nK \\cdot W(\\mathbb{P}_r, \\mathbb{P}_\\theta) =\n \\max_{w \\in \\mathcal{W}}\n \\mathbb{E}_{x \\sim \\mathbb{P}_r} [f_w(x)]- \\mathbb{E}_{z \\sim p(z)} [f_w(g_\\theta(z))]\n$$\n\nNow to converge $g_\\theta$ with $\\mathbb{P}_{r}$ we can gradient descent on $\\theta$\nto minimize above formula.\n\nSimilarly we can find $\\max_{w \\in \\mathcal{W}}$ by ascending on $w$,\nwhile keeping $K$ bounded. *One way to keep $K$ bounded is to clip all weights in the neural\nnetwork that defines $f$ clipped within a range.*\n\nHere is the code to try this on a [simple MNIST generation experiment](experiment.html).\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/gan/wasserstein/experiment.ipynb)\n\"\"\"\n\nimport torch.utils.data\nfrom torch.nn import functional as F\n\nfrom labml_helpers.module import Module\n\n\nclass DiscriminatorLoss(Module):\n    \"\"\"\n    ## Discriminator Loss\n\n    We want to find $w$ to maximize\n    $$\\mathbb{E}_{x \\sim \\mathbb{P}_r} [f_w(x)]- \\mathbb{E}_{z \\sim p(z)} [f_w(g_\\theta(z))]$$,\n    so we minimize,\n    $$-\\frac{1}{m} \\sum_{i=1}^m f_w \\big(x^{(i)} \\big) +\n     \\frac{1}{m} \\sum_{i=1}^m f_w \\big( g_\\theta(z^{(i)}) \\big)$$\n    \"\"\"\n\n    def forward(self, f_real: torch.Tensor, f_fake: torch.Tensor):\n        \"\"\"\n        * `f_real` is $f_w(x)$\n        * `f_fake` is $f_w(g_\\theta(z))$\n\n        This returns the a tuple with losses for $f_w(x)$ and $f_w(g_\\theta(z))$,\n        which are later added.\n        They are kept separate for logging.\n        \"\"\"\n\n        # We use ReLUs to clip the loss to keep $f \\in [-1, +1]$ range.\n        return F.relu(1 - f_real).mean(), F.relu(1 + f_fake).mean()\n\n\nclass GeneratorLoss(Module):\n    \"\"\"\n    ## Generator Loss\n\n    We want to find $\\theta$ to minimize\n    $$\\mathbb{E}_{x \\sim \\mathbb{P}_r} [f_w(x)]- \\mathbb{E}_{z \\sim p(z)} [f_w(g_\\theta(z))]$$\n    The first component is independent of $\\theta$,\n    so we minimize,\n    $$-\\frac{1}{m} \\sum_{i=1}^m f_w \\big( g_\\theta(z^{(i)}) \\big)$$\n\n    \"\"\"\n\n    def forward(self, f_fake: torch.Tensor):\n        \"\"\"\n        * `f_fake` is $f_w(g_\\theta(z))$\n        \"\"\"\n        return -f_fake.mean()\n", "labml_nn/gan/wasserstein/gradient_penalty/experiment.py": "\"\"\"\n---\ntitle: WGAN-GP experiment with MNIST\nsummary: This experiment generates MNIST images using convolutional neural network.\n---\n\n# WGAN-GP experiment with MNIST\n\"\"\"\n\nimport torch\n\nfrom labml import experiment, tracker\n# Import configurations from [Wasserstein experiment](../experiment.html)\nfrom labml_nn.gan.wasserstein.experiment import Configs as OriginalConfigs\n#\nfrom labml_nn.gan.wasserstein.gradient_penalty import GradientPenalty\n\n\nclass Configs(OriginalConfigs):\n    \"\"\"\n    ## Configuration class\n\n    We extend [original GAN implementation](../../original/experiment.html) and override the discriminator (critic) loss\n    calculation to include gradient penalty.\n    \"\"\"\n\n    # Gradient penalty coefficient $\\lambda$\n    gradient_penalty_coefficient: float = 10.0\n    #\n    gradient_penalty = GradientPenalty()\n\n    def calc_discriminator_loss(self, data: torch.Tensor):\n        \"\"\"\n        This overrides the original discriminator loss calculation and\n        includes gradient penalty.\n        \"\"\"\n        # Require gradients on $x$ to calculate gradient penalty\n        data.requires_grad_()\n        # Sample $z \\sim p(z)$\n        latent = self.sample_z(data.shape[0])\n        # $D(x)$\n        f_real = self.discriminator(data)\n        # $D(G_\\theta(z))$\n        f_fake = self.discriminator(self.generator(latent).detach())\n        # Get discriminator losses\n        loss_true, loss_false = self.discriminator_loss(f_real, f_fake)\n        # Calculate gradient penalties in training mode\n        if self.mode.is_train:\n            gradient_penalty = self.gradient_penalty(data, f_real)\n            tracker.add(\"loss.gp.\", gradient_penalty)\n            loss = loss_true + loss_false + self.gradient_penalty_coefficient * gradient_penalty\n        # Skip gradient penalty otherwise\n        else:\n            loss = loss_true + loss_false\n\n        # Log stuff\n        tracker.add(\"loss.discriminator.true.\", loss_true)\n        tracker.add(\"loss.discriminator.false.\", loss_false)\n        tracker.add(\"loss.discriminator.\", loss)\n\n        return loss\n\n\ndef main():\n    # Create configs object\n    conf = Configs()\n    # Create experiment\n    experiment.create(name='mnist_wassertein_gp_dcgan')\n    # Override configurations\n    experiment.configs(conf,\n                       {\n                           'discriminator': 'cnn',\n                           'generator': 'cnn',\n                           'label_smoothing': 0.01,\n                           'generator_loss': 'wasserstein',\n                           'discriminator_loss': 'wasserstein',\n                           'discriminator_k': 5,\n                       })\n\n    # Start the experiment and run training loop\n    with experiment.start():\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/gan/wasserstein/gradient_penalty/__init__.py": "r\"\"\"\n---\ntitle: Gradient Penalty for Wasserstein GAN (WGAN-GP)\nsummary: >\n An annotated PyTorch implementation/tutorial of\n  Improved Training of Wasserstein GANs.\n---\n\n# Gradient Penalty for Wasserstein GAN (WGAN-GP)\n\nThis is an implementation of\n[Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028).\n\n[WGAN](../index.html) suggests clipping weights to enforce Lipschitz constraint\non the discriminator network (critic).\nThis and other weight constraints like L2 norm clipping, weight normalization,\nL1, L2 weight decay have problems:\n\n1. Limiting the capacity of the discriminator\n2. Exploding and vanishing gradients (without [Batch Normalization](../../../normalization/batch_norm/index.html)).\n\nThe paper [Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028)\nproposal a better way to improve Lipschitz constraint, a gradient penalty.\n\n$$\\mathcal{L}_{GP} = \\lambda \\underset{\\hat{x} \\sim \\mathbb{P}_{\\hat{x}}}{\\mathbb{E}}\n\\Big[ \\big(\\Vert \\nabla_{\\hat{x}} D(\\hat{x}) \\Vert_2 - 1\\big)^2 \\Big]\n$$\n\nwhere $\\lambda$ is the penalty weight and\n\n\\begin{align}\nx &\\sim \\mathbb{P}_r \\\\\nz &\\sim p(z) \\\\\n\\epsilon &\\sim U[0,1] \\\\\n\\tilde{x} &\\leftarrow G_\\theta (z) \\\\\n\\hat{x} &\\leftarrow \\epsilon x + (1 - \\epsilon) \\tilde{x}\n\\end{align}\n\nThat is we try to keep the gradient norm $\\Vert \\nabla_{\\hat{x}} D(\\hat{x}) \\Vert_2$ close to $1$.\n\nIn this implementation we set $\\epsilon = 1$.\n\nHere is the [code for an experiment](experiment.html) that uses gradient penalty.\n\"\"\"\n\nimport torch\nimport torch.autograd\n\nfrom labml_helpers.module import Module\n\n\nclass GradientPenalty(Module):\n    \"\"\"\n    ## Gradient Penalty\n    \"\"\"\n\n    def forward(self, x: torch.Tensor, f: torch.Tensor):\n        \"\"\"\n        * `x` is $x \\sim \\mathbb{P}_r$\n        * `f` is $D(x)$\n\n        $\\hat{x} \\leftarrow x$\n        since we set $\\epsilon = 1$ for this implementation.\n        \"\"\"\n\n        # Get batch size\n        batch_size = x.shape[0]\n\n        # Calculate gradients of $D(x)$ with respect to $x$.\n        # `grad_outputs` is set to ones since we want the gradients of $D(x)$,\n        # and we need to create and retain graph since we have to compute gradients\n        # with respect to weight on this loss.\n        gradients, *_ = torch.autograd.grad(outputs=f,\n                                            inputs=x,\n                                            grad_outputs=f.new_ones(f.shape),\n                                            create_graph=True)\n\n        # Reshape gradients to calculate the norm\n        gradients = gradients.reshape(batch_size, -1)\n        # Calculate the norm $\\Vert \\nabla_{\\hat{x}} D(\\hat{x}) \\Vert_2$\n        norm = gradients.norm(2, dim=-1)\n        # Return the loss $\\big(\\Vert \\nabla_{\\hat{x}} D(\\hat{x}) \\Vert_2 - 1\\big)^2$\n        return torch.mean((norm - 1) ** 2)\n", "labml_nn/gan/cycle_gan/__init__.py": "\"\"\"\n---\ntitle: Cycle GAN\nsummary: >\n  A simple PyTorch implementation/tutorial of Cycle GAN introduced in paper\n  Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.\n---\n\n# Cycle GAN\n\nThis is a [PyTorch](https://pytorch.org) implementation/tutorial of the paper\n[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593).\n\nI've taken pieces of code from [eriklindernoren/PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN).\nIt is a very good resource if you want to checkout other GAN variations too.\n\nCycle GAN does image-to-image translation.\nIt trains a model to translate an image from given distribution to another, say, images of class A and B.\nImages of a certain distribution could be things like images of a certain style, or nature.\nThe models do not need paired images between A and B.\nJust a set of images of each class is enough.\nThis works very well on changing between image styles, lighting changes, pattern changes, etc.\nFor example, changing summer to winter, painting style to photos, and horses to zebras.\n\nCycle GAN trains two generator models and two discriminator models.\nOne generator translates images from A to B and the other from B to A.\nThe discriminators test whether the generated images look real.\n\nThis file contains the model code as well as the training code.\nWe also have a Google Colab notebook.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/gan/cycle_gan/experiment.ipynb)\n\"\"\"\n\nimport itertools\nimport random\nimport zipfile\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import InterpolationMode\nfrom torchvision.utils import make_grid\n\nfrom labml import lab, tracker, experiment, monit\nfrom labml.configs import BaseConfigs\nfrom labml.utils.download import download_file\nfrom labml.utils.pytorch import get_modules\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_helpers.module import Module\n\n\nclass GeneratorResNet(Module):\n    \"\"\"\n    The generator is a residual network.\n    \"\"\"\n\n    def __init__(self, input_channels: int, n_residual_blocks: int):\n        super().__init__()\n        # This first block runs a $7\\times7$ convolution and maps the image to\n        # a feature map.\n        # The output feature map has the same height and width because we have\n        # a padding of $3$.\n        # Reflection padding is used because it gives better image quality at edges.\n        #\n        # `inplace=True` in `ReLU` saves a little bit of memory.\n        out_features = 64\n        layers = [\n            nn.Conv2d(input_channels, out_features, kernel_size=7, padding=3, padding_mode='reflect'),\n            nn.InstanceNorm2d(out_features),\n            nn.ReLU(inplace=True),\n        ]\n        in_features = out_features\n\n        # We down-sample with two $3 \\times 3$ convolutions\n        # with stride of 2\n        for _ in range(2):\n            out_features *= 2\n            layers += [\n                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # We take this through `n_residual_blocks`.\n        # This module is defined below.\n        for _ in range(n_residual_blocks):\n            layers += [ResidualBlock(out_features)]\n\n        # Then the resulting feature map is up-sampled\n        # to match the original image height and width.\n        for _ in range(2):\n            out_features //= 2\n            layers += [\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(in_features, out_features, kernel_size=3, stride=1, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Finally we map the feature map to an RGB image\n        layers += [nn.Conv2d(out_features, input_channels, 7, padding=3, padding_mode='reflect'), nn.Tanh()]\n\n        # Create a sequential module with the layers\n        self.layers = nn.Sequential(*layers)\n\n        # Initialize weights to $\\mathcal{N}(0, 0.2)$\n        self.apply(weights_init_normal)\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass ResidualBlock(Module):\n    \"\"\"\n    This is the residual block, with two convolution layers.\n    \"\"\"\n\n    def __init__(self, in_features: int):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, padding_mode='reflect'),\n            nn.InstanceNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, padding_mode='reflect'),\n            nn.InstanceNorm2d(in_features),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x: torch.Tensor):\n        return x + self.block(x)\n\n\nclass Discriminator(Module):\n    \"\"\"\n    This is the discriminator.\n    \"\"\"\n\n    def __init__(self, input_shape: Tuple[int, int, int]):\n        super().__init__()\n        channels, height, width = input_shape\n\n        # Output of the discriminator is also a map of probabilities,\n        # whether each region of the image is real or generated\n        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n\n        self.layers = nn.Sequential(\n            # Each of these blocks will shrink the height and width by a factor of 2\n            DiscriminatorBlock(channels, 64, normalize=False),\n            DiscriminatorBlock(64, 128),\n            DiscriminatorBlock(128, 256),\n            DiscriminatorBlock(256, 512),\n            # Zero pad on top and left to keep the output height and width same\n            # with the $4 \\times 4$ kernel\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n        )\n\n        # Initialize weights to $\\mathcal{N}(0, 0.2)$\n        self.apply(weights_init_normal)\n\n    def forward(self, img):\n        return self.layers(img)\n\n\nclass DiscriminatorBlock(Module):\n    \"\"\"\n    This is the discriminator block module.\n    It does a convolution, an optional normalization, and a leaky ReLU.\n\n    It shrinks the height and width of the input feature map by half.\n    \"\"\"\n\n    def __init__(self, in_filters: int, out_filters: int, normalize: bool = True):\n        super().__init__()\n        layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_filters))\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor):\n        return self.layers(x)\n\n\ndef weights_init_normal(m):\n    \"\"\"\n    Initialize convolution layer weights to $\\mathcal{N}(0, 0.2)$\n    \"\"\"\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n\n\ndef load_image(path: str):\n    \"\"\"\n    Load an image and change to RGB if in grey-scale.\n    \"\"\"\n    image = Image.open(path)\n    if image.mode != 'RGB':\n        image = Image.new(\"RGB\", image.size).paste(image)\n\n    return image\n\n\nclass ImageDataset(Dataset):\n    \"\"\"\n    ### Dataset to load images\n    \"\"\"\n\n    @staticmethod\n    def download(dataset_name: str):\n        \"\"\"\n        #### Download dataset and extract data\n        \"\"\"\n        # URL\n        url = f'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/{dataset_name}.zip'\n        # Download folder\n        root = lab.get_data_path() / 'cycle_gan'\n        if not root.exists():\n            root.mkdir(parents=True)\n        # Download destination\n        archive = root / f'{dataset_name}.zip'\n        # Download file (generally ~100MB)\n        download_file(url, archive)\n        # Extract the archive\n        with zipfile.ZipFile(archive, 'r') as f:\n            f.extractall(root)\n\n    def __init__(self, dataset_name: str, transforms_, mode: str):\n        \"\"\"\n        #### Initialize the dataset\n\n        * `dataset_name` is the name of the dataset\n        * `transforms_` is the set of image transforms\n        * `mode` is either `train` or `test`\n        \"\"\"\n        # Dataset path\n        root = lab.get_data_path() / 'cycle_gan' / dataset_name\n        # Download if missing\n        if not root.exists():\n            self.download(dataset_name)\n\n        # Image transforms\n        self.transform = transforms.Compose(transforms_)\n\n        # Get image paths\n        path_a = root / f'{mode}A'\n        path_b = root / f'{mode}B'\n        self.files_a = sorted(str(f) for f in path_a.iterdir())\n        self.files_b = sorted(str(f) for f in path_b.iterdir())\n\n    def __getitem__(self, index):\n        # Return a pair of images.\n        # These pairs get batched together, and they do not act like pairs in training.\n        # So it is kind of ok that we always keep giving the same pair.\n        return {\"x\": self.transform(load_image(self.files_a[index % len(self.files_a)])),\n                \"y\": self.transform(load_image(self.files_b[index % len(self.files_b)]))}\n\n    def __len__(self):\n        # Number of images in the dataset\n        return max(len(self.files_a), len(self.files_b))\n\n\nclass ReplayBuffer:\n    \"\"\"\n    ### Replay Buffer\n\n    Replay buffer is used to train the discriminator.\n    Generated images are added to the replay buffer and sampled from it.\n\n    The replay buffer returns the newly added image with a probability of $0.5$.\n    Otherwise, it sends an older generated image and replaces the older image\n    with the newly generated image.\n\n    This is done to reduce model oscillation.\n    \"\"\"\n\n    def __init__(self, max_size: int = 50):\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data: torch.Tensor):\n        \"\"\"Add/retrieve an image\"\"\"\n        data = data.detach()\n        res = []\n        for element in data:\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                res.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    res.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    res.append(element)\n        return torch.stack(res)\n\n\nclass Configs(BaseConfigs):\n    \"\"\"## Configurations\"\"\"\n\n    # `DeviceConfigs` will pick a GPU if available\n    device: torch.device = DeviceConfigs()\n\n    # Hyper-parameters\n    epochs: int = 200\n    dataset_name: str = 'monet2photo'\n    batch_size: int = 1\n\n    data_loader_workers = 8\n\n    learning_rate = 0.0002\n    adam_betas = (0.5, 0.999)\n    decay_start = 100\n\n    # The paper suggests using a least-squares loss instead of\n    # negative log-likelihood, at it is found to be more stable.\n    gan_loss = torch.nn.MSELoss()\n\n    # L1 loss is used for cycle loss and identity loss\n    cycle_loss = torch.nn.L1Loss()\n    identity_loss = torch.nn.L1Loss()\n\n    # Image dimensions\n    img_height = 256\n    img_width = 256\n    img_channels = 3\n\n    # Number of residual blocks in the generator\n    n_residual_blocks = 9\n\n    # Loss coefficients\n    cyclic_loss_coefficient = 10.0\n    identity_loss_coefficient = 5.\n\n    sample_interval = 500\n\n    # Models\n    generator_xy: GeneratorResNet\n    generator_yx: GeneratorResNet\n    discriminator_x: Discriminator\n    discriminator_y: Discriminator\n\n    # Optimizers\n    generator_optimizer: torch.optim.Adam\n    discriminator_optimizer: torch.optim.Adam\n\n    # Learning rate schedules\n    generator_lr_scheduler: torch.optim.lr_scheduler.LambdaLR\n    discriminator_lr_scheduler: torch.optim.lr_scheduler.LambdaLR\n\n    # Data loaders\n    dataloader: DataLoader\n    valid_dataloader: DataLoader\n\n    def sample_images(self, n: int):\n        \"\"\"Generate samples from test set and save them\"\"\"\n        batch = next(iter(self.valid_dataloader))\n        self.generator_xy.eval()\n        self.generator_yx.eval()\n        with torch.no_grad():\n            data_x, data_y = batch['x'].to(self.generator_xy.device), batch['y'].to(self.generator_yx.device)\n            gen_y = self.generator_xy(data_x)\n            gen_x = self.generator_yx(data_y)\n\n            # Arrange images along x-axis\n            data_x = make_grid(data_x, nrow=5, normalize=True)\n            data_y = make_grid(data_y, nrow=5, normalize=True)\n            gen_x = make_grid(gen_x, nrow=5, normalize=True)\n            gen_y = make_grid(gen_y, nrow=5, normalize=True)\n\n            # Arrange images along y-axis\n            image_grid = torch.cat((data_x, gen_y, data_y, gen_x), 1)\n\n        # Show samples\n        plot_image(image_grid)\n\n    def initialize(self):\n        \"\"\"\n        ## Initialize models and data loaders\n        \"\"\"\n        input_shape = (self.img_channels, self.img_height, self.img_width)\n\n        # Create the models\n        self.generator_xy = GeneratorResNet(self.img_channels, self.n_residual_blocks).to(self.device)\n        self.generator_yx = GeneratorResNet(self.img_channels, self.n_residual_blocks).to(self.device)\n        self.discriminator_x = Discriminator(input_shape).to(self.device)\n        self.discriminator_y = Discriminator(input_shape).to(self.device)\n\n        # Create the optmizers\n        self.generator_optimizer = torch.optim.Adam(\n            itertools.chain(self.generator_xy.parameters(), self.generator_yx.parameters()),\n            lr=self.learning_rate, betas=self.adam_betas)\n        self.discriminator_optimizer = torch.optim.Adam(\n            itertools.chain(self.discriminator_x.parameters(), self.discriminator_y.parameters()),\n            lr=self.learning_rate, betas=self.adam_betas)\n\n        # Create the learning rate schedules.\n        # The learning rate stars flat until `decay_start` epochs,\n        # and then linearly reduce to $0$ at end of training.\n        decay_epochs = self.epochs - self.decay_start\n        self.generator_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n            self.generator_optimizer, lr_lambda=lambda e: 1.0 - max(0, e - self.decay_start) / decay_epochs)\n        self.discriminator_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n            self.discriminator_optimizer, lr_lambda=lambda e: 1.0 - max(0, e - self.decay_start) / decay_epochs)\n\n        # Image transformations\n        transforms_ = [\n            transforms.Resize(int(self.img_height * 1.12), InterpolationMode.BICUBIC),\n            transforms.RandomCrop((self.img_height, self.img_width)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n\n        # Training data loader\n        self.dataloader = DataLoader(\n            ImageDataset(self.dataset_name, transforms_, 'train'),\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.data_loader_workers,\n        )\n\n        # Validation data loader\n        self.valid_dataloader = DataLoader(\n            ImageDataset(self.dataset_name, transforms_, \"test\"),\n            batch_size=5,\n            shuffle=True,\n            num_workers=self.data_loader_workers,\n        )\n\n    def run(self):\n        \"\"\"\n        ## Training\n\n        We aim to solve:\n        $$G^{*}, F^{*} = \\arg \\min_{G,F} \\max_{D_X, D_Y} \\mathcal{L}(G, F, D_X, D_Y)$$\n\n        where,\n        $G$ translates images from $X \\rightarrow Y$,\n        $F$ translates images from $Y \\rightarrow X$,\n        $D_X$ tests if images are from $X$ space,\n        $D_Y$ tests if images are from $Y$ space, and\n\n        \\begin{align}\n        \\mathcal{L}(G, F, D_X, D_Y)\n            &= \\mathcal{L}_{GAN}(G, D_Y, X, Y) \\\\\n            &+ \\mathcal{L}_{GAN}(F, D_X, Y, X) \\\\\n            &+ \\lambda_1 \\mathcal{L}_{cyc}(G, F) \\\\\n            &+ \\lambda_2 \\mathcal{L}_{identity}(G, F) \\\\\n        \\\\\n        \\mathcal{L}_{GAN}(G, F, D_Y, X, Y)\n            &= \\mathbb{E}_{y \\sim p_{data}(y)} \\Big[log D_Y(y)\\Big] \\\\\n            &+ \\mathbb{E}_{x \\sim p_{data}(x)} \\bigg[log\\Big(1 - D_Y(G(x))\\Big)\\bigg] \\\\\n            &+ \\mathbb{E}_{x \\sim p_{data}(x)} \\Big[log D_X(x)\\Big] \\\\\n            &+ \\mathbb{E}_{y \\sim p_{data}(y)} \\bigg[log\\Big(1 - D_X(F(y))\\Big)\\bigg] \\\\\n        \\\\\n        \\mathcal{L}_{cyc}(G, F)\n            &= \\mathbb{E}_{x \\sim p_{data}(x)} \\Big[\\lVert F(G(x)) - x \\lVert_1\\Big] \\\\\n            &+ \\mathbb{E}_{y \\sim p_{data}(y)} \\Big[\\lVert G(F(y)) - y \\rVert_1\\Big] \\\\\n        \\\\\n        \\mathcal{L}_{identity}(G, F)\n            &= \\mathbb{E}_{x \\sim p_{data}(x)} \\Big[\\lVert F(x) - x \\lVert_1\\Big] \\\\\n            &+ \\mathbb{E}_{y \\sim p_{data}(y)} \\Big[\\lVert G(y) - y \\rVert_1\\Big] \\\\\n        \\end{align}\n\n        $\\mathcal{L}_{GAN}$ is the generative adversarial loss from the original\n        GAN paper.\n\n        $\\mathcal{L}_{cyc}$ is the cyclic loss, where we try to get $F(G(x))$ to be similar to $x$,\n        and $G(F(y))$ to be similar to $y$.\n        Basically if the two generators (transformations) are applied in series it should give back the\n        original image.\n        This is the main contribution of this paper.\n        It trains the generators to generate an image of the other distribution that is similar to\n        the original image.\n        Without this loss $G(x)$ could generate anything that's from the distribution of $Y$.\n        Now it needs to generate something from the distribution of $Y$ but still has properties of $x$,\n        so that $F(G(x)$ can re-generate something like $x$.\n\n        $\\mathcal{L}_{cyc}$ is the identity loss.\n        This was used to encourage the mapping to preserve color composition between\n        the input and the output.\n\n        To solve $$G^*, F^*$$,\n        discriminators $D_X$ and $D_Y$ should **ascend** on the gradient,\n\n        \\begin{align}\n        \\nabla_{\\theta_{D_X, D_Y}} \\frac{1}{m} \\sum_{i=1}^m\n        &\\Bigg[\n        \\log D_Y\\Big(y^{(i)}\\Big) \\\\\n        &+ \\log \\Big(1 - D_Y\\Big(G\\Big(x^{(i)}\\Big)\\Big)\\Big) \\\\\n        &+ \\log D_X\\Big(x^{(i)}\\Big) \\\\\n        & +\\log\\Big(1 - D_X\\Big(F\\Big(y^{(i)}\\Big)\\Big)\\Big)\n        \\Bigg]\n        \\end{align}\n\n        That is descend on *negative* log-likelihood loss.\n\n        In order to stabilize the training the negative log- likelihood objective\n        was replaced by a least-squared loss -\n        the least-squared error of discriminator, labelling real images with 1,\n        and generated images with 0.\n        So we want to descend on the gradient,\n\n        \\begin{align}\n        \\nabla_{\\theta_{D_X, D_Y}} \\frac{1}{m} \\sum_{i=1}^m\n        &\\Bigg[\n            \\bigg(D_Y\\Big(y^{(i)}\\Big) - 1\\bigg)^2 \\\\\n            &+ D_Y\\Big(G\\Big(x^{(i)}\\Big)\\Big)^2 \\\\\n            &+ \\bigg(D_X\\Big(x^{(i)}\\Big) - 1\\bigg)^2 \\\\\n            &+ D_X\\Big(F\\Big(y^{(i)}\\Big)\\Big)^2\n        \\Bigg]\n        \\end{align}\n\n        We use least-squares for generators also.\n        The generators should *descend* on the gradient,\n\n        \\begin{align}\n        \\nabla_{\\theta_{F, G}} \\frac{1}{m} \\sum_{i=1}^m\n        &\\Bigg[\n            \\bigg(D_Y\\Big(G\\Big(x^{(i)}\\Big)\\Big) - 1\\bigg)^2 \\\\\n            &+ \\bigg(D_X\\Big(F\\Big(y^{(i)}\\Big)\\Big) - 1\\bigg)^2 \\\\\n            &+ \\mathcal{L}_{cyc}(G, F)\n            + \\mathcal{L}_{identity}(G, F)\n        \\Bigg]\n        \\end{align}\n\n        We use `generator_xy` for $G$ and `generator_yx` for $F$.\n        We use `discriminator_x` for $D_X$ and `discriminator_y` for $D_Y$.\n        \"\"\"\n\n        # Replay buffers to keep generated samples\n        gen_x_buffer = ReplayBuffer()\n        gen_y_buffer = ReplayBuffer()\n\n        # Loop through epochs\n        for epoch in monit.loop(self.epochs):\n            # Loop through the dataset\n            for i, batch in monit.enum('Train', self.dataloader):\n                # Move images to the device\n                data_x, data_y = batch['x'].to(self.device), batch['y'].to(self.device)\n\n                # true labels equal to $1$\n                true_labels = torch.ones(data_x.size(0), *self.discriminator_x.output_shape,\n                                         device=self.device, requires_grad=False)\n                # false labels equal to $0$\n                false_labels = torch.zeros(data_x.size(0), *self.discriminator_x.output_shape,\n                                           device=self.device, requires_grad=False)\n\n                # Train the generators.\n                # This returns the generated images.\n                gen_x, gen_y = self.optimize_generators(data_x, data_y, true_labels)\n\n                #  Train discriminators\n                self.optimize_discriminator(data_x, data_y,\n                                            gen_x_buffer.push_and_pop(gen_x), gen_y_buffer.push_and_pop(gen_y),\n                                            true_labels, false_labels)\n\n                # Save training statistics and increment the global step counter\n                tracker.save()\n                tracker.add_global_step(max(len(data_x), len(data_y)))\n\n                # Save images at intervals\n                batches_done = epoch * len(self.dataloader) + i\n                if batches_done % self.sample_interval == 0:\n                    # Save models when sampling images\n                    experiment.save_checkpoint()\n                    # Sample images\n                    self.sample_images(batches_done)\n\n            # Update learning rates\n            self.generator_lr_scheduler.step()\n            self.discriminator_lr_scheduler.step()\n            # New line\n            tracker.new_line()\n\n    def optimize_generators(self, data_x: torch.Tensor, data_y: torch.Tensor, true_labels: torch.Tensor):\n        \"\"\"\n        ### Optimize the generators with identity, gan and cycle losses.\n        \"\"\"\n\n        #  Change to training mode\n        self.generator_xy.train()\n        self.generator_yx.train()\n\n        # Identity loss\n        # $$\\lVert F(G(x^{(i)})) - x^{(i)} \\lVert_1\\\n        #   \\lVert G(F(y^{(i)})) - y^{(i)} \\rVert_1$$\n        loss_identity = (self.identity_loss(self.generator_yx(data_x), data_x) +\n                         self.identity_loss(self.generator_xy(data_y), data_y))\n\n        # Generate images $G(x)$ and $F(y)$\n        gen_y = self.generator_xy(data_x)\n        gen_x = self.generator_yx(data_y)\n\n        # GAN loss\n        # $$\\bigg(D_Y\\Big(G\\Big(x^{(i)}\\Big)\\Big) - 1\\bigg)^2\n        #  + \\bigg(D_X\\Big(F\\Big(y^{(i)}\\Big)\\Big) - 1\\bigg)^2$$\n        loss_gan = (self.gan_loss(self.discriminator_y(gen_y), true_labels) +\n                    self.gan_loss(self.discriminator_x(gen_x), true_labels))\n\n        # Cycle loss\n        # $$\n        # \\lVert F(G(x^{(i)})) - x^{(i)} \\lVert_1 +\n        # \\lVert G(F(y^{(i)})) - y^{(i)} \\rVert_1\n        # $$\n        loss_cycle = (self.cycle_loss(self.generator_yx(gen_y), data_x) +\n                      self.cycle_loss(self.generator_xy(gen_x), data_y))\n\n        # Total loss\n        loss_generator = (loss_gan +\n                          self.cyclic_loss_coefficient * loss_cycle +\n                          self.identity_loss_coefficient * loss_identity)\n\n        # Take a step in the optimizer\n        self.generator_optimizer.zero_grad()\n        loss_generator.backward()\n        self.generator_optimizer.step()\n\n        # Log losses\n        tracker.add({'loss.generator': loss_generator,\n                     'loss.generator.cycle': loss_cycle,\n                     'loss.generator.gan': loss_gan,\n                     'loss.generator.identity': loss_identity})\n\n        # Return generated images\n        return gen_x, gen_y\n\n    def optimize_discriminator(self, data_x: torch.Tensor, data_y: torch.Tensor,\n                               gen_x: torch.Tensor, gen_y: torch.Tensor,\n                               true_labels: torch.Tensor, false_labels: torch.Tensor):\n        \"\"\"\n        ### Optimize the discriminators with gan loss.\n        \"\"\"\n\n        # GAN Loss\n        #\n        # \\begin{align}\n        # \\bigg(D_Y\\Big(y ^ {(i)}\\Big) - 1\\bigg) ^ 2\n        # + D_Y\\Big(G\\Big(x ^ {(i)}\\Big)\\Big) ^ 2 + \\\\\n        # \\bigg(D_X\\Big(x ^ {(i)}\\Big) - 1\\bigg) ^ 2\n        # + D_X\\Big(F\\Big(y ^ {(i)}\\Big)\\Big) ^ 2\n        # \\end{align}\n        loss_discriminator = (self.gan_loss(self.discriminator_x(data_x), true_labels) +\n                              self.gan_loss(self.discriminator_x(gen_x), false_labels) +\n                              self.gan_loss(self.discriminator_y(data_y), true_labels) +\n                              self.gan_loss(self.discriminator_y(gen_y), false_labels))\n\n        # Take a step in the optimizer\n        self.discriminator_optimizer.zero_grad()\n        loss_discriminator.backward()\n        self.discriminator_optimizer.step()\n\n        # Log losses\n        tracker.add({'loss.discriminator': loss_discriminator})\n\n\ndef train():\n    \"\"\"\n    ## Train Cycle GAN\n    \"\"\"\n    # Create configurations\n    conf = Configs()\n    # Create an experiment\n    experiment.create(name='cycle_gan')\n    # Calculate configurations.\n    # It will calculate `conf.run` and all other configs required by it.\n    experiment.configs(conf, {'dataset_name': 'summer2winter_yosemite'})\n    conf.initialize()\n\n    # Register models for saving and loading.\n    # `get_modules` gives a dictionary of `nn.Modules` in `conf`.\n    # You can also specify a custom dictionary of models.\n    experiment.add_pytorch_models(get_modules(conf))\n    # Start and watch the experiment\n    with experiment.start():\n        # Run the training\n        conf.run()\n\n\ndef plot_image(img: torch.Tensor):\n    \"\"\"\n    ### Plot an image with matplotlib\n    \"\"\"\n    from matplotlib import pyplot as plt\n\n    # Move tensor to CPU\n    img = img.cpu()\n    # Get min and max values of the image for normalization\n    img_min, img_max = img.min(), img.max()\n    # Scale image values to be [0...1]\n    img = (img - img_min) / (img_max - img_min + 1e-5)\n    # We have to change the order of dimensions to HWC.\n    img = img.permute(1, 2, 0)\n    # Show Image\n    plt.imshow(img)\n    # We don't need axes\n    plt.axis('off')\n    # Display\n    plt.show()\n\n\ndef evaluate():\n    \"\"\"\n    ## Evaluate trained Cycle GAN\n    \"\"\"\n    # Set the run UUID from the training run\n    trained_run_uuid = 'f73c1164184711eb9190b74249275441'\n    # Create configs object\n    conf = Configs()\n    # Create experiment\n    experiment.create(name='cycle_gan_inference')\n    # Load hyper parameters set for training\n    conf_dict = experiment.load_configs(trained_run_uuid)\n    # Calculate configurations. We specify the generators `'generator_xy', 'generator_yx'`\n    # so that it only loads those and their dependencies.\n    # Configs like `device` and `img_channels` will be calculated, since these are required by\n    # `generator_xy` and `generator_yx`.\n    #\n    # If you want other parameters like `dataset_name` you should specify them here.\n    # If you specify nothing, all the configurations will be calculated, including data loaders.\n    # Calculation of configurations and their dependencies will happen when you call `experiment.start`\n    experiment.configs(conf, conf_dict)\n    conf.initialize()\n\n    # Register models for saving and loading.\n    # `get_modules` gives a dictionary of `nn.Modules` in `conf`.\n    # You can also specify a custom dictionary of models.\n    experiment.add_pytorch_models(get_modules(conf))\n    # Specify which run to load from.\n    # Loading will actually happen when you call `experiment.start`\n    experiment.load(trained_run_uuid)\n\n    # Start the experiment\n    with experiment.start():\n        # Image transformations\n        transforms_ = [\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n\n        # Load your own data. Here we try the test set.\n        # I was trying with Yosemite photos, they look awesome.\n        # You can use `conf.dataset_name`, if you specified `dataset_name` as something you wanted to be calculated\n        # in the call to `experiment.configs`\n        dataset = ImageDataset(conf.dataset_name, transforms_, 'train')\n        # Get an image from dataset\n        x_image = dataset[10]['x']\n        # Display the image\n        plot_image(x_image)\n\n        # Evaluation mode\n        conf.generator_xy.eval()\n        conf.generator_yx.eval()\n\n        # We don't need gradients\n        with torch.no_grad():\n            # Add batch dimension and move to the device we use\n            data = x_image.unsqueeze(0).to(conf.device)\n            generated_y = conf.generator_xy(data)\n\n        # Display the generated image.\n        plot_image(generated_y[0].cpu())\n\n\nif __name__ == '__main__':\n    train()\n    # evaluate()\n", "labml_nn/gan/stylegan/experiment.py": "\"\"\"\n---\ntitle: StyleGAN 2 Model Training\nsummary: >\n An annotated PyTorch implementation of StyleGAN2 model training code.\n---\n\n# [StyleGAN 2](index.html) Model Training\n\nThis is the training code for [StyleGAN 2](index.html) model.\n\n![Generated Images](generated_64.png)\n\n---*These are $64 \\times 64$ images generated after training for about 80K steps.*---\n\n*Our implementation is a minimalistic StyleGAN 2 model training code.\nOnly single GPU training is supported to keep the implementation simple.\nWe managed to shrink it to keep it at less than 500 lines of code, including the training loop.*\n\n*Without DDP (distributed data parallel) and multi-gpu training it will not be possible to train the model\nfor large resolutions (128+).\nIf you want training code with fp16 and DDP take a look at\n[lucidrains/stylegan2-pytorch](https://github.com/lucidrains/stylegan2-pytorch).*\n\nWe trained this on [CelebA-HQ dataset](https://github.com/tkarras/progressive_growing_of_gans).\nYou can find the download instruction in this\n[discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\nSave the images inside [`data/stylegan` folder](#dataset_path).\n\"\"\"\n\nimport math\nfrom pathlib import Path\nfrom typing import Iterator, Tuple\n\nimport torch\nimport torch.utils.data\nimport torchvision\nfrom PIL import Image\n\nfrom labml import tracker, lab, monit, experiment\nfrom labml.configs import BaseConfigs\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_helpers.train_valid import ModeState, hook_model_outputs\nfrom labml_nn.gan.stylegan import Discriminator, Generator, MappingNetwork, GradientPenalty, PathLengthPenalty\nfrom labml_nn.gan.wasserstein import DiscriminatorLoss, GeneratorLoss\nfrom labml_nn.utils import cycle_dataloader\n\n\nclass Dataset(torch.utils.data.Dataset):\n    \"\"\"\n    ## Dataset\n\n    This loads the training dataset and resize it to the give image size.\n    \"\"\"\n\n    def __init__(self, path: str, image_size: int):\n        \"\"\"\n        * `path` path to the folder containing the images\n        * `image_size` size of the image\n        \"\"\"\n        super().__init__()\n\n        # Get the paths of all `jpg` files\n        self.paths = [p for p in Path(path).glob(f'**/*.jpg')]\n\n        # Transformation\n        self.transform = torchvision.transforms.Compose([\n            # Resize the image\n            torchvision.transforms.Resize(image_size),\n            # Convert to PyTorch tensor\n            torchvision.transforms.ToTensor(),\n        ])\n\n    def __len__(self):\n        \"\"\"Number of images\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        \"\"\"Get the the `index`-th image\"\"\"\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)\n\n\nclass Configs(BaseConfigs):\n    \"\"\"\n    ## Configurations\n    \"\"\"\n\n    # Device to train the model on.\n    # [`DeviceConfigs`](https://docs.labml.ai/api/helpers.html#labml_helpers.device.DeviceConfigs)\n    #  picks up an available CUDA device or defaults to CPU.\n    device: torch.device = DeviceConfigs()\n\n    # [StyleGAN2 Discriminator](index.html#discriminator)\n    discriminator: Discriminator\n    # [StyleGAN2 Generator](index.html#generator)\n    generator: Generator\n    # [Mapping network](index.html#mapping_network)\n    mapping_network: MappingNetwork\n\n    # Discriminator and generator loss functions.\n    # We use [Wasserstein loss](../wasserstein/index.html)\n    discriminator_loss: DiscriminatorLoss\n    generator_loss: GeneratorLoss\n\n    # Optimizers\n    generator_optimizer: torch.optim.Adam\n    discriminator_optimizer: torch.optim.Adam\n    mapping_network_optimizer: torch.optim.Adam\n\n    # [Gradient Penalty Regularization Loss](index.html#gradient_penalty)\n    gradient_penalty = GradientPenalty()\n    # Gradient penalty coefficient $\\gamma$\n    gradient_penalty_coefficient: float = 10.\n\n    # [Path length penalty](index.html#path_length_penalty)\n    path_length_penalty: PathLengthPenalty\n\n    # Data loader\n    loader: Iterator\n\n    # Batch size\n    batch_size: int = 32\n    # Dimensionality of $z$ and $w$\n    d_latent: int = 512\n    # Height/width of the image\n    image_size: int = 32\n    # Number of layers in the mapping network\n    mapping_network_layers: int = 8\n    # Generator & Discriminator learning rate\n    learning_rate: float = 1e-3\n    # Mapping network learning rate ($100 \\times$ lower than the others)\n    mapping_network_learning_rate: float = 1e-5\n    # Number of steps to accumulate gradients on. Use this to increase the effective batch size.\n    gradient_accumulate_steps: int = 1\n    # $\\beta_1$ and $\\beta_2$ for Adam optimizer\n    adam_betas: Tuple[float, float] = (0.0, 0.99)\n    # Probability of mixing styles\n    style_mixing_prob: float = 0.9\n\n    # Total number of training steps\n    training_steps: int = 150_000\n\n    # Number of blocks in the generator (calculated based on image resolution)\n    n_gen_blocks: int\n\n    # ### Lazy regularization\n    # Instead of calculating the regularization losses, the paper proposes lazy regularization\n    # where the regularization terms are calculated once in a while.\n    # This improves the training efficiency a lot.\n\n    # The interval at which to compute gradient penalty\n    lazy_gradient_penalty_interval: int = 4\n    # Path length penalty calculation interval\n    lazy_path_penalty_interval: int = 32\n    # Skip calculating path length penalty during the initial phase of training\n    lazy_path_penalty_after: int = 5_000\n\n    # How often to log generated images\n    log_generated_interval: int = 500\n    # How often to save model checkpoints\n    save_checkpoint_interval: int = 2_000\n\n    # Training mode state for logging activations\n    mode: ModeState\n    # Whether to log model layer outputs\n    log_layer_outputs: bool = False\n\n    # <a id=\"dataset_path\"></a>\n    # We trained this on [CelebA-HQ dataset](https://github.com/tkarras/progressive_growing_of_gans).\n    # You can find the download instruction in this\n    # [discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\n    # Save the images inside `data/stylegan` folder.\n    dataset_path: str = str(lab.get_data_path() / 'stylegan2')\n\n    def init(self):\n        \"\"\"\n        ### Initialize\n        \"\"\"\n        # Create dataset\n        dataset = Dataset(self.dataset_path, self.image_size)\n        # Create data loader\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, num_workers=8,\n                                                 shuffle=True, drop_last=True, pin_memory=True)\n        # Continuous [cyclic loader](../../utils.html#cycle_dataloader)\n        self.loader = cycle_dataloader(dataloader)\n\n        # $\\log_2$ of image resolution\n        log_resolution = int(math.log2(self.image_size))\n\n        # Create discriminator and generator\n        self.discriminator = Discriminator(log_resolution).to(self.device)\n        self.generator = Generator(log_resolution, self.d_latent).to(self.device)\n        # Get number of generator blocks for creating style and noise inputs\n        self.n_gen_blocks = self.generator.n_blocks\n        # Create mapping network\n        self.mapping_network = MappingNetwork(self.d_latent, self.mapping_network_layers).to(self.device)\n        # Create path length penalty loss\n        self.path_length_penalty = PathLengthPenalty(0.99).to(self.device)\n\n        # Add model hooks to monitor layer outputs\n        if self.log_layer_outputs:\n            hook_model_outputs(self.mode, self.discriminator, 'discriminator')\n            hook_model_outputs(self.mode, self.generator, 'generator')\n            hook_model_outputs(self.mode, self.mapping_network, 'mapping_network')\n\n        # Discriminator and generator losses\n        self.discriminator_loss = DiscriminatorLoss().to(self.device)\n        self.generator_loss = GeneratorLoss().to(self.device)\n\n        # Create optimizers\n        self.discriminator_optimizer = torch.optim.Adam(\n            self.discriminator.parameters(),\n            lr=self.learning_rate, betas=self.adam_betas\n        )\n        self.generator_optimizer = torch.optim.Adam(\n            self.generator.parameters(),\n            lr=self.learning_rate, betas=self.adam_betas\n        )\n        self.mapping_network_optimizer = torch.optim.Adam(\n            self.mapping_network.parameters(),\n            lr=self.mapping_network_learning_rate, betas=self.adam_betas\n        )\n\n        # Set tracker configurations\n        tracker.set_image(\"generated\", True)\n\n    def get_w(self, batch_size: int):\n        \"\"\"\n        ### Sample $w$\n\n        This samples $z$ randomly and get $w$ from the mapping network.\n\n        We also apply style mixing sometimes where we generate two latent variables\n        $z_1$ and $z_2$ and get corresponding $w_1$ and $w_2$.\n        Then we randomly sample a cross-over point and apply $w_1$ to\n        the generator blocks before the cross-over point and\n        $w_2$ to the blocks after.\n        \"\"\"\n\n        # Mix styles\n        if torch.rand(()).item() < self.style_mixing_prob:\n            # Random cross-over point\n            cross_over_point = int(torch.rand(()).item() * self.n_gen_blocks)\n            # Sample $z_1$ and $z_2$\n            z2 = torch.randn(batch_size, self.d_latent).to(self.device)\n            z1 = torch.randn(batch_size, self.d_latent).to(self.device)\n            # Get $w_1$ and $w_2$\n            w1 = self.mapping_network(z1)\n            w2 = self.mapping_network(z2)\n            # Expand $w_1$ and $w_2$ for the generator blocks and concatenate\n            w1 = w1[None, :, :].expand(cross_over_point, -1, -1)\n            w2 = w2[None, :, :].expand(self.n_gen_blocks - cross_over_point, -1, -1)\n            return torch.cat((w1, w2), dim=0)\n        # Without mixing\n        else:\n            # Sample $z$ and $z$\n            z = torch.randn(batch_size, self.d_latent).to(self.device)\n            # Get $w$ and $w$\n            w = self.mapping_network(z)\n            # Expand $w$ for the generator blocks\n            return w[None, :, :].expand(self.n_gen_blocks, -1, -1)\n\n    def get_noise(self, batch_size: int):\n        \"\"\"\n        ### Generate noise\n\n        This generates noise for each [generator block](index.html#generator_block)\n        \"\"\"\n        # List to store noise\n        noise = []\n        # Noise resolution starts from $4$\n        resolution = 4\n\n        # Generate noise for each generator block\n        for i in range(self.n_gen_blocks):\n            # The first block has only one $3 \\times 3$ convolution\n            if i == 0:\n                n1 = None\n            # Generate noise to add after the first convolution layer\n            else:\n                n1 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n            # Generate noise to add after the second convolution layer\n            n2 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n\n            # Add noise tensors to the list\n            noise.append((n1, n2))\n\n            # Next block has $2 \\times$ resolution\n            resolution *= 2\n\n        # Return noise tensors\n        return noise\n\n    def generate_images(self, batch_size: int):\n        \"\"\"\n        ### Generate images\n\n        This generate images using the generator\n        \"\"\"\n\n        # Get $w$\n        w = self.get_w(batch_size)\n        # Get noise\n        noise = self.get_noise(batch_size)\n\n        # Generate images\n        images = self.generator(w, noise)\n\n        # Return images and $w$\n        return images, w\n\n    def step(self, idx: int):\n        \"\"\"\n        ### Training Step\n        \"\"\"\n\n        # Train the discriminator\n        with monit.section('Discriminator'):\n            # Reset gradients\n            self.discriminator_optimizer.zero_grad()\n\n            # Accumulate gradients for `gradient_accumulate_steps`\n            for i in range(self.gradient_accumulate_steps):\n                # Update `mode`. Set whether to log activation\n                with self.mode.update(is_log_activations=(idx + 1) % self.log_generated_interval == 0):\n                    # Sample images from generator\n                    generated_images, _ = self.generate_images(self.batch_size)\n                    # Discriminator classification for generated images\n                    fake_output = self.discriminator(generated_images.detach())\n\n                    # Get real images from the data loader\n                    real_images = next(self.loader).to(self.device)\n                    # We need to calculate gradients w.r.t. real images for gradient penalty\n                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n                        real_images.requires_grad_()\n                    # Discriminator classification for real images\n                    real_output = self.discriminator(real_images)\n\n                    # Get discriminator loss\n                    real_loss, fake_loss = self.discriminator_loss(real_output, fake_output)\n                    disc_loss = real_loss + fake_loss\n\n                    # Add gradient penalty\n                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n                        # Calculate and log gradient penalty\n                        gp = self.gradient_penalty(real_images, real_output)\n                        tracker.add('loss.gp', gp)\n                        # Multiply by coefficient and add gradient penalty\n                        disc_loss = disc_loss + 0.5 * self.gradient_penalty_coefficient * gp * self.lazy_gradient_penalty_interval\n\n                    # Compute gradients\n                    disc_loss.backward()\n\n                    # Log discriminator loss\n                    tracker.add('loss.discriminator', disc_loss)\n\n            if (idx + 1) % self.log_generated_interval == 0:\n                # Log discriminator model parameters occasionally\n                tracker.add('discriminator', self.discriminator)\n\n            # Clip gradients for stabilization\n            torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=1.0)\n            # Take optimizer step\n            self.discriminator_optimizer.step()\n\n        # Train the generator\n        with monit.section('Generator'):\n            # Reset gradients\n            self.generator_optimizer.zero_grad()\n            self.mapping_network_optimizer.zero_grad()\n\n            # Accumulate gradients for `gradient_accumulate_steps`\n            for i in range(self.gradient_accumulate_steps):\n                # Sample images from generator\n                generated_images, w = self.generate_images(self.batch_size)\n                # Discriminator classification for generated images\n                fake_output = self.discriminator(generated_images)\n\n                # Get generator loss\n                gen_loss = self.generator_loss(fake_output)\n\n                # Add path length penalty\n                if idx > self.lazy_path_penalty_after and (idx + 1) % self.lazy_path_penalty_interval == 0:\n                    # Calculate path length penalty\n                    plp = self.path_length_penalty(w, generated_images)\n                    # Ignore if `nan`\n                    if not torch.isnan(plp):\n                        tracker.add('loss.plp', plp)\n                        gen_loss = gen_loss + plp\n\n                # Calculate gradients\n                gen_loss.backward()\n\n                # Log generator loss\n                tracker.add('loss.generator', gen_loss)\n\n            if (idx + 1) % self.log_generated_interval == 0:\n                # Log discriminator model parameters occasionally\n                tracker.add('generator', self.generator)\n                tracker.add('mapping_network', self.mapping_network)\n\n            # Clip gradients for stabilization\n            torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=1.0)\n            torch.nn.utils.clip_grad_norm_(self.mapping_network.parameters(), max_norm=1.0)\n\n            # Take optimizer step\n            self.generator_optimizer.step()\n            self.mapping_network_optimizer.step()\n\n        # Log generated images\n        if (idx + 1) % self.log_generated_interval == 0:\n            tracker.add('generated', torch.cat([generated_images[:6], real_images[:3]], dim=0))\n        # Save model checkpoints\n        if (idx + 1) % self.save_checkpoint_interval == 0:\n            experiment.save_checkpoint()\n\n        # Flush tracker\n        tracker.save()\n\n    def train(self):\n        \"\"\"\n        ## Train model\n        \"\"\"\n\n        # Loop for `training_steps`\n        for i in monit.loop(self.training_steps):\n            # Take a training step\n            self.step(i)\n            #\n            if (i + 1) % self.log_generated_interval == 0:\n                tracker.new_line()\n\n\ndef main():\n    \"\"\"\n    ### Train StyleGAN2\n    \"\"\"\n\n    # Create an experiment\n    experiment.create(name='stylegan2')\n    # Create configurations object\n    configs = Configs()\n\n    # Set configurations and override some\n    experiment.configs(configs, {\n        'device.cuda_device': 0,\n        'image_size': 64,\n        'log_generated_interval': 200\n    })\n\n    # Initialize\n    configs.init()\n    # Set models for saving and loading\n    experiment.add_pytorch_models(mapping_network=configs.mapping_network,\n                                  generator=configs.generator,\n                                  discriminator=configs.discriminator)\n\n    # Start the experiment\n    with experiment.start():\n        # Run the training loop\n        configs.train()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/gan/stylegan/__init__.py": "\"\"\"\n---\ntitle: StyleGAN 2\nsummary: >\n An annotated PyTorch implementation of StyleGAN2.\n---\n\n# StyleGAN 2\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n [Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/abs/1912.04958)\n which introduces **StyleGAN 2**.\nStyleGAN 2 is an improvement over **StyleGAN** from the paper\n [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948).\nAnd StyleGAN is based on **Progressive GAN** from the paper\n [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196).\nAll three papers are from the same authors from [NVIDIA AI](https://twitter.com/NVIDIAAI).\n\n*Our implementation is a minimalistic StyleGAN 2 model training code.\nOnly single GPU training is supported to keep the implementation simple.\nWe managed to shrink it to keep it at less than 500 lines of code, including the training loop.*\n\n**\ud83c\udfc3 Here's the training code: [`experiment.py`](experiment.html).**\n\n![Generated Images](generated_64.png)\n\n---*These are $64 \\times 64$ images generated after training for about 80K steps.*---\n\n\nWe'll first introduce the three papers at a high level.\n\n## Generative Adversarial Networks\n\nGenerative adversarial networks have two components; the generator and the discriminator.\nThe generator network takes a random latent vector ($z \\in \\mathcal{Z}$)\n and tries to generate a realistic image.\nThe discriminator network tries to differentiate the real images from generated images.\nWhen we train the two networks together the generator starts generating images indistinguishable from real images.\n\n## Progressive GAN\n\nProgressive GAN generates high-resolution images ($1080 \\times 1080$) of size.\nIt does so by *progressively* increasing the image size.\nFirst, it trains a network that produces a $4 \\times 4$ image, then $8 \\times 8$ ,\n then an $16 \\times 16$  image, and so on up to the desired image resolution.\n\nAt each resolution, the generator network produces an image in latent space which is converted into RGB,\nwith a $1 \\times 1$  convolution.\nWhen we progress from a lower resolution to a higher resolution\n (say from $4 \\times 4$  to $8 \\times 8$ ) we scale the latent image by $2\\times$\n and add a new block (two $3 \\times 3$  convolution layers)\n and a new $1 \\times 1$  layer to get RGB.\nThe transition is done smoothly by adding a residual connection to\n the $2\\times$ scaled $4 \\times 4$  RGB image.\nThe weight of this residual connection is slowly reduced, to let the new block take over.\n\nThe discriminator is a mirror image of the generator network.\nThe progressive growth of the discriminator is done similarly.\n\n![progressive_gan.svg](progressive_gan.svg)\n\n---*$2\\times$ and $0.5\\times$ denote feature map resolution scaling and scaling.\n$4\\times4$, $8\\times4$, ... denote feature map resolution at the generator or discriminator block.\nEach discriminator and generator block consists of 2 convolution layers with leaky ReLU activations.*---\n\nThey use **minibatch standard deviation** to increase variation and\n **equalized learning rate** which we discussed below in the implementation.\nThey also use **pixel-wise normalization** where at each pixel the feature vector is normalized.\nThey apply this to all the convolution layer outputs (except RGB).\n\n\n## StyleGAN\n\nStyleGAN improves the generator of Progressive GAN keeping the discriminator architecture the same.\n\n#### Mapping Network\n\nIt maps the random latent vector ($z \\in \\mathcal{Z}$)\n into a different latent space ($w \\in \\mathcal{W}$),\n with an 8-layer neural network.\nThis gives an intermediate latent space $\\mathcal{W}$\nwhere the factors of variations are more linear (disentangled).\n\n#### AdaIN\n\nThen $w$ is transformed into two vectors (**styles**) per layer,\n $i$, $y_i = (y_{s,i}, y_{b,i}) = f_{A_i}(w)$ and used for scaling and shifting (biasing)\n in each layer with $\\text{AdaIN}$ operator (normalize and scale):\n$$\\text{AdaIN}(x_i, y_i) = y_{s, i} \\frac{x_i - \\mu(x_i)}{\\sigma(x_i)} + y_{b,i}$$\n\n#### Style Mixing\n\nTo prevent the generator from assuming adjacent styles are correlated,\n they randomly use different styles for different blocks.\nThat is, they sample two latent vectors $(z_1, z_2)$ and corresponding $(w_1, w_2)$ and\n use $w_1$ based styles for some blocks and $w_2$ based styles for some blacks randomly.\n\n#### Stochastic Variation\n\nNoise is made available to each block which helps the generator create more realistic images.\nNoise is scaled per channel by a learned weight.\n\n#### Bilinear Up and Down Sampling\n\nAll the up and down-sampling operations are accompanied by bilinear smoothing.\n\n![style_gan.svg](style_gan.svg)\n\n---*$A$ denotes a linear layer.\n$B$ denotes a broadcast and scaling operation (noise is a single channel).\nStyleGAN also uses progressive growing like Progressive GAN.*---\n\n## StyleGAN 2\n\nStyleGAN 2 changes both the generator and the discriminator of StyleGAN.\n\n#### Weight Modulation and Demodulation\n\nThey remove the $\\text{AdaIN}$ operator and replace it with\n the weight modulation and demodulation step.\nThis is supposed to improve what they call droplet artifacts that are present in generated images,\n which are caused by the normalization in $\\text{AdaIN}$ operator.\nStyle vector per layer is calculated from $w_i \\in \\mathcal{W}$ as $s_i = f_{A_i}(w_i)$.\n\nThen the convolution weights $w$ are modulated as follows.\n($w$ here on refers to weights not intermediate latent space,\n we are sticking to the same notation as the paper.)\n\n$$w'_{i, j, k} = s_i \\cdot w_{i, j, k}$$\nThen it's demodulated by normalizing,\n$$w''_{i,j,k} = \\frac{w'_{i,j,k}}{\\sqrt{\\sum_{i,k}{w'_{i, j, k}}^2 + \\epsilon}}$$\nwhere $i$ is the input channel, $j$ is the output channel, and $k$ is the kernel index.\n\n#### Path Length Regularization\n\nPath length regularization encourages a fixed-size step in $\\mathcal{W}$ to result in a non-zero,\n fixed-magnitude change in the generated image.\n\n#### No Progressive Growing\n\nStyleGAN2 uses residual connections (with down-sampling) in the discriminator and skip connections\n in the generator with up-sampling\n  (the RGB outputs from each layer are added - no residual connections in feature maps).\nThey show that with experiments that the contribution of low-resolution layers is higher\n at beginning of the training and then high-resolution layers take over.\n\"\"\"\n\nimport math\nfrom typing import Tuple, Optional, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom torch import nn\n\n\nclass MappingNetwork(nn.Module):\n    \"\"\"\n    <a id=\"mapping_network\"></a>\n\n    ## Mapping Network\n\n    ![Mapping Network](mapping_network.svg)\n\n    This is an MLP with 8 linear layers.\n    The mapping network maps the latent vector $z \\in \\mathcal{W}$\n    to an intermediate latent space $w \\in \\mathcal{W}$.\n    $\\mathcal{W}$ space will be disentangled from the image space\n    where the factors of variation become more linear.\n    \"\"\"\n\n    def __init__(self, features: int, n_layers: int):\n        \"\"\"\n        * `features` is the number of features in $z$ and $w$\n        * `n_layers` is the number of layers in the mapping network.\n        \"\"\"\n        super().__init__()\n\n        # Create the MLP\n        layers = []\n        for i in range(n_layers):\n            # [Equalized learning-rate linear layers](#equalized_linear)\n            layers.append(EqualizedLinear(features, features))\n            # Leaky Relu\n            layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True))\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, z: torch.Tensor):\n        # Normalize $z$\n        z = F.normalize(z, dim=1)\n        # Map $z$ to $w$\n        return self.net(z)\n\n\nclass Generator(nn.Module):\n    \"\"\"\n    <a id=\"generator\"></a>\n\n    ## StyleGAN2 Generator\n\n    ![Generator](style_gan2.svg)\n\n    ---*$A$ denotes a linear layer.\n    $B$ denotes a broadcast and scaling operation (noise is a single channel).\n    [`toRGB`](#to_rgb) also has a style modulation which is not shown in the diagram to keep it simple.*---\n\n    The generator starts with a learned constant.\n    Then it has a series of blocks. The feature map resolution is doubled at each block\n    Each block outputs an RGB image and they are scaled up and summed to get the final RGB image.\n    \"\"\"\n\n    def __init__(self, log_resolution: int, d_latent: int, n_features: int = 32, max_features: int = 512):\n        \"\"\"\n        * `log_resolution` is the $\\log_2$ of image resolution\n        * `d_latent` is the dimensionality of $w$\n        * `n_features` number of features in the convolution layer at the highest resolution (final block)\n        * `max_features` maximum number of features in any generator block\n        \"\"\"\n        super().__init__()\n\n        # Calculate the number of features for each block\n        #\n        # Something like `[512, 512, 256, 128, 64, 32]`\n        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 2, -1, -1)]\n        # Number of generator blocks\n        self.n_blocks = len(features)\n\n        # Trainable $4 \\times 4$ constant\n        self.initial_constant = nn.Parameter(torch.randn((1, features[0], 4, 4)))\n\n        # First style block for $4 \\times 4$ resolution and layer to get RGB\n        self.style_block = StyleBlock(d_latent, features[0], features[0])\n        self.to_rgb = ToRGB(d_latent, features[0])\n\n        # Generator blocks\n        blocks = [GeneratorBlock(d_latent, features[i - 1], features[i]) for i in range(1, self.n_blocks)]\n        self.blocks = nn.ModuleList(blocks)\n\n        # $2 \\times$ up sampling layer. The feature space is up sampled\n        # at each block\n        self.up_sample = UpSample()\n\n    def forward(self, w: torch.Tensor, input_noise: List[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]]):\n        \"\"\"\n        * `w` is $w$. In order to mix-styles (use different $w$ for different layers), we provide a separate\n        $w$ for each [generator block](#generator_block). It has shape `[n_blocks, batch_size, d_latent]`.\n        * `input_noise` is the noise for each block.\n        It's a list of pairs of noise sensors because each block (except the initial) has two noise inputs\n        after each convolution layer (see the diagram).\n        \"\"\"\n\n        # Get batch size\n        batch_size = w.shape[1]\n\n        # Expand the learned constant to match batch size\n        x = self.initial_constant.expand(batch_size, -1, -1, -1)\n\n        # The first style block\n        x = self.style_block(x, w[0], input_noise[0][1])\n        # Get first rgb image\n        rgb = self.to_rgb(x, w[0])\n\n        # Evaluate rest of the blocks\n        for i in range(1, self.n_blocks):\n            # Up sample the feature map\n            x = self.up_sample(x)\n            # Run it through the [generator block](#generator_block)\n            x, rgb_new = self.blocks[i - 1](x, w[i], input_noise[i])\n            # Up sample the RGB image and add to the rgb from the block\n            rgb = self.up_sample(rgb) + rgb_new\n\n        # Return the final RGB image\n        return rgb\n\n\nclass GeneratorBlock(nn.Module):\n    \"\"\"\n    <a id=\"generator_block\"></a>\n\n    ### Generator Block\n\n    ![Generator block](generator_block.svg)\n\n    ---*$A$ denotes a linear layer.\n    $B$ denotes a broadcast and scaling operation (noise is a single channel).\n    [`toRGB`](#to_rgb) also has a style modulation which is not shown in the diagram to keep it simple.*---\n\n    The generator block consists of two [style blocks](#style_block) ($3 \\times 3$ convolutions with style modulation)\n    and an RGB output.\n    \"\"\"\n\n    def __init__(self, d_latent: int, in_features: int, out_features: int):\n        \"\"\"\n        * `d_latent` is the dimensionality of $w$\n        * `in_features` is the number of features in the input feature map\n        * `out_features` is the number of features in the output feature map\n        \"\"\"\n        super().__init__()\n\n        # First [style block](#style_block) changes the feature map size to `out_features`\n        self.style_block1 = StyleBlock(d_latent, in_features, out_features)\n        # Second [style block](#style_block)\n        self.style_block2 = StyleBlock(d_latent, out_features, out_features)\n\n        # *toRGB* layer\n        self.to_rgb = ToRGB(d_latent, out_features)\n\n    def forward(self, x: torch.Tensor, w: torch.Tensor, noise: Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]):\n        \"\"\"\n        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n        * `w` is $w$ with shape `[batch_size, d_latent]`\n        * `noise` is a tuple of two noise tensors of shape `[batch_size, 1, height, width]`\n        \"\"\"\n        # First style block with first noise tensor.\n        # The output is of shape `[batch_size, out_features, height, width]`\n        x = self.style_block1(x, w, noise[0])\n        # Second style block with second noise tensor.\n        # The output is of shape `[batch_size, out_features, height, width]`\n        x = self.style_block2(x, w, noise[1])\n\n        # Get RGB image\n        rgb = self.to_rgb(x, w)\n\n        # Return feature map and rgb image\n        return x, rgb\n\n\nclass StyleBlock(nn.Module):\n    \"\"\"\n    <a id=\"style_block\"></a>\n\n    ### Style Block\n\n    ![Style block](style_block.svg)\n\n    ---*$A$ denotes a linear layer.\n    $B$ denotes a broadcast and scaling operation (noise is single channel).*---\n\n    Style block has a weight modulation convolution layer.\n    \"\"\"\n\n    def __init__(self, d_latent: int, in_features: int, out_features: int):\n        \"\"\"\n        * `d_latent` is the dimensionality of $w$\n        * `in_features` is the number of features in the input feature map\n        * `out_features` is the number of features in the output feature map\n        \"\"\"\n        super().__init__()\n        # Get style vector from $w$ (denoted by $A$ in the diagram) with\n        # an [equalized learning-rate linear layer](#equalized_linear)\n        self.to_style = EqualizedLinear(d_latent, in_features, bias=1.0)\n        # Weight modulated convolution layer\n        self.conv = Conv2dWeightModulate(in_features, out_features, kernel_size=3)\n        # Noise scale\n        self.scale_noise = nn.Parameter(torch.zeros(1))\n        # Bias\n        self.bias = nn.Parameter(torch.zeros(out_features))\n\n        # Activation function\n        self.activation = nn.LeakyReLU(0.2, True)\n\n    def forward(self, x: torch.Tensor, w: torch.Tensor, noise: Optional[torch.Tensor]):\n        \"\"\"\n        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n        * `w` is $w$ with shape `[batch_size, d_latent]`\n        * `noise` is a tensor of shape `[batch_size, 1, height, width]`\n        \"\"\"\n        # Get style vector $s$\n        s = self.to_style(w)\n        # Weight modulated convolution\n        x = self.conv(x, s)\n        # Scale and add noise\n        if noise is not None:\n            x = x + self.scale_noise[None, :, None, None] * noise\n        # Add bias and evaluate activation function\n        return self.activation(x + self.bias[None, :, None, None])\n\n\nclass ToRGB(nn.Module):\n    \"\"\"\n    <a id=\"to_rgb\"></a>\n\n    ### To RGB\n\n    ![To RGB](to_rgb.svg)\n\n    ---*$A$ denotes a linear layer.*---\n\n    Generates an RGB image from a feature map using $1 \\times 1$ convolution.\n    \"\"\"\n\n    def __init__(self, d_latent: int, features: int):\n        \"\"\"\n        * `d_latent` is the dimensionality of $w$\n        * `features` is the number of features in the feature map\n        \"\"\"\n        super().__init__()\n        # Get style vector from $w$ (denoted by $A$ in the diagram) with\n        # an [equalized learning-rate linear layer](#equalized_linear)\n        self.to_style = EqualizedLinear(d_latent, features, bias=1.0)\n\n        # Weight modulated convolution layer without demodulation\n        self.conv = Conv2dWeightModulate(features, 3, kernel_size=1, demodulate=False)\n        # Bias\n        self.bias = nn.Parameter(torch.zeros(3))\n        # Activation function\n        self.activation = nn.LeakyReLU(0.2, True)\n\n    def forward(self, x: torch.Tensor, w: torch.Tensor):\n        \"\"\"\n        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n        * `w` is $w$ with shape `[batch_size, d_latent]`\n        \"\"\"\n        # Get style vector $s$\n        style = self.to_style(w)\n        # Weight modulated convolution\n        x = self.conv(x, style)\n        # Add bias and evaluate activation function\n        return self.activation(x + self.bias[None, :, None, None])\n\n\nclass Conv2dWeightModulate(nn.Module):\n    \"\"\"\n    ### Convolution with Weight Modulation and Demodulation\n\n    This layer scales the convolution weights by the style vector and demodulates by normalizing it.\n    \"\"\"\n\n    def __init__(self, in_features: int, out_features: int, kernel_size: int,\n                 demodulate: float = True, eps: float = 1e-8):\n        \"\"\"\n        * `in_features` is the number of features in the input feature map\n        * `out_features` is the number of features in the output feature map\n        * `kernel_size` is the size of the convolution kernel\n        * `demodulate` is flag whether to normalize weights by its standard deviation\n        * `eps` is the $\\epsilon$ for normalizing\n        \"\"\"\n        super().__init__()\n        # Number of output features\n        self.out_features = out_features\n        # Whether to normalize weights\n        self.demodulate = demodulate\n        # Padding size\n        self.padding = (kernel_size - 1) // 2\n\n        # [Weights parameter with equalized learning rate](#equalized_weight)\n        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n        # $\\epsilon$\n        self.eps = eps\n\n    def forward(self, x: torch.Tensor, s: torch.Tensor):\n        \"\"\"\n        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n        * `s` is style based scaling tensor of shape `[batch_size, in_features]`\n        \"\"\"\n\n        # Get batch size, height and width\n        b, _, h, w = x.shape\n\n        # Reshape the scales\n        s = s[:, None, :, None, None]\n        # Get [learning rate equalized weights](#equalized_weight)\n        weights = self.weight()[None, :, :, :, :]\n        # $$w`_{i,j,k} = s_i * w_{i,j,k}$$\n        # where $i$ is the input channel, $j$ is the output channel, and $k$ is the kernel index.\n        #\n        # The result has shape `[batch_size, out_features, in_features, kernel_size, kernel_size]`\n        weights = weights * s\n\n        # Demodulate\n        if self.demodulate:\n            # $$\\sigma_j = \\sqrt{\\sum_{i,k} (w'_{i, j, k})^2 + \\epsilon}$$\n            sigma_inv = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n            # $$w''_{i,j,k} = \\frac{w'_{i,j,k}}{\\sqrt{\\sum_{i,k} (w'_{i, j, k})^2 + \\epsilon}}$$\n            weights = weights * sigma_inv\n\n        # Reshape `x`\n        x = x.reshape(1, -1, h, w)\n\n        # Reshape weights\n        _, _, *ws = weights.shape\n        weights = weights.reshape(b * self.out_features, *ws)\n\n        # Use grouped convolution to efficiently calculate the convolution with sample wise kernel.\n        # i.e. we have a different kernel (weights) for each sample in the batch\n        x = F.conv2d(x, weights, padding=self.padding, groups=b)\n\n        # Reshape `x` to `[batch_size, out_features, height, width]` and return\n        return x.reshape(-1, self.out_features, h, w)\n\n\nclass Discriminator(nn.Module):\n    \"\"\"\n    <a id=\"discriminator\"></a>\n\n    ## StyleGAN 2 Discriminator\n\n    ![Discriminator](style_gan2_disc.svg)\n\n    Discriminator first transforms the image to a feature map of the same resolution and then\n    runs it through a series of blocks with residual connections.\n    The resolution is down-sampled by $2 \\times$ at each block while doubling the\n    number of features.\n    \"\"\"\n\n    def __init__(self, log_resolution: int, n_features: int = 64, max_features: int = 512):\n        \"\"\"\n        * `log_resolution` is the $\\log_2$ of image resolution\n        * `n_features` number of features in the convolution layer at the highest resolution (first block)\n        * `max_features` maximum number of features in any generator block\n        \"\"\"\n        super().__init__()\n\n        # Layer to convert RGB image to a feature map with `n_features` number of features.\n        self.from_rgb = nn.Sequential(\n            EqualizedConv2d(3, n_features, 1),\n            nn.LeakyReLU(0.2, True),\n        )\n\n        # Calculate the number of features for each block.\n        #\n        # Something like `[64, 128, 256, 512, 512, 512]`.\n        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 1)]\n        # Number of [discirminator blocks](#discriminator_block)\n        n_blocks = len(features) - 1\n        # Discriminator blocks\n        blocks = [DiscriminatorBlock(features[i], features[i + 1]) for i in range(n_blocks)]\n        self.blocks = nn.Sequential(*blocks)\n\n        # [Mini-batch Standard Deviation](#mini_batch_std_dev)\n        self.std_dev = MiniBatchStdDev()\n        # Number of features after adding the standard deviations map\n        final_features = features[-1] + 1\n        # Final $3 \\times 3$ convolution layer\n        self.conv = EqualizedConv2d(final_features, final_features, 3)\n        # Final linear layer to get the classification\n        self.final = EqualizedLinear(2 * 2 * final_features, 1)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input image of shape `[batch_size, 3, height, width]`\n        \"\"\"\n\n        # Try to normalize the image (this is totally optional, but sped up the early training a little)\n        x = x - 0.5\n        # Convert from RGB\n        x = self.from_rgb(x)\n        # Run through the [discriminator blocks](#discriminator_block)\n        x = self.blocks(x)\n\n        # Calculate and append [mini-batch standard deviation](#mini_batch_std_dev)\n        x = self.std_dev(x)\n        # $3 \\times 3$ convolution\n        x = self.conv(x)\n        # Flatten\n        x = x.reshape(x.shape[0], -1)\n        # Return the classification score\n        return self.final(x)\n\n\nclass DiscriminatorBlock(nn.Module):\n    \"\"\"\n    <a id=\"discriminator_black\"></a>\n\n    ### Discriminator Block\n\n    ![Discriminator block](discriminator_block.svg)\n\n    Discriminator block consists of two $3 \\times 3$ convolutions with a residual connection.\n    \"\"\"\n\n    def __init__(self, in_features, out_features):\n        \"\"\"\n        * `in_features` is the number of features in the input feature map\n        * `out_features` is the number of features in the output feature map\n        \"\"\"\n        super().__init__()\n        # Down-sampling and $1 \\times 1$ convolution layer for the residual connection\n        self.residual = nn.Sequential(DownSample(),\n                                      EqualizedConv2d(in_features, out_features, kernel_size=1))\n\n        # Two $3 \\times 3$ convolutions\n        self.block = nn.Sequential(\n            EqualizedConv2d(in_features, in_features, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2, True),\n            EqualizedConv2d(in_features, out_features, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2, True),\n        )\n\n        # Down-sampling layer\n        self.down_sample = DownSample()\n\n        # Scaling factor $\\frac{1}{\\sqrt 2}$ after adding the residual\n        self.scale = 1 / math.sqrt(2)\n\n    def forward(self, x):\n        # Get the residual connection\n        residual = self.residual(x)\n\n        # Convolutions\n        x = self.block(x)\n        # Down-sample\n        x = self.down_sample(x)\n\n        # Add the residual and scale\n        return (x + residual) * self.scale\n\n\nclass MiniBatchStdDev(nn.Module):\n    \"\"\"\n    <a id=\"mini_batch_std_dev\"></a>\n\n    ### Mini-batch Standard Deviation\n\n    Mini-batch standard deviation calculates the standard deviation\n    across a mini-batch (or a subgroups within the mini-batch)\n    for each feature in the feature map. Then it takes the mean of all\n    the standard deviations and appends it to the feature map as one extra feature.\n    \"\"\"\n\n    def __init__(self, group_size: int = 4):\n        \"\"\"\n        * `group_size` is the number of samples to calculate standard deviation across.\n        \"\"\"\n        super().__init__()\n        self.group_size = group_size\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the feature map\n        \"\"\"\n        # Check if the batch size is divisible by the group size\n        assert x.shape[0] % self.group_size == 0\n        # Split the samples into groups of `group_size`, we flatten the feature map to a single dimension\n        # since we want to calculate the standard deviation for each feature.\n        grouped = x.view(self.group_size, -1)\n        # Calculate the standard deviation for each feature among `group_size` samples\n        #\n        # \\begin{align}\n        # \\mu_{i} &= \\frac{1}{N} \\sum_g x_{g,i} \\\\\n        # \\sigma_{i} &= \\sqrt{\\frac{1}{N} \\sum_g (x_{g,i} - \\mu_i)^2  + \\epsilon}\n        # \\end{align}\n        std = torch.sqrt(grouped.var(dim=0) + 1e-8)\n        # Get the mean standard deviation\n        std = std.mean().view(1, 1, 1, 1)\n        # Expand the standard deviation to append to the feature map\n        b, _, h, w = x.shape\n        std = std.expand(b, -1, h, w)\n        # Append (concatenate) the standard deviations to the feature map\n        return torch.cat([x, std], dim=1)\n\n\nclass DownSample(nn.Module):\n    \"\"\"\n    <a id=\"down_sample\"></a>\n\n    ### Down-sample\n\n    The down-sample operation [smoothens](#smooth) each feature channel and\n     scale $2 \\times$ using bilinear interpolation.\n    This is based on the paper\n     [Making Convolutional Networks Shift-Invariant Again](https://arxiv.org/abs/1904.11486).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Smoothing layer\n        self.smooth = Smooth()\n\n    def forward(self, x: torch.Tensor):\n        # Smoothing or blurring\n        x = self.smooth(x)\n        # Scaled down\n        return F.interpolate(x, (x.shape[2] // 2, x.shape[3] // 2), mode='bilinear', align_corners=False)\n\n\nclass UpSample(nn.Module):\n    \"\"\"\n    <a id=\"up_sample\"></a>\n\n    ### Up-sample\n\n    The up-sample operation scales the image up by $2 \\times$ and [smoothens](#smooth) each feature channel.\n    This is based on the paper\n     [Making Convolutional Networks Shift-Invariant Again](https://arxiv.org/abs/1904.11486).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Up-sampling layer\n        self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        # Smoothing layer\n        self.smooth = Smooth()\n\n    def forward(self, x: torch.Tensor):\n        # Up-sample and smoothen\n        return self.smooth(self.up_sample(x))\n\n\nclass Smooth(nn.Module):\n    \"\"\"\n    <a id=\"smooth\"></a>\n\n    ### Smoothing Layer\n\n    This layer blurs each channel\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Blurring kernel\n        kernel = [[1, 2, 1],\n                  [2, 4, 2],\n                  [1, 2, 1]]\n        # Convert the kernel to a PyTorch tensor\n        kernel = torch.tensor([[kernel]], dtype=torch.float)\n        # Normalize the kernel\n        kernel /= kernel.sum()\n        # Save kernel as a fixed parameter (no gradient updates)\n        self.kernel = nn.Parameter(kernel, requires_grad=False)\n        # Padding layer\n        self.pad = nn.ReplicationPad2d(1)\n\n    def forward(self, x: torch.Tensor):\n        # Get shape of the input feature map\n        b, c, h, w = x.shape\n        # Reshape for smoothening\n        x = x.view(-1, 1, h, w)\n\n        # Add padding\n        x = self.pad(x)\n\n        # Smoothen (blur) with the kernel\n        x = F.conv2d(x, self.kernel)\n\n        # Reshape and return\n        return x.view(b, c, h, w)\n\n\nclass EqualizedLinear(nn.Module):\n    \"\"\"\n    <a id=\"equalized_linear\"></a>\n\n    ## Learning-rate Equalized Linear Layer\n\n    This uses [learning-rate equalized weights](#equalized_weights) for a linear layer.\n    \"\"\"\n\n    def __init__(self, in_features: int, out_features: int, bias: float = 0.):\n        \"\"\"\n        * `in_features` is the number of features in the input feature map\n        * `out_features` is the number of features in the output feature map\n        * `bias` is the bias initialization constant\n        \"\"\"\n\n        super().__init__()\n        # [Learning-rate equalized weights](#equalized_weights)\n        self.weight = EqualizedWeight([out_features, in_features])\n        # Bias\n        self.bias = nn.Parameter(torch.ones(out_features) * bias)\n\n    def forward(self, x: torch.Tensor):\n        # Linear transformation\n        return F.linear(x, self.weight(), bias=self.bias)\n\n\nclass EqualizedConv2d(nn.Module):\n    \"\"\"\n    <a id=\"equalized_conv2d\"></a>\n\n    ## Learning-rate Equalized 2D Convolution Layer\n\n    This uses [learning-rate equalized weights](#equalized_weights) for a convolution layer.\n    \"\"\"\n\n    def __init__(self, in_features: int, out_features: int,\n                 kernel_size: int, padding: int = 0):\n        \"\"\"\n        * `in_features` is the number of features in the input feature map\n        * `out_features` is the number of features in the output feature map\n        * `kernel_size` is the size of the convolution kernel\n        * `padding` is the padding to be added on both sides of each size dimension\n        \"\"\"\n        super().__init__()\n        # Padding size\n        self.padding = padding\n        # [Learning-rate equalized weights](#equalized_weights)\n        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n        # Bias\n        self.bias = nn.Parameter(torch.ones(out_features))\n\n    def forward(self, x: torch.Tensor):\n        # Convolution\n        return F.conv2d(x, self.weight(), bias=self.bias, padding=self.padding)\n\n\nclass EqualizedWeight(nn.Module):\n    \"\"\"\n    <a id=\"equalized_weight\"></a>\n\n    ## Learning-rate Equalized Weights Parameter\n\n    This is based on equalized learning rate introduced in the Progressive GAN paper.\n    Instead of initializing weights at $\\mathcal{N}(0,c)$ they initialize weights\n    to $\\mathcal{N}(0, 1)$ and then multiply them by $c$ when using it.\n    $$w_i = c \\hat{w}_i$$\n\n    The gradients on stored parameters $\\hat{w}$ get multiplied by $c$ but this doesn't have\n    an affect since optimizers such as Adam normalize them by a running mean of the squared gradients.\n\n    The optimizer updates on $\\hat{w}$ are proportionate to the learning rate $\\lambda$.\n    But the effective weights $w$ get updated proportionately to $c \\lambda$.\n    Without equalized learning rate, the effective weights will get updated proportionately to just $\\lambda$.\n\n    So we are effectively scaling the learning rate by $c$ for these weight parameters.\n    \"\"\"\n\n    def __init__(self, shape: List[int]):\n        \"\"\"\n        * `shape` is the shape of the weight parameter\n        \"\"\"\n        super().__init__()\n\n        # He initialization constant\n        self.c = 1 / math.sqrt(np.prod(shape[1:]))\n        # Initialize the weights with $\\mathcal{N}(0, 1)$\n        self.weight = nn.Parameter(torch.randn(shape))\n        # Weight multiplication coefficient\n\n    def forward(self):\n        # Multiply the weights by $c$ and return\n        return self.weight * self.c\n\n\nclass GradientPenalty(nn.Module):\n    \"\"\"\n    <a id=\"gradient_penalty\"></a>\n\n    ## Gradient Penalty\n\n    This is the $R_1$ regularization penality from the paper\n    [Which Training Methods for GANs do actually Converge?](https://arxiv.org/abs/1801.04406).\n\n    $$R_1(\\psi) = \\frac{\\gamma}{2} \\mathbb{E}_{p_\\mathcal{D}(x)}\n    \\Big[\\Vert \\nabla_x D_\\psi(x)^2 \\Vert\\Big]$$\n\n    That is we try to reduce the L2 norm of gradients of the discriminator with\n    respect to images, for real images ($P_\\mathcal{D}$).\n    \"\"\"\n\n    def forward(self, x: torch.Tensor, d: torch.Tensor):\n        \"\"\"\n        * `x` is $x \\sim \\mathcal{D}$\n        * `d` is $D(x)$\n        \"\"\"\n\n        # Get batch size\n        batch_size = x.shape[0]\n\n        # Calculate gradients of $D(x)$ with respect to $x$.\n        # `grad_outputs` is set to $1$ since we want the gradients of $D(x)$,\n        # and we need to create and retain graph since we have to compute gradients\n        # with respect to weight on this loss.\n        gradients, *_ = torch.autograd.grad(outputs=d,\n                                            inputs=x,\n                                            grad_outputs=d.new_ones(d.shape),\n                                            create_graph=True)\n\n        # Reshape gradients to calculate the norm\n        gradients = gradients.reshape(batch_size, -1)\n        # Calculate the norm $\\Vert \\nabla_{x} D(x)^2 \\Vert$\n        norm = gradients.norm(2, dim=-1)\n        # Return the loss $\\Vert \\nabla_x D_\\psi(x)^2 \\Vert$\n        return torch.mean(norm ** 2)\n\n\nclass PathLengthPenalty(nn.Module):\n    \"\"\"\n    <a id=\"path_length_penalty\"></a>\n\n    ## Path Length Penalty\n\n    This regularization encourages a fixed-size step in $w$ to result in a fixed-magnitude\n    change in the image.\n\n    $$\\mathbb{E}_{w \\sim f(z), y \\sim \\mathcal{N}(0, \\mathbf{I})}\n      \\Big(\\Vert \\mathbf{J}^\\top_{w} y \\Vert_2 - a \\Big)^2$$\n\n    where $\\mathbf{J}_w$ is the Jacobian\n    $\\mathbf{J}_w = \\frac{\\partial g}{\\partial w}$,\n    $w$ are sampled from $w \\in \\mathcal{W}$ from the mapping network, and\n    $y$ are images with noise $\\mathcal{N}(0, \\mathbf{I})$.\n\n    $a$ is the exponential moving average of $\\Vert \\mathbf{J}^\\top_{w} y \\Vert_2$\n    as the training progresses.\n\n    $\\mathbf{J}^\\top_{w} y$ is calculated without explicitly calculating the Jacobian using\n    $$\\mathbf{J}^\\top_{w} y = \\nabla_w \\big(g(w) \\cdot y \\big)$$\n    \"\"\"\n\n    def __init__(self, beta: float):\n        \"\"\"\n        * `beta` is the constant $\\beta$ used to calculate the exponential moving average $a$\n        \"\"\"\n        super().__init__()\n\n        # $\\beta$\n        self.beta = beta\n        # Number of steps calculated $N$\n        self.steps = nn.Parameter(torch.tensor(0.), requires_grad=False)\n        # Exponential sum of $\\mathbf{J}^\\top_{w} y$\n        # $$\\sum^N_{i=1} \\beta^{(N - i)}[\\mathbf{J}^\\top_{w} y]_i$$\n        # where $[\\mathbf{J}^\\top_{w} y]_i$ is the value of it at $i$-th step of training\n        self.exp_sum_a = nn.Parameter(torch.tensor(0.), requires_grad=False)\n\n    def forward(self, w: torch.Tensor, x: torch.Tensor):\n        \"\"\"\n        * `w` is the batch of $w$ of shape `[batch_size, d_latent]`\n        * `x` are the generated images of shape `[batch_size, 3, height, width]`\n        \"\"\"\n\n        # Get the device\n        device = x.device\n        # Get number of pixels\n        image_size = x.shape[2] * x.shape[3]\n        # Calculate $y \\in \\mathcal{N}(0, \\mathbf{I})$\n        y = torch.randn(x.shape, device=device)\n        # Calculate $\\big(g(w) \\cdot y \\big)$ and normalize by the square root of image size.\n        # This is scaling is not mentioned in the paper but was present in\n        # [their implementation](https://github.com/NVlabs/stylegan2/blob/master/training/loss.py#L167).\n        output = (x * y).sum() / math.sqrt(image_size)\n\n        # Calculate gradients to get $\\mathbf{J}^\\top_{w} y$\n        gradients, *_ = torch.autograd.grad(outputs=output,\n                                            inputs=w,\n                                            grad_outputs=torch.ones(output.shape, device=device),\n                                            create_graph=True)\n\n        # Calculate L2-norm of $\\mathbf{J}^\\top_{w} y$\n        norm = (gradients ** 2).sum(dim=2).mean(dim=1).sqrt()\n\n        # Regularize after first step\n        if self.steps > 0:\n            # Calculate $a$\n            # $$\\frac{1}{1 - \\beta^N} \\sum^N_{i=1} \\beta^{(N - i)}[\\mathbf{J}^\\top_{w} y]_i$$\n            a = self.exp_sum_a / (1 - self.beta ** self.steps)\n            # Calculate the penalty\n            # $$\\mathbb{E}_{w \\sim f(z), y \\sim \\mathcal{N}(0, \\mathbf{I})}\n            # \\Big(\\Vert \\mathbf{J}^\\top_{w} y \\Vert_2 - a \\Big)^2$$\n            loss = torch.mean((norm - a) ** 2)\n        else:\n            # Return a dummy loss if we can't calculate $a$\n            loss = norm.new_tensor(0)\n\n        # Calculate the mean of $\\Vert \\mathbf{J}^\\top_{w} y \\Vert_2$\n        mean = norm.mean().detach()\n        # Update exponential sum\n        self.exp_sum_a.mul_(self.beta).add_(mean, alpha=1 - self.beta)\n        # Increment $N$\n        self.steps.add_(1.)\n\n        # Return the penalty\n        return loss\n", "labml_nn/gan/dcgan/__init__.py": "\"\"\"\n---\ntitle: Deep Convolutional Generative Adversarial Networks (DCGAN)\nsummary: A simple PyTorch implementation/tutorial of Deep Convolutional Generative Adversarial Networks (DCGAN).\n---\n\n# Deep Convolutional Generative Adversarial Networks (DCGAN)\n\nThis is a [PyTorch](https://pytorch.org) implementation of paper\n[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434).\n\nThis implementation is based on the [PyTorch DCGAN Tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html).\n\"\"\"\n\nimport torch.nn as nn\n\nfrom labml import experiment\nfrom labml.configs import calculate\nfrom labml_helpers.module import Module\nfrom labml_nn.gan.original.experiment import Configs\n\n\nclass Generator(Module):\n    \"\"\"\n    ### Convolutional Generator Network\n\n    This is similar to the de-convolutional network used for CelebA faces,\n    but modified for MNIST images.\n\n    ![DCGan Architecture](https://pytorch.org/tutorials/_images/dcgan_generator.png)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # The input is $1 \\times 1$ with 100 channels\n        self.layers = nn.Sequential(\n            # This gives $3 \\times 3$ output\n            nn.ConvTranspose2d(100, 1024, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(True),\n            # This gives $7 \\times 7$\n            nn.ConvTranspose2d(1024, 512, 3, 2, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            # This gives $14 \\times 14$\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            # This gives $28 \\times 28$\n            nn.ConvTranspose2d(256, 1, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n        self.apply(_weights_init)\n\n    def forward(self, x):\n        # Change from shape `[batch_size, 100]` to `[batch_size, 100, 1, 1]`\n        x = x.unsqueeze(-1).unsqueeze(-1)\n        x = self.layers(x)\n        return x\n\n\nclass Discriminator(Module):\n    \"\"\"\n    ### Convolutional Discriminator Network\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # The input is $28 \\times 28$ with one channel\n        self.layers = nn.Sequential(\n            # This gives $14 \\times 14$\n            nn.Conv2d(1, 256, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # This gives $7 \\times 7$\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            # This gives $3 \\times 3$\n            nn.Conv2d(512, 1024, 3, 2, 0, bias=False),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            # This gives $1 \\times 1$\n            nn.Conv2d(1024, 1, 3, 1, 0, bias=False),\n        )\n        self.apply(_weights_init)\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x.view(x.shape[0], -1)\n\n\ndef _weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n\n# We import the [simple gan experiment](../original/experiment.html) and change the\n# generator and discriminator networks\ncalculate(Configs.generator, 'cnn', lambda c: Generator().to(c.device))\ncalculate(Configs.discriminator, 'cnn', lambda c: Discriminator().to(c.device))\n\n\ndef main():\n    conf = Configs()\n    experiment.create(name='mnist_dcgan')\n    experiment.configs(conf,\n                       {'discriminator': 'cnn',\n                        'generator': 'cnn',\n                        'label_smoothing': 0.01})\n    with experiment.start():\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/resnet/experiment.py": "\"\"\"\n---\ntitle: Train a ResNet on CIFAR 10\nsummary: >\n  Train a ResNet on CIFAR 10\n---\n\n# Train a [ResNet](index.html) on CIFAR 10\n\"\"\"\nfrom typing import List, Optional\n\nfrom torch import nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.experiments.cifar10 import CIFAR10Configs\nfrom labml_nn.resnet import ResNetBase\n\n\nclass Configs(CIFAR10Configs):\n    \"\"\"\n    ## Configurations\n\n    We use [`CIFAR10Configs`](../experiments/cifar10.html) which defines all the\n    dataset related configurations, optimizer, and a training loop.\n    \"\"\"\n\n    # Number fo blocks for each feature map size\n    n_blocks: List[int] = [3, 3, 3]\n    # Number of channels for each feature map size\n    n_channels: List[int] = [16, 32, 64]\n    # Bottleneck sizes\n    bottlenecks: Optional[List[int]] = None\n    # Kernel size of the initial convolution layer\n    first_kernel_size: int = 3\n\n\n@option(Configs.model)\ndef _resnet(c: Configs):\n    \"\"\"\n    ### Create model\n    \"\"\"\n    # [ResNet](index.html)\n    base = ResNetBase(c.n_blocks, c.n_channels, c.bottlenecks, img_channels=3, first_kernel_size=c.first_kernel_size)\n    # Linear layer for classification\n    classification = nn.Linear(c.n_channels[-1], 10)\n\n    # Stack them\n    model = nn.Sequential(base, classification)\n    # Move the model to the device\n    return model.to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='resnet', comment='cifar10')\n    # Create configurations\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf, {\n        'bottlenecks': [8, 16, 16],\n        'n_blocks': [6, 6, 6],\n\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n\n        'epochs': 500,\n        'train_batch_size': 256,\n\n        'train_dataset': 'cifar10_train_augmented',\n        'valid_dataset': 'cifar10_valid_no_augment',\n    })\n    # Set model for saving/loading\n    experiment.add_pytorch_models({'model': conf.model})\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/resnet/__init__.py": "\"\"\"\n---\ntitle: Deep Residual Learning for Image Recognition (ResNet)\nsummary: >\n A PyTorch implementation/tutorial of Deep Residual Learning for Image Recognition (ResNet).\n---\n\n# Deep Residual Learning for Image Recognition (ResNet)\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385).\n\nResNets train layers as residual functions to overcome the\n*degradation problem*.\nThe degradation problem is the accuracy of deep neural networks degrading when\nthe number of layers becomes very high.\nThe accuracy increases as the number of layers increase, then saturates,\nand then starts to degrade.\n\nThe paper argues that deeper models should perform at least as well as shallower\nmodels because the extra layers can just learn to perform an identity mapping.\n\n## Residual Learning\n\nIf $\\mathcal{H}(x)$ is the mapping that needs to be learned by a few layers,\nthey train the residual function\n\n$$\\mathcal{F}(x) = \\mathcal{H}(x) - x$$\n\ninstead. And the original function becomes $\\mathcal{F}(x) + x$.\n\nIn this case, learning identity mapping for $\\mathcal{H}(x)$ is\nequivalent to learning $\\mathcal{F}(x)$ to be $0$, which is easier to\nlearn.\n\nIn the parameterized form this can be written as,\n\n$$\\mathcal{F}(x, \\{W_i\\}) + x$$\n\nand when the feature map sizes of $\\mathcal{F}(x, {W_i})$ and $x$ are different\nthe paper suggests doing a linear projection, with learned weights $W_s$.\n\n$$\\mathcal{F}(x, \\{W_i\\}) + W_s x$$\n\nPaper experimented with zero padding instead of linear projections and found linear projections\nto work better. Also when the feature map sizes match they found identity mapping\nto be better than linear projections.\n\n$\\mathcal{F}$ should have more than one layer, otherwise the sum $\\mathcal{F}(x, \\{W_i\\}) + W_s x$\nalso won't have non-linearities and will be like a linear layer.\n\nHere is [the training code](experiment.html) for training a ResNet on CIFAR-10.\n\"\"\"\n\nfrom typing import List, Optional\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass ShortcutProjection(Module):\n    \"\"\"\n    ## Linear projections for shortcut connection\n\n    This does the $W_s x$ projection described above.\n    \"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, stride: int):\n        \"\"\"\n        * `in_channels` is the number of channels in $x$\n        * `out_channels` is the number of channels in $\\mathcal{F}(x, \\{W_i\\})$\n        * `stride` is the stride length in the convolution operation for $F$.\n        We do the same stride on the shortcut connection, to match the feature-map size.\n        \"\"\"\n        super().__init__()\n\n        # Convolution layer for linear projection $W_s x$\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n        # Paper suggests adding batch normalization after each convolution operation\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x: torch.Tensor):\n        # Convolution and batch normalization\n        return self.bn(self.conv(x))\n\n\nclass ResidualBlock(Module):\n    \"\"\"\n    <a id=\"residual_block\"></a>\n\n    ## Residual Block\n\n    This implements the residual block described in the paper.\n    It has two $3 \\times 3$ convolution layers.\n\n    ![Residual Block](residual_block.svg)\n\n    The first convolution layer maps from `in_channels` to `out_channels`,\n    where the `out_channels` is higher than `in_channels` when we reduce the\n    feature map size with a stride length greater than $1$.\n\n    The second convolution layer maps from `out_channels` to `out_channels` and\n    always has a stride length of 1.\n\n    Both convolution layers are followed by batch normalization.\n    \"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, stride: int):\n        \"\"\"\n        * `in_channels` is the number of channels in $x$\n        * `out_channels` is the number of output channels\n        * `stride` is the stride length in the convolution operation.\n        \"\"\"\n        super().__init__()\n\n        # First $3 \\times 3$ convolution layer, this maps to `out_channels`\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        # Batch normalization after the first convolution\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        # First activation function (ReLU)\n        self.act1 = nn.ReLU()\n\n        # Second $3 \\times 3$ convolution layer\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        # Batch normalization after the second convolution\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # Shortcut connection should be a projection if the stride length is not $1$\n        # or if the number of channels change\n        if stride != 1 or in_channels != out_channels:\n            # Projection $W_s x$\n            self.shortcut = ShortcutProjection(in_channels, out_channels, stride)\n        else:\n            # Identity $x$\n            self.shortcut = nn.Identity()\n\n        # Second activation function (ReLU) (after adding the shortcut)\n        self.act2 = nn.ReLU()\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input of shape `[batch_size, in_channels, height, width]`\n        \"\"\"\n        # Get the shortcut connection\n        shortcut = self.shortcut(x)\n        # First convolution and activation\n        x = self.act1(self.bn1(self.conv1(x)))\n        # Second convolution\n        x = self.bn2(self.conv2(x))\n        # Activation function after adding the shortcut\n        return self.act2(x + shortcut)\n\n\nclass BottleneckResidualBlock(Module):\n    \"\"\"\n    <a id=\"bottleneck_residual_block\"></a>\n\n    ## Bottleneck Residual Block\n\n    This implements the bottleneck block described in the paper.\n    It has $1 \\times 1$, $3 \\times 3$, and $1 \\times 1$ convolution layers.\n\n    ![Bottlenext Block](bottleneck_block.svg)\n\n    The first convolution layer maps from `in_channels` to `bottleneck_channels` with a $1 \\times 1$\n    convolution,\n    where the `bottleneck_channels` is lower than `in_channels`.\n\n    The second $3 \\times 3$ convolution layer maps from `bottleneck_channels` to `bottleneck_channels`.\n    This can have a stride length greater than $1$ when we want to compress the\n    feature map size.\n\n    The third, final $1 \\times 1$ convolution layer maps to `out_channels`.\n    `out_channels` is higher than `in_channels` if the stride length is greater than $1$;\n    otherwise, $out_channels$ is equal to `in_channels`.\n\n    `bottleneck_channels` is less than `in_channels` and the $3 \\times 3$ convolution is performed\n    on this shrunk space (hence the bottleneck). The two $1 \\times 1$ convolution decreases and increases\n    the number of channels.\n    \"\"\"\n\n    def __init__(self, in_channels: int, bottleneck_channels: int, out_channels: int, stride: int):\n        \"\"\"\n        * `in_channels` is the number of channels in $x$\n        * `bottleneck_channels` is the number of channels for the $3 \\times 3$ convlution\n        * `out_channels` is the number of output channels\n        * `stride` is the stride length in the $3 \\times 3$ convolution operation.\n        \"\"\"\n        super().__init__()\n\n        # First $1 \\times 1$ convolution layer, this maps to `bottleneck_channels`\n        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, stride=1)\n        # Batch normalization after the first convolution\n        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n        # First activation function (ReLU)\n        self.act1 = nn.ReLU()\n\n        # Second $3 \\times 3$ convolution layer\n        self.conv2 = nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, stride=stride, padding=1)\n        # Batch normalization after the second convolution\n        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n        # Second activation function (ReLU)\n        self.act2 = nn.ReLU()\n\n        # Third $1 \\times 1$ convolution layer, this maps to `out_channels`.\n        self.conv3 = nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, stride=1)\n        # Batch normalization after the second convolution\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        # Shortcut connection should be a projection if the stride length is not $1$\n        # or if the number of channels change\n        if stride != 1 or in_channels != out_channels:\n            # Projection $W_s x$\n            self.shortcut = ShortcutProjection(in_channels, out_channels, stride)\n        else:\n            # Identity $x$\n            self.shortcut = nn.Identity()\n\n        # Second activation function (ReLU) (after adding the shortcut)\n        self.act3 = nn.ReLU()\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input of shape `[batch_size, in_channels, height, width]`\n        \"\"\"\n        # Get the shortcut connection\n        shortcut = self.shortcut(x)\n        # First convolution and activation\n        x = self.act1(self.bn1(self.conv1(x)))\n        # Second convolution and activation\n        x = self.act2(self.bn2(self.conv2(x)))\n        # Third convolution\n        x = self.bn3(self.conv3(x))\n        # Activation function after adding the shortcut\n        return self.act3(x + shortcut)\n\n\nclass ResNetBase(Module):\n    \"\"\"\n    ## ResNet Model\n\n    This is a the base of the resnet model without\n    the final linear layer and softmax for classification.\n\n    The resnet is made of stacked [residual blocks](#residual_block) or\n    [bottleneck residual blocks](#bottleneck_residual_block).\n    The feature map size is halved after a few blocks with a block of stride length $2$.\n    The number of channels is increased when the feature map size is reduced.\n    Finally the feature map is average pooled to get a vector representation.\n    \"\"\"\n\n    def __init__(self, n_blocks: List[int], n_channels: List[int],\n                 bottlenecks: Optional[List[int]] = None,\n                 img_channels: int = 3, first_kernel_size: int = 7):\n        \"\"\"\n        * `n_blocks` is a list of of number of blocks for each feature map size.\n        * `n_channels` is the number of channels for each feature map size.\n        * `bottlenecks` is the number of channels the bottlenecks.\n        If this is `None`, [residual blocks](#residual_block) are used.\n        * `img_channels` is the number of channels in the input.\n        * `first_kernel_size` is the kernel size of the initial convolution layer\n        \"\"\"\n        super().__init__()\n\n        # Number of blocks and number of channels for each feature map size\n        assert len(n_blocks) == len(n_channels)\n        # If [bottleneck residual blocks](#bottleneck_residual_block) are used,\n        # the number of channels in bottlenecks should be provided for each feature map size\n        assert bottlenecks is None or len(bottlenecks) == len(n_channels)\n\n        # Initial convolution layer maps from `img_channels` to number of channels in the first\n        # residual block (`n_channels[0]`)\n        self.conv = nn.Conv2d(img_channels, n_channels[0],\n                              kernel_size=first_kernel_size, stride=2, padding=first_kernel_size // 2)\n        # Batch norm after initial convolution\n        self.bn = nn.BatchNorm2d(n_channels[0])\n\n        # List of blocks\n        blocks = []\n        # Number of channels from previous layer (or block)\n        prev_channels = n_channels[0]\n        # Loop through each feature map size\n        for i, channels in enumerate(n_channels):\n            # The first block for the new feature map size, will have a stride length of $2$\n            # except fro the very first block\n            stride = 2 if len(blocks) == 0 else 1\n\n            if bottlenecks is None:\n                # [residual blocks](#residual_block) that maps from `prev_channels` to `channels`\n                blocks.append(ResidualBlock(prev_channels, channels, stride=stride))\n            else:\n                # [bottleneck residual blocks](#bottleneck_residual_block)\n                # that maps from `prev_channels` to `channels`\n                blocks.append(BottleneckResidualBlock(prev_channels, bottlenecks[i], channels,\n                                                      stride=stride))\n\n            # Change the number of channels\n            prev_channels = channels\n            # Add rest of the blocks - no change in feature map size or channels\n            for _ in range(n_blocks[i] - 1):\n                if bottlenecks is None:\n                    # [residual blocks](#residual_block)\n                    blocks.append(ResidualBlock(channels, channels, stride=1))\n                else:\n                    # [bottleneck residual blocks](#bottleneck_residual_block)\n                    blocks.append(BottleneckResidualBlock(channels, bottlenecks[i], channels, stride=1))\n\n        # Stack the blocks\n        self.blocks = nn.Sequential(*blocks)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` has shape `[batch_size, img_channels, height, width]`\n        \"\"\"\n\n        # Initial convolution and batch normalization\n        x = self.bn(self.conv(x))\n        # Residual (or bottleneck) blocks\n        x = self.blocks(x)\n        # Change `x` from shape `[batch_size, channels, h, w]` to `[batch_size, channels, h * w]`\n        x = x.view(x.shape[0], x.shape[1], -1)\n        # Global average pooling\n        return x.mean(dim=-1)\n", "labml_nn/adaptive_computation/parity.py": "\"\"\"\n---\ntitle: \"Parity Task\"\nsummary: >\n  This creates data for Parity Task from the paper Adaptive Computation Time\n  for Recurrent Neural Networks\n---\n\n# Parity Task\n\nThis creates data for Parity Task from the paper\n[Adaptive Computation Time for Recurrent Neural Networks](https://arxiv.org/abs/1603.08983).\n\nThe input of the parity task is a vector with $0$'s $1$'s and $-1$'s.\nThe output is the parity of $1$'s - one if there is an odd number of $1$'s and zero otherwise.\nThe input is generated by making a random number of elements in the vector either $1$ or $-1$'s.\n\"\"\"\n\nfrom typing import Tuple\n\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass ParityDataset(Dataset):\n    \"\"\"\n    ### Parity dataset\n    \"\"\"\n\n    def __init__(self, n_samples: int, n_elems: int = 64):\n        \"\"\"\n        * `n_samples` is the number of samples\n        * `n_elems` is the number of elements in the input vector\n        \"\"\"\n        self.n_samples = n_samples\n        self.n_elems = n_elems\n\n    def __len__(self):\n        \"\"\"\n        Size of the dataset\n        \"\"\"\n        return self.n_samples\n\n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Generate a sample\n        \"\"\"\n\n        # Empty vector\n        x = torch.zeros((self.n_elems,))\n        # Number of non-zero elements - a random number between $1$ and total number of elements\n        n_non_zero = torch.randint(1, self.n_elems + 1, (1,)).item()\n        # Fill non-zero elements with $1$'s and $-1$'s\n        x[:n_non_zero] = torch.randint(0, 2, (n_non_zero,)) * 2 - 1\n        # Randomly permute the elements\n        x = x[torch.randperm(self.n_elems)]\n\n        # The parity\n        y = (x == 1.).sum() % 2\n\n        #\n        return x, y\n", "labml_nn/adaptive_computation/__init__.py": "\"\"\"\n---\ntitle: Neural Networks with Adaptive Computation\nsummary: >\n A set of PyTorch implementations/tutorials related to adaptive computation\n---\n\n# Neural Networks with Adaptive Computation\n\nThese are neural network architectures that change the computation complexity based on the\ncomplexity of the input sample.\n\n* \ud83d\udea7 TODO: Adaptive Computation Time for Recurrent Neural Networks\n* [PonderNet: Learning to Ponder](ponder_net/index.html)\n\"\"\"\n", "labml_nn/adaptive_computation/ponder_net/experiment.py": "\"\"\"\n---\ntitle: \"PonderNet Parity Task Experiment\"\nsummary: >\n  This trains is a PonderNet on Parity Task\n---\n\n# [PonderNet](index.html) [Parity Task](../parity.html) Experiment\n\nThis trains a [PonderNet](index.html) on [Parity Task](../parity.html).\n\"\"\"\n\nfrom typing import Any\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom labml import tracker, experiment\nfrom labml_helpers.metrics.accuracy import AccuracyDirect\nfrom labml_helpers.train_valid import SimpleTrainValidConfigs, BatchIndex\nfrom labml_nn.adaptive_computation.parity import ParityDataset\nfrom labml_nn.adaptive_computation.ponder_net import ParityPonderGRU, ReconstructionLoss, RegularizationLoss\n\n\nclass Configs(SimpleTrainValidConfigs):\n    \"\"\"\n    Configurations with a\n     [simple training loop](https://docs.labml.ai/api/helpers.html#labml_helpers.train_valid.SimpleTrainValidConfigs)\n    \"\"\"\n\n    # Number of epochs\n    epochs: int = 100\n    # Number of batches per epoch\n    n_batches: int = 500\n    # Batch size\n    batch_size: int = 128\n\n    # Model\n    model: ParityPonderGRU\n\n    # $L_{Rec}$\n    loss_rec: ReconstructionLoss\n    # $L_{Reg}$\n    loss_reg: RegularizationLoss\n\n    # The number of elements in the input vector.\n    # *We keep it low for demonstration; otherwise, training takes a lot of time.\n    # Although the parity task seems simple, figuring out the pattern by looking at samples\n    # is quite hard.*\n    n_elems: int = 8\n    # Number of units in the hidden layer (state)\n    n_hidden: int = 64\n    # Maximum number of steps $N$\n    max_steps: int = 20\n\n    # $\\lambda_p$ for the geometric distribution $p_G(\\lambda_p)$\n    lambda_p: float = 0.2\n    # Regularization loss $L_{Reg}$ coefficient $\\beta$\n    beta: float = 0.01\n\n    # Gradient clipping by norm\n    grad_norm_clip: float = 1.0\n\n    # Training and validation loaders\n    train_loader: DataLoader\n    valid_loader: DataLoader\n\n    # Accuracy calculator\n    accuracy = AccuracyDirect()\n\n    def init(self):\n        # Print indicators to screen\n        tracker.set_scalar('loss.*', True)\n        tracker.set_scalar('loss_reg.*', True)\n        tracker.set_scalar('accuracy.*', True)\n        tracker.set_scalar('steps.*', True)\n\n        # We need to set the metrics to calculate them for the epoch for training and validation\n        self.state_modules = [self.accuracy]\n\n        # Initialize the model\n        self.model = ParityPonderGRU(self.n_elems, self.n_hidden, self.max_steps).to(self.device)\n        # $L_{Rec}$\n        self.loss_rec = ReconstructionLoss(nn.BCEWithLogitsLoss(reduction='none')).to(self.device)\n        # $L_{Reg}$\n        self.loss_reg = RegularizationLoss(self.lambda_p, self.max_steps).to(self.device)\n\n        # Training and validation loaders\n        self.train_loader = DataLoader(ParityDataset(self.batch_size * self.n_batches, self.n_elems),\n                                       batch_size=self.batch_size)\n        self.valid_loader = DataLoader(ParityDataset(self.batch_size * 32, self.n_elems),\n                                       batch_size=self.batch_size)\n\n    def step(self, batch: Any, batch_idx: BatchIndex):\n        \"\"\"\n        This method gets called by the trainer for each batch\n        \"\"\"\n        # Set the model mode\n        self.model.train(self.mode.is_train)\n\n        # Get the input and labels and move them to the model's device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Increment step in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Run the model\n        p, y_hat, p_sampled, y_hat_sampled = self.model(data)\n\n        # Calculate the reconstruction loss\n        loss_rec = self.loss_rec(p, y_hat, target.to(torch.float))\n        tracker.add(\"loss.\", loss_rec)\n\n        # Calculate the regularization loss\n        loss_reg = self.loss_reg(p)\n        tracker.add(\"loss_reg.\", loss_reg)\n\n        # $L = L_{Rec} + \\beta L_{Reg}$\n        loss = loss_rec + self.beta * loss_reg\n\n        # Calculate the expected number of steps taken\n        steps = torch.arange(1, p.shape[0] + 1, device=p.device)\n        expected_steps = (p * steps[:, None]).sum(dim=0)\n        tracker.add(\"steps.\", expected_steps)\n\n        # Call accuracy metric\n        self.accuracy(y_hat_sampled > 0, target)\n\n        if self.mode.is_train:\n            # Compute gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Optimizer\n            self.optimizer.step()\n            # Clear gradients\n            self.optimizer.zero_grad()\n            #\n            tracker.save()\n\n\ndef main():\n    \"\"\"\n    Run the experiment\n    \"\"\"\n    experiment.create(name='ponder_net')\n\n    conf = Configs()\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 0.0003,\n    })\n\n    with experiment.start():\n        conf.run()\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/adaptive_computation/ponder_net/__init__.py": "\"\"\"\n---\ntitle: \"PonderNet: Learning to Ponder\"\nsummary: >\n A PyTorch implementation/tutorial of PonderNet: Learning to Ponder.\n---\n\n# PonderNet: Learning to Ponder\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[PonderNet: Learning to Ponder](https://arxiv.org/abs/2107.05407).\n\nPonderNet adapts the computation based on the input.\nIt changes the number of steps to take on a recurrent network based on the input.\nPonderNet learns this with end-to-end gradient descent.\n\nPonderNet has a step function of the form\n\n$$\\hat{y}_n, h_{n+1}, \\lambda_n = s(x, h_n)$$\n\nwhere $x$ is the input, $h_n$ is the state, $\\hat{y}_n$ is the prediction at step $n$,\nand $\\lambda_n$ is the probability of halting (stopping) at current step.\n\n$s$ can be any neural network (e.g. LSTM, MLP, GRU, Attention layer).\n\nThe unconditioned probability of halting at step $n$ is then,\n\n$$p_n = \\lambda_n \\prod_{j=1}^{n-1} (1 - \\lambda_j)$$\n\nThat is the probability of not being halted at any of the previous steps and halting at step $n$.\n\nDuring inference, we halt by sampling based on the halting probability $\\lambda_n$\n and get the prediction at the halting layer $\\hat{y}_n$ as the final output.\n\nDuring training, we get the predictions from all the layers and calculate the losses for each of them.\nAnd then take the weighted average of the losses based on the probabilities of getting halted at each layer\n$p_n$.\n\nThe step function is applied to a maximum number of steps donated by $N$.\n\nThe overall loss of PonderNet is\n\n\\begin{align}\nL &= L_{Rec} + \\beta L_{Reg} \\\\\nL_{Rec} &= \\sum_{n=1}^N p_n \\mathcal{L}(y, \\hat{y}_n) \\\\\nL_{Reg} &= \\mathop{KL} \\Big(p_n \\Vert p_G(\\lambda_p) \\Big)\n\\end{align}\n\n$\\mathcal{L}$ is the normal loss function between target $y$ and prediction $\\hat{y}_n$.\n\n$\\mathop{KL}$ is the [Kullback\u2013Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n\n$p_G$ is the [Geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) parameterized by\n$\\lambda_p$. *$\\lambda_p$ has nothing to do with $\\lambda_n$; we are just sticking to same notation as the paper*.\n$$Pr_{p_G(\\lambda_p)}(X = k) = (1 - \\lambda_p)^k \\lambda_p$$.\n\nThe regularization loss biases the network towards taking $\\frac{1}{\\lambda_p}$ steps and incentivizes\n non-zero probabilities for all steps; i.e. promotes exploration.\n\nHere is the [training code `experiment.py`](experiment.html) to train a PonderNet on [Parity Task](../parity.html).\n\"\"\"\n\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass ParityPonderGRU(Module):\n    \"\"\"\n    ## PonderNet with GRU for Parity Task\n\n    This is a simple model that uses a [GRU Cell](https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html)\n    as the step function.\n\n    This model is for the [Parity Task](../parity.html) where the input is a vector of `n_elems`.\n    Each element of the vector is either `0`, `1` or `-1` and the output is the parity\n    - a binary value that is true if the number of `1`s is odd and false otherwise.\n\n    The prediction of the model is the log probability of the parity being $1$.\n    \"\"\"\n\n    def __init__(self, n_elems: int, n_hidden: int, max_steps: int):\n        \"\"\"\n        * `n_elems` is the number of elements in the input vector\n        * `n_hidden` is the state vector size of the GRU\n        * `max_steps` is the maximum number of steps $N$\n        \"\"\"\n        super().__init__()\n\n        self.max_steps = max_steps\n        self.n_hidden = n_hidden\n\n        # GRU\n        # $$h_{n+1} = s_h(x, h_n)$$\n        self.gru = nn.GRUCell(n_elems, n_hidden)\n        # $$\\hat{y}_n = s_y(h_n)$$\n        # We could use a layer that takes the concatenation of $h$ and $x$ as input\n        # but we went with this for simplicity.\n        self.output_layer = nn.Linear(n_hidden, 1)\n        # $$\\lambda_n = s_\\lambda(h_n)$$\n        self.lambda_layer = nn.Linear(n_hidden, 1)\n        self.lambda_prob = nn.Sigmoid()\n        # An option to set during inference so that computation is actually halted at inference time\n        self.is_halt = False\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        * `x` is the input of shape `[batch_size, n_elems]`\n\n        This outputs a tuple of four tensors:\n\n        1. $p_1 \\dots p_N$ in a tensor of shape `[N, batch_size]`\n        2. $\\hat{y}_1 \\dots \\hat{y}_N$ in a tensor of shape `[N, batch_size]` - the log probabilities of the parity being $1$\n        3. $p_m$ of shape `[batch_size]`\n        4. $\\hat{y}_m$ of shape `[batch_size]` where the computation was halted at step $m$\n        \"\"\"\n\n        #\n        batch_size = x.shape[0]\n\n        # We get initial state $h_1 = s_h(x)$\n        h = x.new_zeros((x.shape[0], self.n_hidden))\n        h = self.gru(x, h)\n\n        # Lists to store $p_1 \\dots p_N$ and $\\hat{y}_1 \\dots \\hat{y}_N$\n        p = []\n        y = []\n        # $\\prod_{j=1}^{n-1} (1 - \\lambda_j)$\n        un_halted_prob = h.new_ones((batch_size,))\n\n        # A vector to maintain which samples has halted computation\n        halted = h.new_zeros((batch_size,))\n        # $p_m$ and $\\hat{y}_m$ where the computation was halted at step $m$\n        p_m = h.new_zeros((batch_size,))\n        y_m = h.new_zeros((batch_size,))\n\n        # Iterate for $N$ steps\n        for n in range(1, self.max_steps + 1):\n            # The halting probability $\\lambda_N = 1$ for the last step\n            if n == self.max_steps:\n                lambda_n = h.new_ones(h.shape[0])\n            # $\\lambda_n = s_\\lambda(h_n)$\n            else:\n                lambda_n = self.lambda_prob(self.lambda_layer(h))[:, 0]\n            # $\\hat{y}_n = s_y(h_n)$\n            y_n = self.output_layer(h)[:, 0]\n\n            # $$p_n = \\lambda_n \\prod_{j=1}^{n-1} (1 - \\lambda_j)$$\n            p_n = un_halted_prob * lambda_n\n            # Update $\\prod_{j=1}^{n-1} (1 - \\lambda_j)$\n            un_halted_prob = un_halted_prob * (1 - lambda_n)\n\n            # Halt based on halting probability $\\lambda_n$\n            halt = torch.bernoulli(lambda_n) * (1 - halted)\n\n            # Collect $p_n$ and $\\hat{y}_n$\n            p.append(p_n)\n            y.append(y_n)\n\n            # Update $p_m$ and $\\hat{y}_m$ based on what was halted at current step $n$\n            p_m = p_m * (1 - halt) + p_n * halt\n            y_m = y_m * (1 - halt) + y_n * halt\n\n            # Update halted samples\n            halted = halted + halt\n            # Get next state $h_{n+1} = s_h(x, h_n)$\n            h = self.gru(x, h)\n\n            # Stop the computation if all samples have halted\n            if self.is_halt and halted.sum() == batch_size:\n                break\n\n        #\n        return torch.stack(p), torch.stack(y), p_m, y_m\n\n\nclass ReconstructionLoss(Module):\n    \"\"\"\n    ## Reconstruction loss\n\n    $$L_{Rec} = \\sum_{n=1}^N p_n \\mathcal{L}(y, \\hat{y}_n)$$\n\n    $\\mathcal{L}$ is the normal loss function between target $y$ and prediction $\\hat{y}_n$.\n    \"\"\"\n\n    def __init__(self, loss_func: nn.Module):\n        \"\"\"\n        * `loss_func` is the loss function $\\mathcal{L}$\n        \"\"\"\n        super().__init__()\n        self.loss_func = loss_func\n\n    def forward(self, p: torch.Tensor, y_hat: torch.Tensor, y: torch.Tensor):\n        \"\"\"\n        * `p` is $p_1 \\dots p_N$ in a tensor of shape `[N, batch_size]`\n        * `y_hat` is $\\hat{y}_1 \\dots \\hat{y}_N$ in a tensor of shape `[N, batch_size, ...]`\n        * `y` is the target of shape `[batch_size, ...]`\n        \"\"\"\n\n        # The total $\\sum_{n=1}^N p_n \\mathcal{L}(y, \\hat{y}_n)$\n        total_loss = p.new_tensor(0.)\n        # Iterate upto $N$\n        for n in range(p.shape[0]):\n            # $p_n \\mathcal{L}(y, \\hat{y}_n)$ for each sample and the mean of them\n            loss = (p[n] * self.loss_func(y_hat[n], y)).mean()\n            # Add to total loss\n            total_loss = total_loss + loss\n\n        #\n        return total_loss\n\n\nclass RegularizationLoss(Module):\n    \"\"\"\n    ## Regularization loss\n\n    $$L_{Reg} = \\mathop{KL} \\Big(p_n \\Vert p_G(\\lambda_p) \\Big)$$\n\n    $\\mathop{KL}$ is the [Kullback\u2013Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n\n    $p_G$ is the [Geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) parameterized by\n    $\\lambda_p$. *$\\lambda_p$ has nothing to do with $\\lambda_n$; we are just sticking to same notation as the paper*.\n    $$Pr_{p_G(\\lambda_p)}(X = k) = (1 - \\lambda_p)^k \\lambda_p$$.\n\n    The regularization loss biases the network towards taking $\\frac{1}{\\lambda_p}$ steps and incentivies non-zero probabilities\n    for all steps; i.e. promotes exploration.\n    \"\"\"\n\n    def __init__(self, lambda_p: float, max_steps: int = 1_000):\n        \"\"\"\n        * `lambda_p` is $\\lambda_p$ - the success probability of geometric distribution\n        * `max_steps` is the highest $N$; we use this to pre-compute $p_G(\\lambda_p)$\n        \"\"\"\n        super().__init__()\n\n        # Empty vector to calculate $p_G(\\lambda_p)$\n        p_g = torch.zeros((max_steps,))\n        # $(1 - \\lambda_p)^k$\n        not_halted = 1.\n        # Iterate upto `max_steps`\n        for k in range(max_steps):\n            # $$Pr_{p_G(\\lambda_p)}(X = k) = (1 - \\lambda_p)^k \\lambda_p$$\n            p_g[k] = not_halted * lambda_p\n            # Update $(1 - \\lambda_p)^k$\n            not_halted = not_halted * (1 - lambda_p)\n\n        # Save $Pr_{p_G(\\lambda_p)}$\n        self.p_g = nn.Parameter(p_g, requires_grad=False)\n\n        # KL-divergence loss\n        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, p: torch.Tensor):\n        \"\"\"\n        * `p` is $p_1 \\dots p_N$ in a tensor of shape `[N, batch_size]`\n        \"\"\"\n        # Transpose `p` to `[batch_size, N]`\n        p = p.transpose(0, 1)\n        # Get $Pr_{p_G(\\lambda_p)}$ upto $N$ and expand it across the batch dimension\n        p_g = self.p_g[None, :p.shape[1]].expand_as(p)\n\n        # Calculate the KL-divergence.\n        # *The [PyTorch KL-divergence](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html)\n        # implementation accepts log probabilities.*\n        return self.kl_div(p.log(), p_g)\n", "labml_nn/cfr/analytics.py": "from typing import List\n\nimport altair as alt\nimport numpy as np\n\nfrom labml import analytics\nfrom labml.analytics import IndicatorCollection\n\n\ndef calculate_percentages(means: List[np.ndarray], names: List[List[str]]):\n    normalized = []\n\n    for i in range(len(means)):\n        total = np.zeros_like(means[i])\n        for j, n in enumerate(names):\n            if n[-1][:-1] == names[i][-1][:-1]:\n                total += means[j]\n        normalized.append(means[i] / (total + np.finfo(float).eps))\n\n    return normalized\n\n\ndef plot_infosets(indicators: IndicatorCollection, *,\n                  is_normalize: bool = True,\n                  width: int = 600,\n                  height: int = 300):\n    data, names = analytics.indicator_data(indicators)\n    step = [d[:, 0] for d in data]\n    means = [d[:, 5] for d in data]\n\n    if is_normalize:\n        normalized = calculate_percentages(means, names)\n    else:\n        normalized = means\n\n    common = names[0][-1]\n    for i, n in enumerate(names):\n        n = n[-1]\n        if len(n) < len(common):\n            common = common[:len(n)]\n        for j in range(len(common)):\n            if common[j] != n[j]:\n                common = common[:j]\n                break\n\n    table = []\n    for i, n in enumerate(names):\n        for j, v in zip(step[i], normalized[i]):\n            table.append({\n                'series': n[-1][len(common):],\n                'step': j,\n                'value': v\n            })\n\n    table = alt.Data(values=table)\n\n    selection = alt.selection_multi(fields=['series'], bind='legend')\n\n    return alt.Chart(table).mark_line().encode(\n        alt.X('step:Q'),\n        alt.Y('value:Q'),\n        alt.Color('series:N', scale=alt.Scale(scheme='tableau20')),\n        opacity=alt.condition(selection, alt.value(1), alt.value(0.0001))\n    ).add_selection(\n        selection\n    ).properties(width=width, height=height)\n", "labml_nn/cfr/infoset_saver.py": "import json\nimport pathlib\nfrom typing import Dict\n\nfrom labml import experiment\nfrom labml_nn.cfr import InfoSet\n\n\nclass InfoSetSaver(experiment.ModelSaver):\n    def __init__(self, infosets: Dict[str, InfoSet]):\n        self.infosets = infosets\n\n    def save(self, checkpoint_path: pathlib.Path) -> any:\n        data = {key: infoset.to_dict() for key, infoset in self.infosets.items()}\n        file_name = f\"infosets.json\"\n\n        with open(str(checkpoint_path / file_name), 'w') as f:\n            f.write(json.dumps(data))\n\n        return file_name\n\n    def load(self, checkpoint_path: pathlib.Path, file_name: str):\n        with open(str(checkpoint_path / file_name), 'w') as f:\n            data = json.loads(f.read())\n\n        for key, d in data.items():\n            self.infosets[key] = InfoSet.from_dict(d)\n", "labml_nn/cfr/__init__.py": "\"\"\"\n---\ntitle: Regret Minimization in Games with Incomplete Information (CFR)\nsummary: >\n  This is an annotated implementation/tutorial of Regret Minimization in Games with Incomplete Information\n---\n\n# Regret Minimization in Games with Incomplete Information (CFR)\n\nThe paper\n[Regret Minimization in Games with Incomplete Information](http://martin.zinkevich.org/publications/regretpoker.pdf)\nintroduces counterfactual regret and how minimizing counterfactual regret through self-play\ncan be used to reach Nash equilibrium.\nThe algorithm is called Counterfactual Regret Minimization (**CFR**).\n\nThe paper\n[Monte Carlo Sampling for Regret Minimization in Extensive Games](http://mlanctot.info/files/papers/nips09mccfr.pdf)\nintroduces Monte Carlo Counterfactual Regret Minimization (**MCCFR**),\nwhere we sample from the game tree and estimate the regrets.\n\nWe tried to keep our Python implementation easy-to-understand like a tutorial.\nWe run it on [a very simple imperfect information game called Kuhn poker](kuhn/index.html).\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/cfr/kuhn/experiment.ipynb)\n\n[![Twitter thread](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Ftwitter.com%2Flabmlai%2Fstatus%2F1407186002255380484)](https://twitter.com/labmlai/status/1407186002255380484)\nTwitter thread\n\n## Introduction\n\nWe implement Monte Carlo Counterfactual Regret Minimization (MCCFR) with chance sampling (CS).\nIt iteratively, explores part of the game tree by trying all player actions,\nbut sampling chance events.\nChance events are things like dealing cards; they are kept sampled once per iteration.\nThen it calculates, for each action, the *regret* of following the current strategy instead of taking that action.\nThen it updates the strategy based on these regrets for the next iteration, using regret matching.\nFinally, it computes the average of the strategies throughout the iterations,\nwhich is very close to the Nash equilibrium if we ran enough iterations.\n\nWe will first introduce the mathematical notation and theory.\n\n### Player\n\nA player is denoted by $i \\in N$, where $N$ is the set of players.\n\n### [History](#History)\n\nHistory $h \\in H$ is a sequence of actions including chance events,\n and $H$ is the set of all histories.\n\n$Z \\subseteq H$ is the set of terminal histories (game over).\n\n### Action\n\nAction $a$, $A(h) = {a: (h, a) \\in H}$ where $h \\in H$ is a non-terminal [history](#History).\n\n### [Information Set $I_i$](#InfoSet)\n\n**Information set** $I_i \\in \\mathcal{I}_i$ for player $i$\nis similar to a history $h \\in H$\nbut only contains the actions visible to player $i$.\nThat is, the history $h$ will contain actions/events such as cards dealt to the\nopposing player while $I_i$ will not have them.\n\n$\\mathcal{I}_i$ is known as the **information partition** of player $i$.\n\n$h \\in I$ is the set of all histories that belong to a given information set;\ni.e. all those histories look the same in the eye of the player.\n\n<a id=\"Strategy\"></a>\n\n### Strategy\n\n**Strategy of player** $i$, $\\sigma_i \\in \\Sigma_i$ is a distribution over actions $A(I_i)$,\nwhere $\\Sigma_i$ is the set of all strategies for player $i$.\nStrategy on $t$-th iteration is denoted by $\\sigma^t_i$.\n\nStrategy is defined as a probability for taking an action $a$ in for a given information set $I$,\n\n$$\\sigma_i(I)(a)$$\n\n$\\sigma$ is the **strategy profile** which consists of strategies of all players\n $\\sigma_1, \\sigma_2, \\ldots$\n\n$\\sigma_{-i}$ is strategies of all players except $\\sigma_i$\n\n<a id=\"HistoryProbability\"></a>\n\n### Probability of History\n\n$\\pi^\\sigma(h)$ is the probability of reaching the history $h$ with strategy profile $\\sigma$.\n$\\pi^\\sigma(h)_{-i}$ is the probability of reaching $h$ without player $i$'s contribution;\n i.e. player $i$ took the actions to follow $h$ with a probability of $1$.\n\n$\\pi^\\sigma(h)_{i}$ is the probability of reaching $h$ with only player $i$'s contribution.\nThat is,\n$$\\pi^\\sigma(h) = \\pi^\\sigma(h)_{i} \\pi^\\sigma(h)_{-i}$$\n\nProbability of reaching a information set $I$ is,\n$$\\pi^\\sigma(I) = \\sum_{h \\in I} \\pi^\\sigma(h)$$\n\n### Utility (Pay off)\n\nThe [terminal utility](#terminal_utility) is the utility (or pay off)\n of a player $i$ for a terminal history $h$.\n\n$$u_i(h)$$ where $h \\in Z$\n\n$u_i(\\sigma)$ is the expected utility (payoff) for player $i$ with strategy profile $\\sigma$.\n\n$$u_i(\\sigma) = \\sum_{h \\in Z} u_i(h) \\pi^\\sigma(h)$$\n\n<a id=\"NashEquilibrium\"></a>\n\n### Nash Equilibrium\n\nNash equilibrium is a state where none of the players can increase their expected utility (or payoff)\nby changing their strategy alone.\n\nFor two players, Nash equilibrium is a [strategy profile](#Strategy) where\n\n\\begin{align}\nu_1(\\sigma) &\\ge \\max_{\\sigma'_1 \\in \\Sigma_1} u_1(\\sigma'_1, \\sigma_2) \\\\\nu_2(\\sigma) &\\ge \\max_{\\sigma'_2 \\in \\Sigma_2} u_1(\\sigma_1, \\sigma'_2) \\\\\n\\end{align}\n\n$\\epsilon$-Nash equilibrium is,\n\n\\begin{align}\nu_1(\\sigma) + \\epsilon &\\ge \\max_{\\sigma'_1 \\in \\Sigma_1} u_1(\\sigma'_1, \\sigma_2) \\\\\nu_2(\\sigma)  + \\epsilon &\\ge \\max_{\\sigma'_2 \\in \\Sigma_2} u_1(\\sigma_1, \\sigma'_2) \\\\\n\\end{align}\n\n### Regret Minimization\n\nRegret is the utility (or pay off) that the player didn't get because\n she didn't follow the optimal strategy or took the best action.\n\nAverage overall regret for Player $i$ is the average regret of not following the\noptimal strategy in all $T$ rounds of iterations.\n\n$$R^T_i = \\frac{1}{T} \\max_{\\sigma^*_i \\in \\Sigma_i} \\sum_{t=1}^T\n\\Big( u_i(\\sigma^*_i, \\sigma^t_{-i}) - u_i(\\sigma^t) \\Big)$$\n\nwhere $\\sigma^t$ is the strategy profile of all players in iteration $t$,\nand\n\n$$(\\sigma^*_i, \\sigma^t_{-i})$$\n\nis the strategy profile $\\sigma^t$ with player $i$'s strategy\nreplaced with $\\sigma^*_i$.\n\nThe average strategy is the average of strategies followed in each round,\n for all $I \\in \\mathcal{I}, a \\in A(I)$\n\n$$\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)} =\n \\frac{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}}{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)}$$\n\nThat is the mean regret of not playing with the optimal strategy.\n\nIf $R^T_i < \\epsilon$ for all players then $\\bar{\\sigma}^T_i(I)(a)$ is a\n$2\\epsilon$-Nash equilibrium.\n\n\\begin{align}\nR^T_i &< \\epsilon \\\\\nR^T_i &= \\frac{1}{T} \\max_{\\sigma^*_i \\in \\Sigma_i} \\sum_{t=1}^T\n\\Big( u_i(\\sigma^*_i, \\sigma^t_{-i}) - u_i(\\sigma^t) \\Big) \\\\\n&= \\frac{1}{T} \\max_{\\sigma^*_i \\in \\Sigma_i} \\sum_{t=1}^T u_i(\\sigma^*_i, \\sigma^t_{-i})\n- \\frac{1}{T} \\sum_{t=1}^T u_i(\\sigma^t) < \\epsilon\n\\end{align}\n\nSince $u_1 = -u_2$ because it's a zero-sum game, we can add $R^T_1$ and $R^T_i$ and the\nsecond term will cancel out.\n\n\\begin{align}\n2\\epsilon &>\n\\frac{1}{T} \\max_{\\sigma^*_1 \\in \\Sigma_1} \\sum_{t=1}^T u_1(\\sigma^*_1, \\sigma^t_{-1}) +\n\\frac{1}{T} \\max_{\\sigma^*_2 \\in \\Sigma_2} \\sum_{t=1}^T u_2(\\sigma^*_2, \\sigma^t_{-2})\n\\end{align}\n\nThe average of utilities over a set of strategies is equal to the utility of the average strategy.\n\n$$\\frac{1}{T} \\sum_{t=1}^T u_i(\\sigma^t) = u_i(\\bar{\\sigma}^T)$$\n\nTherefore,\n\n\\begin{align}\n2\\epsilon &>\n\\max_{\\sigma^*_1 \\in \\Sigma_1} u_1(\\sigma^*_1, \\bar{\\sigma}^T_{-1}) +\n\\max_{\\sigma^*_2 \\in \\Sigma_2} u_2(\\sigma^*_2, \\bar{\\sigma}^T_{-2})\n\\end{align}\n\nFrom the definition of $\\max$,\n$$\\max_{\\sigma^*_2 \\in \\Sigma_2} u_2(\\sigma^*_2, \\bar{\\sigma}^T_{-2}) \\ge u_2(\\bar{\\sigma}^T)\n = -u_1(\\bar{\\sigma}^T)$$\n\nThen,\n\n\\begin{align}\n2\\epsilon &>\n\\max_{\\sigma^*_1 \\in \\Sigma_1} u_1(\\sigma^*_1, \\bar{\\sigma}^T_{-1}) +\n-u_1(\\bar{\\sigma}^T) \\\\\nu_1(\\bar{\\sigma}^T) + 2\\epsilon &> \\max_{\\sigma^*_1 \\in \\Sigma_1} u_1(\\sigma^*_1, \\bar{\\sigma}^T_{-1})\n\\end{align}\n\nThis is $2\\epsilon$-Nash equilibrium.\nYou can similarly prove for games with more than 2 players.\n\nSo we need to minimize $R^T_i$ to get close to a Nash equilibrium.\n\n<a id=\"CounterfactualRegret\"></a>\n\n### Counterfactual regret\n\n**Counterfactual value** $\\textcolor{pink}{v_i(\\sigma, I)}$ is the expected utility for player $i$ if\n if player $i$ tried to reach $I$ (took the actions leading to $I$ with a probability of $1$).\n\n$$\\textcolor{pink}{v_i(\\sigma, I)} = \\sum_{z \\in Z_I} \\pi^\\sigma_{-i}(z[I]) \\pi^\\sigma(z[I], z) u_i(z)$$\n\nwhere $Z_I$ is the set of terminal histories reachable from $I$,\nand $z[I]$ is the prefix of $z$ up to $I$.\n$\\pi^\\sigma(z[I], z)$ is the probability of reaching z from $z[I]$.\n\n**Immediate counterfactual regret** is,\n\n$$R^T_{i,imm}(I) = \\max_{a \\in A{I}} R^T_{i,imm}(I, a)$$\n\nwhere\n\n$$R^T_{i,imm}(I) = \\frac{1}{T} \\sum_{t=1}^T\n\\Big(\n\\textcolor{pink}{v_i(\\sigma^t |_{I \\rightarrow a}, I)} - \\textcolor{pink}{v_i(\\sigma^t, I)}\n\\Big)$$\n\nwhere $\\sigma |_{I \\rightarrow a}$ is the strategy profile $\\sigma$ with the modification\nof always taking action $a$ at information set $I$.\n\nThe [paper](http://martin.zinkevich.org/publications/regretpoker.pdf) proves that (Theorem 3),\n\n$$R^T_i \\le \\sum_{I \\in \\mathcal{I}} R^{T,+}_{i,imm}(I)$$\nwhere $$R^{T,+}_{i,imm}(I) = \\max(R^T_{i,imm}(I), 0)$$\n\n<a id=\"RegretMatching\"></a>\n\n### Regret Matching\n\nThe strategy is calculated using regret matching.\n\nThe regret for each information set and action pair $\\textcolor{orange}{R^T_i(I, a)}$ is maintained,\n\n\\begin{align}\n\\textcolor{coral}{r^t_i(I, a)} &=\n \\textcolor{pink}{v_i(\\sigma^t |_{I \\rightarrow a}, I)} - \\textcolor{pink}{v_i(\\sigma^t, I)}\n \\\\\n\\textcolor{orange}{R^T_i(I, a)} &=\n \\frac{1}{T} \\sum_{t=1}^T \\textcolor{coral}{r^t_i(I, a)}\n\\end{align}\n\nand the strategy is calculated with regret matching,\n\n\\begin{align}\n\\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)} =\n\\begin{cases}\n\\frac{\\textcolor{orange}{R^{T,+}_i(I, a)}}{\\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')}},\n  & \\text{if} \\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')} \\gt 0 \\\\\n\\frac{1}{\\lvert A(I) \\rvert},\n & \\text{otherwise}\n\\end{cases}\n\\end{align}\n\nwhere $\\textcolor{orange}{R^{T,+}_i(I, a)} = \\max \\Big(\\textcolor{orange}{R^T_i(I, a)}, 0 \\Big)$\n\nThe paper\nThe paper\n[Regret Minimization in Games with Incomplete Information](http://martin.zinkevich.org/publications/regretpoker.pdf)\nproves that if the strategy is selected according to above equation\n$R^T_i$ gets smaller proportionate to $\\frac{1}{\\sqrt T}$, and\ntherefore reaches $\\epsilon$-[Nash equilibrium](#NashEquilibrium).\n\n<a id=\"MCCFR\"></a>\n\n### Monte Carlo CFR (MCCFR)\n\nComputing $\\textcolor{coral}{r^t_i(I, a)}$ requires expanding the full game tree\non each iteration.\n\nThe paper\n[Monte Carlo Sampling for Regret Minimization in Extensive Games](http://mlanctot.info/files/papers/nips09mccfr.pdf)\nshows we can sample from the game tree and estimate the regrets.\n\n$\\mathcal{Q} = {Q_1, \\ldots, Q_r}$ is a set of subsets of $Z$ ($Q_j \\subseteq Z$) where\nwe look at only a single block $Q_j$ in an iteration.\nUnion of all subsets spans $Z$ ($Q_1 \\cap \\ldots \\cap Q_r = Z$).\n$q_j$ is the probability of picking block $Q_j$.\n\n$q(z)$ is the probability of picking $z$ in current iteration; i.e. $q(z) = \\sum_{j:z \\in Q_j} q_j$ -\nthe sum of $q_j$ where $z \\in Q_j$.\n\nThen we get **sampled counterfactual value** fro block $j$,\n\n$$\\textcolor{pink}{\\tilde{v}(\\sigma, I|j)} =\n \\sum_{z \\in Q_j} \\frac{1}{q(z)}\n \\pi^\\sigma_{-i}(z[I]) \\pi^\\sigma(z[I], z) u_i(z)$$\n\nThe paper shows that\n\n$$\\mathbb{E}_{j \\sim q_j} \\Big[ \\textcolor{pink}{\\tilde{v}(\\sigma, I|j)} \\Big]\n= \\textcolor{pink}{v_i(\\sigma, I)}$$\n\nwith a simple proof.\n\nTherefore we can sample a part of the game tree and calculate the regrets.\nWe calculate an estimate of regrets\n\n$$\n\\textcolor{coral}{\\tilde{r}^t_i(I, a)} =\n \\textcolor{pink}{\\tilde{v}_i(\\sigma^t |_{I \\rightarrow a}, I)} - \\textcolor{pink}{\\tilde{v}_i(\\sigma^t, I)}\n$$\n\nAnd use that to update $\\textcolor{orange}{R^T_i(I, a)}$ and calculate\n the strategy $\\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)}$ on each iteration.\nFinally, we calculate the overall average strategy $\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)}$.\n\nHere is a [Kuhn Poker](kuhn/index.html) implementation to try CFR on Kuhn Poker.\n\n*Let's dive into the code!*\n\"\"\"\nfrom typing import NewType, Dict, List, Callable, cast\n\nfrom labml import monit, tracker, logger, experiment\nfrom labml.configs import BaseConfigs, option\n\n# A player $i \\in N$ where $N$ is the set of players\nPlayer = NewType('Player', int)\n# Action $a$, $A(h) = {a: (h, a) \\in H}$ where $h \\in H$ is a non-terminal [history](#History)\nAction = NewType('Action', str)\n\n\nclass History:\n    \"\"\"\n    <a id=\"History\"></a>\n\n    ## History\n\n    History $h \\in H$ is a sequence of actions including chance events,\n     and $H$ is the set of all histories.\n\n    This class should be extended with game specific logic.\n    \"\"\"\n\n    def is_terminal(self):\n        \"\"\"\n        Whether it's a terminal history; i.e. game over.\n        $h \\in Z$\n        \"\"\"\n        raise NotImplementedError()\n\n    def terminal_utility(self, i: Player) -> float:\n        \"\"\"\n        <a id=\"terminal_utility\"></a>\n        Utility of player $i$ for a terminal history.\n        $u_i(h)$ where $h \\in Z$\n        \"\"\"\n        raise NotImplementedError()\n\n    def player(self) -> Player:\n        \"\"\"\n        Get current player, denoted by $P(h)$, where $P$ is known as **Player function**.\n\n        If $P(h) = c$ it means that current event is a chance $c$ event.\n        Something like dealing cards, or opening common cards in poker.\n        \"\"\"\n        raise NotImplementedError()\n\n    def is_chance(self) -> bool:\n        \"\"\"\n        Whether the next step is a chance step; something like dealing a new card.\n        $P(h) = c$\n        \"\"\"\n        raise NotImplementedError()\n\n    def sample_chance(self) -> Action:\n        \"\"\"\n        Sample a chance when $P(h) = c$.\n        \"\"\"\n        raise NotImplementedError()\n\n    def __add__(self, action: Action):\n        \"\"\"\n        Add an action to the history.\n        \"\"\"\n        raise NotImplementedError()\n\n    def info_set_key(self) -> str:\n        \"\"\"\n        Get [information set](#InfoSet) for the current player\n        \"\"\"\n        raise NotImplementedError\n\n    def new_info_set(self) -> 'InfoSet':\n        \"\"\"\n        Create a new [information set](#InfoSet) for the current player\n        \"\"\"\n        raise NotImplementedError()\n\n    def __repr__(self):\n        \"\"\"\n        Human readable representation\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass InfoSet:\n    \"\"\"\n    <a id=\"InfoSet\"></a>\n\n    ## Information Set $I_i$\n    \"\"\"\n\n    # Unique key identifying the information set\n    key: str\n    # $\\sigma_i$, the [strategy](#Strategy) of player $i$\n    strategy: Dict[Action, float]\n    # Total regret of not taking each action $A(I_i)$,\n    #\n    # \\begin{align}\n    # \\textcolor{coral}{\\tilde{r}^t_i(I, a)} &=\n    #  \\textcolor{pink}{\\tilde{v}_i(\\sigma^t |_{I \\rightarrow a}, I)} -\n    #  \\textcolor{pink}{\\tilde{v}_i(\\sigma^t, I)}\n    # \\\\\n    # \\textcolor{orange}{R^T_i(I, a)} &=\n    #  \\frac{1}{T} \\sum_{t=1}^T \\textcolor{coral}{\\tilde{r}^t_i(I, a)}\n    # \\end{align}\n    #\n    # We maintain $T \\textcolor{orange}{R^T_i(I, a)}$ instead of $\\textcolor{orange}{R^T_i(I, a)}$\n    # since $\\frac{1}{T}$ term cancels out anyway when computing strategy\n    # $\\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)}$\n    regret: Dict[Action, float]\n    # We maintain the cumulative strategy\n    # $$\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}$$\n    # to compute overall average strategy\n    #\n    # $$\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)} =\n    #  \\frac{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}}{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)}$$\n    cumulative_strategy: Dict[Action, float]\n\n    def __init__(self, key: str):\n        \"\"\"\n        Initialize\n        \"\"\"\n        self.key = key\n        self.regret = {a: 0 for a in self.actions()}\n        self.cumulative_strategy = {a: 0 for a in self.actions()}\n        self.calculate_strategy()\n\n    def actions(self) -> List[Action]:\n        \"\"\"\n        Actions $A(I_i)$\n        \"\"\"\n        raise NotImplementedError()\n\n    @staticmethod\n    def from_dict(data: Dict[str, any]) -> 'InfoSet':\n        \"\"\"\n        Load information set from a saved dictionary\n        \"\"\"\n        raise NotImplementedError()\n\n    def to_dict(self):\n        \"\"\"\n        Save the information set to a dictionary\n        \"\"\"\n        return {\n            'key': self.key,\n            'regret': self.regret,\n            'average_strategy': self.cumulative_strategy,\n        }\n\n    def load_dict(self, data: Dict[str, any]):\n        \"\"\"\n        Load data from a saved dictionary\n        \"\"\"\n        self.regret = data['regret']\n        self.cumulative_strategy = data['average_strategy']\n        self.calculate_strategy()\n\n    def calculate_strategy(self):\n        \"\"\"\n        ## Calculate strategy\n\n        Calculate current strategy using [regret matching](#RegretMatching).\n\n        \\begin{align}\n        \\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)} =\n        \\begin{cases}\n        \\frac{\\textcolor{orange}{R^{T,+}_i(I, a)}}{\\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')}},\n          & \\text{if} \\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')} \\gt 0 \\\\\n        \\frac{1}{\\lvert A(I) \\rvert},\n         & \\text{otherwise}\n        \\end{cases}\n        \\end{align}\n\n        where $\\textcolor{orange}{R^{T,+}_i(I, a)} = \\max \\Big(\\textcolor{orange}{R^T_i(I, a)}, 0 \\Big)$\n        \"\"\"\n        # $$\\textcolor{orange}{R^{T,+}_i(I, a)} = \\max \\Big(\\textcolor{orange}{R^T_i(I, a)}, 0 \\Big)$$\n        regret = {a: max(r, 0) for a, r in self.regret.items()}\n        # $$\\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')}$$\n        regret_sum = sum(regret.values())\n        # if $\\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')} \\gt 0$,\n        if regret_sum > 0:\n            # $$\\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)} =\n            # \\frac{\\textcolor{orange}{R^{T,+}_i(I, a)}}{\\sum_{a'\\in A(I)}\\textcolor{orange}{R^{T,+}_i(I, a')}}$$\n            self.strategy = {a: r / regret_sum for a, r in regret.items()}\n        # Otherwise,\n        else:\n            # $\\lvert A(I) \\rvert$\n            count = len(list(a for a in self.regret))\n            # $$\\textcolor{lightgreen}{\\sigma_i^{T+1}(I)(a)} =\n            # \\frac{1}{\\lvert A(I) \\rvert}$$\n            self.strategy = {a: 1 / count for a, r in regret.items()}\n\n    def get_average_strategy(self):\n        \"\"\"\n        ## Get average strategy\n\n        $$\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)} =\n         \\frac{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}}\n         {\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)}$$\n        \"\"\"\n        # $$\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I) \\textcolor{lightgreen}{\\sigma^t(I)(a)}$$\n        cum_strategy = {a: self.cumulative_strategy.get(a, 0.) for a in self.actions()}\n        # $$\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I) =\n        # \\sum_{a \\in A(I)} \\sum_{t=1}^T\n        # \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}$$\n        strategy_sum = sum(cum_strategy.values())\n        # If $\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I) > 0$,\n        if strategy_sum > 0:\n            # $$\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)} =\n            #  \\frac{\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}}\n            #  {\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)}$$\n            return {a: s / strategy_sum for a, s in cum_strategy.items()}\n        # Otherwise,\n        else:\n            # $\\lvert A(I) \\rvert$\n            count = len(list(a for a in cum_strategy))\n            # $$\\textcolor{cyan}{\\bar{\\sigma}^T_i(I)(a)} =\n            # \\frac{1}{\\lvert A(I) \\rvert}$$\n            return {a: 1 / count for a, r in cum_strategy.items()}\n\n    def __repr__(self):\n        \"\"\"\n        Human readable representation\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass CFR:\n    \"\"\"\n    ## Counterfactual Regret Minimization (CFR) Algorithm\n\n    We do chance sampling (**CS**) where all the chance events (nodes) are sampled and\n    all other events (nodes) are explored.\n\n    We can ignore the term $q(z)$ since it's the same for all terminal histories\n    since we are doing chance sampling and it cancels out when calculating\n    strategy (common in numerator and denominator).\n    \"\"\"\n\n    # $\\mathcal{I}$ set of all information sets.\n    info_sets: Dict[str, InfoSet]\n\n    def __init__(self, *,\n                 create_new_history: Callable[[], History],\n                 epochs: int,\n                 n_players: int = 2):\n        \"\"\"\n        * `create_new_history` creates a new empty history\n        * `epochs` is the number of iterations to train on $T$\n        * `n_players` is the number of players\n        \"\"\"\n        self.n_players = n_players\n        self.epochs = epochs\n        self.create_new_history = create_new_history\n        # A dictionary for $\\mathcal{I}$ set of all information sets\n        self.info_sets = {}\n        # Tracker for analytics\n        self.tracker = InfoSetTracker()\n\n    def _get_info_set(self, h: History):\n        \"\"\"\n        Returns the information set $I$ of the current player for a given history $h$\n        \"\"\"\n        info_set_key = h.info_set_key()\n        if info_set_key not in self.info_sets:\n            self.info_sets[info_set_key] = h.new_info_set()\n        return self.info_sets[info_set_key]\n\n    def walk_tree(self, h: History, i: Player, pi_i: float, pi_neg_i: float) -> float:\n        \"\"\"\n        ### Walk Tree\n\n        This function walks the game tree.\n\n        * `h` is the current history $h$\n        * `i` is the player $i$ that we are computing regrets of\n        * [`pi_i`](#HistoryProbability) is\n         $\\pi^{\\sigma^t}_i(h)$\n        * [`pi_neg_i`](#HistoryProbability) is\n         $\\pi^{\\sigma^t}_{-i}(h)$\n\n        It returns the expected utility, for the history $h$\n        $$\\sum_{z \\in Z_h} \\pi^\\sigma(h, z) u_i(z)$$\n        where $Z_h$ is the set of terminal histories with prefix $h$\n\n        While walking the tee it updates the total regrets $\\textcolor{orange}{R^T_i(I, a)}$.\n        \"\"\"\n\n        # If it's a terminal history $h \\in Z$ return the terminal utility $u_i(h)$.\n        if h.is_terminal():\n            return h.terminal_utility(i)\n        # If it's a chance event $P(h) = c$ sample a and go to next step.\n        elif h.is_chance():\n            a = h.sample_chance()\n            return self.walk_tree(h + a, i, pi_i, pi_neg_i)\n\n        # Get current player's information set for $h$\n        I = self._get_info_set(h)\n        # To store $\\sum_{z \\in Z_h} \\pi^\\sigma(h, z) u_i(z)$\n        v = 0\n        # To store\n        # $$\\sum_{z \\in Z_h} \\pi^{\\sigma^t |_{I \\rightarrow a}}(h, z) u_i(z)$$\n        # for each action $a \\in A(h)$\n        va = {}\n\n        # Iterate through all actions\n        for a in I.actions():\n            # If the current player is $i$,\n            if i == h.player():\n                # \\begin{align}\n                # \\pi^{\\sigma^t}_i(h + a) &= \\pi^{\\sigma^t}_i(h) \\sigma^t_i(I)(a) \\\\\n                # \\pi^{\\sigma^t}_{-i}(h + a) &= \\pi^{\\sigma^t}_{-i}(h)\n                # \\end{align}\n                va[a] = self.walk_tree(h + a, i, pi_i * I.strategy[a], pi_neg_i)\n            # Otherwise,\n            else:\n                # \\begin{align}\n                # \\pi^{\\sigma^t}_i(h + a) &= \\pi^{\\sigma^t}_i(h)  \\\\\n                # \\pi^{\\sigma^t}_{-i}(h + a) &= \\pi^{\\sigma^t}_{-i}(h) * \\sigma^t_i(I)(a)\n                # \\end{align}\n                va[a] = self.walk_tree(h + a, i, pi_i, pi_neg_i * I.strategy[a])\n            # $$\\sum_{z \\in Z_h} \\pi^\\sigma(h, z) u_i(z) =\n            # \\sum_{a \\in A(I)} \\Bigg[ \\sigma^t_i(I)(a)\n            # \\sum_{z \\in Z_h} \\pi^{\\sigma^t |_{I \\rightarrow a}}(h, z) u_i(z)\n            # \\Bigg]$$\n            v = v + I.strategy[a] * va[a]\n\n        # If the current player is $i$,\n        # update the cumulative strategies and total regrets\n        if h.player() == i:\n            # Update cumulative strategies\n            # $$\\sum_{t=1}^T \\pi_i^{\\sigma^t}(I)\\textcolor{lightgreen}{\\sigma^t(I)(a)}\n            # = \\sum_{t=1}^T \\Big[ \\sum_{h \\in I} \\pi_i^{\\sigma^t}(h)\n            # \\textcolor{lightgreen}{\\sigma^t(I)(a)} \\Big]$$\n            for a in I.actions():\n                I.cumulative_strategy[a] = I.cumulative_strategy[a] + pi_i * I.strategy[a]\n            # \\begin{align}\n            # \\textcolor{coral}{\\tilde{r}^t_i(I, a)} &=\n            #  \\textcolor{pink}{\\tilde{v}_i(\\sigma^t |_{I \\rightarrow a}, I)} -\n            #  \\textcolor{pink}{\\tilde{v}_i(\\sigma^t, I)} \\\\\n            #  &=\n            #  \\pi^{\\sigma^t}_{-i} (h) \\Big(\n            #  \\sum_{z \\in Z_h} \\pi^{\\sigma^t |_{I \\rightarrow a}}(h, z) u_i(z) -\n            #  \\sum_{z \\in Z_h} \\pi^\\sigma(h, z) u_i(z)\n            #  \\Big) \\\\\n            # T \\textcolor{orange}{R^T_i(I, a)} &=\n            #  \\sum_{t=1}^T \\textcolor{coral}{\\tilde{r}^t_i(I, a)}\n            # \\end{align}\n            for a in I.actions():\n                I.regret[a] += pi_neg_i * (va[a] - v)\n\n            # Update the strategy $\\textcolor{lightgreen}{\\sigma^t(I)(a)}$\n            I.calculate_strategy()\n\n        # Return the expected utility for player $i$,\n        # $$\\sum_{z \\in Z_h} \\pi^\\sigma(h, z) u_i(z)$$\n        return v\n\n    def iterate(self):\n        \"\"\"\n        ### Iteratively update $\\textcolor{lightgreen}{\\sigma^t(I)(a)}$\n\n        This updates the strategies for $T$ iterations.\n        \"\"\"\n\n        # Loop for `epochs` times\n        for t in monit.iterate('Train', self.epochs):\n            # Walk tree and update regrets for each player\n            for i in range(self.n_players):\n                self.walk_tree(self.create_new_history(), cast(Player, i), 1, 1)\n\n            # Track data for analytics\n            tracker.add_global_step()\n            self.tracker(self.info_sets)\n            tracker.save()\n\n            # Save checkpoints every $1,000$ iterations\n            if (t + 1) % 1_000 == 0:\n                experiment.save_checkpoint()\n\n        # Print the information sets\n        logger.inspect(self.info_sets)\n\n\nclass InfoSetTracker:\n    \"\"\"\n    ### Information set tracker\n\n    This is a small helper class to track data from information sets\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Set tracking indicators\n        \"\"\"\n        tracker.set_histogram(f'strategy.*')\n        tracker.set_histogram(f'average_strategy.*')\n        tracker.set_histogram(f'regret.*')\n\n    def __call__(self, info_sets: Dict[str, InfoSet]):\n        \"\"\"\n        Track the data from all information sets\n        \"\"\"\n        for I in info_sets.values():\n            avg_strategy = I.get_average_strategy()\n            for a in I.actions():\n                tracker.add({\n                    f'strategy.{I.key}.{a}': I.strategy[a],\n                    f'average_strategy.{I.key}.{a}': avg_strategy[a],\n                    f'regret.{I.key}.{a}': I.regret[a],\n                })\n\n\nclass CFRConfigs(BaseConfigs):\n    \"\"\"\n    ### Configurable CFR module\n    \"\"\"\n    create_new_history: Callable[[], History]\n    epochs: int = 1_00_000\n    cfr: CFR = 'simple_cfr'\n\n\n@option(CFRConfigs.cfr)\ndef simple_cfr(c: CFRConfigs):\n    \"\"\"\n    Initialize **CFR** algorithm\n    \"\"\"\n    return CFR(create_new_history=c.create_new_history,\n               epochs=c.epochs)\n", "labml_nn/cfr/kuhn/__init__.py": "\"\"\"\n---\ntitle: CFR on Kuhn Poker\nsummary: >\n  This is an annotated implementation/tutorial of CFR on Kuhn Poker\n---\n\n# [Counterfactual Regret Minimization (CFR)](../index.html) on Kuhn Poker\n\nThis applies [Counterfactual Regret Minimization (CFR)](../index.html) to Kuhn poker.\n\n[Kuhn Poker](https://en.wikipedia.org/wiki/Kuhn_poker) is a two player 3-card betting game.\nThe players are dealt one card each out of Ace, King and Queen (no suits).\nThere are only three cards in the pack so one card is left out.\nAce beats King and Queen and King beats Queen - just like in normal ranking of cards.\n\nBoth players ante $1$ chip (blindly bet $1$ chip).\nAfter looking at the cards, the first player can either pass or bet $1$ chip.\nIf first player passes, the the player with higher card wins the pot.\nIf first player bets, the second play can bet (i.e. call) $1$ chip or pass (i.e. fold).\nIf the second player bets and the player with the higher card wins the pot.\nIf the second player passes (i.e. folds) the first player gets the pot.\nThis game is played repeatedly and a good strategy will optimize for the long term utility (or winnings).\n\nHere's some example games:\n\n* `KAp` - Player 1 gets K. Player 2 gets A. Player 1 passes. Player 2 doesn't get a betting chance and Player 2 wins the pot of $2$ chips.\n* `QKbp` - Player 1 gets Q. Player 2 gets K. Player 1 bets a chip. Player 2 passes (folds). Player 1 gets the pot of $4$ because Player 2 folded.\n* `QAbb` - Player 1 gets Q. Player 2 gets A. Player 1 bets a chip. Player 2 also bets (calls). Player 2 wins the pot of $4$.\n\nHe we extend the `InfoSet` class and `History` class defined in [`__init__.py`](../index.html)\nwith Kuhn Poker specifics.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/cfr/kuhn/experiment.ipynb)\n\"\"\"\n\nfrom typing import List, cast, Dict\n\nimport numpy as np\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.cfr import History as _History, InfoSet as _InfoSet, Action, Player, CFRConfigs\nfrom labml_nn.cfr.infoset_saver import InfoSetSaver\n\n# Kuhn poker actions are pass (`p`) or bet (`b`)\nACTIONS = cast(List[Action], ['p', 'b'])\n# The three cards in play are Ace, King and Queen\nCHANCES = cast(List[Action], ['A', 'K', 'Q'])\n# There are two players\nPLAYERS = cast(List[Player], [0, 1])\n\n\nclass InfoSet(_InfoSet):\n    \"\"\"\n    ## [Information set](../index.html#InfoSet)\n    \"\"\"\n\n    @staticmethod\n    def from_dict(data: Dict[str, any]) -> 'InfoSet':\n        \"\"\"Does not support save/load\"\"\"\n        pass\n\n    def actions(self) -> List[Action]:\n        \"\"\"\n        Return the list of actions. Terminal states are handled by `History` class.\n        \"\"\"\n        return ACTIONS\n\n    def __repr__(self):\n        \"\"\"\n        Human readable string representation - it gives the betting probability\n        \"\"\"\n        total = sum(self.cumulative_strategy.values())\n        total = max(total, 1e-6)\n        bet = self.cumulative_strategy[cast(Action, 'b')] / total\n        return f'{bet * 100: .1f}%'\n\n\nclass History(_History):\n    \"\"\"\n    ## [History](../index.html#History)\n\n    This defines when a game ends, calculates the utility and sample chance events (dealing cards).\n\n    The history is stored in a string:\n\n    * First two characters are the cards dealt to player 1 and player 2\n    * The third character is the action by the first player\n    * Fourth character is the action by the second player\n    \"\"\"\n\n    # History\n    history: str\n\n    def __init__(self, history: str = ''):\n        \"\"\"\n        Initialize with a given history string\n        \"\"\"\n        self.history = history\n\n    def is_terminal(self):\n        \"\"\"\n        Whether the history is terminal (game over).\n        \"\"\"\n        # Players are yet to take actions\n        if len(self.history) <= 2:\n            return False\n        # Last player to play passed (game over)\n        elif self.history[-1] == 'p':\n            return True\n        # Both players called (bet) (game over)\n        elif self.history[-2:] == 'bb':\n            return True\n        # Any other combination\n        else:\n            return False\n\n    def _terminal_utility_p1(self) -> float:\n        \"\"\"\n        Calculate the terminal utility for player $1$,  $u_1(z)$\n        \"\"\"\n        # $+1$ if Player 1 has a better card and $-1$ otherwise\n        winner = -1 + 2 * (self.history[0] < self.history[1])\n\n        # Second player passed\n        if self.history[-2:] == 'bp':\n            return 1\n        # Both players called, the player with better card wins $2$ chips\n        elif self.history[-2:] == 'bb':\n            return winner * 2\n        # First player passed, the player with better card wins $1$ chip\n        elif self.history[-1] == 'p':\n            return winner\n        # History is non-terminal\n        else:\n            raise RuntimeError()\n\n    def terminal_utility(self, i: Player) -> float:\n        \"\"\"\n        Get the terminal utility for player $i$\n        \"\"\"\n        # If $i$ is Player 1\n        if i == PLAYERS[0]:\n            return self._terminal_utility_p1()\n        # Otherwise, $u_2(z) = -u_1(z)$\n        else:\n            return -1 * self._terminal_utility_p1()\n\n    def is_chance(self) -> bool:\n        \"\"\"\n        The first two events are card dealing; i.e. chance events\n        \"\"\"\n        return len(self.history) < 2\n\n    def __add__(self, other: Action):\n        \"\"\"\n        Add an action to the history and return a new history\n        \"\"\"\n        return History(self.history + other)\n\n    def player(self) -> Player:\n        \"\"\"\n        Current player\n        \"\"\"\n        return cast(Player, len(self.history) % 2)\n\n    def sample_chance(self) -> Action:\n        \"\"\"\n        Sample a chance action\n        \"\"\"\n        while True:\n            # Randomly pick a card\n            r = np.random.randint(len(CHANCES))\n            chance = CHANCES[r]\n            # See if the card was dealt before\n            for c in self.history:\n                if c == chance:\n                    chance = None\n                    break\n\n            # Return the card if it was not dealt before\n            if chance is not None:\n                return cast(Action, chance)\n\n    def __repr__(self):\n        \"\"\"\n        Human readable representation\n        \"\"\"\n        return repr(self.history)\n\n    def info_set_key(self) -> str:\n        \"\"\"\n        Information set key for the current history.\n        This is a string of actions only visible to the current player.\n        \"\"\"\n        # Get current player\n        i = self.player()\n        # Current player sees her card and the betting actions\n        return self.history[i] + self.history[2:]\n\n    def new_info_set(self) -> InfoSet:\n        # Create a new information set object\n        return InfoSet(self.info_set_key())\n\n\ndef create_new_history():\n    \"\"\"A function to create an empty history object\"\"\"\n    return History()\n\n\nclass Configs(CFRConfigs):\n    \"\"\"\n    Configurations extends the CFR configurations class\n    \"\"\"\n    pass\n\n\n@option(Configs.create_new_history)\ndef _cnh():\n    \"\"\"\n    Set the `create_new_history` method for Kuhn Poker\n    \"\"\"\n    return create_new_history\n\n\ndef main():\n    \"\"\"\n    ### Run the experiment\n    \"\"\"\n\n    # Create an experiment, we only write tracking information to `sqlite` to speed things up.\n    # Since the algorithm iterates fast and we track data on each iteration, writing to\n    # other destinations such as Tensorboard can be relatively time consuming.\n    # SQLite is enough for our analytics.\n    experiment.create(name='kuhn_poker', writers={'sqlite'})\n    # Initialize configuration\n    conf = Configs()\n    # Load configuration\n    experiment.configs(conf)\n    # Start the experiment\n    with experiment.start():\n        # Start iterating\n        conf.cfr.iterate()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/uncertainty/__init__.py": "\"\"\"\n---\ntitle: Neural Networks with Uncertainty Estimation\nsummary: >\n A set of PyTorch implementations/tutorials related to uncertainty estimation\n---\n\n# Neural Networks with Uncertainty Estimation\n\nThese are neural network architectures that estimate the uncertainty of the predictions.\n\n* [Evidential Deep Learning to Quantify Classification Uncertainty](evidence/index.html)\n\"\"\"\n", "labml_nn/uncertainty/evidence/experiment.py": "\"\"\"\n---\ntitle: \"Evidential Deep Learning to Quantify Classification Uncertainty Experiment\"\nsummary: >\n  This trains is EDL model on MNIST\n---\n\n# [Evidential Deep Learning to Quantify Classification Uncertainty](index.html) Experiment\n\nThis trains a model based on [Evidential Deep Learning to Quantify Classification Uncertainty](index.html)\n on MNIST dataset.\n\"\"\"\n\nfrom typing import Any\n\nimport torch.nn as nn\nimport torch.utils.data\n\nfrom labml import tracker, experiment\nfrom labml.configs import option, calculate\nfrom labml_helpers.module import Module\nfrom labml_helpers.schedule import Schedule, RelativePiecewise\nfrom labml_helpers.train_valid import BatchIndex\nfrom labml_nn.experiments.mnist import MNISTConfigs\nfrom labml_nn.uncertainty.evidence import KLDivergenceLoss, TrackStatistics, MaximumLikelihoodLoss, \\\n    CrossEntropyBayesRisk, SquaredErrorBayesRisk\n\n\nclass Model(Module):\n    \"\"\"\n    ## LeNet based model fro MNIST classification\n    \"\"\"\n\n    def __init__(self, dropout: float):\n        super().__init__()\n        # First $5x5$ convolution layer\n        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n        # ReLU activation\n        self.act1 = nn.ReLU()\n        # $2x2$ max-pooling\n        self.max_pool1 = nn.MaxPool2d(2, 2)\n        # Second $5x5$ convolution layer\n        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n        # ReLU activation\n        self.act2 = nn.ReLU()\n        # $2x2$ max-pooling\n        self.max_pool2 = nn.MaxPool2d(2, 2)\n        # First fully-connected layer that maps to $500$ features\n        self.fc1 = nn.Linear(50 * 4 * 4, 500)\n        # ReLU activation\n        self.act3 = nn.ReLU()\n        # Final fully connected layer to output evidence for $10$ classes.\n        # The ReLU or Softplus activation is applied to this outside the model to get the\n        # non-negative evidence\n        self.fc2 = nn.Linear(500, 10)\n        # Dropout for the hidden layer\n        self.dropout = nn.Dropout(p=dropout)\n\n    def __call__(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the batch of MNIST images of shape `[batch_size, 1, 28, 28]`\n        \"\"\"\n        # Apply first convolution and max pooling.\n        # The result has shape `[batch_size, 20, 12, 12]`\n        x = self.max_pool1(self.act1(self.conv1(x)))\n        # Apply second convolution and max pooling.\n        # The result has shape `[batch_size, 50, 4, 4]`\n        x = self.max_pool2(self.act2(self.conv2(x)))\n        # Flatten the tensor to shape `[batch_size, 50 * 4 * 4]`\n        x = x.view(x.shape[0], -1)\n        # Apply hidden layer\n        x = self.act3(self.fc1(x))\n        # Apply dropout\n        x = self.dropout(x)\n        # Apply final layer and return\n        return self.fc2(x)\n\n\nclass Configs(MNISTConfigs):\n    \"\"\"\n    ## Configurations\n\n    We use [`MNISTConfigs`](../../experiments/mnist.html#MNISTConfigs) configurations.\n    \"\"\"\n\n    # [KL Divergence regularization](index.html#KLDivergenceLoss)\n    kl_div_loss = KLDivergenceLoss()\n    # KL Divergence regularization coefficient schedule\n    kl_div_coef: Schedule\n    # KL Divergence regularization coefficient schedule\n    kl_div_coef_schedule = [(0, 0.), (0.2, 0.01), (1, 1.)]\n    # [Stats module](index.html#TrackStatistics) for tracking\n    stats = TrackStatistics()\n    # Dropout\n    dropout: float = 0.5\n    # Module to convert the model output to non-zero evidences\n    outputs_to_evidence: Module\n\n    def init(self):\n        \"\"\"\n        ### Initialization\n        \"\"\"\n        # Set tracker configurations\n        tracker.set_scalar(\"loss.*\", True)\n        tracker.set_scalar(\"accuracy.*\", True)\n        tracker.set_histogram('u.*', True)\n        tracker.set_histogram('prob.*', False)\n        tracker.set_scalar('annealing_coef.*', False)\n        tracker.set_scalar('kl_div_loss.*', False)\n\n        #\n        self.state_modules = []\n\n    def step(self, batch: Any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training or validation step\n        \"\"\"\n\n        # Training/Evaluation mode\n        self.model.train(self.mode.is_train)\n\n        # Move data to the device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # One-hot coded targets\n        eye = torch.eye(10).to(torch.float).to(self.device)\n        target = eye[target]\n\n        # Update global step (number of samples processed) when in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Get model outputs\n        outputs = self.model(data)\n        # Get evidences $e_k \\ge 0$\n        evidence = self.outputs_to_evidence(outputs)\n\n        # Calculate loss\n        loss = self.loss_func(evidence, target)\n        # Calculate KL Divergence regularization loss\n        kl_div_loss = self.kl_div_loss(evidence, target)\n        tracker.add(\"loss.\", loss)\n        tracker.add(\"kl_div_loss.\", kl_div_loss)\n\n        # KL Divergence loss coefficient $\\lambda_t$\n        annealing_coef = min(1., self.kl_div_coef(tracker.get_global_step()))\n        tracker.add(\"annealing_coef.\", annealing_coef)\n\n        # Total loss\n        loss = loss + annealing_coef * kl_div_loss\n\n        # Track statistics\n        self.stats(evidence, target)\n\n        # Train the model\n        if self.mode.is_train:\n            # Calculate gradients\n            loss.backward()\n            # Take optimizer step\n            self.optimizer.step()\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save the tracked metrics\n        tracker.save()\n\n\n@option(Configs.model)\ndef mnist_model(c: Configs):\n    \"\"\"\n    ### Create model\n    \"\"\"\n    return Model(c.dropout).to(c.device)\n\n\n@option(Configs.kl_div_coef)\ndef kl_div_coef(c: Configs):\n    \"\"\"\n    ### KL Divergence Loss Coefficient Schedule\n    \"\"\"\n\n    # Create a [relative piecewise schedule](https://docs.labml.ai/api/helpers.html#labml_helpers.schedule.Piecewise)\n    return RelativePiecewise(c.kl_div_coef_schedule, c.epochs * len(c.train_dataset))\n\n\n# [Maximum Likelihood Loss](index.html#MaximumLikelihoodLoss)\ncalculate(Configs.loss_func, 'max_likelihood_loss', lambda: MaximumLikelihoodLoss())\n# [Cross Entropy Bayes Risk](index.html#CrossEntropyBayesRisk)\ncalculate(Configs.loss_func, 'cross_entropy_bayes_risk', lambda: CrossEntropyBayesRisk())\n# [Squared Error Bayes Risk](index.html#SquaredErrorBayesRisk)\ncalculate(Configs.loss_func, 'squared_error_bayes_risk', lambda: SquaredErrorBayesRisk())\n\n# ReLU to calculate evidence\ncalculate(Configs.outputs_to_evidence, 'relu', lambda: nn.ReLU())\n# Softplus to calculate evidence\ncalculate(Configs.outputs_to_evidence, 'softplus', lambda: nn.Softplus())\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='evidence_mnist')\n    # Create configurations\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 0.001,\n        'optimizer.weight_decay': 0.005,\n\n        # 'loss_func': 'max_likelihood_loss',\n        # 'loss_func': 'cross_entropy_bayes_risk',\n        'loss_func': 'squared_error_bayes_risk',\n\n        'outputs_to_evidence': 'softplus',\n\n        'dropout': 0.5,\n    })\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/uncertainty/evidence/__init__.py": "\"\"\"\n---\ntitle: \"Evidential Deep Learning to Quantify Classification Uncertainty\"\nsummary: >\n A PyTorch implementation/tutorial of the paper Evidential Deep Learning to Quantify Classification\n Uncertainty.\n---\n\n# Evidential Deep Learning to Quantify Classification Uncertainty\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[Evidential Deep Learning to Quantify Classification Uncertainty](https://arxiv.org/abs/1806.01768).\n\n[Dampster-Shafer Theory of Evidence](https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory)\nassigns belief masses a set of classes (unlike assigning a probability to a single class).\nSum of the masses of all subsets is $1$.\nIndividual class probabilities (plausibilities) can be derived from these masses.\n\nAssigning a mass to the set of all classes means it can be any one of the classes; i.e. saying \"I don't know\".\n\nIf there are $K$ classes, we assign masses $b_k \\ge 0$ to each of the classes and\n an overall uncertainty mass $u \\ge 0$ to all classes.\n\n$$u + \\sum_{k=1}^K b_k = 1$$\n\nBelief masses $b_k$ and $u$ can be computed from evidence $e_k \\ge 0$, as $b_k = \\frac{e_k}{S}$\nand $u = \\frac{K}{S}$ where $S = \\sum_{k=1}^K (e_k + 1)$.\nPaper uses term evidence as a measure of the amount of support\ncollected from data in favor of a sample to be classified into a certain class.\n\nThis corresponds to a [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)\nwith parameters $\\textcolor{orange}{\\alpha_k} = e_k + 1$, and\n $\\textcolor{orange}{\\alpha_0} = S = \\sum_{k=1}^K \\textcolor{orange}{\\alpha_k}$ is known as the Dirichlet strength.\nDirichlet distribution $D(\\mathbf{p} \\vert \\textcolor{orange}{\\mathbf{\\alpha}})$\n is a distribution over categorical distribution; i.e. you can sample class probabilities\nfrom a Dirichlet distribution.\nThe expected probability for class $k$ is $\\hat{p}_k = \\frac{\\textcolor{orange}{\\alpha_k}}{S}$.\n\nWe get the model to output evidences\n$$\\mathbf{e} = \\textcolor{orange}{\\mathbf{\\alpha}} - 1 = f(\\mathbf{x} | \\Theta)$$\n for a given input $\\mathbf{x}$.\nWe use a function such as\n [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) or a\n [Softplus](https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html)\n at the final layer to get $f(\\mathbf{x} | \\Theta) \\ge 0$.\n\nThe paper proposes a few loss functions to train the model, which we have implemented below.\n\nHere is the [training code `experiment.py`](experiment.html) to train a model on MNIST dataset.\n\"\"\"\n\nimport torch\n\nfrom labml import tracker\nfrom labml_helpers.module import Module\n\n\nclass MaximumLikelihoodLoss(Module):\n    \"\"\"\n    <a id=\"MaximumLikelihoodLoss\"></a>\n\n    ## Type II Maximum Likelihood Loss\n\n    The distribution $D(\\mathbf{p} \\vert \\textcolor{orange}{\\mathbf{\\alpha}})$ is a prior on the likelihood\n    $Multi(\\mathbf{y} \\vert p)$,\n     and the negative log marginal likelihood is calculated by integrating over class probabilities\n     $\\mathbf{p}$.\n\n    If target probabilities (one-hot targets) are $y_k$ for a given sample the loss is,\n\n    \\begin{align}\n    \\mathcal{L}(\\Theta)\n    &= -\\log \\Bigg(\n     \\int\n      \\prod_{k=1}^K p_k^{y_k}\n      \\frac{1}{B(\\textcolor{orange}{\\mathbf{\\alpha}})}\n      \\prod_{k=1}^K p_k^{\\textcolor{orange}{\\alpha_k} - 1}\n     d\\mathbf{p}\n     \\Bigg ) \\\\\n    &= \\sum_{k=1}^K y_k \\bigg( \\log S - \\log \\textcolor{orange}{\\alpha_k} \\bigg)\n    \\end{align}\n    \"\"\"\n    def forward(self, evidence: torch.Tensor, target: torch.Tensor):\n        \"\"\"\n        * `evidence` is $\\mathbf{e} \\ge 0$ with shape `[batch_size, n_classes]`\n        * `target` is $\\mathbf{y}$ with shape `[batch_size, n_classes]`\n        \"\"\"\n        # $\\textcolor{orange}{\\alpha_k} = e_k + 1$\n        alpha = evidence + 1.\n        # $S = \\sum_{k=1}^K \\textcolor{orange}{\\alpha_k}$\n        strength = alpha.sum(dim=-1)\n\n        # Losses $\\mathcal{L}(\\Theta) = \\sum_{k=1}^K y_k \\bigg( \\log S - \\log \\textcolor{orange}{\\alpha_k} \\bigg)$\n        loss = (target * (strength.log()[:, None] - alpha.log())).sum(dim=-1)\n\n        # Mean loss over the batch\n        return loss.mean()\n\n\nclass CrossEntropyBayesRisk(Module):\n    \"\"\"\n    <a id=\"CrossEntropyBayesRisk\"></a>\n\n    ## Bayes Risk with Cross Entropy Loss\n\n    Bayes risk is the overall maximum cost of making incorrect estimates.\n    It takes a cost function that gives the cost of making an incorrect estimate\n    and sums it over all possible outcomes based on probability distribution.\n\n    Here the cost function is cross-entropy loss, for one-hot coded $\\mathbf{y}$\n    $$\\sum_{k=1}^K -y_k \\log p_k$$\n\n    We integrate this cost over all $\\mathbf{p}$\n\n    \\begin{align}\n    \\mathcal{L}(\\Theta)\n    &= -\\log \\Bigg(\n     \\int\n      \\Big[ \\sum_{k=1}^K -y_k \\log p_k \\Big]\n      \\frac{1}{B(\\textcolor{orange}{\\mathbf{\\alpha}})}\n      \\prod_{k=1}^K p_k^{\\textcolor{orange}{\\alpha_k} - 1}\n     d\\mathbf{p}\n     \\Bigg ) \\\\\n    &= \\sum_{k=1}^K y_k \\bigg( \\psi(S) - \\psi( \\textcolor{orange}{\\alpha_k} ) \\bigg)\n    \\end{align}\n\n    where $\\psi(\\cdot)$ is the $digamma$ function.\n    \"\"\"\n\n    def forward(self, evidence: torch.Tensor, target: torch.Tensor):\n        \"\"\"\n        * `evidence` is $\\mathbf{e} \\ge 0$ with shape `[batch_size, n_classes]`\n        * `target` is $\\mathbf{y}$ with shape `[batch_size, n_classes]`\n        \"\"\"\n        # $\\textcolor{orange}{\\alpha_k} = e_k + 1$\n        alpha = evidence + 1.\n        # $S = \\sum_{k=1}^K \\textcolor{orange}{\\alpha_k}$\n        strength = alpha.sum(dim=-1)\n\n        # Losses $\\mathcal{L}(\\Theta) = \\sum_{k=1}^K y_k \\bigg( \\psi(S) - \\psi( \\textcolor{orange}{\\alpha_k} ) \\bigg)$\n        loss = (target * (torch.digamma(strength)[:, None] - torch.digamma(alpha))).sum(dim=-1)\n\n        # Mean loss over the batch\n        return loss.mean()\n\n\nclass SquaredErrorBayesRisk(Module):\n    \"\"\"\n    <a id=\"SquaredErrorBayesRisk\"></a>\n\n    ## Bayes Risk with Squared Error Loss\n\n    Here the cost function is squared error,\n    $$\\sum_{k=1}^K (y_k - p_k)^2 = \\Vert \\mathbf{y} - \\mathbf{p} \\Vert_2^2$$\n\n    We integrate this cost over all $\\mathbf{p}$\n\n    \\begin{align}\n    \\mathcal{L}(\\Theta)\n    &= -\\log \\Bigg(\n     \\int\n      \\Big[ \\sum_{k=1}^K (y_k - p_k)^2 \\Big]\n      \\frac{1}{B(\\textcolor{orange}{\\mathbf{\\alpha}})}\n      \\prod_{k=1}^K p_k^{\\textcolor{orange}{\\alpha_k} - 1}\n     d\\mathbf{p}\n     \\Bigg ) \\\\\n    &= \\sum_{k=1}^K \\mathbb{E} \\Big[ y_k^2 -2 y_k p_k + p_k^2 \\Big] \\\\\n    &= \\sum_{k=1}^K \\Big( y_k^2 -2 y_k \\mathbb{E}[p_k] + \\mathbb{E}[p_k^2] \\Big)\n    \\end{align}\n\n    Where $$\\mathbb{E}[p_k] = \\hat{p}_k = \\frac{\\textcolor{orange}{\\alpha_k}}{S}$$\n    is the expected probability when sampled from the Dirichlet distribution\n    and $$\\mathbb{E}[p_k^2] = \\mathbb{E}[p_k]^2 + \\text{Var}(p_k)$$\n     where\n    $$\\text{Var}(p_k) = \\frac{\\textcolor{orange}{\\alpha_k}(S - \\textcolor{orange}{\\alpha_k})}{S^2 (S + 1)}\n    = \\frac{\\hat{p}_k(1 - \\hat{p}_k)}{S + 1}$$\n     is the variance.\n\n    This gives,\n\n    \\begin{align}\n    \\mathcal{L}(\\Theta)\n    &= \\sum_{k=1}^K \\Big( y_k^2 -2 y_k \\mathbb{E}[p_k] + \\mathbb{E}[p_k^2] \\Big) \\\\\n    &= \\sum_{k=1}^K \\Big( y_k^2 -2 y_k \\mathbb{E}[p_k] +  \\mathbb{E}[p_k]^2 + \\text{Var}(p_k) \\Big) \\\\\n    &= \\sum_{k=1}^K \\Big( \\big( y_k -\\mathbb{E}[p_k] \\big)^2 + \\text{Var}(p_k) \\Big) \\\\\n    &= \\sum_{k=1}^K \\Big( ( y_k -\\hat{p}_k)^2 + \\frac{\\hat{p}_k(1 - \\hat{p}_k)}{S + 1} \\Big)\n    \\end{align}\n\n    This first part of the equation $\\big(y_k -\\mathbb{E}[p_k]\\big)^2$ is the error term and\n    the second part is the variance.\n    \"\"\"\n\n    def forward(self, evidence: torch.Tensor, target: torch.Tensor):\n        \"\"\"\n        * `evidence` is $\\mathbf{e} \\ge 0$ with shape `[batch_size, n_classes]`\n        * `target` is $\\mathbf{y}$ with shape `[batch_size, n_classes]`\n        \"\"\"\n        # $\\textcolor{orange}{\\alpha_k} = e_k + 1$\n        alpha = evidence + 1.\n        # $S = \\sum_{k=1}^K \\textcolor{orange}{\\alpha_k}$\n        strength = alpha.sum(dim=-1)\n        # $\\hat{p}_k = \\frac{\\textcolor{orange}{\\alpha_k}}{S}$\n        p = alpha / strength[:, None]\n\n        # Error $(y_k -\\hat{p}_k)^2$\n        err = (target - p) ** 2\n        # Variance $\\text{Var}(p_k) = \\frac{\\hat{p}_k(1 - \\hat{p}_k)}{S + 1}$\n        var = p * (1 - p) / (strength[:, None] + 1)\n\n        # Sum of them\n        loss = (err + var).sum(dim=-1)\n\n        # Mean loss over the batch\n        return loss.mean()\n\n\nclass KLDivergenceLoss(Module):\n    \"\"\"\n    <a id=\"KLDivergenceLoss\"></a>\n\n    ## KL Divergence Regularization Loss\n\n    This tries to shrink the total evidence to zero if the sample cannot be correctly classified.\n\n    First we calculate $\\tilde{\\alpha}_k = y_k + (1 - y_k) \\textcolor{orange}{\\alpha_k}$ the\n    Dirichlet parameters after remove the correct evidence.\n\n    \\begin{align}\n    &KL \\Big[ D(\\mathbf{p} \\vert \\mathbf{\\tilde{\\alpha}}) \\Big \\Vert\n    D(\\mathbf{p} \\vert <1, \\dots, 1>\\Big] \\\\\n    &= \\log \\Bigg( \\frac{\\Gamma \\Big( \\sum_{k=1}^K \\tilde{\\alpha}_k \\Big)}\n    {\\Gamma(K) \\prod_{k=1}^K \\Gamma(\\tilde{\\alpha}_k)} \\Bigg)\n    + \\sum_{k=1}^K (\\tilde{\\alpha}_k - 1)\n    \\Big[ \\psi(\\tilde{\\alpha}_k) - \\psi(\\tilde{S}) \\Big]\n    \\end{align}\n\n    where $\\Gamma(\\cdot)$ is the gamma function,\n    $\\psi(\\cdot)$ is the $digamma$ function and\n    $\\tilde{S} = \\sum_{k=1}^K \\tilde{\\alpha}_k$\n    \"\"\"\n    def forward(self, evidence: torch.Tensor, target: torch.Tensor):\n        \"\"\"\n        * `evidence` is $\\mathbf{e} \\ge 0$ with shape `[batch_size, n_classes]`\n        * `target` is $\\mathbf{y}$ with shape `[batch_size, n_classes]`\n        \"\"\"\n        # $\\textcolor{orange}{\\alpha_k} = e_k + 1$\n        alpha = evidence + 1.\n        # Number of classes\n        n_classes = evidence.shape[-1]\n        # Remove non-misleading evidence\n        # $$\\tilde{\\alpha}_k = y_k + (1 - y_k) \\textcolor{orange}{\\alpha_k}$$\n        alpha_tilde = target + (1 - target) * alpha\n        # $\\tilde{S} = \\sum_{k=1}^K \\tilde{\\alpha}_k$\n        strength_tilde = alpha_tilde.sum(dim=-1)\n\n        # The first term\n        #\n        # \\begin{align}\n        # &\\log \\Bigg( \\frac{\\Gamma \\Big( \\sum_{k=1}^K \\tilde{\\alpha}_k \\Big)}\n        #     {\\Gamma(K) \\prod_{k=1}^K \\Gamma(\\tilde{\\alpha}_k)} \\Bigg) \\\\\n        # &= \\log \\Gamma \\Big( \\sum_{k=1}^K \\tilde{\\alpha}_k \\Big)\n        #   - \\log \\Gamma(K)\n        #   - \\sum_{k=1}^K \\log \\Gamma(\\tilde{\\alpha}_k)\n        # \\end{align}\n        first = (torch.lgamma(alpha_tilde.sum(dim=-1))\n                 - torch.lgamma(alpha_tilde.new_tensor(float(n_classes)))\n                 - (torch.lgamma(alpha_tilde)).sum(dim=-1))\n\n        # The second term\n        # $$\\sum_{k=1}^K (\\tilde{\\alpha}_k - 1)\n        #     \\Big[ \\psi(\\tilde{\\alpha}_k) - \\psi(\\tilde{S}) \\Big]$$\n        second = (\n                (alpha_tilde - 1) *\n                (torch.digamma(alpha_tilde) - torch.digamma(strength_tilde)[:, None])\n        ).sum(dim=-1)\n\n        # Sum of the terms\n        loss = first + second\n\n        # Mean loss over the batch\n        return loss.mean()\n\n\nclass TrackStatistics(Module):\n    \"\"\"\n    <a id=\"TrackStatistics\"></a>\n\n    ### Track statistics\n\n    This module computes statistics and tracks them with [labml `tracker`](https://docs.labml.ai/api/tracker.html).\n    \"\"\"\n    def forward(self, evidence: torch.Tensor, target: torch.Tensor):\n        # Number of classes\n        n_classes = evidence.shape[-1]\n        # Predictions that correctly match with the target (greedy sampling based on highest probability)\n        match = evidence.argmax(dim=-1).eq(target.argmax(dim=-1))\n        # Track accuracy\n        tracker.add('accuracy.', match.sum() / match.shape[0])\n\n        # $\\textcolor{orange}{\\alpha_k} = e_k + 1$\n        alpha = evidence + 1.\n        # $S = \\sum_{k=1}^K \\textcolor{orange}{\\alpha_k}$\n        strength = alpha.sum(dim=-1)\n\n        # $\\hat{p}_k = \\frac{\\textcolor{orange}{\\alpha_k}}{S}$\n        expected_probability = alpha / strength[:, None]\n        # Expected probability of the selected (greedy highset probability) class\n        expected_probability, _ = expected_probability.max(dim=-1)\n\n        # Uncertainty mass $u = \\frac{K}{S}$\n        uncertainty_mass = n_classes / strength\n\n        # Track $u$ for correctly predictions\n        tracker.add('u.succ.', uncertainty_mass.masked_select(match))\n        # Track $u$ for incorrect predictions\n        tracker.add('u.fail.', uncertainty_mass.masked_select(~match))\n        # Track $\\hat{p}_k$ for correctly predictions\n        tracker.add('prob.succ.', expected_probability.masked_select(match))\n        # Track $\\hat{p}_k$ for incorrect predictions\n        tracker.add('prob.fail.', expected_probability.masked_select(~match))\n", "labml_nn/experiments/cifar10.py": "\"\"\"\n---\ntitle: CIFAR10 Experiment\nsummary: >\n  This is a reusable trainer for CIFAR10 dataset\n---\n\n# CIFAR10 Experiment\n\"\"\"\nfrom typing import List\n\nimport torch.nn as nn\n\nfrom labml import lab\nfrom labml.configs import option\nfrom labml_helpers.datasets.cifar10 import CIFAR10Configs as CIFAR10DatasetConfigs\nfrom labml_helpers.module import Module\nfrom labml_nn.experiments.mnist import MNISTConfigs\n\n\nclass CIFAR10Configs(CIFAR10DatasetConfigs, MNISTConfigs):\n    \"\"\"\n    ## Configurations\n\n    This extends from CIFAR 10 dataset configurations from\n     [`labml_helpers`](https://github.com/labmlai/labml/tree/master/helpers)\n     and [`MNISTConfigs`](mnist.html).\n    \"\"\"\n    # Use CIFAR10 dataset by default\n    dataset_name: str = 'CIFAR10'\n\n\n@option(CIFAR10Configs.train_dataset)\ndef cifar10_train_augmented():\n    \"\"\"\n    ### Augmented CIFAR 10 train dataset\n    \"\"\"\n    from torchvision.datasets import CIFAR10\n    from torchvision.transforms import transforms\n    return CIFAR10(str(lab.get_data_path()),\n                   train=True,\n                   download=True,\n                   transform=transforms.Compose([\n                       # Pad and crop\n                       transforms.RandomCrop(32, padding=4),\n                       # Random horizontal flip\n                       transforms.RandomHorizontalFlip(),\n                       #\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                   ]))\n\n\n@option(CIFAR10Configs.valid_dataset)\ndef cifar10_valid_no_augment():\n    \"\"\"\n    ### Non-augmented CIFAR 10 validation dataset\n    \"\"\"\n    from torchvision.datasets import CIFAR10\n    from torchvision.transforms import transforms\n    return CIFAR10(str(lab.get_data_path()),\n                   train=False,\n                   download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                   ]))\n\n\nclass CIFAR10VGGModel(Module):\n    \"\"\"\n    ### VGG model for CIFAR-10 classification\n    \"\"\"\n\n    def conv_block(self, in_channels, out_channels) -> nn.Module:\n        \"\"\"\n        Convolution and activation combined\n        \"\"\"\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n\n    def __init__(self, blocks: List[List[int]]):\n        super().__init__()\n\n        # 5 $2 \\times 2$ pooling layers will produce a output of size $1 \\ times 1$.\n        # CIFAR 10 image size is $32 \\times 32$\n        assert len(blocks) == 5\n        layers = []\n        # RGB channels\n        in_channels = 3\n        # Number of channels in each layer in each block\n        for block in blocks:\n            # Convolution, Normalization and Activation layers\n            for channels in block:\n                layers += self.conv_block(in_channels, channels)\n                in_channels = channels\n            # Max pooling at end of each block\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n\n        # Create a sequential model with the layers\n        self.layers = nn.Sequential(*layers)\n        # Final logits layer\n        self.fc = nn.Linear(in_channels, 10)\n\n    def forward(self, x):\n        # The VGG layers\n        x = self.layers(x)\n        # Reshape for classification layer\n        x = x.view(x.shape[0], -1)\n        # Final linear layer\n        return self.fc(x)\n", "labml_nn/experiments/nlp_classification.py": "\"\"\"\n---\ntitle: NLP classification trainer\nsummary: >\n  This is a reusable trainer for classification tasks\n---\n\n# NLP model trainer for classification\n\"\"\"\n\nfrom collections import Counter\nfrom typing import Callable\n\nimport torch\nimport torchtext\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport torchtext.vocab\nfrom torchtext.vocab import Vocab\n\nfrom labml import lab, tracker, monit\nfrom labml.configs import option\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_helpers.metrics.accuracy import Accuracy\nfrom labml_helpers.module import Module\nfrom labml_helpers.train_valid import TrainValidConfigs, hook_model_outputs, BatchIndex\nfrom labml_nn.optimizers.configs import OptimizerConfigs\n\n\nclass NLPClassificationConfigs(TrainValidConfigs):\n    \"\"\"\n    <a id=\"NLPClassificationConfigs\"></a>\n\n    ## Trainer configurations\n\n    This has the basic configurations for NLP classification task training.\n    All the properties are configurable.\n    \"\"\"\n\n    # Optimizer\n    optimizer: torch.optim.Adam\n    # Training device\n    device: torch.device = DeviceConfigs()\n\n    # Autoregressive model\n    model: Module\n    # Batch size\n    batch_size: int = 16\n    # Length of the sequence, or context size\n    seq_len: int = 512\n    # Vocabulary\n    vocab: Vocab = 'ag_news'\n    # Number of token in vocabulary\n    n_tokens: int\n    # Number of classes\n    n_classes: int = 'ag_news'\n    # Tokenizer\n    tokenizer: Callable = 'character'\n\n    # Whether to periodically save models\n    is_save_models = True\n\n    # Loss function\n    loss_func = nn.CrossEntropyLoss()\n    # Accuracy function\n    accuracy = Accuracy()\n    # Model embedding size\n    d_model: int = 512\n    # Gradient clipping\n    grad_norm_clip: float = 1.0\n\n    # Training data loader\n    train_loader: DataLoader = 'ag_news'\n    # Validation data loader\n    valid_loader: DataLoader = 'ag_news'\n\n    # Whether to log model parameters and gradients (once per epoch).\n    # These are summarized stats per layer, but it could still lead\n    # to many indicators for very deep networks.\n    is_log_model_params_grads: bool = False\n\n    # Whether to log model activations (once per epoch).\n    # These are summarized stats per layer, but it could still lead\n    # to many indicators for very deep networks.\n    is_log_model_activations: bool = False\n\n    def init(self):\n        \"\"\"\n        ### Initialization\n        \"\"\"\n        # Set tracker configurations\n        tracker.set_scalar(\"accuracy.*\", True)\n        tracker.set_scalar(\"loss.*\", True)\n        # Add a hook to log module outputs\n        hook_model_outputs(self.mode, self.model, 'model')\n        # Add accuracy as a state module.\n        # The name is probably confusing, since it's meant to store\n        # states between training and validation for RNNs.\n        # This will keep the accuracy metric stats separate for training and validation.\n        self.state_modules = [self.accuracy]\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training or validation step\n        \"\"\"\n\n        # Move data to the device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Update global step (number of tokens processed) when in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(data.shape[1])\n\n        # Whether to capture model outputs\n        with self.mode.update(is_log_activations=batch_idx.is_last and self.is_log_model_activations):\n            # Get model outputs.\n            # It's returning a tuple for states when using RNNs.\n            # This is not implemented yet. \ud83d\ude1c\n            output, *_ = self.model(data)\n\n        # Calculate and log loss\n        loss = self.loss_func(output, target)\n        tracker.add(\"loss.\", loss)\n\n        # Calculate and log accuracy\n        self.accuracy(output, target)\n        self.accuracy.track()\n\n        # Train the model\n        if self.mode.is_train:\n            # Calculate gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Take optimizer step\n            self.optimizer.step()\n            # Log the model parameters and gradients on last batch of every epoch\n            if batch_idx.is_last and self.is_log_model_params_grads:\n                tracker.add('model', self.model)\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save the tracked metrics\n        tracker.save()\n\n\n@option(NLPClassificationConfigs.optimizer)\ndef _optimizer(c: NLPClassificationConfigs):\n    \"\"\"\n    ### Default [optimizer configurations](../optimizers/configs.html)\n    \"\"\"\n\n    optimizer = OptimizerConfigs()\n    optimizer.parameters = c.model.parameters()\n    optimizer.optimizer = 'Adam'\n    optimizer.d_model = c.d_model\n\n    return optimizer\n\n\n@option(NLPClassificationConfigs.tokenizer)\ndef basic_english():\n    \"\"\"\n    ### Basic  english tokenizer\n\n    We use character level tokenizer in this experiment.\n    You can switch by setting,\n\n    ```\n    'tokenizer': 'basic_english',\n    ```\n\n    in the configurations dictionary when starting the experiment.\n\n    \"\"\"\n    from torchtext.data import get_tokenizer\n    return get_tokenizer('basic_english')\n\n\ndef character_tokenizer(x: str):\n    \"\"\"\n    ### Character level tokenizer\n    \"\"\"\n    return list(x)\n\n\n@option(NLPClassificationConfigs.tokenizer)\ndef character():\n    \"\"\"\n    Character level tokenizer configuration\n    \"\"\"\n    return character_tokenizer\n\n\n@option(NLPClassificationConfigs.n_tokens)\ndef _n_tokens(c: NLPClassificationConfigs):\n    \"\"\"\n    Get number of tokens\n    \"\"\"\n    return len(c.vocab) + 2\n\n\nclass CollateFunc:\n    \"\"\"\n    ## Function to load data into batches\n    \"\"\"\n\n    def __init__(self, tokenizer, vocab: Vocab, seq_len: int, padding_token: int, classifier_token: int):\n        \"\"\"\n        * `tokenizer` is the tokenizer function\n        * `vocab` is the vocabulary\n        * `seq_len` is the length of the sequence\n        * `padding_token` is the token used for padding when the `seq_len` is larger than the text length\n        * `classifier_token` is the `[CLS]` token which we set at end of the input\n        \"\"\"\n        self.classifier_token = classifier_token\n        self.padding_token = padding_token\n        self.seq_len = seq_len\n        self.vocab = vocab\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        \"\"\"\n        * `batch` is the batch of data collected by the `DataLoader`\n        \"\"\"\n\n        # Input data tensor, initialized with `padding_token`\n        data = torch.full((self.seq_len, len(batch)), self.padding_token, dtype=torch.long)\n        # Empty labels tensor\n        labels = torch.zeros(len(batch), dtype=torch.long)\n\n        # Loop through the samples\n        for (i, (_label, _text)) in enumerate(batch):\n            # Set the label\n            labels[i] = int(_label) - 1\n            # Tokenize the input text\n            _text = [self.vocab[token] for token in self.tokenizer(_text)]\n            # Truncate upto `seq_len`\n            _text = _text[:self.seq_len]\n            # Transpose and add to data\n            data[:len(_text), i] = data.new_tensor(_text)\n\n        # Set the final token in the sequence to `[CLS]`\n        data[-1, :] = self.classifier_token\n\n        #\n        return data, labels\n\n\n@option([NLPClassificationConfigs.n_classes,\n         NLPClassificationConfigs.vocab,\n         NLPClassificationConfigs.train_loader,\n         NLPClassificationConfigs.valid_loader])\ndef ag_news(c: NLPClassificationConfigs):\n    \"\"\"\n    ### AG News dataset\n\n    This loads the AG News dataset and the set the values for\n     `n_classes`, `vocab`, `train_loader`, and `valid_loader`.\n    \"\"\"\n\n    # Get training and validation datasets\n    train, valid = torchtext.datasets.AG_NEWS(root=str(lab.get_data_path() / 'ag_news'), split=('train', 'test'))\n\n    # Load data to memory\n    with monit.section('Load data'):\n        from labml_nn.utils import MapStyleDataset\n\n        # Create [map-style datasets](../utils.html#map_style_dataset)\n        train, valid = MapStyleDataset(train), MapStyleDataset(valid)\n\n    # Get tokenizer\n    tokenizer = c.tokenizer\n\n    # Create a counter\n    counter = Counter()\n    # Collect tokens from training dataset\n    for (label, line) in train:\n        counter.update(tokenizer(line))\n    # Collect tokens from validation dataset\n    for (label, line) in valid:\n        counter.update(tokenizer(line))\n    # Create vocabulary\n    vocab = torchtext.vocab.vocab(counter, min_freq=1)\n\n    # Create training data loader\n    train_loader = DataLoader(train, batch_size=c.batch_size, shuffle=True,\n                              collate_fn=CollateFunc(tokenizer, vocab, c.seq_len, len(vocab), len(vocab) + 1))\n    # Create validation data loader\n    valid_loader = DataLoader(valid, batch_size=c.batch_size, shuffle=True,\n                              collate_fn=CollateFunc(tokenizer, vocab, c.seq_len, len(vocab), len(vocab) + 1))\n\n    # Return `n_classes`, `vocab`, `train_loader`, and `valid_loader`\n    return 4, vocab, train_loader, valid_loader\n", "labml_nn/experiments/nlp_autoregression.py": "\"\"\"\n---\ntitle: NLP auto-regression trainer\nsummary: >\n  This is a reusable trainer for auto-regressive tasks\n---\n\n# Auto-regressive NLP model trainer\n\"\"\"\n\nfrom typing import Callable\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, RandomSampler\n\nfrom labml import lab, monit, logger, tracker\nfrom labml.configs import option\nfrom labml.logger import Text\nfrom labml_helpers.datasets.text import TextDataset, SequentialDataLoader, SequentialUnBatchedDataset, TextFileDataset\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_helpers.metrics.accuracy import Accuracy\nfrom labml_helpers.module import Module\nfrom labml_helpers.train_valid import TrainValidConfigs, hook_model_outputs, BatchIndex\nfrom labml_nn.optimizers.configs import OptimizerConfigs\n\n\nclass CrossEntropyLoss(Module):\n    \"\"\"\n    ### Cross entropy loss\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.loss = nn.CrossEntropyLoss()\n\n    def forward(self, outputs, targets):\n        return self.loss(outputs.view(-1, outputs.shape[-1]), targets.view(-1))\n\n\nclass NLPAutoRegressionConfigs(TrainValidConfigs):\n    \"\"\"\n    <a id=\"NLPAutoRegressionConfigs\"></a>\n\n    ## Trainer configurations\n\n    This has the basic configurations for NLP auto-regressive task training.\n    All the properties are configurable.\n    \"\"\"\n\n    # Optimizer\n    optimizer: torch.optim.Adam\n    # Training device\n    device: torch.device = DeviceConfigs()\n\n    # Autoregressive model\n    model: Module\n    # Text dataset\n    text: TextDataset\n    # Batch size\n    batch_size: int = 16\n    # Length of the sequence, or context size\n    seq_len: int = 512\n    # Number of token in vocabulary\n    n_tokens: int\n    # Tokenizer\n    tokenizer: Callable = 'character'\n\n    # Text prompt to start sampling (for illustration)\n    prompt: str\n    # The token separator when sampling (blank for character level tokenization)\n    prompt_separator: str\n\n    # Whether to periodically save models\n    is_save_models = True\n\n    # Loss function\n    loss_func = CrossEntropyLoss()\n    # Accuracy function\n    accuracy = Accuracy()\n    # Model embedding size\n    d_model: int = 512\n    # Gradient clipping\n    grad_norm_clip: float = 1.0\n\n    # Training data loader\n    train_loader: DataLoader = 'shuffled_train_loader'\n    # Validation data loader\n    valid_loader: DataLoader = 'shuffled_valid_loader'\n\n    # Data loaders shuffle with replacement\n    dataloader_shuffle_with_replacement: bool = False\n\n    # Whether to log model parameters and gradients (once per epoch).\n    # These are summarized stats per layer, but it could still lead\n    # to many indicators for very deep networks.\n    is_log_model_params_grads: bool = False\n\n    # Whether to log model activations (once per epoch).\n    # These are summarized stats per layer, but it could still lead\n    # to many indicators for very deep networks.\n    is_log_model_activations: bool = False\n\n    def init(self):\n        \"\"\"\n        ### Initialization\n        \"\"\"\n        # Set tracker configurations\n        tracker.set_scalar(\"accuracy.*\", True)\n        tracker.set_scalar(\"loss.*\", True)\n        tracker.set_text(\"sampled\", False)\n        # Add a hook to log module outputs\n        hook_model_outputs(self.mode, self.model, 'model')\n        # Add accuracy as a state module.\n        # The name is probably confusing, since it's meant to store\n        # states between training and validation for RNNs.\n        # This will keep the accuracy metric stats separate for training and validation.\n        self.state_modules = [self.accuracy]\n\n    def other_metrics(self, output: torch.Tensor, target: torch.Tensor):\n        \"\"\"Override to calculate and log other metrics\"\"\"\n        pass\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training or validation step\n        \"\"\"\n\n        # Set training/eval mode\n        self.model.train(self.mode.is_train)\n\n        # Move data to the device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Update global step (number of tokens processed) when in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(data.shape[0] * data.shape[1])\n\n        # Whether to capture model outputs\n        with self.mode.update(is_log_activations=batch_idx.is_last and self.is_log_model_activations):\n            # Get model outputs.\n            # It's returning a tuple for states when using RNNs.\n            # This is not implemented yet. \ud83d\ude1c\n            output, *_ = self.model(data)\n\n        # Calculate and log loss\n        loss = self.loss_func(output, target)\n        tracker.add(\"loss.\", loss)\n\n        # Calculate and log accuracy\n        self.accuracy(output, target)\n        self.accuracy.track()\n\n        self.other_metrics(output, target)\n\n        # Train the model\n        if self.mode.is_train:\n            # Calculate gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Take optimizer step\n            self.optimizer.step()\n            # Log the model parameters and gradients on last batch of every epoch\n            if batch_idx.is_last and self.is_log_model_params_grads:\n                tracker.add('model', self.model)\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save the tracked metrics\n        tracker.save()\n\n    def sample(self):\n        \"\"\"\n        ### Sampling function to generate samples periodically while training\n        \"\"\"\n\n        # Starting prompt\n        prompt = self.prompt\n        # Collect output for printing\n        log = [(prompt, Text.subtle)]\n        # Sample 25 tokens\n        for i in monit.iterate('Sample', 25):\n            # Tokenize the prompt\n            data = self.text.text_to_i(prompt).unsqueeze(-1)\n            data = data.to(self.device)\n            # Get the model output\n            output, *_ = self.model(data)\n            # Get the model prediction (greedy)\n            output = output.argmax(dim=-1).squeeze()\n            # Add the prediction to prompt\n            prompt += self.prompt_separator + self.text.itos[output[-1]]\n            # Add the prediction for logging\n            log += [(self.prompt_separator + self.text.itos[output[-1]], Text.value)]\n\n        tracker.add({'sampled': prompt})\n        # Print the sampled output\n        logger.log(log)\n\n\n@option(NLPAutoRegressionConfigs.optimizer)\ndef _optimizer(c: NLPAutoRegressionConfigs):\n    \"\"\"\n    ### Default [optimizer configurations](../optimizers/configs.html)\n    \"\"\"\n\n    optimizer = OptimizerConfigs()\n    optimizer.parameters = c.model.parameters()\n    optimizer.optimizer = 'Adam'\n    optimizer.d_model = c.d_model\n\n    return optimizer\n\n\n@option(NLPAutoRegressionConfigs.n_tokens)\ndef _n_tokens(c: NLPAutoRegressionConfigs):\n    \"\"\"\n    Get number of tokens\n    \"\"\"\n    return c.text.n_tokens\n\n\n@option(NLPAutoRegressionConfigs.tokenizer)\ndef basic_english():\n    \"\"\"\n    ### Basic  english tokenizer\n\n    We use character level tokenizer in this experiment.\n    You can switch by setting,\n\n    ```\n    'tokenizer': 'basic_english',\n    ```\n\n    in the configurations dictionary when starting the experiment.\n    \"\"\"\n\n    from torchtext.data import get_tokenizer\n    return get_tokenizer('basic_english')\n\n\ndef character_tokenizer(x: str):\n    \"\"\"\n    ### Character level tokenizer\n    \"\"\"\n    return list(x)\n\n\n@option(NLPAutoRegressionConfigs.tokenizer)\ndef character():\n    \"\"\"\n    ### Character level tokenizer configuration\n    \"\"\"\n    return character_tokenizer\n\n\n@option(NLPAutoRegressionConfigs.text)\ndef tiny_shakespeare(c: NLPAutoRegressionConfigs):\n    \"\"\"\n    ### Tiny Shakespeare dataset\n\n    It will download from the url if not present\n    \"\"\"\n    return TextFileDataset(\n        lab.get_data_path() / 'tiny_shakespeare.txt',\n        c.tokenizer,\n        url='https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n\n\n@option(NLPAutoRegressionConfigs.train_loader)\ndef sequential_train_loader(c: NLPAutoRegressionConfigs):\n    \"\"\"\n    ### Sequential training data loader\n    \"\"\"\n    return SequentialDataLoader(text=c.text.train,\n                                dataset=c.text,\n                                batch_size=c.batch_size,\n                                seq_len=c.seq_len)\n\n\n@option(NLPAutoRegressionConfigs.valid_loader)\ndef sequential_valid_loader(c: NLPAutoRegressionConfigs):\n    \"\"\"\n    ### Sequential validation data loader\n    \"\"\"\n    return SequentialDataLoader(text=c.text.valid,\n                                dataset=c.text,\n                                batch_size=c.batch_size,\n                                seq_len=c.seq_len)\n\n\ndef transpose_batch(batch):\n    \"\"\"\n    ### Transpose batch\n\n    `DataLoader` collects the batches on the first dimension.\n    We need to transpose it to be sequence first.\n    \"\"\"\n\n    transposed_data = list(zip(*batch))\n    # Stack the batch along the second dimension `dim=1`\n    src = torch.stack(transposed_data[0], dim=1)\n    tgt = torch.stack(transposed_data[1], dim=1)\n\n    return src, tgt\n\n\n@option(NLPAutoRegressionConfigs.train_loader)\ndef shuffled_train_loader(c: NLPAutoRegressionConfigs):\n    \"\"\"\n    ### Shuffled training data loader\n    \"\"\"\n    dataset = SequentialUnBatchedDataset(text=c.text.train,\n                                         dataset=c.text,\n                                         seq_len=c.seq_len)\n    sampler = RandomSampler(dataset, replacement=c.dataloader_shuffle_with_replacement)\n\n    return DataLoader(dataset,\n                      batch_size=c.batch_size,\n                      collate_fn=transpose_batch,\n                      sampler=sampler)\n\n\n@option(NLPAutoRegressionConfigs.valid_loader)\ndef shuffled_valid_loader(c: NLPAutoRegressionConfigs):\n    \"\"\"\n    ### Shuffled validation data loader\n    \"\"\"\n    dataset = SequentialUnBatchedDataset(text=c.text.valid,\n                                         dataset=c.text,\n                                         seq_len=c.seq_len)\n    sampler = RandomSampler(dataset, replacement=c.dataloader_shuffle_with_replacement)\n\n    return DataLoader(dataset,\n                      batch_size=c.batch_size,\n                      collate_fn=transpose_batch,\n                      sampler=sampler)\n", "labml_nn/experiments/mnist.py": "\"\"\"\n---\ntitle: MNIST Experiment\nsummary: >\n  This is a reusable trainer for MNIST dataset\n---\n\n# MNIST Experiment\n\"\"\"\n\nimport torch.nn as nn\nimport torch.utils.data\nfrom labml_helpers.module import Module\n\nfrom labml import tracker\nfrom labml.configs import option\nfrom labml_helpers.datasets.mnist import MNISTConfigs as MNISTDatasetConfigs\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_helpers.metrics.accuracy import Accuracy\nfrom labml_helpers.train_valid import TrainValidConfigs, BatchIndex, hook_model_outputs\nfrom labml_nn.optimizers.configs import OptimizerConfigs\n\n\nclass MNISTConfigs(MNISTDatasetConfigs, TrainValidConfigs):\n    \"\"\"\n    <a id=\"MNISTConfigs\"></a>\n\n    ## Trainer configurations\n    \"\"\"\n\n    # Optimizer\n    optimizer: torch.optim.Adam\n    # Training device\n    device: torch.device = DeviceConfigs()\n\n    # Classification model\n    model: Module\n    # Number of epochs to train for\n    epochs: int = 10\n\n    # Number of times to switch between training and validation within an epoch\n    inner_iterations = 10\n\n    # Accuracy function\n    accuracy = Accuracy()\n    # Loss function\n    loss_func = nn.CrossEntropyLoss()\n\n    def init(self):\n        \"\"\"\n        ### Initialization\n        \"\"\"\n        # Set tracker configurations\n        tracker.set_scalar(\"loss.*\", True)\n        tracker.set_scalar(\"accuracy.*\", True)\n        # Add a hook to log module outputs\n        hook_model_outputs(self.mode, self.model, 'model')\n        # Add accuracy as a state module.\n        # The name is probably confusing, since it's meant to store\n        # states between training and validation for RNNs.\n        # This will keep the accuracy metric stats separate for training and validation.\n        self.state_modules = [self.accuracy]\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training or validation step\n        \"\"\"\n\n        # Training/Evaluation mode\n        self.model.train(self.mode.is_train)\n\n        # Move data to the device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Update global step (number of samples processed) when in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Whether to capture model outputs\n        with self.mode.update(is_log_activations=batch_idx.is_last):\n            # Get model outputs.\n            output = self.model(data)\n\n        # Calculate and log loss\n        loss = self.loss_func(output, target)\n        tracker.add(\"loss.\", loss)\n\n        # Calculate and log accuracy\n        self.accuracy(output, target)\n        self.accuracy.track()\n\n        # Train the model\n        if self.mode.is_train:\n            # Calculate gradients\n            loss.backward()\n            # Take optimizer step\n            self.optimizer.step()\n            # Log the model parameters and gradients on last batch of every epoch\n            if batch_idx.is_last:\n                tracker.add('model', self.model)\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save the tracked metrics\n        tracker.save()\n\n\n@option(MNISTConfigs.optimizer)\ndef _optimizer(c: MNISTConfigs):\n    \"\"\"\n    ### Default optimizer configurations\n    \"\"\"\n    opt_conf = OptimizerConfigs()\n    opt_conf.parameters = c.model.parameters()\n    opt_conf.optimizer = 'Adam'\n    return opt_conf\n", "labml_nn/experiments/__init__.py": "", "labml_nn/distillation/small.py": "\"\"\"\n---\ntitle: Train a small model on CIFAR 10\nsummary: >\n  Train a small model on CIFAR 10 to test how much distillation benefits.\n---\n\n#  Train a small model on CIFAR 10\n\nThis trains a small model on CIFAR 10 to test how much [distillation](index.html) benefits.\n\"\"\"\n\nimport torch.nn as nn\n\nfrom labml import experiment, logger\nfrom labml.configs import option\nfrom labml_nn.experiments.cifar10 import CIFAR10Configs, CIFAR10VGGModel\nfrom labml_nn.normalization.batch_norm import BatchNorm\n\n\nclass Configs(CIFAR10Configs):\n    \"\"\"\n    ## Configurations\n\n    We use [`CIFAR10Configs`](../experiments/cifar10.html) which defines all the\n    dataset related configurations, optimizer, and a training loop.\n    \"\"\"\n    pass\n\n\nclass SmallModel(CIFAR10VGGModel):\n    \"\"\"\n    ### VGG style model for CIFAR-10 classification\n\n    This derives from the [generic VGG style architecture](../experiments/cifar10.html).\n    \"\"\"\n\n    def conv_block(self, in_channels, out_channels) -> nn.Module:\n        \"\"\"\n        Create a convolution layer and the activations\n        \"\"\"\n        return nn.Sequential(\n            # Convolution layer\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            # Batch normalization\n            BatchNorm(out_channels, track_running_stats=False),\n            # ReLU activation\n            nn.ReLU(inplace=True),\n        )\n\n    def __init__(self):\n        # Create a model with given convolution sizes (channels)\n        super().__init__([[32, 32], [64, 64], [128], [128], [128]])\n\n\n@option(Configs.model)\ndef _small_model(c: Configs):\n    \"\"\"\n    ### Create model\n    \"\"\"\n    return SmallModel().to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='cifar10', comment='small model')\n    # Create configurations\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n    })\n    # Set model for saving/loading\n    experiment.add_pytorch_models({'model': conf.model})\n    # Print number of parameters in the model\n    logger.inspect(params=(sum(p.numel() for p in conf.model.parameters() if p.requires_grad)))\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/distillation/large.py": "\"\"\"\n---\ntitle: Train a large model on CIFAR 10\nsummary: >\n  Train a large model on CIFAR 10 for distillation.\n---\n\n#  Train a large model on CIFAR 10\n\nThis trains a large model on CIFAR 10 for [distillation](index.html).\n\"\"\"\n\nimport torch.nn as nn\n\nfrom labml import experiment, logger\nfrom labml.configs import option\nfrom labml_nn.experiments.cifar10 import CIFAR10Configs, CIFAR10VGGModel\nfrom labml_nn.normalization.batch_norm import BatchNorm\n\n\nclass Configs(CIFAR10Configs):\n    \"\"\"\n    ## Configurations\n\n    We use [`CIFAR10Configs`](../experiments/cifar10.html) which defines all the\n    dataset related configurations, optimizer, and a training loop.\n    \"\"\"\n    pass\n\n\nclass LargeModel(CIFAR10VGGModel):\n    \"\"\"\n    ### VGG style model for CIFAR-10 classification\n\n    This derives from the [generic VGG style architecture](../experiments/cifar10.html).\n    \"\"\"\n\n    def conv_block(self, in_channels, out_channels) -> nn.Module:\n        \"\"\"\n        Create a convolution layer and the activations\n        \"\"\"\n        return nn.Sequential(\n            # Dropout\n            nn.Dropout(0.1),\n            # Convolution layer\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            # Batch normalization\n            BatchNorm(out_channels, track_running_stats=False),\n            # ReLU activation\n            nn.ReLU(inplace=True),\n        )\n\n    def __init__(self):\n        # Create a model with given convolution sizes (channels)\n        super().__init__([[64, 64], [128, 128], [256, 256, 256], [512, 512, 512], [512, 512, 512]])\n\n\n@option(Configs.model)\ndef _large_model(c: Configs):\n    \"\"\"\n    ### Create model\n    \"\"\"\n    return LargeModel().to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='cifar10', comment='large model')\n    # Create configurations\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n        'is_save_models': True,\n        'epochs': 20,\n    })\n    # Set model for saving/loading\n    experiment.add_pytorch_models({'model': conf.model})\n    # Print number of parameters in the model\n    logger.inspect(params=(sum(p.numel() for p in conf.model.parameters() if p.requires_grad)))\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/distillation/__init__.py": "\"\"\"\n---\ntitle: Distilling the Knowledge in a Neural Network\nsummary: >\n  PyTorch implementation and tutorial of the paper\n  Distilling the Knowledge in a Neural Network.\n---\n\n# Distilling the Knowledge in a Neural Network\n\nThis is a [PyTorch](https://pytorch.org) implementation/tutorial of the paper\n[Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531).\n\nIt's a way of training a small network using the knowledge in a trained larger network;\ni.e. distilling the knowledge from the large network.\n\nA large model with regularization or an ensemble of models (using dropout) generalizes\nbetter than a small model when trained directly on the data and labels.\nHowever, a small model can be trained to generalize better with help of a large model.\nSmaller models are better in production: faster, less compute, less memory.\n\nThe output probabilities of a trained model give more information than the labels\nbecause it assigns non-zero probabilities to incorrect classes as well.\nThese probabilities tell us that a sample has a chance of belonging to certain classes.\nFor instance, when classifying digits, when given an image of digit *7*,\na generalized model will give a high probability to 7 and a small but non-zero\nprobability to 2, while assigning almost zero probability to other digits.\nDistillation uses this information to train a small model better.\n\n## Soft Targets\n\nThe probabilities are usually computed with a softmax operation,\n\n$$q_i = \\frac{\\exp (z_i)}{\\sum_j \\exp (z_j)}$$\n\nwhere $q_i$ is the probability for class $i$ and $z_i$ is the logit.\n\nWe train the small model to minimize the Cross entropy or KL Divergence between its output\nprobability distribution and the large network's output probability distribution\n(soft targets).\n\nOne of the problems here is that the probabilities assigned to incorrect classes by the\nlarge network are often very small and don't contribute to the loss.\nSo they soften the probabilities by applying a temperature $T$,\n\n$$q_i = \\frac{\\exp (\\frac{z_i}{T})}{\\sum_j \\exp (\\frac{z_j}{T})}$$\n\nwhere higher values for $T$ will produce softer probabilities.\n\n## Training\n\nPaper suggests adding a second loss term for predicting the actual labels\nwhen training the small model.\nWe calculate the composite loss as the weighted sum of the two loss terms:\n soft targets and actual labels.\n\nThe dataset for distillation is called *the transfer set*, and the paper\nsuggests using the same training data.\n\n## Our experiment\n\nWe train on CIFAR-10 dataset.\nWe [train a large model](large.html) that has $14,728,266$ parameters\nwith dropout and it gives an accuracy of 85% on the validation set.\nA [small model](small.html) with $437,034$ parameters\ngives an accuracy of 80%.\n\nWe then train the small model with distillation from the large model,\nand it gives an accuracy of 82%; a 2% increase in the accuracy.\n\"\"\"\n\nimport torch\nimport torch.nn.functional\nfrom torch import nn\n\nfrom labml import experiment, tracker\nfrom labml.configs import option\nfrom labml_helpers.train_valid import BatchIndex\nfrom labml_nn.distillation.large import LargeModel\nfrom labml_nn.distillation.small import SmallModel\nfrom labml_nn.experiments.cifar10 import CIFAR10Configs\n\n\nclass Configs(CIFAR10Configs):\n    \"\"\"\n    ## Configurations\n\n    This extends from [`CIFAR10Configs`](../experiments/cifar10.html) which defines all the\n    dataset related configurations, optimizer, and a training loop.\n    \"\"\"\n    # The small model\n    model: SmallModel\n    # The large model\n    large: LargeModel\n    # KL Divergence loss for soft targets\n    kl_div_loss = nn.KLDivLoss(log_target=True)\n    # Cross entropy loss for true label loss\n    loss_func = nn.CrossEntropyLoss()\n    # Temperature, $T$\n    temperature: float = 5.\n    # Weight for soft targets loss.\n    #\n    # The gradients produced by soft targets get scaled by $\\frac{1}{T^2}$.\n    # To compensate for this the paper suggests scaling the soft targets loss\n    # by a factor of $T^2$\n    soft_targets_weight: float = 100.\n    # Weight for true label cross entropy loss\n    label_loss_weight: float = 0.5\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training/validation step\n\n        We define a custom training/validation step to include the distillation\n        \"\"\"\n\n        # Training/Evaluation mode for the small model\n        self.model.train(self.mode.is_train)\n        # Large model in evaluation mode\n        self.large.eval()\n\n        # Move data to the device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Update global step (number of samples processed) when in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Get the output logits, $v_i$, from the large model\n        with torch.no_grad():\n            large_logits = self.large(data)\n\n        # Get the output logits, $z_i$, from the small model\n        output = self.model(data)\n\n        # Soft targets\n        # $$p_i = \\frac{\\exp (\\frac{v_i}{T})}{\\sum_j \\exp (\\frac{v_j}{T})}$$\n        soft_targets = nn.functional.log_softmax(large_logits / self.temperature, dim=-1)\n        # Temperature adjusted probabilities of the small model\n        # $$q_i = \\frac{\\exp (\\frac{z_i}{T})}{\\sum_j \\exp (\\frac{z_j}{T})}$$\n        soft_prob = nn.functional.log_softmax(output / self.temperature, dim=-1)\n\n        # Calculate the soft targets loss\n        soft_targets_loss = self.kl_div_loss(soft_prob, soft_targets)\n        # Calculate the true label loss\n        label_loss = self.loss_func(output, target)\n        # Weighted sum of the two losses\n        loss = self.soft_targets_weight * soft_targets_loss + self.label_loss_weight * label_loss\n        # Log the losses\n        tracker.add({\"loss.kl_div.\": soft_targets_loss,\n                     \"loss.nll\": label_loss,\n                     \"loss.\": loss})\n\n        # Calculate and log accuracy\n        self.accuracy(output, target)\n        self.accuracy.track()\n\n        # Train the model\n        if self.mode.is_train:\n            # Calculate gradients\n            loss.backward()\n            # Take optimizer step\n            self.optimizer.step()\n            # Log the model parameters and gradients on last batch of every epoch\n            if batch_idx.is_last:\n                tracker.add('model', self.model)\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save the tracked metrics\n        tracker.save()\n\n\n@option(Configs.large)\ndef _large_model(c: Configs):\n    \"\"\"\n    ### Create large model\n    \"\"\"\n    return LargeModel().to(c.device)\n\n\n@option(Configs.model)\ndef _small_student_model(c: Configs):\n    \"\"\"\n    ### Create small model\n    \"\"\"\n    return SmallModel().to(c.device)\n\n\ndef get_saved_model(run_uuid: str, checkpoint: int):\n    \"\"\"\n    ### Load [trained large model](large.html)\n    \"\"\"\n\n    from labml_nn.distillation.large import Configs as LargeConfigs\n\n    # In evaluation mode (no recording)\n    experiment.evaluate()\n    # Initialize configs of the large model training experiment\n    conf = LargeConfigs()\n    # Load saved configs\n    experiment.configs(conf, experiment.load_configs(run_uuid))\n    # Set models for saving/loading\n    experiment.add_pytorch_models({'model': conf.model})\n    # Set which run and checkpoint to load\n    experiment.load(run_uuid, checkpoint)\n    # Start the experiment - this will load the model, and prepare everything\n    experiment.start()\n\n    # Return the model\n    return conf.model\n\n\ndef main(run_uuid: str, checkpoint: int):\n    \"\"\"\n    Train a small model with distillation\n    \"\"\"\n    # Load saved model\n    large_model = get_saved_model(run_uuid, checkpoint)\n    # Create experiment\n    experiment.create(name='distillation', comment='cifar10')\n    # Create configurations\n    conf = Configs()\n    # Set the loaded large model\n    conf.large = large_model\n    # Load configurations\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n        'model': '_small_student_model',\n    })\n    # Set model for saving/loading\n    experiment.add_pytorch_models({'model': conf.model})\n    # Start experiment from scratch\n    experiment.load(None, None)\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main('d46cd53edaec11eb93c38d6538aee7d6', 1_000_000)\n", "labml_nn/RWKV/experiment.py": "import inspect\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom labml_nn.RWKV.configs import RWKVConfigs\n\nfrom labml_nn.RWKV import RWKV\nfrom labml_nn.RWKV import TimeMixing\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`NLPAutoRegressionConfigs`](../../experiments/nlp_autoregression.html#NLPAutoRegressionConfigs)\n    \"\"\"\n\n    # RWKV model\n    model: RWKV\n\n    rwkv: RWKVConfigs\n    # number of warmup iterations\n    warmup_iters: int = 2000\n    # total number of training iterations\n    max_iters: int = 600000\n    # weight decay\n    weight_decay: float = 1e-1\n    # Custom optimizer\n    beta1: float = 0.9\n    beta2: float = 0.95\n    optimizer = 'rwkv_optimizer'\n\n\n@option(Configs.rwkv, 'RWKV')\ndef _rwkv_configs(c: Configs):\n    \"\"\"\n    ### RWKV configurations\n    \"\"\"\n\n    # We use our\n    # [configurable RWKV implementation](../configs.html#RWKVConfigs)\n    conf = RWKVConfigs()\n    # Set the vocabulary sizes for embeddings and generating logits\n    conf.n_src_vocab = c.n_tokens\n    conf.n_tgt_vocab = c.n_tokens\n\n    return conf\n\n\ndef _init_weights(module, rwkv: RWKVConfigs):\n    # initialize Vector Parameters in TimeMixing\n    if isinstance(module, TimeMixing):\n        layer_id = module.layer_id\n        n_layer = module.n_layer\n        n_embd = module.n_embd\n        attn_sz = n_embd\n\n        with torch.no_grad():\n            ratio_0_to_1 = layer_id / (n_layer - 1)  # 0 to 1\n            ratio_1_to_almost0 = 1.0 - (layer_id / n_layer)  # 1 to ~0\n            ddd = torch.ones(1, 1, n_embd)\n            for i in range(n_embd):\n                ddd[0, 0, i] = i / n_embd\n\n            decay_speed = torch.ones(attn_sz)\n            for h in range(attn_sz):\n                decay_speed[h] = -5 + 8 * (h / (attn_sz - 1)) ** (0.7 + 1.3 * ratio_0_to_1)\n            module.time_decay = nn.Parameter(decay_speed)\n\n            zigzag = torch.tensor([(i + 1) % 3 - 1 for i in range(attn_sz)]) * 0.5\n            module.time_first = nn.Parameter(torch.ones(attn_sz) * math.log(0.3) + zigzag)\n            module.time_mix_key = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n            module.time_mix_value = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)\n            module.time_mix_receptance = nn.Parameter(torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    Create RWKV model and initialize weights\n    \"\"\"\n    m = RWKV(c.rwkv).to(c.device)\n\n    # Apply custom weight initialization\n    m.apply(_init_weights, c.rwkv)\n\n    return m\n\n\n@option(NLPAutoRegressionConfigs.optimizer)\ndef _configure_optimizers(c: NLPAutoRegressionConfigs):\n    # start with all of the candidate parameters\n    param_dict = {pn: p for pn, p in c.model.named_parameters()}\n    # filter out those that do not require grad\n    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n    # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n    # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n    optim_groups = [\n        {'params': decay_params, 'weight_decay': c.weight_decay},\n        {'params': nodecay_params, 'weight_decay': 0.0}\n    ]\n    num_decay_params = sum(p.numel() for p in decay_params)\n    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n    print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n    print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n    # Create AdamW optimizer and use the fused version if it is available\n    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n    use_fused = fused_available and c.device_type == 'cuda'\n    extra_args = dict(fused=True) if use_fused else dict()\n    optimizer = torch.optim.AdamW(optim_groups, lr=c.learning_rate, betas=c.betas, **extra_args)\n    print(f\"using fused AdamW: {use_fused}\")\n\n    return optimizer\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"RWKV\")\n    # Create configs\n    conf = Configs()\n    print(conf.model)\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $128$\n        'seq_len': 128,\n        # Train for $32$ epochs\n        'epochs': 32,\n        # Batch size $128$\n        'batch_size': 128,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        'rwkv.block_size': 1024,\n        # model\n        'rwkv.n_layer': 12,\n        'rwkv.n_heads': 12,\n        'rwkv.n_embd': 768\n    })\n\n    print(conf.model)\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/RWKV/configs.py": "from labml.configs import BaseConfigs\n\n\nclass RWKVConfigs(BaseConfigs):\n    \"\"\"\n    ## Transformer Configurations\n\n    This defines configurations for a transformer.\n    The configurations are calculate using option functions.\n    These are lazy loaded and therefore only the necessary modules\n    are calculated.\n    \"\"\"\n    # Number of attention heads\n    n_heads: int = 8\n    # Transformer embedding size\n    d_model: int = 512\n    # Number of layers\n    n_layers: int = 6\n    # Dropout probability\n    dropout: float = 0.1\n    # Number of tokens in the source vocabulary (for token embeddings)\n    n_src_vocab: int\n    # Number of tokens in the target vocabulary (to generate logits for prediction)\n    n_tgt_vocab: int\n", "labml_nn/RWKV/__init__.py": "\"\"\"\n\n---\ntitle: Receptance Weighted Key Value (RWKV)\nsummary: >\n  This implements the RWKV model \n  using PyTorch with explanations.\n---\n\n# Receptance Weighted Key Value (RWKV)\n\nThis is a tutorial/implementation of RWKV\nfrom paper [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/pdf/2305.13048.pdf)\nin [PyTorch](https://pytorch.org/).\n\nFull definition of a RWKV Language Model, all of it in this single file.\nReferences:\n1) [the official RWKV PyTorch implementation released by Bo Peng](https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v4neo/src/model.py)\n2) [huggingface/transformers PyTorch implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/rwkv/modeling_rwkv.py)\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom labml_helpers.module import Module\n\nPREV_X_TIME = 0\nNUM_STATE = 1\nDEN_STATE = 2\nMAX_STATE = 3\nPREV_X_CHANNEL = 4\n\n\nclass LayerNorm(Module):\n    \"\"\"\n    ### Layer normalization with bias\n    \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\n\nclass L2Wrap(torch.autograd.Function):\n    \"\"\"\n    ### L2 loss wrapper\n\n    [ref](https://github.com/BlinkDL/RWKV-LM/blob/cca1b5e8e597cf40675882bb10b46287c844e35c/RWKV-v4/src/model.py#L21)\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, loss, y):\n        ctx.save_for_backward(y)\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        y = ctx.saved_tensors[0]\n        # to encourage the logits to be close to 0\n        factor = 1e-4 / (y.shape[0] * y.shape[1])\n        maxx, ids = torch.max(y, -1, keepdim=True)\n        gy = torch.zeros_like(y)\n        gy.scatter_(-1, ids, maxx * factor)\n        return grad_output, gy\n\n\nclass ChannelMixing(Module):\n    \"\"\"\n    ### Channel Mixing\n    \"\"\"\n\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        # token shifting\n        self.layer_id = layer_id\n\n        n_embd = config.n_embd\n        intermediate_size = (\n            config.intermediate_size if config.intermediate_size is not None else 4 * n_embd\n        )\n\n        # Learnable Matrix\n        self.key_proj = nn.Linear(n_embd, intermediate_size, bias=False)\n        self.value_proj = nn.Linear(intermediate_size, n_embd, bias=False)\n        self.receptance_proj = nn.Linear(n_embd, n_embd, bias=False)\n\n        # Learnable Vector\n        self.time_mix_key = nn.Parameter(torch.empty(1, 1, n_embd))\n        self.time_mix_receptance = nn.Parameter(torch.empty(1, 1, n_embd))\n\n    def forward(self, x, state=None):\n        \"\"\"\n        # x = (Batch,Time,Channel)\n        \"\"\"\n        if state is not None:\n            prev_x = state[self.layer_id, :, [PREV_X_CHANNEL], :]\n            state[self.layer_id, :, [PREV_X_CHANNEL], :] = x\n        else:\n            prev_x = self.time_shift(x)\n\n        # $r_t=W_r \\cdot (\\mu_r x_t + (1-\\mu_r)x_{t-1})$\n        receptance = x * self.time_mix_receptance + prev_x * (1 - self.time_mix_receptance)\n        receptance = self.receptance_proj(receptance)\n\n        # $k_t=W_k \\cdot (\\mu_k x_t + (1-\\mu_k)x_{t-1})$\n        key = x * self.time_mix_key + prev_x * (1 - self.time_mix_key)\n        key = self.key_proj(key)\n\n        # $V_t=W_v \\cdot max(k_t,0)^2$\n        value = self.value_proj(torch.square(torch.relu(key)))\n\n        # $o_t=\\sigma(r_t) \\odot v_t$\n        out = F.sigmoid(receptance) * value\n        return out, state\n\n\nclass TimeMixing(Module):\n    \"\"\"\n    ### Time Mixing\n    \"\"\"\n\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.config = config\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        self.layer_id = layer_id\n\n        n_embd = config.n_embd\n        attn_sz = n_embd\n\n        # learnable matrix\n        self.key_proj = nn.Linear(n_embd, attn_sz, bias=False)\n        self.value_proj = nn.Linear(n_embd, attn_sz, bias=False)\n        self.receptance_proj = nn.Linear(n_embd, attn_sz, bias=False)\n        self.output_proj = nn.Linear(attn_sz, n_embd, bias=False)\n\n        # learnable vector\n        self.time_decay = nn.Parameter(torch.empty(attn_sz))\n        self.time_first = nn.Parameter(torch.empty(attn_sz))\n        self.time_mix_key = nn.Parameter(torch.empty(1, 1, n_embd))\n        self.time_mix_value = nn.Parameter(torch.empty(1, 1, n_embd))\n        self.time_mix_receptance = nn.Parameter(torch.empty(1, 1, n_embd))\n\n    def forward(self, x, state=None):\n        \"\"\"\n        x = (Batch,Time,Channel)\n        \"\"\"\n        if state is not None:\n            prev_x = state[self.layer_id, :, [PREV_X_TIME], :]\n            state[self.layer_id, :, [PREV_X_TIME], :] = x\n        else:\n            prev_x = self.time_shift(x)\n\n        # $r_t=W_r \\cdot (\\mu_r x_t + (1-\\mu_r)x_{t-1})$\n        receptance = x * self.time_mix_receptance + prev_x * (1 - self.time_mix_receptance)\n        receptance = self.receptance_proj(receptance)\n\n        # $k_t=W_k \\cdot (\\mu_k x_t + (1-\\mu_k)x_{t-1})$\n        key = x * self.time_mix_key + prev_x * (1 - self.time_mix_key)\n        key = self.key_proj(key)\n\n        # $v_t=W_v \\cdot (\\mu_v x_t + (1-\\mu_v)x_{t-1})$\n        value = x * self.time_mix_value + prev_x * (1 - self.time_mix_value)\n        value = self.value_proj(value)\n\n        # WKV calculation\n        _, seq_length, _ = key.size()\n        output = torch.zeros_like(key)\n\n        if state is None:\n            num_state = torch.zeros_like(key[:, 0], dtype=torch.float32)\n            den_state = torch.zeros_like(key[:, 0], dtype=torch.float32)\n            max_state = torch.zeros_like(key[:, 0], dtype=torch.float32) - 1e38\n        else:\n            num_state = state[self.layer_id, :, NUM_STATE, :]\n            den_state = state[self.layer_id, :, DEN_STATE, :]\n            max_state = state[self.layer_id, :, MAX_STATE, :]\n\n        time_decay = -torch.exp(self.time_decay)\n\n        for current_index in range(seq_length):\n            current_key = key[:, current_index].float()\n            current_value = value[:, current_index]\n\n            # $wkv_t=\\frac{\\sum^{t-1}_{i=1}d^{-(t-1-i)w+k_i}v_i+e^{u+k_t}v_t}{\\sum^{t-1}_{i=1}e^{-(t-1-i)w+k_i}+e^{u+k_t}}$\n            max_for_output = torch.maximum(max_state, current_key + self.time_first)\n            e1 = torch.exp(max_state - max_for_output)\n            e2 = torch.exp(current_key + self.time_first - max_for_output)\n            numerator = e1 * num_state + e2 * current_value\n            denominator = e1 * den_state + e2\n            output[:, current_index] = (numerator / denominator).to(output.dtype)\n\n            # Update state for next iteration\n            max_for_state = torch.maximum(max_state + time_decay, current_key)\n            e1 = torch.exp(max_state + time_decay - max_for_state)\n            e2 = torch.exp(current_key - max_for_state)\n            num_state = e1 * num_state + e2 * current_value\n            den_state = e1 * den_state + e2\n            max_state = max_for_state\n\n        # update states\n        state[self.layer_id, :, NUM_STATE, :] = num_state\n        state[self.layer_id, :, DEN_STATE, :] = den_state\n        state[self.layer_id, :, MAX_STATE, :] = max_state\n        wkv, state = self.wkv_function(key, value, use_customized_cuda_kernel=self.config.use_customized_cuda_kernel,\n                                       state=state)\n\n        # $o_t=W_o \\cdot (\\sigma(r_t) \\odot wkv_t)$\n        rwkv = F.sigmoid(receptance) * wkv\n        rwkv = self.output_proj(rwkv)\n\n        return rwkv, state\n\n\nclass Block(Module):\n    \"\"\"\n    ## RWKV block element\n    \"\"\"\n\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = TimeMixing(config, layer_id)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.ffn = ChannelMixing(config, layer_id)\n\n    def forward(self, x, state=None):\n        # state: [batch_size, 5 , n_embd]\n\n        # time mixing\n        residual = x\n        x, state = self.attn(self.ln_1(x), state=state)\n        x = x + residual\n\n        # channel mixing\n        residual = x\n        x, state = self.ffn(self.ln_2(x), state=state)\n        x = x + residual\n        return x, state\n\n\nclass RWKV(Module):\n    \"\"\"\n    ## RWKV\n    \"\"\"\n    def __init__(self, config, lr_init=0.0008):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n        self.lr_init = lr_init  ## used to initialize embedding parameters\n        self.n_layer = config.n_layer\n        self.n_embd = config.n_embd\n\n        # Initiate model layers\n        self.rwkv = nn.ModuleDict(dict(\n            wte=nn.Embedding(config.vocab_size, config.n_embd),\n            ln_p=LayerNorm(config.n_embd, bias=config.bias),\n            h=nn.ModuleList([Block(config, layer_id) for layer_id in range(config.n_layer)]),\n            ln_f=LayerNorm(config.n_embd, bias=config.bias),\n        ))\n\n        # Output linear layer\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n    def forward(self, idx, targets=None, state=None, return_state=False):\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\n        # Embedding Layer\n        x = self.rwkv.wte(idx)\n\n        # Layer Norm\n        x = self.rwkv.ln_p(x)\n\n        # RWKV Blocks\n        for block_idx, block in enumerate(self.rwkv.h):\n            x, state = block(x, state)\n        x = self.rwkv.ln_f(x)\n\n        # Logit Layer and loss Function (for training)\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n            if self.training:\n                loss = L2Wrap.apply(loss, logits)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :])  # note: using list [-1] to preserve the time dim\n            loss = None\n\n        # Return Logits and loss\n        if return_state:\n            return logits, loss, state\n        else:\n            return logits, loss\n", "labml_nn/capsule_networks/mnist.py": "\"\"\"\n---\ntitle: Classify MNIST digits with Capsule Networks\nsummary: Code for training Capsule Networks on MNIST dataset\n---\n\n# Classify MNIST digits with Capsule Networks\n\nThis is an annotated PyTorch code to classify MNIST digits with PyTorch.\n\nThis paper implements the experiment described in paper\n[Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829).\n\"\"\"\nfrom typing import Any\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\n\nfrom labml import experiment, tracker\nfrom labml.configs import option\nfrom labml_helpers.datasets.mnist import MNISTConfigs\nfrom labml_helpers.metrics.accuracy import AccuracyDirect\nfrom labml_helpers.module import Module\nfrom labml_helpers.train_valid import SimpleTrainValidConfigs, BatchIndex\nfrom labml_nn.capsule_networks import Squash, Router, MarginLoss\n\n\nclass MNISTCapsuleNetworkModel(Module):\n    \"\"\"\n    ## Model for classifying MNIST digits\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # First convolution layer has $256$, $9 \\times 9$ convolution kernels\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9, stride=1)\n        # The second layer (Primary Capsules) s a convolutional capsule layer with $32$ channels\n        # of convolutional $8D$ capsules ($8$ features per capsule).\n        # That is, each primary capsule contains 8 convolutional units with a 9 \u00d7 9 kernel and a stride of 2.\n        # In order to implement this we create a convolutional layer with $32 \\times 8$ channels and\n        # reshape and permutate its output to get the capsules of $8$ features each.\n        self.conv2 = nn.Conv2d(in_channels=256, out_channels=32 * 8, kernel_size=9, stride=2, padding=0)\n        self.squash = Squash()\n\n        # Routing layer gets the $32 \\times 6 \\times 6$ primary capsules and produces $10$ capsules.\n        # Each of the primary capsules have $8$ features, while output capsules (Digit Capsules)\n        # have $16$ features.\n        # The routing algorithm iterates $3$ times.\n        self.digit_capsules = Router(32 * 6 * 6, 10, 8, 16, 3)\n\n        # This is the decoder mentioned in the paper.\n        # It takes the outputs of the $10$ digit capsules, each with $16$ features to reproduce the\n        # image. It goes through linear layers of sizes $512$ and $1024$ with $ReLU$ activations.\n        self.decoder = nn.Sequential(\n            nn.Linear(16 * 10, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 784),\n            nn.Sigmoid()\n        )\n\n    def forward(self, data: torch.Tensor):\n        \"\"\"\n        `data` are the MNIST images, with shape `[batch_size, 1, 28, 28]`\n        \"\"\"\n        # Pass through the first convolution layer.\n        # Output of this layer has shape `[batch_size, 256, 20, 20]`\n        x = F.relu(self.conv1(data))\n        # Pass through the second convolution layer.\n        # Output of this has shape `[batch_size, 32 * 8, 6, 6]`.\n        # *Note that this layer has a stride length of $2$*.\n        x = self.conv2(x)\n\n        # Resize and permutate to get the capsules\n        caps = x.view(x.shape[0], 8, 32 * 6 * 6).permute(0, 2, 1)\n        # Squash the capsules\n        caps = self.squash(caps)\n        # Take them through the router to get digit capsules.\n        # This has shape `[batch_size, 10, 16]`.\n        caps = self.digit_capsules(caps)\n\n        # Get masks for reconstructioon\n        with torch.no_grad():\n            # The prediction by the capsule network is the capsule with longest length\n            pred = (caps ** 2).sum(-1).argmax(-1)\n            # Create a mask to maskout all the other capsules\n            mask = torch.eye(10, device=data.device)[pred]\n\n        # Mask the digit capsules to get only the capsule that made the prediction and\n        # take it through decoder to get reconstruction\n        reconstructions = self.decoder((caps * mask[:, :, None]).view(x.shape[0], -1))\n        # Reshape the reconstruction to match the image dimensions\n        reconstructions = reconstructions.view(-1, 1, 28, 28)\n\n        return caps, reconstructions, pred\n\n\nclass Configs(MNISTConfigs, SimpleTrainValidConfigs):\n    \"\"\"\n    Configurations with MNIST data and Train & Validation setup\n    \"\"\"\n    epochs: int = 10\n    model: nn.Module = 'capsule_network_model'\n    reconstruction_loss = nn.MSELoss()\n    margin_loss = MarginLoss(n_labels=10)\n    accuracy = AccuracyDirect()\n\n    def init(self):\n        # Print losses and accuracy to screen\n        tracker.set_scalar('loss.*', True)\n        tracker.set_scalar('accuracy.*', True)\n\n        # We need to set the metrics to calculate them for the epoch for training and validation\n        self.state_modules = [self.accuracy]\n\n    def step(self, batch: Any, batch_idx: BatchIndex):\n        \"\"\"\n        This method gets called by the trainer\n        \"\"\"\n        # Set the model mode\n        self.model.train(self.mode.is_train)\n\n        # Get the images and labels and move them to the model's device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Increment step in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Whether to log activations\n        with self.mode.update(is_log_activations=batch_idx.is_last):\n            # Run the model\n            caps, reconstructions, pred = self.model(data)\n\n        # Calculate the total loss\n        loss = self.margin_loss(caps, target) + 0.0005 * self.reconstruction_loss(reconstructions, data)\n        tracker.add(\"loss.\", loss)\n\n        # Call accuracy metric\n        self.accuracy(pred, target)\n\n        if self.mode.is_train:\n            loss.backward()\n\n            self.optimizer.step()\n            # Log parameters and gradients\n            if batch_idx.is_last:\n                tracker.add('model', self.model)\n            self.optimizer.zero_grad()\n\n            tracker.save()\n\n\n@option(Configs.model)\ndef capsule_network_model(c: Configs):\n    \"\"\"Set the model\"\"\"\n    return MNISTCapsuleNetworkModel().to(c.device)\n\n\ndef main():\n    \"\"\"\n    Run the experiment\n    \"\"\"\n    experiment.create(name='capsule_network_mnist')\n    conf = Configs()\n    experiment.configs(conf, {'optimizer.optimizer': 'Adam',\n                              'optimizer.learning_rate': 1e-3})\n\n    experiment.add_pytorch_models({'model': conf.model})\n\n    with experiment.start():\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/capsule_networks/__init__.py": "\"\"\"\n---\ntitle: Capsule Networks\nsummary: >\n  PyTorch implementation and tutorial of Capsule Networks.\n  Capsule network is a neural network architecture that embeds features\n  as capsules and routes them with a voting mechanism to next layer of capsules.\n---\n\n# Capsule Networks\n\nThis is a [PyTorch](https://pytorch.org) implementation/tutorial of\n[Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829).\n\nCapsule network is a neural network architecture that embeds features\nas capsules and routes them with a voting mechanism to next layer of capsules.\n\nUnlike in other implementations of models, we've included a sample, because\nit is difficult to understand some concepts with just the modules.\n[This is the annotated code for a model that uses capsules to classify MNIST dataset](mnist.html)\n\nThis file holds the implementations of the core modules of Capsule Networks.\n\nI used [jindongwang/Pytorch-CapsuleNet](https://github.com/jindongwang/Pytorch-CapsuleNet) to clarify some\nconfusions I had with the paper.\n\nHere's a notebook for training a Capsule Network on MNIST dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/capsule_networks/mnist.ipynb)\n\"\"\"\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\n\nfrom labml_helpers.module import Module\n\n\nclass Squash(Module):\n    \"\"\"\n    ## Squash\n\n    This is **squashing** function from paper, given by equation $(1)$.\n\n    $$\\mathbf{v}_j = \\frac{{\\lVert \\mathbf{s}_j \\rVert}^2}{1 + {\\lVert \\mathbf{s}_j \\rVert}^2}\n     \\frac{\\mathbf{s}_j}{\\lVert \\mathbf{s}_j \\rVert}$$\n\n    $\\frac{\\mathbf{s}_j}{\\lVert \\mathbf{s}_j \\rVert}$\n    normalizes the length of all the capsules, whilst\n    $\\frac{{\\lVert \\mathbf{s}_j \\rVert}^2}{1 + {\\lVert \\mathbf{s}_j \\rVert}^2}$\n    shrinks the capsules that have a length smaller than one .\n    \"\"\"\n\n    def __init__(self, epsilon=1e-8):\n        super().__init__()\n        self.epsilon = epsilon\n\n    def forward(self, s: torch.Tensor):\n        \"\"\"\n        The shape of `s` is `[batch_size, n_capsules, n_features]`\n        \"\"\"\n\n        # ${\\lVert \\mathbf{s}_j \\rVert}^2$\n        s2 = (s ** 2).sum(dim=-1, keepdims=True)\n\n        # We add an epsilon when calculating $\\lVert \\mathbf{s}_j \\rVert$ to make sure it doesn't become zero.\n        # If this becomes zero it starts giving out `nan` values and training fails.\n        # $$\\mathbf{v}_j = \\frac{{\\lVert \\mathbf{s}_j \\rVert}^2}{1 + {\\lVert \\mathbf{s}_j \\rVert}^2}\n        # \\frac{\\mathbf{s}_j}{\\sqrt{{\\lVert \\mathbf{s}_j \\rVert}^2 + \\epsilon}}$$\n        return (s2 / (1 + s2)) * (s / torch.sqrt(s2 + self.epsilon))\n\n\nclass Router(Module):\n    \"\"\"\n    ## Routing Algorithm\n\n    This is the routing mechanism described in the paper.\n    You can use multiple routing layers in your models.\n\n    This combines calculating $\\mathbf{s}_j$ for this layer and\n    the routing algorithm described in *Procedure 1*.\n    \"\"\"\n\n    def __init__(self, in_caps: int, out_caps: int, in_d: int, out_d: int, iterations: int):\n        \"\"\"\n        `in_caps` is the number of capsules, and `in_d` is the number of features per capsule from the layer below.\n        `out_caps` and `out_d` are the same for this layer.\n\n        `iterations` is the number of routing iterations, symbolized by $r$ in the paper.\n        \"\"\"\n        super().__init__()\n        self.in_caps = in_caps\n        self.out_caps = out_caps\n        self.iterations = iterations\n        self.softmax = nn.Softmax(dim=1)\n        self.squash = Squash()\n\n        # This is the weight matrix $\\mathbf{W}_{ij}$. It maps each capsule in the\n        # lower layer to each capsule in this layer\n        self.weight = nn.Parameter(torch.randn(in_caps, out_caps, in_d, out_d), requires_grad=True)\n\n    def forward(self, u: torch.Tensor):\n        \"\"\"\n        The shape of `u` is `[batch_size, n_capsules, n_features]`.\n        These are the capsules from the lower layer.\n        \"\"\"\n\n        # $$\\hat{\\mathbf{u}}_{j|i} = \\mathbf{W}_{ij} \\mathbf{u}_i$$\n        # Here $j$ is used to index capsules in this layer, whilst $i$ is\n        # used to index capsules in the layer below (previous).\n        u_hat = torch.einsum('ijnm,bin->bijm', self.weight, u)\n\n        # Initial logits $b_{ij}$ are the log prior probabilities that capsule $i$\n        # should be coupled with $j$.\n        # We initialize these at zero\n        b = u.new_zeros(u.shape[0], self.in_caps, self.out_caps)\n\n        v = None\n\n        # Iterate\n        for i in range(self.iterations):\n            # routing softmax $$c_{ij} = \\frac{\\exp({b_{ij}})}{\\sum_k\\exp({b_{ik}})}$$\n            c = self.softmax(b)\n            # $$\\mathbf{s}_j = \\sum_i{c_{ij} \\hat{\\mathbf{u}}_{j|i}}$$\n            s = torch.einsum('bij,bijm->bjm', c, u_hat)\n            # $$\\mathbf{v}_j = squash(\\mathbf{s}_j)$$\n            v = self.squash(s)\n            # $$a_{ij} = \\mathbf{v}_j \\cdot \\hat{\\mathbf{u}}_{j|i}$$\n            a = torch.einsum('bjm,bijm->bij', v, u_hat)\n            # $$b_{ij} \\gets b_{ij} + \\mathbf{v}_j \\cdot \\hat{\\mathbf{u}}_{j|i}$$\n            b = b + a\n\n        return v\n\n\nclass MarginLoss(Module):\n    \"\"\"\n    ## Margin loss for class existence\n\n    A separate margin loss is used for each output capsule and the total loss is the sum of them.\n    The length of each output capsule is the probability that class is present in the input.\n\n    Loss for each output capsule or class $k$ is,\n    $$\\mathcal{L}_k = T_k \\max(0, m^{+} - \\lVert\\mathbf{v}_k\\rVert)^2 +\n    \\lambda (1 - T_k) \\max(0, \\lVert\\mathbf{v}_k\\rVert - m^{-})^2$$\n\n    $T_k$ is $1$ if the class $k$ is present and $0$ otherwise.\n    The first component of the loss is $0$ when the class is not present,\n    and the second component is $0$ if the class is present.\n    The $\\max(0, x)$ is used to avoid predictions going to extremes.\n    $m^{+}$ is set to be $0.9$ and $m^{-}$ to be $0.1$ in the paper.\n\n    The $\\lambda$ down-weighting is used to stop the length of all capsules from\n    falling during the initial phase of training.\n    \"\"\"\n    def __init__(self, *, n_labels: int, lambda_: float = 0.5, m_positive: float = 0.9, m_negative: float = 0.1):\n        super().__init__()\n\n        self.m_negative = m_negative\n        self.m_positive = m_positive\n        self.lambda_ = lambda_\n        self.n_labels = n_labels\n\n    def forward(self, v: torch.Tensor, labels: torch.Tensor):\n        \"\"\"\n        `v`, $\\mathbf{v}_j$ are the squashed output capsules.\n        This has shape `[batch_size, n_labels, n_features]`; that is, there is a capsule for each label.\n\n        `labels` are the labels, and has shape `[batch_size]`.\n        \"\"\"\n        # $$\\lVert \\mathbf{v}_j \\rVert$$\n        v_norm = torch.sqrt((v ** 2).sum(dim=-1))\n\n        # $$\\mathcal{L}$$\n        # `labels` is one-hot encoded labels of shape `[batch_size, n_labels]`\n        labels = torch.eye(self.n_labels, device=labels.device)[labels]\n\n        # $$\\mathcal{L}_k = T_k \\max(0, m^{+} - \\lVert\\mathbf{v}_k\\rVert)^2 +\n        # \\lambda (1 - T_k) \\max(0, \\lVert\\mathbf{v}_k\\rVert - m^{-})^2$$\n        # `loss` has shape `[batch_size, n_labels]`. We have parallelized the computation\n        # of $\\mathcal{L}_k$ for for all $k$.\n        loss = labels * F.relu(self.m_positive - v_norm) + \\\n               self.lambda_ * (1.0 - labels) * F.relu(v_norm - self.m_negative)\n\n        # $$\\sum_k \\mathcal{L}_k$$\n        return loss.sum(dim=-1).mean()\n", "labml_nn/unet/experiment.py": "\"\"\"\n---\ntitle: Training a U-Net on Carvana dataset\nsummary: >\n  Code for training a U-Net model on Carvana dataset.\n---\n\n# Training [U-Net](index.html)\n\nThis trains a [U-Net](index.html) model on [Carvana dataset](carvana.html).\nYou can find the download instructions\n[on Kaggle](https://www.kaggle.com/competitions/carvana-image-masking-challenge/data).\n\nSave the training images inside `carvana/train` folder and the masks in `carvana/train_masks` folder.\n\nFor simplicity, we do not do a training and validation split.\n\"\"\"\n\nimport numpy as np\nimport torch\nimport torch.utils.data\nimport torchvision.transforms.functional\nfrom torch import nn\n\nfrom labml import lab, tracker, experiment, monit\nfrom labml.configs import BaseConfigs\nfrom labml_helpers.device import DeviceConfigs\nfrom labml_nn.unet.carvana import CarvanaDataset\nfrom labml_nn.unet import UNet\n\n\nclass Configs(BaseConfigs):\n    \"\"\"\n    ## Configurations\n    \"\"\"\n    # Device to train the model on.\n    # [`DeviceConfigs`](https://docs.labml.ai/api/helpers.html#labml_helpers.device.DeviceConfigs)\n    #  picks up an available CUDA device or defaults to CPU.\n    device: torch.device = DeviceConfigs()\n\n    # [U-Net](index.html) model\n    model: UNet\n\n    # Number of channels in the image. $3$ for RGB.\n    image_channels: int = 3\n    # Number of channels in the output mask. $1$ for binary mask.\n    mask_channels: int = 1\n\n    # Batch size\n    batch_size: int = 1\n    # Learning rate\n    learning_rate: float = 2.5e-4\n\n    # Number of training epochs\n    epochs: int = 4\n\n    # Dataset\n    dataset: CarvanaDataset\n    # Dataloader\n    data_loader: torch.utils.data.DataLoader\n\n    # Loss function\n    loss_func = nn.BCELoss()\n    # Sigmoid function for binary classification\n    sigmoid = nn.Sigmoid()\n\n    # Adam optimizer\n    optimizer: torch.optim.Adam\n\n    def init(self):\n        # Initialize the [Carvana dataset](carvana.html)\n        self.dataset = CarvanaDataset(lab.get_data_path() / 'carvana' / 'train',\n                                      lab.get_data_path() / 'carvana' / 'train_masks')\n        # Initialize the model\n        self.model = UNet(self.image_channels, self.mask_channels).to(self.device)\n\n        # Create dataloader\n        self.data_loader = torch.utils.data.DataLoader(self.dataset, self.batch_size,\n                                                       shuffle=True, pin_memory=True)\n        # Create optimizer\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n\n        # Image logging\n        tracker.set_image(\"sample\", True)\n\n    @torch.no_grad()\n    def sample(self, idx=-1):\n        \"\"\"\n        ### Sample images\n        \"\"\"\n\n        # Get a random sample\n        x, _ = self.dataset[np.random.randint(len(self.dataset))]\n        # Move data to device\n        x = x.to(self.device)\n\n        # Get predicted mask\n        mask = self.sigmoid(self.model(x[None, :]))\n        # Crop the image to the size of the mask\n        x = torchvision.transforms.functional.center_crop(x, [mask.shape[2], mask.shape[3]])\n        # Log samples\n        tracker.save('sample', x * mask)\n\n    def train(self):\n        \"\"\"\n        ### Train for an epoch\n        \"\"\"\n\n        # Iterate through the dataset.\n        # Use [`mix`](https://docs.labml.ai/api/monit.html#labml.monit.mix)\n        # to sample $50$ times per epoch.\n        for _, (image, mask) in monit.mix(('Train', self.data_loader), (self.sample, list(range(50)))):\n            # Increment global step\n            tracker.add_global_step()\n            # Move data to device\n            image, mask = image.to(self.device), mask.to(self.device)\n\n            # Make the gradients zero\n            self.optimizer.zero_grad()\n            # Get predicted mask logits\n            logits = self.model(image)\n            # Crop the target mask to the size of the logits. Size of the logits will be smaller if we\n            # don't use padding in convolutional layers in the U-Net.\n            mask = torchvision.transforms.functional.center_crop(mask, [logits.shape[2], logits.shape[3]])\n            # Calculate loss\n            loss = self.loss_func(self.sigmoid(logits), mask)\n            # Compute gradients\n            loss.backward()\n            # Take an optimization step\n            self.optimizer.step()\n            # Track the loss\n            tracker.save('loss', loss)\n\n    def run(self):\n        \"\"\"\n        ### Training loop\n        \"\"\"\n        for _ in monit.loop(self.epochs):\n            # Train the model\n            self.train()\n            # New line in the console\n            tracker.new_line()\n            # Save the model\n            experiment.save_checkpoint()\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='unet')\n\n    # Create configurations\n    configs = Configs()\n\n    # Set configurations. You can override the defaults by passing the values in the dictionary.\n    experiment.configs(configs, {})\n\n    # Initialize\n    configs.init()\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': configs.model})\n\n    # Start and run the training loop\n    with experiment.start():\n        configs.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/unet/__init__.py": "\"\"\"\n---\ntitle: U-Net\nsummary: >\n    PyTorch implementation and tutorial of U-Net model.\n---\n\n# U-Net\n\nThis is an implementation of the U-Net model from the paper,\n[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597).\n\nU-Net consists of a contracting path and an expansive path.\nThe contracting path is a series of convolutional layers and pooling layers,\nwhere the resolution of the feature map gets progressively reduced.\nExpansive path is a series of up-sampling layers and convolutional layers\nwhere the resolution of the feature map gets progressively increased.\n\nAt every step in the expansive path the corresponding feature map from the contracting path\nconcatenated with the current feature map.\n\n![U-Net diagram from paper](unet.png)\n\nHere is the [training code](experiment.html) for an experiment that trains a U-Net\non [Carvana dataset](carvana.html).\n\"\"\"\nimport torch\nimport torchvision.transforms.functional\nfrom torch import nn\n\n\nclass DoubleConvolution(nn.Module):\n    \"\"\"\n    ### Two $3 \\times 3$ Convolution Layers\n\n    Each step in the contraction path and expansive path have two $3 \\times 3$\n    convolutional layers followed by ReLU activations.\n\n    In the U-Net paper they used $0$ padding,\n    but we use $1$ padding so that final feature map is not cropped.\n    \"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int):\n        \"\"\"\n        :param in_channels: is the number of input channels\n        :param out_channels: is the number of output channels\n        \"\"\"\n        super().__init__()\n\n        # First $3 \\times 3$ convolutional layer\n        self.first = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.act1 = nn.ReLU()\n        # Second $3 \\times 3$ convolutional layer\n        self.second = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.act2 = nn.ReLU()\n\n    def forward(self, x: torch.Tensor):\n        # Apply the two convolution layers and activations\n        x = self.first(x)\n        x = self.act1(x)\n        x = self.second(x)\n        return self.act2(x)\n\n\nclass DownSample(nn.Module):\n    \"\"\"\n    ### Down-sample\n\n    Each step in the contracting path down-samples the feature map with\n    a $2 \\times 2$ max pooling layer.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Max pooling layer\n        self.pool = nn.MaxPool2d(2)\n\n    def forward(self, x: torch.Tensor):\n        return self.pool(x)\n\n\nclass UpSample(nn.Module):\n    \"\"\"\n    ### Up-sample\n\n    Each step in the expansive path up-samples the feature map with\n    a $2 \\times 2$ up-convolution.\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n\n        # Up-convolution\n        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n\n    def forward(self, x: torch.Tensor):\n        return self.up(x)\n\n\nclass CropAndConcat(nn.Module):\n    \"\"\"\n    ### Crop and Concatenate the feature map\n\n    At every step in the expansive path the corresponding feature map from the contracting path\n    concatenated with the current feature map.\n    \"\"\"\n    def forward(self, x: torch.Tensor, contracting_x: torch.Tensor):\n        \"\"\"\n        :param x: current feature map in the expansive path\n        :param contracting_x: corresponding feature map from the contracting path\n        \"\"\"\n\n        # Crop the feature map from the contracting path to the size of the current feature map\n        contracting_x = torchvision.transforms.functional.center_crop(contracting_x, [x.shape[2], x.shape[3]])\n        # Concatenate the feature maps\n        x = torch.cat([x, contracting_x], dim=1)\n        #\n        return x\n\n\nclass UNet(nn.Module):\n    \"\"\"\n    ## U-Net\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int):\n        \"\"\"\n        :param in_channels: number of channels in the input image\n        :param out_channels: number of channels in the result feature map\n        \"\"\"\n        super().__init__()\n\n        # Double convolution layers for the contracting path.\n        # The number of features gets doubled at each step starting from $64$.\n        self.down_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in\n                                        [(in_channels, 64), (64, 128), (128, 256), (256, 512)]])\n        # Down sampling layers for the contracting path\n        self.down_sample = nn.ModuleList([DownSample() for _ in range(4)])\n\n        # The two convolution layers at the lowest resolution (the bottom of the U).\n        self.middle_conv = DoubleConvolution(512, 1024)\n\n        # Up sampling layers for the expansive path.\n        # The number of features is halved with up-sampling.\n        self.up_sample = nn.ModuleList([UpSample(i, o) for i, o in\n                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n        # Double convolution layers for the expansive path.\n        # Their input is the concatenation of the current feature map and the feature map from the\n        # contracting path. Therefore, the number of input features is double the number of features\n        # from up-sampling.\n        self.up_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in\n                                      [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n        # Crop and concatenate layers for the expansive path.\n        self.concat = nn.ModuleList([CropAndConcat() for _ in range(4)])\n        # Final $1 \\times 1$ convolution layer to produce the output\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: input image\n        \"\"\"\n        # To collect the outputs of contracting path for later concatenation with the expansive path.\n        pass_through = []\n        # Contracting path\n        for i in range(len(self.down_conv)):\n            # Two $3 \\times 3$ convolutional layers\n            x = self.down_conv[i](x)\n            # Collect the output\n            pass_through.append(x)\n            # Down-sample\n            x = self.down_sample[i](x)\n\n        # Two $3 \\times 3$ convolutional layers at the bottom of the U-Net\n        x = self.middle_conv(x)\n\n        # Expansive path\n        for i in range(len(self.up_conv)):\n            # Up-sample\n            x = self.up_sample[i](x)\n            # Concatenate the output of the contracting path\n            x = self.concat[i](x, pass_through.pop())\n            # Two $3 \\times 3$ convolutional layers\n            x = self.up_conv[i](x)\n\n        # Final $1 \\times 1$ convolution layer\n        x = self.final_conv(x)\n\n        #\n        return x\n", "labml_nn/unet/carvana.py": "\"\"\"\n---\ntitle: Carvana dataset for the U-Net experiment\nsummary: >\n  Carvana dataset for the U-Net experiment.\n---\n\n# Carvana Dataset for the [U-Net](index.html) [experiment](experiment.html)\n\nYou can find the download instructions\n[on Kaggle](https://www.kaggle.com/competitions/carvana-image-masking-challenge/data).\n\nSave the training images inside `carvana/train` folder and the masks in `carvana/train_masks` folder.\n\"\"\"\n\nfrom torch import nn\nfrom pathlib import Path\n\nimport torch.utils.data\nimport torchvision.transforms.functional\nfrom PIL import Image\n\nfrom labml import lab\n\n\nclass CarvanaDataset(torch.utils.data.Dataset):\n    \"\"\"\n    ## Carvana Dataset\n    \"\"\"\n\n    def __init__(self, image_path: Path, mask_path: Path):\n        \"\"\"\n        :param image_path: is the path to the images\n        :param mask_path: is the path to the masks\n        \"\"\"\n        # Get a dictionary of images by id\n        self.images = {p.stem: p for p in image_path.iterdir()}\n        # Get a dictionary of masks by id\n        self.masks = {p.stem[:-5]: p for p in mask_path.iterdir()}\n\n        # Image ids list\n        self.ids = list(self.images.keys())\n\n        # Transformations\n        self.transforms = torchvision.transforms.Compose([\n            torchvision.transforms.Resize(572),\n            torchvision.transforms.ToTensor(),\n        ])\n\n    def __getitem__(self, idx: int):\n        \"\"\"\n        #### Get an image and its mask.\n\n        :param idx: is index of the image\n        \"\"\"\n\n        # Get image id\n        id_ = self.ids[idx]\n        # Load image\n        image = Image.open(self.images[id_])\n        # Transform image and convert it to a PyTorch tensor\n        image = self.transforms(image)\n        # Load mask\n        mask = Image.open(self.masks[id_])\n        # Transform mask and convert it to a PyTorch tensor\n        mask = self.transforms(mask)\n\n        # The mask values were not $1$, so we scale it appropriately.\n        mask = mask / mask.max()\n\n        # Return the image and the mask\n        return image, mask\n\n    def __len__(self):\n        \"\"\"\n        #### Size of the dataset\n        \"\"\"\n        return len(self.ids)\n\n\n# Testing code\nif __name__ == '__main__':\n    ds = CarvanaDataset(lab.get_data_path() / 'carvana' / 'train', lab.get_data_path() / 'carvana' / 'train_masks')\n", "labml_nn/neox/model.py": "\"\"\"\n---\ntitle: GPT-NeoX Model Definition\nsummary: >\n    This is the model definition of GPT-NeoX.\n---\n\n# GPT-NeoX Model\n\nHere is the code for layers of GPT-NeoX model and the code to load\n20B checkpoint.\n\nThe method `load_state` in the layers load the checkpoints of that layer.\nThe checkpoint loading helpers are on [`checkpoint.py`](checkpoint.html)\n\"\"\"\nimport copy\nimport math\nfrom typing import Dict, Optional, Set, Callable, Any, Generator, Tuple\n\nimport torch\nfrom torch import nn\nfrom torch.cuda.amp import autocast\n\nfrom labml import monit, logger\nfrom labml.logger import Text\nfrom labml_nn.neox import checkpoint\nfrom labml_nn.neox.utils.cache import get_cache\n\n\nclass NeoXModule(nn.Module):\n    def load_state(self, p1: Dict[str, torch.Tensor], p2: Dict[str, torch.Tensor]):\n        pass\n\n\nclass Embedding(NeoXModule):\n    \"\"\"\n    ## Embedding layer\n\n    This is a standard embeddings layer with code to load the checkpoint.\n    \"\"\"\n\n    def __init__(self, n_vocab: int = 50_432, n_hidden: int = 6_144):\n        \"\"\"\n        :param n_vocab: is the size of the vocabulary\n        :param n_hidden: is the size of the embeddings\n        \"\"\"\n        super().__init__()\n\n        self.emb = nn.Embedding(n_vocab, n_hidden)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: are the token ids of shape `[batch_size, seq_len]`\n        \"\"\"\n        return self.emb(x)\n\n    def load_state(self, p1: Dict[str, torch.Tensor], p2: Dict[str, torch.Tensor]):\n        \"\"\"\n        Code to load the checkpoint\n        \"\"\"\n        with monit.section('Load embedding layer'):\n            checkpoint.merge_params_dim_0(self.emb.weight, 'word_embeddings.weight', p1, p2)\n\n\nclass RoPE(nn.Module):\n    \"\"\"\n    ## Rotary Positional Embeddings\n\n    GPT-NeoX uses [rotary positional embeddings (RoPE)](https://arxiv.org/abs/2104.09864).\n\n    WE have annotated implementation of RoPE [here](https://nn.labml.ai/transformers/rope/index.html)\n    with more notes the theory.\n    \"\"\"\n\n    def __init__(self, d_rope: int, base: float = 10_000.):\n        \"\"\"\n        :param d_rope: is the number of features for RoPE embeddings\n        :param base: is the base for $\\theta_i = 10000^{\\frac{2(i-1)}{d}}$, which defaults to $10000$\n        \"\"\"\n        super().__init__()\n\n        # To store $\\theta_i$ for the features\n        self.theta = None\n        # Cache $\\cos m\\theta_i$ and $\\sin m\\theta_i$\n        self.cos_cached = None\n        self.sin_cached = None\n\n        # Base for $\\theta_i = 10000^{\\frac{2(i-1)}{d}}$\n        self.base = base\n        # Number of features for RoPE\n        self.d_rope = d_rope\n\n    @staticmethod\n    def rotate_half(x: torch.Tensor):\n        \"\"\"\n        ### Rotate the features\n\n        $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., -x^{(\\frac{d}{2})}]$\n        \"\"\"\n        x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n        return torch.cat((-x2, x1), dim=-1)\n\n    def forward(self, x: torch.Tensor, offset: int = 0):\n        \"\"\"\n        :param x: has shape `[..., seq, n_heads, d_k]`\n        :param offset: is the starting position of `x`. This is $\\gt 0$ when we have\n        cached the keys and queries of previous positions\n        \"\"\"\n\n        # Get the actual sequence length\n        seq_len = x.shape[-3] + offset\n\n        # Initialize $\\theta$\n        if self.theta is None:\n            #  $\\theta_i = 10000^{\\frac{2(i-1)}{d}}$\n            theta = 1.0 / (self.base ** (torch.arange(0, self.d_rope, 2).float() / self.d_rope))\n            self.theta = theta.to(x.device).to(x.dtype)\n\n        # Initialize $\\cos m\\theta_i$ and $\\sin m\\theta_i$ cache\n        if (\n                self.cos_cached is None or\n                seq_len > self.cos_cached.shape[1] or\n                self.cos_cached.device != x.device or\n                self.cos_cached.dtype != x.dtype\n        ):\n            # Get position indexes $m$\n            seq_idx = torch.arange(seq_len, device=x.device).type_as(self.theta)\n            # $m \\theta_i$\n            idx_theta = torch.einsum(\"s,d->sd\", seq_idx, self.theta)\n            # Concatenate so that for row $m$ we have\n            #\n            # $$[m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}, m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}]$$\n            idx_theta2 = torch.cat((idx_theta, idx_theta), dim=-1).to(x.device)\n\n            # Calculate $\\cos m\\theta_i$ and $\\sin m\\theta_i$ in fp32\n            with autocast(enabled=False):\n                idx_theta2 = idx_theta2.float()\n                # Add head dimension\n                self.cos_cached = idx_theta2.cos()[:, None, :]\n                self.sin_cached = idx_theta2.sin()[:, None, :]\n\n            # Cache them\n            self.cos_cached = self.cos_cached.to(x.dtype)\n            self.sin_cached = self.sin_cached.to(x.dtype)\n\n        # Split the features. We apply RoPE to only `d_rope` features\n        x_rope, x_pass = x[..., :self.d_rope], x[..., self.d_rope:]\n\n        # Get the sin and cos values from the cache\n        cos, sin = self.cos_cached[offset: seq_len], self.sin_cached[offset: seq_len]\n\n        # RoPE embeddings\n        #\n        # \\begin{align}\n        # \\begin{pmatrix}\n        # x^{(i)}_m \\cos m \\theta_i - x^{(i + \\frac{d}{2})}_m \\sin m \\theta_i \\\\\n        # x^{(i + \\frac{d}{2})}_m \\cos m\\theta_i + x^{(i)}_m \\sin m \\theta_i \\\\\n        # \\end{pmatrix} \\\\\n        # \\end{align}\n        #\n        # for $i \\in {1, 2, ..., \\frac{d}{2}}$\n        x_rope = (x_rope * cos) + (self.rotate_half(x_rope) * sin)\n\n        # Concatenate with features that didn't get RoPE embeddings\n        return torch.cat((x_rope, x_pass), dim=-1)\n\n\nclass AttentionLayer(nn.Module):\n    \"\"\"\n    ## Attention layer\n    \"\"\"\n\n    def __init__(self, n_hidden: int = 6_144, n_heads: int = 64, rope_percentage: float = 0.25,\n                 mask_fill: float = -10_000.0, *, is_flash_attention: bool = False):\n        \"\"\"\n        :param n_hidden: the number of features in embeddings\n        :param n_heads: the number of attention heads\n        :param rope_percentage: percentage of features to add RoPE embeddings\n        :param mask_fill: masking fill value for attention matrix\n        :param is_flash_attention: specifies whether to use\n            [FlashAttention](https://github.com/HazyResearch/flash-attention)\n        \"\"\"\n        super().__init__()\n\n        self.n_heads = n_heads\n        self.mask_fill = mask_fill\n\n        # Linear layer for query, key and value\n        self.qkv_lin = nn.Linear(n_hidden, n_hidden * 3)\n        # Final linear layer\n        self.output = nn.Linear(n_hidden, n_hidden)\n\n        # Number of features per head\n        d_k = n_hidden // n_heads\n        # RoPE embedding module\n        self.rope = RoPE(int(d_k * rope_percentage))\n\n        # Attention scaling factor\n        self.scale = 1 / math.sqrt(d_k)\n\n        # To cache causal mask\n        self.causal_mask = None\n\n        # Attention softmax module\n        self.softmax = nn.Softmax(dim=-2)\n\n        # [FlashAttention](https://github.com/HazyResearch/flash-attention)\n        if is_flash_attention:\n            try:\n                from flash_attn.flash_attention import FlashAttention\n                self.flash_attention = FlashAttention()\n            except ImportError:\n                logger.log('Install flash attention github.com/HazyResearch/flash-attention. '\n                           'Falling back to normal attention', Text.warning)\n                self.flash_attention = None\n        else:\n            self.flash_attention = None\n\n    def _get_mask(self, attn: torch.Tensor):\n        \"\"\"\n        #### Calculate the causal mask\n\n        * `attn` has shape [batch_size, query_seq_len, key_seq_len, n_heads]\n        \"\"\"\n\n        # Query and key lengths\n        nq, nk = attn.shape[1:3]\n\n        # Create mask\n        if (\n                self.causal_mask is None or\n                self.causal_mask.shape[0] != nq or\n                self.causal_mask.shape[1] != nk or\n                self.causal_mask.device != attn.device\n        ):\n            self.causal_mask = torch.triu(attn.new_ones([nq, nk], dtype=torch.bool), 1 + nk - nq)\n\n        # Return from cache\n        return self.causal_mask[None, :, :, None]\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: has shape `[batch_size, seq_len, n_hidden]`\n        \"\"\"\n        # Get query, key and value embeddings (all concatenated).\n        # The last dimension size will change from n_hidden -> `3 x n_hidden`\n        qkv = self.qkv_lin(x)\n\n        # Split into heads by changing the shape to `[batch_size, seq_len, n_heads, 3 * d_k]`\n        qkv = qkv.view(*qkv.shape[:-1], self.n_heads, -1)\n        # Split into query, key and value each of shape `[batch_size, seq_len, n_heads, 3 * d_k]`\n        q, k, v = torch.split(qkv, qkv.shape[-1] // 3, dim=-1)\n\n        # If we are caching the states of previous tokens\n        if get_cache().get('use_cache', False):\n            # Get the state id's. We use to retrieve previous states and store the next states\n            prev_state_id, next_state_id = get_cache().get('state_ids')\n            # If there's cache\n            if prev_state_id is not None:\n                # Get the past keys and values. These will have shape `[batch_size, prev_seq_len, n_heads, d_k]`\n                k_past, v_past = get_cache().pop(f'attn_kv_{prev_state_id}')\n                # Offset of the current embeddings\n                offset = k_past.shape[1]\n\n                # Add RoPE embeddings\n                q = self.rope(q, offset=offset)\n                k = self.rope(k, offset=offset)\n\n                # Concatenate the past\n                k = torch.cat([k_past, k], dim=1)\n                v = torch.cat([v_past, v], dim=1)\n            else:\n                # Add RoPE embeddings\n                q = self.rope(q)\n                k = self.rope(k)\n\n            # Save the current state\n            get_cache().push(f'attn_kv_{next_state_id}', (k, v))\n        else:\n            # No cache - simply add RoPE embeddings\n            q = self.rope(q)\n            k = self.rope(k)\n\n        # Use flash attention\n        if self.flash_attention is not None and q.shape[1] == k.shape[1] and q.shape[-1] <= 128:\n            output = self.compute_flash_attention(q, k, v)\n        # Otherwise, use normal attention\n        else:\n            output = self.compute_attention(q, k, v)\n\n        # Reshape from `[batch_size, seq_len, n_heads, d_k] to `[batch_size, seq_len, n_hidden]`\n        output = output.reshape(*x.shape)\n\n        # Final linear layer\n        return self.output(output)\n\n    def compute_flash_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n        # Stack them into shape `[batch_size, seq_len, 3, n_heads, d_k]`\n        qkv = torch.stack((q, k, v), dim=2)\n        d_k = qkv.shape[-1]\n        if d_k <= 32:\n            pad = 32 - d_k\n        elif d_k <= 64:\n            pad = 64 - d_k\n        elif d_k <= 128:\n            pad = 128 - d_k\n        else:\n            raise ValueError(f'Head size {d_k} too large for flash attention')\n\n        if pad > 0:\n            qkv = torch.cat((qkv, qkv.new_zeros(*qkv.shape[:-1], pad)), dim=-1)\n\n        output, _ = self.flash_attention(qkv, causal=True)\n        # The output is of shape `[batch_size, seq_len, n_heads, d_k + padding]`\n        output = output[:, :, :, :d_k]\n\n        return output\n\n    def compute_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n        # Disable auto-casting to fp16 for attention computation\n        with autocast(enabled=False):\n            if q.dtype == torch.float16:\n                # Convert to fp32 if the current dtype is fp16\n                attn = torch.einsum('bihk,bjhk->bijh', q.float(), k.float())\n            else:\n                # Do not cast for bfloat\n                attn = torch.einsum('bihk,bjhk->bijh', q, k)\n\n            # Scale attention\n            attn = attn * self.scale\n\n            # Get causal mask\n            mask = self._get_mask(attn)\n            # Apply mask\n            attn.masked_fill_(mask, self.mask_fill)\n\n            # Attention softmax\n            attn = self.softmax(attn)\n\n        # Get attention weighted values\n        output = torch.einsum('bijh,bjhk->bihk', attn.to(v.dtype), v)\n\n        return output\n\n\nclass FFNLayer(nn.Module):\n    \"\"\"\n    ## Feedforward Network\n    \"\"\"\n\n    def __init__(self, n_hidden: int = 6_144, d_ff: int = 0):\n        \"\"\"\n        :param n_hidden: is the embedding size\n        \"\"\"\n        super().__init__()\n\n        if not d_ff:\n            d_ff = n_hidden * 4\n\n        # Expansion linear layer\n        self.dense_h_h4 = nn.Linear(n_hidden, d_ff)\n        # GELU activation\n        self.activation = nn.GELU()\n        # Contraction linear layer\n        self.dense_h4_h = nn.Linear(d_ff, n_hidden)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: has shape `[batch_size, seq_len, n_hidden]`\n        \"\"\"\n        x = self.dense_h_h4(x)\n        x = self.activation(x)\n        x = self.dense_h4_h(x)\n\n        return x\n\n\nclass TransformerLayer(NeoXModule):\n    \"\"\"\n    ## Transformer Layer\n    \"\"\"\n\n    def __init__(self, n_hidden: int = 6_144, n_heads: int = 64, *, is_flash_attention: bool = False):\n        \"\"\"\n        :param n_hidden: is the embedding size\n        :param n_heads: is the number of heads\n        :param is_flash_attention: specifies whether to use\n            [FlashAttention](https://github.com/HazyResearch/flash-attention)\n\n        *Out implementation doesn't include dropout*.\n        \"\"\"\n        super().__init__()\n\n        # Layer normalization before attention\n        self.pre_ln_attn = nn.LayerNorm(n_hidden)\n        # Layer normalization before FFN\n        self.pre_ln_ffn = nn.LayerNorm(n_hidden)\n\n        # Attention layer\n        self.attention = AttentionLayer(n_hidden, n_heads, is_flash_attention=is_flash_attention)\n        # FFN layer\n        self.ffn = FFNLayer(n_hidden)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: are the embeddings of shape `[batch_size, seq_len, n_hidden]`\n        \"\"\"\n\n        # Residual connection\n        residual = x\n        # NeoX runs attention and feedforward network in parallel\n        attn = self.attention(self.pre_ln_attn(x))\n        ffn = self.ffn(self.pre_ln_ffn(x))\n        # Add them and the residual connection\n        return attn + ffn + residual\n\n    def load_state(self, p1: Dict[str, torch.Tensor], p2: Dict[str, torch.Tensor]):\n        \"\"\"\n        Code to load the checkpoint\n        \"\"\"\n        with monit.section('Load transformer layer'):\n            # Attention output transform\n            checkpoint.merge_params_sum(self.attention.output.bias, 'attention.dense.bias', p1, p2)\n            checkpoint.merge_params_dim_1(self.attention.output.weight, 'attention.dense.weight', p1, p2)\n\n            # Attention query, key and value transform\n            checkpoint.merge_params_dim_0(self.attention.qkv_lin.bias, 'attention.query_key_value.bias', p1, p2)\n            checkpoint.merge_params_dim_0(self.attention.qkv_lin.weight, 'attention.query_key_value.weight', p1, p2)\n\n            # Layer norm before attention\n            checkpoint.merge_params_duplicate(self.pre_ln_attn.bias, 'input_layernorm.bias', p1, p2)\n            checkpoint.merge_params_duplicate(self.pre_ln_attn.weight, 'input_layernorm.weight', p1, p2)\n\n            # FFN second transform\n            checkpoint.merge_params_dim_0(self.ffn.dense_h_h4.bias, 'mlp.dense_h_to_4h.bias', p1, p2)\n            checkpoint.merge_params_dim_0(self.ffn.dense_h_h4.weight, 'mlp.dense_h_to_4h.weight', p1, p2)\n\n            # FFN first transform\n            checkpoint.merge_params_sum(self.ffn.dense_h4_h.bias, 'mlp.dense_4h_to_h.bias', p1, p2)\n            checkpoint.merge_params_dim_1(self.ffn.dense_h4_h.weight, 'mlp.dense_4h_to_h.weight', p1, p2)\n\n            # Layer norm before FFN\n            checkpoint.merge_params_duplicate(self.pre_ln_ffn.bias, 'post_attention_layernorm.bias', p1, p2)\n            checkpoint.merge_params_duplicate(self.pre_ln_ffn.weight, 'post_attention_layernorm.weight', p1, p2)\n\n\nclass FinalNorm(NeoXModule):\n    \"\"\"\n    ## Final normalization layer\n    \"\"\"\n\n    def __init__(self, n_hidden: int = 6_144):\n        \"\"\"\n        :param n_hidden: is the embedding size\n        \"\"\"\n        super().__init__()\n\n        self.ln = nn.LayerNorm(n_hidden)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: are the embeddings of shape `[batch_size, seq_len, n_hidden]`\n        \"\"\"\n        return self.ln(x)\n\n    def load_state(self, p1: Dict[str, torch.Tensor], p2: Dict[str, torch.Tensor]):\n        \"\"\"\n        Code to load the checkpoint\n        \"\"\"\n        with monit.section('Load final normalization layer'):\n            checkpoint.merge_params_duplicate(self.ln.bias, 'norm.bias', p1, p2)\n            checkpoint.merge_params_duplicate(self.ln.weight, 'norm.weight', p1, p2)\n\n\nclass ReadoutLayer(NeoXModule):\n    \"\"\"\n    Readout layer\n    \"\"\"\n\n    def __init__(self, n_hidden: int = 6_144, n_vocab: int = 50_432):\n        \"\"\"\n        :param n_hidden: is the embedding size\n        :param n_vocab: is the size of the vocabulary\n        \"\"\"\n        super().__init__()\n\n        self.linear = nn.Linear(n_hidden, n_vocab, bias=False)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: are the embeddings of shape `[batch_size, seq_len, n_hidden]`\n        \"\"\"\n        return self.linear(x)\n\n    def load_state(self, p1: Dict[str, torch.Tensor], p2: Dict[str, torch.Tensor]):\n        \"\"\"\n        Code to load the checkpoint\n        \"\"\"\n        with monit.section('Load final linear layer'):\n            checkpoint.merge_params_dim_0(self.linear.weight, 'final_linear.weight', p1, p2)\n\n\nclass LayerGenerator:\n    pre_created_layers: Dict[Any, Optional[NeoXModule]]\n\n    def __init__(self, *, n_vocab: int = 50_432, n_hidden: int = 6_144,\n                 n_layers: int = 44, n_heads: int = 64,\n                 filter_layers: Optional[Set] = None,\n                 is_clone_layers: bool = True,\n                 dtype: torch.dtype = torch.float,\n                 device: torch.device = torch.device('cpu'),\n                 is_llm_int8: bool = False,\n                 llm_int8_threshold: float = 6.0,\n                 is_flash_attention: bool = False\n                 ):\n        \"\"\"\n        ### Generator to create layers\n\n        The layers are generated in the same order as checkpoints.\n\n        It gives `None` when a layer is not available; we use the layer indices as NeoX and there are two\n        transformation layers we don't need in our implementation.\n\n        :param n_vocab: is the number of tokens in the vocabulary\n        :param n_hidden: is the number of features in the embeddings\n        :param n_layers: is the number of transformer layers\n        :param n_heads: is the number of attention heads\n        :param filter_layers: are the set of layers to be used. All layers will be used if None.\n            This is used to test smaller versions of the model with fewer layers\n        :param is_clone_layers: specifies whether to clone the transformer layers (a bit faster)\n        :param dtype: is the data type of the model\n        :param device: is the device of the model\n        :param is_llm_int8: specifies whether to use int8 quantization\n        :param llm_int8_threshold: is the threshold $\\alpha$ used to separate outlier features\n        :param is_flash_attention: specifies whether to use\n            [FlashAttention](https://github.com/HazyResearch/flash-attention)\n        \"\"\"\n        if filter_layers is None:\n            filter_layers = set(range(n_layers + 3))\n\n        self.n_vocab = n_vocab\n        self.n_hidden = n_hidden\n        self.n_layers = n_layers\n        self.n_heads = n_heads\n        self.filter_layers = filter_layers\n        self.is_clone_layers = is_clone_layers\n        self.dtype = dtype\n        self.device = device\n        self.is_llm_int8 = is_llm_int8\n        self.llm_int8_threshold = llm_int8_threshold\n        self.is_flash_attention = is_flash_attention\n\n        self.pre_created_layers = dict(\n            transformer_layer=None,\n        )\n\n    def _prepare_layer(self, layer: NeoXModule):\n        \"\"\"\n        #### Prepares the layer for usage\n\n        We move the layer to the device and convert it to the correct data type\n\n        :param layer: is the layer to prepare\n        :return: the prepared layer\n        \"\"\"\n        return layer.to(self.device, self.dtype)\n\n    @torch.no_grad()\n    def post_load_prepare(self, layer: NeoXModule, *,\n                          is_llm_int8: bool = None,\n                          device: torch.device = None,\n                          llm_int8_threshold: float = None,\n                          ):\n        \"\"\"\n        <a id=\"post_load_prepare\"></a>\n\n        ### Layer transformations after loading the checkpoint\n\n        This function implements layer transformations after loading the checkpoint.\n\n        Currently, it only applies the int8 quantization.\n\n        :param layer: is the layer to prepare\n        :param is_llm_int8: specifies whether to use int8 quantization\n        :param device: is the device of the model\n        :param llm_int8_threshold: is the threshold $\\alpha$ used to separate outlier features\n        :return: the prepared layer\n        \"\"\"\n\n        # Get default values if not specified\n        if is_llm_int8 is None:\n            is_llm_int8 = self.is_llm_int8\n        if device is None:\n            device = self.device\n        if llm_int8_threshold is None:\n            llm_int8_threshold = self.llm_int8_threshold\n\n        # Skip if not using int8 quantization\n        if not is_llm_int8:\n            return layer\n\n        # Only convert the linear layers in the transformer layers\n        if not isinstance(layer, TransformerLayer):\n            return layer\n\n        # Use `make_llm_int8_linear` defined in [utilities](./utils/llm_int8.html).\n        from labml_nn.neox.utils.llm_int8 import make_llm_int8_linear\n\n        # Convert the linear layers\n        with monit.section('Convert to int8'):\n            layer.attention.output = make_llm_int8_linear(layer.attention.output,\n                                                          device=device,\n                                                          threshold=llm_int8_threshold)\n            layer.attention.qkv_lin = make_llm_int8_linear(layer.attention.qkv_lin,\n                                                           device=device,\n                                                           threshold=llm_int8_threshold)\n            layer.ffn.dense_h_h4 = make_llm_int8_linear(layer.ffn.dense_h_h4,\n                                                        device=device,\n                                                        threshold=llm_int8_threshold)\n            layer.ffn.dense_h4_h = make_llm_int8_linear(layer.ffn.dense_h4_h,\n                                                        device=device,\n                                                        threshold=llm_int8_threshold)\n        #\n        return layer\n\n    def _create_and_cache_layer(self, name: str, creator: Callable[[], NeoXModule]):\n        \"\"\"\n        #### Creates and caches a layer\n\n        Copying cached layers is faster than initializing new layers because it takes time to\n        initialize parameters.\n\n        :param name: is the name of the layer\n        :param creator: is the function to create the layer\n        :return: the created layer or a copy of the cached layer\n        \"\"\"\n\n        if not self.is_clone_layers:\n            return self._prepare_layer(creator())\n\n        if self.pre_created_layers[name] is None:\n            self.pre_created_layers[name] = self._prepare_layer(creator())\n\n        layer = copy.deepcopy(self.pre_created_layers[name])\n        return layer\n\n    def _create_transformer_layer(self):\n        return self._create_and_cache_layer(\n            'transformer_layer',\n            lambda: TransformerLayer(self.n_hidden, self.n_heads, is_flash_attention=self.is_flash_attention)\n        )\n\n    def _create_embedding_layer(self):\n        return Embedding(self.n_vocab, self.n_hidden)\n\n    def _create_final_norm_layer(self):\n        return FinalNorm(self.n_hidden)\n\n    def _create_readout_layer(self):\n        return ReadoutLayer(self.n_hidden, self.n_vocab)\n\n    @torch.no_grad()\n    def get_layers(self) -> Generator[Tuple[NeoXModule, Tuple[str, str]], None, None]:\n        \"\"\"\n        ### Generator to get layers\n        \"\"\"\n        # Embedding layer\n        if 0 in self.filter_layers:\n            with monit.section('Embedding layer'):\n                layer = self._prepare_layer(self._create_embedding_layer())\n            yield layer, ('layer_00-model_00-model_states.pt', 'layer_00-model_01-model_states.pt')\n\n        # Transformer layers\n        for i in range(self.n_layers):\n            # Transformer layer\n            if i + 1 in self.filter_layers:\n                with monit.section(f'Transformer Layer {i}'):\n                    yield self._create_transformer_layer(), \\\n                          (f'layer_{i + 2 :02d}-model_00-model_states.pt',\n                           f'layer_{i + 2 :02d}-model_01-model_states.pt')\n\n        # Final normalization layer\n        if self.n_layers + 1 in self.filter_layers:\n            with monit.section('Final norm layer'):\n                layer = self._prepare_layer(self._create_final_norm_layer())\n            yield layer, ('layer_47-model_00-model_states.pt', 'layer_47-model_01-model_states.pt')\n\n        # Readout layer\n        if self.n_layers + 2 in self.filter_layers:\n            with monit.section('Readout layer'):\n                layer = self._prepare_layer(self._create_readout_layer())\n            yield layer, ('layer_48-model_00-model_states.pt', 'layer_48-model_01-model_states.pt')\n\n        for k in self.pre_created_layers.keys():\n            self.pre_created_layers[k] = None\n\n    @property\n    def total_layers(self):\n        \"\"\"\n        ### Returns the total number of layers\n        \"\"\"\n        return self.n_layers + 3\n\n    @torch.no_grad()\n    def load(self) -> Generator[NeoXModule, None, None]:\n        \"\"\"\n        ### Generator to load layers\n        \"\"\"\n        with monit.section(\"Layers\"):\n            for i, (layer, files) in enumerate(self.get_layers()):\n                if files is not None:\n                    layer.load_state(*checkpoint.load_checkpoint_files(files))\n\n                layer = self.post_load_prepare(layer)\n\n                monit.progress(min(0.99, (i + 1) / self.total_layers))\n                yield layer\n", "labml_nn/neox/checkpoint.py": "\"\"\"\n---\ntitle: GPT-NeoX Checkpoints\nsummary: >\n    Code to download checkpoints and helpers to load them.\n---\n\n# GPT-NeoX Checkpoints\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Dict, Union, Tuple, Optional\n\nimport torch\nfrom torch import nn\n\nfrom labml import monit, lab, logger\nfrom labml.logger import Text, inspect\nfrom labml.utils.download import download_file\n\n# Parent url\nCHECKPOINTS_URL = 'https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/'\n\n_CHECKPOINTS_DOWNLOAD_PATH: Optional[Path] = None\n\n\n# Download path\ndef get_checkpoints_download_path():\n    global _CHECKPOINTS_DOWNLOAD_PATH\n\n    if _CHECKPOINTS_DOWNLOAD_PATH is not None:\n        return _CHECKPOINTS_DOWNLOAD_PATH\n\n    _CHECKPOINTS_DOWNLOAD_PATH = lab.get_data_path() / 'neox_fast' / 'slim_weights'\n    if not _CHECKPOINTS_DOWNLOAD_PATH.exists():\n        _CHECKPOINTS_DOWNLOAD_PATH = lab.get_data_path() / 'neox' / 'slim_weights'\n    inspect(neox_checkpoint_path=_CHECKPOINTS_DOWNLOAD_PATH)\n\n    return _CHECKPOINTS_DOWNLOAD_PATH\n\n\ndef get_files_to_download(n_layers: int = 44):\n    \"\"\"\n    ### Get files to download\n\n    :return: a list of files to be downloaded\n    \"\"\"\n    layers = (\n        # Embedding layer\n            [0] +\n            # Transformer layers\n            list(range(2, 2 + n_layers)) +\n            # Final normalization layer and readout layer\n            [47, 48]\n    )\n\n    return (\n        # Vocabulary and configs\n            ['20B_tokenizer.json', 'configs/20B.yml', 'latest'] +\n            # Layer checkpoints\n            [f'global_step150000/layer_{i :02d}-model_{p :02d}-model_states.pt' for i in layers for p in range(2)] +\n            # Empty states (not used)\n            [f'global_step150000/mp_rank_{i :02d}_model_states.pt' for i in range(8)]\n    )\n\n\ndef download(n_layers: int = 44):\n    \"\"\"\n    ## Download all checkpoint files\n    \"\"\"\n\n    # Get files to download\n    files = get_files_to_download(n_layers)\n\n    # Iterate\n    for i, f in monit.enum('Download All', files):\n        # Log\n        logger.log(['Downloading ', (f'{i + 1 :3d}/{len(files)}', Text.meta), ': ', (f, Text.value)])\n        # Download\n        download_file(CHECKPOINTS_URL + f, get_checkpoints_download_path() / f)\n\n\ndef load_checkpoint_files(files: Tuple[str, str]):\n    \"\"\"\n    ### Load a pair of checkpoint files\n\n    :param files: pair of files to load\n    :return: the loaded parameter tensors\n    \"\"\"\n    checkpoint_path = get_checkpoints_download_path() / 'global_step150000'\n    with monit.section('Load checkpoint'):\n        data = [torch.load(checkpoint_path / f) for f in files]\n\n    return data\n\n\ndef merge_params_dim_0(param: Union[nn.Parameter, torch.Tensor], key: str, p1: Dict[str, torch.Tensor],\n                       p2: Dict[str, torch.Tensor]):\n    \"\"\"\n    ### Load a parameter by merging the partitions along first dimension\n\n    :param param: is the parameter\n    :param key: is the name of the parameter\n    :param p1: first partition dictionary\n    :param p2: second partition dictionary\n    \"\"\"\n    w1, w2 = p1[key], p2[key]\n    param.data[:w1.shape[0]] = w1\n    param.data[w1.shape[0]:] = w2\n\n\ndef merge_params_dim_1(param: Union[nn.Parameter, torch.Tensor], key: str, p1: Dict[str, torch.Tensor],\n                       p2: Dict[str, torch.Tensor]):\n    \"\"\"\n    ### Load a parameter by merging the partitions along second dimension\n\n    :param param: is the parameter\n    :param key: is the name of the parameter\n    :param p1: first partition dictionary\n    :param p2: second partition dictionary\n    \"\"\"\n    w1, w2 = p1[key], p2[key]\n    param.data[:, :w1.shape[1]] = w1\n    param.data[:, w1.shape[1]:] = w2\n\n\ndef merge_params_duplicate(param: Union[nn.Parameter, torch.Tensor], key: str, p1: Dict[str, torch.Tensor],\n                           p2: Dict[str, torch.Tensor]):\n    \"\"\"\n    ### Load an un-partitioned parameter\n\n    This does a sanity check to make use both partitions are the same\n\n    :param param: is the parameter\n    :param key: is the name of the parameter\n    :param p1: first partition dictionary\n    :param p2: second partition dictionary\n    \"\"\"\n    w1, w2 = p1[key], p2[key]\n\n    diff = sum((w1 - w2) ** 2).item()\n    assert diff < 1e-4, f'The partitions do not match: {key}'\n\n    param.data[:] = (w1 + w2) / 2.\n\n\ndef merge_params_sum(param: Union[nn.Parameter, torch.Tensor], key: str, p1: Dict[str, torch.Tensor],\n                     p2: Dict[str, torch.Tensor]):\n    \"\"\"\n    ### Load biases that are partitioned which gets added on reduce\n\n    :param param: is the parameter\n    :param key: is the name of the parameter\n    :param p1: first partition dictionary\n    :param p2: second partition dictionary\n    \"\"\"\n    w1, w2 = p1[key], p2[key]\n\n    param.data[:] = w1 + w2\n\n\n#\nif __name__ == '__main__':\n    download()\n", "labml_nn/neox/tokenizer.py": "\"\"\"\n---\ntitle: GPT-NeoX Tokenizer\nsummary: >\n    Loads the GPT-NeoX tokenizer\n---\n\n# GPT-NeoX Tokenizer\n\nThis initializes a Hugging Face tokenizer from the downloaded vocabulary.\n\"\"\"\n\nfrom tokenizers import Tokenizer\n\nfrom labml import lab, monit\n\n\n@monit.func('Load NeoX Tokenizer')\ndef get_tokenizer() -> Tokenizer:\n    \"\"\"\n    ### Load NeoX Tokenizer\n\n    :return: the tokenizer\n    \"\"\"\n    vocab_file = lab.get_data_path() / 'neox' / 'slim_weights' / '20B_tokenizer.json'\n    tokenizer = Tokenizer.from_file(str(vocab_file))\n\n    return tokenizer\n", "labml_nn/neox/__init__.py": "\"\"\"\n---\ntitle: GPT-NeoX\nsummary: >\n    Simple GPT-NeoX implementation\n---\n\n# GPT-NeoX\n\nThis is a simple implementation of [Eleuther GPT-NeoX](https://arxiv.org/abs/2204.06745) for inference and fine-tuning.\n\n\n* [Model definition](model.html)\n* [Tokenizer](tokenizer.html)\n* [Checkpoint downloading and loading helpers](checkpoint.html)\n* [Utilities](utils/index.html)\n* [LLM.int8() quantization](utils/llm_int8.html)\n\n### [Samples](samples/__init__.py)\n\n* [Generating text](samples/generate.html)\n* [Fine-tuning the biases with pipeline-parallel](samples/finetune.html)\n* [Generating text with LLM.int8()](samples/llm_int8.html)\n\n### [Evaluation](evaluation/__init__.py)\n\n* [Evaluating half precision model on a single GPU](evaluation/half_precision.html)\n* [Evaluating LLM.int8() model](evaluation/llm_int8.html)\n\n**Official [Eleuther](https://www.eleuther.ai)\nGPT-NoeX is source code is available at [eleutherai/gpt-neox](https://github.com/eleutherai/gpt-neox).**\n\"\"\"\n", "labml_nn/neox/utils/trainer.py": "from typing import Optional, Set, List\n\nimport torch.nn as nn\nimport torch.optim\nimport torch.utils.data\nfrom torch.cuda import amp\nfrom torch.cuda.amp import GradScaler\n\nfrom labml import monit, tracker\nfrom labml.configs import BaseConfigs, option\nfrom labml_nn.neox.utils.finetune import FineTuner\n\n\ndef get_trainable_params(model: nn.Module):\n    \"\"\"\n    ### Get trainable parameters\n\n    :param model: is the model to train\n    :return: a list of parameters for training\n    \"\"\"\n\n    # Get all parameters\n    params = list(model.parameters())\n    # Filter parameters that require gradients\n    trainable_params = [p for p in params if p.requires_grad]\n\n    #\n    return trainable_params\n\n\nclass TrainerConf(BaseConfigs):\n    model: nn.Module\n    layers: List[nn.Module]\n    optimizer: torch.optim.Optimizer = 'Adam'\n    train_loader: torch.utils.data.DataLoader\n    valid_loader: Optional[torch.utils.data.DataLoader] = None,\n    device: torch.device = torch.device('cuda:0')\n    scaler: Optional[GradScaler] = 'Default'\n    is_amp: bool = True\n    dtype: torch.dtype = torch.float16\n\n    is_clone_layers: bool = True\n\n    loss_func: nn.Module = nn.CrossEntropyLoss()\n    checkpoints_per_epoch: int = 0\n    samples_per_epoch: int = 0\n\n    grad_norm: Optional[float] = 1.0\n    learning_rate: float = 3e-4\n    max_seq_len: int = 1024\n    batch_size: int = 64\n    epochs: int = 16\n\n    n_gpus: int = torch.cuda.device_count()\n\n    filter_layers: Optional[Set] = None\n\n    def get_loss(self, sample, dataset_split: str):\n        \"\"\"\n        :param dataset_split: train/valid\n        :param sample: is the sample\n        :return: the loss, output and the target\n        \"\"\"\n        data, target = sample\n\n        # Forward pass\n        with monit.section('Forward pass'):\n            output = self.model(data.to(self.device))\n        # Move targets to the same device as output\n        target = target.to(output.device)\n        # Calculate loss\n        loss = self.loss_func(output.view(target.numel(), -1), target.view(-1))\n\n        return loss, output, target\n\n    def train(self):\n        for epoch in monit.loop(self.epochs):\n            self.train_epoch()\n            tracker.new_line()\n\n    def sample(self, idx):\n        pass\n\n    def save_checkpoint(self, idx):\n        pass\n\n    def get_iterators(self):\n        # Iterate through the batches\n        iterators = [('train', self.train_loader)]\n        if self.valid_loader is not None:\n            iterators.append(('valid', self.valid_loader))\n\n        if self.samples_per_epoch > 0:\n            iterators.append((self.sample, [i for i in range(self.samples_per_epoch)]))\n\n        if self.checkpoints_per_epoch > 0:\n            iterators.append((self.save_checkpoint, [i for i in range(self.checkpoints_per_epoch)]))\n\n        return iterators\n\n    def train_epoch(self):\n        # Set model for train\n        self.model.train()\n\n        iterators = self.get_iterators()\n        for split_name, sample in monit.mix(1024, *iterators):\n            if split_name == 'train':\n                # Set gradients to zero\n                self.optimizer.zero_grad()\n                tracker.add_global_step()\n\n            with torch.set_grad_enabled(split_name == 'train'):\n                if self.is_amp:\n                    # Forward pass\n                    with amp.autocast():\n                        loss, output, target = self.get_loss(sample, split_name)\n                else:\n                    loss, output, target = self.get_loss(sample, split_name)\n\n                # Get predictions\n                pred = output.argmax(dim=-1)\n                # Calculate accuracy\n                accuracy = pred.eq(target).sum().item() / (target != -100).sum()\n\n                tracker.add({f'loss.{split_name}': loss, f'acc.{split_name}': accuracy * 100})\n\n            if split_name == 'train':\n                if self.scaler is not None:\n                    # Backward pass\n                    loss = self.scaler.scale(loss)\n                    # tracker.add({'loss.scaled': loss})\n\n                with monit.section('Backward pass'):\n                    loss.backward()\n\n                # Optimize\n                with monit.section('Optimize'):\n                    if self.scaler is None:\n                        self.optimizer.step()\n                    else:\n                        self.scaler.unscale_(self.optimizer)\n                        if self.grad_norm is not None:\n                            torch.nn.utils.clip_grad_norm_(get_trainable_params(self.model), self.grad_norm)\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n\n            tracker.save()\n\n\n@option(TrainerConf.optimizer, 'Adam')\ndef adam_optimizer(c: TrainerConf):\n    if c.dtype == torch.float32:\n        return torch.optim.Adam(get_trainable_params(c.model), lr=c.learning_rate)\n    elif c.dtype == torch.float16:\n        from labml_nn.optimizers.adam_fp16 import AdamFP16\n        return AdamFP16(get_trainable_params(c.model), lr=c.learning_rate)\n    else:\n        raise NotImplementedError()\n\n\n@option(TrainerConf.optimizer, 'SGD')\ndef sgd_optimizer(c: TrainerConf):\n    return torch.optim.SGD(get_trainable_params(c.model), lr=c.learning_rate)\n\n\n@option(TrainerConf.scaler, 'Default')\ndef grad_scaler(c: TrainerConf):\n    if not c.is_amp:\n        return None\n\n    if c.dtype == torch.float16:\n        from labml_nn.optimizers.adam_fp16 import GradScalerFP16\n        return GradScalerFP16()\n    else:\n        return GradScaler()\n\n\nclass PipelineParallelTrainerConf(TrainerConf):\n    is_checkpointing: bool = False\n    chunks: int\n\n    fine_tuner: FineTuner\n", "labml_nn/neox/utils/llm_int8.py": "\"\"\"\n---\ntitle: LLM.int8() on GPT-NeoX\nsummary: >\n    Transform nn.Linear layers to 8-bit integer layers.\n---\n\n# LLM.int() on GPT-NeoX\n\nThis implements a utility function to transform a `nn.Linear` layer to LLM.int8() linear layer.\n\n[LLM.int8() paper](https://arxiv.org/abs/eb2bcaee1d0011edaa66a71c10a887e7)\n shows you can use int8 quantization while handling outliers to\nreduce memory footprint without performance degradation in large language models.\nThey convert weights and inputs to scaled 8-bit integers and does matrix multiplication\nproducing int32 results which is then converted back to float16 and rescaled.\nThey show that in large langauge models, some features can give extreme values (outliers)\nthat dominate the model's output.\nThese features get clamped in 8-bit integer space which causes the model performance to degrade.\nAs a solution they pick these outliers (greater than a specified threshold)\nand compute their multiplications separately in float16 space.\nSince the percentage of outliers is around 0.01% this doesn't increase memory usage,\nand prevents the model from degrading performance.\n\nThe code to transform GPT-NoeX layers is defined in [model.py](../model.html#post_load_prepare).\n\nHere are example uses of GPT-NeoX with int8 quantization.\n\n* [Generate Text](../samples/llm_int8.html)\n* [Run Evaluation Tests](../evaluation/llm_int8.html)\n\"\"\"\n\n# Import [`bitsandbytes`](https://github.com/timdettmers/bitsandbytes) package\ntry:\n    from bitsandbytes.nn import Linear8bitLt, Int8Params\nexcept ImportError:\n    raise ImportError('''Please install `bitsandbytes` with `pip install bitsandbytes -U`''')\n\nimport torch\nfrom torch import nn\n\n\ndef make_llm_int8_linear(linear_module: nn.Linear, device: torch.device, threshold: float = 6.0):\n    \"\"\"\n    ## Transform a `nn.Linear` layer to LLM.int8() linear layer\n\n    :param linear_module: is the `nn.Linear` layer to transform\n    :param device: is the device of the model\n    :param threshold: is the threshold $\\alpha$ to use for outlier detection\n    \"\"\"\n\n    #\n    assert isinstance(linear_module, nn.Linear)\n\n    # Create an empty Linear8bitLt module\n    int8_lin = Linear8bitLt(\n        linear_module.in_features,\n        linear_module.out_features,\n        linear_module.bias is not None,\n        has_fp16_weights=False,\n        threshold=threshold,\n    )\n\n    # Quantize the weights\n    int8_lin._parameters['weight'] = Int8Params(linear_module.weight.data.cpu(),\n                                                requires_grad=False,\n                                                has_fp16_weights=False).to(device)\n\n    # Set the bias in float16 space\n    if linear_module.bias is not None:\n        int8_lin._parameters['bias'] = nn.Parameter(linear_module.bias.data,\n                                                    requires_grad=False)\n\n    #\n    return int8_lin\n", "labml_nn/neox/utils/finetune.py": "from typing import List, Dict\n\nimport torch\nfrom torch import nn\n\nfrom labml_nn.neox.model import TransformerLayer, NeoXModule\n\n\nclass FineTuner:\n    def __init__(self, layers: List[NeoXModule]):\n        self.layers = layers\n\n    def get_trainable_params(self) -> Dict[str, nn.Parameter]:\n        params = {}\n        for i, layer in enumerate(self.layers):\n            params.update(self.get_layer_trainable_params(layer, prefix=f'layer_{i :02d}'))\n\n        return params\n\n    def get_layer_trainable_params(self, layer: NeoXModule, prefix: str) -> Dict[str, nn.Parameter]:\n        raise NotImplementedError\n\n    def set_trainable_params(self):\n        for layer in self.layers:\n            # Set `requires_grad` to `False` for the entire layer.\n            layer.requires_grad_(False)\n            #\n            for p in self.get_trainable_params().values():\n                p.requires_grad_(True)\n\n    def state_dict(self):\n        return {n: p.data.cpu() for n, p in self.get_trainable_params().items()}\n\n    def load_state_dict(self, state_dict: Dict[str, torch.Tensor]):\n        params = self.get_trainable_params()\n        for n, p in params.items():\n            p.data[:] = state_dict[n].to(p.data.device)\n\n        for n in state_dict.keys():\n            assert n in params, n\n\n\nclass FineTuneBiases(FineTuner):\n    def get_layer_trainable_params(self, layer: NeoXModule, prefix: str) -> Dict[str, nn.Parameter]:\n        params = {}\n\n        if isinstance(layer, TransformerLayer):\n            # No need to train the mlp bias because we are adding it with attention output\n            params[f'{prefix}.attention.output.bias'] = layer.attention.output.bias\n            params[f'{prefix}.attention.qkv_lin.bias'] = layer.attention.qkv_lin.bias\n            params[f'{prefix}.ffn.dense_h_h4.bias'] = layer.ffn.dense_h_h4.bias\n        else:\n            pass\n\n        return params\n", "labml_nn/neox/utils/cache.py": "\"\"\"\n---\ntitle: Cache for Intermediate Activations\nsummary: >\n    Cache for intermediate activations for faster inference.\n---\n\n# Cache for Intermediate Activations\n\nDuring inference the model outputs token by token.\nWe use this simple cache to store key's and value's attention layers,\nso that we don't have to recompute them for previous tokens.\n\"\"\"\n\nfrom typing import Any\n\n\nclass Cache:\n    \"\"\"\n    ## Cache\n\n    This maintains a key-value cache and queues push values and pop them in the same order.\n    The queues are useful since we have multiple attention layers.\n    \"\"\"\n\n    def __init__(self):\n        self._cache = {}\n\n    def clear_all(self):\n        \"\"\"\n        ### Clear cache\n        \"\"\"\n        self._cache = {}\n\n    def push(self, name: str, value: Any):\n        \"\"\"\n        ### Push a value to a queue\n\n        :param name: is the name of the queue\n        :param value: is the value to be pushed\n        \"\"\"\n\n        # Create an empty queue if it's not present\n        if name not in self._cache:\n            self._cache[name] = []\n\n        # Push to the queue\n        self._cache[name].append(value)\n\n    def q_size(self, name):\n        \"\"\"\n        ### Return the size of the queue\n\n        :param name: is the name of the queue\n        :return: size of the queue if exists else None\n        \"\"\"\n\n        if name not in self._cache:\n            return None\n\n        if type(self._cache[name]) != list:\n            return None\n\n        return len(self._cache[name])\n\n    def pop(self, name: str):\n        \"\"\"\n        ### Pop from a queue\n\n        :param name: is the name of the queue\n        :return: the value\n        \"\"\"\n        return self._cache[name].pop(0)\n\n    def set(self, key: str, value: Any):\n        \"\"\"\n        ### Cache a value\n\n        :param key: is the name of the value to be cached\n        :param value: is the value\n        \"\"\"\n        self._cache[key] = value\n\n    def get(self, key: str, default: Any = None):\n        \"\"\"\n        ### Retrieve a value from cache\n\n        :param key: is the name used when caching\n        :param default: is the default value if the cache is empty\n        :return: the cached value\n        \"\"\"\n        return self._cache.get(key, default)\n\n    def clear(self, key: str):\n        \"\"\"\n        ### Clear a cache value\n\n        :param key: is the name used when caching\n        \"\"\"\n        del self._cache[key]\n\n\n# Singleton for cache\n_INSTANCE = None\n\n\ndef get_cache() -> Cache:\n    \"\"\"\n    ### Get the cache instance\n\n    :return: the cache instance\n    \"\"\"\n    global _INSTANCE\n\n    if _INSTANCE is None:\n        _INSTANCE = Cache()\n\n    return _INSTANCE\n", "labml_nn/neox/utils/__init__.py": "\"\"\"\n---\ntitle: Utilities and Helpers\nsummary: >\n    Utilities and helper functions\n---\n\n# Utilities and Helpers\n\n* [Cache for intermediate activations (for faster inference)](cache.html)\n* [Tools for finetuning](finetune.html)\n* [Trainer](trainer.html)\n* [Text dataset](text_dataset.html)\n\"\"\"\nimport typing\nfrom typing import List, Optional\n\nimport torch\n\nfrom labml import logger\nfrom labml.logger import Text\nfrom labml_nn.neox.tokenizer import get_tokenizer\n\nif typing.TYPE_CHECKING:\n    from tokenizers import Tokenizer\n\n# Tokenizer singleton\n_TOKENIZER: Optional['Tokenizer'] = None\n\n\ndef get_tokens(text: str) -> List[int]:\n    \"\"\"\n    ### Get token ids\n\n    :param text: is the text to tokenize\n    :return: the token ids\n    \"\"\"\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = get_tokenizer()\n    return _TOKENIZER.encode_batch([text])[0].ids\n\n\ndef print_token_outputs(ids: List[int], *xs: torch.Tensor):\n    \"\"\"\n    ### Print tokens from model outputs\n\n    Pretty prints target tokens along side outputs from the model(s).\n\n    :param ids: are the target token ids\n    :param xs: are the model(s) outputs\n    \"\"\"\n    ids = ids + [-1]\n    xs = [[-1] + x[0].max(dim=-1)[1].tolist() for x in xs]\n\n    print_tokens(ids, xs)\n\n\ndef print_tokens(target: List[int], others: List[List[int]]):\n    \"\"\"\n    ### Print tokens\n\n    Pretty prints tokens for comparison\n\n    :param target: are the target token ids\n    :param others: are the sampled outputs from the model(s)\n    \"\"\"\n\n    # Load tokenizer\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = get_tokenizer()\n\n    # Convert the tokens to list of strings\n    text = []\n    for i in range(len(target)):\n        tokens = [_TOKENIZER.decode([target[i]]) if target[i] != -1 else '---']\n        for j in range(len(others)):\n            tokens.append(_TOKENIZER.decode([others[j][i]]) if others[j][i] != -1 else '---')\n\n        text.append(tokens)\n\n    # Stats\n    correct = [0 for _ in others]\n    total = 0\n\n    # Iterate through tokens\n    for i in range(len(target)):\n        parts = [(f'{i}: ', Text.meta)]\n        parts += [('\"', Text.subtle), (text[i][0], Text.subtle), ('\"', Text.subtle), '\\t']\n\n        # Empty target\n        if target[i] == -1:\n            for j in range(len(others)):\n                parts += [('\"', Text.subtle), (text[i][j + 1], Text.subtle), ('\"', Text.subtle), '\\t']\n\n            logger.log(parts)\n            continue\n\n        # Number of tokens\n        total += 1\n\n        # Other outputs\n        for j in range(len(others)):\n            correct[j] += 1 if others[j][i] == target[i] else 0\n\n            parts += [('\"', Text.subtle),\n                      (text[i][j + 1], Text.success if others[j][i] == target[i] else Text.danger),\n                      ('\"', Text.subtle), '\\t']\n\n        logger.log(parts)\n\n    # Stats\n    parts = [(f'{total}', Text.highlight), '\\t']\n    for j in range(len(others)):\n        parts += [(f'{correct[j]}', Text.value), '\\t']\n    logger.log(parts)\n\n\ndef balance_layers_simple(n_layers: int, n_chunks: int):\n    \"\"\"\n    ### Balance layers\n\n    Split the `n_layers` into `n_chunks`. This is used for pipeline parallel training.\n\n    :param n_layers: is the number of layers\n    :param n_chunks: is the number of chunks\n    :return: returns a list with the number of layers for each chunk\n    \"\"\"\n    balance = []\n    for i in range(n_chunks):\n        balance.append((n_layers - sum(balance)) // (n_chunks - i))\n\n    return list(reversed(balance))\n", "labml_nn/neox/samples/llm_int8.py": "\"\"\"\n---\ntitle: Generate Text with GPT-NeoX using LLM.int8() quantization\nsummary: >\n     Generate Text with GPT-NeoX using LLM.int8() quantization\n---\n\n#  Generate Text with GPT-NeoX using LLM.int8() quantization\n\nThis shows how to generate text from GPT-NeoX using [LLM.int8() quantization](../utils/llm_int8.html).\n\nThis needs a GPU with 24GB memory.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml import monit\nfrom labml_nn.neox.model import LayerGenerator\nfrom labml_nn.neox.samples.generate import PROMPT, infer\nfrom labml_nn.neox.utils import get_tokens, print_tokens\nfrom labml_nn.neox.utils.cache import get_cache\n\n\ndef generate():\n    \"\"\"\n    ## Generate text\n    \"\"\"\n\n    # Setup [cache](../utils/cache.html) to cache intermediate key/value pairs for faster generation\n    cache = get_cache()\n    cache.set('use_cache', True)\n\n    # Device\n    device = torch.device('cuda:0')\n\n    # Load layers in float16 into CPU. We convert the layers to int8 later, because doing that\n    # on the fly after loading layers to GPU causes CUDA memory fragmentation\n    # (about 3GB memory can get lost due to fragmentation).\n    layer_generator = LayerGenerator(is_clone_layers=True,\n                                     dtype=torch.float16,\n                                     device=torch.device('cpu'),\n                                     is_llm_int8=False,\n                                     )\n    layers = list(layer_generator.load())\n\n    # This reduces CUDA memory fragmentation\n    for layer in monit.iterate('Convert to int8', layers, is_children_silent=True):\n        layer_generator.post_load_prepare(layer,\n                                          device=device,\n                                          is_llm_int8=True,\n                                          llm_int8_threshold=6.0,\n                                          )\n        layer.to(device)\n\n    # Create `nn.Sequential` model\n    model = nn.Sequential(*layers)\n\n    # Clear cache and print memory summary for debugging\n    torch.cuda.empty_cache()\n    print(torch.cuda.memory_summary())\n\n    # Get token ids\n    ids = get_tokens(PROMPT)\n\n    # Run the model.\n    # We use the [`infer`](generate.html) function defined in [`generate.py`](generate.html)\n    cache.set('state_ids', (None, 1))\n    with monit.section('Infer'):\n        next_token = infer(model, ids, device)[-1]\n\n    # Append the predicted token\n    ids += [next_token]\n\n    # Predict 100 tokens\n    for i in range(1, 100):\n        # Set the state to use cached activations\n        cache.set('state_ids', (i, i + 1))\n        # Get next token. Note that we only feed the last token to the model because\n        # we cache the key/value pairs of previous tokens.\n        with monit.section('Infer'):\n            next_token = infer(model, [next_token], device)[-1]\n        # Append the predicted token\n        ids += [next_token]\n        # Print\n        print_tokens(ids, [ids])\n\n\n#\nif __name__ == '__main__':\n    generate()\n", "labml_nn/neox/samples/finetune.py": "\"\"\"\n---\ntitle: Fine Tune GPT-NeoX\nsummary: >\n    Fine tune GPT-NeoX biases with Fairscale pipeline parallel module\n---\n\n# Fine Tune GPT-NeoX\n\nThis shows how to fine tune GPT-NeoX with pipeline parallelism.\n\"\"\"\n\nimport fairscale\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.utils.data\nimport typing\nfrom torch.utils.data import DataLoader, RandomSampler\n\nfrom labml import experiment, monit, tracker, lab\nfrom labml.configs import option\nfrom labml.logger import inspect\nfrom labml_nn.neox.utils.text_dataset import get_training_data\nfrom labml_nn.neox.utils.finetune import FineTuneBiases\nfrom labml_nn.neox.model import LayerGenerator, NeoXModule\nfrom labml_nn.neox.utils import balance_layers_simple\nfrom labml_nn.neox.utils.trainer import PipelineParallelTrainerConf\n\n\n@option(PipelineParallelTrainerConf.layers, 'PipelineBiases')\ndef neox_layers(c: PipelineParallelTrainerConf):\n    \"\"\"\n    ### Load GPT-NeoX layers\n    \"\"\"\n    return list(LayerGenerator(is_clone_layers=c.is_clone_layers,\n                               filter_layers=c.filter_layers,\n                               dtype=c.dtype,\n                               ).load())\n\n\n@option(PipelineParallelTrainerConf.fine_tuner, 'PipelineBiases')\ndef fine_tune_biases(c: PipelineParallelTrainerConf):\n    \"\"\"\n    ### Create fine tuner for biases\n    \"\"\"\n\n    fine_tuner = FineTuneBiases(typing.cast(typing.List[NeoXModule], c.layers))\n    # Mark biases as trainable\n    fine_tuner.set_trainable_params()\n\n    #\n    return fine_tuner\n\n\n@option(PipelineParallelTrainerConf.model, 'PipelineBiases')\ndef pipe_model(c: PipelineParallelTrainerConf):\n    \"\"\"\n    ### Create pipeline parallel model\n    \"\"\"\n\n    if c.is_checkpointing:\n        raise NotImplementedError()\n    else:\n        layers = c.layers\n\n    # Make sure the finetuner is initialized\n    _ = c.fine_tuner\n\n    # Create the Pipe module\n    with monit.section('Pipe'):\n        # Get the layer distribution across GPUs\n        balance = balance_layers_simple(len(layers), c.n_gpus)\n        inspect(balance=balance)\n        # Devices for each GPU\n        devices = [torch.device(f'cuda:{i}') for i in range(c.n_gpus)]\n        # Create Fairscale Pipe module\n        pipe_model = fairscale.nn.Pipe(nn.Sequential(*layers),\n                                       balance=balance,\n                                       devices=devices,\n                                       chunks=c.chunks)\n\n    #\n    return pipe_model\n\n\n@option(PipelineParallelTrainerConf.train_loader)\ndef tiny_shakespeare(c: PipelineParallelTrainerConf):\n    \"\"\"\n    #### Tiny Shakespeare dataset\n    \"\"\"\n    dataset = get_training_data(c.max_seq_len)\n\n    return DataLoader(dataset,\n                      batch_size=c.batch_size,\n                      sampler=RandomSampler(dataset, replacement=True))\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='pipe_neox_biases',\n                      writers={'screen', 'web_api'})\n\n    # Initialize configs\n    conf = PipelineParallelTrainerConf()\n    experiment.configs(conf, {\n        'learning_rate': 3e-4,\n        'is_checkpointing': False,\n        'max_seq_len': 128,\n        'batch_size': 64,\n        'chunks': 8,\n    })\n\n    # Start the experiment\n    with experiment.start():\n        # Initialize the model. Do this before the loop for cleaner logs.\n        _ = conf.model\n\n        # Train\n        for epoch in monit.loop(conf.epochs):\n            conf.train_epoch()\n            tracker.new_line()\n            torch.save(conf.fine_tuner.state_dict(), str(lab.get_data_path() / 'fine_tune.pt'))\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/neox/samples/generate.py": "\"\"\"\n---\ntitle: Generate Text with GPT-NeoX\nsummary: >\n     Generate Text with GPT-NeoX\n---\n\n#  Generate Text with GPT-NeoX\n\nThis shows how to generate text from GPT-NeoX with a single GPU.\n\nThis needs a GPU with more than 45GB memory.\n\"\"\"\n\n# Imports\nfrom typing import List\n\nimport torch\nfrom torch import nn\n\nfrom labml import monit\nfrom labml_nn.neox.model import LayerGenerator\nfrom labml_nn.neox.utils import get_tokens, print_tokens\nfrom labml_nn.neox.utils.cache import get_cache\n\n# List of layers to load. This is used for testing.\n# You can assign a subset of layers like `{0, 1}` so that it only loads\n# the first to transformer layers.\nLAYERS = None\n\n# Prompt to complete\nPROMPT = 'Einstein was born in the German Empire, but moved to Switzerland in 1895, forsaking his German'\n\n\ndef infer(model: nn.Module, ids: List[int], device: torch.device):\n    \"\"\"\n    ### Predict the next token\n\n    :param model: is the model\n    :param ids: are the input token ids\n    :param device: is the device of the model\n    \"\"\"\n\n    with torch.no_grad():\n        # Get the tokens\n        x = torch.tensor(ids)[None, :].to(device)\n        # Eval model\n        x = model(x)\n\n    # Return predicted token\n    return x[0].max(dim=-1)[1].tolist()\n\n\ndef generate():\n    \"\"\"\n    ## Generate text\n    \"\"\"\n\n    # Setup [cache](../utils/cache.html) to cache intermediate key/value pairs for faster generation\n    cache = get_cache()\n    cache.set('use_cache', True)\n\n    # Device\n    device = torch.device('cuda:0')\n\n    # Load layers\n    layers = list(LayerGenerator(is_clone_layers=True,\n                                 filter_layers=LAYERS,\n                                 dtype=torch.float16,\n                                 device=device,\n                                 ).load())\n\n    model = nn.Sequential(*layers)\n\n    # Get token ids\n    ids = get_tokens(PROMPT)\n\n    # Run the model\n    cache.set('state_ids', (None, 1))\n    with monit.section('Infer'):\n        next_token = infer(model, ids, device)[-1]\n\n    # Append the predicted token\n    ids += [next_token]\n\n    # Predict 100 tokens\n    for i in range(1, 100):\n        # Set the state to use cached activations\n        cache.set('state_ids', (i, i + 1))\n        # Get next token. Note that we only feed the last token to the model because\n        # we cache the key/value pairs of previous tokens.\n        with monit.section('Infer'):\n            next_token = infer(model, [next_token], device)[-1]\n        # Append the predicted token\n        ids += [next_token]\n        # Print\n        print_tokens(ids, [ids])\n\n\n#\nif __name__ == '__main__':\n    generate()\n", "labml_nn/neox/samples/__init__.py": "\"\"\"\n---\ntitle: Samples\nsummary: >\n    Samples for inference and fine-tuning\n---\n\n# Samples\n\n* [Generating text](generate.html)\n* [Fine tuning the biases with pipeline-parallel training](finetune.html)\n\"\"\"", "labml_nn/neox/evaluation/llm_int8.py": "\"\"\"\n---\ntitle: Evaluate GPT-NeoX using LLM.int8() quantization on test suite\nsummary: >\n     Evaluate GPT-NeoX using LLM.int8() quantization on test suite\n---\n\n#  Evaluate GPT-NeoX using LLM.int8() quantization on test suite\n\nThis code evaluate [GPT-NeoX](../index.html) using [LLM.int8() quantization](../utils/llm_int8.html),\non a suite of tasks.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml import monit\nfrom labml_nn.neox.evaluation import run_eval_harness\nfrom labml_nn.neox.model import LayerGenerator\n\n\ndef main():\n    # Device\n    device = torch.device('cuda:0')\n\n    # Load layers in float16 into CPU. We convert the layers to int8 later, because doing that\n    # on the fly after loading layers to GPU causes CUDA memory fragmentation\n    # (about 3GB memory can get lost due to fragmentation).\n    layer_generator = LayerGenerator(is_clone_layers=True,\n                                     dtype=torch.float16,\n                                     device=torch.device('cpu'),\n                                     )\n    # Load layers\n    layers = list(layer_generator.load())\n\n    # This reduces CUDA memory fragmentation\n    for layer in monit.iterate('Convert to int8', layers, is_children_silent=True):\n        layer_generator.post_load_prepare(layer,\n                                          device=device,\n                                          is_llm_int8=True,\n                                          llm_int8_threshold=6.0,\n                                          )\n        layer.to(device)\n\n    # Create `nn.Sequential` model\n    model = nn.Sequential(*layers)\n\n    # Run [evaluation harness](index.html)\n    print(run_eval_harness(model, 'half_precision', [], device))\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/neox/evaluation/half_precision.py": "\"\"\"\n---\ntitle: Evaluate GPT-NeoX using LLM.int8() quantization on test suite\nsummary: >\n     Evaluate GPT-NeoX using LLM.int8() quantization on test suite\n---\n\n#  Evaluate GPT-NeoX using LLM.int8() quantization on test suite\n\nThis code evaluate [GPT-NeoX](../index.html) using, on a suite of tasks.\n\"\"\"\nimport argparse\n\nimport torch\nfrom torch import nn\n\nfrom labml_nn.neox.evaluation import run_eval_harness\nfrom labml_nn.neox.model import LayerGenerator\n\n\ndef main():\n    # Argument parser\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--flash\", action='store_true', help=\"whether to use Flash Attention\")\n\n    opt = parser.parse_args()\n\n    # Device\n    device = torch.device('cuda:0')\n    # Load layers\n    layers = list(LayerGenerator(is_clone_layers=True,\n                                 filter_layers=None,\n                                 dtype=torch.float16,\n                                 device=device,\n                                 is_flash_attention=opt.flash,\n                                 ).load())\n\n    # Create `nn.Sequential` model\n    model = nn.Sequential(*layers)\n\n    # Run [evaluation harness](index.html)\n    print(run_eval_harness(model, 'half_precision', ['lambada'], device))\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/neox/evaluation/__init__.py": "\"\"\"\n---\ntitle: Evaluation\nsummary: >\n    Code to evaluate the model on NLP tasks through lm-evaluation-harness\n---\n\n# Evaluation\n\nThis is the code to test the model on\n[EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\n* [Evaluating half precision model on a single GPU](half_precision.html)\n\"\"\"\nimport math\nfrom typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom lm_eval import tasks, evaluator, utils\nfrom lm_eval.base import BaseLM\nfrom tokenizers import Tokenizer\nfrom torch import nn\nfrom tqdm import tqdm\n\nfrom labml import monit\nfrom labml_nn.neox.tokenizer import get_tokenizer\n\n\nclass EvalHarnessAdapter(BaseLM):\n    \"\"\"\n    ## Evaluation Harness Adapter\n\n    This is based on the [adapter from EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py)\n    \"\"\"\n\n    def __init__(self, tokenizer: Tokenizer, vocab_size: int, batch_size: int):\n        \"\"\"\n        :param tokenizer: is the [Huggingface Tokenizer](huggingface/tokenizers)\n        :param vocab_size: is the size of the vocabulary\n         (this differs from the tokenizer vocab size since neox adds some extra to make the embedding layer\n         model parallel.)\n        :param batch_size: is the batch size\n        \"\"\"\n        super().__init__()\n        self.tokenizer = tokenizer\n        self._eot_token_id = self.tokenizer.token_to_id(\"<|endoftext|>\")\n        self._vocab_size = vocab_size\n\n        self._batch_size = batch_size\n\n    @property\n    def device(self):\n        raise RuntimeError()\n\n    @property\n    def vocab_size(self):\n        \"\"\"Size of the vocabulary\"\"\"\n        return self._vocab_size\n\n    @property\n    def eot_token_id(self):\n        \"\"\"End-of-text token\"\"\"\n        return self._eot_token_id\n\n    @property\n    def max_length(self):\n        \"\"\"Maximum sequence length\"\"\"\n        return 2048\n\n    @property\n    def max_gen_toks(self):\n        \"\"\"Maximum number of tokens to generate\"\"\"\n        return 128\n\n    @property\n    def batch_size(self):\n        \"\"\"\n        Batch size\n        \"\"\"\n        return self._batch_size\n\n    def tok_encode(self, string: str):\n        \"\"\"\n        Encode a given text\n        \"\"\"\n        return self.tokenizer.encode(string).ids\n\n    def tok_decode(self, tokens: List[int]):\n        \"\"\"\n        Decode text from token ids\n        \"\"\"\n        return self.tokenizer.decode(tokens)\n\n    def _model_call(self, inps: torch.Tensor):\n        raise NotImplementedError\n\n    def _model_generate(self, context, max_length, eos_token_id):\n        raise RuntimeError()\n\n    def greedy_until(self, requests):\n        raise RuntimeError()\n\n    @torch.no_grad()\n    def _loglikelihood_tokens(self, requests, disable_tqdm=False):\n        \"\"\"\n        ### Get log-likelihoods of the next tokens\n\n        :param requests: List of requests containing the context and the expected continuation.\n        :param disable_tqdm: If True, disable tqdm progress bar.\n        \"\"\"\n\n        # For results\n        res = []\n\n        # Reorder the requests in the descending order of the lengths,\n        # so that sequences with similar lengths are close\n        def _collate(x):\n            toks = x[1] + x[2]\n            return -len(toks), tuple(toks)\n\n        reord = utils.Reorderer(requests, _collate)\n\n        # Loop through requests with `batch_size` number of requests at a time\n        for chunk in utils.chunks(tqdm(reord.get_reordered(), disable=disable_tqdm), self.batch_size):\n            # To store the inputs for the batch\n            inps = []\n            # The continuations for the batch\n            continuations = []\n            # Lengths of the input sequences\n            inplens = []\n            # Padded length for the batch\n            padded_length = None\n            # Loop through each request in the chunk and collect them into PyTorch tensors with paddings\n            for _, context_enc, continuation_enc in chunk:\n                # Concatenate the context and continuation\n                inp = context_enc + continuation_enc\n                # Truncate from left if the size exceeds the `max_length`\n                inp = inp[-(self.max_length + 1):]\n                # Remove final token\n                inp = inp[:-1]\n                # Create a tensor\n                inp = torch.tensor(inp, dtype=torch.long)\n                # Input length\n                inplen = inp.shape[0]\n\n                # Determine the padded length.\n                # Shorter sequences will get padded.\n                if padded_length is None:\n                    padded_length = int(math.ceil(inplen / 32)) * 32\n                # padded_length = padded_length if padded_length is not None else inplen\n\n                # Padding\n                padding = torch.zeros(padded_length - inplen, dtype=torch.long)\n\n                # Add padding\n                inp = torch.cat([inp, padding], dim=0)\n\n                inps.append(inp)\n                continuations.append(continuation_enc)\n                inplens.append(inplen)\n\n            # Get model logits\n            logits = self._model_call(torch.stack(inps))\n\n            # Get log softmaxes\n            multi_logits = F.log_softmax(logits, dim=-1)\n\n            # Loop through the input/output pairs of the batch\n            for logits, inplen, cont_toks in zip(multi_logits, inplens, continuations):\n                # Get number of predicted tokens\n                contlen = len(cont_toks)\n                # Get logits of those\n                logits = logits[inplen - contlen: inplen]\n                # Get the tokens with the highest probabilities\n                greedy_tokens = logits.argmax(dim=-1)\n                # Get the target tokens\n                cont_toks = torch.tensor(cont_toks, dtype=torch.long).to(logits.device)\n                # Whether there's an exact match\n                max_equal = (greedy_tokens == cont_toks).all()\n                # Log-likelihoods of the target tokens\n                logits = torch.gather(logits, 1, cont_toks[:, None])\n                # Add the total log-likelihoods and whether there was a match to the results\n                res.append((float(logits.sum()), bool(max_equal)))\n\n        # Re-order and return results\n        return reord.get_original(res)\n\n    @torch.no_grad()\n    def run_eval(self, name: str, eval_tasks: List[str]):\n        \"\"\"\n        ### Run given evaluations\n        \"\"\"\n\n        # Run [EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) evaluator\n        results = evaluator.evaluate(lm=self, task_dict=tasks.get_task_dict(eval_tasks))\n\n        # Add configs\n        results[\"config\"] = {\n            \"name\": name,\n        }\n\n        #\n        return results\n\n\nclass NoeXEvalHarnessAdapter(EvalHarnessAdapter):\n    \"\"\"\n    ## Evaluation Harness Adapter\n\n    This is based on the [adapter from EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py)\n    \"\"\"\n\n    def __init__(self, model: nn.Module, tokenizer: Tokenizer, vocab_size: int, batch_size: int, device: torch.device):\n        \"\"\"\n        :param model: is model\n        :param tokenizer: is the [Huggingface Tokenizer](huggingface/tokenizers)\n        :param vocab_size: is the size of the vocabulary\n         (this differs from the tokenizer vocab size since neox adds some extra to make the embedding layer\n         model parallel.)\n        :param batch_size: is the batch size\n        :param device: is the device of the model\n        \"\"\"\n        super().__init__(tokenizer, vocab_size, batch_size)\n        self.model = model\n        self._device = device\n\n    def _model_call(self, inps: torch.Tensor):\n        \"\"\"\n        Call the model\n        \"\"\"\n        return self.model(inps.to(self._device))\n\n\ndef run_eval_harness(model: nn.Module, name: str, eval_tasks: List[str], device: torch.device, batch_size: int = 8):\n    \"\"\"\n    ## Run evaluation harness with a given model\n    \"\"\"\n\n    # Load the tokenizer\n    with monit.section('Load tokenizer'):\n        tokenizer = get_tokenizer()\n\n    # All tasks if nothing is specified\n    if not eval_tasks:\n        eval_tasks = [\n            \"anli_r1\",\n            \"anli_r2\",\n            \"anli_r3\",\n            \"hellaswag\",\n            \"lambada\",\n            \"piqa\",\n            \"winogrande\",\n            \"wsc\",\n            \"mathqa\",\n        ]\n\n    # Create the adapter\n    adapter = NoeXEvalHarnessAdapter(model, tokenizer, 50_432, batch_size, device)\n\n    # Run\n    return adapter.run_eval(name, eval_tasks)\n", "labml_nn/sampling/experiment.py": "\"\"\"\n---\ntitle: Trying out Sampling Techniques for Language Models\nsummary: >\n We try out different sampling techniques for language models on HuggingFace's GPT2 model.\n---\n\n# Trying out Sampling Techniques for Language Models\n\n* [Greedy Sampling](greedy.html)\n* [Temperature Sampling](temperature.html)\n* [Top-k Sampling](top_k.html)\n* [Nucleus Sampling](nucleus.html)\n\nThis experiment uses the above sampling techniques, on HuggingFace's GPT2 model.\n\"\"\"\n\nimport torch\n\nfrom labml import monit, logger, lab\n\nfrom labml.logger import Text\n\nfrom labml_nn.sampling import Sampler\nfrom labml_nn.sampling.greedy import GreedySampler\nfrom labml_nn.sampling.nucleus import NucleusSampler\nfrom labml_nn.sampling.temperature import TemperatureSampler\nfrom labml_nn.sampling.top_k import TopKSampler\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n\n@torch.no_grad()\ndef sample(model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, sampler: Sampler,\n           n_samples: int, n_tokens: int, seq_len: int, prompt: str):\n    \"\"\"\n    ## Sample from model\n\n    :param model: is the model to sample from\n    :param tokenizer: is the tokenizer to use\n    :param sampler: is the sampler to use\n    :param n_samples: is the number of samples to generate\n    :param n_tokens: is the number of tokens to generate\n    :param seq_len: is the maximum sequence length for the model\n    :param prompt: is the starting prompt\n    \"\"\"\n    # Tokenize the `prompt` and make `n_samples` copies of it\n    data = torch.tile(torch.tensor(tokenizer.encode(prompt))[None, :], (n_samples, 1))\n\n    # Collect output for printing\n    logs = [[(prompt, Text.meta)] for _ in range(n_samples)]\n    # Sample `n_tokens`\n    for i in monit.iterate('Sample', n_tokens):\n        # Truncate the data to the maximum sequence length\n        data = data[-seq_len:]\n        # Get the model output. The 'logits' has shape `[batch_size, seq_len, n_tokens]`\n        logits = model(data)[0]\n        # Get the `logits` of the last token\n        logits = logits[:, -1]\n        # Sample from the `logits`\n        res = sampler(logits)\n        # Add the sampled token to the data\n        data = torch.cat([data, res[:, None]], dim=1)\n        # Decode and add the sampled token for logging\n        for j in range(n_samples):\n            logs[j] += [('' + tokenizer.decode(res[j]), Text.value)]\n\n    # Print the sampled outputs\n    for j in range(n_samples):\n        logger.log(logs[j])\n\n\ndef main():\n    \"\"\"\n    ### Try different sampling techniques\n    \"\"\"\n\n    # Load the model and tokenizer\n    with monit.section('Load tokenizer/model'):\n        tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=lab.get_data_path() / 'cache')\n        model = GPT2LMHeadModel.from_pretrained('gpt2', cache_dir=lab.get_data_path() / 'cache')\n    # Set the model to eval mode\n    model.eval()\n\n    # Prompts to use for sampling\n    prompt = 'I saw an interesting dream last night. '\n\n    # [Greedy Sampling](greedy.html)\n    with monit.section('greedy'):\n        sample(model, tokenizer, GreedySampler(), 4, 32, 128, prompt)\n\n    # [Temperature Sampling](temperature.html)\n    with monit.section('temperature=1.'):\n        sample(model, tokenizer, TemperatureSampler(1.), 4, 32, 128, prompt)\n    with monit.section('temperature=.1'):\n        sample(model, tokenizer, TemperatureSampler(.1), 4, 32, 128, prompt)\n    with monit.section('temperature=10.'):\n        sample(model, tokenizer, TemperatureSampler(10.), 4, 32, 128, prompt)\n\n    # [Top-k Sampling](top_k.html)\n    with monit.section('top_k=5'):\n        sample(model, tokenizer, TopKSampler(2, TemperatureSampler(1.)), 4, 32, 128, prompt)\n\n    # [Nucleus Sampling](nucleus.html)\n    with monit.section('nucleus p=.95'):\n        sample(model, tokenizer, NucleusSampler(0.95, TemperatureSampler(1.)), 4, 32, 128, prompt)\n    with monit.section('nucleus p=.1'):\n        sample(model, tokenizer, NucleusSampler(0.1, TemperatureSampler(1.)), 4, 32, 128, prompt)\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/sampling/top_k.py": "\"\"\"\n---\ntitle: Top-k Sampling\nsummary: A PyTorch implementation of top-k sampling from language models.\n---\n\n# Top-k Sampling\n\nHere we first pick the top-k tokens from the distribution of logits, and then\nsample from them.\n\nHere's an [experiment](experiment.html) that uses these sampling techniques.\n\"\"\"\n\nimport torch\n\nfrom labml_nn.sampling import Sampler\n\n\nclass TopKSampler(Sampler):\n    \"\"\"\n    ## Top-k Sampler\n    \"\"\"\n    def __init__(self, k: int, sampler: Sampler):\n        \"\"\"\n        :param k: is the number of tokens to pick\n        :param sampler: is the sampler to use for the top-k tokens\n\n        `sampler` can be any sampler that takes a logits tensor as input and returns a token tensor;\n         e.g. [`TemperatureSampler'](temperature.html).\n        \"\"\"\n        self.k = k\n        self.sampler = sampler\n\n    def __call__(self, logits: torch.Tensor):\n        \"\"\"\n        Sample from logits\n        \"\"\"\n        # New logits filled with $-\\infty$; i.e. zero probability\n        zeros = logits.new_ones(logits.shape) * float('-inf')\n        # Pick the largest $k$ logits and their indices\n        values, indices = torch.topk(logits, self.k, dim=-1)\n        # Set the values of the top-k selected indices to actual logits.\n        # Logits of other tokens remain $-\\infty$\n        zeros.scatter_(-1, indices, values)\n\n        # Sample from the top-k logits with the specified sampler.\n        return self.sampler(zeros)\n", "labml_nn/sampling/nucleus.py": "\"\"\"\n---\ntitle: Nucleus Sampling\nsummary: A PyTorch implementation of nucleus sampling from language models.\n---\n\n# Nucleus Sampling\n\nThis is an implementation of nucleus sampling, introduced in the paper\n[The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751).\n\nThe paper discusses the problems with other sampling methods such as Beam Search,\n[Pure sampling](temperature.html), [Temperature sampling](temperature.html), and\n[Top-k sampling](top_k.html). The paper introduces the idea of nucleus sampling,\nwhich practically performs better than other sampling methods for text generation.\n\nNucleus sampling first picks a subset of the vocabulary $V^{(p)} \\subset V$,\nwhere $V^{(p)}$ is smallest set of tokens such that\n\n$$\\sum_{x_i \\in V^{(p)}} P(x_i | x_{1:i-1}) \\ge p$$\n\nThat is, we pick the highest probable tokens until the sum of their probabilities is less that $p$.\n\nThen we sample from the selected tokens.\n\nHere's an [experiment](experiment.html) that uses these sampling techniques.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_nn.sampling import Sampler\n\n\nclass NucleusSampler(Sampler):\n    \"\"\"\n    ## Nucleus Sampler\n    \"\"\"\n    def __init__(self, p: float, sampler: Sampler):\n        \"\"\"\n        :param p: is the sum of probabilities of tokens to pick $p$\n        :param sampler: is the sampler to use for the selected tokens\n        \"\"\"\n        self.p = p\n        self.sampler = sampler\n        # Softmax to compute $P(x_i | x_{1:i-1})$ from the logits\n        self.softmax = nn.Softmax(dim=-1)\n\n    def __call__(self, logits: torch.Tensor):\n        \"\"\"\n        Sample from logits with Nucleus Sampling\n        \"\"\"\n\n        # Get probabilities $P(x_i | x_{1:i-1})$\n        probs = self.softmax(logits)\n\n        # Sort probabilities in descending order\n        sorted_probs, indices = torch.sort(probs, dim=-1, descending=True)\n        # Get the cumulative sum of probabilities in the sorted order\n        cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)\n        # Find the cumulative sums less than $p$.\n        nucleus = cum_sum_probs < self.p\n        # Prepend ones so that we add one token after the minimum number\n        # of tokens with cumulative probability less that $p$.\n        nucleus = torch.cat([nucleus.new_ones(nucleus.shape[:-1] + (1,)), nucleus[..., :-1]], dim=-1)\n\n        # Get log probabilities and mask out the non-nucleus\n        sorted_log_probs = torch.log(sorted_probs)\n        sorted_log_probs[~nucleus] = float('-inf')\n\n        # Sample from the sampler\n        sampled_sorted_indexes = self.sampler(sorted_log_probs)\n\n        # Get the actual indexes\n        res = indices.gather(-1, sampled_sorted_indexes.unsqueeze(-1))\n\n        #\n        return res.squeeze(-1)\n", "labml_nn/sampling/temperature.py": "\"\"\"\n---\ntitle: Sampling from Language Models with Temperature\nsummary: A PyTorch implementation of sampling from language models with temperature.\n---\n\n# Sampling from Language Models with Temperature\n\nHere we sample from the following probability distribution where $V$ is the vocabulary,\n$u_{1:|V|}$ are the logits of the distribution and T is the temperature:\n\n$$P(x_i=V_l | x_{1:i-1}) = \\frac{\\exp(\\frac{u_l}{T})}{\\sum_j \\exp(\\frac{u_j}{T})}$$\n\n$T = 1$ is normal random sampling.\n\nHere's an [experiment](experiment.html) that uses these sampling techniques.\n\"\"\"\n\nimport torch\nfrom torch.distributions import Categorical\n\nfrom labml_nn.sampling import Sampler\n\n\nclass TemperatureSampler(Sampler):\n    \"\"\"\n    ## Sampler with Temperature\n    \"\"\"\n    def __init__(self, temperature: float = 1.0):\n        \"\"\"\n        :param temperature: is the temperature to sample with\n        \"\"\"\n        self.temperature = temperature\n\n    def __call__(self, logits: torch.Tensor):\n        \"\"\"\n        Sample from logits\n        \"\"\"\n\n        # Create a categorical distribution with temperature adjusted logits\n        dist = Categorical(logits=logits / self.temperature)\n\n        # Sample\n        return dist.sample()\n", "labml_nn/sampling/experiment_tiny.py": "from typing import Tuple\n\nimport torch\n\nfrom labml import experiment, monit\nfrom labml import logger\nfrom labml.logger import Text\nfrom labml_helpers.datasets.text import TextDataset\nfrom labml_nn.sampling import Sampler\nfrom labml_nn.sampling.greedy import GreedySampler\nfrom labml_nn.sampling.nucleus import NucleusSampler\nfrom labml_nn.sampling.temperature import TemperatureSampler\nfrom labml_nn.sampling.top_k import TopKSampler\nfrom labml_nn.transformers.basic.autoregressive_experiment import Configs, AutoregressiveTransformer\n\n\ndef get_model_dataset(run_uuid: str) -> Tuple[AutoregressiveTransformer, TextDataset]:\n    experiment.evaluate()\n\n    conf = Configs()\n\n    experiment.configs(conf, experiment.load_configs(run_uuid))\n\n    experiment.load(run_uuid)\n\n    experiment.add_pytorch_models({'model': conf.model})\n\n    experiment.start()\n\n    return conf.model, conf.text\n\n\ndef sample(model, ds, sampler: Sampler, n_samples: int, n_tokens: int, seq_len: int, prompt: str):\n    with torch.no_grad():\n        data = torch.tile(ds.text_to_i(prompt)[:, None], (1, n_samples))\n\n        # Collect output for printing\n        logs = [[(prompt, Text.meta)] for _ in range(n_samples)]\n        # Sample 25 tokens\n        for i in monit.iterate('Sample', n_tokens):\n            # Tokenize the prompt\n            data = data[-seq_len:]\n            # Get the model output\n            logits, *_ = model(data)\n            logits = logits[-1]\n            # Get the model prediction (greedy)\n            res = sampler(logits)\n            data = torch.cat([data, res[None, :]], dim=0)\n            # Add the prediction for logging\n            for j in range(n_samples):\n                logs[j] += [('' + ds.itos[res[j]], Text.value)]\n\n    # Print the sampled output\n    for j in range(n_samples):\n        logger.log(logs[j])\n\n\ndef main():\n    model, ds = get_model_dataset('074d4004cc6b11ecad7a0242ac1c0002')\n    model.eval()\n\n    with monit.section('greedy'):\n        sample(model, ds, GreedySampler(), 4, 32, 128, 'It is')\n\n    with monit.section('temperature=1.'):\n        sample(model, ds, TemperatureSampler(1.), 4, 32, 128, 'It is')\n    with monit.section('temperature=.1'):\n        sample(model, ds, TemperatureSampler(.1), 4, 32, 128, 'It is')\n    with monit.section('temperature=10.'):\n        sample(model, ds, TemperatureSampler(10.), 4, 32, 128, 'It is')\n\n    with monit.section('top_k=5'):\n        sample(model, ds, TopKSampler(2, TemperatureSampler(1.)), 4, 32, 128, 'It is')\n\n    with monit.section('nucles p=.95'):\n        sample(model, ds, NucleusSampler(0.95, TemperatureSampler(1.)), 4, 32, 128, 'It is')\n    with monit.section('nucles p=.95'):\n        sample(model, ds, NucleusSampler(0.1, TemperatureSampler(1.)), 4, 32, 128, 'It is')\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/sampling/greedy.py": "\"\"\"\n---\ntitle: Greedy Sampling\nsummary: A PyTorch implementation of greedy sampling from language models.\n---\n\n# Greedy Sampling\n\nHere we sample the most likely token from the distribution of logits.\n\nHere's an [experiment](experiment.html) that uses these sampling techniques.\n\"\"\"\n\nimport torch\n\nfrom labml_nn.sampling import Sampler\n\n\nclass GreedySampler(Sampler):\n    def __call__(self, logits: torch.Tensor):\n        \"\"\"\n        Sample the most likely token from the distribution of logits\n        \"\"\"\n        return logits.argmax(dim=-1)\n", "labml_nn/sampling/__init__.py": "\"\"\"\n---\ntitle: Sampling Techniques for Language Models\nsummary: >\n A set of PyTorch implementations/tutorials of sampling techniques for language models.\n---\n\n# Sampling Techniques for Language Models\n\n* [Greedy Sampling](greedy.html)\n* [Temperature Sampling](temperature.html)\n* [Top-k Sampling](top_k.html)\n* [Nucleus Sampling](nucleus.html)\n\nHere's an [experiment](experiment.html) that uses these sampling techniques.\n\"\"\"\n\nimport torch\n\n\nclass Sampler:\n    \"\"\"\n    ### Sampler base class\n    \"\"\"\n    def __call__(self, logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        ### Sample from logits\n\n        :param logits: are the logits of the distribution of shape `[..., n_tokens]`\n        \"\"\"\n        raise NotImplementedError()\n", "labml_nn/transformers/positional_encoding.py": "\"\"\"\n---\ntitle: Fixed Positional Encodings\nsummary: >\n  Implementation with explanation of fixed positional encodings as\n  described in paper Attention is All You Need.\n---\n\n# Fixed Positional Encodings\n\nThe positional encoding encodes the position along the sequence into\n a vector of size `d_model`.\n\n\\begin{align}\nPE_{p,2i} &= sin\\Bigg(\\frac{p}{10000^{\\frac{2i}{d_{model}}}}\\Bigg) \\\\\nPE_{p,2i + 1} &= cos\\Bigg(\\frac{p}{10000^{\\frac{2i}{d_{model}}}}\\Bigg)\n\\end{align}\n\nWhere $1 \\leq 2i, 2i + 1 \\leq d_{model}$\n are the feature indexes in the encoding, and $p$ is the position.\n\"\"\"\n\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, dropout_prob: float, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_prob)\n\n        self.register_buffer('positional_encodings', get_positional_encoding(d_model, max_len), False)\n\n    def forward(self, x: torch.Tensor):\n        pe = self.positional_encodings[:x.shape[0]].detach().requires_grad_(False)\n        x = x + pe\n        x = self.dropout(x)\n        return x\n\n\ndef get_positional_encoding(d_model: int, max_len: int = 5000):\n    # Empty encodings vectors\n    encodings = torch.zeros(max_len, d_model)\n    # Position indexes\n    position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n    # $2 * i$\n    two_i = torch.arange(0, d_model, 2, dtype=torch.float32)\n    # $10000^{\\frac{2i}{d_{model}}}$\n    div_term = torch.exp(two_i * -(math.log(10000.0) / d_model))\n    # $PE_{p,2i} = sin\\Bigg(\\frac{p}{10000^{\\frac{2i}{d_{model}}}}\\Bigg)$\n    encodings[:, 0::2] = torch.sin(position * div_term)\n    # $PE_{p,2i + 1} = cos\\Bigg(\\frac{p}{10000^{\\frac{2i}{d_{model}}}}\\Bigg)$\n    encodings[:, 1::2] = torch.cos(position * div_term)\n\n    # Add batch dimension\n    encodings = encodings.unsqueeze(1).requires_grad_(False)\n\n    return encodings\n\n\ndef _test_positional_encoding():\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(15, 5))\n    pe = get_positional_encoding(20, 100)\n    plt.plot(np.arange(100), pe[:, 0, 4:8].numpy())\n    plt.legend([\"dim %d\" % p for p in [4, 5, 6, 7]])\n    plt.title(\"Positional encoding\")\n    plt.show()\n\n\nif __name__ == '__main__':\n    _test_positional_encoding()\n", "labml_nn/transformers/utils.py": "\"\"\"\n---\ntitle: Utilities for Transformer\nsummary: A bunch of utility functions and classes for transformers.\n---\n\n# Utilities for Transformer\n\"\"\"\n\nimport torch\n\n\ndef subsequent_mask(seq_len):\n    \"\"\"\n    ## Subsequent mask to mask out data from future (subsequent) time steps\n    \"\"\"\n    mask = torch.tril(torch.ones(seq_len, seq_len)).to(torch.bool).unsqueeze(-1)\n    return mask\n\n\ndef _subsequent_mask():\n    from labml.logger import inspect\n    inspect(subsequent_mask(10)[:, :, 0])\n\n\nif __name__ == '__main__':\n    _subsequent_mask()\n", "labml_nn/transformers/label_smoothing_loss.py": "\"\"\"\n---\ntitle: Label Smoothing Loss\nsummary: >\n  This is an implementation of label smoothing loss, that can be used as\n  an alternative to cross entropy loss for improved accuracy.\n---\n\n# Label Smoothing Loss\n\"\"\"\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom labml_helpers.module import Module\n\n\nclass LabelSmoothingLoss(Module):\n    def __init__(self, size: int, padding_idx: int, smoothing: float = 0.0):\n        super().__init__()\n        self.loss = nn.KLDivLoss(reduction='sum')\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n\n    def forward(self, x: torch.Tensor, target: torch.Tensor):\n        assert x.shape[1] == self.size\n        true_dist = x.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target == self.padding_idx, as_tuple=False)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        return self.loss(x, true_dist.detach())\n\n\ndef _test_label_smoothing():\n    smooth_loss = LabelSmoothingLoss(5, 0, 0.4)\n    predict = torch.tensor([[0, 0.2, 0.7, 0.1, 0],\n                            [0, 0.2, 0.7, 0.1, 0],\n                            [0, 0.2, 0.7, 0.1, 0]], dtype=torch.float)\n    _ = smooth_loss(predict.log(),\n                    torch.tensor([2, 1, 0], dtype=torch.long))\n\n    # Show the target distributions expected by the system.\n    plt.imshow(smooth_loss.true_dist)\n    plt.show()\n\n    smooth_loss = LabelSmoothingLoss(5, 0, 0.1)\n\n    def loss_sample(x):\n        d = x + 3 * 1\n        predict2 = torch.tensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n                                 ], dtype=torch.float)\n        # print(predict)\n        return smooth_loss(predict2.log(),\n                           torch.tensor([1], dtype=torch.long)).item()\n\n    plt.plot(np.arange(1, 100), [loss_sample(x) for x in range(1, 100)])\n    plt.show()\n\n\nif __name__ == '__main__':\n    _test_label_smoothing()\n", "labml_nn/transformers/models.py": "\"\"\"\n---\ntitle: Transformer Encoder and Decoder Models\nsummary: >\n  These are PyTorch implementations of Transformer based encoder and decoder models,\n  as well as other related modules.\n---\n\n# Transformer Encoder and Decoder Models\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/basic/autoregressive_experiment.ipynb)\n\"\"\"\nimport math\n\nimport torch\nimport torch.nn as nn\n\nfrom labml_nn.utils import clone_module_list\nfrom .feed_forward import FeedForward\nfrom .mha import MultiHeadAttention\nfrom .positional_encoding import get_positional_encoding\n\n\nclass EmbeddingsWithPositionalEncoding(nn.Module):\n    \"\"\"\n    <a id=\"EmbeddingsWithPositionalEncoding\"></a>\n\n    ## Embed tokens and add [fixed positional encoding](positional_encoding.html)\n    \"\"\"\n\n    def __init__(self, d_model: int, n_vocab: int, max_len: int = 5000):\n        super().__init__()\n        self.linear = nn.Embedding(n_vocab, d_model)\n        self.d_model = d_model\n        self.register_buffer('positional_encodings', get_positional_encoding(d_model, max_len))\n\n    def forward(self, x: torch.Tensor):\n        pe = self.positional_encodings[:x.shape[0]].requires_grad_(False)\n        return self.linear(x) * math.sqrt(self.d_model) + pe\n\n\nclass EmbeddingsWithLearnedPositionalEncoding(nn.Module):\n    \"\"\"\n    <a id=\"EmbeddingsWithLearnedPositionalEncoding\"></a>\n\n    ## Embed tokens and add parameterized positional encodings\n    \"\"\"\n\n    def __init__(self, d_model: int, n_vocab: int, max_len: int = 5000):\n        super().__init__()\n        self.linear = nn.Embedding(n_vocab, d_model)\n        self.d_model = d_model\n        self.positional_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n\n    def forward(self, x: torch.Tensor):\n        pe = self.positional_encodings[:x.shape[0]]\n        return self.linear(x) * math.sqrt(self.d_model) + pe\n\n\nclass TransformerLayer(nn.Module):\n    \"\"\"\n    <a id=\"TransformerLayer\"></a>\n\n    ## Transformer Layer\n\n    This can act as an encoder layer or a decoder layer. We use pre-norm.\n    \"\"\"\n\n    def __init__(self, *,\n                 d_model: int,\n                 self_attn: MultiHeadAttention,\n                 src_attn: MultiHeadAttention = None,\n                 feed_forward: FeedForward,\n                 dropout_prob: float):\n        \"\"\"\n        * `d_model` is the token embedding size\n        * `self_attn` is the self attention module\n        * `src_attn` is the source attention module (when this is used in a decoder)\n        * `feed_forward` is the feed forward module\n        * `dropout_prob` is the probability of dropping out after self attention and FFN\n        \"\"\"\n        super().__init__()\n        self.size = d_model\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.dropout = nn.Dropout(dropout_prob)\n        self.norm_self_attn = nn.LayerNorm([d_model])\n        if self.src_attn is not None:\n            self.norm_src_attn = nn.LayerNorm([d_model])\n        self.norm_ff = nn.LayerNorm([d_model])\n        # Whether to save input to the feed forward layer\n        self.is_save_ff_input = False\n\n    def forward(self, *,\n                x: torch.Tensor,\n                mask: torch.Tensor,\n                src: torch.Tensor = None,\n                src_mask: torch.Tensor = None):\n        # Normalize the vectors before doing self attention\n        z = self.norm_self_attn(x)\n        # Run through self attention, i.e. keys and values are from self\n        self_attn = self.self_attn(query=z, key=z, value=z, mask=mask)\n        # Add the self attention results\n        x = x + self.dropout(self_attn)\n\n        # If a source is provided, get results from attention to source.\n        # This is when you have a decoder layer that pays attention to \n        # encoder outputs\n        if src is not None:\n            # Normalize vectors\n            z = self.norm_src_attn(x)\n            # Attention to source. i.e. keys and values are from source\n            attn_src = self.src_attn(query=z, key=src, value=src, mask=src_mask)\n            # Add the source attention results\n            x = x + self.dropout(attn_src)\n\n        # Normalize for feed-forward\n        z = self.norm_ff(x)\n        # Save the input to the feed forward layer if specified\n        if self.is_save_ff_input:\n            self.ff_input = z.clone()\n        # Pass through the feed-forward network\n        ff = self.feed_forward(z)\n        # Add the feed-forward results back\n        x = x + self.dropout(ff)\n\n        return x\n\n\nclass Encoder(nn.Module):\n    \"\"\"\n    <a id=\"Encoder\"></a>\n\n    ## Transformer Encoder\n    \"\"\"\n\n    def __init__(self, layer: TransformerLayer, n_layers: int):\n        super().__init__()\n        # Make copies of the transformer layer\n        self.layers = clone_module_list(layer, n_layers)\n        # Final normalization layer\n        self.norm = nn.LayerNorm([layer.size])\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n        # Run through each transformer layer\n        for layer in self.layers:\n            x = layer(x=x, mask=mask)\n        # Finally, normalize the vectors\n        return self.norm(x)\n\n\nclass Decoder(nn.Module):\n    \"\"\"\n    <a id=\"Decoder\"></a>\n\n    ## Transformer Decoder\n    \"\"\"\n\n    def __init__(self, layer: TransformerLayer, n_layers: int):\n        super().__init__()\n        # Make copies of the transformer layer\n        self.layers = clone_module_list(layer, n_layers)\n        # Final normalization layer\n        self.norm = nn.LayerNorm([layer.size])\n\n    def forward(self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor):\n        # Run through each transformer layer\n        for layer in self.layers:\n            x = layer(x=x, mask=tgt_mask, src=memory, src_mask=src_mask)\n        # Finally, normalize the vectors\n        return self.norm(x)\n\n\nclass Generator(nn.Module):\n    \"\"\"\n    <a id=\"Generator\"></a>\n\n    ## Generator\n\n    This predicts the tokens and gives the lof softmax of those.\n    You don't need this if you are using `nn.CrossEntropyLoss`.\n    \"\"\"\n\n    def __init__(self, n_vocab: int, d_model: int):\n        super().__init__()\n        self.projection = nn.Linear(d_model, n_vocab)\n\n    def forward(self, x):\n        return self.projection(x)\n\n\nclass EncoderDecoder(nn.Module):\n    \"\"\"\n    <a id=\"EncoderDecoder\"></a>\n\n    ## Combined Encoder-Decoder\n    \"\"\"\n\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: nn.Module, tgt_embed: nn.Module, generator: nn.Module):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\n        # This was important from their code.\n        # Initialize parameters with Glorot / fan_avg.\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor):\n        # Run the source through encoder\n        enc = self.encode(src, src_mask)\n        # Run encodings and targets through decoder\n        return self.decode(enc, src_mask, tgt, tgt_mask)\n\n    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n        return self.encoder(self.src_embed(src), src_mask)\n\n    def decode(self, memory: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n", "labml_nn/transformers/configs.py": "\"\"\"\n---\ntitle: Configurable Transformer Components\nsummary: These are configurable components that can be re-used quite easily.\n---\n\n# Configurable Transformer Components\n\"\"\"\nimport copy\n\nimport torch.nn as nn\n\nfrom labml.configs import BaseConfigs, option, calculate, aggregate\nfrom labml_helpers.module import Module\nfrom .feed_forward import FeedForward\nfrom .mha import MultiHeadAttention\nfrom .models import EmbeddingsWithPositionalEncoding, EmbeddingsWithLearnedPositionalEncoding, TransformerLayer, \\\n    Encoder, Decoder, Generator, EncoderDecoder\n\n\nclass FeedForwardConfigs(BaseConfigs):\n    \"\"\"\n    <a id=\"FFN\"></a>\n\n    ## FFN Configurations\n\n    Creates a Position-wise FeedForward Network defined in\n    [`feed_forward.py`](feed_forward.html).\n    \"\"\"\n    # Position-wise feedforward layer\n    ffn: FeedForward\n    # Number of features in the embedding\n    d_model: int\n    # Number of features in in the hidden layer\n    d_ff: int = 2048\n    # Dropout probability\n    dropout: float = 0.1\n    # Activation in position-wise feedforward layer\n    activation: nn.Module = 'ReLU'\n    # Whether the FFN layer should be gated\n    is_gated: bool = False\n    # Whether the first fully connected layer should have a learnable bias\n    bias1: bool = True\n    # Whether the second fully connected layer should have a learnable bias\n    bias2: bool = True\n    # Whether the fully connected layer for the gate should have a learnable bias\n    bias_gate: bool = False\n    # Predefined GLU variants\n    glu_variant: str = 'none'\n\n\n@option(FeedForwardConfigs.activation, 'ReLU')\ndef _ffn_activation_relu():\n    \"\"\"\n    ### ReLU activation\n\n    $$\\max(0, x)$$\n    \"\"\"\n    return nn.ReLU()\n\n\n@option(FeedForwardConfigs.activation, 'GELU')\ndef _ffn_activation_gelu():\n    \"\"\"\n    ### GELU activation\n\n    $$x \\Phi(x)$$ where $\\Phi(x) = P(X \\le x), X \\sim \\mathcal{N}(0,1)$\n\n    It was introduced in paper [Gaussian Error Linear Units](https://arxiv.org/abs/1606.08415).\n    \"\"\"\n    return nn.GELU()\n\n\n@option(FeedForwardConfigs.ffn, 'default')\ndef _feed_forward(c: FeedForwardConfigs):\n    \"\"\"\n    Initialize a [feed forward network](feed_forward.html)\n    \"\"\"\n    return FeedForward(c.d_model, c.d_ff,\n                       dropout=c.dropout,\n                       activation=c.activation,\n                       is_gated=c.is_gated,\n                       bias1=c.bias1,\n                       bias2=c.bias2,\n                       bias_gate=c.bias_gate)\n\n# ## GLU Variants\n# These are variants with gated hidden layers for the FFN\n# as introduced in paper [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202).\n# We have omitted the bias terms as specified in the paper.\n\n# ### FFN with Gated Linear Units\n#\n# $$FFN_{GLU}(x)(x, W_1, V, W_2) = (\\sigma(x W_1) \\otimes x V) W_2$$\naggregate(FeedForwardConfigs.glu_variant, 'GLU',\n          (FeedForwardConfigs.is_gated, True),\n          (FeedForwardConfigs.bias1, False),\n          (FeedForwardConfigs.bias2, False),\n          (FeedForwardConfigs.bias_gate, False),\n          (FeedForwardConfigs.activation, nn.Sigmoid()))\n\n# ### FFN with Bilinear hidden layer\n#\n# $$FFN_{Bilinear}(x)(x, W_1, V, W_2) = (x W_1 \\otimes x V) W_2$$\naggregate(FeedForwardConfigs.glu_variant, 'Bilinear',\n          (FeedForwardConfigs.is_gated, True),\n          (FeedForwardConfigs.bias1, False),\n          (FeedForwardConfigs.bias2, False),\n          (FeedForwardConfigs.bias_gate, False),\n          (FeedForwardConfigs.activation, nn.Identity()))\n\n# ### FFN with ReLU gate\n#\n# $$FFN_{ReGLU}(x)(x, W_1, V, W_2) = (\\max(0, x W_1) \\otimes x V) W_2$$\naggregate(FeedForwardConfigs.glu_variant, 'ReGLU',\n          (FeedForwardConfigs.is_gated, True),\n          (FeedForwardConfigs.bias1, False),\n          (FeedForwardConfigs.bias2, False),\n          (FeedForwardConfigs.bias_gate, False),\n          (FeedForwardConfigs.activation, nn.ReLU()))\n\n# ### FFN with GELU gate\n#\n# $$FFN_{GEGLU}(x)(x, W_1, V, W_2) = (\\text{GELU}(x W_1) \\otimes x V) W_2$$\naggregate(FeedForwardConfigs.glu_variant, 'GEGLU',\n          (FeedForwardConfigs.is_gated, True),\n          (FeedForwardConfigs.bias1, False),\n          (FeedForwardConfigs.bias2, False),\n          (FeedForwardConfigs.bias_gate, False),\n          (FeedForwardConfigs.activation, nn.GELU()))\n\n# ### FFN with Swish gate\n#\n# $$FFN_{SwiGLU}(x)(x, W_1, V, W_2) = (\\text{Swish}_1(x W_1) \\otimes x V) W_2$$\n# where $\\text{Swish}_\\beta(x) = x \\sigma(\\beta x)$\naggregate(FeedForwardConfigs.glu_variant, 'SwiGLU',\n          (FeedForwardConfigs.is_gated, True),\n          (FeedForwardConfigs.bias1, False),\n          (FeedForwardConfigs.bias2, False),\n          (FeedForwardConfigs.bias_gate, False),\n          (FeedForwardConfigs.activation, nn.SiLU()))\n\n\nclass TransformerConfigs(BaseConfigs):\n    \"\"\"\n    <a id=\"TransformerConfigs\"></a>\n\n    ## Transformer Configurations\n\n    This defines configurations for a transformer.\n    The configurations are calculate using option functions.\n    These are lazy loaded and therefore only the necessary modules\n    are calculated.\n    \"\"\"\n    # Number of attention heads\n    n_heads: int = 8\n    # Transformer embedding size\n    d_model: int = 512\n    # Number of layers\n    n_layers: int = 6\n    # Dropout probability\n    dropout: float = 0.1\n    # Number of tokens in the source vocabulary (for token embeddings)\n    n_src_vocab: int\n    # Number of tokens in the target vocabulary (to generate logits for prediction)\n    n_tgt_vocab: int\n\n    # The encoder self attention\n    encoder_attn: MultiHeadAttention = 'mha'\n    # The decoder self attention\n    decoder_attn: MultiHeadAttention = 'mha'\n    # The decoder memory attention\n    decoder_mem_attn: MultiHeadAttention = 'mha'\n\n    # Configurable Feedforward Layer\n    ffn: FeedForwardConfigs\n\n    # Encoder layer\n    encoder_layer: TransformerLayer = 'default'\n    # Decoder layer\n    decoder_layer: TransformerLayer = 'default'\n\n    # Encoder consisting of multiple encoder layers\n    encoder: Encoder = 'default'\n    # Encoder consisting of multiple decoder layers\n    decoder: Decoder = 'default'\n\n    # Embedding layer for source\n    src_embed: Module = 'fixed_pos'\n    # Embedding layer for target (for decoder)\n    tgt_embed: Module = 'fixed_pos'\n\n    # Logit generator for prediction\n    generator: Generator = 'default'\n\n    # Encoder-decoder\n    encoder_decoder: EncoderDecoder\n\n\n# ### Multi-head Attention\ndef _mha(c: TransformerConfigs):\n    return MultiHeadAttention(c.n_heads, c.d_model, dropout_prob=c.dropout)\n\n\ncalculate(TransformerConfigs.encoder_attn, 'mha', _mha)\ncalculate(TransformerConfigs.decoder_attn, 'mha', _mha)\ncalculate(TransformerConfigs.decoder_mem_attn, 'mha', _mha)\n\n\n# ### Relative Multi-head Attention\ndef _relative_mha(c: TransformerConfigs):\n    from labml_nn.transformers.xl.relative_mha import RelativeMultiHeadAttention\n    return RelativeMultiHeadAttention(c.n_heads, c.d_model)\n\n\ncalculate(TransformerConfigs.encoder_attn, 'relative', _relative_mha)\ncalculate(TransformerConfigs.decoder_attn, 'relative', _relative_mha)\ncalculate(TransformerConfigs.decoder_mem_attn, 'relative', _relative_mha)\n\n\n@option(TransformerConfigs.ffn, 'default')\ndef _feed_forward(c: TransformerConfigs):\n    \"\"\"\n    Create feedforward layer configurations\n    \"\"\"\n    conf = FeedForwardConfigs()\n    conf.set_default(FeedForwardConfigs.d_model, func=lambda: c.d_model)\n    conf.set_default(FeedForwardConfigs.dropout, func=lambda: c.dropout)\n    return conf\n\n\n@option(TransformerConfigs.encoder_layer, 'default')\ndef _encoder_layer(c: TransformerConfigs):\n    \"\"\"\n    Encoder layer\n    \"\"\"\n    return TransformerLayer(d_model=c.d_model, self_attn=c.encoder_attn,\n                            src_attn=None, feed_forward=copy.deepcopy(c.ffn.ffn),\n                            dropout_prob=c.dropout)\n\n\n@option(TransformerConfigs.decoder_layer, 'default')\ndef _decoder_layer(c: TransformerConfigs):\n    \"\"\"\n    Decoder layer\n    \"\"\"\n    return TransformerLayer(d_model=c.d_model, self_attn=c.decoder_attn,\n                            src_attn=c.decoder_mem_attn, feed_forward=copy.deepcopy(c.ffn.ffn),\n                            dropout_prob=c.dropout)\n\n\n@option(TransformerConfigs.encoder, 'default')\ndef _encoder(c: TransformerConfigs):\n    \"\"\"\n    Encoder\n    \"\"\"\n    return Encoder(c.encoder_layer, c.n_layers)\n\n\n@option(TransformerConfigs.decoder, 'default')\ndef _decoder(c: TransformerConfigs):\n    \"\"\"\n    Decoder\n    \"\"\"\n    return Decoder(c.decoder_layer, c.n_layers)\n\n\n@option(TransformerConfigs.generator, 'default')\ndef _generator(c: TransformerConfigs):\n    \"\"\"\n    Logit generator\n    \"\"\"\n    return Generator(c.n_tgt_vocab, c.d_model)\n\n\n# ### Fixed Positional Embeddings\n@option(TransformerConfigs.src_embed, 'fixed_pos')\ndef _src_embed_with_positional(c: TransformerConfigs):\n    \"\"\"\n    Source embedding with fixed positional encodings\n    \"\"\"\n    return EmbeddingsWithPositionalEncoding(c.d_model, c.n_src_vocab)\n\n\n@option(TransformerConfigs.tgt_embed, 'fixed_pos')\ndef _tgt_embed_with_positional(c: TransformerConfigs):\n    \"\"\"\n    Target embedding with fixed positional encodings\n    \"\"\"\n    return EmbeddingsWithPositionalEncoding(c.d_model, c.n_tgt_vocab)\n\n\n# ### Learned Positional Embeddings\n@option(TransformerConfigs.src_embed, 'learned_pos')\ndef _src_embed_with_learned_positional(c: TransformerConfigs):\n    \"\"\"\n    Source embedding with learned positional encodings\n    \"\"\"\n    return EmbeddingsWithLearnedPositionalEncoding(c.d_model, c.n_src_vocab)\n\n\n@option(TransformerConfigs.tgt_embed, 'learned_pos')\ndef _tgt_embed_with_learned_positional(c: TransformerConfigs):\n    \"\"\"\n    Target embedding with learned positional encodings\n    \"\"\"\n    return EmbeddingsWithLearnedPositionalEncoding(c.d_model, c.n_tgt_vocab)\n\n\n# ### No Positional Embeddings\n@option(TransformerConfigs.src_embed, 'no_pos')\ndef _src_embed_without_positional(c: TransformerConfigs):\n    \"\"\"\n    Source embedding without positional encodings\n    \"\"\"\n    return nn.Embedding(c.n_src_vocab, c.d_model)\n\n\n@option(TransformerConfigs.tgt_embed, 'no_pos')\ndef _tgt_embed_without_positional(c: TransformerConfigs):\n    return nn.Embedding(c.n_tgt_vocab, c.d_model)\n\n\n@option(TransformerConfigs.encoder_decoder, 'default')\ndef _encoder_decoder(c: TransformerConfigs):\n    return EncoderDecoder(c.encoder, c.decoder, c.src_embed, c.tgt_embed, c.generator)\n", "labml_nn/transformers/relative_mha.py": "\"\"\"\n---\ntitle: Relative Multi-Headed Attention\nsummary: Relative Multi-Headed Attention from paper Transformer-XL.\nredirect: https://nn.labml.ai/transformers/xl/relative_mha.html\n---\n\"\"\"\n", "labml_nn/transformers/mha.py": "\"\"\"\n---\ntitle: Multi-Headed Attention (MHA)\nsummary: >\n  This implements the Multi-Headed Attention used in transformers\n  using PyTorch with explanations.\n---\n\n# Multi-Headed Attention (MHA)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/basic/autoregressive_experiment.ipynb)\n\nThis is a tutorial/implementation of multi-headed attention\nfrom paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\nin [PyTorch](https://pytorch.org/).\nThe implementation is inspired from [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html).\n\nHere is the [training code](basic/autoregressive_experiment.html) that uses a basic transformer\nwith MHA for NLP auto-regression.\n\n[Here is an experiment implementation](basic/autoregressive_experiment.html) that trains a simple transformer.\n\"\"\"\n\nimport math\nfrom typing import Optional, List\n\nimport torch\nfrom torch import nn\n\nfrom labml import tracker\n\n\nclass PrepareForMultiHeadAttention(nn.Module):\n    \"\"\"\n    <a id=\"PrepareMHA\"></a>\n\n    ## Prepare for multi-head attention\n\n    This module does a linear transformation and splits the vector into given\n    number of heads for multi-head attention.\n    This is used to transform **key**, **query**, and **value** vectors.\n    \"\"\"\n\n    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n        super().__init__()\n        # Linear layer for linear transform\n        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n        # Number of heads\n        self.heads = heads\n        # Number of dimensions in vectors in each head\n        self.d_k = d_k\n\n    def forward(self, x: torch.Tensor):\n        # Input has shape `[seq_len, batch_size, d_model]` or `[batch_size, d_model]`.\n        # We apply the linear transformation to the last dimension and split that into\n        # the heads.\n        head_shape = x.shape[:-1]\n\n        # Linear transform\n        x = self.linear(x)\n\n        # Split last dimension into heads\n        x = x.view(*head_shape, self.heads, self.d_k)\n\n        # Output has shape `[seq_len, batch_size, heads, d_k]` or `[batch_size, heads, d_model]`\n        return x\n\n\nclass MultiHeadAttention(nn.Module):\n    r\"\"\"\n    <a id=\"MHA\"></a>\n\n    ## Multi-Head Attention Module\n\n    This computes scaled multi-headed attention for given `query`, `key` and `value` vectors.\n\n    $$\\mathop{Attention}(Q, K, V) = \\underset{seq}{\\mathop{softmax}}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\n\n    In simple terms, it finds keys that matches the query, and gets the values of\n     those keys.\n\n    It uses dot-product of query and key as the indicator of how matching they are.\n    Before taking the $softmax$ the dot-products are scaled by $\\frac{1}{\\sqrt{d_k}}$.\n    This is done to avoid large dot-product values causing softmax to\n    give very small gradients when $d_k$ is large.\n\n    Softmax is calculated along the axis of of the sequence (or time).\n    \"\"\"\n\n    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1, bias: bool = True):\n        \"\"\"\n        * `heads` is the number of heads.\n        * `d_model` is the number of features in the `query`, `key` and `value` vectors.\n        \"\"\"\n\n        super().__init__()\n\n        # Number of features per head\n        self.d_k = d_model // heads\n        # Number of heads\n        self.heads = heads\n\n        # These transform the `query`, `key` and `value` vectors for multi-headed attention.\n        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=True)\n\n        # Softmax for attention along the time dimension of `key`\n        self.softmax = nn.Softmax(dim=1)\n\n        # Output layer\n        self.output = nn.Linear(d_model, d_model)\n        # Dropout\n        self.dropout = nn.Dropout(dropout_prob)\n        # Scaling factor before the softmax\n        self.scale = 1 / math.sqrt(self.d_k)\n\n        # We store attentions so that it can be used for logging, or other computations if needed\n        self.attn = None\n\n    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n        \"\"\"\n        ### Calculate scores between queries and keys\n\n        This method can be overridden for other variations like relative attention.\n        \"\"\"\n\n        # Calculate $Q K^\\top$ or $S_{ijbh} = \\sum_d Q_{ibhd} K_{jbhd}$\n        return torch.einsum('ibhd,jbhd->ijbh', query, key)\n\n    def prepare_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n        \"\"\"\n        `mask` has shape `[seq_len_q, seq_len_k, batch_size]`, where first dimension is the query dimension.\n        If the query dimension is equal to $1$ it will be broadcasted.\n        \"\"\"\n\n        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n        assert mask.shape[1] == key_shape[0]\n        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n\n        # Same mask applied to all heads.\n        mask = mask.unsqueeze(-1)\n\n        # resulting mask has shape `[seq_len_q, seq_len_k, batch_size, heads]`\n        return mask\n\n    def forward(self, *,\n                query: torch.Tensor,\n                key: torch.Tensor,\n                value: torch.Tensor,\n                mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        `query`, `key` and `value` are the tensors that store\n        collection of *query*, *key* and *value* vectors.\n        They have shape `[seq_len, batch_size, d_model]`.\n\n        `mask` has shape `[seq_len, seq_len, batch_size]` and\n        `mask[i, j, b]` indicates whether for batch `b`,\n        query at position `i` has access to key-value at position `j`.\n        \"\"\"\n\n        # `query`, `key` and `value`  have shape `[seq_len, batch_size, d_model]`\n        seq_len, batch_size, _ = query.shape\n\n        if mask is not None:\n            mask = self.prepare_mask(mask, query.shape, key.shape)\n\n        # Prepare `query`, `key` and `value` for attention computation.\n        # These will then have shape `[seq_len, batch_size, heads, d_k]`.\n        query = self.query(query)\n        key = self.key(key)\n        value = self.value(value)\n\n        # Compute attention scores $Q K^\\top$.\n        # This gives a tensor of shape `[seq_len, seq_len, batch_size, heads]`.\n        scores = self.get_scores(query, key)\n\n        # Scale scores $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n        scores *= self.scale\n\n        # Apply mask\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        # $softmax$ attention along the key sequence dimension\n        # $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n        attn = self.softmax(scores)\n\n        # Save attentions if debugging\n        tracker.debug('attn', attn)\n\n        # Apply dropout\n        attn = self.dropout(attn)\n\n        # Multiply by values\n        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\n        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n\n        # Save attentions for any other calculations \n        self.attn = attn.detach()\n\n        # Concatenate multiple heads\n        x = x.reshape(seq_len, batch_size, -1)\n\n        # Output layer\n        return self.output(x)\n", "labml_nn/transformers/__init__.py": "\"\"\"\n---\ntitle: Transformers\nsummary: >\n  This is a collection of PyTorch implementations/tutorials of\n  transformers and related techniques.\n---\n\n# Transformers\n\nThis module contains [PyTorch](https://pytorch.org/)\nimplementations and explanations of original transformer\nfrom paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762),\nand derivatives and enhancements of it.\n\n* [Multi-head attention](mha.html)\n* [Transformer Encoder and Decoder Models](models.html)\n* [Position-wise Feed Forward Network (FFN)](feed_forward.html)\n* [Fixed positional encoding](positional_encoding.html)\n\n## [Transformer XL](xl/index.html)\nThis implements Transformer XL model using\n[relative multi-head attention](xl/relative_mha.html)\n\n## [Rotary Positional Embeddings](rope/index.html)\nThis implements Rotary Positional Embeddings (RoPE)\n\n## [Attention with Linear Biases](alibi/index.html)\nThis implements Attention with Linear Biases (ALiBi).\n\n## [RETRO](retro/index.html)\nThis implements the Retrieval-Enhanced Transformer (RETRO).\n\n## [Compressive Transformer](compressive/index.html)\n\nThis is an implementation of compressive transformer\nthat extends upon [Transformer XL](xl/index.html) by compressing\nthe oldest memories to give a longer attention span.\n\n## [GPT Architecture](gpt/index.html)\n\nThis is an implementation of GPT-2 architecture.\n\n## [GLU Variants](glu_variants/simple.html)\n\nThis is an implementation of the paper\n[GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202).\n\n## [kNN-LM](knn/index.html)\n\nThis is an implementation of the paper\n[Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/abs/1911.00172).\n\n## [Feedback Transformer](feedback/index.html)\n\nThis is an implementation of the paper\n[Accessing Higher-level Representations in Sequential Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402).\n\n## [Switch Transformer](switch/index.html)\n\nThis is a miniature implementation of the paper\n[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961).\nOur implementation only has a few million parameters and doesn't do model parallel distributed training.\nIt does single GPU training but we implement the concept of switching as described in the paper.\n\n## [Fast Weights Transformer](fast_weights/index.html)\n\nThis is an implementation of the paper\n[Linear Transformers Are Secretly Fast Weight Memory Systems in PyTorch](https://arxiv.org/abs/2102.11174).\n\n## [FNet: Mixing Tokens with Fourier Transforms](fnet/index.html)\n\nThis is an implementation of the paper\n[FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824).\n\n## [Attention Free Transformer](aft/index.html)\n\nThis is an implementation of the paper\n[An Attention Free Transformer](https://arxiv.org/abs/2105.14103).\n\n## [Masked Language Model](mlm/index.html)\n\nThis is an implementation of Masked Language Model used for pre-training in paper\n[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).\n\n## [MLP-Mixer: An all-MLP Architecture for Vision](mlp_mixer/index.html)\n\nThis is an implementation of the paper\n[MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601).\n\n## [Pay Attention to MLPs (gMLP)](gmlp/index.html)\n\nThis is an implementation of the paper\n[Pay Attention to MLPs](https://arxiv.org/abs/2105.08050).\n\n## [Vision Transformer (ViT)](vit/index.html)\n\nThis is an implementation of the paper\n[An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale](https://arxiv.org/abs/2010.11929).\n\n## [Primer EZ](primer_ez/index.html)\n\nThis is an implementation of the paper\n[Primer: Searching for Efficient Transformers for Language Modeling](https://arxiv.org/abs/2109.08668).\n\n## [Hourglass](hour_glass/index.html)\n\nThis is an implementation of the paper\n[Hierarchical Transformers Are More Efficient Language Models](https://arxiv.org/abs/2110.13711)\n\"\"\"\n\nfrom .configs import TransformerConfigs\nfrom .models import TransformerLayer, Encoder, Decoder, Generator, EncoderDecoder\nfrom .mha import MultiHeadAttention\nfrom labml_nn.transformers.xl.relative_mha import RelativeMultiHeadAttention\n", "labml_nn/transformers/feed_forward.py": "\"\"\"\n---\ntitle: Position-wise Feed-Forward Network (FFN)\nsummary: Documented reusable implementation of the position wise feedforward network.\n---\n\n# Position-wise Feed-Forward Network (FFN)\n\nThis is a [PyTorch](https://pytorch.org)  implementation\nof position-wise feedforward network used in transformer.\n\nFFN consists of two fully connected layers.\nNumber of dimensions in the hidden layer $d_{ff}$, is generally set to around\nfour times that of the token embedding $d_{model}$.\nSo it is sometime also called the expand-and-contract network.\n\nThere is an activation at the hidden layer, which is\nusually set to ReLU (Rectified Linear Unit) activation, $$\\max(0, x)$$\n\nThat is, the FFN function is,\n$$FFN(x, W_1, W_2, b_1, b_2) = \\max(0, x W_1 + b_1) W_2 + b_2$$\nwhere $W_1$, $W_2$, $b_1$ and $b_2$ are learnable parameters.\n\nSometimes the\nGELU (Gaussian Error Linear Unit) activation is also used instead of ReLU.\n$$x \\Phi(x)$$ where $\\Phi(x) = P(X \\le x), X \\sim \\mathcal{N}(0,1)$\n\n### Gated Linear Units\n\nThis is a generic implementation that supports different variants including\n[Gated Linear Units](https://arxiv.org/abs/2002.05202) (GLU).\nWe have also implemented experiments on these:\n\n* [experiment that uses `labml.configs`](glu_variants/experiment.html)\n* [simpler version from scratch](glu_variants/simple.html)\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\n\nfrom labml_helpers.module import Module\n\n\nclass FeedForward(Module):\n    \"\"\"\n    ## FFN module\n    \"\"\"\n\n    def __init__(self, d_model: int, d_ff: int,\n                 dropout: float = 0.1,\n                 activation=nn.ReLU(),\n                 is_gated: bool = False,\n                 bias1: bool = True,\n                 bias2: bool = True,\n                 bias_gate: bool = True):\n        \"\"\"\n        * `d_model` is the number of features in a token embedding\n        * `d_ff` is the number of features in the hidden layer of the FFN\n        * `dropout` is dropout probability for the hidden layer\n        * `is_gated` specifies whether the hidden layer is gated\n        * `bias1` specified whether the first fully connected layer should have a learnable bias\n        * `bias2` specified whether the second fully connected layer should have a learnable bias\n        * `bias_gate` specified whether the fully connected layer for the gate should have a learnable bias\n        \"\"\"\n        super().__init__()\n        # Layer one parameterized by weight $W_1$ and bias $b_1$\n        self.layer1 = nn.Linear(d_model, d_ff, bias=bias1)\n        # Layer one parameterized by weight $W_1$ and bias $b_1$\n        self.layer2 = nn.Linear(d_ff, d_model, bias=bias2)\n        # Hidden layer dropout\n        self.dropout = nn.Dropout(dropout)\n        # Activation function $f$\n        self.activation = activation\n        # Whether there is a gate\n        self.is_gated = is_gated\n        if is_gated:\n            # If there is a gate the linear layer to transform inputs to\n            # be multiplied by the gate, parameterized by weight $V$ and bias $c$\n            self.linear_v = nn.Linear(d_model, d_ff, bias=bias_gate)\n\n    def forward(self, x: torch.Tensor):\n        # $f(x W_1 + b_1)$\n        g = self.activation(self.layer1(x))\n        # If gated, $f(x W_1 + b_1) \\otimes (x V + b) $\n        if self.is_gated:\n            x = g * self.linear_v(x)\n        # Otherwise\n        else:\n            x = g\n        # Apply dropout\n        x = self.dropout(x)\n        # $(f(x W_1 + b_1) \\otimes (x V + b)) W_2 + b_2$ or $f(x W_1 + b_1) W_2 + b_2$\n        # depending on whether it is gated\n        return self.layer2(x)\n", "labml_nn/transformers/fast_weights/experiment.py": "\"\"\"\n---\ntitle: Train Fast Weights Transformer\nsummary: This is training code with notes for a Fast Weights Transformer.\n---\n\n# Train Fast Weights Transformer\n\nThis trains a fast weights transformer model for auto-regression.\n\nHere\u2019s a Colab notebook for training a fast weights transformer on Tiny Shakespeare dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/fast_weights/experiment.ipynb)\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml.utils.pytorch import get_modules\nfrom labml_helpers.module import Module\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\n\n\nclass AutoregressiveModel(Module):\n    \"\"\"\n    ## Auto regressive model\n    \"\"\"\n\n    def __init__(self, n_vocab: int, d_model: int, transformer: Module):\n        super().__init__()\n        # Token embedding module\n        self.src_embed = nn.Embedding(n_vocab, d_model)\n        self.transformer = transformer\n        self.generator = nn.Linear(d_model, n_vocab)\n\n    def forward(self, x: torch.Tensor):\n        # Embed the tokens\n        x = self.src_embed(x)\n        # Run it through the the transformer\n        res = self.transformer(x)\n        # Generate logits of the next token\n        return self.generator(res), None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    The default configs can and will be over-ridden when we start the experiment\n    \"\"\"\n\n    model: AutoregressiveModel\n\n    d_model: int = 512\n    nu: int = 1\n    heads: int = 8\n    dropout: float = 0.0\n    d_ff: int = 2048\n    n_layers: int = 6\n\n\n@option(Configs.model)\ndef fast_weights_transformer(c: Configs):\n    \"\"\"\n    Create [fast weights transformer](index.html).\n    \"\"\"\n    from labml_nn.transformers.fast_weights import FastWeightsAttentionTransformer, \\\n        FastWeightsAttentionTransformerLayer, FastWeightsAttention, FeedForward\n\n    from labml_nn.transformers.fast_weights import DPFP\n    return AutoregressiveModel(\n        c.n_tokens, c.d_model,\n        FastWeightsAttentionTransformer(\n            FastWeightsAttentionTransformerLayer(d_model=c.d_model,\n                                                 attn=FastWeightsAttention(c.heads, c.d_model, c.dropout, DPFP(nu=c.nu)),\n                                                 feed_forward=FeedForward(c.d_model, c.d_ff, c.dropout),\n                                                 dropout_prob=c.dropout),\n            c.n_layers)).to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"fast_weights_transformer\")\n    # Create configs\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf,\n                       # A dictionary of configurations to override\n                       {'tokenizer': 'character',\n                        'text': 'tiny_shakespeare',\n                        'optimizer.learning_rate': 1.0,\n                        'optimizer.optimizer': 'Noam',\n                        'prompt': 'It is',\n                        'prompt_separator': '',\n\n                        'train_loader': 'shuffled_train_loader',\n                        'valid_loader': 'shuffled_valid_loader',\n\n                        'seq_len': 128,\n                        'epochs': 128,\n                        'batch_size': 16,\n                        'inner_iterations': 25})\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models(get_modules(conf))\n\n    # Start the experiment\n    with experiment.start():\n        # Run the training loop\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/fast_weights/token_wise.py": "\"\"\"\n---\ntitle: Fast Weight Systems\nsummary: >\n  This is an annotated implementation/tutorial of\n  Linear Transformers Are Secretly Fast Weight Memory Systems in PyTorch.\n---\n\"\"\"\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.transformers.fast_weights import DPFP\nfrom labml_nn.transformers.feed_forward import FeedForward\nfrom labml_nn.transformers.mha import PrepareForMultiHeadAttention\nfrom labml_nn.utils import clone_module_list\n\n\nclass FastWeightsAttention(Module):\n    def __init__(self, heads: int, d_model: int, dropout_prob: float, phi: DPFP):\n        super().__init__()\n\n        # Number of features per head\n        self.d_k = d_model // heads\n        #\n        self.heads = heads\n\n        # These transform the `query` multi-headed attention.\n        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=False)\n        # These transform the `key` and `value` for multi-headed attention.\n        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=False)\n        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=False)\n\n        self.gate = nn.Sequential(PrepareForMultiHeadAttention(d_model, heads, 1, bias=False),\n                                  nn.Sigmoid())\n\n        self.phi = phi\n\n        # Output layer\n        self.output = nn.Linear(d_model, d_model)\n        # Dropout\n        self.dropout = nn.Dropout(dropout_prob)\n\n    def forward(self, x: torch.Tensor, weights: Optional[torch.Tensor]):\n        query = self.phi(self.query(x))\n        key = self.phi(self.key(x))\n        value = self.value(x)\n\n        if weights is None:\n            weights = key.new_zeros((key.shape[0], key.shape[1], value.shape[2], key.shape[2]))\n\n        value_existing = torch.einsum('bhvk,bhk->bhv', weights, key)\n\n        beta = self.gate(x)\n\n        weights = weights + torch.einsum('bhv,bhk->bhvk', beta * (value - value_existing), key)\n\n        x = torch.einsum('bhvk,bhk->bhv', weights, query)\n\n        # Concatenate multiple heads\n        x = x.reshape(x.shape[0], -1)\n\n        # Output layer\n        return self.output(x), weights\n\n\nclass FastWeightsAttentionTransformerLayer(Module):\n    def __init__(self, *,\n                 d_model: int,\n                 attn: FastWeightsAttention,\n                 feed_forward: FeedForward,\n                 dropout_prob: float):\n        super().__init__()\n        # Transformer size $d_{model}$\n        self.size = d_model\n        #\n        self.attn = attn\n        self.feed_forward = feed_forward\n        self.dropout = nn.Dropout(dropout_prob)\n\n        # Normalization layers\n        self.norm_self_attn = nn.LayerNorm([d_model])\n        self.norm_ff = nn.LayerNorm([d_model])\n\n    def forward(self, x: torch.Tensor, weights: Optional[torch.Tensor]):\n        attn, weights = self.attn(x, weights)\n        # Add the self attention results\n        x = x + self.dropout(attn)\n\n        # Normalize for feed-forward\n        z = self.norm_ff(x)\n        # Pass through the feed-forward network\n        ff = self.feed_forward(z)\n        # Add the feed-forward results back\n        x = x + self.dropout(ff)\n\n        #\n        return x, weights\n\n\nclass FastWeightsAttentionTransformer(Module):\n    def __init__(self, layer: FastWeightsAttentionTransformerLayer, n_layers: int):\n        super().__init__()\n        # Make copies of the transformer layer\n        self.layers = clone_module_list(layer, n_layers)\n        # Final normalization layer\n        self.norm = nn.LayerNorm([layer.size])\n\n    def forward(self, x_seq: torch.Tensor):\n        # Split the input to a list along the sequence axis\n        x_seq = torch.unbind(x_seq, dim=0)\n        # List to store the outputs\n        res = []\n        # For each input step\n        weights = [None for _ in range(len(self.layers))]\n\n        for x in x_seq:\n            # Run through each layer\n            for i, layer in enumerate(self.layers):\n                # Get layer output\n                x, weights[i] = layer(x, weights[i])\n\n            res.append(x)\n\n        # Stack the output tensors\n        res = torch.stack(res)\n        # Normalize the output\n        return self.norm(res)\n", "labml_nn/transformers/fast_weights/__init__.py": "\"\"\"\n---\ntitle: Linear Transformers Are Secretly Fast Weight Memory Systems\nsummary: >\n  This is an annotated implementation/tutorial of\n  Linear Transformers Are Secretly Fast Weight Memory Systems in PyTorch.\n---\n\n# Fast weights transformer\n\nThe paper\n[Linear Transformers Are Secretly Fast Weight Memory Systems in PyTorch](https://arxiv.org/abs/2102.11174)\nfinds similarities between linear self-attention and fast weight systems\nand makes modifications to self-attention update rule based on that.\nIt also introduces a simpler, yet effective kernel function.\n\n*The authors have provided an [official implementation](https://github.com/ischlag/fast-weight-transformers)\nof the paper including other variants they compare with in the paper.*\n\n## Fast weights\n\nConsider a sequence of inputs $\\big\\{x^{(i)}\\big\\}^L_{i=1}$ or length $L$\nand each step is a vector of size $d_{in}$; i.e. $x \\in \\mathbb{R}^{d_{in}}$.\nThe fast weight model generates a weight matrix at each step to produce output\n$\\big\\{y^{(i)}\\big\\}^L_{i=1}$, $y \\in \\mathbb{R}^{d_{out}}$\n\n\\begin{align}\na^{(i)}, b^{(i)} &= \\textcolor{orange}{W_a} x^{(i)}, \\textcolor{orange}{W_b} x^{(i)} \\\\\n\\textcolor{cyan}{W^{(i)}} &= \\sigma \\Big( \\textcolor{cyan}{W^{(i-1)}} + a^{(i)} \\otimes b^{(i)} \\Big) \\\\\ny^{(i)} &= \\textcolor{cyan}{W^{(i)}} x^{(i)}\n\\end{align}\n\n$\\otimes$ is the outer product ($a \\otimes b = a b^\\top$), where elements of the two vectors are multiplied with each other\nto give a matrix.\n$\\sigma$ is an activation function.\n$\\textcolor{orange}{W_a}$ and $\\textcolor{orange}{W_b}$ are trainable weights (parameters).\n$\\textcolor{cyan}{W^{(i)}}$ are the fast weights that are generated at each step.\n\n## Linear self-attention\n\nOriginal transformer self-attention is, (omitting $\\frac{1}{d_k}$ for clarity)\n\n\\begin{align}\ny^{(i)} &= \\Big[v^{(1)}, v^{(2)}, ..., v^{(i)}\\Big] \\text{softmax}\n \\bigg(\n    \\Big[k^{(1)}, k^{(2)}, ..., k^{(i)}\\Big] ^\\top\n    q^{(i)}\n \\bigg) \\\\\n &= \\sum^i_{j=1} \\frac\n { v^{(j)} \\kappa(k^{(j)}, q^{(i)}) }\n { \\sum^i_{j'=1} \\kappa(k^{(j')}, q^{(i)}) } \\\\\n\\end{align}\n\nwhere $\\kappa(k, q) = \\text{exp}(k \\cdot q)$\n\nThe idea behind linearizing self attention is to replace softmax\nkernel $\\kappa$ with a different kernel $\\kappa '$ so that we can calculate the\ndenominator of the self attention function faster:\n\n$$\\kappa '(k, q) = \\textcolor{lightgreen}{\\phi(k)}^\\top \\textcolor{lightgreen}{\\phi(q)}$$\n\nThis gives\n\n\\begin{align}\ny^{(i)} &= \\frac\n {\\Big( \\sum^i_{j=1} v^{(j)} \\otimes \\textcolor{lightgreen}{\\phi(k^{(j)})} \\Big)\n  \\textcolor{lightgreen}{\\phi(q^{(i)})} }\n { \\Big( \\sum^i_{j'=1}\n   \\textcolor{lightgreen}{\\phi(k^{(j')})} \\Big)\n    \\textcolor{lightgreen}{\\phi(q^{(i)})} }\n\\end{align}\n\nWith $\\textcolor{cyan}{W^{(i)}} = \\sum^i_{j=1} v^{(j)} \\otimes \\phi(k^{(j)})$ and\n$z^{(i)} = \\sum^i_{j=1} \\textcolor{lightgreen}{\\phi(k^{(j)})}$, we can calculate them efficiently:\n\n\\begin{align}\n\\textcolor{cyan}{W^{(i)}} &= \\textcolor{cyan}{W^{(i-1)}} + v^{(i)} \\otimes \\textcolor{lightgreen}{\\phi(k^{(i)})} \\\\\nz^{(i)} &= z{(i)} + \\textcolor{lightgreen}{\\phi(k^{(i)})} \\\\\ny^{(i)} &= \\frac{1}{z^{(i)} \\cdot \\textcolor{lightgreen}{\\phi(q^{(i)})}}\n    W^{(i)} \\textcolor{lightgreen}{\\phi(q^{(i)})}\n\\end{align}\n\nThis is quite similar to fast weights.\n\nThe paper introduces a new linear attention projection function $\\textcolor{lightgreen}{\\phi}$\na new update rule for $\\textcolor{cyan}{W^{(i)}} = f(\\textcolor{cyan}{W^{(i-1)}})$ and change the normalization\n$\\frac{1}{z^{(i)} \\cdot \\textcolor{lightgreen}{\\phi(q^{(i)})}}$\n\nHere are [the training code](experiment.html) and a notebook for training a fast weights\n transformer on the Tiny Shakespeare dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/fast_weights/experiment.ipynb)\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.transformers.feed_forward import FeedForward\nfrom labml_nn.transformers.mha import PrepareForMultiHeadAttention\nfrom labml_nn.utils import clone_module_list\n\n\nclass DPFP(Module):\n    \"\"\"\n    ## Deterministic Parameter Free Project (DPFP)\n\n    This is the new projection function $\\textcolor{lightgreen}{\\phi}$ introduced in the paper.\n    DPFP projects $k$ of dimensionality $d_{key}$ to dimensionality $d_{dot} = 2 d_{key} \\nu$,\n    where $\\nu \\in \\\\{1, 2, ..., 2 d_{key} - 1 \\\\}$ is a hyper-parameter.\n\n    $$\\textcolor{lightgreen}{\\phi_{2 d_{key} (i - 1)  + j}(k)}\n     = \\text{ReLU}\\Big(\\big[k, -k\\big]\\Big)_{j}\n                        \\text{ReLU}\\Big(\\big[k, -k\\big]\\Big)_{i + j}$$\n\n    where $\\big[k, -k\\big]$ is the concatenation of $k$ and $-k$ to give a vector of\n    size $2 d_{key}$, $i \\in \\\\{1, 2, ..., \\nu \\\\}$, and $j \\in \\\\{1, 2, ..., 2 d_{key}\\\\}$.\n    $x_i$ is the $i$-th element of vector $x$ and is rolled around if\n    $i$ is larger than the number of elements in $x$.\n\n    Basically, it creates a new vector by multiplying elements of $[k, -k]$ shifted by $i$.\n\n    This produces projections that are sparse (only a few elements of $phi$ are non-zero) and\n    orthogonal ($\\textcolor{lightgreen}{\\phi(k^{(i)})} \\cdot \\textcolor{lightgreen}{\\phi(k^{(j)})}\n     \\approx 0$ for most $i, j$\n    unless $k^{(i)}$ and $k^{(j)}$ are very similar.\n\n    ### Normalization\n\n    Paper introduces a simple normalization for $\\textcolor{lightgreen}{\\phi}$,\n\n    $$\\textcolor{lightgreen}{\\phi '(k)} =\n     \\frac{\\textcolor{lightgreen}{\\phi(k)}}{\\sum^{d_{dot}}_{j=1} \\textcolor{lightgreen}{\\phi(k)_j}}$$\n\n    *Check the paper for derivation.*\n    \"\"\"\n\n    def __init__(self, nu: int = 1, eps: float = 1e-6):\n        \"\"\"\n        * `nu` is the hyper-parameter $\\nu$.\n        * `eps` is the small value used to make sure there is no division-by-zero when normalizing.\n        \"\"\"\n        super().__init__()\n        self.nu = nu\n        self.relu = nn.ReLU()\n        self.eps = eps\n\n    def forward(self, k: torch.Tensor):\n        # Get $\\textcolor{lightgreen}{\\phi(k)}$\n        k = self.dpfp(k)\n        # Normalize by $\\sum^{d_{dot}}_{j=1} \\textcolor{lightgreen}{\\phi(k)_j}$\n        return k / (torch.sum(k, dim=-1, keepdim=True) + self.eps)\n\n    def dpfp(self, k: torch.Tensor):\n        \"\"\"\n        $$\\textcolor{lightgreen}{\\phi(k)}$$\n        \"\"\"\n        # $x = \\text{ReLU}\\Big(\\big[k, -k\\big]\\Big)$\n        x = self.relu(torch.cat([k, -k], dim=-1))\n        # Shift and roll by $i \\in \\\\{1, 2, ..., \\nu \\\\}$,\n        # to get $$x'_{i,j} = \\text{ReLU}\\Big(\\big[k, -k\\big]\\Big)_{i+j}$$\n        x_rolled = [x.roll(shifts=i, dims=-1) for i in range(1, self.nu + 1)]\n        # Concatenate to get\n        # $$x'_{2 d_{key} (i - 1)  + j} = \\text{ReLU}\\Big(\\big[k, -k\\big]\\Big)_{i+j}$$\n        x_rolled = torch.cat(x_rolled, dim=-1)\n        # Concatenate copies of $x$\n        x_repeat = torch.cat([x] * self.nu, dim=-1)\n\n        # Multiply them,\n        # $$\\textcolor{lightgreen}{\\phi_{2 d_{key} (i - 1)  + j}(k)}\n        # = \\text{ReLU}\\Big(\\big[k, -k\\big]\\Big)_{j}\n        #                         \\text{ReLU}\\Big(\\big[k, -k\\big]\\Big)_{i + j}$$\n        return x_repeat * x_rolled\n\n\nclass FastWeightsAttention(Module):\n    \"\"\"\n    ## Fast Weights Attention\n\n    The paper introduces a new update rule for calculating $\\textcolor{cyan}{W^{(i)}}$.\n    The model first retrieves the current value\n    $\\bar{v}^{(i)}$ paired with the key $k^{(i)}$.\n    Then stores a combination $v^{(i)}_{new}$\n    of the retrieved value $\\bar{v}^{(i)}$ and the input $v^{(i)}$.\n\n    \\begin{align}\n    k^{(i)}, v^{(i)}, q^{(i)} &=\n     \\textcolor{orange}{W_k} x^{(i)}, \\textcolor{orange}{W_v} x^{(i)}, \\textcolor{orange}{W_q} x^{(i)} \\\\\n    \\bar{v}^{(i)} &= \\textcolor{cyan}{W^{(i-1)}} \\textcolor{lightgreen}{\\phi'(k^{(i)})} \\\\\n    \\beta^{(i)} &= \\sigma \\Big(\\textcolor{orange}{W_\\beta} x^{(i)} \\Big) \\\\\n    v^{(i)}_{new} &= \\beta^{(i)} v^{(i)} + \\Big(1 - \\beta^{(i)} \\Big) \\bar{v}^{(i)} \\\\\n    \\textcolor{cyan}{W^{(i)}}\n     &= \\textcolor{cyan}{W^{(i-1)}} + v^{(i)}_{new} \\otimes \\textcolor{lightgreen}{\\phi'(k^{(i)})} \\\\\n     &= \\textcolor{cyan}{W^{(i-1)}} +\n     \\beta^{(i)} \\Big( v^{(i)} - \\bar{v}^{(i)} \\Big ) \\otimes \\textcolor{lightgreen}{\\phi'(k^{(i)})} \\\\\n    y^{(i)} &= \\textcolor{cyan}{W^{(i)}} \\textcolor{lightgreen}{\\phi'(q^{(i)})}\n    \\end{align}\n\n    where $\\textcolor{orange}{W_\\beta}$ is a trainable parameter and $\\sigma$ is the sigmoid function.\n\n    Note that we don't need the normalization term $z$ because $\\textcolor{lightgreen}{\\phi'}$ is normalized.\n    \"\"\"\n\n    def __init__(self, heads: int, d_model: int, dropout_prob: float, phi: DPFP):\n        super().__init__()\n\n        # Number of features per head $d_k$\n        self.d_k = d_model // heads\n        # Number of heads\n        self.heads = heads\n\n        # These transform the `query`, `key` and `value` multi-headed attention.\n        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=False)\n        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=False)\n        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=False)\n\n        # Interpolation weight function $\\sigma \\Big(\\textcolor{orange}{W_\\beta} x^{(i)} \\Big)$ for each head\n        self.interpolation_weight = nn.Sequential(\n            PrepareForMultiHeadAttention(d_model, heads, 1, bias=False),\n            nn.Sigmoid()\n        )\n\n        # $\\textcolor{lightgreen}{\\phi'}$\n        self.phi = phi\n\n        # Output layer\n        self.output = nn.Linear(d_model, d_model)\n        # Dropout\n        self.dropout = nn.Dropout(dropout_prob)\n\n    def forward(self, x: torch.Tensor):\n        # Get the number of steps $L$\n        seq_len = x.shape[0]\n        # $\\textcolor{lightgreen}{\\phi'(q^{(i)})}$ for all steps and heads\n        query = self.phi(self.query(x))\n        # $\\textcolor{lightgreen}{\\phi'(k^{(i)})}$ for all steps and heads\n        key = self.phi(self.key(x))\n        # $v^{(i)}$ for all steps and heads\n        value = self.value(x)\n        # $\\beta^{(i)}$ for all steps and heads\n        beta = self.interpolation_weight(x)\n\n        # $\\textcolor{cyan}{W^{(0)}}$\n        weights = key.new_zeros((key.shape[1], key.shape[2], value.shape[3], key.shape[3]))\n        # List to store outputs $y^{(i)}$\n        outputs = []\n\n        # Iterate through steps\n        for i in range(seq_len):\n            # $$\\bar{v}^{(i)} = \\textcolor{cyan}{W^{(i-1)}} \\textcolor{lightgreen}{\\phi'(k^{(i)})}$$\n            value_existing = torch.einsum('bhvk,bhk->bhv', weights, key[i])\n\n            # $$\\textcolor{cyan}{W^{(i)}}\n            #      = \\textcolor{cyan}{W^{(i-1)}} +\n            #      \\beta^{(i)} \\Big( v^{(i)} - \\bar{v}^{(i)} \\Big ) \\otimes \\textcolor{lightgreen}{\\phi'(k^{(i)})}$$\n            weights = weights + torch.einsum('bhv,bhk->bhvk', beta[i] * (value[i] - value_existing), key[i])\n\n            # $$y^{(i)} = \\textcolor{cyan}{W^{(i)}} \\textcolor{lightgreen}{\\phi'(q^{(i)})}$$\n            y = torch.einsum('bhvk,bhk->bhv', weights, query[i])\n\n            # Merge multiple heads and append to `outputs`\n            outputs.append(y.reshape(y.shape[0], -1))\n\n        # Stack outputs at each step into a single tensor\n        x = torch.stack(outputs)\n\n        # Output layer\n        return self.output(x)\n\n\nclass FastWeightsAttentionTransformerLayer(Module):\n    \"\"\"\n    This is a general transformer layer that combines self attention and feedforward network.\n    \"\"\"\n    def __init__(self, *,\n                 d_model: int,\n                 attn: FastWeightsAttention,\n                 feed_forward: FeedForward,\n                 dropout_prob: float):\n        super().__init__()\n        # Transformer size $d_{model}$\n        self.size = d_model\n        # Fast weights attention module\n        self.attn = attn\n        # Feed-forward network\n        self.feed_forward = feed_forward\n        # Dropout layer\n        self.dropout = nn.Dropout(dropout_prob)\n\n        # Normalization layers\n        self.norm_self_attn = nn.LayerNorm([d_model])\n        self.norm_ff = nn.LayerNorm([d_model])\n\n    def forward(self, x: torch.Tensor):\n        # Calculate fast weights self attention\n        attn = self.attn(x)\n        # Add the self attention results\n        x = x + self.dropout(attn)\n\n        # Normalize for feed-forward\n        z = self.norm_ff(x)\n        # Pass through the feed-forward network\n        ff = self.feed_forward(z)\n        # Add the feed-forward results back\n        x = x + self.dropout(ff)\n\n        #\n        return x\n\n\nclass FastWeightsAttentionTransformer(Module):\n    \"\"\"\n    This is a general transformer module with multiple transformer layers\n    \"\"\"\n    def __init__(self, layer: FastWeightsAttentionTransformerLayer, n_layers: int):\n        super().__init__()\n        # Make copies of the transformer layer\n        self.layers = clone_module_list(layer, n_layers)\n        # Final normalization layer\n        self.norm = nn.LayerNorm([layer.size])\n\n    def forward(self, x: torch.Tensor):\n        for i, layer in enumerate(self.layers):\n            # Get layer output\n            x = layer(x)\n\n        # Normalize the output\n        return self.norm(x)\n", "labml_nn/transformers/fnet/experiment.py": "\"\"\"\n---\ntitle: FNet Experiment\nsummary: This experiment trains a FNet based model on AG News dataset.\n---\n\n# [FNet](index.html) Experiment\n\nThis is an annotated PyTorch experiment to train a [FNet model](index.html).\n\nThis is based on\n[general training loop and configurations for AG News classification task](../../experiments/nlp_classification.html).\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_nn.experiments.nlp_classification import NLPClassificationConfigs\nfrom labml_nn.transformers import Encoder\nfrom labml_nn.transformers import TransformerConfigs\n\n\nclass TransformerClassifier(nn.Module):\n    \"\"\"\n    # Transformer based classifier model\n    \"\"\"\n    def __init__(self, encoder: Encoder, src_embed: Module, generator: nn.Linear):\n        \"\"\"\n        * `encoder` is the transformer [Encoder](../models.html#Encoder)\n        * `src_embed` is the token\n        [embedding module (with positional encodings)](../models.html#EmbeddingsWithLearnedPositionalEncoding)\n        * `generator` is the [final fully connected layer](../models.html#Generator) that gives the logits.\n        \"\"\"\n        super().__init__()\n        self.src_embed = src_embed\n        self.encoder = encoder\n        self.generator = generator\n\n    def forward(self, x: torch.Tensor):\n        # Get the token embeddings with positional encodings\n        x = self.src_embed(x)\n        # Transformer encoder\n        x = self.encoder(x, None)\n        # Get logits for classification.\n        #\n        # We set the `[CLS]` token at the last position of the sequence.\n        # This is extracted by `x[-1]`, where `x` is of\n        # shape `[seq_len, batch_size, d_model]`\n        x = self.generator(x[-1])\n\n        # Return results\n        # (second value is for state, since our trainer is used with RNNs also)\n        return x, None\n\n\nclass Configs(NLPClassificationConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`NLPClassificationConfigs`](../../experiments/nlp_classification.html)\n    \"\"\"\n\n    # Classification model\n    model: TransformerClassifier\n    # Transformer\n    transformer: TransformerConfigs\n\n\n@option(Configs.transformer)\ndef _transformer_configs(c: Configs):\n    \"\"\"\n    ### Transformer configurations\n    \"\"\"\n\n    # We use our\n    # [configurable transformer implementation](../configs.html#TransformerConfigs)\n    conf = TransformerConfigs()\n    # Set the vocabulary sizes for embeddings and generating logits\n    conf.n_src_vocab = c.n_tokens\n    conf.n_tgt_vocab = c.n_tokens\n\n    #\n    return conf\n\n\n@option(TransformerConfigs.encoder_attn)\ndef fnet_mix():\n    \"\"\"\n    Create `FNetMix` module that can replace the self-attention in\n    [transformer encoder layer](../models.html#TransformerLayer)\n.\n    \"\"\"\n    from labml_nn.transformers.fnet import FNetMix\n    return FNetMix()\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    Create classification model\n    \"\"\"\n    m = TransformerClassifier(c.transformer.encoder,\n                              c.transformer.src_embed,\n                              nn.Linear(c.d_model, c.n_classes)).to(c.device)\n\n    return m\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"fnet\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use world level tokenizer\n        'tokenizer': 'basic_english',\n\n        # Train for $32$ epochs\n        'epochs': 32,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        # Transformer configurations (same as defaults)\n        'transformer.d_model': 512,\n        'transformer.ffn.d_ff': 2048,\n        'transformer.n_heads': 8,\n        'transformer.n_layers': 6,\n\n        # Use [FNet](index.html) instead of self-a\n        # ttention\n        'transformer.encoder_attn': 'fnet_mix',\n\n        # Use [Noam optimizer](../../optimizers/noam.html)\n        'optimizer.optimizer': 'Noam',\n        'optimizer.learning_rate': 1.,\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/fnet/__init__.py": "\"\"\"\n---\ntitle: \"FNet: Mixing Tokens with Fourier Transforms\"\nsummary: >\n  This is an annotated implementation/tutorial of FNet in PyTorch.\n---\n\n# FNet: Mixing Tokens with Fourier Transforms\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824).\n\nThis paper replaces the [self-attention layer](../mha.html) with two\n[Fourier transforms](https://en.wikipedia.org/wiki/Discrete_Fourier_transform) to\n*mix* tokens.\nThis is a $7 \\times$ more efficient than self-attention.\nThe accuracy loss of using this over self-attention is about 92% for\n[BERT](https://paperswithcode.com/method/bert) on\n[GLUE benchmark](https://paperswithcode.com/dataset/glue).\n\n## Mixing tokens with two Fourier transforms\n\nWe apply Fourier transform along the hidden dimension (embedding dimension)\n and then along the sequence dimension.\n\n$$\n\\mathcal{R}\\big(\\mathcal{F}_\\text{seq} \\big(\\mathcal{F}_\\text{hidden} (x) \\big) \\big)\n$$\n\nwhere $x$ is the embedding input, $\\mathcal{F}$ stands for the fourier transform and\n$\\mathcal{R}$ stands for the real component in complex numbers.\n\nThis is very simple to implement on PyTorch - just 1 line of code.\nThe paper suggests using a precomputed DFT matrix and doing matrix multiplication to get the\nFourier transformation.\n\nHere is [the training code](experiment.html) for using a FNet based model for classifying\n[AG News](https://paperswithcode.com/dataset/ag-news).\n\"\"\"\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\n\nclass FNetMix(nn.Module):\n    \"\"\"\n    ## FNet - Mix tokens\n\n    This module simply implements\n    $$\n    \\mathcal{R}\\big(\\mathcal{F}_\\text{seq} \\big(\\mathcal{F}_\\text{hidden} (x) \\big) \\big)\n    $$\n\n    The structure of this module is made similar to a [standard attention module](../mha.html) so that we can simply\n    replace it.\n    \"\"\"\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        The [normal attention module](../mha.html) can be fed with different token embeddings for\n        $\\text{query}$,$\\text{key}$, and $\\text{value}$ and a mask.\n\n        We follow the same function signature so that we can replace it directly.\n\n        For FNet mixing, $$x = \\text{query} = \\text{key} = \\text{value}$$ and masking is not possible.\n        Shape of `query` (and `key` and `value`) is `[seq_len, batch_size, d_model]`.\n        \"\"\"\n\n        # $\\text{query}$,$\\text{key}$, and $\\text{value}$ all should be equal to $x$ for token mixing\n        assert query is key and key is value\n        # Token mixing doesn't support masking. i.e. all tokens will see all other token embeddings.\n        assert mask is None\n\n        # Assign to `x` for clarity\n        x = query\n\n        # Apply the Fourier transform along the hidden (embedding) dimension\n        # $$\\mathcal{F}_\\text{hidden} (x)$$\n        #\n        # The output of the Fourier transform is a tensor of\n        # [complex numbers](https://pytorch.org/docs/stable/complex_numbers.html).\n        fft_hidden = torch.fft.fft(x, dim=2)\n        # Apply the Fourier transform along the sequence dimension\n        # $$\\mathcal{F}_\\text{seq} \\big(\\mathcal{F}_\\text{hidden} (x) \\big)$$\n        fft_seq = torch.fft.fft(fft_hidden, dim=0)\n\n        # Get the real component\n        # $$\\mathcal{R}\\big(\\mathcal{F}_\\text{seq} \\big(\\mathcal{F}_\\text{hidden} (x) \\big) \\big)$$\n        return torch.real(fft_seq)\n", "labml_nn/transformers/gpt/__init__.py": "\"\"\"\n---\ntitle: GPT\nsummary: >\n  Implementation/tutorial of GPT model and training code.\n---\n\n# GPT\n\nThis is a tutorial/implementation of\n[OpenAI GPT architecture](https://openai.com/blog/better-language-models/)\nin [PyTorch](https://pytorch.org).\nWe got a bunch of implementation details from\n[minGPT](https://github.com/karpathy/minGPT)\nby [@karpathy](https://twitter.com/karpathy).\nThis implementation also uses character tiny shakespeare dataset.\n\nGPT model is essentially a standard transformer with a few tweaks.\nGPT-2 and especially GPT-3 models are quite large and won't fit on a\nsingle GPU and will need model parallelism.\nThis implementation doesn't even use data parallelism and is intended to be\nmore of a tutorial.\n\nMain differences of this compared to a simple autoregressive transformer\nare the parameter initialization, weight decay, and learning rate schedule.\nFor the transformer we reuse the\n[existing labml/nn transformer implementation](../transformers/index.html).\n\nHere's a notebook for training a GPT model on Tiny Shakespeare dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/gpt/experiment.ipynb)\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.optimizers.configs import OptimizerConfigs\nfrom labml_nn.transformers import TransformerConfigs, Encoder\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass GPT(Module):\n    \"\"\"\n    ## GPT model\n\n    This consists of a token embedding layer, transformer encoder, and\n    a final linear layer that gives token logits.\n    \"\"\"\n\n    def __init__(self, encoder: Encoder, src_embed: Module, generator: Module):\n        \"\"\"\n        * `encoder` is the transformer [Encoder](../models.html#Encoder)\n        * `src_embed` is the token\n        [embedding module (with positional encodings)](../models.html#EmbeddingsWithLearnedPositionalEncoding)\n        * `generator` is the [final fully connected layer](../models.html#Generator) that gives the logits.\n        \"\"\"\n        super().__init__()\n        self.src_embed = src_embed\n        self.encoder = encoder\n        self.generator = generator\n\n        # The mask will be initialized on the first call\n        self.mask = None\n\n    def forward(self, x: torch.Tensor):\n        # Create subsequent mask if mask is not initialized\n        # or if the size of the mask is different\n        if self.mask is None or self.mask.size(0) != len(x):\n            # Subsequent mask, will mask out tokens from seeing future tokens\n            self.mask = subsequent_mask(len(x)).to(x.device)\n        # Get the token embeddings with positional encodings\n        x = self.src_embed(x)\n        # Transformer encoder\n        x = self.encoder(x, self.mask)\n        # Get logits\n        x = self.generator(x)\n\n        # Return results\n        # (second value is for state, since our trainer is used with RNNs also)\n        return x, None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`NLPAutoRegressionConfigs`](../../experiments/nlp_autoregression.html#NLPAutoRegressionConfigs)\n    \"\"\"\n\n    # GPT model\n    model: GPT\n    # Transformer\n    transformer: TransformerConfigs\n    # Weight decay\n    weight_decay: float = 0.1\n    # Number of tokens for wamup\n    warmup_steps: int = 128 * 128 * 20\n\n    # Custom optimizer\n    optimizer = 'transformer_optimizer'\n\n\n@option(Configs.transformer, 'GPT')\ndef _transformer_configs(c: Configs):\n    \"\"\"\n    ### Transformer configurations\n    \"\"\"\n\n    # We use our\n    # [configurable transformer implementation](../configs.html#TransformerConfigs)\n    conf = TransformerConfigs()\n    # Set the vocabulary sizes for embeddings and generating logits\n    conf.n_src_vocab = c.n_tokens\n    conf.n_tgt_vocab = c.n_tokens\n    # GPT uses GELU activation for position wise feedforward\n    conf.ffn.activation = 'GELU'\n\n    #\n    return conf\n\n\ndef _init_weights(module):\n    \"\"\"\n    ### Initialize weights\n\n    Weights of linear layers and embedding layers are initialized\n    to $\\mathcal{N}(0, 0.02)$\n    instead of the default Xavier initialzation.\n    \"\"\"\n\n    if not isinstance(module, (nn.Linear, nn.Embedding)):\n        return\n\n    module.weight.data.normal_(mean=0.0, std=0.02)\n\n    # Initialize biases to $0$\n    if isinstance(module, nn.Linear) and module.bias is not None:\n        module.bias.data.zero_()\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    Create GPT model and initialize weights\n    \"\"\"\n    m = GPT(c.transformer.encoder,\n            c.transformer.src_embed,\n            c.transformer.generator).to(c.device)\n\n    # Apply custom weight initialization\n    m.apply(_init_weights)\n\n    return m\n\n\n@option(NLPAutoRegressionConfigs.optimizer)\ndef transformer_optimizer(c: NLPAutoRegressionConfigs):\n    \"\"\"\n    ### Create custom optimizer with weight decay\n\n    This code is taken from [minGPT](https://github.com/karpathy/minGPT).\n    This applies weight decay only to weights of linear layers.\n    \"\"\"\n    # Collect names of parameters to apply weight decay\n    decay = set()\n    for mn, m in c.model.named_modules():\n        for pn, p in m.named_parameters():\n            fpn = f'{mn}.{pn}' if mn else pn  # full param name\n\n            if fpn.endswith('weight') and isinstance(m, nn.Linear):\n                decay.add(fpn)\n\n    # Get all the parameters\n    param_dict = {pn: p for pn, p in c.model.named_parameters()}\n    # Parameters that are not decayed\n    no_decay = set(param_dict.keys()) - decay\n\n    # create the pytorch optimizer object\n    opt_groups = [\n        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": c.weight_decay},\n        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n    ]\n\n    # Create a [configurable optimizer](../optimizers/configs.html#OptimizerConfigs),\n    # so that we can change these simply by passing\n    # a config dictionary.\n    optimizer = OptimizerConfigs()\n\n    # Set parameter groups for optimization.\n    optimizer.parameters = opt_groups\n    # Use [cosine decay optimizer](../optimizers/adam_warmup_cosine_decay.html).\n    # This is what GPT uses.\n    optimizer.optimizer = 'AdamWarmupCosineDecay'\n    # Set model embedding size, required if we use [Noam optimizer](../optimizers/noam.html)\n    # which has an exponential decay.\n    optimizer.d_model = c.d_model\n    # Set default weight decay.\n    # This is not required since we set the weight decay in the parameter groups.\n    optimizer.weight_decay = c.weight_decay\n    # GPT uses a maximum learning rate of $6 \\times 10^{-4}$.\n    optimizer.learning_rate = 6e-4\n    # $\\beta_1 = 0.9, \\beta_2 = 0.95$\n    optimizer.betas = (0.9, 0.95)\n    # $\\epsilon = 10^{-8}$\n    optimizer.eps = 1e-8\n    # Weight decay is decoupled from gradients\n    optimizer.weight_decouple = True\n    # Total number of optimization steps for learning rate cosine decay\n    optimizer.total_steps = c.epochs * len(c.text.train) // (c.batch_size * c.seq_len)\n    # Number of warmup optimization steps\n    optimizer.warmup = c.warmup_steps // (c.batch_size * c.seq_len)\n\n    return optimizer\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"gpt\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $128$\n        'seq_len': 128,\n        # Train for $32$ epochs\n        'epochs': 32,\n        # Batch size $128$\n        'batch_size': 128,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        # Transformer configurations\n        'transformer.d_model': 512,\n        'transformer.ffn.d_ff': 2048,\n        'transformer.n_heads': 8,\n        'transformer.n_layers': 6\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/xl/experiment.py": "\"\"\"\n---\ntitle: Transformer XL Experiment\nsummary: This experiment trains a transformer XL model on tiny Shakespeare dataset.\n---\n\n# Transformer XL Experiment\n\nThis is an annotated PyTorch experiment to train a transformer xl model.\n\"\"\"\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom labml.logger import Text\n\nfrom labml import experiment, tracker, monit, logger\nfrom labml.configs import option\nfrom labml_helpers.metrics.simple_state import SimpleStateModule\nfrom labml_helpers.module import Module\nfrom labml_helpers.train_valid import BatchIndex, hook_model_outputs\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers.xl import TransformerXL, TransformerXLLayer\n\n\nclass AutoregressiveModel(Module):\n    \"\"\"\n    ## Auto regressive model\n    \"\"\"\n\n    def __init__(self, n_vocab: int, d_model: int, transformer: TransformerXL):\n        super().__init__()\n        # Token embedding module\n        self.src_embed = nn.Embedding(n_vocab, d_model)\n        # Transformer\n        self.transformer = transformer\n        # Final layer\n        self.generator = nn.Linear(d_model, n_vocab)\n        # Masks\n        self.mask_x = None\n        self.mask_mem = None\n\n    def forward(self, x: torch.Tensor, mem: List[torch.Tensor]):\n        # Length of the memory\n        m_len = len(mem[0]) if mem else 0\n        # Create a subsequent mask for tokens\n        if self.mask_x is None or self.mask_x.shape[0] < len(x):\n            from labml_nn.transformers.utils import subsequent_mask\n            self.mask_x = subsequent_mask(len(x)).to(x.device)\n        # Create an all ones (full visibility) mask for memory\n        if self.mask_mem is None or self.mask_mem.shape[1] < m_len or self.mask_mem.shape[0] < len(x):\n            self.mask_mem = self.mask_x.new_ones(len(x), m_len, 1)\n\n        # Concatenate the masks if there is memory\n        if m_len:\n            mask = torch.cat((self.mask_mem[:len(x), :m_len], self.mask_x[:len(x), :len(x)]), dim=1)\n        # Use the subsequent mask otherwise\n        else:\n            mask = self.mask_x[:len(x), :len(x)]\n\n        # Token embeddings\n        x = self.src_embed(x)\n        # Run it through the transformer\n        res, mem = self.transformer(x, mem, mask)\n        # Generate logits of the next token\n        res = self.generator(res)\n        #\n        return res, mem\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    The default configs can and will be over-ridden when we start the experiment\n    \"\"\"\n\n    model: AutoregressiveModel\n\n    # Token embedding size\n    d_model: int = 128\n    # Number of attention heads\n    heads: int = 4\n    # Dropout probability\n    dropout: float = 0.0\n    # Number of features in FFN hidden layer\n    d_ff: int = 256\n    # Number of transformer layers\n    n_layers: int = 6\n    # Number of memories to keep\n    mem_len: int = 128\n    # State module to maintain memories when switching between training and validation\n    memory = SimpleStateModule()\n\n    def init(self):\n        # Set tracker configurations\n        tracker.set_scalar(\"accuracy.*\", True)\n        tracker.set_scalar(\"loss.*\", True)\n        # Add a hook to log module outputs\n        hook_model_outputs(self.mode, self.model, 'model')\n        # This will keep the accuracy metric stats and memories separate for training and validation.\n        self.state_modules = [self.accuracy, self.memory]\n\n    def merge_memory(self, old_mem, new_mem):\n        \"\"\"\n        Concatenate memories and remove old memories to keep a maximum of\n        `mem_len` memories.\n        \"\"\"\n\n        # If it's configured not to use memory\n        if self.mem_len == 0:\n            return []\n\n        # Concatenate with old memory\n        if old_mem:\n            mem = [torch.cat((m, x), dim=0) for m, x in zip(old_mem, new_mem)]\n        else:\n            mem = new_mem\n\n        # Truncate old memories\n        if len(mem[0]) > self.mem_len:\n            mem = [m[-self.mem_len:] for m in mem]\n\n        #\n        return mem\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training/validation step\n        \"\"\"\n\n        # Move data to the device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Update global step (number of tokens processed) when in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(data.shape[0] * data.shape[1])\n\n        # Whether to capture model outputs\n        with self.mode.update(is_log_activations=batch_idx.is_last):\n            # Get memories\n            mem = self.memory.get()\n            # Run the model\n            output, new_mem = self.model(data, mem)\n            # Merge memory\n            mem = self.merge_memory(mem, new_mem)\n            # Update memories\n            self.memory.set(mem)\n\n        # Calculate and log cross entropy loss\n        loss = self.loss_func(output, target)\n        tracker.add(\"loss.\", loss)\n\n        # Calculate and log accuracy\n        self.accuracy(output, target)\n        self.accuracy.track()\n\n        # Train the model\n        if self.mode.is_train:\n            # Calculate gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Take optimizer step\n            self.optimizer.step()\n            # Log the model parameters and gradients on last batch of every epoch\n            if batch_idx.is_last:\n                tracker.add('model', self.model)\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save the tracked metrics\n        tracker.save()\n\n    def sample(self):\n        \"\"\"\n        ### Sampling function to generate samples periodically while training\n        \"\"\"\n\n        # Starting prompt\n        prompt = self.prompt\n        # Collect output for printing\n        log = [(prompt, Text.subtle)]\n        # memory\n        mem = []\n        # Sample 25 tokens\n        for i in monit.iterate('Sample', 25):\n            # Tokenize the prompt\n            data = self.text.text_to_i(prompt).unsqueeze(-1)\n            # Move to device\n            data = data.to(self.device)\n            # Get the model output\n            output, new_mem = self.model(data, mem)\n            # Get the model prediction (greedy)\n            output = output.argmax(dim=-1).squeeze(1)\n            # Add the prediction to prompt\n            prompt += self.prompt_separator + self.text.itos[output[-1]]\n            # Only feed the last character to model in next iteration, rest will go in as memories\n            prompt = prompt[-1:]\n            # Add the prediction for logging\n            log += [(self.prompt_separator + self.text.itos[output[-1]], Text.value)]\n            # Update memory\n            mem = self.merge_memory(mem, new_mem)\n\n        # Print the sampled output\n        logger.log(log)\n\n\n@option(Configs.model)\ndef autoregressive_model(c: Configs):\n    \"\"\"\n    ### Initialize the auto-regressive model\n    \"\"\"\n    from labml_nn.transformers.xl import RelativeMultiHeadAttention\n    from labml_nn.transformers.feed_forward import FeedForward\n    m = AutoregressiveModel(c.n_tokens, c.d_model, TransformerXL(\n        TransformerXLLayer(d_model=c.d_model,\n                           self_attn=RelativeMultiHeadAttention(c.heads, c.d_model, c.dropout),\n                           feed_forward=FeedForward(c.d_model, c.d_ff, c.dropout),\n                           dropout_prob=c.dropout), c.n_layers))\n    return m.to(c.device)\n\n\ndef main():\n    \"\"\"\n    ### Run the experiment\n    \"\"\"\n    # Create experiment\n    experiment.create(name=\"transformer_xl\", comment='')\n    # Create configs\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf,\n                       # A dictionary of configurations to override\n                       {'tokenizer': 'character',\n                        'text': 'tiny_shakespeare',\n                        'optimizer.learning_rate': 1.,\n                        'optimizer.optimizer': 'Noam',\n                        'prompt': 'It is',\n                        'prompt_separator': '',\n\n                        'train_loader': 'sequential_train_loader',\n                        'valid_loader': 'sequential_valid_loader',\n\n                        'seq_len': 2,\n                        'mem_len': 32,\n                        'epochs': 128,\n                        'batch_size': 32,\n                        'inner_iterations': 25,\n                        })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # `TrainValidConfigs.run`\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/xl/relative_mha.py": "\"\"\"\n---\ntitle: Relative Multi-Headed Attention\nsummary: >\n  Documented implementation with explanations of\n  Relative Multi-Headed Attention from paper Transformer-XL.\n---\n\n# Relative Multi-Headed Attention\n\nThis is an implementation of relative multi-headed attention from paper\n[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)\nin [PyTorch](https://pytorch.org).\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml.logger import inspect\nfrom labml_nn.transformers.mha import MultiHeadAttention\n\n\ndef shift_right(x: torch.Tensor):\n    \"\"\"\n    This method shifts $i^{th}$ row of a matrix by $i$ columns.\n\n    If the input is `[[1, 2 ,3], [4, 5 ,6], [7, 8, 9]]`, the shifted\n    result would be `[[1, 2 ,3], [0, 4, 5], [6, 0, 7]]`.\n    *Ideally we should mask out the lower triangle but it's ok for our purpose*.\n    \"\"\"\n\n    # Concatenate a column of zeros\n    zero_pad = x.new_zeros(x.shape[0], 1, *x.shape[2:])\n    x_padded = torch.cat([x, zero_pad], dim=1)\n\n    # Reshape and remove excess elements from the end\n    x_padded = x_padded.view(x.shape[1] + 1, x.shape[0], *x.shape[2:])\n    x = x_padded[:-1].view_as(x)\n\n    #\n    return x\n\n\nclass RelativeMultiHeadAttention(MultiHeadAttention):\n    \"\"\"\n    ## Relative Multi-Head Attention Module\n\n    We override [Multi-Head Attention](mha.html) module so we only need to \n    write the `get_scores` method.\n    \"\"\"\n\n    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n        # The linear transformations do not need a bias since we\n        # explicitly include it when calculating scores.\n        # However having a bias for `value` might make sense.\n        super().__init__(heads, d_model, dropout_prob, bias=False)\n\n        # Number of relative positions\n        self.P = 2 ** 12\n\n        # Relative positional embeddings for key relative to the query.\n        # We need $2P$ embeddings because the keys can be before or after the query.\n        self.key_pos_embeddings = nn.Parameter(torch.zeros((self.P * 2, heads, self.d_k)), requires_grad=True)\n        # Relative positional embedding bias for key relative to the query.\n        self.key_pos_bias = nn.Parameter(torch.zeros((self.P * 2, heads)), requires_grad=True)\n        # Positional embeddings for the query is independent of the position of the query\n        self.query_pos_bias = nn.Parameter(torch.zeros((heads, self.d_k)), requires_grad=True)\n\n    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n        r\"\"\"\n        ### Get relative attention scores\n\n        With absolute attention\n\n        \\begin{align}\n        A^{abs}_{j} &= lin_q(X^q_i + P_i)^\\top lin_k(X^k_j + P_j) \\\\\n                      &= \\underset{\\textcolor{lightgreen}{A}}{Q_i^\\top K_j} +\n                         \\underset{\\textcolor{lightgreen}{B}}{Q_i^\\top U^K_j} +\n                         \\underset{\\textcolor{lightgreen}{C}}{{U^Q_i}^\\top K_j} +\n                         \\underset{\\textcolor{lightgreen}{D}}{{U^Q_i}^\\top U^K_j}\n        \\end{align}\n\n        where $Q_i, K_j$, are linear transformations of\n         original embeddings $X^q_i, X^k_j$\n         and $U^Q_i, U^K_j$ are linear transformations of\n         absolute positional encodings $P_i, P_j$.\n\n        They reason out that the attention to a given key should be the same regardless of\n        the position of query.\n        Hence replace $\\underset{\\textcolor{lightgreen}{C}}{{U^Q_i}^\\top K_j}$\n        with a constant $\\underset{\\textcolor{lightgreen}{C}}{\\textcolor{orange}{v^\\top} K_j}$.\n\n        For the second and third terms relative positional encodings are introduced.\n        So $\\underset{\\textcolor{lightgreen}{B}}{Q_i^\\top U^K_j}$ is\n        replaced with $\\underset{\\textcolor{lightgreen}{B}}{Q_i^\\top \\textcolor{orange}{R_{i - j}}}$\n        and $\\underset{\\textcolor{lightgreen}{D}}{{U^Q_i}^\\top U^K_j}$\n        with $\\underset{\\textcolor{lightgreen}{D}}{\\textcolor{orange}{S_{i-j}}}$.\n\n        \\begin{align}\n        A^{rel}_{i,j} &= \\underset{\\mathbf{\\textcolor{lightgreen}{A}}}{Q_i^\\top K_j} +\n                         \\underset{\\mathbf{\\textcolor{lightgreen}{B}}}{Q_i^\\top \\textcolor{orange}{R_{i - j}}} +\n                         \\underset{\\mathbf{\\textcolor{lightgreen}{C}}}{\\textcolor{orange}{v^\\top} K_j} +\n                         \\underset{\\mathbf{\\textcolor{lightgreen}{D}}}{\\textcolor{orange}{S_{i-j}}}\n        \\end{align}\n        \"\"\"\n\n        # $\\textcolor{orange}{R_k}$\n        key_pos_emb = self.key_pos_embeddings[self.P - key.shape[0]:self.P + query.shape[0]]\n        # $\\textcolor{orange}{S_k}$\n        key_pos_bias = self.key_pos_bias[self.P - key.shape[0]:self.P + query.shape[0]]\n        # $\\textcolor{orange}{v^\\top}$\n        query_pos_bias = self.query_pos_bias[None, None, :, :]\n\n        # ${(\\textcolor{lightgreen}{\\mathbf{A + C}})}_{i,j} =\n        # Q_i^\\top K_j +\n        # \\textcolor{orange}{v^\\top} K_j$\n        ac = torch.einsum('ibhd,jbhd->ijbh', query + query_pos_bias, key)\n        # $\\textcolor{lightgreen}{\\mathbf{B'}_{i,k}} = Q_i^\\top \\textcolor{orange}{R_k}$\n        b = torch.einsum('ibhd,jhd->ijbh', query, key_pos_emb)\n        # $\\textcolor{lightgreen}{\\mathbf{D'}_{i,k}} = \\textcolor{orange}{S_k}$\n        d = key_pos_bias[None, :, None, :]\n        # Shift the rows of $\\textcolor{lightgreen}{\\mathbf{(B' + D')}_{i,k}}$\n        # to get $$\\textcolor{lightgreen}{\\mathbf{(B + D)}_{i,j} = \\mathbf{(B' + D')}_{i,i - j}}$$\n        bd = shift_right(b + d)\n        # Remove extra positions\n        bd = bd[:, -key.shape[0]:]\n\n        # Return the sum $$\n        # \\underset{\\mathbf{\\textcolor{lightgreen}{A}}}{Q_i^\\top K_j} +\n        # \\underset{\\mathbf{\\textcolor{lightgreen}{B}}}{Q_i^\\top \\textcolor{orange}{R_{i - j}}} +\n        # \\underset{\\mathbf{\\textcolor{lightgreen}{C}}}{\\textcolor{orange}{v^\\top} K_j} +\n        # \\underset{\\mathbf{\\textcolor{lightgreen}{D}}}{\\textcolor{orange}{S_{i-j}}}\n        # $$\n        return ac + bd\n\n\ndef _test_shift_right():\n    x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    inspect(x)\n    inspect(shift_right(x))\n\n    x = torch.arange(1, 6)[None, :, None, None].repeat(5, 1, 1, 1)\n    inspect(x[:, :, 0, 0])\n    inspect(shift_right(x)[:, :, 0, 0])\n\n    x = torch.arange(1, 6)[None, :, None, None].repeat(3, 1, 1, 1)\n    inspect(x[:, :, 0, 0])\n    inspect(shift_right(x)[:, :, 0, 0])\n\n\nif __name__ == '__main__':\n    _test_shift_right()\n", "labml_nn/transformers/xl/__init__.py": "\"\"\"\n---\ntitle: Transformer XL\nsummary: >\n  Documented implementation with explanations of a\n  Transformer-XL model.\n---\n\n# Transformer XL\n\nThis is an implementation of\n[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)\nin [PyTorch](https://pytorch.org).\n\nTransformer has a limited attention span,\nequal to the length of the sequence trained in parallel.\nAll these positions have a fixed positional encoding.\nTransformer XL increases this attention span by letting\neach of the positions pay attention to precalculated past embeddings.\nFor instance if the context length is $l$, it will keep the embeddings of\nall layers for previous batch of length $l$ and feed them to current step.\nIf we use fixed-positional encodings these pre-calculated embeddings will have\nthe same positions as the current context.\nThey introduce relative positional encoding, where the positional encodings\nare introduced at the attention calculation.\n\nAnnotated implementation of relative multi-headed attention is in [`relative_mha.py`](relative_mha.html).\n\nHere's [the training code](experiment.html) and a notebook for training a transformer XL model on Tiny Shakespeare dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/xl/experiment.ipynb)\n\"\"\"\n\n\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.utils import clone_module_list\nfrom .relative_mha import RelativeMultiHeadAttention\nfrom ..feed_forward import FeedForward\n\n\nclass TransformerXLLayer(Module):\n    \"\"\"\n    ## Transformer XL Layer\n\n    The transformer XL model comprises of a number of these layers.\n    \"\"\"\n    def __init__(self, *,\n                 d_model: int,\n                 self_attn: RelativeMultiHeadAttention,\n                 feed_forward: FeedForward,\n                 dropout_prob: float):\n        \"\"\"\n        * `d_model` is the token embedding size\n        * `self_attn` is the [self attention module](relative_mha.html)\n        * `feed_forward` is the feed forward module\n        * `dropout_prob` is the probability of dropping out after self attention and FFN\n        \"\"\"\n        super().__init__()\n        self.size = d_model\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.dropout = nn.Dropout(dropout_prob)\n        self.norm_self_attn = nn.LayerNorm([d_model])\n        self.norm_ff = nn.LayerNorm([d_model])\n\n    def forward(self, *,\n                x: torch.Tensor,\n                mem: Optional[torch.Tensor],\n                mask: torch.Tensor):\n        \"\"\"\n        * `x` is a tensor of the token level feature vectors of shape `[seq_len, batch_size, d_model]`\n        * `mem` is a tensor of the past token level feature vectors of shape `[mem_len, batch_size, d_model]`\n        * `mask` is a matrix of shape `[seq_len, mem_len + seq_len, batch_size]` or `[seq_len, mem_len + seq_len, 1]`.\n        `mask[i, j]` is  true if token at `i` can see token at `j`.\n        \"\"\"\n        # Normalize the vectors before doing self attention\n        z = self.norm_self_attn(x)\n        # If there is memory\n        if mem is not None:\n            # Normalize it\n            mem = self.norm_self_attn(mem)\n            # Concatenate with `z`\n            m_z = torch.cat((mem, z), dim=0)\n        # Ignore if there is no memory\n        else:\n            m_z = z\n        # Attention\n        self_attn = self.self_attn(query=z, key=m_z, value=m_z, mask=mask)\n        # Add the attention results\n        x = x + self.dropout(self_attn)\n\n        # Normalize for feed-forward\n        z = self.norm_ff(x)\n        # Pass through the feed-forward network\n        ff = self.feed_forward(z)\n        # Add the feed-forward results back\n        x = x + self.dropout(ff)\n\n        #\n        return x\n\n\nclass TransformerXL(Module):\n    \"\"\"\n    ## Transformer XL Model\n\n    This consists of multiple transformer XL layers\n    \"\"\"\n\n    def __init__(self, layer: TransformerXLLayer, n_layers: int):\n        super().__init__()\n        # Make copies of the transformer layer\n        self.layers = clone_module_list(layer, n_layers)\n        # Final normalization layer\n        self.norm = nn.LayerNorm([layer.size])\n\n    def forward(self, x: torch.Tensor, mem: List[torch.Tensor], mask: torch.Tensor):\n        \"\"\"\n        * `x` is a tensor of the token embeddings vectors of shape `[seq_len, batch_size, d_model]`\n        * `mem` is a list of tensors of the past token level feature vectors of shape\n        `[mem_len, batch_size, d_model]`  for each layer\n        * `mask` is the masking matrix\n        \"\"\"\n        # List to store token level feature vectors,\n        # which will become the memories for the next sequential batch.\n        new_mem = []\n        # Run through each transformer layer\n        for i, layer in enumerate(self.layers):\n            # Add to the list of feature vectors\n            new_mem.append(x.detach())\n            # Memory\n            m = mem[i] if mem else None\n            # Run through the transformer XL layer\n            x = layer(x=x, mem=m, mask=mask)\n        # Finally, normalize the vectors\n        return self.norm(x), new_mem\n", "labml_nn/transformers/vit/experiment.py": "\"\"\"\n---\ntitle: Train a Vision Transformer (ViT) on CIFAR 10\nsummary: >\n  Train a Vision Transformer (ViT) on CIFAR 10\n---\n\n#  Train a [Vision Transformer (ViT)](index.html) on CIFAR 10\n\"\"\"\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.experiments.cifar10 import CIFAR10Configs\nfrom labml_nn.transformers import TransformerConfigs\n\n\nclass Configs(CIFAR10Configs):\n    \"\"\"\n    ## Configurations\n\n    We use [`CIFAR10Configs`](../../experiments/cifar10.html) which defines all the\n    dataset related configurations, optimizer, and a training loop.\n    \"\"\"\n\n    # [Transformer configurations](../configs.html#TransformerConfigs)\n    # to get [transformer layer](../models.html#TransformerLayer)\n    transformer: TransformerConfigs\n\n    # Size of a patch\n    patch_size: int = 4\n    # Size of the hidden layer in classification head\n    n_hidden_classification: int = 2048\n    # Number of classes in the task\n    n_classes: int = 10\n\n\n@option(Configs.transformer)\ndef _transformer():\n    \"\"\"\n    Create transformer configs\n    \"\"\"\n    return TransformerConfigs()\n\n\n@option(Configs.model)\ndef _vit(c: Configs):\n    \"\"\"\n    ### Create model\n    \"\"\"\n    from labml_nn.transformers.vit import VisionTransformer, LearnedPositionalEmbeddings, ClassificationHead, \\\n        PatchEmbeddings\n\n    # Transformer size from [Transformer configurations](../configs.html#TransformerConfigs)\n    d_model = c.transformer.d_model\n    # Create a vision transformer\n    return VisionTransformer(c.transformer.encoder_layer, c.transformer.n_layers,\n                             PatchEmbeddings(d_model, c.patch_size, 3),\n                             LearnedPositionalEmbeddings(d_model),\n                             ClassificationHead(d_model, c.n_hidden_classification, c.n_classes)).to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name='ViT', comment='cifar10')\n    # Create configurations\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf, {\n        # Optimizer\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n\n        # Transformer embedding size\n        'transformer.d_model': 512,\n\n        # Training epochs and batch size\n        'epochs': 32,\n        'train_batch_size': 64,\n\n        # Augment CIFAR 10 images for training\n        'train_dataset': 'cifar10_train_augmented',\n        # Do not augment CIFAR 10 images for validation\n        'valid_dataset': 'cifar10_valid_no_augment',\n    })\n    # Set model for saving/loading\n    experiment.add_pytorch_models({'model': conf.model})\n    # Start the experiment and run the training loop\n    with experiment.start():\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/vit/__init__.py": "\"\"\"\n---\ntitle: Vision Transformer (ViT)\nsummary: >\n A PyTorch implementation/tutorial of the paper\n \"An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale\"\n---\n\n#  Vision Transformer (ViT)\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale](https://arxiv.org/abs/2010.11929).\n\nVision transformer applies a pure transformer to images\nwithout any convolution layers.\nThey split the image into patches and apply a transformer on patch embeddings.\n[Patch embeddings](#PathEmbeddings) are generated by applying a simple linear transformation\nto the flattened pixel values of the patch.\nThen a standard transformer encoder is fed with the patch embeddings, along with a\nclassification token `[CLS]`.\nThe encoding on the `[CLS]` token is used to classify the image with an MLP.\n\nWhen feeding the transformer with the patches, learned positional embeddings are\nadded to the patch embeddings, because the patch embeddings do not have any information\nabout where that patch is from.\nThe positional embeddings are a set of vectors for each patch location that get trained\nwith gradient descent along with other parameters.\n\nViTs perform well when they are pre-trained on large datasets.\nThe paper suggests pre-training them with an MLP classification head and\nthen using a single linear layer when fine-tuning.\nThe paper beats SOTA with a ViT pre-trained on a 300 million image dataset.\nThey also use higher resolution images during inference while keeping the\npatch size the same.\nThe positional embeddings for new patch locations are calculated by interpolating\nlearning positional embeddings.\n\nHere's [an experiment](experiment.html) that trains ViT on CIFAR-10.\nThis doesn't do very well because it's trained on a small dataset.\nIt's a simple experiment that anyone can run and play with ViTs.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.transformers import TransformerLayer\nfrom labml_nn.utils import clone_module_list\n\n\nclass PatchEmbeddings(Module):\n    \"\"\"\n    <a id=\"PatchEmbeddings\"></a>\n\n    ## Get patch embeddings\n\n    The paper splits the image into patches of equal size and do a linear transformation\n    on the flattened pixels for each patch.\n\n    We implement the same thing through a convolution layer, because it's simpler to implement.\n    \"\"\"\n\n    def __init__(self, d_model: int, patch_size: int, in_channels: int):\n        \"\"\"\n        * `d_model` is the transformer embeddings size\n        * `patch_size` is the size of the patch\n        * `in_channels` is the number of channels in the input image (3 for rgb)\n        \"\"\"\n        super().__init__()\n\n        # We create a convolution layer with a kernel size and and stride length equal to patch size.\n        # This is equivalent to splitting the image into patches and doing a linear\n        # transformation on each patch.\n        self.conv = nn.Conv2d(in_channels, d_model, patch_size, stride=patch_size)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input image of shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Apply convolution layer\n        x = self.conv(x)\n        # Get the shape.\n        bs, c, h, w = x.shape\n        # Rearrange to shape `[patches, batch_size, d_model]`\n        x = x.permute(2, 3, 0, 1)\n        x = x.view(h * w, bs, c)\n\n        # Return the patch embeddings\n        return x\n\n\nclass LearnedPositionalEmbeddings(Module):\n    \"\"\"\n    <a id=\"LearnedPositionalEmbeddings\"></a>\n\n    ## Add parameterized positional encodings\n\n    This adds learned positional embeddings to patch embeddings.\n    \"\"\"\n\n    def __init__(self, d_model: int, max_len: int = 5_000):\n        \"\"\"\n        * `d_model` is the transformer embeddings size\n        * `max_len` is the maximum number of patches\n        \"\"\"\n        super().__init__()\n        # Positional embeddings for each location\n        self.positional_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the patch embeddings of shape `[patches, batch_size, d_model]`\n        \"\"\"\n        # Get the positional embeddings for the given patches\n        pe = self.positional_encodings[:x.shape[0]]\n        # Add to patch embeddings and return\n        return x + pe\n\n\nclass ClassificationHead(Module):\n    \"\"\"\n    <a id=\"ClassificationHead\"></a>\n\n    ## MLP Classification Head\n\n    This is the two layer MLP head to classify the image based on `[CLS]` token embedding.\n    \"\"\"\n    def __init__(self, d_model: int, n_hidden: int, n_classes: int):\n        \"\"\"\n        * `d_model` is the transformer embedding size\n        * `n_hidden` is the size of the hidden layer\n        * `n_classes` is the number of classes in the classification task\n        \"\"\"\n        super().__init__()\n        # First layer\n        self.linear1 = nn.Linear(d_model, n_hidden)\n        # Activation\n        self.act = nn.ReLU()\n        # Second layer\n        self.linear2 = nn.Linear(n_hidden, n_classes)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the transformer encoding for `[CLS]` token\n        \"\"\"\n        # First layer and activation\n        x = self.act(self.linear1(x))\n        # Second layer\n        x = self.linear2(x)\n\n        #\n        return x\n\n\nclass VisionTransformer(Module):\n    \"\"\"\n    ## Vision Transformer\n\n    This combines the [patch embeddings](#PatchEmbeddings),\n    [positional embeddings](#LearnedPositionalEmbeddings),\n    transformer and the [classification head](#ClassificationHead).\n    \"\"\"\n    def __init__(self, transformer_layer: TransformerLayer, n_layers: int,\n                 patch_emb: PatchEmbeddings, pos_emb: LearnedPositionalEmbeddings,\n                 classification: ClassificationHead):\n        \"\"\"\n        * `transformer_layer` is a copy of a single [transformer layer](../models.html#TransformerLayer).\n         We make copies of it to make the transformer with `n_layers`.\n        * `n_layers` is the number of [transformer layers](../models.html#TransformerLayer).\n        * `patch_emb` is the [patch embeddings layer](#PatchEmbeddings).\n        * `pos_emb` is the [positional embeddings layer](#LearnedPositionalEmbeddings).\n        * `classification` is the [classification head](#ClassificationHead).\n        \"\"\"\n        super().__init__()\n        # Patch embeddings\n        self.patch_emb = patch_emb\n        self.pos_emb = pos_emb\n        # Classification head\n        self.classification = classification\n        # Make copies of the transformer layer\n        self.transformer_layers = clone_module_list(transformer_layer, n_layers)\n\n        # `[CLS]` token embedding\n        self.cls_token_emb = nn.Parameter(torch.randn(1, 1, transformer_layer.size), requires_grad=True)\n        # Final normalization layer\n        self.ln = nn.LayerNorm([transformer_layer.size])\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input image of shape `[batch_size, channels, height, width]`\n        \"\"\"\n        # Get patch embeddings. This gives a tensor of shape `[patches, batch_size, d_model]`\n        x = self.patch_emb(x)\n        # Concatenate the `[CLS]` token embeddings before feeding the transformer\n        cls_token_emb = self.cls_token_emb.expand(-1, x.shape[1], -1)\n        x = torch.cat([cls_token_emb, x])\n        # Add positional embeddings\n        x = self.pos_emb(x)\n\n        # Pass through transformer layers with no attention masking\n        for layer in self.transformer_layers:\n            x = layer(x=x, mask=None)\n\n        # Get the transformer output of the `[CLS]` token (which is the first in the sequence).\n        x = x[0]\n\n        # Layer normalization\n        x = self.ln(x)\n\n        # Classification head, to get logits\n        x = self.classification(x)\n\n        #\n        return x\n", "labml_nn/transformers/switch/experiment.py": "\"\"\"\n---\ntitle: Switch Transformer Experiment\nsummary: This experiment trains a small switch transformer on tiny Shakespeare dataset.\n---\n\n# Switch Transformer Experiment\n\nThis is an annotated PyTorch experiment to train a switch transformer.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/switch/experiment.ipynb)\n\"\"\"\n\nimport torch\nimport torch.nn as nn\n\nfrom labml import experiment, tracker\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_helpers.train_valid import BatchIndex\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\n\n\nclass AutoregressiveModel(Module):\n    \"\"\"\n    ## Auto regressive model\n    \"\"\"\n\n    def __init__(self, n_vocab: int, d_model: int, transformer: Module):\n        super().__init__()\n        # Token embedding module\n        self.src_embed = nn.Embedding(n_vocab, d_model)\n        # Transformer\n        self.transformer = transformer\n        # Final layer\n        self.generator = nn.Linear(d_model, n_vocab)\n        self.mask = None\n\n    def forward(self, x: torch.Tensor):\n        # Initialize the subsequent mask\n        if self.mask is None or self.mask.size(0) != len(x):\n            from labml_nn.transformers.utils import subsequent_mask\n            self.mask = subsequent_mask(len(x)).to(x.device)\n        # Token embeddings\n        x = self.src_embed(x)\n        # Run it through the transformer\n        res, counts, route_prob, n_dropped, route_prob_max = self.transformer(x, self.mask)\n        # Generate logits of the next token\n        res = self.generator(res)\n        #\n        return res, counts, route_prob, n_dropped, route_prob_max\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This extends [`NLPAutoRegressionConfigs`](../../experiments/nlp_autoregression.html).\n\n    The default configs can and will be over-ridden when we start the experiment\n    \"\"\"\n\n    model: AutoregressiveModel\n    transformer: Module\n\n    # Token embedding size\n    d_model: int = 128\n    # Number of attention heads\n    heads: int = 4\n    # Dropout probability\n    dropout: float = 0.0\n    # Number of features in FFN hidden layer\n    d_ff: int = 256\n    # Number of transformer layers\n    n_layers: int = 6\n    # Number of experts\n    n_experts: int = 4\n    # Load balancing coefficient\n    load_balancing_loss_ceof = 0.01\n    # Whether to scale the chosen expert outputs by the routing probability\n    is_scale_prob: bool = True\n    # Whether to drop tokens\n    drop_tokens: bool = False\n    # Capacity factor to determine capacity of each model\n    capacity_factor: float = 1.0\n\n    def init(self):\n        super().init()\n        # Initialize tracking indicators\n        tracker.set_scalar(\"lb_loss.*\", False)\n        tracker.set_scalar(\"route.*\", False)\n        tracker.set_scalar(\"dropped.*\", False)\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training or validation step\n        \"\"\"\n\n        # Move data to the device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Update global step (number of tokens processed) when in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(data.shape[0] * data.shape[1])\n\n        # Whether to capture model outputs\n        with self.mode.update(is_log_activations=batch_idx.is_last):\n            # Get model outputs.\n            output, counts, route_prob, n_dropped, route_prob_max = self.model(data)\n\n        # Calculate and cross entropy loss\n        cross_entropy_loss = self.loss_func(output, target)\n        # Total number of tokens processed, $T$, in the current batch $\\mathscr{B}$\n        total = counts.sum(dim=-1, keepdims=True)\n        # Fraction of tokens routed to each expert\n        # $$f_i = \\frac{1}{T} \\sum_{x \\in \\mathscr{B}} \\mathbf{1} \\{ \\mathop{argmax} p(x), i \\}$$\n        # $f_i$ is the count of tokens where the argmax of $p(x)$ is equal to $i$.\n        route_frac = counts / total\n        # Mean routing probability\n        # $$P_i = \\frac{1}{T} \\sum_{x \\in \\mathscr{B}} p_i (x)$$\n        route_prob = route_prob / total\n        # Load balancing loss\n        # $$\\mathscr{L} = N \\sum_{i=1}^N f_i \\cdot P_i$$\n        # $\\mathscr{L}$ is the loss for a single layer and here we are\n        # taking the sum of losses across all layers.\n        load_balancing_loss = self.n_experts * (route_frac * route_prob).sum()\n\n        # Track stats\n        tracker.add('dropped.', total.new_tensor(n_dropped) / total)\n        tracker.add('route.min.', route_frac.min())\n        tracker.add('route.max.', route_frac.max())\n        tracker.add('route.std.', route_frac.std())\n        tracker.add('route.max_prob.', route_prob_max)\n        tracker.add(\"loss.\", cross_entropy_loss)\n        tracker.add(\"lb_loss.\", load_balancing_loss)\n\n        # Combined loss.\n        # The load balancing loss is multiplied by a coefficient $\\alpha$ which is\n        # set to something small like $\\alpha = 0.01$.\n        loss = cross_entropy_loss + self.load_balancing_loss_ceof * load_balancing_loss\n\n        # Calculate and log accuracy\n        self.accuracy(output, target)\n        self.accuracy.track()\n\n        # Train the model\n        if self.mode.is_train:\n            # Calculate gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Take optimizer step\n            self.optimizer.step()\n            # Log the model parameters and gradients on last batch of every epoch\n            if batch_idx.is_last:\n                tracker.add('model', self.model)\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save the tracked metrics\n        tracker.save()\n\n\n@option(Configs.model)\ndef autoregressive_model(c: Configs):\n    \"\"\"\n    ### Initialize the auto-regressive model\n    \"\"\"\n    m = AutoregressiveModel(c.n_tokens, c.d_model, c.transformer)\n    return m.to(c.device)\n\n\n@option(Configs.transformer)\ndef switch_transformer(c: Configs):\n    \"\"\"\n    ### Initialize the switch transformer\n    \"\"\"\n    from labml_nn.transformers.switch import SwitchTransformer, SwitchTransformerLayer, SwitchFeedForward\n    from labml_nn.transformers import MultiHeadAttention\n    from labml_nn.transformers.feed_forward import FeedForward\n\n    return SwitchTransformer(\n        SwitchTransformerLayer(d_model=c.d_model,\n                               attn=MultiHeadAttention(c.heads, c.d_model, c.dropout),\n                               feed_forward=SwitchFeedForward(capacity_factor=c.capacity_factor,\n                                                              drop_tokens=c.drop_tokens,\n                                                              is_scale_prob=c.is_scale_prob,\n                                                              n_experts=c.n_experts,\n                                                              expert=FeedForward(c.d_model, c.d_ff, c.dropout),\n                                                              d_model=c.d_model),\n                               dropout_prob=c.dropout),\n        c.n_layers)\n\n\ndef main():\n    \"\"\"\n    ### Run the experiment\n    \"\"\"\n    # Create experiment\n    experiment.create(name=\"switch_transformer\", comment='')\n    # Create configs\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf,\n                       # A dictionary of configurations to override\n                       {'tokenizer': 'character',\n                        'text': 'tiny_shakespeare',\n                        'optimizer.learning_rate': 1.,\n                        'optimizer.optimizer': 'Noam',\n                        'prompt': 'It is',\n                        'prompt_separator': '',\n\n                        'transformer': 'switch_transformer',\n                        'n_experts': 4,\n\n                        'drop_tokens': True,\n                        'capacity_factor': 1.2,\n\n                        'train_loader': 'shuffled_train_loader',\n                        'valid_loader': 'shuffled_valid_loader',\n\n                        'seq_len': 64,\n                        'epochs': 128,\n                        'batch_size': 32,\n                        'inner_iterations': 25,\n                        })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # `TrainValidConfigs.run`\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/switch/__init__.py": "\"\"\"\n---\ntitle: Switch Transformer\nsummary: >\n  This is an annotated implementation/tutorial a miniature version of Switch Transformer in PyTorch.\n---\n\n# Switch Transformer\n\nThis is a miniature [PyTorch](https://pytorch.org) implementation of the paper\n[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961).\nOur implementation only has a few million parameters and doesn't do model parallel distributed training.\nIt does single GPU training, but we implement the concept of switching as described in the paper.\n\nThe Switch Transformer uses different parameters for each token by switching among parameters\nbased on the token.\nTherefore, only a fraction of parameters are chosen for each token.\nSo you can have more parameters but less computational cost.\n\nThe switching happens at the Position-wise Feedforward network (FFN) of each transformer block.\nPosition-wise feedforward network consists of two sequentially fully connected layers.\nIn switch transformer we have multiple FFNs (multiple experts),\nand we chose which one to use based on a router.\nThe output is a set of probabilities for picking a FFN,\nand we pick the one with the highest probability and only evaluate that.\nSo essentially the computational cost is the same as having a single FFN.\nIn our implementation this doesn't parallelize well when you have many or large FFNs since it's all\nhappening on a single GPU.\nIn a distributed setup you would have each FFN (each very large) on a different device.\n\nThe paper introduces another loss term to balance load among the experts (FFNs) and\ndiscusses dropping tokens when routing is not balanced.\n\nHere's [the training code](experiment.html) and a notebook for training a switch transformer on Tiny Shakespeare dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/switch/experiment.ipynb)\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.transformers.feed_forward import FeedForward\nfrom labml_nn.transformers.mha import MultiHeadAttention\nfrom labml_nn.utils import clone_module_list\n\n\nclass SwitchFeedForward(Module):\n    \"\"\"\n    ## Routing among multiple FFNs\n    \"\"\"\n\n    def __init__(self, *,\n                 capacity_factor: float,\n                 drop_tokens: bool,\n                 is_scale_prob: bool,\n                 n_experts: int,\n                 expert: FeedForward,\n                 d_model: int):\n        \"\"\"\n        * `capacity_factor` is the capacity of each expert as a factor relative to ideally balanced load\n        * `drop_tokens` specifies whether to drop tokens if more tokens are routed to an expert than the capacity\n        * `is_scale_prob` specifies whether to multiply the input to the FFN by the routing probability\n        * `n_experts` is the number of experts\n        * `expert` is the expert layer, a [FFN module](../feed_forward.html)\n        * `d_model` is the number of features in a token embedding\n        * `d_ff` is the number of features in the hidden layer of the FFN\n        * `dropout` is dropout probability in the FFN\n        \"\"\"\n        super().__init__()\n\n        self.capacity_factor = capacity_factor\n        self.is_scale_prob = is_scale_prob\n        self.n_experts = n_experts\n        self.drop_tokens = drop_tokens\n\n        # make copies of the FFNs\n        self.experts = clone_module_list(expert, n_experts)\n        # Routing layer and softmax\n        self.switch = nn.Linear(d_model, n_experts)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the input to the switching module with shape `[seq_len, batch_size, d_model]`\n        \"\"\"\n\n        # Capture the shape to change shapes later\n        seq_len, batch_size, d_model = x.shape\n        # Flatten the sequence and batch dimensions\n        x = x.view(-1, d_model)\n\n        # Get routing probabilities for each of the tokens.\n        # $$p_i(x) = \\frac{e^{h(x)_i}}{\\sum^N_j e^{h(x)_j}}$$\n        # where $N$ is the number of experts `n_experts` and\n        # $h(\\cdot)$ is the linear transformation of token embeddings.\n        route_prob = self.softmax(self.switch(x))\n\n        # Get the maximum routing probabilities and the routes.\n        # We route to the expert with highest probability\n        route_prob_max, routes = torch.max(route_prob, dim=-1)\n\n        # Get indexes of tokens going to each expert\n        indexes_list = [torch.eq(routes, i).nonzero(as_tuple=True)[0] for i in range(self.n_experts)]\n\n        # Initialize an empty tensor to store outputs\n        final_output = x.new_zeros(x.shape)\n\n        # Capacity of each expert.\n        # $$\\mathrm{expert\\;capacity} =\n        # \\frac{\\mathrm{tokens\\;per\\;batch}}{\\mathrm{number\\;of\\;experts}}\n        # \\times \\mathrm{capacity\\;factor}$$\n        capacity = int(self.capacity_factor * len(x) / self.n_experts)\n        # Number of tokens routed to each expert.\n        counts = x.new_tensor([len(indexes_list[i]) for i in range(self.n_experts)])\n\n        # Initialize an empty list of dropped tokens\n        dropped = []\n        # Only drop tokens if `drop_tokens` is `True`.\n        if self.drop_tokens:\n            # Drop tokens in each of the experts\n            for i in range(self.n_experts):\n                # Ignore if the expert is not over capacity\n                if len(indexes_list[i]) <= capacity:\n                    continue\n                # Shuffle indexes before dropping\n                indexes_list[i] = indexes_list[i][torch.randperm(len(indexes_list[i]))]\n                # Collect the tokens over capacity as dropped tokens\n                dropped.append(indexes_list[i][capacity:])\n                # Keep only the tokens upto the capacity of the expert\n                indexes_list[i] = indexes_list[i][:capacity]\n\n        # Get outputs of the expert FFNs\n        expert_output = [self.experts[i](x[indexes_list[i], :]) for i in range(self.n_experts)]\n\n        # Assign to final output\n        for i in range(self.n_experts):\n            final_output[indexes_list[i], :] = expert_output[i]\n\n        # Pass through the dropped tokens\n        if dropped:\n            dropped = torch.cat(dropped)\n            final_output[dropped, :] = x[dropped, :]\n\n        if self.is_scale_prob:\n            # Multiply by the expert outputs by the probabilities $y = p_i(x) E_i(x)$\n            final_output = final_output * route_prob_max.view(-1, 1)\n        else:\n            # Don't scale the values but multiply by $\\frac{p}{\\hat{p}} = 1$ so that the gradients flow\n            # (this is something we experimented with).\n            final_output = final_output * (route_prob_max / route_prob_max.detach()).view(-1, 1)\n\n        # Change the shape of the final output back to `[seq_len, batch_size, d_model]`\n        final_output = final_output.view(seq_len, batch_size, d_model)\n\n        # Return\n        #\n        # * the final output\n        # * number of tokens routed to each expert\n        # * sum of probabilities for each expert\n        # * number of tokens dropped.\n        # * routing probabilities of the selected experts\n        #\n        # These are used for the load balancing loss and logging\n        return final_output, counts, route_prob.sum(0), len(dropped), route_prob_max\n\n\nclass SwitchTransformerLayer(Module):\n    \"\"\"\n    # Switch Transformer Block\n\n    This is the same as [normal transformer block](../models.html#TransformerLayer)\n    with handling extra outputs of switch feedforward module.\n    \"\"\"\n\n    def __init__(self, *,\n                 d_model: int,\n                 attn: MultiHeadAttention,\n                 feed_forward: SwitchFeedForward,\n                 dropout_prob: float):\n        \"\"\"\n        * `d_model` is the token embedding size\n        * `attn` is the attention module\n        * `feed_forward` is the feed forward module (which is the switching module in this case)\n        * `dropout_prob` is the probability of dropping out after self attention and FFN\n        \"\"\"\n        super().__init__()\n        self.size = d_model\n        self.attn = attn\n        self.feed_forward = feed_forward\n        self.dropout = nn.Dropout(dropout_prob)\n        self.norm_self_attn = nn.LayerNorm([d_model])\n        self.norm_ff = nn.LayerNorm([d_model])\n\n    def forward(self, *,\n                x: torch.Tensor,\n                mask: torch.Tensor):\n        # Normalize the vectors before doing self attention\n        z = self.norm_self_attn(x)\n        # Run through self attention, i.e. keys and values are from self\n        self_attn = self.attn(query=z, key=z, value=z, mask=mask)\n        # Add the self attention results\n        x = x + self.dropout(self_attn)\n\n        # Normalize for feed-forward\n        z = self.norm_ff(x)\n        # Pass through the switching feed-forward network\n        ff, counts, route_prob, n_dropped, route_prob_max = self.feed_forward(z)\n        # Add the feed-forward results back\n        x = x + self.dropout(ff)\n\n        return x, counts, route_prob, n_dropped, route_prob_max\n\n\nclass SwitchTransformer(Module):\n    \"\"\"\n    ## Switch Transformer\n    \"\"\"\n\n    def __init__(self, layer: SwitchTransformerLayer, n_layers: int):\n        super().__init__()\n        # Make copies of the transformer layer\n        self.layers = clone_module_list(layer, n_layers)\n        # Final normalization layer\n        self.norm = nn.LayerNorm([layer.size])\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n        # Run through each transformer layer\n        counts, route_prob, n_dropped, route_prob_max = [], [], [], []\n        for layer in self.layers:\n            x, f, p, n_d, p_max = layer(x=x, mask=mask)\n            counts.append(f)\n            route_prob.append(p)\n            n_dropped.append(n_d)\n            route_prob_max.append(p_max)\n        # Finally, normalize the vectors\n        x = self.norm(x)\n        #\n        return x, torch.stack(counts), torch.stack(route_prob), n_dropped, torch.stack(route_prob_max)\n", "labml_nn/transformers/mlm/experiment.py": "\"\"\"\n---\ntitle: Masked Language Model Experiment\nsummary: This experiment trains Masked Language Model (MLM) on Tiny Shakespeare dataset.\n---\n\n# [Masked Language Model (MLM)](index.html) Experiment\n\nThis is an annotated PyTorch experiment to train a [Masked Language Model](index.html).\n\"\"\"\nfrom typing import List\n\nimport torch\nfrom torch import nn\n\nfrom labml import experiment, tracker, logger\nfrom labml.configs import option\nfrom labml.logger import Text\nfrom labml_helpers.metrics.accuracy import Accuracy\nfrom labml_helpers.module import Module\nfrom labml_helpers.train_valid import BatchIndex\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers import Encoder, Generator\nfrom labml_nn.transformers import TransformerConfigs\nfrom labml_nn.transformers.mlm import MLM\n\n\nclass TransformerMLM(nn.Module):\n    \"\"\"\n    # Transformer based model for MLM\n    \"\"\"\n\n    def __init__(self, *, encoder: Encoder, src_embed: Module, generator: Generator):\n        \"\"\"\n        * `encoder` is the transformer [Encoder](../models.html#Encoder)\n        * `src_embed` is the token\n        [embedding module (with positional encodings)](../models.html#EmbeddingsWithLearnedPositionalEncoding)\n        * `generator` is the [final fully connected layer](../models.html#Generator) that gives the logits.\n        \"\"\"\n        super().__init__()\n        self.generator = generator\n        self.src_embed = src_embed\n        self.encoder = encoder\n\n    def forward(self, x: torch.Tensor):\n        # Get the token embeddings with positional encodings\n        x = self.src_embed(x)\n        # Transformer encoder\n        x = self.encoder(x, None)\n        # Logits for the output\n        y = self.generator(x)\n\n        # Return results\n        # (second value is for state, since our trainer is used with RNNs also)\n        return y, None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`NLPAutoRegressionConfigs`](../../experiments/nlp_autoregression.html)\n    because it has the data pipeline implementations that we reuse here.\n    We have implemented a custom training step form MLM.\n    \"\"\"\n\n    # MLM model\n    model: TransformerMLM\n    # Transformer\n    transformer: TransformerConfigs\n\n    # Number of tokens\n    n_tokens: int = 'n_tokens_mlm'\n    # Tokens that shouldn't be masked\n    no_mask_tokens: List[int] = []\n    # Probability of masking a token\n    masking_prob: float = 0.15\n    # Probability of replacing the mask with a random token\n    randomize_prob: float = 0.1\n    # Probability of replacing the mask with original token\n    no_change_prob: float = 0.1\n    # [Masked Language Model (MLM) class](index.html) to generate the mask\n    mlm: MLM\n\n    # `[MASK]` token\n    mask_token: int\n    # `[PADDING]` token\n    padding_token: int\n\n    # Prompt to sample\n    prompt: str = [\n        \"We are accounted poor citizens, the patricians good.\",\n        \"What authority surfeits on would relieve us: if they\",\n        \"would yield us but the superfluity, while it were\",\n        \"wholesome, we might guess they relieved us humanely;\",\n        \"but they think we are too dear: the leanness that\",\n        \"afflicts us, the object of our misery, is as an\",\n        \"inventory to particularise their abundance; our\",\n        \"sufferance is a gain to them Let us revenge this with\",\n        \"our pikes, ere we become rakes: for the gods know I\",\n        \"speak this in hunger for bread, not in thirst for revenge.\",\n    ]\n\n    def init(self):\n        \"\"\"\n        ### Initialization\n        \"\"\"\n\n        # `[MASK]` token\n        self.mask_token = self.n_tokens - 1\n        # `[PAD]` token\n        self.padding_token = self.n_tokens - 2\n\n        # [Masked Language Model (MLM) class](index.html) to generate the mask\n        self.mlm = MLM(padding_token=self.padding_token,\n                       mask_token=self.mask_token,\n                       no_mask_tokens=self.no_mask_tokens,\n                       n_tokens=self.n_tokens,\n                       masking_prob=self.masking_prob,\n                       randomize_prob=self.randomize_prob,\n                       no_change_prob=self.no_change_prob)\n\n        # Accuracy metric (ignore the labels equal to `[PAD]`)\n        self.accuracy = Accuracy(ignore_index=self.padding_token)\n        # Cross entropy loss (ignore the labels equal to `[PAD]`)\n        self.loss_func = nn.CrossEntropyLoss(ignore_index=self.padding_token)\n        #\n        super().init()\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training or validation step\n        \"\"\"\n\n        # Move the input to the device\n        data = batch[0].to(self.device)\n\n        # Update global step (number of tokens processed) when in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(data.shape[0] * data.shape[1])\n\n        # Get the masked input and labels\n        with torch.no_grad():\n            data, labels = self.mlm(data)\n\n        # Whether to capture model outputs\n        with self.mode.update(is_log_activations=batch_idx.is_last):\n            # Get model outputs.\n            # It's returning a tuple for states when using RNNs.\n            # This is not implemented yet.\n            output, *_ = self.model(data)\n\n        # Calculate and log the loss\n        loss = self.loss_func(output.view(-1, output.shape[-1]), labels.view(-1))\n        tracker.add(\"loss.\", loss)\n\n        # Calculate and log accuracy\n        self.accuracy(output, labels)\n        self.accuracy.track()\n\n        # Train the model\n        if self.mode.is_train:\n            # Calculate gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Take optimizer step\n            self.optimizer.step()\n            # Log the model parameters and gradients on last batch of every epoch\n            if batch_idx.is_last:\n                tracker.add('model', self.model)\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save the tracked metrics\n        tracker.save()\n\n    @torch.no_grad()\n    def sample(self):\n        \"\"\"\n        ### Sampling function to generate samples periodically while training\n        \"\"\"\n\n        # Empty tensor for data filled with `[PAD]`.\n        data = torch.full((self.seq_len, len(self.prompt)), self.padding_token, dtype=torch.long)\n        # Add the prompts one by one\n        for i, p in enumerate(self.prompt):\n            # Get token indexes\n            d = self.text.text_to_i(p)\n            # Add to the tensor\n            s = min(self.seq_len, len(d))\n            data[:s, i] = d[:s]\n        # Move the tensor to current device\n        data = data.to(self.device)\n\n        # Get masked input and labels\n        data, labels = self.mlm(data)\n        # Get model outputs\n        output, *_ = self.model(data)\n\n        # Print the samples generated\n        for j in range(data.shape[1]):\n            # Collect output from printing\n            log = []\n            # For each token\n            for i in range(len(data)):\n                # If the label is not `[PAD]`\n                if labels[i, j] != self.padding_token:\n                    # Get the prediction\n                    t = output[i, j].argmax().item()\n                    # If it's a printable character\n                    if t < len(self.text.itos):\n                        # Correct prediction\n                        if t == labels[i, j]:\n                            log.append((self.text.itos[t], Text.value))\n                        # Incorrect prediction\n                        else:\n                            log.append((self.text.itos[t], Text.danger))\n                    # If it's not a printable character\n                    else:\n                        log.append(('*', Text.danger))\n                # If the label is `[PAD]` (unmasked) print the original.\n                elif data[i, j] < len(self.text.itos):\n                    log.append((self.text.itos[data[i, j]], Text.subtle))\n\n            # Print\n            logger.log(log)\n\n\n@option(Configs.n_tokens)\ndef n_tokens_mlm(c: Configs):\n    \"\"\"\n    Number of tokens including `[PAD]` and `[MASK]`\n    \"\"\"\n    return c.text.n_tokens + 2\n\n\n@option(Configs.transformer)\ndef _transformer_configs(c: Configs):\n    \"\"\"\n    ### Transformer configurations\n    \"\"\"\n\n    # We use our\n    # [configurable transformer implementation](../configs.html#TransformerConfigs)\n    conf = TransformerConfigs()\n    # Set the vocabulary sizes for embeddings and generating logits\n    conf.n_src_vocab = c.n_tokens\n    conf.n_tgt_vocab = c.n_tokens\n    # Embedding size\n    conf.d_model = c.d_model\n\n    #\n    return conf\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    Create classification model\n    \"\"\"\n    m = TransformerMLM(encoder=c.transformer.encoder,\n                       src_embed=c.transformer.src_embed,\n                       generator=c.transformer.generator).to(c.device)\n\n    return m\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"mlm\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Batch size\n        'batch_size': 64,\n        # Sequence length of $32$. We use a short sequence length to train faster.\n        # Otherwise it takes forever to train.\n        'seq_len': 32,\n\n        # Train for 1024 epochs.\n        'epochs': 1024,\n        # Switch between training and validation for $1$ times\n        # per epoch\n        'inner_iterations': 1,\n\n        # Transformer configurations (same as defaults)\n        'd_model': 128,\n        'transformer.ffn.d_ff': 256,\n        'transformer.n_heads': 8,\n        'transformer.n_layers': 6,\n\n        # Use [Noam optimizer](../../optimizers/noam.html)\n        'optimizer.optimizer': 'Noam',\n        'optimizer.learning_rate': 1.,\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/mlm/__init__.py": "\"\"\"\n---\ntitle: Masked Language Model\nsummary: >\n  This is an annotated implementation/tutorial of the Masked Language Model in PyTorch.\n---\n\n# Masked Language Model (MLM)\n\nThis is a [PyTorch](https://pytorch.org) implementation of the Masked Language Model (MLM)\n used to pre-train the BERT model introduced in the paper\n[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).\n\n## BERT Pretraining\n\nBERT model is a transformer model.\nThe paper pre-trains the model using MLM and with next sentence prediction.\nWe have only implemented MLM here.\n\n### Next sentence prediction\n\nIn *next sentence prediction*, the model is given two sentences `A` and `B` and the model\nmakes a binary prediction whether `B` is the sentence that follows `A` in the actual text.\nThe model is fed with actual sentence pairs 50% of the time and random pairs 50% of the time.\nThis classification is done while applying MLM. *We haven't implemented this here.*\n\n## Masked LM\n\nThis masks a percentage of tokens at random and trains the model to predict\nthe masked tokens.\nThey **mask 15% of the tokens** by replacing them with a special `[MASK]` token.\n\nThe loss is computed on predicting the masked tokens only.\nThis causes a problem during fine-tuning and actual usage since there are no `[MASK]` tokens\n at that time.\nTherefore we might not get any meaningful representations.\n\nTo overcome this **10% of the masked tokens are replaced with the original token**,\nand another **10% of the masked tokens are replaced with a random token**.\nThis trains the model to give representations about the actual token whether or not the\ninput token at that position is a `[MASK]`.\nAnd replacing with a random token causes it to\ngive a representation that has information from the context as well;\nbecause it has to use the context to fix randomly replaced tokens.\n\n## Training\n\nMLMs are harder to train than autoregressive models because they have a smaller training signal.\ni.e. only a small percentage of predictions are trained per sample.\n\nAnother problem is since the model is bidirectional, any token can see any other token.\nThis makes the \"credit assignment\" harder.\nLet's say you have the character level model trying to predict `home *s where i want to be`.\nAt least during the early stages of the training, it'll be super hard to figure out why the\nreplacement for `*` should be `i`, it could be anything from the whole sentence.\nWhilst, in an autoregressive setting the model will only have to use `h` to predict `o` and\n`hom` to predict `e` and so on. So the model will initially start predicting with a shorter context first\nand then learn to use longer contexts later.\nSince MLMs have this problem it's a lot faster to train if you start with a smaller sequence length\ninitially and then use a longer sequence length later.\n\nHere is [the training code](experiment.html) for a simple MLM model.\n\"\"\"\n\nfrom typing import List\n\nimport torch\n\n\nclass MLM:\n    \"\"\"\n    ## Masked LM (MLM)\n\n    This class implements the masking procedure for a given batch of token sequences.\n    \"\"\"\n\n    def __init__(self, *,\n                 padding_token: int, mask_token: int, no_mask_tokens: List[int], n_tokens: int,\n                 masking_prob: float = 0.15, randomize_prob: float = 0.1, no_change_prob: float = 0.1,\n                 ):\n        \"\"\"\n        * `padding_token` is the padding token `[PAD]`.\n          We will use this to mark the labels that shouldn't be used for loss calculation.\n        * `mask_token` is the masking token `[MASK]`.\n        * `no_mask_tokens` is a list of tokens that should not be masked.\n        This is useful if we are training the MLM with another task like classification at the same time,\n        and we have tokens such as `[CLS]` that shouldn't be masked.\n        * `n_tokens` total number of tokens (used for generating random tokens)\n        * `masking_prob` is the masking probability\n        * `randomize_prob` is the probability of replacing with a random token\n        * `no_change_prob` is the probability of replacing with original token\n        \"\"\"\n        self.n_tokens = n_tokens\n        self.no_change_prob = no_change_prob\n        self.randomize_prob = randomize_prob\n        self.masking_prob = masking_prob\n        self.no_mask_tokens = no_mask_tokens + [padding_token, mask_token]\n        self.padding_token = padding_token\n        self.mask_token = mask_token\n\n    def __call__(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the batch of input token sequences.\n         It's a tensor of type `long` with shape `[seq_len, batch_size]`.\n        \"\"\"\n\n        # Mask `masking_prob` of tokens\n        full_mask = torch.rand(x.shape, device=x.device) < self.masking_prob\n        # Unmask `no_mask_tokens`\n        for t in self.no_mask_tokens:\n            full_mask &= x != t\n\n        # A mask for tokens to be replaced with original tokens\n        unchanged = full_mask & (torch.rand(x.shape, device=x.device) < self.no_change_prob)\n        # A mask for tokens to be replaced with a random token\n        random_token_mask = full_mask & (torch.rand(x.shape, device=x.device) < self.randomize_prob)\n        # Indexes of tokens to be replaced with random tokens\n        random_token_idx = torch.nonzero(random_token_mask, as_tuple=True)\n        # Random tokens for each of the locations\n        random_tokens = torch.randint(0, self.n_tokens, (len(random_token_idx[0]),), device=x.device)\n        # The final set of tokens that are going to be replaced by `[MASK]`\n        mask = full_mask & ~random_token_mask & ~unchanged\n\n        # Make a clone of the input for the labels\n        y = x.clone()\n\n        # Replace with `[MASK]` tokens;\n        # note that this doesn't include the tokens that will have the original token unchanged and\n        # those that get replace with a random token.\n        x.masked_fill_(mask, self.mask_token)\n        # Assign random tokens\n        x[random_token_idx] = random_tokens\n\n        # Assign token `[PAD]` to all the other locations in the labels.\n        # The labels equal to `[PAD]` will not be used in the loss.\n        y.masked_fill_(~full_mask, self.padding_token)\n\n        # Return the masked input and the labels\n        return x, y\n", "labml_nn/transformers/glu_variants/experiment.py": "\"\"\"\n---\ntitle: Gated Linear Units and Variants\nsummary: >\n  Train an auto-regressive transformer with Gated Linear Units and variants\n  for the position-wise feedforward network (FFN).\n---\n\n# Gated Linear Units and Variants\n\nThis trains a simple [transformer](../../) model for auto-regression.\nWe try different variants for the [position-wise feedforward network](../feed_forward).\nThe reusable & configurable are defined in [`configs.py`](configs.html).\n\"\"\"\n\nimport torch\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml.utils.pytorch import get_modules\nfrom labml_helpers.module import Module\n\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers import Encoder, Generator, TransformerConfigs\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass AutoregressiveModel(Module):\n    \"\"\"\n    ## Auto regressive model\n    \"\"\"\n\n    def __init__(self, src_embed: Module, encoder: Encoder, generator: Generator):\n        super().__init__()\n        # Token embedding module\n        self.src_embed = src_embed\n        # Transformer based encoder\n        self.encoder = encoder\n        # Next token generation layer;\n        # this give logits  of the the next token\n        self.generator = generator\n        # This will be initialized on the first call\n        self.src_mask = None\n\n    def forward(self, src: torch.Tensor):\n        # Create subsequent mask, so that the transformer can only pay attention to past tokens.\n        if self.src_mask is None or self.src_mask.size(0) != len(src):\n            self.src_mask = subsequent_mask(len(src)).to(src.device)\n        # Embed the tokens (`src`) and run it through the the transformer\n        res = self.encoder(self.src_embed(src), self.src_mask)\n        # Generate logits of the next token\n        return self.generator(res), None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    The default configs can and will be over-ridden when we start the experiment\n    \"\"\"\n\n    transformer: TransformerConfigs\n    model: AutoregressiveModel\n\n\n@option(Configs.model)\ndef autoregressive_model(c: Configs):\n    \"\"\"\n    Initialize the auto-regressive model\n    \"\"\"\n    m = AutoregressiveModel(c.transformer.src_embed, c.transformer.encoder, c.transformer.generator)\n    return m.to(c.device)\n\n\n@option(Configs.transformer)\ndef transformer_c(c: Configs):\n    \"\"\"\n    Initialize the [configurable transformer](../configs.html) encoder for our autoregressive model.\n    \"\"\"\n    tc = TransformerConfigs()\n    tc.n_src_vocab = c.n_tokens\n    tc.n_tgt_vocab = c.n_tokens\n\n    return tc\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"glu_variants\")\n    # Create configs\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf,\n                       # A dictionary of configurations to override\n                       {'tokenizer': 'character',\n                        'prompt_separator': '',\n                        'prompt': 'It is ',\n                        'text': 'tiny_shakespeare',\n\n                        'optimizer.optimizer': 'Noam',\n                        'optimizer.learning_rate': 1.,\n                        'optimizer.d_model': 256,\n\n                        'seq_len': 1024,\n                        'epochs': 128,\n                        'batch_size': 6,\n                        'inner_iterations': 10,\n\n                        # GLU Variant, one of GLU, Bilinear, ReGLU, GEGLU, SwiGLU\n                        #\n                        # These are defined in the [configurable FFN](../configs.html#FFN)\n                        # implementation\n                        'transformer.ffn.glu_variant': 'Bilinear',\n\n                        # Transformer configurations\n                        'transformer.d_model': 256,\n                        'transformer.ffn.d_ff': 1024,\n                        'transformer.n_heads': 8,\n                        'transformer.n_layers': 6})\n\n    # This is needed to initialize models\n    conf.n_tokens = conf.text.n_tokens\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models(get_modules(conf))\n\n    # Start the experiment\n    with experiment.start():\n        # `TrainValidConfigs.run`\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/glu_variants/simple.py": "\"\"\"\n---\ntitle: Gated Linear Units and Variants\nsummary: >\n  Train an auto-regressive transformer with Gated Linear Units and variants\n  for the position-wise feedforward network (FFN).\n---\n\n# Gated Linear Units and Variants\n\nThis trains a simple [transformer](../../) model for auto-regression.\nWe try different variants for the [position-wise feedforward network](../feed_forward).\n\n*This is a simpler implementation that doesn't use [`labml.configs`](experiment.html) module.\nWe decided to write a simpler implementation to make it easier for readers who are not familiar.*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/glu_variants/simple.ipynb)\n\"\"\"\nimport dataclasses\n\nimport torch\nfrom labml_helpers.module import Module\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom labml import experiment, lab, tracker, monit, logger\nfrom labml.logger import Text\nfrom labml.utils.download import download_file\nfrom labml_nn.experiments.nlp_autoregression import transpose_batch\nfrom labml_nn.optimizers.noam import Noam\nfrom labml_nn.transformers import Encoder, MultiHeadAttention\nfrom labml_nn.transformers.feed_forward import FeedForward\nfrom labml_nn.transformers.models import EmbeddingsWithPositionalEncoding, TransformerLayer\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass AutoregressiveModel(Module):\n    \"\"\"\n    ## Auto regressive model\n    \"\"\"\n\n    def __init__(self, src_embed: Module, encoder: Encoder, generator: Module):\n        super().__init__()\n        # Token embedding module\n        self.src_embed = src_embed\n        # Transformer based encoder\n        self.encoder = encoder\n        # Next token generation layer;\n        # this gives logits of the the next token\n        self.generator = generator\n        # This will be initialized on the first call\n        self.src_mask = None\n\n    def forward(self, src: torch.Tensor):\n        # Create subsequent mask, so that the transformer can only pay attention to past tokens.\n        if self.src_mask is None or self.src_mask.size(0) != len(src):\n            self.src_mask = subsequent_mask(len(src)).to(src.device)\n        # Embed the tokens (`src`) and run it through the the transformer\n        res = self.encoder(self.src_embed(src), self.src_mask)\n        # Generate logits of the next token\n        return self.generator(res)\n\n\n@dataclasses.dataclass\nclass Configs:\n    \"\"\"\n    ### Configurations\n    \"\"\"\n    d_model: int = 512\n    seq_len: int = 128\n    batch_size: int = 32\n    n_layers: int = 6\n    n_heads: int = 8\n    dropout: float = 0.1\n    d_ff: int = 2048\n    glu_variant: str = 'GLU'\n    epochs: int = 5\n    grad_norm_clip: float = 0.5\n\n\nclass TinyShakespeareDataset(Dataset):\n    \"\"\"\n    ### Tiny Shakespeare Dataset\n    \"\"\"\n\n    def __init__(self, seq_len: int):\n        # Location of the text file\n        path = lab.get_data_path() / 'tiny_shakespeare.txt'\n        # Download the file\n        download_file('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt', path)\n        # Read the downloaded file\n        with open(str(path), 'r') as f:\n            text = f.read()\n\n        # Extract the characters\n        chars = list(set(text))\n        # Character to id (integer) map\n        self.stoi = {c: i for i, c in enumerate(chars)}\n        # Id to character map\n        self.itos = {i: c for i, c in enumerate(chars)}\n        # Length of a training sample\n        self.seq_len = seq_len\n        # Data in the form of a tensor of ids\n        self.data = self.text_to_i(text)\n\n    def text_to_i(self, text: str):\n        \"\"\"\n        Transform the text into a tensor of ids\n        \"\"\"\n        return torch.tensor([self.stoi[c] for c in text], dtype=torch.long)\n\n    def __len__(self):\n        \"\"\"\n        Number of samples in the dataset.\n\n        *This will read the dataset `seq_len` times in a single epoch.*\n        \"\"\"\n        return len(self.data) - self.seq_len - 1\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Return a sample\n        \"\"\"\n        return self.data[idx:idx + self.seq_len], self.data[idx + 1:idx + self.seq_len + 1]\n\n\nclass Trainer:\n    \"\"\"\n    ## Trainer\n    \"\"\"\n\n    def __init__(self, configs: Configs):\n        # Get the device\n        self.device = torch.device('cpu')\n        if torch.cuda.is_available():\n            self.device = torch.device('cuda:0')\n        # Initialize the dataset\n        self.dataset = TinyShakespeareDataset(configs.seq_len)\n        # Initialize the dataloader\n        self.dataloader = DataLoader(self.dataset,\n                                     batch_size=configs.batch_size,\n                                     collate_fn=transpose_batch,\n                                     shuffle=True)\n\n        # FFN with Gated Linear Unit\n        # $$FFN_{GLU}(x)(x, W_1, V, W_2) = (\\sigma(x W_1) \\otimes x V) W_2$$\n        if configs.glu_variant == 'GLU':\n            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.Sigmoid(), True, False, False, False)\n        # FFN with Bilinear hidden layer\n        # $$FFN_{Bilinear}(x)(x, W_1, V, W_2) = (x W_1 \\otimes x V) W_2$$\n        elif configs.glu_variant == 'Bilinear':\n            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.Identity(), True, False, False, False)\n        # FFN with ReLU gate\n        # $$FFN_{ReGLU}(x)(x, W_1, V, W_2) = (\\max(0, x W_1) \\otimes x V) W_2$$\n        elif configs.glu_variant == 'ReGLU':\n            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.ReLU(), True, False, False, False)\n        # FFN with GELU gate\n        # $$FFN_{GEGLU}(x)(x, W_1, V, W_2) = (\\text{GELU}(x W_1) \\otimes x V) W_2$$\n        elif configs.glu_variant == 'GEGLU':\n            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.GELU(), True, False, False, False)\n        # FFN with Swish gate\n        # $$FFN_{SwiGLU}(x)(x, W_1, V, W_2) = (\\text{Swish}_1(x W_1) \\otimes x V) W_2$$\n        # where $\\text{Swish}_\\beta(x) = x \\sigma(\\beta x)$\n        elif configs.glu_variant == 'SwiGLU':\n            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.SiLU(), True, False, False, False)\n        # FFN with ReLU activation\n        # $$FFN_{ReLU}(x)(x, W_1, W_2, b_1, b_2) = \\text{ReLU}_1(x W_1 + b_1) W_2 + b_2$$\n        elif configs.glu_variant == 'ReLU':\n            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.ReLU())\n        # FFN with ReLU activation\n        # $$FFN_{GELU}(x)(x, W_1, W_2, b_1, b_2) = \\text{GELU}_1(x W_1 + b_1) W_2 + b_2$$\n        elif configs.glu_variant == 'GELU':\n            ffn = FeedForward(configs.d_model, configs.d_ff, configs.dropout, nn.GELU())\n        else:\n            raise ValueError(f'Unknown variant {configs.glu_variant}')\n\n        # Number of different characters\n        n_chars = len(self.dataset.stoi)\n\n        # Initialize [Multi-Head Attention module](../mha.html)\n        mha = MultiHeadAttention(configs.n_heads, configs.d_model, configs.dropout)\n        # Initialize the [Transformer Block](../models.html#TransformerLayer)\n        transformer_layer = TransformerLayer(d_model=configs.d_model, self_attn=mha, src_attn=None,\n                                             feed_forward=ffn, dropout_prob=configs.dropout)\n        # Initialize the model with an\n        # [embedding layer](../models.html#EmbeddingsWithPositionalEncoding)\n        # (with fixed positional encoding)\n        # [transformer encoder](../models.html#Encoder) and\n        # a linear layer to generate logits.\n        self.model = AutoregressiveModel(EmbeddingsWithPositionalEncoding(configs.d_model, n_chars),\n                                         Encoder(transformer_layer, configs.n_layers),\n                                         nn.Linear(configs.d_model, n_chars))\n\n        # Move the model to the current device\n        self.model.to(self.device)\n\n        # Initialize [Noam optimizer](../../optimizers/noam.html)\n        self.optimizer = Noam(self.model.parameters(), lr=1.0, warmup=2_000, d_model=configs.d_model)\n\n        # Cross-entropy loss\n        self.loss_func = nn.CrossEntropyLoss()\n        # Number of training epochs;\n        # *note that our dataset definition repeats the data `seq_len` times in a single epoch*\n        self.epochs = configs.epochs\n        # Gradient clipping norm\n        self.grad_norm_clip = configs.grad_norm_clip\n\n        # Set tracker configurations\n        tracker.set_scalar(\"loss.*\", True)\n\n    def sample(self):\n        \"\"\"\n        ### Sampling function to generate samples periodically while training\n        \"\"\"\n\n        # Starting prompt\n        prompt = 'It is'\n        # Collect output for printing\n        log = [(prompt, Text.subtle)]\n        # Sample 25 tokens\n        for i in monit.iterate('Sample', 25):\n            # Tokenize the prompt\n            data = self.dataset.text_to_i(prompt).unsqueeze(-1)\n            data = data.to(self.device)\n            # Get the model output\n            output = self.model(data)\n            # Get the model prediction (greedy)\n            output = output.argmax(dim=-1).squeeze()\n            # Add the prediction to prompt\n            prompt += self.dataset.itos[output[-1].item()]\n            # Add the prediction for logging\n            log += [(self.dataset.itos[output[-1].item()], Text.value)]\n\n        # Print the sampled output\n        logger.log(log)\n\n    def train(self):\n        \"\"\"\n        ### Train the model\n        \"\"\"\n\n        # Loop for the given number of epochs\n        for _ in monit.loop(self.epochs):\n            # Iterate over the minibatches\n            for i, batch in monit.enum('Train', self.dataloader):\n                # Move data to the device\n                data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n                # Set tracker step, as the number of characters trained on\n                tracker.add_global_step(data.shape[0] * data.shape[1])\n\n                # Set model state to training\n                self.model.train()\n                # Evaluate the model\n                output = self.model(data)\n\n                # Calculate loss\n                loss = self.loss_func(output.view(-1, output.shape[-1]), target.view(-1))\n                # Log the loss\n                tracker.add(\"loss.train\", loss)\n\n                # Calculate gradients\n                loss.backward()\n                # Clip gradients\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n                # Take optimizer step\n                self.optimizer.step()\n                # Log the model parameters and gradients\n                if (i + 1) % 100 == 0:\n                    tracker.add('model', self.model)\n                # Clear the gradients\n                self.optimizer.zero_grad()\n\n                # Generate a sample\n                if (i + 1) % 100 == 0:\n                    self.model.eval()\n                    with torch.no_grad():\n                        self.sample()\n\n                # Save the tracked metrics\n                if (i + 1) % 10 == 0:\n                    tracker.save()\n\n            # Save the model\n            experiment.save_checkpoint()\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"glu_variants\")\n    # Create configs\n    configs = Configs()\n    # Load configurations\n    experiment.configs(dataclasses.asdict(configs))\n\n    # Create trainer\n    trainer = Trainer(configs)\n    # Set models for training and loading\n    experiment.add_pytorch_models({'model': trainer.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Train the model\n        trainer.train()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/glu_variants/__init__.py": "\"\"\"\n---\ntitle: Gated Linear Units and Variants\nsummary: >\n  Train an auto-regressive transformer with Gated Linear Units and variants\n  for the position-wise feedforward network (FFN).\n---\n\n# Gated Linear Units and Variants\n\n* [Experiment that uses `labml.configs`](experiment.html)\n* [Simpler version from scratch](simple.html)\n\"\"\"\n", "labml_nn/transformers/gmlp/experiment.py": "\"\"\"\n---\ntitle:  Pay Attention to MLPs (gMLP) Experiment\nsummary: This experiment trains a gMLP based model on Tiny Shakespeare dataset.\n---\n\n# [Pay Attention to MLPs (gMLP)](index.html) Experiment\n\nThis is an annotated PyTorch experiment to train a [gMLP model](index.html).\nThe paper also applies a Stochastic Depth regularization where some layers are removed randomly during training.\nWe have not implemented that here.\n\nThis is based on\n[training loop and configurations for a simple transformer auto-regressive NLP task](../basic/autoregressive_experiment.html).\n\"\"\"\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.transformers import TransformerConfigs\nfrom labml_nn.transformers.basic.autoregressive_experiment import Configs as BasicAutoRegressionConfigs\nfrom labml_nn.transformers.gmlp import GMLPBlock\n\n\nclass Configs(BasicAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [training loop and configurations for a simple transformer auto-regressive NLP task](../basic/autoregressive_transformer.html).\n    \"\"\"\n\n    # Transformer\n    transformer: TransformerConfigs = 'gMLP'\n    # gMLP Block\n    gmlp: GMLPBlock\n    # `d_ffn` for gMLP projection layer\n    d_ffn: int = 2048\n\n\n@option(Configs.gmlp, 'gMLP')\ndef _gmlp_configs(c: Configs):\n    \"\"\"\n    ### Create a gMLP block\n    \"\"\"\n    return GMLPBlock(c.d_model, c.d_ffn, c.seq_len)\n\n\n@option(Configs.transformer, 'gMLP')\ndef _transformer_configs(c: Configs):\n    \"\"\"\n    ### Transformer configurations\n    \"\"\"\n\n    # We use our\n    # [configurable transformer implementation](../configs.html#TransformerConfigs)\n    conf = TransformerConfigs()\n    # Set the vocabulary sizes for embeddings and generating logits\n    conf.n_src_vocab = c.n_tokens\n    conf.n_tgt_vocab = c.n_tokens\n    # Set model size\n    conf.d_model = c.d_model\n    # Replace the encoder layer with a gMLP layer\n    conf.encoder_layer = c.gmlp\n\n    return conf\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"gMLP\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 256,\n        # Train for $128$ epochs\n        'epochs': 128,\n        # Batch size $32$\n        'batch_size': 32,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        # Model size\n        'd_model': 512,\n        'd_ffn': 2048,\n\n        # Use [Noam optimizer](../../optimizers/noam.html)\n        'optimizer.optimizer': 'Noam',\n        'optimizer.learning_rate': 1.,\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/gmlp/__init__.py": "\"\"\"\n---\ntitle: Pay Attention to MLPs (gMLP)\nsummary: >\n  This is an annotated implementation/tutorial of Pay Attention to MLPs (gMLP) in PyTorch.\n---\n\n# Pay Attention to MLPs (gMLP)\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[Pay Attention to MLPs](https://arxiv.org/abs/2105.08050).\n\nThis paper introduces a Multilayer Perceptron (MLP) based architecture with gating,\nwhich they name **gMLP**. It consists of a stack of $L$ *gMLP* blocks.\n\nHere is [the training code](experiment.html) for a gMLP model based autoregressive model.\n\"\"\"\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\n\nclass GMLPBlock(nn.Module):\n    \"\"\"\n    ## gMLP Block\n\n    Each block does the following transformations to input embeddings\n    $X \\in \\mathbb{R}^{n \\times d}$ where $n$ is the sequence length\n    and $d$ is the dimensionality of the embeddings:\n\n    \\begin{align}\n    Z &= \\sigma(XU) \\\\\n    \\tilde{Z} &= s(Z) \\\\\n    Y &= \\tilde{Z}V \\\\\n    \\end{align}\n\n    where $V$ and $U$ are learnable projection weights.\n    $s(\\cdot)$ is the Spacial Gating Unit defined below.\n    Output dimensionality of $s(\\cdot)$ will be half of $Z$.\n    $\\sigma$ is an activation function such as\n    [GeLU](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html).\n    \"\"\"\n\n    def __init__(self, d_model: int, d_ffn: int, seq_len: int):\n        \"\"\"\n        * `d_model` is the dimensionality ($d$) of $X$\n        * `d_ffn` is the dimensionality of $Z$\n        * `seq_len` is the length of the token sequence ($n$)\n        \"\"\"\n        super().__init__()\n        # Normalization layer fro Pre-Norm\n        self.norm = nn.LayerNorm([d_model])\n        # Activation function $\\sigma$\n        self.activation = nn.GELU()\n        # Projection layer for $Z = \\sigma(XU)$\n        self.proj1 = nn.Linear(d_model, d_ffn)\n        # Spacial Gating Unit $s(\\cdot)$\n        self.sgu = SpacialGatingUnit(d_ffn, seq_len)\n        # Projection layer for $Y = \\tilde{Z}V$\n        self.proj2 = nn.Linear(d_ffn // 2, d_model)\n        # Embedding size (required by [Encoder](../models.html#Encoder).\n        # We use the encoder module from transformer architecture and plug\n        # *gMLP* block as a replacement for the [Transformer Layer](../models.html#Encoder).\n        self.size = d_model\n\n    def forward(self, *, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        * `x` is the input embedding tensor $X$ of shape `[seq_len, batch_size, d_model]`\n        * `mask` is a boolean mask of shape `[seq_len, seq_len, 1]` that controls the visibility of tokens\n         among each other.\n        \"\"\"\n        # Keep a copy for shortcut connection\n        shortcut = x\n        # Normalize $X$\n        x = self.norm(x)\n        # Projection and activation $Z = \\sigma(XU)$\n        z = self.activation(self.proj1(x))\n        # Spacial Gating Unit $\\tilde{Z} = s(Z)$\n        z = self.sgu(z, mask)\n        # Final projection $Y = \\tilde{Z}V$\n        z = self.proj2(z)\n\n        # Add the shortcut connection\n        return z + shortcut\n\n\nclass SpacialGatingUnit(nn.Module):\n    \"\"\"\n    ## Spatial Gating Unit\n\n    $$s(Z) = Z_1 \\odot f_{W,b}(Z_2)$$\n\n    where $f_{W,b}(Z) = W Z + b$ is a linear transformation along the sequence dimension,\n    and $\\odot$ is element-wise multiplication.\n    $Z$ is split into to parts of equal size $Z_1$ and $Z_2$ along the channel dimension (embedding dimension).\n    \"\"\"\n    def __init__(self, d_z: int, seq_len: int):\n        \"\"\"\n        * `d_z` is the dimensionality of $Z$\n        * `seq_len` is the sequence length\n        \"\"\"\n        super().__init__()\n        # Normalization layer before applying $f_{W,b}(\\cdot)$\n        self.norm = nn.LayerNorm([d_z // 2])\n        # Weight $W$ in $f_{W,b}(\\cdot)$.\n        #\n        # The paper notes that it's important to initialize weights to small values and the bias to $1$,\n        # so that during the initial training $s(\\cdot)$ is close to identity (apart from the split).\n        self.weight = nn.Parameter(torch.zeros(seq_len, seq_len).uniform_(-0.01, 0.01), requires_grad=True)\n        # Weight $b$ in $f_{W,b}(\\cdot)$\n        #\n        # The paper notes that it's important to initialize bias to $1$.\n        self.bias = nn.Parameter(torch.ones(seq_len), requires_grad=True)\n\n    def forward(self, z: torch.Tensor, mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        * `z` is the input $Z$ of shape `[seq_len, batch_size, d_z]`\n        * `mask` is is a boolean mask of shape `[seq_len, seq_len, 1]` that controls the visibility of tokens\n         among each other. The last dimension of size `1` is the batch, which we have in other transformer\n         implementations and was left for compatibility.\n        \"\"\"\n\n        # Get sequence length\n        seq_len = z.shape[0]\n        # Split $Z$ into $Z_1$ and $Z_2$\n        z1, z2 = torch.chunk(z, 2, dim=-1)\n\n        # Check mask\n        if mask is not None:\n            # `mask` has shape `[seq_len_q, seq_len_k, batch_size]`.\n            # The batch dimension should be of size `1` because this implementation supports\n            # only same mask for all samples in the batch.\n            assert mask.shape[0] == 1 or mask.shape[0] == seq_len\n            assert mask.shape[1] == seq_len\n            # Here we only support the same mask for all samples\n            assert mask.shape[2] == 1\n            # Remove the batch dimension\n            mask = mask[:, :, 0]\n\n        # Normalize $Z_2$ before $f_{W,b}(\\cdot)$\n        z2 = self.norm(z2)\n        # Get the weight matrix; truncate if larger than `seq_len`\n        weight = self.weight[:seq_len, :seq_len]\n        # Apply mask to the weights.\n        #\n        # If $W_{i,j}$ is $0$ then $f_{W,b}(Z_2)_i$ will not get any information\n        # from token $j$.\n        if mask is not None:\n            weight = weight * mask\n\n        # $f_{W,b}(Z_2) = W Z_2 + b$\n        z2 = torch.einsum('ij,jbd->ibd', weight, z2) + self.bias[:seq_len, None, None]\n\n        # $Z_1 \\odot f_{W,b}(Z_2)$\n        return z1 * z2\n", "labml_nn/transformers/mlp_mixer/experiment.py": "\"\"\"\n---\ntitle: MLP Mixer experiment\nsummary: This experiment trains MLP Mixer on Tiny Shakespeare dataset.\n---\n\n# [MLP Mixer](index.html) Experiment\n\nThis is an annotated PyTorch experiment to train a [MLP Mixer Model](index.html).\n\"\"\"\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.transformers import TransformerConfigs\nfrom labml_nn.transformers.configs import FeedForwardConfigs\nfrom labml_nn.transformers.mlm.experiment import TransformerMLM, Configs as MLMConfigs\n\n\nclass Configs(MLMConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`MLMConfigs`](../mlm/experiment.html) where we define an experiment for\n    [Masked Language Models](../mlm.index.html).\n    \"\"\"\n\n    # Configurable [Feed-Forward Network](../feed_forward.html) for the MLP\n    mix_mlp: FeedForwardConfigs\n\n\n@option(Configs.mix_mlp)\ndef _mix_mlp_configs(c: Configs):\n    \"\"\"\n    The mixing MLP configurations\n    \"\"\"\n\n    conf = FeedForwardConfigs()\n    # Size of the MLP is the sequence length, because it is applied across tokens\n    conf.d_model = c.seq_len\n    # The paper suggests $GELU$ activation\n    conf.activation = 'GELU'\n\n    #\n    return conf\n\n\n@option(Configs.transformer)\ndef _transformer_configs(c: Configs):\n    \"\"\"\n    ### Transformer configurations\n    \"\"\"\n\n    # We use our\n    # [configurable transformer implementation](../configs.html#TransformerConfigs)\n    conf = TransformerConfigs()\n    # Set the vocabulary sizes for embeddings and generating logits\n    conf.n_src_vocab = c.n_tokens\n    conf.n_tgt_vocab = c.n_tokens\n    # Embedding size\n    conf.d_model = c.d_model\n    # Change attention module to [MLPMixer](index.html)\n    from labml_nn.transformers.mlp_mixer import MLPMixer\n    conf.encoder_attn = MLPMixer(c.mix_mlp.ffn)\n\n    #\n    return conf\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"mlp_mixer_mlm\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Batch size\n        'batch_size': 64,\n        # Sequence length of $32$. We use a short sequence length to train faster.\n        # Otherwise MLM models take forever to train.\n        'seq_len': 32,\n\n        # Train for 1024 epochs.\n        'epochs': 1024,\n        # Switch between training and validation for $1$ times\n        # per epoch\n        'inner_iterations': 1,\n\n        # Transformer configurations\n        'd_model': 128,\n        'transformer.ffn.d_ff': 256,\n        'transformer.n_heads': 8,\n        'transformer.n_layers': 6,\n        'transformer.ffn.activation': 'GELU',\n\n        # Mixer MLP hidden layer size\n        'mix_mlp.d_ff': 128,\n\n        # Use [Noam optimizer](../../optimizers/noam.html)\n        'optimizer.optimizer': 'Noam',\n        'optimizer.learning_rate': 1.,\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/mlp_mixer/__init__.py": "\"\"\"\n---\ntitle: \"MLP-Mixer: An all-MLP Architecture for Vision\"\nsummary: >\n  This is an annotated implementation/tutorial of MLP-Mixer: An all-MLP Architecture for Vision in PyTorch.\n---\n\n# MLP-Mixer: An all-MLP Architecture for Vision\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601).\n\nThis paper applies the model on vision tasks.\nThe model is similar to a transformer with attention layer being replaced by a MLP\nthat is applied across the patches (or tokens in case of a NLP task).\n\nOur implementation of MLP Mixer is a drop in replacement for the [self-attention layer](../mha.html)\nin [our transformer implementation](../models.html).\nSo it's just a couple of lines of code, transposing the tensor to apply the MLP\nacross the sequence dimension.\n\nAlthough the paper applied MLP Mixer on vision tasks,\nwe tried it on a [masked language model](../mlm/index.html).\n[Here is the experiment code](experiment.html).\n\"\"\"\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\n\nclass MLPMixer(nn.Module):\n    \"\"\"\n    ## MLP Mixer\n\n    This module is a drop-in replacement for [self-attention layer](../mha.html).\n    It transposes the input tensor before feeding it to the MLP and transposes back,\n    so that the MLP is applied across the sequence dimension (across tokens or image patches) instead\n    of the feature dimension.\n    \"\"\"\n\n    def __init__(self, mlp: nn.Module):\n        \"\"\"\n        * `ffn` is the MLP module.\n        \"\"\"\n        super().__init__()\n        self.mlp = mlp\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        The [normal attention module](../mha.html) can be fed with different token embeddings for\n        $\\text{query}$,$\\text{key}$, and $\\text{value}$ and a mask.\n\n        We follow the same function signature so that we can replace it directly.\n\n        For MLP mixing, $$x = \\text{query} = \\text{key} = \\text{value}$$ and masking is not possible.\n        Shape of `query` (and `key` and `value`) is `[seq_len, batch_size, d_model]`.\n        \"\"\"\n\n        # $\\text{query}$,$\\text{key}$, and $\\text{value}$ all should be the same\n        assert query is key and key is value\n        # MLP mixer doesn't support masking. i.e. all tokens will see all other token embeddings.\n        assert mask is None\n\n        # Assign to `x` for clarity\n        x = query\n\n        # Transpose so that the last dimension is the sequence dimension.\n        # New shape is `[d_model, batch_size, seq_len]`\n        x = x.transpose(0, 2)\n        # Apply the MLP across tokens\n        x = self.mlp(x)\n        # Transpose back into original form\n        x = x.transpose(0, 2)\n\n        #\n        return x\n", "labml_nn/transformers/knn/eval_knn.py": "\"\"\"\n---\ntitle: Evaluate k-nearest neighbor language model\nsummary: >\n  This runs the kNN model and merges the kNN results with transformer output to\n  achieve better results than just using the transformer.\n---\n\n# Evaluate k-nearest neighbor language model\n\"\"\"\nfrom typing import Optional, List\n\nimport faiss\nimport numpy as np\nimport torch\n\nfrom labml import monit, lab\nfrom labml.logger import inspect\nfrom labml_nn.transformers.knn.train_model import Configs\n\n\ndef knn(queries: torch.Tensor, index: faiss.IndexFlatL2, keys_store: np.ndarray, vals_store: np.ndarray, n_tokens: int):\n    \"\"\"\n    ## $k$-NN to get $p(w_t, c_t)$\n\n    Here we refer to $f(\\textcolor{yellowgreen}{c_t})$ as queries,\n    $f(c_i)$ as keys and $w_i$ as values.\n    \"\"\"\n\n    # Save shape of queries to reshape results\n    queries_shape = queries.shape\n\n    # Flatten the `batch` and `sequence` dimensions of queries\n    queries = queries.view(-1, queries_shape[-1])\n\n    # Find 10 nearest neighbors of $f(\\textcolor{yellowgreen}{c_t})$ among $f(c_i)$.\n    # `distance` is the distance given by FAISS and `idx`, $i$ is the index of it in `keys_store`.\n    distance, idx = index.search(queries.numpy(), 10)\n\n    # Get $f(c_i)$\n    keys_found = queries.new_tensor(keys_store[idx])\n    # Get $w_i$\n    vals_found = torch.tensor(vals_store[idx]).squeeze(-1)\n\n    # We are going to calculate the cosine similarity between normalized vectors\n\n    # Normalize $f(c_i)$\n    keys_found_n = keys_found / torch.sqrt((keys_found ** 2).sum(-1, keepdims=True) + 1e-10)\n    # Normalize $f(\\textcolor{yellowgreen}{c_t})$\n    queries_n = queries / torch.sqrt((queries ** 2).sum(-1, keepdims=True) + 1e-10)\n\n    # Get the dot-product, or cosine similarity\n    dot_prod = (keys_found_n * queries_n.unsqueeze(1)).sum(-1)\n\n    # Token-wise logits\n    logits_token = dot_prod.new_zeros(queries.shape[0], n_tokens)\n    # Scatter and accumulate token logits based on the nearest neighbors\n    _ = logits_token.scatter_(dim=1, index=vals_found, src=dot_prod, reduce='add')\n\n    # Reshape the logits\n    logits_token = logits_token.reshape(queries_shape[0], queries_shape[1], -1)\n\n    return logits_token\n\n\ndef validation_loss(knn_weights: List[float], last_n: Optional[int], conf: Configs, index: faiss.IndexFlatL2,\n                    keys_store: np.ndarray, vals_store: np.ndarray):\n    \"\"\"\n    ## Calculate validation loss\n\n    We calculate the validation loss of the combined on $k$-NN prediction and transformer prediction.\n    The weight given to the $k$-NN model is given by `knn_weight`.\n    It's a list of weights and we calculate the validation loss for each.\n    \"\"\"\n\n    # List of losses for each `knn_weights`\n    losses = [[] for _ in knn_weights]\n    # Number of samples in each batch\n    n_samples = []\n    with torch.no_grad():\n        # Iterate through validation data\n        for i, batch in monit.enum(\"Validation\", conf.validator.data_loader, is_children_silent=True):\n            # Get data and target labels\n            data, target = batch[0].to(conf.device), batch[1].to(conf.device)\n            # Run the model and get predictions $p(w_t, c_t)$\n            res = conf.model(data)\n            # Get $k$-NN predictions\n            res_knn = knn(conf.model.ff_input.cpu(), index, keys_store, vals_store, conf.n_tokens)\n            res_knn = res_knn.to(conf.device)\n\n            # This is to calculate only the loss for `last_n` tokens.\n            # This is important because the first predictions (along the sequence)\n            # of transformer model has very few past tokens to look at.\n            if last_n:\n                res = res[-last_n:]\n                res_knn = res_knn[-last_n:]\n                target = target[-last_n:]\n\n            # Number of samples\n            n_s = res.shape[0] * data.shape[1]\n            n_samples.append(n_s)\n\n            # Calculate scores for each of `knn_weights`.\n            for i, c in enumerate(knn_weights):\n                # Calculate the loss\n                loss = conf.loss_func(res_knn * c + (1 - c) * res, target)\n                losses[i].append(loss * n_s)\n\n    return losses, n_samples\n\n\ndef load_index(conf: Configs, n_probe: int = 8):\n    \"\"\"\n    ## Load the index\n    \"\"\"\n    # Dimensions of $f(c_i)$\n    d_model = conf.transformer.d_model\n    # Training data loader\n    data_loader = conf.trainer.data_loader\n    # Number of contexts; i.e. number of tokens in the training data minus one.\n    # $\\big(f(c_i), w_i\\big)$ for $i \\in [2, T]$\n    n_keys = data_loader.data.shape[0] * data_loader.data.shape[1] - 1\n\n    # Load FAISS index\n    with monit.section('Load index'):\n        index = faiss.read_index(str(lab.get_data_path() / 'faiss.index'))\n    # Set number of cells to probe\n    index.nprobe = n_probe\n\n    # Load memory mapped numpy arrays\n    keys_store = np.memmap(str(lab.get_data_path() / 'keys.npy'), dtype=np.float32, mode='r', shape=(n_keys, d_model))\n    vals_store = np.memmap(str(lab.get_data_path() / 'vals.npy'), dtype=np.int, mode='r', shape=(n_keys, 1))\n\n    return index, keys_store, vals_store\n\n\ndef main():\n    from labml_nn.transformers.knn.build_index import load_experiment\n    # Load the experiment. Replace the run uuid with you run uuid from\n    # [training the model](train_model.html).\n    conf = load_experiment('4984b85c20bf11eb877a69c1a03717cd')\n    # Set model to evaluation mode\n    conf.model.eval()\n\n    # Load index\n    index, keys_store, vals_store = load_index(conf)\n    # List of weights given to $k$-NN prediction. We will evaluate the validation loss for\n    # each of the weights\n    knn_weights = [i / 20 for i in range(10)]\n    # Evaluate validation loss\n    losses, n_samples = validation_loss(knn_weights, None, conf, index, keys_store, vals_store)\n    # Output the losses for each of `knn_weights`.\n    inspect({c: np.sum(losses[i]) / np.sum(n_samples) for i, c in enumerate(knn_weights)})\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/knn/train_model.py": "\"\"\"\n---\ntitle: Train Autoregressive Transformer\nsummary: This is training code with notes for a basic auto-regressive transformer.\n---\n\n# Train Autoregressive Transformer\n\nThis trains a simple [transformer](../../) model for auto-regression.\n\"\"\"\n\nimport torch\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml.utils.pytorch import get_modules\nfrom labml_helpers.module import Module\n\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers import Encoder, Generator, TransformerConfigs\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass AutoregressiveModel(Module):\n    \"\"\"\n    ## Auto regressive model\n    \"\"\"\n\n    def __init__(self, src_embed: Module, encoder: Encoder, generator: Generator, *,\n                 is_save_ff_input: bool = False):\n        super().__init__()\n        # Token embedding module\n        self.src_embed = src_embed\n        # Transformer based encoder\n        self.encoder = encoder\n        # Whether the last layer of the encoder should\n        # save the input to the feed-forward layer.\n        # This is out $f(c_t)$, the embedding of the context.\n        self.encoder.layers[-1].is_save_ff_input = is_save_ff_input\n        # Next token generation layer;\n        # this give logits  of the the next token\n        self.generator = generator\n        # This will be initialized on the first call\n        self.src_mask = None\n\n    @property\n    def ff_input(self) -> torch.Tensor:\n        \"\"\"\n        Retrieve saved $f(c_t)$\n        \"\"\"\n        return self.encoder.layers[-1].ff_input\n\n    def forward(self, src: torch.Tensor):\n        # Create subsequent mask, so that the transformer can only pay attention to past tokens.\n        if self.src_mask is None or self.src_mask.size(0) != len(src):\n            self.src_mask = subsequent_mask(len(src)).to(src.device)\n        # Embed the tokens (`src`) and run it through the the transformer\n        res = self.encoder(self.src_embed(src), self.src_mask)\n        # Generate logits of the next token\n        return self.generator(res), None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    The default configs can and will be over-ridden when we start the experiment\n    \"\"\"\n\n    transformer: TransformerConfigs\n    model: AutoregressiveModel\n\n    is_save_ff_input = False\n\n\n@option(Configs.model)\ndef autoregressive_model(c: Configs):\n    \"\"\"\n    Initialize the auto-regressive model\n    \"\"\"\n    m = AutoregressiveModel(\n        # Get the source token embedding layer, encoder and\n        # final token generator from configurable transformer\n        src_embed=c.transformer.src_embed,\n        encoder=c.transformer.encoder,\n        generator=c.transformer.generator,\n        # Whether to save $f(c_t)$\n        is_save_ff_input=c.is_save_ff_input)\n    return m.to(c.device)\n\n\n@option(Configs.transformer)\ndef transformer_c(c: Configs):\n    \"\"\"\n    Initialize the configurable transformer encoder for our autoregressive model\n    \"\"\"\n    tc = TransformerConfigs()\n    tc.n_src_vocab = c.n_tokens\n    tc.n_tgt_vocab = c.n_tokens\n\n    return tc\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"knn_lm\")\n    # Create configs\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf,\n                       # A dictionary of configurations to override\n                       {'tokenizer': 'character',\n                        'prompt_separator': '',\n                        'prompt': 'It is ',\n                        'text': 'tiny_shakespeare',\n\n                        'optimizer.optimizer': 'Noam',\n                        'optimizer.learning_rate': 1.,\n                        'optimizer.d_model': 256,\n\n                        'seq_len': 1024,\n                        'epochs': 128,\n                        'batch_size': 6,\n                        'inner_iterations': 10,\n\n                        # Transformer configurations\n                        'transformer.d_model': 256,\n                        'transformer.ffn.d_ff': 1024,\n                        'transformer.n_heads': 8,\n                        'transformer.n_layers': 6})\n\n    # This is needed to initialize models\n    conf.n_tokens = conf.text.n_tokens\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models(get_modules(conf))\n\n    # Start the experiment\n    with experiment.start():\n        # `TrainValidConfigs.run`\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/knn/__init__.py": "\"\"\"\n---\ntitle: k-Nearest Neighbor Language Models\nsummary: >\n  This is a simple PyTorch implementation/tutorial of the paper\n  Generalization through Memorization: Nearest Neighbor Language Models using FAISS.\n  It runs a kNN model on the final transformer layer embeddings to improve the\n  loss of transformer based language models.\n  It's also great for domain adaptation without pre-training.\n---\n\n# k-Nearest Neighbor Language Models\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n [Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/abs/1911.00172).\nIt uses k-nearest neighbors to  improve perplexity of autoregressive transformer models.\n\nAn autoregressive language model estimates $p(w_t | \\textcolor{yellowgreen}{c_t})$,\n where $w_t$ is the token at step $t$\n and $c_t$ is the context, $\\textcolor{yellowgreen}{c_t} = (w_1, w_2, ..., w_{t-1})$.\n\nThis paper, improves  $p(w_t | \\textcolor{yellowgreen}{c_t})$ using a k-nearest neighbor search\n on key-value pairs $\\big(f(c_i), w_i\\big)$, with search key $f(\\textcolor{yellowgreen}{c_t})$.\n Here $f(\\textcolor{yellowgreen}{c_t})$ is an embedding of the context $\\textcolor{yellowgreen}{c_t}$.\n The paper (and this implementation) uses the **input to the feed-forward layer of the\n final layer of the transformer** as $f(\\textcolor{yellowgreen}{c_t})$.\n\nWe use [FAISS](https://github.com/facebookresearch/faiss) to index $f(c_i)$.\n\n### Implementation\n\nSo to run $k$NN-LM we need to:\n\n* [Train a transformer model](train_model.html)\n* [Build an index](build_index.html) of $\\big(f(c_i), w_i\\big)$\n* [Evaluate kNN-ML](eval_knn.html) using $k$NN seach on $\\big(f(c_i), w_i\\big)$\nwith  $f(\\textcolor{yellowgreen}{c_t})$\n\nThis experiment uses a small dataset so that we can run this without using up a few hundred giga-bytes\nof disk space for the index.\n\nThe official implementation of $k$NN-LM can be found [here](https://github.com/urvashik/knnlm).\n\"\"\"\n", "labml_nn/transformers/knn/build_index.py": "\"\"\"\n---\ntitle: Build FAISS index for k-NN search\nsummary: This builds the FAISS index with the transformer embeddings.\n---\n\n# Build FAISS index for k-NN search\n\nWe want to build the index of $\\big(f(c_i), w_i\\big)$.\nWe store $f(c_i)$ and $w_i$ in memory mapped numpy arrays.\nWe find $f(c_i)$ nearest to $f(c_t)$ using [FAISS](https://github.com/facebookresearch/faiss).\nFAISS indexes $\\big(f(c_i), i\\big)$ and we query it with $f(c_t)$.\n\"\"\"\n\nfrom typing import Optional\n\nimport faiss\nimport numpy as np\nimport torch\n\nfrom labml import experiment, monit, lab\nfrom labml.utils.pytorch import get_modules\nfrom labml_nn.transformers.knn.train_model import Configs\n\n\ndef load_experiment(run_uuid: str, checkpoint: Optional[int] = None):\n    \"\"\"\n    Load a saved experiment from [train model](train_model.html).\n    \"\"\"\n\n    # Create configurations object\n    conf = Configs()\n    # Load custom configurations used in the experiment\n    conf_dict = experiment.load_configs(run_uuid)\n    # We need to get inputs to the feed forward layer, $f(c_i)$\n    conf_dict['is_save_ff_input'] = True\n\n    # This experiment is just an evaluation; i.e. nothing is tracked or saved\n    experiment.evaluate()\n    # Initialize configurations\n    experiment.configs(conf, conf_dict)\n    # Set models for saving/loading\n    experiment.add_pytorch_models(get_modules(conf))\n    # Specify the experiment to load from\n    experiment.load(run_uuid, checkpoint)\n\n    # Start the experiment; this is when it actually loads models\n    experiment.start()\n\n    return conf\n\n\ndef gather_keys(conf: Configs):\n    \"\"\"\n    ## Gather $\\big(f(c_i), w_i\\big)$ and save them in numpy arrays\n\n    *Note that these numpy arrays will take up a lot of space (even few hundred gigabytes)\n    depending on the size of your dataset*.\n    \"\"\"\n\n    # Dimensions of $f(c_i)$\n    d_model = conf.transformer.d_model\n    # Training data loader\n    data_loader = conf.trainer.data_loader\n    # Number of contexts; i.e. number of tokens in the training data minus one.\n    # $\\big(f(c_i), w_i\\big)$ for $i \\in [2, T]$\n    n_keys = data_loader.data.shape[0] * data_loader.data.shape[1] - 1\n    # Numpy array for $f(c_i)$\n    keys_store = np.memmap(str(lab.get_data_path() / 'keys.npy'), dtype=np.float32, mode='w+', shape=(n_keys, d_model))\n    # Numpy array for $w_i$\n    vals_store = np.memmap(str(lab.get_data_path() / 'vals.npy'), dtype=np.int, mode='w+', shape=(n_keys, 1))\n\n    # Number of keys $f(c_i)$ collected\n    added = 0\n    with torch.no_grad():\n        # Loop through data\n        for i, batch in monit.enum(\"Collect data\", data_loader, is_children_silent=True):\n            # $w_i$ the target labels\n            vals = batch[1].view(-1, 1)\n            # Input data moved to the device of the model\n            data = batch[0].to(conf.device)\n            # Run the model\n            _ = conf.model(data)\n            # Get $f(c_i)$\n            keys = conf.model.ff_input.view(-1, d_model)\n            # Save keys, $f(c_i)$ in the memory mapped numpy array\n            keys_store[added: added + keys.shape[0]] = keys.cpu()\n            # Save values, $w_i$ in the memory mapped numpy array\n            vals_store[added: added + keys.shape[0]] = vals\n            # Increment the number of collected keys\n            added += keys.shape[0]\n\n\ndef build_index(conf: Configs, n_centeroids: int = 2048, code_size: int = 64, n_probe: int = 8, n_train: int = 200_000):\n    \"\"\"\n    ## Build FAISS index\n\n    [Getting started](https://github.com/facebookresearch/faiss/wiki/Getting-started),\n    [faster search](https://github.com/facebookresearch/faiss/wiki/Faster-search),\n    and [lower memory footprint](https://github.com/facebookresearch/faiss/wiki/Lower-memory-footprint)\n    tutorials on FAISS will help you learn more about FAISS usage.\n    \"\"\"\n    # Dimensions of $f(c_i)$\n    d_model = conf.transformer.d_model\n    # Training data loader\n    data_loader = conf.trainer.data_loader\n    # Number of contexts; i.e. number of tokens in the training data minus one.\n    # $\\big(f(c_i), w_i\\big)$ for $i \\in [2, T]$\n    n_keys = data_loader.data.shape[0] * data_loader.data.shape[1] - 1\n\n    # Build an index with Verenoi cell based faster search with compression that\n    # doesn't store full vectors.\n    quantizer = faiss.IndexFlatL2(d_model)\n    index = faiss.IndexIVFPQ(quantizer, d_model, n_centeroids, code_size, 8)\n    index.nprobe = n_probe\n\n    # Load the memory mapped numpy array of keys\n    keys_store = np.memmap(str(lab.get_data_path() / 'keys.npy'), dtype=np.float32, mode='r', shape=(n_keys, d_model))\n\n    # Pick a random sample of keys to train the index with\n    random_sample = np.random.choice(np.arange(n_keys), size=[min(n_train, n_keys)], replace=False)\n\n    with monit.section('Train index'):\n        # Train the index to store the keys\n        index.train(keys_store[random_sample])\n\n    # Add keys to the index; $\\big(f(c_i), i\\big)$\n    for s in monit.iterate('Index', range(0, n_keys, 1024)):\n        e = min(s + 1024, n_keys)\n        # $f(c_i)$\n        keys = keys_store[s:e]\n        # $i$\n        idx = np.arange(s, e)\n        # Add to index\n        index.add_with_ids(keys, idx)\n\n    with monit.section('Save'):\n        # Save the index\n        faiss.write_index(index, str(lab.get_data_path() / 'faiss.index'))\n\n\ndef main():\n    # Load the experiment. Replace the run uuid with you run uuid from\n    # [training the model](train_model.html).\n    conf = load_experiment('4984b85c20bf11eb877a69c1a03717cd')\n    # Set model to evaluation mode\n    conf.model.eval()\n\n    # Collect $\\big(f(c_i), w_i\\big)$\n    gather_keys(conf)\n    # Add them to the index for fast search\n    build_index(conf)\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/alibi/experiment.py": "\"\"\"\n---\ntitle: Attention with Linear Biases (ALiBi) Experiment\nsummary: This experiment trains an Attention with Linear Biases (ALiBi) based model on Tiny Shakespeare dataset.\n---\n\n# [Attention with Linear Biases (ALiBi)](index.html) Experiment\n\nThis is an annotated PyTorch experiment to train a [ALiBi model](index.html).\n\nThis is based on [our GPT model](../gpt/index.html).\n\"\"\"\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom labml import experiment, tracker\nfrom labml.configs import option, calculate\nfrom labml_helpers.datasets.text import SequentialUnBatchedDataset\nfrom labml_nn.transformers.alibi import AlibiMultiHeadAttention\nfrom labml_nn.experiments.nlp_autoregression import transpose_batch\nfrom labml_nn.transformers import TransformerConfigs\nfrom labml_nn.transformers.gpt import Configs as GPTConfigs\n\n\nclass Configs(GPTConfigs):\n    \"\"\"\n    ## Configurations\n\n    We extend [GPT configurations](../gpt/index.html) and change the attention mechanism.\n    \"\"\"\n\n    # ALiBi based transformer (defined below)\n    transformer: TransformerConfigs = 'GPT_ALiBi'\n    # Longer validation set\n    valid_seq_len: int = 128\n    valid_loader = 'shuffled_longer_valid_loader'\n\n    def other_metrics(self, output: torch.Tensor, target: torch.Tensor):\n        \"\"\"\n        Log losses at the initial and final tokens\n        \"\"\"\n        # If there are more tokens that the training sequence length (during validation),\n        if self.seq_len < output.shape[0]:\n            # Log the loss at training sequence length\n            tracker.add(f'loss.{self.seq_len - 1}.', self.loss_func(output[self.seq_len - 1], target[self.seq_len - 1]))\n            # Log the loss at the first token\n            tracker.add(f'loss.0.', self.loss_func(output[0], target[0]))\n        # Log the loss at the final token\n        tracker.add(f'loss.{int(output.shape[0]) - 1}.', self.loss_func(output[-1], target[-1]))\n\n\ndef _alibi_mha(c: TransformerConfigs):\n    \"\"\"\n    Create an ALiBi attention module\n    \"\"\"\n    return AlibiMultiHeadAttention(c.n_heads, c.d_model, dropout_prob=c.dropout)\n\n\n# Set all attention mechanisms to ALiBi\ncalculate(TransformerConfigs.encoder_attn, 'alibi_mha', _alibi_mha)\ncalculate(TransformerConfigs.decoder_attn, 'alibi_mha', _alibi_mha)\ncalculate(TransformerConfigs.decoder_mem_attn, 'alibi_mha', _alibi_mha)\n\n\n@option(Configs.valid_loader)\ndef shuffled_longer_valid_loader(c: Configs):\n    \"\"\"\n    Shuffled validation data loader with `valid_seq_len` sequence length\n    \"\"\"\n    return DataLoader(SequentialUnBatchedDataset(text=c.text.valid,\n                                                 dataset=c.text,\n                                                 seq_len=c.valid_seq_len),\n                      batch_size=c.batch_size,\n                      collate_fn=transpose_batch,\n                      shuffle=True)\n\n\n@option(Configs.transformer, 'GPT_ALiBi')\ndef _transformer_configs(c: Configs):\n    \"\"\"\n    ### ALiBi based Transformer configurations\n    \"\"\"\n\n    # We use our\n    # [configurable transformer implementation](../configs.html#TransformerConfigs)\n    conf = TransformerConfigs()\n    # Set the vocabulary sizes for embeddings and generating logits\n    conf.n_src_vocab = c.n_tokens\n    conf.n_tgt_vocab = c.n_tokens\n    # GPT uses GELU activation for position wise feedforward\n    conf.ffn.activation = 'GELU'\n\n    # ALiBi doesn't use positional embeddings\n    conf.src_embed = 'no_pos'\n    conf.tgt_embed = 'no_pos'\n\n    # Set all attention mechanisms to ALiBi\n    conf.encoder_attn = 'alibi_mha'\n    conf.decoder_attn = 'alibi_mha'\n    conf.decoder_mem_attn = 'alibi_mha'\n\n    #\n    return conf\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"gpt_alibi\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n        # 'text': 'tiny_shakespeare_no_split',\n\n        # Use a context size of $128$\n        'seq_len': 64,\n        # Use a context size of $128$\n        'valid_seq_len': 80,\n        # Train for $32$ epochs\n        'epochs': 128,\n        # Batch size $128$\n        'batch_size': 128,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        # Transformer configurations\n        'transformer.d_model': 128,\n        'transformer.ffn.d_ff': 512,\n        'transformer.n_heads': 8,\n        'transformer.n_layers': 4,\n        'transformer.dropout': 0.1,\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/alibi/__init__.py": "\"\"\"\n---\ntitle: Attention with Linear Biases (ALiBi)\nsummary: >\n  Documented implementation with explanations of Attention with Linear Biases (ALiBi)\n---\n\n# Attention with Linear Biases (ALiBi)\n\nThis is an implementation of Attention with Linear Biases (ALiBi) from the paper\n[Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409).\n\nThis replaces positional encodings with biases added to attention scores (attention logits, before the softmax).\nThis is a relative scheme tested on autoregressive tasks, and the bias is higher for closeby tokens\nand lower for far-away tokens.\nThe biases decrease linearly in the log scale (because it's before the softmax) and each head has a different slope.\n\nHere's the attention formula for $i$-th token,\n\n\\begin{align}\n\\mathbf{a}_i\n&= \\text{softmax} \\bigg( \\mathbf{q}_i \\mathbf{K}^\\top + m \\cdot \\big[-(i-1), \\dots, -1, 0 \\big] \\bigg) \\\\\n&= \\text{softmax} \\bigg( \\mathbf{q}_i \\mathbf{K}^\\top + m \\cdot \\big[0, 1, \\dots, (i - 1) \\big] \\bigg)\n\\end{align}\n\nwhere $\\mathbf{q}_i \\in \\mathbb{R}^d$ is the query of the $i$-th token, $K \\in \\mathbb{R}^{i \\times d}$ are the keys\nup to $i$, and $d$ the number of features per head.\nNote that the above equality halts because $\\text{softmax}$ is invariant to translations\n (you can add any constant to all elements without changing the result).\n\nHere is [the training code](experiment.html) for a ALiBi model.\n\"\"\"\nimport math\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\nfrom labml.logger import inspect\nfrom labml_nn.transformers.mha import MultiHeadAttention\n\n\ndef get_slopes(n_heads: int):\n    \"\"\"\n    ## Get head-specific slope $m$ for each head\n\n    * `n_heads` is the number of heads in the attention layer $n$\n\n    The slope for first head is\n\n    $$\\frac{1}{2^{\\frac{8}{n}}} = 2^{-\\frac{8}{n}}$$\n\n    The slopes for the rest of the heads are in a geometric series with a ratio same as above.\n\n    For instance when the number of heads is $8$ the slopes are\n    $$\\frac{1}{2^1}, \\frac{1}{2^2}, \\dots, \\frac{1}{2^8}$$\n    \"\"\"\n\n    # Get the closest power of 2 to `n_heads`.\n    # If `n_heads` is not a power of 2, then we first calculate slopes to the closest (smaller) power of 2,\n    # and then add the remaining slopes.\n    n = 2 ** math.floor(math.log2(n_heads))\n    # $2^{-\\frac{8}{n}}$\n    m_0 = 2.0 ** (-8.0 / n)\n    # $2^{-1\\frac{8}{n}}, 2^{-2 \\frac{8}{n}}, 2^{-3 \\frac{8}{n}}, \\dots$\n    m = torch.pow(m_0, torch.arange(1, 1 + n))\n\n    # If `n_heads` is not a power of 2, then we add the remaining slopes.\n    # We calculate the remaining slopes for $n * 2$ (avoiding slopes added previously).\n    # And pick the slopes upto `n_heads`.\n    if n < n_heads:\n        # $2^{-\\frac{8}{2n}}$\n        m_hat_0 = 2.0 ** (-4.0 / n)\n        # $2^{-1\\frac{8}{2n}}, 2^{-3 \\frac{8}{2n}}, 2^{-5 \\frac{8}{2n}}, \\dots$\n        # Note that we take steps by $2$ to avoid slopes added previously.\n        m_hat = torch.pow(m_hat_0, torch.arange(1, 1 + 2 * (n_heads - n), 2))\n        # Concatenate the slopes with the remaining slopes.\n        m = torch.cat([m, m_hat])\n\n    return m\n\n\n@torch.no_grad()\ndef get_alibi_biases(n_heads: int, mask: torch.Tensor):\n    \"\"\"\n    ## Calculate the attention biases matrix\n\n    * `n_heads` is the number of heads in the attention layer\n    * `mask` is the attention mask of shape `[seq_len_q, seq_len_k]`\n\n    This returns a matrix of shape `[seq_len_q, seq_len_k, n_heads, ]` with ALiBi attention biases.\n    \"\"\"\n\n    # Get slopes $m$ for each head\n    m = get_slopes(n_heads).to(mask.device)\n\n    # Calculate distances $[0, 1, \\dots, N]$\n    # Here we calculate the distances using the mask.\n    #\n    # Since it's causal mask we can just use $[0, 1, \\dots, N]$ too.\n    # `distance = torch.arange(mask.shape[1], dtype=torch.long, device=mask.device)[None, :]`\n    distance = mask.cumsum(dim=-1)\n\n    # Multiply them pair-wise to get the AliBi bias matrix\n    return distance[:, :, None] * m[None, None, :]\n\n\nclass AlibiMultiHeadAttention(MultiHeadAttention):\n    \"\"\"\n    ## Attention with Linear Biases (ALiBi)\n\n    We override [Multi-Head Attention](../mha.html).\n    \"\"\"\n\n    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n        super().__init__(heads, d_model, dropout_prob)\n\n        # To cache AliBi the biases\n        self.alibi_biases = None\n\n    def forward(self, *,\n                query: torch.Tensor,\n                key: torch.Tensor,\n                value: torch.Tensor,\n                mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        `query`, `key` and `value` are the tensors that store\n        collection of *query*, *key* and *value* vectors.\n        They have shape `[seq_len, batch_size, d_model]`.\n\n        `mask` has shape `[seq_len, seq_len, batch_size]` and\n        `mask[i, j, b]` indicates whether for batch `b`,\n        query at position `i` has access to key-value at position `j`.\n        \"\"\"\n\n        # ALiBi only works with causal masks.\n        assert mask is not None\n        assert mask.shape[0] == mask.shape[1] and mask.shape[2] == 1\n\n        # `query`, `key` and `value` have shape `[seq_len, batch_size, d_model]`\n        seq_len, batch_size, _ = query.shape\n\n        # Add head dimension to mask and check its shape.\n        mask = self.prepare_mask(mask, query.shape, key.shape)\n\n        # Prepare `query`, `key` and `value` for attention computation.\n        # These will then have shape `[seq_len, batch_size, heads, d_k]`.\n        query = self.query(query)\n        key = self.key(key)\n        value = self.value(value)\n\n        # Compute attention scores $Q K^\\top$.\n        # This gives a tensor of shape `[seq_len, seq_len, batch_size, heads]`.\n        scores = self.get_scores(query, key)\n\n        # Scale scores $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n        scores *= self.scale\n\n        # Create AliBi biases if it's not cached\n        if self.alibi_biases is None or self.alibi_biases.shape[1] < seq_len:\n            # `mask` has shape `[seq_len, seq_len, 1, 1]`\n            self.alibi_biases = get_alibi_biases(scores.shape[-1], mask[:, :, 0, 0])\n\n        # Add AliBi biases to attention scores.\n        # ALiBi biases has shape `[seq_len, seq_len, n_heads]`\n        # and `scores` has shape `[seq_len, seq_len, batch_size, n_heads]`\n        scores += self.alibi_biases[:seq_len, :seq_len, None, :]\n\n        # Apply mask\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        # $softmax$ attention along the key sequence dimension\n        # $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n        attn = self.softmax(scores)\n\n        # Apply dropout\n        attn = self.dropout(attn)\n\n        # Multiply by values\n        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\n        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n\n        # Concatenate multiple heads\n        x = x.reshape(seq_len, batch_size, -1)\n\n        # Output layer\n        return self.output(x)\n\n\ndef _test_alibi():\n    \"\"\"\n    Simple test function to see the slopes.\n    \"\"\"\n    inspect(get_slopes(12).tolist(), _n=-1)\n    from labml_nn.transformers.utils import subsequent_mask\n\n    mask = subsequent_mask(8)[:, :, 0]\n    inspect(mask)\n\n    inspect(get_alibi_biases(12, mask)[:, :, 3], _n=-1)\n\n\n#\nif __name__ == '__main__':\n    _test_alibi()\n", "labml_nn/transformers/rope/experiment.py": "\"\"\"\n---\ntitle: Rotary Positional Embeddings (RoPE) Experiment\nsummary: This experiment trains a transformer model with Rotary Positional Embeddings (RoPE) on tiny Shakespeare dataset.\n---\n\n# Rotary Positional Embeddings (RoPE) Experiment\n\nThis is an annotated PyTorch experiment to train a transformer model with Rotary Positional Embeddings (RoPE).\n\"\"\"\n\nfrom labml import experiment\nfrom labml.configs import option, calculate\nfrom labml_nn.transformers import TransformerConfigs\nfrom labml_nn.transformers.basic.autoregressive_experiment import AutoregressiveTransformer, Configs\n\n\n# ### Rotary PE attention\ndef _rotary_pe_mha(c: TransformerConfigs):\n    from labml_nn.transformers.rope import RotaryPEMultiHeadAttention\n    return RotaryPEMultiHeadAttention(c.n_heads, c.d_model, 1.)\n\n\n# Configuration options\ncalculate(TransformerConfigs.encoder_attn, 'rotary', _rotary_pe_mha)\ncalculate(TransformerConfigs.decoder_attn, 'rotary', _rotary_pe_mha)\ncalculate(TransformerConfigs.decoder_mem_attn, 'rotary', _rotary_pe_mha)\n\n\n@option(Configs.model, 'rotary_pe_transformer')\ndef _model(c: Configs):\n    \"\"\"\n    Create an autoregressive model and initialize weights\n    \"\"\"\n    m = AutoregressiveTransformer(c.transformer.encoder,\n                                  c.transformer.src_embed,\n                                  c.transformer.generator).to(c.device)\n\n    return m\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"rotary_pe_transformer\", writers={'screen'})\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # No fixed positional embeddings\n        'transformer.src_embed': 'no_pos',\n        'transformer.tgt_embed': 'no_pos',\n\n        # Encoder with RoPE\n        'transformer.encoder_attn': 'rotary',\n\n        #\n        'model': 'rotary_pe_transformer',\n\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 512,\n        # Train for 32 epochs\n        'epochs': 32,\n        # Batch size $4$\n        'batch_size': 4,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        # Model size\n        'd_model': 128,\n        'transformer.ffn.d_ff': 512,\n        'transformer.n_heads': 16,\n        'transformer.dropout': 0.0,\n\n        # Use [Noam optimizer](../../optimizers/noam.html)\n        'optimizer.optimizer': 'Noam',\n        'optimizer.learning_rate': 1.,\n\n        'dataloader_shuffle_with_replacement': True\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/rope/__init__.py": "\"\"\"\n---\ntitle: Rotary Positional Embeddings (RoPE)\nsummary: >\n  Annotated implementation of RoPE from paper\n  RoFormer: Enhanced Transformer with Rotary Position Embedding\n---\n\n# Rotary Positional Embeddings (RoPE)\n\nThis is an implementation of\n[Rotary Positional Embeddings (RoPE)](https://arxiv.org/abs/2104.09864)\nin [PyTorch](https://pytorch.org).\n\nRotary Positional Embeddings (RoPE) encode position information of tokens\nwith a rotation matrix that naturally incorporates explicit relative position\ndependency.\n\nHere's [the training code](experiment.html) for training a transformer model with RoPE\n on Tiny Shakespeare dataset.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml.logger import inspect\nfrom labml_nn.transformers.mha import MultiHeadAttention\n\n\nclass RotaryPositionalEmbeddings(nn.Module):\n    \"\"\"\n    ## RoPE module\n\n    Rotary encoding transforms pairs of features by rotating in the 2D plane.\n    That is, it organizes the $d$ features as $\\frac{d}{2}$ pairs.\n    Each pair can be considered a coordinate in a 2D plane, and the encoding will rotate it\n    by an angle depending on the position of the token.\n\n    ### For a pair of features\n\n    Let $x^{(1)}_m$ and $x^{(2)}_m$ be two features of the\n    key or query of any head at position $m$.\n    Or for simplicity assume $x$ has only two features.\n    Then the transformation is,\n\n    \\begin{align}\n    RoPE\\big(x^{(1)}_m, x^{(2)}_m, m\\big) &=\n    \\begin{pmatrix}\n    \\cos m \\theta & - \\sin m \\theta \\\\\n    \\sin m \\theta & \\cos m \\theta\n    \\end{pmatrix}\n    \\begin{pmatrix}\n    x^{(1)}_m \\\\\n    x^{(2)}_m \\\\\n    \\end{pmatrix} \\\\\n    &=\n    \\begin{pmatrix}\n    x^{(1)}_m \\cos m\\theta - x^{(2)}_m \\sin m \\theta \\\\\n    x^{(2)}_m \\cos m\\theta + x^{(1)}_m \\sin m \\theta \\\\\n    \\end{pmatrix} \\\\\n    \\end{align}\n\n    where $\\theta$ is a constant angle. The other pairs of features are transformed similarly.\n\n    ### Attention is relative\n\n    For a pair of features, dot-product attention score between two positions $m$ and $n$ would be\n\n    \\begin{align}\n    \\Big \\langle RoPE\\big(x^{(1)}_m, x^{(2)}_m, m\\big),  RoPE\\big(x^{(1)}_n, x^{(2)}_n, n\\big) \\Big \\rangle &= \\\\\n    (x^{(1)}_m \\cos m\\theta - x^{(2)}_m \\sin m \\theta)(x^{(1)}_n \\cos n\\theta - x^{(2)}_n \\sin n \\theta) &+ \\\\\n    (x^{(2)}_m \\cos m\\theta + x^{(1)}_m \\sin m \\theta)(x^{(2)}_n \\cos n\\theta + x^{(1)}_n \\sin n \\theta) &= \\\\\n    x^{(1)}_m x^{(1)}_n (\\cos m\\theta \\cos n\\theta + \\sin m \\theta \\sin n \\theta) &+ \\\\\n    x^{(1)}_m x^{(2)}_n (-\\cos m\\theta \\sin n\\theta + \\sin m \\theta \\cos n \\theta) &+ \\\\\n    x^{(2)}_m x^{(1)}_n (-\\sin m\\theta \\cos n\\theta + \\cos m \\theta \\sin n \\theta) &+ \\\\\n    x^{(2)}_m x^{(2)}_n (\\sin m\\theta \\sin n\\theta + \\cos m \\theta \\cos n \\theta) &= \\\\\n\n    x^{(1)}_m x^{(1)}_n \\cos (m - n) \\theta +\n    x^{(1)}_m x^{(2)}_n \\sin(m - n) \\theta &+ \\\\\n    - x^{(2)}_m x^{(1)}_n \\sin (m - n) \\theta +\n    x^{(2)}_m x^{(2)}_n \\cos (m - n) \\theta &= \\\\\n\n    \\big(x^{(1)}_m \\cos (m - n)\\theta - x^{(2)}_m \\sin (m - n) \\theta\\big) x^{(1)}_n &+ \\\\\n    \\big(x^{(2)}_m \\cos (m - n)m\\theta + x^{(1)}_m \\sin (m - n) \\theta\\big) x^{(2)}_n  &= \\\\\n\n    \\Big \\langle RoPE\\big(x^{(1)}_m, x^{(2)}_m, m - n\\big),  RoPE\\big(x^{(1)}_n, x^{(2)}_n, 0\\big) \\Big \\rangle\n    \\end{align}\n\n    This shows that for dot-production attention the rotary encodings gives relative attention.\n\n    ### For all features\n\n    The features are grouped into pairs and handled as above. They use a different $\\theta$ for each pair.\n\n    The paper suggests using $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n    for the $\\frac{d}{2}$ pairs of features.\n\n    We pair feature $i$ with feature $i + \\frac{d}{2}$. So for position $m$ we transform\n\n    \\begin{align}\n    \\begin{pmatrix}\n    x^{(i)}_m \\\\\n    x^{(i + \\frac{d}{2})}_m\n    \\end{pmatrix}\n    \\end{align}\n\n    to\n\n    \\begin{align}\n    \\begin{pmatrix}\n    x^{(i)}_m \\cos m \\theta_i - x^{(i + \\frac{d}{2})}_m \\sin m \\theta_i \\\\\n    x^{(i + \\frac{d}{2})}_m \\cos m\\theta_i + x^{(i)}_m \\sin m \\theta_i \\\\\n    \\end{pmatrix} \\\\\n    \\end{align}\n    \"\"\"\n\n    def __init__(self, d: int, base: int = 10_000):\n        \"\"\"\n        * `d` is the number of features $d$\n        * `base` is the constant used for calculating $\\Theta$\n        \"\"\"\n        super().__init__()\n\n        self.base = base\n        self.d = d\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def _build_cache(self, x: torch.Tensor):\n        \"\"\"\n        Cache $\\cos$ and $\\sin$ values\n        \"\"\"\n        # Return if cache is already built\n        if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n            return\n\n        # Get sequence length\n        seq_len = x.shape[0]\n\n        # $\\Theta = {\\theta_i = 10000^{-\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n        theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device)\n\n        # Create position indexes `[0, 1, ..., seq_len - 1]`\n        seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device)\n\n        # Calculate the product of position index and $\\theta_i$\n        idx_theta = torch.einsum('n,d->nd', seq_idx, theta)\n\n        # Concatenate so that for row $m$ we have\n        # $[m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}, m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}]$\n        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)\n\n        # Cache them\n        self.cos_cached = idx_theta2.cos()[:, None, None, :]\n        self.sin_cached = idx_theta2.sin()[:, None, None, :]\n\n    def _neg_half(self, x: torch.Tensor):\n        # $\\frac{d}{2}$\n        d_2 = self.d // 2\n\n        # Calculate $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\\frac{d}{2})}]$\n        return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the Tensor at the head of a key or a query with shape `[seq_len, batch_size, n_heads, d]`\n        \"\"\"\n        # Cache $\\cos$ and $\\sin$ values\n        self._build_cache(x)\n\n        # Split the features, we can choose to apply rotary embeddings only to a partial set of features.\n        x_rope, x_pass = x[..., :self.d], x[..., self.d:]\n\n        # Calculate\n        # $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\\frac{d}{2})}]$\n        neg_half_x = self._neg_half(x_rope)\n\n        # Calculate\n        #\n        # \\begin{align}\n        # \\begin{pmatrix}\n        # x^{(i)}_m \\cos m \\theta_i - x^{(i + \\frac{d}{2})}_m \\sin m \\theta_i \\\\\n        # x^{(i + \\frac{d}{2})}_m \\cos m\\theta_i + x^{(i)}_m \\sin m \\theta_i \\\\\n        # \\end{pmatrix} \\\\\n        # \\end{align}\n        #\n        # for $i \\in {1, 2, ..., \\frac{d}{2}}$\n        x_rope = (x_rope * self.cos_cached[:x.shape[0]]) + (neg_half_x * self.sin_cached[:x.shape[0]])\n\n        #\n        return torch.cat((x_rope, x_pass), dim=-1)\n\n\nclass RotaryPEMultiHeadAttention(MultiHeadAttention):\n    \"\"\"\n    ## Multi-head attention with rotary positional embeddings\n\n    We override [multi-head attention from original transformer](../mha.html).\n    \"\"\"\n\n    def __init__(self, heads: int, d_model: int, rope_percentage: float = 0.5, dropout_prob: float = 0.0):\n        super().__init__(heads, d_model, dropout_prob)\n\n        # Rotary positional embedding layers\n        d_rope = int(self.d_k * rope_percentage)\n        self.query_rotary_pe = RotaryPositionalEmbeddings(d_rope)\n        self.key_rotary_pe = RotaryPositionalEmbeddings(d_rope)\n\n    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n        \"\"\"\n        ### Calculate scores between queries and keys\n        \"\"\"\n\n        # Calculate dot-product with RoPE\n        return torch.einsum('ibhd,jbhd->ijbh', self.query_rotary_pe(query), self.key_rotary_pe(key))\n\n\ndef _test_rotary():\n    \"\"\"\n    Testing RoPE with a simple example\n    \"\"\"\n    x = torch.tensor([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]], dtype=torch.float)\n    x = x[:, None, None, :]\n    inspect(x)\n\n    rotary_pe = RotaryPositionalEmbeddings(4)\n    inspect(rotary_pe(x))\n\n\nif __name__ == '__main__':\n    _test_rotary()\n", "labml_nn/transformers/rope/value_pe/experiment.py": "\"\"\"\n---\ntitle: Rotary Positional Embeddings (RoPE) Experiment\nsummary: This experiment trains a transformer model with Rotary Positional Embeddings (RoPE) on tiny Shakespeare dataset.\n---\n\n# Rotary Positional Embeddings (RoPE) Experiment\n\nThis is an annotated PyTorch experiment to train a transformer model with Rotary Positional Embeddings (RoPE).\n\"\"\"\n\nfrom labml import experiment\nfrom labml.configs import calculate\nfrom labml_nn.transformers import TransformerConfigs\nfrom labml_nn.transformers.rope.experiment import Configs as RoPEConfigs\n\n\n# ### Rotary PE attention\n\nclass Configs(RoPEConfigs):  # , ArithmeticAutoregression):\n    pass\n\n\ndef _rotary_value_pe_mha(c: TransformerConfigs):\n    from labml_nn.transformers.rope.value_pe import RotaryValuePEMultiHeadAttention\n    return RotaryValuePEMultiHeadAttention(c.n_heads, c.d_model, 1., 1.)\n\n\n# Configuration options\ncalculate(TransformerConfigs.encoder_attn, 'rotary_value', _rotary_value_pe_mha)\ncalculate(TransformerConfigs.decoder_attn, 'rotary_value', _rotary_value_pe_mha)\ncalculate(TransformerConfigs.decoder_mem_attn, 'rotary_value', _rotary_value_pe_mha)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"rotary_shakespeare\", comment=\"rotary value\", writers={'screen', 'labml'})\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # No fixed positional embeddings\n        'transformer.src_embed': 'no_pos',\n        'transformer.tgt_embed': 'no_pos',\n\n        # Encoder with RoPE\n        'transformer.encoder_attn': 'rotary_value',\n        # 'transformer.encoder_attn': 'rotary',\n\n        #\n        'model': 'rotary_pe_transformer',\n\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 512,\n        # Train for 32 epochs\n        'epochs': 24,\n        # Batch size $4$\n        'batch_size': 16,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 4,\n\n        # Model size\n        'd_model': 128,\n        'transformer.ffn.d_ff': 512,\n        'transformer.n_heads': 4,\n        'transformer.dropout': 0.0,\n\n        # Use [Adam optimizer](../../optimizers/noam.html)\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n\n        'dataloader_shuffle_with_replacement': True\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/rope/value_pe/arithmetic_experiment.py": "\"\"\"\n---\ntitle: Rotary Positional Embeddings with Relative distance (RoPER) Experiment\nsummary: This experiment trains a transformer model with Rotary Positional Embeddings with\n Relative Distance (RoPER) on the arithmetic addition task.\n---\n\n# Rotary Positional Embeddings with Relative distance ([RoPER](index.html)) Experiment\n\"\"\"\n\nfrom labml import experiment\nfrom labml.configs import calculate\nfrom labml_nn.experiments.arithmetic_dataset import ArithmeticAutoregression\nfrom labml_nn.transformers import TransformerConfigs\nfrom labml_nn.transformers.rope.experiment import Configs as RoPEConfigs\n\n\nclass Configs(RoPEConfigs, ArithmeticAutoregression):\n    \"\"\"\n    We inherit [RoPE experiment](../experiment.html) and use it for\n    [arithmetic addition task](../../experiments/arithmetic_dataset.html).\n\n    We add the option to change attention to use Rotary Positional Embeddings with Relative distance (RoPER)\n    below.\n    \"\"\"\n    pass\n\n\ndef _rotary_value_pe_mha(c: TransformerConfigs):\n    \"\"\"\n    Use Rotary Positional Embeddings with Relative distance ([RoPER](index.html)) in attention.\n    \"\"\"\n    from labml_nn.transformers.rope.value_pe import RotaryValuePEMultiHeadAttention\n    return RotaryValuePEMultiHeadAttention(c.n_heads, c.d_model, 1., 1.)\n\n\n# Configuration options\ncalculate(TransformerConfigs.encoder_attn, 'rotary_value', _rotary_value_pe_mha)\ncalculate(TransformerConfigs.decoder_attn, 'rotary_value', _rotary_value_pe_mha)\ncalculate(TransformerConfigs.decoder_mem_attn, 'rotary_value', _rotary_value_pe_mha)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"roper_addition\", comment=\"rotary value 7\", writers={'screen', 'labml'})\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        'max_digits': 7,\n\n        # No fixed positional embeddings\n        'transformer.src_embed': 'no_pos',\n        'transformer.tgt_embed': 'no_pos',\n\n        # Encoder with RoPER attention\n        'transformer.encoder_attn': 'rotary_value',\n        # Encoder with RoPE attention\n        # 'transformer.encoder_attn': 'rotary',\n\n        #\n        'model': 'rotary_pe_transformer',\n\n        # Use a context size of $256$\n        'seq_len': 512,\n        # Train for 32 epochs\n        'epochs': 20,\n        # Batch size $4$\n        'batch_size': 16,\n\n        # Model size\n        'd_model': 128,\n        'transformer.ffn.d_ff': 512,\n        'transformer.n_heads': 4,\n        'transformer.dropout': 0.0,\n\n        # Use [Adam optimizer](../../optimizers/noam.html)\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/rope/value_pe/__init__.py": "\"\"\"\n---\ntitle: Rotary Positional Embeddings with Relative distance (RoPER)\nsummary: >\n  This is an implementation of RoPER which adds relative distance information to embeddings on\n  top of RoPE introduced in RoFormer: Enhanced Transformer with Rotary Position Embedding\n---\n\n*RoPER is work by [Georges Harik (@gharik)](https://twitter.com/gharik),\nand this implementation is based on his original code.*\n\n# Rotary Positional Embeddings with Relative distance (RoPER)\n\n[Rotary Positional Embeddings (RoPE)](https://arxiv.org/abs/2104.09864) includes\nrelative positions in attention score calculation.\nHowever, the embeddings themselves do not get any positional information\n, [except what it can get implicitly from causal attention](https://arxiv.org/abs/2c364684b15b11ecac827bce58715ee7).\n\nRoPER adds relative positional information explicitly to value embeddings.\nSpecifically, it adds the relative positions of the tokens it paid attention to.\nWe use same rotary positional embeddings to rotate the values in attention,\nThen, after taking the weighted sum,\n we rotate the final in the opposite direction.\nWhich is equivalent to rotating each of the values (before attention) relative to the current position.\n\nHere's [the training code](experiment.html) for training a transformer model with RoPER\n on an arithmetic addition where we can see significant improvement over RoPE.\n\n### Relative distances in embeddings\n\nFor any head, let $a_{n,i}$ be the attention from position $n$ to position $i$,\nand $v_i$ be the value embeddings at position $i$. Let's denote individual features\nas $v^{(1)}_i, v^{(2)}_i, \\dots$.\n\nNormally, we would take the weight sum of value embeddings\n\n$$o^{(j)}_n = \\sum_i a_{n,i} v^{(j)}_i$$\n\nThis doesn't explicitly add any distance information about the positions $i$ to final\nresult $o^{(j)}_n$.\n\nRoPER pairs features like RoPE and transform them.\nFor a pair $v^{(1)}_m$ and $v^{(2)}_m$ it transforms them by\n $RoPE\\big(v^{(1)}_m, v^{(2)}_m, m\\big)$.\nLet us donate the transformed features with $\\hat{v}^{(1)}_m, \\hat{v}^{(2)}_m$.\nThen it rotates the weighted sum $\\hat{o}^{(j)}_n$ in the the reverse direction with\n $RoPE\\big(\\hat{o}^{(1)}_n, \\hat{o}^{(2)}_n, -n\\big)$.\n*Note the *$-n$.\n\nNote that,\n\n\\begin{align}\nRoPE\\big(x^{(1)}_m, x^{(2)}_m, m\\big) &=\n\\begin{pmatrix}\n\\cos m \\theta & - \\sin m \\theta \\\\\n\\sin m \\theta & \\cos m \\theta\n\\end{pmatrix}\n\\begin{pmatrix}\nx^{(1)}_m \\\\\nx^{(2)}_m \\\\\n\\end{pmatrix} \\\\\n&=\n\\begin{pmatrix}\nx^{(1)}_m \\cos m\\theta - x^{(2)}_m \\sin m \\theta \\\\\nx^{(2)}_m \\cos m\\theta + x^{(1)}_m \\sin m \\theta \\\\\n\\end{pmatrix} \\\\\n\\end{align}\n\nFinal output after with the transformations is,\n\n\\begin{align}\nRoPE\\big(\\hat{o}^{(1)}_n, \\hat{o}^{(2)}_n, -n\\big) &= \\\\\n\\begin{pmatrix}\n\\hat{o}^{(1)}_n \\cos n\\theta + \\hat{o}^{(2)}_n \\sin n \\theta \\\\\n\\hat{o}^{(2)}_n \\cos n\\theta - \\hat{o}^{(1)}_n \\sin n \\theta \\\\\n\\end{pmatrix} \\\\\n\\end{align}\n\n*Note that *$\\sin (-n \\theta) = -\\sin n \\theta$.\n\nLet's expand the first term $\\hat{o}^{(1)}_n \\cos n\\theta + \\hat{o}^{(2)}_n \\sin n \\theta$,\n\n\\begin{align}\n\\hat{o}^{(1)}_n \\cos n\\theta + \\hat{o}^{(2)}_n \\sin n \\theta &= \\\\\n\\sum_i a_{n,i} \\hat{v}^{(1)}_i \\cos n\\theta + \\sum_i a_{n,i} \\hat{v}^{(2)}_i \\sin n \\theta &= \\\\\n\n\\sum_i a_{n,i} \\Big( v^{(1)}_i \\cos i\\theta - v^{(2)}_i \\sin i \\theta \\Big) \\cos n\\theta &+ \\\\\n\\sum_i a_{n,i} \\Big( v^{(2)}_i \\cos i\\theta + v^{(1)}_i \\sin i \\theta \\Big) \\sin m \\theta &= \\\\\n\n\\sum_i a_{n,i} v^{(1)}_i \\Big( \\cos i\\theta \\cos n\\theta + \\sin i \\theta \\sin n \\theta \\Big) &+ \\\\\n\\sum_i a_{n,i} v^{(2)}_i \\Big( \\cos i\\theta \\sin n\\theta - \\sin i \\theta \\cos n \\theta \\Big) &= \\\\\n\n\\sum_i a_{n,i} v^{(1)}_i \\cos (i - n) \\theta - \\sum_i a_{n,i} v^{(2)}_i \\sin (i - n) \\theta &= \\\\\n\n\\sum_i a_{n,i} v^{(1)}_i \\cos (i - n) \\theta - \\sum_i a_{n,i} v^{(2)}_i \\sin (i - n) \\theta\n\\end{align}\n\nSimiarly we can show the second term is equal to,\n\n$$\\sum_i a_{n,i} v^{(1)}_i \\cos (i - n) \\theta + \\sum_i a_{n,i} v^{(2)}_i \\sin (i - n) \\theta$$\n\nWhich gives,\n\n\\begin{align}\nRoPE\\big(\\hat{o}^{(1)}_n, \\hat{o}^{(2)}_n, -n\\big) &= \\\\\n\\begin{pmatrix}\n\\sum_i a_{n,i} v^{(1)}_i \\cos (i - n) \\theta - \\sum_i a_{n,i} v^{(2)}_i \\sin (i - n) \\theta \\\\\n\\sum_i a_{n,i} v^{(1)}_i \\cos (i - n) \\theta + \\sum_i a_{n,i} v^{(2)}_i \\sin (i - n) \\theta \\\\\n\\end{pmatrix} &= \\\\\n\\sum_i a_{n,i} RoPE \\big (v^{(1)}_i, v^{(1)}_i, (i - n) \\theta \\big)\n\\end{align}\n\nThat is, the weighted average of values rotated relative to current position.\n\n[Here's an experiment](arithmetic_experiment.html) that uses RoPER on an arthmetic addition task.\n\"\"\"\n\nfrom typing import Optional\n\nimport torch\n\nfrom labml_nn.transformers.rope import RotaryPositionalEmbeddings, RotaryPEMultiHeadAttention\n\n\nclass ReverseRotaryPositionalEmbeddings(RotaryPositionalEmbeddings):\n    \"\"\"\n    ## RoPE module that rotates in the opposite direction\n\n    This inherits from [RoPE rotation implementation](../index.html) and changes the direction.\n    \"\"\"\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the Tensor at the head of a key or a query with shape `[seq_len, batch_size, n_heads, d]`\n        \"\"\"\n        # Cache $\\cos$ and $\\sin$ values\n        self._build_cache(x)\n\n        # Split the features, we can choose to apply rotary embeddings only to a partial set of features.\n        x_rope, x_pass = x[..., :self.d], x[..., self.d:]\n\n        # Calculate\n        # $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\\frac{d}{2})}]$\n        neg_half_x = self._neg_half(x_rope)\n\n        # Calculate\n        #\n        # \\begin{align}\n        # \\begin{pmatrix}\n        # x^{(i)}_m \\cos -m \\theta_i - x^{(i + \\frac{d}{2})}_m \\sin -m \\theta_i \\\\\n        # x^{(i + \\frac{d}{2})}_m \\cos -m\\theta_i + x^{(i)}_m \\sin -m \\theta_i \\\\\n        # \\end{pmatrix} = \\\\\n        # \\begin{pmatrix}\n        # x^{(i)}_m \\cos m \\theta_i + x^{(i + \\frac{d}{2})}_m \\sin m \\theta_i \\\\\n        # x^{(i + \\frac{d}{2})}_m \\cos m\\theta_i - x^{(i)}_m \\sin m \\theta_i \\\\\n        # \\end{pmatrix} \\\\\n        # \\end{align}\n        #\n        # for $i \\in {1, 2, ..., \\frac{d}{2}}$\n        x_rope = (x_rope * self.cos_cached[:x.shape[0]]) - (neg_half_x * self.sin_cached[:x.shape[0]])\n\n        #\n        return torch.cat((x_rope, x_pass), dim=-1)\n\n\nclass RotaryValuePEMultiHeadAttention(RotaryPEMultiHeadAttention):\n    \"\"\"\n    ## Multi-head attention with rotary positional embeddings\n\n    We override [multi-head attention from original transformer](../mha.html).\n    \"\"\"\n\n    def __init__(self, heads: int, d_model: int,\n                 rope_percentage: float = 0.5, rope_value_percentage: float = 0.5,\n                 dropout_prob: float = 0.0):\n        super().__init__(heads, d_model, rope_percentage, dropout_prob)\n\n        # Rotary positional embedding layers\n        d_rope_value = int(self.d_k * rope_value_percentage)\n\n        self.value_rotary_pe = RotaryPositionalEmbeddings(d_rope_value)\n        self.value_reverse_rotary_pe = ReverseRotaryPositionalEmbeddings(d_rope_value)\n\n    def forward(self, *,\n                query: torch.Tensor,\n                key: torch.Tensor,\n                value: torch.Tensor,\n                mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        `query`, `key` and `value` are the tensors that store\n        collection of *query*, *key* and *value* vectors.\n        They have shape `[seq_len, batch_size, d_model]`.\n\n        `mask` has shape `[seq_len, seq_len, batch_size]` and\n        `mask[i, j, b]` indicates whether for batch `b`,\n        query at position `i` has access to key-value at position `j`.\n        \"\"\"\n\n        # `query`, `key` and `value`  have shape `[seq_len, batch_size, d_model]`\n        seq_len, batch_size, _ = query.shape\n\n        if mask is not None:\n            mask = self.prepare_mask(mask, query.shape, key.shape)\n\n        # Prepare `query`, `key` and `value` for attention computation.\n        # These will then have shape `[seq_len, batch_size, heads, d_k]`.\n        query = self.query(query)\n        key = self.key(key)\n        value = self.value(value)\n\n        # Compute attention scores $Q K^\\top$.\n        # This gives a tensor of shape `[seq_len, seq_len, batch_size, heads]`.\n        scores = self.get_scores(query, key)\n\n        # Scale scores $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n        scores *= self.scale\n\n        # Apply mask\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        # $softmax$ attention along the key sequence dimension\n        # $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n        attn = self.softmax(scores)\n\n        # Apply dropout\n        attn = self.dropout(attn)\n\n        # Rotate value embeddings before taking the weighted sum so that they contain positional information\n        value = self.value_rotary_pe(value)\n\n        # Multiply by values\n        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\n        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n\n        # Rotate in the opposite direction so that each embedding hold the relative positions\n        x = self.value_reverse_rotary_pe(x)\n\n        # Save attentions for any other calculations\n        self.attn = attn.detach()\n\n        # Concatenate multiple heads\n        x = x.reshape(seq_len, batch_size, -1)\n\n        # Output layer\n        return self.output(x)\n", "labml_nn/transformers/retro/model.py": "\"\"\"\n---\ntitle: RETRO model\nsummary: >\n  RETRO model with encoder for neighbors and autoregressive decoder\n---\n\n# RETRO model\n\nThis is the model definition for\n [RETRO](index.html).\n\"\"\"\n\nimport math\nfrom typing import Set\n\nimport torch\nfrom torch import nn\n\nfrom labml.logger import inspect\n\n\nclass RotaryPositionalEmbeddings(nn.Module):\n    \"\"\"\n    ## [RoPE embeddings](../rope/index.html)\n\n    *We use rotary position embeddings in self-attention layers.\n    We assume the positional information gets embedded in embeddings\n    and therefore not use them in causal attention.\n    [Non-causal self-attention needs explicit positional information\n     because it cannot infer it](https://arxiv.org/abs/3999902edc8511eba3db37f65e372566).*\n    \"\"\"\n\n    def __init__(self, d: int, base: int = 10_000):\n        \"\"\"\n        * `d` is the number of features $d$\n        * `base` is the constant used for calculating $\\Theta$\n        \"\"\"\n        super().__init__()\n        # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n        self.theta = nn.Parameter(1. / (base ** (torch.arange(0, d, 2).float() / d)), requires_grad=False)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the Tensor at the head of a key or a query with shape `[ batch_size, seq_len, n_heads, d]`\n        \"\"\"\n        # Extract the shape\n        batch_size, seq_len, n_heads, d = x.shape\n\n        # $\\frac{d}{2}$\n        d_2 = d // 2\n\n        # Create position indexes `[0, 1, ..., seq_len - 1]`\n        seq_idx = torch.arange(seq_len, device=x.device).type_as(self.theta)\n\n        # Calculate the product of position index and $\\theta_i$\n        idx_theta = torch.einsum('n,d->nd', seq_idx, self.theta)\n\n        # Concatenate so that for row $m$ we have\n        # $[m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}, m \\theta 0, m \\theta 1, ..., m \\theta_{\\frac{d}{2}}]$\n        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)\n\n        # Calculate\n        # $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., -x^{(\\frac{d}{2})}]$\n        neg_half_x = torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1)\n\n        # Calculate\n        #\n        # \\begin{align}\n        # \\begin{pmatrix}\n        # x^{(i)}_m \\cos m \\theta_i - x^{(i + \\frac{d}{2})}_m \\sin m \\theta_i \\\\\n        # x^{(i + \\frac{d}{2})}_m \\cos m\\theta_i + x^{(i)}_m \\sin m \\theta_i \\\\\n        # \\end{pmatrix} \\\\\n        # \\end{align}\n        #\n        # for $i \\in {1, 2, ..., \\frac{d}{2}}$\n        rx = (x * idx_theta2.cos()[None, :, None, :]) + (neg_half_x * idx_theta2.sin()[None, :, None, :])\n\n        #\n        return rx\n\n\nclass SelfAttention(nn.Module):\n    \"\"\"\n    ## Self-Attention Layer $\\text{A\\small{TTN}}$\n\n    This applies causal and non-causal [multi-headed self-attention](../mha.html).\n    \"\"\"\n\n    def __init__(self, d_model: int, n_heads: int, d_k: int, is_causal: bool):\n        \"\"\"\n        * `d_model` is the number of features in transformer embeddings\n        * `n_heads` is the number of attention heads\n        * `d_k` is the number of features per head\n        * `is_causal` indicates whether this is causal attention (masked)\n        \"\"\"\n        super().__init__()\n\n        self.is_causal = is_causal\n        self.n_heads = n_heads\n        self.d_k = d_k\n\n        # To scale attentions before softmax by $\\frac{1}{\\sqrt{d_k}}$\n        self.scale = 1 / math.sqrt(self.d_k)\n\n        # Linear layers for query, key and value heads.\n        self.query = nn.Linear(d_model, n_heads * d_k)\n        self.key = nn.Linear(d_model, n_heads * d_k)\n        self.value = nn.Linear(d_model, n_heads * d_k)\n\n        # Pre-norm layer. The paper uses RMSNorm instead.\n        self.norm = nn.LayerNorm(d_model)\n\n        # Softmax for attention probabilities\n        self.softmax = nn.Softmax(dim=-1)\n\n        # Rotary positional embeddings\n        self.rotary_pe = RotaryPositionalEmbeddings(self.d_k)\n\n        # Final linear layer\n        self.output = nn.Linear(n_heads * d_k, d_model)\n\n    def mask_attention(self, attn: torch.Tensor):\n        \"\"\"\n        ### Mask the attention layer for causal attention\n\n        * `attn` is the attention matrix of shape `[batch_size, n_heads, seq_len, seq_len]`\n        \"\"\"\n\n        # No masking for non-causal attention\n        if not self.is_causal:\n            return attn\n\n        # Create a triangular mask\n        mask = torch.tril(attn.new_ones(attn.shape[-2:]))\n        # Filter by the mask\n        return attn.masked_fill(mask == 0, float('-inf'))\n\n    def forward(self, h: torch.Tensor):\n        \"\"\"\n        * `h` is the transformer embeddings of shape `[batch_size, seq_len, d_model]`\n        \"\"\"\n\n        # Residual connection\n        h_res = h\n\n        # Pre-normalization\n        h = self.norm(h)\n\n        # Get query, key, and values and split them in to heads.\n        # These will have shapes `[batch_size, seq_len, n_heads, d_k]`\n        mh_shape = (*h.shape[:-1], self.n_heads, self.d_k)\n        q = self.query(h).view(mh_shape)\n        k = self.key(h).view(mh_shape)\n        v = self.value(h).view(mh_shape)\n\n        # Apply rotary positional embeddings\n        q = self.rotary_pe(q)\n        k = self.rotary_pe(k)\n\n        # Calculate attentions\n        attn = torch.einsum('bihd,bjhd->bhij', q, k)\n        # Scale it by $\\frac{1}{\\sqrt{d_k}}$\n        attn = attn * self.scale\n\n        # Apply masks if it's causal attention\n        attn = self.mask_attention(attn)\n\n        # Calculate attention probabilities\n        attn = self.softmax(attn)\n\n        # Get values\n        h = torch.einsum(\"bhij,bjhd->bihd\", attn, v)\n\n        # Change from shape `[batch_size, seq_len, n_heads, d_k]`\n        # to `[batch_size, seq_len, n_heads * d_k]`\n        h = h.reshape(*h.shape[:-2], -1)\n\n        # Apply final linear layer.\n        # The result will have shape `[batch_size, seq_len, d_model]`\n        h = self.output(h)\n\n        # Add the residual connection\n        return h + h_res\n\n\nclass CrossAttention(nn.Module):\n    \"\"\"\n    ## Cross-Attention Layer $\\text{C\\small{A}}$\n\n    This is similar to the self-attention layer defined above, except that\n    it gets keys and values from a different set of embeddings than the queries.\n\n    This is used in the encoder to encode the retrieved chunks based on the\n    input chunks.\n\n    *We do not use any explicit positional embeddings here.\n    We assume that the model can represent positional information in the embeddings implicitly.*\n    \"\"\"\n\n    def __init__(self, d_model: int, n_heads: int, d_k: int):\n        \"\"\"\n        * `d_model` is the number of features in transformer embeddings\n        * `n_heads` is the number of attention heads\n        * `d_k` is the number of features per head\n        \"\"\"\n        super().__init__()\n\n        self.n_heads = n_heads\n        self.d_k = d_k\n\n        # To scale attentions before softmax by $\\frac{1}{\\sqrt{d_k}}$\n        self.scale = 1 / math.sqrt(self.d_k)\n\n        # Linear layers for query, key and value heads.\n        self.query = nn.Linear(d_model, n_heads * d_k)\n        self.key = nn.Linear(d_model, n_heads * d_k)\n        self.value = nn.Linear(d_model, n_heads * d_k)\n\n        # Pre-norm layer for the query embeddings. The paper uses RMSNorm instead.\n        self.norm = nn.LayerNorm(d_model)\n\n        # Softmax for attention probabilities\n        self.softmax = nn.Softmax(dim=-1)\n\n        # Final linear layer\n        self.output = nn.Linear(n_heads * d_k, d_model)\n\n    def forward(self, e: torch.Tensor, h: torch.Tensor):\n        \"\"\"\n        * `e` are the retrieved nearest neighbor chunk embeddings with shape\n          `[batch_size, chunks, neighbors, neighbor_len, d_model]`\n        * `h` are the input chunks from which the nearest neighbors were retrieved with shape\n          `[batch_size, chunks, chunk_len, d_model]`. This is already normalized.\n        \"\"\"\n\n        # Residual connection\n        e_res = e\n\n        # Normalize retrieved chunks\n        e = self.norm(e)\n\n        # Get query from the retrieved chunks\n        q = self.query(e).view(*e.shape[:-1], self.n_heads, self.d_k)\n        # Get keys and values from the input chunks\n        k = self.key(h).view(*h.shape[:-1], self.n_heads, self.d_k)\n        v = self.value(h).view(*h.shape[:-1], self.n_heads, self.d_k)\n\n        # Calculate attention scores for all chunks.\n        # Each retrieved neighbor will pay attention to the original chunk that retrieved it.\n        # This will have shape `[batch_size, chunks, neighbors, n_heads, neighbor_len, chunk_len]`\n        attn = torch.einsum('bcnihd,bcjhd->bcnhij', q, k)\n        # Scale attention scores\n        attn = attn * self.scale\n\n        # Calculate softmax across the last dimension\n        attn = self.softmax(attn)\n\n        # Gather values\n        e = torch.einsum(\"bcnhij,bcjhd->bcnihd\", attn, v)\n\n        # Change from shape `[batch_size, chunks, neighbors, neighbor_len, n_heads, d_k]`\n        # to `[batch_size, chunks, neighbors, neighbor_len, n_heads * d_k]`\n        e = e.reshape(*e.shape[:-2], -1)\n\n        # Apply final linear layer.\n        # The result will have shape `[batch_size, chunks, neighbors, neighbor_len, d_model]`\n        e = self.output(e)\n\n        # Add residual connection\n        return e + e_res\n\n\nclass ChunkedCrossAttention(nn.Module):\n    \"\"\"\n    ## Chunked Cross-Attention Layer $\\text{C\\small{CA}}$\n\n    This is similar to the cross-attention layer defined above.\n\n    This is used in the decoder to pay attention to the retrieved neighbor chunks.\n\n    *We do not use any explicit positional embeddings here.\n    We assume that the model can represent positional information in the embeddings implicitly.*\n    \"\"\"\n\n    def __init__(self, d_model: int, n_heads: int, d_k: int, chunk_len: int):\n        \"\"\"\n        * `d_model` is the number of features in transformer embeddings\n        * `n_heads` is the number of attention heads\n        * `d_k` is the number of features per head\n        * `chunk_len` is the length of a chunk\n        \"\"\"\n\n        super().__init__()\n\n        self.chunk_len = chunk_len\n        self.n_heads = n_heads\n        self.d_k = d_k\n\n        # To scale attentions before softmax by $\\frac{1}{\\sqrt{d_k}}$\n        self.scale = 1 / math.sqrt(self.d_k)\n\n        # Linear layers for query, key and value heads.\n        self.query = nn.Linear(d_model, n_heads * d_k)\n        self.key = nn.Linear(d_model, n_heads * d_k)\n        self.value = nn.Linear(d_model, n_heads * d_k)\n\n        # Pre-norm layer for the query embeddings. The paper uses RMSNorm instead.\n        self.norm = nn.LayerNorm(d_model)\n\n        # Softmax for attention probabilities\n        self.softmax = nn.Softmax(dim=-1)\n\n        # Final linear layer\n        self.output = nn.Linear(n_heads * d_k, d_model)\n\n    def forward(self, h: torch.Tensor, e: torch.Tensor):\n        \"\"\"\n        `h` are the input embeddings of shape `[batch_size, seq_len, d_model]`\n        `e` are the retrieved nearest neighbors of shape `[batch_size, chunks, neighbors, neighbor_len, d_model]`\n        \"\"\"\n\n        # Get shape\n        batch_size, chunks, neighbors, neighbor_len, d_model = e.shape\n\n        # No attention if there are no chunks (for short inputs when sampling)\n        if chunks == 0:\n            return h\n\n        # Residual connection\n        h_res = h\n\n        # Remove the first `chunk_len - 1` embeddings.\n        # The input pays attention to neighbors retrieved and encoded using the past tokens only;\n        # so that there is no information leakage.\n        # That is the retrieved neighbors from the first chunks will have information from the first chunk.\n        # So by shifting the sequence to the left by `chunk_len - 1` we make sure that information only flows\n        # to the right.\n        h = h[:, self.chunk_len - 1:]\n        # Pre-norm\n        h = self.norm(h)\n        # Append empty embeddings to the end to be able to split the input into chunks\n        if h.shape[1] < chunks * self.chunk_len:\n            h = torch.cat((h, h.new_zeros(batch_size, chunks * self.chunk_len - h.shape[1], d_model)), dim=1)\n        # Reshape the input into chunks.\n        h = h.reshape(batch_size, chunks, self.chunk_len, d_model)\n\n        # Get query from the input\n        q = self.query(h).view(*h.shape[:-1], self.n_heads, self.d_k)\n        # Get keys and values from the retrieved neighbors\n        k = self.key(e).view(*e.shape[:-1], self.n_heads, self.d_k)\n        v = self.value(e).view(*e.shape[:-1], self.n_heads, self.d_k)\n\n        # Calculate attention scores for input chunks.\n        # Each chunk will pay attention to neighbors retrieved by the previous chunk.\n        # This will have shape `[batch_size, chunks, heads, chunk_len, neighbors, neighbor_len]`\n        attn = torch.einsum('bcihd,bcnjhd->bchinj', q, k)\n        # Scale attention scores\n        attn = attn * self.scale\n\n        # Apply softmax over the last two dimensions `neighbors, neighbor_len`\n        attn = self.softmax(attn.view(*attn.shape[:-2], -1)).view(attn.shape)\n\n        # Gather values\n        h = torch.einsum(\"bchinj,bcnjhd->bcihd\", attn, v)\n\n        # Change from shape `[batch_size, chunks, chunk_len, n_heads, d_k]`\n        # to `[batch_size, chunks * chunk_len, n_heads * d_k]`\n        h = h.reshape(batch_size, chunks * self.chunk_len, -1)\n\n        # Apply final linear layer.\n        # The result will have shape `[batch_size, chunks * chunk_len, d_model]`\n        h = self.output(h)\n\n        # Append `chunk_len - 1` zero embedding to the left; i.e. right shift it back\n        h = torch.cat((h.new_zeros(batch_size, self.chunk_len - 1, d_model), h), dim=1)\n\n        # Truncate and add the residual connection\n        return h[:, :h_res.shape[1]] + h_res\n\n\nclass FeedForward(nn.Module):\n    \"\"\"\n    ### Position-wise Feed Forward Layer $\\text{F\\small{FW}}$\n\n    This consists of two linear layers and an activation in the middle.\n    \"\"\"\n\n    def __init__(self, d_model: int, d_ff: int):\n        \"\"\"\n        * `d_model` is the number of features in transformer embeddings\n        * `d_ff` is the number features in the hidden layer\n        \"\"\"\n\n        super().__init__()\n\n        # The two linear layers\n        self.lin1 = nn.Linear(d_model, d_ff)\n        self.lin2 = nn.Linear(d_ff, d_model)\n\n        # ReLU Activation\n        self.act = nn.ReLU()\n\n        # Pre-norm layer\n        self.norm = nn.LayerNorm(d_model)\n\n    def forward(self, h: torch.Tensor):\n        \"\"\"\n        `h` are the embeddings of shape `[batch_size, seq_len, d_model]`\n        \"\"\"\n\n        # Residual\n        h_res = h\n        # Pre-norm\n        h = self.norm(h)\n        # First linear layer\n        h = self.lin1(h)\n        # Activation\n        h = self.act(h)\n        # Second linear layer\n        h = self.lin2(h)\n\n        # Add the residual connection\n        return h + h_res\n\n\nclass NearestNeighborEncoder(nn.Module):\n    \"\"\"\n    ## Nearest Neighbor Encoder $\\text{E\\small{NCODER}}(\\text{R\\small{ET}}(C_u)_{1 \\le u \\le l}, H)$\n\n    This module encodes the retrieved nearest neighbors\n    \"\"\"\n\n    def __init__(self, chunk_len: int, n_layers: int, ca_layers: Set[int],\n                 d_model: int, n_heads: int, d_k: int, d_ff: int):\n        \"\"\"\n        * `chunk_len` is the length of a chunk\n        * `n_layer` is the number of layers in the encoder $L_{\\text{enc}}$\n        * `ca_layers` are the layers with cross attention $P_{\\text{enc}}$\n        * `d_model` is the number of features in embeddings\n        * `n_heads` is the number of heads in attention layers\n        * `d_k` is the size of attention heads\n        * `d_ff` is the size of the feed-forward networks hidden layers\n        \"\"\"\n\n        super().__init__()\n        self.ca_layers = ca_layers\n        self.chunk_len = chunk_len\n        # Cross-attention layers\n        self.ca = nn.ModuleList([CrossAttention(d_model, n_heads, d_k) for _ in range(len(ca_layers))])\n        # Bi-directional self attention layers\n        self.attn = nn.ModuleList([SelfAttention(d_model, n_heads, d_k, is_causal=False) for _ in range(n_layers)])\n        # Feed forward layers\n        self.ffw = nn.ModuleList([FeedForward(d_model, d_ff) for _ in range(n_layers)])\n\n        # Pre-normalization layer for $H$\n        self.norm_h = nn.LayerNorm(d_model)\n\n    def forward(self, e: torch.Tensor, h: torch.Tensor):\n        \"\"\"\n        * `e` are token embeddings of the retrieved nearest neighbors,\n         $\\text{E\\small{MB}}\\big(\\text{R\\small{ET}}(C_u)_{1 \\le u \\le l}\\big)$\n         of shape `[batch_size, chunks, neighbors, neighbor_len, d_model]`\n\n        * `h` is are the input token embeddings, $H$\n         of shape `[batch_size, seq_len, d_model]`\n\n        *The chunks $u \\in [1, l]$ and neighbors $j \\in [1, k]$ are processed in parallel.*\n        \"\"\"\n\n        # Get shape\n        batch_size, chunks, neighbors, neighbor_len, d_model = e.shape\n\n        # $(H_u)_{u \\in [1, l]} \\leftarrow \\text{S\\small{PLIT}}(H)$\n        h_split = h[:, :self.chunk_len * chunks, :].reshape(batch_size, chunks, self.chunk_len, d_model)\n\n        # Pre-norm\n        h_split = self.norm_h(h_split)\n\n        # Keep the index of the cross attention layer\n        p_ca = 0\n        # For all layers $p' \\in [1, L_{\\text{enc}}]$\n        for p in range(len(self.attn)):\n            # Bi-directional self attention\n            # $E^j_u \\leftarrow \\text{A\\small{TTN}}_{\\text{enc}}(E^j_u)$\n            e = self.attn[p](e.view(-1, neighbor_len, d_model)).view(e.shape)\n\n            # Cross attention if $p' \\in P_{\\text{enc}}$\n            if p in self.ca_layers:\n                # $E^j_u \\leftarrow \\text{C\\small{A}}_{\\text{enc}}(E^j_u, H_u)$\n                e = self.ca[p_ca](e, h_split)\n                # Incremnt the cross attention index\n                p_ca += 1\n\n            # Feed forward layer $E^j_u \\leftarrow \\text{F\\small{FW}}_{\\text{enc}}(E^j_u)$\n            e = self.ffw[p](e)\n\n        # return $E$\n        return e\n\n\nclass RetroModel(nn.Module):\n    \"\"\"\n    ## Retro Model\n\n    This is the Retro decoder\n    \"\"\"\n\n    def __init__(self, n_vocab: int, d_model: int, n_layers: int, ca_layers: Set[int], chunk_len: int,\n                 n_heads: int, d_k: int, d_ff: int, encoder: NearestNeighborEncoder):\n        \"\"\"\n        * `v_vocab` is the number of tokens in the vocabulary\n        * `d_model` is the number of features in embeddings\n        * `n_layers` is the number of layers in the decoder $L$\n        * `ca_layers` are the layers with cross attention $P$\n        * `chunk_len` is the length of a chunk\n        * `n_heads` is the number of heads in attention layers\n        * `d_k` is the size of attention heads\n        * `d_ff` is the size of the feed-forward networks hidden layers\n        * `encoder` is the nearest neighbor encoder\n        \"\"\"\n        super().__init__()\n\n        self.ca_layers = ca_layers\n        self.encoder = encoder\n\n        # Token embedding layer\n        self.emb = nn.Embedding(n_vocab, d_model)\n        # Chunked cross attention layers $\\text{C\\small{CA}}$\n        self.cca = nn.ModuleList(\n            [ChunkedCrossAttention(d_model, n_heads, d_k, chunk_len) for _ in range(len(ca_layers))])\n        # Attention layers $\\text{A\\small{TTN}}$\n        self.attn = nn.ModuleList([SelfAttention(d_model, n_heads, d_k, is_causal=True) for _ in range(n_layers)])\n        # Feed forward layers $\\text{F\\small{FW}}$\n        self.ffw = nn.ModuleList([FeedForward(d_model, d_ff) for _ in range(n_layers)])\n        # Readout layer $\\text{R\\small{EAD}}$\n        self.read = nn.Linear(d_model, n_vocab)\n\n        # Pre-normalization layer for nearest neighbor embeddings from\n        # $\\text{E\\small{NCODER}}(\\text{R\\small{ET}}(C_u)_{1 \\le u \\le l}, H)$\n        self.norm_e = nn.LayerNorm(d_model)\n\n    def forward(self, x: torch.Tensor, ret: torch.Tensor):\n        \"\"\"\n        * `x` is the input sequence, $X$ of shape `[batch_size, seq_len]`\n        * `ret` are the retrieved neighbors\n         $\\text{R\\small{ET}}(C_u)_{1 \\le u \\le l}$\n         of shape `[batch_size, chunks, neighbors, neighbor_len]`\n        \"\"\"\n\n        # Get input embeddings $H \\leftarrow \\text{E\\small{MB}}(X)$\n        h = self.emb(x)\n\n        # Embeddings of the retrieved neighbors\n        # $E^j_u = \\text{E\\small{MB}}_{\\text{enc}}\\big(\\text{R\\small{ET}}(C_u)^j\\big)$.\n        #\n        # We use same embeddings for both input and neighbors\n        ret_emb = self.emb(ret)\n\n        # Keep index of the chunked cross attention layer\n        p_ca = 0\n        # For all layers $p \\in [1, L]$\n        for p in range(len(self.attn)):\n            # Causal self attention $H \\leftarrow \\text{A\\small{TTN}}(H)$\n            h = self.attn[p](h)\n\n            # Get encoder embeddings before the first $\\text{C\\small{CA}}$ layer,\n            # when $p = \\min(P)$\n            if self.ca_layers and p == min(self.ca_layers):\n                # $E = \\text{E\\small{NCODER}}(\\text{R\\small{ET}}(C_u)_{1 \\le u \\le l}, H)$\n                #\n                # We passed the embeddings of $\\text{R\\small{ET}}(C_u)_{1 \\le u \\le l}$ to encoder.\n                e = self.encoder(ret_emb, h)\n                # Normalize encoder embeddings\n                e = self.norm_e(e)\n\n            # Chunked-cross attention if $p \\in P$\n            if p in self.ca_layers:\n                # $H \\leftarrow \\text{C\\small{CA}}(H, E)$\n                h = self.cca[p_ca](h, e)\n                # Increment chunked cross-attention index\n                p_ca += 1\n\n            # $H \\leftarrow \\text{F\\small{FW}}(H)$\n            h = self.ffw[p](h)\n\n        # $O \\leftarrow \\text{R\\small{EAD}}(H)$\n        return self.read(h)\n\n\ndef _test():\n    \"\"\"\n    ### Test the model with fake data\n    \"\"\"\n    chunk_len = 4\n    d_model = 8\n    d_ff = 32\n    n_heads = 2\n    d_k = 4\n\n    device = torch.device('cuda:0')\n\n    m = RetroModel(5, d_model, 6, {2, 5}, chunk_len, n_heads, d_k, d_ff,\n                   encoder=NearestNeighborEncoder(chunk_len, 2, {1}, d_model, n_heads, d_k, d_ff))\n\n    m.to(device)\n    x = [1, 2, 4, 4, 0, 1, 2, 3, 4, 3]\n    ret = [\n        [[0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1]],\n        [[0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1]],\n    ]\n    res = m(torch.tensor([x] * 10).to(device), torch.tensor([ret] * 10).to(device))\n\n    inspect(res)\n\n\n#\nif __name__ == '__main__':\n    _test()\n", "labml_nn/transformers/retro/train.py": "\"\"\"\n---\ntitle: RETRO training\nsummary: >\n  Training RETRO model with Tiny Shakespeare dataset\n---\n\n# RETRO training\n\nThis is the training code for\n [RETRO](index.html).\n\"\"\"\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, RandomSampler\n\nfrom labml import monit, lab, tracker, experiment, logger\nfrom labml.logger import Text\nfrom labml_helpers.datasets.text import TextFileDataset\nfrom labml_nn.optimizers.noam import Noam\nfrom labml_nn.transformers.retro import model as retro\nfrom labml_nn.transformers.retro.dataset import Dataset, RetroIndex\nfrom labml_nn.transformers.retro.model import RetroModel, NearestNeighborEncoder\n\n\nclass Sampler:\n    \"\"\"\n    ## Sampler\n\n    This class greedily samples from a model.\n    \"\"\"\n\n    def __init__(self, device: torch.device, model: retro.RetroModel, tds: TextFileDataset, chunk_len: int):\n        \"\"\"\n        * `device` is the device of the model\n        * `model` is the [Retro mode](retro.html)\n        * `tds` is the text dataset (used to get neighbor chunks)\n        * `chunk_len` is the length of a chunk\n        \"\"\"\n        self.chunk_len = chunk_len\n        self.tds = tds\n        self.model = model\n        self.device = device\n\n        # [Retro index](database.html)\n        self.index = RetroIndex()\n\n    def retrieve_nearest_neighbours(self, chunk: str):\n        \"\"\"\n        ### Retrieve nearest neighbors of a given chunk\n        \"\"\"\n\n        # Retrieve the offsets of the nearest neighbors\n        neighbor_offsets = self.index([chunk], None)\n\n        # Get the neighbors (with neighbor length equal to `chunk_len * 2`)\n        text = self.tds.train\n        neighbors = [text[j: j + self.chunk_len * 2] for j in neighbor_offsets[0]]\n\n        #\n        return neighbors\n\n    def sample(self, prompt: str, sample_len: int):\n        \"\"\"\n        ### Sample text from the given prompt\n        \"\"\"\n\n        # To store nearest neighbors as strings\n        neighbors_str = []\n\n        # Sampled text\n        sampled = ''\n\n        # Sample `sample_len` tokens\n        for i in range(sample_len):\n            # We need to retrieve neighbors,\n            # if there are more sampled chunks than we have already retrieved for\n            while len(neighbors_str) < len(prompt) // self.chunk_len:\n                # Get the last chunk for which we haven't retrieved neighbors\n                off = len(neighbors_str) * self.chunk_len\n                chunk = prompt[off: off + self.chunk_len]\n                # Retrieve nearest neighbors\n                neighbors_str.append(self.retrieve_nearest_neighbours(chunk))\n\n            # Tokenize the input\n            src = self.tds.text_to_i(prompt)\n            # Tokenize the retrieved neighbors\n            neighbors = torch.stack([torch.stack([self.tds.text_to_i(n) for n in chunk]) for chunk in neighbors_str])\n\n            # Move them to the same device as the model\n            src = src.to(self.device)\n            neighbors = neighbors.to(self.device)\n\n            # Get model output\n            res = self.model(src[None, :], neighbors[None, :, :, :])\n\n            # Greedily sample the last token\n            token = res[0, -1, :].argmax(dim=-1)\n\n            # Add the sampled token text to the prompt and sample text\n            prompt += self.tds.itos[token.item()]\n            sampled += self.tds.itos[token.item()]\n\n        #\n        return sampled\n\n\nclass Trainer:\n    \"\"\"\n    ## Retro trainer\n    \"\"\"\n\n    def __init__(self, device: torch.device, model: retro.RetroModel,\n                 dataloader: DataLoader, optimizer: torch.optim.Optimizer):\n        \"\"\"\n        * `device` is the device of the model\n        * `model` is the [Retro mode](retro.html)\n        * `dataloader` is the dataloader for the [dataset with pre-retrieved neighbors](dataset.html)\n        * `optimizer` is the optimizer\n        \"\"\"\n        self.optimizer = optimizer\n        self.device = device\n        self.dataloader = dataloader\n        self.model = model\n        self.loss_func = nn.CrossEntropyLoss()\n\n    def __call__(self):\n        \"\"\"\n        ### Train the model for an epoch\n        \"\"\"\n\n        # Iterate through training data\n        for i, (src, tgt, neighbors) in monit.enum('Train', self.dataloader):\n            # Move data to the device\n            src, tgt, neighbors = src.to(self.device), tgt.to(self.device), neighbors.to(self.device)\n\n            # Forward pass\n            res = self.model(src, neighbors)\n            # Calculate loss\n            loss = self.loss_func(res.view(-1, res.shape[-1]), tgt.view(-1))\n\n            # Clear the gradients\n            self.optimizer.zero_grad()\n            # Backward pass\n            loss.backward()\n            # Optimize the model\n            self.optimizer.step()\n\n            # Save training statistics and increment the global step counter\n            tracker.save({'loss.train': loss})\n            tracker.add_global_step(len(src))\n\n\ndef train():\n    \"\"\"\n    ## Create and train a small model\n    \"\"\"\n\n    # Create an experiment\n    experiment.create(name='retro_small')\n\n    # GPU device\n    device = torch.device('cuda:0')\n\n    # Load Tiny Shakespeare dataset\n    tds = TextFileDataset(\n        lab.get_data_path() / 'tiny_shakespeare.txt',\n        list,\n        url='https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n\n    # Load [Retro dataset](dataset.html)\n    train_dataset = Dataset(lab.get_data_path() / 'retro_train_dataset.json', tds)\n\n    # Create dataloader\n    train_dl = DataLoader(train_dataset,\n                          batch_size=4,\n                          sampler=RandomSampler(train_dataset, replacement=True))\n\n    # Hyper-parameters\n    chunk_len = 16\n    d_model = 128\n    d_ff = 512\n    n_heads = 16\n    d_k = 16\n\n    # Create the nearest neighbor encoder\n    nearest_neighbor_encoder = NearestNeighborEncoder(chunk_len, 6, {3}, d_model, n_heads, d_k, d_ff)\n    # Create the model\n    model = RetroModel(tds.n_tokens, d_model, 6,\n                       {3, 5},\n                       chunk_len, n_heads, d_k, d_ff,\n                       encoder=nearest_neighbor_encoder)\n    # Move the model to the device\n    model = model.to(device)\n    # Create the optimizer\n    optimizer = Noam(model.parameters(), lr=1., d_model=d_model, warmup=2_000)\n    # Create the `Trainer`\n    trainer = Trainer(device, model, train_dl, optimizer)\n    # Create the `Sampler`\n    sampler = Sampler(device, model, tds, chunk_len)\n    #\n    prompt = '''Second Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:'''\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models(model=model)\n\n    # Start the experiment\n    with experiment.start():\n        # Train for `32` epochs\n        for epoch in monit.loop(32):\n            # Train\n            trainer()\n            # Print a new line\n            tracker.new_line()\n            # Sample from the `prompt`\n            logger.log([(prompt.replace('\\n', '\\\\n\\n'), Text.subtle),\n                        (sampler.sample(prompt, 128).replace('\\n', '\\\\n\\n'), Text.none)])\n            # Save models\n            experiment.save_checkpoint()\n\n\n#\nif __name__ == '__main__':\n    train()\n", "labml_nn/transformers/retro/__init__.py": "\"\"\"\n---\ntitle: Retrieval-Enhanced Transformer (Retro)\nsummary: >\n  This is a PyTorch implementation/tutorial of the paper\n  Improving language models by retrieving from trillions of tokens.\n  It builds a key-value database of chunks of text and retrieves and uses them when\n  making predictions.\n---\n\n# Retrieval-Enhanced Transformer (Retro)\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426).\n\nIt builds a database of chunks of text.\nIt is a key-value database where the keys are indexed by the BERT embeddings of the chunks.\nThey use a frozen pre-trained BERT model to calculate these embeddings.\nThe values are the corresponding chunks and an equal length of text proceeding that chunk.\n\nThen the model retrieves text similar (nearest neighbors) to the input to the model from this database.\nThese retrieved texts are used to predict the output.\n\nSince we use a frozen BERT model for retrieval we can pre-calculate all the nearest neighbors for the\ntraining dataset.\nThis speeds up the training process.\n\nComponents:\n\n* [BERT embeddings](bert_embeddings.html): Code to get BERT embeddings of chunks of text.\n* [Key-value database](database.html): Build and retrieve chunks\n* [Model](model.html)\n* [Dataset](dataset.html): Pre-calculate the nearest neighbors\n* [Training code](train.html)\n\"\"\"", "labml_nn/transformers/retro/bert_embeddings.py": "\"\"\"\n---\ntitle: BERT Embeddings of chunks of text\nsummary: >\n  Generate BERT embeddings for chunks using a frozen BERT model\n---\n\n# BERT Embeddings of chunks of text\n\nThis is the code to get BERT embeddings of chunks for [RETRO model](index.html).\n\"\"\"\n\nfrom typing import List\n\nimport torch\nfrom transformers import BertTokenizer, BertModel\n\nfrom labml import lab, monit\n\n\nclass BERTChunkEmbeddings:\n    \"\"\"\n    ## BERT Embeddings\n\n    For a given chunk of text $N$ this class generates BERT embeddings $\\text{B\\small{ERT}}(N)$.\n    $\\text{B\\small{ERT}}(N)$ is the average of BERT embeddings of all the tokens in $N$.\n    \"\"\"\n\n    def __init__(self, device: torch.device):\n        self.device = device\n\n        # Load the BERT tokenizer from [HuggingFace](https://huggingface.co/bert-base-uncased)\n        with monit.section('Load BERT tokenizer'):\n            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n                                                           cache_dir=str(\n                                                               lab.get_data_path() / 'cache' / 'bert-tokenizer'))\n\n        # Load the BERT model from [HuggingFace](https://huggingface.co/bert-base-uncased)\n        with monit.section('Load BERT model'):\n            self.model = BertModel.from_pretrained(\"bert-base-uncased\",\n                                                   cache_dir=str(lab.get_data_path() / 'cache' / 'bert-model'))\n\n            # Move the model to `device`\n            self.model.to(device)\n\n    @staticmethod\n    def _trim_chunk(chunk: str):\n        \"\"\"\n        In this implementation, we do not make chunks with a fixed number of tokens.\n        One of the reasons is that this implementation uses character-level tokens and BERT\n        uses its sub-word tokenizer.\n\n        So this method will truncate the text to make sure there are no partial tokens.\n\n        For instance, a chunk could be like `s a popular programming la`, with partial\n        words (partial sub-word tokens) on the ends.\n        We strip them off to get better BERT embeddings.\n        As mentioned earlier this is not necessary if we broke chunks after tokenizing.\n        \"\"\"\n        # Strip whitespace\n        stripped = chunk.strip()\n        # Break words\n        parts = stripped.split()\n        # Remove first and last pieces\n        stripped = stripped[len(parts[0]):-len(parts[-1])]\n\n        # Remove whitespace\n        stripped = stripped.strip()\n\n        # If empty return original string\n        if not stripped:\n            return chunk\n        # Otherwise, return the stripped string\n        else:\n            return stripped\n\n    def __call__(self, chunks: List[str]):\n        \"\"\"\n        ### Get $\\text{B\\small{ERT}}(N)$ for a list of chunks.\n        \"\"\"\n\n        # We don't need to compute gradients\n        with torch.no_grad():\n            # Trim the chunks\n            trimmed_chunks = [self._trim_chunk(c) for c in chunks]\n\n            # Tokenize the chunks with BERT tokenizer\n            tokens = self.tokenizer(trimmed_chunks, return_tensors='pt', add_special_tokens=False, padding=True)\n\n            # Move token ids, attention mask and token types to the device\n            input_ids = tokens['input_ids'].to(self.device)\n            attention_mask = tokens['attention_mask'].to(self.device)\n            token_type_ids = tokens['token_type_ids'].to(self.device)\n            # Evaluate the model\n            output = self.model(input_ids=input_ids,\n                                attention_mask=attention_mask,\n                                token_type_ids=token_type_ids)\n\n            # Get the token embeddings\n            state = output['last_hidden_state']\n            # Calculate the average token embeddings.\n            # Note that the attention mask is `0` if the token is empty padded.\n            # We get empty tokens because the chunks are of different lengths.\n            emb = (state * attention_mask[:, :, None]).sum(dim=1) / attention_mask[:, :, None].sum(dim=1)\n\n            #\n            return emb\n\n\ndef _test():\n    \"\"\"\n    ### Code to test BERT embeddings\n    \"\"\"\n    from labml.logger import inspect\n\n    # Initialize\n    device = torch.device('cuda:0')\n    bert = BERTChunkEmbeddings(device)\n\n    # Sample\n    text = [\"Replace me by any text you'd like.\",\n            \"Second sentence\"]\n\n    # Check BERT tokenizer\n    encoded_input = bert.tokenizer(text, return_tensors='pt', add_special_tokens=False, padding=True)\n\n    inspect(encoded_input, _expand=True)\n\n    # Check BERT model outputs\n    output = bert.model(input_ids=encoded_input['input_ids'].to(device),\n                        attention_mask=encoded_input['attention_mask'].to(device),\n                        token_type_ids=encoded_input['token_type_ids'].to(device))\n\n    inspect({'last_hidden_state': output['last_hidden_state'],\n             'pooler_output': output['pooler_output']},\n            _expand=True)\n\n    # Check recreating text from token ids\n    inspect(bert.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0]), _n=-1)\n    inspect(bert.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][1]), _n=-1)\n\n    # Get chunk embeddings\n    inspect(bert(text))\n\n\n#\nif __name__ == '__main__':\n    _test()\n", "labml_nn/transformers/hour_glass/experiment.py": "\"\"\"\n---\ntitle:  Hierarchical Transformers Are More Efficient Language Models Experiment\nsummary: This experiment trains a hourglass model on Tiny Shakespeare dataset.\n---\n\n# [Hierarchical Transformers Are More Efficient Language Models](index.html) Experiment\n\nThis is an annotated PyTorch experiment to train a [hourglass](index.html).\n\nThis is based on\n[training loop and configurations for a simple transformer auto-regressive NLP task](../basic/autoregressive_experiment.html).\n\"\"\"\nimport math\nfrom typing import List\n\nimport torch\nfrom torch import nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers.hour_glass import HourGlass\nfrom labml_nn.transformers.positional_encoding import PositionalEncoding\n\n\nclass AutoregressiveTransformer(Module):\n    \"\"\"\n    ## Autoregressive language model\n    \"\"\"\n\n    def __init__(self, n_tokens: int, d_model: int, dropout: float, hour_glass: HourGlass):\n        \"\"\"\n        * `n_tokens` is the vocabulary size\n        * `d_model` is the size of the token embeddings\n        * `dropout` is the dropout probability\n        * `hour_glass` is the [hourglass model](index.html)\n        \"\"\"\n        super().__init__()\n        # Token embeddings\n        self.embedding = nn.Embedding(n_tokens, d_model)\n        # [Fixed positional embeddings](../positional_encoding.html).\n        #\n        # \ud83d\udcdd The\n        # [official paper implementation](https://github.com/google/trax/blob/master/trax/models/research/hourglass.py)\n        # use [relative attention](../xl/relative_mha.html)\n        self.pos_embedding = PositionalEncoding(d_model, dropout)\n        # [hourglass model](index.html)\n        self.hour_glass = hour_glass\n        # To normalize the final embeddings\n        self.norm = nn.LayerNorm([d_model])\n        # Embedding size\n        self.d_model = d_model\n        # Final linear layer to predict the logits\n        self.output = nn.Linear(d_model, n_tokens)\n\n    def __call__(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is the tensor with token indexes of shape `[seq_len, batch_size]`\n        \"\"\"\n        # Get embeddings\n        x = self.embedding(x)\n\n        # Add [positional embeddings](../positional_encoding.html)\n        if self.pos_embedding is not None:\n            x = self.pos_embedding(x * math.sqrt(self.d_model))\n\n        # Hourglass\n        x = self.hour_glass(x)\n\n        # Get logits\n        output = self.output(self.norm(x))\n\n        # Return the logits\n        return output, None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [training loop and configurations for a simple transformer auto-regressive NLP task](../basic/autoregressive_transformer.html).\n    \"\"\"\n    # Model\n    model: AutoregressiveTransformer\n    # Number of attention heads\n    n_heads: int = 8\n    # Dropout probability\n    dropout: float = 0.1\n    # Size of feed-forward hidden layer\n    d_ff: int = 512\n    # Token embedding size\n    d_model: int = 256\n    # Shortening factors\n    shortening_factors: List[int] = [8, 4]\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    Create the model\n    \"\"\"\n\n    # Create hourglass model\n    hour_glass = HourGlass(c.n_heads, c.d_model, c.dropout, c.d_ff, c.shortening_factors)\n    # Create the auto-regressive wrapper\n    m = AutoregressiveTransformer(c.n_tokens, c.d_model, c.dropout, hour_glass).to(c.device)\n\n    #\n    return m\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"hour_glass\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 256,\n        # Train for $128$ epochs\n        'epochs': 128,\n        # Batch size $32$\n        'batch_size': 32,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        # Use [Noam optimizer](../../optimizers/noam.html)\n        'optimizer.optimizer': 'Noam',\n        'optimizer.learning_rate': 1.,\n        #\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/hour_glass/__init__.py": "\"\"\"\n---\ntitle: Hierarchical Transformers Are More Efficient Language Models\nsummary: >\n  This is an annotated implementation/tutorial of hourglass model in PyTorch.\n---\n\n# Hierarchical Transformers Are More Efficient Language Models\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[Hierarchical Transformers Are More Efficient Language Models](https://arxiv.org/abs/2110.13711).\n\nThis paper introduces a hierarchical transformer architecture to handle long sequences\nefficiently. The first half of the transformer layers down-sample tokens and the second\nhalf up-samples with direct skip connections between layers of the same resolution.\nThis is a little similar to [U-Net](../../diffusion/ddpm/unet.html) for vision tasks.\n\nThey try different up-sampling and down-sampling techniques and build a model\nwith the best performing up and down-sampling techniques which they call the\nhourglass model.\n\nHere we have implemented the simplest up-sampling and down-sampling techniques for simplicity.\nWe will consider adding more complex (and better performing) implementations later.\n\nHere is [the training code](experiment.html) for the hourglass model.\n\"\"\"\n\nfrom typing import List\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.transformers import MultiHeadAttention, TransformerLayer\nfrom labml_nn.transformers.feed_forward import FeedForward\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass HourGlass(Module):\n    \"\"\"\n    ## Hourglass model\n\n    This model recursively adds layers to the middle while shortening the sequence by down-sampling.\n    The shortened sequence processed by another hourglass model is sandwiched between two normal transformer\n    layers. (A transformer layer has a [self-attention layer](../mha.html)\n     and a [position-wise feed-forward layer](../feed_forward.html)).\n    \"\"\"\n\n    def __init__(self, n_heads: int, d_model: int, dropout: float, d_ff: int, shortening_factors: List[int]):\n        \"\"\"\n        * `n_heads` is the number of heads in [multi-head attention layers](../mha.html)\n        * `d_model` is the size of the token embeddings\n        * `dropout` is the dropout probability\n        * `d_ff` is the dimensionality of the hidden layer in [position-wise feed-forward layers](../feed_forward.html)\n        * `shortening_factors` is the list of shortening factors\n        \"\"\"\n        super().__init__()\n\n        # The transformer layer before down-sampling\n        self.pre = TransformerLayer(d_model=d_model,\n                                    # [Multi-head attention layer](../mha.html)\n                                    self_attn=MultiHeadAttention(n_heads, d_model, dropout),\n                                    # [Position wise feed-forward layers](.. / feed_forward.html)\n                                    feed_forward=FeedForward(d_model, d_ff, dropout),\n                                    #\n                                    dropout_prob=dropout)\n        # Auto-regressive mask\n        self.mask = AutoregressiveMask()\n\n        # The shortening factor $k$ (or the down-sampling rate)\n        k = shortening_factors[0]\n\n        # We shift the tokens to the right by $k - 1$ steps to make sure\n        # information doesn't leak from the future tokens to past tokens\n        # as a result of down-sampling and up-sampling\n        self.shift_right = ShiftRight(k - 1)\n        # Shortening or the down-sampling layer. We use the simplest form - average pooling.\n        # The paper shows that attention based down sampling works best, which we haven't implemented yet.\n        self.shortening = AvgPoolShortening(k)\n\n        # If there are no more shortening (middle of the hourglass)\n        if len(shortening_factors) == 1:\n            # The center layer is another transformer layer\n            self.shortened = TransformerLayer(d_model=d_model,\n                                              self_attn=MultiHeadAttention(n_heads, d_model, dropout),\n                                              feed_forward=FeedForward(d_model, d_ff, dropout),\n                                              dropout_prob=dropout)\n            # Autoregressive mask\n            self.mask_short = AutoregressiveMask()\n            self.hour_glass = None\n        else:\n            # Insert another hourglass model recursively\n            self.hour_glass = HourGlass(n_heads, d_model, dropout, d_ff, shortening_factors[1:])\n\n        # Up-sampling layer. We use naive up-sampling for simplicity and the paper shows attention based up sampling\n        # works better.\n        self.up_sampling = NaiveUpSampling(k)\n\n        # The final transformer layer after up-sampling\n        self.post = TransformerLayer(d_model=d_model,\n                                     self_attn=MultiHeadAttention(n_heads, d_model, dropout),\n                                     feed_forward=FeedForward(d_model, d_ff, dropout),\n                                     dropout_prob=dropout)\n\n    def forward(self, x: torch.Tensor):\n        # Initial transformer layer\n        # $$x \\leftarrow PreVanillaLayers(x)$$\n        x = self.pre(x=x, mask=self.mask(x))\n        # Shifting and shortening\n        # $$x' \\leftarrow Shortening(ShiftRight(x,k\u22121),k)$$\n        x_short = self.shortening(self.shift_right(x))\n\n        # If we are at the center of the hourglass,\n        # $$\\textbf{\\small if } \\text{\\small E\\scriptsize MPTY}(shorten\\_factors) \\textbf{\\small then}$$\n        if self.hour_glass is None:\n            # Center transformer layer\n            # $$x' \\leftarrow ShortenedLayers(x')$$\n            x_short = self.shortened(x=x_short, mask=self.mask_short(x_short))\n        # $$\\textbf{else}$$\n        else:\n            # $$x' \\leftarrow \\text{\\small H\\scriptsize OURGLASS}(x, shorten\\_factors)$$\n            x_short = self.hour_glass(x_short)\n\n        # Up-sample the shortened sequence and add a skip connection\n        # $$x \\leftarrow x + Upsampling(x, x', k)$$\n        x = x + self.up_sampling(x, x_short)\n        # Final transformer layer\n        # $$x \\leftarrow PostVanillaLayers(x)$$\n        x = self.post(x=x, mask=self.mask(x))\n\n        #\n        return x\n\n\nclass ShiftRight(Module):\n    \"\"\"\n    ### Shift right operation\n\n    This shifts the sequence to the right by the given number of steps\n    \"\"\"\n\n    def __init__(self, shift: int):\n        \"\"\"\n        * `shift` is the number of steps to shift by\n        \"\"\"\n        super().__init__()\n        # cannot be negative\n        assert shift >= 0\n        #\n        self.shift = shift\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is a tensor of shape `[seq_len, ...]`\n        \"\"\"\n        # If the shift is $0$ return the original\n        if self.shift == 0:\n            return x\n        # Zeros to be appended to the left\n        prefix = x.new_zeros([self.shift, *x.shape[1:]])\n        # Concatenate the zeros and truncate the right\n        return torch.cat([prefix, x[:-self.shift]])\n\n\nclass AvgPoolShortening(Module):\n    \"\"\"\n    ### Average pool shortening\n\n    This down-samples by a given factor with average pooling\n    \"\"\"\n\n    def __init__(self, k: int):\n        \"\"\"\n        * `k` is the shortening factor\n        \"\"\"\n        super().__init__()\n        # Average pooling layer\n        self.pool = nn.AvgPool1d(k, ceil_mode=True)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        * `x` is of shape `[seq_len, batch_size, d_model]`\n        \"\"\"\n        # Pooling layer accepts shape `[batch_size, d_model, seq_len]` so we\n        # permute axes.\n        return self.pool(x.permute(1, 2, 0)).permute(2, 0, 1)\n\n\nclass NaiveUpSampling(Module):\n    \"\"\"\n    ### Naive up-sampling\n\n    This up-samples by repeating\n    \"\"\"\n\n    def __init__(self, k: int):\n        \"\"\"\n        * `k` is the shortening factor\n        \"\"\"\n        super().__init__()\n        self.k = k\n\n    def forward(self, x: torch.Tensor, x_short: torch.Tensor):\n        \"\"\"\n        * `x` is the tensor with embeddings before down-sampling\n        * `x_short` is the tensor of higher density (to be up-sampled) representations\n        \"\"\"\n        # Repeat across the sequence dimension\n        expanded = torch.repeat_interleave(x_short, self.k, dim=0)\n        # Truncate the extra embeddings at the end\n        expanded = expanded[:x.shape[0]]\n\n        #\n        return expanded\n\n\nclass AutoregressiveMask(Module):\n    \"\"\"\n    ### Generate auto-regressive mask\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.mask = None\n\n    def forward(self, x: torch.Tensor):\n        # Create a mask if we haven't created or sizes have changed\n        if self.mask is None or self.mask.size(0) != len(x):\n            # [Subsequent mask](../utils.html), will mask out tokens from seeing future tokens\n            self.mask = subsequent_mask(len(x)).to(x.device)\n\n        #\n        return self.mask\n\n\nclass LinearPoolingShortening(Module):\n    \"\"\"\n    ### \ud83d\udea7 Linear pooling for down-sampling\n\n    This concatenates the consecutive tokens embeddings that need to be merged and do a linear\n    transformation to map it to the size of a single token embedding.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        raise NotImplementedError\n\n\nclass AttentionBasedShortening(Module):\n    \"\"\"\n    ### \ud83d\udea7 Down-sampling with attention\n\n    \\begin{align}\n    x' &= S(x) + Attention \\Big(Q=S(x),K = x, V =x \\Big) \\\\\n    x' &= x' + FFN(x')\n    \\end{align}\n\n    where $S(x)$ is average pooling or linear pooling.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        raise NotImplementedError\n\n\nclass LinearUpSampling(Module):\n    \"\"\"\n    ### \ud83d\udea7 Linear projection for up-sampling\n\n    Make a linear projection of dense token embeddings to a size of $d_{\\text{model}} k$.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        raise NotImplementedError\n\n\nclass AttentionBasedUpSampling(Module):\n    \"\"\"\n    ### \ud83d\udea7 Attention based up-sampling\n\n    \\begin{align}\n    x &= U(x,x') + Attention \\Big(Q=U(x,x'),K = x', V = x' \\Big) \\\\\n    x &= x + FFN(x)\n    \\end{align}\n\n    where $U(x,x') = x + LinearUpsampling(x')$\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        raise NotImplementedError\n", "labml_nn/transformers/aft/experiment.py": "\"\"\"\n---\ntitle: Attention Free Transformer (AFT) Experiment\nsummary: This experiment trains an Attention Free Transformer (AFT) based model on Tiny Shakespeare dataset.\n---\n\n# [Attention Free Transformer (AFT)](index.html) Experiment\n\nThis is an annotated PyTorch experiment to train a [AFT model](index.html).\n\nThis is based on\n[general training loop and configurations for auto-regressive NLP task](../../experiments/nlp_autoregression.html).\n\"\"\"\nimport torch\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers import TransformerConfigs, Encoder\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass AutoregressiveTransformer(Module):\n    \"\"\"\n    ## Simple autoregressive model\n\n    This consists of a token embedding layer, transformer encoder, and\n    a final linear layer that gives token logits.\n    \"\"\"\n\n    def __init__(self, encoder: Encoder, src_embed: Module, generator: Module):\n        \"\"\"\n        * `encoder` is the transformer [Encoder](../models.html#Encoder)\n        * `src_embed` is the token\n        [embedding module (with positional encodings)](../models.html#EmbeddingsWithLearnedPositionalEncoding)\n        * `generator` is the [final fully connected layer](../models.html#Generator) that gives the logits.\n        \"\"\"\n        super().__init__()\n        self.src_embed = src_embed\n        self.encoder = encoder\n        self.generator = generator\n\n        # The mask will be initialized on the first call\n        self.mask = None\n\n    def forward(self, x: torch.Tensor):\n        # Create subsequent mask if mask is not initialized\n        # or if the size of the mask is different\n        if self.mask is None or self.mask.size(0) != len(x):\n            # Subsequent mask, will mask out tokens from seeing future tokens\n            self.mask = subsequent_mask(len(x)).to(x.device)\n\n        # Get the token embeddings with positional encodings\n        x = self.src_embed(x)\n        # Transformer encoder\n        x = self.encoder(x, self.mask)\n        # Get logits\n        x = self.generator(x)\n\n        # Return results\n        # (second value is for state, since our trainer is used with RNNs also)\n        return x, None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`NLPAutoRegressionConfigs`](../../experiments/nlp_autoregression.html#NLPAutoRegressionConfigs)\n    \"\"\"\n\n    # GPT model\n    model: AutoregressiveTransformer\n    # Transformer\n    transformer: TransformerConfigs\n\n    local_window_size: int = 32\n\n\n@option(Configs.transformer, 'Transformer')\ndef _transformer_configs(c: Configs):\n    \"\"\"\n    ### Transformer configurations\n    \"\"\"\n\n    # We use our\n    # [configurable transformer implementation](../configs.html#TransformerConfigs)\n    conf = TransformerConfigs()\n    # Set the vocabulary sizes for embeddings and generating logits\n    conf.n_src_vocab = c.n_tokens\n    conf.n_tgt_vocab = c.n_tokens\n    # Set the embedding size\n    conf.d_model = c.d_model\n    # Replace self-attention with an [AFT Local Module](index.html)\n    from labml_nn.transformers.aft import AFTLocal\n    conf.encoder_attn = AFTLocal(c.d_model, c.seq_len, c.local_window_size)\n\n    #\n    return conf\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    Create an auto-regressive model\n    \"\"\"\n    m = AutoregressiveTransformer(c.transformer.encoder,\n                                  c.transformer.src_embed,\n                                  c.transformer.generator).to(c.device)\n\n    return m\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"aft\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $128$\n        'seq_len': 256,\n        # Train for $32$ epochs\n        'epochs': 128,\n        # Batch size $128$\n        'batch_size': 32,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        # Embedding size\n        'd_model': 128,\n        # FFN hidden dimension size\n        'transformer.ffn.d_ff': 256,\n\n        # Optimizer\n        'optimizer.optimizer': 'Noam',\n        'optimizer.learning_rate': 1.,\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/aft/__init__.py": "\"\"\"\n---\ntitle: An Attention Free Transformer\nsummary: >\n  This is an annotated implementation/tutorial of the AFT (Attention Free Transformer) in PyTorch.\n---\n\n# An Attention Free Transformer\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[An Attention Free Transformer](https://arxiv.org/abs/2105.14103).\n\nThis paper replaces the [self-attention layer](../mha.html) with a new efficient operation,\nthat has memory complexity of $\\mathcal{O}(Td)$, where $T$ is the sequence length\nand $d$ is the dimensionality of embeddings.\n\nThe paper introduces AFT along with AFT-local and AFT-conv.\nHere we have implemented AFT-local which pays attention to closeby tokens\nin an autoregressive model.\n\n## Attention Free Transformer\n\nAFT (similar to [MHA](../mha.html)) first transforms the embeddings $X$ into\nquery $Q = XW^Q$, key $K = XW^K$ and value $V = XW^V$ tensors with learned weights.\nThe output for each position $t \\in [1, T]$ is calculated with the following operation.\n\n$$Y_t = \\sigma(Q_t) \\odot\n \\frac{\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'}) \\odot V_{t'}}\n {\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'})}$$\n\n, where $\\odot$ is element-wise product, $\\sigma$ is a non-linearity (sigmoid) and\n$w \\in \\mathbb{R}^{T \\times T}$ is a learned matrix of pair-wise position biases.\n\nThis means that we take the weighted average of values\nand multiply them by the query. This eliminates the need to calculate the $T \\times T$ attention\nmatrix that [MHA](../mha.html) requires, and therefore reduce the memory requirement.\n\n## AFT Local\n\nAFT Local only apply learned pair-wise position biases locally:\n\n\\begin{align}\nw'_{t,t'} =\n\\begin{cases}\nw_{t,t'},  & {\\text{for } \\lvert t-t' \\rvert \\lt s} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\end{align}\n\n, where $s \\le T$ is the local window size.\n\nAlthough $w'_{t,t'}$ is $0$ outside the local window the AFT operation still uses key-value pairs from\nother areas. This is different from local transformers where embeddings outside the local window are\n completely not visible.\n\nHere is [the training code](experiment.html) for a AFT Local model.\n\"\"\"\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass AFTLocal(Module):\n    \"\"\"\n    ### AFT Local Operation\n\n    $$Y_t = \\sigma(Q_t) \\odot\n     \\frac{\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'}) \\odot V_{t'}}\n     {\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'})}$$\n\n    where,\n\n    \\begin{align}\n    w'_{t,t'} =\n    \\begin{cases}\n    w_{t,t'},  & {\\text{for } \\lvert t-t' \\rvert \\lt s} \\\\\n    0, & \\text{otherwise}\n    \\end{cases}\n    \\end{align}\n    \"\"\"\n\n    def __init__(self, d_model: int, seq_len: int, local_window_size: int, bias: bool = True):\n        \"\"\"\n        * `d_model` is the number of features in the `query`, `key` and `value` vectors.\n        * `seq_len` is $T$\n        * `local_window_size` is the local window size $s$\n        * `bias` is whether to have a bias parameter for transformations for $Q$, $K$ and $V$.\n        \"\"\"\n\n        super().__init__()\n\n        # Local window size $s$\n        self.local_window_size = local_window_size\n        # These transform the `query`, `key` and `value` vectors.\n        self.query = nn.Linear(d_model, d_model, bias=bias)\n        self.key = nn.Linear(d_model, d_model, bias=bias)\n        self.value = nn.Linear(d_model, d_model, bias=bias)\n        # Pair-wise positional biases $w \\in \\mathbb{R}^{T \\times T}$\n        self.pos_bias = nn.Parameter(torch.zeros(seq_len, seq_len), requires_grad=True)\n        # Mask for $w_{t,t'}$\n        self.local_mask = nn.Parameter(self.create_local_mask(seq_len, local_window_size), requires_grad=False)\n        # Activation $\\sigma$\n        self.activation = nn.Sigmoid()\n        # Output layer\n        self.output = nn.Linear(d_model, d_model)\n\n    @staticmethod\n    def create_local_mask(seq_len, local_window_size):\n        \"\"\"\n        #### Create local mask\n\n        This creates a mask for\n\n        \\begin{align}\n        m_{t,t'} =\n        \\begin{cases}\n        1,  & {\\text{for } \\lvert t-t' \\rvert \\lt s} \\\\\n        0, & \\text{otherwise}\n        \\end{cases}\n        \\end{align}\n        \"\"\"\n\n        # Initialize to ones\n        local_mask = torch.ones(seq_len, seq_len, dtype=torch.bool)\n        # Make $t' - t \\ge s$ zero\n        local_mask = torch.tril(local_mask, local_window_size - 1)\n        # Make $t - t' \\ge s$ zero\n        local_mask = torch.triu(local_mask, -(local_window_size - 1))\n\n        #\n        return local_mask\n\n    def forward(self, *,\n                query: torch.Tensor,\n                key: torch.Tensor,\n                value: torch.Tensor,\n                mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        `query`, `key` and `value` are the tensors that store\n        collection of token embeddings for  *query*, *key* and *value*.\n        They have shape `[seq_len, batch_size, d_model]`.\n\n        `mask` has shape `[seq_len, seq_len, batch_size]` and\n        `mask[i, j, b]` indicates whether for batch `b`,\n        query at position `i` has access to key-value at position `j`.\n        \"\"\"\n\n        # `query`, `key` and `value`  have shape `[seq_len, batch_size, d_model]`\n        seq_len, _, _ = query.shape\n\n        if mask is not None:\n            # `mask` has shape `[seq_len_q, seq_len_k, batch_size]`,\n            # where first dimension is the query dimension.\n            # If the query dimension is equal to $1$ it will be broadcasted.\n            assert mask.shape[0] == 1 or mask.shape[0] == query.shape[0]\n            assert mask.shape[1] == key.shape[0]\n            assert mask.shape[2] == 1 or mask.shape[2] == query.shape[1]\n\n        # Transform query, key and value embeddings\n        query = self.query(query)\n        key = self.key(key)\n        value = self.value(value)\n\n        # Get\n        #\n        #     \\begin{align}\n        #     w'_{t,t'} =\n        #     \\begin{cases}\n        #     w_{t,t'},  & {\\text{for }\\lvert t-t' \\rvert \\lt s} \\\\\n        #     0, & \\text{otherwise}\n        #     \\end{cases}\n        #     \\end{align}\n        #\n        # using the mask\n        pos_bias = self.pos_bias[:seq_len, :seq_len] * self.local_mask[:seq_len, :seq_len]\n        pos_bias = pos_bias.unsqueeze(-1)\n        pos_bias.masked_fill_(~mask, float('-inf'))\n\n        # \\begin{align}\n        # Y_t &= \\sigma(Q_t) \\odot\n        # \\frac{\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'}) \\odot V_{t'}}\n        # {\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'})} \\\\\n        # &= \\sigma(Q_t) \\odot\n        #    \\frac{\\sum_{t'=1}^T \\exp(w_{t,t'}) \\odot \\exp(K_{t'}) \\odot V_{t'}}\n        #    {\\sum_{t'=1}^T \\exp(w_{t,t'}) \\odot \\exp(K_{t'})}\n        # \\end{align}\n        #\n        # We compute $\\exp(w_{t,t'})$, $\\exp(K_{t'}) \\odot V_{t'}$ and $\\exp(K_{t'})$\n        # separately and do a matrix multiplication. We use einsum for clarity.\n\n        # We subtract $\\max_{t'}(K_{t'})$ and $\\max_{t'}(w_{t,t'})$ before calculating the exponents to stabilize\n        # the softmax calculation.\n        #\n        # If $x_i$ is large $\\exp(x_i)$ becomes huge and the computation of\n        # $\\frac{\\sum\\exp(x_i)y_i}{\\sum\\exp(x_i)}$becomes unstable.\n        # Subtracting a constant before calculating the exponent from numerator and denominator will cancel out.\n        # and can help stabilize the computation.\n        # So we subtract $\\max(x_i)$ to stabilize the computation.\n        max_key = key.max(dim=0, keepdims=True)[0]\n        max_pos_bias = pos_bias.max(dim=1,  keepdims=True)[0]\n\n        # $\\exp \\big(K_{t'}- \\max_{t'}(K_{t'})\\big)$\n        exp_key = torch.exp(key - max_key)\n        # $\\exp \\big(w_{t,t'} - \\max_{t'}(w_{t,t'})\\big)$\n        exp_pos_bias = torch.exp(pos_bias - max_pos_bias)\n\n        # The numerator part $\\sum_{t'=1}^T \\exp(w_{t,t'}) \\odot \\exp(K_{t'}) \\odot V_{t'}$\n        num = torch.einsum('ijb,jbd->ibd', exp_pos_bias, exp_key * value)\n        # The denominator part $\\sum_{t'=1}^T \\exp(w_{t,t'}) \\odot \\exp(K_{t'})$\n        den = torch.einsum('ijb,jbd->ibd', exp_pos_bias, exp_key)\n\n        # Output $$Y_t = \\sigma(Q_t) \\odot\n        #         \\frac{\\sum_{t'=1}^T \\exp(w_{t,t'}) \\odot \\exp(K_{t'}) \\odot V_{t'}}\n        #         {\\sum_{t'=1}^T \\exp(w_{t,t'}) \\odot \\exp(K_{t'})}$$\n        y = self.activation(query) * num / den\n\n        # Output layer\n        return self.output(y)\n\n\ndef _test_local_mask():\n    \"\"\"\n    Test local mask\n    \"\"\"\n    from labml.logger import inspect\n    inspect(AFTLocal.create_local_mask(10, 4))\n\n\n#\nif __name__ == '__main__':\n    _test_local_mask()\n", "labml_nn/transformers/feedback/experiment.py": "\"\"\"\n---\ntitle: Train Feedback Transformer\nsummary: This is training code with notes for a feedback transformer.\n---\n\n# Train Feedback Transformer\n\nThis trains a [feedback transformer](index.html) model for auto-regression.\nYou can pick the original feedback transformer or the new version\nwhere the keys and values are precalculated.\n\nHere's a Colab notebook for training a feedback transformer on Tiny Shakespeare dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/feedback/experiment.ipynb)\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml.utils.pytorch import get_modules\nfrom labml_helpers.module import Module\n\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers import Encoder, Generator, TransformerConfigs\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass AutoregressiveModel(Module):\n    \"\"\"\n    ## Auto regressive model\n    \"\"\"\n\n    def __init__(self, n_vocab: int, d_model: int, transformer: Module):\n        super().__init__()\n        # Token embedding module\n        self.src_embed = nn.Embedding(n_vocab, d_model)\n        self.transformer = transformer\n        self.generator = nn.Linear(d_model, n_vocab)\n\n    def forward(self, x: torch.Tensor):\n        # Embed the tokens\n        x = self.src_embed(x)\n        # Run it through the the transformer\n        res = self.transformer(x)\n        # Generate logits of the next token\n        return self.generator(res), None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    The default configs can and will be over-ridden when we start the experiment\n    \"\"\"\n\n    model: AutoregressiveModel\n\n    d_model: int = 512\n    heads: int = 8\n    dropout: float = 0.0\n    d_ff: int = 2048\n    n_layers: int = 6\n\n\n@option(Configs.model)\ndef feedback_transformer(c: Configs):\n    \"\"\"\n    Create [original feedback transformer](index.html).\n    \"\"\"\n    from labml_nn.transformers.feedback import FeedbackTransformer, FeedbackTransformerLayer, \\\n        FeedbackAttention, FeedForward\n\n    return AutoregressiveModel(\n        c.n_tokens, c.d_model,\n        FeedbackTransformer(\n            FeedbackTransformerLayer(d_model=c.d_model,\n                                     attn=FeedbackAttention(c.heads, c.d_model, c.dropout),\n                                     feed_forward=FeedForward(c.d_model, c.d_ff, c.dropout),\n                                     dropout_prob=c.dropout),\n            c.n_layers)).to(c.device)\n\n\n@option(Configs.model)\ndef feedback_transformer_kv(c: Configs):\n    \"\"\"\n    Create [updated feedback transformer](index.html#kv_shared), with precalculated keys and values.\n    \"\"\"\n    from labml_nn.transformers.feedback import FeedbackTransformerKV, FeedbackTransformerLayer, \\\n        FeedbackAttention, FeedForward\n\n    return AutoregressiveModel(\n        c.n_tokens, c.d_model,\n        FeedbackTransformerKV(\n            FeedbackTransformerLayer(d_model=c.d_model,\n                                     attn=FeedbackAttention(c.heads, c.d_model, c.dropout,\n                                                            is_kv_precomputed=True),\n                                     feed_forward=FeedForward(c.d_model, c.d_ff, c.dropout),\n                                     dropout_prob=c.dropout),\n            c.n_layers, c.d_model, c.heads)).to(c.device)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"feedback_transformer\")\n    # Create configs\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf,\n                       # A dictionary of configurations to override\n                       {'tokenizer': 'character',\n                        'text': 'tiny_shakespeare',\n                        'optimizer.learning_rate': 1.0,\n                        'optimizer.optimizer': 'Noam',\n                        'prompt': 'It is',\n                        'prompt_separator': '',\n\n                        # Use `feedback_transformer` for original feedback transformer\n                        'model': 'feedback_transformer_kv',\n\n                        'train_loader': 'shuffled_train_loader',\n                        'valid_loader': 'shuffled_valid_loader',\n\n                        'seq_len': 128,\n                        'epochs': 128,\n                        'batch_size': 64,\n                        'inner_iterations': 25})\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models(get_modules(conf))\n\n    # Start the experiment\n    with experiment.start():\n        # Run the training loop\n        conf.run()\n\n\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/feedback/__init__.py": "\"\"\"\n---\ntitle: Feedback Transformer\nsummary: >\n  This is an annotated implementation/tutorial the Feedback Transformer in PyTorch.\n---\n\n# Feedback Transformer\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[Accessing Higher-level Representations in Sequential Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402).\n\nNormal transformers process tokens in parallel. Each transformer layer pays attention\nto the outputs of the previous layer.\nFeedback transformer pays attention to the output of all layers in previous steps.\nSo this adds recurrence, and we need to process token-by-token.\nThis slows down the training significantly (about 5X - 10X depending on the sequence length).\nHowever, when predicting Feedback Transformer is faster because you can predict the next token\nif you cache the memory vectors.\n\nIn order to speed up the training, the paper discusses starting with a short sequence length and\ngradually increasing it.\nThey also discuss using a pretrained parallel transformer as the starting point.\n\nThe original feedback transformer doesn't keep the outputs of all layers.\nInstead it keeps weighted sum of the output of all layers.\nThis reduces the memory used for caching during prediction.\nThe first half of this file implements this.\n\nThe updated feedback transformer shares weights $W^l_k$ and $W^l_v$ used\nto calculate keys and values among the layers.\nWe then calculate the keys and values for each step only once and keep\nthem cached.\nThe [second half](#shared_kv) of this file implements this.\nWe implemented a custom PyTorch function to improve performance.\n\nHere's [the training code](experiment.html) and a notebook for training a feedback transformer on Tiny Shakespeare dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/feedback/experiment.ipynb)\n\"\"\"\n\nimport math\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.transformers.feed_forward import FeedForward\nfrom labml_nn.transformers.mha import PrepareForMultiHeadAttention\nfrom labml_nn.utils import clone_module_list\n\n\nclass FeedbackAttention(Module):\n    r\"\"\"\n    ## Feedback Attention\n\n\n    This module computes recurrent attention similar to attention from original transformers\n    paper.\n\n    $$\\mathop{Attention}(Q, K, V) = \\underset{seq}{\\mathop{softmax}}\\Bigg(\\frac{Q^\\top K}{\\sqrt{d_k}}\\Bigg)V$$\n    \"\"\"\n\n    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1, *,\n                 is_kv_precomputed: bool = False):\n        \"\"\"\n        * 'heads' is the number of attention heads\n        * `d_model` is the number of features in the transformer\n        * `dropout_prob` is the attention dropout probability\n        * `is_kv_precomputed` is whether key, value tensors are already calculated\n        \"\"\"\n\n        super().__init__()\n\n        # Number of features per head\n        self.d_k = d_model // heads\n        #\n        self.heads = heads\n\n        # These transform the `query` multi-headed attention.\n        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=False)\n        # These transform the `key` and `value` for multi-headed attention.\n        if not is_kv_precomputed:\n            self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=False)\n            self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=True)\n        # Keys and values are already calculated\n        else:\n            self.key = None\n            self.value = None\n\n        # Output layer\n        self.output = nn.Linear(d_model, d_model)\n        # Dropout\n        self.dropout = nn.Dropout(dropout_prob)\n        # Scaling factor before the softmax\n        self.scale = 1 / math.sqrt(self.d_k)\n\n        # Softmax for attention along the time dimension of `key`\n        self.softmax = nn.Softmax(dim=0)\n\n        # Number of relative positions\n        self.P = 2 ** 12\n\n        # Relative positional embeddings for key relative to the query.\n        self.key_pos_embeddings = nn.Parameter(torch.zeros((self.P, heads, self.d_k)), requires_grad=True)\n        # Relative positional embedding bias for key relative to the query.\n        self.key_pos_bias = nn.Parameter(torch.zeros((self.P, heads)), requires_grad=True)\n        # Positional embeddings for the query is independent of the position of the query\n        self.query_pos_bias = nn.Parameter(torch.zeros((heads, self.d_k)), requires_grad=True)\n\n        # We store attentions so that it can be used for logging, or other computations if needed\n        self.attn = None\n\n    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n        r\"\"\"\n        ### Get attention scores\n\n        We use relative positional encodings for attention, similar\n        to [relative multi-head attention form Transformer-XL paper](../relative_mha.html).\n\n        Attention from current step's query to key in step $j$ (relative to current step) is,\n\n        \\begin{align}\n        A_{j} &= Q^\\top K_j \\\\\n            &= lin_q(X^q + P_q)^\\top lin_k(X^k_j + P_j) \\\\\n            &= (Q + U^Q)^\\top(K_j + U^K_j) \\\\\n            &= \\underset{\\textcolor{lightgreen}{A}}{Q^\\top K_j} +\n               \\underset{\\textcolor{lightgreen}{B}}{Q^\\top U^K_j} +\n               \\underset{\\textcolor{lightgreen}{C}}{{U^Q}^\\top K_j} +\n               \\underset{\\textcolor{lightgreen}{D}}{{U^Q}^\\top U^K_j}\n        \\end{align}\n\n        where $Q, K_j$, are linear transformations of\n         original embeddings $X^q, X^k_j$\n         and $U^Q, U^K_j$ are linear transformations of\n         positional encodings $P_q, P_j$.\n\n        We replace term $\\textcolor{lightgreen}{D}$ with $S_j$.\n        \"\"\"\n\n        # $U^K_j$\n        key_pos_emb = self.key_pos_embeddings[-key.shape[0]:]\n        # $U^Q$\n        query_pos_bias = self.query_pos_bias[None, :, :]\n        # $S_j$\n        key_pos_bias = self.key_pos_bias[-key.shape[0]:]\n\n        # $\\underset{\\textcolor{lightgreen}{A}}{Q^\\top K_j} + \\underset{\\textcolor{lightgreen}{C}}{{U^Q}^\\top K_j}$\n        ac = torch.einsum('bhd,jbhd->jbh', query + query_pos_bias, key)\n        # $\\underset{\\textcolor{lightgreen}{B}}{Q^\\top U^K_j} + \\underset{\\textcolor{lightgreen}{D}}{S_j}$\n        bd = torch.einsum('bhd,jhd->jbh', query, key_pos_emb) + key_pos_bias[:, None, :]\n\n        # $A_j$\n        return ac + bd\n\n    def forward(self, *,\n                query: torch.Tensor,\n                key: torch.Tensor,\n                value: torch.Tensor):\n        \"\"\"\n        * `query` has shape `[batch_size, d_model]`\n        * `key` and `value` has shape `[seq_len, batch_size, d_model]`\n        \"\"\"\n\n        # Prepare `query`, `key` and `value` for attention computation\n        # `key` and `value`  will then have shape `[seq_len, batch_size, heads, d_k]`\n        # and `query` will have shape `[batch_size, heads, d_k]`\n        query = self.query(query)\n        if self.key:\n            key = self.key(key)\n        if self.value:\n            value = self.value(value)\n\n        # Compute attention scores.\n        # Results in a tensor of shape `[seq_len, batch_size, heads]`\n        scores = self.get_scores(query, key)\n\n        # Scale scores $\\frac{1}{\\sqrt{d_k}}$\n        scores *= self.scale\n\n        # Softmax\n        attn = self.softmax(scores)\n\n        # Apply dropout\n        attn = self.dropout(attn)\n\n        # Multiply by the values\n        x = torch.einsum(\"jbh,jbhd->bhd\", attn, value)\n\n        # Concatenate multiple heads\n        x = x.reshape(x.shape[0], -1)\n\n        # Output layer\n        return self.output(x)\n\n\nclass FeedbackTransformerLayer(Module):\n    \"\"\"\n    ## Feedback Transformer Layer\n\n    This implements a single transformer layer in the feedback transformer.\n    \"\"\"\n\n    def __init__(self, *,\n                 d_model: int,\n                 attn: FeedbackAttention,\n                 feed_forward: FeedForward,\n                 dropout_prob: float):\n        \"\"\"\n        * `d_model` is the number of features in the transformer\n        * `attn` is the feedback attention module\n        * `feed_forward` is the position-wise feed forward layer\n        * `dropout_prob` is the dropout probability for dropout layers after attention and feed-forward\n        \"\"\"\n        super().__init__()\n        # Transformer size $d_{model}$\n        self.size = d_model\n        #\n        self.attn = attn\n        self.feed_forward = feed_forward\n        self.dropout = nn.Dropout(dropout_prob)\n\n        # Normalization layers\n        self.norm_self_attn = nn.LayerNorm([d_model])\n        self.norm_ff = nn.LayerNorm([d_model])\n\n    def forward(self, *,\n                x: torch.Tensor,\n                key: Optional[torch.Tensor],\n                value: Optional[torch.Tensor]):\n        # If there is memory\n        if key is not None:\n            # Normalize the vectors before doing self attention\n            z = self.norm_self_attn(x)\n            # Run through self attention, i.e. keys and values are from self\n            self_attn = self.attn(query=z, key=key, value=value)\n            # Add the self attention results\n            x = x + self.dropout(self_attn)\n\n        # Normalize for feed-forward\n        z = self.norm_ff(x)\n        # Pass through the feed-forward network\n        ff = self.feed_forward(z)\n        # Add the feed-forward results back\n        x = x + self.dropout(ff)\n\n        #\n        return x\n\n\nclass FeedbackTransformer(Module):\n    \"\"\"\n    ## Feedback Transformer Module\n    \"\"\"\n\n    def __init__(self, layer: FeedbackTransformerLayer, n_layers: int):\n        \"\"\"\n        * `layer` is the feedback transformer layer, which we clone for each layer\n        * `n_layers` is the number of layers in the transformer\n        \"\"\"\n\n        super().__init__()\n        # Make copies of the transformer layer\n        self.layers = clone_module_list(layer, n_layers)\n        # Final normalization layer\n        self.norm = nn.LayerNorm([layer.size])\n        # Memory vectors are computed as a weighted sum of representations of each layer.\n        # This is the weights parameter for that.\n        self.weights = nn.Parameter(torch.ones(n_layers + 1), requires_grad=True)\n        # Softmax for weights before taking the weighted sum\n        self.softmax = nn.Softmax(0)\n\n    def forward(self, x_seq: torch.Tensor):\n        \"\"\"\n        * `x_seq` is the input with shape `[seq_len, batch_size, d_model]`\n        \"\"\"\n\n        # Split the input to a list along the sequence axis\n        x_seq = torch.unbind(x_seq, dim=0)\n        # List to store the outputs\n        res = []\n        # List to store the memory vectors\n        mem = []\n        # For each input step\n        for x in x_seq:\n            # List to store layer outputs\n            layer_outputs = [x]\n\n            # If there is memory, stack them into a vector\n            mem_tensor = torch.stack(mem) if mem else None\n\n            # Run through each layer\n            for layer in self.layers:\n                # Get layer output\n                x = layer(x=x, key=mem_tensor, value=mem_tensor)\n                # Append them to the list of layer outputs\n                layer_outputs.append(x)\n\n            # Stack the layer outputs to a tensor\n            layer_outputs = torch.stack(layer_outputs)\n            # Calculate the memory vector as a weighted sum of layer outputs\n            mem.append(torch.einsum('lbd,l->bd', layer_outputs, self.softmax(self.weights)))\n            # Append the output to results\n            res.append(x)\n\n        # Stack the output tensors\n        res = torch.stack(res)\n        # Normalize the output\n        return self.norm(res)\n\n\n# <a id=\"shared_kv\"></a>\n#\n# # Shared keys and values among layers\n\nclass StackFunction(torch.autograd.Function):\n    \"\"\"\n    ### Stack Function implementation\n\n    We implement a custom function instead of appending to a python list\n    and then doing `torch.stack`.\n    This greatly improves the performance over calling `torch.stack` at\n    each step along the sequence.\n    Everytime `torch.stack` is called, it creates a new tensor, while\n    this method and the accompanying class `Stack` share memory for each step.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, memory, memory_grad, last, n):\n        \"\"\"\n        * `ctx` is the context of the function (which lets us cache stuff)\n        * `memory` is the shared memory tensor where we stack and store the values of each step (keys & values)\n        * `memory_grad` is the shared memory tensor to store and accumulate gradients of each step\n        * `last` is the last value stacked\n        * `n` is the number of steps (i.e. size of the stack)\n\n        This returns the stacked tensor for steps upto `n`.\n        \"\"\"\n\n        # Cache accumulated gradients\n        ctx._mem_grad = memory_grad\n        # Cache the size of the stack\n        ctx._n = n\n        # Return the stack\n        return memory[:n + 1]\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        * `grad_output` is the gradient with respect to the output of about `forward` function\n\n        This accumulates the gradients in the shared memory tensor and return the\n        gradients with respect to the `last` result in the stack.\n        \"\"\"\n        # Get the current size of the stack\n        n = ctx._n\n        # Get the accumulated gradients\n        memory_grad = ctx._mem_grad\n        # Add the gradients\n        memory_grad[:n + 1] += grad_output\n        # Return the gradients w.r.t to last value in the stack\n        return None, None, memory_grad[n], None\n\n\nclass Stack:\n    \"\"\"\n    ### Stack Module\n\n    This uses the stack function defined above, and does the necessary initializations.\n    \"\"\"\n\n    def __init__(self, max_len: int):\n        \"\"\"\n        * `max_len` is the maximum size of the stack\n        \"\"\"\n        self.max_len = max_len\n        self.memory = None\n        self.memory_grad = None\n        self.last = None\n        self.n = -1\n        self.last_get_n = -1\n\n    def append(self, n: int, value: torch.Tensor):\n        \"\"\"\n        * `n` is the size of the stack\n        * `value` is the tensor that needs to be added to the stack\n        \"\"\"\n\n        # You need to get (use) the stack after adding a value.\n        # Otherwise this implementation fails\n        assert n == 0 or self.last_get_n == n - 1, f\"{n}, {self.last_get_n}\"\n\n        # Do this without gradients\n        with torch.no_grad():\n            # Initialize the shared memory tensor to keep the stack\n            if self.memory is None or self.memory.shape[1:] != value.shape:\n                # This should only happen when the stack is empty\n                assert n == 0\n                # Create a tensor for the stack\n                self.memory = value.new_zeros(self.max_len, *value.shape, requires_grad=False)\n                # Create a tensor to accumulate the gradients\n                self.memory_grad = value.new_zeros(self.memory.shape, requires_grad=False)\n            # The memory is already initialized but we are resetting the stack.\n            #\n            # This could have been another function like `reset`, but\n            # we found this easier to use.\n            elif n == 0:\n                # Reset accumulated gradients\n                self.memory_grad.fill_(0.)\n\n            # Set the value in the correct position of the stack\n            self.memory.data[n] = value.detach()\n            # Keep track of the stack (for debugging)\n            self.n = n\n\n        # Keep track of the last value added to the stack.\n        # We need this to be passed on to `StackFunction` in order\n        # to get the gradients propagated backwards.\n        self.last = value\n\n    def get(self):\n        \"\"\"\n        Returns the stack\n        \"\"\"\n\n        # Keep track of the size of the stack when it was used.\n        # This is used for a sanity check in `append`.\n        self.last_get_n = self.n\n        # Take it all through `StackFunction` so that `StackFunction.backwards`\n        # is called by PyTorch during backpropagation.\n        return StackFunction.apply(self.memory, self.memory_grad, self.last, self.n)\n\n    def free(self):\n        \"\"\"\n        To release memory\n        \"\"\"\n\n        self.memory = None\n        self.memory_grad = None\n        self.last = None\n\n\nclass FeedbackTransformerKV(Module):\n    \"\"\"\n    ## Updated Feedback Transformer Module\n\n    This is the updated feedback transformer module that caches the keys and values.\n    \"\"\"\n\n    def __init__(self, layer: FeedbackTransformerLayer, n_layers: int, d_model: int, heads: int):\n        \"\"\"\n        * `layer` is the feedback transformer layer, which we clone for each layer\n        * `n_layers` is the number of layers in the transformer\n        * `d_model` is the number of features in the transformer\n        * 'heads' is the number of attention heads\n        \"\"\"\n\n        super().__init__()\n        # Make copies of the transformer layer\n        self.layers = clone_module_list(layer, n_layers)\n        # Final normalization layer\n        self.norm = nn.LayerNorm([layer.size])\n        # Memory vectors are computed as a weighted sum of representations of each layer.\n        # This is the weights parameter for that.\n        self.weights = nn.Parameter(torch.ones(n_layers + 1), requires_grad=True)\n        # Softmax for weights before taking the weighted sum\n        self.softmax = nn.Softmax(0)\n\n        # Number of features in a head\n        d_k = d_model // heads\n        # Module to transform embeddings (memory) to get keys\n        self.key = PrepareForMultiHeadAttention(d_model, heads, d_k, bias=False)\n        # Module to transform embeddings (memory) to get keys\n        self.value = PrepareForMultiHeadAttention(d_model, heads, d_k, bias=False)\n\n        # Memory for stacked keys\n        self.mem_key = Stack(512)\n        # Memory for stacked values\n        self.mem_value = Stack(512)\n\n    def forward(self, x_seq: torch.Tensor):\n        \"\"\"\n        * `x_seq` is the input with shape `[seq_len, batch_size, d_model]`\n        \"\"\"\n\n        # Split the input to a list along the sequence axis\n        x_seq = torch.unbind(x_seq, dim=0)\n        # List to store the outputs\n        res = []\n        # For each input step\n        for step, x in enumerate(x_seq):\n            # List to store layer outputs\n            layer_outputs = [x]\n\n            # Stack of keys and values\n            key_tensor = None\n            value_tensor = None\n            # Get the keys and values tensors if we are beyond the initial step\n            if step > 0:\n                key_tensor = self.mem_key.get()\n                value_tensor = self.mem_value.get()\n\n            # Run through each layer\n            for layer in self.layers:\n                # Get layer output\n                x = layer(x=x, key=key_tensor, value=value_tensor)\n                # Append them to the list of layer outputs\n                layer_outputs.append(x)\n\n            # Stack the layer outputs to a tensor\n            layer_outputs = torch.stack(layer_outputs)\n            # Calculate the memory vector as a weighted sum of layer outputs\n            mem = torch.einsum('lbd,l->bd', layer_outputs, self.softmax(self.weights))\n            # Calculate the keys from memory and add it to the stack\n            self.mem_key.append(step, self.key(mem))\n            # Calculate the values from memory and add it to the stack\n            self.mem_value.append(step, self.value(mem))\n            # Append the output to results\n            res.append(x)\n\n        # Stack the output tensors\n        res = torch.stack(res)\n        # Normalize the output\n        return self.norm(res)\n\n    def free(self):\n        self.mem_key.free()\n        self.mem_value.free()\n", "labml_nn/transformers/basic/autoregressive_experiment.py": "\"\"\"\n---\ntitle: Transformer Auto-Regression Experiment\nsummary: >\n  This trains a simple transformer model on NLP auto-regression.\n---\n\n# Transformer Auto-Regression Experiment\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/basic/autoregressive_experiment.ipynb)\n\nThis trains a simple transformer introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\non an NLP auto-regression task (with Tiny Shakespeare dataset).\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers import TransformerConfigs, Encoder\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass AutoregressiveTransformer(nn.Module):\n    \"\"\"\n    ## Auto-Regressive model\n    \"\"\"\n    def __init__(self, encoder: Encoder, src_embed: nn.Module, generator: nn.Module):\n        \"\"\"\n        * `encoder` is the transformer [Encoder](../models.html#Encoder)\n        * `src_embed` is the token\n        [embedding module (with positional encodings)](../models.html#EmbeddingsWithLearnedPositionalEncoding)\n        * `generator` is the [final fully connected layer](../models.html#Generator) that gives the logits.\n        \"\"\"\n        super().__init__()\n        self.src_embed = src_embed\n        self.encoder = encoder\n        self.generator = generator\n\n        # The mask will be initialized on the first call\n        self.mask = None\n\n    def forward(self, x: torch.Tensor):\n        # Create subsequent mask if mask is not initialized\n        # or if the size of the mask is different\n        if self.mask is None or self.mask.size(0) != len(x):\n            # Subsequent mask, will mask out tokens from seeing future tokens\n            self.mask = subsequent_mask(len(x)).to(x.device)\n        # Get the token embeddings with positional encodings\n        x = self.src_embed(x)\n        # Transformer encoder\n        x = self.encoder(x, self.mask)\n        # Get logits\n        x = self.generator(x)\n\n        # Return results\n        # (second value is for state, since our trainer is used with RNNs also)\n        return x, None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`NLPAutoRegressionConfigs`](../../experiments/nlp_autoregression.html#NLPAutoRegressionConfigs)\n    \"\"\"\n\n    # GPT model\n    model: AutoregressiveTransformer\n    # Transformer\n    transformer: TransformerConfigs\n\n\n@option(Configs.transformer, 'Transformer')\ndef _transformer_configs(c: Configs):\n    \"\"\"\n    ### Transformer configurations\n    \"\"\"\n\n    # We use our\n    # [configurable transformer implementation](../configs.html#TransformerConfigs)\n    conf = TransformerConfigs()\n    # Set the vocabulary sizes for embeddings and generating logits\n    conf.n_src_vocab = c.n_tokens\n    conf.n_tgt_vocab = c.n_tokens\n    #\n    conf.d_model = c.d_model\n\n    #\n    return conf\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    Create GPT model and initialize weights\n    \"\"\"\n    m = AutoregressiveTransformer(c.transformer.encoder,\n                                  c.transformer.src_embed,\n                                  c.transformer.generator).to(c.device)\n\n    return m\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"transformer\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 512,\n        # Train for 32 epochs\n        'epochs': 32,\n        # Batch size $32$\n        'batch_size': 16,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        # Model size\n        'd_model': 256,\n        'transformer.n_heads': 16,\n        'transformer.ffn.d_ff': 1024,\n\n        # Use [Noam optimizer](../../optimizers/noam.html)\n        'optimizer.optimizer': 'Noam',\n        'optimizer.learning_rate': 1.,\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/basic/with_sophia.py": "\"\"\"\n---\ntitle: Transformer Auto-Regression Experiment with [Sophia-G optimizer](../../optimizers/sophia.html)\nsummary: >\n  This trains a simple transformer model on NLP auto-regression with Sophia-G optimizer.\n---\n\n# Transformer Auto-Regression Experiment with [Sophia-G optimizer](../../optimizers/sophia.html)\n\nThis trains a simple transformer introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\non an NLP auto-regression task (with Tiny Shakespeare dataset) with [Sophia-G optimizer](../../optimizers/sophia.html).\n\"\"\"\nimport torch\n\nfrom labml import experiment, tracker\nfrom labml_helpers.train_valid import BatchIndex\nfrom labml_nn.optimizers.sophia import Sophia\nfrom labml_nn.transformers.basic.autoregressive_experiment import Configs as TransformerAutoRegressionConfigs\n\n\nclass Configs(TransformerAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from [`Configs`](autoregressive_experiment.html)\n    \"\"\"\n\n    hess_interval: int = 10\n\n    optimizer: Sophia\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training or validation step with Gauss-Newton-Bartlett (GNB) Hessian diagonal estimator\n        \"\"\"\n\n        # Set training/eval mode\n        self.model.train(self.mode.is_train)\n\n        # Move data to the device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Estimate the Hessian diagonal every $k$ steps\n        if isinstance(self.optimizer, Sophia) and self.mode.is_train and batch_idx.idx % self.hess_interval == 0:\n            # Get model outputs\n            output, *_ = self.model(data)\n\n            # Create a categorical distribution from logits\n            samp_dist = torch.distributions.Categorical(logits=output)\n            # Sample $\\hat{y}$\n            y_sample = samp_dist.sample()\n\n            # Calculate and log loss\n            loss = self.loss_func(output, y_sample)\n            tracker.add(\"loss.hess.\", loss)\n\n            # Calculate gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Update EMA Hessian diagonal\n            #\n            # \\begin{align}\n            # \\hat{h}_t &= B \\cdot \\nabla_\\theta \\hat{L} (\\theta) \\odot \\nabla_\\theta \\hat{L} (\\theta) \\\\\n            # h_t &= \\beta_2 h_{t-k} + (1 - \\beta_2) \\hat{h}_t\n            # \\end{align}\n            self.optimizer.update_hessian(data.numel())\n            # Clear the gradients\n            self.optimizer.zero_grad()\n        else:\n            # Move data to the device\n            data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n            # Update global step (number of tokens processed) when in training mode\n            if self.mode.is_train:\n                tracker.add_global_step(data.shape[0] * data.shape[1])\n\n            # Whether to capture model outputs\n            with self.mode.update(is_log_activations=batch_idx.is_last and self.is_log_model_activations):\n                # Get model outputs.\n                # It's returning a tuple for states when using RNNs.\n                # This is not implemented yet. \ud83d\ude1c\n                output, *_ = self.model(data)\n\n            # Calculate and log loss\n            loss = self.loss_func(output, target)\n            tracker.add(\"loss.\", loss)\n\n            # Calculate and log accuracy\n            self.accuracy(output, target)\n            self.accuracy.track()\n\n            self.other_metrics(output, target)\n\n            # Train the model\n            if self.mode.is_train:\n                # Calculate gradients\n                loss.backward()\n                # Clip gradients\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n                # Take optimizer step\n                self.optimizer.step()\n                # Log the model parameters and gradients on last batch of every epoch\n                if batch_idx.is_last and self.is_log_model_params_grads:\n                    tracker.add('model', self.model)\n                # Clear the gradients\n                self.optimizer.zero_grad()\n\n            # Save the tracked metrics\n            tracker.save()\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"transformer\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 512,\n        # Train for 32 epochs\n        'epochs': 32,\n        # Batch size $32$\n        'batch_size': 16,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        # Model size\n        'd_model': 256,\n        'transformer.n_heads': 16,\n        'transformer.ffn.d_ff': 1024,\n\n        # Use [Sophia optimizer](../../optimizers/sophia.html)\n        'optimizer.optimizer': 'Sophia',\n        'optimizer.learning_rate': 3e-4,\n        'optimizer.rho': 0.03,\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/basic/__init__.py": "", "labml_nn/transformers/compressive/experiment.py": "\"\"\"\n---\ntitle: Compressive Transformer Experiment\nsummary: This experiment trains a compressive transformer model on tiny Shakespeare dataset.\n---\n\n# Compressive Transformer Experiment\n\nThis is an annotated PyTorch experiment to train a compressive transformer model.\n\"\"\"\nfrom typing import List, Tuple, NamedTuple\n\nimport torch\nimport torch.nn as nn\n\nfrom labml import experiment, tracker, monit, logger\nfrom labml.configs import option\nfrom labml.logger import Text\nfrom labml_helpers.metrics.simple_state import SimpleStateModule\nfrom labml_helpers.module import Module\nfrom labml_helpers.train_valid import BatchIndex, hook_model_outputs\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers.compressive import CompressiveTransformer, AttentionReconstructionLoss, \\\n    CompressiveTransformerLayer, Conv1dCompression\n\n\nclass CompressedMemory(NamedTuple):\n    mem: List[torch.Tensor]\n    c_mem: List[torch.Tensor]\n\n\nclass AutoregressiveModel(Module):\n    \"\"\"\n    ## Auto regressive model\n    \"\"\"\n\n    def __init__(self, n_vocab: int, d_model: int, transformer: CompressiveTransformer):\n        super().__init__()\n        # Token embedding module\n        self.src_embed = nn.Embedding(n_vocab, d_model)\n        # Transformer\n        self.transformer = transformer\n        # Final layer\n        self.generator = nn.Linear(d_model, n_vocab)\n        # Masks\n        self.mask_x = None\n        self.mask_mem = None\n\n    def forward(self, x: torch.Tensor, mem: CompressedMemory):\n        # Get memory and compressed memory\n        if mem is not None:\n            mem, c_mem = mem.mem, mem.c_mem\n        else:\n            mem = []\n            c_mem = []\n\n        # Total length of the memory and compressed memory (for masks)\n        m_len = len(mem[0]) if mem else 0\n        if c_mem:\n            m_len += len(c_mem[0])\n\n        # Create a subsequent mask for tokens\n        if self.mask_x is None or self.mask_x.shape[0] < len(x):\n            from labml_nn.transformers.utils import subsequent_mask\n            self.mask_x = subsequent_mask(len(x)).to(x.device)\n        # Create an all ones (full visibility) mask for memory\n        if self.mask_mem is None or self.mask_mem.shape[1] < m_len or self.mask_mem.shape[0] < len(x):\n            self.mask_mem = self.mask_x.new_ones(len(x), m_len, 1)\n\n        # Concatenate the masks if there is memory\n        if m_len:\n            mask = torch.cat((self.mask_mem[:len(x), :m_len], self.mask_x[:len(x), :len(x)]), dim=1)\n        # Use only the subsequent mask otherwise\n        else:\n            mask = self.mask_x[:len(x), :len(x)]\n\n        # Token embeddings\n        x = self.src_embed(x)\n        # Run it through the transformer\n        res, mem = self.transformer(x, mem, c_mem, mask)\n        # Generate logits of the next token\n        res = self.generator(res)\n        #\n        return res, mem\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    The default configurations can and will be overridden when we start the experiment.\n    \"\"\"\n\n    model: AutoregressiveModel\n\n    # Token embedding size\n    d_model: int = 128\n    # Number of attention heads\n    heads: int = 4\n    # Dropout probability\n    dropout: float = 0.0\n    # Number of features in FFN hidden layer\n    d_ff: int = 256\n    # Number of transformer layers\n    n_layers: int = 6\n    # Number of memories to keep\n    mem_len: int = 8\n    # State module to maintain memories when switching between training and validation\n    memory = SimpleStateModule()\n    # Attention Reconstruction Loss\n    attention_reconstruction_loss: AttentionReconstructionLoss\n    # Compression rate\n    compression_rate: int = 4\n    # Compressed memory length\n    c_mem_len: int = 128\n\n    def init(self):\n        # Set tracker configurations\n        tracker.set_scalar(\"accuracy.*\", True)\n        tracker.set_scalar(\"loss.*\", True)\n        # Do not print the attention reconstruction loss in the terminal\n        tracker.set_scalar(\"ar_loss.*\", False)\n        # Add a hook to log module outputs\n        hook_model_outputs(self.mode, self.model, 'model')\n        # This will keep the accuracy metric stats and memories separate for training and validation.\n        self.state_modules = [self.accuracy, self.memory]\n\n    @torch.no_grad()\n    def merge_compress_memory(self, mem: CompressedMemory, new_mem: List[torch.Tensor]) \\\n            -> Tuple[CompressedMemory, List[torch.Tensor]]:\n        \"\"\"\n        Concatenate new memories and compress the oldest memories.\n        \"\"\"\n\n        # If the configurations specify not to use memory\n        if self.mem_len == 0 and self.c_mem_len == 0:\n            return CompressedMemory([], []), []\n\n        # Get memory and compressed memory\n        if mem is not None:\n            mem, c_mem = mem.mem, mem.c_mem\n        else:\n            mem, c_mem = [], []\n\n        # Concatenate new memories with old memory\n        if mem:\n            mem = [torch.cat((m, x), dim=0) for m, x in zip(mem, new_mem)]\n        else:\n            mem = new_mem\n\n        # Compress the oldest memories if there are more memories than `mem_len`\n        if len(mem[0]) > self.mem_len:\n            # Calculate the number of compressed memories to make $n_{cm} = \\bigg\\lceil\\frac{n'_m - N_m}{c}\\bigg\\rceil$,\n            # where $n'_m$ is the number of memories we have\n            # and $N_m$ is the maximum number of memories we maintain (`mem_len`).\n            n_c_mem = (len(mem[0]) - self.mem_len + self.compression_rate - 1) // self.compression_rate\n            # Number of memories to compress $c n_{cm}$\n            n_old = n_c_mem * self.compression_rate\n            # A list to keep memories that need to be compressed for each layer.\n            mem_to_compress = []\n            # A list to keep the memories that do not get compressed for each layer.\n            uncompressed_mem = []\n            # Iterate through memories of each layer.\n            for m in mem:\n                # Split the memories at $c n_{cm}$\n                cm, m = torch.split(m, [n_old, len(m) - n_old])\n                # Collect memories to compress\n                mem_to_compress.append(cm)\n                # Collect remaining memories\n                uncompressed_mem.append(m)\n            # Update the memories\n            mem = uncompressed_mem\n\n            # Compress the memories\n            new_c_mem = []\n            for i, layer in enumerate(self.model.transformer.layers):\n                new_c_mem.append(layer.compress(mem_to_compress[i]))\n\n            # Concatenate newly compressed memories with old compressed memories\n            if c_mem:\n                c_mem = [torch.cat((m, nm), dim=0) for m, nm in zip(c_mem, new_c_mem)]\n            # If there are no old compressed memories\n            else:\n                c_mem = new_c_mem\n\n            # Truncate old memories\n            if len(c_mem[0]) > self.c_mem_len:\n                c_mem = [m[-self.c_mem_len:] for m in c_mem]\n        # No memories are compressed if the number of memories is less than `mem_len`\n        else:\n            mem_to_compress = []\n\n        # Return memories and the memories that were compressed.\n        # Memories that were compressed are needed for the reconstruction loss computation.\n        return CompressedMemory(mem, c_mem), mem_to_compress\n\n    def step(self, batch: any, batch_idx: BatchIndex):\n        \"\"\"\n        ### Training/validation step\n        \"\"\"\n\n        # Move data to the device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Update global step (number of tokens processed) when in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(data.shape[0] * data.shape[1])\n\n        # Whether to capture model outputs\n        with self.mode.update(is_log_activations=batch_idx.is_last):\n            # Get memories\n            mem = self.memory.get()\n            # Run the model\n            output, new_mem = self.model(data, mem)\n            # Merge and compress memory\n            mem, mem_to_compress = self.merge_compress_memory(mem, new_mem)\n            # Update memories\n            self.memory.set(mem)\n\n        # Calculate and log cross entropy loss\n        loss = self.loss_func(output, target)\n        tracker.add(\"loss.\", loss)\n\n        # Calculate attention reconstruction loss if memories were compressed in this step\n        if mem_to_compress:\n            # Get attention reconstruction loss\n            ar_loss = self.attention_reconstruction_loss(new_mem, mem_to_compress)\n            # Track attention reconstruction loss\n            tracker.add(\"ar_loss.\", ar_loss)\n            # Add attention reconstruction loss to loss\n            loss = loss + ar_loss\n\n        # Calculate and log accuracy\n        self.accuracy(output, target)\n        self.accuracy.track()\n\n        # Train the model\n        if self.mode.is_train:\n            # Calculate gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Take optimizer step\n            self.optimizer.step()\n            # Log the model parameters and gradients on last batch of every epoch\n            if batch_idx.is_last:\n                tracker.add('model', self.model)\n            # Clear the gradients\n            self.optimizer.zero_grad()\n\n        # Save the tracked metrics\n        tracker.save()\n\n    def sample(self):\n        \"\"\"\n        ### Sampling function to generate samples periodically while training\n        \"\"\"\n\n        # Starting prompt\n        prompt = self.prompt\n        # Collect output for printing\n        log = [(prompt, Text.subtle)]\n        # memory\n        mem = CompressedMemory([], [])\n        # Sample 25 tokens\n        for i in monit.iterate('Sample', 25):\n            # Tokenize the prompt\n            data = self.text.text_to_i(prompt).unsqueeze(-1)\n            # Move to device\n            data = data.to(self.device)\n            # Get the model output\n            output, new_mem = self.model(data, mem)\n            # Get the model prediction (greedy)\n            output = output.argmax(dim=-1).squeeze(1)\n            # Add the prediction to prompt\n            prompt += self.prompt_separator + self.text.itos[output[-1]]\n            # Only feed the last character to model in next iteration, rest will go in as memories\n            prompt = prompt[-1:]\n            # Add the prediction for logging\n            log += [(self.prompt_separator + self.text.itos[output[-1]], Text.value)]\n            # Update and compress memory\n            mem, _ = self.merge_compress_memory(mem, new_mem)\n\n        # Print the sampled output\n        logger.log(log)\n\n\n@option(Configs.model)\ndef autoregressive_model(c: Configs):\n    \"\"\"\n    ### Initialize the auto-regressive model\n    \"\"\"\n    from labml_nn.transformers.xl import RelativeMultiHeadAttention\n    from labml_nn.transformers.feed_forward import FeedForward\n    m = AutoregressiveModel(c.n_tokens, c.d_model, CompressiveTransformer(\n        CompressiveTransformerLayer(d_model=c.d_model,\n                                    self_attn=RelativeMultiHeadAttention(c.heads, c.d_model, c.dropout),\n                                    feed_forward=FeedForward(c.d_model, c.d_ff, c.dropout),\n                                    dropout_prob=c.dropout,\n                                    compress=Conv1dCompression(c.compression_rate, c.d_model)), c.n_layers))\n    return m.to(c.device)\n\n\n@option(Configs.attention_reconstruction_loss)\ndef attention_reconstruction_loss(c: Configs):\n    \"\"\"\n    ### Initialize the attention reconstruction loss\n    \"\"\"\n    return AttentionReconstructionLoss(c.model.transformer.layers)\n\n\ndef main():\n    \"\"\"\n    ### Run the experiment\n    \"\"\"\n    # Create experiment\n    experiment.create(name=\"compressive_transformer\", comment='')\n    # Create configs\n    conf = Configs()\n    # Load configurations\n    experiment.configs(conf,\n                       # A dictionary of configurations to override\n                       {'tokenizer': 'character',\n                        'text': 'tiny_shakespeare',\n                        'optimizer.learning_rate': 2.5e-4,\n                        'optimizer.optimizer': 'AdamW',\n                        'prompt': 'It is',\n                        'prompt_separator': '',\n\n                        'train_loader': 'sequential_train_loader',\n                        'valid_loader': 'sequential_valid_loader',\n\n                        'seq_len': 8,\n                        'mem_len': 8,\n                        'epochs': 128,\n                        'batch_size': 32,\n                        'inner_iterations': 25,\n                        'compression_rate': 2,\n                        })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # `TrainValidConfigs.run`\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/compressive/__init__.py": "\"\"\"\n---\ntitle: Compressive Transformer\nsummary: >\n  Documented implementation with explanations of a\n  Compressive Transformer model.\n---\n\n# Compressive Transformer\n\nThis is an implementation of\n[Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)\nin [PyTorch](https://pytorch.org).\n\nThis is an extension of [Transformer XL](../xl/index.html) where past memories\nare compressed to give a longer attention range.\nThat is, the furthest $n_{cm} c$ memories are compressed into\n$n_{cm}$ memories, where $c$ is the compression rate.\n\n## Compression operation\n\nThe compression operation is defined as\n$f_c: \\mathbb{R}^{nc \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$.\nThe paper introduces multiple choices for $f_c$ and we have only implemented\n1D convolution which seems to give the best results.\nEach layer has a separate compression operation $f_c^{(i)}$ where\n$i$ is the layer number.\n\n## Training compression operation\n\nSince training compression with BPTT requires maintaining\na very large computational graph (many time steps), the paper proposes\nan *auto-encoding loss* and an *attention reconstruction loss*.\nThe auto-encoding loss decodes the original memories from the compressed memories\nand calculates the loss.\nAttention reconstruction loss computes the multi-headed attention results\non the compressed memory and on uncompressed memory and gets a mean squared error\nbetween them.\nWe have implemented the latter here since it gives better results.\n\nThis implementation uses pre-layer normalization\nwhile the paper uses post-layer normalization.\nPre-layer norm does the layer norm before [FFN](../feedforward.html) and\nself-attention, and the pass-through in the residual connection is not normalized.\nThis is supposed to be more stable in standard transformer setups.\n\nHere are [the training code](experiment.html) and a notebook for training a compressive transformer\nmodel on the Tiny Shakespeare dataset.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/compressive/experiment.ipynb)\n\"\"\"\n\nfrom typing import Optional, List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom labml_helpers.module import Module, TypedModuleList\nfrom labml_nn.transformers.feed_forward import FeedForward\nfrom labml_nn.transformers.mha import PrepareForMultiHeadAttention\nfrom labml_nn.transformers.xl.relative_mha import RelativeMultiHeadAttention\nfrom labml_nn.utils import clone_module_list\n\n\nclass Conv1dCompression(Module):\n    \"\"\"\n    ## 1D Convolution Compression $f_c$\n\n    This is a simple wrapper around\n    [`nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n    with some tensor dimension permutations.\n    \"\"\"\n    def __init__(self, compression_rate: int, d_model: int):\n        \"\"\"\n        * `compression_rate` $c$\n        * `d_model` is the embedding size\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv1d(d_model, d_model, kernel_size=compression_rate, stride=compression_rate)\n\n    def forward(self, mem: torch.Tensor):\n        \"\"\"\n        `mem` has shape `[seq_len, batch, d_model]`\n        \"\"\"\n\n        # Permute the dimensions of `mem` so that we can run it through the convolution layer.\n        # The convolution layer accepts in the form `[batch, features, sequence]`\n        mem = mem.permute(1, 2, 0)\n        # Get compressed memory by running it through the convolution layer\n        c_mem = self.conv(mem)\n        # Permute back to form `[seq_len, batch, d_model]`\n        return c_mem.permute(2, 0, 1)\n\n\nclass CompressiveTransformerLayer(Module):\n    \"\"\"\n    ## Compressive Transformer Layer\n\n    This is the implementation of a single compressive transformer layer\n    \"\"\"\n    def __init__(self, *,\n                 d_model: int,\n                 self_attn: RelativeMultiHeadAttention,\n                 feed_forward: FeedForward,\n                 dropout_prob: float,\n                 compress: Conv1dCompression):\n        \"\"\"\n        * `d_model` is the token embedding size\n        * `self_attn` is the [self attention module](../xl/relative_mha.html)\n        * `feed_forward` is the [feed forward module](../feed_forward.html)\n        * `dropout_prob` is the probability of dropping out after self attention and FFN\n        * `compress` is the compression function $f_c$\n        \"\"\"\n        super().__init__()\n        self.compress = compress\n        self.size = d_model\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.dropout = nn.Dropout(dropout_prob)\n        self.norm_self_attn = nn.LayerNorm([d_model])\n        self.norm_ff = nn.LayerNorm([d_model])\n\n    def concat_memory(self, z: torch.Tensor, mem: Optional[torch.Tensor], c_mem: Optional[torch.Tensor]):\n        \"\"\"\n        Concatenate the normalized token embeddings with memory and compressed memory.\n\n        * `z` is layer normalized token embeddings.\n        * `mem` and `c_mem` are memory and compressed memory (not normalized).\n        \"\"\"\n\n        # If there is no memory just return the token embeddings\n        if mem is None:\n            return z\n\n        # If there are compressed memory concatenate that with memory\n        if c_mem is not None:\n            mem = torch.cat((c_mem, mem), dim=0)\n\n        # Run the memory through the normalization layer\n        mem = self.norm_self_attn(mem)\n        # Concatenate normalized memory and normalized token embeddings\n        return torch.cat((mem, z), dim=0)\n\n    def forward(self, *,\n                x: torch.Tensor,\n                mem: Optional[torch.Tensor],\n                c_mem: Optional[torch.Tensor],\n                mask: torch.Tensor):\n        \"\"\"\n        * `x` is a tensor of token level feature vectors of shape `[seq_len, batch_size, d_model]`\n        * `mem` is a tensor of the past token level feature vectors (memory) of shape `[mem_len, batch_size, d_model]`\n        * `c_mem` is a tensor of the compressed memory `[c_mem_len, batch_size, d_model]`\n        * `mask` is a matrix of shape `[seq_len, c_mem_len + mem_len + seq_len, batch_size]` or `[seq_len, c_mem_len + mem_len + seq_len, 1]`.\n        `mask[i, j]` is  true if token at `i` can see token at `j`.\n        \"\"\"\n\n        # Normalize the vectors before doing self attention\n        z = self.norm_self_attn(x)\n        # Normalize and concatenate memory and compressed memory\n        m_z = self.concat_memory(z, mem, c_mem)\n        # Attention\n        self_attn = self.self_attn(query=z, key=m_z, value=m_z, mask=mask)\n        # Add the attention results\n        x = x + self.dropout(self_attn)\n\n        # Normalize for feed-forward\n        z = self.norm_ff(x)\n        # Pass through the feed-forward network\n        ff = self.feed_forward(z)\n        # Add the feed-forward results back\n        x = x + self.dropout(ff)\n\n        #\n        return x\n\n\nclass CompressiveTransformer(Module):\n    \"\"\"\n    ## Compressive Transformer Model\n\n    This consists of multiple compressive transformer layers\n    \"\"\"\n\n    def __init__(self, layer: CompressiveTransformerLayer, n_layers: int):\n        super().__init__()\n        # Make copies of the transformer layer\n        self.layers = clone_module_list(layer, n_layers)\n        # Final normalization layer\n        self.norm = nn.LayerNorm([layer.size])\n\n    def forward(self, x: torch.Tensor, mem: List[torch.Tensor], c_mem: List[torch.Tensor], mask: torch.Tensor):\n        \"\"\"\n        * `x` is a tensor of the token embeddings vectors of shape `[seq_len, batch_size, d_model]`\n        * `mem` is a list of tensors of the past token level feature vectors of shape\n         `[mem_len, batch_size, d_model]` for each layer\n        * `c_mem` is a list of tensors of the compressed memory\n         `[c_mem_len, batch_size, d_model]` for each layer\n        * `mask` is the masking matrix\n        \"\"\"\n        # List to store token level feature vectors,\n        # which will become the memories for the next sequential batch.\n        new_mem = []\n        # Run through each transformer layer\n        for i, layer in enumerate(self.layers):\n            # Add to the list of feature vectors\n            new_mem.append(x.detach())\n            # Memory\n            m = mem[i] if mem else None\n            # Compressed Memory\n            cm = c_mem[i] if c_mem else None\n            # Run through the transformer XL layer\n            x = layer(x=x, mem=m, c_mem=cm, mask=mask)\n        # Finally, normalize the vectors\n        return self.norm(x), new_mem\n\n\nclass AttentionReconstructionLoss:\n    \"\"\"\n    ## Attention Reconstruction Loss\n\n    Attention reconstruction loss recreates the self-attention output with\n    uncompressed memory and with compressed memory and calculates the mean squared error\n    between the two. It does this without positional encoding.\n\n    When calculating and training the compression function $f_c$ with attention\n    reconstruction loss, all parameters but $f_c$ are frozen.\n    This includes key/value projections and bias/scaling after normalization.\n\n    Since this loss can be computed independently of the cross-entropy-loss of the model\n    you can have a separate optimizer that only updates $f_c$.\n    However, we use the same optimizer to update $f_c$ so when calculating\n    attention reconstruction loss, we detach all other parameters except $f_c$\n    from the gradient computation.\n    \"\"\"\n    def __init__(self, layers: TypedModuleList[CompressiveTransformerLayer]):\n        \"\"\"\n        `layers` is the list of Compressive Transformer layers\n        \"\"\"\n        self.layers = layers\n        self.loss_func = nn.MSELoss()\n\n    def prepare_for_attn(self, pmha: PrepareForMultiHeadAttention, x: torch.Tensor):\n        \"\"\"\n        This is a reimplementation of ['PrepareForMultiHeadAttention'](../mha.html#PrepareMHA)\n        where the projections are done with the parameters detached from gradient computation.\n\n        * `pmha` is the ['PrepareForMultiHeadAttention'](../mha.html#PrepareMHA) module\n        * `x` is tensor with the token embeddings\n        \"\"\"\n\n        # Shape of the input except embedding dimension; `[seq_len, batch_size]`.\n        head_shape = x.shape[:-1]\n\n        # Detach projection weights and bias\n        weight = pmha.linear.weight.detach()\n        bias = pmha.linear.bias.detach() if pmha.linear.bias is not None else None\n        # Linear transform\n        x = F.linear(x, weight, bias)\n\n        # Split last dimension into heads\n        x = x.view(*head_shape, pmha.heads, pmha.d_k)\n\n        # Output has shape `[seq_len, batch_size, heads, d_k]` or `[batch_size, d_model]`\n        return x\n\n    def attn(self, layer: RelativeMultiHeadAttention, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n        \"\"\"\n        This is a reimplementation of ['Multi-Head Attention'](../mha.html#MHA) which calls\n        `prepare_for_attn` instead of ['PrepareForMultiHeadAttention'](../mha.html#PrepareMHA)\n        to detach projection parameters.\n        \"\"\"\n        # Calculate query, key and value projections\n        query = self.prepare_for_attn(layer.query, query)\n        key = self.prepare_for_attn(layer.key, key)\n        value = self.prepare_for_attn(layer.value, value)\n\n        # Compute attention scores $Q K^\\top$.\n        # This gives a tensor of shape `[seq_len, seq_len, batch_size, heads]`.\n        scores = torch.einsum('ibhd,jbhd->ijbh', query, key)\n\n        # Scale scores $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n        scores *= layer.scale\n\n        # $softmax$ attention along the key sequence dimension\n        # $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n        attn = layer.softmax(scores)\n\n        # Multiply by values\n        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)V$$\n        return torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n\n    def norm(self, ln: nn.LayerNorm, x: torch.Tensor):\n        \"\"\"\n        Perform layer normalization with shift and scale parameters detached.\n        \"\"\"\n\n        # Detach shift(`bias`) and scaling(`weight`) parameters\n        weight = ln.weight.detach() if ln.weight is not None else None\n        bias = ln.bias.detach() if ln.bias is not None else None\n\n        # Layer normalization\n        return F.layer_norm(x, ln.normalized_shape, weight, bias, ln.eps)\n\n    def calc_loss(self, layer: CompressiveTransformerLayer, h: torch.Tensor, mem: torch.Tensor):\n        \"\"\"\n        This calculates the loss for a layer\n        \"\"\"\n\n        # Detach the token embeddings and memory.\n        h = h.detach()\n        mem = mem.detach()\n\n        # Compress the memory with $f_c^{(i)}$.\n        # The parameters of $f_c^{(i)}$ are the only parameters not detached from gradient computation.\n        c_mem = layer.compress(mem)\n\n        # Normalize the embeddings and memories\n        h = self.norm(layer.norm_self_attn, h)\n        mem = self.norm(layer.norm_self_attn, mem)\n        c_mem = self.norm(layer.norm_self_attn, c_mem)\n\n        # Calculate the attention with uncompressed memory\n        attn_mem = self.attn(layer.self_attn, h, mem, mem)\n        # Calculate the attention with compressed memory\n        attn_cmem = self.attn(layer.self_attn, h, c_mem, c_mem)\n\n        # Calculate the mean square error\n        return self.loss_func(attn_cmem, attn_mem)\n\n    def __call__(self, h: List[torch.Tensor], mem: List[torch.Tensor]):\n        # Calculate the losses for each layer\n        losses = [self.calc_loss(layer, h[n], mem[n]) for n, layer in enumerate(self.layers)]\n        # Sum of the losses\n        return sum(losses)\n", "labml_nn/transformers/primer_ez/experiment.py": "\"\"\"\n---\ntitle: Primer EZ experiment\nsummary: This experiment trains Primer EZ on Tiny Shakespeare dataset.\n---\n\n# [Primer EZ](index.html) Experiment\n\nThis is an annotated PyTorch experiment to train a [Primer EZ transformer](index.html).\n\nThis is based on our [vanilla transformer experiment](../basic/experiment.html).\nWe use the same experiment and add the Primer EZ modifications.\n\"\"\"\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_nn.transformers import TransformerConfigs\nfrom labml_nn.transformers.basic.autoregressive_experiment import Configs\nfrom labml_nn.transformers.configs import FeedForwardConfigs\nfrom labml_nn.transformers.primer_ez import SquaredReLU\n\n\n@option(FeedForwardConfigs.activation, 'SquaredReLU')\ndef _squared_relu():\n    \"\"\"\n    Add the [option](https://docs.labml.ai/api/configs.html#labml.configs.option)\n     of [**squared ReLU**](index.html) to [configurable](../configs.html#FFN)\n     [feed forward module](../feed_forward.html).\n    \"\"\"\n    return SquaredReLU()\n\n\n@option(TransformerConfigs.encoder_attn, 'MultiDConvHeadAttention')\ndef _d_conv_mha(c: TransformerConfigs):\n    \"\"\"\n    Add the [option](https://docs.labml.ai/api/configs.html#labml.configs.option)\n     of [**Multi-DConv-Head Attention**](index.html) to\n     [configurable transformer](../configs.html#TransformerConfigs)\n    \"\"\"\n    from labml_nn.transformers.primer_ez import MultiDConvHeadAttention\n    return MultiDConvHeadAttention(c.n_heads, c.d_model, dropout_prob=c.dropout)\n\n\n@option(TransformerConfigs.encoder_attn, 'MultiDSharedConvHeadAttention')\ndef _d_shared_conv_mha(c: TransformerConfigs):\n    \"\"\"\n    Add the [option](https://docs.labml.ai/api/configs.html#labml.configs.option)\n     of [**Multi Depth-wise Shared Conv Head Attention**](variations.html) to\n     [configurable transformer](../configs.html#TransformerConfigs)\n\n    \ud83d\udcdd *This is a variation we tried*\n    \"\"\"\n    from labml_nn.transformers.primer_ez.variations import MultiDSharedConvHeadAttention\n    return MultiDSharedConvHeadAttention(c.n_heads, c.d_model, dropout_prob=c.dropout)\n\n\n@option(TransformerConfigs.encoder_attn, 'MultiDPHConvHeadAttention')\ndef _d_per_head_conv_mha(c: TransformerConfigs):\n    \"\"\"\n    Add the [option](https://docs.labml.ai/api/configs.html#labml.configs.option)\n     of [**Multi Depth-wise Per Head Conv Head Attention**](variation.html) to\n     [configurable transformer](../configs.html#TransformerConfigs)\n\n    \ud83d\udcdd *This is a variation we tried*\n    \"\"\"\n    from labml_nn.transformers.primer_ez.variations import MultiDPHConvHeadAttention\n    return MultiDPHConvHeadAttention(c.n_heads, c.d_model, dropout_prob=c.dropout)\n\n\ndef main():\n    # Create experiment\n    experiment.create(name=\"primer_ez\")\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 256,\n        # Train for $128$ epochs\n        'epochs': 128,\n        # Batch size $32$\n        'batch_size': 32,\n        # Switch between training and validation for $10$ times\n        # per epoch\n        'inner_iterations': 10,\n\n        # Model size\n        'd_model': 512,\n        'transformer.ffn.d_ff': 2048,\n\n        # Use Adam optimizer\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 2.5e-4,\n\n        # \u2b50\ufe0f Use [**squared ReLU**](index.html) activation in the feed forward network.\n        #\n        # *Replace this with `ReLU` for $ReLU$.*\n        'transformer.ffn.activation': 'SquaredReLU',\n\n        # \u2b50\ufe0f Use [**Multi-DConv-Head Attention**](index.html) for encoder attention.\n        #\n        # *Replace this with `mha` for original multi-head attention.*\n        'transformer.encoder_attn': 'MultiDConvHeadAttention',\n    })\n\n    # Set models for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n", "labml_nn/transformers/primer_ez/variations.py": "\"\"\"\n---\ntitle: Primer EZ variations\nsummary: We tried some variations to Primer EZ.\n---\n\n# [Primer EZ](index.html) Variations\n\nWe tried some variations to see which changes in Primer EZ has most benefits.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.transformers import MultiHeadAttention\n\n\nclass SpatialDepthWiseSharedConvolution(Module):\n    \"\"\"\n    ## Spatial Depth Wise Shared Convolution\n\n    We share the same kernel across all channels.\n    \"\"\"\n\n    def __init__(self, kernel_size: int = 3):\n        \"\"\"\n        \"\"\"\n        super().__init__()\n        self.kernel_size = kernel_size\n        # We use PyTorch's `Conv1d` module.\n        # We add padding to both sides and later crop the right most `kernel_size - 1` results\n        self.conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=(kernel_size,), padding=(kernel_size - 1,))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        `x` has shape `[seq_len, batch_size, heads, d_k]`\n        \"\"\"\n\n        # Get the shape\n        seq_len, batch_size, heads, d_k = x.shape\n        # Permute to `[batch_size, heads, d_k, seq_len]`\n        x = x.permute(1, 2, 3, 0)\n        # Change the shape to `[batch_size * heads * d_k, seq_len]`\n        x = x.view(batch_size * heads * d_k, 1, seq_len)\n\n        # 1D convolution accepts input of the form `[N, channels, sequence]`\n        x = self.conv(x)\n        # Crop the right most `kernel_size - 1` results since we padded both sides\n        x = x[:, :, :-(self.kernel_size - 1)]\n        # Reshape to `[batch_size, heads, d_k, seq_len]`\n        x = x.view(batch_size, heads, d_k, seq_len)\n        # Permute to `[seq_len, batch_size, heads, d_k]`\n        x = x.permute(3, 0, 1, 2)\n\n        #\n        return x\n\n\nclass MultiDSharedConvHeadAttention(MultiHeadAttention):\n    \"\"\"\n    ## Multi-Depth-wise-Shared-Conv-Head Attention\n\n    We extend our original implementation of [Multi-Head Attention](../mha.html#MHA)\n    and add the spatial depth-wise shared convolution to query, key and value projections.\n    \"\"\"\n\n    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n        super().__init__(heads, d_model, dropout_prob)\n\n        # [Multi-Head Attention](../mha.html#MHA) will create query, key and value projection modules\n        # `self.query`, `self.key`, and `self.value`.\n        #\n        # We combine a spatial depth-wise shared convolution layer to each of them and replace\n        # `self.query`, `self.key`, and `self.value`.\n        self.query = nn.Sequential(self.query, SpatialDepthWiseSharedConvolution())\n        self.key = nn.Sequential(self.key, SpatialDepthWiseSharedConvolution())\n        self.value = nn.Sequential(self.value, SpatialDepthWiseSharedConvolution())\n\n\nclass SpatialDepthWisePerHeadConvolution(Module):\n    \"\"\"\n    ## Spatial Depth Wise Per Head Convolution\n    \"\"\"\n\n    def __init__(self, heads: int, d_k: int, kernel_size: int = 3):\n        \"\"\"\n        * `heads` is the number of heads\n        * `d_k` is the number of channels in each head\n        \"\"\"\n        super().__init__()\n        self.kernel_size = kernel_size\n        # We use PyTorch's `Conv1d` module.\n        # We set the number of groups to be equal to the number of channels from each head\n        # so that it does a separate convolution\n        # (with different kernels) for each channel and head.\n        # We add padding to both sides and later crop the right most `kernel_size - 1` results\n        self.conv = nn.Conv1d(in_channels=d_k * heads, out_channels=d_k * heads,\n                              kernel_size=(kernel_size,), padding=(kernel_size - 1,), groups=d_k * heads)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        `x` has shape `[seq_len, batch_size, heads, d_k]`\n        \"\"\"\n\n        # Get the shape\n        seq_len, batch_size, heads, d_k = x.shape\n        # Permute to `[batch_size, heads, d_k, seq_len]`\n        x = x.permute(1, 2, 3, 0)\n        # Change the shape to `[batch_size heads * d_k, seq_len]`\n        x = x.view(batch_size, heads * d_k, seq_len)\n\n        # 1D convolution accepts input of the form `[N, channels, sequence]`\n        x = self.conv(x)\n        # Crop the right most `kernel_size - 1` results since we padded both sides\n        x = x[:, :, :-(self.kernel_size - 1)]\n        # Reshape to `[batch_size, heads, d_k, seq_len]`\n        x = x.view(batch_size, heads, d_k, seq_len)\n        # Permute to `[seq_len, batch_size, heads, d_k]`\n        x = x.permute(3, 0, 1, 2)\n\n        #\n        return x\n\n\nclass MultiDPHConvHeadAttention(MultiHeadAttention):\n    \"\"\"\n    ## Multi-per-Head-Depth-wise-Conv-Head Attention\n\n    We extend our original implementation of [Multi-Head Attention](../mha.html#MHA)\n    and add the spatial depth-wise convolution to query, key and value projections.\n    \"\"\"\n\n    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n        super().__init__(heads, d_model, dropout_prob)\n\n        # [Multi-Head Attention](../mha.html#MHA) will create query, key and value projection modules\n        # `self.query`, `self.key`, and `self.value`.\n        #\n        # We combine a spatial per-head depth-wise convolution layer to each of them and replace\n        # `self.query`, `self.key`, and `self.value`.\n        self.query = nn.Sequential(self.query, SpatialDepthWisePerHeadConvolution(heads, self.d_k))\n        self.key = nn.Sequential(self.key, SpatialDepthWisePerHeadConvolution(heads, self.d_k))\n        self.value = nn.Sequential(self.value, SpatialDepthWisePerHeadConvolution(heads, self.d_k))\n", "labml_nn/transformers/primer_ez/efficient.py": "import math\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.transformers import MultiHeadAttention\n\n\nclass SpatialDepthWiseConvolution(Module):\n    \"\"\"\n    ## Spatial Depth Wise Convolution\n\n    This is actually slower\n    \"\"\"\n\n    def __init__(self, d_k: int, kernel_size: int = 3):\n        \"\"\"\n        * `d_k` is the number of channels in each head\n        \"\"\"\n        super().__init__()\n        self.kernel_size = kernel_size\n        # We use PyTorch's `Conv1d` module.\n        # We set the number of groups to be equal to the number of channels so that it does a separate convolution\n        # (with different kernels) for each channel.\n        # We add padding to both sides and later crop the right most `kernel_size - 1` results\n        rng = 1 / math.sqrt(kernel_size)\n        self.kernels = nn.Parameter(torch.zeros((kernel_size, d_k)).uniform_(-rng, rng))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        `x` has shape `[seq_len, batch_size, heads, d_k]`\n        \"\"\"\n\n        res = x * self.kernels[0].view(1, 1, 1, -1)\n\n        for i in range(1, len(self.kernels)):\n            res[i:] += x[:-i] * self.kernels[i].view(1, 1, 1, -1)\n\n        return res\n\n\nclass MultiDConvHeadAttention(MultiHeadAttention):\n    \"\"\"\n    ## Multi-DConv-Head Attention (MDHA)\n\n    We extend our original implementation of [Multi-Head Attention](../mha.html#MHA)\n    and add the spatial depth-wise convolution to query, key and value projections.\n        \"\"\"\n\n    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n        super().__init__(heads, d_model, dropout_prob)\n\n        # [Multi-Head Attention](../mha.html#MHA) will create query, key and value projection modules\n        # `self.query`, `self.key`, and `self.value`.\n        #\n        # We combine a spatial depth-wise convolution layer to each of them and replace\n        # `self.query`, `self.key`, and `self.value`.\n        self.query = nn.Sequential(self.query, SpatialDepthWiseConvolution(self.d_k))\n        self.key = nn.Sequential(self.key, SpatialDepthWiseConvolution(self.d_k))\n        self.value = nn.Sequential(self.value, SpatialDepthWiseConvolution(self.d_k))\n", "labml_nn/transformers/primer_ez/__init__.py": "\"\"\"\n---\ntitle: \"Primer: Searching for Efficient Transformers for Language Modeling\"\nsummary: >\n  This is an annotated implementation/tutorial of\n  Primer: Searching for Efficient Transformers for Language Modeling for Vision in PyTorch.\n---\n\n# Primer: Searching for Efficient Transformers for Language Modeling\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[Primer: Searching for Efficient Transformers for Language Modeling](https://arxiv.org/abs/2109.08668).\n\nThe authors do an evolutionary search for transformer architectures.\nThey name the architecture found using the search Primer (PRIMitives searched transformER).\n**Primer EZ** is the architecture with the two most robust modifications in Primer compared to\n the original transformer.\nPrimer EZ trains a lot faster than the vanilla transformer.\n\n### Squared ReLU\n\nThe most effective modification found by the search is using a square ReLU instead of ReLU in\nthe [position-wise feedforward module](../feed_forward.html).\n\n$$y = {\\max(x, 0)}^2$$\n\n### Multi-DConv-Head Attention (MDHA)\n\nThe next effective modification is a depth-wise $3 \\times 1$ convolution after multi-head projection\n for queries, keys, and values.\nThe convolution is along the sequence dimension and per channel (depth-wise).\nTo be clear, if the number of channels in each head is $d_k$ the convolution will have $1 \\times 3$\nkernels for each of the $d_k$ channels.\n\n[Here is the experiment code](experiment.html), for Primer EZ.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\nfrom labml_nn.transformers import MultiHeadAttention\n\n\nclass SquaredReLU(Module):\n    \"\"\"\n    ## Squared ReLU activation\n\n    $$y = {\\max(x, 0)}^2$$\n\n    Squared ReLU is used as the activation function in the\n     [position wise feedforward module](../feed_forward.html).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.relu = nn.ReLU()\n\n    def forward(self, x: torch.Tensor):\n        # Apply ReLU\n        x = self.relu(x)\n        # Square it\n        return x * x\n\n\nclass SpatialDepthWiseConvolution(Module):\n    \"\"\"\n    ## Spatial Depth Wise Convolution\n    \"\"\"\n\n    def __init__(self, d_k: int, kernel_size: int = 3):\n        \"\"\"\n        * `d_k` is the number of channels in each head\n        \"\"\"\n        super().__init__()\n        self.kernel_size = kernel_size\n        # We use PyTorch's `Conv1d` module.\n        # We set the number of groups to be equal to the number of channels so that it does a separate convolution\n        # (with different kernels) for each channel.\n        # We add padding to both sides and later crop the right most `kernel_size - 1` results\n        self.conv = nn.Conv1d(in_channels=d_k, out_channels=d_k,\n                              kernel_size=(kernel_size,), padding=(kernel_size - 1,), groups=d_k)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        `x` has shape `[seq_len, batch_size, heads, d_k]`\n        \"\"\"\n\n        # Get the shape\n        seq_len, batch_size, heads, d_k = x.shape\n        # Permute to `[batch_size, heads, d_k, seq_len]`\n        x = x.permute(1, 2, 3, 0)\n        # Change the shape to `[batch_size * heads, d_k, seq_len]`\n        x = x.view(batch_size * heads, d_k, seq_len)\n\n        # 1D convolution accepts input of the form `[N, channels, sequence]`\n        x = self.conv(x)\n        # Crop the right most `kernel_size - 1` results since we padded both sides\n        x = x[:, :, :-(self.kernel_size - 1)]\n        # Reshape to `[batch_size, heads, d_k, seq_len]`\n        x = x.view(batch_size, heads, d_k, seq_len)\n        # Permute to `[seq_len, batch_size, heads, d_k]`\n        x = x.permute(3, 0, 1, 2)\n\n        #\n        return x\n\n\nclass MultiDConvHeadAttention(MultiHeadAttention):\n    \"\"\"\n    ## Multi-DConv-Head Attention (MDHA)\n\n    We extend our original implementation of [Multi-Head Attention](../mha.html#MHA)\n    and add the spatial depth-wise convolution to query, key and value projections.\n        \"\"\"\n\n    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n        super().__init__(heads, d_model, dropout_prob)\n\n        # [Multi-Head Attention](../mha.html#MHA) will create query, key and value projection modules\n        # `self.query`, `self.key`, and `self.value`.\n        #\n        # We combine a spatial depth-wise convolution layer to each of them and replace\n        # `self.query`, `self.key`, and `self.value`.\n        #\n        # \ud83d\udcdd *We feel this cleaner implementation is easier to understand since it clearly shows the difference\n        # between this and vanilla transformer multi-head attention*.\n        self.query = nn.Sequential(self.query, SpatialDepthWiseConvolution(self.d_k))\n        self.key = nn.Sequential(self.key, SpatialDepthWiseConvolution(self.d_k))\n        self.value = nn.Sequential(self.value, SpatialDepthWiseConvolution(self.d_k))\n"}