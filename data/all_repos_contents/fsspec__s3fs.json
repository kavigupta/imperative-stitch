{"versioneer.py": "# Version: 0.29\n\n\"\"\"The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/python-versioneer/python-versioneer\n* Brian Warner\n* License: Public Domain (Unlicense)\n* Compatible with: Python 3.7, 3.8, 3.9, 3.10, 3.11 and pypy3\n* [![Latest Version][pypi-image]][pypi-url]\n* [![Build Status][travis-image]][travis-url]\n\nThis is a tool for managing a recorded version number in setuptools-based\npython projects. The goal is to remove the tedious and error-prone \"update\nthe embedded version string\" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\nVersioneer provides two installation modes. The \"classic\" vendored mode installs\na copy of versioneer into your repository. The experimental build-time dependency mode\nis intended to allow you to skip this step and simplify the process of upgrading.\n\n### Vendored mode\n\n* `pip install versioneer` to somewhere in your $PATH\n   * A [conda-forge recipe](https://github.com/conda-forge/versioneer-feedstock) is\n     available, so you can also use `conda install -c conda-forge versioneer`\n* add a `[tool.versioneer]` section to your `pyproject.toml` or a\n  `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n   * Note that you will need to add `tomli; python_version < \"3.11\"` to your\n     build-time dependencies if you use `pyproject.toml`\n* run `versioneer install --vendor` in your source tree, commit the results\n* verify version information with `python setup.py version`\n\n### Build-time dependency mode\n\n* `pip install versioneer` to somewhere in your $PATH\n   * A [conda-forge recipe](https://github.com/conda-forge/versioneer-feedstock) is\n     available, so you can also use `conda install -c conda-forge versioneer`\n* add a `[tool.versioneer]` section to your `pyproject.toml` or a\n  `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n* add `versioneer` (with `[toml]` extra, if configuring in `pyproject.toml`)\n  to the `requires` key of the `build-system` table in `pyproject.toml`:\n  ```toml\n  [build-system]\n  requires = [\"setuptools\", \"versioneer[toml]\"]\n  build-backend = \"setuptools.build_meta\"\n  ```\n* run `versioneer install --no-vendor` in your source tree, commit the results\n* verify version information with `python setup.py version`\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github's\n  \"tarball from tag\" feature\n* a release tarball, produced by \"setup.py sdist\", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. \"git describe\" (for checkouts), which knows\n  about recent \"tags\" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. \"myproject-1.2\" instead of just \"1.2\"), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n\"0.7-1-g574ab98-dirty\" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of \"574ab98\", and is \"dirty\" (it has\nuncommitted changes).\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a 'setup.py sdist' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the \"outside\" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `['version']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project's version\n  string. The default \"pep440\" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the \"Styles\" section\n  below for alternative styles.\n\n* `['full-revisionid']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. \"1076c978a8d3cfc70f408fe5974aa6c092c949ac\".\n\n* `['date']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `['dirty']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `['error']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of \"unknown\".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an \"about\" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()['version']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, \"pep440\", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional \"local\nversion\" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example \"0.11+2.g1076c97.dirty\" indicates that the\ntree is like the \"1076c97\" commit but has uncommitted changes (\".dirty\"), and\nthat this commit is two revisions (\"+2\") beyond the \"0.11\" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. \"0.11\".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of \"0+unknown\". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/python-versioneer/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  \"master\" and \"slave\" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other languages) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/python-versioneer/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/python-versioneer/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n\"Entry-point scripts\" (`setup(entry_points={\"console_scripts\": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/python-versioneer/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg` and `pyproject.toml`, if necessary,\n  to include any new configuration settings indicated by the release notes.\n  See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install --[no-]vendor` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n## Similar projects\n\n* [setuptools_scm](https://github.com/pypa/setuptools_scm/) - a non-vendored build-time\n  dependency\n* [minver](https://github.com/jbweston/miniver) - a lightweight reimplementation of\n  versioneer\n* [versioningit](https://github.com/jwodder/versioningit) - a PEP 518-based setuptools\n  plugin\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the \"Unlicense\", as described in\nhttps://unlicense.org/.\n\n[pypi-image]: https://img.shields.io/pypi/v/versioneer.svg\n[pypi-url]: https://pypi.python.org/pypi/versioneer/\n[travis-image]:\nhttps://img.shields.io/travis/com/python-versioneer/python-versioneer.svg\n[travis-url]: https://travis-ci.com/github/python-versioneer/python-versioneer\n\n\"\"\"\n# pylint:disable=invalid-name,import-outside-toplevel,missing-function-docstring\n# pylint:disable=missing-class-docstring,too-many-branches,too-many-statements\n# pylint:disable=raise-missing-from,too-many-lines,too-many-locals,import-error\n# pylint:disable=too-few-public-methods,redefined-outer-name,consider-using-with\n# pylint:disable=attribute-defined-outside-init,too-many-arguments\n\nimport configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union\nfrom typing import NoReturn\nimport functools\n\nhave_tomllib = True\nif sys.version_info >= (3, 11):\n    import tomllib\nelse:\n    try:\n        import tomli as tomllib\n    except ImportError:\n        have_tomllib = False\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n    VCS: str\n    style: str\n    tag_prefix: str\n    versionfile_source: str\n    versionfile_build: Optional[str]\n    parentdir_prefix: Optional[str]\n    verbose: Optional[bool]\n\n\ndef get_root() -> str:\n    \"\"\"Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    \"\"\"\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, \"setup.py\")\n    pyproject_toml = os.path.join(root, \"pyproject.toml\")\n    versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (\n        os.path.exists(setup_py)\n        or os.path.exists(pyproject_toml)\n        or os.path.exists(versioneer_py)\n    ):\n        # allow 'python path/to/setup.py COMMAND'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, \"setup.py\")\n        pyproject_toml = os.path.join(root, \"pyproject.toml\")\n        versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (\n        os.path.exists(setup_py)\n        or os.path.exists(pyproject_toml)\n        or os.path.exists(versioneer_py)\n    ):\n        err = (\n            \"Versioneer was unable to run the project root directory. \"\n            \"Versioneer requires setup.py to be executed from \"\n            \"its immediate directory (like 'python setup.py COMMAND'), \"\n            \"or in a way that lets it use sys.argv[0] to find the root \"\n            \"(like 'python path/to/setup.py COMMAND').\"\n        )\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # \"versioneer\" may be imported multiple times, and python's shared\n        # module-import table will cache the first one. So we can't use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        my_path = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(my_path)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir and \"VERSIONEER_PEP518\" not in globals():\n            print(\n                \"Warning: build in %s is using versioneer.py from %s\"\n                % (os.path.dirname(my_path), versioneer_py)\n            )\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root: str) -> VersioneerConfig:\n    \"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n    # This might raise OSError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    root_pth = Path(root)\n    pyproject_toml = root_pth / \"pyproject.toml\"\n    setup_cfg = root_pth / \"setup.cfg\"\n    section: Union[Dict[str, Any], configparser.SectionProxy, None] = None\n    if pyproject_toml.exists() and have_tomllib:\n        try:\n            with open(pyproject_toml, \"rb\") as fobj:\n                pp = tomllib.load(fobj)\n            section = pp[\"tool\"][\"versioneer\"]\n        except (tomllib.TOMLDecodeError, KeyError) as e:\n            print(f\"Failed to load config from {pyproject_toml}: {e}\")\n            print(\"Try to load it from setup.cfg\")\n    if not section:\n        parser = configparser.ConfigParser()\n        with open(setup_cfg) as cfg_file:\n            parser.read_file(cfg_file)\n        parser.get(\"versioneer\", \"VCS\")  # raise error if missing\n\n        section = parser[\"versioneer\"]\n\n    # `cast`` really shouldn't be used, but its simplest for the\n    # common VersioneerConfig users at the moment. We verify against\n    # `None` values elsewhere where it matters\n\n    cfg = VersioneerConfig()\n    cfg.VCS = section[\"VCS\"]\n    cfg.style = section.get(\"style\", \"\")\n    cfg.versionfile_source = cast(str, section.get(\"versionfile_source\"))\n    cfg.versionfile_build = section.get(\"versionfile_build\")\n    cfg.tag_prefix = cast(str, section.get(\"tag_prefix\"))\n    if cfg.tag_prefix in (\"''\", '\"\"', None):\n        cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = section.get(\"parentdir_prefix\")\n    if isinstance(section, configparser.SectionProxy):\n        # Make sure configparser translates to bool\n        cfg.verbose = section.getboolean(\"verbose\")\n    else:\n        cfg.verbose = section.get(\"verbose\")\n\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY: Dict[str, str] = {}\nHANDLERS: Dict[str, Dict[str, Callable]] = {}\n\n\ndef register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f: Callable) -> Callable:\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(\n    commands: List[str],\n    args: List[str],\n    cwd: Optional[str] = None,\n    verbose: bool = False,\n    hide_stderr: bool = False,\n    env: Optional[Dict[str, str]] = None,\n) -> Tuple[Optional[str], Optional[int]]:\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs: Dict[str, Any] = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen(\n                [command] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n                **popen_kwargs,\n            )\n            break\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %s\" % (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n\n\nLONG_VERSION_PY[\n    \"git\"\n] = r'''\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain.\n# Generated by versioneer-0.29\n# https://github.com/python-versioneer/python-versioneer\n\n\"\"\"Git implementation of _version.py.\"\"\"\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\nimport functools\n\n\ndef get_keywords() -> Dict[str, str]:\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"%(DOLLAR)sFormat:%%d%(DOLLAR)s\"\n    git_full = \"%(DOLLAR)sFormat:%%H%(DOLLAR)s\"\n    git_date = \"%(DOLLAR)sFormat:%%ci%(DOLLAR)s\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n    VCS: str\n    style: str\n    tag_prefix: str\n    parentdir_prefix: str\n    versionfile_source: str\n    verbose: bool\n\n\ndef get_config() -> VersioneerConfig:\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"%(STYLE)s\"\n    cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n    cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n    cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY: Dict[str, str] = {}\nHANDLERS: Dict[str, Dict[str, Callable]] = {}\n\n\ndef register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f: Callable) -> Callable:\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(\n    commands: List[str],\n    args: List[str],\n    cwd: Optional[str] = None,\n    verbose: bool = False,\n    hide_stderr: bool = False,\n    env: Optional[Dict[str, str]] = None,\n) -> Tuple[Optional[str], Optional[int]]:\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs: Dict[str, Any] = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n                                       stdout=subprocess.PIPE,\n                                       stderr=(subprocess.PIPE if hide_stderr\n                                               else None), **popen_kwargs)\n            break\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %%s\" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %%s\" %% (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %%s (error)\" %% dispcmd)\n            print(\"stdout was %%s\" %% stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n\n\ndef versions_from_parentdir(\n    parentdir_prefix: str,\n    root: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\"version\": dirname[len(parentdir_prefix):],\n                    \"full-revisionid\": None,\n                    \"dirty\": False, \"error\": None, \"date\": None}\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\"Tried directories %%s but none started with prefix %%s\" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs: str) -> Dict[str, str]:\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords: Dict[str, str] = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(\n    keywords: Dict[str, str],\n    tag_prefix: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r'\\d', r)}\n        if verbose:\n            print(\"discarding '%%s', no digits\" %% \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %%s\" %% \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r'\\d', r):\n                continue\n            if verbose:\n                print(\"picking %%s\" %% r)\n            return {\"version\": r,\n                    \"full-revisionid\": keywords[\"full\"].strip(),\n                    \"dirty\": False, \"error\": None,\n                    \"date\": date}\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\"version\": \"0+unknown\",\n            \"full-revisionid\": keywords[\"full\"].strip(),\n            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(\n    tag_prefix: str,\n    root: str,\n    verbose: bool,\n    runner: Callable = run_command\n) -> Dict[str, Any]:\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n                   hide_stderr=not verbose)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %%s not under git control\" %% root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(GITS, [\n        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n    ], cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces: Dict[str, Any] = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n                             cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%%s'\"\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%%s' doesn't start with prefix '%%s'\"\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%%s' doesn't start with prefix '%%s'\"\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n        pieces[\"distance\"] = len(out.split())  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces: Dict[str, Any]) -> str:\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces: Dict[str, Any]) -> str:\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n\n\ndef render_pep440_pre(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%%d.dev%%d\" %% (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%%d\" %% (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%%d\" %% pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%%s\" %% pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%%s\" %% pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_post_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%%s\" %% pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%%s\" %% pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_old(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces: Dict[str, Any], style: str) -> Dict[str, Any]:\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%%s'\" %% style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None,\n            \"date\": pieces.get(\"date\")}\n\n\ndef get_versions() -> Dict[str, Any]:\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for _ in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n                \"dirty\": None,\n                \"error\": \"unable to find root of source tree\",\n                \"date\": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to compute version\", \"date\": None}\n'''\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs: str) -> Dict[str, str]:\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords: Dict[str, str] = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(\n    keywords: Dict[str, str],\n    tag_prefix: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r\"\\d\", r)}\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r\"\\d\", r):\n                continue\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\n                \"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": date,\n            }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False,\n        \"error\": \"no suitable tags\",\n        \"date\": None,\n    }\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(\n    tag_prefix: str, root: str, verbose: bool, runner: Callable = run_command\n) -> Dict[str, Any]:\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root, hide_stderr=not verbose)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(\n        GITS,\n        [\n            \"describe\",\n            \"--tags\",\n            \"--dirty\",\n            \"--always\",\n            \"--long\",\n            \"--match\",\n            f\"{tag_prefix}[[:digit:]]*\",\n        ],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces: Dict[str, Any] = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\"^(.+)-(\\d+)-g([0-9a-f]+)$\", git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = \"unable to parse git-describe output: '%s'\" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = \"tag '%s' doesn't start with prefix '%s'\" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n        pieces[\"distance\"] = len(out.split())  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef do_vcs_install(versionfile_source: str, ipy: Optional[str]) -> None:\n    \"\"\"Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n    files = [versionfile_source]\n    if ipy:\n        files.append(ipy)\n    if \"VERSIONEER_PEP518\" not in globals():\n        try:\n            my_path = __file__\n            if my_path.endswith((\".pyc\", \".pyo\")):\n                my_path = os.path.splitext(my_path)[0] + \".py\"\n            versioneer_file = os.path.relpath(my_path)\n        except NameError:\n            versioneer_file = \"versioneer.py\"\n        files.append(versioneer_file)\n    present = False\n    try:\n        with open(\".gitattributes\", \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(versionfile_source):\n                    if \"export-subst\" in line.strip().split()[1:]:\n                        present = True\n                        break\n    except OSError:\n        pass\n    if not present:\n        with open(\".gitattributes\", \"a+\") as fobj:\n            fobj.write(f\"{versionfile_source} export-subst\\n\")\n        files.append(\".gitattributes\")\n    run_command(GITS, [\"add\", \"--\"] + files)\n\n\ndef versions_from_parentdir(\n    parentdir_prefix: str,\n    root: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                \"version\": dirname[len(parentdir_prefix) :],\n                \"full-revisionid\": None,\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": None,\n            }\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            \"Tried directories %s but none started with prefix %s\"\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\nSHORT_VERSION_PY = \"\"\"\n# This file was generated by 'versioneer.py' (0.29) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = '''\n%s\n'''  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n\"\"\"\n\n\ndef versions_from_file(filename: str) -> Dict[str, Any]:\n    \"\"\"Try to determine the version from _version.py if present.\"\"\"\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except OSError:\n        raise NotThisMethod(\"unable to read _version.py\")\n    mo = re.search(\n        r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\", contents, re.M | re.S\n    )\n    if not mo:\n        mo = re.search(\n            r\"version_json = '''\\r\\n(.*)'''  # END VERSION_JSON\", contents, re.M | re.S\n        )\n    if not mo:\n        raise NotThisMethod(\"no version_json in _version.py\")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename: str, versions: Dict[str, Any]) -> None:\n    \"\"\"Write the given version number to the given _version.py file.\"\"\"\n    contents = json.dumps(versions, sort_keys=True, indent=1, separators=(\",\", \": \"))\n    with open(filename, \"w\") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(\"set %s to '%s'\" % (filename, versions[\"version\"]))\n\n\ndef plus_or_dot(pieces: Dict[str, Any]) -> str:\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces: Dict[str, Any]) -> str:\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n\n\ndef render_pep440_pre(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_post_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_old(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces: Dict[str, Any], style: str) -> Dict[str, Any]:\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\n            \"version\": \"unknown\",\n            \"full-revisionid\": pieces.get(\"long\"),\n            \"dirty\": None,\n            \"error\": pieces[\"error\"],\n            \"date\": None,\n        }\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\n        \"version\": rendered,\n        \"full-revisionid\": pieces[\"long\"],\n        \"dirty\": pieces[\"dirty\"],\n        \"error\": None,\n        \"date\": pieces.get(\"date\"),\n    }\n\n\nclass VersioneerBadRootError(Exception):\n    \"\"\"The project root directory is unknown or missing key files.\"\"\"\n\n\ndef get_versions(verbose: bool = False) -> Dict[str, Any]:\n    \"\"\"Get the project version from whatever source is available.\n\n    Returns dict with two keys: 'version' and 'full'.\n    \"\"\"\n    if \"versioneer\" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[\"versioneer\"]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\n    verbose = verbose or bool(cfg.verbose)  # `bool()` used to avoid `None`\n    assert (\n        cfg.versionfile_source is not None\n    ), \"please set versioneer.versionfile_source\"\n    assert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. 'git\n    # describe'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by 'setup.py sdist',\n    # and for users of a tarball/zipball created by 'git archive' or github's\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(\"get_keywords\")\n    from_keywords_f = handlers.get(\"keywords\")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(\"got version from expanded keyword %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(\"got version from file %s %s\" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(\"pieces_from_vcs\")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(\"got version from VCS %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(\"got version from parentdir %s\" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(\"unable to compute version\")\n\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\",\n        \"date\": None,\n    }\n\n\ndef get_version() -> str:\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]\n\n\ndef get_cmdclass(cmdclass: Optional[Dict[str, Any]] = None):\n    \"\"\"Get the custom setuptools subclasses used by Versioneer.\n\n    If the package uses a different cmdclass (e.g. one from numpy), it\n    should be provide as an argument.\n    \"\"\"\n    if \"versioneer\" in sys.modules:\n        del sys.modules[\"versioneer\"]\n        # this fixes the \"python setup.py develop\" case (also 'install' and\n        # 'easy_install .'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n        # sys.modules by the time B's setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it's pre-build state, so the\n        # parent is protected against the child's \"import versioneer\". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent's versioneer too.\n        # Also see https://github.com/python-versioneer/python-versioneer/issues/52\n\n    cmds = {} if cmdclass is None else cmdclass.copy()\n\n    # we add \"version\" to setuptools\n    from setuptools import Command\n\n    class cmd_version(Command):\n        description = \"report generated version string\"\n        user_options: List[Tuple[str, str, str]] = []\n        boolean_options: List[str] = []\n\n        def initialize_options(self) -> None:\n            pass\n\n        def finalize_options(self) -> None:\n            pass\n\n        def run(self) -> None:\n            vers = get_versions(verbose=True)\n            print(\"Version: %s\" % vers[\"version\"])\n            print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n            print(\" dirty: %s\" % vers.get(\"dirty\"))\n            print(\" date: %s\" % vers.get(\"date\"))\n            if vers[\"error\"]:\n                print(\" error: %s\" % vers[\"error\"])\n\n    cmds[\"version\"] = cmd_version\n\n    # we override \"build_py\" in setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn't copied too, 'git describe' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # pip install -e . and setuptool/editable_wheel will invoke build_py\n    # but the build_py command is not expected to copy any files.\n\n    # we override different \"build_py\" commands for both environments\n    if \"build_py\" in cmds:\n        _build_py: Any = cmds[\"build_py\"]\n    else:\n        from setuptools.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self) -> None:\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            if getattr(self, \"editable_mode\", False):\n                # During editable installs `.py` and data files are\n                # not copied to build_lib\n                return\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n    cmds[\"build_py\"] = cmd_build_py\n\n    if \"build_ext\" in cmds:\n        _build_ext: Any = cmds[\"build_ext\"]\n    else:\n        from setuptools.command.build_ext import build_ext as _build_ext\n\n    class cmd_build_ext(_build_ext):\n        def run(self) -> None:\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_ext.run(self)\n            if self.inplace:\n                # build_ext --inplace will only build extensions in\n                # build/lib<..> dir with no _version.py to write to.\n                # As in place builds will already have a _version.py\n                # in the module dir, we do not need to write one.\n                return\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if not cfg.versionfile_build:\n                return\n            target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n            if not os.path.exists(target_versionfile):\n                print(\n                    f\"Warning: {target_versionfile} does not exist, skipping \"\n                    \"version update. This can happen if you are running build_ext \"\n                    \"without first running build_py.\"\n                )\n                return\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(target_versionfile, versions)\n\n    cmds[\"build_ext\"] = cmd_build_ext\n\n    if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe  # type: ignore\n\n        # nczeczulin reports that py2exe won't like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   \"version\": versioneer.get_version().split(\"+\", 1)[0], # FILEVERSION\n        #   \"product_version\": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self) -> None:\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            \"DOLLAR\": \"$\",\n                            \"STYLE\": cfg.style,\n                            \"TAG_PREFIX\": cfg.tag_prefix,\n                            \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                            \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[\"build_exe\"] = cmd_build_exe\n        del cmds[\"build_py\"]\n\n    if \"py2exe\" in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.setuptools_buildexe import py2exe as _py2exe  # type: ignore\n        except ImportError:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # type: ignore\n\n        class cmd_py2exe(_py2exe):\n            def run(self) -> None:\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            \"DOLLAR\": \"$\",\n                            \"STYLE\": cfg.style,\n                            \"TAG_PREFIX\": cfg.tag_prefix,\n                            \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                            \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[\"py2exe\"] = cmd_py2exe\n\n    # sdist farms its file list building out to egg_info\n    if \"egg_info\" in cmds:\n        _egg_info: Any = cmds[\"egg_info\"]\n    else:\n        from setuptools.command.egg_info import egg_info as _egg_info\n\n    class cmd_egg_info(_egg_info):\n        def find_sources(self) -> None:\n            # egg_info.find_sources builds the manifest list and writes it\n            # in one shot\n            super().find_sources()\n\n            # Modify the filelist and normalize it\n            root = get_root()\n            cfg = get_config_from_root(root)\n            self.filelist.append(\"versioneer.py\")\n            if cfg.versionfile_source:\n                # There are rare cases where versionfile_source might not be\n                # included by default, so we must be explicit\n                self.filelist.append(cfg.versionfile_source)\n            self.filelist.sort()\n            self.filelist.remove_duplicates()\n\n            # The write method is hidden in the manifest_maker instance that\n            # generated the filelist and was thrown away\n            # We will instead replicate their final normalization (to unicode,\n            # and POSIX-style paths)\n            from setuptools import unicode_utils\n\n            normalized = [\n                unicode_utils.filesys_decode(f).replace(os.sep, \"/\")\n                for f in self.filelist.files\n            ]\n\n            manifest_filename = os.path.join(self.egg_info, \"SOURCES.txt\")\n            with open(manifest_filename, \"w\") as fobj:\n                fobj.write(\"\\n\".join(normalized))\n\n    cmds[\"egg_info\"] = cmd_egg_info\n\n    # we override different \"sdist\" commands for both environments\n    if \"sdist\" in cmds:\n        _sdist: Any = cmds[\"sdist\"]\n    else:\n        from setuptools.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self) -> None:\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[\"version\"]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir: str, files: List[str]) -> None:\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(\n                target_versionfile, self._versioneer_generated_versions\n            )\n\n    cmds[\"sdist\"] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = \"\"\"\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or 'python versioneer.py setup'.\n\"\"\"\n\nSAMPLE_CONFIG = \"\"\"\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run 'versioneer.py setup' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n\"\"\"\n\nOLD_SNIPPET = \"\"\"\nfrom ._version import get_versions\n__version__ = get_versions()['version']\ndel get_versions\n\"\"\"\n\nINIT_PY_SNIPPET = \"\"\"\nfrom . import {0}\n__version__ = {0}.get_versions()['version']\n\"\"\"\n\n\ndef do_setup() -> int:\n    \"\"\"Do main VCS-independent setup function for installing Versioneer.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print(\" creating %s\" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, \"w\") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(\n            LONG\n            % {\n                \"DOLLAR\": \"$\",\n                \"STYLE\": cfg.style,\n                \"TAG_PREFIX\": cfg.tag_prefix,\n                \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n            }\n        )\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), \"__init__.py\")\n    maybe_ipy: Optional[str] = ipy\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, \"r\") as f:\n                old = f.read()\n        except OSError:\n            old = \"\"\n        module = os.path.splitext(os.path.basename(cfg.versionfile_source))[0]\n        snippet = INIT_PY_SNIPPET.format(module)\n        if OLD_SNIPPET in old:\n            print(\" replacing boilerplate in %s\" % ipy)\n            with open(ipy, \"w\") as f:\n                f.write(old.replace(OLD_SNIPPET, snippet))\n        elif snippet not in old:\n            print(\" appending to %s\" % ipy)\n            with open(ipy, \"a\") as f:\n                f.write(snippet)\n        else:\n            print(\" %s unmodified\" % ipy)\n    else:\n        print(\" %s doesn't exist, ok\" % ipy)\n        maybe_ipy = None\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(cfg.versionfile_source, maybe_ipy)\n    return 0\n\n\ndef scan_setup_py() -> int:\n    \"\"\"Validate the contents of setup.py against Versioneer's expectations.\"\"\"\n    found = set()\n    setters = False\n    errors = 0\n    with open(\"setup.py\", \"r\") as f:\n        for line in f.readlines():\n            if \"import versioneer\" in line:\n                found.add(\"import\")\n            if \"versioneer.get_cmdclass()\" in line:\n                found.add(\"cmdclass\")\n            if \"versioneer.get_version()\" in line:\n                found.add(\"get_version\")\n            if \"versioneer.VCS\" in line:\n                setters = True\n            if \"versioneer.versionfile_source\" in line:\n                setters = True\n    if len(found) != 3:\n        print(\"\")\n        print(\"Your setup.py appears to be missing some important items\")\n        print(\"(but I might be wrong). Please make sure it has something\")\n        print(\"roughly like the following:\")\n        print(\"\")\n        print(\" import versioneer\")\n        print(\" setup( version=versioneer.get_version(),\")\n        print(\"        cmdclass=versioneer.get_cmdclass(),  ...)\")\n        print(\"\")\n        errors += 1\n    if setters:\n        print(\"You should remove lines like 'versioneer.VCS = ' and\")\n        print(\"'versioneer.versionfile_source = ' . This configuration\")\n        print(\"now lives in setup.cfg, and should be removed from setup.py\")\n        print(\"\")\n        errors += 1\n    return errors\n\n\ndef setup_command() -> NoReturn:\n    \"\"\"Set up Versioneer and exit with appropriate error code.\"\"\"\n    errors = do_setup()\n    errors += scan_setup_py()\n    sys.exit(1 if errors else 0)\n\n\nif __name__ == \"__main__\":\n    cmd = sys.argv[1]\n    if cmd == \"setup\":\n        setup_command()\n", "setup.py": "#!/usr/bin/env python\n\nfrom setuptools import setup\nimport versioneer\n\nwith open(\"requirements.txt\") as file:\n    aiobotocore_version_suffix = \"\"\n    for line in file:\n        parts = line.rstrip().split(\"aiobotocore\")\n        if len(parts) == 2:\n            aiobotocore_version_suffix = parts[1]\n            break\n\nsetup(\n    name=\"s3fs\",\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n    ],\n    description=\"Convenient Filesystem interface over S3\",\n    url=\"http://github.com/fsspec/s3fs/\",\n    maintainer=\"Martin Durant\",\n    maintainer_email=\"mdurant@continuum.io\",\n    license=\"BSD\",\n    keywords=\"s3, boto\",\n    packages=[\"s3fs\"],\n    python_requires=\">= 3.8\",\n    install_requires=[open(\"requirements.txt\").read().strip().split(\"\\n\")],\n    extras_require={\n        \"awscli\": [f\"aiobotocore[awscli]{aiobotocore_version_suffix}\"],\n        \"boto3\": [f\"aiobotocore[boto3]{aiobotocore_version_suffix}\"],\n    },\n    zip_safe=False,\n)\n", "s3fs/core.py": "# -*- coding: utf-8 -*-\nimport asyncio\nimport errno\nimport logging\nimport mimetypes\nimport os\nimport socket\nfrom typing import Tuple, Optional\nimport weakref\nimport re\n\nfrom urllib3.exceptions import IncompleteRead\n\nimport fsspec  # noqa: F401\nfrom fsspec.spec import AbstractBufferedFile\nfrom fsspec.utils import infer_storage_options, tokenize, setup_logging as setup_logger\nfrom fsspec.asyn import (\n    AsyncFileSystem,\n    AbstractAsyncStreamedFile,\n    sync,\n    sync_wrapper,\n    FSTimeoutError,\n    _run_coros_in_chunks,\n)\nfrom fsspec.callbacks import _DEFAULT_CALLBACK\n\nimport aiobotocore\nimport botocore\nimport aiobotocore.session\nfrom aiobotocore.config import AioConfig\nfrom botocore.exceptions import ClientError, HTTPClientError, ParamValidationError\nfrom botocore.parsers import ResponseParserError\n\nfrom s3fs.errors import translate_boto_error\nfrom s3fs.utils import S3BucketRegionCache, ParamKwargsHelper, _get_brange, FileExpired\n\n# ClientPayloadError can be thrown during an incomplete read. aiohttp is a dependency of\n# aiobotocore, we guard the import here in case this dependency is replaced in a future version\n# of aiobotocore.\ntry:\n    from aiohttp import ClientPayloadError\nexcept ImportError:\n    ClientPayloadError = None\n\n\nlogger = logging.getLogger(\"s3fs\")\n\n\ndef setup_logging(level=None):\n\n    setup_logger(logger=logger, level=(level or os.environ[\"S3FS_LOGGING_LEVEL\"]))\n\n\nif \"S3FS_LOGGING_LEVEL\" in os.environ:\n    setup_logging()\n\n\nMANAGED_COPY_THRESHOLD = 5 * 2**30\n# Certain rate-limiting responses can send invalid XML\n# (see https://github.com/fsspec/s3fs/issues/484), which can result in a parser error\n# deep within botocore. So we treat those as retryable as well, even though there could\n# be some false positives.\nS3_RETRYABLE_ERRORS = (\n    socket.timeout,\n    HTTPClientError,\n    IncompleteRead,\n    FSTimeoutError,\n    ResponseParserError,\n)\n\nif ClientPayloadError is not None:\n    S3_RETRYABLE_ERRORS += (ClientPayloadError,)\n\n_VALID_FILE_MODES = {\"r\", \"w\", \"a\", \"rb\", \"wb\", \"ab\"}\n\n_PRESERVE_KWARGS = [\n    \"CacheControl\",\n    \"ContentDisposition\",\n    \"ContentEncoding\",\n    \"ContentLanguage\",\n    \"ContentLength\",\n    \"ContentType\",\n    \"Expires\",\n    \"WebsiteRedirectLocation\",\n    \"ServerSideEncryption\",\n    \"SSECustomerAlgorithm\",\n    \"SSEKMSKeyId\",\n    \"BucketKeyEnabled\",\n    \"StorageClass\",\n    \"ObjectLockMode\",\n    \"ObjectLockRetainUntilDate\",\n    \"ObjectLockLegalHoldStatus\",\n    \"Metadata\",\n]\n\nkey_acls = {\n    \"private\",\n    \"public-read\",\n    \"public-read-write\",\n    \"authenticated-read\",\n    \"aws-exec-read\",\n    \"bucket-owner-read\",\n    \"bucket-owner-full-control\",\n}\nbuck_acls = {\"private\", \"public-read\", \"public-read-write\", \"authenticated-read\"}\n\n\nasync def _error_wrapper(func, *, args=(), kwargs=None, retries):\n    if kwargs is None:\n        kwargs = {}\n    for i in range(retries):\n        try:\n            return await func(*args, **kwargs)\n        except S3_RETRYABLE_ERRORS as e:\n            err = e\n            logger.debug(\"Retryable error: %s\", e)\n            await asyncio.sleep(min(1.7**i * 0.1, 15))\n        except ClientError as e:\n            logger.debug(\"Client error (maybe retryable): %s\", e)\n            err = e\n            wait_time = min(1.7**i * 0.1, 15)\n            if \"SlowDown\" in str(e):\n                await asyncio.sleep(wait_time)\n            elif \"reduce your request rate\" in str(e):\n                await asyncio.sleep(wait_time)\n            elif \"XAmzContentSHA256Mismatch\" in str(e):\n                await asyncio.sleep(wait_time)\n            else:\n                break\n        except Exception as e:\n            logger.debug(\"Nonretryable error: %s\", e)\n            err = e\n            break\n\n    if \"'coroutine'\" in str(err):\n        # aiobotocore internal error - fetch original botocore error\n        tb = err.__traceback__\n        while tb.tb_next:\n            tb = tb.tb_next\n        try:\n            await tb.tb_frame.f_locals[\"response\"]\n        except Exception as e:\n            err = e\n    err = translate_boto_error(err)\n    raise err\n\n\ndef version_id_kw(version_id):\n    \"\"\"Helper to make versionId kwargs.\n\n    Not all boto3 methods accept a None / empty versionId so dictionary expansion solves\n    that problem.\n    \"\"\"\n    if version_id:\n        return {\"VersionId\": version_id}\n    else:\n        return {}\n\n\ndef _coalesce_version_id(*args):\n    \"\"\"Helper to coalesce a list of version_ids down to one\"\"\"\n    version_ids = set(args)\n    if None in version_ids:\n        version_ids.remove(None)\n    if len(version_ids) > 1:\n        raise ValueError(\n            \"Cannot coalesce version_ids where more than one are defined,\"\n            \" {}\".format(version_ids)\n        )\n    elif len(version_ids) == 0:\n        return None\n    else:\n        return version_ids.pop()\n\n\nclass S3FileSystem(AsyncFileSystem):\n    \"\"\"\n    Access S3 as if it were a file system.\n\n    This exposes a filesystem-like API (ls, cp, open, etc.) on top of S3\n    storage.\n\n    Provide credentials either explicitly (``key=``, ``secret=``) or depend\n    on boto's credential methods. See botocore documentation for more\n    information. If no credentials are available, use ``anon=True``.\n\n    Parameters\n    ----------\n    anon : bool (False)\n        Whether to use anonymous connection (public buckets only). If False,\n        uses the key/secret given, or boto's credential resolver (client_kwargs,\n        environment, variables, config files, EC2 IAM server, in that order)\n    endpoint_url : string (None)\n        Use this endpoint_url, if specified. Needed for connecting to non-AWS\n        S3 buckets. Takes precedence over `endpoint_url` in client_kwargs.\n    key : string (None)\n        If not anonymous, use this access key ID, if specified. Takes precedence\n        over `aws_access_key_id` in client_kwargs.\n    secret : string (None)\n        If not anonymous, use this secret access key, if specified. Takes\n        precedence over `aws_secret_access_key` in client_kwargs.\n    token : string (None)\n        If not anonymous, use this security token, if specified\n    use_ssl : bool (True)\n        Whether to use SSL in connections to S3; may be faster without, but\n        insecure. If ``use_ssl`` is also set in ``client_kwargs``,\n        the value set in ``client_kwargs`` will take priority.\n    s3_additional_kwargs : dict of parameters that are used when calling s3 api\n        methods. Typically used for things like \"ServerSideEncryption\".\n    client_kwargs : dict of parameters for the botocore client\n    requester_pays : bool (False)\n        If RequesterPays buckets are supported.\n    default_block_size: int (None)\n        If given, the default block size value used for ``open()``, if no\n        specific value is given at all time. The built-in default is 5MB.\n    default_fill_cache : Bool (True)\n        Whether to use cache filling with open by default. Refer to\n        ``S3File.open``.\n    default_cache_type : string (\"readahead\")\n        If given, the default cache_type value used for ``open()``. Set to \"none\"\n        if no caching is desired. See fsspec's documentation for other available\n        cache_type values. Default cache_type is \"readahead\".\n    version_aware : bool (False)\n        Whether to support bucket versioning.  If enable this will require the\n        user to have the necessary IAM permissions for dealing with versioned\n        objects. Note that in the event that you only need to work with the\n        latest version of objects in a versioned bucket, and do not need the\n        VersionId for those objects, you should set ``version_aware`` to False\n        for performance reasons. When set to True, filesystem instances will\n        use the S3 ListObjectVersions API call to list directory contents,\n        which requires listing all historical object versions.\n    cache_regions : bool (False)\n        Whether to cache bucket regions or not. Whenever a new bucket is used,\n        it will first find out which region it belongs and then use the client\n        for that region.\n    asynchronous :  bool (False)\n        Whether this instance is to be used from inside coroutines.\n    config_kwargs : dict of parameters passed to ``botocore.client.Config``\n    kwargs : other parameters for core session.\n    session : aiobotocore AioSession object to be used for all connections.\n         This session will be used inplace of creating a new session inside S3FileSystem.\n         For example: aiobotocore.session.AioSession(profile='test_user')\n    max_concurrency : int (1)\n        The maximum number of concurrent transfers to use per file for multipart\n        upload (``put()``) operations. Defaults to 1 (sequential). When used in\n        conjunction with ``S3FileSystem.put(batch_size=...)`` the maximum number of\n        simultaneous connections is ``max_concurrency * batch_size``. We may extend\n        this parameter to affect ``pipe()``, ``cat()`` and ``get()``. Increasing this\n        value will result in higher memory usage during multipart upload operations (by\n        ``max_concurrency * chunksize`` bytes per file).\n\n    The following parameters are passed on to fsspec:\n\n    skip_instance_cache: to control reuse of instances\n    use_listings_cache, listings_expiry_time, max_paths: to control reuse of directory listings\n\n    Examples\n    --------\n    >>> s3 = S3FileSystem(anon=False)  # doctest: +SKIP\n    >>> s3.ls('my-bucket/')  # doctest: +SKIP\n    ['my-file.txt']\n\n    >>> with s3.open('my-bucket/my-file.txt', mode='rb') as f:  # doctest: +SKIP\n    ...     print(f.read())  # doctest: +SKIP\n    b'Hello, world!'\n    \"\"\"\n\n    root_marker = \"\"\n    connect_timeout = 5\n    retries = 5\n    read_timeout = 15\n    default_block_size = 5 * 2**20\n    protocol = (\"s3\", \"s3a\")\n    _extra_tokenize_attributes = (\"default_block_size\",)\n\n    def __init__(\n        self,\n        anon=False,\n        endpoint_url=None,\n        key=None,\n        secret=None,\n        token=None,\n        use_ssl=True,\n        client_kwargs=None,\n        requester_pays=False,\n        default_block_size=None,\n        default_fill_cache=True,\n        default_cache_type=\"readahead\",\n        version_aware=False,\n        config_kwargs=None,\n        s3_additional_kwargs=None,\n        session=None,\n        username=None,\n        password=None,\n        cache_regions=False,\n        asynchronous=False,\n        loop=None,\n        max_concurrency=1,\n        **kwargs,\n    ):\n        if key and username:\n            raise KeyError(\"Supply either key or username, not both\")\n        if secret and password:\n            raise KeyError(\"Supply secret or password, not both\")\n        if username:\n            key = username\n        if password:\n            secret = password\n\n        self.endpoint_url = endpoint_url\n\n        self.anon = anon\n        self.key = key\n        self.secret = secret\n        self.token = token\n        self.kwargs = kwargs\n        super_kwargs = {\n            k: kwargs.pop(k)\n            for k in [\"use_listings_cache\", \"listings_expiry_time\", \"max_paths\"]\n            if k in kwargs\n        }  # passed to fsspec superclass\n        super().__init__(loop=loop, asynchronous=asynchronous, **super_kwargs)\n\n        self.default_block_size = default_block_size or self.default_block_size\n        self.default_fill_cache = default_fill_cache\n        self.default_cache_type = default_cache_type\n        self.version_aware = version_aware\n        self.client_kwargs = client_kwargs or {}\n        self.config_kwargs = config_kwargs or {}\n        self.req_kw = {\"RequestPayer\": \"requester\"} if requester_pays else {}\n        self.s3_additional_kwargs = s3_additional_kwargs or {}\n        self.use_ssl = use_ssl\n        self.cache_regions = cache_regions\n        self._s3 = None\n        self.session = session\n        if max_concurrency < 1:\n            raise ValueError(\"max_concurrency must be >= 1\")\n        self.max_concurrency = max_concurrency\n\n    @property\n    def s3(self):\n        if self._s3 is None:\n            if self.asynchronous:\n                raise RuntimeError(\"please await ``.set_session`` before anything else\")\n            self.connect()\n        return self._s3\n\n    def _filter_kwargs(self, s3_method, kwargs):\n        return self._kwargs_helper.filter_dict(s3_method.__name__, kwargs)\n\n    async def get_s3(self, bucket=None):\n        if self.cache_regions and bucket is not None:\n            return await self._s3creator.get_bucket_client(bucket)\n        else:\n            return self._s3\n\n    async def _call_s3(self, method, *akwarglist, **kwargs):\n        await self.set_session()\n        s3 = await self.get_s3(kwargs.get(\"Bucket\"))\n        method = getattr(s3, method)\n        kw2 = kwargs.copy()\n        kw2.pop(\"Body\", None)\n        logger.debug(\"CALL: %s - %s - %s\", method.__name__, akwarglist, kw2)\n        additional_kwargs = self._get_s3_method_kwargs(method, *akwarglist, **kwargs)\n        return await _error_wrapper(\n            method, kwargs=additional_kwargs, retries=self.retries\n        )\n\n    call_s3 = sync_wrapper(_call_s3)\n\n    def _get_s3_method_kwargs(self, method, *akwarglist, **kwargs):\n        additional_kwargs = self.s3_additional_kwargs.copy()\n        for akwargs in akwarglist:\n            additional_kwargs.update(akwargs)\n        # Add the normal kwargs in\n        additional_kwargs.update(kwargs)\n        # filter all kwargs\n        return self._filter_kwargs(method, additional_kwargs)\n\n    @staticmethod\n    def _get_kwargs_from_urls(urlpath):\n        \"\"\"\n        When we have a urlpath that contains a ?versionId=\n\n        Assume that we want to use version_aware mode for\n        the filesystem.\n        \"\"\"\n        url_storage_opts = infer_storage_options(urlpath)\n        url_query = url_storage_opts.get(\"url_query\")\n        out = {}\n        if url_query is not None:\n            from urllib.parse import parse_qs\n\n            parsed = parse_qs(url_query)\n            if \"versionId\" in parsed:\n                out[\"version_aware\"] = True\n        return out\n\n    def _find_bucket_key(self, s3_path):\n        \"\"\"\n        This is a helper function that given an s3 path such that the path is of\n        the form: bucket/key\n        It will return the bucket and the key represented by the s3 path\n        \"\"\"\n\n        bucket_format_list = [\n            re.compile(\n                r\"^(?P<bucket>arn:(aws).*:s3:[a-z\\-0-9]*:[0-9]{12}:accesspoint[:/][^/]+)/?\"\n                r\"(?P<key>.*)$\"\n            ),\n            re.compile(\n                r\"^(?P<bucket>arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:]\"\n                r\"[a-zA-Z0-9\\-]{1,63}[/:](bucket|accesspoint)[/:][a-zA-Z0-9\\-]{1,63})[/:]?(?P<key>.*)$\"\n            ),\n            re.compile(\n                r\"^(?P<bucket>arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:]\"\n                r\"[a-zA-Z0-9\\-]{1,63}[/:]bucket[/:]\"\n                r\"[a-zA-Z0-9\\-]{1,63})[/:]?(?P<key>.*)$\"\n            ),\n            re.compile(\n                r\"^(?P<bucket>arn:(aws).*:s3-object-lambda:[a-z\\-0-9]+:[0-9]{12}:\"\n                r\"accesspoint[/:][a-zA-Z0-9\\-]{1,63})[/:]?(?P<key>.*)$\"\n            ),\n        ]\n        for bucket_format in bucket_format_list:\n            match = bucket_format.match(s3_path)\n            if match:\n                return match.group(\"bucket\"), match.group(\"key\")\n        s3_components = s3_path.split(\"/\", 1)\n        bucket = s3_components[0]\n        s3_key = \"\"\n        if len(s3_components) > 1:\n            s3_key = s3_components[1]\n        return bucket, s3_key\n\n    def split_path(self, path) -> Tuple[str, str, Optional[str]]:\n        \"\"\"\n        Normalise S3 path string into bucket and key.\n\n        Parameters\n        ----------\n        path : string\n            Input path, like `s3://mybucket/path/to/file`\n\n        Examples\n        --------\n        >>> split_path(\"s3://mybucket/path/to/file\")\n        ['mybucket', 'path/to/file', None]\n\n        >>> split_path(\"s3://mybucket/path/to/versioned_file?versionId=some_version_id\")\n        ['mybucket', 'path/to/versioned_file', 'some_version_id']\n        \"\"\"\n        path = self._strip_protocol(path)\n        path = path.lstrip(\"/\")\n        if \"/\" not in path:\n            return path, \"\", None\n        else:\n            bucket, keypart = self._find_bucket_key(path)\n            key, _, version_id = keypart.partition(\"?versionId=\")\n            return (\n                bucket,\n                key,\n                version_id if self.version_aware and version_id else None,\n            )\n\n    def _prepare_config_kwargs(self):\n        config_kwargs = self.config_kwargs.copy()\n        if \"connect_timeout\" not in config_kwargs.keys():\n            config_kwargs[\"connect_timeout\"] = self.connect_timeout\n        if \"read_timeout\" not in config_kwargs.keys():\n            config_kwargs[\"read_timeout\"] = self.read_timeout\n        return config_kwargs\n\n    async def set_session(self, refresh=False, kwargs={}):\n        \"\"\"Establish S3 connection object.\n        Returns\n        -------\n        Session to be closed later with await .close()\n        \"\"\"\n        if self._s3 is not None and not refresh:\n            return self._s3\n        logger.debug(\"Setting up s3fs instance\")\n\n        client_kwargs = self.client_kwargs.copy()\n        init_kwargs = dict(\n            aws_access_key_id=self.key,\n            aws_secret_access_key=self.secret,\n            aws_session_token=self.token,\n            endpoint_url=self.endpoint_url,\n        )\n        init_kwargs = {\n            key: value\n            for key, value in init_kwargs.items()\n            if value is not None and value != client_kwargs.get(key)\n        }\n        if \"use_ssl\" not in client_kwargs.keys():\n            init_kwargs[\"use_ssl\"] = self.use_ssl\n        config_kwargs = self._prepare_config_kwargs()\n        if self.anon:\n            from botocore import UNSIGNED\n\n            drop_keys = {\n                \"aws_access_key_id\",\n                \"aws_secret_access_key\",\n                \"aws_session_token\",\n            }\n            init_kwargs = {\n                key: value for key, value in init_kwargs.items() if key not in drop_keys\n            }\n            client_kwargs = {\n                key: value\n                for key, value in client_kwargs.items()\n                if key not in drop_keys\n            }\n            config_kwargs[\"signature_version\"] = UNSIGNED\n\n        conf = AioConfig(**config_kwargs)\n        if self.session is None:\n            self.session = aiobotocore.session.AioSession(**self.kwargs)\n\n        for parameters in (config_kwargs, self.kwargs, init_kwargs, client_kwargs):\n            for option in (\"region_name\", \"endpoint_url\"):\n                if parameters.get(option):\n                    self.cache_regions = False\n                    break\n        else:\n            cache_regions = self.cache_regions\n\n        logger.debug(\n            \"RC: caching enabled? %r (explicit option is %r)\",\n            cache_regions,\n            self.cache_regions,\n        )\n        self.cache_regions = cache_regions\n        if self.cache_regions:\n            s3creator = S3BucketRegionCache(\n                self.session, config=conf, **init_kwargs, **client_kwargs\n            )\n            self._s3 = await s3creator.get_client()\n        else:\n            s3creator = self.session.create_client(\n                \"s3\", config=conf, **init_kwargs, **client_kwargs\n            )\n            self._s3 = await s3creator.__aenter__()\n\n        self._s3creator = s3creator\n        # the following actually closes the aiohttp connection; use of privates\n        # might break in the future, would cause exception at gc time\n        if not self.asynchronous:\n            weakref.finalize(self, self.close_session, self.loop, self._s3creator)\n        self._kwargs_helper = ParamKwargsHelper(self._s3)\n        return self._s3\n\n    _connect = set_session\n\n    connect = sync_wrapper(set_session)\n\n    @staticmethod\n    def close_session(loop, s3):\n        if loop is not None and loop.is_running():\n            try:\n                loop = asyncio.get_event_loop()\n                loop.create_task(s3.__aexit__(None, None, None))\n                return\n            except RuntimeError:\n                pass\n            try:\n                sync(loop, s3.__aexit__, None, None, None, timeout=0.1)\n                return\n            except FSTimeoutError:\n                pass\n        try:\n            # close the actual socket\n            s3._client._endpoint.http_session._connector._close()\n        except AttributeError:\n            # but during shutdown, it may have gone\n            pass\n\n    async def _get_delegated_s3pars(self, exp=3600):\n        \"\"\"Get temporary credentials from STS, appropriate for sending across a\n        network. Only relevant where the key/secret were explicitly provided.\n\n        Parameters\n        ----------\n        exp : int\n            Time in seconds that credentials are good for\n\n        Returns\n        -------\n        dict of parameters\n        \"\"\"\n        if self.anon:\n            return {\"anon\": True}\n        if self.token:  # already has temporary cred\n            return {\n                \"key\": self.key,\n                \"secret\": self.secret,\n                \"token\": self.token,\n                \"anon\": False,\n            }\n        if self.key is None or self.secret is None:  # automatic credentials\n            return {\"anon\": False}\n        async with self.session.create_client(\"sts\") as sts:\n            cred = sts.get_session_token(DurationSeconds=exp)[\"Credentials\"]\n            return {\n                \"key\": cred[\"AccessKeyId\"],\n                \"secret\": cred[\"SecretAccessKey\"],\n                \"token\": cred[\"SessionToken\"],\n                \"anon\": False,\n            }\n\n    get_delegated_s3pars = sync_wrapper(_get_delegated_s3pars)\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        acl=False,\n        version_id=None,\n        fill_cache=None,\n        cache_type=None,\n        autocommit=True,\n        size=None,\n        requester_pays=None,\n        cache_options=None,\n        **kwargs,\n    ):\n        \"\"\"Open a file for reading or writing\n\n        Parameters\n        ----------\n        path: string\n            Path of file on S3\n        mode: string\n            One of 'r', 'w', 'a', 'rb', 'wb', or 'ab'. These have the same meaning\n            as they do for the built-in `open` function.\n        block_size: int\n            Size of data-node blocks if reading\n        fill_cache: bool\n            If seeking to new a part of the file beyond the current buffer,\n            with this True, the buffer will be filled between the sections to\n            best support random access. When reading only a few specific chunks\n            out of a file, performance may be better if False.\n        acl: str\n            Canned ACL to set when writing. False sends no parameter and uses the bucket's\n            preset default; otherwise it should be a member of the `key_acls` set.\n        version_id : str\n            Explicit version of the object to open.  This requires that the s3\n            filesystem is version aware and bucket versioning is enabled on the\n            relevant bucket.\n        encoding : str\n            The encoding to use if opening the file in text mode. The platform's\n            default text encoding is used if not given.\n        cache_type : str\n            See fsspec's documentation for available cache_type values. Set to \"none\"\n            if no caching is desired. If None, defaults to ``self.default_cache_type``.\n        requester_pays : bool (optional)\n            If RequesterPays buckets are supported.  If None, defaults to the\n            value used when creating the S3FileSystem (which defaults to False.)\n        kwargs: dict-like\n            Additional parameters used for s3 methods.  Typically used for\n            ServerSideEncryption.\n        \"\"\"\n        if block_size is None:\n            block_size = self.default_block_size\n        if fill_cache is None:\n            fill_cache = self.default_fill_cache\n        if requester_pays is None:\n            requester_pays = bool(self.req_kw)\n\n        acl = (\n            acl\n            or self.s3_additional_kwargs.get(\"ACL\", False)\n            or self.s3_additional_kwargs.get(\"acl\", False)\n        )\n        kw = self.s3_additional_kwargs.copy()\n        kw.update(kwargs)\n        if not self.version_aware and version_id:\n            raise ValueError(\n                \"version_id cannot be specified if the filesystem \"\n                \"is not version aware\"\n            )\n\n        if cache_type is None:\n            cache_type = self.default_cache_type\n\n        return S3File(\n            self,\n            path,\n            mode,\n            block_size=block_size,\n            acl=acl,\n            version_id=version_id,\n            fill_cache=fill_cache,\n            s3_additional_kwargs=kw,\n            cache_type=cache_type,\n            autocommit=autocommit,\n            requester_pays=requester_pays,\n            cache_options=cache_options,\n            size=size,\n        )\n\n    async def _lsdir(\n        self,\n        path,\n        refresh=False,\n        max_items=None,\n        delimiter=\"/\",\n        prefix=\"\",\n        versions=False,\n    ):\n        bucket, key, _ = self.split_path(path)\n        if not prefix:\n            prefix = \"\"\n        if key:\n            prefix = key.lstrip(\"/\") + \"/\" + prefix\n        if path not in self.dircache or refresh or not delimiter or versions:\n            try:\n                logger.debug(\"Get directory listing page for %s\" % path)\n                dirs = []\n                files = []\n                async for c in self._iterdir(\n                    bucket,\n                    max_items=max_items,\n                    delimiter=delimiter,\n                    prefix=prefix,\n                    versions=versions,\n                ):\n                    if c[\"type\"] == \"directory\":\n                        dirs.append(c)\n                    else:\n                        files.append(c)\n                files += dirs\n            except ClientError as e:\n                raise translate_boto_error(e)\n\n            if delimiter and files and not versions:\n                self.dircache[path] = files\n            return files\n        return self.dircache[path]\n\n    async def _iterdir(\n        self, bucket, max_items=None, delimiter=\"/\", prefix=\"\", versions=False\n    ):\n        \"\"\"Iterate asynchronously over files and directories under `prefix`.\n\n        The contents are yielded in arbitrary order as info dicts.\n        \"\"\"\n        if versions and not self.version_aware:\n            raise ValueError(\n                \"versions cannot be specified if the filesystem is not version aware\"\n            )\n        await self.set_session()\n        s3 = await self.get_s3(bucket)\n        if self.version_aware:\n            method = \"list_object_versions\"\n            contents_key = \"Versions\"\n        else:\n            method = \"list_objects_v2\"\n            contents_key = \"Contents\"\n        pag = s3.get_paginator(method)\n        config = {}\n        if max_items is not None:\n            config.update(MaxItems=max_items, PageSize=2 * max_items)\n        it = pag.paginate(\n            Bucket=bucket,\n            Prefix=prefix,\n            Delimiter=delimiter,\n            PaginationConfig=config,\n            **self.req_kw,\n        )\n        async for i in it:\n            for l in i.get(\"CommonPrefixes\", []):\n                c = {\n                    \"Key\": l[\"Prefix\"][:-1],\n                    \"Size\": 0,\n                    \"StorageClass\": \"DIRECTORY\",\n                    \"type\": \"directory\",\n                }\n                self._fill_info(c, bucket, versions=False)\n                yield c\n            for c in i.get(contents_key, []):\n                if not self.version_aware or c.get(\"IsLatest\") or versions:\n                    c[\"type\"] = \"file\"\n                    c[\"size\"] = c[\"Size\"]\n                    self._fill_info(c, bucket, versions=versions)\n                    yield c\n\n    @staticmethod\n    def _fill_info(f, bucket, versions=False):\n        f[\"size\"] = f[\"Size\"]\n        f[\"Key\"] = \"/\".join([bucket, f[\"Key\"]])\n        f[\"name\"] = f[\"Key\"]\n        version_id = f.get(\"VersionId\")\n        if versions and version_id and version_id != \"null\":\n            f[\"name\"] += f\"?versionId={version_id}\"\n\n    async def _glob(self, path, **kwargs):\n        if path.startswith(\"*\"):\n            raise ValueError(\"Cannot traverse all of S3\")\n        return await super()._glob(path, **kwargs)\n\n    async def _find(\n        self, path, maxdepth=None, withdirs=None, detail=False, prefix=\"\", **kwargs\n    ):\n        \"\"\"List all files below path.\n        Like posix ``find`` command without conditions\n\n        Parameters\n        ----------\n        path : str\n        maxdepth: int or None\n            If not None, the maximum number of levels to descend\n        withdirs: bool\n            Whether to include directory paths in the output. This is True\n            when used by glob, but users usually only want files.\n        prefix: str\n            Only return files that match ``^{path}/{prefix}`` (if there is an\n            exact match ``filename == {path}/{prefix}``, it also will be included)\n        \"\"\"\n        path = self._strip_protocol(path)\n        bucket, key, _ = self.split_path(path)\n        if not bucket:\n            raise ValueError(\"Cannot traverse all of S3\")\n        if (withdirs or maxdepth) and prefix:\n            # TODO: perhaps propagate these to a glob(f\"path/{prefix}*\") call\n            raise ValueError(\n                \"Can not specify 'prefix' option alongside 'withdirs'/'maxdepth' options.\"\n            )\n        if maxdepth:\n            return await super()._find(\n                bucket + \"/\" + key,\n                maxdepth=maxdepth,\n                withdirs=withdirs,\n                detail=detail,\n                **kwargs,\n            )\n        # TODO: implement find from dircache, if all listings are present\n        # if refresh is False:\n        #     out = incomplete_tree_dirs(self.dircache, path)\n        #     if len(out) == 1:\n        #         await self._find(out[0])\n        #         return super().find(path)\n        #     elif len(out) == 0:\n        #         return super().find(path)\n        #     # else: we refresh anyway, having at least two missing trees\n        out = await self._lsdir(path, delimiter=\"\", prefix=prefix, **kwargs)\n        if not out and key:\n            try:\n                out = [await self._info(path)]\n            except FileNotFoundError:\n                out = []\n        dirs = []\n        sdirs = set()\n        thisdircache = {}\n        for o in out:\n            par = self._parent(o[\"name\"])\n            if par not in sdirs:\n                sdirs.add(par)\n                d = False\n                if len(path) <= len(par):\n                    d = {\n                        \"Key\": self.split_path(par)[1],\n                        \"Size\": 0,\n                        \"name\": par,\n                        \"StorageClass\": \"DIRECTORY\",\n                        \"type\": \"directory\",\n                        \"size\": 0,\n                    }\n                    dirs.append(d)\n                thisdircache[par] = []\n                ppar = self._parent(par)\n                if ppar in thisdircache:\n                    if d and d not in thisdircache[ppar]:\n                        thisdircache[ppar].append(d)\n            if par in sdirs:\n                thisdircache[par].append(o)\n\n        # Explicitly add directories to their parents in the dircache\n        for d in dirs:\n            par = self._parent(d[\"name\"])\n            if par in thisdircache:\n                thisdircache[par].append(d)\n\n        if not prefix:\n            for k, v in thisdircache.items():\n                if k not in self.dircache and len(k) >= len(path):\n                    self.dircache[k] = v\n        if withdirs:\n            out = sorted(out + dirs, key=lambda x: x[\"name\"])\n        if detail:\n            return {o[\"name\"]: o for o in out}\n        return [o[\"name\"] for o in out]\n\n    find = sync_wrapper(_find)\n\n    async def _mkdir(self, path, acl=False, create_parents=True, **kwargs):\n        path = self._strip_protocol(path).rstrip(\"/\")\n        if not path:\n            raise ValueError\n        bucket, key, _ = self.split_path(path)\n        if await self._exists(bucket):\n            if not key:\n                # requested to create bucket, but bucket already exist\n                raise FileExistsError\n            # else: # do nothing as bucket is already created.\n        elif not key or create_parents:\n            if acl and acl not in buck_acls:\n                raise ValueError(\"ACL not in %s\", buck_acls)\n            try:\n                params = {\"Bucket\": bucket}\n                if acl:\n                    params[\"ACL\"] = acl\n                region_name = kwargs.get(\"region_name\", None) or self.client_kwargs.get(\n                    \"region_name\", None\n                )\n                if region_name:\n                    params[\"CreateBucketConfiguration\"] = {\n                        \"LocationConstraint\": region_name\n                    }\n                await self._call_s3(\"create_bucket\", **params)\n                self.invalidate_cache(\"\")\n                self.invalidate_cache(bucket)\n            except ClientError as e:\n                raise translate_boto_error(e)\n            except ParamValidationError as e:\n                raise ValueError(\"Bucket create failed %r: %s\" % (bucket, e))\n        else:\n            # raises if bucket doesn't exist and doesn't get create flag.\n            await self._ls(bucket)\n\n    mkdir = sync_wrapper(_mkdir)\n\n    async def _makedirs(self, path, exist_ok=False):\n        try:\n            await self._mkdir(path, create_parents=True)\n        except FileExistsError:\n            if exist_ok:\n                pass\n            else:\n                raise\n\n    makedirs = sync_wrapper(_makedirs)\n\n    async def _rmdir(self, path):\n        bucket, key, _ = self.split_path(path)\n        if key:\n            if await self._exists(path):\n                # User may have meant rm(path, recursive=True)\n                raise FileExistsError\n            raise FileNotFoundError\n\n        try:\n            await self._call_s3(\"delete_bucket\", Bucket=path)\n        except botocore.exceptions.ClientError as e:\n            if \"NoSuchBucket\" in str(e):\n                raise FileNotFoundError(path) from e\n            if \"BucketNotEmpty\" in str(e):\n                raise OSError from e\n            raise\n        self.invalidate_cache(path)\n        self.invalidate_cache(\"\")\n\n    rmdir = sync_wrapper(_rmdir)\n\n    async def _lsbuckets(self, refresh=False):\n        if \"\" not in self.dircache or refresh:\n            if self.anon:\n                # cannot list buckets if not logged in\n                return []\n            try:\n                files = (await self._call_s3(\"list_buckets\"))[\"Buckets\"]\n            except ClientError:\n                # listbucket permission missing\n                return []\n            for f in files:\n                f[\"Key\"] = f[\"Name\"]\n                f[\"Size\"] = 0\n                f[\"StorageClass\"] = \"BUCKET\"\n                f[\"size\"] = 0\n                f[\"type\"] = \"directory\"\n                f[\"name\"] = f[\"Name\"]\n                del f[\"Name\"]\n            self.dircache[\"\"] = files\n            return files\n        return self.dircache[\"\"]\n\n    async def _ls(self, path, detail=False, refresh=False, versions=False):\n        \"\"\"List files in given bucket, or list of buckets.\n\n        Listing is cached unless `refresh=True`.\n\n        Note: only your buckets associated with the login will be listed by\n        `ls('')`, not any public buckets (even if already accessed).\n\n        Parameters\n        ----------\n        path : string/bytes\n            location at which to list files\n        refresh : bool (=False)\n            if False, look in local cache for file details first\n        \"\"\"\n        path = self._strip_protocol(path).rstrip(\"/\")\n        if path in [\"\", \"/\"]:\n            files = await self._lsbuckets(refresh)\n        else:\n            files = await self._lsdir(path, refresh, versions=versions)\n            if not files and \"/\" in path:\n                try:\n                    files = await self._lsdir(\n                        self._parent(path), refresh=refresh, versions=versions\n                    )\n                except IOError:\n                    pass\n                files = [\n                    o\n                    for o in files\n                    if o[\"name\"].rstrip(\"/\") == path and o[\"type\"] != \"directory\"\n                ]\n                if not files:\n                    raise FileNotFoundError(path)\n            if detail:\n                return files\n        return files if detail else sorted([o[\"name\"] for o in files])\n\n    def _exists_in_cache(self, path, bucket, key, version_id):\n        fullpath = \"/\".join((bucket, key))\n\n        try:\n            entries = self._ls_from_cache(fullpath)\n        except FileNotFoundError:\n            return False\n\n        if entries is None:\n            return None\n\n        if not self.version_aware or version_id is None:\n            return True\n\n        for entry in entries:\n            if entry[\"name\"] == fullpath and entry.get(\"VersionId\") == version_id:\n                return True\n\n        # dircache doesn't support multiple versions, so we really can't tell if\n        # the one we want exists.\n        return None\n\n    async def _exists(self, path):\n        if path in [\"\", \"/\"]:\n            # the root always exists, even if anon\n            return True\n        path = self._strip_protocol(path)\n        bucket, key, version_id = self.split_path(path)\n        if key:\n            exists_in_cache = self._exists_in_cache(path, bucket, key, version_id)\n            if exists_in_cache is not None:\n                return exists_in_cache\n\n            try:\n                await self._info(path, bucket, key, version_id=version_id)\n                return True\n            except FileNotFoundError:\n                return False\n        elif self.dircache.get(bucket, False):\n            return True\n        else:\n            try:\n                if self._ls_from_cache(bucket):\n                    return True\n            except FileNotFoundError:\n                # might still be a bucket we can access but don't own\n                pass\n            try:\n                await self._call_s3(\n                    \"list_objects_v2\", MaxKeys=1, Bucket=bucket, **self.req_kw\n                )\n                return True\n            except Exception:\n                pass\n            try:\n                await self._call_s3(\"get_bucket_location\", Bucket=bucket, **self.req_kw)\n                return True\n            except Exception:\n                return False\n\n    exists = sync_wrapper(_exists)\n\n    async def _touch(self, path, truncate=True, data=None, **kwargs):\n        \"\"\"Create empty file or truncate\"\"\"\n        bucket, key, version_id = self.split_path(path)\n        if version_id:\n            raise ValueError(\"S3 does not support touching existing versions of files\")\n        if not truncate and await self._exists(path):\n            raise ValueError(\"S3 does not support touching existent files\")\n        try:\n            write_result = await self._call_s3(\n                \"put_object\", Bucket=bucket, Key=key, **kwargs\n            )\n        except ClientError as ex:\n            raise translate_boto_error(ex)\n        self.invalidate_cache(self._parent(path))\n        return write_result\n\n    touch = sync_wrapper(_touch)\n\n    async def _cat_file(self, path, version_id=None, start=None, end=None):\n        bucket, key, vers = self.split_path(path)\n        if start is not None or end is not None:\n            head = {\"Range\": await self._process_limits(path, start, end)}\n        else:\n            head = {}\n\n        async def _call_and_read():\n            resp = await self._call_s3(\n                \"get_object\",\n                Bucket=bucket,\n                Key=key,\n                **version_id_kw(version_id or vers),\n                **head,\n                **self.req_kw,\n            )\n            try:\n                return await resp[\"Body\"].read()\n            finally:\n                resp[\"Body\"].close()\n\n        return await _error_wrapper(_call_and_read, retries=self.retries)\n\n    async def _pipe_file(self, path, data, chunksize=50 * 2**20, **kwargs):\n        bucket, key, _ = self.split_path(path)\n        size = len(data)\n        # 5 GB is the limit for an S3 PUT\n        if size < min(5 * 2**30, 2 * chunksize):\n            return await self._call_s3(\n                \"put_object\", Bucket=bucket, Key=key, Body=data, **kwargs\n            )\n        else:\n\n            mpu = await self._call_s3(\n                \"create_multipart_upload\", Bucket=bucket, Key=key, **kwargs\n            )\n\n            # TODO: cancel MPU if the following fails\n            out = [\n                await self._call_s3(\n                    \"upload_part\",\n                    Bucket=bucket,\n                    PartNumber=i + 1,\n                    UploadId=mpu[\"UploadId\"],\n                    Body=data[off : off + chunksize],\n                    Key=key,\n                )\n                for i, off in enumerate(range(0, len(data), chunksize))\n            ]\n\n            parts = [\n                {\"PartNumber\": i + 1, \"ETag\": o[\"ETag\"]} for i, o in enumerate(out)\n            ]\n            await self._call_s3(\n                \"complete_multipart_upload\",\n                Bucket=bucket,\n                Key=key,\n                UploadId=mpu[\"UploadId\"],\n                MultipartUpload={\"Parts\": parts},\n            )\n        self.invalidate_cache(path)\n\n    async def _put_file(\n        self,\n        lpath,\n        rpath,\n        callback=_DEFAULT_CALLBACK,\n        chunksize=50 * 2**20,\n        max_concurrency=None,\n        **kwargs,\n    ):\n        bucket, key, _ = self.split_path(rpath)\n        if os.path.isdir(lpath):\n            if key:\n                # don't make remote \"directory\"\n                return\n            else:\n                await self._mkdir(lpath)\n        size = os.path.getsize(lpath)\n        callback.set_size(size)\n\n        if \"ContentType\" not in kwargs:\n            content_type, _ = mimetypes.guess_type(lpath)\n            if content_type is not None:\n                kwargs[\"ContentType\"] = content_type\n\n        with open(lpath, \"rb\") as f0:\n            if size < min(5 * 2**30, 2 * chunksize):\n                chunk = f0.read()\n                await self._call_s3(\n                    \"put_object\", Bucket=bucket, Key=key, Body=chunk, **kwargs\n                )\n                callback.relative_update(size)\n            else:\n\n                mpu = await self._call_s3(\n                    \"create_multipart_upload\", Bucket=bucket, Key=key, **kwargs\n                )\n                out = await self._upload_file_part_concurrent(\n                    bucket,\n                    key,\n                    mpu,\n                    f0,\n                    callback=callback,\n                    chunksize=chunksize,\n                    max_concurrency=max_concurrency,\n                )\n                parts = [\n                    {\"PartNumber\": i + 1, \"ETag\": o[\"ETag\"]} for i, o in enumerate(out)\n                ]\n                await self._call_s3(\n                    \"complete_multipart_upload\",\n                    Bucket=bucket,\n                    Key=key,\n                    UploadId=mpu[\"UploadId\"],\n                    MultipartUpload={\"Parts\": parts},\n                )\n        while rpath:\n            self.invalidate_cache(rpath)\n            rpath = self._parent(rpath)\n\n    async def _upload_file_part_concurrent(\n        self,\n        bucket,\n        key,\n        mpu,\n        f0,\n        callback=_DEFAULT_CALLBACK,\n        chunksize=50 * 2**20,\n        max_concurrency=None,\n    ):\n        max_concurrency = max_concurrency or self.max_concurrency\n        if max_concurrency < 1:\n            raise ValueError(\"max_concurrency must be >= 1\")\n\n        async def _upload_chunk(chunk, part_number):\n            result = await self._call_s3(\n                \"upload_part\",\n                Bucket=bucket,\n                PartNumber=part_number,\n                UploadId=mpu[\"UploadId\"],\n                Body=chunk,\n                Key=key,\n            )\n            callback.relative_update(len(chunk))\n            return result\n\n        out = []\n        while True:\n            chunks = []\n            for i in range(max_concurrency):\n                chunk = f0.read(chunksize)\n                if chunk:\n                    chunks.append(chunk)\n            if not chunks:\n                break\n            if len(chunks) > 1:\n                out.extend(\n                    await asyncio.gather(\n                        *[\n                            _upload_chunk(chunk, len(out) + i)\n                            for i, chunk in enumerate(chunks, 1)\n                        ]\n                    )\n                )\n            else:\n                out.append(await _upload_chunk(chunk, len(out) + 1))\n        return out\n\n    async def _get_file(\n        self, rpath, lpath, callback=_DEFAULT_CALLBACK, version_id=None, **kwargs\n    ):\n        if os.path.isdir(lpath):\n            return\n        bucket, key, vers = self.split_path(rpath)\n\n        async def _open_file(range: int):\n            kw = self.req_kw.copy()\n            if range:\n                kw[\"Range\"] = f\"bytes={range}-\"\n            resp = await self._call_s3(\n                \"get_object\",\n                Bucket=bucket,\n                Key=key,\n                **version_id_kw(version_id or vers),\n                **kw,\n            )\n            return resp[\"Body\"], resp.get(\"ContentLength\", None)\n\n        body, content_length = await _open_file(range=0)\n        callback.set_size(content_length)\n\n        failed_reads = 0\n        bytes_read = 0\n\n        try:\n            with open(lpath, \"wb\") as f0:\n                while True:\n                    try:\n                        chunk = await body.read(2**16)\n                    except S3_RETRYABLE_ERRORS:\n                        failed_reads += 1\n                        if failed_reads >= self.retries:\n                            # Give up if we've failed too many times.\n                            raise\n                        # Closing the body may result in an exception if we've failed to read from it.\n                        try:\n                            body.close()\n                        except Exception:\n                            pass\n\n                        await asyncio.sleep(min(1.7**failed_reads * 0.1, 15))\n                        # Byte ranges are inclusive, which means we need to be careful to not read the same data twice\n                        # in a failure.\n                        # Examples:\n                        # Read 1 byte -> failure, retry with read_range=0, byte range should be 0-\n                        # Read 1 byte, success. Read 1 byte: failure. Retry with read_range=1, byte-range should be 1-\n                        # Read 1 bytes, success. Read 1 bytes: success. Read 1 byte, failure. Retry with read_range=2,\n                        # byte-range should be 2-.\n                        body, _ = await _open_file(bytes_read)\n                        continue\n\n                    if not chunk:\n                        break\n                    bytes_read += len(chunk)\n                    segment_len = f0.write(chunk)\n                    callback.relative_update(segment_len)\n        finally:\n            try:\n                body.close()\n            except Exception:\n                pass\n\n    async def _info(self, path, bucket=None, key=None, refresh=False, version_id=None):\n        path = self._strip_protocol(path)\n        bucket, key, path_version_id = self.split_path(path)\n        fullpath = \"/\".join((bucket, key))\n\n        if version_id is not None:\n            if not self.version_aware:\n                raise ValueError(\n                    \"version_id cannot be specified if the \"\n                    \"filesystem is not version aware\"\n                )\n        if path in [\"/\", \"\"]:\n            return {\"name\": path, \"size\": 0, \"type\": \"directory\"}\n        version_id = _coalesce_version_id(path_version_id, version_id)\n        if not refresh:\n            out = self._ls_from_cache(fullpath)\n            if out is not None:\n                if self.version_aware and version_id is not None:\n                    # If cached info does not match requested version_id,\n                    # fallback to calling head_object\n                    out = [\n                        o\n                        for o in out\n                        if o[\"name\"] == fullpath and version_id == o.get(\"VersionId\")\n                    ]\n                    if out:\n                        return out[0]\n                else:\n                    out = [o for o in out if o[\"name\"] == fullpath]\n                    if out:\n                        return out[0]\n                    return {\"name\": path, \"size\": 0, \"type\": \"directory\"}\n        if key:\n            try:\n                out = await self._call_s3(\n                    \"head_object\",\n                    self.kwargs,\n                    Bucket=bucket,\n                    Key=key,\n                    **version_id_kw(version_id),\n                    **self.req_kw,\n                )\n                return {\n                    \"ETag\": out.get(\"ETag\", \"\"),\n                    \"LastModified\": out.get(\"LastModified\", \"\"),\n                    \"size\": out[\"ContentLength\"],\n                    \"name\": \"/\".join([bucket, key]),\n                    \"type\": \"file\",\n                    \"StorageClass\": out.get(\"StorageClass\", \"STANDARD\"),\n                    \"VersionId\": out.get(\"VersionId\"),\n                    \"ContentType\": out.get(\"ContentType\"),\n                }\n            except FileNotFoundError:\n                pass\n            except ClientError as e:\n                raise translate_boto_error(e, set_cause=False)\n\n        try:\n            # We check to see if the path is a directory by attempting to list its\n            # contexts. If anything is found, it is indeed a directory\n            out = await self._call_s3(\n                \"list_objects_v2\",\n                self.kwargs,\n                Bucket=bucket,\n                Prefix=key.rstrip(\"/\") + \"/\" if key else \"\",\n                Delimiter=\"/\",\n                MaxKeys=1,\n                **self.req_kw,\n            )\n            if (\n                out.get(\"KeyCount\", 0) > 0\n                or out.get(\"Contents\", [])\n                or out.get(\"CommonPrefixes\", [])\n            ):\n                return {\n                    \"name\": \"/\".join([bucket, key]),\n                    \"type\": \"directory\",\n                    \"size\": 0,\n                    \"StorageClass\": \"DIRECTORY\",\n                }\n\n            raise FileNotFoundError(path)\n        except ClientError as e:\n            raise translate_boto_error(e, set_cause=False)\n        except ParamValidationError as e:\n            raise ValueError(\"Failed to list path %r: %s\" % (path, e))\n\n    async def _checksum(self, path, refresh=False):\n        \"\"\"\n        Unique value for current version of file\n\n        If the checksum is the same from one moment to another, the contents\n        are guaranteed to be the same. If the checksum changes, the contents\n        *might* have changed.\n\n        Parameters\n        ----------\n        path : string/bytes\n            path of file to get checksum for\n        refresh : bool (=False)\n            if False, look in local cache for file details first\n\n        \"\"\"\n\n        info = await self._info(path, refresh=refresh)\n\n        if info[\"type\"] != \"directory\":\n            return int(info[\"ETag\"].strip('\"').split(\"-\")[0], 16)\n        else:\n            return int(tokenize(info), 16)\n\n    checksum = sync_wrapper(_checksum)\n\n    async def _isdir(self, path):\n        path = self._strip_protocol(path).strip(\"/\")\n        # Send buckets to super\n        if \"/\" not in path:\n            if path == \"\":\n                return True\n            try:\n                out = await self._lsdir(path)\n                return True\n            except FileNotFoundError:\n                return False\n\n        if path in self.dircache:\n            for fp in self.dircache[path]:\n                # For files the dircache can contain itself.\n                # If it contains anything other than itself it is a directory.\n                if fp[\"name\"] != path:\n                    return True\n            return False\n\n        parent = self._parent(path)\n        if parent in self.dircache:\n            for f in self.dircache[parent]:\n                if f[\"name\"] == path:\n                    # If we find ourselves return whether we are a directory\n                    return f[\"type\"] == \"directory\"\n            return False\n\n        # This only returns things within the path and NOT the path object itself\n        try:\n            return bool(await self._lsdir(path))\n        except FileNotFoundError:\n            return False\n\n    isdir = sync_wrapper(_isdir)\n\n    async def _object_version_info(self, path, **kwargs):\n        if not self.version_aware:\n            raise ValueError(\n                \"version specific functionality is disabled for \"\n                \"non-version aware filesystems\"\n            )\n        bucket, key, _ = self.split_path(path)\n        kwargs = {}\n        out = {\"IsTruncated\": True}\n        versions = []\n        while out[\"IsTruncated\"]:\n            out = await self._call_s3(\n                \"list_object_versions\",\n                kwargs,\n                Bucket=bucket,\n                Prefix=key,\n                **self.req_kw,\n            )\n            versions.extend(out[\"Versions\"])\n            kwargs.update(\n                {\n                    \"VersionIdMarker\": out.get(\"NextVersionIdMarker\", \"\"),\n                    \"KeyMarker\": out.get(\"NextKeyMarker\", \"\"),\n                }\n            )\n        return versions\n\n    object_version_info = sync_wrapper(_object_version_info)\n\n    _metadata_cache = {}\n\n    async def _metadata(self, path, refresh=False, **kwargs):\n        \"\"\"Return metadata of path.\n\n        Parameters\n        ----------\n        path : string/bytes\n            filename to get metadata for\n        refresh : bool (=False)\n            (ignored)\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n        response = await self._call_s3(\n            \"head_object\",\n            kwargs,\n            Bucket=bucket,\n            Key=key,\n            **version_id_kw(version_id),\n            **self.req_kw,\n        )\n        meta = {k.replace(\"_\", \"-\"): v for k, v in response[\"Metadata\"].items()}\n        return meta\n\n    metadata = sync_wrapper(_metadata)\n\n    def get_tags(self, path):\n        \"\"\"Retrieve tag key/values for the given path\n\n        Returns\n        -------\n        {str: str}\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n        response = self.call_s3(\n            \"get_object_tagging\",\n            Bucket=bucket,\n            Key=key,\n            **version_id_kw(version_id),\n        )\n        return {v[\"Key\"]: v[\"Value\"] for v in response[\"TagSet\"]}\n\n    def put_tags(self, path, tags, mode=\"o\"):\n        \"\"\"Set tags for given existing key\n\n        Tags are a str:str mapping that can be attached to any key, see\n        https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocation-tag-restrictions.html\n\n        This is similar to, but distinct from, key metadata, which is usually\n        set at key creation time.\n\n        Parameters\n        ----------\n        path: str\n            Existing key to attach tags to\n        tags: dict str, str\n            Tags to apply.\n        mode:\n            One of 'o' or 'm'\n            'o': Will over-write any existing tags.\n            'm': Will merge in new tags with existing tags.  Incurs two remote\n            calls.\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n\n        if mode == \"m\":\n            existing_tags = self.get_tags(path=path)\n            existing_tags.update(tags)\n            new_tags = [{\"Key\": k, \"Value\": v} for k, v in existing_tags.items()]\n        elif mode == \"o\":\n            new_tags = [{\"Key\": k, \"Value\": v} for k, v in tags.items()]\n        else:\n            raise ValueError(\"Mode must be {'o', 'm'}, not %s\" % mode)\n\n        tag = {\"TagSet\": new_tags}\n        self.call_s3(\n            \"put_object_tagging\",\n            Bucket=bucket,\n            Key=key,\n            Tagging=tag,\n            **version_id_kw(version_id),\n        )\n\n    async def _getxattr(self, path, attr_name, **kwargs):\n        \"\"\"Get an attribute from the metadata.\n\n        Examples\n        --------\n        >>> mys3fs.getxattr('mykey', 'attribute_1')  # doctest: +SKIP\n        'value_1'\n        \"\"\"\n        attr_name = attr_name.replace(\"_\", \"-\")\n        xattr = await self._metadata(path, **kwargs)\n        if attr_name in xattr:\n            return xattr[attr_name]\n        return None\n\n    getxattr = sync_wrapper(_getxattr)\n\n    async def _setxattr(self, path, copy_kwargs=None, **kw_args):\n        \"\"\"Set metadata.\n\n        Attributes have to be of the form documented in the\n        `Metadata Reference`_.\n\n        Parameters\n        ----------\n        kw_args : key-value pairs like field=\"value\", where the values must be\n            strings. Does not alter existing fields, unless\n            the field appears here - if the value is None, delete the\n            field.\n        copy_kwargs : dict, optional\n            dictionary of additional params to use for the underlying\n            s3.copy_object.\n\n        Examples\n        --------\n        >>> mys3file.setxattr(attribute_1='value1', attribute_2='value2')  # doctest: +SKIP\n        # Example for use with copy_args\n        >>> mys3file.setxattr(copy_kwargs={'ContentType': 'application/pdf'},\n        ...     attribute_1='value1')  # doctest: +SKIP\n\n        .. _Metadata Reference: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-metadata\n        \"\"\"\n\n        kw_args = {k.replace(\"_\", \"-\"): v for k, v in kw_args.items()}\n        bucket, key, version_id = self.split_path(path)\n        metadata = await self._metadata(path)\n        metadata.update(**kw_args)\n        copy_kwargs = copy_kwargs or {}\n\n        # remove all keys that are None\n        for kw_key in kw_args:\n            if kw_args[kw_key] is None:\n                metadata.pop(kw_key, None)\n\n        src = {\"Bucket\": bucket, \"Key\": key}\n        if version_id:\n            src[\"VersionId\"] = version_id\n\n        await self._call_s3(\n            \"copy_object\",\n            copy_kwargs,\n            CopySource=src,\n            Bucket=bucket,\n            Key=key,\n            Metadata=metadata,\n            MetadataDirective=\"REPLACE\",\n        )\n\n        # refresh metadata\n        self._metadata_cache[path] = metadata\n\n    setxattr = sync_wrapper(_setxattr)\n\n    async def _chmod(self, path, acl, recursive=False, **kwargs):\n        \"\"\"Set Access Control on a bucket/key\n\n        See http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl\n\n        Parameters\n        ----------\n        path : string\n            the object to set\n        acl : string\n            the value of ACL to apply\n        recursive : bool\n            whether to apply the ACL to all keys below the given path too\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n        if recursive:\n            allfiles = await self._find(path, withdirs=False)\n            await asyncio.gather(\n                *[self._chmod(p, acl, recursive=False) for p in allfiles]\n            )\n        elif key:\n            if acl not in key_acls:\n                raise ValueError(\"ACL not in %s\", key_acls)\n            await self._call_s3(\n                \"put_object_acl\",\n                kwargs,\n                Bucket=bucket,\n                Key=key,\n                ACL=acl,\n                **version_id_kw(version_id),\n            )\n        if not key:\n            if acl not in buck_acls:\n                raise ValueError(\"ACL not in %s\", buck_acls)\n            await self._call_s3(\"put_bucket_acl\", kwargs, Bucket=bucket, ACL=acl)\n\n    chmod = sync_wrapper(_chmod)\n\n    async def _url(self, path, expires=3600, client_method=\"get_object\", **kwargs):\n        \"\"\"Generate presigned URL to access path by HTTP\n\n        Parameters\n        ----------\n        path : string\n            the key path we are interested in\n        expires : int\n            the number of seconds this signature will be good for.\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n        await self.set_session()\n        s3 = await self.get_s3(bucket)\n        return await s3.generate_presigned_url(\n            ClientMethod=client_method,\n            Params=dict(Bucket=bucket, Key=key, **version_id_kw(version_id), **kwargs),\n            ExpiresIn=expires,\n        )\n\n    url = sync_wrapper(_url)\n\n    async def _merge(self, path, filelist, **kwargs):\n        \"\"\"Create single S3 file from list of S3 files\n\n        Uses multi-part, no data is downloaded. The original files are\n        not deleted.\n\n        Parameters\n        ----------\n        path : str\n            The final file to produce\n        filelist : list of str\n            The paths, in order, to assemble into the final file.\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n        if version_id:\n            raise ValueError(\"Cannot write to an explicit versioned file!\")\n        mpu = await self._call_s3(\n            \"create_multipart_upload\", kwargs, Bucket=bucket, Key=key\n        )\n        # TODO: Make this support versions?\n        out = await asyncio.gather(\n            *[\n                self._call_s3(\n                    \"upload_part_copy\",\n                    kwargs,\n                    Bucket=bucket,\n                    Key=key,\n                    UploadId=mpu[\"UploadId\"],\n                    CopySource=f,\n                    PartNumber=i + 1,\n                )\n                for (i, f) in enumerate(filelist)\n            ]\n        )\n        parts = [\n            {\"PartNumber\": i + 1, \"ETag\": o[\"CopyPartResult\"][\"ETag\"]}\n            for (i, o) in enumerate(out)\n        ]\n        part_info = {\"Parts\": parts}\n        await self._call_s3(\n            \"complete_multipart_upload\",\n            Bucket=bucket,\n            Key=key,\n            UploadId=mpu[\"UploadId\"],\n            MultipartUpload=part_info,\n        )\n        self.invalidate_cache(path)\n\n    merge = sync_wrapper(_merge)\n\n    async def _copy_basic(self, path1, path2, **kwargs):\n        \"\"\"Copy file between locations on S3\n\n        Not allowed where the origin is >5GB - use copy_managed\n        \"\"\"\n        buc1, key1, ver1 = self.split_path(path1)\n        buc2, key2, ver2 = self.split_path(path2)\n        if ver2:\n            raise ValueError(\"Cannot copy to a versioned file!\")\n        try:\n            copy_src = {\"Bucket\": buc1, \"Key\": key1}\n            if ver1:\n                copy_src[\"VersionId\"] = ver1\n            await self._call_s3(\n                \"copy_object\", kwargs, Bucket=buc2, Key=key2, CopySource=copy_src\n            )\n        except ClientError as e:\n            raise translate_boto_error(e)\n        except ParamValidationError as e:\n            raise ValueError(\"Copy failed (%r -> %r): %s\" % (path1, path2, e)) from e\n        self.invalidate_cache(path2)\n\n    async def _copy_etag_preserved(self, path1, path2, size, total_parts, **kwargs):\n        \"\"\"Copy file between locations on S3 as multi-part while preserving\n        the etag (using the same part sizes for each part\"\"\"\n\n        bucket1, key1, version1 = self.split_path(path1)\n        bucket2, key2, version2 = self.split_path(path2)\n\n        mpu = await self._call_s3(\n            \"create_multipart_upload\", Bucket=bucket2, Key=key2, **kwargs\n        )\n        part_infos = await asyncio.gather(\n            *[\n                self._call_s3(\"head_object\", Bucket=bucket1, Key=key1, PartNumber=i)\n                for i in range(1, total_parts + 1)\n            ]\n        )\n\n        parts = []\n        brange_first = 0\n        for i, part_info in enumerate(part_infos, 1):\n            part_size = part_info[\"ContentLength\"]\n            brange_last = brange_first + part_size - 1\n            if brange_last > size:\n                brange_last = size - 1\n\n            part = await self._call_s3(\n                \"upload_part_copy\",\n                Bucket=bucket2,\n                Key=key2,\n                PartNumber=i,\n                UploadId=mpu[\"UploadId\"],\n                CopySource=path1,\n                CopySourceRange=\"bytes=%i-%i\" % (brange_first, brange_last),\n            )\n            parts.append({\"PartNumber\": i, \"ETag\": part[\"CopyPartResult\"][\"ETag\"]})\n            brange_first += part_size\n\n        await self._call_s3(\n            \"complete_multipart_upload\",\n            Bucket=bucket2,\n            Key=key2,\n            UploadId=mpu[\"UploadId\"],\n            MultipartUpload={\"Parts\": parts},\n        )\n        self.invalidate_cache(path2)\n\n    async def _copy_managed(self, path1, path2, size, block=5 * 2**30, **kwargs):\n        \"\"\"Copy file between locations on S3 as multi-part\n\n        block: int\n            The size of the pieces, must be larger than 5MB and at most 5GB.\n            Smaller blocks mean more calls, only useful for testing.\n        \"\"\"\n        if block < 5 * 2**20 or block > 5 * 2**30:\n            raise ValueError(\"Copy block size must be 5MB<=block<=5GB\")\n        bucket, key, version = self.split_path(path2)\n        mpu = await self._call_s3(\n            \"create_multipart_upload\", Bucket=bucket, Key=key, **kwargs\n        )\n        # attempting to do the following calls concurrently with gather causes\n        # occasional \"upload is smaller than the minimum allowed\"\n        out = [\n            await self._call_s3(\n                \"upload_part_copy\",\n                Bucket=bucket,\n                Key=key,\n                PartNumber=i + 1,\n                UploadId=mpu[\"UploadId\"],\n                CopySource=path1,\n                CopySourceRange=\"bytes=%i-%i\" % (brange_first, brange_last),\n            )\n            for i, (brange_first, brange_last) in enumerate(_get_brange(size, block))\n        ]\n        parts = [\n            {\"PartNumber\": i + 1, \"ETag\": o[\"CopyPartResult\"][\"ETag\"]}\n            for i, o in enumerate(out)\n        ]\n        await self._call_s3(\n            \"complete_multipart_upload\",\n            Bucket=bucket,\n            Key=key,\n            UploadId=mpu[\"UploadId\"],\n            MultipartUpload={\"Parts\": parts},\n        )\n        self.invalidate_cache(path2)\n\n    async def _cp_file(self, path1, path2, preserve_etag=None, **kwargs):\n        \"\"\"Copy file between locations on S3.\n\n        preserve_etag: bool\n            Whether to preserve etag while copying. If the file is uploaded\n            as a single part, then it will be always equalivent to the md5\n            hash of the file hence etag will always be preserved. But if the\n            file is uploaded in multi parts, then this option will try to\n            reproduce the same multipart upload while copying and preserve\n            the generated etag.\n        \"\"\"\n        path1 = self._strip_protocol(path1)\n        bucket, key, vers = self.split_path(path1)\n\n        info = await self._info(path1, bucket, key, version_id=vers)\n        size = info[\"size\"]\n\n        _, _, parts_suffix = info.get(\"ETag\", \"\").strip('\"').partition(\"-\")\n        if preserve_etag and parts_suffix:\n            await self._copy_etag_preserved(\n                path1, path2, size, total_parts=int(parts_suffix)\n            )\n        elif size <= MANAGED_COPY_THRESHOLD:\n            # simple copy allowed for <5GB\n            await self._copy_basic(path1, path2, **kwargs)\n        else:\n            # if the preserve_etag is true, either the file is uploaded\n            # on multiple parts or the size is lower than 5GB\n            assert not preserve_etag\n\n            # serial multipart copy\n            await self._copy_managed(path1, path2, size, **kwargs)\n\n    async def _list_multipart_uploads(self, bucket):\n        out = await self._call_s3(\"list_multipart_uploads\", Bucket=bucket)\n        return out.get(\"Contents\", []) or out.get(\"Uploads\", [])\n\n    list_multipart_uploads = sync_wrapper(_list_multipart_uploads)\n\n    async def _clear_multipart_uploads(self, bucket):\n        \"\"\"Remove any partial uploads in the bucket\"\"\"\n        out = await self._list_multipart_uploads(bucket)\n        await asyncio.gather(\n            *[\n                self._call_s3(\n                    \"abort_multipart_upload\",\n                    Bucket=bucket,\n                    Key=upload[\"Key\"],\n                    UploadId=upload[\"UploadId\"],\n                )\n                for upload in out\n            ]\n        )\n\n    clear_multipart_uploads = sync_wrapper(_clear_multipart_uploads)\n\n    async def _bulk_delete(self, pathlist, **kwargs):\n        \"\"\"\n        Remove multiple keys with one call\n\n        Parameters\n        ----------\n        pathlist : list(str)\n            The keys to remove, must all be in the same bucket.\n            Must have 0 < len <= 1000\n        \"\"\"\n        if not pathlist:\n            return []\n        buckets = {self.split_path(path)[0] for path in pathlist}\n        if len(buckets) > 1:\n            raise ValueError(\"Bulk delete files should refer to only one bucket\")\n        bucket = buckets.pop()\n        if len(pathlist) > 1000:\n            raise ValueError(\"Max number of files to delete in one call is 1000\")\n        delete_keys = {\n            \"Objects\": [{\"Key\": self.split_path(path)[1]} for path in pathlist],\n            \"Quiet\": True,\n        }\n        for path in pathlist:\n            self.invalidate_cache(self._parent(path))\n        out = await self._call_s3(\n            \"delete_objects\", kwargs, Bucket=bucket, Delete=delete_keys\n        )\n        # TODO: we report on successes but don't raise on any errors, effectively\n        #  on_error=\"omit\"\n        return [f\"{bucket}/{_['Key']}\" for _ in out.get(\"Deleted\", [])]\n\n    async def _rm_file(self, path, **kwargs):\n        bucket, key, _ = self.split_path(path)\n        self.invalidate_cache(path)\n\n        try:\n            await self._call_s3(\"delete_object\", Bucket=bucket, Key=key)\n        except ClientError as e:\n            raise translate_boto_error(e)\n\n    async def _rm(self, path, recursive=False, **kwargs):\n        if recursive and isinstance(path, str):\n            bucket, key, _ = self.split_path(path)\n            if not key and await self._is_bucket_versioned(bucket):\n                # special path to completely remove versioned bucket\n                await self._rm_versioned_bucket_contents(bucket)\n        paths = await self._expand_path(path, recursive=recursive)\n        files = [p for p in paths if self.split_path(p)[1]]\n        dirs = [p for p in paths if not self.split_path(p)[1]]\n        # TODO: fails if more than one bucket in list\n        out = await _run_coros_in_chunks(\n            [\n                self._bulk_delete(files[i : i + 1000])\n                for i in range(0, len(files), 1000)\n            ],\n            batch_size=3,\n            nofiles=True,\n        )\n        await asyncio.gather(*[self._rmdir(d) for d in dirs])\n        [\n            (self.invalidate_cache(p), self.invalidate_cache(self._parent(p)))\n            for p in paths\n        ]\n        return sum(out, [])\n\n    async def _is_bucket_versioned(self, bucket):\n        return (await self._call_s3(\"get_bucket_versioning\", Bucket=bucket)).get(\n            \"Status\", \"\"\n        ) == \"Enabled\"\n\n    is_bucket_versioned = sync_wrapper(_is_bucket_versioned)\n\n    async def _make_bucket_versioned(self, bucket, versioned: bool = True):\n        \"\"\"Set bucket versioning status\"\"\"\n        status = \"Enabled\" if versioned else \"Suspended\"\n        return await self._call_s3(\n            \"put_bucket_versioning\",\n            Bucket=bucket,\n            VersioningConfiguration={\"Status\": status},\n        )\n\n    make_bucket_versioned = sync_wrapper(_make_bucket_versioned)\n\n    async def _rm_versioned_bucket_contents(self, bucket):\n        \"\"\"Remove a versioned bucket and all contents\"\"\"\n        await self.set_session()\n        s3 = await self.get_s3(bucket)\n        pag = s3.get_paginator(\"list_object_versions\")\n        async for plist in pag.paginate(Bucket=bucket):\n            obs = plist.get(\"Versions\", []) + plist.get(\"DeleteMarkers\", [])\n            delete_keys = {\n                \"Objects\": [\n                    {\"Key\": i[\"Key\"], \"VersionId\": i[\"VersionId\"]} for i in obs\n                ],\n                \"Quiet\": True,\n            }\n            if obs:\n                await self._call_s3(\"delete_objects\", Bucket=bucket, Delete=delete_keys)\n\n    def invalidate_cache(self, path=None):\n        if path is None:\n            self.dircache.clear()\n        else:\n            path = self._strip_protocol(path)\n            self.dircache.pop(path, None)\n            while path:\n                self.dircache.pop(path, None)\n                path = self._parent(path)\n\n    async def _walk(self, path, maxdepth=None, **kwargs):\n        if path in [\"\", \"*\"] + [\"{}://\".format(p) for p in self.protocol]:\n            raise ValueError(\"Cannot crawl all of S3\")\n        async for _ in super()._walk(path, maxdepth=maxdepth, **kwargs):\n            yield _\n\n    def modified(self, path, version_id=None, refresh=False):\n        \"\"\"Return the last modified timestamp of file at `path` as a datetime\"\"\"\n        info = self.info(path=path, version_id=version_id, refresh=refresh)\n        if \"LastModified\" not in info:\n            # This path is a bucket or folder, which do not currently have a modified date\n            raise IsADirectoryError\n        return info[\"LastModified\"]\n\n    def sign(self, path, expiration=100, **kwargs):\n        return self.url(path, expires=expiration, **kwargs)\n\n    async def _invalidate_region_cache(self):\n        \"\"\"Invalidate the region cache (associated with buckets)\n        if ``cache_regions`` is turned on.\"\"\"\n        if not self.cache_regions:\n            return None\n\n        # If the region cache is not initialized, then\n        # do nothing.\n        cache = getattr(self, \"_s3creator\", None)\n        if cache is not None:\n            await cache.clear()\n\n    invalidate_region_cache = sync_wrapper(_invalidate_region_cache)\n\n    async def open_async(self, path, mode=\"rb\", **kwargs):\n        if \"b\" not in mode or kwargs.get(\"compression\"):\n            raise ValueError\n        return S3AsyncStreamedFile(self, path, mode)\n\n\nclass S3File(AbstractBufferedFile):\n    \"\"\"\n    Open S3 key as a file. Data is only loaded and cached on demand.\n\n    Parameters\n    ----------\n    s3 : S3FileSystem\n        botocore connection\n    path : string\n        S3 bucket/key to access\n    mode : str\n        One of 'rb', 'wb', 'ab'. These have the same meaning\n        as they do for the built-in `open` function.\n    block_size : int\n        read-ahead size for finding delimiters\n    fill_cache : bool\n        If seeking to new a part of the file beyond the current buffer,\n        with this True, the buffer will be filled between the sections to\n        best support random access. When reading only a few specific chunks\n        out of a file, performance may be better if False.\n    acl: str\n        Canned ACL to apply\n    version_id : str\n        Optional version to read the file at.  If not specified this will\n        default to the current version of the object.  This is only used for\n        reading.\n    requester_pays : bool (False)\n        If RequesterPays buckets are supported.\n\n    Examples\n    --------\n    >>> s3 = S3FileSystem()  # doctest: +SKIP\n    >>> with s3.open('my-bucket/my-file.txt', mode='rb') as f:  # doctest: +SKIP\n    ...     ...  # doctest: +SKIP\n\n    See Also\n    --------\n    S3FileSystem.open: used to create ``S3File`` objects\n\n    \"\"\"\n\n    retries = 5\n    part_min = 5 * 2**20\n    part_max = 5 * 2**30\n\n    def __init__(\n        self,\n        s3,\n        path,\n        mode=\"rb\",\n        block_size=5 * 2**20,\n        acl=False,\n        version_id=None,\n        fill_cache=True,\n        s3_additional_kwargs=None,\n        autocommit=True,\n        cache_type=\"readahead\",\n        requester_pays=False,\n        cache_options=None,\n        size=None,\n    ):\n        bucket, key, path_version_id = s3.split_path(path)\n        if not key:\n            raise ValueError(\"Attempt to open non key-like path: %s\" % path)\n        self.bucket = bucket\n        self.key = key\n        self.version_id = _coalesce_version_id(version_id, path_version_id)\n        self.acl = acl\n        if self.acl and self.acl not in key_acls:\n            raise ValueError(\"ACL not in %s\", key_acls)\n        self.mpu = None\n        self.parts = None\n        self.fill_cache = fill_cache\n        self.s3_additional_kwargs = s3_additional_kwargs or {}\n        self.req_kw = {\"RequestPayer\": \"requester\"} if requester_pays else {}\n        if \"r\" not in mode:\n            if block_size < 5 * 2**20:\n                raise ValueError(\"Block size must be >=5MB\")\n        else:\n            if version_id and s3.version_aware:\n                self.version_id = version_id\n                self.details = s3.info(path, version_id=version_id)\n                self.size = self.details[\"size\"]\n            elif s3.version_aware:\n                # In this case we have not managed to get the VersionId out of details and\n                # we should invalidate the cache and perform a full head_object since it\n                # has likely been partially populated by ls.\n                s3.invalidate_cache(path)\n                self.details = s3.info(path)\n                self.version_id = self.details.get(\"VersionId\")\n        super().__init__(\n            s3,\n            path,\n            mode,\n            block_size,\n            autocommit=autocommit,\n            cache_type=cache_type,\n            cache_options=cache_options,\n            size=size,\n        )\n        self.s3 = self.fs  # compatibility\n\n        # when not using autocommit we want to have transactional state to manage\n        self.append_block = False\n\n        if \"a\" in mode and s3.exists(path):\n            # See:\n            # put: https://boto3.amazonaws.com/v1/documentation/api/latest\n            # /reference/services/s3.html#S3.Client.put_object\n            #\n            # head: https://boto3.amazonaws.com/v1/documentation/api/latest\n            # /reference/services/s3.html#S3.Client.head_object\n            head = self._call_s3(\n                \"head_object\",\n                self.kwargs,\n                Bucket=bucket,\n                Key=key,\n                **version_id_kw(version_id),\n                **self.req_kw,\n            )\n\n            head = {\n                key: value\n                for key, value in head.items()\n                if key in _PRESERVE_KWARGS and key not in self.s3_additional_kwargs\n            }\n\n            loc = head.pop(\"ContentLength\")\n            if loc < 5 * 2**20:\n                # existing file too small for multi-upload: download\n                self.write(self.fs.cat(self.path))\n            else:\n                self.append_block = True\n            self.loc = loc\n\n            # Reflect head\n            self.s3_additional_kwargs.update(head)\n\n        if \"r\" in mode and size is None and \"ETag\" in self.details:\n            self.req_kw[\"IfMatch\"] = self.details[\"ETag\"]\n\n    def _call_s3(self, method, *kwarglist, **kwargs):\n        return self.fs.call_s3(method, self.s3_additional_kwargs, *kwarglist, **kwargs)\n\n    def _initiate_upload(self):\n        if self.autocommit and not self.append_block and self.tell() < self.blocksize:\n            # only happens when closing small file, use on-shot PUT\n            return\n        logger.debug(\"Initiate upload for %s\" % self)\n        self.parts = []\n        kw = dict(\n            Bucket=self.bucket,\n            Key=self.key,\n        )\n        if self.acl:\n            kw[\"ACL\"] = self.acl\n        self.mpu = self._call_s3(\"create_multipart_upload\", **kw)\n\n        if self.append_block:\n            # use existing data in key when appending,\n            # and block is big enough\n            out = self._call_s3(\n                \"upload_part_copy\",\n                self.s3_additional_kwargs,\n                Bucket=self.bucket,\n                Key=self.key,\n                PartNumber=1,\n                UploadId=self.mpu[\"UploadId\"],\n                CopySource=self.path,\n            )\n            self.parts.append({\"PartNumber\": 1, \"ETag\": out[\"CopyPartResult\"][\"ETag\"]})\n\n    def metadata(self, refresh=False, **kwargs):\n        \"\"\"Return metadata of file.\n        See :func:`~s3fs.S3Filesystem.metadata`.\n\n        Metadata is cached unless `refresh=True`.\n        \"\"\"\n        return self.fs.metadata(self.path, refresh, **kwargs)\n\n    def getxattr(self, xattr_name, **kwargs):\n        \"\"\"Get an attribute from the metadata.\n        See :func:`~s3fs.S3Filesystem.getxattr`.\n\n        Examples\n        --------\n        >>> mys3file.getxattr('attribute_1')  # doctest: +SKIP\n        'value_1'\n        \"\"\"\n        return self.fs.getxattr(self.path, xattr_name, **kwargs)\n\n    def setxattr(self, copy_kwargs=None, **kwargs):\n        \"\"\"Set metadata.\n        See :func:`~s3fs.S3Filesystem.setxattr`.\n\n        Examples\n        --------\n        >>> mys3file.setxattr(attribute_1='value1', attribute_2='value2')  # doctest: +SKIP\n        \"\"\"\n        if self.writable():\n            raise NotImplementedError(\n                \"cannot update metadata while file is open for writing\"\n            )\n        return self.fs.setxattr(self.path, copy_kwargs=copy_kwargs, **kwargs)\n\n    def url(self, **kwargs):\n        \"\"\"HTTP URL to read this file (if it already exists)\"\"\"\n        return self.fs.url(self.path, **kwargs)\n\n    def _fetch_range(self, start, end):\n        try:\n            return _fetch_range(\n                self.fs,\n                self.bucket,\n                self.key,\n                self.version_id,\n                start,\n                end,\n                req_kw=self.req_kw,\n            )\n\n        except OSError as ex:\n            if ex.args[0] == errno.EINVAL and \"pre-conditions\" in ex.args[1]:\n                raise FileExpired(\n                    filename=self.details[\"name\"], e_tag=self.details.get(\"ETag\")\n                ) from ex\n            else:\n                raise\n\n    def _upload_chunk(self, final=False):\n        bucket, key, _ = self.fs.split_path(self.path)\n        logger.debug(\n            \"Upload for %s, final=%s, loc=%s, buffer loc=%s\"\n            % (self, final, self.loc, self.buffer.tell())\n        )\n        if (\n            self.autocommit\n            and not self.append_block\n            and final\n            and self.tell() < self.blocksize\n        ):\n            # only happens when closing small file, use on-shot PUT\n            data1 = False\n        else:\n            self.buffer.seek(0)\n            (data0, data1) = (None, self.buffer.read(self.blocksize))\n\n        while data1:\n            (data0, data1) = (data1, self.buffer.read(self.blocksize))\n            data1_size = len(data1)\n\n            if 0 < data1_size < self.blocksize:\n                remainder = data0 + data1\n                remainder_size = self.blocksize + data1_size\n\n                if remainder_size <= self.part_max:\n                    (data0, data1) = (remainder, None)\n                else:\n                    partition = remainder_size // 2\n                    (data0, data1) = (remainder[:partition], remainder[partition:])\n\n            part = len(self.parts) + 1\n            logger.debug(\"Upload chunk %s, %s\" % (self, part))\n\n            out = self._call_s3(\n                \"upload_part\",\n                Bucket=bucket,\n                PartNumber=part,\n                UploadId=self.mpu[\"UploadId\"],\n                Body=data0,\n                Key=key,\n            )\n\n            part_header = {\"PartNumber\": part, \"ETag\": out[\"ETag\"]}\n            if \"ChecksumSHA256\" in out:\n                part_header[\"ChecksumSHA256\"] = out[\"ChecksumSHA256\"]\n            self.parts.append(part_header)\n\n        if self.autocommit and final:\n            self.commit()\n        return not final\n\n    def commit(self):\n        logger.debug(\"Commit %s\" % self)\n        if self.tell() == 0:\n            if self.buffer is not None:\n                logger.debug(\"Empty file committed %s\" % self)\n                self._abort_mpu()\n                write_result = self.fs.touch(self.path, **self.kwargs)\n        elif not self.parts:\n            if self.buffer is not None:\n                logger.debug(\"One-shot upload of %s\" % self)\n                self.buffer.seek(0)\n                data = self.buffer.read()\n                kw = dict(Key=self.key, Bucket=self.bucket, Body=data, **self.kwargs)\n                if self.acl:\n                    kw[\"ACL\"] = self.acl\n                write_result = self._call_s3(\"put_object\", **kw)\n            else:\n                raise RuntimeError\n        else:\n            logger.debug(\"Complete multi-part upload for %s \" % self)\n            part_info = {\"Parts\": self.parts}\n            write_result = self._call_s3(\n                \"complete_multipart_upload\",\n                Bucket=self.bucket,\n                Key=self.key,\n                UploadId=self.mpu[\"UploadId\"],\n                MultipartUpload=part_info,\n            )\n\n        if self.fs.version_aware:\n            self.version_id = write_result.get(\"VersionId\")\n        # complex cache invalidation, since file's appearance can cause several\n        # directories\n        self.buffer = None\n        parts = self.path.split(\"/\")\n        path = parts[0]\n        for p in parts[1:]:\n            if path in self.fs.dircache and not [\n                True for f in self.fs.dircache[path] if f[\"name\"] == path + \"/\" + p\n            ]:\n                self.fs.invalidate_cache(path)\n            path = path + \"/\" + p\n\n    def discard(self):\n        self._abort_mpu()\n        self.buffer = None  # file becomes unusable\n\n    def _abort_mpu(self):\n        if self.mpu:\n            self._call_s3(\n                \"abort_multipart_upload\",\n                Bucket=self.bucket,\n                Key=self.key,\n                UploadId=self.mpu[\"UploadId\"],\n            )\n            self.mpu = None\n\n\nclass S3AsyncStreamedFile(AbstractAsyncStreamedFile):\n    def __init__(self, fs, path, mode):\n        self.fs = fs\n        self.path = path\n        self.mode = mode\n        self.r = None\n        self.loc = 0\n        self.size = None\n\n    async def read(self, length=-1):\n        if self.r is None:\n            bucket, key, gen = self.fs.split_path(self.path)\n            r = await self.fs._call_s3(\"get_object\", Bucket=bucket, Key=key)\n            self.size = int(r[\"ResponseMetadata\"][\"HTTPHeaders\"][\"content-length\"])\n            self.r = r[\"Body\"]\n        out = await self.r.read(length)\n        self.loc += len(out)\n        return out\n\n\ndef _fetch_range(fs, bucket, key, version_id, start, end, req_kw=None):\n    if req_kw is None:\n        req_kw = {}\n    if start == end:\n        logger.debug(\n            \"skip fetch for negative range - bucket=%s,key=%s,start=%d,end=%d\",\n            bucket,\n            key,\n            start,\n            end,\n        )\n        return b\"\"\n    logger.debug(\"Fetch: %s/%s, %s-%s\", bucket, key, start, end)\n    return sync(fs.loop, _inner_fetch, fs, bucket, key, version_id, start, end, req_kw)\n\n\nasync def _inner_fetch(fs, bucket, key, version_id, start, end, req_kw=None):\n    async def _call_and_read():\n        resp = await fs._call_s3(\n            \"get_object\",\n            Bucket=bucket,\n            Key=key,\n            Range=\"bytes=%i-%i\" % (start, end - 1),\n            **version_id_kw(version_id),\n            **req_kw,\n        )\n        try:\n            return await resp[\"Body\"].read()\n        finally:\n            resp[\"Body\"].close()\n\n    return await _error_wrapper(_call_and_read, retries=fs.retries)\n", "s3fs/utils.py": "import errno\nimport logging\nfrom contextlib import contextmanager, AsyncExitStack\nfrom botocore.exceptions import ClientError\n\n\nlogger = logging.getLogger(\"s3fs\")\n\n\n@contextmanager\ndef ignoring(*exceptions):\n    try:\n        yield\n    except exceptions:\n        pass\n\n\nclass S3BucketRegionCache:\n    # See https://github.com/aio-libs/aiobotocore/issues/866\n    # for details.\n\n    def __init__(self, session, **client_kwargs):\n        self._session = session\n        self._stack = AsyncExitStack()\n        self._client = None\n        self._client_kwargs = client_kwargs\n        self._buckets = {}\n        self._regions = {}\n\n    async def get_bucket_client(self, bucket_name=None):\n        if bucket_name in self._buckets:\n            return self._buckets[bucket_name]\n\n        general_client = await self.get_client()\n        if bucket_name is None:\n            return general_client\n\n        try:\n            response = await general_client.head_bucket(Bucket=bucket_name)\n        except ClientError as e:\n            region = (\n                e.response[\"ResponseMetadata\"]\n                .get(\"HTTPHeaders\", {})\n                .get(\"x-amz-bucket-region\")\n            )\n            if not region:\n                logger.debug(\n                    \"RC: HEAD_BUCKET call for %r has failed, returning the general client\",\n                    bucket_name,\n                )\n                return general_client\n        else:\n            region = response[\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amz-bucket-region\"]\n\n        if region not in self._regions:\n            logger.debug(\n                \"RC: Creating a new regional client for %r on the region %r\",\n                bucket_name,\n                region,\n            )\n            self._regions[region] = await self._stack.enter_async_context(\n                self._session.create_client(\n                    \"s3\", region_name=region, **self._client_kwargs\n                )\n            )\n\n        client = self._buckets[bucket_name] = self._regions[region]\n        return client\n\n    async def get_client(self):\n        if not self._client:\n            self._client = await self._stack.enter_async_context(\n                self._session.create_client(\"s3\", **self._client_kwargs)\n            )\n        return self._client\n\n    async def clear(self):\n        logger.debug(\"RC: discarding all clients\")\n        self._buckets.clear()\n        self._regions.clear()\n        self._client = None\n        await self._stack.aclose()\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc_args):\n        await self.clear()\n\n\nclass FileExpired(IOError):\n    \"\"\"\n    Is raised, when the file content has been changed from a different process after\n    opening the file. Reading the file would lead to invalid or inconsistent output.\n    This can also be triggered by outdated file-information inside the directory cache.\n    In this case ``S3FileSystem.invalidate_cache`` can be used to force an update of\n    the file-information when opening the file.\n    \"\"\"\n\n    def __init__(self, filename: str, e_tag: str):\n        super().__init__(\n            errno.EBUSY,\n            \"The remote file corresponding to filename %s and Etag %s no longer exists.\"\n            % (filename, e_tag),\n        )\n\n\ndef title_case(string):\n    \"\"\"\n    TitleCases a given string.\n\n    Parameters\n    ----------\n    string : underscore separated string\n    \"\"\"\n    return \"\".join(x.capitalize() for x in string.split(\"_\"))\n\n\nclass ParamKwargsHelper(object):\n    \"\"\"\n    Utility class to help extract the subset of keys that an s3 method is\n    actually using\n\n    Parameters\n    ----------\n    s3 : boto S3FileSystem\n    \"\"\"\n\n    _kwarg_cache = {}\n\n    def __init__(self, s3):\n        self.s3 = s3\n\n    def _get_valid_keys(self, model_name):\n        if model_name not in self._kwarg_cache:\n            model = self.s3.meta.service_model.operation_model(model_name)\n            valid_keys = (\n                set(model.input_shape.members.keys())\n                if model.input_shape is not None\n                else set()\n            )\n            self._kwarg_cache[model_name] = valid_keys\n        return self._kwarg_cache[model_name]\n\n    def filter_dict(self, method_name, d):\n        model_name = title_case(method_name)\n        valid_keys = self._get_valid_keys(model_name)\n        if isinstance(d, SSEParams):\n            d = d.to_kwargs()\n        return {k: v for k, v in d.items() if k in valid_keys}\n\n\nclass SSEParams(object):\n    def __init__(\n        self,\n        server_side_encryption=None,\n        sse_customer_algorithm=None,\n        sse_customer_key=None,\n        sse_kms_key_id=None,\n    ):\n        self.ServerSideEncryption = server_side_encryption\n        self.SSECustomerAlgorithm = sse_customer_algorithm\n        self.SSECustomerKey = sse_customer_key\n        self.SSEKMSKeyId = sse_kms_key_id\n\n    def to_kwargs(self):\n        return {k: v for k, v in self.__dict__.items() if v is not None}\n\n\ndef _get_brange(size, block):\n    \"\"\"\n    Chunk up a file into zero-based byte ranges\n\n    Parameters\n    ----------\n    size : file size\n    block : block size\n    \"\"\"\n    for offset in range(0, size, block):\n        yield offset, min(offset + block - 1, size - 1)\n", "s3fs/mapping.py": "from .core import S3FileSystem\n\n\ndef S3Map(root, s3, check=False, create=False):\n    \"\"\"Mirror previous class, not implemented in fsspec\"\"\"\n    s3 = s3 or S3FileSystem.current()\n    return s3.get_mapper(root, check=check, create=create)\n", "s3fs/errors.py": "\"\"\"S3 error codes adapted into more natural Python ones.\n\nAdapted from: https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html\n\"\"\"\n\nimport errno\nimport functools\n\n\n# Fallback values since some systems might not have these.\nENAMETOOLONG = getattr(errno, \"ENAMETOOLONG\", errno.EINVAL)\nENOTEMPTY = getattr(errno, \"ENOTEMPTY\", errno.EINVAL)\nEMSGSIZE = getattr(errno, \"EMSGSIZE\", errno.EINVAL)\nEREMOTEIO = getattr(errno, \"EREMOTEIO\", errno.EIO)\nEREMCHG = getattr(errno, \"EREMCHG\", errno.ENOENT)\n\n\nERROR_CODE_TO_EXCEPTION = {\n    \"AccessDenied\": PermissionError,\n    \"AccountProblem\": PermissionError,\n    \"AllAccessDisabled\": PermissionError,\n    \"AmbiguousGrantByEmailAddress\": functools.partial(IOError, errno.EINVAL),\n    \"AuthorizationHeaderMalformed\": functools.partial(IOError, errno.EINVAL),\n    \"BadDigest\": functools.partial(IOError, errno.EINVAL),\n    \"BucketAlreadyExists\": FileExistsError,\n    \"BucketAlreadyOwnedByYou\": FileExistsError,\n    \"BucketNotEmpty\": functools.partial(IOError, ENOTEMPTY),\n    \"CredentialsNotSupported\": functools.partial(IOError, errno.EINVAL),\n    \"CrossLocationLoggingProhibited\": PermissionError,\n    \"EntityTooSmall\": functools.partial(IOError, errno.EINVAL),\n    \"EntityTooLarge\": functools.partial(IOError, EMSGSIZE),\n    \"ExpiredToken\": PermissionError,\n    \"IllegalLocationConstraintException\": PermissionError,\n    \"IllegalVersioningConfigurationException\": functools.partial(IOError, errno.EINVAL),\n    \"IncompleteBody\": functools.partial(IOError, errno.EINVAL),\n    \"IncorrectNumberOfFilesInPostRequest\": functools.partial(IOError, errno.EINVAL),\n    \"InlineDataTooLarge\": functools.partial(IOError, EMSGSIZE),\n    \"InternalError\": functools.partial(IOError, EREMOTEIO),\n    \"InvalidAccessKeyId\": PermissionError,\n    \"InvalidAddressingHeader\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidArgument\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidBucketName\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidBucketState\": functools.partial(IOError, errno.EPERM),\n    \"InvalidDigest\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidEncryptionAlgorithmError\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidLocationConstraint\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidObjectState\": PermissionError,\n    \"InvalidPart\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidPartOrder\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidPayer\": PermissionError,\n    \"InvalidPolicyDocument\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidRange\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidRequest\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidSecurity\": PermissionError,\n    \"InvalidSOAPRequest\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidStorageClass\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidTargetBucketForLogging\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidToken\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidURI\": functools.partial(IOError, errno.EINVAL),\n    \"KeyTooLongError\": functools.partial(IOError, ENAMETOOLONG),\n    \"MalformedACLError\": functools.partial(IOError, errno.EINVAL),\n    \"MalformedPOSTRequest\": functools.partial(IOError, errno.EINVAL),\n    \"MalformedXML\": functools.partial(IOError, errno.EINVAL),\n    \"MaxMessageLengthExceeded\": functools.partial(IOError, EMSGSIZE),\n    \"MaxPostPreDataLengthExceededError\": functools.partial(IOError, EMSGSIZE),\n    \"MetadataTooLarge\": functools.partial(IOError, EMSGSIZE),\n    \"MethodNotAllowed\": functools.partial(IOError, errno.EPERM),\n    \"MissingAttachment\": functools.partial(IOError, errno.EINVAL),\n    \"MissingContentLength\": functools.partial(IOError, errno.EINVAL),\n    \"MissingRequestBodyError\": functools.partial(IOError, errno.EINVAL),\n    \"MissingSecurityElement\": functools.partial(IOError, errno.EINVAL),\n    \"MissingSecurityHeader\": functools.partial(IOError, errno.EINVAL),\n    \"NoLoggingStatusForKey\": functools.partial(IOError, errno.EINVAL),\n    \"NoSuchBucket\": FileNotFoundError,\n    \"NoSuchBucketPolicy\": FileNotFoundError,\n    \"NoSuchKey\": FileNotFoundError,\n    \"NoSuchLifecycleConfiguration\": FileNotFoundError,\n    \"NoSuchUpload\": FileNotFoundError,\n    \"NoSuchVersion\": FileNotFoundError,\n    \"NotImplemented\": functools.partial(IOError, errno.ENOSYS),\n    \"NotSignedUp\": PermissionError,\n    \"OperationAborted\": functools.partial(IOError, errno.EBUSY),\n    \"PermanentRedirect\": functools.partial(IOError, EREMCHG),\n    \"PreconditionFailed\": functools.partial(IOError, errno.EINVAL),\n    \"Redirect\": functools.partial(IOError, EREMCHG),\n    \"RestoreAlreadyInProgress\": functools.partial(IOError, errno.EBUSY),\n    \"RequestIsNotMultiPartContent\": functools.partial(IOError, errno.EINVAL),\n    \"RequestTimeout\": TimeoutError,\n    \"RequestTimeTooSkewed\": PermissionError,\n    \"RequestTorrentOfBucketError\": functools.partial(IOError, errno.EPERM),\n    \"SignatureDoesNotMatch\": PermissionError,\n    \"ServiceUnavailable\": functools.partial(IOError, errno.EBUSY),\n    \"SlowDown\": functools.partial(IOError, errno.EBUSY),\n    \"TemporaryRedirect\": functools.partial(IOError, EREMCHG),\n    \"TokenRefreshRequired\": functools.partial(IOError, errno.EINVAL),\n    \"TooManyBuckets\": functools.partial(IOError, errno.EINVAL),\n    \"UnexpectedContent\": functools.partial(IOError, errno.EINVAL),\n    \"UnresolvableGrantByEmailAddress\": functools.partial(IOError, errno.EINVAL),\n    \"UserKeyMustBeSpecified\": functools.partial(IOError, errno.EINVAL),\n    \"301\": functools.partial(IOError, EREMCHG),  # PermanentRedirect\n    \"307\": functools.partial(IOError, EREMCHG),  # Redirect\n    \"400\": functools.partial(IOError, errno.EINVAL),\n    \"403\": PermissionError,\n    \"404\": FileNotFoundError,\n    \"405\": functools.partial(IOError, errno.EPERM),\n    \"409\": functools.partial(IOError, errno.EBUSY),\n    \"412\": functools.partial(IOError, errno.EINVAL),  # PreconditionFailed\n    \"416\": functools.partial(IOError, errno.EINVAL),  # InvalidRange\n    \"500\": functools.partial(IOError, EREMOTEIO),  # InternalError\n    \"501\": functools.partial(IOError, errno.ENOSYS),  # NotImplemented\n    \"503\": functools.partial(IOError, errno.EBUSY),  # SlowDown\n}\n\n\ndef translate_boto_error(error, message=None, set_cause=True, *args, **kwargs):\n    \"\"\"Convert a ClientError exception into a Python one.\n\n    Parameters\n    ----------\n\n    error : botocore.exceptions.ClientError\n        The exception returned by the boto API.\n    message : str\n        An error message to use for the returned exception. If not given, the\n        error message returned by the server is used instead.\n    set_cause : bool\n        Whether to set the __cause__ attribute to the previous exception if the\n        exception is translated.\n    *args, **kwargs :\n        Additional arguments to pass to the exception constructor, after the\n        error message. Useful for passing the filename arguments to ``IOError``.\n\n    Returns\n    -------\n\n    An instantiated exception ready to be thrown. If the error code isn't\n    recognized, an IOError with the original error message is returned.\n    \"\"\"\n    error_response = getattr(error, \"response\", None)\n    if error_response is None:\n        # non-http error, or response is None:\n        return error\n    code = error_response[\"Error\"].get(\"Code\")\n    constructor = ERROR_CODE_TO_EXCEPTION.get(code)\n    if constructor:\n        if not message:\n            message = error_response[\"Error\"].get(\"Message\", str(error))\n        custom_exc = constructor(message, *args, **kwargs)\n    else:\n        # No match found, wrap this in an IOError with the appropriate message.\n        custom_exc = IOError(errno.EIO, message or str(error), *args)\n\n    if set_cause:\n        custom_exc.__cause__ = error\n    return custom_exc\n", "s3fs/__init__.py": "from .core import S3FileSystem, S3File\nfrom .mapping import S3Map\n\nfrom ._version import get_versions\n\n__version__ = get_versions()[\"version\"]\ndel get_versions\n", "s3fs/_version.py": "# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain.\n# Generated by versioneer-0.29\n# https://github.com/python-versioneer/python-versioneer\n\n\"\"\"Git implementation of _version.py.\"\"\"\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\nimport functools\n\n\ndef get_keywords() -> Dict[str, str]:\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"$Format:%d$\"\n    git_full = \"$Format:%H$\"\n    git_date = \"$Format:%ci$\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n    VCS: str\n    style: str\n    tag_prefix: str\n    parentdir_prefix: str\n    versionfile_source: str\n    verbose: bool\n\n\ndef get_config() -> VersioneerConfig:\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"s3fs/_version.py\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY: Dict[str, str] = {}\nHANDLERS: Dict[str, Dict[str, Callable]] = {}\n\n\ndef register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f: Callable) -> Callable:\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(\n    commands: List[str],\n    args: List[str],\n    cwd: Optional[str] = None,\n    verbose: bool = False,\n    hide_stderr: bool = False,\n    env: Optional[Dict[str, str]] = None,\n) -> Tuple[Optional[str], Optional[int]]:\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs: Dict[str, Any] = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen(\n                [command] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n                **popen_kwargs,\n            )\n            break\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %s\" % (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n\n\ndef versions_from_parentdir(\n    parentdir_prefix: str,\n    root: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                \"version\": dirname[len(parentdir_prefix) :],\n                \"full-revisionid\": None,\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": None,\n            }\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            \"Tried directories %s but none started with prefix %s\"\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs: str) -> Dict[str, str]:\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords: Dict[str, str] = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(\n    keywords: Dict[str, str],\n    tag_prefix: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r\"\\d\", r)}\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r\"\\d\", r):\n                continue\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\n                \"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": date,\n            }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False,\n        \"error\": \"no suitable tags\",\n        \"date\": None,\n    }\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(\n    tag_prefix: str, root: str, verbose: bool, runner: Callable = run_command\n) -> Dict[str, Any]:\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root, hide_stderr=not verbose)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(\n        GITS,\n        [\n            \"describe\",\n            \"--tags\",\n            \"--dirty\",\n            \"--always\",\n            \"--long\",\n            \"--match\",\n            f\"{tag_prefix}[[:digit:]]*\",\n        ],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces: Dict[str, Any] = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\"^(.+)-(\\d+)-g([0-9a-f]+)$\", git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = \"unable to parse git-describe output: '%s'\" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = \"tag '%s' doesn't start with prefix '%s'\" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n        pieces[\"distance\"] = len(out.split())  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces: Dict[str, Any]) -> str:\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces: Dict[str, Any]) -> str:\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n\n\ndef render_pep440_pre(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_post_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_old(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces: Dict[str, Any], style: str) -> Dict[str, Any]:\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\n            \"version\": \"unknown\",\n            \"full-revisionid\": pieces.get(\"long\"),\n            \"dirty\": None,\n            \"error\": pieces[\"error\"],\n            \"date\": None,\n        }\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\n        \"version\": rendered,\n        \"full-revisionid\": pieces[\"long\"],\n        \"dirty\": pieces[\"dirty\"],\n        \"error\": None,\n        \"date\": pieces.get(\"date\"),\n    }\n\n\ndef get_versions() -> Dict[str, Any]:\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for _ in cfg.versionfile_source.split(\"/\"):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\n            \"version\": \"0+unknown\",\n            \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to find root of source tree\",\n            \"date\": None,\n        }\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\",\n        \"date\": None,\n    }\n"}