{"versioneer.py": "# Version: 0.29\n\n\"\"\"The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/python-versioneer/python-versioneer\n* Brian Warner\n* License: Public Domain (Unlicense)\n* Compatible with: Python 3.7, 3.8, 3.9, 3.10, 3.11 and pypy3\n* [![Latest Version][pypi-image]][pypi-url]\n* [![Build Status][travis-image]][travis-url]\n\nThis is a tool for managing a recorded version number in setuptools-based\npython projects. The goal is to remove the tedious and error-prone \"update\nthe embedded version string\" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\nVersioneer provides two installation modes. The \"classic\" vendored mode installs\na copy of versioneer into your repository. The experimental build-time dependency mode\nis intended to allow you to skip this step and simplify the process of upgrading.\n\n### Vendored mode\n\n* `pip install versioneer` to somewhere in your $PATH\n   * A [conda-forge recipe](https://github.com/conda-forge/versioneer-feedstock) is\n     available, so you can also use `conda install -c conda-forge versioneer`\n* add a `[tool.versioneer]` section to your `pyproject.toml` or a\n  `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n   * Note that you will need to add `tomli; python_version < \"3.11\"` to your\n     build-time dependencies if you use `pyproject.toml`\n* run `versioneer install --vendor` in your source tree, commit the results\n* verify version information with `python setup.py version`\n\n### Build-time dependency mode\n\n* `pip install versioneer` to somewhere in your $PATH\n   * A [conda-forge recipe](https://github.com/conda-forge/versioneer-feedstock) is\n     available, so you can also use `conda install -c conda-forge versioneer`\n* add a `[tool.versioneer]` section to your `pyproject.toml` or a\n  `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n* add `versioneer` (with `[toml]` extra, if configuring in `pyproject.toml`)\n  to the `requires` key of the `build-system` table in `pyproject.toml`:\n  ```toml\n  [build-system]\n  requires = [\"setuptools\", \"versioneer[toml]\"]\n  build-backend = \"setuptools.build_meta\"\n  ```\n* run `versioneer install --no-vendor` in your source tree, commit the results\n* verify version information with `python setup.py version`\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github's\n  \"tarball from tag\" feature\n* a release tarball, produced by \"setup.py sdist\", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. \"git describe\" (for checkouts), which knows\n  about recent \"tags\" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. \"myproject-1.2\" instead of just \"1.2\"), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n\"0.7-1-g574ab98-dirty\" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of \"574ab98\", and is \"dirty\" (it has\nuncommitted changes).\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a 'setup.py sdist' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the \"outside\" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `['version']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project's version\n  string. The default \"pep440\" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the \"Styles\" section\n  below for alternative styles.\n\n* `['full-revisionid']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. \"1076c978a8d3cfc70f408fe5974aa6c092c949ac\".\n\n* `['date']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `['dirty']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `['error']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of \"unknown\".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an \"about\" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()['version']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, \"pep440\", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional \"local\nversion\" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example \"0.11+2.g1076c97.dirty\" indicates that the\ntree is like the \"1076c97\" commit but has uncommitted changes (\".dirty\"), and\nthat this commit is two revisions (\"+2\") beyond the \"0.11\" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. \"0.11\".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of \"0+unknown\". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/python-versioneer/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  \"master\" and \"slave\" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other languages) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/python-versioneer/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/python-versioneer/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n\"Entry-point scripts\" (`setup(entry_points={\"console_scripts\": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/python-versioneer/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg` and `pyproject.toml`, if necessary,\n  to include any new configuration settings indicated by the release notes.\n  See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install --[no-]vendor` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n## Similar projects\n\n* [setuptools_scm](https://github.com/pypa/setuptools_scm/) - a non-vendored build-time\n  dependency\n* [minver](https://github.com/jbweston/miniver) - a lightweight reimplementation of\n  versioneer\n* [versioningit](https://github.com/jwodder/versioningit) - a PEP 518-based setuptools\n  plugin\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the \"Unlicense\", as described in\nhttps://unlicense.org/.\n\n[pypi-image]: https://img.shields.io/pypi/v/versioneer.svg\n[pypi-url]: https://pypi.python.org/pypi/versioneer/\n[travis-image]:\nhttps://img.shields.io/travis/com/python-versioneer/python-versioneer.svg\n[travis-url]: https://travis-ci.com/github/python-versioneer/python-versioneer\n\n\"\"\"\n# pylint:disable=invalid-name,import-outside-toplevel,missing-function-docstring\n# pylint:disable=missing-class-docstring,too-many-branches,too-many-statements\n# pylint:disable=raise-missing-from,too-many-lines,too-many-locals,import-error\n# pylint:disable=too-few-public-methods,redefined-outer-name,consider-using-with\n# pylint:disable=attribute-defined-outside-init,too-many-arguments\n\nimport configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union\nfrom typing import NoReturn\nimport functools\n\nhave_tomllib = True\nif sys.version_info >= (3, 11):\n    import tomllib\nelse:\n    try:\n        import tomli as tomllib\n    except ImportError:\n        have_tomllib = False\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n    VCS: str\n    style: str\n    tag_prefix: str\n    versionfile_source: str\n    versionfile_build: Optional[str]\n    parentdir_prefix: Optional[str]\n    verbose: Optional[bool]\n\n\ndef get_root() -> str:\n    \"\"\"Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    \"\"\"\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, \"setup.py\")\n    pyproject_toml = os.path.join(root, \"pyproject.toml\")\n    versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (\n        os.path.exists(setup_py)\n        or os.path.exists(pyproject_toml)\n        or os.path.exists(versioneer_py)\n    ):\n        # allow 'python path/to/setup.py COMMAND'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, \"setup.py\")\n        pyproject_toml = os.path.join(root, \"pyproject.toml\")\n        versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (\n        os.path.exists(setup_py)\n        or os.path.exists(pyproject_toml)\n        or os.path.exists(versioneer_py)\n    ):\n        err = (\n            \"Versioneer was unable to run the project root directory. \"\n            \"Versioneer requires setup.py to be executed from \"\n            \"its immediate directory (like 'python setup.py COMMAND'), \"\n            \"or in a way that lets it use sys.argv[0] to find the root \"\n            \"(like 'python path/to/setup.py COMMAND').\"\n        )\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # \"versioneer\" may be imported multiple times, and python's shared\n        # module-import table will cache the first one. So we can't use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        my_path = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(my_path)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir and \"VERSIONEER_PEP518\" not in globals():\n            print(\n                \"Warning: build in %s is using versioneer.py from %s\"\n                % (os.path.dirname(my_path), versioneer_py)\n            )\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root: str) -> VersioneerConfig:\n    \"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n    # This might raise OSError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    root_pth = Path(root)\n    pyproject_toml = root_pth / \"pyproject.toml\"\n    setup_cfg = root_pth / \"setup.cfg\"\n    section: Union[Dict[str, Any], configparser.SectionProxy, None] = None\n    if pyproject_toml.exists() and have_tomllib:\n        try:\n            with open(pyproject_toml, \"rb\") as fobj:\n                pp = tomllib.load(fobj)\n            section = pp[\"tool\"][\"versioneer\"]\n        except (tomllib.TOMLDecodeError, KeyError) as e:\n            print(f\"Failed to load config from {pyproject_toml}: {e}\")\n            print(\"Try to load it from setup.cfg\")\n    if not section:\n        parser = configparser.ConfigParser()\n        with open(setup_cfg) as cfg_file:\n            parser.read_file(cfg_file)\n        parser.get(\"versioneer\", \"VCS\")  # raise error if missing\n\n        section = parser[\"versioneer\"]\n\n    # `cast`` really shouldn't be used, but its simplest for the\n    # common VersioneerConfig users at the moment. We verify against\n    # `None` values elsewhere where it matters\n\n    cfg = VersioneerConfig()\n    cfg.VCS = section[\"VCS\"]\n    cfg.style = section.get(\"style\", \"\")\n    cfg.versionfile_source = cast(str, section.get(\"versionfile_source\"))\n    cfg.versionfile_build = section.get(\"versionfile_build\")\n    cfg.tag_prefix = cast(str, section.get(\"tag_prefix\"))\n    if cfg.tag_prefix in (\"''\", '\"\"', None):\n        cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = section.get(\"parentdir_prefix\")\n    if isinstance(section, configparser.SectionProxy):\n        # Make sure configparser translates to bool\n        cfg.verbose = section.getboolean(\"verbose\")\n    else:\n        cfg.verbose = section.get(\"verbose\")\n\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY: Dict[str, str] = {}\nHANDLERS: Dict[str, Dict[str, Callable]] = {}\n\n\ndef register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f: Callable) -> Callable:\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(\n    commands: List[str],\n    args: List[str],\n    cwd: Optional[str] = None,\n    verbose: bool = False,\n    hide_stderr: bool = False,\n    env: Optional[Dict[str, str]] = None,\n) -> Tuple[Optional[str], Optional[int]]:\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs: Dict[str, Any] = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen(\n                [command] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n                **popen_kwargs,\n            )\n            break\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %s\" % (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n\n\nLONG_VERSION_PY[\n    \"git\"\n] = r'''\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain.\n# Generated by versioneer-0.29\n# https://github.com/python-versioneer/python-versioneer\n\n\"\"\"Git implementation of _version.py.\"\"\"\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\nimport functools\n\n\ndef get_keywords() -> Dict[str, str]:\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"%(DOLLAR)sFormat:%%d%(DOLLAR)s\"\n    git_full = \"%(DOLLAR)sFormat:%%H%(DOLLAR)s\"\n    git_date = \"%(DOLLAR)sFormat:%%ci%(DOLLAR)s\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n    VCS: str\n    style: str\n    tag_prefix: str\n    parentdir_prefix: str\n    versionfile_source: str\n    verbose: bool\n\n\ndef get_config() -> VersioneerConfig:\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"%(STYLE)s\"\n    cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n    cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n    cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY: Dict[str, str] = {}\nHANDLERS: Dict[str, Dict[str, Callable]] = {}\n\n\ndef register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f: Callable) -> Callable:\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(\n    commands: List[str],\n    args: List[str],\n    cwd: Optional[str] = None,\n    verbose: bool = False,\n    hide_stderr: bool = False,\n    env: Optional[Dict[str, str]] = None,\n) -> Tuple[Optional[str], Optional[int]]:\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs: Dict[str, Any] = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n                                       stdout=subprocess.PIPE,\n                                       stderr=(subprocess.PIPE if hide_stderr\n                                               else None), **popen_kwargs)\n            break\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %%s\" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %%s\" %% (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %%s (error)\" %% dispcmd)\n            print(\"stdout was %%s\" %% stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n\n\ndef versions_from_parentdir(\n    parentdir_prefix: str,\n    root: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\"version\": dirname[len(parentdir_prefix):],\n                    \"full-revisionid\": None,\n                    \"dirty\": False, \"error\": None, \"date\": None}\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\"Tried directories %%s but none started with prefix %%s\" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs: str) -> Dict[str, str]:\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords: Dict[str, str] = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(\n    keywords: Dict[str, str],\n    tag_prefix: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r'\\d', r)}\n        if verbose:\n            print(\"discarding '%%s', no digits\" %% \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %%s\" %% \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r'\\d', r):\n                continue\n            if verbose:\n                print(\"picking %%s\" %% r)\n            return {\"version\": r,\n                    \"full-revisionid\": keywords[\"full\"].strip(),\n                    \"dirty\": False, \"error\": None,\n                    \"date\": date}\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\"version\": \"0+unknown\",\n            \"full-revisionid\": keywords[\"full\"].strip(),\n            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(\n    tag_prefix: str,\n    root: str,\n    verbose: bool,\n    runner: Callable = run_command\n) -> Dict[str, Any]:\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n                   hide_stderr=not verbose)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %%s not under git control\" %% root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(GITS, [\n        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n    ], cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces: Dict[str, Any] = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n                             cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%%s'\"\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%%s' doesn't start with prefix '%%s'\"\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%%s' doesn't start with prefix '%%s'\"\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n        pieces[\"distance\"] = len(out.split())  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces: Dict[str, Any]) -> str:\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces: Dict[str, Any]) -> str:\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n\n\ndef render_pep440_pre(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%%d.dev%%d\" %% (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%%d\" %% (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%%d\" %% pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%%s\" %% pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%%s\" %% pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_post_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%%s\" %% pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%%s\" %% pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_old(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces: Dict[str, Any], style: str) -> Dict[str, Any]:\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%%s'\" %% style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None,\n            \"date\": pieces.get(\"date\")}\n\n\ndef get_versions() -> Dict[str, Any]:\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for _ in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n                \"dirty\": None,\n                \"error\": \"unable to find root of source tree\",\n                \"date\": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to compute version\", \"date\": None}\n'''\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs: str) -> Dict[str, str]:\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords: Dict[str, str] = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(\n    keywords: Dict[str, str],\n    tag_prefix: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r\"\\d\", r)}\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r\"\\d\", r):\n                continue\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\n                \"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": date,\n            }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False,\n        \"error\": \"no suitable tags\",\n        \"date\": None,\n    }\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(\n    tag_prefix: str, root: str, verbose: bool, runner: Callable = run_command\n) -> Dict[str, Any]:\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root, hide_stderr=not verbose)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(\n        GITS,\n        [\n            \"describe\",\n            \"--tags\",\n            \"--dirty\",\n            \"--always\",\n            \"--long\",\n            \"--match\",\n            f\"{tag_prefix}[[:digit:]]*\",\n        ],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces: Dict[str, Any] = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\"^(.+)-(\\d+)-g([0-9a-f]+)$\", git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = \"unable to parse git-describe output: '%s'\" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = \"tag '%s' doesn't start with prefix '%s'\" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n        pieces[\"distance\"] = len(out.split())  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef do_vcs_install(versionfile_source: str, ipy: Optional[str]) -> None:\n    \"\"\"Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n    files = [versionfile_source]\n    if ipy:\n        files.append(ipy)\n    if \"VERSIONEER_PEP518\" not in globals():\n        try:\n            my_path = __file__\n            if my_path.endswith((\".pyc\", \".pyo\")):\n                my_path = os.path.splitext(my_path)[0] + \".py\"\n            versioneer_file = os.path.relpath(my_path)\n        except NameError:\n            versioneer_file = \"versioneer.py\"\n        files.append(versioneer_file)\n    present = False\n    try:\n        with open(\".gitattributes\", \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(versionfile_source):\n                    if \"export-subst\" in line.strip().split()[1:]:\n                        present = True\n                        break\n    except OSError:\n        pass\n    if not present:\n        with open(\".gitattributes\", \"a+\") as fobj:\n            fobj.write(f\"{versionfile_source} export-subst\\n\")\n        files.append(\".gitattributes\")\n    run_command(GITS, [\"add\", \"--\"] + files)\n\n\ndef versions_from_parentdir(\n    parentdir_prefix: str,\n    root: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                \"version\": dirname[len(parentdir_prefix) :],\n                \"full-revisionid\": None,\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": None,\n            }\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            \"Tried directories %s but none started with prefix %s\"\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\nSHORT_VERSION_PY = \"\"\"\n# This file was generated by 'versioneer.py' (0.29) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = '''\n%s\n'''  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n\"\"\"\n\n\ndef versions_from_file(filename: str) -> Dict[str, Any]:\n    \"\"\"Try to determine the version from _version.py if present.\"\"\"\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except OSError:\n        raise NotThisMethod(\"unable to read _version.py\")\n    mo = re.search(\n        r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\", contents, re.M | re.S\n    )\n    if not mo:\n        mo = re.search(\n            r\"version_json = '''\\r\\n(.*)'''  # END VERSION_JSON\", contents, re.M | re.S\n        )\n    if not mo:\n        raise NotThisMethod(\"no version_json in _version.py\")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename: str, versions: Dict[str, Any]) -> None:\n    \"\"\"Write the given version number to the given _version.py file.\"\"\"\n    contents = json.dumps(versions, sort_keys=True, indent=1, separators=(\",\", \": \"))\n    with open(filename, \"w\") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(\"set %s to '%s'\" % (filename, versions[\"version\"]))\n\n\ndef plus_or_dot(pieces: Dict[str, Any]) -> str:\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces: Dict[str, Any]) -> str:\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n\n\ndef render_pep440_pre(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_post_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_old(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces: Dict[str, Any], style: str) -> Dict[str, Any]:\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\n            \"version\": \"unknown\",\n            \"full-revisionid\": pieces.get(\"long\"),\n            \"dirty\": None,\n            \"error\": pieces[\"error\"],\n            \"date\": None,\n        }\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\n        \"version\": rendered,\n        \"full-revisionid\": pieces[\"long\"],\n        \"dirty\": pieces[\"dirty\"],\n        \"error\": None,\n        \"date\": pieces.get(\"date\"),\n    }\n\n\nclass VersioneerBadRootError(Exception):\n    \"\"\"The project root directory is unknown or missing key files.\"\"\"\n\n\ndef get_versions(verbose: bool = False) -> Dict[str, Any]:\n    \"\"\"Get the project version from whatever source is available.\n\n    Returns dict with two keys: 'version' and 'full'.\n    \"\"\"\n    if \"versioneer\" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[\"versioneer\"]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\n    verbose = verbose or bool(cfg.verbose)  # `bool()` used to avoid `None`\n    assert (\n        cfg.versionfile_source is not None\n    ), \"please set versioneer.versionfile_source\"\n    assert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. 'git\n    # describe'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by 'setup.py sdist',\n    # and for users of a tarball/zipball created by 'git archive' or github's\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(\"get_keywords\")\n    from_keywords_f = handlers.get(\"keywords\")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(\"got version from expanded keyword %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(\"got version from file %s %s\" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(\"pieces_from_vcs\")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(\"got version from VCS %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(\"got version from parentdir %s\" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(\"unable to compute version\")\n\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\",\n        \"date\": None,\n    }\n\n\ndef get_version() -> str:\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]\n\n\ndef get_cmdclass(cmdclass: Optional[Dict[str, Any]] = None):\n    \"\"\"Get the custom setuptools subclasses used by Versioneer.\n\n    If the package uses a different cmdclass (e.g. one from numpy), it\n    should be provide as an argument.\n    \"\"\"\n    if \"versioneer\" in sys.modules:\n        del sys.modules[\"versioneer\"]\n        # this fixes the \"python setup.py develop\" case (also 'install' and\n        # 'easy_install .'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n        # sys.modules by the time B's setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it's pre-build state, so the\n        # parent is protected against the child's \"import versioneer\". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent's versioneer too.\n        # Also see https://github.com/python-versioneer/python-versioneer/issues/52\n\n    cmds = {} if cmdclass is None else cmdclass.copy()\n\n    # we add \"version\" to setuptools\n    from setuptools import Command\n\n    class cmd_version(Command):\n        description = \"report generated version string\"\n        user_options: List[Tuple[str, str, str]] = []\n        boolean_options: List[str] = []\n\n        def initialize_options(self) -> None:\n            pass\n\n        def finalize_options(self) -> None:\n            pass\n\n        def run(self) -> None:\n            vers = get_versions(verbose=True)\n            print(\"Version: %s\" % vers[\"version\"])\n            print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n            print(\" dirty: %s\" % vers.get(\"dirty\"))\n            print(\" date: %s\" % vers.get(\"date\"))\n            if vers[\"error\"]:\n                print(\" error: %s\" % vers[\"error\"])\n\n    cmds[\"version\"] = cmd_version\n\n    # we override \"build_py\" in setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn't copied too, 'git describe' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # pip install -e . and setuptool/editable_wheel will invoke build_py\n    # but the build_py command is not expected to copy any files.\n\n    # we override different \"build_py\" commands for both environments\n    if \"build_py\" in cmds:\n        _build_py: Any = cmds[\"build_py\"]\n    else:\n        from setuptools.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self) -> None:\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            if getattr(self, \"editable_mode\", False):\n                # During editable installs `.py` and data files are\n                # not copied to build_lib\n                return\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n    cmds[\"build_py\"] = cmd_build_py\n\n    if \"build_ext\" in cmds:\n        _build_ext: Any = cmds[\"build_ext\"]\n    else:\n        from setuptools.command.build_ext import build_ext as _build_ext\n\n    class cmd_build_ext(_build_ext):\n        def run(self) -> None:\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_ext.run(self)\n            if self.inplace:\n                # build_ext --inplace will only build extensions in\n                # build/lib<..> dir with no _version.py to write to.\n                # As in place builds will already have a _version.py\n                # in the module dir, we do not need to write one.\n                return\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if not cfg.versionfile_build:\n                return\n            target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n            if not os.path.exists(target_versionfile):\n                print(\n                    f\"Warning: {target_versionfile} does not exist, skipping \"\n                    \"version update. This can happen if you are running build_ext \"\n                    \"without first running build_py.\"\n                )\n                return\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(target_versionfile, versions)\n\n    cmds[\"build_ext\"] = cmd_build_ext\n\n    if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe  # type: ignore\n\n        # nczeczulin reports that py2exe won't like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   \"version\": versioneer.get_version().split(\"+\", 1)[0], # FILEVERSION\n        #   \"product_version\": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self) -> None:\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            \"DOLLAR\": \"$\",\n                            \"STYLE\": cfg.style,\n                            \"TAG_PREFIX\": cfg.tag_prefix,\n                            \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                            \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[\"build_exe\"] = cmd_build_exe\n        del cmds[\"build_py\"]\n\n    if \"py2exe\" in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.setuptools_buildexe import py2exe as _py2exe  # type: ignore\n        except ImportError:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # type: ignore\n\n        class cmd_py2exe(_py2exe):\n            def run(self) -> None:\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            \"DOLLAR\": \"$\",\n                            \"STYLE\": cfg.style,\n                            \"TAG_PREFIX\": cfg.tag_prefix,\n                            \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                            \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[\"py2exe\"] = cmd_py2exe\n\n    # sdist farms its file list building out to egg_info\n    if \"egg_info\" in cmds:\n        _egg_info: Any = cmds[\"egg_info\"]\n    else:\n        from setuptools.command.egg_info import egg_info as _egg_info\n\n    class cmd_egg_info(_egg_info):\n        def find_sources(self) -> None:\n            # egg_info.find_sources builds the manifest list and writes it\n            # in one shot\n            super().find_sources()\n\n            # Modify the filelist and normalize it\n            root = get_root()\n            cfg = get_config_from_root(root)\n            self.filelist.append(\"versioneer.py\")\n            if cfg.versionfile_source:\n                # There are rare cases where versionfile_source might not be\n                # included by default, so we must be explicit\n                self.filelist.append(cfg.versionfile_source)\n            self.filelist.sort()\n            self.filelist.remove_duplicates()\n\n            # The write method is hidden in the manifest_maker instance that\n            # generated the filelist and was thrown away\n            # We will instead replicate their final normalization (to unicode,\n            # and POSIX-style paths)\n            from setuptools import unicode_utils\n\n            normalized = [\n                unicode_utils.filesys_decode(f).replace(os.sep, \"/\")\n                for f in self.filelist.files\n            ]\n\n            manifest_filename = os.path.join(self.egg_info, \"SOURCES.txt\")\n            with open(manifest_filename, \"w\") as fobj:\n                fobj.write(\"\\n\".join(normalized))\n\n    cmds[\"egg_info\"] = cmd_egg_info\n\n    # we override different \"sdist\" commands for both environments\n    if \"sdist\" in cmds:\n        _sdist: Any = cmds[\"sdist\"]\n    else:\n        from setuptools.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self) -> None:\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[\"version\"]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir: str, files: List[str]) -> None:\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(\n                target_versionfile, self._versioneer_generated_versions\n            )\n\n    cmds[\"sdist\"] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = \"\"\"\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or 'python versioneer.py setup'.\n\"\"\"\n\nSAMPLE_CONFIG = \"\"\"\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run 'versioneer.py setup' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n\"\"\"\n\nOLD_SNIPPET = \"\"\"\nfrom ._version import get_versions\n__version__ = get_versions()['version']\ndel get_versions\n\"\"\"\n\nINIT_PY_SNIPPET = \"\"\"\nfrom . import {0}\n__version__ = {0}.get_versions()['version']\n\"\"\"\n\n\ndef do_setup() -> int:\n    \"\"\"Do main VCS-independent setup function for installing Versioneer.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print(\" creating %s\" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, \"w\") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(\n            LONG\n            % {\n                \"DOLLAR\": \"$\",\n                \"STYLE\": cfg.style,\n                \"TAG_PREFIX\": cfg.tag_prefix,\n                \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n            }\n        )\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), \"__init__.py\")\n    maybe_ipy: Optional[str] = ipy\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, \"r\") as f:\n                old = f.read()\n        except OSError:\n            old = \"\"\n        module = os.path.splitext(os.path.basename(cfg.versionfile_source))[0]\n        snippet = INIT_PY_SNIPPET.format(module)\n        if OLD_SNIPPET in old:\n            print(\" replacing boilerplate in %s\" % ipy)\n            with open(ipy, \"w\") as f:\n                f.write(old.replace(OLD_SNIPPET, snippet))\n        elif snippet not in old:\n            print(\" appending to %s\" % ipy)\n            with open(ipy, \"a\") as f:\n                f.write(snippet)\n        else:\n            print(\" %s unmodified\" % ipy)\n    else:\n        print(\" %s doesn't exist, ok\" % ipy)\n        maybe_ipy = None\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(cfg.versionfile_source, maybe_ipy)\n    return 0\n\n\ndef scan_setup_py() -> int:\n    \"\"\"Validate the contents of setup.py against Versioneer's expectations.\"\"\"\n    found = set()\n    setters = False\n    errors = 0\n    with open(\"setup.py\", \"r\") as f:\n        for line in f.readlines():\n            if \"import versioneer\" in line:\n                found.add(\"import\")\n            if \"versioneer.get_cmdclass()\" in line:\n                found.add(\"cmdclass\")\n            if \"versioneer.get_version()\" in line:\n                found.add(\"get_version\")\n            if \"versioneer.VCS\" in line:\n                setters = True\n            if \"versioneer.versionfile_source\" in line:\n                setters = True\n    if len(found) != 3:\n        print(\"\")\n        print(\"Your setup.py appears to be missing some important items\")\n        print(\"(but I might be wrong). Please make sure it has something\")\n        print(\"roughly like the following:\")\n        print(\"\")\n        print(\" import versioneer\")\n        print(\" setup( version=versioneer.get_version(),\")\n        print(\"        cmdclass=versioneer.get_cmdclass(),  ...)\")\n        print(\"\")\n        errors += 1\n    if setters:\n        print(\"You should remove lines like 'versioneer.VCS = ' and\")\n        print(\"'versioneer.versionfile_source = ' . This configuration\")\n        print(\"now lives in setup.cfg, and should be removed from setup.py\")\n        print(\"\")\n        errors += 1\n    return errors\n\n\ndef setup_command() -> NoReturn:\n    \"\"\"Set up Versioneer and exit with appropriate error code.\"\"\"\n    errors = do_setup()\n    errors += scan_setup_py()\n    sys.exit(1 if errors else 0)\n\n\nif __name__ == \"__main__\":\n    cmd = sys.argv[1]\n    if cmd == \"setup\":\n        setup_command()\n", "setup.py": "#!/usr/bin/env python\n\nfrom setuptools import setup\nimport versioneer\n\nwith open(\"requirements.txt\") as file:\n    aiobotocore_version_suffix = \"\"\n    for line in file:\n        parts = line.rstrip().split(\"aiobotocore\")\n        if len(parts) == 2:\n            aiobotocore_version_suffix = parts[1]\n            break\n\nsetup(\n    name=\"s3fs\",\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n    ],\n    description=\"Convenient Filesystem interface over S3\",\n    url=\"http://github.com/fsspec/s3fs/\",\n    maintainer=\"Martin Durant\",\n    maintainer_email=\"mdurant@continuum.io\",\n    license=\"BSD\",\n    keywords=\"s3, boto\",\n    packages=[\"s3fs\"],\n    python_requires=\">= 3.8\",\n    install_requires=[open(\"requirements.txt\").read().strip().split(\"\\n\")],\n    extras_require={\n        \"awscli\": [f\"aiobotocore[awscli]{aiobotocore_version_suffix}\"],\n        \"boto3\": [f\"aiobotocore[boto3]{aiobotocore_version_suffix}\"],\n    },\n    zip_safe=False,\n)\n", "s3fs/core.py": "# -*- coding: utf-8 -*-\nimport asyncio\nimport errno\nimport logging\nimport mimetypes\nimport os\nimport socket\nfrom typing import Tuple, Optional\nimport weakref\nimport re\n\nfrom urllib3.exceptions import IncompleteRead\n\nimport fsspec  # noqa: F401\nfrom fsspec.spec import AbstractBufferedFile\nfrom fsspec.utils import infer_storage_options, tokenize, setup_logging as setup_logger\nfrom fsspec.asyn import (\n    AsyncFileSystem,\n    AbstractAsyncStreamedFile,\n    sync,\n    sync_wrapper,\n    FSTimeoutError,\n    _run_coros_in_chunks,\n)\nfrom fsspec.callbacks import _DEFAULT_CALLBACK\n\nimport aiobotocore\nimport botocore\nimport aiobotocore.session\nfrom aiobotocore.config import AioConfig\nfrom botocore.exceptions import ClientError, HTTPClientError, ParamValidationError\nfrom botocore.parsers import ResponseParserError\n\nfrom s3fs.errors import translate_boto_error\nfrom s3fs.utils import S3BucketRegionCache, ParamKwargsHelper, _get_brange, FileExpired\n\n# ClientPayloadError can be thrown during an incomplete read. aiohttp is a dependency of\n# aiobotocore, we guard the import here in case this dependency is replaced in a future version\n# of aiobotocore.\ntry:\n    from aiohttp import ClientPayloadError\nexcept ImportError:\n    ClientPayloadError = None\n\n\nlogger = logging.getLogger(\"s3fs\")\n\n\ndef setup_logging(level=None):\n\n    setup_logger(logger=logger, level=(level or os.environ[\"S3FS_LOGGING_LEVEL\"]))\n\n\nif \"S3FS_LOGGING_LEVEL\" in os.environ:\n    setup_logging()\n\n\nMANAGED_COPY_THRESHOLD = 5 * 2**30\n# Certain rate-limiting responses can send invalid XML\n# (see https://github.com/fsspec/s3fs/issues/484), which can result in a parser error\n# deep within botocore. So we treat those as retryable as well, even though there could\n# be some false positives.\nS3_RETRYABLE_ERRORS = (\n    socket.timeout,\n    HTTPClientError,\n    IncompleteRead,\n    FSTimeoutError,\n    ResponseParserError,\n)\n\nif ClientPayloadError is not None:\n    S3_RETRYABLE_ERRORS += (ClientPayloadError,)\n\n_VALID_FILE_MODES = {\"r\", \"w\", \"a\", \"rb\", \"wb\", \"ab\"}\n\n_PRESERVE_KWARGS = [\n    \"CacheControl\",\n    \"ContentDisposition\",\n    \"ContentEncoding\",\n    \"ContentLanguage\",\n    \"ContentLength\",\n    \"ContentType\",\n    \"Expires\",\n    \"WebsiteRedirectLocation\",\n    \"ServerSideEncryption\",\n    \"SSECustomerAlgorithm\",\n    \"SSEKMSKeyId\",\n    \"BucketKeyEnabled\",\n    \"StorageClass\",\n    \"ObjectLockMode\",\n    \"ObjectLockRetainUntilDate\",\n    \"ObjectLockLegalHoldStatus\",\n    \"Metadata\",\n]\n\nkey_acls = {\n    \"private\",\n    \"public-read\",\n    \"public-read-write\",\n    \"authenticated-read\",\n    \"aws-exec-read\",\n    \"bucket-owner-read\",\n    \"bucket-owner-full-control\",\n}\nbuck_acls = {\"private\", \"public-read\", \"public-read-write\", \"authenticated-read\"}\n\n\nasync def _error_wrapper(func, *, args=(), kwargs=None, retries):\n    if kwargs is None:\n        kwargs = {}\n    for i in range(retries):\n        try:\n            return await func(*args, **kwargs)\n        except S3_RETRYABLE_ERRORS as e:\n            err = e\n            logger.debug(\"Retryable error: %s\", e)\n            await asyncio.sleep(min(1.7**i * 0.1, 15))\n        except ClientError as e:\n            logger.debug(\"Client error (maybe retryable): %s\", e)\n            err = e\n            wait_time = min(1.7**i * 0.1, 15)\n            if \"SlowDown\" in str(e):\n                await asyncio.sleep(wait_time)\n            elif \"reduce your request rate\" in str(e):\n                await asyncio.sleep(wait_time)\n            elif \"XAmzContentSHA256Mismatch\" in str(e):\n                await asyncio.sleep(wait_time)\n            else:\n                break\n        except Exception as e:\n            logger.debug(\"Nonretryable error: %s\", e)\n            err = e\n            break\n\n    if \"'coroutine'\" in str(err):\n        # aiobotocore internal error - fetch original botocore error\n        tb = err.__traceback__\n        while tb.tb_next:\n            tb = tb.tb_next\n        try:\n            await tb.tb_frame.f_locals[\"response\"]\n        except Exception as e:\n            err = e\n    err = translate_boto_error(err)\n    raise err\n\n\ndef version_id_kw(version_id):\n    \"\"\"Helper to make versionId kwargs.\n\n    Not all boto3 methods accept a None / empty versionId so dictionary expansion solves\n    that problem.\n    \"\"\"\n    if version_id:\n        return {\"VersionId\": version_id}\n    else:\n        return {}\n\n\ndef _coalesce_version_id(*args):\n    \"\"\"Helper to coalesce a list of version_ids down to one\"\"\"\n    version_ids = set(args)\n    if None in version_ids:\n        version_ids.remove(None)\n    if len(version_ids) > 1:\n        raise ValueError(\n            \"Cannot coalesce version_ids where more than one are defined,\"\n            \" {}\".format(version_ids)\n        )\n    elif len(version_ids) == 0:\n        return None\n    else:\n        return version_ids.pop()\n\n\nclass S3FileSystem(AsyncFileSystem):\n    \"\"\"\n    Access S3 as if it were a file system.\n\n    This exposes a filesystem-like API (ls, cp, open, etc.) on top of S3\n    storage.\n\n    Provide credentials either explicitly (``key=``, ``secret=``) or depend\n    on boto's credential methods. See botocore documentation for more\n    information. If no credentials are available, use ``anon=True``.\n\n    Parameters\n    ----------\n    anon : bool (False)\n        Whether to use anonymous connection (public buckets only). If False,\n        uses the key/secret given, or boto's credential resolver (client_kwargs,\n        environment, variables, config files, EC2 IAM server, in that order)\n    endpoint_url : string (None)\n        Use this endpoint_url, if specified. Needed for connecting to non-AWS\n        S3 buckets. Takes precedence over `endpoint_url` in client_kwargs.\n    key : string (None)\n        If not anonymous, use this access key ID, if specified. Takes precedence\n        over `aws_access_key_id` in client_kwargs.\n    secret : string (None)\n        If not anonymous, use this secret access key, if specified. Takes\n        precedence over `aws_secret_access_key` in client_kwargs.\n    token : string (None)\n        If not anonymous, use this security token, if specified\n    use_ssl : bool (True)\n        Whether to use SSL in connections to S3; may be faster without, but\n        insecure. If ``use_ssl`` is also set in ``client_kwargs``,\n        the value set in ``client_kwargs`` will take priority.\n    s3_additional_kwargs : dict of parameters that are used when calling s3 api\n        methods. Typically used for things like \"ServerSideEncryption\".\n    client_kwargs : dict of parameters for the botocore client\n    requester_pays : bool (False)\n        If RequesterPays buckets are supported.\n    default_block_size: int (None)\n        If given, the default block size value used for ``open()``, if no\n        specific value is given at all time. The built-in default is 5MB.\n    default_fill_cache : Bool (True)\n        Whether to use cache filling with open by default. Refer to\n        ``S3File.open``.\n    default_cache_type : string (\"readahead\")\n        If given, the default cache_type value used for ``open()``. Set to \"none\"\n        if no caching is desired. See fsspec's documentation for other available\n        cache_type values. Default cache_type is \"readahead\".\n    version_aware : bool (False)\n        Whether to support bucket versioning.  If enable this will require the\n        user to have the necessary IAM permissions for dealing with versioned\n        objects. Note that in the event that you only need to work with the\n        latest version of objects in a versioned bucket, and do not need the\n        VersionId for those objects, you should set ``version_aware`` to False\n        for performance reasons. When set to True, filesystem instances will\n        use the S3 ListObjectVersions API call to list directory contents,\n        which requires listing all historical object versions.\n    cache_regions : bool (False)\n        Whether to cache bucket regions or not. Whenever a new bucket is used,\n        it will first find out which region it belongs and then use the client\n        for that region.\n    asynchronous :  bool (False)\n        Whether this instance is to be used from inside coroutines.\n    config_kwargs : dict of parameters passed to ``botocore.client.Config``\n    kwargs : other parameters for core session.\n    session : aiobotocore AioSession object to be used for all connections.\n         This session will be used inplace of creating a new session inside S3FileSystem.\n         For example: aiobotocore.session.AioSession(profile='test_user')\n    max_concurrency : int (1)\n        The maximum number of concurrent transfers to use per file for multipart\n        upload (``put()``) operations. Defaults to 1 (sequential). When used in\n        conjunction with ``S3FileSystem.put(batch_size=...)`` the maximum number of\n        simultaneous connections is ``max_concurrency * batch_size``. We may extend\n        this parameter to affect ``pipe()``, ``cat()`` and ``get()``. Increasing this\n        value will result in higher memory usage during multipart upload operations (by\n        ``max_concurrency * chunksize`` bytes per file).\n\n    The following parameters are passed on to fsspec:\n\n    skip_instance_cache: to control reuse of instances\n    use_listings_cache, listings_expiry_time, max_paths: to control reuse of directory listings\n\n    Examples\n    --------\n    >>> s3 = S3FileSystem(anon=False)  # doctest: +SKIP\n    >>> s3.ls('my-bucket/')  # doctest: +SKIP\n    ['my-file.txt']\n\n    >>> with s3.open('my-bucket/my-file.txt', mode='rb') as f:  # doctest: +SKIP\n    ...     print(f.read())  # doctest: +SKIP\n    b'Hello, world!'\n    \"\"\"\n\n    root_marker = \"\"\n    connect_timeout = 5\n    retries = 5\n    read_timeout = 15\n    default_block_size = 5 * 2**20\n    protocol = (\"s3\", \"s3a\")\n    _extra_tokenize_attributes = (\"default_block_size\",)\n\n    def __init__(\n        self,\n        anon=False,\n        endpoint_url=None,\n        key=None,\n        secret=None,\n        token=None,\n        use_ssl=True,\n        client_kwargs=None,\n        requester_pays=False,\n        default_block_size=None,\n        default_fill_cache=True,\n        default_cache_type=\"readahead\",\n        version_aware=False,\n        config_kwargs=None,\n        s3_additional_kwargs=None,\n        session=None,\n        username=None,\n        password=None,\n        cache_regions=False,\n        asynchronous=False,\n        loop=None,\n        max_concurrency=1,\n        **kwargs,\n    ):\n        if key and username:\n            raise KeyError(\"Supply either key or username, not both\")\n        if secret and password:\n            raise KeyError(\"Supply secret or password, not both\")\n        if username:\n            key = username\n        if password:\n            secret = password\n\n        self.endpoint_url = endpoint_url\n\n        self.anon = anon\n        self.key = key\n        self.secret = secret\n        self.token = token\n        self.kwargs = kwargs\n        super_kwargs = {\n            k: kwargs.pop(k)\n            for k in [\"use_listings_cache\", \"listings_expiry_time\", \"max_paths\"]\n            if k in kwargs\n        }  # passed to fsspec superclass\n        super().__init__(loop=loop, asynchronous=asynchronous, **super_kwargs)\n\n        self.default_block_size = default_block_size or self.default_block_size\n        self.default_fill_cache = default_fill_cache\n        self.default_cache_type = default_cache_type\n        self.version_aware = version_aware\n        self.client_kwargs = client_kwargs or {}\n        self.config_kwargs = config_kwargs or {}\n        self.req_kw = {\"RequestPayer\": \"requester\"} if requester_pays else {}\n        self.s3_additional_kwargs = s3_additional_kwargs or {}\n        self.use_ssl = use_ssl\n        self.cache_regions = cache_regions\n        self._s3 = None\n        self.session = session\n        if max_concurrency < 1:\n            raise ValueError(\"max_concurrency must be >= 1\")\n        self.max_concurrency = max_concurrency\n\n    @property\n    def s3(self):\n        if self._s3 is None:\n            if self.asynchronous:\n                raise RuntimeError(\"please await ``.set_session`` before anything else\")\n            self.connect()\n        return self._s3\n\n    def _filter_kwargs(self, s3_method, kwargs):\n        return self._kwargs_helper.filter_dict(s3_method.__name__, kwargs)\n\n    async def get_s3(self, bucket=None):\n        if self.cache_regions and bucket is not None:\n            return await self._s3creator.get_bucket_client(bucket)\n        else:\n            return self._s3\n\n    async def _call_s3(self, method, *akwarglist, **kwargs):\n        await self.set_session()\n        s3 = await self.get_s3(kwargs.get(\"Bucket\"))\n        method = getattr(s3, method)\n        kw2 = kwargs.copy()\n        kw2.pop(\"Body\", None)\n        logger.debug(\"CALL: %s - %s - %s\", method.__name__, akwarglist, kw2)\n        additional_kwargs = self._get_s3_method_kwargs(method, *akwarglist, **kwargs)\n        return await _error_wrapper(\n            method, kwargs=additional_kwargs, retries=self.retries\n        )\n\n    call_s3 = sync_wrapper(_call_s3)\n\n    def _get_s3_method_kwargs(self, method, *akwarglist, **kwargs):\n        additional_kwargs = self.s3_additional_kwargs.copy()\n        for akwargs in akwarglist:\n            additional_kwargs.update(akwargs)\n        # Add the normal kwargs in\n        additional_kwargs.update(kwargs)\n        # filter all kwargs\n        return self._filter_kwargs(method, additional_kwargs)\n\n    @staticmethod\n    def _get_kwargs_from_urls(urlpath):\n        \"\"\"\n        When we have a urlpath that contains a ?versionId=\n\n        Assume that we want to use version_aware mode for\n        the filesystem.\n        \"\"\"\n        url_storage_opts = infer_storage_options(urlpath)\n        url_query = url_storage_opts.get(\"url_query\")\n        out = {}\n        if url_query is not None:\n            from urllib.parse import parse_qs\n\n            parsed = parse_qs(url_query)\n            if \"versionId\" in parsed:\n                out[\"version_aware\"] = True\n        return out\n\n    def _find_bucket_key(self, s3_path):\n        \"\"\"\n        This is a helper function that given an s3 path such that the path is of\n        the form: bucket/key\n        It will return the bucket and the key represented by the s3 path\n        \"\"\"\n\n        bucket_format_list = [\n            re.compile(\n                r\"^(?P<bucket>arn:(aws).*:s3:[a-z\\-0-9]*:[0-9]{12}:accesspoint[:/][^/]+)/?\"\n                r\"(?P<key>.*)$\"\n            ),\n            re.compile(\n                r\"^(?P<bucket>arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:]\"\n                r\"[a-zA-Z0-9\\-]{1,63}[/:](bucket|accesspoint)[/:][a-zA-Z0-9\\-]{1,63})[/:]?(?P<key>.*)$\"\n            ),\n            re.compile(\n                r\"^(?P<bucket>arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:]\"\n                r\"[a-zA-Z0-9\\-]{1,63}[/:]bucket[/:]\"\n                r\"[a-zA-Z0-9\\-]{1,63})[/:]?(?P<key>.*)$\"\n            ),\n            re.compile(\n                r\"^(?P<bucket>arn:(aws).*:s3-object-lambda:[a-z\\-0-9]+:[0-9]{12}:\"\n                r\"accesspoint[/:][a-zA-Z0-9\\-]{1,63})[/:]?(?P<key>.*)$\"\n            ),\n        ]\n        for bucket_format in bucket_format_list:\n            match = bucket_format.match(s3_path)\n            if match:\n                return match.group(\"bucket\"), match.group(\"key\")\n        s3_components = s3_path.split(\"/\", 1)\n        bucket = s3_components[0]\n        s3_key = \"\"\n        if len(s3_components) > 1:\n            s3_key = s3_components[1]\n        return bucket, s3_key\n\n    def split_path(self, path) -> Tuple[str, str, Optional[str]]:\n        \"\"\"\n        Normalise S3 path string into bucket and key.\n\n        Parameters\n        ----------\n        path : string\n            Input path, like `s3://mybucket/path/to/file`\n\n        Examples\n        --------\n        >>> split_path(\"s3://mybucket/path/to/file\")\n        ['mybucket', 'path/to/file', None]\n\n        >>> split_path(\"s3://mybucket/path/to/versioned_file?versionId=some_version_id\")\n        ['mybucket', 'path/to/versioned_file', 'some_version_id']\n        \"\"\"\n        path = self._strip_protocol(path)\n        path = path.lstrip(\"/\")\n        if \"/\" not in path:\n            return path, \"\", None\n        else:\n            bucket, keypart = self._find_bucket_key(path)\n            key, _, version_id = keypart.partition(\"?versionId=\")\n            return (\n                bucket,\n                key,\n                version_id if self.version_aware and version_id else None,\n            )\n\n    def _prepare_config_kwargs(self):\n        config_kwargs = self.config_kwargs.copy()\n        if \"connect_timeout\" not in config_kwargs.keys():\n            config_kwargs[\"connect_timeout\"] = self.connect_timeout\n        if \"read_timeout\" not in config_kwargs.keys():\n            config_kwargs[\"read_timeout\"] = self.read_timeout\n        return config_kwargs\n\n    async def set_session(self, refresh=False, kwargs={}):\n        \"\"\"Establish S3 connection object.\n        Returns\n        -------\n        Session to be closed later with await .close()\n        \"\"\"\n        if self._s3 is not None and not refresh:\n            return self._s3\n        logger.debug(\"Setting up s3fs instance\")\n\n        client_kwargs = self.client_kwargs.copy()\n        init_kwargs = dict(\n            aws_access_key_id=self.key,\n            aws_secret_access_key=self.secret,\n            aws_session_token=self.token,\n            endpoint_url=self.endpoint_url,\n        )\n        init_kwargs = {\n            key: value\n            for key, value in init_kwargs.items()\n            if value is not None and value != client_kwargs.get(key)\n        }\n        if \"use_ssl\" not in client_kwargs.keys():\n            init_kwargs[\"use_ssl\"] = self.use_ssl\n        config_kwargs = self._prepare_config_kwargs()\n        if self.anon:\n            from botocore import UNSIGNED\n\n            drop_keys = {\n                \"aws_access_key_id\",\n                \"aws_secret_access_key\",\n                \"aws_session_token\",\n            }\n            init_kwargs = {\n                key: value for key, value in init_kwargs.items() if key not in drop_keys\n            }\n            client_kwargs = {\n                key: value\n                for key, value in client_kwargs.items()\n                if key not in drop_keys\n            }\n            config_kwargs[\"signature_version\"] = UNSIGNED\n\n        conf = AioConfig(**config_kwargs)\n        if self.session is None:\n            self.session = aiobotocore.session.AioSession(**self.kwargs)\n\n        for parameters in (config_kwargs, self.kwargs, init_kwargs, client_kwargs):\n            for option in (\"region_name\", \"endpoint_url\"):\n                if parameters.get(option):\n                    self.cache_regions = False\n                    break\n        else:\n            cache_regions = self.cache_regions\n\n        logger.debug(\n            \"RC: caching enabled? %r (explicit option is %r)\",\n            cache_regions,\n            self.cache_regions,\n        )\n        self.cache_regions = cache_regions\n        if self.cache_regions:\n            s3creator = S3BucketRegionCache(\n                self.session, config=conf, **init_kwargs, **client_kwargs\n            )\n            self._s3 = await s3creator.get_client()\n        else:\n            s3creator = self.session.create_client(\n                \"s3\", config=conf, **init_kwargs, **client_kwargs\n            )\n            self._s3 = await s3creator.__aenter__()\n\n        self._s3creator = s3creator\n        # the following actually closes the aiohttp connection; use of privates\n        # might break in the future, would cause exception at gc time\n        if not self.asynchronous:\n            weakref.finalize(self, self.close_session, self.loop, self._s3creator)\n        self._kwargs_helper = ParamKwargsHelper(self._s3)\n        return self._s3\n\n    _connect = set_session\n\n    connect = sync_wrapper(set_session)\n\n    @staticmethod\n    def close_session(loop, s3):\n        if loop is not None and loop.is_running():\n            try:\n                loop = asyncio.get_event_loop()\n                loop.create_task(s3.__aexit__(None, None, None))\n                return\n            except RuntimeError:\n                pass\n            try:\n                sync(loop, s3.__aexit__, None, None, None, timeout=0.1)\n                return\n            except FSTimeoutError:\n                pass\n        try:\n            # close the actual socket\n            s3._client._endpoint.http_session._connector._close()\n        except AttributeError:\n            # but during shutdown, it may have gone\n            pass\n\n    async def _get_delegated_s3pars(self, exp=3600):\n        \"\"\"Get temporary credentials from STS, appropriate for sending across a\n        network. Only relevant where the key/secret were explicitly provided.\n\n        Parameters\n        ----------\n        exp : int\n            Time in seconds that credentials are good for\n\n        Returns\n        -------\n        dict of parameters\n        \"\"\"\n        if self.anon:\n            return {\"anon\": True}\n        if self.token:  # already has temporary cred\n            return {\n                \"key\": self.key,\n                \"secret\": self.secret,\n                \"token\": self.token,\n                \"anon\": False,\n            }\n        if self.key is None or self.secret is None:  # automatic credentials\n            return {\"anon\": False}\n        async with self.session.create_client(\"sts\") as sts:\n            cred = sts.get_session_token(DurationSeconds=exp)[\"Credentials\"]\n            return {\n                \"key\": cred[\"AccessKeyId\"],\n                \"secret\": cred[\"SecretAccessKey\"],\n                \"token\": cred[\"SessionToken\"],\n                \"anon\": False,\n            }\n\n    get_delegated_s3pars = sync_wrapper(_get_delegated_s3pars)\n\n    def _open(\n        self,\n        path,\n        mode=\"rb\",\n        block_size=None,\n        acl=False,\n        version_id=None,\n        fill_cache=None,\n        cache_type=None,\n        autocommit=True,\n        size=None,\n        requester_pays=None,\n        cache_options=None,\n        **kwargs,\n    ):\n        \"\"\"Open a file for reading or writing\n\n        Parameters\n        ----------\n        path: string\n            Path of file on S3\n        mode: string\n            One of 'r', 'w', 'a', 'rb', 'wb', or 'ab'. These have the same meaning\n            as they do for the built-in `open` function.\n        block_size: int\n            Size of data-node blocks if reading\n        fill_cache: bool\n            If seeking to new a part of the file beyond the current buffer,\n            with this True, the buffer will be filled between the sections to\n            best support random access. When reading only a few specific chunks\n            out of a file, performance may be better if False.\n        acl: str\n            Canned ACL to set when writing. False sends no parameter and uses the bucket's\n            preset default; otherwise it should be a member of the `key_acls` set.\n        version_id : str\n            Explicit version of the object to open.  This requires that the s3\n            filesystem is version aware and bucket versioning is enabled on the\n            relevant bucket.\n        encoding : str\n            The encoding to use if opening the file in text mode. The platform's\n            default text encoding is used if not given.\n        cache_type : str\n            See fsspec's documentation for available cache_type values. Set to \"none\"\n            if no caching is desired. If None, defaults to ``self.default_cache_type``.\n        requester_pays : bool (optional)\n            If RequesterPays buckets are supported.  If None, defaults to the\n            value used when creating the S3FileSystem (which defaults to False.)\n        kwargs: dict-like\n            Additional parameters used for s3 methods.  Typically used for\n            ServerSideEncryption.\n        \"\"\"\n        if block_size is None:\n            block_size = self.default_block_size\n        if fill_cache is None:\n            fill_cache = self.default_fill_cache\n        if requester_pays is None:\n            requester_pays = bool(self.req_kw)\n\n        acl = (\n            acl\n            or self.s3_additional_kwargs.get(\"ACL\", False)\n            or self.s3_additional_kwargs.get(\"acl\", False)\n        )\n        kw = self.s3_additional_kwargs.copy()\n        kw.update(kwargs)\n        if not self.version_aware and version_id:\n            raise ValueError(\n                \"version_id cannot be specified if the filesystem \"\n                \"is not version aware\"\n            )\n\n        if cache_type is None:\n            cache_type = self.default_cache_type\n\n        return S3File(\n            self,\n            path,\n            mode,\n            block_size=block_size,\n            acl=acl,\n            version_id=version_id,\n            fill_cache=fill_cache,\n            s3_additional_kwargs=kw,\n            cache_type=cache_type,\n            autocommit=autocommit,\n            requester_pays=requester_pays,\n            cache_options=cache_options,\n            size=size,\n        )\n\n    async def _lsdir(\n        self,\n        path,\n        refresh=False,\n        max_items=None,\n        delimiter=\"/\",\n        prefix=\"\",\n        versions=False,\n    ):\n        bucket, key, _ = self.split_path(path)\n        if not prefix:\n            prefix = \"\"\n        if key:\n            prefix = key.lstrip(\"/\") + \"/\" + prefix\n        if path not in self.dircache or refresh or not delimiter or versions:\n            try:\n                logger.debug(\"Get directory listing page for %s\" % path)\n                dirs = []\n                files = []\n                async for c in self._iterdir(\n                    bucket,\n                    max_items=max_items,\n                    delimiter=delimiter,\n                    prefix=prefix,\n                    versions=versions,\n                ):\n                    if c[\"type\"] == \"directory\":\n                        dirs.append(c)\n                    else:\n                        files.append(c)\n                files += dirs\n            except ClientError as e:\n                raise translate_boto_error(e)\n\n            if delimiter and files and not versions:\n                self.dircache[path] = files\n            return files\n        return self.dircache[path]\n\n    async def _iterdir(\n        self, bucket, max_items=None, delimiter=\"/\", prefix=\"\", versions=False\n    ):\n        \"\"\"Iterate asynchronously over files and directories under `prefix`.\n\n        The contents are yielded in arbitrary order as info dicts.\n        \"\"\"\n        if versions and not self.version_aware:\n            raise ValueError(\n                \"versions cannot be specified if the filesystem is not version aware\"\n            )\n        await self.set_session()\n        s3 = await self.get_s3(bucket)\n        if self.version_aware:\n            method = \"list_object_versions\"\n            contents_key = \"Versions\"\n        else:\n            method = \"list_objects_v2\"\n            contents_key = \"Contents\"\n        pag = s3.get_paginator(method)\n        config = {}\n        if max_items is not None:\n            config.update(MaxItems=max_items, PageSize=2 * max_items)\n        it = pag.paginate(\n            Bucket=bucket,\n            Prefix=prefix,\n            Delimiter=delimiter,\n            PaginationConfig=config,\n            **self.req_kw,\n        )\n        async for i in it:\n            for l in i.get(\"CommonPrefixes\", []):\n                c = {\n                    \"Key\": l[\"Prefix\"][:-1],\n                    \"Size\": 0,\n                    \"StorageClass\": \"DIRECTORY\",\n                    \"type\": \"directory\",\n                }\n                self._fill_info(c, bucket, versions=False)\n                yield c\n            for c in i.get(contents_key, []):\n                if not self.version_aware or c.get(\"IsLatest\") or versions:\n                    c[\"type\"] = \"file\"\n                    c[\"size\"] = c[\"Size\"]\n                    self._fill_info(c, bucket, versions=versions)\n                    yield c\n\n    @staticmethod\n    def _fill_info(f, bucket, versions=False):\n        f[\"size\"] = f[\"Size\"]\n        f[\"Key\"] = \"/\".join([bucket, f[\"Key\"]])\n        f[\"name\"] = f[\"Key\"]\n        version_id = f.get(\"VersionId\")\n        if versions and version_id and version_id != \"null\":\n            f[\"name\"] += f\"?versionId={version_id}\"\n\n    async def _glob(self, path, **kwargs):\n        if path.startswith(\"*\"):\n            raise ValueError(\"Cannot traverse all of S3\")\n        return await super()._glob(path, **kwargs)\n\n    async def _find(\n        self, path, maxdepth=None, withdirs=None, detail=False, prefix=\"\", **kwargs\n    ):\n        \"\"\"List all files below path.\n        Like posix ``find`` command without conditions\n\n        Parameters\n        ----------\n        path : str\n        maxdepth: int or None\n            If not None, the maximum number of levels to descend\n        withdirs: bool\n            Whether to include directory paths in the output. This is True\n            when used by glob, but users usually only want files.\n        prefix: str\n            Only return files that match ``^{path}/{prefix}`` (if there is an\n            exact match ``filename == {path}/{prefix}``, it also will be included)\n        \"\"\"\n        path = self._strip_protocol(path)\n        bucket, key, _ = self.split_path(path)\n        if not bucket:\n            raise ValueError(\"Cannot traverse all of S3\")\n        if (withdirs or maxdepth) and prefix:\n            # TODO: perhaps propagate these to a glob(f\"path/{prefix}*\") call\n            raise ValueError(\n                \"Can not specify 'prefix' option alongside 'withdirs'/'maxdepth' options.\"\n            )\n        if maxdepth:\n            return await super()._find(\n                bucket + \"/\" + key,\n                maxdepth=maxdepth,\n                withdirs=withdirs,\n                detail=detail,\n                **kwargs,\n            )\n        # TODO: implement find from dircache, if all listings are present\n        # if refresh is False:\n        #     out = incomplete_tree_dirs(self.dircache, path)\n        #     if len(out) == 1:\n        #         await self._find(out[0])\n        #         return super().find(path)\n        #     elif len(out) == 0:\n        #         return super().find(path)\n        #     # else: we refresh anyway, having at least two missing trees\n        out = await self._lsdir(path, delimiter=\"\", prefix=prefix, **kwargs)\n        if not out and key:\n            try:\n                out = [await self._info(path)]\n            except FileNotFoundError:\n                out = []\n        dirs = []\n        sdirs = set()\n        thisdircache = {}\n        for o in out:\n            par = self._parent(o[\"name\"])\n            if par not in sdirs:\n                sdirs.add(par)\n                d = False\n                if len(path) <= len(par):\n                    d = {\n                        \"Key\": self.split_path(par)[1],\n                        \"Size\": 0,\n                        \"name\": par,\n                        \"StorageClass\": \"DIRECTORY\",\n                        \"type\": \"directory\",\n                        \"size\": 0,\n                    }\n                    dirs.append(d)\n                thisdircache[par] = []\n                ppar = self._parent(par)\n                if ppar in thisdircache:\n                    if d and d not in thisdircache[ppar]:\n                        thisdircache[ppar].append(d)\n            if par in sdirs:\n                thisdircache[par].append(o)\n\n        # Explicitly add directories to their parents in the dircache\n        for d in dirs:\n            par = self._parent(d[\"name\"])\n            if par in thisdircache:\n                thisdircache[par].append(d)\n\n        if not prefix:\n            for k, v in thisdircache.items():\n                if k not in self.dircache and len(k) >= len(path):\n                    self.dircache[k] = v\n        if withdirs:\n            out = sorted(out + dirs, key=lambda x: x[\"name\"])\n        if detail:\n            return {o[\"name\"]: o for o in out}\n        return [o[\"name\"] for o in out]\n\n    find = sync_wrapper(_find)\n\n    async def _mkdir(self, path, acl=False, create_parents=True, **kwargs):\n        path = self._strip_protocol(path).rstrip(\"/\")\n        if not path:\n            raise ValueError\n        bucket, key, _ = self.split_path(path)\n        if await self._exists(bucket):\n            if not key:\n                # requested to create bucket, but bucket already exist\n                raise FileExistsError\n            # else: # do nothing as bucket is already created.\n        elif not key or create_parents:\n            if acl and acl not in buck_acls:\n                raise ValueError(\"ACL not in %s\", buck_acls)\n            try:\n                params = {\"Bucket\": bucket}\n                if acl:\n                    params[\"ACL\"] = acl\n                region_name = kwargs.get(\"region_name\", None) or self.client_kwargs.get(\n                    \"region_name\", None\n                )\n                if region_name:\n                    params[\"CreateBucketConfiguration\"] = {\n                        \"LocationConstraint\": region_name\n                    }\n                await self._call_s3(\"create_bucket\", **params)\n                self.invalidate_cache(\"\")\n                self.invalidate_cache(bucket)\n            except ClientError as e:\n                raise translate_boto_error(e)\n            except ParamValidationError as e:\n                raise ValueError(\"Bucket create failed %r: %s\" % (bucket, e))\n        else:\n            # raises if bucket doesn't exist and doesn't get create flag.\n            await self._ls(bucket)\n\n    mkdir = sync_wrapper(_mkdir)\n\n    async def _makedirs(self, path, exist_ok=False):\n        try:\n            await self._mkdir(path, create_parents=True)\n        except FileExistsError:\n            if exist_ok:\n                pass\n            else:\n                raise\n\n    makedirs = sync_wrapper(_makedirs)\n\n    async def _rmdir(self, path):\n        bucket, key, _ = self.split_path(path)\n        if key:\n            if await self._exists(path):\n                # User may have meant rm(path, recursive=True)\n                raise FileExistsError\n            raise FileNotFoundError\n\n        try:\n            await self._call_s3(\"delete_bucket\", Bucket=path)\n        except botocore.exceptions.ClientError as e:\n            if \"NoSuchBucket\" in str(e):\n                raise FileNotFoundError(path) from e\n            if \"BucketNotEmpty\" in str(e):\n                raise OSError from e\n            raise\n        self.invalidate_cache(path)\n        self.invalidate_cache(\"\")\n\n    rmdir = sync_wrapper(_rmdir)\n\n    async def _lsbuckets(self, refresh=False):\n        if \"\" not in self.dircache or refresh:\n            if self.anon:\n                # cannot list buckets if not logged in\n                return []\n            try:\n                files = (await self._call_s3(\"list_buckets\"))[\"Buckets\"]\n            except ClientError:\n                # listbucket permission missing\n                return []\n            for f in files:\n                f[\"Key\"] = f[\"Name\"]\n                f[\"Size\"] = 0\n                f[\"StorageClass\"] = \"BUCKET\"\n                f[\"size\"] = 0\n                f[\"type\"] = \"directory\"\n                f[\"name\"] = f[\"Name\"]\n                del f[\"Name\"]\n            self.dircache[\"\"] = files\n            return files\n        return self.dircache[\"\"]\n\n    async def _ls(self, path, detail=False, refresh=False, versions=False):\n        \"\"\"List files in given bucket, or list of buckets.\n\n        Listing is cached unless `refresh=True`.\n\n        Note: only your buckets associated with the login will be listed by\n        `ls('')`, not any public buckets (even if already accessed).\n\n        Parameters\n        ----------\n        path : string/bytes\n            location at which to list files\n        refresh : bool (=False)\n            if False, look in local cache for file details first\n        \"\"\"\n        path = self._strip_protocol(path).rstrip(\"/\")\n        if path in [\"\", \"/\"]:\n            files = await self._lsbuckets(refresh)\n        else:\n            files = await self._lsdir(path, refresh, versions=versions)\n            if not files and \"/\" in path:\n                try:\n                    files = await self._lsdir(\n                        self._parent(path), refresh=refresh, versions=versions\n                    )\n                except IOError:\n                    pass\n                files = [\n                    o\n                    for o in files\n                    if o[\"name\"].rstrip(\"/\") == path and o[\"type\"] != \"directory\"\n                ]\n                if not files:\n                    raise FileNotFoundError(path)\n            if detail:\n                return files\n        return files if detail else sorted([o[\"name\"] for o in files])\n\n    def _exists_in_cache(self, path, bucket, key, version_id):\n        fullpath = \"/\".join((bucket, key))\n\n        try:\n            entries = self._ls_from_cache(fullpath)\n        except FileNotFoundError:\n            return False\n\n        if entries is None:\n            return None\n\n        if not self.version_aware or version_id is None:\n            return True\n\n        for entry in entries:\n            if entry[\"name\"] == fullpath and entry.get(\"VersionId\") == version_id:\n                return True\n\n        # dircache doesn't support multiple versions, so we really can't tell if\n        # the one we want exists.\n        return None\n\n    async def _exists(self, path):\n        if path in [\"\", \"/\"]:\n            # the root always exists, even if anon\n            return True\n        path = self._strip_protocol(path)\n        bucket, key, version_id = self.split_path(path)\n        if key:\n            exists_in_cache = self._exists_in_cache(path, bucket, key, version_id)\n            if exists_in_cache is not None:\n                return exists_in_cache\n\n            try:\n                await self._info(path, bucket, key, version_id=version_id)\n                return True\n            except FileNotFoundError:\n                return False\n        elif self.dircache.get(bucket, False):\n            return True\n        else:\n            try:\n                if self._ls_from_cache(bucket):\n                    return True\n            except FileNotFoundError:\n                # might still be a bucket we can access but don't own\n                pass\n            try:\n                await self._call_s3(\n                    \"list_objects_v2\", MaxKeys=1, Bucket=bucket, **self.req_kw\n                )\n                return True\n            except Exception:\n                pass\n            try:\n                await self._call_s3(\"get_bucket_location\", Bucket=bucket, **self.req_kw)\n                return True\n            except Exception:\n                return False\n\n    exists = sync_wrapper(_exists)\n\n    async def _touch(self, path, truncate=True, data=None, **kwargs):\n        \"\"\"Create empty file or truncate\"\"\"\n        bucket, key, version_id = self.split_path(path)\n        if version_id:\n            raise ValueError(\"S3 does not support touching existing versions of files\")\n        if not truncate and await self._exists(path):\n            raise ValueError(\"S3 does not support touching existent files\")\n        try:\n            write_result = await self._call_s3(\n                \"put_object\", Bucket=bucket, Key=key, **kwargs\n            )\n        except ClientError as ex:\n            raise translate_boto_error(ex)\n        self.invalidate_cache(self._parent(path))\n        return write_result\n\n    touch = sync_wrapper(_touch)\n\n    async def _cat_file(self, path, version_id=None, start=None, end=None):\n        bucket, key, vers = self.split_path(path)\n        if start is not None or end is not None:\n            head = {\"Range\": await self._process_limits(path, start, end)}\n        else:\n            head = {}\n\n        async def _call_and_read():\n            resp = await self._call_s3(\n                \"get_object\",\n                Bucket=bucket,\n                Key=key,\n                **version_id_kw(version_id or vers),\n                **head,\n                **self.req_kw,\n            )\n            try:\n                return await resp[\"Body\"].read()\n            finally:\n                resp[\"Body\"].close()\n\n        return await _error_wrapper(_call_and_read, retries=self.retries)\n\n    async def _pipe_file(self, path, data, chunksize=50 * 2**20, **kwargs):\n        bucket, key, _ = self.split_path(path)\n        size = len(data)\n        # 5 GB is the limit for an S3 PUT\n        if size < min(5 * 2**30, 2 * chunksize):\n            return await self._call_s3(\n                \"put_object\", Bucket=bucket, Key=key, Body=data, **kwargs\n            )\n        else:\n\n            mpu = await self._call_s3(\n                \"create_multipart_upload\", Bucket=bucket, Key=key, **kwargs\n            )\n\n            # TODO: cancel MPU if the following fails\n            out = [\n                await self._call_s3(\n                    \"upload_part\",\n                    Bucket=bucket,\n                    PartNumber=i + 1,\n                    UploadId=mpu[\"UploadId\"],\n                    Body=data[off : off + chunksize],\n                    Key=key,\n                )\n                for i, off in enumerate(range(0, len(data), chunksize))\n            ]\n\n            parts = [\n                {\"PartNumber\": i + 1, \"ETag\": o[\"ETag\"]} for i, o in enumerate(out)\n            ]\n            await self._call_s3(\n                \"complete_multipart_upload\",\n                Bucket=bucket,\n                Key=key,\n                UploadId=mpu[\"UploadId\"],\n                MultipartUpload={\"Parts\": parts},\n            )\n        self.invalidate_cache(path)\n\n    async def _put_file(\n        self,\n        lpath,\n        rpath,\n        callback=_DEFAULT_CALLBACK,\n        chunksize=50 * 2**20,\n        max_concurrency=None,\n        **kwargs,\n    ):\n        bucket, key, _ = self.split_path(rpath)\n        if os.path.isdir(lpath):\n            if key:\n                # don't make remote \"directory\"\n                return\n            else:\n                await self._mkdir(lpath)\n        size = os.path.getsize(lpath)\n        callback.set_size(size)\n\n        if \"ContentType\" not in kwargs:\n            content_type, _ = mimetypes.guess_type(lpath)\n            if content_type is not None:\n                kwargs[\"ContentType\"] = content_type\n\n        with open(lpath, \"rb\") as f0:\n            if size < min(5 * 2**30, 2 * chunksize):\n                chunk = f0.read()\n                await self._call_s3(\n                    \"put_object\", Bucket=bucket, Key=key, Body=chunk, **kwargs\n                )\n                callback.relative_update(size)\n            else:\n\n                mpu = await self._call_s3(\n                    \"create_multipart_upload\", Bucket=bucket, Key=key, **kwargs\n                )\n                out = await self._upload_file_part_concurrent(\n                    bucket,\n                    key,\n                    mpu,\n                    f0,\n                    callback=callback,\n                    chunksize=chunksize,\n                    max_concurrency=max_concurrency,\n                )\n                parts = [\n                    {\"PartNumber\": i + 1, \"ETag\": o[\"ETag\"]} for i, o in enumerate(out)\n                ]\n                await self._call_s3(\n                    \"complete_multipart_upload\",\n                    Bucket=bucket,\n                    Key=key,\n                    UploadId=mpu[\"UploadId\"],\n                    MultipartUpload={\"Parts\": parts},\n                )\n        while rpath:\n            self.invalidate_cache(rpath)\n            rpath = self._parent(rpath)\n\n    async def _upload_file_part_concurrent(\n        self,\n        bucket,\n        key,\n        mpu,\n        f0,\n        callback=_DEFAULT_CALLBACK,\n        chunksize=50 * 2**20,\n        max_concurrency=None,\n    ):\n        max_concurrency = max_concurrency or self.max_concurrency\n        if max_concurrency < 1:\n            raise ValueError(\"max_concurrency must be >= 1\")\n\n        async def _upload_chunk(chunk, part_number):\n            result = await self._call_s3(\n                \"upload_part\",\n                Bucket=bucket,\n                PartNumber=part_number,\n                UploadId=mpu[\"UploadId\"],\n                Body=chunk,\n                Key=key,\n            )\n            callback.relative_update(len(chunk))\n            return result\n\n        out = []\n        while True:\n            chunks = []\n            for i in range(max_concurrency):\n                chunk = f0.read(chunksize)\n                if chunk:\n                    chunks.append(chunk)\n            if not chunks:\n                break\n            if len(chunks) > 1:\n                out.extend(\n                    await asyncio.gather(\n                        *[\n                            _upload_chunk(chunk, len(out) + i)\n                            for i, chunk in enumerate(chunks, 1)\n                        ]\n                    )\n                )\n            else:\n                out.append(await _upload_chunk(chunk, len(out) + 1))\n        return out\n\n    async def _get_file(\n        self, rpath, lpath, callback=_DEFAULT_CALLBACK, version_id=None, **kwargs\n    ):\n        if os.path.isdir(lpath):\n            return\n        bucket, key, vers = self.split_path(rpath)\n\n        async def _open_file(range: int):\n            kw = self.req_kw.copy()\n            if range:\n                kw[\"Range\"] = f\"bytes={range}-\"\n            resp = await self._call_s3(\n                \"get_object\",\n                Bucket=bucket,\n                Key=key,\n                **version_id_kw(version_id or vers),\n                **kw,\n            )\n            return resp[\"Body\"], resp.get(\"ContentLength\", None)\n\n        body, content_length = await _open_file(range=0)\n        callback.set_size(content_length)\n\n        failed_reads = 0\n        bytes_read = 0\n\n        try:\n            with open(lpath, \"wb\") as f0:\n                while True:\n                    try:\n                        chunk = await body.read(2**16)\n                    except S3_RETRYABLE_ERRORS:\n                        failed_reads += 1\n                        if failed_reads >= self.retries:\n                            # Give up if we've failed too many times.\n                            raise\n                        # Closing the body may result in an exception if we've failed to read from it.\n                        try:\n                            body.close()\n                        except Exception:\n                            pass\n\n                        await asyncio.sleep(min(1.7**failed_reads * 0.1, 15))\n                        # Byte ranges are inclusive, which means we need to be careful to not read the same data twice\n                        # in a failure.\n                        # Examples:\n                        # Read 1 byte -> failure, retry with read_range=0, byte range should be 0-\n                        # Read 1 byte, success. Read 1 byte: failure. Retry with read_range=1, byte-range should be 1-\n                        # Read 1 bytes, success. Read 1 bytes: success. Read 1 byte, failure. Retry with read_range=2,\n                        # byte-range should be 2-.\n                        body, _ = await _open_file(bytes_read)\n                        continue\n\n                    if not chunk:\n                        break\n                    bytes_read += len(chunk)\n                    segment_len = f0.write(chunk)\n                    callback.relative_update(segment_len)\n        finally:\n            try:\n                body.close()\n            except Exception:\n                pass\n\n    async def _info(self, path, bucket=None, key=None, refresh=False, version_id=None):\n        path = self._strip_protocol(path)\n        bucket, key, path_version_id = self.split_path(path)\n        fullpath = \"/\".join((bucket, key))\n\n        if version_id is not None:\n            if not self.version_aware:\n                raise ValueError(\n                    \"version_id cannot be specified if the \"\n                    \"filesystem is not version aware\"\n                )\n        if path in [\"/\", \"\"]:\n            return {\"name\": path, \"size\": 0, \"type\": \"directory\"}\n        version_id = _coalesce_version_id(path_version_id, version_id)\n        if not refresh:\n            out = self._ls_from_cache(fullpath)\n            if out is not None:\n                if self.version_aware and version_id is not None:\n                    # If cached info does not match requested version_id,\n                    # fallback to calling head_object\n                    out = [\n                        o\n                        for o in out\n                        if o[\"name\"] == fullpath and version_id == o.get(\"VersionId\")\n                    ]\n                    if out:\n                        return out[0]\n                else:\n                    out = [o for o in out if o[\"name\"] == fullpath]\n                    if out:\n                        return out[0]\n                    return {\"name\": path, \"size\": 0, \"type\": \"directory\"}\n        if key:\n            try:\n                out = await self._call_s3(\n                    \"head_object\",\n                    self.kwargs,\n                    Bucket=bucket,\n                    Key=key,\n                    **version_id_kw(version_id),\n                    **self.req_kw,\n                )\n                return {\n                    \"ETag\": out.get(\"ETag\", \"\"),\n                    \"LastModified\": out.get(\"LastModified\", \"\"),\n                    \"size\": out[\"ContentLength\"],\n                    \"name\": \"/\".join([bucket, key]),\n                    \"type\": \"file\",\n                    \"StorageClass\": out.get(\"StorageClass\", \"STANDARD\"),\n                    \"VersionId\": out.get(\"VersionId\"),\n                    \"ContentType\": out.get(\"ContentType\"),\n                }\n            except FileNotFoundError:\n                pass\n            except ClientError as e:\n                raise translate_boto_error(e, set_cause=False)\n\n        try:\n            # We check to see if the path is a directory by attempting to list its\n            # contexts. If anything is found, it is indeed a directory\n            out = await self._call_s3(\n                \"list_objects_v2\",\n                self.kwargs,\n                Bucket=bucket,\n                Prefix=key.rstrip(\"/\") + \"/\" if key else \"\",\n                Delimiter=\"/\",\n                MaxKeys=1,\n                **self.req_kw,\n            )\n            if (\n                out.get(\"KeyCount\", 0) > 0\n                or out.get(\"Contents\", [])\n                or out.get(\"CommonPrefixes\", [])\n            ):\n                return {\n                    \"name\": \"/\".join([bucket, key]),\n                    \"type\": \"directory\",\n                    \"size\": 0,\n                    \"StorageClass\": \"DIRECTORY\",\n                }\n\n            raise FileNotFoundError(path)\n        except ClientError as e:\n            raise translate_boto_error(e, set_cause=False)\n        except ParamValidationError as e:\n            raise ValueError(\"Failed to list path %r: %s\" % (path, e))\n\n    async def _checksum(self, path, refresh=False):\n        \"\"\"\n        Unique value for current version of file\n\n        If the checksum is the same from one moment to another, the contents\n        are guaranteed to be the same. If the checksum changes, the contents\n        *might* have changed.\n\n        Parameters\n        ----------\n        path : string/bytes\n            path of file to get checksum for\n        refresh : bool (=False)\n            if False, look in local cache for file details first\n\n        \"\"\"\n\n        info = await self._info(path, refresh=refresh)\n\n        if info[\"type\"] != \"directory\":\n            return int(info[\"ETag\"].strip('\"').split(\"-\")[0], 16)\n        else:\n            return int(tokenize(info), 16)\n\n    checksum = sync_wrapper(_checksum)\n\n    async def _isdir(self, path):\n        path = self._strip_protocol(path).strip(\"/\")\n        # Send buckets to super\n        if \"/\" not in path:\n            if path == \"\":\n                return True\n            try:\n                out = await self._lsdir(path)\n                return True\n            except FileNotFoundError:\n                return False\n\n        if path in self.dircache:\n            for fp in self.dircache[path]:\n                # For files the dircache can contain itself.\n                # If it contains anything other than itself it is a directory.\n                if fp[\"name\"] != path:\n                    return True\n            return False\n\n        parent = self._parent(path)\n        if parent in self.dircache:\n            for f in self.dircache[parent]:\n                if f[\"name\"] == path:\n                    # If we find ourselves return whether we are a directory\n                    return f[\"type\"] == \"directory\"\n            return False\n\n        # This only returns things within the path and NOT the path object itself\n        try:\n            return bool(await self._lsdir(path))\n        except FileNotFoundError:\n            return False\n\n    isdir = sync_wrapper(_isdir)\n\n    async def _object_version_info(self, path, **kwargs):\n        if not self.version_aware:\n            raise ValueError(\n                \"version specific functionality is disabled for \"\n                \"non-version aware filesystems\"\n            )\n        bucket, key, _ = self.split_path(path)\n        kwargs = {}\n        out = {\"IsTruncated\": True}\n        versions = []\n        while out[\"IsTruncated\"]:\n            out = await self._call_s3(\n                \"list_object_versions\",\n                kwargs,\n                Bucket=bucket,\n                Prefix=key,\n                **self.req_kw,\n            )\n            versions.extend(out[\"Versions\"])\n            kwargs.update(\n                {\n                    \"VersionIdMarker\": out.get(\"NextVersionIdMarker\", \"\"),\n                    \"KeyMarker\": out.get(\"NextKeyMarker\", \"\"),\n                }\n            )\n        return versions\n\n    object_version_info = sync_wrapper(_object_version_info)\n\n    _metadata_cache = {}\n\n    async def _metadata(self, path, refresh=False, **kwargs):\n        \"\"\"Return metadata of path.\n\n        Parameters\n        ----------\n        path : string/bytes\n            filename to get metadata for\n        refresh : bool (=False)\n            (ignored)\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n        response = await self._call_s3(\n            \"head_object\",\n            kwargs,\n            Bucket=bucket,\n            Key=key,\n            **version_id_kw(version_id),\n            **self.req_kw,\n        )\n        meta = {k.replace(\"_\", \"-\"): v for k, v in response[\"Metadata\"].items()}\n        return meta\n\n    metadata = sync_wrapper(_metadata)\n\n    def get_tags(self, path):\n        \"\"\"Retrieve tag key/values for the given path\n\n        Returns\n        -------\n        {str: str}\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n        response = self.call_s3(\n            \"get_object_tagging\",\n            Bucket=bucket,\n            Key=key,\n            **version_id_kw(version_id),\n        )\n        return {v[\"Key\"]: v[\"Value\"] for v in response[\"TagSet\"]}\n\n    def put_tags(self, path, tags, mode=\"o\"):\n        \"\"\"Set tags for given existing key\n\n        Tags are a str:str mapping that can be attached to any key, see\n        https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocation-tag-restrictions.html\n\n        This is similar to, but distinct from, key metadata, which is usually\n        set at key creation time.\n\n        Parameters\n        ----------\n        path: str\n            Existing key to attach tags to\n        tags: dict str, str\n            Tags to apply.\n        mode:\n            One of 'o' or 'm'\n            'o': Will over-write any existing tags.\n            'm': Will merge in new tags with existing tags.  Incurs two remote\n            calls.\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n\n        if mode == \"m\":\n            existing_tags = self.get_tags(path=path)\n            existing_tags.update(tags)\n            new_tags = [{\"Key\": k, \"Value\": v} for k, v in existing_tags.items()]\n        elif mode == \"o\":\n            new_tags = [{\"Key\": k, \"Value\": v} for k, v in tags.items()]\n        else:\n            raise ValueError(\"Mode must be {'o', 'm'}, not %s\" % mode)\n\n        tag = {\"TagSet\": new_tags}\n        self.call_s3(\n            \"put_object_tagging\",\n            Bucket=bucket,\n            Key=key,\n            Tagging=tag,\n            **version_id_kw(version_id),\n        )\n\n    async def _getxattr(self, path, attr_name, **kwargs):\n        \"\"\"Get an attribute from the metadata.\n\n        Examples\n        --------\n        >>> mys3fs.getxattr('mykey', 'attribute_1')  # doctest: +SKIP\n        'value_1'\n        \"\"\"\n        attr_name = attr_name.replace(\"_\", \"-\")\n        xattr = await self._metadata(path, **kwargs)\n        if attr_name in xattr:\n            return xattr[attr_name]\n        return None\n\n    getxattr = sync_wrapper(_getxattr)\n\n    async def _setxattr(self, path, copy_kwargs=None, **kw_args):\n        \"\"\"Set metadata.\n\n        Attributes have to be of the form documented in the\n        `Metadata Reference`_.\n\n        Parameters\n        ----------\n        kw_args : key-value pairs like field=\"value\", where the values must be\n            strings. Does not alter existing fields, unless\n            the field appears here - if the value is None, delete the\n            field.\n        copy_kwargs : dict, optional\n            dictionary of additional params to use for the underlying\n            s3.copy_object.\n\n        Examples\n        --------\n        >>> mys3file.setxattr(attribute_1='value1', attribute_2='value2')  # doctest: +SKIP\n        # Example for use with copy_args\n        >>> mys3file.setxattr(copy_kwargs={'ContentType': 'application/pdf'},\n        ...     attribute_1='value1')  # doctest: +SKIP\n\n        .. _Metadata Reference: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-metadata\n        \"\"\"\n\n        kw_args = {k.replace(\"_\", \"-\"): v for k, v in kw_args.items()}\n        bucket, key, version_id = self.split_path(path)\n        metadata = await self._metadata(path)\n        metadata.update(**kw_args)\n        copy_kwargs = copy_kwargs or {}\n\n        # remove all keys that are None\n        for kw_key in kw_args:\n            if kw_args[kw_key] is None:\n                metadata.pop(kw_key, None)\n\n        src = {\"Bucket\": bucket, \"Key\": key}\n        if version_id:\n            src[\"VersionId\"] = version_id\n\n        await self._call_s3(\n            \"copy_object\",\n            copy_kwargs,\n            CopySource=src,\n            Bucket=bucket,\n            Key=key,\n            Metadata=metadata,\n            MetadataDirective=\"REPLACE\",\n        )\n\n        # refresh metadata\n        self._metadata_cache[path] = metadata\n\n    setxattr = sync_wrapper(_setxattr)\n\n    async def _chmod(self, path, acl, recursive=False, **kwargs):\n        \"\"\"Set Access Control on a bucket/key\n\n        See http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl\n\n        Parameters\n        ----------\n        path : string\n            the object to set\n        acl : string\n            the value of ACL to apply\n        recursive : bool\n            whether to apply the ACL to all keys below the given path too\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n        if recursive:\n            allfiles = await self._find(path, withdirs=False)\n            await asyncio.gather(\n                *[self._chmod(p, acl, recursive=False) for p in allfiles]\n            )\n        elif key:\n            if acl not in key_acls:\n                raise ValueError(\"ACL not in %s\", key_acls)\n            await self._call_s3(\n                \"put_object_acl\",\n                kwargs,\n                Bucket=bucket,\n                Key=key,\n                ACL=acl,\n                **version_id_kw(version_id),\n            )\n        if not key:\n            if acl not in buck_acls:\n                raise ValueError(\"ACL not in %s\", buck_acls)\n            await self._call_s3(\"put_bucket_acl\", kwargs, Bucket=bucket, ACL=acl)\n\n    chmod = sync_wrapper(_chmod)\n\n    async def _url(self, path, expires=3600, client_method=\"get_object\", **kwargs):\n        \"\"\"Generate presigned URL to access path by HTTP\n\n        Parameters\n        ----------\n        path : string\n            the key path we are interested in\n        expires : int\n            the number of seconds this signature will be good for.\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n        await self.set_session()\n        s3 = await self.get_s3(bucket)\n        return await s3.generate_presigned_url(\n            ClientMethod=client_method,\n            Params=dict(Bucket=bucket, Key=key, **version_id_kw(version_id), **kwargs),\n            ExpiresIn=expires,\n        )\n\n    url = sync_wrapper(_url)\n\n    async def _merge(self, path, filelist, **kwargs):\n        \"\"\"Create single S3 file from list of S3 files\n\n        Uses multi-part, no data is downloaded. The original files are\n        not deleted.\n\n        Parameters\n        ----------\n        path : str\n            The final file to produce\n        filelist : list of str\n            The paths, in order, to assemble into the final file.\n        \"\"\"\n        bucket, key, version_id = self.split_path(path)\n        if version_id:\n            raise ValueError(\"Cannot write to an explicit versioned file!\")\n        mpu = await self._call_s3(\n            \"create_multipart_upload\", kwargs, Bucket=bucket, Key=key\n        )\n        # TODO: Make this support versions?\n        out = await asyncio.gather(\n            *[\n                self._call_s3(\n                    \"upload_part_copy\",\n                    kwargs,\n                    Bucket=bucket,\n                    Key=key,\n                    UploadId=mpu[\"UploadId\"],\n                    CopySource=f,\n                    PartNumber=i + 1,\n                )\n                for (i, f) in enumerate(filelist)\n            ]\n        )\n        parts = [\n            {\"PartNumber\": i + 1, \"ETag\": o[\"CopyPartResult\"][\"ETag\"]}\n            for (i, o) in enumerate(out)\n        ]\n        part_info = {\"Parts\": parts}\n        await self._call_s3(\n            \"complete_multipart_upload\",\n            Bucket=bucket,\n            Key=key,\n            UploadId=mpu[\"UploadId\"],\n            MultipartUpload=part_info,\n        )\n        self.invalidate_cache(path)\n\n    merge = sync_wrapper(_merge)\n\n    async def _copy_basic(self, path1, path2, **kwargs):\n        \"\"\"Copy file between locations on S3\n\n        Not allowed where the origin is >5GB - use copy_managed\n        \"\"\"\n        buc1, key1, ver1 = self.split_path(path1)\n        buc2, key2, ver2 = self.split_path(path2)\n        if ver2:\n            raise ValueError(\"Cannot copy to a versioned file!\")\n        try:\n            copy_src = {\"Bucket\": buc1, \"Key\": key1}\n            if ver1:\n                copy_src[\"VersionId\"] = ver1\n            await self._call_s3(\n                \"copy_object\", kwargs, Bucket=buc2, Key=key2, CopySource=copy_src\n            )\n        except ClientError as e:\n            raise translate_boto_error(e)\n        except ParamValidationError as e:\n            raise ValueError(\"Copy failed (%r -> %r): %s\" % (path1, path2, e)) from e\n        self.invalidate_cache(path2)\n\n    async def _copy_etag_preserved(self, path1, path2, size, total_parts, **kwargs):\n        \"\"\"Copy file between locations on S3 as multi-part while preserving\n        the etag (using the same part sizes for each part\"\"\"\n\n        bucket1, key1, version1 = self.split_path(path1)\n        bucket2, key2, version2 = self.split_path(path2)\n\n        mpu = await self._call_s3(\n            \"create_multipart_upload\", Bucket=bucket2, Key=key2, **kwargs\n        )\n        part_infos = await asyncio.gather(\n            *[\n                self._call_s3(\"head_object\", Bucket=bucket1, Key=key1, PartNumber=i)\n                for i in range(1, total_parts + 1)\n            ]\n        )\n\n        parts = []\n        brange_first = 0\n        for i, part_info in enumerate(part_infos, 1):\n            part_size = part_info[\"ContentLength\"]\n            brange_last = brange_first + part_size - 1\n            if brange_last > size:\n                brange_last = size - 1\n\n            part = await self._call_s3(\n                \"upload_part_copy\",\n                Bucket=bucket2,\n                Key=key2,\n                PartNumber=i,\n                UploadId=mpu[\"UploadId\"],\n                CopySource=path1,\n                CopySourceRange=\"bytes=%i-%i\" % (brange_first, brange_last),\n            )\n            parts.append({\"PartNumber\": i, \"ETag\": part[\"CopyPartResult\"][\"ETag\"]})\n            brange_first += part_size\n\n        await self._call_s3(\n            \"complete_multipart_upload\",\n            Bucket=bucket2,\n            Key=key2,\n            UploadId=mpu[\"UploadId\"],\n            MultipartUpload={\"Parts\": parts},\n        )\n        self.invalidate_cache(path2)\n\n    async def _copy_managed(self, path1, path2, size, block=5 * 2**30, **kwargs):\n        \"\"\"Copy file between locations on S3 as multi-part\n\n        block: int\n            The size of the pieces, must be larger than 5MB and at most 5GB.\n            Smaller blocks mean more calls, only useful for testing.\n        \"\"\"\n        if block < 5 * 2**20 or block > 5 * 2**30:\n            raise ValueError(\"Copy block size must be 5MB<=block<=5GB\")\n        bucket, key, version = self.split_path(path2)\n        mpu = await self._call_s3(\n            \"create_multipart_upload\", Bucket=bucket, Key=key, **kwargs\n        )\n        # attempting to do the following calls concurrently with gather causes\n        # occasional \"upload is smaller than the minimum allowed\"\n        out = [\n            await self._call_s3(\n                \"upload_part_copy\",\n                Bucket=bucket,\n                Key=key,\n                PartNumber=i + 1,\n                UploadId=mpu[\"UploadId\"],\n                CopySource=path1,\n                CopySourceRange=\"bytes=%i-%i\" % (brange_first, brange_last),\n            )\n            for i, (brange_first, brange_last) in enumerate(_get_brange(size, block))\n        ]\n        parts = [\n            {\"PartNumber\": i + 1, \"ETag\": o[\"CopyPartResult\"][\"ETag\"]}\n            for i, o in enumerate(out)\n        ]\n        await self._call_s3(\n            \"complete_multipart_upload\",\n            Bucket=bucket,\n            Key=key,\n            UploadId=mpu[\"UploadId\"],\n            MultipartUpload={\"Parts\": parts},\n        )\n        self.invalidate_cache(path2)\n\n    async def _cp_file(self, path1, path2, preserve_etag=None, **kwargs):\n        \"\"\"Copy file between locations on S3.\n\n        preserve_etag: bool\n            Whether to preserve etag while copying. If the file is uploaded\n            as a single part, then it will be always equalivent to the md5\n            hash of the file hence etag will always be preserved. But if the\n            file is uploaded in multi parts, then this option will try to\n            reproduce the same multipart upload while copying and preserve\n            the generated etag.\n        \"\"\"\n        path1 = self._strip_protocol(path1)\n        bucket, key, vers = self.split_path(path1)\n\n        info = await self._info(path1, bucket, key, version_id=vers)\n        size = info[\"size\"]\n\n        _, _, parts_suffix = info.get(\"ETag\", \"\").strip('\"').partition(\"-\")\n        if preserve_etag and parts_suffix:\n            await self._copy_etag_preserved(\n                path1, path2, size, total_parts=int(parts_suffix)\n            )\n        elif size <= MANAGED_COPY_THRESHOLD:\n            # simple copy allowed for <5GB\n            await self._copy_basic(path1, path2, **kwargs)\n        else:\n            # if the preserve_etag is true, either the file is uploaded\n            # on multiple parts or the size is lower than 5GB\n            assert not preserve_etag\n\n            # serial multipart copy\n            await self._copy_managed(path1, path2, size, **kwargs)\n\n    async def _list_multipart_uploads(self, bucket):\n        out = await self._call_s3(\"list_multipart_uploads\", Bucket=bucket)\n        return out.get(\"Contents\", []) or out.get(\"Uploads\", [])\n\n    list_multipart_uploads = sync_wrapper(_list_multipart_uploads)\n\n    async def _clear_multipart_uploads(self, bucket):\n        \"\"\"Remove any partial uploads in the bucket\"\"\"\n        out = await self._list_multipart_uploads(bucket)\n        await asyncio.gather(\n            *[\n                self._call_s3(\n                    \"abort_multipart_upload\",\n                    Bucket=bucket,\n                    Key=upload[\"Key\"],\n                    UploadId=upload[\"UploadId\"],\n                )\n                for upload in out\n            ]\n        )\n\n    clear_multipart_uploads = sync_wrapper(_clear_multipart_uploads)\n\n    async def _bulk_delete(self, pathlist, **kwargs):\n        \"\"\"\n        Remove multiple keys with one call\n\n        Parameters\n        ----------\n        pathlist : list(str)\n            The keys to remove, must all be in the same bucket.\n            Must have 0 < len <= 1000\n        \"\"\"\n        if not pathlist:\n            return []\n        buckets = {self.split_path(path)[0] for path in pathlist}\n        if len(buckets) > 1:\n            raise ValueError(\"Bulk delete files should refer to only one bucket\")\n        bucket = buckets.pop()\n        if len(pathlist) > 1000:\n            raise ValueError(\"Max number of files to delete in one call is 1000\")\n        delete_keys = {\n            \"Objects\": [{\"Key\": self.split_path(path)[1]} for path in pathlist],\n            \"Quiet\": True,\n        }\n        for path in pathlist:\n            self.invalidate_cache(self._parent(path))\n        out = await self._call_s3(\n            \"delete_objects\", kwargs, Bucket=bucket, Delete=delete_keys\n        )\n        # TODO: we report on successes but don't raise on any errors, effectively\n        #  on_error=\"omit\"\n        return [f\"{bucket}/{_['Key']}\" for _ in out.get(\"Deleted\", [])]\n\n    async def _rm_file(self, path, **kwargs):\n        bucket, key, _ = self.split_path(path)\n        self.invalidate_cache(path)\n\n        try:\n            await self._call_s3(\"delete_object\", Bucket=bucket, Key=key)\n        except ClientError as e:\n            raise translate_boto_error(e)\n\n    async def _rm(self, path, recursive=False, **kwargs):\n        if recursive and isinstance(path, str):\n            bucket, key, _ = self.split_path(path)\n            if not key and await self._is_bucket_versioned(bucket):\n                # special path to completely remove versioned bucket\n                await self._rm_versioned_bucket_contents(bucket)\n        paths = await self._expand_path(path, recursive=recursive)\n        files = [p for p in paths if self.split_path(p)[1]]\n        dirs = [p for p in paths if not self.split_path(p)[1]]\n        # TODO: fails if more than one bucket in list\n        out = await _run_coros_in_chunks(\n            [\n                self._bulk_delete(files[i : i + 1000])\n                for i in range(0, len(files), 1000)\n            ],\n            batch_size=3,\n            nofiles=True,\n        )\n        await asyncio.gather(*[self._rmdir(d) for d in dirs])\n        [\n            (self.invalidate_cache(p), self.invalidate_cache(self._parent(p)))\n            for p in paths\n        ]\n        return sum(out, [])\n\n    async def _is_bucket_versioned(self, bucket):\n        return (await self._call_s3(\"get_bucket_versioning\", Bucket=bucket)).get(\n            \"Status\", \"\"\n        ) == \"Enabled\"\n\n    is_bucket_versioned = sync_wrapper(_is_bucket_versioned)\n\n    async def _make_bucket_versioned(self, bucket, versioned: bool = True):\n        \"\"\"Set bucket versioning status\"\"\"\n        status = \"Enabled\" if versioned else \"Suspended\"\n        return await self._call_s3(\n            \"put_bucket_versioning\",\n            Bucket=bucket,\n            VersioningConfiguration={\"Status\": status},\n        )\n\n    make_bucket_versioned = sync_wrapper(_make_bucket_versioned)\n\n    async def _rm_versioned_bucket_contents(self, bucket):\n        \"\"\"Remove a versioned bucket and all contents\"\"\"\n        await self.set_session()\n        s3 = await self.get_s3(bucket)\n        pag = s3.get_paginator(\"list_object_versions\")\n        async for plist in pag.paginate(Bucket=bucket):\n            obs = plist.get(\"Versions\", []) + plist.get(\"DeleteMarkers\", [])\n            delete_keys = {\n                \"Objects\": [\n                    {\"Key\": i[\"Key\"], \"VersionId\": i[\"VersionId\"]} for i in obs\n                ],\n                \"Quiet\": True,\n            }\n            if obs:\n                await self._call_s3(\"delete_objects\", Bucket=bucket, Delete=delete_keys)\n\n    def invalidate_cache(self, path=None):\n        if path is None:\n            self.dircache.clear()\n        else:\n            path = self._strip_protocol(path)\n            self.dircache.pop(path, None)\n            while path:\n                self.dircache.pop(path, None)\n                path = self._parent(path)\n\n    async def _walk(self, path, maxdepth=None, **kwargs):\n        if path in [\"\", \"*\"] + [\"{}://\".format(p) for p in self.protocol]:\n            raise ValueError(\"Cannot crawl all of S3\")\n        async for _ in super()._walk(path, maxdepth=maxdepth, **kwargs):\n            yield _\n\n    def modified(self, path, version_id=None, refresh=False):\n        \"\"\"Return the last modified timestamp of file at `path` as a datetime\"\"\"\n        info = self.info(path=path, version_id=version_id, refresh=refresh)\n        if \"LastModified\" not in info:\n            # This path is a bucket or folder, which do not currently have a modified date\n            raise IsADirectoryError\n        return info[\"LastModified\"]\n\n    def sign(self, path, expiration=100, **kwargs):\n        return self.url(path, expires=expiration, **kwargs)\n\n    async def _invalidate_region_cache(self):\n        \"\"\"Invalidate the region cache (associated with buckets)\n        if ``cache_regions`` is turned on.\"\"\"\n        if not self.cache_regions:\n            return None\n\n        # If the region cache is not initialized, then\n        # do nothing.\n        cache = getattr(self, \"_s3creator\", None)\n        if cache is not None:\n            await cache.clear()\n\n    invalidate_region_cache = sync_wrapper(_invalidate_region_cache)\n\n    async def open_async(self, path, mode=\"rb\", **kwargs):\n        if \"b\" not in mode or kwargs.get(\"compression\"):\n            raise ValueError\n        return S3AsyncStreamedFile(self, path, mode)\n\n\nclass S3File(AbstractBufferedFile):\n    \"\"\"\n    Open S3 key as a file. Data is only loaded and cached on demand.\n\n    Parameters\n    ----------\n    s3 : S3FileSystem\n        botocore connection\n    path : string\n        S3 bucket/key to access\n    mode : str\n        One of 'rb', 'wb', 'ab'. These have the same meaning\n        as they do for the built-in `open` function.\n    block_size : int\n        read-ahead size for finding delimiters\n    fill_cache : bool\n        If seeking to new a part of the file beyond the current buffer,\n        with this True, the buffer will be filled between the sections to\n        best support random access. When reading only a few specific chunks\n        out of a file, performance may be better if False.\n    acl: str\n        Canned ACL to apply\n    version_id : str\n        Optional version to read the file at.  If not specified this will\n        default to the current version of the object.  This is only used for\n        reading.\n    requester_pays : bool (False)\n        If RequesterPays buckets are supported.\n\n    Examples\n    --------\n    >>> s3 = S3FileSystem()  # doctest: +SKIP\n    >>> with s3.open('my-bucket/my-file.txt', mode='rb') as f:  # doctest: +SKIP\n    ...     ...  # doctest: +SKIP\n\n    See Also\n    --------\n    S3FileSystem.open: used to create ``S3File`` objects\n\n    \"\"\"\n\n    retries = 5\n    part_min = 5 * 2**20\n    part_max = 5 * 2**30\n\n    def __init__(\n        self,\n        s3,\n        path,\n        mode=\"rb\",\n        block_size=5 * 2**20,\n        acl=False,\n        version_id=None,\n        fill_cache=True,\n        s3_additional_kwargs=None,\n        autocommit=True,\n        cache_type=\"readahead\",\n        requester_pays=False,\n        cache_options=None,\n        size=None,\n    ):\n        bucket, key, path_version_id = s3.split_path(path)\n        if not key:\n            raise ValueError(\"Attempt to open non key-like path: %s\" % path)\n        self.bucket = bucket\n        self.key = key\n        self.version_id = _coalesce_version_id(version_id, path_version_id)\n        self.acl = acl\n        if self.acl and self.acl not in key_acls:\n            raise ValueError(\"ACL not in %s\", key_acls)\n        self.mpu = None\n        self.parts = None\n        self.fill_cache = fill_cache\n        self.s3_additional_kwargs = s3_additional_kwargs or {}\n        self.req_kw = {\"RequestPayer\": \"requester\"} if requester_pays else {}\n        if \"r\" not in mode:\n            if block_size < 5 * 2**20:\n                raise ValueError(\"Block size must be >=5MB\")\n        else:\n            if version_id and s3.version_aware:\n                self.version_id = version_id\n                self.details = s3.info(path, version_id=version_id)\n                self.size = self.details[\"size\"]\n            elif s3.version_aware:\n                # In this case we have not managed to get the VersionId out of details and\n                # we should invalidate the cache and perform a full head_object since it\n                # has likely been partially populated by ls.\n                s3.invalidate_cache(path)\n                self.details = s3.info(path)\n                self.version_id = self.details.get(\"VersionId\")\n        super().__init__(\n            s3,\n            path,\n            mode,\n            block_size,\n            autocommit=autocommit,\n            cache_type=cache_type,\n            cache_options=cache_options,\n            size=size,\n        )\n        self.s3 = self.fs  # compatibility\n\n        # when not using autocommit we want to have transactional state to manage\n        self.append_block = False\n\n        if \"a\" in mode and s3.exists(path):\n            # See:\n            # put: https://boto3.amazonaws.com/v1/documentation/api/latest\n            # /reference/services/s3.html#S3.Client.put_object\n            #\n            # head: https://boto3.amazonaws.com/v1/documentation/api/latest\n            # /reference/services/s3.html#S3.Client.head_object\n            head = self._call_s3(\n                \"head_object\",\n                self.kwargs,\n                Bucket=bucket,\n                Key=key,\n                **version_id_kw(version_id),\n                **self.req_kw,\n            )\n\n            head = {\n                key: value\n                for key, value in head.items()\n                if key in _PRESERVE_KWARGS and key not in self.s3_additional_kwargs\n            }\n\n            loc = head.pop(\"ContentLength\")\n            if loc < 5 * 2**20:\n                # existing file too small for multi-upload: download\n                self.write(self.fs.cat(self.path))\n            else:\n                self.append_block = True\n            self.loc = loc\n\n            # Reflect head\n            self.s3_additional_kwargs.update(head)\n\n        if \"r\" in mode and size is None and \"ETag\" in self.details:\n            self.req_kw[\"IfMatch\"] = self.details[\"ETag\"]\n\n    def _call_s3(self, method, *kwarglist, **kwargs):\n        return self.fs.call_s3(method, self.s3_additional_kwargs, *kwarglist, **kwargs)\n\n    def _initiate_upload(self):\n        if self.autocommit and not self.append_block and self.tell() < self.blocksize:\n            # only happens when closing small file, use on-shot PUT\n            return\n        logger.debug(\"Initiate upload for %s\" % self)\n        self.parts = []\n        kw = dict(\n            Bucket=self.bucket,\n            Key=self.key,\n        )\n        if self.acl:\n            kw[\"ACL\"] = self.acl\n        self.mpu = self._call_s3(\"create_multipart_upload\", **kw)\n\n        if self.append_block:\n            # use existing data in key when appending,\n            # and block is big enough\n            out = self._call_s3(\n                \"upload_part_copy\",\n                self.s3_additional_kwargs,\n                Bucket=self.bucket,\n                Key=self.key,\n                PartNumber=1,\n                UploadId=self.mpu[\"UploadId\"],\n                CopySource=self.path,\n            )\n            self.parts.append({\"PartNumber\": 1, \"ETag\": out[\"CopyPartResult\"][\"ETag\"]})\n\n    def metadata(self, refresh=False, **kwargs):\n        \"\"\"Return metadata of file.\n        See :func:`~s3fs.S3Filesystem.metadata`.\n\n        Metadata is cached unless `refresh=True`.\n        \"\"\"\n        return self.fs.metadata(self.path, refresh, **kwargs)\n\n    def getxattr(self, xattr_name, **kwargs):\n        \"\"\"Get an attribute from the metadata.\n        See :func:`~s3fs.S3Filesystem.getxattr`.\n\n        Examples\n        --------\n        >>> mys3file.getxattr('attribute_1')  # doctest: +SKIP\n        'value_1'\n        \"\"\"\n        return self.fs.getxattr(self.path, xattr_name, **kwargs)\n\n    def setxattr(self, copy_kwargs=None, **kwargs):\n        \"\"\"Set metadata.\n        See :func:`~s3fs.S3Filesystem.setxattr`.\n\n        Examples\n        --------\n        >>> mys3file.setxattr(attribute_1='value1', attribute_2='value2')  # doctest: +SKIP\n        \"\"\"\n        if self.writable():\n            raise NotImplementedError(\n                \"cannot update metadata while file is open for writing\"\n            )\n        return self.fs.setxattr(self.path, copy_kwargs=copy_kwargs, **kwargs)\n\n    def url(self, **kwargs):\n        \"\"\"HTTP URL to read this file (if it already exists)\"\"\"\n        return self.fs.url(self.path, **kwargs)\n\n    def _fetch_range(self, start, end):\n        try:\n            return _fetch_range(\n                self.fs,\n                self.bucket,\n                self.key,\n                self.version_id,\n                start,\n                end,\n                req_kw=self.req_kw,\n            )\n\n        except OSError as ex:\n            if ex.args[0] == errno.EINVAL and \"pre-conditions\" in ex.args[1]:\n                raise FileExpired(\n                    filename=self.details[\"name\"], e_tag=self.details.get(\"ETag\")\n                ) from ex\n            else:\n                raise\n\n    def _upload_chunk(self, final=False):\n        bucket, key, _ = self.fs.split_path(self.path)\n        logger.debug(\n            \"Upload for %s, final=%s, loc=%s, buffer loc=%s\"\n            % (self, final, self.loc, self.buffer.tell())\n        )\n        if (\n            self.autocommit\n            and not self.append_block\n            and final\n            and self.tell() < self.blocksize\n        ):\n            # only happens when closing small file, use on-shot PUT\n            data1 = False\n        else:\n            self.buffer.seek(0)\n            (data0, data1) = (None, self.buffer.read(self.blocksize))\n\n        while data1:\n            (data0, data1) = (data1, self.buffer.read(self.blocksize))\n            data1_size = len(data1)\n\n            if 0 < data1_size < self.blocksize:\n                remainder = data0 + data1\n                remainder_size = self.blocksize + data1_size\n\n                if remainder_size <= self.part_max:\n                    (data0, data1) = (remainder, None)\n                else:\n                    partition = remainder_size // 2\n                    (data0, data1) = (remainder[:partition], remainder[partition:])\n\n            part = len(self.parts) + 1\n            logger.debug(\"Upload chunk %s, %s\" % (self, part))\n\n            out = self._call_s3(\n                \"upload_part\",\n                Bucket=bucket,\n                PartNumber=part,\n                UploadId=self.mpu[\"UploadId\"],\n                Body=data0,\n                Key=key,\n            )\n\n            part_header = {\"PartNumber\": part, \"ETag\": out[\"ETag\"]}\n            if \"ChecksumSHA256\" in out:\n                part_header[\"ChecksumSHA256\"] = out[\"ChecksumSHA256\"]\n            self.parts.append(part_header)\n\n        if self.autocommit and final:\n            self.commit()\n        return not final\n\n    def commit(self):\n        logger.debug(\"Commit %s\" % self)\n        if self.tell() == 0:\n            if self.buffer is not None:\n                logger.debug(\"Empty file committed %s\" % self)\n                self._abort_mpu()\n                write_result = self.fs.touch(self.path, **self.kwargs)\n        elif not self.parts:\n            if self.buffer is not None:\n                logger.debug(\"One-shot upload of %s\" % self)\n                self.buffer.seek(0)\n                data = self.buffer.read()\n                kw = dict(Key=self.key, Bucket=self.bucket, Body=data, **self.kwargs)\n                if self.acl:\n                    kw[\"ACL\"] = self.acl\n                write_result = self._call_s3(\"put_object\", **kw)\n            else:\n                raise RuntimeError\n        else:\n            logger.debug(\"Complete multi-part upload for %s \" % self)\n            part_info = {\"Parts\": self.parts}\n            write_result = self._call_s3(\n                \"complete_multipart_upload\",\n                Bucket=self.bucket,\n                Key=self.key,\n                UploadId=self.mpu[\"UploadId\"],\n                MultipartUpload=part_info,\n            )\n\n        if self.fs.version_aware:\n            self.version_id = write_result.get(\"VersionId\")\n        # complex cache invalidation, since file's appearance can cause several\n        # directories\n        self.buffer = None\n        parts = self.path.split(\"/\")\n        path = parts[0]\n        for p in parts[1:]:\n            if path in self.fs.dircache and not [\n                True for f in self.fs.dircache[path] if f[\"name\"] == path + \"/\" + p\n            ]:\n                self.fs.invalidate_cache(path)\n            path = path + \"/\" + p\n\n    def discard(self):\n        self._abort_mpu()\n        self.buffer = None  # file becomes unusable\n\n    def _abort_mpu(self):\n        if self.mpu:\n            self._call_s3(\n                \"abort_multipart_upload\",\n                Bucket=self.bucket,\n                Key=self.key,\n                UploadId=self.mpu[\"UploadId\"],\n            )\n            self.mpu = None\n\n\nclass S3AsyncStreamedFile(AbstractAsyncStreamedFile):\n    def __init__(self, fs, path, mode):\n        self.fs = fs\n        self.path = path\n        self.mode = mode\n        self.r = None\n        self.loc = 0\n        self.size = None\n\n    async def read(self, length=-1):\n        if self.r is None:\n            bucket, key, gen = self.fs.split_path(self.path)\n            r = await self.fs._call_s3(\"get_object\", Bucket=bucket, Key=key)\n            self.size = int(r[\"ResponseMetadata\"][\"HTTPHeaders\"][\"content-length\"])\n            self.r = r[\"Body\"]\n        out = await self.r.read(length)\n        self.loc += len(out)\n        return out\n\n\ndef _fetch_range(fs, bucket, key, version_id, start, end, req_kw=None):\n    if req_kw is None:\n        req_kw = {}\n    if start == end:\n        logger.debug(\n            \"skip fetch for negative range - bucket=%s,key=%s,start=%d,end=%d\",\n            bucket,\n            key,\n            start,\n            end,\n        )\n        return b\"\"\n    logger.debug(\"Fetch: %s/%s, %s-%s\", bucket, key, start, end)\n    return sync(fs.loop, _inner_fetch, fs, bucket, key, version_id, start, end, req_kw)\n\n\nasync def _inner_fetch(fs, bucket, key, version_id, start, end, req_kw=None):\n    async def _call_and_read():\n        resp = await fs._call_s3(\n            \"get_object\",\n            Bucket=bucket,\n            Key=key,\n            Range=\"bytes=%i-%i\" % (start, end - 1),\n            **version_id_kw(version_id),\n            **req_kw,\n        )\n        try:\n            return await resp[\"Body\"].read()\n        finally:\n            resp[\"Body\"].close()\n\n    return await _error_wrapper(_call_and_read, retries=fs.retries)\n", "s3fs/utils.py": "import errno\nimport logging\nfrom contextlib import contextmanager, AsyncExitStack\nfrom botocore.exceptions import ClientError\n\n\nlogger = logging.getLogger(\"s3fs\")\n\n\n@contextmanager\ndef ignoring(*exceptions):\n    try:\n        yield\n    except exceptions:\n        pass\n\n\nclass S3BucketRegionCache:\n    # See https://github.com/aio-libs/aiobotocore/issues/866\n    # for details.\n\n    def __init__(self, session, **client_kwargs):\n        self._session = session\n        self._stack = AsyncExitStack()\n        self._client = None\n        self._client_kwargs = client_kwargs\n        self._buckets = {}\n        self._regions = {}\n\n    async def get_bucket_client(self, bucket_name=None):\n        if bucket_name in self._buckets:\n            return self._buckets[bucket_name]\n\n        general_client = await self.get_client()\n        if bucket_name is None:\n            return general_client\n\n        try:\n            response = await general_client.head_bucket(Bucket=bucket_name)\n        except ClientError as e:\n            region = (\n                e.response[\"ResponseMetadata\"]\n                .get(\"HTTPHeaders\", {})\n                .get(\"x-amz-bucket-region\")\n            )\n            if not region:\n                logger.debug(\n                    \"RC: HEAD_BUCKET call for %r has failed, returning the general client\",\n                    bucket_name,\n                )\n                return general_client\n        else:\n            region = response[\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amz-bucket-region\"]\n\n        if region not in self._regions:\n            logger.debug(\n                \"RC: Creating a new regional client for %r on the region %r\",\n                bucket_name,\n                region,\n            )\n            self._regions[region] = await self._stack.enter_async_context(\n                self._session.create_client(\n                    \"s3\", region_name=region, **self._client_kwargs\n                )\n            )\n\n        client = self._buckets[bucket_name] = self._regions[region]\n        return client\n\n    async def get_client(self):\n        if not self._client:\n            self._client = await self._stack.enter_async_context(\n                self._session.create_client(\"s3\", **self._client_kwargs)\n            )\n        return self._client\n\n    async def clear(self):\n        logger.debug(\"RC: discarding all clients\")\n        self._buckets.clear()\n        self._regions.clear()\n        self._client = None\n        await self._stack.aclose()\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc_args):\n        await self.clear()\n\n\nclass FileExpired(IOError):\n    \"\"\"\n    Is raised, when the file content has been changed from a different process after\n    opening the file. Reading the file would lead to invalid or inconsistent output.\n    This can also be triggered by outdated file-information inside the directory cache.\n    In this case ``S3FileSystem.invalidate_cache`` can be used to force an update of\n    the file-information when opening the file.\n    \"\"\"\n\n    def __init__(self, filename: str, e_tag: str):\n        super().__init__(\n            errno.EBUSY,\n            \"The remote file corresponding to filename %s and Etag %s no longer exists.\"\n            % (filename, e_tag),\n        )\n\n\ndef title_case(string):\n    \"\"\"\n    TitleCases a given string.\n\n    Parameters\n    ----------\n    string : underscore separated string\n    \"\"\"\n    return \"\".join(x.capitalize() for x in string.split(\"_\"))\n\n\nclass ParamKwargsHelper(object):\n    \"\"\"\n    Utility class to help extract the subset of keys that an s3 method is\n    actually using\n\n    Parameters\n    ----------\n    s3 : boto S3FileSystem\n    \"\"\"\n\n    _kwarg_cache = {}\n\n    def __init__(self, s3):\n        self.s3 = s3\n\n    def _get_valid_keys(self, model_name):\n        if model_name not in self._kwarg_cache:\n            model = self.s3.meta.service_model.operation_model(model_name)\n            valid_keys = (\n                set(model.input_shape.members.keys())\n                if model.input_shape is not None\n                else set()\n            )\n            self._kwarg_cache[model_name] = valid_keys\n        return self._kwarg_cache[model_name]\n\n    def filter_dict(self, method_name, d):\n        model_name = title_case(method_name)\n        valid_keys = self._get_valid_keys(model_name)\n        if isinstance(d, SSEParams):\n            d = d.to_kwargs()\n        return {k: v for k, v in d.items() if k in valid_keys}\n\n\nclass SSEParams(object):\n    def __init__(\n        self,\n        server_side_encryption=None,\n        sse_customer_algorithm=None,\n        sse_customer_key=None,\n        sse_kms_key_id=None,\n    ):\n        self.ServerSideEncryption = server_side_encryption\n        self.SSECustomerAlgorithm = sse_customer_algorithm\n        self.SSECustomerKey = sse_customer_key\n        self.SSEKMSKeyId = sse_kms_key_id\n\n    def to_kwargs(self):\n        return {k: v for k, v in self.__dict__.items() if v is not None}\n\n\ndef _get_brange(size, block):\n    \"\"\"\n    Chunk up a file into zero-based byte ranges\n\n    Parameters\n    ----------\n    size : file size\n    block : block size\n    \"\"\"\n    for offset in range(0, size, block):\n        yield offset, min(offset + block - 1, size - 1)\n", "s3fs/mapping.py": "from .core import S3FileSystem\n\n\ndef S3Map(root, s3, check=False, create=False):\n    \"\"\"Mirror previous class, not implemented in fsspec\"\"\"\n    s3 = s3 or S3FileSystem.current()\n    return s3.get_mapper(root, check=check, create=create)\n", "s3fs/errors.py": "\"\"\"S3 error codes adapted into more natural Python ones.\n\nAdapted from: https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html\n\"\"\"\n\nimport errno\nimport functools\n\n\n# Fallback values since some systems might not have these.\nENAMETOOLONG = getattr(errno, \"ENAMETOOLONG\", errno.EINVAL)\nENOTEMPTY = getattr(errno, \"ENOTEMPTY\", errno.EINVAL)\nEMSGSIZE = getattr(errno, \"EMSGSIZE\", errno.EINVAL)\nEREMOTEIO = getattr(errno, \"EREMOTEIO\", errno.EIO)\nEREMCHG = getattr(errno, \"EREMCHG\", errno.ENOENT)\n\n\nERROR_CODE_TO_EXCEPTION = {\n    \"AccessDenied\": PermissionError,\n    \"AccountProblem\": PermissionError,\n    \"AllAccessDisabled\": PermissionError,\n    \"AmbiguousGrantByEmailAddress\": functools.partial(IOError, errno.EINVAL),\n    \"AuthorizationHeaderMalformed\": functools.partial(IOError, errno.EINVAL),\n    \"BadDigest\": functools.partial(IOError, errno.EINVAL),\n    \"BucketAlreadyExists\": FileExistsError,\n    \"BucketAlreadyOwnedByYou\": FileExistsError,\n    \"BucketNotEmpty\": functools.partial(IOError, ENOTEMPTY),\n    \"CredentialsNotSupported\": functools.partial(IOError, errno.EINVAL),\n    \"CrossLocationLoggingProhibited\": PermissionError,\n    \"EntityTooSmall\": functools.partial(IOError, errno.EINVAL),\n    \"EntityTooLarge\": functools.partial(IOError, EMSGSIZE),\n    \"ExpiredToken\": PermissionError,\n    \"IllegalLocationConstraintException\": PermissionError,\n    \"IllegalVersioningConfigurationException\": functools.partial(IOError, errno.EINVAL),\n    \"IncompleteBody\": functools.partial(IOError, errno.EINVAL),\n    \"IncorrectNumberOfFilesInPostRequest\": functools.partial(IOError, errno.EINVAL),\n    \"InlineDataTooLarge\": functools.partial(IOError, EMSGSIZE),\n    \"InternalError\": functools.partial(IOError, EREMOTEIO),\n    \"InvalidAccessKeyId\": PermissionError,\n    \"InvalidAddressingHeader\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidArgument\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidBucketName\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidBucketState\": functools.partial(IOError, errno.EPERM),\n    \"InvalidDigest\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidEncryptionAlgorithmError\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidLocationConstraint\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidObjectState\": PermissionError,\n    \"InvalidPart\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidPartOrder\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidPayer\": PermissionError,\n    \"InvalidPolicyDocument\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidRange\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidRequest\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidSecurity\": PermissionError,\n    \"InvalidSOAPRequest\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidStorageClass\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidTargetBucketForLogging\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidToken\": functools.partial(IOError, errno.EINVAL),\n    \"InvalidURI\": functools.partial(IOError, errno.EINVAL),\n    \"KeyTooLongError\": functools.partial(IOError, ENAMETOOLONG),\n    \"MalformedACLError\": functools.partial(IOError, errno.EINVAL),\n    \"MalformedPOSTRequest\": functools.partial(IOError, errno.EINVAL),\n    \"MalformedXML\": functools.partial(IOError, errno.EINVAL),\n    \"MaxMessageLengthExceeded\": functools.partial(IOError, EMSGSIZE),\n    \"MaxPostPreDataLengthExceededError\": functools.partial(IOError, EMSGSIZE),\n    \"MetadataTooLarge\": functools.partial(IOError, EMSGSIZE),\n    \"MethodNotAllowed\": functools.partial(IOError, errno.EPERM),\n    \"MissingAttachment\": functools.partial(IOError, errno.EINVAL),\n    \"MissingContentLength\": functools.partial(IOError, errno.EINVAL),\n    \"MissingRequestBodyError\": functools.partial(IOError, errno.EINVAL),\n    \"MissingSecurityElement\": functools.partial(IOError, errno.EINVAL),\n    \"MissingSecurityHeader\": functools.partial(IOError, errno.EINVAL),\n    \"NoLoggingStatusForKey\": functools.partial(IOError, errno.EINVAL),\n    \"NoSuchBucket\": FileNotFoundError,\n    \"NoSuchBucketPolicy\": FileNotFoundError,\n    \"NoSuchKey\": FileNotFoundError,\n    \"NoSuchLifecycleConfiguration\": FileNotFoundError,\n    \"NoSuchUpload\": FileNotFoundError,\n    \"NoSuchVersion\": FileNotFoundError,\n    \"NotImplemented\": functools.partial(IOError, errno.ENOSYS),\n    \"NotSignedUp\": PermissionError,\n    \"OperationAborted\": functools.partial(IOError, errno.EBUSY),\n    \"PermanentRedirect\": functools.partial(IOError, EREMCHG),\n    \"PreconditionFailed\": functools.partial(IOError, errno.EINVAL),\n    \"Redirect\": functools.partial(IOError, EREMCHG),\n    \"RestoreAlreadyInProgress\": functools.partial(IOError, errno.EBUSY),\n    \"RequestIsNotMultiPartContent\": functools.partial(IOError, errno.EINVAL),\n    \"RequestTimeout\": TimeoutError,\n    \"RequestTimeTooSkewed\": PermissionError,\n    \"RequestTorrentOfBucketError\": functools.partial(IOError, errno.EPERM),\n    \"SignatureDoesNotMatch\": PermissionError,\n    \"ServiceUnavailable\": functools.partial(IOError, errno.EBUSY),\n    \"SlowDown\": functools.partial(IOError, errno.EBUSY),\n    \"TemporaryRedirect\": functools.partial(IOError, EREMCHG),\n    \"TokenRefreshRequired\": functools.partial(IOError, errno.EINVAL),\n    \"TooManyBuckets\": functools.partial(IOError, errno.EINVAL),\n    \"UnexpectedContent\": functools.partial(IOError, errno.EINVAL),\n    \"UnresolvableGrantByEmailAddress\": functools.partial(IOError, errno.EINVAL),\n    \"UserKeyMustBeSpecified\": functools.partial(IOError, errno.EINVAL),\n    \"301\": functools.partial(IOError, EREMCHG),  # PermanentRedirect\n    \"307\": functools.partial(IOError, EREMCHG),  # Redirect\n    \"400\": functools.partial(IOError, errno.EINVAL),\n    \"403\": PermissionError,\n    \"404\": FileNotFoundError,\n    \"405\": functools.partial(IOError, errno.EPERM),\n    \"409\": functools.partial(IOError, errno.EBUSY),\n    \"412\": functools.partial(IOError, errno.EINVAL),  # PreconditionFailed\n    \"416\": functools.partial(IOError, errno.EINVAL),  # InvalidRange\n    \"500\": functools.partial(IOError, EREMOTEIO),  # InternalError\n    \"501\": functools.partial(IOError, errno.ENOSYS),  # NotImplemented\n    \"503\": functools.partial(IOError, errno.EBUSY),  # SlowDown\n}\n\n\ndef translate_boto_error(error, message=None, set_cause=True, *args, **kwargs):\n    \"\"\"Convert a ClientError exception into a Python one.\n\n    Parameters\n    ----------\n\n    error : botocore.exceptions.ClientError\n        The exception returned by the boto API.\n    message : str\n        An error message to use for the returned exception. If not given, the\n        error message returned by the server is used instead.\n    set_cause : bool\n        Whether to set the __cause__ attribute to the previous exception if the\n        exception is translated.\n    *args, **kwargs :\n        Additional arguments to pass to the exception constructor, after the\n        error message. Useful for passing the filename arguments to ``IOError``.\n\n    Returns\n    -------\n\n    An instantiated exception ready to be thrown. If the error code isn't\n    recognized, an IOError with the original error message is returned.\n    \"\"\"\n    error_response = getattr(error, \"response\", None)\n    if error_response is None:\n        # non-http error, or response is None:\n        return error\n    code = error_response[\"Error\"].get(\"Code\")\n    constructor = ERROR_CODE_TO_EXCEPTION.get(code)\n    if constructor:\n        if not message:\n            message = error_response[\"Error\"].get(\"Message\", str(error))\n        custom_exc = constructor(message, *args, **kwargs)\n    else:\n        # No match found, wrap this in an IOError with the appropriate message.\n        custom_exc = IOError(errno.EIO, message or str(error), *args)\n\n    if set_cause:\n        custom_exc.__cause__ = error\n    return custom_exc\n", "s3fs/__init__.py": "from .core import S3FileSystem, S3File\nfrom .mapping import S3Map\n\nfrom ._version import get_versions\n\n__version__ = get_versions()[\"version\"]\ndel get_versions\n", "s3fs/_version.py": "# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain.\n# Generated by versioneer-0.29\n# https://github.com/python-versioneer/python-versioneer\n\n\"\"\"Git implementation of _version.py.\"\"\"\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\nimport functools\n\n\ndef get_keywords() -> Dict[str, str]:\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"$Format:%d$\"\n    git_full = \"$Format:%H$\"\n    git_date = \"$Format:%ci$\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n    VCS: str\n    style: str\n    tag_prefix: str\n    parentdir_prefix: str\n    versionfile_source: str\n    verbose: bool\n\n\ndef get_config() -> VersioneerConfig:\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"s3fs/_version.py\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY: Dict[str, str] = {}\nHANDLERS: Dict[str, Dict[str, Callable]] = {}\n\n\ndef register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f: Callable) -> Callable:\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(\n    commands: List[str],\n    args: List[str],\n    cwd: Optional[str] = None,\n    verbose: bool = False,\n    hide_stderr: bool = False,\n    env: Optional[Dict[str, str]] = None,\n) -> Tuple[Optional[str], Optional[int]]:\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs: Dict[str, Any] = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen(\n                [command] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n                **popen_kwargs,\n            )\n            break\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %s\" % (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n\n\ndef versions_from_parentdir(\n    parentdir_prefix: str,\n    root: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                \"version\": dirname[len(parentdir_prefix) :],\n                \"full-revisionid\": None,\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": None,\n            }\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            \"Tried directories %s but none started with prefix %s\"\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs: str) -> Dict[str, str]:\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords: Dict[str, str] = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(\n    keywords: Dict[str, str],\n    tag_prefix: str,\n    verbose: bool,\n) -> Dict[str, Any]:\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r\"\\d\", r)}\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r\"\\d\", r):\n                continue\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\n                \"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": date,\n            }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False,\n        \"error\": \"no suitable tags\",\n        \"date\": None,\n    }\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(\n    tag_prefix: str, root: str, verbose: bool, runner: Callable = run_command\n) -> Dict[str, Any]:\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root, hide_stderr=not verbose)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(\n        GITS,\n        [\n            \"describe\",\n            \"--tags\",\n            \"--dirty\",\n            \"--always\",\n            \"--long\",\n            \"--match\",\n            f\"{tag_prefix}[[:digit:]]*\",\n        ],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces: Dict[str, Any] = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\"^(.+)-(\\d+)-g([0-9a-f]+)$\", git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = \"unable to parse git-describe output: '%s'\" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = \"tag '%s' doesn't start with prefix '%s'\" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n        pieces[\"distance\"] = len(out.split())  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces: Dict[str, Any]) -> str:\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces: Dict[str, Any]) -> str:\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n\n\ndef render_pep440_pre(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_post_branch(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_old(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces: Dict[str, Any]) -> str:\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces: Dict[str, Any], style: str) -> Dict[str, Any]:\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\n            \"version\": \"unknown\",\n            \"full-revisionid\": pieces.get(\"long\"),\n            \"dirty\": None,\n            \"error\": pieces[\"error\"],\n            \"date\": None,\n        }\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\n        \"version\": rendered,\n        \"full-revisionid\": pieces[\"long\"],\n        \"dirty\": pieces[\"dirty\"],\n        \"error\": None,\n        \"date\": pieces.get(\"date\"),\n    }\n\n\ndef get_versions() -> Dict[str, Any]:\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for _ in cfg.versionfile_source.split(\"/\"):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\n            \"version\": \"0+unknown\",\n            \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to find root of source tree\",\n            \"date\": None,\n        }\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\",\n        \"date\": None,\n    }\n", "s3fs/tests/test_s3fs.py": "# -*- coding: utf-8 -*-\nimport asyncio\nimport errno\nimport datetime\nfrom contextlib import contextmanager\nimport json\nfrom concurrent.futures import ProcessPoolExecutor\nimport io\nimport os\nimport random\nimport requests\nimport time\nimport sys\nimport pytest\nimport moto\nfrom moto.moto_server.threaded_moto_server import ThreadedMotoServer\nfrom itertools import chain\nimport fsspec.core\nfrom dateutil.tz import tzutc\n\nimport s3fs.core\nfrom s3fs.core import S3FileSystem\nfrom s3fs.utils import ignoring, SSEParams\nfrom botocore.exceptions import NoCredentialsError\nfrom fsspec.asyn import sync\nfrom fsspec.callbacks import Callback\nfrom packaging import version\n\ntest_bucket_name = \"test\"\nsecure_bucket_name = \"test-secure\"\nversioned_bucket_name = \"test-versioned\"\nfiles = {\n    \"test/accounts.1.json\": (\n        b'{\"amount\": 100, \"name\": \"Alice\"}\\n'\n        b'{\"amount\": 200, \"name\": \"Bob\"}\\n'\n        b'{\"amount\": 300, \"name\": \"Charlie\"}\\n'\n        b'{\"amount\": 400, \"name\": \"Dennis\"}\\n'\n    ),\n    \"test/accounts.2.json\": (\n        b'{\"amount\": 500, \"name\": \"Alice\"}\\n'\n        b'{\"amount\": 600, \"name\": \"Bob\"}\\n'\n        b'{\"amount\": 700, \"name\": \"Charlie\"}\\n'\n        b'{\"amount\": 800, \"name\": \"Dennis\"}\\n'\n    ),\n}\n\ncsv_files = {\n    \"2014-01-01.csv\": (\n        b\"name,amount,id\\n\" b\"Alice,100,1\\n\" b\"Bob,200,2\\n\" b\"Charlie,300,3\\n\"\n    ),\n    \"2014-01-02.csv\": (b\"name,amount,id\\n\"),\n    \"2014-01-03.csv\": (\n        b\"name,amount,id\\n\" b\"Dennis,400,4\\n\" b\"Edith,500,5\\n\" b\"Frank,600,6\\n\"\n    ),\n}\ntext_files = {\n    \"nested/file1\": b\"hello\\n\",\n    \"nested/file2\": b\"world\",\n    \"nested/nested2/file1\": b\"hello\\n\",\n    \"nested/nested2/file2\": b\"world\",\n}\nglob_files = {\"file.dat\": b\"\", \"filexdat\": b\"\"}\na = test_bucket_name + \"/tmp/test/a\"\nb = test_bucket_name + \"/tmp/test/b\"\nc = test_bucket_name + \"/tmp/test/c\"\nd = test_bucket_name + \"/tmp/test/d\"\nport = 5555\nendpoint_uri = \"http://127.0.0.1:%s/\" % port\n\n\n@pytest.fixture(scope=\"module\")\ndef s3_base():\n    # writable local S3 system\n\n    # This fixture is module-scoped, meaning that we can re-use the MotoServer across all tests\n    server = ThreadedMotoServer(ip_address=\"127.0.0.1\", port=port)\n    server.start()\n    if \"AWS_SECRET_ACCESS_KEY\" not in os.environ:\n        os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"foo\"\n    if \"AWS_ACCESS_KEY_ID\" not in os.environ:\n        os.environ[\"AWS_ACCESS_KEY_ID\"] = \"foo\"\n\n    print(\"server up\")\n    yield\n    print(\"moto done\")\n    server.stop()\n\n\n@pytest.fixture(autouse=True)\ndef reset_s3_fixture():\n    # We reuse the MotoServer for all tests\n    # But we do want a clean state for every test\n    requests.post(f\"{endpoint_uri}/moto-api/reset\")\n\n\ndef get_boto3_client():\n    from botocore.session import Session\n\n    # NB: we use the sync botocore client for setup\n    session = Session()\n    return session.create_client(\"s3\", endpoint_url=endpoint_uri)\n\n\n@pytest.fixture()\ndef s3(s3_base):\n    client = get_boto3_client()\n    client.create_bucket(Bucket=test_bucket_name, ACL=\"public-read\")\n\n    client.create_bucket(Bucket=versioned_bucket_name, ACL=\"public-read\")\n    client.put_bucket_versioning(\n        Bucket=versioned_bucket_name, VersioningConfiguration={\"Status\": \"Enabled\"}\n    )\n\n    # initialize secure bucket\n    client.create_bucket(Bucket=secure_bucket_name, ACL=\"public-read\")\n    policy = json.dumps(\n        {\n            \"Version\": \"2012-10-17\",\n            \"Id\": \"PutObjPolicy\",\n            \"Statement\": [\n                {\n                    \"Sid\": \"DenyUnEncryptedObjectUploads\",\n                    \"Effect\": \"Deny\",\n                    \"Principal\": \"*\",\n                    \"Action\": \"s3:PutObject\",\n                    \"Resource\": \"arn:aws:s3:::{bucket_name}/*\".format(\n                        bucket_name=secure_bucket_name\n                    ),\n                    \"Condition\": {\n                        \"StringNotEquals\": {\n                            \"s3:x-amz-server-side-encryption\": \"aws:kms\"\n                        }\n                    },\n                }\n            ],\n        }\n    )\n    client.put_bucket_policy(Bucket=secure_bucket_name, Policy=policy)\n    for flist in [files, csv_files, text_files, glob_files]:\n        for f, data in flist.items():\n            client.put_object(Bucket=test_bucket_name, Key=f, Body=data)\n\n    S3FileSystem.clear_instance_cache()\n    s3 = S3FileSystem(anon=False, client_kwargs={\"endpoint_url\": endpoint_uri})\n    s3.invalidate_cache()\n    yield s3\n\n\n@contextmanager\ndef expect_errno(expected_errno):\n    \"\"\"Expect an OSError and validate its errno code.\"\"\"\n    with pytest.raises(OSError) as error:\n        yield\n    assert error.value.errno == expected_errno, \"OSError has wrong error code.\"\n\n\ndef test_simple(s3):\n    data = b\"a\" * (10 * 2**20)\n\n    with s3.open(a, \"wb\") as f:\n        f.write(data)\n\n    with s3.open(a, \"rb\") as f:\n        out = f.read(len(data))\n        assert len(data) == len(out)\n        assert out == data\n\n\ndef test_with_size(s3):\n    data = b\"a\" * (10 * 2**20)\n\n    with s3.open(a, \"wb\") as f:\n        f.write(data)\n\n    with s3.open(a, \"rb\", size=100) as f:\n        assert f.size == 100\n        out = f.read()\n        assert len(out) == 100\n\n\n@pytest.mark.parametrize(\"default_cache_type\", [\"none\", \"bytes\", \"mmap\", \"readahead\"])\ndef test_default_cache_type(s3, default_cache_type):\n    data = b\"a\" * (10 * 2**20)\n    s3 = S3FileSystem(\n        anon=False,\n        default_cache_type=default_cache_type,\n        client_kwargs={\"endpoint_url\": endpoint_uri},\n    )\n\n    with s3.open(a, \"wb\") as f:\n        f.write(data)\n\n    with s3.open(a, \"rb\") as f:\n        assert isinstance(f.cache, fsspec.core.caches[default_cache_type])\n        out = f.read(len(data))\n        assert len(data) == len(out)\n        assert out == data\n\n\ndef test_ssl_off():\n    s3 = S3FileSystem(use_ssl=False, client_kwargs={\"endpoint_url\": endpoint_uri})\n    assert s3.s3.meta.endpoint_url.startswith(\"http://\")\n\n\ndef test_client_kwargs():\n    s3 = S3FileSystem(client_kwargs={\"endpoint_url\": \"http://foo\"})\n    assert s3.s3.meta.endpoint_url.startswith(\"http://foo\")\n\n\ndef test_config_kwargs():\n    s3 = S3FileSystem(\n        config_kwargs={\"signature_version\": \"s3v4\"},\n        client_kwargs={\"endpoint_url\": endpoint_uri},\n    )\n    assert s3.connect().meta.config.signature_version == \"s3v4\"\n\n\ndef test_config_kwargs_class_attributes_default():\n    s3 = S3FileSystem(client_kwargs={\"endpoint_url\": endpoint_uri})\n    assert s3.connect().meta.config.connect_timeout == 5\n    assert s3.connect().meta.config.read_timeout == 15\n\n\ndef test_config_kwargs_class_attributes_override():\n    s3 = S3FileSystem(\n        config_kwargs={\n            \"connect_timeout\": 60,\n            \"read_timeout\": 120,\n        },\n        client_kwargs={\"endpoint_url\": endpoint_uri},\n    )\n    assert s3.connect().meta.config.connect_timeout == 60\n    assert s3.connect().meta.config.read_timeout == 120\n\n\ndef test_user_session_is_preserved():\n    from aiobotocore.session import get_session\n\n    session = get_session()\n    s3 = S3FileSystem(session=session)\n    s3.connect()\n    assert s3.session == session\n\n\ndef test_idempotent_connect(s3):\n    first = s3.s3\n    assert s3.connect(refresh=True) is not first\n\n\ndef test_multiple_objects(s3):\n    s3.connect()\n    s3.ls(\"test\")\n    s32 = S3FileSystem(anon=False, client_kwargs={\"endpoint_url\": endpoint_uri})\n    assert s32.session\n    assert s3.ls(\"test\") == s32.ls(\"test\")\n\n\ndef test_info(s3):\n    s3.touch(a)\n    s3.touch(b)\n    info = s3.info(a)\n    linfo = s3.ls(a, detail=True)[0]\n    assert abs(info.pop(\"LastModified\") - linfo.pop(\"LastModified\")).seconds < 1\n    info.pop(\"VersionId\")\n    info.pop(\"ContentType\")\n    linfo.pop(\"Key\")\n    linfo.pop(\"Size\")\n    assert info == linfo\n    parent = a.rsplit(\"/\", 1)[0]\n    s3.invalidate_cache()  # remove full path from the cache\n    s3.ls(parent)  # fill the cache with parent dir\n    assert s3.info(a) == s3.dircache[parent][0]  # correct value\n    assert id(s3.info(a)) == id(s3.dircache[parent][0])  # is object from cache\n    assert id(s3.info(f\"/{a}\")) == id(s3.dircache[parent][0])  # is object from cache\n\n    new_parent = test_bucket_name + \"/foo\"\n    s3.mkdir(new_parent)\n    with pytest.raises(FileNotFoundError):\n        s3.info(new_parent)\n    with pytest.raises(FileNotFoundError):\n        s3.ls(new_parent)\n    with pytest.raises(FileNotFoundError):\n        s3.info(new_parent)\n\n\ndef test_info_cached(s3):\n    path = test_bucket_name + \"/tmp/\"\n    fqpath = \"s3://\" + path\n    s3.touch(path + \"test\")\n    info = s3.info(fqpath)\n    assert info == s3.info(fqpath)\n    assert info == s3.info(path)\n\n\ndef test_checksum(s3):\n    bucket = test_bucket_name\n    d = \"checksum\"\n    prefix = d + \"/e\"\n    o1 = prefix + \"1\"\n    o2 = prefix + \"2\"\n    path1 = bucket + \"/\" + o1\n    path2 = bucket + \"/\" + o2\n\n    client = s3.s3\n\n    # init client and files\n    sync(s3.loop, client.put_object, Bucket=bucket, Key=o1, Body=\"\")\n    sync(s3.loop, client.put_object, Bucket=bucket, Key=o2, Body=\"\")\n\n    # change one file, using cache\n    sync(s3.loop, client.put_object, Bucket=bucket, Key=o1, Body=\"foo\")\n    checksum = s3.checksum(path1)\n    s3.ls(path1)  # force caching\n    sync(s3.loop, client.put_object, Bucket=bucket, Key=o1, Body=\"bar\")\n    # refresh == False => checksum doesn't change\n    assert checksum == s3.checksum(path1)\n\n    # change one file, without cache\n    sync(s3.loop, client.put_object, Bucket=bucket, Key=o1, Body=\"foo\")\n    checksum = s3.checksum(path1, refresh=True)\n    s3.ls(path1)  # force caching\n    sync(s3.loop, client.put_object, Bucket=bucket, Key=o1, Body=\"bar\")\n    # refresh == True => checksum changes\n    assert checksum != s3.checksum(path1, refresh=True)\n\n    # Test for nonexistent file\n    sync(s3.loop, client.put_object, Bucket=bucket, Key=o1, Body=\"bar\")\n    s3.ls(path1)  # force caching\n    sync(s3.loop, client.delete_object, Bucket=bucket, Key=o1)\n    with pytest.raises(FileNotFoundError):\n        s3.checksum(o1, refresh=True)\n\n    # Test multipart upload\n    upload_id = sync(\n        s3.loop,\n        client.create_multipart_upload,\n        Bucket=bucket,\n        Key=o1,\n    )[\"UploadId\"]\n    etag1 = sync(\n        s3.loop,\n        client.upload_part,\n        Bucket=bucket,\n        Key=o1,\n        UploadId=upload_id,\n        PartNumber=1,\n        Body=\"0\" * (5 * 1024 * 1024),\n    )[\"ETag\"]\n    etag2 = sync(\n        s3.loop,\n        client.upload_part,\n        Bucket=bucket,\n        Key=o1,\n        UploadId=upload_id,\n        PartNumber=2,\n        Body=\"0\",\n    )[\"ETag\"]\n    sync(\n        s3.loop,\n        client.complete_multipart_upload,\n        Bucket=bucket,\n        Key=o1,\n        UploadId=upload_id,\n        MultipartUpload={\n            \"Parts\": [\n                {\"PartNumber\": 1, \"ETag\": etag1},\n                {\"PartNumber\": 2, \"ETag\": etag2},\n            ]\n        },\n    )\n    s3.checksum(path1, refresh=True)\n\n\ndef test_multi_checksum(s3):\n    # Moto accepts the request to add checksum, and accepts the checksum mode,\n    # but doesn't actually return the checksum\n    # So, this is mostly a stub test\n    file_key = \"checksum\"\n    path = test_bucket_name + \"/\" + file_key\n    s3 = S3FileSystem(\n        anon=False,\n        client_kwargs={\"endpoint_url\": endpoint_uri},\n        s3_additional_kwargs={\"ChecksumAlgorithm\": \"SHA256\"},\n    )\n    with s3.open(\n        path,\n        \"wb\",\n        blocksize=5 * 2**20,\n    ) as f:\n        f.write(b\"0\" * (5 * 2**20 + 1))  # starts multipart and puts first part\n        f.write(b\"data\")  # any extra data\n    assert s3.cat(path) == b\"0\" * (5 * 2**20 + 1) + b\"data\"\n    FileHead = sync(\n        s3.loop,\n        s3.s3.head_object,\n        Bucket=test_bucket_name,\n        Key=file_key,\n        ChecksumMode=\"ENABLED\",\n    )\n    # assert \"ChecksumSHA256\" in FileHead\n\n\ntest_xattr_sample_metadata = {\"testxattr\": \"1\"}\n\n\ndef test_xattr(s3):\n    bucket, key = (test_bucket_name, \"tmp/test/xattr\")\n    filename = bucket + \"/\" + key\n    body = b\"aaaa\"\n    public_read_acl = {\n        \"Permission\": \"READ\",\n        \"Grantee\": {\n            \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\",\n            \"Type\": \"Group\",\n        },\n    }\n\n    resp = sync(\n        s3.loop,\n        s3.s3.put_object,\n        Bucket=bucket,\n        Key=key,\n        ACL=\"public-read\",\n        Metadata=test_xattr_sample_metadata,\n        Body=body,\n    )\n\n    # save etag for later\n    etag = s3.info(filename)[\"ETag\"]\n    assert (\n        public_read_acl\n        in sync(s3.loop, s3.s3.get_object_acl, Bucket=bucket, Key=key)[\"Grants\"]\n    )\n\n    assert s3.getxattr(filename, \"testxattr\") == test_xattr_sample_metadata[\"testxattr\"]\n    assert s3.metadata(filename) == {\"testxattr\": \"1\"}  # note _ became -\n\n    s3file = s3.open(filename)\n    assert s3file.getxattr(\"testxattr\") == test_xattr_sample_metadata[\"testxattr\"]\n    assert s3file.metadata() == {\"testxattr\": \"1\"}  # note _ became -\n\n    s3file.setxattr(testxattr=\"2\")\n    assert s3file.getxattr(\"testxattr\") == \"2\"\n    s3file.setxattr(**{\"testxattr\": None})\n    assert s3file.metadata() == {}\n    assert s3.cat(filename) == body\n\n    # check that ACL and ETag are preserved after updating metadata\n    assert (\n        public_read_acl\n        in sync(s3.loop, s3.s3.get_object_acl, Bucket=bucket, Key=key)[\"Grants\"]\n    )\n    assert s3.info(filename)[\"ETag\"] == etag\n\n\ndef test_xattr_setxattr_in_write_mode(s3):\n    s3file = s3.open(a, \"wb\")\n    with pytest.raises(NotImplementedError):\n        s3file.setxattr(test_xattr=\"1\")\n\n\n@pytest.mark.xfail()\ndef test_delegate(s3):\n    out = s3.get_delegated_s3pars()\n    assert out\n    assert out[\"token\"]\n    s32 = S3FileSystem(client_kwargs={\"endpoint_url\": endpoint_uri}, **out)\n    assert not s32.anon\n    assert out == s32.get_delegated_s3pars()\n\n\ndef test_not_delegate():\n    s3 = S3FileSystem(anon=True, client_kwargs={\"endpoint_url\": endpoint_uri})\n    out = s3.get_delegated_s3pars()\n    assert out == {\"anon\": True}\n    s3 = S3FileSystem(\n        anon=False, client_kwargs={\"endpoint_url\": endpoint_uri}\n    )  # auto credentials\n    out = s3.get_delegated_s3pars()\n    assert out == {\"anon\": False}\n\n\ndef test_ls(s3):\n    assert set(s3.ls(\"\", detail=False)) == {\n        test_bucket_name,\n        secure_bucket_name,\n        versioned_bucket_name,\n    }\n    with pytest.raises(FileNotFoundError):\n        s3.ls(\"nonexistent\")\n    fn = test_bucket_name + \"/test/accounts.1.json\"\n    assert fn in s3.ls(test_bucket_name + \"/test\", detail=False)\n\n\ndef test_pickle(s3):\n    import pickle\n\n    s32 = pickle.loads(pickle.dumps(s3))\n    assert s3.ls(\"test\") == s32.ls(\"test\")\n    s33 = pickle.loads(pickle.dumps(s32))\n    assert s3.ls(\"test\") == s33.ls(\"test\")\n\n\ndef test_ls_touch(s3):\n    assert not s3.exists(test_bucket_name + \"/tmp/test\")\n    s3.touch(a)\n    s3.touch(b)\n    L = s3.ls(test_bucket_name + \"/tmp/test\", True)\n    assert {d[\"Key\"] for d in L} == {a, b}\n    L = s3.ls(test_bucket_name + \"/tmp/test\", False)\n    assert set(L) == {a, b}\n\n\n@pytest.mark.parametrize(\"version_aware\", [True, False])\ndef test_exists_versioned(s3, version_aware):\n    \"\"\"Test to ensure that a prefix exists when using a versioned bucket\"\"\"\n    import uuid\n\n    n = 3\n    s3 = S3FileSystem(\n        anon=False,\n        version_aware=version_aware,\n        client_kwargs={\"endpoint_url\": endpoint_uri},\n    )\n    segments = [versioned_bucket_name] + [str(uuid.uuid4()) for _ in range(n)]\n    path = \"/\".join(segments)\n    for i in range(2, n + 1):\n        assert not s3.exists(\"/\".join(segments[:i]))\n    s3.touch(path)\n    for i in range(2, n + 1):\n        assert s3.exists(\"/\".join(segments[:i]))\n\n\ndef test_isfile(s3):\n    assert not s3.isfile(\"\")\n    assert not s3.isfile(\"/\")\n    assert not s3.isfile(test_bucket_name)\n    assert not s3.isfile(test_bucket_name + \"/test\")\n\n    assert not s3.isfile(test_bucket_name + \"/test/foo\")\n    assert s3.isfile(test_bucket_name + \"/test/accounts.1.json\")\n    assert s3.isfile(test_bucket_name + \"/test/accounts.2.json\")\n\n    assert not s3.isfile(a)\n    s3.touch(a)\n    assert s3.isfile(a)\n\n    assert not s3.isfile(b)\n    assert not s3.isfile(b + \"/\")\n    s3.mkdir(b)\n    assert not s3.isfile(b)\n    assert not s3.isfile(b + \"/\")\n\n    assert not s3.isfile(c)\n    assert not s3.isfile(c + \"/\")\n    s3.mkdir(c + \"/\")\n    assert not s3.isfile(c)\n    assert not s3.isfile(c + \"/\")\n\n\ndef test_isdir(s3):\n    assert s3.isdir(\"\")\n    assert s3.isdir(\"/\")\n    assert s3.isdir(test_bucket_name)\n    assert s3.isdir(test_bucket_name + \"/test\")\n\n    assert not s3.isdir(test_bucket_name + \"/test/foo\")\n    assert not s3.isdir(test_bucket_name + \"/test/accounts.1.json\")\n    assert not s3.isdir(test_bucket_name + \"/test/accounts.2.json\")\n\n    assert not s3.isdir(a)\n    s3.touch(a)\n    assert not s3.isdir(a)\n\n    assert not s3.isdir(b)\n    assert not s3.isdir(b + \"/\")\n\n    assert not s3.isdir(c)\n    assert not s3.isdir(c + \"/\")\n\n    # test cache\n    s3.invalidate_cache()\n    assert not s3.dircache\n    s3.ls(test_bucket_name + \"/nested\")\n    assert test_bucket_name + \"/nested\" in s3.dircache\n    assert not s3.isdir(test_bucket_name + \"/nested/file1\")\n    assert not s3.isdir(test_bucket_name + \"/nested/file2\")\n    assert s3.isdir(test_bucket_name + \"/nested/nested2\")\n    assert s3.isdir(test_bucket_name + \"/nested/nested2/\")\n\n\ndef test_rm(s3):\n    assert not s3.exists(a)\n    s3.touch(a)\n    assert s3.exists(a)\n    s3.rm(a)\n    assert not s3.exists(a)\n    # the API is OK with deleting non-files; maybe this is an effect of using bulk\n    # with pytest.raises(FileNotFoundError):\n    #    s3.rm(test_bucket_name + '/nonexistent')\n    with pytest.raises(FileNotFoundError):\n        s3.rm(\"nonexistent\")\n    out = s3.rm(test_bucket_name + \"/nested\", recursive=True)\n    assert test_bucket_name + \"/nested/nested2/file1\" in out\n    assert not s3.exists(test_bucket_name + \"/nested/nested2/file1\")\n\n    # whole bucket\n    out = s3.rm(test_bucket_name, recursive=True)\n    assert test_bucket_name + \"/2014-01-01.csv\" in out\n    assert not s3.exists(test_bucket_name + \"/2014-01-01.csv\")\n    assert not s3.exists(test_bucket_name)\n\n\ndef test_rmdir(s3):\n    bucket = \"test1_bucket\"\n    s3.mkdir(bucket)\n    s3.rmdir(bucket)\n    assert bucket not in s3.ls(\"/\")\n\n    # Issue 689, s3fs rmdir command returns error when given a valid s3 path.\n    dir = test_bucket_name + \"/dir\"\n\n    assert not s3.exists(dir)\n    with pytest.raises(FileNotFoundError):\n        s3.rmdir(dir)\n\n    s3.touch(dir + \"/file\")\n    assert s3.exists(dir)\n    assert s3.exists(dir + \"/file\")\n    with pytest.raises(FileExistsError):\n        s3.rmdir(dir)\n\n    with pytest.raises(OSError):\n        s3.rmdir(test_bucket_name)\n\n\ndef test_mkdir(s3):\n    bucket = \"test1_bucket\"\n    s3.mkdir(bucket)\n    assert bucket in s3.ls(\"/\")\n\n\ndef test_mkdir_existing_bucket(s3):\n    # mkdir called on existing bucket should be no-op and not calling create_bucket\n    # creating a s3 bucket\n    bucket = \"test1_bucket\"\n    s3.mkdir(bucket)\n    assert bucket in s3.ls(\"/\")\n    # a second call.\n    with pytest.raises(FileExistsError):\n        s3.mkdir(bucket)\n\n\ndef test_mkdir_bucket_and_key_1(s3):\n    bucket = \"test1_bucket\"\n    file = bucket + \"/a/b/c\"\n    s3.mkdir(file, create_parents=True)\n    assert bucket in s3.ls(\"/\")\n\n\ndef test_mkdir_bucket_and_key_2(s3):\n    bucket = \"test1_bucket\"\n    file = bucket + \"/a/b/c\"\n    with pytest.raises(FileNotFoundError):\n        s3.mkdir(file, create_parents=False)\n    assert bucket not in s3.ls(\"/\")\n\n\ndef test_mkdir_region_name(s3):\n    bucket = \"test2_bucket\"\n    s3.mkdir(bucket, region_name=\"eu-central-1\")\n    assert bucket in s3.ls(\"/\")\n\n\ndef test_mkdir_client_region_name(s3):\n    bucket = \"test3_bucket\"\n    s3 = S3FileSystem(\n        anon=False,\n        client_kwargs={\"region_name\": \"eu-central-1\", \"endpoint_url\": endpoint_uri},\n    )\n    s3.mkdir(bucket)\n    assert bucket in s3.ls(\"/\")\n\n\ndef test_makedirs(s3):\n    bucket = \"test_makedirs_bucket\"\n    test_file = bucket + \"/a/b/c/file\"\n    s3.makedirs(test_file)\n    assert bucket in s3.ls(\"/\")\n\n\ndef test_makedirs_existing_bucket(s3):\n    bucket = \"test_makedirs_bucket\"\n    s3.mkdir(bucket)\n    assert bucket in s3.ls(\"/\")\n    test_file = bucket + \"/a/b/c/file\"\n    # no-op, and no error.\n    s3.makedirs(test_file)\n\n\ndef test_makedirs_pure_bucket_exist_ok(s3):\n    bucket = \"test1_bucket\"\n    s3.mkdir(bucket)\n    s3.makedirs(bucket, exist_ok=True)\n\n\ndef test_makedirs_pure_bucket_error_on_exist(s3):\n    bucket = \"test1_bucket\"\n    s3.mkdir(bucket)\n    with pytest.raises(FileExistsError):\n        s3.makedirs(bucket, exist_ok=False)\n\n\ndef test_bulk_delete(s3):\n    with pytest.raises(FileNotFoundError):\n        s3.rm([\"nonexistent/file\"])\n    filelist = s3.find(test_bucket_name + \"/nested\")\n    s3.rm(filelist)\n    assert not s3.exists(test_bucket_name + \"/nested/nested2/file1\")\n\n\n@pytest.mark.xfail(reason=\"anon user is still privileged on moto\")\ndef test_anonymous_access(s3):\n    with ignoring(NoCredentialsError):\n        s3 = S3FileSystem(anon=True, client_kwargs={\"endpoint_url\": endpoint_uri})\n        assert s3.ls(\"\") == []\n        # TODO: public bucket doesn't work through moto\n\n    with pytest.raises(PermissionError):\n        s3.mkdir(\"newbucket\")\n\n\ndef test_s3_file_access(s3):\n    fn = test_bucket_name + \"/nested/file1\"\n    data = b\"hello\\n\"\n    assert s3.cat(fn) == data\n    assert s3.head(fn, 3) == data[:3]\n    assert s3.tail(fn, 3) == data[-3:]\n    assert s3.tail(fn, 10000) == data\n\n\ndef test_s3_file_info(s3):\n    fn = test_bucket_name + \"/nested/file1\"\n    data = b\"hello\\n\"\n    assert fn in s3.find(test_bucket_name)\n    assert s3.exists(fn)\n    assert not s3.exists(fn + \"another\")\n    assert s3.info(fn)[\"Size\"] == len(data)\n    with pytest.raises(FileNotFoundError):\n        s3.info(fn + \"another\")\n\n\ndef test_content_type_is_set(s3, tmpdir):\n    test_file = str(tmpdir) + \"/test.json\"\n    destination = test_bucket_name + \"/test.json\"\n    open(test_file, \"w\").write(\"text\")\n    s3.put(test_file, destination)\n    assert s3.info(destination)[\"ContentType\"] == \"application/json\"\n\n\ndef test_content_type_is_not_overrided(s3, tmpdir):\n    test_file = os.path.join(str(tmpdir), \"test.json\")\n    destination = os.path.join(test_bucket_name, \"test.json\")\n    open(test_file, \"w\").write(\"text\")\n    s3.put(test_file, destination, ContentType=\"text/css\")\n    assert s3.info(destination)[\"ContentType\"] == \"text/css\"\n\n\ndef test_bucket_exists(s3):\n    assert s3.exists(test_bucket_name)\n    assert not s3.exists(test_bucket_name + \"x\")\n    s3 = S3FileSystem(anon=True, client_kwargs={\"endpoint_url\": endpoint_uri})\n    assert s3.exists(test_bucket_name)\n    assert not s3.exists(test_bucket_name + \"x\")\n\n\ndef test_du(s3):\n    d = s3.du(test_bucket_name, total=False)\n    assert all(isinstance(v, int) and v >= 0 for v in d.values())\n    assert test_bucket_name + \"/nested/file1\" in d\n\n    assert s3.du(test_bucket_name + \"/test/\", total=True) == sum(\n        map(len, files.values())\n    )\n    assert s3.du(test_bucket_name) == s3.du(\"s3://\" + test_bucket_name)\n\n    # Issue 450, s3.du of non-existent directory\n    dir = test_bucket_name + \"/does-not-exist\"\n    assert not s3.exists(dir)\n    assert s3.du(dir) == 0\n    assert s3.du(dir + \"/\") == 0\n\n\ndef test_s3_ls(s3):\n    fn = test_bucket_name + \"/nested/file1\"\n    assert fn not in s3.ls(test_bucket_name + \"/\")\n    assert fn in s3.ls(test_bucket_name + \"/nested/\")\n    assert fn in s3.ls(test_bucket_name + \"/nested\")\n    assert s3.ls(\"s3://\" + test_bucket_name + \"/nested/\") == s3.ls(\n        test_bucket_name + \"/nested\"\n    )\n\n\ndef test_s3_big_ls(s3):\n    for x in range(1200):\n        s3.touch(test_bucket_name + \"/thousand/%i.part\" % x)\n    assert len(s3.find(test_bucket_name)) > 1200\n    s3.rm(test_bucket_name + \"/thousand/\", recursive=True)\n    assert len(s3.find(test_bucket_name + \"/thousand/\")) == 0\n\n\ndef test_s3_ls_detail(s3):\n    L = s3.ls(test_bucket_name + \"/nested\", detail=True)\n    assert all(isinstance(item, dict) for item in L)\n\n\ndef test_s3_glob(s3):\n    fn = test_bucket_name + \"/nested/file1\"\n    assert fn not in s3.glob(test_bucket_name + \"/\")\n    assert fn not in s3.glob(test_bucket_name + \"/*\")\n    assert fn not in s3.glob(test_bucket_name + \"/nested\")\n    assert fn in s3.glob(test_bucket_name + \"/nested/*\")\n    assert fn in s3.glob(test_bucket_name + \"/nested/file*\")\n    assert fn in s3.glob(test_bucket_name + \"/*/*\")\n    assert all(\n        any(p.startswith(f + \"/\") or p == f for p in s3.find(test_bucket_name))\n        for f in s3.glob(test_bucket_name + \"/nested/*\")\n    )\n    assert [test_bucket_name + \"/nested/nested2\"] == s3.glob(\n        test_bucket_name + \"/nested/nested2\"\n    )\n    out = s3.glob(test_bucket_name + \"/nested/nested2/*\")\n    assert {\"test/nested/nested2/file1\", \"test/nested/nested2/file2\"} == set(out)\n\n    with pytest.raises(ValueError):\n        s3.glob(\"*\")\n\n    # Make sure glob() deals with the dot character (.) correctly.\n    assert test_bucket_name + \"/file.dat\" in s3.glob(test_bucket_name + \"/file.*\")\n    assert test_bucket_name + \"/filexdat\" not in s3.glob(test_bucket_name + \"/file.*\")\n\n\ndef test_get_list_of_summary_objects(s3):\n    L = s3.ls(test_bucket_name + \"/test\")\n\n    assert len(L) == 2\n    assert [l.lstrip(test_bucket_name).lstrip(\"/\") for l in sorted(L)] == sorted(\n        list(files)\n    )\n\n    L2 = s3.ls(\"s3://\" + test_bucket_name + \"/test\")\n\n    assert L == L2\n\n\ndef test_read_keys_from_bucket(s3):\n    for k, data in files.items():\n        file_contents = s3.cat(\"/\".join([test_bucket_name, k]))\n        assert file_contents == data\n\n        assert s3.cat(\"/\".join([test_bucket_name, k])) == s3.cat(\n            \"s3://\" + \"/\".join([test_bucket_name, k])\n        )\n\n\ndef test_url(s3):\n    fn = test_bucket_name + \"/nested/file1\"\n    url = s3.url(fn, expires=100)\n    assert \"http\" in url\n    import urllib.parse\n\n    components = urllib.parse.urlparse(url)\n    query = urllib.parse.parse_qs(components.query)\n    exp = int(query[\"Expires\"][0])\n\n    delta = abs(exp - time.time() - 100)\n    assert delta < 5\n\n    with s3.open(fn) as f:\n        assert \"http\" in f.url()\n\n\ndef test_seek(s3):\n    with s3.open(a, \"wb\") as f:\n        f.write(b\"123\")\n\n    with s3.open(a) as f:\n        f.seek(1000)\n        with pytest.raises(ValueError):\n            f.seek(-1)\n        with pytest.raises(ValueError):\n            f.seek(-5, 2)\n        with pytest.raises(ValueError):\n            f.seek(0, 10)\n        f.seek(0)\n        assert f.read(1) == b\"1\"\n        f.seek(0)\n        assert f.read(1) == b\"1\"\n        f.seek(3)\n        assert f.read(1) == b\"\"\n        f.seek(-1, 2)\n        assert f.read(1) == b\"3\"\n        f.seek(-1, 1)\n        f.seek(-1, 1)\n        assert f.read(1) == b\"2\"\n        for i in range(4):\n            assert f.seek(i) == i\n\n\ndef test_bad_open(s3):\n    with pytest.raises(ValueError):\n        s3.open(\"\")\n\n\ndef test_copy(s3):\n    fn = test_bucket_name + \"/test/accounts.1.json\"\n    s3.copy(fn, fn + \"2\")\n    assert s3.cat(fn) == s3.cat(fn + \"2\")\n\n\ndef test_copy_managed(s3):\n    data = b\"abc\" * 12 * 2**20\n    fn = test_bucket_name + \"/test/biggerfile\"\n    with s3.open(fn, \"wb\") as f:\n        f.write(data)\n    sync(s3.loop, s3._copy_managed, fn, fn + \"2\", size=len(data), block=5 * 2**20)\n    assert s3.cat(fn) == s3.cat(fn + \"2\")\n    with pytest.raises(ValueError):\n        sync(s3.loop, s3._copy_managed, fn, fn + \"3\", size=len(data), block=4 * 2**20)\n    with pytest.raises(ValueError):\n        sync(s3.loop, s3._copy_managed, fn, fn + \"3\", size=len(data), block=6 * 2**30)\n\n\n@pytest.mark.parametrize(\"recursive\", [True, False])\ndef test_move(s3, recursive):\n    fn = test_bucket_name + \"/test/accounts.1.json\"\n    data = s3.cat(fn)\n    s3.mv(fn, fn + \"2\", recursive=recursive)\n    assert s3.cat(fn + \"2\") == data\n    assert not s3.exists(fn)\n\n\ndef test_get_put(s3, tmpdir):\n    test_file = str(tmpdir.join(\"test.json\"))\n\n    s3.get(test_bucket_name + \"/test/accounts.1.json\", test_file)\n    data = files[\"test/accounts.1.json\"]\n    assert open(test_file, \"rb\").read() == data\n    s3.put(test_file, test_bucket_name + \"/temp\")\n    assert s3.du(test_bucket_name + \"/temp\", total=False)[\n        test_bucket_name + \"/temp\"\n    ] == len(data)\n    assert s3.cat(test_bucket_name + \"/temp\") == data\n\n\ndef test_get_put_big(s3, tmpdir):\n    test_file = str(tmpdir.join(\"test\"))\n    data = b\"1234567890A\" * 2**20\n    open(test_file, \"wb\").write(data)\n\n    s3.put(test_file, test_bucket_name + \"/bigfile\")\n    test_file = str(tmpdir.join(\"test2\"))\n    s3.get(test_bucket_name + \"/bigfile\", test_file)\n    assert open(test_file, \"rb\").read() == data\n\n\ndef test_get_put_with_callback(s3, tmpdir):\n    test_file = str(tmpdir.join(\"test.json\"))\n\n    class BranchingCallback(Callback):\n        def branch(self, path_1, path_2, kwargs):\n            kwargs[\"callback\"] = BranchingCallback()\n\n    cb = BranchingCallback()\n    s3.get(test_bucket_name + \"/test/accounts.1.json\", test_file, callback=cb)\n    assert cb.size == 1\n    assert cb.value == 1\n\n    cb = BranchingCallback()\n    s3.put(test_file, test_bucket_name + \"/temp\", callback=cb)\n    assert cb.size == 1\n    assert cb.value == 1\n\n\ndef test_get_file_with_callback(s3, tmpdir):\n    test_file = str(tmpdir.join(\"test.json\"))\n\n    cb = Callback()\n    s3.get_file(test_bucket_name + \"/test/accounts.1.json\", test_file, callback=cb)\n    assert cb.size == os.stat(test_file).st_size\n    assert cb.value == cb.size\n\n\ndef test_get_file_with_kwargs(s3, tmpdir):\n    test_file = str(tmpdir.join(\"test.json\"))\n\n    get_file_kwargs = {\"max_concurency\": 1, \"random_kwarg\": \"value\"}\n    s3.get_file(\n        test_bucket_name + \"/test/accounts.1.json\", test_file, **get_file_kwargs\n    )\n\n\n@pytest.mark.parametrize(\"size\", [2**10, 10 * 2**20])\ndef test_put_file_with_callback(s3, tmpdir, size):\n    test_file = str(tmpdir.join(\"test.json\"))\n    with open(test_file, \"wb\") as f:\n        f.write(b\"1234567890A\" * size)\n\n    cb = Callback()\n    s3.put_file(test_file, test_bucket_name + \"/temp\", callback=cb)\n    assert cb.size == os.stat(test_file).st_size\n    assert cb.value == cb.size\n\n\n@pytest.mark.parametrize(\"size\", [2**10, 2**20, 10 * 2**20])\ndef test_pipe_cat_big(s3, size):\n    data = b\"1234567890A\" * size\n    s3.pipe(test_bucket_name + \"/bigfile\", data)\n    assert s3.cat(test_bucket_name + \"/bigfile\") == data\n\n\ndef test_errors(s3):\n    with pytest.raises(FileNotFoundError):\n        s3.open(test_bucket_name + \"/tmp/test/shfoshf\", \"rb\")\n\n    # This is fine, no need for interleaving directories on S3\n    # with pytest.raises((IOError, OSError)):\n    #    s3.touch('tmp/test/shfoshf/x')\n\n    # Deleting nonexistent or zero paths is allowed for now\n    # with pytest.raises(FileNotFoundError):\n    #    s3.rm(test_bucket_name + '/tmp/test/shfoshf/x')\n\n    with pytest.raises(FileNotFoundError):\n        s3.mv(test_bucket_name + \"/tmp/test/shfoshf/x\", \"tmp/test/shfoshf/y\")\n\n    with pytest.raises(ValueError):\n        s3.open(\"x\", \"rb\")\n\n    with pytest.raises(FileNotFoundError):\n        s3.rm(\"unknown\")\n\n    with pytest.raises(ValueError):\n        with s3.open(test_bucket_name + \"/temp\", \"wb\") as f:\n            f.read()\n\n    with pytest.raises(ValueError):\n        f = s3.open(test_bucket_name + \"/temp\", \"rb\")\n        f.close()\n        f.read()\n\n    with pytest.raises(ValueError):\n        s3.mkdir(\"/\")\n\n    with pytest.raises(ValueError):\n        s3.find(\"\")\n\n    with pytest.raises(ValueError):\n        s3.find(\"s3://\")\n\n\ndef test_errors_cause_preservings(monkeypatch, s3):\n    # We translate the error, and preserve the original one\n    with pytest.raises(FileNotFoundError) as exc:\n        s3.rm(\"unknown\")\n\n    assert type(exc.value.__cause__).__name__ == \"NoSuchBucket\"\n\n    async def head_object(*args, **kwargs):\n        raise NoCredentialsError\n\n    monkeypatch.setattr(type(s3.s3), \"head_object\", head_object)\n\n    # Since the error is not translate, the __cause__ would\n    # be None\n    with pytest.raises(NoCredentialsError) as exc:\n        s3.info(\"test/a.txt\")\n\n    assert exc.value.__cause__ is None\n\n\ndef test_read_small(s3):\n    fn = test_bucket_name + \"/2014-01-01.csv\"\n    with s3.open(fn, \"rb\", block_size=10, cache_type=\"bytes\") as f:\n        out = []\n        while True:\n            data = f.read(3)\n            if data == b\"\":\n                break\n            out.append(data)\n        assert s3.cat(fn) == b\"\".join(out)\n        # cache drop\n        assert len(f.cache) < len(out)\n\n\ndef test_read_s3_block(s3):\n    data = files[\"test/accounts.1.json\"]\n    lines = io.BytesIO(data).readlines()\n    path = test_bucket_name + \"/test/accounts.1.json\"\n    assert s3.read_block(path, 1, 35, b\"\\n\") == lines[1]\n    assert s3.read_block(path, 0, 30, b\"\\n\") == lines[0]\n    assert s3.read_block(path, 0, 35, b\"\\n\") == lines[0] + lines[1]\n    assert s3.read_block(path, 0, 5000, b\"\\n\") == data\n    assert len(s3.read_block(path, 0, 5)) == 5\n    assert len(s3.read_block(path, 4, 5000)) == len(data) - 4\n    assert s3.read_block(path, 5000, 5010) == b\"\"\n\n    assert s3.read_block(path, 5, None) == s3.read_block(path, 5, 1000)\n\n\ndef test_new_bucket(s3):\n    assert not s3.exists(\"new\")\n    s3.mkdir(\"new\")\n    assert s3.exists(\"new\")\n    with s3.open(\"new/temp\", \"wb\") as f:\n        f.write(b\"hello\")\n    with pytest.raises(OSError):\n        s3.rmdir(\"new\")\n\n    s3.rm(\"new/temp\")\n    s3.rmdir(\"new\")\n    assert \"new\" not in s3.ls(\"\")\n    assert not s3.exists(\"new\")\n    with pytest.raises(FileNotFoundError):\n        s3.ls(\"new\")\n\n\ndef test_new_bucket_auto(s3):\n    assert not s3.exists(\"new\")\n    with pytest.raises(Exception):\n        s3.mkdir(\"new/other\", create_parents=False)\n    s3.mkdir(\"new/other\", create_parents=True)\n    assert s3.exists(\"new\")\n    s3.touch(\"new/afile\")\n    with pytest.raises(Exception):\n        s3.rm(\"new\")\n    with pytest.raises(Exception):\n        s3.rmdir(\"new\")\n    s3.rm(\"new\", recursive=True)\n    assert not s3.exists(\"new\")\n\n\ndef test_dynamic_add_rm(s3):\n    s3.mkdir(\"one\")\n    s3.mkdir(\"one/two\")\n    assert s3.exists(\"one\")\n    s3.ls(\"one\")\n    s3.touch(\"one/two/file_a\")\n    assert s3.exists(\"one/two/file_a\")\n    s3.rm(\"one\", recursive=True)\n    assert not s3.exists(\"one\")\n\n\ndef test_write_small(s3):\n    with s3.open(test_bucket_name + \"/test\", \"wb\") as f:\n        f.write(b\"hello\")\n    assert s3.cat(test_bucket_name + \"/test\") == b\"hello\"\n    s3.open(test_bucket_name + \"/test\", \"wb\").close()\n    assert s3.info(test_bucket_name + \"/test\")[\"size\"] == 0\n\n\ndef test_write_small_with_acl(s3):\n    bucket, key = (test_bucket_name, \"test-acl\")\n    filename = bucket + \"/\" + key\n    body = b\"hello\"\n    public_read_acl = {\n        \"Permission\": \"READ\",\n        \"Grantee\": {\n            \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\",\n            \"Type\": \"Group\",\n        },\n    }\n\n    with s3.open(filename, \"wb\", acl=\"public-read\") as f:\n        f.write(body)\n    assert s3.cat(filename) == body\n\n    assert (\n        public_read_acl\n        in sync(s3.loop, s3.s3.get_object_acl, Bucket=bucket, Key=key)[\"Grants\"]\n    )\n\n\ndef test_write_large(s3):\n    \"flush() chunks buffer when processing large singular payload\"\n    mb = 2**20\n    payload_size = int(2.5 * 5 * mb)\n    payload = b\"0\" * payload_size\n\n    with s3.open(test_bucket_name + \"/test\", \"wb\") as fd:\n        fd.write(payload)\n\n    assert s3.cat(test_bucket_name + \"/test\") == payload\n    assert s3.info(test_bucket_name + \"/test\")[\"size\"] == payload_size\n\n\ndef test_write_limit(s3):\n    \"flush() respects part_max when processing large singular payload\"\n    mb = 2**20\n    block_size = 15 * mb\n    payload_size = 44 * mb\n    payload = b\"0\" * payload_size\n\n    with s3.open(test_bucket_name + \"/test\", \"wb\", blocksize=block_size) as fd:\n        fd.write(payload)\n\n    assert s3.cat(test_bucket_name + \"/test\") == payload\n\n    assert s3.info(test_bucket_name + \"/test\")[\"size\"] == payload_size\n\n\ndef test_write_small_secure(s3):\n    # Unfortunately moto does not yet support enforcing SSE policies.  It also\n    # does not return the correct objects that can be used to test the results\n    # effectively.\n    # This test is left as a placeholder in case moto eventually supports this.\n    sse_params = SSEParams(server_side_encryption=\"aws:kms\")\n    with s3.open(secure_bucket_name + \"/test\", \"wb\", writer_kwargs=sse_params) as f:\n        f.write(b\"hello\")\n    assert s3.cat(secure_bucket_name + \"/test\") == b\"hello\"\n    sync(s3.loop, s3.s3.head_object, Bucket=secure_bucket_name, Key=\"test\")\n\n\ndef test_write_large_secure(s3):\n    # build our own s3fs with the relevant additional kwarg\n    s3 = S3FileSystem(\n        s3_additional_kwargs={\"ServerSideEncryption\": \"AES256\"},\n        client_kwargs={\"endpoint_url\": endpoint_uri},\n    )\n    s3.mkdir(\"mybucket\")\n\n    with s3.open(\"mybucket/myfile\", \"wb\") as f:\n        f.write(b\"hello hello\" * 10**6)\n\n    assert s3.cat(\"mybucket/myfile\") == b\"hello hello\" * 10**6\n\n\ndef test_write_fails(s3):\n    with pytest.raises(ValueError):\n        s3.touch(test_bucket_name + \"/temp\")\n        s3.open(test_bucket_name + \"/temp\", \"rb\").write(b\"hello\")\n    with pytest.raises(ValueError):\n        s3.open(test_bucket_name + \"/temp\", \"wb\", block_size=10)\n    f = s3.open(test_bucket_name + \"/temp\", \"wb\")\n    f.close()\n    with pytest.raises(ValueError):\n        f.write(b\"hello\")\n    with pytest.raises(FileNotFoundError):\n        s3.open(\"nonexistentbucket/temp\", \"wb\").close()\n\n\ndef test_write_blocks(s3):\n    with s3.open(test_bucket_name + \"/temp\", \"wb\") as f:\n        f.write(b\"a\" * 2 * 2**20)\n        assert f.buffer.tell() == 2 * 2**20\n        assert not (f.parts)\n        f.flush()\n        assert f.buffer.tell() == 2 * 2**20\n        assert not (f.parts)\n        f.write(b\"a\" * 2 * 2**20)\n        f.write(b\"a\" * 2 * 2**20)\n        assert f.mpu\n        assert f.parts\n    assert s3.info(test_bucket_name + \"/temp\")[\"size\"] == 6 * 2**20\n    with s3.open(test_bucket_name + \"/temp\", \"wb\", block_size=10 * 2**20) as f:\n        f.write(b\"a\" * 15 * 2**20)\n        assert f.buffer.tell() == 0\n    assert s3.info(test_bucket_name + \"/temp\")[\"size\"] == 15 * 2**20\n\n\ndef test_readline(s3):\n    all_items = chain.from_iterable(\n        [files.items(), csv_files.items(), text_files.items()]\n    )\n    for k, data in all_items:\n        with s3.open(\"/\".join([test_bucket_name, k]), \"rb\") as f:\n            result = f.readline()\n            expected = data.split(b\"\\n\")[0] + (b\"\\n\" if data.count(b\"\\n\") else b\"\")\n            assert result == expected\n\n\ndef test_readline_empty(s3):\n    data = b\"\"\n    with s3.open(a, \"wb\") as f:\n        f.write(data)\n    with s3.open(a, \"rb\") as f:\n        result = f.readline()\n        assert result == data\n\n\ndef test_readline_blocksize(s3):\n    data = b\"ab\\n\" + b\"a\" * (10 * 2**20) + b\"\\nab\"\n    with s3.open(a, \"wb\") as f:\n        f.write(data)\n    with s3.open(a, \"rb\") as f:\n        result = f.readline()\n        expected = b\"ab\\n\"\n        assert result == expected\n\n        result = f.readline()\n        expected = b\"a\" * (10 * 2**20) + b\"\\n\"\n        assert result == expected\n\n        result = f.readline()\n        expected = b\"ab\"\n        assert result == expected\n\n\ndef test_next(s3):\n    expected = csv_files[\"2014-01-01.csv\"].split(b\"\\n\")[0] + b\"\\n\"\n    with s3.open(test_bucket_name + \"/2014-01-01.csv\") as f:\n        result = next(f)\n        assert result == expected\n\n\ndef test_iterable(s3):\n    data = b\"abc\\n123\"\n    with s3.open(a, \"wb\") as f:\n        f.write(data)\n    with s3.open(a) as f, io.BytesIO(data) as g:\n        for froms3, fromio in zip(f, g):\n            assert froms3 == fromio\n        f.seek(0)\n        assert f.readline() == b\"abc\\n\"\n        assert f.readline() == b\"123\"\n        f.seek(1)\n        assert f.readline() == b\"bc\\n\"\n\n    with s3.open(a) as f:\n        out = list(f)\n    with s3.open(a) as f:\n        out2 = f.readlines()\n    assert out == out2\n    assert b\"\".join(out) == data\n\n\ndef test_readable(s3):\n    with s3.open(a, \"wb\") as f:\n        assert not f.readable()\n\n    with s3.open(a, \"rb\") as f:\n        assert f.readable()\n\n\ndef test_seekable(s3):\n    with s3.open(a, \"wb\") as f:\n        assert not f.seekable()\n\n    with s3.open(a, \"rb\") as f:\n        assert f.seekable()\n\n\ndef test_writable(s3):\n    with s3.open(a, \"wb\") as f:\n        assert f.writable()\n\n    with s3.open(a, \"rb\") as f:\n        assert not f.writable()\n\n\ndef test_merge(s3):\n    with s3.open(a, \"wb\") as f:\n        f.write(b\"a\" * 10 * 2**20)\n\n    with s3.open(b, \"wb\") as f:\n        f.write(b\"a\" * 10 * 2**20)\n    s3.merge(test_bucket_name + \"/joined\", [a, b])\n    assert s3.info(test_bucket_name + \"/joined\")[\"size\"] == 2 * 10 * 2**20\n\n\ndef test_append(s3):\n    data = text_files[\"nested/file1\"]\n    with s3.open(test_bucket_name + \"/nested/file1\", \"ab\") as f:\n        assert f.tell() == len(data)  # append, no write, small file\n    assert s3.cat(test_bucket_name + \"/nested/file1\") == data\n    with s3.open(test_bucket_name + \"/nested/file1\", \"ab\") as f:\n        f.write(b\"extra\")  # append, write, small file\n    assert s3.cat(test_bucket_name + \"/nested/file1\") == data + b\"extra\"\n\n    with s3.open(a, \"wb\") as f:\n        f.write(b\"a\" * 10 * 2**20)\n    with s3.open(a, \"ab\") as f:\n        pass  # append, no write, big file\n    assert s3.cat(a) == b\"a\" * 10 * 2**20\n\n    with s3.open(a, \"ab\") as f:\n        assert f.parts is None\n        f._initiate_upload()\n        assert f.parts\n        assert f.tell() == 10 * 2**20\n        f.write(b\"extra\")  # append, small write, big file\n    assert s3.cat(a) == b\"a\" * 10 * 2**20 + b\"extra\"\n\n    with s3.open(a, \"ab\") as f:\n        assert f.tell() == 10 * 2**20 + 5\n        f.write(b\"b\" * 10 * 2**20)  # append, big write, big file\n        assert f.tell() == 20 * 2**20 + 5\n    assert s3.cat(a) == b\"a\" * 10 * 2**20 + b\"extra\" + b\"b\" * 10 * 2**20\n\n    # Keep Head Metadata\n    head = dict(\n        CacheControl=\"public\",\n        ContentDisposition=\"string\",\n        ContentEncoding=\"gzip\",\n        ContentLanguage=\"ru-RU\",\n        ContentType=\"text/csv\",\n        Expires=datetime.datetime(2015, 1, 1, 0, 0, tzinfo=tzutc()),\n        Metadata={\"string\": \"string\"},\n        ServerSideEncryption=\"AES256\",\n        StorageClass=\"REDUCED_REDUNDANCY\",\n        WebsiteRedirectLocation=\"https://www.example.com/\",\n    )\n    with s3.open(a, \"wb\", **head) as f:\n        f.write(b\"data\")\n\n    with s3.open(a, \"ab\") as f:\n        f.write(b\"other\")\n\n    with s3.open(a) as f:\n        filehead = {\n            k: v\n            for k, v in f._call_s3(\n                \"head_object\", f.kwargs, Bucket=f.bucket, Key=f.key\n            ).items()\n            if k in head\n        }\n        assert filehead == head\n\n\ndef test_bigger_than_block_read(s3):\n    with s3.open(test_bucket_name + \"/2014-01-01.csv\", \"rb\", block_size=3) as f:\n        out = []\n        while True:\n            data = f.read(20)\n            out.append(data)\n            if len(data) == 0:\n                break\n    assert b\"\".join(out) == csv_files[\"2014-01-01.csv\"]\n\n\ndef test_current(s3):\n    s3._cache.clear()\n    s3 = S3FileSystem(client_kwargs={\"endpoint_url\": endpoint_uri})\n    assert s3.current() is s3\n    assert S3FileSystem.current() is s3\n\n\ndef test_array(s3):\n    from array import array\n\n    data = array(\"B\", [65] * 1000)\n\n    with s3.open(a, \"wb\") as f:\n        f.write(data)\n\n    with s3.open(a, \"rb\") as f:\n        out = f.read()\n        assert out == b\"A\" * 1000\n\n\ndef _get_s3_id(s3):\n    return id(s3.s3)\n\n\n@pytest.mark.parametrize(\n    \"method\",\n    [\n        \"spawn\",\n        pytest.param(\n            \"forkserver\",\n            marks=pytest.mark.skipif(\n                sys.platform.startswith(\"win\"),\n                reason=\"'forkserver' not available on windows\",\n            ),\n        ),\n    ],\n)\ndef test_no_connection_sharing_among_processes(s3, method):\n    import multiprocessing as mp\n\n    ctx = mp.get_context(method)\n    executor = ProcessPoolExecutor(mp_context=ctx)\n    conn_id = executor.submit(_get_s3_id, s3).result()\n    assert id(s3.connect()) != conn_id, \"Processes should not share S3 connections.\"\n\n\n@pytest.mark.xfail()\ndef test_public_file(s3):\n    # works on real s3, not on moto\n    test_bucket_name = \"s3fs_public_test\"\n    other_bucket_name = \"s3fs_private_test\"\n\n    s3.touch(test_bucket_name)\n    s3.touch(test_bucket_name + \"/afile\")\n    s3.touch(other_bucket_name, acl=\"public-read\")\n    s3.touch(other_bucket_name + \"/afile\", acl=\"public-read\")\n\n    s = S3FileSystem(anon=True, client_kwargs={\"endpoint_url\": endpoint_uri})\n    with pytest.raises(PermissionError):\n        s.ls(test_bucket_name)\n    s.ls(other_bucket_name)\n\n    s3.chmod(test_bucket_name, acl=\"public-read\")\n    s3.chmod(other_bucket_name, acl=\"private\")\n    with pytest.raises(PermissionError):\n        s.ls(other_bucket_name, refresh=True)\n    assert s.ls(test_bucket_name, refresh=True)\n\n    # public file in private bucket\n    with s3.open(other_bucket_name + \"/see_me\", \"wb\", acl=\"public-read\") as f:\n        f.write(b\"hello\")\n    assert s.cat(other_bucket_name + \"/see_me\") == b\"hello\"\n\n\ndef test_upload_with_s3fs_prefix(s3):\n    path = \"s3://test/prefix/key\"\n\n    with s3.open(path, \"wb\") as f:\n        f.write(b\"a\" * (10 * 2**20))\n\n    with s3.open(path, \"ab\") as f:\n        f.write(b\"b\" * (10 * 2**20))\n\n\ndef test_multipart_upload_blocksize(s3):\n    blocksize = 5 * (2**20)\n    expected_parts = 3\n\n    s3f = s3.open(a, \"wb\", block_size=blocksize)\n    for _ in range(3):\n        data = b\"b\" * blocksize\n        s3f.write(data)\n\n    # Ensure that the multipart upload consists of only 3 parts\n    assert len(s3f.parts) == expected_parts\n    s3f.close()\n\n\ndef test_default_pars(s3):\n    s3 = S3FileSystem(\n        default_block_size=20,\n        default_fill_cache=False,\n        client_kwargs={\"endpoint_url\": endpoint_uri},\n    )\n    fn = test_bucket_name + \"/\" + list(files)[0]\n    with s3.open(fn) as f:\n        assert f.blocksize == 20\n        assert f.fill_cache is False\n    with s3.open(fn, block_size=40, fill_cache=True) as f:\n        assert f.blocksize == 40\n        assert f.fill_cache is True\n\n\ndef test_tags(s3):\n    tagset = {\"tag1\": \"value1\", \"tag2\": \"value2\"}\n    fname = list(files)[0]\n    s3.touch(fname)\n    s3.put_tags(fname, tagset)\n    assert s3.get_tags(fname) == tagset\n\n    # Ensure merge mode updates value of existing key and adds new one\n    new_tagset = {\"tag2\": \"updatedvalue2\", \"tag3\": \"value3\"}\n    s3.put_tags(fname, new_tagset, mode=\"m\")\n    tagset.update(new_tagset)\n    assert s3.get_tags(fname) == tagset\n\n\n@pytest.mark.parametrize(\"prefix\", [\"\", \"/dir\", \"/dir/subdir\"])\ndef test_versions(s3, prefix):\n    parent = versioned_bucket_name + prefix\n    versioned_file = parent + \"/versioned_file\"\n\n    s3 = S3FileSystem(\n        anon=False, version_aware=True, client_kwargs={\"endpoint_url\": endpoint_uri}\n    )\n    with s3.open(versioned_file, \"wb\") as fo:\n        fo.write(b\"1\")\n    first_version = fo.version_id\n\n    with s3.open(versioned_file, \"wb\") as fo:\n        fo.write(b\"2\")\n    second_version = fo.version_id\n\n    assert s3.isfile(versioned_file)\n    versions = s3.object_version_info(versioned_file)\n    assert len(versions) == 2\n    assert {version[\"VersionId\"] for version in versions} == {\n        first_version,\n        second_version,\n    }\n\n    with s3.open(versioned_file) as fo:\n        assert fo.version_id == second_version\n        assert fo.read() == b\"2\"\n\n    with s3.open(versioned_file, version_id=first_version) as fo:\n        assert fo.version_id == first_version\n        assert fo.read() == b\"1\"\n\n    versioned_file_v1 = f\"{versioned_file}?versionId={first_version}\"\n    versioned_file_v2 = f\"{versioned_file}?versionId={second_version}\"\n\n    assert s3.ls(parent) == [versioned_file]\n    assert set(s3.ls(parent, versions=True)) == {versioned_file_v1, versioned_file_v2}\n\n    assert s3.exists(versioned_file_v1)\n    assert s3.info(versioned_file_v1)\n    assert s3.exists(versioned_file_v2)\n    assert s3.info(versioned_file_v2)\n\n\ndef test_list_versions_many(s3):\n    # moto doesn't actually behave in the same way that s3 does here so this doesn't test\n    # anything really in moto 1.2\n    s3 = S3FileSystem(\n        anon=False, version_aware=True, client_kwargs={\"endpoint_url\": endpoint_uri}\n    )\n    versioned_file = versioned_bucket_name + \"/versioned_file2\"\n    for i in range(1200):\n        with s3.open(versioned_file, \"wb\") as fo:\n            fo.write(b\"1\")\n    versions = s3.object_version_info(versioned_file)\n    assert len(versions) == 1200\n\n\ndef test_fsspec_versions_multiple(s3):\n    \"\"\"Test that the standard fsspec.core.get_fs_token_paths behaves as expected for versionId urls\"\"\"\n    s3 = S3FileSystem(\n        anon=False, version_aware=True, client_kwargs={\"endpoint_url\": endpoint_uri}\n    )\n    versioned_file = versioned_bucket_name + \"/versioned_file3\"\n    version_lookup = {}\n    for i in range(20):\n        contents = str(i).encode()\n        with s3.open(versioned_file, \"wb\") as fo:\n            fo.write(contents)\n        version_lookup[fo.version_id] = contents\n    urls = [\n        \"s3://{}?versionId={}\".format(versioned_file, version)\n        for version in version_lookup.keys()\n    ]\n    fs, token, paths = fsspec.core.get_fs_token_paths(\n        urls, storage_options=dict(client_kwargs={\"endpoint_url\": endpoint_uri})\n    )\n    assert isinstance(fs, S3FileSystem)\n    assert fs.version_aware\n    for path in paths:\n        with fs.open(path, \"rb\") as fo:\n            contents = fo.read()\n            assert contents == version_lookup[fo.version_id]\n\n\ndef test_versioned_file_fullpath(s3):\n    versioned_file = versioned_bucket_name + \"/versioned_file_fullpath\"\n    s3 = S3FileSystem(\n        anon=False, version_aware=True, client_kwargs={\"endpoint_url\": endpoint_uri}\n    )\n    with s3.open(versioned_file, \"wb\") as fo:\n        fo.write(b\"1\")\n    # moto doesn't correctly return a versionId for a multipart upload. So we resort to this.\n    # version_id = fo.version_id\n    versions = s3.object_version_info(versioned_file)\n    version_ids = [version[\"VersionId\"] for version in versions]\n    version_id = version_ids[0]\n\n    with s3.open(versioned_file, \"wb\") as fo:\n        fo.write(b\"2\")\n\n    file_with_version = \"{}?versionId={}\".format(versioned_file, version_id)\n\n    with s3.open(file_with_version, \"rb\") as fo:\n        assert fo.version_id == version_id\n        assert fo.read() == b\"1\"\n\n    versions = s3.object_version_info(versioned_file)\n    version_ids = [version[\"VersionId\"] for version in versions]\n    assert set(s3.ls(versioned_bucket_name, versions=True)) == {\n        f\"{versioned_file}?versionId={vid}\" for vid in version_ids\n    }\n\n\ndef test_versions_unaware(s3):\n    versioned_file = versioned_bucket_name + \"/versioned_file3\"\n    s3 = S3FileSystem(\n        anon=False, version_aware=False, client_kwargs={\"endpoint_url\": endpoint_uri}\n    )\n    with s3.open(versioned_file, \"wb\") as fo:\n        fo.write(b\"1\")\n    with s3.open(versioned_file, \"wb\") as fo:\n        fo.write(b\"2\")\n\n    with s3.open(versioned_file) as fo:\n        assert fo.version_id is None\n        assert fo.read() == b\"2\"\n\n    with pytest.raises(ValueError):\n        with s3.open(versioned_file, version_id=\"0\"):\n            fo.read()\n\n\ndef test_versions_dircached(s3):\n    versioned_file = versioned_bucket_name + \"/dir/versioned_file\"\n    s3 = S3FileSystem(\n        anon=False, version_aware=True, client_kwargs={\"endpoint_url\": endpoint_uri}\n    )\n    with s3.open(versioned_file, \"wb\") as fo:\n        fo.write(b\"1\")\n    first_version = fo.version_id\n    with s3.open(versioned_file, \"wb\") as fo:\n        fo.write(b\"2\")\n    second_version = fo.version_id\n    s3.find(versioned_bucket_name)\n    cached = s3.dircache[versioned_bucket_name + \"/dir\"][0]\n\n    assert cached.get(\"VersionId\") == second_version\n    assert s3.info(versioned_file) == cached\n\n    assert (\n        s3.info(versioned_file, version_id=first_version).get(\"VersionId\")\n        == first_version\n    )\n    assert (\n        s3.info(versioned_file, version_id=second_version).get(\"VersionId\")\n        == second_version\n    )\n\n\ndef test_text_io__stream_wrapper_works(s3):\n    \"\"\"Ensure using TextIOWrapper works.\"\"\"\n    s3.mkdir(\"bucket\")\n\n    with s3.open(\"bucket/file.txt\", \"wb\") as fd:\n        fd.write(\"\\u00af\\\\_(\\u30c4)_/\\u00af\".encode(\"utf-16-le\"))\n\n    with s3.open(\"bucket/file.txt\", \"rb\") as fd:\n        with io.TextIOWrapper(fd, \"utf-16-le\") as stream:\n            assert stream.readline() == \"\\u00af\\\\_(\\u30c4)_/\\u00af\"\n\n\ndef test_text_io__basic(s3):\n    \"\"\"Text mode is now allowed.\"\"\"\n    s3.mkdir(\"bucket\")\n\n    with s3.open(\"bucket/file.txt\", \"w\", encoding=\"utf-8\") as fd:\n        fd.write(\"\\u00af\\\\_(\\u30c4)_/\\u00af\")\n\n    with s3.open(\"bucket/file.txt\", \"r\", encoding=\"utf-8\") as fd:\n        assert fd.read() == \"\\u00af\\\\_(\\u30c4)_/\\u00af\"\n\n\ndef test_text_io__override_encoding(s3):\n    \"\"\"Allow overriding the default text encoding.\"\"\"\n    s3.mkdir(\"bucket\")\n\n    with s3.open(\"bucket/file.txt\", \"w\", encoding=\"ibm500\") as fd:\n        fd.write(\"Hello, World!\")\n\n    with s3.open(\"bucket/file.txt\", \"r\", encoding=\"ibm500\") as fd:\n        assert fd.read() == \"Hello, World!\"\n\n\ndef test_readinto(s3):\n    s3.mkdir(\"bucket\")\n\n    with s3.open(\"bucket/file.txt\", \"wb\") as fd:\n        fd.write(b\"Hello, World!\")\n\n    contents = bytearray(15)\n\n    with s3.open(\"bucket/file.txt\", \"rb\") as fd:\n        assert fd.readinto(contents) == 13\n\n    assert contents.startswith(b\"Hello, World!\")\n\n\ndef test_change_defaults_only_subsequent():\n    \"\"\"Test for Issue #135\n\n    Ensure that changing the default block size doesn't affect existing file\n    systems that were created using that default. It should only affect file\n    systems created after the change.\n    \"\"\"\n    try:\n        S3FileSystem.cachable = False  # don't reuse instances with same pars\n\n        fs_default = S3FileSystem(client_kwargs={\"endpoint_url\": endpoint_uri})\n        assert fs_default.default_block_size == 5 * (1024**2)\n\n        fs_overridden = S3FileSystem(\n            default_block_size=64 * (1024**2),\n            client_kwargs={\"endpoint_url\": endpoint_uri},\n        )\n        assert fs_overridden.default_block_size == 64 * (1024**2)\n\n        # Suppose I want all subsequent file systems to have a block size of 1 GiB\n        # instead of 5 MiB:\n        S3FileSystem.default_block_size = 1024**3\n\n        fs_big = S3FileSystem(client_kwargs={\"endpoint_url\": endpoint_uri})\n        assert fs_big.default_block_size == 1024**3\n\n        # Test the other file systems created to see if their block sizes changed\n        assert fs_overridden.default_block_size == 64 * (1024**2)\n        assert fs_default.default_block_size == 5 * (1024**2)\n    finally:\n        S3FileSystem.default_block_size = 5 * (1024**2)\n        S3FileSystem.cachable = True\n\n\ndef test_cache_after_copy(s3):\n    # https://github.com/dask/dask/issues/5134\n    s3.touch(\"test/afile\")\n    assert \"test/afile\" in s3.ls(\"s3://test\", False)\n    s3.cp(\"test/afile\", \"test/bfile\")\n    assert \"test/bfile\" in s3.ls(\"s3://test\", False)\n\n\ndef test_autocommit(s3):\n    auto_file = test_bucket_name + \"/auto_file\"\n    committed_file = test_bucket_name + \"/commit_file\"\n    aborted_file = test_bucket_name + \"/aborted_file\"\n    s3 = S3FileSystem(\n        anon=False, version_aware=True, client_kwargs={\"endpoint_url\": endpoint_uri}\n    )\n\n    def write_and_flush(path, autocommit):\n        with s3.open(path, \"wb\", autocommit=autocommit) as fo:\n            fo.write(b\"1\")\n        return fo\n\n    # regular behavior\n    fo = write_and_flush(auto_file, autocommit=True)\n    assert fo.autocommit\n    assert s3.exists(auto_file)\n\n    fo = write_and_flush(committed_file, autocommit=False)\n    assert not fo.autocommit\n    assert not s3.exists(committed_file)\n    fo.commit()\n    assert s3.exists(committed_file)\n\n    fo = write_and_flush(aborted_file, autocommit=False)\n    assert not s3.exists(aborted_file)\n    fo.discard()\n    assert not s3.exists(aborted_file)\n    # Cannot commit a file that was discarded\n    with pytest.raises(Exception):\n        fo.commit()\n\n\ndef test_autocommit_mpu(s3):\n    \"\"\"When not autocommitting we always want to use multipart uploads\"\"\"\n    path = test_bucket_name + \"/auto_commit_with_mpu\"\n    with s3.open(path, \"wb\", autocommit=False) as fo:\n        fo.write(b\"1\")\n    assert fo.mpu is not None\n    assert len(fo.parts) == 1\n\n\ndef test_touch(s3):\n    # create\n    fn = test_bucket_name + \"/touched\"\n    assert not s3.exists(fn)\n    s3.touch(fn)\n    assert s3.exists(fn)\n    assert s3.size(fn) == 0\n\n    # truncates\n    with s3.open(fn, \"wb\") as f:\n        f.write(b\"data\")\n    assert s3.size(fn) == 4\n    s3.touch(fn, truncate=True)\n    assert s3.size(fn) == 0\n\n    # exists error\n    with s3.open(fn, \"wb\") as f:\n        f.write(b\"data\")\n    assert s3.size(fn) == 4\n    with pytest.raises(ValueError):\n        s3.touch(fn, truncate=False)\n    assert s3.size(fn) == 4\n\n\ndef test_touch_versions(s3):\n    versioned_file = versioned_bucket_name + \"/versioned_file\"\n    s3 = S3FileSystem(\n        anon=False, version_aware=True, client_kwargs={\"endpoint_url\": endpoint_uri}\n    )\n\n    with s3.open(versioned_file, \"wb\") as fo:\n        fo.write(b\"1\")\n    first_version = fo.version_id\n    with s3.open(versioned_file, \"wb\") as fo:\n        fo.write(b\"\")\n    second_version = fo.version_id\n\n    assert s3.isfile(versioned_file)\n    versions = s3.object_version_info(versioned_file)\n    assert len(versions) == 2\n    assert {version[\"VersionId\"] for version in versions} == {\n        first_version,\n        second_version,\n    }\n\n    with s3.open(versioned_file) as fo:\n        assert fo.version_id == second_version\n        assert fo.read() == b\"\"\n\n    with s3.open(versioned_file, version_id=first_version) as fo:\n        assert fo.version_id == first_version\n        assert fo.read() == b\"1\"\n\n\ndef test_cat_missing(s3):\n    fn0 = test_bucket_name + \"/file0\"\n    fn1 = test_bucket_name + \"/file1\"\n    s3.touch(fn0)\n    with pytest.raises(FileNotFoundError):\n        s3.cat([fn0, fn1], on_error=\"raise\")\n    out = s3.cat([fn0, fn1], on_error=\"omit\")\n    assert list(out) == [fn0]\n    out = s3.cat([fn0, fn1], on_error=\"return\")\n    assert fn1 in out\n    assert isinstance(out[fn1], FileNotFoundError)\n\n\ndef test_get_directories(s3, tmpdir):\n    s3.touch(test_bucket_name + \"/dir/dirkey/key0\")\n    s3.touch(test_bucket_name + \"/dir/dirkey/key1\")\n    s3.touch(test_bucket_name + \"/dir/dirkey\")\n    s3.touch(test_bucket_name + \"/dir/dir/key\")\n    d = str(tmpdir)\n\n    # Target directory with trailing slash\n    s3.get(test_bucket_name + \"/dir/\", d, recursive=True)\n    assert {\"dirkey\", \"dir\"} == set(os.listdir(d))\n    assert [\"key\"] == os.listdir(os.path.join(d, \"dir\"))\n    assert {\"key0\", \"key1\"} == set(os.listdir(os.path.join(d, \"dirkey\")))\n\n    local_fs = fsspec.filesystem(\"file\")\n    local_fs.rm(os.path.join(d, \"dir\"), recursive=True)\n    local_fs.rm(os.path.join(d, \"dirkey\"), recursive=True)\n\n    # Target directory without trailing slash\n    s3.get(test_bucket_name + \"/dir\", d, recursive=True)\n    assert [\"dir\"] == os.listdir(d)\n    assert {\"dirkey\", \"dir\"} == set(os.listdir(os.path.join(d, \"dir\")))\n    assert {\"key0\", \"key1\"} == set(os.listdir(os.path.join(d, \"dir\", \"dirkey\")))\n\n\ndef test_seek_reads(s3):\n    fn = test_bucket_name + \"/myfile\"\n    with s3.open(fn, \"wb\") as f:\n        f.write(b\"a\" * 175627146)\n    with s3.open(fn, \"rb\", blocksize=100) as f:\n        f.seek(175561610)\n        d1 = f.read(65536)\n\n        f.seek(4)\n        size = 17562198\n        d2 = f.read(size)\n        assert len(d2) == size\n\n        f.seek(17562288)\n        size = 17562187\n        d3 = f.read(size)\n        assert len(d3) == size\n\n\ndef test_connect_many(s3):\n    from multiprocessing.pool import ThreadPool\n\n    def task(i):\n        S3FileSystem(anon=False, client_kwargs={\"endpoint_url\": endpoint_uri}).ls(\"\")\n        return True\n\n    pool = ThreadPool(processes=20)\n    out = pool.map(task, range(40))\n    assert all(out)\n    pool.close()\n    pool.join()\n\n\ndef test_requester_pays(s3):\n    fn = test_bucket_name + \"/myfile\"\n    s3 = S3FileSystem(requester_pays=True, client_kwargs={\"endpoint_url\": endpoint_uri})\n    assert s3.req_kw[\"RequestPayer\"] == \"requester\"\n    s3.touch(fn)\n    with s3.open(fn, \"rb\") as f:\n        assert f.req_kw[\"RequestPayer\"] == \"requester\"\n\n\ndef test_credentials():\n    s3 = S3FileSystem(\n        key=\"foo\", secret=\"foo\", client_kwargs={\"endpoint_url\": endpoint_uri}\n    )\n    assert s3.s3._request_signer._credentials.access_key == \"foo\"\n    assert s3.s3._request_signer._credentials.secret_key == \"foo\"\n    s3 = S3FileSystem(\n        client_kwargs={\n            \"aws_access_key_id\": \"bar\",\n            \"aws_secret_access_key\": \"bar\",\n            \"endpoint_url\": endpoint_uri,\n        }\n    )\n    assert s3.s3._request_signer._credentials.access_key == \"bar\"\n    assert s3.s3._request_signer._credentials.secret_key == \"bar\"\n    s3 = S3FileSystem(\n        key=\"foo\",\n        client_kwargs={\"aws_secret_access_key\": \"bar\", \"endpoint_url\": endpoint_uri},\n    )\n    assert s3.s3._request_signer._credentials.access_key == \"foo\"\n    assert s3.s3._request_signer._credentials.secret_key == \"bar\"\n    s3 = S3FileSystem(\n        key=\"foobar\",\n        secret=\"foobar\",\n        client_kwargs={\n            \"aws_access_key_id\": \"foobar\",\n            \"aws_secret_access_key\": \"foobar\",\n            \"endpoint_url\": endpoint_uri,\n        },\n    )\n    assert s3.s3._request_signer._credentials.access_key == \"foobar\"\n    assert s3.s3._request_signer._credentials.secret_key == \"foobar\"\n    with pytest.raises((TypeError, KeyError)):\n        # should be TypeError: arg passed twice; but in moto can be KeyError\n        S3FileSystem(\n            key=\"foo\",\n            secret=\"foo\",\n            client_kwargs={\n                \"aws_access_key_id\": \"bar\",\n                \"aws_secret_access_key\": \"bar\",\n                \"endpoint_url\": endpoint_uri,\n            },\n        ).s3\n\n\ndef test_modified(s3):\n    dir_path = test_bucket_name + \"/modified\"\n    file_path = dir_path + \"/file\"\n\n    # Test file\n    s3.touch(file_path)\n    modified = s3.modified(path=file_path)\n    assert isinstance(modified, datetime.datetime)\n    assert modified.tzinfo is not None\n\n    # Test directory\n    with pytest.raises(IsADirectoryError):\n        modified = s3.modified(path=dir_path)\n\n    # Test bucket\n    with pytest.raises(IsADirectoryError):\n        s3.modified(path=test_bucket_name)\n\n\ndef test_async_s3(s3):\n    async def _():\n        s3 = S3FileSystem(\n            anon=False,\n            asynchronous=True,\n            loop=asyncio.get_running_loop(),\n            client_kwargs={\"region_name\": \"eu-central-1\", \"endpoint_url\": endpoint_uri},\n        )\n\n        fn = test_bucket_name + \"/nested/file1\"\n        data = b\"hello\\n\"\n\n        # Is good with or without connect()\n        await s3._cat_file(fn)\n\n        session = await s3.set_session()  # creates client\n\n        assert await s3._cat_file(fn) == data\n\n        assert await s3._cat_file(fn, start=0, end=3) == data[:3]\n\n        # TODO: file IO is *not* async\n        # with s3.open(fn, \"rb\") as f:\n        #     assert f.read() == data\n\n        try:\n            await session.close()\n        except AttributeError:\n            # bug in aiobotocore 1.4.1\n            await session._endpoint.http_session._session.close()\n\n    asyncio.run(_())\n\n\ndef test_cat_ranges(s3):\n    data = b\"a string to select from\"\n    fn = test_bucket_name + \"/parts\"\n    s3.pipe(fn, data)\n\n    assert s3.cat_file(fn) == data\n    assert s3.cat_file(fn, start=5) == data[5:]\n    assert s3.cat_file(fn, end=5) == data[:5]\n    assert s3.cat_file(fn, start=1, end=-1) == data[1:-1]\n    assert s3.cat_file(fn, start=-5) == data[-5:]\n\n\ndef test_async_s3_old(s3):\n    async def _():\n        s3 = S3FileSystem(\n            anon=False,\n            asynchronous=True,\n            loop=asyncio.get_running_loop(),\n            client_kwargs={\"region_name\": \"eu-central-1\", \"endpoint_url\": endpoint_uri},\n        )\n\n        fn = test_bucket_name + \"/nested/file1\"\n        data = b\"hello\\n\"\n\n        # Check old API\n        session = await s3._connect()\n        assert await s3._cat_file(fn, start=0, end=3) == data[:3]\n        try:\n            await session.close()\n        except AttributeError:\n            # bug in aiobotocore 1.4.1\n            await session._endpoint.http_session._session.close()\n\n    asyncio.run(_())\n\n\ndef test_via_fsspec(s3):\n    import fsspec\n\n    s3.mkdir(\"mine\")\n    with fsspec.open(\n        \"s3://mine/oi\", \"wb\", client_kwargs={\"endpoint_url\": endpoint_uri}\n    ) as f:\n        f.write(b\"hello\")\n    with fsspec.open(\n        \"s3://mine/oi\", \"rb\", client_kwargs={\"endpoint_url\": endpoint_uri}\n    ) as f:\n        assert f.read() == b\"hello\"\n\n\ndef test_repeat_exists(s3):\n    fn = \"s3://\" + test_bucket_name + \"/file1\"\n    s3.touch(fn)\n\n    assert s3.exists(fn)\n    assert s3.exists(fn)\n\n\ndef test_with_xzarr(s3):\n    da = pytest.importorskip(\"dask.array\")\n    xr = pytest.importorskip(\"xarray\")\n    name = \"sample\"\n\n    nana = xr.DataArray(da.random.random((1024, 1024, 10, 9, 1)))\n\n    s3_path = f\"{test_bucket_name}/{name}\"\n    s3store = s3.get_mapper(s3_path)\n\n    s3.ls(\"\")\n    nana.to_dataset().to_zarr(store=s3store, mode=\"w\", consolidated=True, compute=True)\n\n\ndef test_async_close():\n    async def _():\n        loop = asyncio.get_event_loop()\n        s3 = S3FileSystem(anon=False, asynchronous=True, loop=loop)\n        await s3._connect()\n\n        fn = test_bucket_name + \"/afile\"\n\n        async def async_wrapper():\n            coros = [\n                asyncio.ensure_future(s3._get_file(fn, \"/nonexistent/a/b/c\"), loop=loop)\n                for _ in range(3)\n            ]\n            completed, pending = await asyncio.wait(coros)\n            for future in completed:\n                with pytest.raises(OSError):\n                    future.result()\n\n        await asyncio.gather(*[async_wrapper() for __ in range(2)])\n        try:\n            await s3._s3.close()\n        except AttributeError:\n            # bug in aiobotocore 1.4.1\n            await s3._s3._endpoint.http_session._session.close()\n\n    asyncio.run(_())\n\n\ndef test_put_single(s3, tmpdir):\n    fn = os.path.join(str(tmpdir), \"dir\")\n    os.mkdir(fn)\n    open(os.path.join(fn, \"abc\"), \"w\").write(\"text\")\n\n    # Put with trailing slash\n    s3.put(fn + \"/\", test_bucket_name)  # no-op, no files\n    assert not s3.exists(test_bucket_name + \"/abc\")\n    assert not s3.exists(test_bucket_name + \"/dir\")\n\n    s3.put(fn + \"/\", test_bucket_name, recursive=True)\n    assert s3.cat(test_bucket_name + \"/abc\") == b\"text\"\n\n    # Put without trailing slash\n    s3.put(fn, test_bucket_name, recursive=True)\n    assert s3.cat(test_bucket_name + \"/dir/abc\") == b\"text\"\n\n\ndef test_shallow_find(s3):\n    \"\"\"Test that find method respects maxdepth.\n\n    Verify that the ``find`` method respects the ``maxdepth`` parameter.  With\n    ``maxdepth=1``, the results of ``find`` should be the same as those of\n    ``ls``, without returning subdirectories.  See also issue 378.\n    \"\"\"\n    ls_output = s3.ls(test_bucket_name)\n    assert sorted(ls_output + [test_bucket_name]) == s3.find(\n        test_bucket_name, maxdepth=1, withdirs=True\n    )\n    assert ls_output == s3.glob(test_bucket_name + \"/*\")\n\n\ndef test_multi_find(s3):\n    s3.mkdir(\"bucket/test\")\n    s3.mkdir(\"bucket/test/sub\")\n    s3.write_text(\"bucket/test/file.txt\", \"some_text\")\n    s3.write_text(\"bucket/test/sub/file.txt\", \"some_text\")\n\n    out1 = s3.find(\"bucket\", withdirs=True)\n    out2 = s3.find(\"bucket\", withdirs=True)\n    assert (\n        out1\n        == out2\n        == [\n            \"bucket/test\",\n            \"bucket/test/file.txt\",\n            \"bucket/test/sub\",\n            \"bucket/test/sub/file.txt\",\n        ]\n    )\n    out1 = s3.find(\"bucket\", withdirs=False)\n    out2 = s3.find(\"bucket\", withdirs=False)\n    assert out1 == out2 == [\"bucket/test/file.txt\", \"bucket/test/sub/file.txt\"]\n\n\ndef test_version_sizes(s3):\n    # protect against caching of incorrect version details\n    s3 = S3FileSystem(\n        anon=False, version_aware=True, client_kwargs={\"endpoint_url\": endpoint_uri}\n    )\n    import gzip\n\n    path = f\"s3://{versioned_bucket_name}/test.txt.gz\"\n    versions = [\n        s3.pipe_file(path, gzip.compress(text))\n        for text in (\n            b\"good morning!\",\n            b\"hello!\",\n            b\"hi!\",\n            b\"hello!\",\n        )\n    ]\n    for version in versions:\n        version_id = version[\"VersionId\"]\n        with s3.open(path, version_id=version_id) as f:\n            with gzip.GzipFile(fileobj=f) as zfp:\n                zfp.read()\n\n\ndef test_find_no_side_effect(s3):\n    infos1 = s3.find(test_bucket_name, maxdepth=1, withdirs=True, detail=True)\n    s3.find(test_bucket_name, maxdepth=None, withdirs=True, detail=True)\n    infos3 = s3.find(test_bucket_name, maxdepth=1, withdirs=True, detail=True)\n    assert infos1.keys() == infos3.keys()\n\n\ndef test_get_file_info_with_selector(s3):\n    fs = s3\n    base_dir = \"selector-dir/\"\n    file_a = \"selector-dir/test_file_a\"\n    file_b = \"selector-dir/test_file_b\"\n    dir_a = \"selector-dir/test_dir_a\"\n    file_c = \"selector-dir/test_dir_a/test_file_c\"\n\n    try:\n        fs.mkdir(base_dir)\n        with fs.open(file_a, mode=\"wb\"):\n            pass\n        with fs.open(file_b, mode=\"wb\"):\n            pass\n        fs.mkdir(dir_a)\n        with fs.open(file_c, mode=\"wb\"):\n            pass\n\n        infos = fs.find(base_dir, maxdepth=None, withdirs=True, detail=True)\n        assert len(infos) == 5  # includes base_dir directory\n\n        for info in infos.values():\n            if info[\"name\"].endswith(file_a):\n                assert info[\"type\"] == \"file\"\n            elif info[\"name\"].endswith(file_b):\n                assert info[\"type\"] == \"file\"\n            elif info[\"name\"].endswith(file_c):\n                assert info[\"type\"] == \"file\"\n            elif info[\"name\"].rstrip(\"/\").endswith(dir_a):\n                assert info[\"type\"] == \"directory\"\n    finally:\n        fs.rm(base_dir, recursive=True)\n\n\n@pytest.mark.xfail(\n    condition=version.parse(moto.__version__) <= version.parse(\"1.3.16\"),\n    reason=\"Moto 1.3.16 is not supporting pre-conditions.\",\n)\ndef test_raise_exception_when_file_has_changed_during_reading(s3):\n    test_file_name = \"file1\"\n    test_file = \"s3://\" + test_bucket_name + \"/\" + test_file_name\n    content1 = b\"123\"\n    content2 = b\"ABCDEFG\"\n\n    boto3_client = get_boto3_client()\n\n    def create_file(content: bytes):\n        boto3_client.put_object(\n            Bucket=test_bucket_name, Key=test_file_name, Body=content\n        )\n\n    create_file(b\"123\")\n\n    with s3.open(test_file, \"rb\") as f:\n        content = f.read()\n        assert content == content1\n\n    with s3.open(test_file, \"rb\") as f:\n        create_file(content2)\n        with expect_errno(errno.EBUSY):\n            f.read()\n\n\ndef test_s3fs_etag_preserving_multipart_copy(monkeypatch, s3):\n    # Set this to a lower value so that we can actually\n    # test this without creating giant objects in memory\n    monkeypatch.setattr(s3fs.core, \"MANAGED_COPY_THRESHOLD\", 5 * 2**20)\n\n    test_file1 = test_bucket_name + \"/test/multipart-upload.txt\"\n    test_file2 = test_bucket_name + \"/test/multipart-upload-copy.txt\"\n\n    with s3.open(test_file1, \"wb\", block_size=5 * 2**21) as stream:\n        for _ in range(5):\n            stream.write(b\"b\" * (stream.blocksize + random.randrange(200)))\n\n    file_1 = s3.info(test_file1)\n\n    s3.copy(test_file1, test_file2)\n    file_2 = s3.info(test_file2)\n    s3.rm(test_file2)\n\n    # normal copy() uses a block size of 5GB\n    assert file_1[\"ETag\"] != file_2[\"ETag\"]\n\n    s3.copy(test_file1, test_file2, preserve_etag=True)\n    file_2 = s3.info(test_file2)\n    s3.rm(test_file2)\n\n    # etag preserving copy() determines each part size for the destination\n    # by checking out the matching part's size on the source\n    assert file_1[\"ETag\"] == file_2[\"ETag\"]\n\n    s3.rm(test_file1)\n\n\ndef test_sync_from_wihin_async(s3):\n    # if treating as sync but within an even loop, e.g., calling from jupyter;\n    # IO happens on dedicated thread.\n    async def f():\n        S3FileSystem.clear_instance_cache()\n        s3 = S3FileSystem(anon=False, client_kwargs={\"endpoint_url\": endpoint_uri})\n        assert s3.ls(test_bucket_name)\n\n    asyncio.run(f())\n\n\ndef test_token_paths(s3):\n    fs, tok, files = fsspec.get_fs_token_paths(\n        \"s3://\" + test_bucket_name + \"/*.csv\",\n        storage_options={\"client_kwargs\": {\"endpoint_url\": endpoint_uri}},\n    )\n    assert files\n\n\ndef test_same_name_but_no_exact(s3):\n    s3.touch(test_bucket_name + \"/very/similar/prefix1\")\n    s3.touch(test_bucket_name + \"/very/similar/prefix2\")\n    s3.touch(test_bucket_name + \"/very/similar/prefix3/something\")\n    assert not s3.exists(test_bucket_name + \"/very/similar/prefix\")\n    assert not s3.exists(test_bucket_name + \"/very/similar/prefi\")\n    assert not s3.exists(test_bucket_name + \"/very/similar/pref\")\n\n    assert s3.exists(test_bucket_name + \"/very/similar/\")\n    assert s3.exists(test_bucket_name + \"/very/similar/prefix1\")\n    assert s3.exists(test_bucket_name + \"/very/similar/prefix2\")\n    assert s3.exists(test_bucket_name + \"/very/similar/prefix3\")\n    assert s3.exists(test_bucket_name + \"/very/similar/prefix3/\")\n    assert s3.exists(test_bucket_name + \"/very/similar/prefix3/something\")\n\n    assert not s3.exists(test_bucket_name + \"/very/similar/prefix3/some\")\n\n    s3.touch(test_bucket_name + \"/starting/very/similar/prefix\")\n\n    assert not s3.exists(test_bucket_name + \"/starting/very/similar/prefix1\")\n    assert not s3.exists(test_bucket_name + \"/starting/very/similar/prefix2\")\n    assert not s3.exists(test_bucket_name + \"/starting/very/similar/prefix3\")\n    assert not s3.exists(test_bucket_name + \"/starting/very/similar/prefix3/\")\n    assert not s3.exists(test_bucket_name + \"/starting/very/similar/prefix3/something\")\n\n    assert s3.exists(test_bucket_name + \"/starting/very/similar/prefix\")\n    assert s3.exists(test_bucket_name + \"/starting/very/similar/prefix/\")\n\n\ndef test_leading_forward_slash(s3):\n    s3.touch(test_bucket_name + \"/some/file\")\n    assert s3.ls(test_bucket_name + \"/some/\")\n    assert s3.exists(test_bucket_name + \"/some/file\")\n    assert s3.exists(\"s3://\" + test_bucket_name + \"/some/file\")\n\n\ndef test_lsdir(s3):\n    # https://github.com/fsspec/s3fs/issues/475\n    s3.find(test_bucket_name)\n\n    d = test_bucket_name + \"/test\"\n    assert d in s3.ls(test_bucket_name)\n\n\ndef test_rm_recursive_folder(s3):\n    s3.touch(test_bucket_name + \"/sub/file\")\n    s3.rm(test_bucket_name + \"/sub\", recursive=True)\n    assert not s3.exists(test_bucket_name + \"/sub/file\")\n    assert not s3.exists(test_bucket_name + \"/sub\")\n\n    s3.touch(test_bucket_name + \"/sub/file\")\n    s3.touch(test_bucket_name + \"/sub/\")  # placeholder\n    s3.rm(test_bucket_name + \"/sub\", recursive=True)\n    assert not s3.exists(test_bucket_name + \"/sub/file\")\n    assert not s3.exists(test_bucket_name + \"/sub\")\n\n    s3.touch(test_bucket_name + \"/sub/file\")\n    s3.rm(test_bucket_name, recursive=True)\n    assert not s3.exists(test_bucket_name + \"/sub/file\")\n    assert not s3.exists(test_bucket_name + \"/sub\")\n    assert not s3.exists(test_bucket_name)\n\n\ndef test_copy_file_without_etag(s3, monkeypatch):\n\n    s3.touch(test_bucket_name + \"/copy_tests/file\")\n    s3.ls(test_bucket_name + \"/copy_tests/\")\n\n    [file] = s3.dircache[test_bucket_name + \"/copy_tests\"]\n\n    assert file[\"name\"] == test_bucket_name + \"/copy_tests/file\"\n    file.pop(\"ETag\")\n\n    assert s3.info(file[\"name\"]).get(\"ETag\", None) is None\n\n    s3.cp_file(file[\"name\"], test_bucket_name + \"/copy_tests/file2\")\n    assert s3.info(test_bucket_name + \"/copy_tests/file2\")[\"ETag\"] is not None\n\n\ndef test_find_with_prefix(s3):\n    for cursor in range(100):\n        s3.touch(test_bucket_name + f\"/prefixes/test_{cursor}\")\n\n    s3.touch(test_bucket_name + \"/prefixes2\")\n    assert len(s3.find(test_bucket_name + \"/prefixes\")) == 100\n    assert len(s3.find(test_bucket_name, prefix=\"prefixes\")) == 101\n\n    assert len(s3.find(test_bucket_name + \"/prefixes/test_\")) == 0\n    assert len(s3.find(test_bucket_name + \"/prefixes\", prefix=\"test_\")) == 100\n    assert len(s3.find(test_bucket_name + \"/prefixes/\", prefix=\"test_\")) == 100\n\n    test_1s = s3.find(test_bucket_name + \"/prefixes/test_1\")\n    assert len(test_1s) == 1\n    assert test_1s[0] == test_bucket_name + \"/prefixes/test_1\"\n\n    test_1s = s3.find(test_bucket_name + \"/prefixes/\", prefix=\"test_1\")\n    assert len(test_1s) == 11\n    assert test_1s == [test_bucket_name + \"/prefixes/test_1\"] + [\n        test_bucket_name + f\"/prefixes/test_{cursor}\" for cursor in range(10, 20)\n    ]\n    assert s3.find(test_bucket_name + \"/prefixes/\") == s3.find(\n        test_bucket_name + \"/prefixes/\", prefix=None\n    )\n\n\ndef test_list_after_find(s3):\n    before = s3.ls(\"s3://test\")\n    s3.invalidate_cache(\"s3://test/2014-01-01.csv\")\n    s3.find(\"s3://test/2014-01-01.csv\")\n    after = s3.ls(\"s3://test\")\n    assert before == after\n\n\ndef test_upload_recursive_to_bucket(s3, tmpdir):\n    # GH#491\n    folders = [os.path.join(tmpdir, d) for d in [\"outer\", \"outer/inner\"]]\n    files = [os.path.join(tmpdir, f) for f in [\"outer/afile\", \"outer/inner/bfile\"]]\n    for d in folders:\n        os.mkdir(d)\n    for f in files:\n        open(f, \"w\").write(\"hello\")\n    s3.put(folders[0], \"newbucket\", recursive=True)\n\n\ndef test_rm_file(s3):\n    target = test_bucket_name + \"/to_be_removed/file\"\n    s3.touch(target)\n    s3.rm_file(target)\n    assert not s3.exists(target)\n    assert not s3.exists(test_bucket_name + \"/to_be_removed\")\n\n\ndef test_exists_isdir(s3):\n    bad_path = \"s3://nyc-tlc-asdfasdf/trip data/\"\n    assert not s3.exists(bad_path)\n    assert not s3.isdir(bad_path)\n\n\ndef test_list_del_multipart(s3):\n    path = test_bucket_name + \"/afile\"\n    f = s3.open(path, \"wb\")\n    f.write(b\"0\" * 6 * 2**20)\n\n    out = s3.list_multipart_uploads(test_bucket_name)\n    assert [_ for _ in out if _[\"Key\"] == \"afile\"]\n\n    s3.clear_multipart_uploads(test_bucket_name)\n    out = s3.list_multipart_uploads(test_bucket_name)\n    assert not [_ for _ in out if _[\"Key\"] == \"afile\"]\n\n    try:\n        f.close()  # may error\n    except Exception:\n        pass\n\n\ndef test_split_path(s3):\n    buckets = [\n        \"my-test-bucket\",\n        \"arn:aws:s3:region:123456789012:accesspoint/my-access-point-name\",\n        \"arn:aws:s3-outposts:region:123456789012:outpost/outpost-id/bucket/my-test-bucket\",\n        \"arn:aws:s3-outposts:region:123456789012:outpost/outpost-id/accesspoint/my-accesspoint-name\",\n        \"arn:aws:s3-object-lambda:region:123456789012:accesspoint/my-lambda-object-name\",\n    ]\n    test_key = \"my/test/path\"\n    for test_bucket in buckets:\n        bucket, key, _ = s3.split_path(\"s3://\" + test_bucket + \"/\" + test_key)\n        assert bucket == test_bucket\n        assert key == test_key\n\n\ndef test_cp_directory_recursive(s3):\n    src = test_bucket_name + \"/src\"\n    src_file = src + \"/file\"\n    s3.mkdir(src)\n    s3.touch(src_file)\n\n    target = test_bucket_name + \"/target\"\n\n    # cp without slash\n    assert not s3.exists(target)\n    for loop in range(2):\n        s3.cp(src, target, recursive=True)\n        assert s3.isdir(target)\n\n        if loop == 0:\n            correct = [target + \"/file\"]\n            assert s3.find(target) == correct\n        else:\n            correct = [target + \"/file\", target + \"/src/file\"]\n            assert sorted(s3.find(target)) == correct\n\n    s3.rm(target, recursive=True)\n\n    # cp with slash\n    assert not s3.exists(target)\n    for loop in range(2):\n        s3.cp(src + \"/\", target, recursive=True)\n        assert s3.isdir(target)\n        correct = [target + \"/file\"]\n        assert s3.find(target) == correct\n\n\ndef test_get_directory_recursive(s3, tmpdir):\n    src = test_bucket_name + \"/src\"\n    src_file = src + \"/file\"\n    s3.mkdir(src)\n    s3.touch(src_file)\n\n    target = os.path.join(tmpdir, \"target\")\n    target_fs = fsspec.filesystem(\"file\")\n\n    # get without slash\n    assert not target_fs.exists(target)\n    for loop in range(2):\n        s3.get(src, target, recursive=True)\n        assert target_fs.isdir(target)\n\n        if loop == 0:\n            assert target_fs.find(target) == [os.path.join(target, \"file\")]\n        else:\n            assert sorted(target_fs.find(target)) == [\n                os.path.join(target, \"file\"),\n                os.path.join(target, \"src\", \"file\"),\n            ]\n\n    target_fs.rm(target, recursive=True)\n\n    # get with slash\n    assert not target_fs.exists(target)\n    for loop in range(2):\n        s3.get(src + \"/\", target, recursive=True)\n        assert target_fs.isdir(target)\n        assert target_fs.find(target) == [os.path.join(target, \"file\")]\n\n\ndef test_put_directory_recursive(s3, tmpdir):\n    src = os.path.join(tmpdir, \"src\")\n    src_file = os.path.join(src, \"file\")\n    source_fs = fsspec.filesystem(\"file\")\n    source_fs.mkdir(src)\n    source_fs.touch(src_file)\n\n    target = test_bucket_name + \"/target\"\n\n    # put without slash\n    assert not s3.exists(target)\n    for loop in range(2):\n        s3.put(src, target, recursive=True)\n        assert s3.isdir(target)\n\n        if loop == 0:\n            assert s3.find(target) == [target + \"/file\"]\n        else:\n            assert sorted(s3.find(target)) == [target + \"/file\", target + \"/src/file\"]\n\n    s3.rm(target, recursive=True)\n\n    # put with slash\n    assert not s3.exists(target)\n    for loop in range(2):\n        s3.put(src + \"/\", target, recursive=True)\n        assert s3.isdir(target)\n        assert s3.find(target) == [target + \"/file\"]\n\n\ndef test_cp_two_files(s3):\n    src = test_bucket_name + \"/src\"\n    file0 = src + \"/file0\"\n    file1 = src + \"/file1\"\n    s3.mkdir(src)\n    s3.touch(file0)\n    s3.touch(file1)\n\n    target = test_bucket_name + \"/target\"\n    assert not s3.exists(target)\n\n    s3.cp([file0, file1], target)\n\n    assert s3.isdir(target)\n    assert sorted(s3.find(target)) == [\n        target + \"/file0\",\n        target + \"/file1\",\n    ]\n\n\ndef test_async_stream(s3_base):\n    fn = test_bucket_name + \"/target\"\n    data = b\"hello world\" * 1000\n    out = []\n\n    async def read_stream():\n        fs = S3FileSystem(\n            anon=False,\n            client_kwargs={\"endpoint_url\": endpoint_uri},\n            skip_instance_cache=True,\n        )\n        await fs._mkdir(test_bucket_name)\n        await fs._pipe(fn, data)\n        f = await fs.open_async(fn, mode=\"rb\", block_seze=1000)\n        while True:\n            got = await f.read(1000)\n            assert f.size == len(data)\n            assert f.tell()\n            if not got:\n                break\n            out.append(got)\n\n    asyncio.run(read_stream())\n    assert b\"\".join(out) == data\n\n\ndef test_rm_invalidates_cache(s3):\n    # Issue 761: rm_file does not invalidate cache\n    fn = test_bucket_name + \"/2014-01-01.csv\"\n    assert s3.exists(fn)\n    assert fn in s3.ls(test_bucket_name)\n    s3.rm(fn)\n    assert not s3.exists(fn)\n    assert fn not in s3.ls(test_bucket_name)\n\n    fn = test_bucket_name + \"/2014-01-02.csv\"\n    assert s3.exists(fn)\n    assert fn in s3.ls(test_bucket_name)\n    s3.rm_file(fn)\n    assert not s3.exists(fn)\n    assert fn not in s3.ls(test_bucket_name)\n\n\ndef test_cache_handles_find_with_maxdepth(s3):\n    # Issue 773: invalidate_cache should not be needed when find is called with different maxdepth\n    base_name = test_bucket_name + \"/main\"\n    dir = base_name + \"/dir1/fileB\"\n    file = base_name + \"/fileA\"\n    s3.touch(dir)\n    s3.touch(file)\n\n    # Find with maxdepth=None\n    f = s3.find(base_name, maxdepth=None, withdirs=False)\n    assert base_name + \"/fileA\" in f\n    assert base_name + \"/dir1\" not in f\n    assert base_name + \"/dir1/fileB\" in f\n\n    # Find with maxdepth=1.\n    # Performed twice with cache invalidated between them which should give same result\n    for _ in range(2):\n        f = s3.find(base_name, maxdepth=1, withdirs=True)\n        assert base_name + \"/fileA\" in f\n        assert base_name + \"/dir1\" in f\n        assert base_name + \"/dir1/fileB\" not in f\n\n        s3.invalidate_cache()\n\n\ndef test_bucket_versioning(s3):\n    s3.mkdir(\"maybe_versioned\")\n    assert not s3.is_bucket_versioned(\"maybe_versioned\")\n    s3.make_bucket_versioned(\"maybe_versioned\")\n    assert s3.is_bucket_versioned(\"maybe_versioned\")\n    s3.make_bucket_versioned(\"maybe_versioned\", False)\n    assert not s3.is_bucket_versioned(\"maybe_versioned\")\n", "s3fs/tests/__init__.py": "", "s3fs/tests/test_utils.py": "import s3fs.utils as utils\n\n\ndef test_get_brange():\n    assert list(utils._get_brange(100, 24)) == [\n        (0, 23),\n        (24, 47),\n        (48, 71),\n        (72, 95),\n        (96, 99),\n    ]\n    assert list(utils._get_brange(100, 25)) == [(0, 24), (25, 49), (50, 74), (75, 99)]\n    assert list(utils._get_brange(100, 26)) == [(0, 25), (26, 51), (52, 77), (78, 99)]\n", "s3fs/tests/test_mapping.py": "import pytest\nfrom s3fs.tests.test_s3fs import s3_base, s3, test_bucket_name\nfrom s3fs import S3Map, S3FileSystem\n\nroot = test_bucket_name + \"/mapping\"\n\n\ndef test_simple(s3):\n    d = s3.get_mapper(root)\n    assert not d\n\n    assert list(d) == list(d.keys()) == []\n    assert list(d.values()) == []\n    assert list(d.items()) == []\n    s3.get_mapper(root)\n\n\ndef test_default_s3filesystem(s3):\n    d = s3.get_mapper(root)\n    assert d.fs is s3\n\n\ndef test_errors(s3):\n    d = s3.get_mapper(root)\n    with pytest.raises(KeyError):\n        d[\"nonexistent\"]\n\n    try:\n        s3.get_mapper(\"does-not-exist\", check=True)\n    except Exception as e:\n        assert \"does-not-exist\" in str(e)\n\n\ndef test_with_data(s3):\n    d = s3.get_mapper(root)\n    d[\"x\"] = b\"123\"\n    assert list(d) == list(d.keys()) == [\"x\"]\n    assert list(d.values()) == [b\"123\"]\n    assert list(d.items()) == [(\"x\", b\"123\")]\n    assert d[\"x\"] == b\"123\"\n    assert bool(d)\n\n    assert s3.find(root) == [test_bucket_name + \"/mapping/x\"]\n    d[\"x\"] = b\"000\"\n    assert d[\"x\"] == b\"000\"\n\n    d[\"y\"] = b\"456\"\n    assert d[\"y\"] == b\"456\"\n    assert set(d) == {\"x\", \"y\"}\n\n    d.clear()\n    assert list(d) == []\n\n\ndef test_complex_keys(s3):\n    d = s3.get_mapper(root)\n    d[1] = b\"hello\"\n    assert d[1] == b\"hello\"\n    del d[1]\n\n    d[1, 2] = b\"world\"\n    assert d[1, 2] == b\"world\"\n    del d[1, 2]\n\n    d[\"x\", 1, 2] = b\"hello world\"\n    assert d[\"x\", 1, 2] == b\"hello world\"\n\n    assert (\"x\", 1, 2) in d\n\n\ndef test_clear_empty(s3):\n    d = s3.get_mapper(root)\n    d.clear()\n    assert list(d) == []\n    d[1] = b\"1\"\n    assert list(d) == [\"1\"]\n    d.clear()\n    assert list(d) == []\n\n\ndef test_no_dircache(s3):\n    from s3fs.tests.test_s3fs import endpoint_uri\n    import fsspec\n\n    d = fsspec.get_mapper(\n        \"s3://\" + root,\n        anon=False,\n        client_kwargs={\"endpoint_url\": endpoint_uri},\n        use_listings_cache=False,\n    )\n    d.clear()\n    assert list(d) == []\n    d[1] = b\"1\"\n    assert list(d) == [\"1\"]\n    d.clear()\n    assert list(d) == []\n\n\ndef test_pickle(s3):\n    d = s3.get_mapper(root)\n    d[\"x\"] = b\"1\"\n\n    import pickle\n\n    d2 = pickle.loads(pickle.dumps(d))\n\n    assert d2[\"x\"] == b\"1\"\n\n\ndef test_array(s3):\n    from array import array\n\n    d = s3.get_mapper(root)\n    d[\"x\"] = array(\"B\", [65] * 1000)\n\n    assert d[\"x\"] == b\"A\" * 1000\n\n\ndef test_bytearray(s3):\n    d = s3.get_mapper(root)\n    d[\"x\"] = bytearray(b\"123\")\n\n    assert d[\"x\"] == b\"123\"\n\n\ndef test_new_bucket(s3):\n    try:\n        s3.get_mapper(\"new-bucket\", check=True)\n        assert False\n    except ValueError as e:\n        assert \"create\" in str(e)\n\n    d = s3.get_mapper(\"new-bucket\", create=True)\n    assert not d\n\n    d = s3.get_mapper(\"new-bucket/new-directory\")\n    assert not d\n\n\ndef test_old_api(s3):\n    import fsspec.mapping\n\n    assert isinstance(S3Map(root, s3), fsspec.mapping.FSMap)\n", "s3fs/tests/derived/s3fs_test.py": "import fsspec.tests.abstract as abstract\nfrom s3fs.tests.derived.s3fs_fixtures import S3fsFixtures\n\n\nclass TestS3fsCopy(abstract.AbstractCopyTests, S3fsFixtures):\n    pass\n\n\nclass TestS3fsGet(abstract.AbstractGetTests, S3fsFixtures):\n    pass\n\n\nclass TestS3fsPut(abstract.AbstractPutTests, S3fsFixtures):\n    pass\n", "s3fs/tests/derived/s3fs_fixtures.py": "import json\nimport os\nimport pytest\nimport requests\nimport time\n\nfrom fsspec.tests.abstract import AbstractFixtures\nfrom s3fs.core import S3FileSystem\n\n\ntest_bucket_name = \"test\"\nsecure_bucket_name = \"test-secure\"\nversioned_bucket_name = \"test-versioned\"\nport = 5555\nendpoint_uri = \"http://127.0.0.1:%s/\" % port\n\n\nclass S3fsFixtures(AbstractFixtures):\n    @pytest.fixture(scope=\"class\")\n    def fs(self, _s3_base, _get_boto3_client):\n        client = _get_boto3_client\n        client.create_bucket(Bucket=test_bucket_name, ACL=\"public-read\")\n\n        client.create_bucket(Bucket=versioned_bucket_name, ACL=\"public-read\")\n        client.put_bucket_versioning(\n            Bucket=versioned_bucket_name, VersioningConfiguration={\"Status\": \"Enabled\"}\n        )\n\n        # initialize secure bucket\n        client.create_bucket(Bucket=secure_bucket_name, ACL=\"public-read\")\n        policy = json.dumps(\n            {\n                \"Version\": \"2012-10-17\",\n                \"Id\": \"PutObjPolicy\",\n                \"Statement\": [\n                    {\n                        \"Sid\": \"DenyUnEncryptedObjectUploads\",\n                        \"Effect\": \"Deny\",\n                        \"Principal\": \"*\",\n                        \"Action\": \"s3:PutObject\",\n                        \"Resource\": \"arn:aws:s3:::{bucket_name}/*\".format(\n                            bucket_name=secure_bucket_name\n                        ),\n                        \"Condition\": {\n                            \"StringNotEquals\": {\n                                \"s3:x-amz-server-side-encryption\": \"aws:kms\"\n                            }\n                        },\n                    }\n                ],\n            }\n        )\n        client.put_bucket_policy(Bucket=secure_bucket_name, Policy=policy)\n\n        S3FileSystem.clear_instance_cache()\n        s3 = S3FileSystem(anon=False, client_kwargs={\"endpoint_url\": endpoint_uri})\n        s3.invalidate_cache()\n        yield s3\n\n    @pytest.fixture\n    def fs_path(self):\n        return test_bucket_name\n\n    @pytest.fixture\n    def supports_empty_directories(self):\n        return False\n\n    @pytest.fixture(scope=\"class\")\n    def _get_boto3_client(self):\n        from botocore.session import Session\n\n        # NB: we use the sync botocore client for setup\n        session = Session()\n        return session.create_client(\"s3\", endpoint_url=endpoint_uri)\n\n    @pytest.fixture(scope=\"class\")\n    def _s3_base(self):\n        # writable local S3 system\n        import shlex\n        import subprocess\n\n        try:\n            # should fail since we didn't start server yet\n            r = requests.get(endpoint_uri)\n        except:\n            pass\n        else:\n            if r.ok:\n                raise RuntimeError(\"moto server already up\")\n        if \"AWS_SECRET_ACCESS_KEY\" not in os.environ:\n            os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"foo\"\n        if \"AWS_ACCESS_KEY_ID\" not in os.environ:\n            os.environ[\"AWS_ACCESS_KEY_ID\"] = \"foo\"\n        proc = subprocess.Popen(\n            shlex.split(\"moto_server s3 -p %s\" % port),\n            stderr=subprocess.DEVNULL,\n            stdout=subprocess.DEVNULL,\n            stdin=subprocess.DEVNULL,\n        )\n\n        timeout = 5\n        while timeout > 0:\n            try:\n                print(\"polling for moto server\")\n                r = requests.get(endpoint_uri)\n                if r.ok:\n                    break\n            except:\n                pass\n            timeout -= 0.1\n            time.sleep(0.1)\n        print(\"server up\")\n        yield\n        print(\"moto done\")\n        proc.terminate()\n        proc.wait()\n", "s3fs/tests/derived/__init__.py": "", "docs/source/conf.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# S3Fs documentation build configuration file, created by\n# sphinx-quickstart on Mon Mar 21 15:20:01 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath('.'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.todo',\n    'sphinx.ext.ifconfig',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.extlinks',\n    'sphinx.ext.napoleon',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = 'S3Fs'\ncopyright = '2016, Continuum Analytics'\nauthor = 'Continuum Analytics'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nimport s3fs\nversion = s3fs.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = version\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\nextlinks = {\n    \"pr\": (\"https://github.com/fsspec/s3fs/pull/%s\", \"PR #%s\"),\n}\n\n\n# -- Options for HTML output ----------------------------------------------\n\nhtml_theme = 'sphinx_rtd_theme'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# Custom CSS file to override read the docs default CSS.\n# Contains workaround for issue #790.\nhtml_css_files = [\"custom.css\"]\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   'da', 'de', 'en', 'es', 'fi', 'fr', 'h', 'it', 'ja'\n#   'nl', 'no', 'pt', 'ro', 'r', 'sv', 'tr'\n#html_search_language = 'en'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only 'ja' uses this config value\n#html_search_options = {'type': 'default'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = 'scorer.js'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'S3Fsdoc'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size ('letterpaper' or 'a4paper').\n#'papersize': 'letterpaper',\n\n# The font size ('10pt', '11pt' or '12pt').\n#'pointsize': '10pt',\n\n# Additional stuff for the LaTeX preamble.\n#'preamble': '',\n\n# Latex figure (float) alignment\n#'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, 'S3Fs.tex', 'S3Fs Documentation',\n     'Continuum Analytics', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 's3fs', 'S3Fs Documentation',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, 'S3Fs', 'S3Fs Documentation',\n     author, 'S3Fs', 'One line description of project.',\n     'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n#texinfo_no_detailmenu = False\n"}