{"cmake-format.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# cmake-format configuration file\n# Use `archery lint --cmake-format --fix` to reformat all cmake files in the\n# source tree\n\n# -----------------------------\n# Options affecting formatting.\n# -----------------------------\nwith section(\"format\"):\n    # How wide to allow formatted cmake files\n    line_width = 90\n\n    # How many spaces to tab for indent\n    tab_size = 2\n\n    # If a positional argument group contains more than this many arguments,\n    # then force it to a vertical layout.\n    max_pargs_hwrap = 4\n\n    # If the statement spelling length (including space and parenthesis) is\n    # smaller than this amount, then force reject nested layouts.\n    # This value only comes into play when considering whether or not to nest\n    # arguments below their parent. If the number of characters in the parent\n    # is less than this value, we will not nest.\n    min_prefix_chars = 32\n\n    # If true, separate flow control names from their parentheses with a space\n    separate_ctrl_name_with_space = False\n\n    # If true, separate function names from parentheses with a space\n    separate_fn_name_with_space = False\n\n    # If a statement is wrapped to more than one line, than dangle the closing\n    # parenthesis on it's own line\n    dangle_parens = False\n\n    # What style line endings to use in the output.\n    line_ending = 'unix'\n\n    # Format command names consistently as 'lower' or 'upper' case\n    command_case = 'lower'\n\n    # Format keywords consistently as 'lower' or 'upper' case\n    keyword_case = 'unchanged'\n\n# ------------------------------------------------\n# Options affecting comment reflow and formatting.\n# ------------------------------------------------\nwith section(\"markup\"):\n    # enable comment markup parsing and reflow\n    enable_markup = False\n\n    # If comment markup is enabled, don't reflow the first comment block in\n    # eachlistfile. Use this to preserve formatting of your\n    # copyright/licensestatements.\n    first_comment_is_literal = True\n\n    # If comment markup is enabled, don't reflow any comment block which\n    # matches this (regex) pattern. Default is `None` (disabled).\n    literal_comment_pattern = None\n", "java/c/src/test/python/integration_tests.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport decimal\nimport gc\nimport os\nimport sys\nimport unittest\nimport xml.etree.ElementTree as ET\n\nimport jpype\nimport pyarrow as pa\nimport pyarrow.ipc as ipc\nfrom pyarrow.cffi import ffi\n\n\ndef setup_jvm():\n    # This test requires Arrow Java to be built in the same source tree\n    try:\n        arrow_dir = os.environ[\"ARROW_SOURCE_DIR\"]\n    except KeyError:\n        arrow_dir = os.path.join(os.path.dirname(\n            __file__), '..', '..', '..', '..', '..')\n    pom_path = os.path.join(arrow_dir, 'java', 'pom.xml')\n    tree = ET.parse(pom_path)\n    version = tree.getroot().find(\n        'POM:version',\n        namespaces={\n            'POM': 'http://maven.apache.org/POM/4.0.0'\n        }).text\n    jar_path = os.path.join(\n        arrow_dir, 'java', 'tools', 'target',\n        'arrow-tools-{}-jar-with-dependencies.jar'.format(version))\n    jar_path = os.getenv(\"ARROW_TOOLS_JAR\", jar_path)\n    jar_path += \":{}\".format(os.path.join(arrow_dir,\n                                          \"java\", \"c/target/arrow-c-data-{}.jar\".format(version)))\n    kwargs = {}\n    # This will be the default behaviour in jpype 0.8+\n    kwargs['convertStrings'] = False\n\n    # For debugging purpose please uncomment the following, and include *jvm_args, before **kwargs\n    # in startJVM function call\n    # jvm_args = [\n    #     \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005\"\n    # ]\n\n    jpype.startJVM(jpype.getDefaultJVMPath(), \"-Djava.class.path=\" + jar_path, **kwargs)\n\n\nclass Bridge:\n    def __init__(self):\n        self.java_allocator = jpype.JPackage(\n            \"org\").apache.arrow.memory.RootAllocator(sys.maxsize)\n        self.java_c = jpype.JPackage(\"org\").apache.arrow.c\n\n    def java_to_python_field(self, jfield):\n        c_schema = ffi.new(\"struct ArrowSchema*\")\n        ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n        self.java_c.Data.exportField(self.java_allocator, jfield, None,\n                                     self.java_c.ArrowSchema.wrap(ptr_schema))\n        return pa.Field._import_from_c(ptr_schema)\n\n    def java_to_python_array(self, vector, dictionary_provider=None):\n        c_schema = ffi.new(\"struct ArrowSchema*\")\n        ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n        c_array = ffi.new(\"struct ArrowArray*\")\n        ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n        self.java_c.Data.exportVector(self.java_allocator, vector, dictionary_provider, self.java_c.ArrowArray.wrap(\n            ptr_array), self.java_c.ArrowSchema.wrap(ptr_schema))\n        return pa.Array._import_from_c(ptr_array, ptr_schema)\n\n    def java_to_python_record_batch(self, root):\n        c_schema = ffi.new(\"struct ArrowSchema*\")\n        ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n        c_array = ffi.new(\"struct ArrowArray*\")\n        ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n        self.java_c.Data.exportVectorSchemaRoot(self.java_allocator, root, None, self.java_c.ArrowArray.wrap(\n            ptr_array), self.java_c.ArrowSchema.wrap(ptr_schema))\n        return pa.RecordBatch._import_from_c(ptr_array, ptr_schema)\n\n    def java_to_python_reader(self, reader):\n        c_stream = ffi.new(\"struct ArrowArrayStream*\")\n        ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))\n        self.java_c.Data.exportArrayStream(self.java_allocator, reader,\n                                           self.java_c.ArrowArrayStream.wrap(ptr_stream))\n        return pa.RecordBatchReader._import_from_c(ptr_stream)\n\n    def python_to_java_field(self, field):\n        c_schema = self.java_c.ArrowSchema.allocateNew(self.java_allocator)\n        field._export_to_c(c_schema.memoryAddress())\n        return self.java_c.Data.importField(self.java_allocator, c_schema, None)\n\n    def python_to_java_array(self, array, dictionary_provider=None):\n        c_schema = self.java_c.ArrowSchema.allocateNew(self.java_allocator)\n        c_array = self.java_c.ArrowArray.allocateNew(self.java_allocator)\n        array._export_to_c(c_array.memoryAddress(), c_schema.memoryAddress())\n        return self.java_c.Data.importVector(self.java_allocator, c_array, c_schema, dictionary_provider)\n\n    def python_to_java_record_batch(self, record_batch):\n        c_schema = self.java_c.ArrowSchema.allocateNew(self.java_allocator)\n        c_array = self.java_c.ArrowArray.allocateNew(self.java_allocator)\n        record_batch._export_to_c(\n            c_array.memoryAddress(), c_schema.memoryAddress())\n        return self.java_c.Data.importVectorSchemaRoot(self.java_allocator, c_array, c_schema, None)\n\n    def python_to_java_reader(self, reader):\n        c_stream = self.java_c.ArrowArrayStream.allocateNew(self.java_allocator)\n        reader._export_to_c(c_stream.memoryAddress())\n        return self.java_c.Data.importArrayStream(self.java_allocator, c_stream)\n\n    def close(self):\n        self.java_allocator.close()\n\n\nclass TestPythonIntegration(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls) -> None:\n        setup_jvm()\n\n    def setUp(self):\n        gc.collect()\n        self.old_allocated_python = pa.total_allocated_bytes()\n        self.bridge = Bridge()\n\n    def tearDown(self):\n        self.bridge.close()\n        gc.collect()\n        diff_python = pa.total_allocated_bytes() - self.old_allocated_python\n        self.assertEqual(\n            pa.total_allocated_bytes(), self.old_allocated_python,\n            f\"PyArrow memory was not adequately released: {diff_python} bytes lost\")\n\n    def round_trip_field(self, field_generator):\n        original_field = field_generator()\n        java_field = self.bridge.python_to_java_field(original_field)\n        del original_field\n        new_field = self.bridge.java_to_python_field(java_field)\n        del java_field\n\n        expected = field_generator()\n        self.assertEqual(expected, new_field)\n\n    def round_trip_array(self, array_generator, check_metadata=True):\n        original_arr = array_generator()\n        with self.bridge.java_c.CDataDictionaryProvider() as dictionary_provider, \\\n                self.bridge.python_to_java_array(original_arr, dictionary_provider) as vector:\n            del original_arr\n            new_array = self.bridge.java_to_python_array(vector, dictionary_provider)\n\n        expected = array_generator()\n\n        self.assertEqual(expected, new_array)\n        if check_metadata:\n            self.assertTrue(new_array.type.equals(expected.type, check_metadata=True))\n\n    def round_trip_record_batch(self, rb_generator):\n        original_rb = rb_generator()\n        with self.bridge.python_to_java_record_batch(original_rb) as root:\n            del original_rb\n            new_rb = self.bridge.java_to_python_record_batch(root)\n\n        expected = rb_generator()\n        self.assertEqual(expected, new_rb)\n\n    def round_trip_reader(self, schema, batches):\n        reader = pa.RecordBatchReader.from_batches(schema, batches)\n\n        java_reader = self.bridge.python_to_java_reader(reader)\n        del reader\n        py_reader = self.bridge.java_to_python_reader(java_reader)\n        del java_reader\n\n        actual = list(py_reader)\n        self.assertEqual(batches, actual)\n\n    def test_string_array(self):\n        self.round_trip_array(lambda: pa.array([None, \"a\", \"bb\", \"ccc\"]))\n\n    def test_stringview_array(self):\n        # with nulls short strings\n        self.round_trip_array(lambda: pa.array([None, \"a\", \"bb\", \"c\"], type=pa.string_view()))\n        # with nulls long and strings\n        self.round_trip_array(lambda: pa.array([None, \"a\", \"bb\"*10, \"c\"*13], type=pa.string_view()))\n        # without nulls short strings\n        self.round_trip_array(lambda: pa.array([\"a\", \"bb\", \"c\"], type=pa.string_view()))\n        # without nulls long and strings\n        self.round_trip_array(lambda: pa.array([\"a\", \"bb\"*10, \"c\"*13], type=pa.string_view()))\n        # with multiple data buffers\n        arr1 = pa.array([\"a\", \"bb\", \"c\"], type=pa.string_view())\n        arr2 = pa.array([\"b\", \"ee\" * 10, \"f\" * 20], type=pa.string_view())\n        arr3 = pa.array([\"c\", \"abc\" * 20, \"efg\" * 30], type=pa.string_view())\n        arr4 = pa.array([\"d\", \"abcd\" * 100, \"efgh\" * 200], type=pa.string_view())\n        self.round_trip_array(lambda: pa.concat_arrays([arr1, arr2, arr3, arr4]))\n        # empty strings\n        self.round_trip_array(lambda: pa.array([\"\", \"bb\" * 10, \"c\", \"\", \"d\", \"\"], type=pa.string_view()))\n        # null value variations\n        self.round_trip_array(lambda: pa.array([\"bb\" * 10, None, \"\", \"d\", None], type=pa.string_view()))\n        # empty array\n        self.round_trip_array(lambda: pa.array([], type=pa.string_view()))\n        # all null array\n        self.round_trip_array(lambda: pa.array([None, None, None], type=pa.string_view()))\n\n    def test_binaryview_array(self):\n        # with nulls short binary values\n        self.round_trip_array(lambda: pa.array([None, bytes([97]), bytes([98, 98]), bytes([99])], type=pa.binary_view()))\n        # with nulls long binary values\n        self.round_trip_array(lambda: pa.array([None, bytes([97]), bytes([98, 98] * 10), bytes([99] * 13)], type=pa.binary_view()))\n        # without nulls short binary values\n        self.round_trip_array(lambda: pa.array([bytes([97]), bytes([98, 98]), bytes([99])], type=pa.binary_view()))\n        # without nulls long binary values\n        self.round_trip_array(lambda: pa.array([bytes([97]), bytes([98, 98] * 10), bytes([99] * 13)], type=pa.binary_view()))\n        # with multiple data buffers\n        arr1 = pa.array([bytes([97]), bytes([98, 98]), bytes([99])], type=pa.binary_view())\n        arr2 = pa.array([bytes([98]), bytes([98, 98] * 10), bytes([99] * 13)], type=pa.binary_view())\n        arr3 = pa.array([bytes([99]), bytes([98, 100] * 100), bytes([99, 100]) * 30], type=pa.binary_view())\n        arr4 = pa.array([bytes([100]), bytes([98, 100, 101] * 200), bytes([98, 99]) * 300], type=pa.binary_view())\n        self.round_trip_array(lambda: pa.concat_arrays([arr1, arr2, arr3, arr4]))\n        # empty binary values\n        self.round_trip_array(lambda: pa.array([bytes([]), bytes([97, 97]) * 10, bytes([98]), bytes([]), bytes([97]), bytes([])],\n                                               type=pa.binary_view()))\n        # null value variations\n        self.round_trip_array(lambda: pa.array([bytes([97, 97]) * 10, None, bytes([]), bytes([99]), None], type=pa.binary_view()))\n        # empty array\n        self.round_trip_array(lambda: pa.array([], type=pa.binary_view()))\n        # all null array\n        self.round_trip_array(lambda: pa.array([None, None, None], type=pa.binary_view()))\n\n    def test_decimal_array(self):\n        data = [\n            round(decimal.Decimal(722.82), 2),\n            round(decimal.Decimal(-934.11), 2),\n            None,\n        ]\n        self.round_trip_array(lambda: pa.array(data, pa.decimal128(5, 2)))\n\n    def test_int_array(self):\n        self.round_trip_array(lambda: pa.array([1, 2, 3], type=pa.int32()))\n\n    def test_list_array(self):\n        self.round_trip_array(lambda: pa.array(\n            [[], [0], [1, 2], [4, 5, 6]], pa.list_(pa.int64())\n            # disabled check_metadata since the list internal field name (\"item\")\n            # is not preserved during round trips (it becomes \"$data$\").\n        ), check_metadata=False)\n\n    def test_empty_list_array(self):\n        \"\"\"Validates GH-37056 fix.\n        Empty list of int32 produces a vector with empty child data buffer, however with non-zero capacity.\n        Using streaming forces the c-data array which represent the child data buffer to be NULL (pointer is 0).\n        On Java side, an attempt to import such array triggered an exception described in GH-37056.\n        \"\"\"\n        with pa.BufferOutputStream() as bos:\n            schema = pa.schema([pa.field(\"f0\", pa.list_(pa.int32()), True)])\n            with ipc.new_stream(bos, schema) as writer:\n                src = pa.RecordBatch.from_arrays([pa.array([[]])], schema=schema)\n                writer.write(src)\n        data_bytes = bos.getvalue()\n\n        def recreate_batch():\n            with pa.input_stream(data_bytes) as ios:\n                with ipc.open_stream(ios) as reader:\n                    return reader.read_next_batch()\n\n        self.round_trip_record_batch(recreate_batch)\n\n    def test_struct_array(self):\n        fields = [\n            (\"f1\", pa.int32()),\n            (\"f2\", pa.string()),\n        ]\n        data = [\n            {\"f1\": 1, \"f2\": \"a\"},\n            None,\n            {\"f1\": 3, \"f2\": None},\n            {\"f1\": None, \"f2\": \"d\"},\n            {\"f1\": None, \"f2\": None},\n        ]\n        self.round_trip_array(lambda: pa.array(data, type=pa.struct(fields)))\n\n    def test_dict(self):\n        self.round_trip_array(\n            lambda: pa.array([\"a\", \"b\", None, \"d\"], pa.dictionary(pa.int64(), pa.utf8())))\n\n    def test_map(self):\n        offsets = [0, None, 2, 6]\n        pykeys = [b\"a\", b\"b\", b\"c\", b\"d\", b\"e\", b\"f\"]\n        pyitems = [1, 2, 3, None, 4, 5]\n        keys = pa.array(pykeys, type=\"binary\")\n        items = pa.array(pyitems, type=\"i4\")\n        self.round_trip_array(\n            lambda: pa.MapArray.from_arrays(offsets, keys, items))\n\n    def test_field(self):\n        self.round_trip_field(lambda: pa.field(\"aa\", pa.bool_()))\n\n    def test_field_nested(self):\n        self.round_trip_field(lambda: pa.field(\n            \"test\", pa.list_(pa.int32()), nullable=True))\n\n    def test_field_metadata(self):\n        self.round_trip_field(lambda: pa.field(\"aa\", pa.bool_(), {\"a\": \"b\"}))\n\n    def test_record_batch_with_list(self):\n        data = [\n            pa.array([[1], [2], [3], [4, 5, 6]]),\n            pa.array([1, 2, 3, 4]),\n            pa.array(['foo', 'bar', 'baz', None]),\n            pa.array([True, None, False, True])\n        ]\n        self.round_trip_record_batch(\n            lambda: pa.RecordBatch.from_arrays(data, ['f0', 'f1', 'f2', 'f3']))\n\n    def test_reader_roundtrip(self):\n        schema = pa.schema([(\"ints\", pa.int64()), (\"strs\", pa.string())])\n        data = [\n            pa.record_batch([[1, 2, 3, None],\n                             [\"a\", \"bc\", None, \"\"]],\n                            schema=schema),\n            pa.record_batch([[None, 4, 5, 6],\n                             [None, \"\", \"def\", \"g\"]],\n                            schema=schema),\n        ]\n        self.round_trip_reader(schema, data)\n\n    def test_reader_complex_roundtrip(self):\n        schema = pa.schema([\n            (\"str_dict\", pa.dictionary(pa.int8(), pa.string())),\n            (\"int_list\", pa.list_(pa.int64())),\n        ])\n        dictionary = pa.array([\"a\", \"bc\", None])\n        data = [\n            pa.record_batch([pa.DictionaryArray.from_arrays([0, 2], dictionary),\n                             [[1, 2, 3], None]],\n                            schema=schema),\n            pa.record_batch([pa.DictionaryArray.from_arrays([None, 1], dictionary),\n                             [[], [4]]],\n                            schema=schema),\n        ]\n        self.round_trip_reader(schema, data)\n\n\nif __name__ == '__main__':\n    unittest.main(verbosity=2)\n", "dev/test_merge_arrow_pr.py": "#!/usr/bin/env python\n\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom collections import namedtuple\n\nimport pytest\n\nimport merge_arrow_pr\n\n\nFakeIssue = namedtuple('issue', ['fields'])\nFakeFields = namedtuple('fields', ['status', 'summary', 'assignee',\n                                   'components', 'fixVersions', 'milestone'])\nFakeAssignee = namedtuple('assignee', ['displayName'])\nFakeStatus = namedtuple('status', ['name'])\nFakeComponent = namedtuple('component', ['name'])\nFakeVersion = namedtuple('version', ['name', 'raw'])\nFakeMilestone = namedtuple('milestone', ['state'])\n\nRAW_VERSION_JSON = [\n    {'name': 'JS-0.4.0', 'released': False},\n    {'name': '1.0.0', 'released': False},\n    {'name': '2.0.0', 'released': False},\n    {'name': '0.9.0', 'released': False},\n    {'name': '0.10.0', 'released': False},\n    {'name': '0.8.0', 'released': True},\n    {'name': '0.7.0', 'released': True}\n]\n\n\nSOURCE_VERSIONS = [FakeVersion(raw['name'], raw)\n                   for raw in RAW_VERSION_JSON]\n\nTRANSITIONS = [{'name': 'Resolve Issue', 'id': 1}]\n\njira_id = 'ARROW-1234'\nstatus = FakeStatus('In Progress')\nfields = FakeFields(status, 'issue summary', FakeAssignee('groundhog'),\n                    [FakeComponent('C++'), FakeComponent('Format')],\n                    [], FakeMilestone('closed')._asdict())\nFAKE_ISSUE_1 = FakeIssue(fields)\n\n\nclass FakeJIRA:\n\n    def __init__(self, issue=None, project_versions=None, transitions=None,\n                 current_fix_versions=None):\n        self._issue = issue\n        self._project_versions = project_versions\n        self._transitions = transitions\n\n    def issue(self, jira_id):\n        return self._issue\n\n    def transitions(self, jira_id):\n        return self._transitions\n\n    def transition_issue(self, jira_id, transition_id, comment=None,\n                         fixVersions=None):\n        self.captured_transition = {\n            'jira_id': jira_id,\n            'transition_id': transition_id,\n            'comment': comment,\n            'fixVersions': fixVersions\n        }\n\n    @property\n    def current_versions(self):\n        all_versions = self._project_versions or SOURCE_VERSIONS\n        return [\n            v for v in all_versions if not v.raw.get(\"released\")\n        ] + ['0.11.0']\n\n    @property\n    def current_fix_versions(self):\n        return 'JS-0.4.0'\n\n    def project_versions(self, project):\n        return self._project_versions\n\n\nclass FakeGitHub:\n\n    def __init__(self, issue=None, project_versions=None):\n        self._issue = issue\n        self._project_versions = project_versions\n\n    @property\n    def issue(self):\n        return self._issue.fields._asdict()\n\n    @property\n    def current_versions(self):\n        all_versions = self._project_versions or SOURCE_VERSIONS\n        return [\n            v for v in all_versions if not v.raw.get(\"released\")\n        ] + ['0.11.0']\n\n    @property\n    def current_fix_versions(self):\n        return 'JS-0.4.0'\n\n    def project_versions(self, project):\n        return self._project_versions\n\n\nclass FakeCLI:\n\n    def __init__(self, responses=()):\n        self.responses = responses\n        self.position = 0\n\n    def prompt(self, prompt):\n        response = self.responses[self.position]\n        self.position += 1\n        return response\n\n    def fail(self, msg):\n        raise Exception(msg)\n\n\ndef test_jira_fix_versions():\n    jira = FakeJIRA(project_versions=SOURCE_VERSIONS,\n                    transitions=TRANSITIONS)\n\n    issue = merge_arrow_pr.JiraIssue(jira, 'ARROW-1234', 'ARROW', FakeCLI())\n    fix_version = merge_arrow_pr.get_candidate_fix_version(\n        issue.current_versions\n    )\n    assert fix_version == '1.0.0'\n\n\ndef test_jira_fix_versions_filters_maintenance():\n    maintenance_branches = [\"maint-1.0.0\"]\n    jira = FakeJIRA(project_versions=SOURCE_VERSIONS,\n                    transitions=TRANSITIONS)\n\n    issue = merge_arrow_pr.JiraIssue(jira, 'ARROW-1234', 'ARROW', FakeCLI())\n    fix_version = merge_arrow_pr.get_candidate_fix_version(\n        issue.current_versions,\n        maintenance_branches=maintenance_branches\n    )\n    assert fix_version == '2.0.0'\n\n\ndef test_jira_only_suggest_major_release():\n    versions_json = [\n        {'name': '0.9.1', 'released': False},\n        {'name': '0.10.0', 'released': False},\n        {'name': '1.0.0', 'released': False},\n    ]\n\n    versions = [FakeVersion(raw['name'], raw) for raw in versions_json]\n\n    jira = FakeJIRA(project_versions=versions, transitions=TRANSITIONS)\n    issue = merge_arrow_pr.JiraIssue(jira, 'ARROW-1234', 'ARROW', FakeCLI())\n    fix_version = merge_arrow_pr.get_candidate_fix_version(\n        issue.current_versions\n    )\n    assert fix_version == '1.0.0'\n\n\ndef test_jira_parquet_no_suggest_non_cpp():\n    # ARROW-7351\n    versions_json = [\n        {'name': 'cpp-1.5.0', 'released': True},\n        {'name': 'cpp-1.6.0', 'released': False},\n        {'name': 'cpp-1.7.0', 'released': False},\n        {'name': 'cpp-2.0.0', 'released': False},\n        {'name': '1.11.0', 'released': False},\n        {'name': '1.12.0', 'released': False},\n        {'name': '2.0.0', 'released': False}\n    ]\n\n    versions = [FakeVersion(raw['name'], raw)\n                for raw in versions_json]\n\n    jira = FakeJIRA(project_versions=versions, transitions=TRANSITIONS)\n    issue = merge_arrow_pr.JiraIssue(jira, 'PARQUET-1713', 'PARQUET',\n                                     FakeCLI())\n    fix_version = merge_arrow_pr.get_candidate_fix_version(\n        issue.current_versions\n    )\n    assert fix_version == 'cpp-2.0.0'\n\n\ndef test_jira_invalid_issue():\n    class Mock:\n\n        def issue(self, jira_id):\n            raise Exception(\"not found\")\n\n    with pytest.raises(Exception):\n        merge_arrow_pr.JiraIssue(Mock(), 'ARROW-1234', 'ARROW', FakeCLI())\n\n\ndef test_jira_resolve():\n    jira = FakeJIRA(issue=FAKE_ISSUE_1,\n                    project_versions=SOURCE_VERSIONS,\n                    transitions=TRANSITIONS)\n\n    my_comment = 'my comment'\n    fix_version = \"0.10.0\"\n\n    issue = merge_arrow_pr.JiraIssue(jira, 'ARROW-1234', 'ARROW', FakeCLI())\n    issue.resolve(fix_version, my_comment)\n\n    assert jira.captured_transition == {\n        'jira_id': 'ARROW-1234',\n        'transition_id': 1,\n        'comment': my_comment,\n        'fixVersions': [{'name': '0.10.0', 'released': False}]\n    }\n\n\ndef test_jira_resolve_non_mainline():\n    jira = FakeJIRA(issue=FAKE_ISSUE_1,\n                    project_versions=SOURCE_VERSIONS,\n                    transitions=TRANSITIONS)\n\n    my_comment = 'my comment'\n    fix_version = \"JS-0.4.0\"\n\n    issue = merge_arrow_pr.JiraIssue(jira, 'ARROW-1234', 'ARROW', FakeCLI())\n    issue.resolve(fix_version, my_comment)\n\n    assert jira.captured_transition == {\n        'jira_id': 'ARROW-1234',\n        'transition_id': 1,\n        'comment': my_comment,\n        'fixVersions': [{'name': 'JS-0.4.0', 'released': False}]\n    }\n\n\ndef test_jira_resolve_released_fix_version():\n    # ARROW-5083\n    jira = FakeGitHub(issue=FAKE_ISSUE_1,\n                      project_versions=SOURCE_VERSIONS)\n\n    cmd = FakeCLI(responses=['1.0.0'])\n    fix_versions_json = merge_arrow_pr.prompt_for_fix_version(cmd, jira)\n    assert fix_versions_json == \"1.0.0\"\n\n\ndef test_multiple_authors_bad_input():\n    a0 = 'Jimbob Crawfish <jimbob.crawfish@gmail.com>'\n    a1 = 'Jarvis McCratchett <jarvis.mccratchett@hotmail.com>'\n    a2 = 'Hank Miller <hank.miller@protonmail.com>'\n    distinct_authors = [a0, a1]\n\n    cmd = FakeCLI(responses=[''])\n    primary_author, distinct_other_authors = \\\n        merge_arrow_pr.get_primary_author(cmd, distinct_authors)\n    assert primary_author == a0\n    assert distinct_other_authors == [a1]\n\n    cmd = FakeCLI(responses=['oops', a1])\n    primary_author, distinct_other_authors = \\\n        merge_arrow_pr.get_primary_author(cmd, distinct_authors)\n    assert primary_author == a1\n    assert distinct_other_authors == [a0]\n\n    cmd = FakeCLI(responses=[a2])\n    primary_author, distinct_other_authors = \\\n        merge_arrow_pr.get_primary_author(cmd, distinct_authors)\n    assert primary_author == a2\n    assert distinct_other_authors == [a0, a1]\n\n\ndef test_jira_already_resolved():\n    status = FakeStatus('Resolved')\n    fields = FakeFields(status, 'issue summary', FakeAssignee('groundhog'),\n                        [FakeComponent('Java')], [], None)\n    issue = FakeIssue(fields)\n\n    jira = FakeJIRA(issue=issue,\n                    project_versions=SOURCE_VERSIONS,\n                    transitions=TRANSITIONS)\n\n    fix_versions = [SOURCE_VERSIONS[0].raw]\n    issue = merge_arrow_pr.JiraIssue(jira, 'ARROW-1234', 'ARROW', FakeCLI())\n\n    with pytest.raises(Exception,\n                       match=\"ARROW-1234 already has status 'Resolved'\"):\n        issue.resolve(fix_versions, \"\")\n\n\ndef test_no_unset_point_release_fix_version():\n    # ARROW-6915: We have had the problem of issues marked with a point release\n    # having their fix versions overwritten by the merge tool. This verifies\n    # that existing patch release versions are carried over\n    status = FakeStatus('In Progress')\n\n    versions_json = {\n        '0.14.2': {'name': '0.14.2', 'id': 1},\n        '0.15.1': {'name': '0.15.1', 'id': 2},\n        '0.16.0': {'name': '0.16.0', 'id': 3},\n        '0.17.0': {'name': '0.17.0', 'id': 4}\n    }\n\n    fields = FakeFields(status, 'summary', FakeAssignee('someone'),\n                        [FakeComponent('Java')],\n                        [FakeVersion(v, versions_json[v])\n                         for v in ['0.17.0', '0.15.1', '0.14.2']], None)\n    issue = FakeIssue(fields)\n\n    jira = FakeJIRA(\n        issue=issue,\n        project_versions=[\n            FakeVersion(v, vdata) for v, vdata in versions_json.items()\n        ],\n        transitions=TRANSITIONS\n    )\n\n    issue = merge_arrow_pr.JiraIssue(jira, 'ARROW-1234', 'ARROW', FakeCLI())\n    issue.resolve('0.16.0', \"a comment\")\n\n    assert jira.captured_transition == {\n        'jira_id': 'ARROW-1234',\n        'transition_id': 1,\n        'comment': 'a comment',\n        'fixVersions': [versions_json[v]\n                        for v in ['0.16.0', '0.15.1', '0.14.2']]\n    }\n\n    issue.resolve([versions_json['0.15.1']], \"a comment\")\n\n    assert jira.captured_transition == {\n        'jira_id': 'ARROW-1234',\n        'transition_id': 1,\n        'comment': 'a comment',\n        'fixVersions': [versions_json[v] for v in ['0.15.1', '0.14.2']]\n    }\n\n\ndef test_jira_output_no_components():\n    # ARROW-5472\n    status = 'Interesting work'\n    components = []\n    output = merge_arrow_pr.format_issue_output(\n        \"jira\", 'ARROW-1234', 'Resolved', status,\n        FakeAssignee('Foo Bar'), components\n    )\n\n    assert output == \"\"\"=== JIRA ARROW-1234 ===\nSummary\\t\\tInteresting work\nAssignee\\tFoo Bar\nComponents\\tNO COMPONENTS!!!\nStatus\\t\\tResolved\nURL\\t\\thttps://issues.apache.org/jira/browse/ARROW-1234\"\"\"\n\n    output = merge_arrow_pr.format_issue_output(\n        \"jira\", 'ARROW-1234', 'Resolved', status, FakeAssignee('Foo Bar'),\n        [FakeComponent('C++'), FakeComponent('Python')]\n    )\n\n    assert output == \"\"\"=== JIRA ARROW-1234 ===\nSummary\\t\\tInteresting work\nAssignee\\tFoo Bar\nComponents\\tC++, Python\nStatus\\t\\tResolved\nURL\\t\\thttps://issues.apache.org/jira/browse/ARROW-1234\"\"\"\n\n\ndef test_sorting_versions():\n    versions_json = [\n        {'name': '11.0.0', 'released': False},\n        {'name': '9.0.0', 'released': False},\n        {'name': '10.0.0', 'released': False},\n    ]\n    versions = [FakeVersion(raw['name'], raw) for raw in versions_json]\n    fix_version = merge_arrow_pr.get_candidate_fix_version(versions)\n    assert fix_version == \"9.0.0\"\n", "dev/merge_arrow_pr.py": "#!/usr/bin/env python3\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Utility for creating well-formed pull request merges and pushing them to\n# Apache.\n#   usage: ./merge_arrow_pr.py  <pr-number>  (see config env vars below)\n#\n# This utility assumes:\n#   - you already have a local Arrow git clone\n#   - you have added remotes corresponding to both:\n#       (i) the GitHub Apache Arrow mirror\n#       (ii) the Apache git repo\n#\n# There are several pieces of authorization possibly needed via environment\n# variables.\n#\n# Configuration environment variables:\n#   - APACHE_JIRA_TOKEN: your Apache JIRA Personal Access Token\n#   - ARROW_GITHUB_API_TOKEN: a GitHub API token to use for API requests\n#   - ARROW_GITHUB_ORG: the GitHub organisation ('apache' by default)\n#   - DEBUG: use for testing to avoid pushing to apache (0 by default)\n\nimport configparser\nimport os\nimport pprint\nimport re\nimport subprocess\nimport sys\nimport requests\nimport getpass\n\nfrom six.moves import input\nimport six\n\ntry:\n    import jira.client\n    import jira.exceptions\nexcept ImportError:\n    print(\"Could not find jira library. \"\n          \"Run 'pip install jira' to install.\")\n    print(\"Exiting without trying to close the associated JIRA.\")\n    sys.exit(1)\n\n# Remote name which points to the GitHub site\nORG_NAME = (\n    os.environ.get(\"ARROW_GITHUB_ORG\") or\n    os.environ.get(\"PR_REMOTE_NAME\") or  # backward compatibility\n    \"apache\"\n)\nPROJECT_NAME = os.environ.get('ARROW_PROJECT_NAME') or \"arrow\"\n\n# For testing to avoid accidentally pushing to apache\nDEBUG = bool(int(os.environ.get(\"DEBUG\", 0)))\n\nif DEBUG:\n    print(\"**************** DEBUGGING ****************\")\n\n\nJIRA_API_BASE = \"https://issues.apache.org/jira\"\n\n\ndef get_json(url, headers=None):\n    response = requests.get(url, headers=headers)\n    if response.status_code != 200:\n        raise ValueError(response.json())\n    # GitHub returns a link header with the next, previous, last\n    # page if there is pagination on the response. See:\n    # https://docs.github.com/en/rest/guides/using-pagination-in-the-rest-api#using-link-headers\n    next_responses = None\n    if \"link\" in response.headers:\n        links = response.headers['link'].split(', ')\n        for link in links:\n            if 'rel=\"next\"' in link:\n                # Format: '<url>; rel=\"next\"'\n                next_url = link.split(\";\")[0][1:-1]\n                next_responses = get_json(next_url, headers)\n    responses = response.json()\n    if next_responses:\n        if isinstance(responses, list):\n            responses.extend(next_responses)\n        else:\n            raise ValueError('GitHub response was paginated and is not a list')\n    return responses\n\n\ndef run_cmd(cmd):\n    if isinstance(cmd, six.string_types):\n        cmd = cmd.split(' ')\n\n    try:\n        output = subprocess.check_output(cmd)\n    except subprocess.CalledProcessError as e:\n        # this avoids hiding the stdout / stderr of failed processes\n        print('Command failed: %s' % cmd)\n        print('With output:')\n        print('--------------')\n        print(e.output)\n        print('--------------')\n        raise e\n\n    if isinstance(output, six.binary_type):\n        output = output.decode('utf-8')\n    return output\n\n\n_REGEX_CI_DIRECTIVE = re.compile(r'\\[[^\\]]*\\]')\n\n\ndef strip_ci_directives(commit_message):\n    # Remove things like '[force ci]', '[skip appveyor]' from the assembled\n    # commit message\n    return _REGEX_CI_DIRECTIVE.sub('', commit_message)\n\n\ndef fix_version_from_branch(versions):\n    # Note: Assumes this is a sorted (newest->oldest) list of un-released\n    # versions\n    return versions[-1]\n\n\nMIGRATION_COMMENT_REGEX = re.compile(\n    r\"This issue has been migrated to \\[issue #(?P<issue_id>(\\d+))\"\n)\n\n\nclass JiraIssue(object):\n\n    def __init__(self, jira_con, jira_id, project, cmd):\n        self.jira_con = jira_con\n        self.jira_id = jira_id\n        self.project = project\n        self.cmd = cmd\n\n        try:\n            self.issue = jira_con.issue(jira_id)\n        except Exception as e:\n            self.cmd.fail(\"ASF JIRA could not find %s\\n%s\" % (jira_id, e))\n\n    @property\n    def current_fix_versions(self):\n        return self.issue.fields.fixVersions\n\n    @property\n    def current_versions(self):\n        # Only suggest versions starting with a number, like 0.x but not JS-0.x\n        all_versions = self.jira_con.project_versions(self.project)\n        unreleased_versions = [x for x in all_versions\n                               if not x.raw['released']]\n\n        mainline_versions = self._filter_mainline_versions(unreleased_versions)\n        return mainline_versions\n\n    def _filter_mainline_versions(self, versions):\n        if self.project == 'PARQUET':\n            mainline_regex = re.compile(r'cpp-\\d.*')\n        else:\n            mainline_regex = re.compile(r'\\d.*')\n\n        return [x for x in versions if mainline_regex.match(x.name)]\n\n    def resolve(self, fix_version, comment, *args):\n        fields = self.issue.fields\n        cur_status = fields.status.name\n\n        if cur_status == \"Resolved\" or cur_status == \"Closed\":\n            self.cmd.fail(\"JIRA issue %s already has status '%s'\"\n                          % (self.jira_id, cur_status))\n\n        resolve = [x for x in self.jira_con.transitions(self.jira_id)\n                   if x['name'] == \"Resolve Issue\"][0]\n\n        # ARROW-6915: do not overwrite existing fix versions corresponding to\n        # point releases\n        fix_versions = [v.raw for v in self.jira_con.project_versions(\n            self.project) if v.name == fix_version]\n        fix_version_names = set(x['name'] for x in fix_versions)\n        for version in self.current_fix_versions:\n            major, minor, patch = version.name.split('.')\n            if patch != '0' and version.name not in fix_version_names:\n                fix_versions.append(version.raw)\n\n        if DEBUG:\n            print(\"JIRA issue %s untouched -> %s\" %\n                  (self.jira_id, [v[\"name\"] for v in fix_versions]))\n        else:\n            self.jira_con.transition_issue(self.jira_id, resolve[\"id\"],\n                                           comment=comment,\n                                           fixVersions=fix_versions)\n            print(\"Successfully resolved %s!\" % (self.jira_id))\n\n        self.issue = self.jira_con.issue(self.jira_id)\n        self.show()\n\n    def show(self):\n        fields = self.issue.fields\n        print(format_issue_output(\"jira\", self.jira_id, fields.status.name,\n                                  fields.summary, fields.assignee,\n                                  fields.components))\n\n    def github_issue_id(self):\n        try:\n            last_jira_comment = self.issue.fields.comment.comments[-1].body\n        except Exception:\n            # If no comment found or other issues ignore\n            return None\n        matches = MIGRATION_COMMENT_REGEX.search(last_jira_comment)\n        if matches:\n            values = matches.groupdict()\n            return \"GH-\" + values['issue_id']\n\n\nclass GitHubIssue(object):\n\n    def __init__(self, github_api, github_id, cmd):\n        self.github_api = github_api\n        self.github_id = github_id\n        self.cmd = cmd\n\n        try:\n            self.issue = self.github_api.get_issue_data(github_id)\n        except Exception as e:\n            self.cmd.fail(\"GitHub could not find %s\\n%s\" % (github_id, e))\n\n    def get_label(self, prefix):\n        prefix = f\"{prefix}:\"\n        return [\n            lbl[\"name\"][len(prefix):].strip()\n            for lbl in self.issue[\"labels\"] if lbl[\"name\"].startswith(prefix)\n        ]\n\n    @property\n    def components(self):\n        return self.get_label(\"Component\")\n\n    @property\n    def assignees(self):\n        return [a[\"login\"] for a in self.issue[\"assignees\"]]\n\n    @property\n    def current_fix_versions(self):\n        try:\n            return self.issue.get(\"milestone\", {}).get(\"title\")\n        except AttributeError:\n            pass\n\n    @property\n    def current_versions(self):\n        all_versions = self.github_api.get_milestones()\n\n        unreleased_versions = [x for x in all_versions if x[\"state\"] == \"open\"]\n        unreleased_versions = [x[\"title\"] for x in unreleased_versions]\n\n        return unreleased_versions\n\n    def resolve(self, fix_version, comment, pr_body):\n        cur_status = self.issue[\"state\"]\n\n        if cur_status == \"closed\":\n            self.cmd.fail(\"GitHub issue %s already has status '%s'\"\n                          % (self.github_id, cur_status))\n\n        if DEBUG:\n            print(\"GitHub issue %s untouched -> %s\" %\n                  (self.github_id, fix_version))\n        else:\n            self.github_api.assign_milestone(self.github_id, fix_version)\n            if f\"Closes: #{self.github_id}\" not in pr_body:\n                self.github_api.close_issue(self.github_id, comment)\n            print(\"Successfully resolved %s!\" % (self.github_id))\n\n        self.issue = self.github_api.get_issue_data(self.github_id)\n        self.show()\n\n    def show(self):\n        issue = self.issue\n        print(format_issue_output(\"github\", self.github_id, issue[\"state\"],\n                                  issue[\"title\"], ', '.join(self.assignees),\n                                  self.components))\n\n\ndef get_candidate_fix_version(mainline_versions,\n                              maintenance_branches=()):\n\n    all_versions = [getattr(v, \"name\", v) for v in mainline_versions]\n\n    def version_tuple(x):\n        # Parquet versions are something like cpp-1.2.0\n        numeric_version = getattr(x, \"name\", x).split(\"-\", 1)[-1]\n        return tuple(int(_) for _ in numeric_version.split(\".\"))\n    all_versions = sorted(all_versions, key=version_tuple, reverse=True)\n\n    # Only suggest versions starting with a number, like 0.x but not JS-0.x\n    mainline_versions = all_versions\n    major_versions = [v for v in mainline_versions if v.endswith('.0.0')]\n\n    if len(mainline_versions) > len(major_versions):\n        # If there is a future major release, suggest that\n        mainline_versions = major_versions\n\n    mainline_versions = [v for v in mainline_versions\n                         if f\"maint-{v}\" not in maintenance_branches]\n    default_fix_versions = fix_version_from_branch(mainline_versions)\n\n    return default_fix_versions\n\n\ndef format_issue_output(issue_type, issue_id, status,\n                        summary, assignee, components):\n    if not assignee:\n        assignee = \"NOT ASSIGNED!!!\"\n    else:\n        assignee = getattr(assignee, \"displayName\", assignee)\n\n    if len(components) == 0:\n        components = 'NO COMPONENTS!!!'\n    else:\n        components = ', '.join((getattr(x, \"name\", x) for x in components))\n\n    if issue_type == \"jira\":\n        url = '/'.join((JIRA_API_BASE, 'browse', issue_id))\n    else:\n        url = (\n            f'https://github.com/{ORG_NAME}/{PROJECT_NAME}/issues/{issue_id}'\n        )\n\n    return \"\"\"=== {} {} ===\nSummary\\t\\t{}\nAssignee\\t{}\nComponents\\t{}\nStatus\\t\\t{}\nURL\\t\\t{}\"\"\".format(issue_type.upper(), issue_id, summary, assignee,\n                    components, status, url)\n\n\nclass GitHubAPI(object):\n\n    def __init__(self, project_name, cmd):\n        self.github_api = (\n            f\"https://api.github.com/repos/{ORG_NAME}/{project_name}\"\n        )\n\n        token = None\n        config = load_configuration()\n        if \"github\" in config.sections():\n            token = config[\"github\"][\"api_token\"]\n        if not token:\n            token = os.environ.get('ARROW_GITHUB_API_TOKEN')\n        if not token:\n            token = cmd.prompt('Env ARROW_GITHUB_API_TOKEN not set, '\n                               'please enter your GitHub API token '\n                               '(GitHub personal access token):')\n        headers = {\n            'Accept': 'application/vnd.github.v3+json',\n            'Authorization': 'token {0}'.format(token),\n        }\n        self.headers = headers\n\n    def get_milestones(self):\n        return get_json(\"%s/milestones\" % (self.github_api, ),\n                        headers=self.headers)\n\n    def get_milestone_number(self, version):\n        return next((\n            m[\"number\"] for m in self.get_milestones() if m[\"title\"] == version\n        ), None)\n\n    def get_issue_data(self, number):\n        return get_json(\"%s/issues/%s\" % (self.github_api, number),\n                        headers=self.headers)\n\n    def get_pr_data(self, number):\n        return get_json(\"%s/pulls/%s\" % (self.github_api, number),\n                        headers=self.headers)\n\n    def get_pr_commits(self, number):\n        return get_json(\"%s/pulls/%s/commits\" % (self.github_api, number),\n                        headers=self.headers)\n\n    def get_branches(self):\n        return get_json(\"%s/branches\" % (self.github_api),\n                        headers=self.headers)\n\n    def close_issue(self, number, comment):\n        issue_url = f'{self.github_api}/issues/{number}'\n        comment_url = f'{self.github_api}/issues/{number}/comments'\n\n        r = requests.post(comment_url, json={\n                          \"body\": comment}, headers=self.headers)\n        if not r.ok:\n            raise ValueError(\n                f\"Failed request: {comment_url}:{r.status_code} -> {r.json()}\")\n\n        r = requests.patch(\n            issue_url, json={\"state\": \"closed\"}, headers=self.headers)\n        if not r.ok:\n            raise ValueError(\n                f\"Failed request: {issue_url}:{r.status_code} -> {r.json()}\")\n\n    def assign_milestone(self, number, version):\n        url = f'{self.github_api}/issues/{number}'\n        milestone_number = self.get_milestone_number(version)\n        if not milestone_number:\n            raise ValueError(f\"Invalid version {version}, milestone not found\")\n        payload = {\n            'milestone': milestone_number\n        }\n        r = requests.patch(url, headers=self.headers, json=payload)\n        if not r.ok:\n            raise ValueError(\n                f\"Failed request: {url}:{r.status_code} -> {r.json()}\")\n        return r.json()\n\n    def merge_pr(self, number, commit_title, commit_message):\n        url = f'{self.github_api}/pulls/{number}/merge'\n        payload = {\n            'commit_title': commit_title,\n            'commit_message': commit_message,\n            'merge_method': 'squash',\n        }\n        response = requests.put(url, headers=self.headers, json=payload)\n        result = response.json()\n        if response.status_code == 200 and 'merged' in result:\n            self.clear_pr_state_labels(number)\n        else:\n            result['merged'] = False\n            result['message'] += f': {url}'\n        return result\n\n    def clear_pr_state_labels(self, number):\n        url = f'{self.github_api}/issues/{number}/labels'\n        response = requests.get(url, headers=self.headers)\n        labels = response.json()\n        for label in labels:\n            # All PR workflow state labels starts with \"awaiting\"\n            if label['name'].startswith('awaiting'):\n                label_url = f\"{url}/{label['name']}\"\n                requests.delete(label_url, headers=self.headers)\n\n\nclass CommandInput(object):\n    \"\"\"\n    Interface to input(...) to enable unit test mocks to be created\n    \"\"\"\n\n    def fail(self, msg):\n        raise Exception(msg)\n\n    def prompt(self, prompt):\n        return input(prompt)\n\n    def getpass(self, prompt):\n        return getpass.getpass(prompt)\n\n    def continue_maybe(self, prompt):\n        while True:\n            result = input(\"\\n%s (y/n): \" % prompt)\n            if result.lower() == \"y\":\n                return\n            elif result.lower() == \"n\":\n                self.fail(\"Okay, exiting\")\n            else:\n                prompt = \"Please input 'y' or 'n'\"\n\n\nclass PullRequest(object):\n    GITHUB_PR_TITLE_PATTERN = re.compile(r'^GH-([0-9]+)\\b.*$')\n    # We can merge PARQUET patches from JIRA or GH prefixed issues\n    JIRA_SUPPORTED_PROJECTS = ['PARQUET']\n    JIRA_PR_TITLE_REGEXEN = [\n        (project, re.compile(r'^(' + project + r'-[0-9]+)\\b.*$'))\n        for project in JIRA_SUPPORTED_PROJECTS\n    ]\n    JIRA_UNSUPPORTED_ARROW = re.compile(r'^(ARROW-[0-9]+)\\b.*$')\n\n    def __init__(self, cmd, github_api, git_remote, jira_con, number):\n        self.cmd = cmd\n        self._github_api = github_api\n        self.git_remote = git_remote\n        self.con = jira_con\n        self.number = number\n        self._pr_data = github_api.get_pr_data(number)\n        try:\n            self.url = self._pr_data[\"url\"]\n            self.title = self._pr_data[\"title\"]\n            self.body = self._pr_data[\"body\"]\n            self.target_ref = self._pr_data[\"base\"][\"ref\"]\n            self.user_login = self._pr_data[\"user\"][\"login\"]\n            self.base_ref = self._pr_data[\"head\"][\"ref\"]\n        except KeyError:\n            pprint.pprint(self._pr_data)\n            raise\n        self.description = \"%s/%s\" % (self.user_login, self.base_ref)\n\n        self.issue = self._get_issue()\n\n    def show(self):\n        print(\"\\n=== Pull Request #%s ===\" % self.number)\n        print(\"title\\t%s\\nsource\\t%s\\ntarget\\t%s\\nurl\\t%s\"\n              % (self.title, self.description, self.target_ref, self.url))\n        if self.issue is not None:\n            self.issue.show()\n        else:\n            print(\"Minor PR.  Please ensure it meets guidelines for minor.\\n\")\n\n    @property\n    def is_merged(self):\n        return bool(self._pr_data[\"merged\"])\n\n    @property\n    def is_mergeable(self):\n        return bool(self._pr_data[\"mergeable\"])\n\n    @property\n    def maintenance_branches(self):\n        return [x[\"name\"] for x in self._github_api.get_branches()\n                if x[\"name\"].startswith(\"maint-\")]\n\n    def _get_issue(self):\n        if self.title.startswith(\"MINOR:\"):\n            return None\n\n        m = self.GITHUB_PR_TITLE_PATTERN.search(self.title)\n        if m:\n            github_id = m.group(1)\n            return GitHubIssue(self._github_api, github_id, self.cmd)\n\n        m = self.JIRA_UNSUPPORTED_ARROW.search(self.title)\n        if m:\n            old_jira_id = m.group(1)\n            jira_issue = JiraIssue(self.con, old_jira_id, 'ARROW', self.cmd)\n            self.cmd.fail(\"PR titles with ARROW- prefixed tickets on JIRA \"\n                          \"are unsupported, update the PR title from \"\n                          f\"{old_jira_id}. Possible GitHub id could be: \"\n                          f\"{jira_issue.github_issue_id()}\")\n\n        for project, regex in self.JIRA_PR_TITLE_REGEXEN:\n            m = regex.search(self.title)\n            if m:\n                jira_id = m.group(1)\n                return JiraIssue(self.con, jira_id, project, self.cmd)\n\n        options = ' or '.join(\n            '{0}-XXX'.format(project)\n            for project in self.JIRA_SUPPORTED_PROJECTS + [\"GH\"]\n        )\n        self.cmd.fail(\"PR title should be prefixed by a GitHub ID or a \"\n                      \"Jira ID, like: {0}, but found {1}\".format(\n                          options, self.title))\n\n    def merge(self):\n        \"\"\"\n        merge the requested PR and return the merge hash\n        \"\"\"\n        commits = self._github_api.get_pr_commits(self.number)\n\n        def format_commit_author(commit):\n            author = commit['commit']['author']\n            name = author['name']\n            email = author['email']\n            return f'{name} <{email}>'\n        commit_authors = [format_commit_author(commit) for commit in commits]\n        co_authored_by_re = re.compile(\n            r'^Co-authored-by:\\s*(.*)', re.MULTILINE)\n\n        def extract_co_authors(commit):\n            message = commit['commit']['message']\n            return co_authored_by_re.findall(message)\n        commit_co_authors = []\n        for commit in commits:\n            commit_co_authors.extend(extract_co_authors(commit))\n\n        all_commit_authors = commit_authors + commit_co_authors\n        distinct_authors = sorted(set(all_commit_authors),\n                                  key=lambda x: commit_authors.count(x),\n                                  reverse=True)\n\n        for i, author in enumerate(distinct_authors):\n            print(\"Author {}: {}\".format(i + 1, author))\n\n        if len(distinct_authors) > 1:\n            primary_author, distinct_other_authors = get_primary_author(\n                self.cmd, distinct_authors)\n        else:\n            # If there is only one author, do not prompt for a lead author\n            primary_author = distinct_authors.pop()\n            distinct_other_authors = []\n\n        commit_title = f'{self.title} (#{self.number})'\n        commit_message_chunks = []\n        if self.body is not None:\n            # Remove comments (i.e. <-- comment -->) from the PR description.\n            body = re.sub(r\"<!--.*?-->\", \"\", self.body, flags=re.DOTALL)\n            # avoid github user name references by inserting a space after @\n            body = re.sub(r\"@(\\w+)\", \"@ \\\\1\", body)\n            commit_message_chunks.append(body)\n\n        committer_name = run_cmd(\"git config --get user.name\").strip()\n        committer_email = run_cmd(\"git config --get user.email\").strip()\n\n        authors = (\"Authored-by:\" if len(distinct_other_authors) == 0\n                   else \"Lead-authored-by:\")\n        authors += \" %s\" % primary_author\n        if len(distinct_authors) > 0:\n            authors += \"\\n\" + \"\\n\".join([\"Co-authored-by: %s\" % a\n                                         for a in distinct_other_authors])\n        authors += \"\\n\" + \"Signed-off-by: %s <%s>\" % (committer_name,\n                                                      committer_email)\n        commit_message_chunks.append(authors)\n\n        commit_message = \"\\n\\n\".join(commit_message_chunks)\n\n        # Normalize line ends and collapse extraneous newlines. We allow two\n        # consecutive newlines for paragraph breaks but not more.\n        commit_message = \"\\n\".join(commit_message.splitlines())\n        commit_message = re.sub(\"\\n{2,}\", \"\\n\\n\", commit_message)\n\n        if DEBUG:\n            print(\"*** Commit title ***\")\n            print(commit_title)\n            print()\n            print(\"*** Commit message ***\")\n            print(commit_message)\n\n        if DEBUG:\n            merge_hash = None\n        else:\n            result = self._github_api.merge_pr(self.number,\n                                               commit_title,\n                                               commit_message)\n            if not result['merged']:\n                message = result['message']\n                self.cmd.fail(f'Failed to merge pull request: {message}')\n            merge_hash = result['sha']\n\n        print(\"Pull request #%s merged!\" % self.number)\n        print(\"Merge hash: %s\" % merge_hash)\n\n\ndef get_primary_author(cmd, distinct_authors):\n    author_pat = re.compile(r'(.*) <(.*)>')\n\n    while True:\n        primary_author = cmd.prompt(\n            \"Enter primary author in the format of \"\n            \"\\\"name <email>\\\" [%s]: \" % distinct_authors[0])\n\n        if primary_author == \"\":\n            return distinct_authors[0], distinct_authors[1:]\n\n        if author_pat.match(primary_author):\n            break\n        print('Bad author \"{}\", please try again'.format(primary_author))\n\n    # When primary author is specified manually, de-dup it from\n    # author list and put it at the head of author list.\n    distinct_other_authors = [x for x in distinct_authors\n                              if x != primary_author]\n    return primary_author, distinct_other_authors\n\n\ndef prompt_for_fix_version(cmd, issue, maintenance_branches=()):\n    default_fix_version = get_candidate_fix_version(\n        mainline_versions=issue.current_versions,\n        maintenance_branches=maintenance_branches\n    )\n\n    current_fix_versions = issue.current_fix_versions\n    if (current_fix_versions and\n            current_fix_versions != default_fix_version):\n        print(\"\\n=== The assigned milestone is not the default ===\")\n        print(f\"Assigned milestone: {current_fix_versions}\")\n        print(f\"Current milestone: {default_fix_version}\")\n        if issue.issue[\"milestone\"].get(\"state\") == 'closed':\n            print(\"The assigned milestone state is closed. Contact the \")\n            print(\"Release Manager if it has to be added to a closed Release\")\n        print(\"Please ensure to assign the correct milestone.\")\n        # Default to existing assigned milestone\n        default_fix_version = current_fix_versions\n\n    issue_fix_version = cmd.prompt(\"Enter fix version [%s]: \"\n                                   % default_fix_version)\n    if issue_fix_version == \"\":\n        issue_fix_version = default_fix_version\n    issue_fix_version = issue_fix_version.strip()\n    return issue_fix_version\n\n\nCONFIG_FILE = \"~/.config/arrow/merge.conf\"\n\n\ndef load_configuration():\n    config = configparser.ConfigParser()\n    config.read(os.path.expanduser(CONFIG_FILE))\n    return config\n\n\ndef get_credentials(cmd):\n    token = None\n\n    config = load_configuration()\n    if \"jira\" in config.sections():\n        token = config[\"jira\"].get(\"token\")\n\n    # Fallback to environment variables\n    if not token:\n        token = os.environ.get(\"APACHE_JIRA_TOKEN\")\n\n    # Fallback to user tty prompt\n    if not token:\n        token = cmd.prompt(\"Env APACHE_JIRA_TOKEN not set, \"\n                           \"please enter your Jira API token \"\n                           \"(Jira personal access token):\")\n\n    return token\n\n\ndef connect_jira(cmd):\n    return jira.client.JIRA(options={'server': JIRA_API_BASE},\n                            token_auth=get_credentials(cmd))\n\n\ndef get_pr_num():\n    if len(sys.argv) == 2:\n        return sys.argv[1]\n\n    return input(\"Which pull request would you like to merge? (e.g. 34): \")\n\n\ndef cli():\n    # Location of your Arrow git clone\n    ARROW_HOME = os.path.abspath(os.path.dirname(__file__))\n    print(f\"ARROW_HOME = {ARROW_HOME}\")\n    print(f\"ORG_NAME = {ORG_NAME}\")\n    print(f\"PROJECT_NAME = {PROJECT_NAME}\")\n\n    cmd = CommandInput()\n\n    pr_num = get_pr_num()\n\n    os.chdir(ARROW_HOME)\n\n    github_api = GitHubAPI(PROJECT_NAME, cmd)\n\n    jira_con = connect_jira(cmd)\n    pr = PullRequest(cmd, github_api, ORG_NAME, jira_con, pr_num)\n\n    if pr.is_merged:\n        print(\"Pull request %s has already been merged\" % pr_num)\n        sys.exit(0)\n\n    if not pr.is_mergeable:\n        print(\"Pull request %s is not mergeable in its current form\" % pr_num)\n        sys.exit(1)\n\n    pr.show()\n\n    cmd.continue_maybe(\"Proceed with merging pull request #%s?\" % pr_num)\n\n    pr.merge()\n\n    if pr.issue is None:\n        print(\"Minor PR.  No issue to update.\\n\")\n        return\n\n    cmd.continue_maybe(\"Would you like to update the associated issue?\")\n    issue_comment = (\n        \"Issue resolved by pull request %s\\n%s\"\n        % (pr_num,\n           f\"https://github.com/{ORG_NAME}/{PROJECT_NAME}/pull/{pr_num}\")\n    )\n    fix_version = prompt_for_fix_version(cmd, pr.issue,\n                                         pr.maintenance_branches)\n    pr.issue.resolve(fix_version, issue_comment, pr.body)\n\n\nif __name__ == '__main__':\n    try:\n        cli()\n    except Exception:\n        raise\n", "dev/tasks/conda-recipes/clean.py": "import subprocess\nfrom typing import Set\n\nimport json\nimport pandas as pd\nimport sys\n\nfrom packaging.version import Version\n\n\nVERSIONS_TO_KEEP = 5\nDELETE_BEFORE = pd.Timestamp.now() - pd.Timedelta(days=30)\n\nPLATFORMS = [\n    \"linux-64\",\n    \"linux-aarch64\",\n    \"linux-ppc64le\",\n    \"osx-64\",\n    \"osx-arm64\",\n    \"win-64\",\n]\n\n\nclass CommandFailedException(Exception):\n\n    def __init__(self, cmdline, output):\n        self.cmdline = cmdline\n        self.output = output\n\n\ndef run_command(cmdline, **kwargs):\n    kwargs.setdefault('capture_output', True)\n    p = subprocess.run(cmdline, **kwargs)\n    if p.returncode != 0:\n        print(f\"Command {cmdline} returned non-zero exit status \"\n              f\"{p.returncode}\", file=sys.stderr)\n        output = \"\"\n        if p.stdout:\n            print(\"Stdout was:\\n\" + \"-\" * 70, file=sys.stderr)\n            output = p.stdout.decode().rstrip()\n            print(output, file=sys.stderr)\n            print(\"-\" * 70, file=sys.stderr)\n        if p.stderr:\n            print(\"Stderr was:\\n\" + \"-\" * 70, file=sys.stderr)\n            output = p.stderr.decode().rstrip()\n            print(p.stderr.decode().rstrip(), file=sys.stderr)\n            print(\"-\" * 70, file=sys.stderr)\n        raise CommandFailedException(cmdline=cmdline, output=output)\n    return p.stdout\n\n\ndef builds_to_delete(platform: str, to_delete: Set[str]) -> int:\n    try:\n        pkgs_json = run_command(\n            [\n                \"conda\",\n                \"search\",\n                \"--json\",\n                \"-c\",\n                \"arrow-nightlies\",\n                \"--override-channels\",\n                \"--subdir\",\n                platform\n            ],\n        )\n    except CommandFailedException as ex:\n        # If the command failed due to no packages found, return\n        # 0 builds to delete.\n        if \"PackagesNotFoundError\" in ex.output:\n            return 0\n        else:\n            sys.exit(1)\n\n    pkgs = json.loads(pkgs_json)\n    num_builds = 0\n\n    for package_name, builds in pkgs.items():\n        num_builds += len(builds)\n        builds = pd.DataFrame(builds)\n        builds[\"version\"] = builds[\"version\"].map(Version)\n        # May be NaN if package doesn't depend on Python\n        builds[\"py_version\"] = builds[\"build\"].str.extract(r'(py\\d+)')\n        builds[\"timestamp\"] = pd.to_datetime(builds['timestamp'], unit='ms')\n        builds[\"stale\"] = builds[\"timestamp\"] < DELETE_BEFORE\n        # Some packages can be present in several \"features\" (e.g. CUDA),\n        # others miss that column in which case we set a default value.\n        if \"track_features\" not in builds.columns:\n            if package_name == \"arrow-cpp-proc\":\n                # XXX arrow-cpp-proc puts the features in the build field...\n                builds[\"track_features\"] = builds[\"build\"]\n            else:\n                builds[\"track_features\"] = 0\n\n        # Detect old builds for each configuration:\n        # a product of (architecture, Python version, features).\n        for (subdir, python, features, stale), group in builds.groupby(\n                [\"subdir\", \"py_version\", \"track_features\", \"stale\"],\n                dropna=False):\n            del_candidates = []\n            if stale:\n                del_candidates = group\n            else:\n                group = group.sort_values(by=\"version\", ascending=False)\n                if len(group) > VERSIONS_TO_KEEP:\n                    del_candidates = group[VERSIONS_TO_KEEP:]\n\n            if len(del_candidates):\n                to_delete.update(\n                    f\"arrow-nightlies/{package_name}/\"\n                    + del_candidates[\"version\"].astype(str)\n                    + del_candidates[\"url\"].str.replace(\n                        \"https://conda.anaconda.org/arrow-nightlies\", \"\",\n                        regex=False\n                    )\n                )\n\n    return num_builds\n\n\nif __name__ == \"__main__\":\n    to_delete = set()\n    num_builds = 0\n    for platform in PLATFORMS:\n        num_builds += builds_to_delete(platform, to_delete)\n\n    to_delete = sorted(to_delete)\n\n    print(f\"{len(to_delete)} builds may be deleted out of {num_builds}\")\n    for name in to_delete:\n        print(f\"- {name}\")\n\n    if \"FORCE\" in sys.argv and len(to_delete) > 0:\n        print(\"Deleting ...\")\n        run_command([\"anaconda\", \"remove\", \"-f\"] + to_delete)\n", "dev/tasks/conda-recipes/arrow-cpp/test_read_parquet.py": "import pyarrow as pa\nimport pyarrow.parquet as pq\n\ntable = pa.Table.from_pydict({\"a\": [1, 2]})\npq.write_table(table, \"test.parquet\")\n", "dev/archery/setup.py": "#!/usr/bin/env python\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport functools\nimport operator\nimport sys\nfrom setuptools import setup, find_packages\n\n# pygit2>=1.14.0 requires python 3.9, so crossbow and all\n# both technically require python 3.9 \u2014 however we still need to\n# support 3.8 when using docker. When 3.8 is EOLed and we bump\n# to Python 3.9 this will resolve itself.\nif sys.version_info < (3, 8):\n    sys.exit('Python < 3.8 is not supported')\n\n# For pathlib.Path compatibility\njinja_req = 'jinja2>=2.11'\n\nextras = {\n    'benchmark': ['pandas'],\n    'crossbow': ['github3.py', jinja_req, 'pygit2>=1.14.0', 'requests',\n                 'ruamel.yaml', 'setuptools_scm<8.0.0'],\n    'crossbow-upload': ['github3.py', jinja_req, 'ruamel.yaml',\n                        'setuptools_scm'],\n    'docker': ['ruamel.yaml', 'python-dotenv'],\n    'integration': ['cffi'],\n    'integration-java': ['jpype1'],\n    'lint': ['numpydoc==1.1.0', 'autopep8', 'flake8==6.1.0', 'cython-lint',\n             'cmake_format==0.6.13', 'sphinx-lint==0.9.1'],\n    'numpydoc': ['numpydoc==1.1.0'],\n    'release': ['pygithub', jinja_req, 'jira', 'semver', 'gitpython'],\n}\nextras['bot'] = extras['crossbow'] + ['pygithub', 'jira']\nextras['all'] = list(set(functools.reduce(operator.add, extras.values())))\n\nsetup(\n    name='archery',\n    version=\"0.1.0\",\n    description='Apache Arrow Developers Tools',\n    url='http://github.com/apache/arrow',\n    maintainer='Arrow Developers',\n    maintainer_email='dev@arrow.apache.org',\n    packages=find_packages(),\n    include_package_data=True,\n    python_requires='>=3.8',\n    install_requires=['click>=7'],\n    tests_require=['pytest', 'responses'],\n    extras_require=extras,\n    entry_points='''\n        [console_scripts]\n        archery=archery.cli:archery\n    '''\n)\n", "dev/archery/conftest.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pathlib\n\nimport pytest\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        \"--enable-integration\",\n        action=\"store_true\",\n        default=False,\n        help=\"run slow tests\"\n    )\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(\n        \"markers\",\n        (\n            \"integration: mark test as integration tests involving more \"\n            \"extensive setup (only used for crossbow at the moment)\"\n        )\n    )\n\n\ndef pytest_collection_modifyitems(config, items):\n    if config.getoption(\"--enable-integration\"):\n        return\n    marker = pytest.mark.skip(reason=\"need --enable-integration option to run\")\n    for item in items:\n        if \"integration\" in item.keywords:\n            item.add_marker(marker)\n\n\n@pytest.fixture\ndef load_fixture(request):\n    current_test_directory = pathlib.Path(request.node.fspath).parent\n\n    def decoder(path):\n        with path.open('r') as fp:\n            if path.suffix == '.json':\n                import json\n                return json.load(fp)\n            elif path.suffix == '.yaml':\n                import yaml\n                return yaml.load(fp)\n            else:\n                return fp.read()\n\n    def loader(name, decoder=decoder):\n        path = current_test_directory / 'fixtures' / name\n        return decoder(path)\n\n    return loader\n", "dev/archery/archery/cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import namedtuple\nfrom io import StringIO\nimport click\nimport json\nimport logging\nimport os\nimport pathlib\nimport sys\n\nfrom .benchmark.codec import JsonEncoder\nfrom .benchmark.compare import RunnerComparator, DEFAULT_THRESHOLD\nfrom .benchmark.runner import CppBenchmarkRunner, JavaBenchmarkRunner\nfrom .compat import _import_pandas\nfrom .lang.cpp import CppCMakeDefinition, CppConfiguration\nfrom .utils.cli import ArrowBool, validate_arrow_sources, add_optional_command\nfrom .utils.lint import linter, python_numpydoc, LintValidationException\nfrom .utils.logger import logger, ctx as log_ctx\nfrom .utils.source import ArrowSources\nfrom .utils.tmpdir import tmpdir\n\n# Set default logging to INFO in command line.\nlogging.basicConfig(level=logging.INFO)\n\n\nBOOL = ArrowBool()\n\n\n@click.group(context_settings={\"help_option_names\": [\"-h\", \"--help\"]})\n@click.option(\"--debug\", type=BOOL, is_flag=True, default=False,\n              envvar='ARCHERY_DEBUG',\n              help=\"Increase logging with debugging output.\")\n@click.option(\"--pdb\", type=BOOL, is_flag=True, default=False,\n              help=\"Invoke pdb on uncaught exception.\")\n@click.option(\"-q\", \"--quiet\", type=BOOL, is_flag=True, default=False,\n              help=\"Silence executed commands.\")\n@click.pass_context\ndef archery(ctx, debug, pdb, quiet):\n    \"\"\" Apache Arrow developer utilities.\n\n    See sub-commands help with `archery <cmd> --help`.\n\n    \"\"\"\n    # Ensure ctx.obj exists\n    ctx.ensure_object(dict)\n\n    log_ctx.quiet = quiet\n    if debug:\n        logger.setLevel(logging.DEBUG)\n\n    ctx.obj['debug'] = debug\n\n    if pdb:\n        import pdb\n        sys.excepthook = lambda t, v, e: pdb.pm()\n\n\nbuild_dir_type = click.Path(dir_okay=True, file_okay=False, resolve_path=True)\n# Supported build types\nbuild_type = click.Choice([\"debug\", \"relwithdebinfo\", \"release\"],\n                          case_sensitive=False)\n# Supported warn levels\nwarn_level_type = click.Choice([\"everything\", \"checkin\", \"production\"],\n                               case_sensitive=False)\n\nsimd_level = click.Choice([\"NONE\", \"SSE4_2\", \"AVX2\", \"AVX512\"],\n                          case_sensitive=True)\n\n\ndef cpp_toolchain_options(cmd):\n    options = [\n        click.option(\"--cc\", metavar=\"<compiler>\", help=\"C compiler.\"),\n        click.option(\"--cxx\", metavar=\"<compiler>\", help=\"C++ compiler.\"),\n        click.option(\"--cxx-flags\", help=\"C++ compiler flags.\"),\n        click.option(\"--cpp-package-prefix\",\n                     help=(\"Value to pass for ARROW_PACKAGE_PREFIX and \"\n                           \"use ARROW_DEPENDENCY_SOURCE=SYSTEM\"))\n    ]\n    return _apply_options(cmd, options)\n\n\ndef java_toolchain_options(cmd):\n    options = [\n        click.option(\"--java-home\", metavar=\"<java_home>\",\n                     help=\"Path to Java Developers Kit.\"),\n        click.option(\"--java-options\", help=\"java compiler options.\"),\n    ]\n    return _apply_options(cmd, options)\n\n\ndef _apply_options(cmd, options):\n    for option in options:\n        cmd = option(cmd)\n    return cmd\n\n\n@archery.command(short_help=\"Initialize an Arrow C++ build\")\n@click.option(\"--src\", metavar=\"<arrow_src>\", default=None,\n              callback=validate_arrow_sources,\n              help=\"Specify Arrow source directory\")\n# toolchain\n@cpp_toolchain_options\n@click.option(\"--build-type\", default=None, type=build_type,\n              help=\"CMake's CMAKE_BUILD_TYPE\")\n@click.option(\"--build-static\", default=True, type=BOOL,\n              help=\"Build static libraries\")\n@click.option(\"--build-shared\", default=True, type=BOOL,\n              help=\"Build shared libraries\")\n@click.option(\"--build-unity\", default=True, type=BOOL,\n              help=\"Use CMAKE_UNITY_BUILD\")\n@click.option(\"--warn-level\", default=\"production\", type=warn_level_type,\n              help=\"Controls compiler warnings -W(no-)error.\")\n@click.option(\"--use-gold-linker\", default=True, type=BOOL,\n              help=\"Toggles ARROW_USE_LD_GOLD option.\")\n@click.option(\"--simd-level\", default=\"DEFAULT\", type=simd_level,\n              help=\"Toggles ARROW_SIMD_LEVEL option.\")\n# Tests and benchmarks\n@click.option(\"--with-tests\", default=True, type=BOOL,\n              help=\"Build with tests.\")\n@click.option(\"--with-benchmarks\", default=None, type=BOOL,\n              help=\"Build with benchmarks.\")\n@click.option(\"--with-examples\", default=None, type=BOOL,\n              help=\"Build with examples.\")\n@click.option(\"--with-integration\", default=None, type=BOOL,\n              help=\"Build with integration test executables.\")\n# Static checks\n@click.option(\"--use-asan\", default=None, type=BOOL,\n              help=\"Toggle ARROW_USE_ASAN sanitizer.\")\n@click.option(\"--use-tsan\", default=None, type=BOOL,\n              help=\"Toggle ARROW_USE_TSAN sanitizer.\")\n@click.option(\"--use-ubsan\", default=None, type=BOOL,\n              help=\"Toggle ARROW_USE_UBSAN sanitizer.\")\n@click.option(\"--with-fuzzing\", default=None, type=BOOL,\n              help=\"Toggle ARROW_FUZZING.\")\n# Components\n@click.option(\"--with-compute\", default=None, type=BOOL,\n              help=\"Build the Arrow compute module.\")\n@click.option(\"--with-csv\", default=None, type=BOOL,\n              help=\"Build the Arrow CSV parser module.\")\n@click.option(\"--with-cuda\", default=None, type=BOOL,\n              help=\"Build the Arrow CUDA extensions.\")\n@click.option(\"--with-dataset\", default=None, type=BOOL,\n              help=\"Build the Arrow dataset module.\")\n@click.option(\"--with-filesystem\", default=None, type=BOOL,\n              help=\"Build the Arrow filesystem layer.\")\n@click.option(\"--with-flight\", default=None, type=BOOL,\n              help=\"Build with Flight rpc support.\")\n@click.option(\"--with-gandiva\", default=None, type=BOOL,\n              help=\"Build with Gandiva expression compiler support.\")\n@click.option(\"--with-gcs\", default=None, type=BOOL,\n              help=\"Build Arrow with Google Cloud Storage (GCS) support.\")\n@click.option(\"--with-hdfs\", default=None, type=BOOL,\n              help=\"Build the Arrow HDFS bridge.\")\n@click.option(\"--with-hiveserver2\", default=None, type=BOOL,\n              help=\"Build the HiveServer2 client and arrow adapter.\")\n@click.option(\"--with-ipc\", default=None, type=BOOL,\n              help=\"Build the Arrow IPC extensions.\")\n@click.option(\"--with-json\", default=None, type=BOOL,\n              help=\"Build the Arrow JSON parser module.\")\n@click.option(\"--with-mimalloc\", default=None, type=BOOL,\n              help=\"Build the Arrow mimalloc based allocator.\")\n@click.option(\"--with-parquet\", default=None, type=BOOL,\n              help=\"Build with Parquet file support.\")\n@click.option(\"--with-python\", default=None, type=BOOL,\n              help=\"Build the Arrow CPython extensions.\")\n@click.option(\"--with-r\", default=None, type=BOOL,\n              help=\"Build the Arrow R extensions. This is not a CMake option, \"\n              \"it will toggle required options\")\n@click.option(\"--with-s3\", default=None, type=BOOL,\n              help=\"Build Arrow with S3 support.\")\n# Compressions\n@click.option(\"--with-brotli\", default=None, type=BOOL,\n              help=\"Build Arrow with brotli compression.\")\n@click.option(\"--with-bz2\", default=None, type=BOOL,\n              help=\"Build Arrow with bz2 compression.\")\n@click.option(\"--with-lz4\", default=None, type=BOOL,\n              help=\"Build Arrow with lz4 compression.\")\n@click.option(\"--with-snappy\", default=None, type=BOOL,\n              help=\"Build Arrow with snappy compression.\")\n@click.option(\"--with-zlib\", default=None, type=BOOL,\n              help=\"Build Arrow with zlib compression.\")\n@click.option(\"--with-zstd\", default=None, type=BOOL,\n              help=\"Build Arrow with zstd compression.\")\n# CMake extra feature\n@click.option(\"--cmake-extras\", type=str, multiple=True,\n              help=\"Extra flags/options to pass to cmake invocation. \"\n              \"Can be stacked\")\n@click.option(\"--install-prefix\", type=str,\n              help=\"Destination directory where files are installed. Expand to\"\n              \"CMAKE_INSTALL_PREFIX. Defaults to to $CONDA_PREFIX if the\"\n              \"variable exists.\")\n# misc\n@click.option(\"-f\", \"--force\", type=BOOL, is_flag=True, default=False,\n              help=\"Delete existing build directory if found.\")\n@click.option(\"--targets\", type=str, multiple=True,\n              help=\"Generator targets to run. Can be stacked.\")\n@click.argument(\"build_dir\", type=build_dir_type)\n@click.pass_context\ndef build(ctx, src, build_dir, force, targets, **kwargs):\n    \"\"\" Initialize a C++ build directory.\n\n    The build command creates a directory initialized with Arrow's cpp source\n    cmake and configuration. It can also optionally invoke the generator to\n    test the build (and used in scripts).\n\n    Note that archery will carry the caller environment. It will also not touch\n    an existing directory, one must use the `--force` option to remove the\n    existing directory.\n\n    Examples:\n\n    \\b\n    # Initialize build with clang8 and avx2 support in directory `clang8-build`\n    \\b\n    archery build --cc=clang-8 --cxx=clang++-8 --cxx-flags=-mavx2 clang8-build\n\n    \\b\n    # Builds and run test\n    archery build --targets=all --targets=test build\n    \"\"\"\n    # Arrow's cpp cmake configuration\n    conf = CppConfiguration(**kwargs)\n    # This is a closure around cmake invocation, e.g. calling `def.build()`\n    # yields a directory ready to be run with the generator\n    cmake_def = CppCMakeDefinition(src.cpp, conf)\n    # Create build directory\n    build = cmake_def.build(build_dir, force=force)\n\n    for target in targets:\n        build.run(target)\n\n\nLintCheck = namedtuple('LintCheck', ('option_name', 'help'))\n\nlint_checks = [\n    LintCheck('clang-format', \"Format C++ files with clang-format.\"),\n    LintCheck('clang-tidy', \"Lint C++ files with clang-tidy.\"),\n    LintCheck('cpplint', \"Lint C++ files with cpplint.\"),\n    LintCheck('iwyu', \"Lint changed C++ files with Include-What-You-Use.\"),\n    LintCheck('python',\n              \"Format and lint Python files with autopep8 and flake8.\"),\n    LintCheck('numpydoc', \"Lint Python files with numpydoc.\"),\n    LintCheck('cmake-format', \"Format CMake files with cmake-format.py.\"),\n    LintCheck('rat',\n              \"Check all sources files for license texts via Apache RAT.\"),\n    LintCheck('r', \"Lint R files.\"),\n    LintCheck('docker', \"Lint Dockerfiles with hadolint.\"),\n    LintCheck('docs', \"Lint docs with sphinx-lint.\"),\n]\n\n\ndef decorate_lint_command(cmd):\n    \"\"\"\n    Decorate the lint() command function to add individual per-check options.\n    \"\"\"\n    for check in lint_checks:\n        option = click.option(\"--{0}/--no-{0}\".format(check.option_name),\n                              default=None, help=check.help)\n        cmd = option(cmd)\n    return cmd\n\n\n@archery.command(short_help=\"Check Arrow source tree for errors\")\n@click.option(\"--src\", metavar=\"<arrow_src>\", default=None,\n              callback=validate_arrow_sources,\n              help=\"Specify Arrow source directory\")\n@click.option(\"--fix\", is_flag=True, type=BOOL, default=False,\n              help=\"Toggle fixing the lint errors if the linter supports it.\")\n@click.option(\"--iwyu_all\", is_flag=True, type=BOOL, default=False,\n              help=\"Run IWYU on all C++ files if enabled\")\n@click.option(\"-a\", \"--all\", is_flag=True, default=False,\n              help=\"Enable all checks.\")\n@click.argument(\"path\", required=False)\n@decorate_lint_command\n@click.pass_context\ndef lint(ctx, src, fix, iwyu_all, path, **checks):\n    if checks.pop('all'):\n        # \"--all\" is given => enable all non-selected checks\n        for k, v in checks.items():\n            if v is None:\n                checks[k] = True\n    if not any(checks.values()):\n        raise click.UsageError(\n            \"Need to enable at least one lint check (try --help)\")\n    try:\n        linter(src, fix, iwyu_all=iwyu_all, path=path, **checks)\n    except LintValidationException:\n        sys.exit(1)\n\n\ndef _flatten_numpydoc_rules(rules):\n    flattened = []\n    for rule in rules:\n        flattened.extend(filter(None, rule.split(',')))\n    return flattened\n\n\n@archery.command(short_help=\"Lint python docstring with NumpyDoc\")\n@click.argument('symbols', nargs=-1)\n@click.option(\"--src\", metavar=\"<arrow_src>\", default=None,\n              callback=validate_arrow_sources,\n              help=\"Specify Arrow source directory\")\n@click.option(\"--allow-rule\", \"-a\", multiple=True,\n              help=\"Allow only these rules (can be comma-separated)\")\n@click.option(\"--disallow-rule\", \"-d\", multiple=True,\n              help=\"Disallow these rules (can be comma-separated)\")\ndef numpydoc(src, symbols, allow_rule, disallow_rule):\n    \"\"\"\n    Pass list of modules or symbols as arguments to restrict the validation.\n\n    By default all modules of pyarrow are tried to be validated.\n\n    Examples\n    --------\n    archery numpydoc pyarrow.dataset\n    archery numpydoc pyarrow.csv pyarrow.json pyarrow.parquet\n    archery numpydoc pyarrow.array\n    \"\"\"\n    disallow_rule = disallow_rule or {'GL01', 'SA01', 'EX01', 'ES01'}\n    try:\n        results = python_numpydoc(\n            symbols, allow_rules=_flatten_numpydoc_rules(allow_rule),\n            disallow_rules=_flatten_numpydoc_rules(disallow_rule))\n        for result in results:\n            result.ok()\n    except LintValidationException:\n        sys.exit(1)\n\n\n@archery.group()\n@click.pass_context\ndef benchmark(ctx):\n    \"\"\" Arrow benchmarking.\n\n    Use the diff sub-command to benchmark revisions, and/or build directories.\n    \"\"\"\n    pass\n\n\ndef benchmark_common_options(cmd):\n    def check_language(ctx, param, value):\n        if value not in {\"cpp\", \"java\"}:\n            raise click.BadParameter(\"cpp or java is supported now\")\n        return value\n\n    options = [\n        click.option(\"--src\", metavar=\"<arrow_src>\", show_default=True,\n                     default=None, callback=validate_arrow_sources,\n                     help=\"Specify Arrow source directory\"),\n        click.option(\"--preserve\", type=BOOL, default=False, show_default=True,\n                     is_flag=True,\n                     help=\"Preserve workspace for investigation.\"),\n        click.option(\"--output\", metavar=\"<output>\",\n                     type=click.File(\"w\", encoding=\"utf8\"), default=None,\n                     help=\"Capture output result into file.\"),\n        click.option(\"--language\", metavar=\"<lang>\", type=str, default=\"cpp\",\n                     show_default=True, callback=check_language,\n                     help=\"Specify target language for the benchmark\"),\n        click.option(\"--build-extras\", type=str, multiple=True,\n                     help=\"Extra flags/options to pass to mvn build. \"\n                     \"Can be stacked. For language=java\"),\n        click.option(\"--benchmark-extras\", type=str, multiple=True,\n                     help=\"Extra flags/options to pass to mvn benchmark. \"\n                     \"Can be stacked. For language=java\"),\n        click.option(\"--cmake-extras\", type=str, multiple=True,\n                     help=\"Extra flags/options to pass to cmake invocation. \"\n                     \"Can be stacked. For language=cpp\"),\n        click.option(\"--cpp-benchmark-extras\", type=str, multiple=True,\n                     help=\"Extra flags/options to pass to C++ benchmark executables. \"\n                     \"Can be stacked. For language=cpp\"),\n    ]\n\n    cmd = java_toolchain_options(cmd)\n    cmd = cpp_toolchain_options(cmd)\n    return _apply_options(cmd, options)\n\n\ndef benchmark_filter_options(cmd):\n    options = [\n        click.option(\"--suite-filter\", metavar=\"<regex>\", show_default=True,\n                     type=str, default=None,\n                     help=\"Regex filtering benchmark suites.\"),\n        click.option(\"--benchmark-filter\", metavar=\"<regex>\",\n                     show_default=True, type=str, default=None,\n                     help=\"Regex filtering benchmarks.\")\n    ]\n    return _apply_options(cmd, options)\n\n\n@benchmark.command(name=\"list\", short_help=\"List benchmark suite\")\n@click.argument(\"rev_or_path\", metavar=\"[<rev_or_path>]\",\n                default=\"WORKSPACE\", required=False)\n@benchmark_common_options\n@click.pass_context\ndef benchmark_list(ctx, rev_or_path, src, preserve, output, cmake_extras,\n                   java_home, java_options, build_extras, benchmark_extras,\n                   cpp_benchmark_extras, language, **kwargs):\n    \"\"\" List benchmark suite.\n    \"\"\"\n    with tmpdir(preserve=preserve) as root:\n        logger.debug(\"Running benchmark {}\".format(rev_or_path))\n\n        if language == \"cpp\":\n            conf = CppBenchmarkRunner.default_configuration(\n                cmake_extras=cmake_extras, **kwargs)\n\n            runner_base = CppBenchmarkRunner.from_rev_or_path(\n                src, root, rev_or_path, conf,\n                benchmark_extras=cpp_benchmark_extras)\n\n        elif language == \"java\":\n            for key in {'cpp_package_prefix', 'cxx_flags', 'cxx', 'cc'}:\n                del kwargs[key]\n            conf = JavaBenchmarkRunner.default_configuration(\n                java_home=java_home, java_options=java_options,\n                build_extras=build_extras, benchmark_extras=benchmark_extras,\n                **kwargs)\n\n            runner_base = JavaBenchmarkRunner.from_rev_or_path(\n                src, root, rev_or_path, conf)\n\n        for b in runner_base.list_benchmarks:\n            click.echo(b, file=output or sys.stdout)\n\n\n@benchmark.command(name=\"run\", short_help=\"Run benchmark suite\")\n@click.argument(\"rev_or_path\", metavar=\"[<rev_or_path>]\",\n                default=\"WORKSPACE\", required=False)\n@benchmark_common_options\n@benchmark_filter_options\n@click.option(\"--repetitions\", type=int, default=-1,\n              help=(\"Number of repetitions of each benchmark. Increasing \"\n                    \"may improve result precision. \"\n                    \"[default: 1 for cpp, 5 for java]\"))\n@click.option(\"--repetition-min-time\", type=float, default=None,\n              help=(\"Minimum duration of each repetition in seconds. \"\n                    \"Currently only supported for language=cpp. \"\n                    \"[default: use runner-specific defaults]\"))\n@click.pass_context\ndef benchmark_run(ctx, rev_or_path, src, preserve, output, cmake_extras,\n                  java_home, java_options, build_extras, benchmark_extras,\n                  language, suite_filter, benchmark_filter, repetitions,\n                  repetition_min_time, cpp_benchmark_extras, **kwargs):\n    \"\"\" Run benchmark suite.\n\n    This command will run the benchmark suite for a single build. This is\n    used to capture (and/or publish) the results.\n\n    The caller can optionally specify a target which is either a git revision\n    (commit, tag, special values like HEAD) or a cmake build directory.\n\n    When a commit is referenced, a local clone of the arrow sources (specified\n    via --src) is performed and the proper branch is created. This is done in\n    a temporary directory which can be left intact with the `--preserve` flag.\n\n    The special token \"WORKSPACE\" is reserved to specify the current git\n    workspace. This imply that no clone will be performed.\n\n    Examples:\n\n    \\b\n    # Run the benchmarks on current git workspace\n    \\b\n    archery benchmark run\n\n    \\b\n    # Run the benchmarks on an existing build directory\n    \\b\n    archery benchmark run /build/cpp\n\n    \\b\n    # Run the benchmarks on current previous commit\n    \\b\n    archery benchmark run HEAD~1\n\n    \\b\n    # Run the benchmarks on current git workspace and output results as a JSON file.\n    \\b\n    archery benchmark run --output=run.json\n    \"\"\"\n    with tmpdir(preserve=preserve) as root:\n        logger.debug(\"Running benchmark {}\".format(rev_or_path))\n\n        if language == \"cpp\":\n            conf = CppBenchmarkRunner.default_configuration(\n                cmake_extras=cmake_extras, **kwargs)\n\n            repetitions = repetitions if repetitions != -1 else 1\n            runner_base = CppBenchmarkRunner.from_rev_or_path(\n                src, root, rev_or_path, conf,\n                repetitions=repetitions, repetition_min_time=repetition_min_time,\n                suite_filter=suite_filter, benchmark_filter=benchmark_filter,\n                benchmark_extras=cpp_benchmark_extras)\n\n        elif language == \"java\":\n            for key in {'cpp_package_prefix', 'cxx_flags', 'cxx', 'cc'}:\n                del kwargs[key]\n            conf = JavaBenchmarkRunner.default_configuration(\n                java_home=java_home, java_options=java_options,\n                build_extras=build_extras, benchmark_extras=benchmark_extras,\n                **kwargs)\n\n            repetitions = repetitions if repetitions != -1 else 5\n            runner_base = JavaBenchmarkRunner.from_rev_or_path(\n                src, root, rev_or_path, conf,\n                repetitions=repetitions,\n                benchmark_filter=benchmark_filter)\n\n        # XXX for some reason, the benchmark runner only does its work\n        # when asked to JSON-serialize the results, so produce a JSON\n        # output even when none is requested.\n        json_out = json.dumps(runner_base, cls=JsonEncoder)\n        if output is not None:\n            output.write(json_out)\n\n\n@benchmark.command(name=\"diff\", short_help=\"Compare benchmark suites\")\n@benchmark_common_options\n@benchmark_filter_options\n@click.option(\"--threshold\", type=float, default=DEFAULT_THRESHOLD,\n              show_default=True,\n              help=\"Regression failure threshold in percentage.\")\n@click.option(\"--repetitions\", type=int, default=1, show_default=True,\n              help=(\"Number of repetitions of each benchmark. Increasing \"\n                    \"may improve result precision. \"\n                    \"[default: 1 for cpp, 5 for java\"))\n@click.option(\"--no-counters\", type=BOOL, default=False, is_flag=True,\n              help=\"Hide counters field in diff report.\")\n@click.argument(\"contender\", metavar=\"[<contender>\",\n                default=ArrowSources.WORKSPACE, required=False)\n@click.argument(\"baseline\", metavar=\"[<baseline>]]\", default=\"origin/HEAD\",\n                required=False)\n@click.pass_context\ndef benchmark_diff(ctx, src, preserve, output, language, cmake_extras,\n                   suite_filter, benchmark_filter, repetitions, no_counters,\n                   java_home, java_options, build_extras, benchmark_extras,\n                   cpp_benchmark_extras, threshold, contender, baseline,\n                   **kwargs):\n    \"\"\"Compare (diff) benchmark runs.\n\n    This command acts like git-diff but for benchmark results.\n\n    The caller can optionally specify both the contender and the baseline. If\n    unspecified, the contender will default to the current workspace (like git)\n    and the baseline will default to the mainline development branch (i.e.\n    default git branch).\n\n    Each target (contender or baseline) can either be a git revision\n    (commit, tag, special values like HEAD) or a cmake build directory. This\n    allow comparing git commits, and/or different compilers and/or compiler\n    flags.\n\n    When a commit is referenced, a local clone of the arrow sources (specified\n    via --src) is performed and the proper branch is created. This is done in\n    a temporary directory which can be left intact with the `--preserve` flag.\n\n    The special token \"WORKSPACE\" is reserved to specify the current git\n    workspace. This imply that no clone will be performed.\n\n    Examples:\n\n    \\b\n    # Compare workspace (contender) against the mainline development branch\n    # (baseline)\n    \\b\n    archery benchmark diff\n\n    \\b\n    # Compare the mainline development branch (contender) against the latest\n    # version (baseline)\n    \\b\n    export LAST=$(git tag -l \"apache-arrow-[0-9]*\" | sort -rV | head -1)\n    \\b\n    archery benchmark diff <default-branch> \"$LAST\"\n\n    \\b\n    # Compare g++7 (contender) with clang++-8 (baseline) builds\n    \\b\n    archery build --with-benchmarks=true \\\\\n            --cxx-flags=-ftree-vectorize \\\\\n            --cc=gcc-7 --cxx=g++-7 gcc7-build\n    \\b\n    archery build --with-benchmarks=true \\\\\n            --cxx-flags=-flax-vector-conversions \\\\\n            --cc=clang-8 --cxx=clang++-8 clang8-build\n    \\b\n    archery benchmark diff gcc7-build clang8-build\n\n    \\b\n    # Compare default targets but scoped to the suites matching\n    # `^arrow-compute-aggregate` and benchmarks matching `(Sum|Mean)Kernel`.\n    \\b\n    archery benchmark diff --suite-filter=\"^arrow-compute-aggregate\" \\\\\n            --benchmark-filter=\"(Sum|Mean)Kernel\"\n\n    \\b\n    # Capture result in file `result.json`\n    \\b\n    archery benchmark diff --output=result.json\n    \\b\n    # Equivalently with no stdout clutter.\n    archery --quiet benchmark diff > result.json\n\n    \\b\n    # Comparing with a cached results from `archery benchmark run`\n    \\b\n    archery benchmark run --output=run.json HEAD~1\n    \\b\n    # This should not recompute the benchmark from run.json\n    archery --quiet benchmark diff WORKSPACE run.json > result.json\n    \"\"\"\n    with tmpdir(preserve=preserve) as root:\n        logger.debug(\"Comparing {} (contender) with {} (baseline)\"\n                     .format(contender, baseline))\n\n        if language == \"cpp\":\n            conf = CppBenchmarkRunner.default_configuration(\n                cmake_extras=cmake_extras, **kwargs)\n\n            repetitions = repetitions if repetitions != -1 else 1\n            runner_cont = CppBenchmarkRunner.from_rev_or_path(\n                src, root, contender, conf,\n                repetitions=repetitions,\n                suite_filter=suite_filter,\n                benchmark_filter=benchmark_filter,\n                benchmark_extras=cpp_benchmark_extras)\n            runner_base = CppBenchmarkRunner.from_rev_or_path(\n                src, root, baseline, conf,\n                repetitions=repetitions,\n                suite_filter=suite_filter,\n                benchmark_filter=benchmark_filter,\n                benchmark_extras=cpp_benchmark_extras)\n\n        elif language == \"java\":\n            for key in {'cpp_package_prefix', 'cxx_flags', 'cxx', 'cc'}:\n                del kwargs[key]\n            conf = JavaBenchmarkRunner.default_configuration(\n                java_home=java_home, java_options=java_options,\n                build_extras=build_extras, benchmark_extras=benchmark_extras,\n                **kwargs)\n\n            repetitions = repetitions if repetitions != -1 else 5\n            runner_cont = JavaBenchmarkRunner.from_rev_or_path(\n                src, root, contender, conf,\n                repetitions=repetitions,\n                benchmark_filter=benchmark_filter)\n            runner_base = JavaBenchmarkRunner.from_rev_or_path(\n                src, root, baseline, conf,\n                repetitions=repetitions,\n                benchmark_filter=benchmark_filter)\n\n        runner_comp = RunnerComparator(runner_cont, runner_base, threshold)\n\n        # TODO(kszucs): test that the output is properly formatted jsonlines\n        comparisons_json = _get_comparisons_as_json(runner_comp.comparisons)\n        ren_counters = language == \"java\"\n        formatted = _format_comparisons_with_pandas(comparisons_json,\n                                                    no_counters, ren_counters)\n        print(formatted, file=output or sys.stdout)\n\n\ndef _get_comparisons_as_json(comparisons):\n    buf = StringIO()\n    for comparator in comparisons:\n        json.dump(comparator, buf, cls=JsonEncoder)\n        buf.write(\"\\n\")\n\n    return buf.getvalue()\n\n\ndef _format_comparisons_with_pandas(comparisons_json, no_counters,\n                                    ren_counters):\n    pd = _import_pandas()\n    df = pd.read_json(StringIO(comparisons_json), lines=True)\n    # parse change % so we can sort by it\n    df['change %'] = df.pop('change').str[:-1].map(float)\n    first_regression = len(df) - df['regression'].sum()\n\n    fields = ['benchmark', 'baseline', 'contender', 'change %']\n    if not no_counters:\n        fields += ['counters']\n\n    df = df[fields]\n    if ren_counters:\n        df = df.rename(columns={'counters': 'configurations'})\n    df = df.sort_values(by='change %', ascending=False)\n\n    def labelled(title, df):\n        if len(df) == 0:\n            return ''\n        title += ': ({})'.format(len(df))\n        df_str = df.to_string(index=False)\n        bar = '-' * df_str.index('\\n')\n        return '\\n'.join([bar, title, bar, df_str])\n\n    return '\\n\\n'.join([labelled('Non-regressions', df[:first_regression]),\n                        labelled('Regressions', df[first_regression:])])\n\n\n# ----------------------------------------------------------------------\n# Integration testing\n\ndef _set_default(opt, default):\n    if opt is None:\n        return default\n    return opt\n\n\n@archery.command(short_help=\"Execute protocol and Flight integration tests\")\n@click.option('--with-all', is_flag=True, default=False,\n              help=('Include all known languages by default '\n                    'in integration tests'))\n@click.option('--random-seed', type=int, default=12345,\n              help=\"Seed for PRNG when generating test data\")\n@click.option('--with-cpp', type=bool, default=False,\n              help='Include C++ in integration tests')\n@click.option('--with-csharp', type=bool, default=False,\n              help='Include C# in integration tests')\n@click.option('--with-java', type=bool, default=False,\n              help='Include Java in integration tests')\n@click.option('--with-js', type=bool, default=False,\n              help='Include JavaScript in integration tests')\n@click.option('--with-go', type=bool, default=False,\n              help='Include Go in integration tests')\n@click.option('--with-nanoarrow', type=bool, default=False,\n              help='Include nanoarrow in integration tests',\n              envvar=\"ARCHERY_INTEGRATION_WITH_NANOARROW\")\n@click.option('--with-rust', type=bool, default=False,\n              help='Include Rust in integration tests',\n              envvar=\"ARCHERY_INTEGRATION_WITH_RUST\")\n@click.option('--write_generated_json', default=\"\",\n              help='Generate test JSON to indicated path')\n@click.option('--run-ipc', is_flag=True, default=False,\n              help='Run IPC integration tests')\n@click.option('--run-flight', is_flag=True, default=False,\n              help='Run Flight integration tests')\n@click.option('--run-c-data', is_flag=True, default=False,\n              help='Run C Data Interface integration tests')\n@click.option('--debug', is_flag=True, default=False,\n              help='Run executables in debug mode as relevant')\n@click.option('--serial', is_flag=True, default=False,\n              help='Run tests serially, rather than in parallel')\n@click.option('--tempdir', default=None,\n              help=('Directory to use for writing '\n                    'integration test temporary files'))\n@click.option('stop_on_error', '-x', '--stop-on-error',\n              is_flag=True, default=False,\n              help='Stop on first error')\n@click.option('--gold-dirs', multiple=True,\n              help=\"gold integration test file paths\")\n@click.option('-k', '--match',\n              help=(\"Substring for test names to include in run, \"\n                    \"e.g. -k primitive\"))\ndef integration(with_all=False, random_seed=12345, **args):\n    from .integration.runner import write_js_test_json, run_all_tests\n    import numpy as np\n\n    # FIXME(bkietz) Include help strings for individual testers.\n    # For example, CPPTester's ARROW_CPP_EXE_PATH environment variable.\n\n    # Make runs involving data generation deterministic\n    np.random.seed(random_seed)\n\n    gen_path = args['write_generated_json']\n\n    languages = ['cpp', 'csharp', 'java', 'js', 'go', 'nanoarrow', 'rust']\n    formats = ['ipc', 'flight', 'c_data']\n\n    enabled_languages = 0\n    for lang in languages:\n        param = f'with_{lang}'\n        if with_all:\n            args[param] = with_all\n        enabled_languages += args[param]\n\n    enabled_formats = 0\n    for fmt in formats:\n        param = f'run_{fmt}'\n        enabled_formats += args[param]\n\n    if gen_path:\n        # XXX See GH-37575: this option is only used by the JS test suite\n        # and might not be useful anymore.\n        os.makedirs(gen_path, exist_ok=True)\n        write_js_test_json(gen_path)\n    else:\n        if enabled_formats == 0:\n            raise click.UsageError(\n                \"Need to enable at least one format to test \"\n                \"(IPC, Flight, C Data Interface); try --help\")\n        if enabled_languages == 0:\n            raise click.UsageError(\n                \"Need to enable at least one language to test; try --help\")\n        run_all_tests(**args)\n\n\n@archery.command()\n@click.option('--arrow-token', envvar='ARROW_GITHUB_TOKEN',\n              help='OAuth token for responding comment in the arrow repo')\n@click.option('--committers-file', '-c', type=click.File('r', encoding='utf8'))\n@click.option('--event-name', '-n', required=True)\n@click.option('--event-payload', '-p', type=click.File('r', encoding='utf8'),\n              default='-', required=True)\ndef trigger_bot(arrow_token, committers_file, event_name, event_payload):\n    from .bot import CommentBot, PullRequestWorkflowBot, actions\n    from ruamel.yaml import YAML\n\n    event_payload = json.loads(event_payload.read())\n    if 'comment' in event_name:\n        bot = CommentBot(name='github-actions', handler=actions, token=arrow_token)\n        bot.handle(event_name, event_payload)\n    else:\n        committers = None\n        if committers_file:\n            committers = [committer['alias']\n                          for committer in YAML().load(committers_file)]\n        bot = PullRequestWorkflowBot(event_name, event_payload, token=arrow_token,\n                                     committers=committers)\n        bot.handle()\n\n\n@archery.group(\"linking\")\n@click.pass_obj\ndef linking(obj):\n    \"\"\"\n    Quick and dirty utilities for checking library linkage.\n    \"\"\"\n    pass\n\n\n@linking.command(\"check-dependencies\")\n@click.argument(\"paths\", nargs=-1)\n@click.option(\"--allow\", \"-a\", \"allowed\", multiple=True,\n              help=\"Name of the allowed libraries\")\n@click.option(\"--disallow\", \"-d\", \"disallowed\", multiple=True,\n              help=\"Name of the disallowed libraries\")\n@click.pass_obj\ndef linking_check_dependencies(obj, allowed, disallowed, paths):\n    from .linking import check_dynamic_library_dependencies, DependencyError\n\n    allowed, disallowed = set(allowed), set(disallowed)\n    try:\n        for path in map(pathlib.Path, paths):\n            check_dynamic_library_dependencies(path, allowed=allowed,\n                                               disallowed=disallowed)\n    except DependencyError as e:\n        raise click.ClickException(str(e))\n\n\nadd_optional_command(\"docker\", module=\".docker.cli\", function=\"docker\",\n                     parent=archery)\nadd_optional_command(\"release\", module=\".release.cli\", function=\"release\",\n                     parent=archery)\nadd_optional_command(\"crossbow\", module=\".crossbow.cli\", function=\"crossbow\",\n                     parent=archery)\n\n\nif __name__ == \"__main__\":\n    archery(obj={})\n", "dev/archery/archery/linking.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport platform\nimport subprocess\n\nfrom .utils.command import Command\n\n\n_ldd = Command(\"ldd\")\n_otool = Command(\"otool\")\n\n\nclass DependencyError(Exception):\n    pass\n\n\nclass DynamicLibrary:\n\n    def __init__(self, path):\n        self.path = path\n\n    def list_dependencies(self):\n        \"\"\"\n        List the full name of the library dependencies.\n        \"\"\"\n        system = platform.system()\n        if system == \"Linux\":\n            result = _ldd.run(self.path, stdout=subprocess.PIPE)\n            lines = result.stdout.splitlines()\n            return [ll.split(None, 1)[0].decode() for ll in lines]\n        elif system == \"Darwin\":\n            result = _otool.run(\"-L\", self.path, stdout=subprocess.PIPE)\n            lines = result.stdout.splitlines()\n            return [dl.split(None, 1)[0].decode() for dl in lines]\n        else:\n            raise ValueError(f\"{platform} is not supported\")\n\n    def list_dependency_names(self):\n        \"\"\"\n        List the truncated names of the dynamic library dependencies.\n        \"\"\"\n        names = []\n        for dependency in self.list_dependencies():\n            *_, library = dependency.rsplit(\"/\", 1)\n            name, *_ = library.split(\".\", 1)\n            names.append(name)\n        return names\n\n\ndef check_dynamic_library_dependencies(path, allowed, disallowed):\n    dylib = DynamicLibrary(path)\n    for dep in dylib.list_dependency_names():\n        if allowed and dep not in allowed:\n            raise DependencyError(\n                f\"Unexpected shared dependency found in {dylib.path}: `{dep}`\"\n            )\n        if disallowed and dep in disallowed:\n            raise DependencyError(\n                f\"Disallowed shared dependency found in {dylib.path}: `{dep}`\"\n            )\n", "dev/archery/archery/testing.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom contextlib import contextmanager\nimport os\nfrom unittest import mock\nimport re\n\n\nclass DotDict(dict):\n\n    def __getattr__(self, key):\n        try:\n            item = self[key]\n        except KeyError:\n            raise AttributeError(key)\n        if isinstance(item, dict):\n            return DotDict(item)\n        else:\n            return item\n\n\nclass PartialEnv(dict):\n\n    def __eq__(self, other):\n        return self.items() <= other.items()\n\n\n_mock_call_type = type(mock.call())\n\n\ndef _ensure_mock_call_object(obj, **kwargs):\n    if isinstance(obj, _mock_call_type):\n        return obj\n    elif isinstance(obj, str):\n        cmd = re.split(r\"\\s+\", obj)\n        return mock.call(cmd, **kwargs)\n    elif isinstance(obj, list):\n        return mock.call(obj, **kwargs)\n    else:\n        raise TypeError(obj)\n\n\nclass SuccessfulSubprocessResult:\n\n    def check_returncode(self):\n        return\n\n\n@contextmanager\ndef assert_subprocess_calls(expected_commands_or_calls, **kwargs):\n    calls = [\n        _ensure_mock_call_object(obj, **kwargs)\n        for obj in expected_commands_or_calls\n    ]\n    with mock.patch('subprocess.run', autospec=True) as run:\n        run.return_value = SuccessfulSubprocessResult()\n        yield run\n        run.assert_has_calls(calls)\n\n\n@contextmanager\ndef override_env(mapping):\n    original = os.environ\n    try:\n        os.environ = dict(os.environ, **mapping)\n        yield os.environ\n    finally:\n        os.environ = original\n", "dev/archery/archery/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "dev/archery/archery/compat.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pathlib\nimport sys\n\n\ndef _is_path_like(path):\n    return isinstance(path, str) or hasattr(path, '__fspath__')\n\n\ndef _ensure_path(path):\n    if isinstance(path, pathlib.Path):\n        return path\n    else:\n        return pathlib.Path(_stringify_path(path))\n\n\ndef _stringify_path(path):\n    \"\"\"\n    Convert *path* to a string or unicode path if possible.\n    \"\"\"\n    if isinstance(path, str):\n        return path\n\n    # checking whether path implements the filesystem protocol\n    try:\n        return path.__fspath__()\n    except AttributeError:\n        pass\n\n    raise TypeError(\"not a path-like object\")\n\n\ndef _import_pandas():\n    # ARROW-13425: avoid importing PyArrow from Pandas\n    sys.modules['pyarrow'] = None\n    import pandas as pd\n    return pd\n\n\ndef _get_module(obj, *, default=None):\n    \"\"\"\n    Try to find the name of the module *obj* is defined on.\n    \"\"\"\n    try:\n        return obj.__module__\n    except AttributeError:\n        # Might be a method/property descriptor as generated by Cython,\n        # look up the enclosing class.\n        try:\n            return obj.__objclass__.__module__\n        except AttributeError:\n            return default\n", "dev/archery/archery/bot.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport enum\nimport os\nimport shlex\nfrom pathlib import Path\nfrom functools import lru_cache, partial\nimport tempfile\n\nimport click\nimport github\n\nfrom .utils.git import git\nfrom .utils.logger import logger\nfrom .crossbow import Repo, Queue, Config, Target, Job, CommentReport\n\n\ndef cached_property(fn):\n    return property(lru_cache(maxsize=1)(fn))\n\n\nclass EventError(Exception):\n    pass\n\n\nclass CommandError(Exception):\n\n    def __init__(self, message):\n        self.message = message\n\n\nclass _CommandMixin:\n\n    def get_help_option(self, ctx):\n        def show_help(ctx, param, value):\n            if value and not ctx.resilient_parsing:\n                raise click.UsageError(ctx.get_help())\n        option = super().get_help_option(ctx)\n        option.callback = show_help\n        return option\n\n    def __call__(self, message, **kwargs):\n        args = shlex.split(message)\n        try:\n            with self.make_context(self.name, args=args, obj=kwargs) as ctx:\n                return self.invoke(ctx)\n        except click.ClickException as e:\n            raise CommandError(e.format_message())\n\n\nclass Command(_CommandMixin, click.Command):\n    pass\n\n\nclass Group(_CommandMixin, click.Group):\n\n    def command(self, *args, **kwargs):\n        kwargs.setdefault('cls', Command)\n        return super().command(*args, **kwargs)\n\n    def group(self, *args, **kwargs):\n        kwargs.setdefault('cls', Group)\n        return super().group(*args, **kwargs)\n\n    def parse_args(self, ctx, args):\n        if not args and self.no_args_is_help and not ctx.resilient_parsing:\n            raise click.UsageError(ctx.get_help())\n        return super().parse_args(ctx, args)\n\n\ncommand = partial(click.command, cls=Command)\ngroup = partial(click.group, cls=Group)\n\n\nLABEL_PREFIX = \"awaiting\"\n\n\n@enum.unique\nclass PullRequestState(enum.Enum):\n    \"\"\"State of a pull request.\"\"\"\n\n    review = f\"{LABEL_PREFIX} review\"\n    committer_review = f\"{LABEL_PREFIX} committer review\"\n    changes = f\"{LABEL_PREFIX} changes\"\n    change_review = f\"{LABEL_PREFIX} change review\"\n    merge = f\"{LABEL_PREFIX} merge\"\n\n\nCOMMITTER_ROLES = {'OWNER', 'MEMBER'}\n\n\nclass PullRequestWorkflowBot:\n\n    def __init__(self, event_name, event_payload, token=None, committers=None):\n        kwargs = {}\n        if token is not None:\n            kwargs[\"auth\"] = github.Auth.Token(token)\n        self.github = github.Github(**kwargs)\n        self.event_name = event_name\n        self.event_payload = event_payload\n        self.committers = committers\n\n    @cached_property\n    def pull(self):\n        \"\"\"\n        Returns a github.PullRequest object associated with the event.\n        \"\"\"\n        return self.repo.get_pull(self.event_payload['pull_request']['number'])\n\n    @cached_property\n    def repo(self):\n        return self.github.get_repo(self.event_payload['repository']['id'], lazy=True)\n\n    def is_committer(self, action):\n        \"\"\"\n        Returns whether the author of the action is a committer or not.\n        If the list of committer usernames is not available it will use the\n        author_association as a fallback mechanism.\n        \"\"\"\n        if self.committers:\n            return (self.event_payload[action]['user']['login'] in\n                    self.committers)\n        return (self.event_payload[action]['author_association'] in\n                COMMITTER_ROLES)\n\n    def handle(self):\n        current_state = None\n        try:\n            current_state = self.get_current_state()\n        except EventError:\n            # In case of error (more than one state) we clear state labels\n            # only possible if a label has been manually added.\n            self.clear_current_state()\n        next_state = self.compute_next_state(current_state)\n        if not current_state or current_state != next_state:\n            if current_state:\n                self.clear_current_state()\n            self.set_state(next_state)\n\n    def get_current_state(self):\n        \"\"\"\n        Returns a PullRequestState with the current PR state label\n        based on label starting with LABEL_PREFIX.\n        If more than one label is found raises EventError.\n        If no label is found returns None.\n        \"\"\"\n        states = [label.name for label in self.pull.get_labels()\n                  if label.name.startswith(LABEL_PREFIX)]\n        if len(states) > 1:\n            raise EventError(f\"PR cannot be on more than one states - {states}\")\n        elif states:\n            return PullRequestState(states[0])\n\n    def clear_current_state(self):\n        \"\"\"\n        Removes all existing labels starting with LABEL_PREFIX\n        \"\"\"\n        for label in self.pull.get_labels():\n            if label.name.startswith(LABEL_PREFIX):\n                self.pull.remove_from_labels(label)\n\n    def compute_next_state(self, current_state):\n        \"\"\"\n        Returns the expected next state based on the event and\n        the current state.\n        \"\"\"\n        if (self.event_name == \"pull_request_target\" and\n                self.event_payload['action'] == 'opened'):\n            if self.is_committer('pull_request'):\n                return PullRequestState.committer_review\n            else:\n                return PullRequestState.review\n        elif (self.event_name == \"pull_request_review\" and\n                self.event_payload[\"action\"] == \"submitted\"):\n            review_state = self.event_payload[\"review\"][\"state\"].lower()\n            if not self.is_committer('review'):\n                # Non-committer reviews cannot change state once committer has already\n                # reviewed, requested changes or approved\n                if current_state in (\n                        PullRequestState.change_review,\n                        PullRequestState.changes,\n                        PullRequestState.merge):\n                    return current_state\n                else:\n                    return PullRequestState.committer_review\n            if review_state == 'approved':\n                return PullRequestState.merge\n            else:\n                return PullRequestState.changes\n        elif (self.event_name == \"pull_request_target\" and\n              self.event_payload['action'] == 'synchronize' and\n              current_state == PullRequestState.changes):\n            return PullRequestState.change_review\n        # Default already opened PRs to Review state.\n        if current_state is None:\n            current_state = PullRequestState.review\n        return current_state\n\n    def set_state(self, state):\n        \"\"\"Sets the State label to the PR.\"\"\"\n        self.pull.add_to_labels(state.value)\n\n\nclass CommentBot:\n\n    def __init__(self, name, handler, token=None):\n        # TODO(kszucs): validate\n        assert isinstance(name, str)\n        assert callable(handler)\n        self.name = name\n        self.handler = handler\n        kwargs = {}\n        if token is not None:\n            kwargs[\"auth\"] = github.Auth.Token(token)\n        self.github = github.Github(**kwargs)\n\n    def parse_command(self, payload):\n        mention = '@{}'.format(self.name)\n        comment = payload['comment']\n\n        if payload['sender']['login'] == self.name:\n            raise EventError(\"Don't respond to itself\")\n        elif payload['action'] not in {'created', 'edited'}:\n            raise EventError(\"Don't respond to comment deletion\")\n        elif not comment['body'].lstrip().startswith(mention):\n            raise EventError(\"The bot is not mentioned\")\n\n        # Parse the comment, removing the bot mentioned (and everything\n        # before it)\n        command = payload['comment']['body'].split(mention)[-1]\n\n        # then split on newlines and keep only the first line\n        # (ignoring all other lines)\n        return command.split(\"\\n\")[0].strip()\n\n    def handle(self, event, payload):\n        try:\n            command = self.parse_command(payload)\n        except EventError as e:\n            logger.error(e)\n            # see the possible reasons in the validate method\n            return\n\n        if event == 'issue_comment':\n            return self.handle_issue_comment(command, payload)\n        elif event == 'pull_request_review_comment':\n            return self.handle_review_comment(command, payload)\n        else:\n            raise ValueError(\"Unexpected event type {}\".format(event))\n\n    def handle_issue_comment(self, command, payload):\n        repo = self.github.get_repo(payload['repository']['id'], lazy=True)\n        issue = repo.get_issue(payload['issue']['number'])\n\n        try:\n            pull = issue.as_pull_request()\n        except github.GithubException:\n            return issue.create_comment(\n                \"The comment bot only listens to pull request comments!\"\n            )\n\n        comment = pull.get_issue_comment(payload['comment']['id'])\n        try:\n            # Only allow users of apache org to submit commands, for more see\n            # https://developer.github.com/v4/enum/commentauthorassociation/\n            # Checking  privileges here enables the bot to respond\n            # without relying on the handler.\n            allowed_roles = {'OWNER', 'MEMBER', 'COLLABORATOR'}\n            if payload['comment']['author_association'] not in allowed_roles:\n                raise EventError(\n                    \"Only contributors can submit requests to this bot. \"\n                    \"Please ask someone from the community for help with \"\n                    \"getting the first commit in.\"\n                )\n            self.handler(command, issue=issue, pull_request=pull,\n                         comment=comment)\n        except Exception as e:\n            logger.exception(e)\n            url = \"{server}/{repo}/actions/runs/{run_id}\".format(\n                server=os.environ[\"GITHUB_SERVER_URL\"],\n                repo=os.environ[\"GITHUB_REPOSITORY\"],\n                run_id=os.environ[\"GITHUB_RUN_ID\"],\n            )\n            pull.create_issue_comment(\n                f\"```\\n{e}\\nThe Archery job run can be found at: {url}\\n```\")\n            comment.create_reaction('-1')\n        else:\n            comment.create_reaction('+1')\n\n    def handle_review_comment(self, payload):\n        raise NotImplementedError()\n\n\n@group(name='@github-actions')\n@click.pass_context\ndef actions(ctx):\n    \"\"\"Ursabot\"\"\"\n    ctx.ensure_object(dict)\n\n\n@actions.group()\n@click.option('--crossbow', '-c', default='ursacomputing/crossbow',\n              help='Crossbow repository on github to use')\n@click.pass_obj\ndef crossbow(obj, crossbow):\n    \"\"\"\n    Trigger crossbow builds for this pull request\n    \"\"\"\n    obj['crossbow_repo'] = crossbow\n\n\ndef _clone_arrow_and_crossbow(dest, crossbow_repo, arrow_repo_url, pr_number):\n    \"\"\"\n    Clone the repositories and initialize crossbow objects.\n\n    Parameters\n    ----------\n    dest : Path\n        Filesystem path to clone the repositories to.\n    crossbow_repo : str\n        GitHub repository name, like kszucs/crossbow.\n    arrow_repo_url : str\n        Target Apache Arrow repository's clone URL, such as\n        \"https://github.com/apache/arrow.git\".\n    pr_number : int\n        Target PR number.\n    \"\"\"\n    arrow_path = dest / 'arrow'\n    queue_path = dest / 'crossbow'\n\n    # we use unique branch name instead of fork's branch name to avoid\n    # branch name conflict such as 'main' (GH-39996)\n    local_branch = f'archery/pr-{pr_number}'\n    # 1. clone arrow and checkout the PR's branch\n    pr_ref = f'pull/{pr_number}/head:{local_branch}'\n    git.clone('--no-checkout', arrow_repo_url, str(arrow_path))\n    # fetch the PR's branch into the clone\n    git.fetch('origin', pr_ref, git_dir=arrow_path)\n    # checkout the PR's branch into the clone\n    git.checkout(local_branch, git_dir=arrow_path)\n\n    # 2. clone crossbow repository\n    crossbow_url = 'https://github.com/{}'.format(crossbow_repo)\n    git.clone(crossbow_url, str(queue_path))\n\n    # 3. initialize crossbow objects\n    github_token = os.environ['CROSSBOW_GITHUB_TOKEN']\n    arrow = Repo(arrow_path)\n    queue = Queue(queue_path, github_token=github_token, require_https=True)\n\n    return (arrow, queue)\n\n\n@crossbow.command()\n@click.argument('tasks', nargs=-1, required=False)\n@click.option('--group', '-g', 'groups', multiple=True,\n              help='Submit task groups as defined in tests.yml')\n@click.option('--param', '-p', 'params', multiple=True,\n              help='Additional task parameters for rendering the CI templates')\n@click.option('--arrow-version', '-v', default=None,\n              help='Set target version explicitly.')\n@click.option('--wait', default=60,\n              help='Wait the specified seconds before generating a report.')\n@click.pass_obj\ndef submit(obj, tasks, groups, params, arrow_version, wait):\n    \"\"\"\n    Submit crossbow testing tasks.\n\n    See groups defined in arrow/dev/tasks/tasks.yml\n    \"\"\"\n    crossbow_repo = obj['crossbow_repo']\n    pull_request = obj['pull_request']\n    with tempfile.TemporaryDirectory() as tmpdir:\n        tmpdir = Path(tmpdir)\n        arrow, queue = _clone_arrow_and_crossbow(\n            dest=Path(tmpdir),\n            crossbow_repo=crossbow_repo,\n            arrow_repo_url=pull_request.base.repo.clone_url,\n            pr_number=pull_request.number,\n        )\n        # load available tasks configuration and groups from yaml\n        config = Config.load_yaml(arrow.path / \"dev\" / \"tasks\" / \"tasks.yml\")\n        config.validate()\n\n        # initialize the crossbow build's target repository\n        target = Target.from_repo(arrow, version=arrow_version,\n                                  remote=pull_request.head.repo.clone_url,\n                                  branch=pull_request.head.ref)\n\n        # parse additional job parameters\n        params = dict([p.split(\"=\") for p in params])\n        params['pr_number'] = pull_request.number\n\n        # instantiate the job object\n        job = Job.from_config(config=config, target=target, tasks=tasks,\n                              groups=groups, params=params)\n\n        # add the job to the crossbow queue and push to the remote repository\n        queue.put(job, prefix=\"actions\", increment_job_id=False)\n        queue.push()\n\n        # render the response comment's content\n        report = CommentReport(job, crossbow_repo=crossbow_repo,\n                               wait_for_task=wait)\n\n        # send the response\n        pull_request.create_issue_comment(report.show())\n", "dev/archery/archery/lang/python.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom contextlib import contextmanager\nfrom enum import EnumMeta\nimport inspect\nimport tokenize\n\ntry:\n    from numpydoc.validate import Docstring, validate\nexcept ImportError:\n    have_numpydoc = False\nelse:\n    have_numpydoc = True\n\nfrom ..compat import _get_module\nfrom ..utils.logger import logger\nfrom ..utils.command import Command, capture_stdout, default_bin\n\n\nclass PythonCommand(Command):\n    def __init__(self, python_bin=None):\n        self.bin = default_bin(python_bin, \"python\")\n\n\nclass Flake8(Command):\n    def __init__(self, flake8_bin=None):\n        self.bin = default_bin(flake8_bin, \"flake8\")\n\n\nclass CythonLint(Command):\n    def __init__(self, cython_lint_bin=None):\n        self.bin = default_bin(cython_lint_bin, \"cython-lint\")\n\n\nclass Autopep8(Command):\n    def __init__(self, autopep8_bin=None):\n        self.bin = default_bin(autopep8_bin, \"autopep8\")\n\n    @capture_stdout()\n    def run_captured(self, *args, **kwargs):\n        return self.run(*args, **kwargs)\n\n\ndef _tokenize_signature(s):\n    lines = s.encode('ascii').splitlines()\n    generator = iter(lines).__next__\n    return tokenize.tokenize(generator)\n\n\ndef _convert_typehint(tokens):\n    names = []\n    opening_bracket_reached = False\n    for token in tokens:\n        # omit the tokens before the opening bracket\n        if not opening_bracket_reached:\n            if token.string == '(':\n                opening_bracket_reached = True\n            else:\n                continue\n\n        if token.type == 1:  # type 1 means NAME token\n            names.append(token)\n        else:\n            if len(names) == 1:\n                yield (names[0].type, names[0].string)\n            elif len(names) == 2:\n                # two \"NAME\" tokens follow each other which means a cython\n                # typehint like `bool argument`, so remove the typehint\n                # note that we could convert it to python typehints, but hints\n                # are not supported by _signature_fromstr\n                yield (names[1].type, names[1].string)\n            elif len(names) > 2:\n                raise ValueError('More than two NAME tokens follow each other')\n            names = []\n            yield (token.type, token.string)\n\n\ndef inspect_signature(obj):\n    \"\"\"\n    Custom signature inspection primarily for cython generated callables.\n\n    Cython puts the signatures to the first line of the docstrings, which we\n    can reuse to parse the python signature from, but some gymnastics are\n    required, like removing the cython typehints.\n\n    It converts the cython signature:\n        array(obj, type=None, mask=None, size=None, from_pandas=None,\n              bool safe=True, MemoryPool memory_pool=None)\n    To:\n        <Signature (obj, type=None, mask=None, size=None, from_pandas=None,\n                    safe=True, memory_pool=None)>\n    \"\"\"\n    cython_signature = obj.__doc__.splitlines()[0]\n    cython_tokens = _tokenize_signature(cython_signature)\n    python_tokens = _convert_typehint(cython_tokens)\n    python_signature = tokenize.untokenize(python_tokens)\n    return inspect._signature_fromstr(inspect.Signature, obj, python_signature)\n\n\nclass NumpyDoc:\n    IGNORE_VALIDATION_ERRORS_FOR_TYPE = {\n        # Enum function signatures should never be documented\n        EnumMeta: [\"PR01\"]\n    }\n\n    def __init__(self, symbols=None):\n        if not have_numpydoc:\n            raise RuntimeError(\n                'Numpydoc is not available, install with command: '\n                'pip install numpydoc==1.1.0'\n            )\n        self.symbols = set(symbols or {'pyarrow'})\n\n    def traverse(self, fn, obj, from_package):\n        \"\"\"Apply a function on publicly exposed API components.\n\n        Recursively iterates over the members of the passed object. It omits\n        any '_' prefixed and thirdparty (non pyarrow) symbols.\n\n        Parameters\n        ----------\n        fn : callable\n            A function to apply on all traversed objects.\n        obj : Any\n            The object to start from.\n        from_package : string\n            Predicate to only consider objects from this package.\n        \"\"\"\n        todo = [obj]\n        seen = set()\n\n        while todo:\n            obj = todo.pop()\n            if obj in seen:\n                continue\n            else:\n                seen.add(obj)\n\n            fn(obj)\n\n            for name in dir(obj):\n                if name.startswith('_'):\n                    continue\n\n                member = getattr(obj, name)\n                module = _get_module(member)\n                if module is None or not module.startswith(from_package):\n                    continue\n                # Is it a Cython-generated method? If so, try to detect\n                # whether it only has a implicitly-generated docstring,\n                # and no user-defined docstring following it.\n                # The generated docstring would lack description of method\n                # parameters and therefore fail Numpydoc validation.\n                if hasattr(member, '__objclass__'):\n                    doc = getattr(member, '__doc__', None)\n                    # The Cython-generated docstring would be a one-liner,\n                    # such as \"ReadOptions.equals(self, ReadOptions other)\".\n                    if (doc and '\\n' not in doc and f'.{name}(' in doc):\n                        continue\n                todo.append(member)\n\n    @contextmanager\n    def _apply_patches(self):\n        \"\"\"\n        Patch Docstring class to bypass loading already loaded python objects.\n        \"\"\"\n        orig_load_obj = Docstring._load_obj\n        orig_signature = inspect.signature\n\n        @staticmethod\n        def _load_obj(obj):\n            # By default it expects a qualname and import the object, but we\n            # have already loaded object after the API traversal.\n            if isinstance(obj, str):\n                return orig_load_obj(obj)\n            else:\n                return obj\n\n        def signature(obj):\n            # inspect.signature tries to parse __text_signature__ if other\n            # properties like __signature__ doesn't exists, but cython\n            # doesn't set that property despite that embedsignature cython\n            # directive is set. The only way to inspect a cython compiled\n            # callable's signature to parse it from __doc__ while\n            # embedsignature directive is set during the build phase.\n            # So path inspect.signature function to attempt to parse the first\n            # line of callable.__doc__ as a signature.\n            try:\n                return orig_signature(obj)\n            except Exception as orig_error:\n                try:\n                    return inspect_signature(obj)\n                except Exception:\n                    raise orig_error\n\n        try:\n            Docstring._load_obj = _load_obj\n            inspect.signature = signature\n            yield\n        finally:\n            Docstring._load_obj = orig_load_obj\n            inspect.signature = orig_signature\n\n    def validate(self, from_package='', allow_rules=None,\n                 disallow_rules=None):\n        results = []\n\n        def callback(obj):\n            try:\n                result = validate(obj)\n            except OSError as e:\n                symbol = f\"{_get_module(obj, default='')}.{obj.__name__}\"\n                logger.warning(f\"Unable to validate `{symbol}` due to `{e}`\")\n                return\n\n            errors = []\n            for errcode, errmsg in result.get('errors', []):\n                if allow_rules and errcode not in allow_rules:\n                    continue\n                if disallow_rules and errcode in disallow_rules:\n                    continue\n                if any(isinstance(obj, obj_type) and errcode in errcode_list\n                       for obj_type, errcode_list\n                       in NumpyDoc.IGNORE_VALIDATION_ERRORS_FOR_TYPE.items()):\n                    continue\n                errors.append((errcode, errmsg))\n\n            if len(errors):\n                result['errors'] = errors\n                results.append((obj, result))\n\n        with self._apply_patches():\n            for symbol in self.symbols:\n                try:\n                    obj = Docstring._load_obj(symbol)\n                except (ImportError, AttributeError):\n                    print('{} is not available for import'.format(symbol))\n                else:\n                    self.traverse(callback, obj, from_package=from_package)\n\n        return results\n", "dev/archery/archery/lang/java.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nfrom ..utils.command import Command, CommandStackMixin, default_bin\nfrom ..utils.maven import MavenDefinition\n\n\nclass Java(Command):\n    def __init__(self, java_bin=None):\n        self.bin = default_bin(java_bin, \"java\")\n\n\nclass Jar(CommandStackMixin, Java):\n    def __init__(self, jar, *args, **kwargs):\n        self.jar = jar\n        self.argv = (\"-jar\", jar)\n        Java.__init__(self, *args, **kwargs)\n\n\nclass JavaConfiguration:\n    def __init__(self,\n\n                 # toolchain\n                 java_home=None, java_options=None,\n                 # build & benchmark\n                 build_extras=None, benchmark_extras=None):\n        self.java_home = java_home\n        self.java_options = java_options\n\n        self.build_extras = list(build_extras) if build_extras else []\n        self.benchmark_extras = list(\n            benchmark_extras) if benchmark_extras else []\n\n    @property\n    def build_definitions(self):\n        return self.build_extras\n\n    @property\n    def benchmark_definitions(self):\n        return self.benchmark_extras\n\n    @property\n    def environment(self):\n        env = os.environ.copy()\n\n        if self.java_home:\n            env[\"JAVA_HOME\"] = self.java_home\n\n        if self.java_options:\n            env[\"JAVA_OPTIONS\"] = self.java_options\n\n        return env\n\n\nclass JavaMavenDefinition(MavenDefinition):\n    def __init__(self, source, conf, **kwargs):\n        self.configuration = conf\n        super().__init__(source, **kwargs,\n                         build_definitions=conf.build_definitions,\n                         benchmark_definitions=conf.benchmark_definitions,\n                         env=conf.environment)\n", "dev/archery/archery/lang/cpp.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nfrom ..utils.cmake import CMakeDefinition\n\n\ndef truthifier(value):\n    return \"ON\" if value else \"OFF\"\n\n\ndef or_else(value, default):\n    return value if value else default\n\n\ndef coalesce(value, fallback):\n    return fallback if value is None else value\n\n\nLLVM_VERSION = 7\n\n\nclass CppConfiguration:\n    def __init__(self,\n\n                 # toolchain\n                 cc=None, cxx=None, cxx_flags=None,\n                 build_type=None, warn_level=None,\n                 cpp_package_prefix=None, install_prefix=None, use_conda=None,\n                 build_static=True, build_shared=True, build_unity=True,\n                 # tests & examples\n                 with_tests=None, with_benchmarks=None, with_examples=None,\n                 with_integration=None,\n                 # static checks\n                 use_asan=None, use_tsan=None, use_ubsan=None,\n                 with_fuzzing=None,\n                 # Components\n                 with_compute=None, with_csv=None, with_cuda=None,\n                 with_dataset=None, with_filesystem=None, with_flight=None,\n                 with_gandiva=None, with_gcs=None, with_hdfs=None,\n                 with_hiveserver2=None,\n                 with_ipc=True, with_json=None,\n                 with_mimalloc=None, with_jemalloc=None,\n                 with_parquet=None, with_python=True,\n                 with_r=None, with_s3=None,\n                 # Compressions\n                 with_brotli=None, with_bz2=None, with_lz4=None,\n                 with_snappy=None, with_zlib=None, with_zstd=None,\n                 # extras\n                 with_lint_only=False,\n                 use_gold_linker=True,\n                 simd_level=\"DEFAULT\",\n                 cmake_extras=None):\n        self._cc = cc\n        self._cxx = cxx\n        self.cxx_flags = cxx_flags\n\n        self._build_type = build_type\n        self.warn_level = warn_level\n        self._install_prefix = install_prefix\n        self._package_prefix = cpp_package_prefix\n        self._use_conda = use_conda\n        self.build_static = build_static\n        self.build_shared = build_shared\n        self.build_unity = build_unity\n\n        self.with_tests = with_tests\n        self.with_benchmarks = with_benchmarks\n        self.with_examples = with_examples\n        self.with_integration = with_integration\n\n        self.use_asan = use_asan\n        self.use_tsan = use_tsan\n        self.use_ubsan = use_ubsan\n        self.with_fuzzing = with_fuzzing\n\n        self.with_compute = with_compute\n        self.with_csv = with_csv\n        self.with_cuda = with_cuda\n        self.with_dataset = with_dataset\n        self.with_filesystem = with_filesystem\n        self.with_flight = with_flight\n        self.with_gandiva = with_gandiva\n        self.with_gcs = with_gcs\n        self.with_hdfs = with_hdfs\n        self.with_hiveserver2 = with_hiveserver2\n        self.with_ipc = with_ipc\n        self.with_json = with_json\n        self.with_mimalloc = with_mimalloc\n        self.with_jemalloc = with_jemalloc\n        self.with_parquet = with_parquet\n        self.with_python = with_python\n        self.with_r = with_r\n        self.with_s3 = with_s3\n\n        self.with_brotli = with_brotli\n        self.with_bz2 = with_bz2\n        self.with_lz4 = with_lz4\n        self.with_snappy = with_snappy\n        self.with_zlib = with_zlib\n        self.with_zstd = with_zstd\n\n        self.with_lint_only = with_lint_only\n        self.use_gold_linker = use_gold_linker\n        self.simd_level = simd_level\n\n        self.cmake_extras = cmake_extras\n\n        # Fixup required dependencies by providing sane defaults if the caller\n        # didn't specify the option.\n        if self.with_r:\n            self.with_csv = coalesce(with_csv, True)\n            self.with_dataset = coalesce(with_dataset, True)\n            self.with_filesystem = coalesce(with_filesystem, True)\n            self.with_ipc = coalesce(with_ipc, True)\n            self.with_json = coalesce(with_json, True)\n            self.with_parquet = coalesce(with_parquet, True)\n\n        if self.with_python:\n            self.with_compute = coalesce(with_compute, True)\n            self.with_csv = coalesce(with_csv, True)\n            self.with_dataset = coalesce(with_dataset, True)\n            self.with_filesystem = coalesce(with_filesystem, True)\n            self.with_hdfs = coalesce(with_hdfs, True)\n            self.with_json = coalesce(with_json, True)\n            self.with_lz4 = coalesce(with_lz4, True)\n            self.with_zlib = coalesce(with_zlib, True)\n\n        if self.with_dataset:\n            self.with_filesystem = coalesce(with_filesystem, True)\n            self.with_parquet = coalesce(with_parquet, True)\n\n        if self.with_parquet:\n            self.with_snappy = coalesce(with_snappy, True)\n\n    @property\n    def build_type(self):\n        if self._build_type:\n            return self._build_type\n\n        if self.with_fuzzing:\n            return \"relwithdebinfo\"\n\n        return \"release\"\n\n    @property\n    def cc(self):\n        if self._cc:\n            return self._cc\n\n        if self.with_fuzzing:\n            return \"clang-{}\".format(LLVM_VERSION)\n\n        return None\n\n    @property\n    def cxx(self):\n        if self._cxx:\n            return self._cxx\n\n        if self.with_fuzzing:\n            return \"clang++-{}\".format(LLVM_VERSION)\n\n        return None\n\n    def _gen_defs(self):\n        if self.cxx_flags:\n            yield (\"ARROW_CXXFLAGS\", self.cxx_flags)\n\n        yield (\"CMAKE_EXPORT_COMPILE_COMMANDS\", truthifier(True))\n        yield (\"CMAKE_BUILD_TYPE\", self.build_type)\n\n        if not self.with_lint_only:\n            yield (\"BUILD_WARNING_LEVEL\",\n                   or_else(self.warn_level, \"production\"))\n\n        # if not ctx.quiet:\n        #   yield (\"ARROW_VERBOSE_THIRDPARTY_BUILD\", \"ON\")\n\n        maybe_prefix = self.install_prefix\n        if maybe_prefix:\n            yield (\"CMAKE_INSTALL_PREFIX\", maybe_prefix)\n\n        if self._package_prefix is not None:\n            yield (\"ARROW_DEPENDENCY_SOURCE\", \"SYSTEM\")\n            yield (\"ARROW_PACKAGE_PREFIX\", self._package_prefix)\n\n        yield (\"ARROW_BUILD_STATIC\", truthifier(self.build_static))\n        yield (\"ARROW_BUILD_SHARED\", truthifier(self.build_shared))\n        yield (\"CMAKE_UNITY_BUILD\", truthifier(self.build_unity))\n\n        # Tests and benchmarks\n        yield (\"ARROW_BUILD_TESTS\", truthifier(self.with_tests))\n        yield (\"ARROW_BUILD_BENCHMARKS\", truthifier(self.with_benchmarks))\n        yield (\"ARROW_BUILD_EXAMPLES\", truthifier(self.with_examples))\n        yield (\"ARROW_BUILD_INTEGRATION\", truthifier(self.with_integration))\n\n        # Static checks\n        yield (\"ARROW_USE_ASAN\", truthifier(self.use_asan))\n        yield (\"ARROW_USE_TSAN\", truthifier(self.use_tsan))\n        yield (\"ARROW_USE_UBSAN\", truthifier(self.use_ubsan))\n        yield (\"ARROW_FUZZING\", truthifier(self.with_fuzzing))\n\n        # Components\n        yield (\"ARROW_COMPUTE\", truthifier(self.with_compute))\n        yield (\"ARROW_CSV\", truthifier(self.with_csv))\n        yield (\"ARROW_CUDA\", truthifier(self.with_cuda))\n        yield (\"ARROW_DATASET\", truthifier(self.with_dataset))\n        yield (\"ARROW_FILESYSTEM\", truthifier(self.with_filesystem))\n        yield (\"ARROW_FLIGHT\", truthifier(self.with_flight))\n        yield (\"ARROW_GANDIVA\", truthifier(self.with_gandiva))\n        yield (\"ARROW_GCS\", truthifier(self.with_gcs))\n        yield (\"ARROW_HDFS\", truthifier(self.with_hdfs))\n        yield (\"ARROW_IPC\", truthifier(self.with_ipc))\n        yield (\"ARROW_JSON\", truthifier(self.with_json))\n        yield (\"ARROW_MIMALLOC\", truthifier(self.with_mimalloc))\n        yield (\"ARROW_JEMALLOC\", truthifier(self.with_jemalloc))\n        yield (\"ARROW_PARQUET\", truthifier(self.with_parquet))\n        yield (\"ARROW_S3\", truthifier(self.with_s3))\n\n        # Compressions\n        yield (\"ARROW_WITH_BROTLI\", truthifier(self.with_brotli))\n        yield (\"ARROW_WITH_BZ2\", truthifier(self.with_bz2))\n        yield (\"ARROW_WITH_LZ4\", truthifier(self.with_lz4))\n        yield (\"ARROW_WITH_SNAPPY\", truthifier(self.with_snappy))\n        yield (\"ARROW_WITH_ZLIB\", truthifier(self.with_zlib))\n        yield (\"ARROW_WITH_ZSTD\", truthifier(self.with_zstd))\n\n        yield (\"ARROW_LINT_ONLY\", truthifier(self.with_lint_only))\n\n        # Some configurations don't like gnu gold linker.\n        broken_with_gold_ld = [self.with_fuzzing, self.with_gandiva]\n        if self.use_gold_linker and not any(broken_with_gold_ld):\n            yield (\"ARROW_USE_LD_GOLD\", truthifier(self.use_gold_linker))\n        yield (\"ARROW_SIMD_LEVEL\", or_else(self.simd_level, \"DEFAULT\"))\n\n        # Detect custom conda toolchain\n        if self.use_conda:\n            for d, v in [('CMAKE_AR', 'AR'), ('CMAKE_RANLIB', 'RANLIB')]:\n                v = os.environ.get(v)\n                if v:\n                    yield (d, v)\n\n    @property\n    def install_prefix(self):\n        if self._install_prefix:\n            return self._install_prefix\n\n        if self.use_conda:\n            return os.environ.get(\"CONDA_PREFIX\")\n\n        return None\n\n    @property\n    def use_conda(self):\n        # If the user didn't specify a preference, guess via environment\n        if self._use_conda is None:\n            return os.environ.get(\"CONDA_PREFIX\") is not None\n\n        return self._use_conda\n\n    @property\n    def definitions(self):\n        extras = list(self.cmake_extras) if self.cmake_extras else []\n        definitions = [\"-D{}={}\".format(d[0], d[1]) for d in self._gen_defs()]\n        return definitions + extras\n\n    @property\n    def environment(self):\n        env = os.environ.copy()\n\n        if self.cc:\n            env[\"CC\"] = self.cc\n\n        if self.cxx:\n            env[\"CXX\"] = self.cxx\n\n        return env\n\n\nclass CppCMakeDefinition(CMakeDefinition):\n    def __init__(self, source, conf, **kwargs):\n        self.configuration = conf\n        super().__init__(source, **kwargs,\n                         definitions=conf.definitions, env=conf.environment,\n                         build_type=conf.build_type)\n", "dev/archery/archery/lang/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "dev/archery/archery/utils/command.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport shlex\nimport shutil\nimport subprocess\n\nfrom .logger import logger, ctx\n\n\ndef default_bin(name, default):\n    assert default\n    env_name = \"ARCHERY_{0}_BIN\".format(default.upper())\n    return name if name else os.environ.get(env_name, default)\n\n\n# Decorator running a command and returning stdout\nclass capture_stdout:\n    def __init__(self, strip=False, listify=False):\n        self.strip = strip\n        self.listify = listify\n\n    def __call__(self, f):\n        def strip_it(x):\n            return x.strip() if self.strip else x\n\n        def list_it(x):\n            return x.decode('utf-8').splitlines() if self.listify else x\n\n        def wrapper(*argv, **kwargs):\n            # Ensure stdout is captured\n            kwargs[\"stdout\"] = subprocess.PIPE\n            return list_it(strip_it(f(*argv, **kwargs).stdout))\n        return wrapper\n\n\nclass Command:\n    \"\"\"\n    A runnable command.\n\n    Class inheriting from the Command class must provide the bin\n    property/attribute.\n    \"\"\"\n\n    def __init__(self, bin):\n        self.bin = bin\n\n    def run(self, *argv, **kwargs):\n        assert hasattr(self, \"bin\")\n        invocation = shlex.split(self.bin)\n        invocation.extend(argv)\n\n        for key in [\"stdout\", \"stderr\"]:\n            # Preserve caller intention, otherwise silence\n            if key not in kwargs and ctx.quiet:\n                kwargs[key] = subprocess.PIPE\n\n        # Prefer safe by default\n        if \"check\" not in kwargs:\n            kwargs[\"check\"] = True\n\n        logger.debug(\"Executing `{}`\".format(invocation))\n        return subprocess.run(invocation, **kwargs)\n\n    @property\n    def available(self):\n        \"\"\"\n        Indicate if the command binary is found in PATH.\n        \"\"\"\n        binary = shlex.split(self.bin)[0]\n        return shutil.which(binary) is not None\n\n    def __call__(self, *argv, **kwargs):\n        return self.run(*argv, **kwargs)\n\n\nclass CommandStackMixin:\n    def run(self, *argv, **kwargs):\n        stacked_args = self.argv + argv\n        return super(CommandStackMixin, self).run(*stacked_args, **kwargs)\n\n\nclass Bash(Command):\n    def __init__(self, bash_bin=None):\n        self.bin = default_bin(bash_bin, \"bash\")\n", "dev/archery/archery/utils/rat.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport fnmatch\nimport re\nfrom xml.etree import ElementTree\n\nfrom ..lang.java import Jar\nfrom .cache import Cache\nfrom .command import capture_stdout\n\nRAT_VERSION = 0.13\nRAT_JAR_FILENAME = \"apache-rat-{}.jar\".format(RAT_VERSION)\nRAT_URL_ = \"https://repo1.maven.org/maven2/org/apache/rat/apache-rat\"\nRAT_URL = \"/\".join([RAT_URL_, str(RAT_VERSION), RAT_JAR_FILENAME])\n\n\nclass Rat(Jar):\n    def __init__(self):\n        jar = Cache().get_or_insert_from_url(RAT_JAR_FILENAME, RAT_URL)\n        Jar.__init__(self, jar)\n\n    @capture_stdout(strip=False)\n    def run_report(self, archive_path, **kwargs):\n        return self.run(\"--xml\", archive_path, **kwargs)\n\n    def report(self, archive_path, **kwargs):\n        return RatReport(self.run_report(archive_path, **kwargs))\n\n\ndef exclusion_from_globs(exclusions_path):\n    with open(exclusions_path, 'r') as exclusions_fd:\n        exclusions = [e.strip() for e in exclusions_fd]\n        return lambda path: any([fnmatch.fnmatch(path, e) for e in exclusions])\n\n\nclass RatReport:\n    def __init__(self, xml):\n        self.xml = xml\n        self.tree = ElementTree.fromstring(xml)\n\n    def __repr__(self):\n        return \"RatReport({})\".format(self.xml)\n\n    def validate(self, exclusion=None):\n        for r in self.tree.findall('resource'):\n            approvals = r.findall('license-approval')\n            if not approvals or approvals[0].attrib['name'] == 'true':\n                continue\n\n            clean_name = re.sub('^[^/]+/', '', r.attrib['name'])\n\n            if exclusion and exclusion(clean_name):\n                continue\n\n            yield clean_name\n", "dev/archery/archery/utils/cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport importlib\n\nimport click\n\nfrom .source import ArrowSources, InvalidArrowSource\n\n\nclass ArrowBool(click.types.BoolParamType):\n    \"\"\"\n    ArrowBool supports the 'ON' and 'OFF' values on top of the values\n    supported by BoolParamType. This is convenient to port script which exports\n    CMake options variables.\n    \"\"\"\n    name = \"boolean\"\n\n    def convert(self, value, param, ctx):\n        if isinstance(value, str):\n            lowered = value.lower()\n            if lowered == \"on\":\n                return True\n            elif lowered == \"off\":\n                return False\n\n        return super().convert(value, param, ctx)\n\n\ndef validate_arrow_sources(ctx, param, src):\n    \"\"\"\n    Ensure a directory contains Arrow cpp sources.\n    \"\"\"\n    try:\n        return ArrowSources.find(src)\n    except InvalidArrowSource as e:\n        raise click.BadParameter(str(e))\n\n\ndef add_optional_command(name, module, function, parent):\n    try:\n        module = importlib.import_module(module, package=\"archery\")\n        command = getattr(module, function)\n    except ImportError as exc:\n        error_message = exc.name\n\n        @parent.command(\n            name,\n            context_settings={\n                \"allow_extra_args\": True,\n                \"ignore_unknown_options\": True,\n            }\n        )\n        def command():\n            raise click.ClickException(\n                f\"Couldn't import command `{name}` due to {error_message}\"\n            )\n    else:\n        parent.add_command(command)\n", "dev/archery/archery/utils/cmake.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport re\nfrom shutil import rmtree, which\n\nfrom .command import Command, default_bin\n\n\nclass CMake(Command):\n    def __init__(self, cmake_bin=None):\n        self.bin = default_bin(cmake_bin, \"cmake\")\n\n    @staticmethod\n    def default_generator():\n        \"\"\" Infer default generator.\n\n        Gives precedence to ninja if there exists an executable named `ninja`\n        in the search path.\n        \"\"\"\n        found_ninja = which(\"ninja\")\n        return \"Ninja\" if found_ninja else \"Unix Makefiles\"\n\n\ncmake = CMake()\n\n\nclass CMakeDefinition:\n    \"\"\" CMakeDefinition captures the cmake invocation arguments.\n\n    It allows creating build directories with the same definition, e.g.\n    ```\n    build_1 = cmake_def.build(\"/tmp/build-1\")\n    build_2 = cmake_def.build(\"/tmp/build-2\")\n\n    ...\n\n    build1.all()\n    build2.all()\n    \"\"\"\n\n    def __init__(self, source, build_type=\"release\", generator=None,\n                 definitions=None, env=None):\n        \"\"\" Initialize a CMakeDefinition\n\n        Parameters\n        ----------\n        source : str\n                 Source directory where the top-level CMakeLists.txt is\n                 located. This is usually the root of the project.\n        generator : str, optional\n        definitions: list(str), optional\n        env : dict(str,str), optional\n              Environment to use when invoking cmake. This can be required to\n              work around cmake deficiencies, e.g. CC and CXX.\n        \"\"\"\n        self.source = os.path.abspath(source)\n        self.build_type = build_type\n        self.generator = generator if generator else cmake.default_generator()\n        self.definitions = definitions if definitions else []\n        self.env = env\n\n    @property\n    def arguments(self):\n        \"\"\"\" Return the arguments to cmake invocation. \"\"\"\n        arguments = [\n            \"-G{}\".format(self.generator),\n        ] + self.definitions + [\n            self.source\n        ]\n        return arguments\n\n    def build(self, build_dir, force=False, cmd_kwargs=None, **kwargs):\n        \"\"\" Invoke cmake into a build directory.\n\n        Parameters\n        ----------\n        build_dir : str\n                    Directory in which the CMake build will be instantiated.\n        force : bool\n                If the build folder exists, delete it before. Otherwise if it's\n                present, an error will be returned.\n        \"\"\"\n        if os.path.exists(build_dir):\n            # Extra safety to ensure we're deleting a build folder.\n            if not CMakeBuild.is_build_dir(build_dir):\n                raise FileExistsError(\n                    \"{} is not a cmake build\".format(build_dir)\n                )\n            if not force:\n                raise FileExistsError(\n                    \"{} exists use force=True\".format(build_dir)\n                )\n            rmtree(build_dir)\n\n        os.mkdir(build_dir)\n\n        cmd_kwargs = cmd_kwargs if cmd_kwargs else {}\n        cmake(*self.arguments, cwd=build_dir, env=self.env, **cmd_kwargs)\n        return CMakeBuild(build_dir, self.build_type, definition=self,\n                          **kwargs)\n\n    def __repr__(self):\n        return \"CMakeDefinition[source={}]\".format(self.source)\n\n\nCMAKE_BUILD_TYPE_RE = re.compile(\"CMAKE_BUILD_TYPE:STRING=([a-zA-Z]+)\")\n\n\nclass CMakeBuild(CMake):\n    \"\"\" CMakeBuild represents a build directory initialized by cmake.\n\n    The build instance can be used to build/test/install. It alleviates the\n    user to know which generator is used.\n    \"\"\"\n\n    def __init__(self, build_dir, build_type, definition=None):\n        \"\"\" Initialize a CMakeBuild.\n\n        The caller must ensure that cmake was invoked in the build directory.\n\n        Parameters\n        ----------\n        definition : CMakeDefinition\n                     The definition to build from.\n        build_dir : str\n                    The build directory to setup into.\n        \"\"\"\n        assert CMakeBuild.is_build_dir(build_dir)\n        super().__init__()\n        self.build_dir = os.path.abspath(build_dir)\n        self.build_type = build_type\n        self.definition = definition\n\n    @property\n    def binaries_dir(self):\n        return os.path.join(self.build_dir, self.build_type)\n\n    def run(self, *argv, verbose=False, **kwargs):\n        cmake_args = [\"--build\", self.build_dir, \"--\"]\n        extra = []\n        if verbose:\n            extra.append(\"-v\" if self.bin.endswith(\"ninja\") else \"VERBOSE=1\")\n        # Commands must be ran under the build directory\n        return super().run(*cmake_args, *extra,\n                           *argv, **kwargs, cwd=self.build_dir)\n\n    def all(self):\n        return self.run(\"all\")\n\n    def clean(self):\n        return self.run(\"clean\")\n\n    def install(self):\n        return self.run(\"install\")\n\n    def test(self):\n        return self.run(\"test\")\n\n    @staticmethod\n    def is_build_dir(path):\n        \"\"\" Indicate if a path is CMake build directory.\n\n        This method only checks for the existence of paths and does not do any\n        validation whatsoever.\n        \"\"\"\n        cmake_cache = os.path.join(path, \"CMakeCache.txt\")\n        cmake_files = os.path.join(path, \"CMakeFiles\")\n        return os.path.exists(cmake_cache) and os.path.exists(cmake_files)\n\n    @staticmethod\n    def from_path(path):\n        \"\"\" Instantiate a CMakeBuild from a path.\n\n        This is used to recover from an existing physical directory (created\n        with or without CMakeBuild).\n\n        Note that this method is not idempotent as the original definition will\n        be lost. Only build_type is recovered.\n        \"\"\"\n        if not CMakeBuild.is_build_dir(path):\n            raise ValueError(\"Not a valid CMakeBuild path: {}\".format(path))\n\n        build_type = None\n        # Infer build_type by looking at CMakeCache.txt and looking for a magic\n        # definition\n        cmake_cache_path = os.path.join(path, \"CMakeCache.txt\")\n        with open(cmake_cache_path, \"r\") as cmake_cache:\n            candidates = CMAKE_BUILD_TYPE_RE.findall(cmake_cache.read())\n            build_type = candidates[0].lower() if candidates else \"release\"\n\n        return CMakeBuild(path, build_type)\n\n    def __repr__(self):\n        return (\"CMakeBuild[\"\n                \"build = {},\"\n                \"build_type = {},\"\n                \"definition = {}]\".format(self.build_dir,\n                                          self.build_type,\n                                          self.definition))\n", "dev/archery/archery/utils/source.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nfrom pathlib import Path\nimport subprocess\nimport tempfile\n\nfrom .command import Command\nfrom .git import git\n\n\nARROW_ROOT_DEFAULT = os.environ.get(\n    'ARROW_ROOT',\n    Path(__file__).resolve().parents[4]\n)\n\n\ndef arrow_path(path):\n    \"\"\"\n    Return full path to a file given its path inside the Arrow repo.\n    \"\"\"\n    return os.path.join(ARROW_ROOT_DEFAULT, path)\n\n\nclass InvalidArrowSource(Exception):\n    pass\n\n\nclass ArrowSources:\n    \"\"\" ArrowSources is a companion class representing a directory containing\n    Apache Arrow's sources.\n    \"\"\"\n    # Note that WORKSPACE is a reserved git revision name by this module to\n    # reference the current git workspace. In other words, this indicates to\n    # ArrowSources.at_revision that no cloning/checkout is required.\n    WORKSPACE = \"WORKSPACE\"\n\n    def __init__(self, path):\n        \"\"\" Initialize an ArrowSources\n\n        The caller must ensure that path is valid arrow source directory (can\n        be checked with ArrowSources.valid)\n\n        Parameters\n        ----------\n        path : src\n        \"\"\"\n        path = Path(path)\n        # validate by checking a specific path in the arrow source tree\n        if not (path / 'cpp' / 'CMakeLists.txt').exists():\n            raise InvalidArrowSource(\n                \"No Arrow C++ sources found in {}.\".format(path)\n            )\n        self.path = path\n\n    @property\n    def archery(self):\n        \"\"\" Returns the archery directory of an Arrow sources. \"\"\"\n        return self.dev / \"archery\"\n\n    @property\n    def cpp(self):\n        \"\"\" Returns the cpp directory of an Arrow sources. \"\"\"\n        return self.path / \"cpp\"\n\n    @property\n    def dev(self):\n        \"\"\" Returns the dev directory of an Arrow sources. \"\"\"\n        return self.path / \"dev\"\n\n    @property\n    def java(self):\n        \"\"\" Returns the java directory of an Arrow sources. \"\"\"\n        return self.path / \"java\"\n\n    @property\n    def python(self):\n        \"\"\" Returns the python directory of an Arrow sources. \"\"\"\n        return self.path / \"python\"\n\n    @property\n    def pyarrow(self):\n        \"\"\" Returns the python/pyarrow directory of an Arrow sources. \"\"\"\n        return self.python / \"pyarrow\"\n\n    @property\n    def r(self):\n        \"\"\" Returns the r directory of an Arrow sources. \"\"\"\n        return self.path / \"r\"\n\n    @property\n    def git_backed(self):\n        \"\"\" Indicate if the sources are backed by git. \"\"\"\n        return (self.path / \".git\").exists()\n\n    @property\n    def git_dirty(self):\n        \"\"\" Indicate if the sources is a dirty git directory. \"\"\"\n        return self.git_backed and git.dirty(git_dir=self.path)\n\n    def archive(self, path, dereference=False, compressor=None, revision=None):\n        \"\"\" Saves a git archive at path. \"\"\"\n        if not self.git_backed:\n            raise ValueError(\"{} is not backed by git\".format(self))\n\n        rev = revision if revision else \"HEAD\"\n        archive = git.archive(\"--prefix=apache-arrow.tmp/\", rev,\n                              git_dir=self.path)\n        with tempfile.TemporaryDirectory() as tmp:\n            tmp = Path(tmp)\n            tar_path = tmp / \"apache-arrow.tar\"\n            with open(tar_path, \"wb\") as tar:\n                tar.write(archive)\n            Command(\"tar\").run(\"xf\", tar_path, \"-C\", tmp)\n            # Must use the same logic in dev/release/02-source.sh\n            Command(\"cp\").run(\"-R\", \"-L\", tmp /\n                              \"apache-arrow.tmp\", tmp / \"apache-arrow\")\n            Command(\"tar\").run(\"cf\", tar_path, \"-C\", tmp, \"apache-arrow\")\n            with open(tar_path, \"rb\") as tar:\n                archive = tar.read()\n\n        if compressor:\n            archive = compressor(archive)\n\n        with open(path, \"wb\") as archive_fd:\n            archive_fd.write(archive)\n\n    def at_revision(self, revision, clone_dir):\n        \"\"\" Return a copy of the current sources for a specified git revision.\n\n        This method may return the current object if no checkout is required.\n        The caller is responsible to remove the cloned repository directory.\n\n        The user can use the special WORKSPACE token to mean the current git\n        workspace (no checkout performed).\n\n        The second value of the returned tuple indicates if a clone was\n        performed.\n\n        Parameters\n        ----------\n        revision : str\n                   Revision to checkout sources at.\n        clone_dir : str\n                    Path to checkout the local clone.\n        \"\"\"\n        if not self.git_backed:\n            raise ValueError(\"{} is not backed by git\".format(self))\n\n        if revision == ArrowSources.WORKSPACE:\n            return self, False\n\n        # A local clone is required to leave the current sources intact such\n        # that builds depending on said sources are not invalidated (or worse\n        # slightly affected when re-invoking the generator).\n        # \"--local\" only works when dest dir is on same volume of source dir.\n        # \"--shared\" works even if dest dir is on different volume.\n        git.clone(\"--shared\", self.path, clone_dir)\n\n        # Revision can reference \"origin/\" (or any remotes) that are not found\n        # in the local clone. Thus, revisions are dereferenced in the source\n        # repository.\n        original_revision = git.rev_parse(revision)\n\n        git.checkout(original_revision, git_dir=clone_dir)\n\n        return ArrowSources(clone_dir), True\n\n    @staticmethod\n    def find(path=None):\n        \"\"\" Infer Arrow sources directory from various method.\n\n        The following guesses are done in order until a valid match is found:\n\n        1. Checks the given optional parameter.\n\n        2. Checks if the environment variable `ARROW_SRC` is defined and use\n           this.\n\n        3. Checks if the current working directory (cwd) is an Arrow source\n           directory.\n\n        4. Checks if this file (cli.py) is still in the original source\n           repository. If so, returns the relative path to the source\n           directory.\n        \"\"\"\n\n        # Explicit via environment\n        env = os.environ.get(\"ARROW_SRC\")\n\n        # Implicit via cwd\n        cwd = Path.cwd()\n\n        # Implicit via current file\n        try:\n            this = Path(__file__).parents[4]\n        except IndexError:\n            this = None\n\n        # Implicit via git repository (if archery is installed system wide)\n        try:\n            repo = git.repository_root(git_dir=cwd)\n        except subprocess.CalledProcessError:\n            # We're not inside a git repository.\n            repo = None\n\n        paths = list(filter(None, [path, env, cwd, this, repo]))\n        for p in paths:\n            try:\n                return ArrowSources(p)\n            except InvalidArrowSource:\n                pass\n\n        searched_paths = \"\\n\".join([\" - {}\".format(p) for p in paths])\n        raise InvalidArrowSource(\n            \"Unable to locate Arrow's source directory. \"\n            \"Searched paths are:\\n{}\".format(searched_paths)\n        )\n\n    def __repr__(self):\n        return os.fspath(self.path)\n", "dev/archery/archery/utils/report.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom abc import ABCMeta, abstractmethod\nimport datetime\n\nimport jinja2\n\n\ndef markdown_escape(s):\n    for char in ('*', '#', '_', '~', '`', '>'):\n        s = s.replace(char, '\\\\' + char)\n    return s\n\n\nclass Report(metaclass=ABCMeta):\n\n    def __init__(self, **kwargs):\n        for field in self.fields:\n            if field not in kwargs:\n                raise ValueError('Missing keyword argument {}'.format(field))\n        self._data = kwargs\n\n    def __getattr__(self, key):\n        return self._data[key]\n\n    @abstractmethod\n    def fields(self):\n        pass\n\n    @property\n    @abstractmethod\n    def templates(self):\n        pass\n\n\nclass JinjaReport(Report):\n\n    def __init__(self, **kwargs):\n        self.env = jinja2.Environment(\n            loader=jinja2.PackageLoader('archery', 'templates')\n        )\n        self.env.filters['md'] = markdown_escape\n        self.env.globals['today'] = datetime.date.today\n        super().__init__(**kwargs)\n\n    def render(self, template_name):\n        template_path = self.templates[template_name]\n        template = self.env.get_template(template_path)\n        return template.render(**self._data)\n", "dev/archery/archery/utils/git.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom .command import Command, capture_stdout, default_bin\nfrom ..compat import _stringify_path\n\n\n# Decorator prepending argv with the git sub-command found with the method\n# name.\ndef git_cmd(fn):\n    # function name is the subcommand\n    sub_cmd = fn.__name__.replace(\"_\", \"-\")\n\n    def wrapper(self, *argv, **kwargs):\n        return fn(self, sub_cmd, *argv, **kwargs)\n    return wrapper\n\n\nclass Git(Command):\n    def __init__(self, git_bin=None):\n        self.bin = default_bin(git_bin, \"git\")\n\n    def run_cmd(self, cmd, *argv, git_dir=None, **kwargs):\n        \"\"\" Inject flags before sub-command in argv. \"\"\"\n        opts = []\n        if git_dir is not None:\n            opts.extend([\"-C\", _stringify_path(git_dir)])\n\n        return self.run(*opts, cmd, *argv, **kwargs)\n\n    @capture_stdout(strip=False)\n    @git_cmd\n    def archive(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @git_cmd\n    def clone(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @git_cmd\n    def fetch(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @git_cmd\n    def checkout(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    def dirty(self, **kwargs):\n        return len(self.status(\"--short\", **kwargs)) > 0\n\n    @git_cmd\n    def log(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @capture_stdout(strip=True, listify=True)\n    @git_cmd\n    def ls_files(self, *argv, listify=False, **kwargs):\n        stdout = self.run_cmd(*argv, **kwargs)\n        return stdout\n\n    @capture_stdout(strip=True)\n    @git_cmd\n    def rev_parse(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @capture_stdout(strip=True)\n    @git_cmd\n    def status(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @capture_stdout(strip=True)\n    def head(self, **kwargs):\n        \"\"\" Return commit pointed by HEAD. \"\"\"\n        return self.rev_parse(\"HEAD\", **kwargs)\n\n    @capture_stdout(strip=True)\n    def current_branch(self, **kwargs):\n        return self.rev_parse(\"--abbrev-ref\", \"HEAD\", **kwargs)\n\n    def repository_root(self, git_dir=None, **kwargs):\n        \"\"\" Locates the repository's root path from a subdirectory. \"\"\"\n        stdout = self.rev_parse(\"--show-toplevel\", git_dir=git_dir, **kwargs)\n        return stdout.decode('utf-8')\n\n\ngit = Git()\n", "dev/archery/archery/utils/tmpdir.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom contextlib import contextmanager\nfrom tempfile import mkdtemp, TemporaryDirectory\n\n\n@contextmanager\ndef tmpdir(preserve=False, prefix=\"arrow-archery-\"):\n    if preserve:\n        yield mkdtemp(prefix=prefix)\n    else:\n        with TemporaryDirectory(prefix=prefix) as tmp:\n            yield tmp\n", "dev/archery/archery/utils/cache.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom pathlib import Path\nimport os\nfrom urllib.request import urlopen\n\nfrom .logger import logger\n\nARCHERY_CACHE_DIR = Path.home() / \".cache\" / \"archery\"\n\n\nclass Cache:\n    \"\"\" Cache stores downloaded objects, notably apache-rat.jar. \"\"\"\n\n    def __init__(self, path=ARCHERY_CACHE_DIR):\n        self.root = path\n\n        if not path.exists():\n            os.makedirs(path)\n\n    def key_path(self, key):\n        \"\"\" Return the full path of a key. \"\"\"\n        return self.root/key\n\n    def get(self, key):\n        \"\"\" Return the full path of a key if cached, None otherwise. \"\"\"\n        path = self.key_path(key)\n        return path if path.exists() else None\n\n    def delete(self, key):\n        \"\"\" Remove a key (and the file) from the cache. \"\"\"\n        path = self.get(key)\n        if path:\n            path.unlink()\n\n    def get_or_insert(self, key, create):\n        \"\"\"\n        Get or Insert a key from the cache. If the key is not found, the\n        `create` closure will be evaluated.\n\n        The `create` closure takes a single parameter, the path where the\n        object should be store. The file should only be created upon success.\n        \"\"\"\n        path = self.key_path(key)\n\n        if not path.exists():\n            create(path)\n\n        return path\n\n    def get_or_insert_from_url(self, key, url):\n        \"\"\"\n        Get or Insert a key from the cache. If the key is not found, the file\n        is downloaded from `url`.\n        \"\"\"\n        def download(path):\n            \"\"\" Tiny wrapper that download a file and save as key. \"\"\"\n            logger.debug(\"Downloading {} as {}\".format(url, path))\n            conn = urlopen(url)\n            # Ensure the download is completed before writing to disks.\n            content = conn.read()\n            with open(path, \"wb\") as path_fd:\n                path_fd.write(content)\n\n        return self.get_or_insert(key, download)\n", "dev/archery/archery/utils/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "dev/archery/archery/utils/logger.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport logging\n\n\"\"\" Global logger. \"\"\"\nlogger = logging.getLogger(\"archery\")\n\n\nclass LoggingContext:\n    def __init__(self, quiet=False):\n        self.quiet = quiet\n\n\nctx = LoggingContext()\n", "dev/archery/archery/utils/maven.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nfrom .command import Command, default_bin\n\n\nclass Maven(Command):\n    def __init__(self, maven_bin=None):\n        self.bin = default_bin(maven_bin, \"mvn\")\n\n\nmaven = Maven()\n\n\nclass MavenDefinition:\n    \"\"\" MavenDefinition captures the maven invocation arguments.\n\n    It allows creating build directories with the same definition, e.g.\n    ```\n    build_1 = maven_def.build(\"/tmp/build-1\")\n    build_2 = maven_def.build(\"/tmp/build-2\")\n\n    ...\n\n    build1.install()\n    build2.install()\n    \"\"\"\n\n    def __init__(self, source, build_definitions=None,\n                 benchmark_definitions=None, env=None):\n        \"\"\" Initialize a MavenDefinition\n\n        Parameters\n        ----------\n        source : str\n                 Source directory where the top-level pom.xml is\n                 located. This is usually the root of the project.\n        build_definitions: list(str), optional\n        benchmark_definitions: list(str), optional\n        \"\"\"\n        self.source = os.path.abspath(source)\n        self.build_definitions = build_definitions if build_definitions else []\n        self.benchmark_definitions =\\\n            benchmark_definitions if benchmark_definitions else []\n        self.env = env\n\n    @property\n    def build_arguments(self):\n        \"\"\"\" Return the arguments to maven invocation for build. \"\"\"\n        arguments = self.build_definitions + [\n            \"-B\", \"-DskipTests\", \"-Drat.skip=true\",\n            \"-Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.\"\n            \"Slf4jMavenTransferListener=warn\",\n            \"-T\", \"2C\", \"install\"\n        ]\n        return arguments\n\n    def build(self, build_dir, force=False, cmd_kwargs=None, **kwargs):\n        \"\"\" Invoke maven into a build directory.\n\n        Parameters\n        ----------\n        build_dir : str\n                    Directory in which the Maven build will be instantiated.\n        force : bool\n                not used now\n        \"\"\"\n        if os.path.exists(build_dir):\n            # Extra safety to ensure we're deleting a build folder.\n            if not MavenBuild.is_build_dir(build_dir):\n                raise FileExistsError(\n                    \"{} is not a maven build\".format(build_dir)\n                )\n\n        cmd_kwargs = cmd_kwargs if cmd_kwargs else {}\n        assert MavenBuild.is_build_dir(build_dir)\n        maven(*self.build_arguments, cwd=build_dir, env=self.env, **cmd_kwargs)\n        return MavenBuild(build_dir, definition=self, **kwargs)\n\n    @property\n    def list_arguments(self):\n        \"\"\"\" Return the arguments to maven invocation for list \"\"\"\n        arguments = [\n            \"-Dskip.perf.benchmarks=false\", \"-Dbenchmark.list=-lp\", \"install\"\n        ]\n        return arguments\n\n    @property\n    def benchmark_arguments(self):\n        \"\"\"\" Return the arguments to maven invocation for benchmark \"\"\"\n        arguments = self.benchmark_definitions + [\n            \"-Dskip.perf.benchmarks=false\", \"-Dbenchmark.fork=1\",\n            \"-Dbenchmark.jvmargs=\\\"-Darrow.enable_null_check_for_get=false \"\n            \"-Darrow.enable_unsafe_memory_access=true\\\"\",\n            \"install\"\n        ]\n        return arguments\n\n    def __repr__(self):\n        return \"MavenDefinition[source={}]\".format(self.source)\n\n\nclass MavenBuild(Maven):\n    \"\"\" MavenBuild represents a build directory initialized by maven.\n\n    The build instance can be used to build/test/install. It alleviates the\n    user to know which generator is used.\n    \"\"\"\n\n    def __init__(self, build_dir, definition=None):\n        \"\"\" Initialize a MavenBuild.\n\n        The caller must ensure that maven was invoked in the build directory.\n\n        Parameters\n        ----------\n        definition : MavenDefinition\n                     The definition to build from.\n        build_dir : str\n                    The build directory to setup into.\n        \"\"\"\n        assert MavenBuild.is_build_dir(build_dir)\n        super().__init__()\n        self.build_dir = os.path.abspath(build_dir)\n        self.definition = definition\n\n    @property\n    def binaries_dir(self):\n        return self.build_dir\n\n    def run(self, *argv, verbose=False, cwd=None, **kwargs):\n        extra = []\n        if verbose:\n            extra.append(\"-X\")\n        if cwd is None:\n            cwd = self.build_dir\n        # Commands must be ran under the directory where pom.xml exists\n        return super().run(*extra, *argv, **kwargs, cwd=cwd)\n\n    def build(self, *argv, verbose=False, **kwargs):\n        definition_args = self.definition.build_arguments\n        cwd = self.binaries_dir\n        return self.run(*argv, *definition_args, verbose=verbose, cwd=cwd,\n                        env=self.definition.env, **kwargs)\n\n    def list(self, *argv, verbose=False, **kwargs):\n        definition_args = self.definition.list_arguments\n        cwd = self.binaries_dir + \"/performance\"\n        return self.run(*argv, *definition_args, verbose=verbose, cwd=cwd,\n                        env=self.definition.env, **kwargs)\n\n    def benchmark(self, *argv, verbose=False, **kwargs):\n        definition_args = self.definition.benchmark_arguments\n        cwd = self.binaries_dir + \"/performance\"\n        return self.run(*argv, *definition_args, verbose=verbose, cwd=cwd,\n                        env=self.definition.env, **kwargs)\n\n    @staticmethod\n    def is_build_dir(path):\n        \"\"\" Indicate if a path is Maven top directory.\n\n        This method only checks for the existence of paths and does not do any\n        validation whatsoever.\n        \"\"\"\n        pom_xml = os.path.join(path, \"pom.xml\")\n        performance_dir = os.path.join(path, \"performance\")\n        return os.path.exists(pom_xml) and os.path.isdir(performance_dir)\n\n    @staticmethod\n    def from_path(path):\n        \"\"\" Instantiate a Maven from a path.\n\n        This is used to recover from an existing physical directory (created\n        with or without Maven).\n\n        Note that this method is not idempotent as the original definition will\n        be lost.\n        \"\"\"\n        if not MavenBuild.is_build_dir(path):\n            raise ValueError(\"Not a valid MavenBuild path: {}\".format(path))\n\n        return MavenBuild(path, definition=None)\n\n    def __repr__(self):\n        return (\"MavenBuild[\"\n                \"build = {},\"\n                \"definition = {}]\".format(self.build_dir,\n                                          self.definition))\n", "dev/archery/archery/utils/lint.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport fnmatch\nimport gzip\nimport os\nfrom pathlib import Path\n\nimport click\n\nfrom .command import Bash, Command, default_bin\nfrom ..compat import _get_module\nfrom .cmake import CMake\nfrom .git import git\nfrom .logger import logger\nfrom ..lang.cpp import CppCMakeDefinition, CppConfiguration\nfrom ..lang.python import Autopep8, Flake8, CythonLint, NumpyDoc, PythonCommand\nfrom .rat import Rat, exclusion_from_globs\nfrom .tmpdir import tmpdir\n\n\n_archery_install_msg = (\n    \"Please install archery using: `pip install -e dev/archery[lint]`. \"\n)\n\n\nclass LintValidationException(Exception):\n    pass\n\n\nclass LintResult:\n    def __init__(self, success, reason=None):\n        self.success = success\n\n    def ok(self):\n        if not self.success:\n            raise LintValidationException\n\n    @staticmethod\n    def from_cmd(command_result):\n        return LintResult(command_result.returncode == 0)\n\n\ndef cpp_linter(src, build_dir, clang_format=True, cpplint=True,\n               clang_tidy=False, iwyu=False, iwyu_all=False,\n               fix=False):\n    \"\"\" Run clang-format, cpplint and clang-tidy on cpp/ codebase. \"\"\"\n    logger.info(\"Running C++ linters\")\n\n    cmake = CMake()\n    if not cmake.available:\n        logger.error(\"cpp linter requested but cmake binary not found.\")\n        return\n\n    # A cmake build directory is required to populate `compile_commands.json`\n    # which in turn is required by clang-tidy. It also provides a convenient\n    # way to hide clang-format/clang-tidy invocation via the Generate\n    # (ninja/make) targets.\n\n    # ARROW_LINT_ONLY exits early but ignore building compile_command.json\n    lint_only = not (iwyu or clang_tidy)\n    cmake_args = {\"with_python\": False, \"with_lint_only\": lint_only}\n    cmake_def = CppCMakeDefinition(src.cpp, CppConfiguration(**cmake_args))\n\n    build = cmake_def.build(build_dir)\n    if clang_format:\n        target = \"format\" if fix else \"check-format\"\n        yield LintResult.from_cmd(build.run(target, check=False))\n\n    if cpplint:\n        yield LintResult.from_cmd(build.run(\"lint\", check=False))\n        yield LintResult.from_cmd(build.run(\"lint_cpp_cli\", check=False))\n\n    if clang_tidy:\n        yield LintResult.from_cmd(build.run(\"check-clang-tidy\", check=False))\n\n    if iwyu:\n        if iwyu_all:\n            iwyu_cmd = \"iwyu-all\"\n        else:\n            iwyu_cmd = \"iwyu\"\n        yield LintResult.from_cmd(build.run(iwyu_cmd, check=False))\n\n\nclass CMakeFormat(Command):\n\n    def __init__(self, paths, cmake_format_bin=None):\n        self.check_version()\n        self.bin = default_bin(cmake_format_bin, \"cmake-format\")\n        self.paths = paths\n\n    @classmethod\n    def from_patterns(cls, base_path, include_patterns, exclude_patterns):\n        paths = {\n            str(path.as_posix())\n            for pattern in include_patterns\n            for path in base_path.glob(pattern)\n        }\n        for pattern in exclude_patterns:\n            pattern = (base_path / pattern).as_posix()\n            paths -= set(fnmatch.filter(paths, str(pattern)))\n        return cls(paths)\n\n    @staticmethod\n    def check_version():\n        try:\n            # cmake_format is part of the cmakelang package\n            import cmakelang\n        except ImportError:\n            raise ImportError(\n\n            )\n        # pin a specific version of cmake_format, must be updated in setup.py\n        if cmakelang.__version__ != \"0.6.13\":\n            raise LintValidationException(\n                f\"Wrong version of cmake_format is detected. \"\n                f\"{_archery_install_msg}\"\n            )\n\n    def check(self):\n        return self.run(\"-l\", \"error\", \"--check\", *self.paths, check=False)\n\n    def fix(self):\n        return self.run(\"--in-place\", *self.paths, check=False)\n\n\ndef cmake_linter(src, fix=False):\n    \"\"\"\n    Run cmake-format on all CMakeFiles.txt\n    \"\"\"\n    logger.info(\"Running cmake-format linters\")\n\n    cmake_format = CMakeFormat.from_patterns(\n        src.path,\n        include_patterns=[\n            'ci/**/*.cmake',\n            'cpp/CMakeLists.txt',\n            'cpp/src/**/*.cmake',\n            'cpp/src/**/*.cmake.in',\n            'cpp/src/**/CMakeLists.txt',\n            'cpp/examples/**/CMakeLists.txt',\n            'cpp/cmake_modules/*.cmake',\n            'go/**/CMakeLists.txt',\n            'java/**/CMakeLists.txt',\n            'matlab/**/CMakeLists.txt',\n            'python/**/CMakeLists.txt',\n        ],\n        exclude_patterns=[\n            'cpp/cmake_modules/FindNumPy.cmake',\n            'cpp/cmake_modules/FindPythonLibsNew.cmake',\n            'cpp/cmake_modules/UseCython.cmake',\n            'cpp/src/arrow/util/*.h.cmake',\n        ]\n    )\n    method = cmake_format.fix if fix else cmake_format.check\n\n    yield LintResult.from_cmd(method())\n\n\ndef python_linter(src, fix=False):\n    \"\"\"Run Python linters on python/pyarrow, python/examples, setup.py\n    and dev/. \"\"\"\n    setup_py = os.path.join(src.python, \"setup.py\")\n    setup_cfg = os.path.join(src.python, \"setup.cfg\")\n\n    logger.info(\"Running Python formatter (autopep8)\")\n\n    autopep8 = Autopep8()\n    if not autopep8.available:\n        logger.error(\n            \"Python formatter requested but autopep8 binary not found. \"\n            f\"{_archery_install_msg}\")\n        return\n\n    # Gather files for autopep8\n    patterns = [\"python/benchmarks/**/*.py\",\n                \"python/examples/**/*.py\",\n                \"python/pyarrow/**/*.py\",\n                \"python/pyarrow/**/*.pyx\",\n                \"python/pyarrow/**/*.pxd\",\n                \"python/pyarrow/**/*.pxi\",\n                \"dev/*.py\",\n                \"dev/archery/**/*.py\",\n                \"dev/release/**/*.py\"]\n    files = [setup_py]\n    for pattern in patterns:\n        files += list(map(str, Path(src.path).glob(pattern)))\n\n    args = ['--global-config', setup_cfg, '--ignore-local-config']\n    if fix:\n        args += ['-j0', '--in-place']\n        args += sorted(files)\n        yield LintResult.from_cmd(autopep8(*args))\n    else:\n        # XXX `-j0` doesn't work well with `--exit-code`, so instead\n        # we capture the diff and check whether it's empty\n        # (https://github.com/hhatto/autopep8/issues/543)\n        args += ['-j0', '--diff']\n        args += sorted(files)\n        diff = autopep8.run_captured(*args)\n        if diff:\n            print(diff.decode('utf8'))\n            yield LintResult(success=False)\n        else:\n            yield LintResult(success=True)\n\n    # Run flake8 after autopep8 (the latter may have modified some files)\n    logger.info(\"Running Python linter (flake8)\")\n\n    flake8 = Flake8()\n    if not flake8.available:\n        logger.error(\n            \"Python linter requested but flake8 binary not found. \"\n            f\"{_archery_install_msg}\")\n        return\n\n    flake8_exclude = ['.venv*', 'vendored']\n\n    yield LintResult.from_cmd(\n        flake8(\"--extend-exclude=\" + ','.join(flake8_exclude),\n               \"--config=\" + os.path.join(src.python, \"setup.cfg\"),\n               setup_py, src.pyarrow, os.path.join(src.python, \"benchmarks\"),\n               os.path.join(src.python, \"examples\"), src.dev, check=False))\n\n    logger.info(\"Running Cython linter (cython-lint)\")\n\n    cython_lint = CythonLint()\n    if not cython_lint.available:\n        logger.error(\n            \"Cython linter requested but cython-lint binary not found. \"\n            f\"{_archery_install_msg}\")\n        return\n\n    # Gather files for cython-lint\n    patterns = [\"python/pyarrow/**/*.pyx\",\n                \"python/pyarrow/**/*.pxd\",\n                \"python/pyarrow/**/*.pxi\",\n                \"python/examples/**/*.pyx\",\n                \"python/examples/**/*.pxd\",\n                \"python/examples/**/*.pxi\",\n                ]\n    files = []\n    for pattern in patterns:\n        files += list(map(str, Path(src.path).glob(pattern)))\n    args = ['--no-pycodestyle']\n    args += sorted(files)\n    yield LintResult.from_cmd(cython_lint(*args))\n\n\ndef python_cpp_linter(src, clang_format=True, fix=False):\n    \"\"\"Run C++ linters on python/pyarrow/src/arrow/python.\"\"\"\n    cpp_src = os.path.join(src.python, \"pyarrow\", \"src\", \"arrow\", \"python\")\n\n    python = PythonCommand()\n\n    if clang_format:\n        logger.info(\"Running clang-format for python/pyarrow/src/arrow/python\")\n\n        if \"CLANG_TOOLS_PATH\" in os.environ:\n            clang_format_binary = os.path.join(\n                os.environ[\"CLANG_TOOLS_PATH\"], \"clang-format\")\n        else:\n            clang_format_binary = \"clang-format-14\"\n\n        run_clang_format = os.path.join(src.cpp, \"build-support\",\n                                        \"run_clang_format.py\")\n        args = [run_clang_format, \"--source_dir\", cpp_src,\n                \"--clang_format_binary\", clang_format_binary]\n        if fix:\n            args += [\"--fix\"]\n\n        yield LintResult.from_cmd(python.run(*args))\n\n\ndef python_numpydoc(symbols=None, allow_rules=None, disallow_rules=None):\n    \"\"\"Run numpydoc linter on python.\n\n    Pyarrow must be available for import.\n    \"\"\"\n    logger.info(\"Running Python docstring linters\")\n    # by default try to run on all pyarrow package\n    symbols = symbols or {\n        'pyarrow',\n        'pyarrow.compute',\n        'pyarrow.csv',\n        'pyarrow.dataset',\n        'pyarrow.feather',\n        # 'pyarrow.flight',\n        'pyarrow.fs',\n        'pyarrow.gandiva',\n        'pyarrow.ipc',\n        'pyarrow.json',\n        'pyarrow.orc',\n        'pyarrow.parquet',\n        'pyarrow.types',\n    }\n    try:\n        numpydoc = NumpyDoc(symbols)\n    except RuntimeError as e:\n        logger.error(str(e))\n        yield LintResult(success=False)\n        return\n\n    results = numpydoc.validate(\n        # limit the validation scope to the pyarrow package\n        from_package='pyarrow',\n        allow_rules=allow_rules,\n        disallow_rules=disallow_rules\n    )\n\n    if len(results) == 0:\n        yield LintResult(success=True)\n        return\n\n    number_of_violations = 0\n    for obj, result in results:\n        errors = result['errors']\n\n        # inspect doesn't play nice with cython generated source code,\n        # to use a hacky way to represent a proper __qualname__\n        doc = getattr(obj, '__doc__', '')\n        name = getattr(obj, '__name__', '')\n        qualname = getattr(obj, '__qualname__', '')\n        module = _get_module(obj, default='')\n        instance = getattr(obj, '__self__', '')\n        if instance:\n            klass = instance.__class__.__name__\n        else:\n            klass = ''\n\n        try:\n            cython_signature = doc.splitlines()[0]\n        except Exception:\n            cython_signature = ''\n\n        desc = '.'.join(filter(None, [module, klass, qualname or name]))\n\n        click.echo()\n        click.echo(click.style(desc, bold=True, fg='yellow'))\n        if cython_signature:\n            qualname_with_signature = '.'.join([module, cython_signature])\n            click.echo(\n                click.style(\n                    '-> {}'.format(qualname_with_signature),\n                    fg='yellow'\n                )\n            )\n\n        for error in errors:\n            number_of_violations += 1\n            click.echo('{}: {}'.format(*error))\n\n    msg = 'Total number of docstring violations: {}'.format(\n        number_of_violations\n    )\n    click.echo()\n    click.echo(click.style(msg, fg='red'))\n\n    yield LintResult(success=False)\n\n\ndef rat_linter(src, root):\n    \"\"\"Run apache-rat license linter.\"\"\"\n    logger.info(\"Running apache-rat linter\")\n\n    if src.git_dirty:\n        logger.warn(\"Due to the usage of git-archive, uncommitted files will\"\n                    \" not be checked for rat violations. \")\n\n    exclusion = exclusion_from_globs(\n        os.path.join(src.dev, \"release\", \"rat_exclude_files.txt\"))\n\n    # Creates a git-archive of ArrowSources, apache-rat expects a gzip\n    # compressed tar archive.\n    archive_path = os.path.join(root, \"apache-arrow.tar.gz\")\n    src.archive(archive_path, compressor=gzip.compress)\n    report = Rat().report(archive_path)\n\n    violations = list(report.validate(exclusion=exclusion))\n    for violation in violations:\n        print(\"apache-rat license violation: {}\".format(violation))\n\n    yield LintResult(len(violations) == 0)\n\n\ndef r_linter(src):\n    \"\"\"Run R linter.\"\"\"\n    logger.info(\"Running R linter\")\n    r_lint_sh = os.path.join(src.r, \"lint.sh\")\n    yield LintResult.from_cmd(Bash().run(r_lint_sh, check=False))\n\n\nclass Hadolint(Command):\n    def __init__(self, hadolint_bin=None):\n        self.bin = default_bin(hadolint_bin, \"hadolint\")\n\n\ndef is_docker_image(path):\n    dirname = os.path.dirname(path)\n    filename = os.path.basename(path)\n\n    excluded = dirname.startswith(\n        \"dev\") or dirname.startswith(\"python/manylinux\")\n\n    return filename.startswith(\"Dockerfile\") and not excluded\n\n\ndef docker_linter(src):\n    \"\"\"Run Hadolint docker linter.\"\"\"\n    logger.info(\"Running Docker linter\")\n\n    hadolint = Hadolint()\n\n    if not hadolint.available:\n        logger.error(\n            \"hadolint linter requested but hadolint binary not found.\")\n        return\n\n    for path in git.ls_files(git_dir=src.path):\n        if is_docker_image(path):\n            yield LintResult.from_cmd(hadolint.run(path, check=False,\n                                                   cwd=src.path))\n\n\nclass SphinxLint(Command):\n    def __init__(self, src, path=None, sphinx_lint_bin=None, disable=None, enable=None):\n        self.src = src\n        self.path = path\n        self.bin = default_bin(sphinx_lint_bin, \"sphinx-lint\")\n        self.disable = disable or \"all\"\n        self.enable = enable\n\n    def lint(self, *args, check=False):\n        docs_path = os.path.join(self.src.path, \"docs\")\n\n        args = []\n\n        if self.disable:\n            args.extend([\"--disable\", self.disable])\n\n        if self.enable:\n            args.extend([\"--enable\", self.enable])\n\n        if self.path is not None:\n            args.extend([self.path])\n        else:\n            args.extend([docs_path])\n\n        return self.run(*args, check=check)\n\n\ndef docs_linter(src, path=None):\n    \"\"\"Run sphinx-lint on docs.\"\"\"\n    logger.info(\"Running docs linter (sphinx-lint)\")\n\n    sphinx_lint = SphinxLint(\n        src,\n        path=path,\n        disable=\"all\",\n        enable=\"trailing-whitespace,missing-final-newline\"\n    )\n\n    if not sphinx_lint.available:\n        logger.error(\"sphinx-lint linter requested but sphinx-lint binary not found\")\n        return\n\n    yield LintResult.from_cmd(sphinx_lint.lint())\n\n\ndef linter(src, fix=False, path=None, *, clang_format=False, cpplint=False,\n           clang_tidy=False, iwyu=False, iwyu_all=False,\n           python=False, numpydoc=False, cmake_format=False, rat=False,\n           r=False, docker=False, docs=False):\n    \"\"\"Run all linters.\"\"\"\n    with tmpdir(prefix=\"arrow-lint-\") as root:\n        build_dir = os.path.join(root, \"cpp-build\")\n\n        # Linters yield LintResult without raising exceptions on failure.\n        # This allows running all linters in one pass and exposing all\n        # errors to the user.\n        results = []\n\n        if clang_format or cpplint or clang_tidy or iwyu:\n            results.extend(cpp_linter(src, build_dir,\n                                      clang_format=clang_format,\n                                      cpplint=cpplint,\n                                      clang_tidy=clang_tidy,\n                                      iwyu=iwyu,\n                                      iwyu_all=iwyu_all,\n                                      fix=fix))\n\n        if python:\n            results.extend(python_linter(src, fix=fix))\n\n        if python and clang_format:\n            results.extend(python_cpp_linter(src,\n                                             clang_format=clang_format,\n                                             fix=fix))\n\n        if numpydoc:\n            results.extend(python_numpydoc())\n\n        if cmake_format:\n            results.extend(cmake_linter(src, fix=fix))\n\n        if rat:\n            results.extend(rat_linter(src, root))\n\n        if r:\n            results.extend(r_linter(src))\n\n        if docker:\n            results.extend(docker_linter(src))\n\n        if docs:\n            results.extend(docs_linter(src, path))\n\n        # Raise error if one linter failed, ensuring calling code can exit with\n        # non-zero.\n        for result in results:\n            result.ok()\n", "dev/archery/archery/docker/cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport sys\n\nimport click\n\nfrom ..utils.cli import validate_arrow_sources\nfrom .core import DockerCompose, UndefinedImage\n\n\ndef _mock_compose_calls(compose):\n    from types import MethodType\n    from subprocess import CompletedProcess\n\n    def _mock(compose, executable):\n        def _execute(self, *args, **kwargs):\n            params = ['{}={}'.format(k, v)\n                      for k, v in self.config.params.items()]\n            command = ' '.join(params + [executable] + list(args))\n            click.echo(command)\n            return CompletedProcess([], 0)\n        return MethodType(_execute, compose)\n\n    compose._execute_docker = _mock(compose, executable='docker')\n    compose._execute_compose = _mock(compose, executable='docker-compose')\n\n\n@click.group()\n@click.option(\"--src\", metavar=\"<arrow_src>\", default=None,\n              callback=validate_arrow_sources,\n              help=\"Specify Arrow source directory.\")\n@click.option('--dry-run/--execute', default=False,\n              help=\"Display the docker commands instead of executing them.\")\n@click.option('--using-docker-cli', default=False, is_flag=True,\n              envvar='ARCHERY_USE_DOCKER_CLI',\n              help=\"Use docker CLI directly for building instead of calling \"\n                   \"docker-compose. This may help to reuse cached layers.\")\n@click.option('--using-docker-buildx', default=False, is_flag=True,\n              envvar='ARCHERY_USE_DOCKER_BUILDX',\n              help=\"Use buildx with docker CLI directly for building instead \"\n                   \"of calling docker-compose or the plain docker build \"\n                   \"command. This option makes the build cache reusable \"\n                   \"across hosts.\")\n@click.pass_context\ndef docker(ctx, src, dry_run, using_docker_cli, using_docker_buildx):\n    \"\"\"\n    Interact with docker-compose based builds.\n    \"\"\"\n    ctx.ensure_object(dict)\n\n    config_path = src.path / 'docker-compose.yml'\n    if not config_path.exists():\n        raise click.ClickException(\n            \"Docker compose configuration cannot be found in directory {}, \"\n            \"try to pass the arrow source directory explicitly.\".format(src)\n        )\n\n    # take the docker-compose parameters like PYTHON, PANDAS, UBUNTU from the\n    # environment variables to keep the usage similar to docker-compose\n    using_docker_cli |= using_docker_buildx\n    compose = DockerCompose(config_path, params=os.environ,\n                            using_docker=using_docker_cli,\n                            using_buildx=using_docker_buildx,\n                            debug=ctx.obj.get('debug', False),\n                            compose_bin=(\"docker compose\" if using_docker_cli\n                                         else \"docker-compose\"))\n    if dry_run:\n        _mock_compose_calls(compose)\n    ctx.obj['compose'] = compose\n\n\n@docker.command(\"check-config\")\n@click.pass_obj\ndef check_config(obj):\n    \"\"\"\n    Validate docker-compose configuration.\n    \"\"\"\n    # executes the body of the docker function above which does the validation\n    # during the configuration loading\n\n\n@docker.command('pull')\n@click.argument('image')\n@click.option('--pull-leaf/--no-leaf', default=True,\n              help=\"Whether to pull leaf images too.\")\n@click.option('--ignore-pull-failures/--no-ignore-pull-failures', default=True,\n              help=\"Whether to ignore pull failures.\")\n@click.pass_obj\ndef docker_pull(obj, image, *, pull_leaf, ignore_pull_failures):\n    \"\"\"\n    Execute docker-compose pull.\n    \"\"\"\n    compose = obj['compose']\n\n    try:\n        compose.pull(image, pull_leaf=pull_leaf,\n                     ignore_pull_failures=ignore_pull_failures)\n    except UndefinedImage as e:\n        raise click.ClickException(\n            \"There is no service/image defined in docker-compose.yml with \"\n            \"name: {}\".format(str(e))\n        )\n    except RuntimeError as e:\n        raise click.ClickException(str(e))\n\n\n@docker.command('build')\n@click.argument('image')\n@click.option('--force-pull/--no-pull', default=True,\n              help=\"Whether to force pull the image and its ancestor images\")\n@click.option('--use-cache/--no-cache', default=True,\n              help=\"Whether to use cache when building the image and its \"\n                   \"ancestor images\")\n@click.option('--use-leaf-cache/--no-leaf-cache', default=True,\n              help=\"Whether to use cache when building only the (leaf) image \"\n                   \"passed as the argument. To disable caching for both the \"\n                   \"image and its ancestors use --no-cache option.\")\n@click.pass_obj\ndef docker_build(obj, image, *, force_pull, use_cache, use_leaf_cache):\n    \"\"\"\n    Execute docker-compose builds.\n    \"\"\"\n    compose = obj['compose']\n\n    try:\n        if force_pull:\n            compose.pull(image, pull_leaf=use_leaf_cache)\n        compose.build(image, use_cache=use_cache,\n                      use_leaf_cache=use_leaf_cache,\n                      pull_parents=force_pull)\n    except UndefinedImage as e:\n        raise click.ClickException(\n            \"There is no service/image defined in docker-compose.yml with \"\n            \"name: {}\".format(str(e))\n        )\n    except RuntimeError as e:\n        raise click.ClickException(str(e))\n\n\n@docker.command('run')\n@click.argument('image')\n@click.argument('command', required=False, default=None)\n@click.option('--env', '-e', multiple=True,\n              help=\"Set environment variable within the container\")\n@click.option('--user', '-u', default=None,\n              help=\"Username or UID to run the container with\")\n@click.option('--force-pull/--no-pull', default=True,\n              help=\"Whether to force pull the image and its ancestor images\")\n@click.option('--force-build/--no-build', default=True,\n              help=\"Whether to force build the image and its ancestor images\")\n@click.option('--build-only', default=False, is_flag=True,\n              help=\"Pull and/or build the image, but do not run it\")\n@click.option('--use-cache/--no-cache', default=True,\n              help=\"Whether to use cache when building the image and its \"\n                   \"ancestor images\")\n@click.option('--use-leaf-cache/--no-leaf-cache', default=True,\n              help=\"Whether to use cache when building only the (leaf) image \"\n                   \"passed as the argument. To disable caching for both the \"\n                   \"image and its ancestors use --no-cache option.\")\n@click.option('--resource-limit', default=None,\n              help=\"A CPU/memory limit preset to mimic CI environments like \"\n                   \"GitHub Actions. Mandates --using-docker-cli. Note that \"\n                   \"exporting ARCHERY_DOCKER_BIN=\\\"sudo docker\\\" is likely \"\n                   \"required, unless Docker is configured with cgroups v2 \"\n                   \"(else Docker will silently ignore the limits).\")\n@click.option('--volume', '-v', multiple=True,\n              help=\"Set volume within the container\")\n@click.pass_obj\ndef docker_run(obj, image, command, *, env, user, force_pull, force_build,\n               build_only, use_cache, use_leaf_cache, resource_limit,\n               volume):\n    \"\"\"\n    Execute docker-compose builds.\n\n    To see the available builds run `archery docker images`.\n\n    Examples:\n\n    # execute a single build\n    archery docker run conda-python\n\n    # execute the builds but disable the image pulling\n    archery docker run --no-cache conda-python\n\n    # pass a docker-compose parameter, like the python version\n    PYTHON=3.12 archery docker run conda-python\n\n    # disable the cache only for the leaf image\n    PANDAS=upstream_devel archery docker run --no-leaf-cache \\\n        conda-python-pandas\n\n    # entirely skip building the image\n    archery docker run --no-pull --no-build conda-python\n\n    # pass runtime parameters via docker environment variables\n    archery docker run -e CMAKE_BUILD_TYPE=release ubuntu-cpp\n\n    # set a volume\n    archery docker run -v $PWD/build:/build ubuntu-cpp\n\n    # starting an interactive bash session for debugging\n    archery docker run ubuntu-cpp bash\n    \"\"\"\n    compose = obj['compose']\n\n    env = dict(kv.split('=', 1) for kv in env)\n    try:\n        if force_pull:\n            compose.pull(image, pull_leaf=use_leaf_cache)\n        if force_build:\n            compose.build(image, use_cache=use_cache,\n                          use_leaf_cache=use_leaf_cache)\n        if build_only:\n            return\n        compose.run(\n            image,\n            command=command,\n            env=env,\n            user=user,\n            resource_limit=resource_limit,\n            volumes=volume\n        )\n    except UndefinedImage as e:\n        raise click.ClickException(\n            \"There is no service/image defined in docker-compose.yml with \"\n            \"name: {}\".format(str(e))\n        )\n    except RuntimeError as e:\n        raise click.ClickException(str(e))\n\n\n@docker.command('push')\n@click.argument('image')\n@click.option('--user', '-u', required=False, envvar='ARCHERY_DOCKER_USER',\n              help='Docker repository username')\n@click.option('--password', '-p', required=False,\n              envvar='ARCHERY_DOCKER_PASSWORD',\n              help='Docker repository password')\n@click.pass_obj\ndef docker_compose_push(obj, image, user, password):\n    \"\"\"Push the generated docker-compose image.\"\"\"\n    compose = obj['compose']\n    compose.push(image, user=user, password=password)\n\n\n@docker.command('images')\n@click.pass_obj\ndef docker_compose_images(obj):\n    \"\"\"List the available docker-compose images.\"\"\"\n    compose = obj['compose']\n    click.echo('Available images:')\n    for image in compose.images():\n        click.echo(f' - {image}')\n\n\n@docker.command('info')\n@click.argument('service_name')\n@click.option('--show', '-s', required=False,\n              help=\"Show only specific docker-compose key. Examples of keys:\"\n                   \" command, environment, build, dockerfile\")\n@click.pass_obj\ndef docker_compose_info(obj, service_name, show):\n    \"\"\"Show docker-compose definition info for service_name.\n\n    SERVICE_NAME is the name of the docker service defined on\n    the docker-compose. Look at `archery docker images` output for names.\n    \"\"\"\n    compose = obj['compose']\n    try:\n        service = compose.config.raw_config[\"services\"][service_name]\n    except KeyError:\n        click.echo(f'Service name {service_name} could not be found', err=True)\n        sys.exit(1)\n    else:\n        click.echo(f'Service {service_name} docker-compose config:')\n        output = \"\\n\".join(compose.info(service, show))\n        click.echo(output)\n", "dev/archery/archery/docker/core.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport shlex\nimport subprocess\nfrom io import StringIO\n\nfrom dotenv import dotenv_values\nfrom ruamel.yaml import YAML\n\nfrom ..utils.command import Command, default_bin\nfrom ..utils.source import arrow_path\nfrom ..compat import _ensure_path\n\n\ndef flatten(node, parents=None):\n    parents = list(parents or [])\n    if isinstance(node, str):\n        yield (node, parents)\n    elif isinstance(node, list):\n        for value in node:\n            yield from flatten(value, parents=parents)\n    elif isinstance(node, dict):\n        for key, value in node.items():\n            yield (key, parents)\n            yield from flatten(value, parents=parents + [key])\n    else:\n        raise TypeError(node)\n\n\n_arch_short_mapping = {\n    'arm64v8': 'arm64',\n}\n_arch_alias_mapping = {\n    'amd64': 'x86_64',\n    'arm64v8': 'aarch64',\n}\n\n\nclass UndefinedImage(Exception):\n    pass\n\n\nclass ComposeConfig:\n\n    def __init__(self, config_path, dotenv_path, compose_bin,\n                 using_docker=False, using_buildx=False,\n                 params=None, debug=False):\n        self.using_docker = using_docker\n        self.using_buildx = using_buildx\n        self.debug = debug\n        config_path = _ensure_path(config_path)\n        if dotenv_path:\n            dotenv_path = _ensure_path(dotenv_path)\n        else:\n            dotenv_path = config_path.parent / '.env'\n        if self.debug:\n            # Log docker version\n            Docker().run('version')\n\n        self._read_env(dotenv_path, params)\n        self._read_config(config_path, compose_bin)\n\n    def _read_env(self, dotenv_path, params):\n        \"\"\"\n        Read .env and merge it with explicitly passed parameters.\n        \"\"\"\n        self.dotenv = dotenv_values(str(dotenv_path))\n        if params is None:\n            self.params = {}\n        else:\n            self.params = {k: v for k, v in params.items() if k in self.dotenv}\n\n        # forward the process' environment variables\n        self.env = os.environ.copy()\n        # set the defaults from the dotenv files\n        self.env.update(self.dotenv)\n        # override the defaults passed as parameters\n        self.env.update(self.params)\n\n        # translate docker's architecture notation to a more widely used one\n        arch = self.env.get('ARCH', 'amd64')\n        self.env['ARCH_ALIAS'] = _arch_alias_mapping.get(arch, arch)\n        self.env['ARCH_SHORT'] = _arch_short_mapping.get(arch, arch)\n\n    def _read_config(self, config_path, compose_bin):\n        \"\"\"\n        Validate and read the docker-compose.yml\n        \"\"\"\n        yaml = YAML()\n        with config_path.open() as fp:\n            self.raw_config = yaml.load(fp)\n\n        services = self.raw_config['services'].keys()\n        self.hierarchy = dict(flatten(self.raw_config.get('x-hierarchy', {})))\n        self.limit_presets = self.raw_config.get('x-limit-presets', {})\n        self.with_gpus = self.raw_config.get('x-with-gpus', [])\n        nodes = self.hierarchy.keys()\n        errors = []\n\n        for name in self.with_gpus:\n            if name not in services:\n                errors.append(\n                    'Service `{}` defined in `x-with-gpus` bot not in '\n                    '`services`'.format(name)\n                )\n        for name in nodes - services:\n            errors.append(\n                'Service `{}` is defined in `x-hierarchy` bot not in '\n                '`services`'.format(name)\n            )\n        for name in services - nodes:\n            errors.append(\n                'Service `{}` is defined in `services` but not in '\n                '`x-hierarchy`'.format(name)\n            )\n\n        # trigger docker-compose's own validation\n        if self.using_docker:\n            compose = Docker()\n            args = ['compose']\n        else:\n            compose = Command('docker-compose')\n            args = []\n        args += ['--file', str(config_path), 'config']\n        result = compose.run(*args, env=self.env, check=False,\n                             stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\n        if result.returncode != 0:\n            # strip the intro line of docker-compose errors\n            errors += result.stderr.decode().splitlines()\n\n        if errors:\n            msg = '\\n'.join([' - {}'.format(msg) for msg in errors])\n            raise ValueError(\n                'Found errors with docker-compose:\\n{}'.format(msg)\n            )\n\n        rendered_config = StringIO(result.stdout.decode())\n        self.path = config_path\n        self.config = yaml.load(rendered_config)\n\n    def get(self, service_name):\n        try:\n            service = self.config['services'][service_name]\n        except KeyError:\n            raise UndefinedImage(service_name)\n        service['name'] = service_name\n        service['need_gpu'] = service_name in self.with_gpus\n        service['ancestors'] = self.hierarchy[service_name]\n        return service\n\n    def __getitem__(self, service_name):\n        return self.get(service_name)\n\n\nclass Docker(Command):\n\n    def __init__(self, docker_bin=None):\n        self.bin = default_bin(docker_bin, \"docker\")\n\n\nclass DockerCompose(Command):\n\n    def __init__(self, config_path, dotenv_path=None, compose_bin=None,\n                 using_docker=False, using_buildx=False, params=None,\n                 debug=False):\n        compose_bin = default_bin(compose_bin, 'docker-compose')\n        self.config = ComposeConfig(config_path, dotenv_path, compose_bin,\n                                    params=params, using_docker=using_docker,\n                                    using_buildx=using_buildx, debug=debug)\n        self.bin = compose_bin\n        self.pull_memory = set()\n\n    def clear_pull_memory(self):\n        self.pull_memory = set()\n\n    def _execute_compose(self, *args, **kwargs):\n        # execute as a docker compose command\n        try:\n            result = super().run('--file', str(self.config.path), *args,\n                                 env=self.config.env, **kwargs)\n            result.check_returncode()\n        except subprocess.CalledProcessError as e:\n            def formatdict(d, template):\n                return '\\n'.join(\n                    template.format(k, v) for k, v in sorted(d.items())\n                )\n            msg = (\n                \"`{cmd}` exited with a non-zero exit code {code}, see the \"\n                \"process log above.\\n\\nThe docker-compose command was \"\n                \"invoked with the following parameters:\\n\\nDefaults defined \"\n                \"in .env:\\n{dotenv}\\n\\nArchery was called with:\\n{params}\"\n            )\n            raise RuntimeError(\n                msg.format(\n                    cmd=' '.join(e.cmd),\n                    code=e.returncode,\n                    dotenv=formatdict(self.config.dotenv, template='  {}: {}'),\n                    params=formatdict(\n                        self.config.params, template='  export {}={}'\n                    )\n                )\n            )\n\n    def _execute_docker(self, *args, **kwargs):\n        # execute as a plain docker cli command\n        try:\n            result = Docker().run(*args, **kwargs)\n            result.check_returncode()\n        except subprocess.CalledProcessError as e:\n            raise RuntimeError(\n                \"{} exited with non-zero exit code {}\".format(\n                    ' '.join(e.cmd), e.returncode\n                )\n            )\n\n    def pull(self, service_name, pull_leaf=True, ignore_pull_failures=True):\n        def _pull(service):\n            args = ['pull']\n            if service['image'] in self.pull_memory:\n                return\n\n            if self.config.using_docker:\n                try:\n                    self._execute_docker(*args, service['image'])\n                except Exception as e:\n                    if ignore_pull_failures:\n                        # better --ignore-pull-failures handling\n                        print(e)\n                    else:\n                        raise\n            else:\n                if ignore_pull_failures:\n                    args.append('--ignore-pull-failures')\n                self._execute_compose(*args, service['name'])\n\n            self.pull_memory.add(service['image'])\n\n        service = self.config.get(service_name)\n        for ancestor in service['ancestors']:\n            _pull(self.config.get(ancestor))\n        if pull_leaf:\n            _pull(service)\n\n    def build(self, service_name, use_cache=True, use_leaf_cache=True,\n              pull_parents=True):\n        def _build(service, use_cache):\n            if 'build' not in service:\n                # nothing to do\n                return\n\n            args = []\n            cache_from = list(service.get('build', {}).get('cache_from', []))\n            if pull_parents:\n                for image in cache_from:\n                    if image not in self.pull_memory:\n                        try:\n                            self._execute_docker('pull', image)\n                        except Exception as e:\n                            print(e)\n                        finally:\n                            self.pull_memory.add(image)\n\n            if not use_cache:\n                args.append('--no-cache')\n\n            # turn on inline build cache, this is a docker buildx feature\n            # used to bundle the image build cache to the pushed image manifest\n            # so the build cache can be reused across hosts, documented at\n            # https://github.com/docker/buildx#--cache-tonametypetypekeyvalue\n            if self.config.env.get('BUILDKIT_INLINE_CACHE') == '1':\n                args.extend(['--build-arg', 'BUILDKIT_INLINE_CACHE=1'])\n\n            if self.config.using_buildx:\n                for k, v in service['build'].get('args', {}).items():\n                    args.extend(['--build-arg', '{}={}'.format(k, v)])\n\n                if use_cache:\n                    cache_ref = '{}-cache'.format(service['image'])\n                    cache_from = 'type=registry,ref={}'.format(cache_ref)\n                    cache_to = (\n                        'type=registry,ref={},mode=max'.format(cache_ref)\n                    )\n                    args.extend([\n                        '--cache-from', cache_from,\n                        '--cache-to', cache_to,\n                    ])\n\n                args.extend([\n                    '--output', 'type=docker',\n                    '-f', arrow_path(service['build']['dockerfile']),\n                    '-t', service['image'],\n                    service['build'].get('context', '.')\n                ])\n                self._execute_docker(\"buildx\", \"build\", *args)\n            elif self.config.using_docker:\n                # better for caching\n                if self.config.debug and os.name != \"nt\":\n                    args.append(\"--progress=plain\")\n                for k, v in service['build'].get('args', {}).items():\n                    args.extend(['--build-arg', '{}={}'.format(k, v)])\n                for img in cache_from:\n                    args.append('--cache-from=\"{}\"'.format(img))\n                args.extend([\n                    '-f', arrow_path(service['build']['dockerfile']),\n                    '-t', service['image'],\n                    service['build'].get('context', '.')\n                ])\n                self._execute_docker(\"build\", *args)\n            else:\n                if self.config.debug and os.name != \"nt\":\n                    args.append(\"--progress=plain\")\n                self._execute_compose(\"build\", *args, service['name'])\n\n        service = self.config.get(service_name)\n        # build ancestor services\n        for ancestor in service['ancestors']:\n            _build(self.config.get(ancestor), use_cache=use_cache)\n        # build the leaf/target service\n        _build(service, use_cache=use_cache and use_leaf_cache)\n\n    def run(self, service_name, command=None, *, env=None, volumes=None,\n            user=None, resource_limit=None):\n        service = self.config.get(service_name)\n\n        args = []\n        if user is not None:\n            args.extend(['-u', user])\n\n        if env is not None:\n            for k, v in env.items():\n                args.extend(['-e', '{}={}'.format(k, v)])\n\n        if volumes is not None:\n            for volume in volumes:\n                args.extend(['--volume', volume])\n\n        if self.config.using_docker or service['need_gpu'] or resource_limit:\n            # use gpus, requires docker>=19.03\n            if service['need_gpu']:\n                args.extend(['--gpus', 'all'])\n\n            if service.get('shm_size'):\n                args.extend(['--shm-size', service['shm_size']])\n\n            # append env variables from the compose conf\n            for k, v in service.get('environment', {}).items():\n                if v is not None:\n                    args.extend(['-e', '{}={}'.format(k, v)])\n\n            # append volumes from the compose conf\n            for v in service.get('volumes', []):\n                if not isinstance(v, str):\n                    # if not the compact string volume definition\n                    v = \"{}:{}\".format(v['source'], v['target'])\n                args.extend(['-v', v])\n\n            # append capabilities from the compose conf\n            for c in service.get('cap_add', []):\n                args.extend([f'--cap-add={c}'])\n\n            # infer whether an interactive shell is desired or not\n            if command in ['cmd.exe', 'bash', 'sh', 'powershell']:\n                args.append('-it')\n\n            if resource_limit:\n                limits = self.config.limit_presets.get(resource_limit)\n                if not limits:\n                    raise ValueError(\n                        f\"Unknown resource limit preset '{resource_limit}'\")\n                cpuset = limits.get('cpuset_cpus', [])\n                if cpuset:\n                    args.append(f'--cpuset-cpus={\",\".join(map(str, cpuset))}')\n                memory = limits.get('memory')\n                if memory:\n                    args.append(f'--memory={memory}')\n                    args.append(f'--memory-swap={memory}')\n\n            # get the actual docker image name instead of the compose service\n            # name which we refer as image in general\n            args.append(service['image'])\n\n            # add command from compose if it wasn't overridden\n            if command is not None:\n                args.append(command)\n            else:\n                cmd = service.get('command', '')\n                if cmd:\n                    # service command might be already defined as a list\n                    # on the docker-compose yaml file.\n                    if isinstance(cmd, list):\n                        cmd = shlex.join(cmd)\n                    # Match behaviour from docker compose\n                    # to interpolate environment variables\n                    # https://docs.docker.com/compose/compose-file/12-interpolation/\n                    cmd = cmd.replace(\"$$\", \"$\")\n                    args.extend(shlex.split(cmd))\n\n            # execute as a plain docker cli command\n            self._execute_docker('run', '--rm', *args)\n        else:\n            # execute as a docker-compose command\n            args.append(service_name)\n            if command is not None:\n                args.append(command)\n            self._execute_compose('run', '--rm', *args)\n\n    def push(self, service_name, user=None, password=None):\n        def _push(service):\n            if self.config.using_docker:\n                return self._execute_docker('push', service['image'])\n            else:\n                return self._execute_compose('push', service['name'])\n\n        if user is not None:\n            try:\n                # TODO(kszucs): have an option for a prompt\n                self._execute_docker('login', '-u', user, '-p', password)\n            except subprocess.CalledProcessError:\n                # hide credentials\n                msg = ('Failed to push `{}`, check the passed credentials'\n                       .format(service_name))\n                raise RuntimeError(msg) from None\n\n        service = self.config.get(service_name)\n        for ancestor in service['ancestors']:\n            _push(self.config.get(ancestor))\n        _push(service)\n\n    def images(self):\n        return sorted(self.config.hierarchy.keys())\n\n    def info(self, key_name, filters=None, prefix=' '):\n        output = []\n        for key, value in key_name.items():\n            if hasattr(value, 'items'):\n                temp_filters = filters\n                if key == filters or filters is None:\n                    output.append(f'{prefix} {key}')\n                    # Keep showing this specific key\n                    # as parent matched filter\n                    temp_filters = None\n                output.extend(self.info(value, temp_filters, prefix + \"  \"))\n            else:\n                if key == filters or filters is None:\n                    output.append(\n                        f'{prefix} {key}: ' +\n                        f'{value if value is not None else \"<inherited>\"}'\n                    )\n        return output\n", "dev/archery/archery/docker/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom .core import DockerCompose, UndefinedImage  # noqa\n", "dev/archery/archery/docker/tests/test_docker_cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom unittest.mock import patch\n\nfrom click.testing import CliRunner\n\nfrom archery.docker import DockerCompose\nfrom archery.docker.cli import docker\n\n\n@patch.object(DockerCompose, \"pull\")\n@patch.object(DockerCompose, \"build\")\n@patch.object(DockerCompose, \"run\")\ndef test_docker_run_with_custom_command(run, build, pull):\n    # with custom command\n    args = [\"run\", \"ubuntu-cpp\", \"bash\"]\n    result = CliRunner().invoke(docker, args)\n\n    assert result.exit_code == 0\n    pull.assert_called_once_with(\n        \"ubuntu-cpp\", pull_leaf=True,\n    )\n    build.assert_called_once_with(\n        \"ubuntu-cpp\",\n        use_cache=True,\n        use_leaf_cache=True,\n    )\n    run.assert_called_once_with(\n        \"ubuntu-cpp\",\n        command=\"bash\",\n        env={},\n        resource_limit=None,\n        user=None,\n        volumes=(),\n    )\n\n\n@patch.object(DockerCompose, \"pull\")\n@patch.object(DockerCompose, \"build\")\n@patch.object(DockerCompose, \"run\")\ndef test_docker_run_options(run, build, pull):\n    # environment variables and volumes\n    args = [\n        \"run\",\n        \"-e\",\n        \"ARROW_GANDIVA=OFF\",\n        \"-e\",\n        \"ARROW_FLIGHT=ON\",\n        \"--volume\",\n        \"./build:/build\",\n        \"-v\",\n        \"./ccache:/ccache:delegated\",\n        \"-u\",\n        \"root\",\n        \"ubuntu-cpp\",\n    ]\n    result = CliRunner().invoke(docker, args)\n    assert result.exit_code == 0\n    pull.assert_called_once_with(\n        \"ubuntu-cpp\", pull_leaf=True,\n    )\n    build.assert_called_once_with(\n        \"ubuntu-cpp\",\n        use_cache=True,\n        use_leaf_cache=True,\n    )\n    run.assert_called_once_with(\n        \"ubuntu-cpp\",\n        command=None,\n        env={\"ARROW_GANDIVA\": \"OFF\", \"ARROW_FLIGHT\": \"ON\"},\n        resource_limit=None,\n        user=\"root\",\n        volumes=(\n            \"./build:/build\",\n            \"./ccache:/ccache:delegated\",\n        ),\n    )\n\n\n@patch.object(DockerCompose, \"run\")\ndef test_docker_limit_options(run):\n    # environment variables and volumes\n    args = [\n        \"run\",\n        \"-e\",\n        \"ARROW_GANDIVA=OFF\",\n        \"-e\",\n        \"ARROW_FLIGHT=ON\",\n        \"--volume\",\n        \"./build:/build\",\n        \"-v\",\n        \"./ccache:/ccache:delegated\",\n        \"-u\",\n        \"root\",\n        \"--resource-limit=github\",\n        \"--no-build\",\n        \"--no-pull\",\n        \"ubuntu-cpp\",\n    ]\n    result = CliRunner().invoke(docker, args)\n    assert result.exit_code == 0\n    run.assert_called_once_with(\n        \"ubuntu-cpp\",\n        command=None,\n        env={\"ARROW_GANDIVA\": \"OFF\", \"ARROW_FLIGHT\": \"ON\"},\n        resource_limit=\"github\",\n        user=\"root\",\n        volumes=(\n            \"./build:/build\",\n            \"./ccache:/ccache:delegated\",\n        ),\n    )\n\n\n@patch.object(DockerCompose, \"run\")\ndef test_docker_run_without_pulling_or_building(run):\n    args = [\"run\", \"--no-pull\", \"--no-build\", \"ubuntu-cpp\"]\n    result = CliRunner().invoke(docker, args)\n    assert result.exit_code == 0\n    run.assert_called_once_with(\n        \"ubuntu-cpp\",\n        command=None,\n        env={},\n        resource_limit=None,\n        user=None,\n        volumes=(),\n    )\n\n\n@patch.object(DockerCompose, \"pull\")\n@patch.object(DockerCompose, \"build\")\ndef test_docker_run_only_pulling_and_building(build, pull):\n    args = [\"run\", \"ubuntu-cpp\", \"--build-only\"]\n    result = CliRunner().invoke(docker, args)\n    assert result.exit_code == 0\n    pull.assert_called_once_with(\n        \"ubuntu-cpp\", pull_leaf=True,\n    )\n    build.assert_called_once_with(\n        \"ubuntu-cpp\",\n        use_cache=True,\n        use_leaf_cache=True,\n    )\n\n\n@patch.object(DockerCompose, \"build\")\n@patch.object(DockerCompose, \"run\")\ndef test_docker_run_without_build_cache(run, build):\n    args = [\n        \"run\",\n        \"--no-pull\",\n        \"--force-build\",\n        \"--user\",\n        \"me\",\n        \"--no-cache\",\n        \"--no-leaf-cache\",\n        \"ubuntu-cpp\",\n    ]\n    result = CliRunner().invoke(docker, args)\n    assert result.exit_code == 0\n    build.assert_called_once_with(\n        \"ubuntu-cpp\",\n        use_cache=False,\n        use_leaf_cache=False,\n    )\n    run.assert_called_once_with(\n        \"ubuntu-cpp\",\n        command=None,\n        env={},\n        resource_limit=None,\n        user=\"me\",\n        volumes=(),\n    )\n", "dev/archery/archery/docker/tests/test_docker.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport collections\nimport os\nimport re\nimport subprocess\nfrom unittest import mock\n\nimport pytest\n\nfrom archery.docker import DockerCompose\nfrom archery.testing import assert_subprocess_calls, override_env, PartialEnv\n\n\nmissing_service_compose_yml = \"\"\"\nversion: '3.5'\n\nx-hierarchy:\n  - foo:\n    - sub-foo:\n      - sub-sub-foo\n      - another-sub-sub-foo\n  - bar:\n    - sub-bar\n  - baz\n\nservices:\n  foo:\n    image: org/foo\n  sub-sub-foo:\n    image: org/sub-sub-foo\n  another-sub-sub-foo:\n    image: org/another-sub-sub-foo\n  bar:\n    image: org/bar\n  sub-bar:\n    image: org/sub-bar\n  baz:\n    image: org/baz\n\"\"\"\n\nmissing_node_compose_yml = \"\"\"\nversion: '3.5'\n\nx-hierarchy:\n  - foo:\n    - sub-foo:\n      - sub-sub-foo\n      - another-sub-sub-foo\n  - bar\n  - baz\n\nservices:\n  foo:\n    image: org/foo\n  sub-foo:\n    image: org/sub-foo\n  sub-sub-foo:\n    image: org/sub-foo-foo\n  another-sub-sub-foo:\n    image: org/another-sub-sub-foo\n  bar:\n    image: org/bar\n  sub-bar:\n    image: org/sub-bar\n  baz:\n    image: org/baz\n\"\"\"\n\nok_compose_yml = \"\"\"\nversion: '3.5'\n\nx-hierarchy:\n  - foo:\n    - sub-foo:\n      - sub-sub-foo\n      - another-sub-sub-foo\n  - bar:\n    - sub-bar\n  - baz\n\nservices:\n  foo:\n    image: org/foo\n  sub-foo:\n    image: org/sub-foo\n  sub-sub-foo:\n    image: org/sub-sub-foo\n  another-sub-sub-foo:\n    image: org/another-sub-sub-foo\n  bar:\n    image: org/bar\n  sub-bar:\n    image: org/sub-bar\n  baz:\n    image: org/baz\n\"\"\"\n\narrow_compose_yml = \"\"\"\nversion: '3.5'\n\nx-sccache: &sccache\n  AWS_ACCESS_KEY_ID:\n  AWS_SECRET_ACCESS_KEY:\n  SCCACHE_BUCKET:\n\nx-with-gpus:\n  - ubuntu-cuda\n\nx-hierarchy:\n  - conda-cpp:\n    - conda-python:\n      - conda-python-pandas\n      - conda-python-dask\n  - ubuntu-cpp:\n    - ubuntu-cpp-cmake32\n    - ubuntu-c-glib:\n      - ubuntu-ruby\n  - ubuntu-cuda\n\nx-limit-presets:\n  github:\n    cpuset_cpus: [0, 1]\n    memory: 7g\n\nservices:\n  conda-cpp:\n    image: org/conda-cpp\n    build:\n      context: .\n      dockerfile: ci/docker/conda-cpp.dockerfile\n  conda-python:\n    image: org/conda-python\n    build:\n      context: .\n      dockerfile: ci/docker/conda-cpp.dockerfile\n      args:\n        python: 3.8\n  conda-python-pandas:\n    image: org/conda-python-pandas\n    build:\n      context: .\n      dockerfile: ci/docker/conda-python-pandas.dockerfile\n  conda-python-dask:\n    image: org/conda-python-dask\n  ubuntu-cpp:\n    image: org/ubuntu-cpp\n    build:\n      context: .\n      dockerfile: ci/docker/ubuntu-${UBUNTU}-cpp.dockerfile\n  ubuntu-cpp-cmake32:\n    image: org/ubuntu-cpp-cmake32\n  ubuntu-c-glib:\n    image: org/ubuntu-c-glib\n    environment:\n      <<: [*sccache]\n  ubuntu-ruby:\n    image: org/ubuntu-ruby\n  ubuntu-cuda:\n    image: org/ubuntu-cuda\n    environment:\n      CUDA_ENV: 1\n      OTHER_ENV: 2\n    volumes:\n     - /host:/container\n    command: /bin/bash -c \"echo 1 > /tmp/dummy && cat /tmp/dummy\"\n\"\"\"\n\narrow_compose_env = {\n    'UBUNTU': '20.04',  # overridden below\n    'PYTHON': '3.8',\n    'PANDAS': 'latest',\n    'DASK': 'latest',  # overridden below\n}\n\n\ndef create_config(directory, yml_content, env_content=None):\n    env_path = directory / '.env'\n    config_path = directory / 'docker-compose.yml'\n\n    with config_path.open('w') as fp:\n        fp.write(yml_content)\n\n    if env_content is not None:\n        with env_path.open('w') as fp:\n            for k, v in env_content.items():\n                fp.write(\"{}={}\\n\".format(k, v))\n\n    return config_path\n\n\ndef format_run(args):\n    cmd = [\"run\", \"--rm\"]\n    if isinstance(args, str):\n        return \" \".join(cmd + [args])\n    else:\n        return cmd + args\n\n\n@pytest.fixture\ndef arrow_compose_path(tmpdir):\n    return create_config(tmpdir, arrow_compose_yml, arrow_compose_env)\n\n\ndef test_config_validation(tmpdir):\n    config_path = create_config(tmpdir, missing_service_compose_yml)\n    msg = \"`sub-foo` is defined in `x-hierarchy` bot not in `services`\"\n    with pytest.raises(ValueError, match=msg):\n        DockerCompose(config_path)\n\n    config_path = create_config(tmpdir, missing_node_compose_yml)\n    msg = \"`sub-bar` is defined in `services` but not in `x-hierarchy`\"\n    with pytest.raises(ValueError, match=msg):\n        DockerCompose(config_path)\n\n    config_path = create_config(tmpdir, ok_compose_yml)\n    DockerCompose(config_path)  # no issue\n\n\ndef assert_docker_calls(compose, expected_args):\n    base_command = ['docker']\n    expected_commands = []\n    for args in expected_args:\n        if isinstance(args, str):\n            args = re.split(r\"\\s\", args)\n        expected_commands.append(base_command + args)\n    return assert_subprocess_calls(expected_commands, check=True)\n\n\ndef assert_compose_calls(compose, expected_args, env=mock.ANY):\n    base_command = ['docker-compose', '--file', str(compose.config.path)]\n    expected_commands = []\n    for args in expected_args:\n        if isinstance(args, str):\n            args = re.split(r\"\\s\", args)\n        expected_commands.append(base_command + args)\n    return assert_subprocess_calls(expected_commands, check=True, env=env)\n\n\ndef test_arrow_example_validation_passes(arrow_compose_path):\n    DockerCompose(arrow_compose_path)\n\n\ndef test_compose_default_params_and_env(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path, params=dict(\n        UBUNTU='18.04',\n        DASK='upstream_devel'\n    ))\n    assert compose.config.dotenv == arrow_compose_env\n    assert compose.config.params == {\n        'UBUNTU': '18.04',\n        'DASK': 'upstream_devel',\n    }\n\n\ndef test_forwarding_env_variables(arrow_compose_path):\n    expected_calls = [\n        \"pull --ignore-pull-failures conda-cpp\",\n        \"build conda-cpp\",\n    ]\n    expected_env = PartialEnv(\n        MY_CUSTOM_VAR_A='a',\n        MY_CUSTOM_VAR_B='b'\n    )\n    with override_env({'MY_CUSTOM_VAR_A': 'a', 'MY_CUSTOM_VAR_B': 'b'}):\n        compose = DockerCompose(arrow_compose_path)\n        with assert_compose_calls(compose, expected_calls, env=expected_env):\n            assert os.environ['MY_CUSTOM_VAR_A'] == 'a'\n            assert os.environ['MY_CUSTOM_VAR_B'] == 'b'\n            compose.pull('conda-cpp')\n            compose.build('conda-cpp')\n\n\ndef test_compose_pull(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path)\n\n    expected_calls = [\n        \"pull --ignore-pull-failures conda-cpp\",\n    ]\n    with assert_compose_calls(compose, expected_calls):\n        compose.clear_pull_memory()\n        compose.pull('conda-cpp')\n\n    expected_calls = [\n        \"pull --ignore-pull-failures conda-cpp\",\n        \"pull --ignore-pull-failures conda-python\",\n        \"pull --ignore-pull-failures conda-python-pandas\"\n    ]\n    with assert_compose_calls(compose, expected_calls):\n        compose.clear_pull_memory()\n        compose.pull('conda-python-pandas')\n\n    expected_calls = [\n        \"pull --ignore-pull-failures conda-cpp\",\n        \"pull --ignore-pull-failures conda-python\",\n    ]\n    with assert_compose_calls(compose, expected_calls):\n        compose.clear_pull_memory()\n        compose.pull('conda-python-pandas', pull_leaf=False)\n\n\ndef test_compose_pull_params(arrow_compose_path):\n    expected_calls = [\n        \"pull --ignore-pull-failures conda-cpp\",\n        \"pull --ignore-pull-failures conda-python\",\n    ]\n    compose = DockerCompose(arrow_compose_path, params=dict(UBUNTU='18.04'))\n    expected_env = PartialEnv(PYTHON='3.8', PANDAS='latest')\n    with assert_compose_calls(compose, expected_calls, env=expected_env):\n        compose.clear_pull_memory()\n        compose.pull('conda-python-pandas', pull_leaf=False)\n\n\ndef test_compose_build(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path)\n\n    expected_calls = [\n        \"build conda-cpp\",\n    ]\n    with assert_compose_calls(compose, expected_calls):\n        compose.build('conda-cpp')\n\n    expected_calls = [\n        \"build --no-cache conda-cpp\"\n    ]\n    with assert_compose_calls(compose, expected_calls):\n        compose.build('conda-cpp', use_cache=False)\n\n    expected_calls = [\n        \"build conda-cpp\",\n        \"build conda-python\",\n        \"build conda-python-pandas\"\n    ]\n    with assert_compose_calls(compose, expected_calls):\n        compose.build('conda-python-pandas')\n\n    expected_calls = [\n        \"build --no-cache conda-cpp\",\n        \"build --no-cache conda-python\",\n        \"build --no-cache conda-python-pandas\",\n    ]\n    with assert_compose_calls(compose, expected_calls):\n        compose.build('conda-python-pandas', use_cache=False)\n\n    expected_calls = [\n        \"build conda-cpp\",\n        \"build conda-python\",\n        \"build --no-cache conda-python-pandas\",\n    ]\n    with assert_compose_calls(compose, expected_calls):\n        compose.build('conda-python-pandas', use_cache=True,\n                      use_leaf_cache=False)\n\n\n@mock.patch.dict(os.environ, {\"BUILDKIT_INLINE_CACHE\": \"1\"})\ndef test_compose_buildkit_inline_cache(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path)\n\n    expected_calls = [\n        \"build --build-arg BUILDKIT_INLINE_CACHE=1 conda-cpp\",\n    ]\n    with assert_compose_calls(compose, expected_calls):\n        compose.build('conda-cpp')\n\n\ndef test_compose_build_params(arrow_compose_path):\n    expected_calls = [\n        \"build ubuntu-cpp\",\n    ]\n\n    compose = DockerCompose(arrow_compose_path, params=dict(UBUNTU='18.04'))\n    expected_env = PartialEnv(UBUNTU=\"18.04\")\n    with assert_compose_calls(compose, expected_calls, env=expected_env):\n        compose.build('ubuntu-cpp')\n\n    compose = DockerCompose(arrow_compose_path, params=dict(UBUNTU='16.04'))\n    expected_env = PartialEnv(UBUNTU=\"16.04\")\n    with assert_compose_calls(compose, expected_calls, env=expected_env):\n        compose.build('ubuntu-cpp')\n\n    expected_calls = [\n        \"build --no-cache conda-cpp\",\n        \"build --no-cache conda-python\",\n        \"build --no-cache conda-python-pandas\",\n    ]\n    compose = DockerCompose(arrow_compose_path, params=dict(UBUNTU='18.04'))\n    expected_env = PartialEnv(PYTHON='3.8', PANDAS='latest')\n    with assert_compose_calls(compose, expected_calls, env=expected_env):\n        compose.build('conda-python-pandas', use_cache=False)\n\n\ndef test_compose_run(arrow_compose_path):\n    expected_calls = [\n        format_run(\"conda-cpp\"),\n    ]\n    compose = DockerCompose(arrow_compose_path)\n    with assert_compose_calls(compose, expected_calls):\n        compose.run('conda-cpp')\n\n    expected_calls = [\n        format_run(\"conda-python\")\n    ]\n    expected_env = PartialEnv(PYTHON='3.8')\n    with assert_compose_calls(compose, expected_calls, env=expected_env):\n        compose.run('conda-python')\n\n    compose = DockerCompose(arrow_compose_path, params=dict(PYTHON='3.9'))\n    expected_env = PartialEnv(PYTHON='3.9')\n    with assert_compose_calls(compose, expected_calls, env=expected_env):\n        compose.run('conda-python')\n\n    compose = DockerCompose(arrow_compose_path, params=dict(PYTHON='3.9'))\n    for command in [\"bash\", \"echo 1\"]:\n        expected_calls = [\n            format_run([\"conda-python\", command]),\n        ]\n        expected_env = PartialEnv(PYTHON='3.9')\n        with assert_compose_calls(compose, expected_calls, env=expected_env):\n            compose.run('conda-python', command)\n\n    expected_calls = [\n        (\n            format_run(\"-e CONTAINER_ENV_VAR_A=a -e CONTAINER_ENV_VAR_B=b \"\n                       \"conda-python\")\n        )\n    ]\n    compose = DockerCompose(arrow_compose_path)\n    expected_env = PartialEnv(PYTHON='3.8')\n    with assert_compose_calls(compose, expected_calls, env=expected_env):\n        env = collections.OrderedDict([\n            (\"CONTAINER_ENV_VAR_A\", \"a\"),\n            (\"CONTAINER_ENV_VAR_B\", \"b\")\n        ])\n        compose.run('conda-python', env=env)\n\n    expected_calls = [\n        (\n            format_run(\"--volume /host/build:/build --volume \"\n                       \"/host/ccache:/ccache:delegated conda-python\")\n        )\n    ]\n    compose = DockerCompose(arrow_compose_path)\n    with assert_compose_calls(compose, expected_calls):\n        volumes = (\"/host/build:/build\", \"/host/ccache:/ccache:delegated\")\n        compose.run('conda-python', volumes=volumes)\n\n\ndef test_compose_run_with_resource_limits(arrow_compose_path):\n    expected_calls = [\n        format_run([\n            \"--cpuset-cpus=0,1\",\n            \"--memory=7g\",\n            \"--memory-swap=7g\",\n            \"org/conda-cpp\"\n        ]),\n    ]\n    compose = DockerCompose(arrow_compose_path)\n    with assert_docker_calls(compose, expected_calls):\n        compose.run('conda-cpp', resource_limit=\"github\")\n\n\ndef test_compose_push(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path, params=dict(PYTHON='3.9'))\n    expected_env = PartialEnv(PYTHON=\"3.9\")\n    expected_calls = [\n        mock.call([\"docker\", \"login\", \"-u\", \"user\", \"-p\", \"pass\"], check=True),\n    ]\n    for image in [\"conda-cpp\", \"conda-python\", \"conda-python-pandas\"]:\n        expected_calls.append(\n            mock.call([\"docker-compose\", \"--file\", str(compose.config.path),\n                       \"push\", image], check=True, env=expected_env)\n        )\n    with assert_subprocess_calls(expected_calls):\n        compose.push('conda-python-pandas', user='user', password='pass')\n\n\ndef test_compose_error(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path, params=dict(\n        PYTHON='3.8',\n        PANDAS='upstream_devel'\n    ))\n\n    error = subprocess.CalledProcessError(99, [])\n    with mock.patch('subprocess.run', side_effect=error):\n        with pytest.raises(RuntimeError) as exc:\n            compose.run('conda-cpp')\n\n    exception_message = str(exc.value)\n    assert \"exited with a non-zero exit code 99\" in exception_message\n    assert \"PANDAS: latest\" in exception_message\n    assert \"export PANDAS=upstream_devel\" in exception_message\n\n\ndef test_image_with_gpu(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path)\n\n    expected_calls = [\n        [\n            \"run\", \"--rm\", \"--gpus\", \"all\",\n            \"-e\", \"CUDA_ENV=1\",\n            \"-e\", \"OTHER_ENV=2\",\n            \"-v\", \"/host:/container:rw\",\n            \"org/ubuntu-cuda\",\n            \"/bin/bash\", \"-c\", \"echo 1 > /tmp/dummy && cat /tmp/dummy\",\n        ]\n    ]\n    with assert_docker_calls(compose, expected_calls):\n        compose.run('ubuntu-cuda')\n\n\ndef test_listing_images(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path)\n    assert sorted(compose.images()) == [\n        'conda-cpp',\n        'conda-python',\n        'conda-python-dask',\n        'conda-python-pandas',\n        'ubuntu-c-glib',\n        'ubuntu-cpp',\n        'ubuntu-cpp-cmake32',\n        'ubuntu-cuda',\n        'ubuntu-ruby',\n    ]\n\n\ndef test_service_info(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path)\n    service = compose.config.raw_config[\"services\"][\"conda-cpp\"]\n    assert compose.info(service) == [\n        \"  image: org/conda-cpp\",\n        \"  build\",\n        \"    context: .\",\n        \"    dockerfile: ci/docker/conda-cpp.dockerfile\"\n    ]\n\n\ndef test_service_info_filters(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path)\n    service = compose.config.raw_config[\"services\"][\"conda-cpp\"]\n    assert compose.info(service, filters=\"dockerfile\") == [\n        \"    dockerfile: ci/docker/conda-cpp.dockerfile\"\n    ]\n\n\ndef test_service_info_non_existing_filters(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path)\n    service = compose.config.raw_config[\"services\"][\"conda-cpp\"]\n    assert compose.info(service, filters=\"non-existing\") == []\n\n\ndef test_service_info_inherited_env(arrow_compose_path):\n    compose = DockerCompose(arrow_compose_path)\n    service = compose.config.raw_config[\"services\"][\"ubuntu-c-glib\"]\n    assert compose.info(service, filters=\"environment\") == [\n        \"  environment\",\n        \"    AWS_ACCESS_KEY_ID: <inherited>\",\n        \"    AWS_SECRET_ACCESS_KEY: <inherited>\",\n        \"    SCCACHE_BUCKET: <inherited>\"\n    ]\n", "dev/archery/archery/tests/test_bot.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport json\nimport os\nfrom unittest import mock\nfrom unittest.mock import Mock\n\nimport click\nimport pytest\nimport responses as rsps\n\nfrom archery.bot import (\n    CommentBot,\n    CommandError,\n    PullRequestState,\n    PullRequestWorkflowBot,\n    group\n)\n\n\n@pytest.fixture\ndef responses():\n    with rsps.RequestsMock() as mock:\n        yield mock\n\n\n@pytest.fixture(autouse=True)\ndef set_env_vars():\n    with mock.patch.dict(os.environ, {\n        \"GITHUB_SERVER_URL\": \"https://github.com\",\n        \"GITHUB_REPOSITORY\": \"apache/arrow\",\n        \"GITHUB_RUN_ID\": \"1463784188\"\n    }):\n        yield\n\n\ndef github_url(path):\n    return 'https://api.github.com:443/{}'.format(path.strip('/'))\n\n\n@group()\ndef custom_handler():\n    pass\n\n\n@custom_handler.command()\n@click.pass_obj\ndef extra(obj):\n    return obj\n\n\n@custom_handler.command()\n@click.option('--force', '-f', is_flag=True)\ndef build(force):\n    return force\n\n\n@custom_handler.command()\n@click.option('--name', required=True)\ndef benchmark(name):\n    return name\n\n\ndef test_click_based_commands():\n    assert custom_handler('build') is False\n    assert custom_handler('build -f') is True\n\n    assert custom_handler('benchmark --name strings') == 'strings'\n    with pytest.raises(CommandError):\n        assert custom_handler('benchmark')\n\n    assert custom_handler('extra', extra='data') == {'extra': 'data'}\n\n\n@pytest.mark.parametrize('fixture_name', [\n    # the bot is not mentioned, nothing to do\n    'event-issue-comment-not-mentioning-ursabot.json',\n    # don't respond to itself, it prevents recursive comment storms!\n    'event-issue-comment-by-ursabot.json',\n])\ndef test_noop_events(load_fixture, fixture_name):\n    payload = load_fixture(fixture_name)\n\n    handler = Mock()\n    bot = CommentBot(name='ursabot', handler=handler)\n    bot.handle('issue_comment', payload)\n\n    handler.assert_not_called()\n\n\ndef test_unauthorized_user_comment(load_fixture, responses):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/issues/26'),\n        json=load_fixture('issue-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/pulls/26'),\n        json=load_fixture('pull-request-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/comments/480243815'),\n        json=load_fixture('pull-request-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.POST,\n        github_url('/repos/ursa-labs/ursabot/issues/26/comments'),\n        json={}\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/pulls/26/reactions'),\n        json=()\n    )\n\n    def handler(command, **kwargs):\n        pass\n\n    payload = load_fixture('event-issue-comment-by-non-authorized-user.json')\n    payload[\"comment\"][\"body\"] = '@ursabot crossbow submit -g nightly'\n    bot = CommentBot(name='ursabot', handler=handler)\n    bot.handle('issue_comment', payload)\n\n    print([c.request.body for c in responses.calls])\n    post = responses.calls[-2]\n    reaction = responses.calls[-1]\n    comment = (\"```\\nOnly contributors can submit requests to this bot. \"\n               \"Please ask someone from the community for help with getting \"\n               \"the first commit in.\\n\"\n               \"The Archery job run can be found at: \"\n               \"https://github.com/apache/arrow/actions/runs/1463784188\\n\"\n               \"```\")\n    assert json.loads(post.request.body) == {\n        \"body\": f'{comment}'}\n    assert json.loads(reaction.request.body) == {'content': '-1'}\n\n\ndef test_issue_comment_without_pull_request(load_fixture, responses):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/issues/19'),\n        json=load_fixture('issue-19.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('repos/ursa-labs/ursabot/pulls/19'),\n        json={},\n        status=404\n    )\n    responses.add(\n        responses.POST,\n        github_url('/repos/ursa-labs/ursabot/issues/19/comments'),\n        json={}\n    )\n\n    def handler(command, **kwargs):\n        pass\n\n    payload = load_fixture('event-issue-comment-without-pull-request.json')\n    bot = CommentBot(name='ursabot', handler=handler)\n    bot.handle('issue_comment', payload)\n\n    post = responses.calls[2]\n    assert json.loads(post.request.body) == {\n        'body': \"The comment bot only listens to pull request comments!\"\n    }\n\n\ndef test_respond_with_usage(load_fixture, responses):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/issues/26'),\n        json=load_fixture('issue-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/pulls/26'),\n        json=load_fixture('pull-request-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/comments/480243811'),\n        json=load_fixture('issue-comment-480243811.json')\n    )\n    responses.add(\n        responses.POST,\n        github_url('/repos/ursa-labs/ursabot/issues/26/comments'),\n        json={}\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/issues/comments/479081273/reactions'),\n        json=()\n    )\n\n    def handler(command, **kwargs):\n        raise CommandError('test-usage')\n\n    payload = load_fixture('event-issue-comment-with-empty-command.json')\n    bot = CommentBot(name='ursabot', handler=handler)\n    bot.handle('issue_comment', payload)\n\n    post = responses.calls[3]\n    assert json.loads(post.request.body) == \\\n        {'body':\n         (\"```\\ntest-usage\\n\"\n          \"The Archery job run can be found at: \"\n          \"https://github.com/apache/arrow/actions/runs/1463784188\\n\"\n          \"```\")\n         }\n\n\n@pytest.mark.parametrize(('command', 'reaction'), [\n    ('@ursabot build', '+1'),\n    ('@ursabot build\\nwith a comment', '+1'),\n])\ndef test_issue_comment_with_commands(load_fixture, responses, command,\n                                     reaction):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/issues/26'),\n        json=load_fixture('issue-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/pulls/26'),\n        json=load_fixture('pull-request-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/comments/480248726'),\n        json=load_fixture('issue-comment-480248726.json')\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/issues/comments/480248726/reactions'\n        ),\n        json={}\n    )\n\n    def handler(command, **kwargs):\n        if command == 'build':\n            return True\n        else:\n            raise ValueError('Only `build` command is supported.')\n\n    payload = load_fixture('event-issue-comment-build-command.json')\n    payload[\"comment\"][\"body\"] = command\n\n    bot = CommentBot(name='ursabot', handler=handler)\n    bot.handle('issue_comment', payload)\n\n    post = responses.calls[3]\n    assert json.loads(post.request.body) == {'content': reaction}\n\n\n@pytest.mark.parametrize(('command', 'reaction'), [\n    ('@ursabot listen', '-1'),\n])\ndef test_issue_comment_invalid_commands(load_fixture, responses, command,\n                                        reaction):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/issues/26'),\n        json=load_fixture('issue-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/pulls/26'),\n        json=load_fixture('pull-request-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/comments/480248726'),\n        json=load_fixture('issue-comment-480248726.json')\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/issues/comments/480248726/reactions'\n        ),\n        json={}\n    )\n    responses.add(\n        responses.POST,\n        github_url('/repos/ursa-labs/ursabot/issues/26/comments'),\n        json={}\n    )\n\n    def handler(command, **kwargs):\n        if command == 'build':\n            return True\n        else:\n            raise ValueError('Only `build` command is supported.')\n\n    payload = load_fixture('event-issue-comment-build-command.json')\n    payload[\"comment\"][\"body\"] = command\n\n    bot = CommentBot(name='ursabot', handler=handler)\n    bot.handle('issue_comment', payload)\n\n    # Setting reaction is always the last call\n    post = responses.calls[-1]\n    assert json.loads(post.request.body) == {'content': reaction}\n\n\ndef test_issue_comment_with_commands_bot_not_first(load_fixture, responses):\n    # when the @-mention is not first, this is a no-op\n    handler = Mock()\n\n    payload = load_fixture('event-issue-comment-build-command.json')\n    payload[\"comment\"][\"body\"] = 'with a comment\\n@ursabot build'\n\n    bot = CommentBot(name='ursabot', handler=handler)\n    bot.handle('issue_comment', payload)\n\n    handler.assert_not_called()\n\n\n@pytest.mark.parametrize(('fixture_name', 'expected_label'), [\n    ('event-pull-request-target-opened-committer.json',\n     PullRequestState.committer_review.value),\n    ('event-pull-request-target-opened-non-committer.json',\n     PullRequestState.review.value),\n])\ndef test_open_pull_request(load_fixture, responses, fixture_name, expected_label):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/pulls/26'),\n        json=load_fixture('pull-request-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels'),\n        json=[],\n        status=200\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/issues/26/labels'\n        ),\n        status=201\n    )\n    payload = load_fixture(fixture_name)\n\n    bot = PullRequestWorkflowBot('pull_request_target', payload)\n    bot.handle()\n\n    # Setting awaiting committer review or awaiting review label\n    post = responses.calls[-1]\n    assert json.loads(post.request.body) == [expected_label]\n\n\n@pytest.mark.parametrize(('fixture_name', 'expected_label'), [\n    ('event-pull-request-target-opened-non-committer.json',\n     PullRequestState.committer_review.value),\n])\ndef test_open_pull_request_with_committer_list(load_fixture, responses, fixture_name,\n                                               expected_label):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/pulls/26'),\n        json=load_fixture('pull-request-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels'),\n        json=[],\n        status=200\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/issues/26/labels'\n        ),\n        status=201\n    )\n    payload = load_fixture(fixture_name)\n\n    # Even though the author_association is not committer the list overrides.\n    bot = PullRequestWorkflowBot(\n        'pull_request_target', payload, committers=['kszucs'])\n    bot.handle()\n\n    # Setting awaiting committer review or awaiting review label\n    post = responses.calls[-1]\n    assert json.loads(post.request.body) == [expected_label]\n\n\n@pytest.mark.parametrize(('fixture_name', 'expected_label'), [\n    ('event-pull-request-target-opened-committer.json',\n     PullRequestState.committer_review.value),\n])\ndef test_open_pull_request_with_existing_label(\n        load_fixture, responses, fixture_name, expected_label):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/pulls/26'),\n        json=load_fixture('pull-request-26-awaiting-review.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels'),\n        json=load_fixture('label-awaiting-review.json'),\n        status=200\n    )\n    responses.add(\n        responses.DELETE,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels/awaiting%20review'),\n        status=200\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/issues/26/labels'\n        ),\n        status=201\n    )\n    payload = load_fixture(fixture_name)\n    payload['pull_request']['labels'] = ['awaiting review']\n\n    bot = PullRequestWorkflowBot('pull_request_target', payload)\n    bot.handle()\n\n    post = responses.calls[-1]\n    assert json.loads(post.request.body) == [expected_label]\n\n\n@pytest.mark.parametrize(('fixture_name', 'review_state', 'expected_label'), [\n    ('event-pr-review-committer.json', 'commented', PullRequestState.changes.value),\n    ('event-pr-review-committer.json', 'changes_requested',\n     PullRequestState.changes.value),\n    ('event-pr-review-committer.json', 'approved', PullRequestState.merge.value),\n    ('event-pr-review-non-committer.json', 'commented',\n     PullRequestState.committer_review.value),\n    ('event-pr-review-non-committer.json', 'changes_requested',\n     PullRequestState.committer_review.value),\n    ('event-pr-review-non-committer.json', 'approved',\n     PullRequestState.committer_review.value),\n])\ndef test_pull_request_review_awaiting_review(\n        load_fixture, responses, fixture_name, review_state, expected_label):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/pulls/26'),\n        json=load_fixture('pull-request-26-awaiting-review.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels'),\n        json=load_fixture('label-awaiting-review.json'),\n        status=200\n    )\n    responses.add(\n        responses.DELETE,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels/awaiting%20review'),\n        status=200\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/issues/26/labels'\n        ),\n        status=201\n    )\n    payload = load_fixture(fixture_name)\n    payload['pull_request']['labels'] = ['awaiting review']\n    payload['review']['state'] = review_state\n\n    bot = PullRequestWorkflowBot('pull_request_review', payload)\n    bot.handle()\n\n    post = responses.calls[-1]\n    assert json.loads(post.request.body) == [expected_label]\n\n\n@pytest.mark.parametrize(('review_state', 'expected_label'), [\n    ('commented', PullRequestState.changes.value),\n    ('changes_requested', PullRequestState.changes.value),\n    ('approved', PullRequestState.merge.value),\n])\ndef test_pull_request_committer_review_awaiting_change_review(\n        load_fixture, responses, review_state, expected_label):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/pulls/26'),\n        json=load_fixture('pull-request-26-awaiting-review.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels'),\n        json=load_fixture('label-awaiting-change-review.json'),\n        status=200\n    )\n    responses.add(\n        responses.DELETE,\n        github_url('/repos/ursa-labs/ursabot/issues/26/' +\n                   'labels/awaiting%20change%20review'),\n        status=200\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/issues/26/labels'\n        ),\n        status=201\n    )\n    payload = load_fixture('event-pr-review-committer.json')\n    payload['pull_request']['labels'] = ['awaiting change review']\n    payload['review']['state'] = review_state\n\n    bot = PullRequestWorkflowBot('pull_request_review', payload)\n    bot.handle()\n\n    post = responses.calls[-1]\n    assert json.loads(post.request.body) == [expected_label]\n\n\n@pytest.mark.parametrize('review_state', [\n    'commented', 'changes_requested', 'approved'])\ndef test_pull_request_non_committer_review_awaiting_change_review(\n        load_fixture, responses, review_state):\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/pulls/26'),\n        json=load_fixture('pull-request-26-awaiting-review.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels'),\n        json=load_fixture('label-awaiting-change-review.json'),\n        status=200\n    )\n    payload = load_fixture('event-pr-review-non-committer.json')\n    payload['pull_request']['labels'] = ['awaiting change review']\n    payload['review']['state'] = review_state\n\n    bot = PullRequestWorkflowBot('pull_request_review', payload)\n    bot.handle()\n\n    # No requests to delete post new labels on non-committer reviews\n    assert len(responses.calls) == 2\n\n\ndef test_pull_request_synchronize_event_on_awaiting_changes(\n        load_fixture, responses):\n    payload = load_fixture('event-pull-request-target-synchronize.json')\n\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/pulls/26'),\n        json=load_fixture('pull-request-26-awaiting-review.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels'),\n        json=load_fixture('label-awaiting-changes.json'),\n        status=200\n    )\n    responses.add(\n        responses.DELETE,\n        github_url('/repos/ursa-labs/ursabot/issues/26/' +\n                   'labels/awaiting%20changes'),\n        status=200\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/issues/26/labels'\n        ),\n        status=201\n    )\n\n    bot = PullRequestWorkflowBot('pull_request_target', payload)\n    bot.handle()\n    # after push event label changes.\n    post = responses.calls[-1]\n    assert json.loads(post.request.body) == [\"awaiting change review\"]\n\n\ndef test_pull_request_synchronize_event_on_awaiting_review(\n        load_fixture, responses):\n    payload = load_fixture('event-pull-request-target-synchronize.json')\n\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/pulls/26'),\n        json=load_fixture('pull-request-26-awaiting-review.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels'),\n        json=load_fixture('label-awaiting-review.json'),\n        status=200\n    )\n\n    bot = PullRequestWorkflowBot('pull_request_target', payload)\n    bot.handle()\n    # No requests to delete or post new labels on push awaiting review\n    assert len(responses.calls) == 2\n\n\ndef test_pull_request_synchronize_event_on_existing_pr_without_state(\n        load_fixture, responses):\n    payload = load_fixture('event-pull-request-target-synchronize.json')\n\n    responses.add(\n        responses.GET,\n        github_url('/repositories/169101701/pulls/26'),\n        json=load_fixture('pull-request-26.json'),\n        status=200\n    )\n    responses.add(\n        responses.GET,\n        github_url('/repos/ursa-labs/ursabot/issues/26/labels'),\n        json=[],\n        status=200\n    )\n    responses.add(\n        responses.POST,\n        github_url(\n            '/repos/ursa-labs/ursabot/issues/26/labels'\n        ),\n        status=201\n    )\n\n    bot = PullRequestWorkflowBot('pull_request_target', payload)\n    bot.handle()\n    # after push event label get set to default\n    post = responses.calls[-1]\n    assert json.loads(post.request.body) == [\"awaiting review\"]\n", "dev/archery/archery/tests/test_benchmarks.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport json\n\nfrom archery.benchmark.codec import JsonEncoder\nfrom archery.benchmark.core import Benchmark, median\nfrom archery.benchmark.compare import (\n    BenchmarkComparator, RunnerComparator\n)\nfrom archery.benchmark.google import (\n    GoogleBenchmark, GoogleBenchmarkObservation\n)\nfrom archery.benchmark.runner import StaticBenchmarkRunner\n\n\ndef test_benchmark_comparator():\n    unit = \"micros\"\n\n    assert not BenchmarkComparator(\n        Benchmark(\"contender\", unit, True, [10], unit, [1]),\n        Benchmark(\"baseline\", unit, True, [20], unit, [1]),\n    ).regression\n\n    assert BenchmarkComparator(\n        Benchmark(\"contender\", unit, False, [10], unit, [1]),\n        Benchmark(\"baseline\", unit, False, [20], unit, [1]),\n    ).regression\n\n    assert BenchmarkComparator(\n        Benchmark(\"contender\", unit, True, [20], unit, [1]),\n        Benchmark(\"baseline\", unit, True, [10], unit, [1]),\n    ).regression\n\n    assert not BenchmarkComparator(\n        Benchmark(\"contender\", unit, False, [20], unit, [1]),\n        Benchmark(\"baseline\", unit, False, [10], unit, [1]),\n    ).regression\n\n\ndef test_static_runner_from_json_not_a_regression():\n    archery_result = {\n        \"suites\": [\n            {\n                \"name\": \"arrow-value-parsing-benchmark\",\n                \"benchmarks\": [\n                    {\n                        \"name\": \"FloatParsing<DoubleType>\",\n                        \"unit\": \"items_per_second\",\n                        \"less_is_better\": False,\n                        \"values\": [\n                            109941112.87296811\n                        ],\n                        \"time_unit\": \"ns\",\n                        \"times\": [\n                            9095.800104330105\n                        ]\n                    },\n                ]\n            }\n        ]\n    }\n\n    contender = StaticBenchmarkRunner.from_json(json.dumps(archery_result))\n    baseline = StaticBenchmarkRunner.from_json(json.dumps(archery_result))\n    [comparison] = RunnerComparator(contender, baseline).comparisons\n    assert not comparison.regression\n\n\ndef test_static_runner_from_json_multiple_values_not_a_regression():\n    # Same as above, but with multiple repetitions\n    archery_result = {\n        \"suites\": [\n            {\n                \"name\": \"arrow-value-parsing-benchmark\",\n                \"benchmarks\": [\n                    {\n                        \"name\": \"FloatParsing<DoubleType>\",\n                        \"unit\": \"items_per_second\",\n                        \"less_is_better\": False,\n                        \"values\": [\n                            93588476.22327498,\n                            94873831.3818328,\n                            95593675.20810866,\n                            95797325.6543961,\n                            96134728.05794072\n                        ],\n                        \"time_unit\": \"ns\",\n                        \"times\": [\n                            10537.724568456104,\n                            10575.162068480413,\n                            10599.271208720838,\n                            10679.028059166194,\n                            10827.995119861762\n                        ],\n                        \"counters\": {\n                            \"family_index\": 0,\n                            \"per_family_instance_index\": 0,\n                            \"run_name\": \"FloatParsing<DoubleType>\",\n                            \"repetitions\": 5,\n                            \"repetition_index\": 0,\n                            \"threads\": 1,\n                            \"iterations\": 10656\n                        }\n                    }\n                ]\n            }\n        ]\n    }\n\n    contender = StaticBenchmarkRunner.from_json(json.dumps(archery_result))\n    baseline = StaticBenchmarkRunner.from_json(json.dumps(archery_result))\n    [comparison] = RunnerComparator(contender, baseline).comparisons\n    assert not comparison.regression\n\n\ndef test_static_runner_from_json_regression():\n    archery_result = {\n        \"suites\": [\n            {\n                \"name\": \"arrow-value-parsing-benchmark\",\n                \"benchmarks\": [\n                    {\n                        \"name\": \"FloatParsing<DoubleType>\",\n                        \"unit\": \"items_per_second\",\n                        \"less_is_better\": False,\n                        \"values\": [\n                            109941112.87296811\n                        ],\n                        \"time_unit\": \"ns\",\n                        \"times\": [\n                            9095.800104330105\n                        ]\n                    },\n                ]\n            }\n        ]\n    }\n\n    contender = StaticBenchmarkRunner.from_json(json.dumps(archery_result))\n\n    # introduce artificial regression\n    archery_result['suites'][0]['benchmarks'][0]['values'][0] *= 2\n    baseline = StaticBenchmarkRunner.from_json(json.dumps(archery_result))\n\n    [comparison] = RunnerComparator(contender, baseline).comparisons\n    assert comparison.regression\n\n\ndef test_static_runner_from_json_multiple_values_regression():\n    # Same as above, but with multiple repetitions\n    archery_result = {\n        \"suites\": [\n            {\n                \"name\": \"arrow-value-parsing-benchmark\",\n                \"benchmarks\": [\n                    {\n                        \"name\": \"FloatParsing<DoubleType>\",\n                        \"unit\": \"items_per_second\",\n                        \"less_is_better\": False,\n                        \"values\": [\n                            93588476.22327498,\n                            94873831.3818328,\n                            95593675.20810866,\n                            95797325.6543961,\n                            96134728.05794072\n                        ],\n                        \"time_unit\": \"ns\",\n                        \"times\": [\n                            10537.724568456104,\n                            10575.162068480413,\n                            10599.271208720838,\n                            10679.028059166194,\n                            10827.995119861762\n                        ],\n                        \"counters\": {\n                            \"family_index\": 0,\n                            \"per_family_instance_index\": 0,\n                            \"run_name\": \"FloatParsing<DoubleType>\",\n                            \"repetitions\": 5,\n                            \"repetition_index\": 0,\n                            \"threads\": 1,\n                            \"iterations\": 10656\n                        }\n                    }\n                ]\n            }\n        ]\n    }\n\n    contender = StaticBenchmarkRunner.from_json(json.dumps(archery_result))\n\n    # introduce artificial regression\n    values = archery_result['suites'][0]['benchmarks'][0]['values']\n    values[:] = [v * 2 for v in values]\n    baseline = StaticBenchmarkRunner.from_json(json.dumps(archery_result))\n\n    [comparison] = RunnerComparator(contender, baseline).comparisons\n    assert comparison.regression\n\n\ndef test_benchmark_median():\n    assert median([10]) == 10\n    assert median([1, 2, 3]) == 2\n    assert median([1, 2]) == 1.5\n    assert median([1, 2, 3, 4]) == 2.5\n    assert median([1, 1, 1, 1]) == 1\n    try:\n        median([])\n        assert False\n    except ValueError:\n        pass\n\n\ndef assert_benchmark(name, google_result, archery_result):\n    observation = GoogleBenchmarkObservation(**google_result)\n    benchmark = GoogleBenchmark(name, [observation])\n    result = json.dumps(benchmark, cls=JsonEncoder)\n    assert json.loads(result) == archery_result\n\n\ndef test_items_per_second():\n    name = \"ArrayArrayKernel<AddChecked, UInt8Type>/32768/0\"\n    google_result = {\n        \"cpu_time\": 116292.58886653671,\n        \"items_per_second\": 281772039.9844759,\n        \"iterations\": 5964,\n        \"name\": name,\n        \"null_percent\": 0.0,\n        \"real_time\": 119811.77313729875,\n        \"repetition_index\": 0,\n        \"repetitions\": 0,\n        \"run_name\": name,\n        \"run_type\": \"iteration\",\n        \"size\": 32768.0,\n        \"threads\": 1,\n        \"time_unit\": \"ns\",\n    }\n    archery_result = {\n        \"counters\": {\"iterations\": 5964,\n                     \"null_percent\": 0.0,\n                     \"repetition_index\": 0,\n                     \"repetitions\": 0,\n                     \"run_name\": name,\n                     \"threads\": 1},\n        \"name\": name,\n        \"unit\": \"items_per_second\",\n        \"less_is_better\": False,\n        \"values\": [281772039.9844759],\n        \"time_unit\": \"ns\",\n        \"times\": [119811.77313729875],\n    }\n    assert \"items_per_second\" in google_result\n    assert \"bytes_per_second\" not in google_result\n    assert_benchmark(name, google_result, archery_result)\n\n\ndef test_bytes_per_second():\n    name = \"BufferOutputStreamLargeWrites/real_time\"\n    google_result = {\n        \"bytes_per_second\": 1890209037.3405428,\n        \"cpu_time\": 17018127.659574457,\n        \"iterations\": 47,\n        \"name\": name,\n        \"real_time\": 17458386.53190963,\n        \"repetition_index\": 1,\n        \"repetitions\": 0,\n        \"run_name\": name,\n        \"run_type\": \"iteration\",\n        \"threads\": 1,\n        \"time_unit\": \"ns\",\n    }\n    archery_result = {\n        \"counters\": {\"iterations\": 47,\n                     \"repetition_index\": 1,\n                     \"repetitions\": 0,\n                     \"run_name\": name,\n                     \"threads\": 1},\n        \"name\": name,\n        \"unit\": \"bytes_per_second\",\n        \"less_is_better\": False,\n        \"values\": [1890209037.3405428],\n        \"time_unit\": \"ns\",\n        \"times\": [17458386.53190963],\n    }\n    assert \"items_per_second\" not in google_result\n    assert \"bytes_per_second\" in google_result\n    assert_benchmark(name, google_result, archery_result)\n\n\ndef test_both_items_and_bytes_per_second():\n    name = \"ArrayArrayKernel<AddChecked, UInt8Type>/32768/0\"\n    google_result = {\n        \"bytes_per_second\": 281772039.9844759,\n        \"cpu_time\": 116292.58886653671,\n        \"items_per_second\": 281772039.9844759,\n        \"iterations\": 5964,\n        \"name\": name,\n        \"null_percent\": 0.0,\n        \"real_time\": 119811.77313729875,\n        \"repetition_index\": 0,\n        \"repetitions\": 0,\n        \"run_name\": name,\n        \"run_type\": \"iteration\",\n        \"size\": 32768.0,\n        \"threads\": 1,\n        \"time_unit\": \"ns\",\n    }\n    # Note that bytes_per_second trumps items_per_second\n    archery_result = {\n        \"counters\": {\"iterations\": 5964,\n                     \"null_percent\": 0.0,\n                     \"repetition_index\": 0,\n                     \"repetitions\": 0,\n                     \"run_name\": name,\n                     \"threads\": 1},\n        \"name\": name,\n        \"unit\": \"bytes_per_second\",\n        \"less_is_better\": False,\n        \"values\": [281772039.9844759],\n        \"time_unit\": \"ns\",\n        \"times\": [119811.77313729875],\n    }\n    assert \"items_per_second\" in google_result\n    assert \"bytes_per_second\" in google_result\n    assert_benchmark(name, google_result, archery_result)\n\n\ndef test_neither_items_nor_bytes_per_second():\n    name = \"AllocateDeallocate<Jemalloc>/size:1048576/real_time\"\n    google_result = {\n        \"cpu_time\": 1778.6004847419827,\n        \"iterations\": 352765,\n        \"name\": name,\n        \"real_time\": 1835.3137357788837,\n        \"repetition_index\": 0,\n        \"repetitions\": 0,\n        \"run_name\": name,\n        \"run_type\": \"iteration\",\n        \"threads\": 1,\n        \"time_unit\": \"ns\",\n    }\n    archery_result = {\n        \"counters\": {\"iterations\": 352765,\n                     \"repetition_index\": 0,\n                     \"repetitions\": 0,\n                     \"run_name\": name,\n                     \"threads\": 1},\n        \"name\": name,\n        \"unit\": \"ns\",\n        \"less_is_better\": True,\n        \"values\": [1835.3137357788837],\n        \"time_unit\": \"ns\",\n        \"times\": [1835.3137357788837],\n    }\n    assert \"items_per_second\" not in google_result\n    assert \"bytes_per_second\" not in google_result\n    assert_benchmark(name, google_result, archery_result)\n\n\ndef test_prefer_real_time():\n    name = \"AllocateDeallocate<Jemalloc>/size:1048576/real_time\"\n    google_result = {\n        \"cpu_time\": 1778.6004847419827,\n        \"iterations\": 352765,\n        \"name\": name,\n        \"real_time\": 1835.3137357788837,\n        \"repetition_index\": 0,\n        \"repetitions\": 0,\n        \"run_name\": name,\n        \"run_type\": \"iteration\",\n        \"threads\": 1,\n        \"time_unit\": \"ns\",\n    }\n    archery_result = {\n        \"counters\": {\"iterations\": 352765,\n                     \"repetition_index\": 0,\n                     \"repetitions\": 0,\n                     \"run_name\": name,\n                     \"threads\": 1},\n        \"name\": name,\n        \"unit\": \"ns\",\n        \"less_is_better\": True,\n        \"values\": [1835.3137357788837],\n        \"time_unit\": \"ns\",\n        \"times\": [1835.3137357788837],\n    }\n    assert name.endswith(\"/real_time\")\n    assert_benchmark(name, google_result, archery_result)\n\n\ndef test_prefer_cpu_time():\n    name = \"AllocateDeallocate<Jemalloc>/size:1048576\"\n    google_result = {\n        \"cpu_time\": 1778.6004847419827,\n        \"iterations\": 352765,\n        \"name\": name,\n        \"real_time\": 1835.3137357788837,\n        \"repetition_index\": 0,\n        \"repetitions\": 0,\n        \"run_name\": name,\n        \"run_type\": \"iteration\",\n        \"threads\": 1,\n        \"time_unit\": \"ns\",\n    }\n    archery_result = {\n        \"counters\": {\"iterations\": 352765,\n                     \"repetition_index\": 0,\n                     \"repetitions\": 0,\n                     \"run_name\": name,\n                     \"threads\": 1},\n        \"name\": name,\n        \"unit\": \"ns\",\n        \"less_is_better\": True,\n        \"values\": [1778.6004847419827],\n        \"time_unit\": \"ns\",\n        \"times\": [1835.3137357788837],\n    }\n    assert not name.endswith(\"/real_time\")\n    assert_benchmark(name, google_result, archery_result)\n\n\ndef test_omits_aggregates():\n    name = \"AllocateDeallocate<Jemalloc>/size:1048576/real_time\"\n    google_aggregate = {\n        \"aggregate_name\": \"mean\",\n        \"cpu_time\": 1757.428694267678,\n        \"iterations\": 3,\n        \"name\": \"AllocateDeallocate<Jemalloc>/size:1048576/real_time_mean\",\n        \"real_time\": 1849.3869337041162,\n        \"repetitions\": 0,\n        \"run_name\": name,\n        \"run_type\": \"aggregate\",\n        \"threads\": 1,\n        \"time_unit\": \"ns\",\n    }\n    google_result = {\n        \"cpu_time\": 1778.6004847419827,\n        \"iterations\": 352765,\n        \"name\": name,\n        \"real_time\": 1835.3137357788837,\n        \"repetition_index\": 0,\n        \"repetitions\": 0,\n        \"run_name\": name,\n        \"run_type\": \"iteration\",\n        \"threads\": 1,\n        \"time_unit\": \"ns\",\n    }\n    archery_result = {\n        \"counters\": {\"iterations\": 352765,\n                     \"repetition_index\": 0,\n                     \"repetitions\": 0,\n                     \"run_name\": name,\n                     \"threads\": 1},\n        \"name\": name,\n        \"unit\": \"ns\",\n        \"less_is_better\": True,\n        \"values\": [1835.3137357788837],\n        \"time_unit\": \"ns\",\n        \"times\": [1835.3137357788837],\n    }\n    assert google_aggregate[\"run_type\"] == \"aggregate\"\n    assert google_result[\"run_type\"] == \"iteration\"\n    observation1 = GoogleBenchmarkObservation(**google_aggregate)\n    observation2 = GoogleBenchmarkObservation(**google_result)\n    benchmark = GoogleBenchmark(name, [observation1, observation2])\n    result = json.dumps(benchmark, cls=JsonEncoder)\n    assert json.loads(result) == archery_result\n\n\ndef test_multiple_observations():\n    name = \"FloatParsing<DoubleType>\"\n    google_results = [\n        {\n            'cpu_time': 10627.38199641615,\n            'family_index': 0,\n            'items_per_second': 94096551.75067839,\n            'iterations': 9487,\n            'name': 'FloatParsing<DoubleType>',\n            'per_family_instance_index': 0,\n            'real_time': 10628.84905663701,\n            'repetition_index': 0,\n            'repetitions': 3,\n            'run_name': 'FloatParsing<DoubleType>',\n            'run_type': 'iteration',\n            'threads': 1,\n            'time_unit': 'ns'\n        },\n        {\n            'cpu_time': 10633.318014124594,\n            'family_index': 0,\n            'items_per_second': 94044022.63448404,\n            'iterations': 9487,\n            'name': 'FloatParsing<DoubleType>',\n            'per_family_instance_index': 0,\n            'real_time': 10634.858754122948,\n            'repetition_index': 1,\n            'repetitions': 3,\n            'run_name': 'FloatParsing<DoubleType>',\n            'run_type': 'iteration',\n            'threads': 1,\n            'time_unit': 'ns'\n        },\n        {\n            'cpu_time': 10664.315484347,\n            'family_index': 0,\n            'items_per_second': 93770669.24434038,\n            'iterations': 9487,\n            'name': 'FloatParsing<DoubleType>',\n            'per_family_instance_index': 0,\n            'real_time': 10665.584589337563,\n            'repetition_index': 2,\n            'repetitions': 3,\n            'run_name': 'FloatParsing<DoubleType>',\n            'run_type': 'iteration',\n            'threads': 1,\n            'time_unit': 'ns'\n        }\n    ]\n\n    archery_result = {\n        'counters': {\n            'family_index': 0,\n            'iterations': 9487,\n            'per_family_instance_index': 0,\n            'repetition_index': 2,\n            'repetitions': 3,\n            'run_name': 'FloatParsing<DoubleType>',\n            'threads': 1\n        },\n        'less_is_better': False,\n        'name': 'FloatParsing<DoubleType>',\n        'time_unit': 'ns',\n        'times': [10628.84905663701, 10634.858754122948, 10665.584589337563],\n        'unit': 'items_per_second',\n        'values': [93770669.24434038, 94044022.63448404, 94096551.75067839]\n    }\n\n    observations = [GoogleBenchmarkObservation(**g) for g in google_results]\n    benchmark = GoogleBenchmark(name, observations)\n    result = json.dumps(benchmark, cls=JsonEncoder)\n    assert json.loads(result) == archery_result\n", "dev/archery/archery/tests/test_cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nfrom click.testing import CliRunner\n\nfrom archery.cli import archery\n\n\n@patch(\"archery.linking.check_dynamic_library_dependencies\")\ndef test_linking_check_dependencies(fn):\n    args = [\n        \"linking\",\n        \"check-dependencies\",\n        \"-a\", \"libarrow\",\n        \"-d\", \"libcurl\",\n        \"somelib.so\"\n    ]\n    result = CliRunner().invoke(archery, args)\n    assert result.exit_code == 0\n    fn.assert_called_once_with(\n        Path('somelib.so'), allowed={'libarrow'}, disallowed={'libcurl'}\n    )\n", "dev/archery/archery/tests/test_testing.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport subprocess\n\nimport pytest\n\nfrom archery.testing import PartialEnv, assert_subprocess_calls\n\n\ndef test_partial_env():\n    assert PartialEnv(a=1, b=2) == {'a': 1, 'b': 2, 'c': 3}\n    assert PartialEnv(a=1) == {'a': 1, 'b': 2, 'c': 3}\n    assert PartialEnv(a=1, b=2) == {'a': 1, 'b': 2}\n    assert PartialEnv(a=1, b=2) != {'b': 2, 'c': 3}\n    assert PartialEnv(a=1, b=2) != {'a': 1, 'c': 3}\n\n\ndef test_assert_subprocess_calls():\n    expected_calls = [\n        \"echo Hello\",\n        [\"echo\", \"World\"]\n    ]\n    with assert_subprocess_calls(expected_calls):\n        subprocess.run(['echo', 'Hello'])\n        subprocess.run(['echo', 'World'])\n\n    expected_env = PartialEnv(\n        CUSTOM_ENV_A='a',\n        CUSTOM_ENV_C='c'\n    )\n    with assert_subprocess_calls(expected_calls, env=expected_env):\n        env = {\n            'CUSTOM_ENV_A': 'a',\n            'CUSTOM_ENV_B': 'b',\n            'CUSTOM_ENV_C': 'c'\n        }\n        subprocess.run(['echo', 'Hello'], env=env)\n        subprocess.run(['echo', 'World'], env=env)\n\n    with pytest.raises(AssertionError):\n        with assert_subprocess_calls(expected_calls, env=expected_env):\n            env = {\n                'CUSTOM_ENV_B': 'b',\n                'CUSTOM_ENV_C': 'c'\n            }\n            subprocess.run(['echo', 'Hello'], env=env)\n            subprocess.run(['echo', 'World'], env=env)\n", "dev/archery/archery/benchmark/compare.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\n# Define a global regression threshold as 5%. This is purely subjective and\n# flawed. This does not track cumulative regression.\nDEFAULT_THRESHOLD = 0.05\n\n\ndef items_per_seconds_fmt(value):\n    if value < 1000:\n        return \"{} items/sec\".format(value)\n    if value < 1000**2:\n        return \"{:.3f}K items/sec\".format(value / 1000)\n    if value < 1000**3:\n        return \"{:.3f}M items/sec\".format(value / 1000**2)\n    else:\n        return \"{:.3f}G items/sec\".format(value / 1000**3)\n\n\ndef bytes_per_seconds_fmt(value):\n    if value < 1024:\n        return \"{} bytes/sec\".format(value)\n    if value < 1024**2:\n        return \"{:.3f} KiB/sec\".format(value / 1024)\n    if value < 1024**3:\n        return \"{:.3f} MiB/sec\".format(value / 1024**2)\n    if value < 1024**4:\n        return \"{:.3f} GiB/sec\".format(value / 1024**3)\n    else:\n        return \"{:.3f} TiB/sec\".format(value / 1024**4)\n\n\ndef change_fmt(value):\n    return \"{:.3%}\".format(value)\n\n\ndef formatter_for_unit(unit):\n    if unit == \"bytes_per_second\":\n        return bytes_per_seconds_fmt\n    elif unit == \"items_per_second\":\n        return items_per_seconds_fmt\n    else:\n        return lambda x: x\n\n\nclass BenchmarkComparator:\n    \"\"\" Compares two benchmarks.\n\n    Encodes the logic of comparing two benchmarks and taking a decision on\n    if it induce a regression.\n    \"\"\"\n\n    def __init__(self, contender, baseline, threshold=DEFAULT_THRESHOLD,\n                 suite_name=None):\n        self.contender = contender\n        self.baseline = baseline\n        self.threshold = threshold\n        self.suite_name = suite_name\n\n    @property\n    def name(self):\n        return self.baseline.name\n\n    @property\n    def less_is_better(self):\n        return self.baseline.less_is_better\n\n    @property\n    def unit(self):\n        return self.baseline.unit\n\n    @property\n    def change(self):\n        new = self.contender.value\n        old = self.baseline.value\n\n        if old == 0 and new == 0:\n            return 0.0\n        if old == 0:\n            return 0.0\n\n        return float(new - old) / abs(old)\n\n    @property\n    def confidence(self):\n        \"\"\" Indicate if a comparison of benchmarks should be trusted. \"\"\"\n        return True\n\n    @property\n    def regression(self):\n        change = self.change\n        adjusted_change = change if self.less_is_better else -change\n        return (self.confidence and adjusted_change > self.threshold)\n\n    @property\n    def formatted(self):\n        fmt = formatter_for_unit(self.unit)\n        return {\n            \"benchmark\": self.name,\n            \"change\": change_fmt(self.change),\n            \"regression\": self.regression,\n            \"baseline\": fmt(self.baseline.value),\n            \"contender\": fmt(self.contender.value),\n            \"unit\": self.unit,\n            \"less_is_better\": self.less_is_better,\n            \"counters\": str(self.baseline.counters)\n        }\n\n    def compare(self, comparator=None):\n        return {\n            \"benchmark\": self.name,\n            \"change\": self.change,\n            \"regression\": self.regression,\n            \"baseline\": self.baseline.value,\n            \"contender\": self.contender.value,\n            \"unit\": self.unit,\n            \"less_is_better\": self.less_is_better,\n            \"counters\": self.baseline.counters\n        }\n\n    def __call__(self, **kwargs):\n        return self.compare(**kwargs)\n\n\ndef pairwise_compare(contender, baseline):\n    dict_contender = {e.name: e for e in contender}\n    dict_baseline = {e.name: e for e in baseline}\n\n    for name in (dict_contender.keys() & dict_baseline.keys()):\n        yield name, (dict_contender[name], dict_baseline[name])\n\n\nclass RunnerComparator:\n    \"\"\" Compares suites/benchmarks from runners.\n\n    It is up to the caller that ensure that runners are compatible (both from\n    the same language implementation).\n    \"\"\"\n\n    def __init__(self, contender, baseline, threshold=DEFAULT_THRESHOLD):\n        self.contender = contender\n        self.baseline = baseline\n        self.threshold = threshold\n\n    @property\n    def comparisons(self):\n        contender = self.contender.suites\n        baseline = self.baseline.suites\n        suites = pairwise_compare(contender, baseline)\n\n        for suite_name, (suite_cont, suite_base) in suites:\n            benchmarks = pairwise_compare(\n                suite_cont.benchmarks, suite_base.benchmarks)\n\n            for _, (bench_cont, bench_base) in benchmarks:\n                yield BenchmarkComparator(bench_cont, bench_base,\n                                          threshold=self.threshold,\n                                          suite_name=suite_name)\n", "dev/archery/archery/benchmark/google.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom itertools import filterfalse, groupby, tee\nimport json\nimport subprocess\nfrom tempfile import NamedTemporaryFile\n\nfrom .core import Benchmark\nfrom ..utils.command import Command\n\n\ndef partition(pred, iterable):\n    # adapted from python's examples\n    t1, t2 = tee(iterable)\n    return list(filter(pred, t1)), list(filterfalse(pred, t2))\n\n\nclass GoogleBenchmarkCommand(Command):\n    \"\"\" Run a google benchmark binary.\n\n    This assumes the binary supports the standard command line options,\n    notably `--benchmark_filter`, `--benchmark_format`, etc...\n    \"\"\"\n\n    def __init__(self, benchmark_bin, benchmark_filter=None, benchmark_extras=None):\n        self.bin = benchmark_bin\n        self.benchmark_filter = benchmark_filter\n        self.benchmark_extras = benchmark_extras or []\n\n    def list_benchmarks(self):\n        argv = [\"--benchmark_list_tests\"]\n        if self.benchmark_filter:\n            argv.append(\"--benchmark_filter={}\".format(self.benchmark_filter))\n        result = self.run(*argv, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE)\n        return str.splitlines(result.stdout.decode(\"utf-8\"))\n\n    def results(self, repetitions=1, repetition_min_time=None):\n        with NamedTemporaryFile() as out:\n            argv = [f\"--benchmark_repetitions={repetitions}\",\n                    f\"--benchmark_out={out.name}\",\n                    \"--benchmark_out_format=json\"]\n\n            if repetition_min_time is not None:\n                argv.append(f\"--benchmark_min_time={repetition_min_time:.6f}s\")\n\n            if self.benchmark_filter:\n                argv.append(f\"--benchmark_filter={self.benchmark_filter}\")\n\n            argv += self.benchmark_extras\n\n            self.run(*argv, check=True)\n            return json.load(out)\n\n\nclass GoogleBenchmarkObservation:\n    \"\"\" Represents one run of a single (google c++) benchmark.\n\n    Aggregates are reported by Google Benchmark executables alongside\n    other observations whenever repetitions are specified (with\n    `--benchmark_repetitions` on the bare benchmark, or with the\n    archery option `--repetitions`). Aggregate observations are not\n    included in `GoogleBenchmark.runs`.\n\n    RegressionSumKernel/32768/0                 1 us          1 us  25.8077GB/s\n    RegressionSumKernel/32768/0                 1 us          1 us  25.7066GB/s\n    RegressionSumKernel/32768/0                 1 us          1 us  25.1481GB/s\n    RegressionSumKernel/32768/0                 1 us          1 us  25.846GB/s\n    RegressionSumKernel/32768/0                 1 us          1 us  25.6453GB/s\n    RegressionSumKernel/32768/0_mean            1 us          1 us  25.6307GB/s\n    RegressionSumKernel/32768/0_median          1 us          1 us  25.7066GB/s\n    RegressionSumKernel/32768/0_stddev          0 us          0 us  288.046MB/s\n    \"\"\"\n\n    def __init__(self, name, real_time, cpu_time, time_unit, run_type,\n                 size=None, bytes_per_second=None, items_per_second=None,\n                 **counters):\n        self._name = name\n        self.real_time = real_time\n        self.cpu_time = cpu_time\n        self.time_unit = time_unit\n        self.run_type = run_type\n        self.size = size\n        self.bytes_per_second = bytes_per_second\n        self.items_per_second = items_per_second\n        self.counters = counters\n\n    @property\n    def is_aggregate(self):\n        \"\"\" Indicate if the observation is a run or an aggregate. \"\"\"\n        return self.run_type == \"aggregate\"\n\n    @property\n    def is_realtime(self):\n        \"\"\" Indicate if the preferred value is realtime instead of cputime. \"\"\"\n        return self.name.find(\"/real_time\") != -1\n\n    @property\n    def name(self):\n        name = self._name\n        return name.rsplit(\"_\", maxsplit=1)[0] if self.is_aggregate else name\n\n    @property\n    def time(self):\n        return self.real_time if self.is_realtime else self.cpu_time\n\n    @property\n    def value(self):\n        \"\"\" Return the benchmark value.\"\"\"\n        return self.bytes_per_second or self.items_per_second or self.time\n\n    @property\n    def unit(self):\n        if self.bytes_per_second:\n            return \"bytes_per_second\"\n        elif self.items_per_second:\n            return \"items_per_second\"\n        else:\n            return self.time_unit\n\n    def __repr__(self):\n        return str(self.value)\n\n\nclass GoogleBenchmark(Benchmark):\n    \"\"\" A set of GoogleBenchmarkObservations. \"\"\"\n\n    def __init__(self, name, runs):\n        \"\"\" Initialize a GoogleBenchmark.\n\n        Parameters\n        ----------\n        name: str\n              Name of the benchmark\n        runs: list(GoogleBenchmarkObservation)\n              Repetitions of GoogleBenchmarkObservation run.\n\n        \"\"\"\n        self.name = name\n        # exclude google benchmark aggregate artifacts\n        _, runs = partition(lambda b: b.is_aggregate, runs)\n        self.runs = sorted(runs, key=lambda b: b.value)\n        unit = self.runs[0].unit\n        time_unit = self.runs[0].time_unit\n        less_is_better = not unit.endswith(\"per_second\")\n        values = [b.value for b in self.runs]\n        times = [b.real_time for b in self.runs]\n        # Slight kludge to extract the UserCounters for each benchmark\n        counters = self.runs[0].counters\n        super().__init__(name, unit, less_is_better, values, time_unit, times,\n                         counters)\n\n    def __repr__(self):\n        return \"GoogleBenchmark[name={},runs={}]\".format(self.names, self.runs)\n\n    @classmethod\n    def from_json(cls, payload):\n        def group_key(x):\n            return x.name\n\n        benchmarks = map(lambda x: GoogleBenchmarkObservation(**x), payload)\n        groups = groupby(sorted(benchmarks, key=group_key), group_key)\n        return [cls(k, list(bs)) for k, bs in groups]\n", "dev/archery/archery/benchmark/core.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\ndef median(values):\n    n = len(values)\n    if n == 0:\n        raise ValueError(\"median requires at least one value\")\n    elif n % 2 == 0:\n        return (values[(n // 2) - 1] + values[n // 2]) / 2\n    else:\n        return values[n // 2]\n\n\nclass Benchmark:\n    def __init__(self, name, unit, less_is_better, values, time_unit,\n                 times, counters=None):\n        self.name = name\n        self.unit = unit\n        self.less_is_better = less_is_better\n        self.values = sorted(values)\n        self.time_unit = time_unit\n        self.times = sorted(times)\n        self.median = median(self.values)\n        self.counters = counters or {}\n\n    @property\n    def value(self):\n        return self.median\n\n    def __repr__(self):\n        return \"Benchmark[name={},value={}]\".format(self.name, self.value)\n\n\nclass BenchmarkSuite:\n    def __init__(self, name, benchmarks):\n        self.name = name\n        self.benchmarks = benchmarks\n\n    def __repr__(self):\n        return \"BenchmarkSuite[name={}, benchmarks={}]\".format(\n            self.name, self.benchmarks\n        )\n", "dev/archery/archery/benchmark/codec.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nimport json\n\nfrom ..benchmark.core import Benchmark, BenchmarkSuite\nfrom ..benchmark.runner import BenchmarkRunner, StaticBenchmarkRunner\nfrom ..benchmark.compare import BenchmarkComparator\n\n\nclass JsonEncoder(json.JSONEncoder):\n    def default(self, o):\n        if isinstance(o, Benchmark):\n            return BenchmarkCodec.encode(o)\n\n        if isinstance(o, BenchmarkSuite):\n            return BenchmarkSuiteCodec.encode(o)\n\n        if isinstance(o, BenchmarkRunner):\n            return BenchmarkRunnerCodec.encode(o)\n\n        if isinstance(o, BenchmarkComparator):\n            return BenchmarkComparatorCodec.encode(o)\n\n        return json.JSONEncoder.default(self, o)\n\n\nclass BenchmarkCodec:\n    @staticmethod\n    def encode(b):\n        return {\n            \"name\": b.name,\n            \"unit\": b.unit,\n            \"less_is_better\": b.less_is_better,\n            \"values\": b.values,\n            \"time_unit\": b.time_unit,\n            \"times\": b.times,\n            \"counters\": b.counters,\n        }\n\n    @staticmethod\n    def decode(dct, **kwargs):\n        return Benchmark(**dct, **kwargs)\n\n\nclass BenchmarkSuiteCodec:\n    @staticmethod\n    def encode(bs):\n        return {\n            \"name\": bs.name,\n            \"benchmarks\": [BenchmarkCodec.encode(b) for b in bs.benchmarks]\n        }\n\n    @staticmethod\n    def decode(dct, **kwargs):\n        benchmarks = [BenchmarkCodec.decode(b)\n                      for b in dct.pop(\"benchmarks\", [])]\n        return BenchmarkSuite(benchmarks=benchmarks, **dct, **kwargs)\n\n\nclass BenchmarkRunnerCodec:\n    @staticmethod\n    def encode(br):\n        return {\"suites\": [BenchmarkSuiteCodec.encode(s) for s in br.suites]}\n\n    @staticmethod\n    def decode(dct, **kwargs):\n        suites = [BenchmarkSuiteCodec.decode(s)\n                  for s in dct.pop(\"suites\", [])]\n        return StaticBenchmarkRunner(suites=suites, **dct, **kwargs)\n\n\nclass BenchmarkComparatorCodec:\n    @staticmethod\n    def encode(bc):\n        comparator = bc.formatted\n\n        suite_name = bc.suite_name\n        if suite_name:\n            comparator[\"suite\"] = suite_name\n\n        return comparator\n", "dev/archery/archery/benchmark/runner.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport glob\nimport json\nimport os\nimport re\n\nfrom .core import BenchmarkSuite\nfrom .google import GoogleBenchmarkCommand, GoogleBenchmark\nfrom .jmh import JavaMicrobenchmarkHarnessCommand, JavaMicrobenchmarkHarness\nfrom ..lang.cpp import CppCMakeDefinition, CppConfiguration\nfrom ..lang.java import JavaMavenDefinition, JavaConfiguration\nfrom ..utils.cmake import CMakeBuild\nfrom ..utils.maven import MavenBuild\nfrom ..utils.logger import logger\n\n\ndef regex_filter(re_expr):\n    if re_expr is None:\n        return lambda s: True\n    re_comp = re.compile(re_expr)\n    return lambda s: re_comp.search(s)\n\n\nDEFAULT_REPETITIONS = 1\n\n\nclass BenchmarkRunner:\n    def __init__(self, suite_filter=None, benchmark_filter=None,\n                 repetitions=DEFAULT_REPETITIONS, repetition_min_time=None):\n        self.suite_filter = suite_filter\n        self.benchmark_filter = benchmark_filter\n        self.repetitions = repetitions\n        self.repetition_min_time = repetition_min_time\n\n    @property\n    def suites(self):\n        raise NotImplementedError(\"BenchmarkRunner must implement suites\")\n\n    @staticmethod\n    def from_rev_or_path(src, root, rev_or_path, cmake_conf, **kwargs):\n        raise NotImplementedError(\n            \"BenchmarkRunner must implement from_rev_or_path\")\n\n\nclass StaticBenchmarkRunner(BenchmarkRunner):\n    \"\"\" Run suites from a (static) set of suites. \"\"\"\n\n    def __init__(self, suites, **kwargs):\n        self._suites = suites\n        super().__init__(**kwargs)\n\n    @property\n    def list_benchmarks(self):\n        for suite in self._suites:\n            for benchmark in suite.benchmarks:\n                yield \"{}.{}\".format(suite.name, benchmark.name)\n\n    @property\n    def suites(self):\n        suite_fn = regex_filter(self.suite_filter)\n        benchmark_fn = regex_filter(self.benchmark_filter)\n\n        for suite in (s for s in self._suites if suite_fn(s.name)):\n            benchmarks = [b for b in suite.benchmarks if benchmark_fn(b.name)]\n            yield BenchmarkSuite(suite.name, benchmarks)\n\n    @classmethod\n    def is_json_result(cls, path_or_str):\n        builder = None\n        try:\n            builder = cls.from_json(path_or_str)\n        except BaseException:\n            pass\n\n        return builder is not None\n\n    @staticmethod\n    def from_json(path_or_str, **kwargs):\n        # .codec imported here to break recursive imports\n        from .codec import BenchmarkRunnerCodec\n        if os.path.isfile(path_or_str):\n            with open(path_or_str) as f:\n                loaded = json.load(f)\n        else:\n            loaded = json.loads(path_or_str)\n        return BenchmarkRunnerCodec.decode(loaded, **kwargs)\n\n    def __repr__(self):\n        return \"BenchmarkRunner[suites={}]\".format(list(self.suites))\n\n\nclass CppBenchmarkRunner(BenchmarkRunner):\n    \"\"\" Run suites from a CMakeBuild. \"\"\"\n\n    def __init__(self, build, benchmark_extras, **kwargs):\n        \"\"\" Initialize a CppBenchmarkRunner. \"\"\"\n        self.build = build\n        self.benchmark_extras = benchmark_extras\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def default_configuration(**kwargs):\n        \"\"\" Returns the default benchmark configuration. \"\"\"\n        return CppConfiguration(\n            build_type=\"release\", with_tests=False, with_benchmarks=True,\n            with_compute=True,\n            with_csv=True,\n            with_dataset=True,\n            with_json=True,\n            with_parquet=True,\n            with_python=False,\n            with_brotli=True,\n            with_bz2=True,\n            with_lz4=True,\n            with_snappy=True,\n            with_zlib=True,\n            with_zstd=True,\n            **kwargs)\n\n    @property\n    def suites_binaries(self):\n        \"\"\" Returns a list of benchmark binaries for this build. \"\"\"\n        # Ensure build is up-to-date to run benchmarks\n        self.build()\n        # Not the best method, but works for now\n        glob_expr = os.path.join(self.build.binaries_dir, \"*-benchmark\")\n        return {os.path.basename(b): b for b in glob.glob(glob_expr)}\n\n    def suite(self, name, suite_bin):\n        \"\"\" Returns the resulting benchmarks for a given suite. \"\"\"\n        suite_cmd = GoogleBenchmarkCommand(suite_bin, self.benchmark_filter,\n                                           self.benchmark_extras)\n\n        # Ensure there will be data\n        benchmark_names = suite_cmd.list_benchmarks()\n        if not benchmark_names:\n            return None\n\n        results = suite_cmd.results(\n            repetitions=self.repetitions,\n            repetition_min_time=self.repetition_min_time)\n        benchmarks = GoogleBenchmark.from_json(results.get(\"benchmarks\"))\n        return BenchmarkSuite(name, benchmarks)\n\n    @property\n    def list_benchmarks(self):\n        for suite_name, suite_bin in self.suites_binaries.items():\n            suite_cmd = GoogleBenchmarkCommand(suite_bin)\n            for benchmark_name in suite_cmd.list_benchmarks():\n                yield \"{}.{}\".format(suite_name, benchmark_name)\n\n    @property\n    def suites(self):\n        \"\"\" Returns all suite for a runner. \"\"\"\n        suite_matcher = regex_filter(self.suite_filter)\n\n        suite_found = False\n        suite_and_binaries = self.suites_binaries\n        for suite_name in suite_and_binaries:\n            if not suite_matcher(suite_name):\n                logger.debug(\"Ignoring suite {}\".format(suite_name))\n                continue\n\n            suite_bin = suite_and_binaries[suite_name]\n            suite = self.suite(suite_name, suite_bin)\n\n            # Filter may exclude all benchmarks\n            if not suite:\n                logger.debug(\"Suite {} executed but no results\"\n                             .format(suite_name))\n                continue\n\n            suite_found = True\n            yield suite\n\n        if not suite_found:\n            raise ValueError(\"No benchmark matches the suite/benchmark filter\")\n\n    @staticmethod\n    def from_rev_or_path(src, root, rev_or_path, cmake_conf, **kwargs):\n        \"\"\" Returns a BenchmarkRunner from a path or a git revision.\n\n        First, it checks if `rev_or_path` is a valid path (or string) of a json\n        object that can deserialize to a BenchmarkRunner. If so, it initialize\n        a StaticBenchmarkRunner from it. This allows memoizing the result of a\n        run in a file or a string.\n\n        Second, it checks if `rev_or_path` points to a valid CMake build\n        directory.  If so, it creates a CppBenchmarkRunner with this existing\n        CMakeBuild.\n\n        Otherwise, it assumes `rev_or_path` is a revision and clone/checkout\n        the given revision and create a fresh CMakeBuild.\n        \"\"\"\n        build = None\n        if StaticBenchmarkRunner.is_json_result(rev_or_path):\n            kwargs.pop('benchmark_extras', None)\n            return StaticBenchmarkRunner.from_json(rev_or_path, **kwargs)\n        elif CMakeBuild.is_build_dir(rev_or_path):\n            build = CMakeBuild.from_path(rev_or_path)\n            return CppBenchmarkRunner(build, **kwargs)\n        else:\n            # Revisions can references remote via the `/` character, ensure\n            # that the revision is path friendly\n            path_rev = rev_or_path.replace(\"/\", \"_\")\n            root_rev = os.path.join(root, path_rev)\n            os.mkdir(root_rev)\n\n            clone_dir = os.path.join(root_rev, \"arrow\")\n            # Possibly checkout the sources at given revision, no need to\n            # perform cleanup on cloned repository as root_rev is reclaimed.\n            src_rev, _ = src.at_revision(rev_or_path, clone_dir)\n            cmake_def = CppCMakeDefinition(src_rev.cpp, cmake_conf)\n            build_dir = os.path.join(root_rev, \"build\")\n            return CppBenchmarkRunner(cmake_def.build(build_dir), **kwargs)\n\n\nclass JavaBenchmarkRunner(BenchmarkRunner):\n    \"\"\" Run suites for Java. \"\"\"\n\n    # default repetitions is 5 for Java microbenchmark harness\n    def __init__(self, build, **kwargs):\n        \"\"\" Initialize a JavaBenchmarkRunner. \"\"\"\n        self.build = build\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def default_configuration(**kwargs):\n        \"\"\" Returns the default benchmark configuration. \"\"\"\n        return JavaConfiguration(**kwargs)\n\n    def suite(self, name):\n        \"\"\" Returns the resulting benchmarks for a given suite. \"\"\"\n        # update .m2 directory, which installs target jars\n        self.build.build()\n\n        suite_cmd = JavaMicrobenchmarkHarnessCommand(\n            self.build, self.benchmark_filter)\n\n        # Ensure there will be data\n        benchmark_names = suite_cmd.list_benchmarks()\n        if not benchmark_names:\n            return None\n\n        # TODO: support `repetition_min_time`\n        results = suite_cmd.results(repetitions=self.repetitions)\n        benchmarks = JavaMicrobenchmarkHarness.from_json(results)\n        return BenchmarkSuite(name, benchmarks)\n\n    @property\n    def list_benchmarks(self):\n        \"\"\" Returns all suite names \"\"\"\n        # Ensure build is up-to-date to run benchmarks\n        self.build.build()\n\n        suite_cmd = JavaMicrobenchmarkHarnessCommand(self.build)\n        benchmark_names = suite_cmd.list_benchmarks()\n        for benchmark_name in benchmark_names:\n            yield \"{}\".format(benchmark_name)\n\n    @property\n    def suites(self):\n        \"\"\" Returns all suite for a runner. \"\"\"\n        suite_name = \"JavaBenchmark\"\n        suite = self.suite(suite_name)\n\n        # Filter may exclude all benchmarks\n        if not suite:\n            logger.debug(\"Suite {} executed but no results\"\n                         .format(suite_name))\n            return\n\n        yield suite\n\n    @staticmethod\n    def from_rev_or_path(src, root, rev_or_path, maven_conf, **kwargs):\n        \"\"\" Returns a BenchmarkRunner from a path or a git revision.\n\n        First, it checks if `rev_or_path` is a valid path (or string) of a json\n        object that can deserialize to a BenchmarkRunner. If so, it initialize\n        a StaticBenchmarkRunner from it. This allows memoizing the result of a\n        run in a file or a string.\n\n        Second, it checks if `rev_or_path` points to a valid Maven build\n        directory.  If so, it creates a JavaBenchmarkRunner with this existing\n        MavenBuild.\n\n        Otherwise, it assumes `rev_or_path` is a revision and clone/checkout\n        the given revision and create a fresh MavenBuild.\n        \"\"\"\n        if StaticBenchmarkRunner.is_json_result(rev_or_path):\n            return StaticBenchmarkRunner.from_json(rev_or_path, **kwargs)\n        elif MavenBuild.is_build_dir(rev_or_path):\n            maven_def = JavaMavenDefinition(rev_or_path, maven_conf)\n            return JavaBenchmarkRunner(maven_def.build(rev_or_path), **kwargs)\n        else:\n            # Revisions can references remote via the `/` character, ensure\n            # that the revision is path friendly\n            path_rev = rev_or_path.replace(\"/\", \"_\")\n            root_rev = os.path.join(root, path_rev)\n            os.mkdir(root_rev)\n\n            clone_dir = os.path.join(root_rev, \"arrow\")\n            # Possibly checkout the sources at given revision, no need to\n            # perform cleanup on cloned repository as root_rev is reclaimed.\n            src_rev, _ = src.at_revision(rev_or_path, clone_dir)\n            maven_def = JavaMavenDefinition(src_rev.java, maven_conf)\n            build_dir = os.path.join(root_rev, \"arrow/java\")\n            return JavaBenchmarkRunner(maven_def.build(build_dir), **kwargs)\n", "dev/archery/archery/benchmark/jmh.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom itertools import filterfalse, groupby, tee\nimport json\nimport subprocess\nfrom tempfile import NamedTemporaryFile\n\nfrom .core import Benchmark\nfrom ..utils.command import Command\nfrom ..utils.maven import Maven\n\n\ndef partition(pred, iterable):\n    # adapted from python's examples\n    t1, t2 = tee(iterable)\n    return list(filter(pred, t1)), list(filterfalse(pred, t2))\n\n\nclass JavaMicrobenchmarkHarnessCommand(Command):\n    \"\"\" Run a Java Micro Benchmark Harness\n\n    This assumes the binary supports the standard command line options,\n    notably `-Dbenchmark_filter`\n    \"\"\"\n\n    def __init__(self, build, benchmark_filter=None):\n        self.benchmark_filter = benchmark_filter\n        self.build = build\n        self.maven = Maven()\n\n    \"\"\" Extract benchmark names from output between \"Benchmarks:\" and \"[INFO]\".\n    Assume the following output:\n      ...\n      Benchmarks:\n      org.apache.arrow.vector.IntBenchmarks.setIntDirectly\n      ...\n      org.apache.arrow.vector.IntBenchmarks.setWithValueHolder\n      org.apache.arrow.vector.IntBenchmarks.setWithWriter\n      ...\n      [INFO]\n    \"\"\"\n\n    def list_benchmarks(self):\n        argv = []\n        if self.benchmark_filter:\n            argv.append(\"-Dbenchmark.filter={}\".format(self.benchmark_filter))\n        result = self.build.list(\n            *argv, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        lists = []\n        benchmarks = False\n        for line in str.splitlines(result.stdout.decode(\"utf-8\")):\n            if not benchmarks:\n                if line.startswith(\"Benchmarks:\"):\n                    benchmarks = True\n            else:\n                if line.startswith(\"org.apache.arrow\"):\n                    lists.append(line)\n                if line.startswith(\"[INFO]\"):\n                    break\n        return lists\n\n    def results(self, repetitions):\n        with NamedTemporaryFile(suffix=\".json\") as out:\n            argv = [\"-Dbenchmark.runs={}\".format(repetitions),\n                    \"-Dbenchmark.resultfile={}\".format(out.name),\n                    \"-Dbenchmark.resultformat=json\"]\n            if self.benchmark_filter:\n                argv.append(\n                    \"-Dbenchmark.filter={}\".format(self.benchmark_filter)\n                )\n\n            self.build.benchmark(*argv, check=True)\n            return json.load(out)\n\n\nclass JavaMicrobenchmarkHarnessObservation:\n    \"\"\" Represents one run of a single Java Microbenchmark Harness\n    \"\"\"\n\n    def __init__(self, benchmark, primaryMetric,\n                 forks, warmupIterations, measurementIterations, **counters):\n        self.name = benchmark\n        self.primaryMetric = primaryMetric\n        self.score = primaryMetric[\"score\"]\n        self.score_unit = primaryMetric[\"scoreUnit\"]\n        self.forks = forks\n        self.warmups = warmupIterations\n        self.runs = measurementIterations\n        self.counters = {\n            \"mode\": counters[\"mode\"],\n            \"threads\": counters[\"threads\"],\n            \"warmups\": warmupIterations,\n            \"warmupTime\": counters[\"warmupTime\"],\n            \"measurements\": measurementIterations,\n            \"measurementTime\": counters[\"measurementTime\"],\n            \"jvmArgs\": counters[\"jvmArgs\"]\n        }\n        self.reciprocal_value = True if self.score_unit.endswith(\n            \"/op\") else False\n        if self.score_unit.startswith(\"ops/\"):\n            idx = self.score_unit.find(\"/\")\n            self.normalizePerSec(self.score_unit[idx+1:])\n        elif self.score_unit.endswith(\"/op\"):\n            idx = self.score_unit.find(\"/\")\n            self.normalizePerSec(self.score_unit[:idx])\n        else:\n            self.normalizeFactor = 1\n\n    @property\n    def value(self):\n        \"\"\" Return the benchmark value.\"\"\"\n        val = 1 / self.score if self.reciprocal_value else self.score\n        return val * self.normalizeFactor\n\n    def normalizePerSec(self, unit):\n        if unit == \"ns\":\n            self.normalizeFactor = 1000 * 1000 * 1000\n        elif unit == \"us\":\n            self.normalizeFactor = 1000 * 1000\n        elif unit == \"ms\":\n            self.normalizeFactor = 1000\n        elif unit == \"min\":\n            self.normalizeFactor = 1 / 60\n        elif unit == \"hr\":\n            self.normalizeFactor = 1 / (60 * 60)\n        elif unit == \"day\":\n            self.normalizeFactor = 1 / (60 * 60 * 24)\n        else:\n            self.normalizeFactor = 1\n\n    @property\n    def unit(self):\n        if self.score_unit.startswith(\"ops/\"):\n            return \"items_per_second\"\n        elif self.score_unit.endswith(\"/op\"):\n            return \"items_per_second\"\n        else:\n            return \"?\"\n\n    def __repr__(self):\n        return str(self.value)\n\n\nclass JavaMicrobenchmarkHarness(Benchmark):\n    \"\"\" A set of JavaMicrobenchmarkHarnessObservations. \"\"\"\n\n    def __init__(self, name, runs):\n        \"\"\" Initialize a JavaMicrobenchmarkHarness.\n\n        Parameters\n        ----------\n        name: str\n              Name of the benchmark\n        forks: int\n        warmups: int\n        runs: int\n        runs: list(JavaMicrobenchmarkHarnessObservation)\n              Repetitions of JavaMicrobenchmarkHarnessObservation run.\n\n        \"\"\"\n        self.name = name\n        self.runs = sorted(runs, key=lambda b: b.value)\n        unit = self.runs[0].unit\n        time_unit = \"N/A\"\n        less_is_better = not unit.endswith(\"per_second\")\n        values = [b.value for b in self.runs]\n        times = []\n        # Slight kludge to extract the UserCounters for each benchmark\n        counters = self.runs[0].counters\n        super().__init__(name, unit, less_is_better, values, time_unit, times,\n                         counters)\n\n    def __repr__(self):\n        return \"JavaMicrobenchmark[name={},runs={}]\".format(\n            self.name, self.runs)\n\n    @classmethod\n    def from_json(cls, payload):\n        def group_key(x):\n            return x.name\n\n        benchmarks = map(\n            lambda x: JavaMicrobenchmarkHarnessObservation(**x), payload)\n        groups = groupby(sorted(benchmarks, key=group_key), group_key)\n        return [cls(k, list(bs)) for k, bs in groups]\n", "dev/archery/archery/benchmark/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "dev/archery/archery/integration/tester_cpp.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport functools\nimport os\nimport subprocess\n\nfrom . import cdata\nfrom .tester import Tester, CDataExporter, CDataImporter\nfrom .util import run_cmd, log\nfrom ..utils.source import ARROW_ROOT_DEFAULT\n\n\n_EXE_PATH = os.environ.get(\n    \"ARROW_CPP_EXE_PATH\", os.path.join(ARROW_ROOT_DEFAULT, \"cpp/build/debug\")\n)\n_INTEGRATION_EXE = os.path.join(_EXE_PATH, \"arrow-json-integration-test\")\n_STREAM_TO_FILE = os.path.join(_EXE_PATH, \"arrow-stream-to-file\")\n_FILE_TO_STREAM = os.path.join(_EXE_PATH, \"arrow-file-to-stream\")\n\n_FLIGHT_SERVER_CMD = [os.path.join(\n    _EXE_PATH, \"flight-test-integration-server\")]\n_FLIGHT_CLIENT_CMD = [\n    os.path.join(_EXE_PATH, \"flight-test-integration-client\"),\n    \"-host\",\n    \"localhost\",\n]\n\n_DLL_PATH = _EXE_PATH\n_ARROW_DLL = os.path.join(_DLL_PATH, \"libarrow\" + cdata.dll_suffix)\n\n\nclass CppTester(Tester):\n    PRODUCER = True\n    CONSUMER = True\n    FLIGHT_SERVER = True\n    FLIGHT_CLIENT = True\n    C_DATA_SCHEMA_EXPORTER = True\n    C_DATA_ARRAY_EXPORTER = True\n    C_DATA_SCHEMA_IMPORTER = True\n    C_DATA_ARRAY_IMPORTER = True\n\n    name = 'C++'\n\n    def _run(\n        self,\n        arrow_path=None,\n        json_path=None,\n        command='VALIDATE',\n        quirks=None\n    ):\n        cmd = [_INTEGRATION_EXE, '--integration']\n\n        if arrow_path is not None:\n            cmd.append('--arrow=' + arrow_path)\n\n        if json_path is not None:\n            cmd.append('--json=' + json_path)\n\n        cmd.append('--mode=' + command)\n\n        if quirks:\n            if \"no_decimal_validate\" in quirks:\n                cmd.append(\"--validate_decimals=false\")\n            if \"no_date64_validate\" in quirks:\n                cmd.append(\"--validate_date64=false\")\n            if \"no_times_validate\" in quirks:\n                cmd.append(\"--validate_times=false\")\n\n        if self.debug:\n            log(' '.join(cmd))\n\n        run_cmd(cmd)\n\n    def validate(self, json_path, arrow_path, quirks=None):\n        return self._run(arrow_path, json_path, 'VALIDATE', quirks=quirks)\n\n    def json_to_file(self, json_path, arrow_path):\n        return self._run(arrow_path, json_path, 'JSON_TO_ARROW')\n\n    def stream_to_file(self, stream_path, file_path):\n        cmd = [_STREAM_TO_FILE, '<', stream_path, '>', file_path]\n        self.run_shell_command(cmd)\n\n    def file_to_stream(self, file_path, stream_path):\n        cmd = [_FILE_TO_STREAM, file_path, '>', stream_path]\n        self.run_shell_command(cmd)\n\n    @contextlib.contextmanager\n    def flight_server(self, scenario_name=None):\n        cmd = _FLIGHT_SERVER_CMD + ['-port=0']\n        if scenario_name:\n            cmd = cmd + [\"-scenario\", scenario_name]\n        if self.debug:\n            log(\" \".join(cmd))\n        server = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        try:\n            output = server.stdout.readline().decode()\n            if not output.startswith(\"Server listening on localhost:\"):\n                server.kill()\n                out, err = server.communicate()\n                raise RuntimeError(\n                    \"Flight-C++ server did not start properly, \"\n                    \"stdout:\\n{}\\n\\nstderr:\\n{}\\n\".format(\n                        output + out.decode(), err.decode()\n                    )\n                )\n            port = int(output.split(\":\")[1])\n            yield port\n        finally:\n            server.kill()\n            server.wait(5)\n\n    def flight_request(self, port, json_path=None, scenario_name=None):\n        cmd = _FLIGHT_CLIENT_CMD + [f'-port={port}']\n        if json_path:\n            cmd.extend(('-path', json_path))\n        elif scenario_name:\n            cmd.extend(('-scenario', scenario_name))\n        else:\n            raise TypeError(\"Must provide one of json_path or scenario_name\")\n\n        if self.debug:\n            log(' '.join(cmd))\n        run_cmd(cmd)\n\n    def make_c_data_exporter(self):\n        return CppCDataExporter(self.debug, self.args)\n\n    def make_c_data_importer(self):\n        return CppCDataImporter(self.debug, self.args)\n\n\n_cpp_c_data_entrypoints = \"\"\"\n    const char* ArrowCpp_CDataIntegration_ExportSchemaFromJson(\n        const char* json_path, struct ArrowSchema* out);\n    const char* ArrowCpp_CDataIntegration_ImportSchemaAndCompareToJson(\n        const char* json_path, struct ArrowSchema* schema);\n\n    const char* ArrowCpp_CDataIntegration_ExportBatchFromJson(\n        const char* json_path, int num_batch, struct ArrowArray* out);\n    const char* ArrowCpp_CDataIntegration_ImportBatchAndCompareToJson(\n        const char* json_path, int num_batch, struct ArrowArray* batch);\n\n    int64_t ArrowCpp_BytesAllocated();\n    \"\"\"\n\n\n@functools.lru_cache\ndef _load_ffi(ffi, lib_path=_ARROW_DLL):\n    os.environ['ARROW_DEBUG_MEMORY_POOL'] = 'trap'\n    ffi.cdef(_cpp_c_data_entrypoints)\n    dll = ffi.dlopen(lib_path)\n    dll.ArrowCpp_CDataIntegration_ExportSchemaFromJson\n    return dll\n\n\nclass _CDataBase:\n\n    def __init__(self, debug, args):\n        self.debug = debug\n        self.args = args\n        self.ffi = cdata.ffi()\n        self.dll = _load_ffi(self.ffi)\n\n    def _check_c_error(self, c_error):\n        \"\"\"\n        Check a `const char*` error return from an integration entrypoint.\n\n        A null means success, a non-empty string is an error message.\n        The string is statically allocated on the C++ side.\n        \"\"\"\n        assert self.ffi.typeof(c_error) is self.ffi.typeof(\"const char*\")\n        if c_error != self.ffi.NULL:\n            error = self.ffi.string(c_error).decode('utf8',\n                                                    errors='replace')\n            raise RuntimeError(\n                f\"C++ C Data Integration call failed: {error}\")\n\n\nclass CppCDataExporter(CDataExporter, _CDataBase):\n\n    def export_schema_from_json(self, json_path, c_schema_ptr):\n        c_error = self.dll.ArrowCpp_CDataIntegration_ExportSchemaFromJson(\n            str(json_path).encode(), c_schema_ptr)\n        self._check_c_error(c_error)\n\n    def export_batch_from_json(self, json_path, num_batch, c_array_ptr):\n        c_error = self.dll.ArrowCpp_CDataIntegration_ExportBatchFromJson(\n            str(json_path).encode(), num_batch, c_array_ptr)\n        self._check_c_error(c_error)\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n\n    def record_allocation_state(self):\n        return self.dll.ArrowCpp_BytesAllocated()\n\n\nclass CppCDataImporter(CDataImporter, _CDataBase):\n\n    def import_schema_and_compare_to_json(self, json_path, c_schema_ptr):\n        c_error = self.dll.ArrowCpp_CDataIntegration_ImportSchemaAndCompareToJson(\n            str(json_path).encode(), c_schema_ptr)\n        self._check_c_error(c_error)\n\n    def import_batch_and_compare_to_json(self, json_path, num_batch,\n                                         c_array_ptr):\n        c_error = self.dll.ArrowCpp_CDataIntegration_ImportBatchAndCompareToJson(\n            str(json_path).encode(), num_batch, c_array_ptr)\n        self._check_c_error(c_error)\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n", "dev/archery/archery/integration/tester.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Base class for language-specific integration test harnesses\n\nfrom abc import ABC, abstractmethod\nimport os\nimport subprocess\nimport typing\n\nfrom .util import log\n\n\n_Predicate = typing.Callable[[], bool]\n\n\nclass CDataExporter(ABC):\n\n    @abstractmethod\n    def export_schema_from_json(self, json_path: os.PathLike,\n                                c_schema_ptr: object):\n        \"\"\"\n        Read a JSON integration file and export its schema.\n\n        Parameters\n        ----------\n        json_path : Path\n            Path to the JSON file\n        c_schema_ptr : cffi pointer value\n            Pointer to the ``ArrowSchema`` struct to export to.\n        \"\"\"\n\n    @abstractmethod\n    def export_batch_from_json(self, json_path: os.PathLike,\n                               num_batch: int,\n                               c_array_ptr: object):\n        \"\"\"\n        Read a JSON integration file and export one of its batches.\n\n        Parameters\n        ----------\n        json_path : Path\n            Path to the JSON file\n        num_batch : int\n            Number of the record batch in the JSON file\n        c_schema_ptr : cffi pointer value\n            Pointer to the ``ArrowArray`` struct to export to.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def supports_releasing_memory(self) -> bool:\n        \"\"\"\n        Whether the implementation is able to release memory deterministically.\n\n        Here, \"release memory\" means that, after the `release` callback of\n        a C Data Interface export is called, `run_gc` is able to trigger\n        the deallocation of the memory underlying the export (such as buffer data).\n\n        If false, then `record_allocation_state` is allowed to raise\n        NotImplementedError.\n        \"\"\"\n\n    def record_allocation_state(self) -> object:\n        \"\"\"\n        Return the current memory allocation state.\n\n        Returns\n        -------\n        state : object\n            Equality-comparable object representing the allocation state,\n            for example the number of allocated or exported bytes.\n        \"\"\"\n        raise NotImplementedError\n\n    def run_gc(self):\n        \"\"\"\n        Run the GC if necessary.\n\n        This should ensure that any temporary objects and data created by\n        previous exporter calls are collected.\n        \"\"\"\n\n    @property\n    def required_gc_runs(self):\n        \"\"\"\n        The maximum number of calls to `run_gc` that need to be issued to\n        ensure proper deallocation. Some implementations may require this\n        to be greater than one.\n        \"\"\"\n        return 1\n\n    def close(self):\n        \"\"\"\n        Final cleanup after usage.\n        \"\"\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc):\n        self.close()\n\n\nclass CDataImporter(ABC):\n\n    @abstractmethod\n    def import_schema_and_compare_to_json(self, json_path: os.PathLike,\n                                          c_schema_ptr: object):\n        \"\"\"\n        Import schema and compare it to the schema of a JSON integration file.\n\n        An error is raised if importing fails or the schemas differ.\n\n        Parameters\n        ----------\n        json_path : Path\n            The path to the JSON file\n        c_schema_ptr : cffi pointer value\n            Pointer to the ``ArrowSchema`` struct to import from.\n        \"\"\"\n\n    @abstractmethod\n    def import_batch_and_compare_to_json(self, json_path: os.PathLike,\n                                         num_batch: int,\n                                         c_array_ptr: object):\n        \"\"\"\n        Import record batch and compare it to one of the batches\n        from a JSON integration file.\n\n        The schema used for importing the record batch is the one from\n        the JSON file.\n\n        An error is raised if importing fails or the batches differ.\n\n        Parameters\n        ----------\n        json_path : Path\n            The path to the JSON file\n        num_batch : int\n            Number of the record batch in the JSON file\n        c_array_ptr : cffi pointer value\n            Pointer to the ``ArrowArray`` struct to import from.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def supports_releasing_memory(self) -> bool:\n        \"\"\"\n        Whether the implementation is able to release memory deterministically.\n\n        Here, \"release memory\" means `run_gc()` is able to trigger the\n        `release` callback of a C Data Interface export (which would then\n        induce a deallocation mechanism on the exporter).\n        \"\"\"\n\n    def run_gc(self):\n        \"\"\"\n        Run the GC if necessary.\n\n        This should ensure that any imported data has its release callback called.\n        \"\"\"\n\n    @property\n    def required_gc_runs(self):\n        \"\"\"\n        The maximum number of calls to `run_gc` that need to be issued to\n        ensure release callbacks are triggered. Some implementations may\n        require this to be greater than one.\n        \"\"\"\n        return 1\n\n    def close(self):\n        \"\"\"\n        Final cleanup after usage.\n        \"\"\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc):\n        # Make sure any exported data is released.\n        for i in range(self.required_gc_runs):\n            self.run_gc()\n        self.close()\n\n\nclass Tester:\n    \"\"\"\n    The interface to declare a tester to run integration tests against.\n    \"\"\"\n    # whether the language supports producing / writing IPC\n    PRODUCER = False\n    # whether the language supports consuming / reading IPC\n    CONSUMER = False\n    # whether the language supports serving Flight\n    FLIGHT_SERVER = False\n    # whether the language supports receiving Flight\n    FLIGHT_CLIENT = False\n    # whether the language supports the C Data Interface as an exporter\n    C_DATA_SCHEMA_EXPORTER = False\n    C_DATA_ARRAY_EXPORTER = False\n    # whether the language supports the C Data Interface as an importer\n    C_DATA_SCHEMA_IMPORTER = False\n    C_DATA_ARRAY_IMPORTER = False\n\n    # the name used for skipping and shown in the logs\n    name = \"unknown\"\n\n    def __init__(self, debug=False, **args):\n        self.args = args\n        self.debug = debug\n\n    def run_shell_command(self, cmd, **kwargs):\n        cmd = ' '.join(cmd)\n        if self.debug:\n            log(cmd)\n        kwargs.update(shell=True)\n        subprocess.check_call(cmd, **kwargs)\n\n    def json_to_file(self, json_path, arrow_path):\n        \"\"\"\n        Run the conversion of an Arrow JSON integration file\n        to an Arrow IPC file\n        \"\"\"\n        raise NotImplementedError\n\n    def stream_to_file(self, stream_path, file_path):\n        \"\"\"\n        Run the conversion of an Arrow IPC stream to an\n        Arrow IPC file\n        \"\"\"\n        raise NotImplementedError\n\n    def file_to_stream(self, file_path, stream_path):\n        \"\"\"\n        Run the conversion of an Arrow IPC file to an Arrow IPC stream\n        \"\"\"\n        raise NotImplementedError\n\n    def validate(self, json_path, arrow_path, quirks=None):\n        \"\"\"\n        Validate that the Arrow IPC file is equal to the corresponding\n        Arrow JSON integration file\n        \"\"\"\n        raise NotImplementedError\n\n    def flight_server(self, scenario_name=None):\n        \"\"\"Start the Flight server on a free port.\n\n        This should be a context manager that returns the port as the\n        managed object, and cleans up the server on exit.\n        \"\"\"\n        raise NotImplementedError\n\n    def flight_request(self, port, json_path=None, scenario_name=None):\n        raise NotImplementedError\n\n    def make_c_data_exporter(self) -> CDataExporter:\n        raise NotImplementedError\n\n    def make_c_data_importer(self) -> CDataImporter:\n        raise NotImplementedError\n", "dev/archery/archery/integration/tester_nanoarrow.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport functools\nimport os\n\nfrom . import cdata\nfrom .tester import Tester, CDataExporter, CDataImporter\nfrom ..utils.source import ARROW_ROOT_DEFAULT\n\n\n_NANOARROW_PATH = os.environ.get(\n    \"ARROW_NANOARROW_PATH\",\n    os.path.join(ARROW_ROOT_DEFAULT, \"nanoarrow/cdata\"),\n)\n\n_INTEGRATION_DLL = os.path.join(\n    _NANOARROW_PATH, \"libnanoarrow_c_data_integration\" + cdata.dll_suffix\n)\n\n\nclass NanoarrowTester(Tester):\n    PRODUCER = False\n    CONSUMER = False\n    FLIGHT_SERVER = False\n    FLIGHT_CLIENT = False\n    C_DATA_SCHEMA_EXPORTER = True\n    C_DATA_ARRAY_EXPORTER = True\n    C_DATA_SCHEMA_IMPORTER = True\n    C_DATA_ARRAY_IMPORTER = True\n\n    name = \"nanoarrow\"\n\n    def validate(self, json_path, arrow_path, quirks=None):\n        raise NotImplementedError()\n\n    def json_to_file(self, json_path, arrow_path):\n        raise NotImplementedError()\n\n    def stream_to_file(self, stream_path, file_path):\n        raise NotImplementedError()\n\n    def file_to_stream(self, file_path, stream_path):\n        raise NotImplementedError()\n\n    def make_c_data_exporter(self):\n        return NanoarrowCDataExporter(self.debug, self.args)\n\n    def make_c_data_importer(self):\n        return NanoarrowCDataImporter(self.debug, self.args)\n\n\n_nanoarrow_c_data_entrypoints = \"\"\"\n    const char* nanoarrow_CDataIntegration_ExportSchemaFromJson(\n        const char* json_path, struct ArrowSchema* out);\n\n    const char* nanoarrow_CDataIntegration_ImportSchemaAndCompareToJson(\n        const char* json_path, struct ArrowSchema* schema);\n\n    const char* nanoarrow_CDataIntegration_ExportBatchFromJson(\n        const char* json_path, int num_batch, struct ArrowArray* out);\n\n    const char* nanoarrow_CDataIntegration_ImportBatchAndCompareToJson(\n        const char* json_path, int num_batch, struct ArrowArray* batch);\n\n    int64_t nanoarrow_BytesAllocated(void);\n    \"\"\"\n\n\n@functools.lru_cache\ndef _load_ffi(ffi, lib_path=_INTEGRATION_DLL):\n    ffi.cdef(_nanoarrow_c_data_entrypoints)\n    dll = ffi.dlopen(lib_path)\n    return dll\n\n\nclass _CDataBase:\n    def __init__(self, debug, args):\n        self.debug = debug\n        self.args = args\n        self.ffi = cdata.ffi()\n        self.dll = _load_ffi(self.ffi)\n\n    def _check_nanoarrow_error(self, na_error):\n        \"\"\"\n        Check a `const char*` error return from an integration entrypoint.\n\n        A null means success, a non-empty string is an error message.\n        The string is statically allocated on the nanoarrow side and does not\n        need to be released.\n        \"\"\"\n        assert self.ffi.typeof(na_error) is self.ffi.typeof(\"const char*\")\n        if na_error != self.ffi.NULL:\n            error = self.ffi.string(na_error).decode(\"utf8\", errors=\"replace\")\n            raise RuntimeError(f\"nanoarrow C Data Integration call failed: {error}\")\n\n\nclass NanoarrowCDataExporter(CDataExporter, _CDataBase):\n    def export_schema_from_json(self, json_path, c_schema_ptr):\n        na_error = self.dll.nanoarrow_CDataIntegration_ExportSchemaFromJson(\n            str(json_path).encode(), c_schema_ptr\n        )\n        self._check_nanoarrow_error(na_error)\n\n    def export_batch_from_json(self, json_path, num_batch, c_array_ptr):\n        na_error = self.dll.nanoarrow_CDataIntegration_ExportBatchFromJson(\n            str(json_path).encode(), num_batch, c_array_ptr\n        )\n        self._check_nanoarrow_error(na_error)\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n\n    def record_allocation_state(self):\n        return self.dll.nanoarrow_BytesAllocated()\n\n\nclass NanoarrowCDataImporter(CDataImporter, _CDataBase):\n    def import_schema_and_compare_to_json(self, json_path, c_schema_ptr):\n        na_error = self.dll.nanoarrow_CDataIntegration_ImportSchemaAndCompareToJson(\n            str(json_path).encode(), c_schema_ptr\n        )\n        self._check_nanoarrow_error(na_error)\n\n    def import_batch_and_compare_to_json(self, json_path, num_batch, c_array_ptr):\n        na_error = self.dll.nanoarrow_CDataIntegration_ImportBatchAndCompareToJson(\n            str(json_path).encode(), num_batch, c_array_ptr\n        )\n        self._check_nanoarrow_error(na_error)\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n", "dev/archery/archery/integration/tester_csharp.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom contextlib import contextmanager\nimport os\n\nfrom . import cdata\nfrom .tester import Tester, CDataExporter, CDataImporter\nfrom .util import run_cmd, log\nfrom ..utils.source import ARROW_ROOT_DEFAULT\n\n\n_ARTIFACTS_PATH = os.path.join(ARROW_ROOT_DEFAULT, \"csharp/artifacts\")\n\n_EXE_PATH = os.path.join(_ARTIFACTS_PATH,\n                         \"Apache.Arrow.IntegrationTest\",\n                         \"Debug/net8.0/Apache.Arrow.IntegrationTest\",\n                         )\n\n_clr_loaded = False\n\n\ndef _load_clr():\n    global _clr_loaded\n    if not _clr_loaded:\n        _clr_loaded = True\n        os.environ['DOTNET_GCHeapHardLimit'] = '0xC800000'  # 200 MiB\n        import pythonnet\n        pythonnet.load(\"coreclr\")\n        import clr\n        clr.AddReference(\n            f\"{_ARTIFACTS_PATH}/Apache.Arrow.IntegrationTest/\"\n            f\"Debug/net8.0/Apache.Arrow.IntegrationTest.dll\")\n        clr.AddReference(\n            f\"{_ARTIFACTS_PATH}/Apache.Arrow.Tests/\"\n            f\"Debug/net8.0/Apache.Arrow.Tests.dll\")\n\n        from Apache.Arrow.IntegrationTest import CDataInterface\n        CDataInterface.Initialize()\n\n\n@contextmanager\ndef _disposing(disposable):\n    \"\"\"\n    Ensure the IDisposable object is disposed of when the enclosed block exits.\n    \"\"\"\n    try:\n        yield disposable\n    finally:\n        disposable.Dispose()\n\n\nclass _CDataBase:\n\n    def __init__(self, debug, args):\n        self.debug = debug\n        self.args = args\n        self.ffi = cdata.ffi()\n        _load_clr()\n\n    def _pointer_to_int(self, c_ptr):\n        return int(self.ffi.cast('uintptr_t', c_ptr))\n\n    def _read_batch_from_json(self, json_path, num_batch):\n        from Apache.Arrow.IntegrationTest import CDataInterface\n\n        return CDataInterface.ParseJsonFile(json_path).ToArrow(num_batch)\n\n    def _run_gc(self):\n        from Apache.Arrow.IntegrationTest import CDataInterface\n        CDataInterface.RunGC()\n\n\nclass CSharpCDataExporter(CDataExporter, _CDataBase):\n\n    def export_schema_from_json(self, json_path, c_schema_ptr):\n        from Apache.Arrow.IntegrationTest import CDataInterface\n\n        jf = CDataInterface.ParseJsonFile(json_path)\n        CDataInterface.ExportSchema(jf.Schema.ToArrow(),\n                                    self._pointer_to_int(c_schema_ptr))\n\n    def export_batch_from_json(self, json_path, num_batch, c_array_ptr):\n        from Apache.Arrow.IntegrationTest import CDataInterface\n\n        _, batch = self._read_batch_from_json(json_path, num_batch)\n        with _disposing(batch):\n            CDataInterface.ExportRecordBatch(batch,\n                                             self._pointer_to_int(c_array_ptr))\n\n    @property\n    def supports_releasing_memory(self):\n        # XXX the C# GC doesn't give reliable allocation measurements\n        return False\n\n    def run_gc(self):\n        self._run_gc()\n\n\nclass CSharpCDataImporter(CDataImporter, _CDataBase):\n\n    def import_schema_and_compare_to_json(self, json_path, c_schema_ptr):\n        from Apache.Arrow.IntegrationTest import CDataInterface\n        from Apache.Arrow.Tests import SchemaComparer\n\n        jf = CDataInterface.ParseJsonFile(json_path)\n        imported_schema = CDataInterface.ImportSchema(\n            self._pointer_to_int(c_schema_ptr))\n        SchemaComparer.Compare(jf.Schema.ToArrow(), imported_schema)\n\n    def import_batch_and_compare_to_json(self, json_path, num_batch,\n                                         c_array_ptr):\n        from Apache.Arrow.IntegrationTest import CDataInterface\n        from Apache.Arrow.Tests import ArrowReaderVerifier\n\n        schema, batch = self._read_batch_from_json(json_path, num_batch)\n        with _disposing(batch):\n            imported_batch = CDataInterface.ImportRecordBatch(\n                self._pointer_to_int(c_array_ptr), schema)\n            with _disposing(imported_batch):\n                ArrowReaderVerifier.CompareBatches(batch, imported_batch,\n                                                   strictCompare=False)\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n\n    def run_gc(self):\n        self._run_gc()\n\n\nclass CSharpTester(Tester):\n    PRODUCER = True\n    CONSUMER = True\n    C_DATA_SCHEMA_EXPORTER = True\n    C_DATA_SCHEMA_IMPORTER = True\n    C_DATA_ARRAY_EXPORTER = True\n    C_DATA_ARRAY_IMPORTER = True\n\n    name = 'C#'\n\n    def _run(self, json_path=None, arrow_path=None, command='validate'):\n        cmd = [_EXE_PATH]\n\n        cmd.extend(['--mode', command])\n\n        if json_path is not None:\n            cmd.extend(['-j', json_path])\n\n        if arrow_path is not None:\n            cmd.extend(['-a', arrow_path])\n\n        if self.debug:\n            log(' '.join(cmd))\n\n        run_cmd(cmd)\n\n    def validate(self, json_path, arrow_path, quirks=None):\n        return self._run(json_path, arrow_path, 'validate')\n\n    def json_to_file(self, json_path, arrow_path):\n        return self._run(json_path, arrow_path, 'json-to-arrow')\n\n    def stream_to_file(self, stream_path, file_path):\n        cmd = [_EXE_PATH]\n        cmd.extend(['--mode', 'stream-to-file', '-a', file_path])\n        cmd.extend(['<', stream_path])\n        self.run_shell_command(cmd)\n\n    def file_to_stream(self, file_path, stream_path):\n        cmd = [_EXE_PATH]\n        cmd.extend(['--mode', 'file-to-stream'])\n        cmd.extend(['-a', file_path, '>', stream_path])\n        self.run_shell_command(cmd)\n\n    def make_c_data_exporter(self):\n        return CSharpCDataExporter(self.debug, self.args)\n\n    def make_c_data_importer(self):\n        return CSharpCDataImporter(self.debug, self.args)\n", "dev/archery/archery/integration/tester_go.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport functools\nimport os\nimport subprocess\n\nfrom . import cdata\nfrom .tester import Tester, CDataExporter, CDataImporter\nfrom .util import run_cmd, log\nfrom ..utils.source import ARROW_ROOT_DEFAULT\n\n\n# FIXME(sbinet): revisit for Go modules\n_HOME = os.getenv(\"HOME\", \"~\")\n_GOPATH = os.getenv(\"GOPATH\", os.path.join(_HOME, \"go\"))\n_GOBIN = os.environ.get(\"GOBIN\", os.path.join(_GOPATH, \"bin\"))\n\n_GO_INTEGRATION_EXE = os.path.join(_GOBIN, \"arrow-json-integration-test\")\n_STREAM_TO_FILE = os.path.join(_GOBIN, \"arrow-stream-to-file\")\n_FILE_TO_STREAM = os.path.join(_GOBIN, \"arrow-file-to-stream\")\n\n_FLIGHT_SERVER_CMD = [os.path.join(_GOBIN, \"arrow-flight-integration-server\")]\n_FLIGHT_CLIENT_CMD = [\n    os.path.join(_GOBIN, \"arrow-flight-integration-client\"),\n    \"-host\",\n    \"localhost\",\n]\n\n_DLL_PATH = os.path.join(\n    ARROW_ROOT_DEFAULT,\n    \"go/arrow/internal/cdata_integration\")\n_INTEGRATION_DLL = os.path.join(_DLL_PATH, \"arrow_go_integration\" + cdata.dll_suffix)\n\n\nclass GoTester(Tester):\n    PRODUCER = True\n    CONSUMER = True\n    FLIGHT_SERVER = True\n    FLIGHT_CLIENT = True\n    C_DATA_SCHEMA_EXPORTER = True\n    C_DATA_ARRAY_EXPORTER = True\n    C_DATA_SCHEMA_IMPORTER = True\n    C_DATA_ARRAY_IMPORTER = True\n\n    name = 'Go'\n\n    def _run(self, arrow_path=None, json_path=None, command='VALIDATE'):\n        cmd = [_GO_INTEGRATION_EXE]\n\n        if arrow_path is not None:\n            cmd.extend(['-arrow', arrow_path])\n\n        if json_path is not None:\n            cmd.extend(['-json', json_path])\n\n        cmd.extend(['-mode', command])\n\n        if self.debug:\n            log(' '.join(cmd))\n\n        run_cmd(cmd)\n\n    def validate(self, json_path, arrow_path, quirks=None):\n        return self._run(arrow_path, json_path, 'VALIDATE')\n\n    def json_to_file(self, json_path, arrow_path):\n        return self._run(arrow_path, json_path, 'JSON_TO_ARROW')\n\n    def stream_to_file(self, stream_path, file_path):\n        cmd = [_STREAM_TO_FILE, '<', stream_path, '>', file_path]\n        self.run_shell_command(cmd)\n\n    def file_to_stream(self, file_path, stream_path):\n        cmd = [_FILE_TO_STREAM, file_path, '>', stream_path]\n        self.run_shell_command(cmd)\n\n    @contextlib.contextmanager\n    def flight_server(self, scenario_name=None):\n        cmd = _FLIGHT_SERVER_CMD + ['-port=0']\n        if scenario_name:\n            cmd = cmd + ['-scenario', scenario_name]\n        if self.debug:\n            log(' '.join(cmd))\n        server = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        try:\n            output = server.stdout.readline().decode()\n            if not output.startswith('Server listening on localhost:'):\n                server.kill()\n                out, err = server.communicate()\n                raise RuntimeError(\n                    'Flight-Go server did not start properly, '\n                    'stdout: \\n{}\\n\\nstderr:\\n{}\\n'.format(\n                        output + out.decode(), err.decode()\n                    )\n                )\n            port = int(output.split(':')[1])\n            yield port\n        finally:\n            server.kill()\n            server.wait(5)\n\n    def flight_request(self, port, json_path=None, scenario_name=None):\n        cmd = _FLIGHT_CLIENT_CMD + [\n            '-port=' + str(port),\n        ]\n        if json_path:\n            cmd.extend(('-path', json_path))\n        elif scenario_name:\n            cmd.extend(('-scenario', scenario_name))\n        else:\n            raise TypeError('Must provide one of json_path or scenario_name')\n\n        if self.debug:\n            log(' '.join(cmd))\n        run_cmd(cmd)\n\n    def make_c_data_exporter(self):\n        return GoCDataExporter(self.debug, self.args)\n\n    def make_c_data_importer(self):\n        return GoCDataImporter(self.debug, self.args)\n\n\n_go_c_data_entrypoints = \"\"\"\n    const char* ArrowGo_ExportSchemaFromJson(const char* json_path,\n                                             uintptr_t out);\n    const char* ArrowGo_ImportSchemaAndCompareToJson(\n        const char* json_path, uintptr_t c_schema);\n\n    const char* ArrowGo_ExportBatchFromJson(const char* json_path,\n                                            int num_batch,\n                                            uintptr_t out);\n    const char* ArrowGo_ImportBatchAndCompareToJson(\n        const char* json_path, int num_batch, uintptr_t c_array);\n\n    int64_t ArrowGo_BytesAllocated();\n    void ArrowGo_RunGC();\n    void ArrowGo_FreeError(const char*);\n    \"\"\"\n\n\n@functools.lru_cache\ndef _load_ffi(ffi, lib_path=_INTEGRATION_DLL):\n    # NOTE that setting Go environment variables here (such as GODEBUG)\n    # would be ignored by the Go runtime. The environment variables need\n    # to be set from the process calling Archery.\n    ffi.cdef(_go_c_data_entrypoints)\n    dll = ffi.dlopen(lib_path)\n    return dll\n\n\nclass _CDataBase:\n\n    def __init__(self, debug, args):\n        self.debug = debug\n        self.args = args\n        self.ffi = cdata.ffi()\n        self.dll = _load_ffi(self.ffi)\n\n    def _pointer_to_int(self, c_ptr):\n        return self.ffi.cast('uintptr_t', c_ptr)\n\n    def _check_go_error(self, go_error):\n        \"\"\"\n        Check a `const char*` error return from an integration entrypoint.\n\n        A null means success, a non-empty string is an error message.\n        The string is dynamically allocated on the Go side.\n        \"\"\"\n        assert self.ffi.typeof(go_error) is self.ffi.typeof(\"const char*\")\n        if go_error != self.ffi.NULL:\n            try:\n                error = self.ffi.string(go_error).decode('utf8',\n                                                         errors='replace')\n                raise RuntimeError(\n                    f\"Go C Data Integration call failed: {error}\")\n            finally:\n                self.dll.ArrowGo_FreeError(go_error)\n\n\nclass GoCDataExporter(CDataExporter, _CDataBase):\n    # Note: the Arrow Go C Data export functions expect their output\n    # ArrowStream or ArrowArray argument to be zero-initialized.\n    # This is currently ensured through the use of `ffi.new`.\n\n    def export_schema_from_json(self, json_path, c_schema_ptr):\n        go_error = self.dll.ArrowGo_ExportSchemaFromJson(\n            str(json_path).encode(), self._pointer_to_int(c_schema_ptr))\n        self._check_go_error(go_error)\n\n    def export_batch_from_json(self, json_path, num_batch, c_array_ptr):\n        go_error = self.dll.ArrowGo_ExportBatchFromJson(\n            str(json_path).encode(), num_batch,\n            self._pointer_to_int(c_array_ptr))\n        self._check_go_error(go_error)\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n\n    def record_allocation_state(self):\n        return self.dll.ArrowGo_BytesAllocated()\n\n    # Note: no need to call the Go GC anywhere thanks to Arrow Go's\n    # explicit refcounting.\n\n\nclass GoCDataImporter(CDataImporter, _CDataBase):\n\n    def import_schema_and_compare_to_json(self, json_path, c_schema_ptr):\n        go_error = self.dll.ArrowGo_ImportSchemaAndCompareToJson(\n            str(json_path).encode(), self._pointer_to_int(c_schema_ptr))\n        self._check_go_error(go_error)\n\n    def import_batch_and_compare_to_json(self, json_path, num_batch,\n                                         c_array_ptr):\n        go_error = self.dll.ArrowGo_ImportBatchAndCompareToJson(\n            str(json_path).encode(), num_batch,\n            self._pointer_to_int(c_array_ptr))\n        self._check_go_error(go_error)\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n", "dev/archery/archery/integration/tester_java.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport functools\nimport os\nfrom pathlib import Path\nimport subprocess\n\nfrom . import cdata\nfrom .tester import Tester, CDataExporter, CDataImporter\nfrom .util import run_cmd, log\n\n\nARROW_BUILD_ROOT = os.environ.get(\n    'ARROW_BUILD_ROOT',\n    Path(__file__).resolve().parents[4]\n)\n\n\ndef load_version_from_pom():\n    import xml.etree.ElementTree as ET\n    tree = ET.parse(os.path.join(ARROW_BUILD_ROOT, 'java', 'pom.xml'))\n    tag_pattern = '{http://maven.apache.org/POM/4.0.0}version'\n    version_tag = list(tree.getroot().findall(tag_pattern))[0]\n    return version_tag.text\n\n\n# NOTE: we don't add \"-Darrow.memory.debug.allocator=true\" here as it adds a\n# couple minutes to total CPU usage of the integration test suite\n# (see setup_jpype() below).\n_JAVA_OPTS = [\n    \"-Dio.netty.tryReflectionSetAccessible=true\",\n    \"-Darrow.struct.conflict.policy=CONFLICT_APPEND\",\n    # GH-39113: avoid failures accessing files in `/tmp/hsperfdata_...`\n    \"-XX:-UsePerfData\",\n]\n\n_arrow_version = load_version_from_pom()\n_ARROW_TOOLS_JAR = os.environ.get(\n    \"ARROW_JAVA_INTEGRATION_JAR\",\n    os.path.join(\n        ARROW_BUILD_ROOT,\n        \"java/tools/target\",\n        f\"arrow-tools-{_arrow_version}-jar-with-dependencies.jar\"\n    )\n)\n_ARROW_C_DATA_JAR = os.environ.get(\n    \"ARROW_C_DATA_JAVA_INTEGRATION_JAR\",\n    os.path.join(\n        ARROW_BUILD_ROOT,\n        \"java/c/target\",\n        f\"arrow-c-data-{_arrow_version}.jar\"\n    )\n)\n_ARROW_FLIGHT_JAR = os.environ.get(\n    \"ARROW_FLIGHT_JAVA_INTEGRATION_JAR\",\n    os.path.join(\n        ARROW_BUILD_ROOT,\n        \"java/flight/flight-integration-tests/target\",\n        f\"flight-integration-tests-{_arrow_version}-jar-with-dependencies.jar\"\n    )\n)\n_ARROW_FLIGHT_SERVER = (\n    \"org.apache.arrow.flight.integration.tests.IntegrationTestServer\"\n)\n_ARROW_FLIGHT_CLIENT = (\n    \"org.apache.arrow.flight.integration.tests.IntegrationTestClient\"\n)\n\n\n@functools.lru_cache\ndef setup_jpype():\n    import jpype\n    jar_path = f\"{_ARROW_TOOLS_JAR}:{_ARROW_C_DATA_JAR}\"\n    # XXX Didn't manage to tone down the logging level here (DEBUG -> INFO)\n    java_opts = _JAVA_OPTS[:]\n    proc = subprocess.run(\n        ['java', '--add-opens'],\n        stderr=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        text=True)\n    if 'Unrecognized option: --add-opens' not in proc.stderr:\n        # Java 9+\n        java_opts.append(\n            '--add-opens=java.base/java.nio='\n            'org.apache.arrow.memory.core,ALL-UNNAMED')\n    jpype.startJVM(jpype.getDefaultJVMPath(),\n                   \"-Djava.class.path=\" + jar_path,\n                   # This flag is too heavy for IPC and Flight tests\n                   \"-Darrow.memory.debug.allocator=true\",\n                   # Reduce internal use of signals by the JVM\n                   \"-Xrs\",\n                   *java_opts)\n\n\nclass _CDataBase:\n\n    def __init__(self, debug, args):\n        import jpype\n        self.debug = debug\n        self.args = args\n        self.ffi = cdata.ffi()\n        setup_jpype()\n        # JPype pointers to java.io, org.apache.arrow...\n        self.java_io = jpype.JPackage(\"java\").io\n        self.java_arrow = jpype.JPackage(\"org\").apache.arrow\n        self.java_allocator = self._make_java_allocator()\n\n    def _pointer_to_int(self, c_ptr):\n        return int(self.ffi.cast('uintptr_t', c_ptr))\n\n    def _wrap_c_schema_ptr(self, c_schema_ptr):\n        return self.java_arrow.c.ArrowSchema.wrap(\n            self._pointer_to_int(c_schema_ptr))\n\n    def _wrap_c_array_ptr(self, c_array_ptr):\n        return self.java_arrow.c.ArrowArray.wrap(\n            self._pointer_to_int(c_array_ptr))\n\n    def _make_java_allocator(self):\n        # Return a new allocator\n        return self.java_arrow.memory.RootAllocator()\n\n    def _assert_schemas_equal(self, expected, actual):\n        # XXX This is fragile for dictionaries, as Schema.equals compares\n        # dictionary ids.\n        self.java_arrow.vector.util.Validator.compareSchemas(\n            expected, actual)\n\n    def _assert_batches_equal(self, expected, actual):\n        self.java_arrow.vector.util.Validator.compareVectorSchemaRoot(\n            expected, actual)\n\n    def _assert_dict_providers_equal(self, expected, actual):\n        self.java_arrow.vector.util.Validator.compareDictionaryProviders(\n            expected, actual)\n\n    # Note: no need to call the Java GC anywhere thanks to AutoCloseable\n\n\nclass JavaCDataExporter(CDataExporter, _CDataBase):\n\n    def export_schema_from_json(self, json_path, c_schema_ptr):\n        json_file = self.java_io.File(json_path)\n        with self.java_arrow.vector.ipc.JsonFileReader(\n                json_file, self.java_allocator) as json_reader:\n            schema = json_reader.start()\n            dict_provider = json_reader\n            self.java_arrow.c.Data.exportSchema(\n                self.java_allocator, schema, dict_provider,\n                self._wrap_c_schema_ptr(c_schema_ptr)\n            )\n\n    def export_batch_from_json(self, json_path, num_batch, c_array_ptr):\n        json_file = self.java_io.File(json_path)\n        with self.java_arrow.vector.ipc.JsonFileReader(\n                json_file, self.java_allocator) as json_reader:\n            json_reader.start()\n            if num_batch > 0:\n                actually_skipped = json_reader.skip(num_batch)\n                assert actually_skipped == num_batch\n            with json_reader.read() as batch:\n                dict_provider = json_reader\n                self.java_arrow.c.Data.exportVectorSchemaRoot(\n                    self.java_allocator, batch, dict_provider,\n                    self._wrap_c_array_ptr(c_array_ptr))\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n\n    def record_allocation_state(self):\n        return self.java_allocator.getAllocatedMemory()\n\n    def close(self):\n        self.java_allocator.close()\n\n\nclass JavaCDataImporter(CDataImporter, _CDataBase):\n\n    def import_schema_and_compare_to_json(self, json_path, c_schema_ptr):\n        json_file = self.java_io.File(json_path)\n        with self.java_arrow.vector.ipc.JsonFileReader(\n                json_file, self.java_allocator) as json_reader:\n            json_schema = json_reader.start()\n            with self.java_arrow.c.CDataDictionaryProvider() as dict_provider:\n                imported_schema = self.java_arrow.c.Data.importSchema(\n                    self.java_allocator,\n                    self._wrap_c_schema_ptr(c_schema_ptr),\n                    dict_provider)\n                self._assert_schemas_equal(json_schema, imported_schema)\n\n    def import_batch_and_compare_to_json(self, json_path, num_batch,\n                                         c_array_ptr):\n        json_file = self.java_io.File(json_path)\n        with self.java_arrow.vector.ipc.JsonFileReader(\n                json_file, self.java_allocator) as json_reader:\n            schema = json_reader.start()\n            if num_batch > 0:\n                actually_skipped = json_reader.skip(num_batch)\n                assert actually_skipped == num_batch\n            with json_reader.read() as batch:\n                with self.java_arrow.vector.VectorSchemaRoot.create(\n                        schema, self.java_allocator) as imported_batch:\n                    # We need to pass a dict provider primed with dictionary ids\n                    # matching those in the schema, hence an empty\n                    # CDataDictionaryProvider would not work here.\n                    dict_provider = (self.java_arrow.vector.dictionary\n                                     .DictionaryProvider.MapDictionaryProvider())\n                    dict_provider.copyStructureFrom(json_reader, self.java_allocator)\n                    with dict_provider:\n                        self.java_arrow.c.Data.importIntoVectorSchemaRoot(\n                            self.java_allocator,\n                            self._wrap_c_array_ptr(c_array_ptr),\n                            imported_batch, dict_provider)\n                        self._assert_batches_equal(batch, imported_batch)\n                        self._assert_dict_providers_equal(json_reader, dict_provider)\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n\n    def close(self):\n        self.java_allocator.close()\n\n\nclass JavaTester(Tester):\n    PRODUCER = True\n    CONSUMER = True\n    FLIGHT_SERVER = True\n    FLIGHT_CLIENT = True\n    C_DATA_SCHEMA_EXPORTER = True\n    C_DATA_SCHEMA_IMPORTER = True\n    C_DATA_ARRAY_EXPORTER = True\n    C_DATA_ARRAY_IMPORTER = True\n\n    name = 'Java'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Detect whether we're on Java 8 or Java 9+\n        self._java_opts = _JAVA_OPTS[:]\n        proc = subprocess.run(\n            ['java', '--add-opens'],\n            stderr=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            text=True)\n        if 'Unrecognized option: --add-opens' not in proc.stderr:\n            # Java 9+\n            self._java_opts.append(\n                '--add-opens=java.base/java.nio='\n                'org.apache.arrow.memory.core,ALL-UNNAMED')\n            self._java_opts.append(\n                '--add-reads=org.apache.arrow.flight.core=ALL-UNNAMED')\n\n    def _run(self, arrow_path=None, json_path=None, command='VALIDATE'):\n        cmd = (\n            ['java'] +\n            self._java_opts +\n            ['-cp', _ARROW_TOOLS_JAR, 'org.apache.arrow.tools.Integration']\n        )\n\n        if arrow_path is not None:\n            cmd.extend(['-a', arrow_path])\n\n        if json_path is not None:\n            cmd.extend(['-j', json_path])\n\n        cmd.extend(['-c', command])\n\n        if self.debug:\n            log(' '.join(cmd))\n\n        run_cmd(cmd)\n\n    def validate(self, json_path, arrow_path, quirks=None):\n        return self._run(arrow_path, json_path, 'VALIDATE')\n\n    def json_to_file(self, json_path, arrow_path):\n        return self._run(arrow_path, json_path, 'JSON_TO_ARROW')\n\n    def stream_to_file(self, stream_path, file_path):\n        cmd = (\n            ['java'] + self._java_opts + [\n                '-cp',\n                _ARROW_TOOLS_JAR,\n                'org.apache.arrow.tools.StreamToFile',\n                stream_path,\n                file_path,\n            ]\n        )\n        if self.debug:\n            log(' '.join(cmd))\n        run_cmd(cmd)\n\n    def file_to_stream(self, file_path, stream_path):\n        cmd = (\n            ['java'] + self._java_opts + [\n                '-cp',\n                _ARROW_TOOLS_JAR,\n                'org.apache.arrow.tools.FileToStream',\n                file_path,\n                stream_path,\n            ]\n        )\n        if self.debug:\n            log(' '.join(cmd))\n        run_cmd(cmd)\n\n    def flight_request(self, port, json_path=None, scenario_name=None):\n        cmd = (\n            ['java'] + self._java_opts + [\n                '-cp', _ARROW_FLIGHT_JAR, _ARROW_FLIGHT_CLIENT, '-port', str(\n                    port)\n            ])\n\n        if json_path:\n            cmd.extend(('-j', json_path))\n        elif scenario_name:\n            cmd.extend(('-scenario', scenario_name))\n        else:\n            raise TypeError('Must provide one of json_path or scenario_name')\n\n        if self.debug:\n            log(' '.join(cmd))\n        run_cmd(cmd)\n\n    @contextlib.contextmanager\n    def flight_server(self, scenario_name=None):\n        cmd = (\n            ['java'] +\n            self._java_opts +\n            ['-cp', _ARROW_FLIGHT_JAR, _ARROW_FLIGHT_SERVER, '-port', '0']\n        )\n        if scenario_name:\n            cmd.extend(('-scenario', scenario_name))\n        if self.debug:\n            log(' '.join(cmd))\n        server = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        try:\n            output = server.stdout.readline().decode()\n            if not output.startswith('Server listening on localhost:'):\n                server.kill()\n                out, err = server.communicate()\n                raise RuntimeError(\n                    'Flight-Java server did not start properly, '\n                    'stdout:\\n{}\\n\\nstderr:\\n{}\\n'.format(\n                        output + out.decode(), err.decode()\n                    )\n                )\n            port = int(output.split(':')[1])\n            yield port\n        finally:\n            server.kill()\n            server.wait(5)\n\n    def make_c_data_exporter(self):\n        return JavaCDataExporter(self.debug, self.args)\n\n    def make_c_data_importer(self):\n        return JavaCDataImporter(self.debug, self.args)\n", "dev/archery/archery/integration/util.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport io\nimport random\nimport socket\nimport subprocess\nimport sys\nimport threading\nimport uuid\n\nimport numpy as np\n\n\ndef guid():\n    return uuid.uuid4().hex\n\n\n# SKIP categories\nSKIP_C_ARRAY = 'c_array'\nSKIP_C_SCHEMA = 'c_schema'\nSKIP_FLIGHT = 'flight'\nSKIP_IPC = 'ipc'\n\n\nclass _Printer:\n    \"\"\"\n    A print()-providing object that can override the stream output on\n    a per-thread basis.\n    \"\"\"\n\n    def __init__(self):\n        self._tls = threading.local()\n\n    def _get_stdout(self):\n        try:\n            return self._tls.stdout\n        except AttributeError:\n            self._tls.stdout = sys.stdout\n            self._tls.corked = False\n            return self._tls.stdout\n\n    def print(self, *args, **kwargs):\n        \"\"\"\n        A variant of print() that writes to a thread-local stream.\n        \"\"\"\n        print(*args, file=self._get_stdout(), **kwargs)\n\n    @property\n    def stdout(self):\n        \"\"\"\n        A thread-local stdout wrapper that may be temporarily buffered\n        using `cork()`.\n        \"\"\"\n        return self._get_stdout()\n\n    @contextlib.contextmanager\n    def cork(self):\n        \"\"\"\n        Temporarily buffer this thread's stream and write out its contents\n        at the end of the context manager.  Useful to avoid interleaved\n        output when multiple threads output progress information.\n        \"\"\"\n        outer_stdout = self._get_stdout()\n        assert not self._tls.corked, \"reentrant call\"\n        inner_stdout = self._tls.stdout = io.StringIO()\n        self._tls.corked = True\n        try:\n            yield\n        finally:\n            self._tls.stdout = outer_stdout\n            self._tls.corked = False\n            outer_stdout.write(inner_stdout.getvalue())\n            outer_stdout.flush()\n\n\nprinter = _Printer()\nlog = printer.print\n\n\n_RAND_CHARS = np.array(list(\"abcdefghijklmnop123456\u00c2rr\u00f4w\u00b5\u00a3\u00b0\u20ac\u77e2\"), dtype=\"U\")\n\n\ndef random_utf8(nchars):\n    \"\"\"\n    Generate one random UTF8 string.\n    \"\"\"\n    return ''.join(np.random.choice(_RAND_CHARS, nchars))\n\n\ndef random_bytes(nbytes):\n    \"\"\"\n    Generate one random binary string.\n    \"\"\"\n    # NOTE getrandbits(0) fails\n    if nbytes > 0:\n        return random.getrandbits(nbytes * 8).to_bytes(nbytes,\n                                                       byteorder='little')\n    else:\n        return b\"\"\n\n\ndef tobytes(o):\n    if isinstance(o, str):\n        return o.encode('utf8')\n    return o\n\n\ndef frombytes(o):\n    if isinstance(o, bytes):\n        return o.decode('utf8')\n    return o\n\n\ndef run_cmd(cmd, **kwargs):\n    if isinstance(cmd, str):\n        cmd = cmd.split(' ')\n\n    try:\n        kwargs.update(stderr=subprocess.STDOUT)\n        output = subprocess.check_output(cmd, **kwargs)\n    except subprocess.CalledProcessError as e:\n        # this avoids hiding the stdout / stderr of failed processes\n        sio = io.StringIO()\n        print('Command failed:', \" \".join(cmd), file=sio)\n        print('With output:', file=sio)\n        print('--------------', file=sio)\n        print(frombytes(e.output), file=sio)\n        print('--------------', file=sio)\n        raise RuntimeError(sio.getvalue())\n\n    return frombytes(output)\n\n\n# Adapted from CPython\ndef find_unused_port(family=socket.AF_INET, socktype=socket.SOCK_STREAM):\n    \"\"\"Returns an unused port that should be suitable for binding.  This is\n    achieved by creating a temporary socket with the same family and type as\n    the 'sock' parameter (default is AF_INET, SOCK_STREAM), and binding it to\n    the specified host address (defaults to 0.0.0.0) with the port set to 0,\n    eliciting an unused ephemeral port from the OS.  The temporary socket is\n    then closed and deleted, and the ephemeral port is returned.\n    \"\"\"\n    with socket.socket(family, socktype) as tempsock:\n        tempsock.bind(('', 0))\n        port = tempsock.getsockname()[1]\n    del tempsock\n    return port\n", "dev/archery/archery/integration/cdata.py": "# licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport cffi\nfrom contextlib import contextmanager\nimport functools\nimport os\nimport sys\n\nfrom .tester import CDataExporter, CDataImporter\n\n\nif sys.platform == \"darwin\":\n    dll_suffix = \".dylib\"\nelif os.name == \"nt\":\n    dll_suffix = \".dll\"\nelse:\n    dll_suffix = \".so\"\n\n_c_data_decls = \"\"\"\n    struct ArrowSchema {\n      // Array type description\n      const char* format;\n      const char* name;\n      const char* metadata;\n      int64_t flags;\n      int64_t n_children;\n      struct ArrowSchema** children;\n      struct ArrowSchema* dictionary;\n\n      // Release callback\n      void (*release)(struct ArrowSchema*);\n      // Opaque producer-specific data\n      void* private_data;\n    };\n\n    struct ArrowArray {\n      // Array data description\n      int64_t length;\n      int64_t null_count;\n      int64_t offset;\n      int64_t n_buffers;\n      int64_t n_children;\n      const void** buffers;\n      struct ArrowArray** children;\n      struct ArrowArray* dictionary;\n\n      // Release callback\n      void (*release)(struct ArrowArray*);\n      // Opaque producer-specific data\n      void* private_data;\n    };\n\n    struct ArrowArrayStream {\n      int (*get_schema)(struct ArrowArrayStream*, struct ArrowSchema* out);\n      int (*get_next)(struct ArrowArrayStream*, struct ArrowArray* out);\n\n      const char* (*get_last_error)(struct ArrowArrayStream*);\n\n      // Release callback\n      void (*release)(struct ArrowArrayStream*);\n      // Opaque producer-specific data\n      void* private_data;\n    };\n    \"\"\"\n\n\n@functools.lru_cache\ndef ffi() -> cffi.FFI:\n    \"\"\"\n    Return a FFI object supporting C Data Interface types.\n    \"\"\"\n    ffi = cffi.FFI()\n    ffi.cdef(_c_data_decls)\n    return ffi\n\n\ndef _release_memory_steps(exporter: CDataExporter, importer: CDataImporter):\n    yield\n    for i in range(max(exporter.required_gc_runs, importer.required_gc_runs)):\n        importer.run_gc()\n        yield\n        exporter.run_gc()\n        yield\n\n\n@contextmanager\ndef check_memory_released(exporter: CDataExporter, importer: CDataImporter):\n    \"\"\"\n    A context manager for memory release checks.\n\n    The context manager arranges cooperation between the exporter and importer\n    to try and release memory at the end of the enclosed block.\n\n    However, if either the exporter or importer doesn't support deterministic\n    memory release, no memory check is performed.\n    \"\"\"\n    do_check = (exporter.supports_releasing_memory and\n                importer.supports_releasing_memory)\n    if do_check:\n        before = exporter.record_allocation_state()\n    yield\n    # Only check for memory state if `yield` didn't raise.\n    if do_check:\n        for _ in _release_memory_steps(exporter, importer):\n            after = exporter.record_allocation_state()\n            if after == before:\n                break\n        if after != before:\n            raise RuntimeError(\n                f\"Memory was not released correctly after roundtrip: \"\n                f\"before = {before}, after = {after} (should have been equal)\")\n", "dev/archery/archery/integration/tester_js.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nfrom pathlib import Path\n\nfrom .tester import Tester\nfrom .util import run_cmd, log\n\n\nARROW_BUILD_ROOT = os.environ.get(\n    'ARROW_BUILD_ROOT',\n    Path(__file__).resolve().parents[4]\n)\nARROW_JS_ROOT = os.path.join(ARROW_BUILD_ROOT, 'js')\n_EXE_PATH = os.path.join(ARROW_JS_ROOT, 'bin')\n_VALIDATE = os.path.join(_EXE_PATH, 'integration.ts')\n_JSON_TO_ARROW = os.path.join(_EXE_PATH, 'json-to-arrow.ts')\n_STREAM_TO_FILE = os.path.join(_EXE_PATH, 'stream-to-file.ts')\n_FILE_TO_STREAM = os.path.join(_EXE_PATH, 'file-to-stream.ts')\n\n\nclass JSTester(Tester):\n    PRODUCER = True\n    CONSUMER = True\n\n    name = 'JS'\n\n    def _run(self, exe_cmd, arrow_path=None, json_path=None,\n             command='VALIDATE'):\n        cmd = [exe_cmd]\n\n        if arrow_path is not None:\n            cmd.extend(['-a', arrow_path])\n\n        if json_path is not None:\n            cmd.extend(['-j', json_path])\n\n        cmd.extend(['--mode', command])\n\n        if self.debug:\n            log(' '.join(cmd))\n\n        run_cmd(cmd, cwd=ARROW_JS_ROOT)\n\n    def validate(self, json_path, arrow_path, quirks=None):\n        return self._run(_VALIDATE, arrow_path, json_path, 'VALIDATE')\n\n    def json_to_file(self, json_path, arrow_path):\n        cmd = [_JSON_TO_ARROW,\n               '-a', arrow_path,\n               '-j', json_path]\n        self.run_shell_command(cmd, cwd=ARROW_JS_ROOT)\n\n    def stream_to_file(self, stream_path, file_path):\n        cmd = [_STREAM_TO_FILE,\n               '<', stream_path,\n               '>', file_path]\n        self.run_shell_command(cmd, cwd=ARROW_JS_ROOT)\n\n    def file_to_stream(self, file_path, stream_path):\n        cmd = [_FILE_TO_STREAM,\n               '<', file_path,\n               '>', stream_path]\n        self.run_shell_command(cmd, cwd=ARROW_JS_ROOT)\n", "dev/archery/archery/integration/tester_rust.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport functools\nimport os\nimport subprocess\n\nfrom . import cdata\nfrom .tester import Tester, CDataExporter, CDataImporter\nfrom .util import run_cmd, log\nfrom ..utils.source import ARROW_ROOT_DEFAULT\n\n\n_EXE_PATH = os.environ.get(\n    \"ARROW_RUST_EXE_PATH\", os.path.join(ARROW_ROOT_DEFAULT, \"rust/target/debug\")\n)\n_INTEGRATION_EXE = os.path.join(_EXE_PATH, \"arrow-json-integration-test\")\n_STREAM_TO_FILE = os.path.join(_EXE_PATH, \"arrow-stream-to-file\")\n_FILE_TO_STREAM = os.path.join(_EXE_PATH, \"arrow-file-to-stream\")\n\n_FLIGHT_SERVER_CMD = [os.path.join(\n    _EXE_PATH, \"flight-test-integration-server\")]\n_FLIGHT_CLIENT_CMD = [\n    os.path.join(_EXE_PATH, \"flight-test-integration-client\"),\n    \"--host\",\n    \"localhost\",\n]\n\n_INTEGRATION_DLL = os.path.join(_EXE_PATH,\n                                \"libarrow_integration_testing\" + cdata.dll_suffix)\n\n\nclass RustTester(Tester):\n    PRODUCER = True\n    CONSUMER = True\n    FLIGHT_SERVER = True\n    FLIGHT_CLIENT = True\n    C_DATA_SCHEMA_EXPORTER = True\n    C_DATA_ARRAY_EXPORTER = True\n    C_DATA_SCHEMA_IMPORTER = True\n    C_DATA_ARRAY_IMPORTER = True\n\n    name = 'Rust'\n\n    def _run(self, arrow_path=None, json_path=None, command='VALIDATE'):\n        cmd = [_INTEGRATION_EXE, '--integration']\n\n        if arrow_path is not None:\n            cmd.append('--arrow=' + arrow_path)\n\n        if json_path is not None:\n            cmd.append('--json=' + json_path)\n\n        cmd.append('--mode=' + command)\n\n        if self.debug:\n            log(' '.join(cmd))\n\n        run_cmd(cmd)\n\n    def validate(self, json_path, arrow_path, quirks=None):\n        return self._run(arrow_path, json_path, 'VALIDATE')\n\n    def json_to_file(self, json_path, arrow_path):\n        return self._run(arrow_path, json_path, 'JSON_TO_ARROW')\n\n    def stream_to_file(self, stream_path, file_path):\n        cmd = [_STREAM_TO_FILE, '<', stream_path, '>', file_path]\n        self.run_shell_command(cmd)\n\n    def file_to_stream(self, file_path, stream_path):\n        cmd = [_FILE_TO_STREAM, file_path, '>', stream_path]\n        self.run_shell_command(cmd)\n\n    @contextlib.contextmanager\n    def flight_server(self, scenario_name=None):\n        cmd = _FLIGHT_SERVER_CMD + ['--port=0']\n        if scenario_name:\n            cmd = cmd + ['--scenario', scenario_name]\n        if self.debug:\n            log(' '.join(cmd))\n        server = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        try:\n            output = server.stdout.readline().decode()\n            if not output.startswith('Server listening on localhost:'):\n                server.kill()\n                out, err = server.communicate()\n                raise RuntimeError(\n                    'Flight-Rust server did not start properly, '\n                    'stdout:\\n{}\\n\\nstderr:\\n{}\\n'.format(\n                        output + out.decode(), err.decode()\n                    )\n                )\n            port = int(output.split(':')[1])\n            yield port\n        finally:\n            server.kill()\n            server.wait(5)\n\n    def flight_request(self, port, json_path=None, scenario_name=None):\n        cmd = _FLIGHT_CLIENT_CMD + [f'--port={port}']\n        if json_path:\n            cmd.extend(('--path', json_path))\n        elif scenario_name:\n            cmd.extend(('--scenario', scenario_name))\n        else:\n            raise TypeError('Must provide one of json_path or scenario_name')\n\n        if self.debug:\n            log(' '.join(cmd))\n        run_cmd(cmd)\n\n    def make_c_data_exporter(self):\n        return RustCDataExporter(self.debug, self.args)\n\n    def make_c_data_importer(self):\n        return RustCDataImporter(self.debug, self.args)\n\n\n_rust_c_data_entrypoints = \"\"\"\n    const char* arrow_rs_cdata_integration_export_schema_from_json(\n        const char* json_path, uintptr_t out);\n    const char* arrow_rs_cdata_integration_import_schema_and_compare_to_json(\n        const char* json_path, uintptr_t c_schema);\n\n    const char* arrow_rs_cdata_integration_export_batch_from_json(\n        const char* json_path, int num_batch, uintptr_t out);\n    const char* arrow_rs_cdata_integration_import_batch_and_compare_to_json(\n        const char* json_path, int num_batch, uintptr_t c_array);\n\n    void arrow_rs_free_error(const char*);\n    \"\"\"\n\n\n@functools.lru_cache\ndef _load_ffi(ffi, lib_path=_INTEGRATION_DLL):\n    ffi.cdef(_rust_c_data_entrypoints)\n    dll = ffi.dlopen(lib_path)\n    return dll\n\n\nclass _CDataBase:\n\n    def __init__(self, debug, args):\n        self.debug = debug\n        self.args = args\n        self.ffi = cdata.ffi()\n        self.dll = _load_ffi(self.ffi)\n\n    def _pointer_to_int(self, c_ptr):\n        return self.ffi.cast('uintptr_t', c_ptr)\n\n    def _check_rust_error(self, rs_error):\n        \"\"\"\n        Check a `const char*` error return from an integration entrypoint.\n\n        A null means success, a non-empty string is an error message.\n        The string is dynamically allocated on the Rust side.\n        \"\"\"\n        assert self.ffi.typeof(rs_error) is self.ffi.typeof(\"const char*\")\n        if rs_error != self.ffi.NULL:\n            try:\n                error = self.ffi.string(rs_error).decode(\n                    'utf8', errors='replace')\n                raise RuntimeError(\n                    f\"Rust C Data Integration call failed: {error}\")\n            finally:\n                self.dll.arrow_rs_free_error(rs_error)\n\n\nclass RustCDataExporter(CDataExporter, _CDataBase):\n\n    def export_schema_from_json(self, json_path, c_schema_ptr):\n        rs_error = self.dll.arrow_rs_cdata_integration_export_schema_from_json(\n            str(json_path).encode(), self._pointer_to_int(c_schema_ptr))\n        self._check_rust_error(rs_error)\n\n    def export_batch_from_json(self, json_path, num_batch, c_array_ptr):\n        rs_error = self.dll.arrow_rs_cdata_integration_export_batch_from_json(\n            str(json_path).encode(), num_batch,\n            self._pointer_to_int(c_array_ptr))\n        self._check_rust_error(rs_error)\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n\n    def record_allocation_state(self):\n        # FIXME we should track the amount of Rust-allocated memory (GH-38822)\n        return 0\n\n\nclass RustCDataImporter(CDataImporter, _CDataBase):\n\n    def import_schema_and_compare_to_json(self, json_path, c_schema_ptr):\n        rs_error = \\\n            self.dll.arrow_rs_cdata_integration_import_schema_and_compare_to_json(\n                str(json_path).encode(), self._pointer_to_int(c_schema_ptr))\n        self._check_rust_error(rs_error)\n\n    def import_batch_and_compare_to_json(self, json_path, num_batch,\n                                         c_array_ptr):\n        rs_error = \\\n            self.dll.arrow_rs_cdata_integration_import_batch_and_compare_to_json(\n                str(json_path).encode(), num_batch, self._pointer_to_int(c_array_ptr))\n        self._check_rust_error(rs_error)\n\n    @property\n    def supports_releasing_memory(self):\n        return True\n", "dev/archery/archery/integration/runner.py": "# licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import namedtuple\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nimport glob\nimport gzip\nimport itertools\nimport os\nimport sys\nimport tempfile\nimport traceback\nfrom typing import Callable, List, Optional\n\nfrom . import cdata\nfrom .scenario import Scenario\nfrom .tester import Tester, CDataExporter, CDataImporter\nfrom .tester_cpp import CppTester\nfrom .tester_go import GoTester\nfrom .tester_rust import RustTester\nfrom .tester_java import JavaTester\nfrom .tester_js import JSTester\nfrom .tester_csharp import CSharpTester\nfrom .tester_nanoarrow import NanoarrowTester\nfrom .util import guid, printer\nfrom .util import SKIP_C_ARRAY, SKIP_C_SCHEMA, SKIP_FLIGHT, SKIP_IPC\nfrom ..utils.source import ARROW_ROOT_DEFAULT\nfrom . import datagen\n\n\nFailure = namedtuple('Failure',\n                     ('test_case', 'producer', 'consumer', 'exc_info'))\n\nlog = printer.print\n\n\nclass Outcome:\n    def __init__(self):\n        self.failure = None\n        self.skipped = False\n\n\nclass IntegrationRunner(object):\n\n    def __init__(self, json_files,\n                 flight_scenarios: List[Scenario],\n                 testers: List[Tester], tempdir=None,\n                 debug=False, stop_on_error=True, gold_dirs=None,\n                 serial=False, match=None, **unused_kwargs):\n        self.json_files = json_files\n        self.flight_scenarios = flight_scenarios\n        self.testers = testers\n        self.temp_dir = tempdir or tempfile.mkdtemp()\n        self.debug = debug\n        self.stop_on_error = stop_on_error\n        self.serial = serial\n        self.gold_dirs = gold_dirs\n        self.failures: List[Outcome] = []\n        self.skips: List[Outcome] = []\n        self.match = match\n\n        if self.match is not None:\n            print(\"-- Only running tests with {} in their name\"\n                  .format(self.match))\n            self.json_files = [json_file for json_file in self.json_files\n                               if self.match in json_file.name]\n\n    def run_ipc(self):\n        \"\"\"\n        Run Arrow IPC integration tests for the matrix of enabled\n        implementations.\n        \"\"\"\n        for producer, consumer in itertools.product(\n                filter(lambda t: t.PRODUCER, self.testers),\n                filter(lambda t: t.CONSUMER, self.testers)):\n            self._compare_ipc_implementations(\n                producer, consumer, self._produce_consume,\n                self.json_files)\n        if self.gold_dirs:\n            for gold_dir, consumer in itertools.product(\n                    self.gold_dirs,\n                    filter(lambda t: t.CONSUMER, self.testers)):\n                log('\\n')\n                log('******************************************************')\n                log('Tests against golden files in {}'.format(gold_dir))\n                log('******************************************************')\n\n                def run_gold(_, consumer, test_case: datagen.File):\n                    return self._run_gold(gold_dir, consumer, test_case)\n                self._compare_ipc_implementations(\n                    consumer, consumer, run_gold,\n                    self._gold_tests(gold_dir))\n        log('\\n')\n\n    def run_flight(self):\n        \"\"\"\n        Run Arrow Flight integration tests for the matrix of enabled\n        implementations.\n        \"\"\"\n        servers = filter(lambda t: t.FLIGHT_SERVER, self.testers)\n        clients = filter(lambda t: (t.FLIGHT_CLIENT and t.CONSUMER),\n                         self.testers)\n        for server, client in itertools.product(servers, clients):\n            self._compare_flight_implementations(server, client)\n        log('\\n')\n\n    def run_c_data(self):\n        \"\"\"\n        Run Arrow C Data interface integration tests for the matrix of\n        enabled implementations.\n        \"\"\"\n        for producer, consumer in itertools.product(\n                filter(lambda t: t.C_DATA_SCHEMA_EXPORTER, self.testers),\n                filter(lambda t: t.C_DATA_SCHEMA_IMPORTER, self.testers)):\n            self._compare_c_data_implementations(producer, consumer)\n        log('\\n')\n\n    def _gold_tests(self, gold_dir):\n        prefix = os.path.basename(os.path.normpath(gold_dir))\n        SUFFIX = \".json.gz\"\n        golds = [jf for jf in os.listdir(gold_dir) if jf.endswith(SUFFIX)]\n        for json_path in golds:\n            name = json_path[json_path.index('_')+1: -len(SUFFIX)]\n            base_name = prefix + \"_\" + name + \".gold.json\"\n            out_path = os.path.join(self.temp_dir, base_name)\n            with gzip.open(os.path.join(gold_dir, json_path)) as i:\n                with open(out_path, \"wb\") as out:\n                    out.write(i.read())\n\n            # Find the generated file with the same name as this gold file\n            try:\n                equiv_json_file = next(f for f in self.json_files\n                                       if f.name == name)\n            except StopIteration:\n                equiv_json_file = None\n\n            skip_testers = set()\n            if name == 'union' and prefix == '0.17.1':\n                skip_testers.add(\"Java\")\n                skip_testers.add(\"JS\")\n            if prefix == '1.0.0-bigendian' or prefix == '1.0.0-littleendian':\n                skip_testers.add(\"C#\")\n                skip_testers.add(\"Java\")\n                skip_testers.add(\"JS\")\n                skip_testers.add(\"Rust\")\n            if prefix == '2.0.0-compression':\n                skip_testers.add(\"JS\")\n\n            # See https://github.com/apache/arrow/pull/9822 for how to\n            # disable specific compression type tests.\n\n            if prefix == '4.0.0-shareddict':\n                skip_testers.add(\"C#\")\n\n            quirks = set()\n            if prefix in {'0.14.1', '0.17.1',\n                          '1.0.0-bigendian', '1.0.0-littleendian'}:\n                # ARROW-13558: older versions generated decimal values that\n                # were out of range for the given precision.\n                quirks.add(\"no_decimal_validate\")\n                quirks.add(\"no_date64_validate\")\n                quirks.add(\"no_times_validate\")\n\n            json_file = datagen.File(name, schema=None, batches=None,\n                                     path=out_path,\n                                     skip_testers=skip_testers,\n                                     quirks=quirks)\n            if equiv_json_file is not None:\n                json_file.add_skips_from(equiv_json_file)\n            yield json_file\n\n    def _run_test_cases(self,\n                        case_runner: Callable[[datagen.File], Outcome],\n                        test_cases: List[datagen.File],\n                        *, serial: Optional[bool] = None) -> None:\n        \"\"\"\n        Populate self.failures with the outcomes of the\n        ``case_runner`` ran against ``test_cases``\n        \"\"\"\n        def case_wrapper(test_case):\n            if serial:\n                return case_runner(test_case)\n            with printer.cork():\n                return case_runner(test_case)\n\n        if serial is None:\n            serial = self.serial\n\n        if self.failures and self.stop_on_error:\n            return\n\n        if serial:\n            for outcome in map(case_wrapper, test_cases):\n                if outcome.failure is not None:\n                    self.failures.append(outcome.failure)\n                    if self.stop_on_error:\n                        break\n                elif outcome.skipped:\n                    self.skips.append(outcome)\n\n        else:\n            with ThreadPoolExecutor() as executor:\n                for outcome in executor.map(case_wrapper, test_cases):\n                    if outcome.failure is not None:\n                        self.failures.append(outcome.failure)\n                        if self.stop_on_error:\n                            break\n                    elif outcome.skipped:\n                        self.skips.append(outcome)\n\n    def _compare_ipc_implementations(\n        self,\n        producer: Tester,\n        consumer: Tester,\n        run_binaries: Callable[[Tester, Tester, datagen.File], None],\n        test_cases: List[datagen.File]\n    ):\n        \"\"\"\n        Compare Arrow IPC for two implementations (one producer, one consumer).\n        \"\"\"\n        log('##########################################################')\n        log('IPC: {0} producing, {1} consuming'\n            .format(producer.name, consumer.name))\n        log('##########################################################')\n\n        case_runner = partial(self._run_ipc_test_case,\n                              producer, consumer, run_binaries)\n        self._run_test_cases(case_runner, test_cases)\n\n    def _run_ipc_test_case(\n        self,\n        producer: Tester,\n        consumer: Tester,\n        run_binaries: Callable[[Tester, Tester, datagen.File], None],\n        test_case: datagen.File,\n    ) -> Outcome:\n        \"\"\"\n        Run one IPC test case.\n        \"\"\"\n        outcome = Outcome()\n\n        json_path = test_case.path\n        log('=' * 70)\n        log('Testing file {0}'.format(json_path))\n\n        if test_case.should_skip(producer.name, SKIP_IPC):\n            log(f'-- Skipping test because producer {producer.name} does '\n                f'not support IPC')\n            outcome.skipped = True\n\n        elif test_case.should_skip(consumer.name, SKIP_IPC):\n            log(f'-- Skipping test because consumer {consumer.name} does '\n                f'not support IPC')\n            outcome.skipped = True\n\n        else:\n            try:\n                run_binaries(producer, consumer, test_case)\n            except Exception:\n                traceback.print_exc(file=printer.stdout)\n                outcome.failure = Failure(test_case, producer, consumer,\n                                          sys.exc_info())\n\n        log('=' * 70)\n\n        return outcome\n\n    def _produce_consume(self,\n                         producer: Tester,\n                         consumer: Tester,\n                         test_case: datagen.File\n                         ) -> None:\n        \"\"\"\n        Given a producer and a consumer, run different combination of\n        tests for the ``test_case``\n        * read and write are consistent\n        * stream to file is consistent\n        \"\"\"\n        # Make the random access file\n        json_path = test_case.path\n        file_id = guid()[:8]\n        name = os.path.splitext(os.path.basename(json_path))[0]\n\n        producer_file_path = os.path.join(self.temp_dir, file_id + '_' +\n                                          name + '.json_as_file')\n        producer_stream_path = os.path.join(self.temp_dir, file_id + '_' +\n                                            name + '.producer_file_as_stream')\n        consumer_file_path = os.path.join(self.temp_dir, file_id + '_' +\n                                          name + '.consumer_stream_as_file')\n\n        log('-- Creating binary inputs')\n        producer.json_to_file(json_path, producer_file_path)\n\n        # Validate the file\n        log('-- Validating file')\n        consumer.validate(json_path, producer_file_path)\n\n        log('-- Validating stream')\n        producer.file_to_stream(producer_file_path, producer_stream_path)\n        consumer.stream_to_file(producer_stream_path, consumer_file_path)\n        consumer.validate(json_path, consumer_file_path)\n\n    def _run_gold(self,\n                  gold_dir: str,\n                  consumer: Tester,\n                  test_case: datagen.File) -> None:\n        \"\"\"\n        Given a directory with:\n        * an ``.arrow_file``\n        * a ``.stream``\n        associated to the json integration file at ``test_case.path``\n\n        verify that the consumer can read both and agrees with\n        what the json file contains; also run ``stream_to_file`` and\n        verify that the consumer produces an equivalent file from the\n        IPC stream.\n        \"\"\"\n        json_path = test_case.path\n\n        # Validate the file\n        log('-- Validating file')\n        producer_file_path = os.path.join(\n            gold_dir, \"generated_\" + test_case.name + \".arrow_file\")\n        consumer.validate(json_path, producer_file_path,\n                          quirks=test_case.quirks)\n\n        log('-- Validating stream')\n        consumer_stream_path = os.path.join(\n            gold_dir, \"generated_\" + test_case.name + \".stream\")\n        file_id = guid()[:8]\n        name = os.path.splitext(os.path.basename(json_path))[0]\n\n        consumer_file_path = os.path.join(self.temp_dir, file_id + '_' +\n                                          name + '.consumer_stream_as_file')\n\n        consumer.stream_to_file(consumer_stream_path, consumer_file_path)\n        consumer.validate(json_path, consumer_file_path,\n                          quirks=test_case.quirks)\n\n    def _compare_flight_implementations(\n        self,\n        producer: Tester,\n        consumer: Tester\n    ):\n        log('##########################################################')\n        log('Flight: {0} serving, {1} requesting'\n            .format(producer.name, consumer.name))\n        log('##########################################################')\n\n        case_runner = partial(self._run_flight_test_case, producer, consumer)\n        self._run_test_cases(\n            case_runner, self.json_files + self.flight_scenarios)\n\n    def _run_flight_test_case(self,\n                              producer: Tester,\n                              consumer: Tester,\n                              test_case: datagen.File) -> Outcome:\n        \"\"\"\n        Run one Flight test case.\n        \"\"\"\n        outcome = Outcome()\n\n        log('=' * 70)\n        log('Testing file {0}'.format(test_case.name))\n\n        if test_case.should_skip(producer.name, SKIP_FLIGHT):\n            log(f'-- Skipping test because producer {producer.name} does '\n                f'not support Flight')\n            outcome.skipped = True\n\n        elif test_case.should_skip(consumer.name, SKIP_FLIGHT):\n            log(f'-- Skipping test because consumer {consumer.name} does '\n                f'not support Flight')\n            outcome.skipped = True\n\n        else:\n            try:\n                if isinstance(test_case, Scenario):\n                    server = producer.flight_server(test_case.name)\n                    client_args = {'scenario_name': test_case.name}\n                else:\n                    server = producer.flight_server()\n                    client_args = {'json_path': test_case.path}\n\n                with server as port:\n                    # Have the client upload the file, then download and\n                    # compare\n                    consumer.flight_request(port, **client_args)\n            except Exception:\n                traceback.print_exc(file=printer.stdout)\n                outcome.failure = Failure(test_case, producer, consumer,\n                                          sys.exc_info())\n\n        log('=' * 70)\n\n        return outcome\n\n    def _compare_c_data_implementations(\n        self,\n        producer: Tester,\n        consumer: Tester\n    ):\n        log('##########################################################')\n        log(f'C Data Interface: '\n            f'{producer.name} exporting, {consumer.name} importing')\n        log('##########################################################')\n\n        # Serial execution is required for proper memory accounting\n        serial = True\n\n        with producer.make_c_data_exporter() as exporter:\n            with consumer.make_c_data_importer() as importer:\n                case_runner = partial(self._run_c_schema_test_case,\n                                      producer, consumer,\n                                      exporter, importer)\n                self._run_test_cases(case_runner, self.json_files, serial=serial)\n\n                if producer.C_DATA_ARRAY_EXPORTER and consumer.C_DATA_ARRAY_IMPORTER:\n                    case_runner = partial(self._run_c_array_test_cases,\n                                          producer, consumer,\n                                          exporter, importer)\n                    self._run_test_cases(case_runner, self.json_files, serial=serial)\n\n    def _run_c_schema_test_case(self,\n                                producer: Tester, consumer: Tester,\n                                exporter: CDataExporter,\n                                importer: CDataImporter,\n                                test_case: datagen.File) -> Outcome:\n        \"\"\"\n        Run one C ArrowSchema test case.\n        \"\"\"\n        outcome = Outcome()\n\n        def do_run():\n            json_path = test_case.path\n            ffi = cdata.ffi()\n            c_schema_ptr = ffi.new(\"struct ArrowSchema*\")\n            with cdata.check_memory_released(exporter, importer):\n                exporter.export_schema_from_json(json_path, c_schema_ptr)\n                importer.import_schema_and_compare_to_json(json_path, c_schema_ptr)\n\n        log('=' * 70)\n        log(f'Testing C ArrowSchema from file {test_case.name!r}')\n\n        if test_case.should_skip(producer.name, SKIP_C_SCHEMA):\n            log(f'-- Skipping test because producer {producer.name} does '\n                f'not support C ArrowSchema')\n            outcome.skipped = True\n\n        elif test_case.should_skip(consumer.name, SKIP_C_SCHEMA):\n            log(f'-- Skipping test because consumer {consumer.name} does '\n                f'not support C ArrowSchema')\n            outcome.skipped = True\n\n        else:\n            try:\n                do_run()\n            except Exception:\n                traceback.print_exc(file=printer.stdout)\n                outcome.failure = Failure(test_case, producer, consumer,\n                                          sys.exc_info())\n\n        log('=' * 70)\n\n        return outcome\n\n    def _run_c_array_test_cases(self,\n                                producer: Tester, consumer: Tester,\n                                exporter: CDataExporter,\n                                importer: CDataImporter,\n                                test_case: datagen.File) -> Outcome:\n        \"\"\"\n        Run one set C ArrowArray test cases.\n        \"\"\"\n        outcome = Outcome()\n\n        def do_run():\n            json_path = test_case.path\n            ffi = cdata.ffi()\n            c_array_ptr = ffi.new(\"struct ArrowArray*\")\n            for num_batch in range(test_case.num_batches):\n                log(f'... with record batch #{num_batch}')\n                with cdata.check_memory_released(exporter, importer):\n                    exporter.export_batch_from_json(json_path,\n                                                    num_batch,\n                                                    c_array_ptr)\n                    importer.import_batch_and_compare_to_json(json_path,\n                                                              num_batch,\n                                                              c_array_ptr)\n\n        log('=' * 70)\n        log(f'Testing C ArrowArray '\n            f'from file {test_case.name!r}')\n\n        if test_case.should_skip(producer.name, SKIP_C_ARRAY):\n            log(f'-- Skipping test because producer {producer.name} does '\n                f'not support C ArrowArray')\n            outcome.skipped = True\n\n        elif test_case.should_skip(consumer.name, SKIP_C_ARRAY):\n            log(f'-- Skipping test because consumer {consumer.name} does '\n                f'not support C ArrowArray')\n            outcome.skipped = True\n\n        else:\n            try:\n                do_run()\n            except Exception:\n                traceback.print_exc(file=printer.stdout)\n                outcome.failure = Failure(test_case, producer, consumer,\n                                          sys.exc_info())\n\n        log('=' * 70)\n\n        return outcome\n\n\ndef get_static_json_files():\n    glob_pattern = os.path.join(ARROW_ROOT_DEFAULT,\n                                'integration', 'data', '*.json')\n    return [\n        datagen.File(name=os.path.basename(p), path=p,\n                     schema=None, batches=None)\n        for p in glob.glob(glob_pattern)\n    ]\n\n\ndef run_all_tests(with_cpp=True, with_java=True, with_js=True,\n                  with_csharp=True, with_go=True, with_rust=False,\n                  with_nanoarrow=False, run_ipc=False, run_flight=False,\n                  run_c_data=False, tempdir=None, **kwargs):\n    tempdir = tempdir or tempfile.mkdtemp(prefix='arrow-integration-')\n\n    testers: List[Tester] = []\n\n    if with_cpp:\n        testers.append(CppTester(**kwargs))\n\n    if with_java:\n        testers.append(JavaTester(**kwargs))\n\n    if with_js:\n        testers.append(JSTester(**kwargs))\n\n    if with_csharp:\n        testers.append(CSharpTester(**kwargs))\n\n    if with_go:\n        testers.append(GoTester(**kwargs))\n\n    if with_nanoarrow:\n        testers.append(NanoarrowTester(**kwargs))\n\n    if with_rust:\n        testers.append(RustTester(**kwargs))\n\n    static_json_files = get_static_json_files()\n    generated_json_files = datagen.get_generated_json_files(tempdir=tempdir)\n    json_files = static_json_files + generated_json_files\n\n    # Additional integration test cases for Arrow Flight.\n    flight_scenarios = [\n        Scenario(\n            \"auth:basic_proto\",\n            description=\"Authenticate using the BasicAuth protobuf.\"),\n        Scenario(\n            \"middleware\",\n            description=\"Ensure headers are propagated via middleware.\",\n        ),\n        Scenario(\n            \"ordered\",\n            description=\"Ensure FlightInfo.ordered is supported.\",\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"expiration_time:do_get\",\n            description=(\"Ensure FlightEndpoint.expiration_time with \"\n                         \"DoGet is working as expected.\"),\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"expiration_time:list_actions\",\n            description=(\"Ensure FlightEndpoint.expiration_time related \"\n                         \"pre-defined actions is working with ListActions \"\n                         \"as expected.\"),\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"expiration_time:cancel_flight_info\",\n            description=(\"Ensure FlightEndpoint.expiration_time and \"\n                         \"CancelFlightInfo are working as expected.\"),\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"expiration_time:renew_flight_endpoint\",\n            description=(\"Ensure FlightEndpoint.expiration_time and \"\n                         \"RenewFlightEndpoint are working as expected.\"),\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"location:reuse_connection\",\n            description=\"Ensure arrow-flight-reuse-connection is accepted.\",\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"session_options\",\n            description=\"Ensure Flight SQL Sessions work as expected.\",\n            skip_testers={\"JS\", \"C#\", \"Rust\"}\n        ),\n        Scenario(\n            \"poll_flight_info\",\n            description=\"Ensure PollFlightInfo is supported.\",\n            skip_testers={\"JS\", \"C#\", \"Rust\"}\n        ),\n        Scenario(\n            \"app_metadata_flight_info_endpoint\",\n            description=\"Ensure support FlightInfo and Endpoint app_metadata\",\n            skip_testers={\"JS\", \"C#\", \"Rust\"}\n        ),\n        Scenario(\n            \"flight_sql\",\n            description=\"Ensure Flight SQL protocol is working as expected.\",\n            skip_testers={\"Rust\"}\n        ),\n        Scenario(\n            \"flight_sql:extension\",\n            description=\"Ensure Flight SQL extensions work as expected.\",\n            skip_testers={\"Rust\"}\n        ),\n        Scenario(\n            \"flight_sql:ingestion\",\n            description=\"Ensure Flight SQL ingestion works as expected.\",\n            skip_testers={\"JS\", \"C#\", \"Rust\", \"Java\"}\n        ),\n    ]\n\n    runner = IntegrationRunner(json_files, flight_scenarios, testers, **kwargs)\n    if run_ipc:\n        runner.run_ipc()\n    if run_flight:\n        runner.run_flight()\n    if run_c_data:\n        runner.run_c_data()\n\n    fail_count = 0\n    if runner.failures:\n        log(\"################# FAILURES #################\")\n        for test_case, producer, consumer, exc_info in runner.failures:\n            fail_count += 1\n            log(\"FAILED TEST:\", end=\" \")\n            log(test_case.name, producer.name, \"producing, \",\n                consumer.name, \"consuming\")\n            if exc_info:\n                exc_type, exc_value, exc_tb = exc_info\n                log(f'{exc_type}: {exc_value}')\n            log()\n\n    log(f\"{fail_count} failures, {len(runner.skips)} skips\")\n    if fail_count > 0:\n        sys.exit(1)\n\n\ndef write_js_test_json(directory):\n    datagen.generate_primitive_case([], name='primitive_no_batches').write(\n        os.path.join(directory, 'primitive-no-batches.json')\n    )\n    datagen.generate_primitive_case([17, 20], name='primitive').write(\n        os.path.join(directory, 'primitive.json')\n    )\n    datagen.generate_primitive_case([0, 0, 0], name='primitive_zerolength').write(\n        os.path.join(directory, 'primitive-empty.json')\n    )\n    # datagen.generate_primitive_large_offsets_case([17, 20]).write(\n    #     os.path.join(directory, 'primitive-large-offsets.json')\n    # )\n    datagen.generate_null_case([10, 0]).write(\n        os.path.join(directory, 'null.json')\n    )\n    datagen.generate_null_trivial_case([0, 0]).write(\n        os.path.join(directory, 'null-trivial.json')\n    )\n    datagen.generate_decimal128_case().write(\n        os.path.join(directory, 'decimal128.json')\n    )\n    # datagen.generate_decimal256_case().write(\n    #     os.path.join(directory, 'decimal256.json')\n    # )\n    datagen.generate_datetime_case().write(\n        os.path.join(directory, 'datetime.json')\n    )\n    # datagen.generate_duration_case().write(\n    #     os.path.join(directory, 'duration.json')\n    # )\n    # datagen.generate_interval_case().write(\n    #     os.path.join(directory, 'interval.json')\n    # )\n    # datagen.generate_month_day_nano_interval_case().write(\n    #     os.path.join(directory, 'month_day_nano_interval.json')\n    # )\n    datagen.generate_map_case().write(\n        os.path.join(directory, 'map.json')\n    )\n    datagen.generate_non_canonical_map_case().write(\n        os.path.join(directory, 'non_canonical_map.json')\n    )\n    datagen.generate_nested_case().write(\n        os.path.join(directory, 'nested.json')\n    )\n    datagen.generate_recursive_nested_case().write(\n        os.path.join(directory, 'recursive-nested.json')\n    )\n    # datagen.generate_nested_large_offsets_case().write(\n    #     os.path.join(directory, 'nested-large-offsets.json')\n    # )\n    datagen.generate_unions_case().write(\n        os.path.join(directory, 'unions.json')\n    )\n    datagen.generate_custom_metadata_case().write(\n        os.path.join(directory, 'custom-metadata.json')\n    )\n    # datagen.generate_duplicate_fieldnames_case().write(\n    #     os.path.join(directory, 'duplicate-fieldnames.json')\n    # )\n    datagen.generate_dictionary_case().write(\n        os.path.join(directory, 'dictionary.json')\n    )\n    datagen.generate_dictionary_unsigned_case().write(\n        os.path.join(directory, 'dictionary-unsigned.json')\n    )\n    datagen.generate_nested_dictionary_case().write(\n        os.path.join(directory, 'dictionary-nested.json')\n    )\n    # datagen.generate_run_end_encoded_case().write(\n    #     os.path.join(directory, 'run_end_encoded.json')\n    # )\n    datagen.generate_extension_case().write(\n        os.path.join(directory, 'extension.json')\n    )\n", "dev/archery/archery/integration/scenario.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nclass Scenario:\n    \"\"\"\n    An integration test scenario for Arrow Flight.\n\n    Does not correspond to a particular IPC JSON file.\n    \"\"\"\n\n    def __init__(self, name, description, skip_testers=None):\n        self.name = name\n        self.description = description\n        self.skipped_testers = skip_testers or set()\n\n    def should_skip(self, tester, format):\n        return tester in self.skipped_testers\n", "dev/archery/archery/integration/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "dev/archery/archery/integration/datagen.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import namedtuple, OrderedDict\nimport binascii\nimport json\nimport os\nimport random\nimport tempfile\n\nimport numpy as np\n\nfrom .util import frombytes, tobytes, random_bytes, random_utf8\nfrom .util import SKIP_C_SCHEMA, SKIP_C_ARRAY\n\n\ndef metadata_key_values(pairs):\n    return [{'key': k, 'value': v} for k, v in pairs]\n\n\nclass Field(object):\n\n    def __init__(self, name, *, nullable=True, metadata=None):\n        self.name = name\n        self.nullable = nullable\n        self.metadata = metadata or []\n\n    def get_json(self):\n        entries = [\n            ('name', self.name),\n            ('type', self._get_type()),\n            ('nullable', self.nullable),\n            ('children', self._get_children()),\n        ]\n\n        dct = self._get_dictionary()\n        if dct:\n            entries.append(('dictionary', dct))\n\n        if self.metadata is not None and len(self.metadata) > 0:\n            entries.append(('metadata', metadata_key_values(self.metadata)))\n\n        return OrderedDict(entries)\n\n    def _get_dictionary(self):\n        return None\n\n    def _make_is_valid(self, size, null_probability=0.4):\n        if self.nullable:\n            return (np.random.random_sample(size) > null_probability\n                    ).astype(np.int8)\n        else:\n            return np.ones(size, dtype=np.int8)\n\n\nclass Column(object):\n\n    def __init__(self, name, count):\n        self.name = name\n        self.count = count\n\n    def __len__(self):\n        return self.count\n\n    def _get_children(self):\n        return []\n\n    def _get_buffers(self):\n        return []\n\n    def get_json(self):\n        entries = [\n            ('name', self.name),\n            ('count', self.count)\n        ]\n\n        buffers = self._get_buffers()\n        entries.extend(buffers)\n\n        children = self._get_children()\n        if len(children) > 0:\n            entries.append(('children', children))\n\n        return OrderedDict(entries)\n\n\nclass PrimitiveField(Field):\n\n    def _get_children(self):\n        return []\n\n\nclass PrimitiveColumn(Column):\n\n    def __init__(self, name, count, is_valid, values):\n        super().__init__(name, count)\n        self.is_valid = is_valid\n        self.values = values\n\n    def _encode_value(self, x):\n        return x\n\n    def _get_buffers(self):\n        return [\n            ('VALIDITY', [int(v) for v in self.is_valid]),\n            ('DATA', list([self._encode_value(x) for x in self.values]))\n        ]\n\n\nclass NullColumn(Column):\n    # This subclass is for readability only\n    pass\n\n\nclass NullField(PrimitiveField):\n\n    def __init__(self, name, metadata=None):\n        super().__init__(name, nullable=True,\n                         metadata=metadata)\n\n    def _get_type(self):\n        return OrderedDict([('name', 'null')])\n\n    def generate_column(self, size, name=None):\n        return NullColumn(name or self.name, size)\n\n\nTEST_INT_MAX = 2 ** 31 - 1\nTEST_INT_MIN = ~TEST_INT_MAX\n\n\nclass IntegerField(PrimitiveField):\n\n    def __init__(self, name, is_signed, bit_width, *, nullable=True,\n                 metadata=None,\n                 min_value=TEST_INT_MIN,\n                 max_value=TEST_INT_MAX):\n        super().__init__(name, nullable=nullable,\n                         metadata=metadata)\n        self.is_signed = is_signed\n        self.bit_width = bit_width\n        self.min_value = min_value\n        self.max_value = max_value\n\n    def _get_generated_data_bounds(self):\n        if self.is_signed:\n            signed_iinfo = np.iinfo('int' + str(self.bit_width))\n            min_value, max_value = signed_iinfo.min, signed_iinfo.max\n        else:\n            unsigned_iinfo = np.iinfo('uint' + str(self.bit_width))\n            min_value, max_value = 0, unsigned_iinfo.max\n\n        lower_bound = max(min_value, self.min_value)\n        upper_bound = min(max_value, self.max_value)\n        return lower_bound, upper_bound\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'int'),\n            ('isSigned', self.is_signed),\n            ('bitWidth', self.bit_width)\n        ])\n\n    def generate_column(self, size, name=None):\n        lower_bound, upper_bound = self._get_generated_data_bounds()\n        return self.generate_range(size, lower_bound, upper_bound,\n                                   name=name, include_extremes=True)\n\n    def generate_range(self, size, lower, upper, name=None,\n                       include_extremes=False):\n        values = np.random.randint(lower, upper, size=size, dtype=np.int64)\n        if include_extremes and size >= 2:\n            values[:2] = [lower, upper]\n        values = list(map(int if self.bit_width < 64 else str, values))\n\n        is_valid = self._make_is_valid(size)\n\n        if name is None:\n            name = self.name\n        return PrimitiveColumn(name, size, is_valid, values)\n\n\n# Integer field that fulfils the requirements for the run ends field of REE.\n# The integers are positive and in a strictly increasing sequence\nclass RunEndsField(IntegerField):\n    # bit_width should only be one of 16/32/64\n    def __init__(self, name, bit_width, *, metadata=None):\n        super().__init__(name, is_signed=True, bit_width=bit_width,\n                         nullable=False, metadata=metadata, min_value=1)\n\n    def generate_range(self, size, lower, upper, name=None,\n                       include_extremes=False):\n        rng = np.random.default_rng()\n        # generate values that are strictly increasing with a min-value of\n        # 1, but don't go higher than the max signed value for the given\n        # bit width. We sort the values to ensure they are strictly increasing\n        # and set replace to False to avoid duplicates, ensuring a valid\n        # run-ends array.\n        values = rng.choice(2 ** (self.bit_width - 1) - 1, size=size, replace=False)\n        values += 1\n        values = sorted(values)\n        values = list(map(int if self.bit_width < 64 else str, values))\n        # RunEnds cannot be null, as such self.nullable == False and this\n        # will generate a validity map of all ones.\n        is_valid = self._make_is_valid(size)\n\n        if name is None:\n            name = self.name\n        return PrimitiveColumn(name, size, is_valid, values)\n\n\nclass DateField(IntegerField):\n\n    DAY = 0\n    MILLISECOND = 1\n\n    # 1/1/1 to 12/31/9999\n    _ranges = {\n        DAY: [-719162, 2932896],\n        MILLISECOND: [-62135596800000, 253402214400000]\n    }\n\n    def __init__(self, name, unit, *, nullable=True, metadata=None):\n        bit_width = 32 if unit == self.DAY else 64\n\n        min_value, max_value = self._ranges[unit]\n        super().__init__(\n            name, True, bit_width,\n            nullable=nullable, metadata=metadata,\n            min_value=min_value, max_value=max_value\n        )\n        self.unit = unit\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'date'),\n            ('unit', 'DAY' if self.unit == self.DAY else 'MILLISECOND')\n        ])\n\n    def generate_range(self, size, lower, upper, name=None,\n                       include_extremes=False):\n        if self.unit == self.DAY:\n            return super().generate_range(size, lower, upper, name)\n\n        full_day_millis = 1000 * 60 * 60 * 24\n        lower = -1 * (abs(lower) // full_day_millis)\n        upper //= full_day_millis\n\n        values = [val * full_day_millis for val in np.random.randint(\n            lower, upper, size=size, dtype=np.int64)]\n        lower *= full_day_millis\n        upper *= full_day_millis\n\n        if include_extremes and size >= 2:\n            values[:2] = [lower, upper]\n        values = list(map(int if self.bit_width < 64 else str, values))\n\n        is_valid = self._make_is_valid(size)\n\n        if name is None:\n            name = self.name\n        return PrimitiveColumn(name, size, is_valid, values)\n\n\nTIMEUNIT_NAMES = {\n    's': 'SECOND',\n    'ms': 'MILLISECOND',\n    'us': 'MICROSECOND',\n    'ns': 'NANOSECOND'\n}\n\n\nclass TimeField(IntegerField):\n\n    BIT_WIDTHS = {\n        's': 32,\n        'ms': 32,\n        'us': 64,\n        'ns': 64\n    }\n\n    _ranges = {\n        's': [0, 86400],\n        'ms': [0, 86400000],\n        'us': [0, 86400000000],\n        'ns': [0, 86400000000000]\n    }\n\n    def __init__(self, name, unit='s', *, nullable=True,\n                 metadata=None):\n        min_val, max_val = self._ranges[unit]\n        super().__init__(name, True, self.BIT_WIDTHS[unit],\n                         nullable=nullable, metadata=metadata,\n                         min_value=min_val, max_value=max_val)\n        self.unit = unit\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'time'),\n            ('unit', TIMEUNIT_NAMES[self.unit]),\n            ('bitWidth', self.bit_width)\n        ])\n\n    def generate_column(self, size, name=None):\n        lower_bound, upper_bound = self._get_generated_data_bounds()\n        return self.generate_range(size, lower_bound, upper_bound,\n                                   name=name)\n\n\nclass TimestampField(IntegerField):\n\n    # 1/1/1 to 12/31/9999\n    _ranges = {\n        's': [-62135596800, 253402214400],\n        'ms': [-62135596800000, 253402214400000],\n        'us': [-62135596800000000, 253402214400000000],\n\n        # Physical range for int64, ~584 years and change\n        'ns': [np.iinfo('int64').min, np.iinfo('int64').max]\n    }\n\n    def __init__(self, name, unit='s', tz=None, *, nullable=True,\n                 metadata=None):\n        min_val, max_val = self._ranges[unit]\n        super().__init__(name, True, 64,\n                         nullable=nullable,\n                         metadata=metadata,\n                         min_value=min_val,\n                         max_value=max_val)\n        self.unit = unit\n        self.tz = tz\n\n    def _get_type(self):\n        fields = [\n            ('name', 'timestamp'),\n            ('unit', TIMEUNIT_NAMES[self.unit])\n        ]\n\n        if self.tz is not None:\n            fields.append(('timezone', self.tz))\n\n        return OrderedDict(fields)\n\n\nclass DurationIntervalField(IntegerField):\n\n    def __init__(self, name, unit='s', *, nullable=True,\n                 metadata=None):\n        min_val, max_val = np.iinfo('int64').min, np.iinfo('int64').max,\n        super().__init__(\n            name, True, 64,\n            nullable=nullable, metadata=metadata,\n            min_value=min_val, max_value=max_val)\n        self.unit = unit\n\n    def _get_type(self):\n        fields = [\n            ('name', 'duration'),\n            ('unit', TIMEUNIT_NAMES[self.unit])\n        ]\n\n        return OrderedDict(fields)\n\n\nclass YearMonthIntervalField(IntegerField):\n    def __init__(self, name, *, nullable=True, metadata=None):\n        min_val, max_val = [-10000*12, 10000*12]  # +/- 10000 years.\n        super().__init__(\n            name, True, 32,\n            nullable=nullable, metadata=metadata,\n            min_value=min_val, max_value=max_val)\n\n    def _get_type(self):\n        fields = [\n            ('name', 'interval'),\n            ('unit', 'YEAR_MONTH'),\n        ]\n\n        return OrderedDict(fields)\n\n\nclass DayTimeIntervalField(PrimitiveField):\n    def __init__(self, name, *, nullable=True, metadata=None):\n        super().__init__(name,\n                         nullable=True,\n                         metadata=metadata)\n\n    @property\n    def numpy_type(self):\n        return object\n\n    def _get_type(self):\n\n        return OrderedDict([\n            ('name', 'interval'),\n            ('unit', 'DAY_TIME'),\n        ])\n\n    def generate_column(self, size, name=None):\n        min_day_value, max_day_value = -10000*366, 10000*366\n        values = [{'days': random.randint(min_day_value, max_day_value),\n                   'milliseconds': random.randint(-86400000, +86400000)}\n                  for _ in range(size)]\n\n        is_valid = self._make_is_valid(size)\n        if name is None:\n            name = self.name\n        return PrimitiveColumn(name, size, is_valid, values)\n\n\nclass MonthDayNanoIntervalField(PrimitiveField):\n    def __init__(self, name, *, nullable=True, metadata=None):\n        super().__init__(name,\n                         nullable=True,\n                         metadata=metadata)\n\n    @property\n    def numpy_type(self):\n        return object\n\n    def _get_type(self):\n\n        return OrderedDict([\n            ('name', 'interval'),\n            ('unit', 'MONTH_DAY_NANO'),\n        ])\n\n    def generate_column(self, size, name=None):\n        I32 = 'int32'\n        min_int_value, max_int_value = np.iinfo(I32).min, np.iinfo(I32).max\n        I64 = 'int64'\n        min_nano_val, max_nano_val = np.iinfo(I64).min, np.iinfo(I64).max,\n        values = [{'months': random.randint(min_int_value, max_int_value),\n                   'days': random.randint(min_int_value, max_int_value),\n                   'nanoseconds': random.randint(min_nano_val, max_nano_val)}\n                  for _ in range(size)]\n\n        is_valid = self._make_is_valid(size)\n        if name is None:\n            name = self.name\n        return PrimitiveColumn(name, size, is_valid, values)\n\n\nclass FloatingPointField(PrimitiveField):\n\n    def __init__(self, name, bit_width, *, nullable=True,\n                 metadata=None):\n        super().__init__(name,\n                         nullable=nullable,\n                         metadata=metadata)\n\n        self.bit_width = bit_width\n        self.precision = {\n            16: 'HALF',\n            32: 'SINGLE',\n            64: 'DOUBLE'\n        }[self.bit_width]\n\n    @property\n    def numpy_type(self):\n        return 'float' + str(self.bit_width)\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'floatingpoint'),\n            ('precision', self.precision)\n        ])\n\n    def generate_column(self, size, name=None):\n        values = np.random.randn(size) * 1000\n        values = np.round(values, 3)\n\n        is_valid = self._make_is_valid(size)\n        if name is None:\n            name = self.name\n        return PrimitiveColumn(name, size, is_valid, values)\n\n\ndef decimal_range_from_precision(precision):\n    assert 1 <= precision <= 76\n    max_value = (10 ** precision) - 1\n    return -max_value, max_value\n\n\nclass DecimalField(PrimitiveField):\n    def __init__(self, name, precision, scale, bit_width, *,\n                 nullable=True, metadata=None):\n        super().__init__(name, nullable=True,\n                         metadata=metadata)\n        self.precision = precision\n        self.scale = scale\n        self.bit_width = bit_width\n\n    @property\n    def numpy_type(self):\n        return object\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'decimal'),\n            ('precision', self.precision),\n            ('scale', self.scale),\n            ('bitWidth', self.bit_width),\n        ])\n\n    def generate_column(self, size, name=None):\n        min_value, max_value = decimal_range_from_precision(self.precision)\n        values = [random.randint(min_value, max_value) for _ in range(size)]\n\n        is_valid = self._make_is_valid(size)\n        if name is None:\n            name = self.name\n        return DecimalColumn(name, size, is_valid, values, self.bit_width)\n\n\nclass DecimalColumn(PrimitiveColumn):\n\n    def __init__(self, name, count, is_valid, values, bit_width):\n        super().__init__(name, count, is_valid, values)\n        self.bit_width = bit_width\n\n    def _encode_value(self, x):\n        return str(x)\n\n\nclass BooleanField(PrimitiveField):\n    bit_width = 1\n\n    def _get_type(self):\n        return OrderedDict([('name', 'bool')])\n\n    @property\n    def numpy_type(self):\n        return 'bool'\n\n    def generate_column(self, size, name=None):\n        values = list(map(bool, np.random.randint(0, 2, size=size)))\n        is_valid = self._make_is_valid(size)\n        if name is None:\n            name = self.name\n        return PrimitiveColumn(name, size, is_valid, values)\n\n\nclass FixedSizeBinaryField(PrimitiveField):\n\n    def __init__(self, name, byte_width, *, nullable=True,\n                 metadata=None):\n        super().__init__(name, nullable=nullable,\n                         metadata=metadata)\n        self.byte_width = byte_width\n\n    @property\n    def numpy_type(self):\n        return object\n\n    @property\n    def column_class(self):\n        return FixedSizeBinaryColumn\n\n    def _get_type(self):\n        return OrderedDict([('name', 'fixedsizebinary'),\n                            ('byteWidth', self.byte_width)])\n\n    def generate_column(self, size, name=None):\n        is_valid = self._make_is_valid(size)\n        values = []\n\n        for i in range(size):\n            values.append(random_bytes(self.byte_width))\n\n        if name is None:\n            name = self.name\n        return self.column_class(name, size, is_valid, values)\n\n\nclass BinaryField(PrimitiveField):\n\n    @property\n    def numpy_type(self):\n        return object\n\n    @property\n    def column_class(self):\n        return BinaryColumn\n\n    def _get_type(self):\n        return OrderedDict([('name', 'binary')])\n\n    def _random_sizes(self, size):\n        return np.random.exponential(scale=4, size=size).astype(np.int32)\n\n    def generate_column(self, size, name=None):\n        is_valid = self._make_is_valid(size)\n        values = []\n\n        sizes = self._random_sizes(size)\n\n        for i, nbytes in enumerate(sizes):\n            if is_valid[i]:\n                values.append(random_bytes(nbytes))\n            else:\n                values.append(b\"\")\n\n        if name is None:\n            name = self.name\n        return self.column_class(name, size, is_valid, values)\n\n\nclass StringField(BinaryField):\n\n    @property\n    def column_class(self):\n        return StringColumn\n\n    def _get_type(self):\n        return OrderedDict([('name', 'utf8')])\n\n    def generate_column(self, size, name=None):\n        K = 7\n        is_valid = self._make_is_valid(size)\n        values = []\n\n        for i in range(size):\n            if is_valid[i]:\n                values.append(tobytes(random_utf8(K)))\n            else:\n                values.append(b\"\")\n\n        if name is None:\n            name = self.name\n        return self.column_class(name, size, is_valid, values)\n\n\nclass LargeBinaryField(BinaryField):\n\n    @property\n    def column_class(self):\n        return LargeBinaryColumn\n\n    def _get_type(self):\n        return OrderedDict([('name', 'largebinary')])\n\n\nclass LargeStringField(StringField):\n\n    @property\n    def column_class(self):\n        return LargeStringColumn\n\n    def _get_type(self):\n        return OrderedDict([('name', 'largeutf8')])\n\n\nclass BinaryViewField(BinaryField):\n\n    @property\n    def column_class(self):\n        return BinaryViewColumn\n\n    def _get_type(self):\n        return OrderedDict([('name', 'binaryview')])\n\n\nclass StringViewField(StringField):\n\n    @property\n    def column_class(self):\n        return StringViewColumn\n\n    def _get_type(self):\n        return OrderedDict([('name', 'utf8view')])\n\n\nclass Schema(object):\n\n    def __init__(self, fields, metadata=None):\n        self.fields = fields\n        self.metadata = metadata\n\n    def get_json(self):\n        entries = [\n            ('fields', [field.get_json() for field in self.fields])\n        ]\n\n        if self.metadata is not None and len(self.metadata) > 0:\n            entries.append(('metadata', metadata_key_values(self.metadata)))\n\n        return OrderedDict(entries)\n\n\nclass _NarrowOffsetsMixin:\n\n    def _encode_offsets(self, offsets):\n        return list(map(int, offsets))\n\n\nclass _LargeOffsetsMixin:\n\n    def _encode_offsets(self, offsets):\n        # 64-bit offsets have to be represented as strings to roundtrip\n        # through JSON.\n        return list(map(str, offsets))\n\n\nclass _BaseBinaryColumn(PrimitiveColumn):\n\n    def _encode_value(self, x):\n        return frombytes(binascii.hexlify(x).upper())\n\n    def _get_buffers(self):\n        offset = 0\n        offsets = [0]\n\n        data = []\n        for i, v in enumerate(self.values):\n            if self.is_valid[i]:\n                offset += len(v)\n            else:\n                v = b\"\"\n\n            offsets.append(offset)\n            data.append(self._encode_value(v))\n\n        return [\n            ('VALIDITY', [int(x) for x in self.is_valid]),\n            ('OFFSET', self._encode_offsets(offsets)),\n            ('DATA', data)\n        ]\n\n\nclass _BaseStringColumn(_BaseBinaryColumn):\n\n    def _encode_value(self, x):\n        return frombytes(x)\n\n\nclass BinaryColumn(_BaseBinaryColumn, _NarrowOffsetsMixin):\n    pass\n\n\nclass StringColumn(_BaseStringColumn, _NarrowOffsetsMixin):\n    pass\n\n\nclass LargeBinaryColumn(_BaseBinaryColumn, _LargeOffsetsMixin):\n    pass\n\n\nclass LargeStringColumn(_BaseStringColumn, _LargeOffsetsMixin):\n    pass\n\n\nclass BinaryViewColumn(PrimitiveColumn):\n\n    def _encode_value(self, x):\n        return frombytes(binascii.hexlify(x).upper())\n\n    def _get_buffers(self):\n        views = []\n        data_buffers = []\n        # a small default data buffer size is used so we can exercise\n        # arrays with multiple data buffers with small data sets\n        DEFAULT_BUFFER_SIZE = 32\n        INLINE_SIZE = 12\n\n        for i, v in enumerate(self.values):\n            if not self.is_valid[i]:\n                v = b''\n            assert isinstance(v, bytes)\n\n            if len(v) <= INLINE_SIZE:\n                # Append an inline view, skip data buffer management.\n                views.append(OrderedDict([\n                    ('SIZE', len(v)),\n                    ('INLINED', self._encode_value(v)),\n                ]))\n                continue\n\n            if len(data_buffers) == 0:\n                # No data buffers have been added yet;\n                # add this string whole (we may append to it later).\n                offset = 0\n                data_buffers.append(v)\n            elif len(data_buffers[-1]) + len(v) > DEFAULT_BUFFER_SIZE:\n                # Appending this string to the current active data buffer\n                # would overflow the default buffer size; add it whole.\n                offset = 0\n                data_buffers.append(v)\n            else:\n                # Append this string to the current active data buffer.\n                offset = len(data_buffers[-1])\n                data_buffers[-1] += v\n\n            # the prefix is always 4 bytes so it may not be utf-8\n            # even if the whole string view is\n            prefix = frombytes(binascii.hexlify(v[:4]).upper())\n\n            views.append(OrderedDict([\n                ('SIZE', len(v)),\n                ('PREFIX_HEX', prefix),\n                ('BUFFER_INDEX', len(data_buffers) - 1),\n                ('OFFSET', offset),\n            ]))\n\n        return [\n            ('VALIDITY', [int(x) for x in self.is_valid]),\n            ('VIEWS', views),\n            ('VARIADIC_DATA_BUFFERS', [\n                frombytes(binascii.hexlify(b).upper())\n                for b in data_buffers\n            ]),\n        ]\n\n\nclass StringViewColumn(BinaryViewColumn):\n\n    def _encode_value(self, x):\n        return frombytes(x)\n\n\nclass FixedSizeBinaryColumn(PrimitiveColumn):\n\n    def _encode_value(self, x):\n        return frombytes(binascii.hexlify(x).upper())\n\n    def _get_buffers(self):\n        data = []\n        for i, v in enumerate(self.values):\n            data.append(self._encode_value(v))\n\n        return [\n            ('VALIDITY', [int(x) for x in self.is_valid]),\n            ('DATA', data)\n        ]\n\n\nclass ListField(Field):\n\n    def __init__(self, name, value_field, *, nullable=True,\n                 metadata=None):\n        super().__init__(name, nullable=nullable,\n                         metadata=metadata)\n        self.value_field = value_field\n\n    @property\n    def column_class(self):\n        return ListColumn\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'list')\n        ])\n\n    def _get_children(self):\n        return [self.value_field.get_json()]\n\n    def generate_column(self, size, name=None):\n        MAX_LIST_SIZE = 4\n\n        is_valid = self._make_is_valid(size)\n        list_sizes = np.random.randint(0, MAX_LIST_SIZE + 1, size=size)\n        offsets = [0]\n\n        offset = 0\n        for i in range(size):\n            if is_valid[i]:\n                offset += int(list_sizes[i])\n            offsets.append(offset)\n\n        # The offset now is the total number of elements in the child array\n        values = self.value_field.generate_column(offset)\n\n        if name is None:\n            name = self.name\n        return self.column_class(name, size, is_valid, offsets, values)\n\n\nclass LargeListField(ListField):\n\n    @property\n    def column_class(self):\n        return LargeListColumn\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'largelist')\n        ])\n\n\nclass _BaseListColumn(Column):\n\n    def __init__(self, name, count, is_valid, offsets, values):\n        super().__init__(name, count)\n        self.is_valid = is_valid\n        self.offsets = offsets\n        self.values = values\n\n    def _get_buffers(self):\n        return [\n            ('VALIDITY', [int(v) for v in self.is_valid]),\n            ('OFFSET', self._encode_offsets(self.offsets))\n        ]\n\n    def _get_children(self):\n        return [self.values.get_json()]\n\n\nclass ListColumn(_BaseListColumn, _NarrowOffsetsMixin):\n    pass\n\n\nclass LargeListColumn(_BaseListColumn, _LargeOffsetsMixin):\n    pass\n\n\nclass ListViewField(Field):\n\n    def __init__(self, name, value_field, *, nullable=True,\n                 metadata=None):\n        super().__init__(name, nullable=nullable,\n                         metadata=metadata)\n        self.value_field = value_field\n\n    @property\n    def column_class(self):\n        return ListViewColumn\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'listview')\n        ])\n\n    def _get_children(self):\n        return [self.value_field.get_json()]\n\n    def generate_column(self, size, name=None):\n        MAX_LIST_SIZE = 4\n        VALUES_SIZE = size * MAX_LIST_SIZE\n\n        is_valid = self._make_is_valid(size)\n\n        MAX_OFFSET = VALUES_SIZE - MAX_LIST_SIZE\n        offsets = np.random.randint(0, MAX_OFFSET + 1, size=size)\n        sizes = np.random.randint(0, MAX_LIST_SIZE + 1, size=size)\n\n        values = self.value_field.generate_column(VALUES_SIZE)\n\n        if name is None:\n            name = self.name\n        return self.column_class(name, size, is_valid, offsets, sizes, values)\n\n\nclass LargeListViewField(ListViewField):\n\n    @property\n    def column_class(self):\n        return LargeListViewColumn\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'largelistview')\n        ])\n\n\nclass _BaseListViewColumn(Column):\n\n    def __init__(self, name, count, is_valid, offsets, sizes, values):\n        super().__init__(name, count)\n        self.is_valid = is_valid\n        self.offsets = offsets\n        self.sizes = sizes\n        self.values = values\n\n    def _get_buffers(self):\n        return [\n            ('VALIDITY', [int(v) for v in self.is_valid]),\n            ('OFFSET', self._encode_offsets(self.offsets)),\n            ('SIZE', self._encode_offsets(self.sizes)),\n        ]\n\n    def _get_children(self):\n        return [self.values.get_json()]\n\n\nclass ListViewColumn(_BaseListViewColumn, _NarrowOffsetsMixin):\n    pass\n\n\nclass LargeListViewColumn(_BaseListViewColumn, _LargeOffsetsMixin):\n    pass\n\n\nclass MapField(Field):\n\n    def __init__(self, name, key_field, item_field, *, nullable=True,\n                 metadata=None, keys_sorted=False, entries_name='entries'):\n        super().__init__(name, nullable=nullable,\n                         metadata=metadata)\n\n        assert not key_field.nullable\n        self.key_field = key_field\n        self.item_field = item_field\n        self.pair_field = StructField(entries_name, [key_field, item_field],\n                                      nullable=False)\n        self.keys_sorted = keys_sorted\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'map'),\n            ('keysSorted', self.keys_sorted)\n        ])\n\n    def _get_children(self):\n        return [self.pair_field.get_json()]\n\n    def generate_column(self, size, name=None):\n        MAX_MAP_SIZE = 4\n\n        is_valid = self._make_is_valid(size)\n        map_sizes = np.random.randint(0, MAX_MAP_SIZE + 1, size=size)\n        offsets = [0]\n\n        offset = 0\n        for i in range(size):\n            if is_valid[i]:\n                offset += int(map_sizes[i])\n            offsets.append(offset)\n\n        # The offset now is the total number of elements in the child array\n        pairs = self.pair_field.generate_column(offset)\n        if name is None:\n            name = self.name\n\n        return MapColumn(name, size, is_valid, offsets, pairs)\n\n\nclass MapColumn(Column):\n\n    def __init__(self, name, count, is_valid, offsets, pairs):\n        super().__init__(name, count)\n        self.is_valid = is_valid\n        self.offsets = offsets\n        self.pairs = pairs\n\n    def _get_buffers(self):\n        return [\n            ('VALIDITY', [int(v) for v in self.is_valid]),\n            ('OFFSET', list(self.offsets))\n        ]\n\n    def _get_children(self):\n        return [self.pairs.get_json()]\n\n\nclass FixedSizeListField(Field):\n\n    def __init__(self, name, value_field, list_size, *, nullable=True,\n                 metadata=None):\n        super().__init__(name, nullable=nullable,\n                         metadata=metadata)\n        self.value_field = value_field\n        self.list_size = list_size\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'fixedsizelist'),\n            ('listSize', self.list_size)\n        ])\n\n    def _get_children(self):\n        return [self.value_field.get_json()]\n\n    def generate_column(self, size, name=None):\n        is_valid = self._make_is_valid(size)\n        values = self.value_field.generate_column(size * self.list_size)\n\n        if name is None:\n            name = self.name\n        return FixedSizeListColumn(name, size, is_valid, values)\n\n\nclass FixedSizeListColumn(Column):\n\n    def __init__(self, name, count, is_valid, values):\n        super().__init__(name, count)\n        self.is_valid = is_valid\n        self.values = values\n\n    def _get_buffers(self):\n        return [\n            ('VALIDITY', [int(v) for v in self.is_valid])\n        ]\n\n    def _get_children(self):\n        return [self.values.get_json()]\n\n\nclass StructField(Field):\n\n    def __init__(self, name, fields, *, nullable=True,\n                 metadata=None):\n        super().__init__(name, nullable=nullable,\n                         metadata=metadata)\n        self.fields = fields\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'struct')\n        ])\n\n    def _get_children(self):\n        return [field.get_json() for field in self.fields]\n\n    def generate_column(self, size, name=None):\n        is_valid = self._make_is_valid(size)\n\n        field_values = [field.generate_column(size) for field in self.fields]\n        if name is None:\n            name = self.name\n        return StructColumn(name, size, is_valid, field_values)\n\n\nclass RunEndEncodedField(Field):\n\n    def __init__(self, name, run_ends_bitwidth, values_field, *, nullable=True,\n                 metadata=None):\n        super().__init__(name, nullable=nullable, metadata=metadata)\n        self.run_ends_field = RunEndsField('run_ends', run_ends_bitwidth)\n        self.values_field = values_field\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'runendencoded')\n        ])\n\n    def _get_children(self):\n        return [\n            self.run_ends_field.get_json(),\n            self.values_field.get_json()\n        ]\n\n    def generate_column(self, size, name=None):\n        values = self.values_field.generate_column(size)\n        run_ends = self.run_ends_field.generate_column(size)\n        if name is None:\n            name = self.name\n        return RunEndEncodedColumn(name, size, run_ends, values)\n\n\nclass _BaseUnionField(Field):\n\n    def __init__(self, name, fields, type_ids=None, *, nullable=True,\n                 metadata=None):\n        super().__init__(name, nullable=nullable, metadata=metadata)\n        if type_ids is None:\n            type_ids = list(range(fields))\n        else:\n            assert len(fields) == len(type_ids)\n        self.fields = fields\n        self.type_ids = type_ids\n        assert all(x >= 0 for x in self.type_ids)\n\n    def _get_type(self):\n        return OrderedDict([\n            ('name', 'union'),\n            ('mode', self.mode),\n            ('typeIds', self.type_ids),\n        ])\n\n    def _get_children(self):\n        return [field.get_json() for field in self.fields]\n\n    def _make_type_ids(self, size):\n        return np.random.choice(self.type_ids, size)\n\n\nclass SparseUnionField(_BaseUnionField):\n    mode = 'SPARSE'\n\n    def generate_column(self, size, name=None):\n        array_type_ids = self._make_type_ids(size)\n        field_values = [field.generate_column(size) for field in self.fields]\n\n        if name is None:\n            name = self.name\n        return SparseUnionColumn(name, size, array_type_ids, field_values)\n\n\nclass DenseUnionField(_BaseUnionField):\n    mode = 'DENSE'\n\n    def generate_column(self, size, name=None):\n        # Reverse mapping {logical type id => physical child id}\n        child_ids = [None] * (max(self.type_ids) + 1)\n        for i, type_id in enumerate(self.type_ids):\n            child_ids[type_id] = i\n\n        array_type_ids = self._make_type_ids(size)\n        offsets = []\n        child_sizes = [0] * len(self.fields)\n\n        for i in range(size):\n            child_id = child_ids[array_type_ids[i]]\n            offset = child_sizes[child_id]\n            offsets.append(offset)\n            child_sizes[child_id] = offset + 1\n\n        field_values = [\n            field.generate_column(child_size)\n            for field, child_size in zip(self.fields, child_sizes)]\n\n        if name is None:\n            name = self.name\n        return DenseUnionColumn(name, size, array_type_ids, offsets,\n                                field_values)\n\n\nclass Dictionary(object):\n\n    def __init__(self, id_, field, size, name=None, ordered=False):\n        self.id_ = id_\n        self.field = field\n        self.values = field.generate_column(size=size, name=name)\n        self.ordered = ordered\n\n    def __len__(self):\n        return len(self.values)\n\n    def get_json(self):\n        dummy_batch = RecordBatch(len(self.values), [self.values])\n        return OrderedDict([\n            ('id', self.id_),\n            ('data', dummy_batch.get_json())\n        ])\n\n\nclass DictionaryField(Field):\n\n    def __init__(self, name, index_field, dictionary, *, nullable=True,\n                 metadata=None):\n        super().__init__(name, nullable=nullable,\n                         metadata=metadata)\n        assert index_field.name == ''\n        assert isinstance(index_field, IntegerField)\n        assert isinstance(dictionary, Dictionary)\n\n        self.index_field = index_field\n        self.dictionary = dictionary\n\n    def _get_type(self):\n        return self.dictionary.field._get_type()\n\n    def _get_children(self):\n        return self.dictionary.field._get_children()\n\n    def _get_dictionary(self):\n        return OrderedDict([\n            ('id', self.dictionary.id_),\n            ('indexType', self.index_field._get_type()),\n            ('isOrdered', self.dictionary.ordered)\n        ])\n\n    def generate_column(self, size, name=None):\n        if name is None:\n            name = self.name\n        return self.index_field.generate_range(size, 0, len(self.dictionary),\n                                               name=name)\n\n\nExtensionType = namedtuple(\n    'ExtensionType', ['extension_name', 'serialized', 'storage_field'])\n\n\nclass ExtensionField(Field):\n\n    def __init__(self, name, extension_type, *, nullable=True, metadata=None):\n        metadata = (metadata or []) + [\n            ('ARROW:extension:name', extension_type.extension_name),\n            ('ARROW:extension:metadata', extension_type.serialized),\n        ]\n        super().__init__(name, nullable=nullable, metadata=metadata)\n        self.extension_type = extension_type\n\n    def _get_type(self):\n        return self.extension_type.storage_field._get_type()\n\n    def _get_children(self):\n        return self.extension_type.storage_field._get_children()\n\n    def _get_dictionary(self):\n        return self.extension_type.storage_field._get_dictionary()\n\n    def generate_column(self, size, name=None):\n        if name is None:\n            name = self.name\n        return self.extension_type.storage_field.generate_column(size, name)\n\n\nclass StructColumn(Column):\n\n    def __init__(self, name, count, is_valid, field_values):\n        super().__init__(name, count)\n        self.is_valid = is_valid\n        self.field_values = field_values\n\n    def _get_buffers(self):\n        return [\n            ('VALIDITY', [int(v) for v in self.is_valid])\n        ]\n\n    def _get_children(self):\n        return [field.get_json() for field in self.field_values]\n\n\nclass RunEndEncodedColumn(Column):\n\n    def __init__(self, name, count, run_ends_field, values_field):\n        super().__init__(name, count)\n        self.run_ends = run_ends_field\n        self.values = values_field\n\n    def _get_buffers(self):\n        return []\n\n    def _get_children(self):\n        return [self.run_ends.get_json(), self.values.get_json()]\n\n\nclass SparseUnionColumn(Column):\n\n    def __init__(self, name, count, type_ids, field_values):\n        super().__init__(name, count)\n        self.type_ids = type_ids\n        self.field_values = field_values\n\n    def _get_buffers(self):\n        return [\n            ('TYPE_ID', [int(v) for v in self.type_ids])\n        ]\n\n    def _get_children(self):\n        return [field.get_json() for field in self.field_values]\n\n\nclass DenseUnionColumn(Column):\n\n    def __init__(self, name, count, type_ids, offsets, field_values):\n        super().__init__(name, count)\n        self.type_ids = type_ids\n        self.offsets = offsets\n        self.field_values = field_values\n\n    def _get_buffers(self):\n        return [\n            ('TYPE_ID', [int(v) for v in self.type_ids]),\n            ('OFFSET', [int(v) for v in self.offsets]),\n        ]\n\n    def _get_children(self):\n        return [field.get_json() for field in self.field_values]\n\n\nclass RecordBatch(object):\n\n    def __init__(self, count, columns):\n        self.count = count\n        self.columns = columns\n\n    def get_json(self):\n        return OrderedDict([\n            ('count', self.count),\n            ('columns', [col.get_json() for col in self.columns])\n        ])\n\n\nclass File(object):\n\n    def __init__(self, name, schema, batches, dictionaries=None,\n                 skip_testers=None, path=None, quirks=None):\n        self.name = name\n        self.schema = schema\n        self.dictionaries = dictionaries or []\n        self.batches = batches\n        self.skipped_testers = set()\n        self.skipped_formats = {}\n        self.path = path\n        if skip_testers:\n            self.skipped_testers.update(skip_testers)\n        # For tracking flags like whether to validate decimal values\n        # fit into the given precision (ARROW-13558).\n        self.quirks = set()\n        if quirks:\n            self.quirks.update(quirks)\n\n    def get_json(self):\n        entries = [\n            ('schema', self.schema.get_json())\n        ]\n\n        if len(self.dictionaries) > 0:\n            entries.append(('dictionaries',\n                            [dictionary.get_json()\n                             for dictionary in self.dictionaries]))\n\n        entries.append(('batches', [batch.get_json()\n                                    for batch in self.batches]))\n        return OrderedDict(entries)\n\n    def write(self, path):\n        with open(path, 'wb') as f:\n            f.write(json.dumps(self.get_json(), indent=2).encode('utf-8'))\n        self.path = path\n\n    def skip_tester(self, tester):\n        \"\"\"Skip this test for the given tester (such as 'C#').\n        \"\"\"\n        self.skipped_testers.add(tester)\n        return self\n\n    def skip_format(self, format, tester='all'):\n        \"\"\"Skip this test for the given format, and optionally tester.\n        \"\"\"\n        self.skipped_formats.setdefault(format, set()).add(tester)\n        return self\n\n    def add_skips_from(self, other_file):\n        \"\"\"Add skips from another File object.\n        \"\"\"\n        self.skipped_testers.update(other_file.skipped_testers)\n        for format, testers in other_file.skipped_formats.items():\n            self.skipped_formats.setdefault(format, set()).update(testers)\n\n    def should_skip(self, tester, format):\n        \"\"\"Whether this (tester, format) combination should be skipped.\n        \"\"\"\n        if tester in self.skipped_testers:\n            return True\n        testers = self.skipped_formats.get(format, ())\n        return 'all' in testers or tester in testers\n\n    @property\n    def num_batches(self):\n        \"\"\"The number of record batches in this file.\n        \"\"\"\n        return len(self.batches)\n\n\ndef get_field(name, type_, **kwargs):\n    if type_ == 'binary':\n        return BinaryField(name, **kwargs)\n    elif type_ == 'utf8':\n        return StringField(name, **kwargs)\n    elif type_ == 'largebinary':\n        return LargeBinaryField(name, **kwargs)\n    elif type_ == 'largeutf8':\n        return LargeStringField(name, **kwargs)\n    elif type_.startswith('fixedsizebinary_'):\n        byte_width = int(type_.split('_')[1])\n        return FixedSizeBinaryField(name, byte_width=byte_width, **kwargs)\n\n    dtype = np.dtype(type_)\n\n    if dtype.kind in ('i', 'u'):\n        signed = dtype.kind == 'i'\n        bit_width = dtype.itemsize * 8\n        return IntegerField(name, signed, bit_width, **kwargs)\n    elif dtype.kind == 'f':\n        bit_width = dtype.itemsize * 8\n        return FloatingPointField(name, bit_width, **kwargs)\n    elif dtype.kind == 'b':\n        return BooleanField(name, **kwargs)\n    else:\n        raise TypeError(dtype)\n\n\ndef _generate_file(name, fields, batch_sizes, *,\n                   dictionaries=None, metadata=None):\n    schema = Schema(fields, metadata=metadata)\n    batches = []\n    for size in batch_sizes:\n        columns = []\n        for field in fields:\n            col = field.generate_column(size)\n            columns.append(col)\n\n        batches.append(RecordBatch(size, columns))\n\n    return File(name, schema, batches, dictionaries)\n\n\ndef generate_custom_metadata_case():\n    def meta(items):\n        # Generate a simple block of metadata where each value is '{}'.\n        # Keys are delimited by whitespace in `items`.\n        return [(k, '{}') for k in items.split()]\n\n    fields = [\n        get_field('sort_of_pandas', 'int8', metadata=meta('pandas')),\n\n        get_field('lots_of_meta', 'int8', metadata=meta('a b c d .. w x y z')),\n\n        get_field(\n            'unregistered_extension', 'int8',\n            metadata=[\n                ('ARROW:extension:name', '!nonexistent'),\n                ('ARROW:extension:metadata', ''),\n                ('ARROW:integration:allow_unregistered_extension', 'true'),\n            ]),\n\n        ListField('list_with_odd_values',\n                  get_field('item', 'int32', metadata=meta('odd_values'))),\n    ]\n\n    batch_sizes = [1]\n    return _generate_file('custom_metadata', fields, batch_sizes,\n                          metadata=meta('schema_custom_0 schema_custom_1'))\n\n\ndef generate_duplicate_fieldnames_case():\n    fields = [\n        get_field('ints', 'int8'),\n        get_field('ints', 'int32'),\n\n        StructField('struct', [get_field('', 'int32'), get_field('', 'utf8')]),\n    ]\n\n    batch_sizes = [1]\n    return _generate_file('duplicate_fieldnames', fields, batch_sizes)\n\n\ndef generate_primitive_case(batch_sizes, name='primitive'):\n    types = ['bool', 'int8', 'int16', 'int32', 'int64',\n             'uint8', 'uint16', 'uint32', 'uint64',\n             'float32', 'float64', 'binary', 'utf8',\n             'fixedsizebinary_19', 'fixedsizebinary_120']\n\n    fields = []\n\n    for type_ in types:\n        fields.append(get_field(type_ + \"_nullable\", type_, nullable=True))\n        fields.append(get_field(type_ + \"_nonnullable\", type_, nullable=False))\n\n    return _generate_file(name, fields, batch_sizes)\n\n\ndef generate_primitive_large_offsets_case(batch_sizes):\n    types = ['largebinary', 'largeutf8']\n\n    fields = []\n\n    for type_ in types:\n        fields.append(get_field(type_ + \"_nullable\", type_, nullable=True))\n        fields.append(get_field(type_ + \"_nonnullable\", type_, nullable=False))\n\n    return _generate_file('primitive_large_offsets', fields, batch_sizes)\n\n\ndef generate_null_case(batch_sizes):\n    # Interleave null with non-null types to ensure the appropriate number of\n    # buffers (0) is read and written\n    fields = [\n        NullField(name='f0'),\n        get_field('f1', 'int32'),\n        NullField(name='f2'),\n        get_field('f3', 'float64'),\n        NullField(name='f4')\n    ]\n    return _generate_file('null', fields, batch_sizes)\n\n\ndef generate_null_trivial_case(batch_sizes):\n    # Generate a case with no buffers\n    fields = [\n        NullField(name='f0'),\n    ]\n    return _generate_file('null_trivial', fields, batch_sizes)\n\n\ndef generate_decimal128_case():\n    fields = [\n        DecimalField(name='f{}'.format(i), precision=precision, scale=2,\n                     bit_width=128)\n        for i, precision in enumerate(range(3, 39))\n    ]\n\n    batch_sizes = [7, 10]\n    # 'decimal' is the original name for the test, and it must match\n    # provide \"gold\" files that test backwards compatibility, so they\n    # can be appropriately skipped.\n    return _generate_file('decimal', fields, batch_sizes)\n\n\ndef generate_decimal256_case():\n    fields = [\n        DecimalField(name='f{}'.format(i), precision=precision, scale=5,\n                     bit_width=256)\n        for i, precision in enumerate(range(37, 70))\n    ]\n\n    batch_sizes = [7, 10]\n    return _generate_file('decimal256', fields, batch_sizes)\n\n\ndef generate_datetime_case():\n    fields = [\n        DateField('f0', DateField.DAY),\n        DateField('f1', DateField.MILLISECOND),\n        TimeField('f2', 's'),\n        TimeField('f3', 'ms'),\n        TimeField('f4', 'us'),\n        TimeField('f5', 'ns'),\n        TimestampField('f6', 's'),\n        TimestampField('f7', 'ms'),\n        TimestampField('f8', 'us'),\n        TimestampField('f9', 'ns'),\n        TimestampField('f10', 'ms', tz=None),\n        TimestampField('f11', 's', tz='UTC'),\n        TimestampField('f12', 'ms', tz='US/Eastern'),\n        TimestampField('f13', 'us', tz='Europe/Paris'),\n        TimestampField('f14', 'ns', tz='US/Pacific'),\n    ]\n\n    batch_sizes = [7, 10]\n    return _generate_file(\"datetime\", fields, batch_sizes)\n\n\ndef generate_duration_case():\n    fields = [\n        DurationIntervalField('f1', 's'),\n        DurationIntervalField('f2', 'ms'),\n        DurationIntervalField('f3', 'us'),\n        DurationIntervalField('f4', 'ns'),\n    ]\n\n    batch_sizes = [7, 10]\n    return _generate_file(\"duration\", fields, batch_sizes)\n\n\ndef generate_interval_case():\n    fields = [\n        YearMonthIntervalField('f5'),\n        DayTimeIntervalField('f6'),\n    ]\n\n    batch_sizes = [7, 10]\n    return _generate_file(\"interval\", fields, batch_sizes)\n\n\ndef generate_month_day_nano_interval_case():\n    fields = [\n        MonthDayNanoIntervalField('f1'),\n    ]\n\n    batch_sizes = [7, 10]\n    return _generate_file(\"interval_mdn\", fields, batch_sizes)\n\n\ndef generate_map_case():\n    fields = [\n        MapField('map_nullable', get_field('key', 'utf8', nullable=False),\n                 get_field('value', 'int32')),\n    ]\n\n    batch_sizes = [7, 10]\n    return _generate_file(\"map\", fields, batch_sizes)\n\n\ndef generate_non_canonical_map_case():\n    fields = [\n        MapField('map_other_names',\n                 get_field('some_key', 'utf8', nullable=False),\n                 get_field('some_value', 'int32'),\n                 entries_name='some_entries'),\n    ]\n\n    batch_sizes = [7]\n    return _generate_file(\"map_non_canonical\", fields, batch_sizes)\n\n\ndef generate_nested_case():\n    fields = [\n        ListField('list_nullable', get_field('item', 'int32')),\n        FixedSizeListField('fixedsizelist_nullable',\n                           get_field('item', 'int32'), 4),\n        StructField('struct_nullable', [get_field('f1', 'int32'),\n                                        get_field('f2', 'utf8')]),\n        # Fails on Go (ARROW-8452)\n        # ListField('list_nonnullable', get_field('item', 'int32'),\n        #           nullable=False),\n    ]\n\n    batch_sizes = [7, 10]\n    return _generate_file(\"nested\", fields, batch_sizes)\n\n\ndef generate_recursive_nested_case():\n    fields = [\n        ListField('lists_list',\n                  ListField('inner_list', get_field('item', 'int16'))),\n        ListField('structs_list',\n                  StructField('inner_struct',\n                              [get_field('f1', 'int32'),\n                               get_field('f2', 'utf8')])),\n    ]\n\n    batch_sizes = [7, 10]\n    return _generate_file(\"recursive_nested\", fields, batch_sizes)\n\n\ndef generate_run_end_encoded_case():\n    fields = [\n        RunEndEncodedField('ree16', 16, get_field('values', 'int32')),\n        RunEndEncodedField('ree32', 32, get_field('values', 'utf8')),\n        RunEndEncodedField('ree64', 64, get_field('values', 'float32')),\n    ]\n    batch_sizes = [0, 7, 10]\n    return _generate_file(\"run_end_encoded\", fields, batch_sizes)\n\n\ndef generate_binary_view_case():\n    fields = [\n        BinaryViewField('bv'),\n        StringViewField('sv'),\n    ]\n    batch_sizes = [0, 7, 256]\n    return _generate_file(\"binary_view\", fields, batch_sizes)\n\n\ndef generate_list_view_case():\n    fields = [\n        ListViewField('lv', get_field('item', 'float32')),\n        LargeListViewField('llv', get_field('item', 'float32')),\n    ]\n    batch_sizes = [0, 7, 256]\n    return _generate_file(\"list_view\", fields, batch_sizes)\n\n\ndef generate_nested_large_offsets_case():\n    fields = [\n        LargeListField('large_list_nullable', get_field('item', 'int32')),\n        LargeListField('large_list_nonnullable',\n                       get_field('item', 'int32'), nullable=False),\n        LargeListField('large_list_nested',\n                       ListField('inner_list', get_field('item', 'int16'))),\n    ]\n\n    batch_sizes = [0, 13]\n    return _generate_file(\"nested_large_offsets\", fields, batch_sizes)\n\n\ndef generate_unions_case():\n    fields = [\n        SparseUnionField('sparse_1', [get_field('f1', 'int32'),\n                                      get_field('f2', 'utf8')],\n                         type_ids=[5, 7]),\n        DenseUnionField('dense_1', [get_field('f1', 'int16'),\n                                    get_field('f2', 'binary')],\n                        type_ids=[10, 20]),\n        SparseUnionField('sparse_2', [get_field('f1', 'float32', nullable=False),\n                                      get_field('f2', 'bool')],\n                         type_ids=[5, 7], nullable=False),\n        DenseUnionField('dense_2', [get_field('f1', 'uint8', nullable=False),\n                                    get_field('f2', 'uint16'),\n                                    NullField('f3')],\n                        type_ids=[42, 43, 44], nullable=False),\n    ]\n\n    batch_sizes = [0, 11]\n    return _generate_file(\"union\", fields, batch_sizes)\n\n\ndef generate_dictionary_case():\n    dict0 = Dictionary(0, StringField('dictionary1'), size=10, name='DICT0')\n    dict1 = Dictionary(1, StringField('dictionary1'), size=5, name='DICT1')\n    dict2 = Dictionary(2, get_field('dictionary2', 'int64'),\n                       size=50, name='DICT2')\n\n    fields = [\n        DictionaryField('dict0', get_field('', 'int8'), dict0),\n        DictionaryField('dict1', get_field('', 'int32'), dict1),\n        DictionaryField('dict2', get_field('', 'int16'), dict2)\n    ]\n    batch_sizes = [7, 10]\n    return _generate_file(\"dictionary\", fields, batch_sizes,\n                          dictionaries=[dict0, dict1, dict2])\n\n\ndef generate_dictionary_unsigned_case():\n    dict0 = Dictionary(0, StringField('dictionary0'), size=5, name='DICT0')\n    dict1 = Dictionary(1, StringField('dictionary1'), size=5, name='DICT1')\n    dict2 = Dictionary(2, StringField('dictionary2'), size=5, name='DICT2')\n\n    # TODO: JavaScript does not support uint64 dictionary indices, so disabled\n    # for now\n    # dict3 = Dictionary(3, StringField('dictionary3'), size=5, name='DICT3')\n    fields = [\n        DictionaryField('f0', get_field('', 'uint8'), dict0),\n        DictionaryField('f1', get_field('', 'uint16'), dict1),\n        DictionaryField('f2', get_field('', 'uint32'), dict2),\n        # DictionaryField('f3', get_field('', 'uint64'), dict3)\n    ]\n    batch_sizes = [7, 10]\n    return _generate_file(\"dictionary_unsigned\", fields, batch_sizes,\n                          dictionaries=[dict0, dict1, dict2])\n\n\ndef generate_nested_dictionary_case():\n    dict0 = Dictionary(0, StringField('str'), size=10, name='DICT0')\n\n    list_of_dict = ListField(\n        'list',\n        DictionaryField('str_dict', get_field('', 'int8'), dict0))\n    dict1 = Dictionary(1, list_of_dict, size=30, name='DICT1')\n\n    struct_of_dict = StructField('struct', [\n        DictionaryField('str_dict_a', get_field('', 'int8'), dict0),\n        DictionaryField('str_dict_b', get_field('', 'int8'), dict0)\n    ])\n    dict2 = Dictionary(2, struct_of_dict, size=30, name='DICT2')\n\n    fields = [\n        DictionaryField('list_dict', get_field('', 'int8'), dict1),\n        DictionaryField('struct_dict', get_field('', 'int8'), dict2)\n    ]\n\n    batch_sizes = [10, 13]\n    return _generate_file(\"nested_dictionary\", fields, batch_sizes,\n                          dictionaries=[dict0, dict1, dict2])\n\n\ndef generate_extension_case():\n    dict0 = Dictionary(0, StringField('dictionary0'), size=5, name='DICT0')\n\n    uuid_type = ExtensionType('uuid', 'uuid-serialized',\n                              FixedSizeBinaryField('', 16))\n    dict_ext_type = ExtensionType(\n        'dict-extension', 'dict-extension-serialized',\n        DictionaryField('str_dict', get_field('', 'int8'), dict0))\n\n    fields = [\n        ExtensionField('uuids', uuid_type),\n        ExtensionField('dict_exts', dict_ext_type),\n    ]\n\n    batch_sizes = [0, 13]\n    return _generate_file(\"extension\", fields, batch_sizes,\n                          dictionaries=[dict0])\n\n\ndef get_generated_json_files(tempdir=None):\n    tempdir = tempdir or tempfile.mkdtemp(prefix='arrow-integration-')\n\n    def _temp_path():\n        return\n\n    file_objs = [\n        generate_primitive_case([], name='primitive_no_batches'),\n        generate_primitive_case([17, 20], name='primitive'),\n        generate_primitive_case([0, 0, 0], name='primitive_zerolength'),\n\n        generate_primitive_large_offsets_case([17, 20])\n        .skip_tester('C#'),\n\n        generate_null_case([10, 0]),\n\n        generate_null_trivial_case([0, 0]),\n\n        generate_decimal128_case(),\n\n        generate_decimal256_case()\n        .skip_tester('JS'),\n\n        generate_datetime_case(),\n\n        generate_duration_case(),\n\n        generate_interval_case()\n        .skip_tester('JS'),  # TODO(ARROW-5239): Intervals + JS\n\n        generate_month_day_nano_interval_case()\n        .skip_tester('JS'),\n\n        generate_map_case(),\n\n        generate_non_canonical_map_case()\n        .skip_tester('Java')  # TODO(ARROW-8715)\n        # Canonical map names are restored on import, so the schemas are unequal\n        .skip_format(SKIP_C_SCHEMA, 'C++'),\n\n        generate_nested_case(),\n\n        generate_recursive_nested_case(),\n\n        generate_nested_large_offsets_case()\n        .skip_tester('C#')\n        .skip_tester('JS'),\n\n        generate_unions_case(),\n\n        generate_custom_metadata_case(),\n\n        generate_duplicate_fieldnames_case()\n        .skip_tester('JS'),\n\n        generate_dictionary_case(),\n\n        generate_dictionary_unsigned_case()\n        .skip_tester('Java'),  # TODO(ARROW-9377)\n\n        generate_nested_dictionary_case()\n        .skip_tester('Java'),  # TODO(ARROW-7779)\n\n        generate_run_end_encoded_case()\n        .skip_tester('C#')\n        .skip_tester('Java')\n        .skip_tester('JS')\n        .skip_tester('nanoarrow')\n        .skip_tester('Rust'),\n\n        generate_binary_view_case()\n        .skip_tester('JS')\n        .skip_tester('nanoarrow')\n        .skip_tester('Rust'),\n\n        generate_list_view_case()\n        .skip_tester('C#')     # Doesn't support large list views\n        .skip_tester('Java')\n        .skip_tester('JS')\n        .skip_tester('nanoarrow')\n        .skip_tester('Rust'),\n\n        generate_extension_case()\n        # TODO: ensure the extension is registered in the C++ entrypoint\n        .skip_format(SKIP_C_SCHEMA, 'C++')\n        .skip_format(SKIP_C_ARRAY, 'C++'),\n    ]\n\n    generated_paths = []\n    for file_obj in file_objs:\n        out_path = os.path.join(tempdir, 'generated_' +\n                                file_obj.name + '.json')\n        file_obj.write(out_path)\n        generated_paths.append(file_obj)\n\n    return generated_paths\n", "dev/archery/archery/release/reports.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nfrom ..utils.report import JinjaReport\n\n\nclass ReleaseCuration(JinjaReport):\n    templates = {\n        'console': 'release_curation.txt.j2'\n    }\n    fields = [\n        'release',\n        'within',\n        'outside',\n        'noissue',\n        'parquet',\n        'nopatch',\n        'minimal',\n        'minor'\n    ]\n\n\nclass ReleaseChangelog(JinjaReport):\n    templates = {\n        'markdown': 'release_changelog.md.j2',\n        'html': 'release_changelog.html.j2'\n    }\n    fields = [\n        'release',\n        'categories'\n    ]\n", "dev/archery/archery/release/cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pathlib\n\nimport click\n\nfrom ..utils.cli import validate_arrow_sources\nfrom .core import IssueTracker, Release\n\n\n@click.group('release')\n@click.option(\"--src\", metavar=\"<arrow_src>\", default=None,\n              callback=validate_arrow_sources,\n              help=\"Specify Arrow source directory.\")\n@click.option('--github-token', '-t', default=None,\n              envvar=\"CROSSBOW_GITHUB_TOKEN\",\n              help='OAuth token for GitHub authentication')\n@click.pass_obj\ndef release(obj, src, github_token):\n    \"\"\"Release related commands.\"\"\"\n\n    obj['issue_tracker'] = IssueTracker(github_token=github_token)\n    obj['repo'] = src.path\n\n\n@release.command('curate', help=\"Lists release related issues.\")\n@click.argument('version')\n@click.option('--minimal/--full', '-m/-f',\n              help=\"Only show actionable issues.\", default=False)\n@click.pass_obj\ndef release_curate(obj, version, minimal):\n    \"\"\"Release curation.\"\"\"\n    release = Release(version, repo=obj['repo'],\n                      issue_tracker=obj['issue_tracker'])\n    curation = release.curate(minimal)\n\n    click.echo(curation.render('console'))\n\n\n@release.group('changelog')\ndef release_changelog():\n    \"\"\"Release changelog.\"\"\"\n    pass\n\n\n@release_changelog.command('add')\n@click.argument('version')\n@click.pass_obj\ndef release_changelog_add(obj, version):\n    \"\"\"Prepend the changelog with the current release\"\"\"\n    repo, issue_tracker = obj['repo'], obj['issue_tracker']\n\n    # just handle the current version\n    release = Release(version, repo=repo, issue_tracker=issue_tracker)\n    if release.is_released:\n        raise ValueError('This version has been already released!')\n\n    changelog = release.changelog()\n    changelog_path = pathlib.Path(repo) / 'CHANGELOG.md'\n\n    current_content = changelog_path.read_text()\n    new_content = changelog.render('markdown') + current_content\n\n    changelog_path.write_text(new_content)\n    click.echo(\"CHANGELOG.md is updated!\")\n\n\n@release_changelog.command('generate')\n@click.argument('version')\n@click.argument('output', type=click.File('w', encoding='utf8'), default='-')\n@click.pass_obj\ndef release_changelog_generate(obj, version, output):\n    \"\"\"Generate the changelog of a specific release.\"\"\"\n    repo, issue_tracker = obj['repo'], obj['issue_tracker']\n\n    # just handle the current version\n    release = Release(version, repo=repo, issue_tracker=issue_tracker)\n\n    changelog = release.changelog()\n    output.write(changelog.render('markdown'))\n\n\n@release_changelog.command('regenerate')\n@click.pass_obj\ndef release_changelog_regenerate(obj):\n    \"\"\"Regenerate the whole CHANGELOG.md file\"\"\"\n    issue_tracker, repo = obj['issue_tracker'], obj['repo']\n    changelogs = []\n    issue_tracker = IssueTracker(issue_tracker=issue_tracker)\n\n    for version in issue_tracker.project_versions():\n        if not version.released:\n            continue\n        release = Release(version, repo=repo,\n                          issue_tracker=issue_tracker)\n        click.echo('Querying changelog for version: {}'.format(version))\n        changelogs.append(release.changelog())\n\n    click.echo('Rendering new CHANGELOG.md file...')\n    changelog_path = pathlib.Path(repo) / 'CHANGELOG.md'\n    with changelog_path.open('w') as fp:\n        for cl in changelogs:\n            fp.write(cl.render('markdown'))\n\n\n@release.command('cherry-pick')\n@click.argument('version')\n@click.option('--dry-run/--execute', default=True,\n              help=\"Display the git commands instead of executing them.\")\n@click.option('--recreate/--continue', default=True,\n              help=\"Recreate the maintenance branch or only apply unapplied \"\n                   \"patches.\")\n@click.pass_obj\ndef release_cherry_pick(obj, version, dry_run, recreate):\n    \"\"\"\n    Cherry pick commits.\n    \"\"\"\n    issue_tracker = obj['issue_tracker']\n    release = Release(version,\n                      repo=obj['repo'], issue_tracker=issue_tracker)\n\n    if not dry_run:\n        release.cherry_pick_commits(recreate_branch=recreate)\n    else:\n        click.echo(f'git checkout -b {release.branch} {release.base_branch}')\n        for commit in release.commits_to_pick():\n            click.echo('git cherry-pick {}'.format(commit.hexsha))\n", "dev/archery/archery/release/core.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom abc import abstractmethod\nfrom collections import defaultdict\nimport functools\nimport os\nimport pathlib\nimport re\nimport warnings\n\nfrom git import Repo\nfrom github import Github\nfrom jira import JIRA\nfrom semver import VersionInfo as SemVer\n\nfrom ..utils.source import ArrowSources\nfrom ..utils.logger import logger\nfrom .reports import ReleaseCuration, ReleaseChangelog\n\n\ndef cached_property(fn):\n    return property(functools.lru_cache(maxsize=1)(fn))\n\n\nclass Version(SemVer):\n\n    __slots__ = ('released', 'release_date')\n\n    def __init__(self, released=False, release_date=None, **kwargs):\n        super().__init__(**kwargs)\n        self.released = released\n        self.release_date = release_date\n\n    @classmethod\n    def parse(cls, version, **kwargs):\n        return cls(**SemVer.parse(version).to_dict(), **kwargs)\n\n    @classmethod\n    def from_jira(cls, jira_version):\n        return cls.parse(\n            jira_version.name,\n            released=jira_version.released,\n            release_date=getattr(jira_version, 'releaseDate', None)\n        )\n\n    @classmethod\n    def from_milestone(cls, milestone):\n        return cls.parse(\n            milestone.title,\n            released=milestone.state == \"closed\",\n            release_date=milestone.due_on\n        )\n\n\nclass Issue:\n\n    def __init__(self, key, type, summary, github_issue=None):\n        self.key = key\n        self.type = type\n        self.summary = summary\n        self.github_issue_id = getattr(github_issue, \"number\", None)\n        self._github_issue = github_issue\n\n    @classmethod\n    def from_jira(cls, jira_issue):\n        return cls(\n            key=jira_issue.key,\n            type=jira_issue.fields.issuetype.name,\n            summary=jira_issue.fields.summary\n        )\n\n    @classmethod\n    def from_github(cls, github_issue):\n        return cls(\n            key=github_issue.number,\n            type=next(\n                iter(\n                    [\n                        label.name for label in github_issue.labels\n                        if label.name.startswith(\"Type:\")\n                    ]\n                ), None),\n            summary=github_issue.title,\n            github_issue=github_issue\n        )\n\n    @property\n    def project(self):\n        if isinstance(self.key, int):\n            return 'GH'\n        return self.key.split('-')[0]\n\n    @property\n    def number(self):\n        if isinstance(self.key, str):\n            return int(self.key.split('-')[1])\n        else:\n            return self.key\n\n    @cached_property\n    def is_pr(self):\n        return bool(self._github_issue and self._github_issue.pull_request)\n\n\nclass Jira(JIRA):\n\n    def __init__(self, url='https://issues.apache.org/jira'):\n        super().__init__(url)\n\n    def issue(self, key):\n        return Issue.from_jira(super().issue(key))\n\n\nclass IssueTracker:\n\n    def __init__(self, github_token=None):\n        github = Github(github_token)\n        self.github_repo = github.get_repo('apache/arrow')\n\n    def project_version(self, version_string):\n        for milestone in self.project_versions():\n            if milestone == version_string:\n                return milestone\n\n    def project_versions(self):\n        versions = []\n        milestones = self.github_repo.get_milestones(state=\"all\")\n        for milestone in milestones:\n            try:\n                versions.append(Version.from_milestone(milestone))\n            except ValueError:\n                # ignore invalid semantic versions like JS-0.4.0\n                continue\n        return sorted(versions, reverse=True)\n\n    def _milestone_from_semver(self, semver):\n        milestones = self.github_repo.get_milestones(state=\"all\")\n        for milestone in milestones:\n            try:\n                if milestone.title == semver:\n                    return milestone\n            except ValueError:\n                # ignore invalid semantic versions like JS-0.3.0\n                continue\n\n    def project_issues(self, version):\n        issues = self.github_repo.get_issues(\n            milestone=self._milestone_from_semver(version),\n            state=\"all\")\n        return list(map(Issue.from_github, issues))\n\n    def issue(self, key):\n        return Issue.from_github(self.github_repo.get_issue(key))\n\n\n_TITLE_REGEX = re.compile(\n    r\"(?P<issue>(?P<project>(ARROW|PARQUET|GH))\\-(?P<issue_id>(\\d+)))?\\s*:?\\s*\"\n    r\"(?P<minor>(MINOR))?\\s*:?\\s*\"\n    r\"(?P<components>\\[.*\\])?\\s*(?P<summary>.*)\"\n)\n_COMPONENT_REGEX = re.compile(r\"\\[([^\\[\\]]+)\\]\")\n\n\nclass CommitTitle:\n\n    def __init__(self, summary, project=None, issue=None, minor=None,\n                 components=None, issue_id=None):\n        self.project = project\n        self.issue = issue\n        self.issue_id = issue_id\n        self.components = components or []\n        self.summary = summary\n        self.minor = bool(minor)\n\n    def __str__(self):\n        return self.to_string()\n\n    def __eq__(self, other):\n        return (\n            self.summary == other.summary and\n            self.project == other.project and\n            self.issue == other.issue and\n            self.minor == other.minor and\n            self.components == other.components\n        )\n\n    def __hash__(self):\n        return hash(\n            (self.summary, self.project, self.issue, tuple(self.components))\n        )\n\n    @classmethod\n    def parse(cls, headline):\n        matches = _TITLE_REGEX.match(headline)\n        if matches is None:\n            warnings.warn(\n                \"Unable to parse commit message `{}`\".format(headline)\n            )\n            return CommitTitle(headline)\n\n        values = matches.groupdict()\n        components = values.get('components') or ''\n        components = _COMPONENT_REGEX.findall(components)\n\n        return CommitTitle(\n            values['summary'],\n            project=values.get('project'),\n            issue=values.get('issue'),\n            issue_id=values.get('issue_id'),\n            minor=values.get('minor'),\n            components=components\n        )\n\n    def to_string(self, with_issue=True, with_components=True):\n        out = \"\"\n        if with_issue and self.issue:\n            out += \"{}: \".format(self.issue)\n        if with_components and self.components:\n            for component in self.components:\n                out += \"[{}]\".format(component)\n            out += \" \"\n        out += self.summary\n        return out\n\n\nclass Commit:\n\n    def __init__(self, wrapped):\n        self._title = CommitTitle.parse(wrapped.summary)\n        self._wrapped = wrapped\n\n    def __getattr__(self, attr):\n        if hasattr(self._title, attr):\n            return getattr(self._title, attr)\n        else:\n            return getattr(self._wrapped, attr)\n\n    def __repr__(self):\n        template = '<Commit sha={!r} issue={!r} components={!r} summary={!r}>'\n        return template.format(self.hexsha, self.issue, self.components,\n                               self.summary)\n\n    @property\n    def url(self):\n        return 'https://github.com/apache/arrow/commit/{}'.format(self.hexsha)\n\n    @property\n    def title(self):\n        return self._title\n\n\nclass Release:\n\n    def __new__(self, version, repo=None, github_token=None,\n                issue_tracker=None):\n        if isinstance(version, str):\n            version = Version.parse(version)\n        elif not isinstance(version, Version):\n            raise TypeError(version)\n\n        # decide the type of the release based on the version number\n        if version.patch == 0:\n            if version.minor == 0:\n                klass = MajorRelease\n            elif version.major == 0:\n                # handle minor releases before 1.0 as major releases\n                klass = MajorRelease\n            else:\n                klass = MinorRelease\n        else:\n            klass = PatchRelease\n\n        return super().__new__(klass)\n\n    def __init__(self, version, repo, issue_tracker):\n        if repo is None:\n            arrow = ArrowSources.find()\n            repo = Repo(arrow.path)\n        elif isinstance(repo, (str, pathlib.Path)):\n            repo = Repo(repo)\n        elif not isinstance(repo, Repo):\n            raise TypeError(\"`repo` argument must be a path or a valid Repo \"\n                            \"instance\")\n\n        if isinstance(version, str):\n            version = issue_tracker.project_version(version)\n\n        elif not isinstance(version, Version):\n            raise TypeError(version)\n\n        self.version = version\n        self.repo = repo\n        self.issue_tracker = issue_tracker\n\n    def __repr__(self):\n        if self.version.released:\n            status = \"released_at={self.version.release_date!r}\"\n        else:\n            status = \"pending\"\n        return f\"<{self.__class__.__name__} {self.version!r} {status}>\"\n\n    @property\n    def is_released(self):\n        return self.version.released\n\n    @property\n    def tag(self):\n        return f\"apache-arrow-{self.version}\"\n\n    @property\n    @abstractmethod\n    def branch(self):\n        \"\"\"\n        Target branch that serves as the base for the release.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def siblings(self):\n        \"\"\"\n        Releases to consider when calculating previous and next releases.\n        \"\"\"\n        ...\n\n    @cached_property\n    def previous(self):\n        # select all non-patch releases\n        position = self.siblings.index(self.version)\n        try:\n            previous = self.siblings[position + 1]\n        except IndexError:\n            # first release doesn't have a previous one\n            return None\n        else:\n            return Release(previous, repo=self.repo,\n                           issue_tracker=self.issue_tracker)\n\n    @cached_property\n    def next(self):\n        # select all non-patch releases\n        position = self.siblings.index(self.version)\n        if position <= 0:\n            raise ValueError(\"There is no upcoming release set in JIRA after \"\n                             f\"version {self.version}\")\n        upcoming = self.siblings[position - 1]\n        return Release(upcoming, repo=self.repo,\n                       issue_tracker=self.issue_tracker)\n\n    @cached_property\n    def issues(self):\n        issues = self.issue_tracker.project_issues(\n            self.version\n        )\n        return {i.key: i for i in issues}\n\n    @cached_property\n    def github_issue_ids(self):\n        return {v.github_issue_id for v in self.issues.values()\n                if v.github_issue_id}\n\n    @cached_property\n    def commits(self):\n        \"\"\"\n        All commits applied between two versions.\n        \"\"\"\n        if self.previous is None:\n            # first release\n            lower = ''\n        else:\n            lower = self.repo.tags[self.previous.tag]\n\n        if self.version.released:\n            try:\n                upper = self.repo.tags[self.tag]\n            except IndexError:\n                warnings.warn(f\"Release tag `{self.tag}` doesn't exist.\")\n                return []\n        else:\n            try:\n                upper = self.repo.branches[self.branch]\n            except IndexError:\n                warnings.warn(f\"Release branch `{self.branch}` doesn't exist.\")\n                return []\n\n        commit_range = f\"{lower}..{upper}\"\n        return list(map(Commit, self.repo.iter_commits(commit_range)))\n\n    @cached_property\n    def jira_instance(self):\n        return Jira()\n\n    @cached_property\n    def default_branch(self):\n        default_branch_name = os.getenv(\"ARCHERY_DEFAULT_BRANCH\")\n\n        if default_branch_name is None:\n            # Set up repo object\n            arrow = ArrowSources.find()\n            repo = Repo(arrow.path)\n            origin = repo.remotes[\"origin\"]\n            origin_refs = origin.refs\n\n            try:\n                # Get git.RemoteReference object to origin/HEAD\n                # If the reference does not exist, a KeyError will be thrown\n                origin_head = origin_refs[\"HEAD\"]\n\n                # Get git.RemoteReference object to origin/default-branch-name\n                origin_head_reference = origin_head.reference\n\n                # Get string value of remote head reference, should return\n                # \"origin/main\" or \"origin/master\"\n                origin_head_name = origin_head_reference.name\n                origin_head_name_tokenized = origin_head_name.split(\"/\")\n\n                # The last token is the default branch name\n                default_branch_name = origin_head_name_tokenized[-1]\n            except (KeyError, IndexError):\n                # Use a hard-coded default value to set default_branch_name\n                default_branch_name = \"main\"\n                warnings.warn('Unable to determine default branch name: '\n                              'ARCHERY_DEFAULT_BRANCH environment variable is '\n                              'not set. Git repository does not contain a '\n                              '\\'refs/remotes/origin/HEAD\\'reference. Setting '\n                              'the default branch name to ' +\n                              default_branch_name, RuntimeWarning)\n\n        return default_branch_name\n\n    def curate(self, minimal=False):\n        # handle commits with parquet issue key specially\n        release_issues = self.issues\n        within, outside, noissue, parquet, minor = [], [], [], [], []\n        for c in self.commits:\n            if c.issue is None:\n                if c.title.minor:\n                    minor.append(c)\n                else:\n                    noissue.append(c)\n            elif c.project == 'GH':\n                if int(c.issue_id) in release_issues:\n                    within.append((release_issues[int(c.issue_id)], c))\n                else:\n                    outside.append(\n                        (self.issue_tracker.issue(int(c.issue_id)), c))\n            elif c.project == 'ARROW':\n                if c.issue in release_issues:\n                    within.append((release_issues[c.issue], c))\n                else:\n                    outside.append((self.jira_instance.issue(c.issue), c))\n            elif c.project == 'PARQUET':\n                parquet.append((self.jira_instance.issue(c.issue), c))\n            else:\n                warnings.warn(\n                    f'Issue {c.issue} does not pertain to GH' +\n                    ', ARROW or PARQUET')\n                outside.append((c.issue, c))\n\n        # remaining jira tickets\n        within_keys = {i.key for i, c in within}\n        # Take into account that some issues milestoned are prs\n        nopatch = [issue for key, issue in release_issues.items()\n                   if key not in within_keys and issue.is_pr is False]\n\n        return ReleaseCuration(release=self, within=within, outside=outside,\n                               noissue=noissue, parquet=parquet,\n                               nopatch=nopatch, minimal=minimal, minor=minor)\n\n    def changelog(self):\n        issue_commit_pairs = []\n\n        # get organized report for the release\n        curation = self.curate()\n\n        # jira tickets having patches in the release\n        issue_commit_pairs.extend(curation.within)\n        # parquet patches in the release\n        issue_commit_pairs.extend(curation.parquet)\n\n        # jira tickets without patches\n        for issue in curation.nopatch:\n            issue_commit_pairs.append((issue, None))\n\n        # organize issues into categories\n        issue_types = {\n            'Bug': 'Bug Fixes',\n            'Improvement': 'New Features and Improvements',\n            'New Feature': 'New Features and Improvements',\n            'Sub-task': 'New Features and Improvements',\n            'Task': 'New Features and Improvements',\n            'Test': 'Bug Fixes',\n            'Wish': 'New Features and Improvements',\n            'Type: bug': 'Bug Fixes',\n            'Type: enhancement': 'New Features and Improvements',\n            'Type: task': 'New Features and Improvements',\n            'Type: test': 'Bug Fixes',\n            'Type: usage': 'New Features and Improvements',\n        }\n        categories = defaultdict(list)\n        for issue, commit in issue_commit_pairs:\n            try:\n                categories[issue_types[issue.type]].append((issue, commit))\n            except KeyError:\n                # If issue or pr don't have a type assume task.\n                # Currently the label for type is not mandatory on GitHub.\n                categories[issue_types['Type: task']].append((issue, commit))\n\n        # sort issues by the issue key in ascending order\n        for issues in categories.values():\n            issues.sort(key=lambda pair: (pair[0].project, pair[0].number))\n\n        return ReleaseChangelog(release=self, categories=categories)\n\n    def commits_to_pick(self, exclude_already_applied=True):\n        # collect commits applied on the default branch since the root of the\n        # maintenance branch (the previous major release)\n        commit_range = f\"{self.previous.tag}..{self.default_branch}\"\n\n        # keeping the original order of the commits helps to minimize the merge\n        # conflicts during cherry-picks\n        commits = map(Commit, self.repo.iter_commits(commit_range))\n\n        # exclude patches that have been already applied to the maintenance\n        # branch, we cannot identify patches based on sha because it changes\n        # after the cherry pick so use commit title instead\n        if exclude_already_applied:\n            already_applied = {c.title for c in self.commits}\n        else:\n            already_applied = set()\n\n        # iterate over the commits applied on the main branch and filter out\n        # the ones that are included in the jira release\n        patches_to_pick = []\n        for c in commits:\n            key = c.issue\n            # For the release we assume all issues that have to be\n            # cherry-picked are merged with the GH issue id instead of the\n            # JIRA ARROW one. That's why we use github_issues along with\n            # issues. This is only to correct the mapping for migrated issues.\n            if c.issue and c.issue.startswith(\"GH-\"):\n                key = int(c.issue_id)\n            if ((key in self.github_issue_ids or key in self.issues) and\n                    c.title not in already_applied):\n                patches_to_pick.append(c)\n        return reversed(patches_to_pick)\n\n    def cherry_pick_commits(self, recreate_branch=True):\n        if recreate_branch:\n            # delete, create and checkout the maintenance branch based off of\n            # the previous tag\n            if self.branch in self.repo.branches:\n                logger.info(f\"Deleting branch {self.branch}\")\n                self.repo.git.branch('-D', self.branch)\n            logger.info(\n                f\"Creating branch {self.branch} from {self.base_branch} branch\"\n            )\n            self.repo.git.checkout(self.base_branch, b=self.branch)\n        else:\n            # just checkout the already existing maintenance branch\n            logger.info(f\"Checking out branch {self.branch}\")\n            self.repo.git.checkout(self.branch)\n\n        # cherry pick the commits based on the jira tickets\n        for commit in self.commits_to_pick():\n            logger.info(f\"Cherry-picking commit {commit.hexsha}\")\n            self.repo.git.cherry_pick(commit.hexsha)\n\n\nclass MajorRelease(Release):\n\n    @property\n    def branch(self):\n        return f\"maint-{self.version}\"\n\n    @property\n    def base_branch(self):\n        return self.default_branch\n\n    @cached_property\n    def siblings(self):\n        \"\"\"\n        Filter only the major releases.\n        \"\"\"\n        # handle minor releases before 1.0 as major releases\n        return [v for v in self.issue_tracker.project_versions()\n                if v.patch == 0 and (v.major == 0 or v.minor == 0)]\n\n\nclass MinorRelease(Release):\n\n    @property\n    def branch(self):\n        return f\"maint-{self.version.major}.x.x\"\n\n    @property\n    def base_branch(self):\n        return self.previous.tag\n\n    @cached_property\n    def siblings(self):\n        \"\"\"\n        Filter the major and minor releases.\n        \"\"\"\n        return [v for v in self.issue_tracker.project_versions()\n                if v.patch == 0]\n\n\nclass PatchRelease(Release):\n\n    @property\n    def branch(self):\n        return f\"maint-{self.version.major}.{self.version.minor}.x\"\n\n    @property\n    def base_branch(self):\n        return self.previous.tag\n\n    @cached_property\n    def siblings(self):\n        \"\"\"\n        No filtering, consider all releases.\n        \"\"\"\n        return self.issue_tracker.project_versions()\n", "dev/archery/archery/release/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom .core import Release, MajorRelease, MinorRelease, PatchRelease  # noqa\n", "dev/archery/archery/release/tests/test_release.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\n\nfrom archery.release.core import (\n    Release, MajorRelease, MinorRelease, PatchRelease,\n    IssueTracker, Version, Issue, CommitTitle, Commit\n)\nfrom archery.testing import DotDict\n\n\n# subset of issues per revision\n_issues = {\n    \"3.0.0\": [\n        Issue(\"GH-9784\", type=\"Bug\", summary=\"[C++] Title\"),\n        Issue(\"GH-9767\", type=\"New Feature\", summary=\"[Crossbow] Title\"),\n        Issue(\"GH-1231\", type=\"Bug\", summary=\"[Java] Title\"),\n        Issue(\"GH-1244\", type=\"Bug\", summary=\"[C++] Title\"),\n        Issue(\"GH-1301\", type=\"Bug\", summary=\"[Python][Archery] Title\")\n    ],\n    \"2.0.0\": [\n        Issue(\"ARROW-9784\", type=\"Bug\", summary=\"[Java] Title\"),\n        Issue(\"ARROW-9767\", type=\"New Feature\", summary=\"[Crossbow] Title\"),\n        Issue(\"GH-1230\", type=\"Bug\", summary=\"[Dev] Title\"),\n        Issue(\"ARROW-9694\", type=\"Bug\", summary=\"[Release] Title\"),\n        Issue(\"ARROW-5643\", type=\"Bug\", summary=\"[Go] Title\"),\n        Issue(\"GH-1243\", type=\"Bug\", summary=\"[Python] Title\"),\n        Issue(\"GH-1300\", type=\"Bug\", summary=\"[CI][Archery] Title\")\n    ],\n    \"1.0.1\": [\n        Issue(\"ARROW-9684\", type=\"Bug\", summary=\"[C++] Title\"),\n        Issue(\"ARROW-9667\", type=\"New Feature\", summary=\"[Crossbow] Title\"),\n        Issue(\"ARROW-9659\", type=\"Bug\", summary=\"[C++] Title\"),\n        Issue(\"ARROW-9644\", type=\"Bug\", summary=\"[C++][Dataset] Title\"),\n        Issue(\"ARROW-9643\", type=\"Bug\", summary=\"[C++] Title\"),\n        Issue(\"ARROW-9609\", type=\"Bug\", summary=\"[C++] Title\"),\n        Issue(\"ARROW-9606\", type=\"Bug\", summary=\"[C++][Dataset] Title\")\n    ],\n    \"1.0.0\": [\n        Issue(\"ARROW-300\", type=\"New Feature\", summary=\"[Format] Title\"),\n        Issue(\"ARROW-4427\", type=\"Task\", summary=\"[Doc] Title\"),\n        Issue(\"ARROW-5035\", type=\"Improvement\", summary=\"[C#] Title\"),\n        Issue(\"ARROW-8473\", type=\"Bug\", summary=\"[Rust] Title\"),\n        Issue(\"ARROW-8472\", type=\"Bug\", summary=\"[Go][Integration] Title\"),\n        Issue(\"ARROW-8471\", type=\"Bug\", summary=\"[C++][Integration] Title\"),\n        Issue(\"ARROW-8974\", type=\"Improvement\", summary=\"[C++] Title\"),\n        Issue(\"ARROW-8973\", type=\"New Feature\", summary=\"[Java] Title\")\n    ],\n    \"0.17.1\": [\n        Issue(\"ARROW-8684\", type=\"Bug\", summary=\"[Python] Title\"),\n        Issue(\"ARROW-8657\", type=\"Bug\", summary=\"[C++][Parquet] Title\"),\n        Issue(\"ARROW-8641\", type=\"Bug\", summary=\"[Python] Title\"),\n        Issue(\"ARROW-8609\", type=\"Bug\", summary=\"[C++] Title\"),\n    ],\n    \"0.17.0\": [\n        Issue(\"ARROW-2882\", type=\"New Feature\", summary=\"[C++][Python] Title\"),\n        Issue(\"ARROW-2587\", type=\"Bug\", summary=\"[Python] Title\"),\n        Issue(\"ARROW-2447\", type=\"Improvement\", summary=\"[C++] Title\"),\n        Issue(\"ARROW-2255\", type=\"Bug\", summary=\"[Integration] Title\"),\n        Issue(\"ARROW-1907\", type=\"Bug\", summary=\"[C++/Python] Title\"),\n        Issue(\"ARROW-1636\", type=\"New Feature\", summary=\"[Format] Title\")\n    ]\n}\n\n\nclass FakeIssueTracker(IssueTracker):\n\n    def __init__(self):\n        pass\n\n    def project_versions(self):\n        return [\n            Version.parse(\"4.0.0\", released=False),\n            Version.parse(\"3.0.0\", released=False),\n            Version.parse(\"2.0.0\", released=False),\n            Version.parse(\"1.1.0\", released=False),\n            Version.parse(\"1.0.1\", released=False),\n            Version.parse(\"1.0.0\", released=True),\n            Version.parse(\"0.17.1\", released=True),\n            Version.parse(\"0.17.0\", released=True),\n            Version.parse(\"0.16.0\", released=True),\n            Version.parse(\"0.15.2\", released=True),\n            Version.parse(\"0.15.1\", released=True),\n            Version.parse(\"0.15.0\", released=True),\n        ]\n\n    def project_issues(self, version):\n        return _issues[str(version)]\n\n\n@pytest.fixture\ndef fake_issue_tracker():\n    return FakeIssueTracker()\n\n\ndef test_version(fake_issue_tracker):\n    v = Version.parse(\"1.2.5\")\n    assert str(v) == \"1.2.5\"\n    assert v.major == 1\n    assert v.minor == 2\n    assert v.patch == 5\n    assert v.released is False\n    assert v.release_date is None\n\n    v = Version.parse(\"1.0.0\", released=True, release_date=\"2020-01-01\")\n    assert str(v) == \"1.0.0\"\n    assert v.major == 1\n    assert v.minor == 0\n    assert v.patch == 0\n    assert v.released is True\n    assert v.release_date == \"2020-01-01\"\n\n\ndef test_issue(fake_issue_tracker):\n    i = Issue(\"ARROW-1234\", type='Bug', summary=\"title\")\n    assert i.key == \"ARROW-1234\"\n    assert i.type == \"Bug\"\n    assert i.summary == \"title\"\n    assert i.project == \"ARROW\"\n    assert i.number == 1234\n\n    i = Issue(\"PARQUET-1111\", type='Improvement', summary=\"another title\")\n    assert i.key == \"PARQUET-1111\"\n    assert i.type == \"Improvement\"\n    assert i.summary == \"another title\"\n    assert i.project == \"PARQUET\"\n    assert i.number == 1111\n\n    fake_jira_issue = DotDict({\n        'key': 'ARROW-2222',\n        'fields': {\n            'issuetype': {\n                'name': 'Feature'\n            },\n            'summary': 'Issue title'\n        }\n    })\n    i = Issue.from_jira(fake_jira_issue)\n    assert i.key == \"ARROW-2222\"\n    assert i.type == \"Feature\"\n    assert i.summary == \"Issue title\"\n    assert i.project == \"ARROW\"\n    assert i.number == 2222\n\n\ndef test_commit_title():\n    t = CommitTitle.parse(\n        \"ARROW-9598: [C++][Parquet] Fix writing nullable structs\"\n    )\n    assert t.project == \"ARROW\"\n    assert t.issue == \"ARROW-9598\"\n    assert t.components == [\"C++\", \"Parquet\"]\n    assert t.summary == \"Fix writing nullable structs\"\n    assert t.minor is False\n\n    t = CommitTitle.parse(\n        \"ARROW-8002: [C++][Dataset][R] Support partitioned dataset writing\"\n    )\n    assert t.project == \"ARROW\"\n    assert t.issue == \"ARROW-8002\"\n    assert t.components == [\"C++\", \"Dataset\", \"R\"]\n    assert t.summary == \"Support partitioned dataset writing\"\n    assert t.minor is False\n\n    t = CommitTitle.parse(\n        \"ARROW-9600: [Rust][Arrow] pin older version of proc-macro2 during \"\n        \"build\"\n    )\n    assert t.project == \"ARROW\"\n    assert t.issue == \"ARROW-9600\"\n    assert t.components == [\"Rust\", \"Arrow\"]\n    assert t.summary == \"pin older version of proc-macro2 during build\"\n    assert t.minor is False\n\n    t = CommitTitle.parse(\"[Release] Update versions for 1.0.0\")\n    assert t.project is None\n    assert t.issue is None\n    assert t.components == [\"Release\"]\n    assert t.summary == \"Update versions for 1.0.0\"\n    assert t.minor is False\n\n    t = CommitTitle.parse(\"MINOR: [Release] Update versions for 1.0.0\")\n    assert t.project is None\n    assert t.issue is None\n    assert t.components == [\"Release\"]\n    assert t.summary == \"Update versions for 1.0.0\"\n    assert t.minor is True\n\n    t = CommitTitle.parse(\"[Python][Doc] Fix rst role dataset.rst (#7725)\")\n    assert t.project is None\n    assert t.issue is None\n    assert t.components == [\"Python\", \"Doc\"]\n    assert t.summary == \"Fix rst role dataset.rst (#7725)\"\n    assert t.minor is False\n\n    t = CommitTitle.parse(\n        \"PARQUET-1882: [C++] Buffered Reads should allow for 0 length\"\n    )\n    assert t.project == 'PARQUET'\n    assert t.issue == 'PARQUET-1882'\n    assert t.components == [\"C++\"]\n    assert t.summary == \"Buffered Reads should allow for 0 length\"\n    assert t.minor is False\n\n    t = CommitTitle.parse(\n        \"ARROW-9340 [R] Use CRAN version of decor package \"\n        \"\\nsomething else\\n\"\n        \"\\nwhich should be truncated\"\n    )\n    assert t.project == 'ARROW'\n    assert t.issue == 'ARROW-9340'\n    assert t.components == [\"R\"]\n    assert t.summary == \"Use CRAN version of decor package \"\n    assert t.minor is False\n\n\ndef test_release_basics(fake_issue_tracker):\n    r = Release(\"1.0.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r, MajorRelease)\n    assert r.is_released is True\n    assert r.branch == 'maint-1.0.0'\n    assert r.tag == 'apache-arrow-1.0.0'\n\n    r = Release(\"1.1.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r, MinorRelease)\n    assert r.is_released is False\n    assert r.branch == 'maint-1.x.x'\n    assert r.tag == 'apache-arrow-1.1.0'\n\n    # minor releases before 1.0 are treated as major releases\n    r = Release(\"0.17.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r, MajorRelease)\n    assert r.is_released is True\n    assert r.branch == 'maint-0.17.0'\n    assert r.tag == 'apache-arrow-0.17.0'\n\n    r = Release(\"0.17.1\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r, PatchRelease)\n    assert r.is_released is True\n    assert r.branch == 'maint-0.17.x'\n    assert r.tag == 'apache-arrow-0.17.1'\n\n\ndef test_previous_and_next_release(fake_issue_tracker):\n    r = Release(\"4.0.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r.previous, MajorRelease)\n    assert r.previous.version == Version.parse(\"3.0.0\")\n    with pytest.raises(ValueError, match=\"There is no upcoming release set\"):\n        assert r.next\n\n    r = Release(\"3.0.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r.previous, MajorRelease)\n    assert isinstance(r.next, MajorRelease)\n    assert r.previous.version == Version.parse(\"2.0.0\")\n    assert r.next.version == Version.parse(\"4.0.0\")\n\n    r = Release(\"1.1.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r.previous, MajorRelease)\n    assert isinstance(r.next, MajorRelease)\n    assert r.previous.version == Version.parse(\"1.0.0\")\n    assert r.next.version == Version.parse(\"2.0.0\")\n\n    r = Release(\"1.0.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r.next, MajorRelease)\n    assert isinstance(r.previous, MajorRelease)\n    assert r.previous.version == Version.parse(\"0.17.0\")\n    assert r.next.version == Version.parse(\"2.0.0\")\n\n    r = Release(\"0.17.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r.previous, MajorRelease)\n    assert r.previous.version == Version.parse(\"0.16.0\")\n\n    r = Release(\"0.15.2\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r.previous, PatchRelease)\n    assert isinstance(r.next, MajorRelease)\n    assert r.previous.version == Version.parse(\"0.15.1\")\n    assert r.next.version == Version.parse(\"0.16.0\")\n\n    r = Release(\"0.15.1\", repo=None, issue_tracker=fake_issue_tracker)\n    assert isinstance(r.previous, MajorRelease)\n    assert isinstance(r.next, PatchRelease)\n    assert r.previous.version == Version.parse(\"0.15.0\")\n    assert r.next.version == Version.parse(\"0.15.2\")\n\n\ndef test_release_issues(fake_issue_tracker):\n    # major release issues\n    r = Release(\"1.0.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert r.issues.keys() == set([\n        \"ARROW-300\",\n        \"ARROW-4427\",\n        \"ARROW-5035\",\n        \"ARROW-8473\",\n        \"ARROW-8472\",\n        \"ARROW-8471\",\n        \"ARROW-8974\",\n        \"ARROW-8973\"\n    ])\n    # minor release issues\n    r = Release(\"0.17.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert r.issues.keys() == set([\n        \"ARROW-2882\",\n        \"ARROW-2587\",\n        \"ARROW-2447\",\n        \"ARROW-2255\",\n        \"ARROW-1907\",\n        \"ARROW-1636\",\n    ])\n    # patch release issues\n    r = Release(\"1.0.1\", repo=None, issue_tracker=fake_issue_tracker)\n    assert r.issues.keys() == set([\n        \"ARROW-9684\",\n        \"ARROW-9667\",\n        \"ARROW-9659\",\n        \"ARROW-9644\",\n        \"ARROW-9643\",\n        \"ARROW-9609\",\n        \"ARROW-9606\"\n    ])\n    r = Release(\"2.0.0\", repo=None, issue_tracker=fake_issue_tracker)\n    assert r.issues.keys() == set([\n        \"ARROW-9784\",\n        \"ARROW-9767\",\n        \"GH-1230\",\n        \"ARROW-9694\",\n        \"ARROW-5643\",\n        \"GH-1243\",\n        \"GH-1300\"\n    ])\n\n\n@pytest.mark.parametrize(('version', 'ncommits'), [\n    (\"1.0.0\", 771),\n    (\"0.17.1\", 27),\n    (\"0.17.0\", 569),\n    (\"0.15.1\", 41)\n])\ndef test_release_commits(fake_issue_tracker, version, ncommits):\n    r = Release(version, repo=None, issue_tracker=fake_issue_tracker)\n    assert len(r.commits) == ncommits\n    for c in r.commits:\n        assert isinstance(c, Commit)\n        assert isinstance(c.title, CommitTitle)\n        assert c.url.endswith(c.hexsha)\n\n\ndef test_maintenance_patch_selection(fake_issue_tracker):\n    r = Release(\"0.17.1\", repo=None, issue_tracker=fake_issue_tracker)\n\n    shas_to_pick = [\n        c.hexsha for c in r.commits_to_pick(exclude_already_applied=False)\n    ]\n    expected = [\n        '8939b4bd446ee406d5225c79d563a27d30fd7d6d',\n        'bcef6c95a324417e85e0140f9745d342cd8784b3',\n        '6002ec388840de5622e39af85abdc57a2cccc9b2',\n        '9123dadfd123bca7af4eaa9455f5b0d1ca8b929d',\n    ]\n    assert shas_to_pick == expected\n", "dev/archery/archery/crossbow/reports.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport collections\nimport csv\nimport operator\nimport fnmatch\nimport functools\nimport time\n\nimport click\nimport requests\n\nfrom archery.utils.report import JinjaReport\n\n\n# TODO(kszucs): use archery.report.JinjaReport instead\nclass Report:\n\n    ROW_HEADERS = [\n        \"task_name\",\n        \"task_status\",\n        \"build_links\",\n        \"crossbow_branch_url\",\n        \"ci_system\",\n        \"extra_params\",\n        \"template\",\n        \"arrow_commit\",\n    ]\n\n    def __init__(self, job, task_filters=None, wait_for_task=None):\n        self.job = job\n\n        tasks = sorted(job.tasks.items())\n        if task_filters:\n            filtered = set()\n            for pattern in task_filters:\n                filtered |= set(fnmatch.filter(job.tasks.keys(), pattern))\n\n            tasks = [(name, task) for name, task in tasks if name in filtered]\n\n        self._tasks = dict(tasks)\n        self._wait_for_task = wait_for_task\n\n    @property\n    def repo_url(self):\n        url = self.job.queue.remote_url\n        return url[:-4] if url.endswith('.git') else url\n\n    def url(self, query):\n        return '{}/branches/all?query={}'.format(self.repo_url, query)\n\n    def branch_url(self, branch):\n        return '{}/tree/{}'.format(self.repo_url, branch)\n\n    def task_url(self, task):\n        build_links = task.status().build_links\n        # Only wait if the link to the actual build is not present\n        # and refresh task status.\n        if not build_links and self._wait_for_task:\n            time.sleep(self._wait_for_task)\n            build_links = task.status(force_query=True).build_links\n        if build_links:\n            # show link to the actual build, some CI providers implement\n            # the statuses API others implement the checks API, retrieve any.\n            return build_links[0]\n        else:\n            # show link to the branch if no status build link was found.\n            return self.branch_url(task.branch)\n\n    @property\n    @functools.lru_cache(maxsize=1)\n    def tasks_by_state(self):\n        tasks_by_state = collections.defaultdict(dict)\n        for task_name, task in self.job.tasks.items():\n            state = task.status().combined_state\n            tasks_by_state[state][task_name] = task\n        return tasks_by_state\n\n    @property\n    def contains_failures(self):\n        return any(self.tasks_by_state[state] for state in (\n            \"error\", \"failure\"))\n\n    @property\n    def tasks(self):\n        return self._tasks\n\n    def show(self):\n        raise NotImplementedError()\n\n    @property\n    def rows(self):\n        \"\"\"\n        Produces a generator that allow us to iterate over\n        the job tasks as a list of rows.\n        Row headers are defined at Report.ROW_HEADERS.\n        \"\"\"\n        for task_name, task in sorted(self.job.tasks.items()):\n            task_status = task.status()\n            row = [\n                task_name,\n                task_status.combined_state,\n                task_status.build_links,\n                self.branch_url(task.branch),\n                task.ci,\n                # We want this to be serialized as a dict instead\n                # of an orderedict.\n                {k: v for k, v in task.params.items()},\n                task.template,\n                # Arrow repository commit\n                self.job.target.head\n            ]\n            yield row\n\n\nclass ConsoleReport(Report):\n    \"\"\"Report the status of a Job to the console using click\"\"\"\n\n    # output table's header template\n    HEADER = '[{state:>7}] {branch:<52} {content:>16}'\n    DETAILS = ' \u2514 {url}'\n\n    # output table's row template for assets\n    ARTIFACT_NAME = '{artifact:>69} '\n    ARTIFACT_STATE = '[{state:>7}]'\n\n    # state color mapping to highlight console output\n    COLORS = {\n        # from CombinedStatus\n        'error': 'red',\n        'failure': 'red',\n        'pending': 'yellow',\n        'success': 'green',\n        # custom state messages\n        'ok': 'green',\n        'missing': 'red'\n    }\n\n    def lead(self, state, branch, n_uploaded, n_expected):\n        line = self.HEADER.format(\n            state=state.upper(),\n            branch=branch,\n            content='uploaded {} / {}'.format(n_uploaded, n_expected)\n        )\n        return click.style(line, fg=self.COLORS[state.lower()])\n\n    def header(self):\n        header = self.HEADER.format(\n            state='state',\n            branch='Task / Branch',\n            content='Artifacts'\n        )\n        delimiter = '-' * len(header)\n        return '{}\\n{}'.format(header, delimiter)\n\n    def artifact(self, state, pattern, asset):\n        if asset is None:\n            artifact = pattern\n            state = 'pending' if state == 'pending' else 'missing'\n        else:\n            artifact = asset.name\n            state = 'ok'\n\n        name_ = self.ARTIFACT_NAME.format(artifact=artifact)\n        state_ = click.style(\n            self.ARTIFACT_STATE.format(state=state.upper()),\n            self.COLORS[state]\n        )\n        return name_ + state_\n\n    def show(self, outstream, asset_callback=None, validate_patterns=True):\n        echo = functools.partial(click.echo, file=outstream)\n\n        # write table's header\n        echo(self.header())\n\n        # write table's body\n        for task_name, task in self.tasks.items():\n            # write summary of the uploaded vs total assets\n            status = task.status()\n            assets = task.assets(validate_patterns=validate_patterns)\n\n            # mapping of artifact pattern to asset or None of not uploaded\n            n_expected = len(task.artifacts)\n            n_uploaded = len(assets.uploaded_assets())\n            echo(self.lead(status.combined_state, task_name, n_uploaded,\n                           n_expected))\n\n            # show link to the actual build, some of the CI providers implement\n            # the statuses API others implement the checks API, so display both\n            for link in status.build_links:\n                echo(self.DETAILS.format(url=link))\n\n            # write per asset status\n            for artifact_pattern, asset in assets.items():\n                if asset_callback is not None:\n                    asset_callback(task_name, task, asset)\n                echo(self.artifact(status.combined_state, artifact_pattern,\n                                   asset))\n\n\nclass ChatReport(JinjaReport):\n    templates = {\n        'text': 'chat_nightly_report.txt.j2',\n    }\n    fields = [\n        'report',\n        'extra_message_success',\n        'extra_message_failure',\n    ]\n\n\nclass ReportUtils:\n\n    @classmethod\n    def send_message(cls, webhook, message):\n        resp = requests.post(webhook, json={\n            \"blocks\": [\n                {\n                    \"type\": \"section\",\n                    \"text\": {\n                            \"type\": \"mrkdwn\",\n                            \"text\": message\n                    }\n                }\n            ]\n        }\n        )\n        return resp\n\n    @classmethod\n    def send_email(cls, smtp_user, smtp_password, smtp_server, smtp_port,\n                   recipient_email, message):\n        import smtplib\n\n        server = smtplib.SMTP_SSL(smtp_server, smtp_port)\n        server.ehlo()\n        server.login(smtp_user, smtp_password)\n        server.sendmail(smtp_user, recipient_email, message)\n        server.close()\n\n    @classmethod\n    def write_csv(cls, report, add_headers=True):\n        with open(f'{report.job.branch}.csv', 'w') as csvfile:\n            task_writer = csv.writer(csvfile)\n            if add_headers:\n                task_writer.writerow(report.ROW_HEADERS)\n            task_writer.writerows(report.rows)\n\n\nclass EmailReport(JinjaReport):\n    templates = {\n        'nightly_report': 'email_nightly_report.txt.j2',\n        'token_expiration': 'email_token_expiration.txt.j2',\n    }\n    fields = [\n        'report',\n        'sender_name',\n        'sender_email',\n        'recipient_email',\n    ]\n\n\nclass CommentReport(Report):\n\n    _markdown_badge = '[![{title}]({badge})]({{url}})'\n\n    badges = {\n        'github': _markdown_badge.format(\n            title='GitHub Actions',\n            badge=(\n                'https://github.com/{repo}/actions/workflows/crossbow.yml/'\n                'badge.svg?branch={branch}'\n            ),\n        ),\n        'azure': _markdown_badge.format(\n            title='Azure',\n            badge=(\n                'https://dev.azure.com/{repo}/_apis/build/status/'\n                '{repo_dotted}?branchName={branch}'\n            )\n        ),\n        'travis': _markdown_badge.format(\n            title='Travis CI',\n            badge='https://img.shields.io/travis/{repo}/{branch}.svg'\n        ),\n        'circle': _markdown_badge.format(\n            title='CircleCI',\n            badge=(\n                'https://img.shields.io/circleci/build/github'\n                '/{repo}/{branch}.svg'\n            )\n        ),\n        'appveyor': _markdown_badge.format(\n            title='AppVeyor',\n            badge='https://img.shields.io/appveyor/ci/{repo}/{branch}.svg'\n        ),\n        'drone': _markdown_badge.format(\n            title='Drone',\n            badge='https://img.shields.io/drone/build/{repo}/{branch}.svg'\n        ),\n    }\n\n    def __init__(self, job, crossbow_repo, wait_for_task=None):\n        self.crossbow_repo = crossbow_repo\n        super().__init__(job, wait_for_task=wait_for_task)\n\n    def show(self):\n        url = 'https://github.com/{repo}/branches/all?query={branch}'\n        sha = self.job.target.head\n\n        msg = 'Revision: {}\\n\\n'.format(sha)\n        msg += 'Submitted crossbow builds: [{repo} @ {branch}]'\n        msg += '({})\\n'.format(url)\n        msg += '\\n|Task|Status|\\n|----|------|'\n\n        tasks = sorted(self.job.tasks.items(), key=operator.itemgetter(0))\n        for key, task in tasks:\n            branch = task.branch\n\n            try:\n                template = self.badges[task.ci]\n                badge = template.format(\n                    repo=self.crossbow_repo,\n                    repo_dotted=self.crossbow_repo.replace('/', '.'),\n                    branch=branch,\n                    url=self.task_url(task)\n                )\n            except KeyError:\n                badge = 'unsupported CI service `{}`'.format(task.ci)\n\n            msg += '\\n|{}|{}|'.format(key, badge)\n\n        return msg.format(repo=self.crossbow_repo, branch=self.job.branch)\n", "dev/archery/archery/crossbow/cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom datetime import date\nfrom pathlib import Path\nimport time\nimport sys\n\nimport click\n\nfrom .core import Config, Repo, Queue, Target, Job, CrossbowError\nfrom .reports import (ChatReport, Report, ReportUtils, ConsoleReport,\n                      EmailReport, CommentReport)\nfrom ..utils.source import ArrowSources\n\n\n_default_arrow_path = ArrowSources.find().path\n_default_queue_path = _default_arrow_path.parent / \"crossbow\"\n_default_config_path = _default_arrow_path / \"dev\" / \"tasks\" / \"tasks.yml\"\n\n\n@click.group()\n@click.option('--github-token', '-t', default=None,\n              envvar=\"CROSSBOW_GITHUB_TOKEN\",\n              help='OAuth token for GitHub authentication')\n@click.option('--arrow-path', '-a',\n              type=click.Path(), default=_default_arrow_path,\n              help='Arrow\\'s repository path. Defaults to the repository of '\n                   'this script')\n@click.option('--queue-path', '-q',\n              envvar=\"CROSSBOW_QUEUE_PATH\",\n              type=click.Path(), default=_default_queue_path,\n              help='The repository path used for scheduling the tasks. '\n                   'Defaults to crossbow directory placed next to arrow')\n@click.option('--queue-remote', '-qr', default=None,\n              help='Force to use this remote URL for the Queue repository')\n@click.option('--output-file', metavar='<output>',\n              type=click.File('w', encoding='utf8'), default='-',\n              help='Capture output result into file.')\n@click.pass_context\ndef crossbow(ctx, github_token, arrow_path, queue_path, queue_remote,\n             output_file):\n    \"\"\"\n    Schedule packaging tasks or nightly builds on CI services.\n    \"\"\"\n    ctx.ensure_object(dict)\n    ctx.obj['output'] = output_file\n    ctx.obj['arrow'] = Repo(arrow_path)\n    ctx.obj['queue'] = Queue(queue_path, remote_url=queue_remote,\n                             github_token=github_token, require_https=True)\n\n\n@crossbow.command()\n@click.option('--config-path', '-c',\n              type=click.Path(exists=True), default=_default_config_path,\n              help='Task configuration yml. Defaults to tasks.yml')\n@click.pass_obj\ndef check_config(obj, config_path):\n    # load available tasks configuration and groups from yaml\n    config = Config.load_yaml(config_path)\n    config.validate()\n\n    output = obj['output']\n    config.show(output)\n\n\n@crossbow.command()\n@click.argument('tasks', nargs=-1, required=False)\n@click.option('--group', '-g', 'groups', multiple=True,\n              help='Submit task groups as defined in task.yml')\n@click.option('--param', '-p', 'params', multiple=True,\n              help='Additional task parameters for rendering the CI templates')\n@click.option('--job-prefix', default='build',\n              help='Arbitrary prefix for branch names, e.g. nightly')\n@click.option('--config-path', '-c',\n              type=click.Path(exists=True), default=_default_config_path,\n              help='Task configuration yml. Defaults to tasks.yml')\n@click.option('--arrow-version', '-v', default=None,\n              help='Set target version explicitly.')\n@click.option('--arrow-remote', '-r', default=None,\n              help='Set GitHub remote explicitly, which is going to be cloned '\n                   'on the CI services. Note, that no validation happens '\n                   'locally. Examples: https://github.com/apache/arrow or '\n                   'https://github.com/kszucs/arrow.')\n@click.option('--arrow-branch', '-b', default=None,\n              help='Give the branch name explicitly, e.g. ARROW-1949.')\n@click.option('--arrow-sha', '-t', default=None,\n              help='Set commit SHA or Tag name explicitly, e.g. f67a515, '\n                   'apache-arrow-0.11.1.')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.option('--dry-run/--commit', default=False,\n              help='Just display the rendered CI configurations without '\n                   'committing them')\n@click.option('--no-push/--push', default=False,\n              help='Don\\'t push the changes')\n@click.pass_obj\ndef submit(obj, tasks, groups, params, job_prefix, config_path, arrow_version,\n           arrow_remote, arrow_branch, arrow_sha, fetch, dry_run, no_push):\n    output = obj['output']\n    queue, arrow = obj['queue'], obj['arrow']\n\n    # load available tasks configuration and groups from yaml\n    config = Config.load_yaml(config_path)\n    try:\n        config.validate()\n    except CrossbowError as e:\n        raise click.ClickException(str(e))\n\n    # Override the detected repo url / remote, branch and sha - this aims to\n    # make release procedure a bit simpler.\n    # Note, that the target revision's crossbow templates must be\n    # compatible with the locally checked out version of crossbow (which is\n    # in case of the release procedure), because the templates still\n    # contain some business logic (dependency installation, deployments)\n    # which will be reduced to a single command in the future.\n    target = Target.from_repo(arrow, remote=arrow_remote, branch=arrow_branch,\n                              head=arrow_sha, version=arrow_version)\n\n    # parse additional job parameters\n    params = dict([p.split(\"=\") for p in params])\n\n    # instantiate the job object\n    try:\n        job = Job.from_config(config=config, target=target, tasks=tasks,\n                              groups=groups, params=params)\n    except CrossbowError as e:\n        raise click.ClickException(str(e))\n\n    job.show(output)\n    if dry_run:\n        return\n\n    if fetch:\n        queue.fetch()\n    queue.put(job, prefix=job_prefix)\n\n    if no_push:\n        click.echo('Branches and commits created but not pushed: `{}`'\n                   .format(job.branch))\n    else:\n        queue.push()\n        click.echo('Pushed job identifier is: `{}`'.format(job.branch))\n\n\n@crossbow.command()\n@click.option('--base-branch', default=None,\n              help='Set base branch for the PR.')\n@click.option('--create-pr', is_flag=True, default=False,\n              help='Create GitHub Pull Request')\n@click.option('--head-branch', default=None,\n              help='Give the branch name explicitly, e.g. release-9.0.0-rc0')\n@click.option('--pr-body', default=None,\n              help='Set body for the PR.')\n@click.option('--pr-title', default=None,\n              help='Set title for the PR.')\n@click.option('--remote', default=None,\n              help='Set GitHub remote explicitly, which is going to be used '\n                   'for the PR. Note, that no validation happens '\n                   'locally. Examples: https://github.com/apache/arrow or '\n                   'https://github.com/raulcd/arrow.')\n@click.option('--rc', default=None,\n              help='Release Candidate number.')\n@click.option('--version', default=None,\n              help='Release version.')\n@click.option('--verify-binaries', is_flag=True, default=False,\n              help='Trigger the verify binaries jobs')\n@click.option('--verify-source', is_flag=True, default=False,\n              help='Trigger the verify source jobs')\n@click.option('--verify-wheels', is_flag=True, default=False,\n              help='Trigger the verify wheels jobs')\n@click.pass_obj\ndef verify_release_candidate(obj, base_branch, create_pr,\n                             head_branch, pr_body, pr_title, remote,\n                             rc, version, verify_binaries, verify_source,\n                             verify_wheels):\n    # The verify-release-candidate command will create a PR (or find one)\n    # and add the verify-rc* comment to trigger the verify tasks\n\n    # Redefine Arrow repo to use the correct arrow remote.\n    arrow = Repo(path=obj['arrow'].path, remote_url=remote)\n\n    response = arrow.github_pr(title=pr_title, head=head_branch,\n                               base=base_branch, body=pr_body,\n                               github_token=obj['queue'].github_token,\n                               create=create_pr)\n\n    # If we want to trigger any verification job we add a comment to the PR.\n    verify_flags = [verify_source, verify_binaries, verify_wheels]\n    if any(verify_flags):\n        command = \"@github-actions crossbow submit\"\n        verify_groups = [\"verify-rc-source\",\n                         \"verify-rc-binaries\", \"verify-rc-wheels\"]\n        job_groups = \"\"\n        for flag, group in zip(verify_flags, verify_groups):\n            if flag:\n                job_groups += f\" --group {group}\"\n        response.create_comment(\n            f\"{command} {job_groups} --param \" +\n            f\"release={version} --param rc={rc}\")\n\n\n@crossbow.command()\n@click.argument('task', required=True)\n@click.option('--config-path', '-c',\n              type=click.Path(exists=True), default=_default_config_path,\n              help='Task configuration yml. Defaults to tasks.yml')\n@click.option('--arrow-version', '-v', default=None,\n              help='Set target version explicitly.')\n@click.option('--arrow-remote', '-r', default=None,\n              help='Set GitHub remote explicitly, which is going to be cloned '\n                   'on the CI services. Note, that no validation happens '\n                   'locally. Examples: https://github.com/apache/arrow or '\n                   'https://github.com/kszucs/arrow.')\n@click.option('--arrow-branch', '-b', default=None,\n              help='Give the branch name explicitly, e.g. ARROW-1949.')\n@click.option('--arrow-sha', '-t', default=None,\n              help='Set commit SHA or Tag name explicitly, e.g. f67a515, '\n                   'apache-arrow-0.11.1.')\n@click.option('--param', '-p', 'params', multiple=True,\n              help='Additional task parameters for rendering the CI templates')\n@click.pass_obj\ndef render(obj, task, config_path, arrow_version, arrow_remote, arrow_branch,\n           arrow_sha, params):\n    \"\"\"\n    Utility command to check the rendered CI templates.\n    \"\"\"\n    from .core import _flatten\n\n    def highlight(code):\n        try:\n            from pygments import highlight\n            from pygments.lexers import YamlLexer\n            from pygments.formatters import TerminalFormatter\n            return highlight(code, YamlLexer(), TerminalFormatter())\n        except ImportError:\n            return code\n\n    arrow = obj['arrow']\n\n    target = Target.from_repo(arrow, remote=arrow_remote, branch=arrow_branch,\n                              head=arrow_sha, version=arrow_version)\n    config = Config.load_yaml(config_path)\n    params = dict([p.split(\"=\") for p in params])\n    params[\"queue_remote_url\"] = \"https://github.com/org/crossbow\"\n    job = Job.from_config(config=config, target=target, tasks=[task],\n                          params=params)\n\n    for task_name, rendered_files in job.render_tasks().items():\n        for path, content in _flatten(rendered_files).items():\n            click.echo('#' * 80)\n            click.echo('### {:^72} ###'.format(\"/\".join(path)))\n            click.echo('#' * 80)\n            click.echo(highlight(content))\n\n\n@crossbow.command()\n@click.argument('job-name', required=True)\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.option('--task-filter', '-f', 'task_filters', multiple=True,\n              help='Glob pattern for filtering relevant tasks')\n@click.option('--validate/--no-validate', default=False,\n              help='Return non-zero exit code '\n                   'if there is any non-success task')\n@click.pass_obj\ndef status(obj, job_name, fetch, task_filters, validate):\n    output = obj['output']\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n    job = queue.get(job_name)\n\n    success = True\n\n    def asset_callback(task_name, task, asset):\n        nonlocal success\n        if task.status().combined_state in {'error', 'failure'}:\n            success = False\n        if asset is None:\n            success = False\n\n    report = ConsoleReport(job, task_filters=task_filters)\n    report.show(output, asset_callback=asset_callback)\n    if validate and not success:\n        sys.exit(1)\n\n\n@crossbow.command()\n@click.option('--arrow-remote', '-r', default=None,\n              help='Set GitHub remote explicitly, which is going to be cloned '\n                   'on the CI services. Note, that no validation happens '\n                   'locally. Examples: \"https://github.com/apache/arrow\" or '\n                   '\"raulcd/arrow\".')\n@click.option('--crossbow', '-c', default='ursacomputing/crossbow',\n              help='Crossbow repository on github to use')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.option('--job-name', required=True)\n@click.option('--pr-title', required=True,\n              help='Track the job submitted on PR with given title')\n@click.pass_obj\ndef report_pr(obj, arrow_remote, crossbow, fetch, job_name, pr_title):\n    arrow = obj['arrow']\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n    job = queue.get(job_name)\n\n    report = CommentReport(job, crossbow_repo=crossbow)\n    target_arrow = Repo(path=arrow.path, remote_url=arrow_remote)\n    pull_request = target_arrow.github_pr(title=pr_title,\n                                          github_token=queue.github_token,\n                                          create=False)\n    # render the response comment's content on the PR\n    pull_request.create_comment(report.show())\n    click.echo(f'Job is tracked on PR {pull_request.html_url}')\n\n\n@crossbow.command()\n@click.argument('prefix', required=True)\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.pass_obj\ndef latest_prefix(obj, prefix, fetch):\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n    latest = queue.latest_for_prefix(prefix)\n    click.echo(latest.branch)\n\n\n@crossbow.command()\n@click.argument('job-name', required=True)\n@click.option('--sender-name', '-n',\n              help='Name to use for report e-mail.')\n@click.option('--sender-email', '-e',\n              help='E-mail to use for report e-mail.')\n@click.option('--recipient-email', '-r',\n              help='Where to send the e-mail report')\n@click.option('--smtp-user', '-u',\n              help='E-mail address to use for SMTP login')\n@click.option('--smtp-password', '-P',\n              help='SMTP password to use for report e-mail.')\n@click.option('--smtp-server', '-s', default='smtp.gmail.com',\n              help='SMTP server to use for report e-mail.')\n@click.option('--smtp-port', '-p', default=465,\n              help='SMTP port to use for report e-mail.')\n@click.option('--poll/--no-poll', default=False,\n              help='Wait for completion if there are tasks pending')\n@click.option('--poll-max-minutes', default=180,\n              help='Maximum amount of time waiting for job completion')\n@click.option('--poll-interval-minutes', default=10,\n              help='Number of minutes to wait to check job status again')\n@click.option('--send/--dry-run', default=False,\n              help='Just display the report, don\\'t send it')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.pass_obj\ndef report(obj, job_name, sender_name, sender_email, recipient_email,\n           smtp_user, smtp_password, smtp_server, smtp_port, poll,\n           poll_max_minutes, poll_interval_minutes, send, fetch):\n    \"\"\"\n    Send an e-mail report showing success/failure of tasks in a Crossbow run\n    \"\"\"\n    output = obj['output']\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n\n    job = queue.get(job_name)\n    email_report = EmailReport(\n        report=Report(job),\n        sender_name=sender_name,\n        sender_email=sender_email,\n        recipient_email=recipient_email\n    )\n\n    if poll:\n        job.wait_until_finished(\n            poll_max_minutes=poll_max_minutes,\n            poll_interval_minutes=poll_interval_minutes\n        )\n\n    if send:\n        ReportUtils.send_email(\n            smtp_user=smtp_user,\n            smtp_password=smtp_password,\n            smtp_server=smtp_server,\n            smtp_port=smtp_port,\n            recipient_email=recipient_email,\n            message=email_report.render(\"nightly_report\")\n        )\n    else:\n        output.write(email_report.render(\"nightly_report\"))\n\n\n@crossbow.command()\n@click.argument('job-name', required=True)\n@click.option('--send/--dry-run', default=False,\n              help='Just display the report, don\\'t send it')\n@click.option('--webhook', '-w',\n              help='Zulip/Slack Webhook address to send the report to')\n@click.option('--extra-message-success', '-s', default=None,\n              help='Extra message, will be appended if no failures.')\n@click.option('--extra-message-failure', '-f', default=None,\n              help='Extra message, will be appended if there are failures.')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.pass_obj\ndef report_chat(obj, job_name, send, webhook, extra_message_success,\n                extra_message_failure, fetch):\n    \"\"\"\n    Send a chat report to a webhook showing success/failure\n    of tasks in a Crossbow run.\n    \"\"\"\n    output = obj['output']\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n\n    job = queue.get(job_name)\n    report_chat = ChatReport(report=Report(job),\n                             extra_message_success=extra_message_success,\n                             extra_message_failure=extra_message_failure)\n    if send:\n        ReportUtils.send_message(webhook, report_chat.render(\"text\"))\n    else:\n        output.write(report_chat.render(\"text\"))\n\n\n@crossbow.command()\n@click.argument('job-name', required=True)\n@click.option('--save/--dry-run', default=False,\n              help='Just display the report, don\\'t save it')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.pass_obj\ndef report_csv(obj, job_name, save, fetch):\n    \"\"\"\n    Generates a CSV report with the different tasks information\n    from a Crossbow run.\n    \"\"\"\n    output = obj['output']\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n\n    job = queue.get(job_name)\n    report = Report(job)\n    if save:\n        ReportUtils.write_csv(report)\n    else:\n        output.write(\"\\n\".join([str(row) for row in report.rows]))\n\n\n@crossbow.command()\n@click.argument('job-name', required=True)\n@click.option('-t', '--target-dir',\n              default=_default_arrow_path / 'packages',\n              type=click.Path(file_okay=False, dir_okay=True),\n              help='Directory to download the build artifacts')\n@click.option('--dry-run/--execute', default=False,\n              help='Just display process, don\\'t download anything')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.option('--task-filter', '-f', 'task_filters', multiple=True,\n              help='Glob pattern for filtering relevant tasks')\n@click.option('--validate-patterns/--skip-pattern-validation', default=True,\n              help='Whether to validate artifact name patterns or not')\n@click.pass_obj\ndef download_artifacts(obj, job_name, target_dir, dry_run, fetch,\n                       validate_patterns, task_filters):\n    \"\"\"Download build artifacts from GitHub releases\"\"\"\n    output = obj['output']\n\n    # fetch the queue repository\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n\n    # query the job's artifacts\n    job = queue.get(job_name)\n\n    # create directory to download the assets to\n    target_dir = Path(target_dir).absolute() / job_name\n    target_dir.mkdir(parents=True, exist_ok=True)\n\n    # download the assets while showing the job status\n    def asset_callback(task_name, task, asset):\n        if asset is not None:\n            path = target_dir / task_name / asset.name\n            path.parent.mkdir(exist_ok=True)\n\n            def need_download():\n                if dry_run:\n                    return False\n                if not path.exists():\n                    return True\n                if path.stat().st_size != asset.size:\n                    return True\n                return False\n\n            if need_download():\n                import github3\n                max_n_retries = 5\n                n_retries = 0\n                while True:\n                    try:\n                        asset.download(path)\n                    except github3.exceptions.GitHubException as error:\n                        n_retries += 1\n                        if n_retries == max_n_retries:\n                            raise\n                        wait_seconds = 60\n                        click.echo(f'Failed to download {path}')\n                        click.echo(f'Retry #{n_retries} after {wait_seconds}s')\n                        click.echo(error)\n                        time.sleep(wait_seconds)\n                    else:\n                        break\n\n    click.echo('Downloading {}\\'s artifacts.'.format(job_name))\n    click.echo('Destination directory is {}'.format(target_dir))\n    click.echo()\n\n    report = ConsoleReport(job, task_filters=task_filters)\n    report.show(\n        output,\n        asset_callback=asset_callback,\n        validate_patterns=validate_patterns\n    )\n\n\n@crossbow.command()\n@click.argument('patterns', nargs=-1, required=True)\n@click.option('--sha', required=True, help='Target committish')\n@click.option('--tag', required=True, help='Target tag')\n@click.option('--method', default='curl', help='Use cURL to upload')\n@click.pass_obj\ndef upload_artifacts(obj, tag, sha, patterns, method):\n    queue = obj['queue']\n    queue.github_overwrite_release_assets(\n        tag_name=tag, target_commitish=sha, method=method, patterns=patterns\n    )\n\n\n@crossbow.command()\n@click.option('--dry-run/--execute', default=False,\n              help='Just display process, don\\'t download anything')\n@click.option('--days', default=90,\n              help='Branches older than this amount of days will be deleted')\n@click.option('--maximum', default=1000,\n              help='Maximum limit of branches to delete for a single run')\n@click.pass_obj\ndef delete_old_branches(obj, dry_run, days, maximum):\n    \"\"\"\n    Deletes branches on queue repository (crossbow) that are older than number\n    of days.\n    With a maximum number of branches to be deleted. This is required to avoid\n    triggering GitHub protection limits.\n    \"\"\"\n    queue = obj['queue']\n    ts = time.time() - days * 24 * 3600\n    refs = []\n    for ref in queue.repo.listall_reference_objects():\n        commit = ref.peel()\n        if commit.commit_time < ts and not ref.name.startswith(\n                \"refs/remotes/origin/pr/\"):\n            # Check if reference is a remote reference to point\n            # to the remote head.\n            ref_name = ref.name\n            if ref_name.startswith(\"refs/remotes/origin\"):\n                ref_name = ref_name.replace(\"remotes/origin\", \"heads\")\n            refs.append(f\":{ref_name}\")\n\n    def batch_gen(iterable, step):\n        total_length = len(iterable)\n        to_delete = min(total_length, maximum)\n        print(f\"Total number of references to be deleted: {to_delete}\")\n        for index in range(0, to_delete, step):\n            yield iterable[index:min(index + step, to_delete)]\n\n    for batch in batch_gen(refs, 50):\n        if not dry_run:\n            queue.push(batch)\n        else:\n            print(batch)\n\n\n@crossbow.command()\n@click.option('--days', default=30,\n              help='Notification will be sent if expiration date is '\n                   'closer than the number of days.')\n@click.option('--sender-name', '-n',\n              help='Name to use for report e-mail.')\n@click.option('--sender-email', '-e',\n              help='E-mail to use for report e-mail.')\n@click.option('--recipient-email', '-r',\n              help='Where to send the e-mail report')\n@click.option('--smtp-user', '-u',\n              help='E-mail address to use for SMTP login')\n@click.option('--smtp-password', '-P',\n              help='SMTP password to use for report e-mail.')\n@click.option('--smtp-server', '-s', default='smtp.gmail.com',\n              help='SMTP server to use for report e-mail.')\n@click.option('--smtp-port', '-p', default=465,\n              help='SMTP port to use for report e-mail.')\n@click.option('--send/--dry-run', default=False,\n              help='Just display the report, don\\'t send it')\n@click.pass_obj\ndef notify_token_expiration(obj, days, sender_name, sender_email,\n                            recipient_email, smtp_user, smtp_password,\n                            smtp_server, smtp_port, send):\n    \"\"\"\n    Check if token is close to expiration and send email notifying.\n    \"\"\"\n    output = obj['output']\n    queue = obj['queue']\n\n    token_expiration_date = queue.token_expiration_date()\n    days_left = 0\n    if token_expiration_date:\n        days_left = (token_expiration_date - date.today()).days\n        if days_left > days:\n            output.write(\"Notification not sent. \" +\n                         f\"Token will expire in {days_left} days.\")\n            return\n\n    class TokenExpirationReport:\n        def __init__(self, token_expiration_date, days_left):\n            self.token_expiration_date = token_expiration_date\n            self.days_left = days_left\n\n    email_report = EmailReport(\n        report=TokenExpirationReport(\n            token_expiration_date or \"ALREADY_EXPIRED\", days_left),\n        sender_name=sender_name,\n        sender_email=sender_email,\n        recipient_email=recipient_email\n    )\n\n    message = email_report.render(\"token_expiration\").strip()\n    if send:\n        ReportUtils.send_email(\n            smtp_user=smtp_user,\n            smtp_password=smtp_password,\n            smtp_server=smtp_server,\n            smtp_port=smtp_port,\n            recipient_email=recipient_email,\n            message=message\n        )\n    else:\n        output.write(message)\n", "dev/archery/archery/crossbow/core.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport re\nimport fnmatch\nimport glob\nimport time\nimport logging\nimport mimetypes\nimport subprocess\nimport textwrap\nimport uuid\nfrom io import StringIO\nfrom pathlib import Path\nfrom datetime import date\nimport warnings\n\nimport jinja2\nfrom ruamel.yaml import YAML\n\ntry:\n    import github3\n    _have_github3 = True\nexcept ImportError:\n    github3 = object\n    _have_github3 = False\n\ntry:\n    import pygit2\nexcept ImportError:\n    PygitRemoteCallbacks = object\n    GitError = Exception\nelse:\n    PygitRemoteCallbacks = pygit2.RemoteCallbacks\n    GitError = pygit2.GitError\n\nfrom ..utils.source import ArrowSources\n\n\nfor pkg in [\"requests\", \"urllib3\", \"github3\"]:\n    logging.getLogger(pkg).setLevel(logging.WARNING)\n\nlogger = logging.getLogger(\"crossbow\")\n\n\nclass CrossbowError(Exception):\n    pass\n\n\ndef _flatten(mapping):\n    \"\"\"Converts a hierarchical mapping to a flat dictionary\"\"\"\n    result = {}\n    for k, v in mapping.items():\n        if isinstance(v, dict):\n            for ik, iv in _flatten(v).items():\n                ik = ik if isinstance(ik, tuple) else (ik,)\n                result[(k,) + ik] = iv\n        elif isinstance(v, list):\n            for ik, iv in enumerate(_flatten(v)):\n                ik = ik if isinstance(ik, tuple) else (ik,)\n                result[(k,) + ik] = iv\n        else:\n            result[(k,)] = v\n    return result\n\n\ndef _unflatten(mapping):\n    \"\"\"Converts a flat tuple => object mapping to hierarchical one\"\"\"\n    result = {}\n    for path, value in mapping.items():\n        parents, leaf = path[:-1], path[-1]\n        # create the hierarchy until we reach the leaf value\n        temp = result\n        for parent in parents:\n            temp.setdefault(parent, {})\n            temp = temp[parent]\n        # set the leaf value\n        temp[leaf] = value\n\n    return result\n\n\ndef _unflatten_tree(files):\n    \"\"\"Converts a flat path => object mapping to a hierarchical directories\n\n    Input:\n        {\n            'path/to/file.a': a_content,\n            'path/to/file.b': b_content,\n            'path/file.c': c_content\n        }\n    Output:\n        {\n            'path': {\n                'to': {\n                    'file.a': a_content,\n                    'file.b': b_content\n                },\n                'file.c': c_content\n            }\n        }\n    \"\"\"\n    files = {tuple(k.split('/')): v for k, v in files.items()}\n    return _unflatten(files)\n\n\ndef _render_jinja_template(searchpath, template, params):\n    def format_all(items, pattern):\n        return [pattern.format(item) for item in items]\n\n    loader = jinja2.FileSystemLoader(searchpath)\n    env = jinja2.Environment(loader=loader, trim_blocks=True,\n                             lstrip_blocks=True,\n                             undefined=jinja2.StrictUndefined)\n    env.filters['format_all'] = format_all\n    template = env.get_template(template)\n    return template.render(**params)\n\n\n# configurations for setting up branch skipping\n# - appveyor has a feature to skip builds without an appveyor.yml\n# - travis reads from the default branch and applies the rules\n# - circle requires the configuration to be present on all branch, even ones\n#   that are configured to be skipped\n# - azure skips branches without azure-pipelines.yml by default\n# - github skips branches without .github/workflows/ by default\n\n_default_travis_yml = \"\"\"\nbranches:\n  only:\n    - master\n    - /.*-travis-.*/\n\nos: linux\ndist: trusty\nlanguage: generic\n\"\"\"\n\n_default_circle_yml = \"\"\"\nversion: 2\n\njobs:\n  build:\n    machine: true\n\nworkflows:\n  version: 2\n  build:\n    jobs:\n      - build:\n          filters:\n            branches:\n              only:\n                - /.*-circle-.*/\n\"\"\"\n\n_default_tree = {\n    '.travis.yml': _default_travis_yml,\n    '.circleci/config.yml': _default_circle_yml\n}\n\n\nclass GitRemoteCallbacks(PygitRemoteCallbacks):\n\n    def __init__(self, token):\n        self.token = token\n        self.attempts = 0\n        super().__init__()\n\n    def push_update_reference(self, refname, message):\n        pass\n\n    def update_tips(self, refname, old, new):\n        pass\n\n    def credentials(self, url, username_from_url, allowed_types):\n        # its a libgit2 bug, that it infinitely retries the authentication\n        self.attempts += 1\n\n        if self.attempts >= 5:\n            # pygit2 doesn't propagate the exception properly\n            msg = 'Wrong oauth personal access token'\n            print(msg)\n            raise CrossbowError(msg)\n\n        if (allowed_types &\n                pygit2.credentials.CredentialType.USERPASS_PLAINTEXT):\n            return pygit2.UserPass('x-oauth-basic', self.token)\n        else:\n            return None\n\n\ndef _git_ssh_to_https(url):\n    return url.replace('git@github.com:', 'https://github.com/')\n\n\ndef _parse_github_user_repo(remote_url):\n    # TODO: use a proper URL parser instead?\n    m = re.match(r'.*\\/([^\\/]+)\\/([^\\/\\.]+)(\\.git|/)?$', remote_url)\n    if m is None:\n        # Perhaps it's simply \"username/reponame\"?\n        m = re.match(r'^(\\w+)/(\\w+)$', remote_url)\n        if m is None:\n            raise CrossbowError(\n                f\"Unable to parse the github owner and repository from the \"\n                f\"repository's remote url {remote_url!r}\"\n            )\n    user, repo = m.group(1), m.group(2)\n    return user, repo\n\n\nclass Repo:\n    \"\"\"\n    Base class for interaction with local git repositories\n\n    A high level wrapper used for both reading revision information from\n    arrow's repository and pushing continuous integration tasks to the queue\n    repository.\n\n    Parameters\n    ----------\n    require_https : boolean, default False\n        Raise exception for SSH origin URLs\n    \"\"\"\n\n    def __init__(self, path, github_token=None, remote_url=None,\n                 require_https=False):\n        self.path = Path(path)\n        self.github_token = github_token\n        self.require_https = require_https\n        self._remote_url = remote_url\n        self._pygit_repo = None\n        self._github_repo = None  # set by as_github_repo()\n        self._updated_refs = []\n\n    def __str__(self):\n        tpl = textwrap.dedent('''\n            Repo: {remote}@{branch}\n            Commit: {head}\n        ''')\n        return tpl.format(\n            remote=self.remote_url,\n            branch=self.branch.branch_name,\n            head=self.head\n        )\n\n    @property\n    def repo(self):\n        if self._pygit_repo is None:\n            self._pygit_repo = pygit2.Repository(str(self.path))\n        return self._pygit_repo\n\n    @property\n    def origin(self):\n        remote = self.repo.remotes['origin']\n        if self.require_https and remote.url.startswith('git@github.com'):\n            raise CrossbowError(\"Change SSH origin URL to HTTPS to use \"\n                                \"Crossbow: {}\".format(remote.url))\n        return remote\n\n    def fetch(self, retry=3):\n        refspec = '+refs/heads/*:refs/remotes/origin/*'\n        attempt = 1\n        while True:\n            try:\n                self.origin.fetch([refspec])\n                break\n            except GitError as e:\n                if retry and attempt < retry:\n                    attempt += 1\n                else:\n                    raise e\n\n    def push(self, refs=None, github_token=None):\n        github_token = github_token or self.github_token\n        if github_token is None:\n            raise RuntimeError(\n                'Could not determine GitHub token. Please set the '\n                'CROSSBOW_GITHUB_TOKEN environment variable to a '\n                'valid GitHub access token or pass one to --github-token.'\n            )\n        callbacks = GitRemoteCallbacks(github_token)\n        refs = refs or []\n        try:\n            self.origin.push(refs + self._updated_refs, callbacks=callbacks)\n        except pygit2.GitError:\n            raise RuntimeError('Failed to push updated references, '\n                               'potentially because of credential issues: {}'\n                               .format(self._updated_refs))\n        else:\n            self.updated_refs = []\n\n    @property\n    def head(self):\n        \"\"\"Currently checked out commit's sha\"\"\"\n        return self.repo.head\n\n    @property\n    def branch(self):\n        \"\"\"Currently checked out branch\"\"\"\n        try:\n            return self.repo.branches[self.repo.head.shorthand]\n        except KeyError:\n            raise CrossbowError(\n                'Cannot determine the current branch of the Arrow repository '\n                'to clone or push to, perhaps it is in detached HEAD state. '\n                'Please checkout a branch.'\n            )\n\n    @property\n    def remote(self):\n        \"\"\"Currently checked out branch's remote counterpart\"\"\"\n        try:\n            return self.repo.remotes[self.branch.upstream.remote_name]\n        except (AttributeError, KeyError):\n            raise CrossbowError(\n                'Cannot determine git remote for the Arrow repository to '\n                'clone or push to, try to push the `{}` branch first to have '\n                'a remote tracking counterpart.'.format(self.branch.name)\n            )\n\n    @property\n    def remote_url(self):\n        \"\"\"Currently checked out branch's remote counterpart URL\n\n        If an SSH github url is set, it will be replaced by the https\n        equivalent usable with GitHub OAuth token.\n        \"\"\"\n        return self._remote_url or _git_ssh_to_https(self.remote.url)\n\n    @property\n    def user_name(self):\n        try:\n            return next(self.repo.config.get_multivar('user.name'))\n        except StopIteration:\n            return os.environ.get('GIT_COMMITTER_NAME', 'unknown')\n\n    @property\n    def user_email(self):\n        try:\n            return next(self.repo.config.get_multivar('user.email'))\n        except StopIteration:\n            return os.environ.get('GIT_COMMITTER_EMAIL', 'unknown')\n\n    @property\n    def signature(self):\n        return pygit2.Signature(self.user_name, self.user_email,\n                                int(time.time()))\n\n    @property\n    def default_branch_name(self):\n        default_branch_name = os.getenv(\"ARCHERY_DEFAULT_BRANCH\")\n\n        if default_branch_name is None:\n            try:\n                ref_obj = self.repo.references[\"refs/remotes/origin/HEAD\"]\n                target_name = ref_obj.target\n                target_name_tokenized = target_name.split(\"/\")\n                default_branch_name = target_name_tokenized[-1]\n            except KeyError:\n                default_branch_name = \"main\"\n                warnings.warn('Unable to determine default branch name: '\n                              'ARCHERY_DEFAULT_BRANCH environment variable is '\n                              'not set. Git repository does not contain a '\n                              '\\'refs/remotes/origin/HEAD\\'reference. Setting '\n                              'the default branch name to ' +\n                              default_branch_name, RuntimeWarning)\n\n        return default_branch_name\n\n    def create_tree(self, files):\n        builder = self.repo.TreeBuilder()\n\n        for filename, content in files.items():\n            if isinstance(content, dict):\n                # create a subtree\n                tree_id = self.create_tree(content)\n                builder.insert(filename, tree_id, pygit2.GIT_FILEMODE_TREE)\n            else:\n                # create a file\n                blob_id = self.repo.create_blob(content)\n                builder.insert(filename, blob_id, pygit2.GIT_FILEMODE_BLOB)\n\n        tree_id = builder.write()\n        return tree_id\n\n    def create_commit(self, files, parents=None, message='',\n                      reference_name=None):\n        if parents is None:\n            # by default use the main branch as the base of the new branch\n            # required to reuse github actions cache across crossbow tasks\n            commit, _ = self.repo.resolve_refish(self.default_branch_name)\n            parents = [commit.id]\n        tree_id = self.create_tree(files)\n\n        author = committer = self.signature\n        commit_id = self.repo.create_commit(reference_name, author, committer,\n                                            message, tree_id, parents)\n        return self.repo[commit_id]\n\n    def create_branch(self, branch_name, files, parents=None, message='',\n                      signature=None):\n        # create commit with the passed tree\n        commit = self.create_commit(files, parents=parents, message=message)\n\n        # create branch pointing to the previously created commit\n        branch = self.repo.create_branch(branch_name, commit)\n\n        # append to the pushable references\n        self._updated_refs.append('refs/heads/{}'.format(branch_name))\n\n        return branch\n\n    def create_tag(self, tag_name, commit_id, message=''):\n        git_object_commit = (\n            pygit2.GIT_OBJECT_COMMIT\n            if getattr(pygit2, 'GIT_OBJECT_COMMIT')\n            else pygit2.GIT_OBJ_COMMIT\n        )\n        tag_id = self.repo.create_tag(tag_name, commit_id,\n                                      git_object_commit,\n                                      self.signature,\n                                      message)\n\n        # append to the pushable references\n        self._updated_refs.append('refs/tags/{}'.format(tag_name))\n\n        return self.repo[tag_id]\n\n    def file_contents(self, commit_id, file):\n        commit = self.repo[commit_id]\n        entry = commit.tree[file]\n        blob = self.repo[entry.id]\n        return blob.data\n\n    def _github_login(self, github_token):\n        \"\"\"Returns a logged in github3.GitHub instance\"\"\"\n        if not _have_github3:\n            raise ImportError('Must install github3.py')\n        github_token = github_token or self.github_token\n        session = github3.session.GitHubSession(\n            default_connect_timeout=10,\n            default_read_timeout=30\n        )\n        github = github3.GitHub(session=session)\n        github.login(token=github_token)\n        return github\n\n    def as_github_repo(self, github_token=None):\n        \"\"\"Converts it to a repository object which wraps the GitHub API\"\"\"\n        if self._github_repo is None:\n            github = self._github_login(github_token)\n            username, reponame = _parse_github_user_repo(self.remote_url)\n            self._github_repo = github.repository(username, reponame)\n        return self._github_repo\n\n    def token_expiration_date(self, github_token=None):\n        \"\"\"Returns the expiration date for the github_token provided\"\"\"\n        github = self._github_login(github_token)\n        # github3 hides the headers from us. Use the _get method\n        # to access the response headers.\n        resp = github._get(github.session.base_url)\n        # Response in the form '2023-01-23 10:40:28 UTC'\n        date_string = resp.headers.get(\n            'github-authentication-token-expiration')\n        if date_string:\n            return date.fromisoformat(date_string.split()[0])\n\n    def github_commit(self, sha):\n        repo = self.as_github_repo()\n        return repo.commit(sha)\n\n    def github_release(self, tag):\n        repo = self.as_github_repo()\n        try:\n            return repo.release_from_tag(tag)\n        except github3.exceptions.NotFoundError:\n            return None\n\n    def github_upload_asset_requests(self, release, path, name, mime,\n                                     max_retries=None, retry_backoff=None):\n        if max_retries is None:\n            max_retries = int(os.environ.get('CROSSBOW_MAX_RETRIES', 8))\n        if retry_backoff is None:\n            retry_backoff = int(os.environ.get('CROSSBOW_RETRY_BACKOFF', 5))\n\n        for i in range(max_retries):\n            try:\n                with open(path, 'rb') as fp:\n                    result = release.upload_asset(name=name, asset=fp,\n                                                  content_type=mime)\n            except github3.exceptions.ResponseError as e:\n                logger.error('Attempt {} has failed with message: {}.'\n                             .format(i + 1, str(e)))\n                logger.error('Error message {}'.format(e.msg))\n                logger.error('List of errors provided by GitHub:')\n                for err in e.errors:\n                    logger.error(' - {}'.format(err))\n\n                if e.code == 422:\n                    # 422 Validation Failed, probably raised because\n                    # ReleaseAsset already exists, so try to remove it before\n                    # reattempting the asset upload\n                    for asset in release.assets():\n                        if asset.name == name:\n                            logger.info('Release asset {} already exists, '\n                                        'removing it...'.format(name))\n                            asset.delete()\n                            logger.info('Asset {} removed.'.format(name))\n                            break\n            except github3.exceptions.ConnectionError as e:\n                logger.error('Attempt {} has failed with message: {}.'\n                             .format(i + 1, str(e)))\n            else:\n                logger.info('Attempt {} has finished.'.format(i + 1))\n                return result\n\n            time.sleep(retry_backoff)\n\n        raise RuntimeError('GitHub asset uploading has failed!')\n\n    def github_upload_asset_curl(self, release, path, name, mime):\n        upload_url, _ = release.upload_url.split('{?')\n        upload_url += '?name={}'.format(name)\n\n        command = [\n            'curl',\n            '--fail',\n            '-H', \"Authorization: token {}\".format(self.github_token),\n            '-H', \"Content-Type: {}\".format(mime),\n            '--data-binary', '@{}'.format(path),\n            upload_url\n        ]\n        return subprocess.run(command, shell=False, check=True)\n\n    def github_overwrite_release_assets(self, tag_name, target_commitish,\n                                        patterns, method='requests'):\n        # Since github has changed something the asset uploading via requests\n        # got instable, so prefer the cURL alternative.\n        # Potential cause:\n        #    sigmavirus24/github3.py/issues/779#issuecomment-379470626\n        repo = self.as_github_repo()\n        if not tag_name:\n            raise CrossbowError('Empty tag name')\n        if not target_commitish:\n            raise CrossbowError('Empty target commit for the release tag')\n\n        # remove the whole release if it already exists\n        try:\n            release = repo.release_from_tag(tag_name)\n        except github3.exceptions.NotFoundError:\n            pass\n        else:\n            release.delete()\n\n        release = repo.create_release(tag_name, target_commitish)\n        for pattern in patterns:\n            for path in glob.glob(pattern, recursive=True):\n                name = os.path.basename(path)\n                size = os.path.getsize(path)\n                mime = mimetypes.guess_type(name)[0] or 'application/zip'\n\n                logger.info(\n                    'Uploading asset `{}` with mimetype {} and size {}...'\n                    .format(name, mime, size)\n                )\n\n                if method == 'requests':\n                    self.github_upload_asset_requests(release, path, name=name,\n                                                      mime=mime)\n                elif method == 'curl':\n                    self.github_upload_asset_curl(release, path, name=name,\n                                                  mime=mime)\n                else:\n                    raise CrossbowError(\n                        'Unsupported upload method {}'.format(method)\n                    )\n\n    def github_pr(self, title, head=None, base=None, body=None,\n                  github_token=None, create=False):\n        if create:\n            # Default value for base is the default_branch_name\n            base = self.default_branch_name if base is None else base\n        github_token = github_token or self.github_token\n        repo = self.as_github_repo(github_token=github_token)\n        if create:\n            return repo.create_pull(title=title, base=base, head=head,\n                                    body=body)\n        else:\n            # Retrieve open PR for base and head.\n            # There should be a single open one with that title.\n            for pull in repo.pull_requests(state=\"open\", head=head,\n                                           base=base):\n                if title in pull.title:\n                    return pull\n            raise CrossbowError(\n                f\"Pull request with Title: {title!r} not found \"\n                f\"in repository {repo.full_name!r}\"\n            )\n\n\nclass Queue(Repo):\n\n    def _latest_prefix_id(self, prefix):\n        pattern = re.compile(r'[\\w\\/-]*{}-(\\d+)'.format(prefix))\n        matches = list(filter(None, map(pattern.match, self.repo.branches)))\n        if matches:\n            latest = max(int(m.group(1)) for m in matches)\n        else:\n            latest = -1\n        return latest\n\n    def _prefix_contains_date(self, prefix):\n        prefix_date_pattern = re.compile(r'[\\w\\/-]*-(\\d+)-(\\d+)-(\\d+)')\n        match_prefix = prefix_date_pattern.match(prefix)\n        if match_prefix:\n            return match_prefix.group(0)[-10:]\n\n    def _latest_prefix_date(self, prefix):\n        pattern = re.compile(r'[\\w\\/-]*{}-(\\d+)-(\\d+)-(\\d+)'.format(prefix))\n        matches = list(filter(None, map(pattern.match, self.repo.branches)))\n        if matches:\n            latest = sorted([m.group(0) for m in matches])[-1]\n            # slice the trailing date part (YYYY-MM-DD)\n            latest = latest[-10:]\n        else:\n            latest = -1\n        return latest\n\n    def _next_job_id(self, prefix):\n        \"\"\"Auto increments the branch's identifier based on the prefix\"\"\"\n        latest_id = self._latest_prefix_id(prefix)\n        return '{}-{}'.format(prefix, latest_id + 1)\n\n    def _new_hex_id(self, prefix):\n        \"\"\"Append a new id to branch's identifier based on the prefix\"\"\"\n        hex_id = uuid.uuid4().hex[:10]\n        return '{}-{}'.format(prefix, hex_id)\n\n    def latest_for_prefix(self, prefix):\n        prefix_date = self._prefix_contains_date(prefix)\n        if prefix.startswith(\"nightly\") and not prefix_date:\n            latest_id = self._latest_prefix_date(prefix)\n            if not latest_id:\n                raise RuntimeError(\n                    f\"No job has been submitted with prefix '{prefix}'' yet\"\n                )\n            latest_id += \"-0\"\n        else:\n            latest_id = self._latest_prefix_id(prefix)\n            if latest_id < 0:\n                raise RuntimeError(\n                    f\"No job has been submitted with prefix '{prefix}' yet\"\n                )\n        job_name = '{}-{}'.format(prefix, latest_id)\n        return self.get(job_name)\n\n    def date_of(self, job):\n        # it'd be better to bound to the queue repository on deserialization\n        # and reorganize these methods to Job\n        branch_name = 'origin/{}'.format(job.branch)\n        branch = self.repo.branches[branch_name]\n        commit = self.repo[branch.target]\n        return date.fromtimestamp(commit.commit_time)\n\n    def jobs(self, pattern):\n        \"\"\"Return jobs sorted by its identifier in reverse order\"\"\"\n        job_names = []\n        for name in self.repo.branches.remote:\n            origin, name = name.split('/', 1)\n            result = re.match(pattern, name)\n            if result:\n                job_names.append(name)\n\n        for name in sorted(job_names, reverse=True):\n            yield self.get(name)\n\n    def get(self, job_name):\n        branch_name = 'origin/{}'.format(job_name)\n        branch = self.repo.branches[branch_name]\n        try:\n            content = self.file_contents(branch.target, 'job.yml')\n        except KeyError:\n            raise CrossbowError(\n                'No job is found with name: {}'.format(job_name)\n            )\n\n        buffer = StringIO(content.decode('utf-8'))\n        job = yaml.load(buffer)\n        job.queue = self\n        return job\n\n    def put(self, job, prefix='build', increment_job_id=True):\n        if not isinstance(job, Job):\n            raise CrossbowError('`job` must be an instance of Job')\n        if job.branch is not None:\n            raise CrossbowError('`job.branch` is automatically generated, '\n                                'thus it must be blank')\n\n        job.queue = self\n        if increment_job_id:\n            # auto increment and set next job id, e.g. build-85\n            job.branch = self._next_job_id(prefix)\n        else:\n            # set new branch to something unique, e.g. build-41d017af40\n            job.branch = self._new_hex_id(prefix)\n\n        # create tasks' branches\n        for task_name, task in job.tasks.items():\n            # adding CI's name to the end of the branch in order to use skip\n            # patterns on travis and circleci\n            task.branch = '{}-{}-{}'.format(job.branch, task.ci, task_name)\n            params = {\n                **job.params,\n                \"arrow\": job.target,\n                \"job\": job,\n                \"queue_remote_url\": self.remote_url\n            }\n            files = task.render_files(job.template_searchpath, params=params)\n            branch = self.create_branch(task.branch, files=files)\n            self.create_tag(task.tag, branch.target)\n            task.commit = str(branch.target)\n\n        # create job's branch with its description\n        return self.create_branch(job.branch, files=job.render_files())\n\n\ndef get_version(root, **kwargs):\n    \"\"\"\n    Parse function for setuptools_scm that ignores tags for non-C++\n    subprojects, e.g. apache-arrow-js-XXX tags.\n    \"\"\"\n    from setuptools_scm.git import parse as parse_git_version\n\n    # query the calculated version based on the git tags\n    kwargs['describe_command'] = (\n        'git describe --dirty --tags --long --match \"apache-arrow-[0-9]*.*\"'\n    )\n    version = parse_git_version(root, **kwargs)\n    tag = str(version.tag)\n\n    # We may get a development tag for the next version, such as \"5.0.0.dev0\",\n    # or the tag of an already released version, such as \"4.0.0\".\n    # In the latter case, we need to increment the version so that the computed\n    # version comes after any patch release (the next feature version after\n    # 4.0.0 is 5.0.0).\n    pattern = r\"^(\\d+)\\.(\\d+)\\.(\\d+)\"\n    match = re.match(pattern, tag)\n    major, minor, patch = map(int, match.groups())\n    if 'dev' not in tag:\n        major += 1\n\n    return \"{}.{}.{}.dev{}\".format(major, minor, patch, version.distance or 0)\n\n\nclass Serializable:\n\n    @classmethod\n    def to_yaml(cls, representer, data):\n        tag = '!{}'.format(cls.__name__)\n        dct = {k: v for k, v in data.__dict__.items() if not k.startswith('_')}\n        return representer.represent_mapping(tag, dct)\n\n\nclass Target(Serializable):\n    \"\"\"\n    Describes target repository and revision the builds run against\n\n    This serializable data container holding information about arrow's\n    git remote, branch, sha and version number as well as some metadata\n    (currently only an email address where the notification should be sent).\n    \"\"\"\n\n    def __init__(self, head, branch, remote, version, r_version, email=None):\n        self.head = head\n        self.email = email\n        self.branch = branch\n        self.remote = remote\n        self.github_repo = \"/\".join(_parse_github_user_repo(remote))\n        self.version = version\n        self.r_version = r_version\n        self.no_rc_version = re.sub(r'-rc\\d+\\Z', '', version)\n        self.no_rc_r_version = re.sub(r'-rc\\d+\\Z', '', r_version)\n        # Semantic Versioning 1.0.0: https://semver.org/spec/v1.0.0.html\n        #\n        # > A pre-release version number MAY be denoted by appending an\n        # > arbitrary string immediately following the patch version and a\n        # > dash. The string MUST be comprised of only alphanumerics plus\n        # > dash [0-9A-Za-z-].\n        #\n        # Example:\n        #\n        #   '0.16.1.dev10' ->\n        #   '0.16.1-dev10'\n        self.no_rc_semver_version = \\\n            re.sub(r'\\.(dev\\d+)\\Z', r'-\\1', self.no_rc_version)\n        # Substitute dev version for SNAPSHOT\n        #\n        # Example:\n        #\n        # '10.0.0.dev235' ->\n        # '10.0.0-SNAPSHOT'\n        self.no_rc_snapshot_version = re.sub(\n            r'\\.(dev\\d+)$', '-SNAPSHOT', self.no_rc_version)\n\n    @classmethod\n    def from_repo(cls, repo, head=None, branch=None, remote=None, version=None,\n                  email=None):\n        \"\"\"Initialize from a repository\n\n        Optionally override detected remote, branch, head, and/or version.\n        \"\"\"\n        assert isinstance(repo, Repo)\n\n        if head is None:\n            head = str(repo.head.target)\n        if branch is None:\n            branch = repo.branch.branch_name\n        if remote is None:\n            remote = repo.remote_url\n        if version is None:\n            version = get_version(repo.path)\n        if email is None:\n            email = repo.user_email\n\n        version_dev_match = re.match(r\".*\\.dev(\\d+)$\", version)\n        if version_dev_match:\n            with open(f\"{repo.path}/r/DESCRIPTION\") as description_file:\n                description = description_file.read()\n                r_version_pattern = re.compile(r\"^Version:\\s*(.*)$\",\n                                               re.MULTILINE)\n                r_version = re.findall(r_version_pattern, description)[0]\n            if r_version:\n                version_dev = int(version_dev_match[1])\n                # \"1_0000_00_00 +\" is for generating a greater version\n                # than YYYYMMDD. For example, 1_0000_00_01\n                # (version_dev == 1 case) is greater than 2022_10_16.\n                #\n                # Why do we need a greater version than YYYYMMDD? It's\n                # for keeping backward compatibility. We used\n                # MAJOR.MINOR.PATCH.YYYYMMDD as our nightly package\n                # version. (See also ARROW-16403). If we use \"9000 +\n                # version_dev\" here, a developer that used\n                # 9.0.0.20221016 can't upgrade to the later nightly\n                # package unless we release 10.0.0. Because 9.0.0.9234\n                # or something is less than 9.0.0.20221016.\n                r_version_dev = 1_0000_00_00 + version_dev\n                # version: 10.0.0.dev234\n                # r_version: 9.0.0.9000\n                # -> 9.0.0.100000234\n                r_version = re.sub(r\"\\.9000\\Z\", f\".{r_version_dev}\", r_version)\n            else:\n                r_version = version\n        else:\n            r_version = version\n\n        return cls(head=head, email=email, branch=branch, remote=remote,\n                   version=version, r_version=r_version)\n\n    def is_default_branch(self):\n        return self.branch == 'main'\n\n\nclass Task(Serializable):\n    \"\"\"\n    Describes a build task and metadata required to render CI templates\n\n    A task is represented as a single git commit and branch containing jinja2\n    rendered files (currently appveyor.yml or .travis.yml configurations).\n\n    A task can't be directly submitted to a queue, must belong to a job.\n    Each task's unique identifier is its branch name, which is generated after\n    submitting the job to a queue.\n    \"\"\"\n\n    def __init__(self, name, ci, template, artifacts=None, params=None):\n        assert ci in {\n            'circle',\n            'travis',\n            'appveyor',\n            'azure',\n            'github',\n            'drone',\n        }\n        self.name = name\n        self.ci = ci\n        self.template = template\n        self.artifacts = artifacts or []\n        self.params = params or {}\n        self.branch = None  # filled after adding to a queue\n        self.commit = None  # filled after adding to a queue\n        self._queue = None  # set by the queue object after put or get\n        self._status = None  # status cache\n        self._assets = None  # assets cache\n\n    def render_files(self, searchpath, params=None):\n        params = {**self.params, **(params or {}), \"task\": self}\n        try:\n            rendered = _render_jinja_template(searchpath, self.template,\n                                              params=params)\n        except jinja2.TemplateError as e:\n            raise RuntimeError(\n                'Failed to render template `{}` with {}: {}'.format(\n                    self.template, e.__class__.__name__, str(e)\n                )\n            )\n\n        tree = {**_default_tree, self.filename: rendered}\n        return _unflatten_tree(tree)\n\n    @property\n    def tag(self):\n        return self.branch\n\n    @property\n    def filename(self):\n        config_files = {\n            'circle': '.circleci/config.yml',\n            'travis': '.travis.yml',\n            'appveyor': 'appveyor.yml',\n            'azure': 'azure-pipelines.yml',\n            'github': '.github/workflows/crossbow.yml',\n            'drone': '.drone.yml',\n        }\n        return config_files[self.ci]\n\n    def status(self, force_query=False):\n        _status = getattr(self, '_status', None)\n        if force_query or _status is None:\n            github_commit = self._queue.github_commit(self.commit)\n            self._status = TaskStatus(github_commit)\n        return self._status\n\n    def assets(self, force_query=False, validate_patterns=True):\n        _assets = getattr(self, '_assets', None)\n        if force_query or _assets is None:\n            github_release = self._queue.github_release(self.tag)\n            self._assets = TaskAssets(github_release,\n                                      artifact_patterns=self.artifacts,\n                                      validate_patterns=validate_patterns)\n        return self._assets\n\n\nclass TaskStatus:\n    \"\"\"\n    Combine the results from status and checks API to a single state.\n\n    Azure pipelines uses checks API which doesn't provide a combined\n    interface like status API does, so we need to manually combine\n    both the commit statuses and the commit checks coming from\n    different API endpoint\n\n    Status.state: error, failure, pending or success, default pending\n    CheckRun.status: queued, in_progress or completed, default: queued\n    CheckRun.conclusion: success, failure, neutral, cancelled, timed_out\n                            or action_required, only set if\n                            CheckRun.status == 'completed'\n\n    1. Convert CheckRun's status and conclusion to one of Status.state\n    2. Merge the states based on the following rules:\n        - failure if any of the contexts report as error or failure\n        - pending if there are no statuses or a context is pending\n        - success if the latest status for all contexts is success\n        error otherwise.\n\n    Parameters\n    ----------\n    commit : github3.Commit\n        Commit to query the combined status for.\n\n    Returns\n    -------\n    TaskStatus(\n        combined_state='error|failure|pending|success',\n        github_status='original github status object',\n        github_check_runs='github checks associated with the commit',\n        total_count='number of statuses and checks'\n    )\n    \"\"\"\n\n    def __init__(self, commit):\n        status = commit.status()\n        check_runs = list(commit.check_runs())\n        states = [s.state for s in status.statuses]\n\n        for check in check_runs:\n            if check.status == 'completed':\n                if check.conclusion in {'success', 'failure'}:\n                    states.append(check.conclusion)\n                elif check.conclusion in {'cancelled', 'timed_out',\n                                          'action_required'}:\n                    states.append('error')\n                # omit `neutral` conclusion\n            else:\n                states.append('pending')\n\n        # it could be more effective, but the following is more descriptive\n        combined_state = 'error'\n        if len(states):\n            if any(state in {'error', 'failure'} for state in states):\n                combined_state = 'failure'\n            elif any(state == 'pending' for state in states):\n                combined_state = 'pending'\n            elif all(state == 'success' for state in states):\n                combined_state = 'success'\n\n        # show link to the actual build, some of the CI providers implement\n        # the statuses API others implement the checks API, so display both\n        build_links = [s.target_url for s in status.statuses]\n        build_links += [c.html_url for c in check_runs]\n\n        self.combined_state = combined_state\n        self.github_status = status\n        self.github_check_runs = check_runs\n        self.total_count = len(states)\n        self.build_links = build_links\n\n\nclass TaskAssets(dict):\n\n    def __init__(self, github_release, artifact_patterns,\n                 validate_patterns=True):\n        # HACK(kszucs): don't expect uploaded assets of no artifacts were\n        # defined for the tasks in order to spare a bit of github rate limit\n        if not artifact_patterns:\n            return\n\n        if github_release is None:\n            github_assets = {}  # no assets have been uploaded for the task\n        else:\n            github_assets = {a.name: a for a in github_release.assets()}\n\n        if not validate_patterns:\n            # shortcut to avoid pattern validation and just set all artifacts\n            return self.update(github_assets)\n\n        for pattern in artifact_patterns:\n            # artifact can be a regex pattern\n            compiled = re.compile(f\"^{pattern}$\")\n            matches = list(\n                filter(None, map(compiled.match, github_assets.keys()))\n            )\n            num_matches = len(matches)\n\n            # validate artifact pattern matches single asset\n            if num_matches == 0:\n                self[pattern] = None\n            elif num_matches == 1:\n                self[pattern] = github_assets[matches[0].group(0)]\n            else:\n                raise CrossbowError(\n                    'Only a single asset should match pattern `{}`, there are '\n                    'multiple ones: {}'.format(pattern, ', '.join(matches))\n                )\n\n    def missing_patterns(self):\n        return [pattern for pattern, asset in self.items() if asset is None]\n\n    def uploaded_assets(self):\n        return [asset for asset in self.values() if asset is not None]\n\n\nclass Job(Serializable):\n    \"\"\"Describes multiple tasks against a single target repository\"\"\"\n\n    def __init__(self, target, tasks, params=None, template_searchpath=None):\n        if not tasks:\n            raise ValueError('no tasks were provided for the job')\n        if not all(isinstance(task, Task) for task in tasks.values()):\n            raise ValueError('each `tasks` mus be an instance of Task')\n        if not isinstance(target, Target):\n            raise ValueError('`target` must be an instance of Target')\n        if not isinstance(params, dict):\n            raise ValueError('`params` must be an instance of dict')\n\n        self.target = target\n        self.tasks = tasks\n        self.params = params or {}  # additional parameters for the tasks\n        self.branch = None  # filled after adding to a queue\n        self._queue = None  # set by the queue object after put or get\n        if template_searchpath is None:\n            self._template_searchpath = ArrowSources.find().path\n        else:\n            self._template_searchpath = template_searchpath\n\n    def render_files(self):\n        with StringIO() as buf:\n            yaml.dump(self, buf)\n            content = buf.getvalue()\n        tree = {**_default_tree, \"job.yml\": content}\n        return _unflatten_tree(tree)\n\n    def render_tasks(self, params=None):\n        result = {}\n        params = {\n            **self.params,\n            \"arrow\": self.target,\n            \"job\": self,\n            **(params or {})\n        }\n        for task_name, task in self.tasks.items():\n            files = task.render_files(self._template_searchpath, params)\n            result[task_name] = files\n        return result\n\n    @property\n    def template_searchpath(self):\n        return self._template_searchpath\n\n    @property\n    def queue(self):\n        assert isinstance(self._queue, Queue)\n        return self._queue\n\n    @queue.setter\n    def queue(self, queue):\n        assert isinstance(queue, Queue)\n        self._queue = queue\n        for task in self.tasks.values():\n            task._queue = queue\n\n    @property\n    def email(self):\n        return os.environ.get('CROSSBOW_EMAIL', self.target.email)\n\n    @property\n    def date(self):\n        return self.queue.date_of(self)\n\n    def show(self, stream=None):\n        return yaml.dump(self, stream=stream)\n\n    @classmethod\n    def from_config(cls, config, target, tasks=None, groups=None, params=None):\n        \"\"\"\n        Instantiate a job from based on a config.\n\n        Parameters\n        ----------\n        config : dict\n            Deserialized content of tasks.yml\n        target : Target\n            Describes target repository and revision the builds run against.\n        tasks : Optional[List[str]], default None\n            List of glob patterns for matching task names.\n        groups : Optional[List[str]], default None\n            List of exact group names matching predefined task sets in the\n            config.\n        params : Optional[Dict[str, str]], default None\n            Additional rendering parameters for the task templates.\n\n        Returns\n        -------\n        Job\n\n        Raises\n        ------\n        Exception:\n            If invalid groups or tasks has been passed.\n        \"\"\"\n        task_definitions = config.select(tasks, groups=groups)\n\n        # instantiate the tasks\n        tasks = {}\n        versions = {\n            'version': target.version,\n            'no_rc_version': target.no_rc_version,\n            'no_rc_semver_version': target.no_rc_semver_version,\n            'no_rc_snapshot_version': target.no_rc_snapshot_version,\n            'r_version': target.r_version,\n            'no_rc_r_version': target.no_rc_r_version,\n        }\n        for task_name, task in task_definitions.items():\n            task = task.copy()\n            artifacts = task.pop('artifacts', None) or []  # because of yaml\n            artifacts = [fn.format(**versions) for fn in artifacts]\n            tasks[task_name] = Task(task_name, artifacts=artifacts, **task)\n        return cls(target=target, tasks=tasks, params=params,\n                   template_searchpath=config.template_searchpath)\n\n    def is_finished(self):\n        for task in self.tasks.values():\n            status = task.status(force_query=True)\n            if status.combined_state == 'pending':\n                return False\n        return True\n\n    def wait_until_finished(self, poll_max_minutes=120,\n                            poll_interval_minutes=10):\n        started_at = time.time()\n        while True:\n            if self.is_finished():\n                break\n\n            waited_for_minutes = (time.time() - started_at) / 60\n            if waited_for_minutes > poll_max_minutes:\n                msg = ('Exceeded the maximum amount of time waiting for job '\n                       'to finish, waited for {} minutes.')\n                raise RuntimeError(msg.format(waited_for_minutes))\n\n            logger.info('Waiting {} minutes and then checking again'\n                        .format(poll_interval_minutes))\n            time.sleep(poll_interval_minutes * 60)\n\n\nclass Config(dict):\n\n    def __init__(self, tasks, template_searchpath):\n        super().__init__(tasks)\n        self.template_searchpath = template_searchpath\n\n    @classmethod\n    def load_yaml(cls, path):\n        path = Path(path)\n        searchpath = path.parent\n        rendered = _render_jinja_template(searchpath, template=path.name,\n                                          params={})\n        config = yaml.load(rendered)\n        return cls(config, template_searchpath=searchpath)\n\n    def show(self, stream=None):\n        return yaml.dump(dict(self), stream=stream)\n\n    def select(self, tasks=None, groups=None):\n        config_groups = dict(self['groups'])\n        config_tasks = dict(self['tasks'])\n        valid_groups = set(config_groups.keys())\n        valid_tasks = set(config_tasks.keys())\n        group_allowlist = list(groups or [])\n        task_allowlist = list(tasks or [])\n\n        # validate that the passed groups are defined in the config\n        requested_groups = set(group_allowlist)\n        invalid_groups = requested_groups - valid_groups\n        if invalid_groups:\n            msg = 'Invalid group(s) {!r}. Must be one of {!r}'.format(\n                invalid_groups, valid_groups\n            )\n            raise CrossbowError(msg)\n\n        # treat the task names as glob patterns to select tasks more easily\n        requested_tasks = set()\n        for pattern in task_allowlist:\n            matches = fnmatch.filter(valid_tasks, pattern)\n            if len(matches):\n                requested_tasks.update(matches)\n            else:\n                raise CrossbowError(\n                    \"Unable to match any tasks for `{}`\".format(pattern)\n                )\n\n        requested_group_tasks = set()\n        for group in group_allowlist:\n            # separate the patterns from the blocklist patterns\n            task_patterns = list(config_groups[group])\n            task_blocklist_patterns = [\n                x.strip(\"~\") for x in task_patterns if x.startswith(\"~\")]\n            task_patterns = [x for x in task_patterns if not x.startswith(\"~\")]\n\n            # treat the task names as glob patterns to select tasks more easily\n            for pattern in task_patterns:\n                matches = fnmatch.filter(valid_tasks, pattern)\n                if len(matches):\n                    requested_group_tasks.update(matches)\n                else:\n                    raise CrossbowError(\n                        \"Unable to match any tasks for `{}`\".format(pattern)\n                    )\n\n            # remove any tasks that are negated with ~task-name\n            for block_pattern in task_blocklist_patterns:\n                matches = fnmatch.filter(valid_tasks, block_pattern)\n                if len(matches):\n                    requested_group_tasks = requested_group_tasks.difference(\n                        matches)\n                else:\n                    raise CrossbowError(\n                        \"Unable to match any tasks for `{}`\".format(pattern)\n                    )\n\n        requested_tasks = requested_tasks.union(requested_group_tasks)\n\n        # validate that the passed and matched tasks are defined in the config\n        invalid_tasks = requested_tasks - valid_tasks\n        if invalid_tasks:\n            msg = 'Invalid task(s) {!r}. Must be one of {!r}'.format(\n                invalid_tasks, valid_tasks\n            )\n            raise CrossbowError(msg)\n\n        return {\n            task_name: config_tasks[task_name] for task_name in requested_tasks\n        }\n\n    def validate(self):\n        # validate that the task groups are properly referring to the tasks\n        for group_name, group in self['groups'].items():\n            for pattern in group:\n                # remove the negation character for blocklisted tasks\n                pattern = pattern.strip(\"~\")\n                tasks = self.select(tasks=[pattern])\n                if not tasks:\n                    raise CrossbowError(\n                        \"The pattern `{}` defined for task group `{}` is not \"\n                        \"matching any of the tasks defined in the \"\n                        \"configuration file.\".format(pattern, group_name)\n                    )\n\n        # validate that the tasks are constructible\n        for task_name, task in self['tasks'].items():\n            try:\n                Task(task_name, **task)\n            except Exception as e:\n                raise CrossbowError(\n                    'Unable to construct a task object from the '\n                    'definition  of task `{}`. The original error message '\n                    'is: `{}`'.format(task_name, str(e))\n                )\n\n        # Get the default branch name from the repository\n        arrow_source_dir = ArrowSources.find()\n        repo = Repo(arrow_source_dir.path)\n\n        # validate that the defined tasks are renderable, in order to to that\n        # define the required object with dummy data\n        target = Target(\n            head='e279a7e06e61c14868ca7d71dea795420aea6539',\n            branch=repo.default_branch_name,\n            remote='https://github.com/apache/arrow',\n            version='1.0.0dev123',\n            r_version='0.13.0.100000123',\n            email='dummy@example.ltd'\n        )\n        job = Job.from_config(config=self,\n                              target=target,\n                              tasks=self['tasks'],\n                              groups=self['groups'],\n                              params={})\n\n        for task_name, task in self['tasks'].items():\n            task = Task(task_name, **task)\n            files = task.render_files(\n                self.template_searchpath,\n                params=dict(\n                    arrow=target,\n                    job=job,\n                    queue_remote_url='https://github.com/org/crossbow'\n                )\n            )\n            if not files:\n                raise CrossbowError('No files have been rendered for task `{}`'\n                                    .format(task_name))\n\n\n# configure yaml serializer\nyaml = YAML()\nyaml.register_class(Job)\nyaml.register_class(Task)\nyaml.register_class(Target)\nyaml.register_class(Queue)\nyaml.register_class(TaskStatus)\n", "dev/archery/archery/crossbow/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom .core import Config, Repo, Queue, Target, Job  # noqa\nfrom .reports import CommentReport, ConsoleReport, EmailReport  # noqa\n", "dev/archery/archery/crossbow/tests/test_crossbow_cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom click.testing import CliRunner\nimport pytest\n\nfrom archery.crossbow.cli import crossbow\nfrom archery.utils.git import git\n\n\n@pytest.mark.integration\ndef test_crossbow_submit(tmp_path):\n    runner = CliRunner()\n\n    def invoke(*args):\n        return runner.invoke(crossbow, ['--queue-path', str(tmp_path), *args])\n\n    # initialize an empty crossbow repository\n    git.run_cmd(\"init\", str(tmp_path))\n    git.run_cmd(\"-C\", str(tmp_path), \"remote\", \"add\", \"origin\",\n                \"https://github.com/dummy/repo\")\n    git.run_cmd(\"-C\", str(tmp_path), \"commit\", \"-m\", \"initial\",\n                \"--allow-empty\")\n\n    result = invoke('check-config')\n    assert result.exit_code == 0\n\n    result = invoke('submit', '--no-fetch', '--no-push', '-g', 'wheel')\n    assert result.exit_code == 0\n", "dev/archery/archery/crossbow/tests/test_reports.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport textwrap\n\nfrom archery.crossbow.core import yaml\nfrom archery.crossbow.reports import (ChatReport, CommentReport, EmailReport,\n                                      Report)\n\n\ndef test_crossbow_comment_formatter(load_fixture):\n    msg = load_fixture('crossbow-success-message.md')\n    job = load_fixture('crossbow-job.yaml', decoder=yaml.load)\n\n    report = CommentReport(job, crossbow_repo='ursa-labs/crossbow')\n    expected = msg.format(\n        repo='ursa-labs/crossbow',\n        branch='ursabot-1',\n        revision='f766a1d615dd1b7ee706d05102e579195951a61c',\n        status='has been succeeded.'\n    )\n    assert report.show() == textwrap.dedent(expected).strip()\n\n\ndef test_crossbow_chat_report(load_fixture):\n    expected_msg = load_fixture('chat-report.txt')\n    job = load_fixture('crossbow-job.yaml', decoder=yaml.load)\n    report = Report(job)\n    assert report.tasks_by_state is not None\n    report_chat = ChatReport(report=report, extra_message_success=None,\n                             extra_message_failure=None)\n\n    assert report_chat.render(\"text\") == textwrap.dedent(expected_msg)\n\n\ndef test_crossbow_chat_report_extra_message_failure(load_fixture):\n    expected_msg = load_fixture('chat-report-extra-message-failure.txt')\n    job = load_fixture('crossbow-job.yaml', decoder=yaml.load)\n    report = Report(job)\n    assert report.tasks_by_state is not None\n    report_chat = ChatReport(report=report,\n                             extra_message_success=\"Should not be present\",\n                             extra_message_failure=\"Failure present\")\n\n    assert report_chat.render(\"text\") == textwrap.dedent(expected_msg)\n\n\ndef test_crossbow_chat_report_extra_message_success(load_fixture):\n    expected_msg = load_fixture('chat-report-extra-message-success.txt')\n    job = load_fixture('crossbow-job-no-failure.yaml', decoder=yaml.load)\n    report = Report(job)\n    assert report.tasks_by_state is not None\n    report_chat = ChatReport(report=report,\n                             extra_message_success=\"Success present\",\n                             extra_message_failure=\"Should not be present\")\n\n    assert report_chat.render(\"text\") == textwrap.dedent(expected_msg)\n\n\ndef test_crossbow_email_report(load_fixture):\n    expected_msg = load_fixture('email-report.txt')\n    job = load_fixture('crossbow-job.yaml', decoder=yaml.load)\n    report = Report(job)\n    assert report.tasks_by_state is not None\n    email_report = EmailReport(report=report, sender_name=\"Sender Reporter\",\n                               sender_email=\"sender@arrow.com\",\n                               recipient_email=\"recipient@arrow.com\")\n\n    assert (\n        email_report.render(\"nightly_report\") == textwrap.dedent(expected_msg)\n    )\n\n\ndef test_crossbow_export_report(load_fixture):\n    job = load_fixture('crossbow-job.yaml', decoder=yaml.load)\n    report = Report(job)\n    assert len(list(report.rows)) == 4\n    expected_first_row = [\n        'docker-cpp-cmake32',\n        'success',\n        ['https://github.com/apache/crossbow/runs/1'],\n        'https://github.com/apache/crossbow/tree/'\n        'ursabot-1-circle-docker-cpp-cmake32',\n        'circle',\n        {'commands': ['docker-compose build cpp-cmake32',\n                      'docker-compose run cpp-cmake32']},\n        'docker-tests/circle.linux.yml',\n        'f766a1d615dd1b7ee706d05102e579195951a61c'\n    ]\n    assert next(report.rows) == expected_first_row\n", "dev/archery/archery/crossbow/tests/test_core.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom archery.utils.source import ArrowSources\nfrom archery.crossbow import Config, Queue\n\nimport pathlib\nfrom unittest import mock\n\n\ndef test_config():\n    src = ArrowSources.find()\n    conf = Config.load_yaml(src.dev / \"tasks\" / \"tasks.yml\")\n    conf.validate()\n\n\ndef test_task_select(request):\n    conf = Config.load_yaml(pathlib.Path(\n        request.node.fspath).parent / \"fixtures\" / \"tasks.yaml\")\n    conf.validate()\n\n    test_out = conf.select(tasks=[\"test-a-test-two\"])\n    assert test_out.keys() >= {\"test-a-test-two\"}\n\n\ndef test_group_select(request):\n    conf = Config.load_yaml(pathlib.Path(\n        request.node.fspath).parent / \"fixtures\" / \"tasks.yaml\")\n    conf.validate()\n\n    test_out = conf.select(groups=[\"test\"])\n    assert test_out.keys() >= {\"test-a-test-two\", \"test-a-test\"}\n\n\ndef test_group_select_blocklist(request):\n    conf = Config.load_yaml(pathlib.Path(\n        request.node.fspath).parent / \"fixtures\" / \"tasks.yaml\")\n    conf.validate()\n\n    # we respect the nightly blocklist\n    nightly_out = conf.select(groups=[\"nightly\"])\n    assert nightly_out.keys() >= {\"test-a-test\", \"nightly-fine\"}\n\n    # but if a task is not blocked in both groups, it shows up at least once\n    test_nightly_out = conf.select(groups=[\"nightly\", \"test\"])\n    assert test_nightly_out.keys() >= {\n        \"test-a-test-two\", \"test-a-test\", \"nightly-fine\"}\n\n    # but can then over-ride by requesting the task\n    test_nightly_out = conf.select(\n        tasks=[\"nightly-not-fine\", \"nightly-fine\"], groups=[\"nightly\", \"test\"])\n    assert test_nightly_out.keys() >= {\n        \"test-a-test-two\", \"test-a-test\", \"nightly-fine\", \"nightly-not-fine\"}\n\n    # and we can glob with the blocklist too!\n    test_nightly_no_test_out = conf.select(groups=[\"nightly-no-test\"])\n    assert test_nightly_no_test_out.keys(\n    ) >= {\"nightly-fine\", \"nightly-not-fine\"}\n\n\ndef test_latest_for_prefix(request):\n    queue = Queue(pathlib.Path(request.node.fspath).parent)\n    with mock.patch(\"archery.crossbow.core.Repo.repo\") as mocked_repo:\n        mocked_repo.branches = [\n            \"origin/nightly-packaging-2022-04-10-0\",\n            \"origin/nightly-packaging-2022-04-11-0\",\n        ]\n        with mock.patch(\"archery.crossbow.core.Queue.get\") as mocked_get:\n            queue.latest_for_prefix(\"nightly-packaging-2022-04-10\")\n            mocked_get.assert_called_once_with(\n                \"nightly-packaging-2022-04-10-0\")\n\n    with mock.patch(\"archery.crossbow.core.Repo.repo\") as mocked_repo:\n        mocked_repo.branches = [\n            \"origin/nightly-packaging-2022-04-10-0\",\n            \"origin/nightly-packaging-2022-04-11-0\",\n        ]\n        with mock.patch(\"archery.crossbow.core.Queue.get\") as mocked_get:\n            queue.latest_for_prefix(\"nightly-packaging\")\n            mocked_get.assert_called_once_with(\n                \"nightly-packaging-2022-04-11-0\")\n", "dev/release/check-rat-report.py": "#!/usr/bin/env python\n##############################################################################\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n##############################################################################\nimport fnmatch\nimport re\nimport sys\nimport xml.etree.ElementTree as ET\n\nif len(sys.argv) != 3:\n    sys.stderr.write(\"Usage: %s exclude_globs.lst rat_report.xml\\n\" %\n                     sys.argv[0])\n    sys.exit(1)\n\nexclude_globs_filename = sys.argv[1]\nxml_filename = sys.argv[2]\n\nglobs = [line.strip() for line in open(exclude_globs_filename, \"r\")]\n\ntree = ET.parse(xml_filename)\nroot = tree.getroot()\nresources = root.findall('resource')\n\nall_ok = True\nfor r in resources:\n    approvals = r.findall('license-approval')\n    if not approvals or approvals[0].attrib['name'] == 'true':\n        continue\n    clean_name = re.sub('^[^/]+/', '', r.attrib['name'])\n    excluded = False\n    for g in globs:\n        if fnmatch.fnmatch(clean_name, g):\n            excluded = True\n            break\n    if not excluded:\n        sys.stdout.write(\"NOT APPROVED: %s (%s): %s\\n\" % (\n            clean_name, r.attrib['name'], approvals[0].attrib['name']))\n        all_ok = False\n\nif not all_ok:\n    sys.exit(1)\n\nprint('OK')\nsys.exit(0)\n", "dev/release/download_rc_binaries.py": "#!/usr/bin/env python\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Download release binaries.\"\"\"\n\nimport argparse\nimport concurrent.futures as cf\nimport functools\nimport json\nimport os\nimport random\nimport re\nimport subprocess\nimport time\nimport urllib.request\n\n\nDEFAULT_PARALLEL_DOWNLOADS = 8\n\n\nclass Downloader:\n\n    def get_file_list(self, prefix, filter=None):\n        def traverse(directory, files, directories):\n            url = f'{self.URL_ROOT}/{directory}'\n            response = urllib.request.urlopen(url).read().decode()\n            paths = re.findall('<a href=\"(.+?)\"', response)\n            for path in paths:\n                path = re.sub(f'^{re.escape(url)}',\n                              '',\n                              path)\n                if path == '../':\n                    continue\n                resolved_path = f'{directory}{path}'\n                if filter and not filter(path):\n                    continue\n                if path.endswith('/'):\n                    directories.append(resolved_path)\n                else:\n                    files.append(resolved_path)\n        files = []\n        if prefix != '' and not prefix.endswith('/'):\n            prefix += '/'\n        directories = [prefix]\n        while len(directories) > 0:\n            directory = directories.pop()\n            traverse(directory, files, directories)\n        return files\n\n    def download_files(self, files, dest=None, num_parallel=None,\n                       re_match=None):\n        \"\"\"\n        Download files from Bintray in parallel. If file already exists, will\n        overwrite if the checksum does not match what Bintray says it should be\n\n        Parameters\n        ----------\n        files : List[Dict]\n            File listing from Bintray\n        dest : str, default None\n            Defaults to current working directory\n        num_parallel : int, default 8\n            Number of files to download in parallel. If set to None, uses\n            default\n        \"\"\"\n        if dest is None:\n            dest = os.getcwd()\n        if num_parallel is None:\n            num_parallel = DEFAULT_PARALLEL_DOWNLOADS\n\n        if re_match is not None:\n            regex = re.compile(re_match)\n            files = [x for x in files if regex.match(x)]\n\n        if num_parallel == 1:\n            for path in files:\n                self._download_file(dest, path)\n        else:\n            parallel_map_terminate_early(\n                functools.partial(self._download_file, dest),\n                files,\n                num_parallel\n            )\n\n    def _download_file(self, dest, path):\n        base, filename = os.path.split(path)\n\n        dest_dir = os.path.join(dest, base)\n        os.makedirs(dest_dir, exist_ok=True)\n\n        dest_path = os.path.join(dest_dir, filename)\n\n        print(\"Downloading {} to {}\".format(path, dest_path))\n\n        url = f'{self.URL_ROOT}/{path}'\n        self._download_url(url, dest_path)\n\n    def _download_url(self, url, dest_path, *, extra_args=None):\n        cmd = [\n            \"curl\",\n            \"--fail\",\n            \"--location\",\n            \"--retry\",\n            \"5\",\n            *(extra_args or []),\n            \"--output\",\n            dest_path,\n            url,\n        ]\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE)\n        stdout, stderr = proc.communicate()\n        if proc.returncode != 0:\n            try:\n                # Don't leave possibly partial file around\n                os.remove(dest_path)\n            except IOError:\n                pass\n            raise Exception(f\"Downloading {url} failed\\n\"\n                            f\"stdout: {stdout}\\nstderr: {stderr}\")\n\n    def _curl_version(self):\n        cmd = [\"curl\", \"--version\"]\n        out = subprocess.run(cmd, capture_output=True, check=True).stdout\n        match = re.search(r\"curl (\\d+)\\.(\\d+)\\.(\\d+) \", out.decode())\n        return (int(match.group(1)), int(match.group(2)), int(match.group(3)))\n\n\nclass Artifactory(Downloader):\n    URL_ROOT = \"https://apache.jfrog.io/artifactory/arrow\"\n\n\nclass Maven(Downloader):\n    URL_ROOT = \"https://repository.apache.org\" + \\\n        \"/content/repositories/staging/org/apache/arrow\"\n\n\nclass GitHub(Downloader):\n    def __init__(self, repository, tag):\n        super().__init__()\n        if repository is None:\n            raise ValueError(\"--repository is required\")\n        if tag is None:\n            raise ValueError(\"--tag is required\")\n        self._repository = repository\n        self._tag = tag\n\n    def get_file_list(self, prefix, filter=None):\n        url = (f\"https://api.github.com/repos/{self._repository}/\"\n               f\"releases/tags/{self._tag}\")\n        print(\"Fetching release from\", url)\n        request = urllib.request.Request(\n            url,\n            method=\"GET\",\n            headers={\n                \"Accept\": \"application/vnd.github+json\",\n            },\n        )\n        raw_response = urllib.request.urlopen(request).read().decode()\n        response = json.loads(raw_response)\n\n        files = []\n        for asset in response[\"assets\"]:\n            if filter and not filter(asset[\"name\"]):\n                continue\n            # Don't use the API URL since it has a fairly strict rate\n            # limit unless logged in, and we have a lot of tiny\n            # artifacts\n            url = (\n                f\"https://github.com/{self._repository}/\"\n                f\"releases/download/{self._tag}/{asset['name']}\"\n            )\n            files.append((asset[\"name\"], url))\n        return files\n\n    def _download_file(self, dest, asset):\n        name, url = asset\n\n        os.makedirs(dest, exist_ok=True)\n        dest_path = os.path.join(dest, name)\n        print(f\"Downloading {url} to {dest_path}\")\n\n        if os.path.isfile(dest_path):\n            print(\"Already downloaded\", dest_path)\n            return\n\n        delay = random.randint(0, 3)\n        print(f\"Waiting {delay} seconds to avoid rate limit\")\n        time.sleep(delay)\n\n        extra_args = [\n            \"--header\",\n            \"Accept: application/octet-stream\",\n        ]\n        if self._curl_version() >= (7, 71, 0):\n            # Also retry 403s\n            extra_args.append(\"--retry-all-errors\")\n        self._download_url(\n            url,\n            dest_path,\n            extra_args=extra_args\n        )\n\n\ndef parallel_map_terminate_early(f, iterable, num_parallel):\n    tasks = []\n    with cf.ProcessPoolExecutor(num_parallel) as pool:\n        for v in iterable:\n            tasks.append(pool.submit(functools.partial(f, v)))\n\n        for task in cf.as_completed(tasks):\n            if task.exception() is not None:\n                e = task.exception()\n                for task in tasks:\n                    task.cancel()\n                raise e\n\n\nARROW_REPOSITORY_PACKAGE_TYPES = [\n    'almalinux',\n    'amazon-linux',\n    'centos',\n    'debian',\n    'ubuntu',\n]\nARROW_STANDALONE_PACKAGE_TYPES = ['nuget', 'python']\nARROW_PACKAGE_TYPES = \\\n    ARROW_REPOSITORY_PACKAGE_TYPES + \\\n    ARROW_STANDALONE_PACKAGE_TYPES\n\n\ndef download_rc_binaries(version, rc_number, re_match=None, dest=None,\n                         num_parallel=None, target_package_type=None,\n                         repository=None, tag=None):\n    version_string = '{}-rc{}'.format(version, rc_number)\n    version_pattern = re.compile(r'\\d+\\.\\d+\\.\\d+')\n    if target_package_type:\n        package_types = [target_package_type]\n    else:\n        package_types = ARROW_PACKAGE_TYPES\n    for package_type in package_types:\n        def is_target(path):\n            match = version_pattern.search(path)\n            if not match:\n                return True\n            return match[0] == version\n        filter = is_target\n\n        if package_type == 'jars':\n            downloader = Maven()\n            prefix = ''\n        elif package_type == 'github':\n            downloader = GitHub(repository, tag)\n            prefix = ''\n            filter = None\n        elif package_type in ARROW_REPOSITORY_PACKAGE_TYPES:\n            downloader = Artifactory()\n            prefix = f'{package_type}-rc'\n        else:\n            downloader = Artifactory()\n            prefix = f'{package_type}-rc/{version_string}'\n            filter = None\n        files = downloader.get_file_list(prefix, filter=filter)\n        downloader.download_files(files, re_match=re_match, dest=dest,\n                                  num_parallel=num_parallel)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='Download release candidate binaries'\n    )\n    parser.add_argument('version', type=str, help='The version number')\n    parser.add_argument('rc_number', type=int,\n                        help='The release candidate number, e.g. 0, 1, etc')\n    parser.add_argument('-e', '--regexp', type=str, default=None,\n                        help=('Regular expression to match on file names '\n                              'to only download certain files'))\n    parser.add_argument('--dest', type=str, default=os.getcwd(),\n                        help='The output folder for the downloaded files')\n    parser.add_argument('--num_parallel', type=int,\n                        default=DEFAULT_PARALLEL_DOWNLOADS,\n                        help='The number of concurrent downloads to do')\n    parser.add_argument('--package_type', type=str, default=None,\n                        help='The package type to be downloaded')\n    parser.add_argument('--repository', type=str,\n                        help=('The repository to pull from '\n                              '(required if --package_type=github)'))\n    parser.add_argument('--tag', type=str,\n                        help=('The release tag to download '\n                              '(required if --package_type=github)'))\n    args = parser.parse_args()\n\n    download_rc_binaries(\n        args.version,\n        args.rc_number,\n        dest=args.dest,\n        re_match=args.regexp,\n        num_parallel=args.num_parallel,\n        target_package_type=args.package_type,\n        repository=args.repository,\n        tag=args.tag,\n    )\n", "dev/release/utils-update-docs-versions.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport json\nimport sys\n\ndir_path = sys.argv[1]\n# X.Y.Z\nversion = sys.argv[2]\n# {X+1}.Y.Z, X.{Y+1}.Z or X.Y.{Z+1}\nnext_version = sys.argv[3]\n\nmain_versions_path = dir_path + \"/docs/source/_static/versions.json\"\nr_versions_path = dir_path + \"/r/pkgdown/assets/versions.json\"\nr_html_path = dir_path + \"/r/pkgdown/assets/versions.html\"\n\nsplit_version = version.split(\".\")\nsplit_next_version = next_version.split(\".\")\n\nif split_next_version[1:] == [\"0\", \"0\"]:\n    release_type = \"major\"\nelif split_next_version[2:] == [\"0\"]:\n    release_type = \"minor\"\nelse:\n    release_type = \"patch\"\n\n# Update main docs version script only when compatible version of the\n# stable version isn't changed. Compatible version is ${MAJOR}.${MINOR}\n# version.\nif release_type != \"patch\":\n    with open(main_versions_path) as json_file:\n        old_versions = json.load(json_file)\n\n    dev_compatible_version = \".\".join(split_next_version[:2])\n    stable_compatible_version = \".\".join(split_version[:2])\n    previous_compatible_version = old_versions[1][\"name\"].split(\" \")[0]\n\n    # previous (compatible version) -> stable (compatible version)\n    #\n    # 13.Y.Z (13.Y) -> 14.0.0 (14.0): Update\n    # 14.0.0 (14.0) -> 14.0.1 (14.0): Not update\n    # 14.0.0 (14.0) -> 14.1.0 (14.1): Update\n    # 14.0.1 (14.0) -> 14.1.0 (14.1): Update\n    if stable_compatible_version != previous_compatible_version:\n        # Create new versions\n        new_versions = [\n            {\"name\": f\"{dev_compatible_version} (dev)\",\n             \"version\": \"dev/\",\n             \"url\": \"https://arrow.apache.org/docs/dev/\"},\n            {\"name\": f\"{stable_compatible_version} (stable)\",\n             \"version\": \"\",\n             \"url\": \"https://arrow.apache.org/docs/\",\n             \"preferred\": True},\n            {\"name\": previous_compatible_version,\n             \"version\": f\"{previous_compatible_version}/\",\n             \"url\": f\"https://arrow.apache.org/docs/{previous_compatible_version}/\"},\n            *old_versions[2:],\n        ]\n        with open(main_versions_path, 'w') as json_file:\n            json.dump(new_versions, json_file, indent=4)\n            json_file.write(\"\\n\")\n\n\n# Update R package version script\n\nwith open(r_versions_path) as json_file:\n    old_r_versions = json.load(json_file)\n\ndev_r_version = f\"{version}.9000\"\nrelease_r_version = version\nprevious_r_name = old_r_versions[1][\"name\"].split(\" \")[0]\nprevious_r_version = \".\".join(previous_r_name.split(\".\")[:2])\n\nif release_type == \"major\" and split_version[1:] == [\"0\", \"0\"]:\n    # 14.0.0 -> 15.0.0\n    new_r_versions = [\n        {\"name\": f\"{dev_r_version} (dev)\", \"version\": \"dev/\"},\n        {\"name\": f\"{release_r_version} (release)\", \"version\": \"\"},\n        {\"name\": previous_r_name, \"version\": f\"{previous_r_version}/\"},\n        *old_r_versions[2:],\n    ]\nelse:\n    # 14.0.1 -> 15.0.0\n    # 14.0.0 -> 14.1.0\n    # 14.0.1 -> 14.1.0\n    # 14.0.0 -> 14.0.1\n    new_r_versions = [\n        {\"name\": f\"{dev_r_version} (dev)\", \"version\": \"dev/\"},\n        {\"name\": f\"{release_r_version} (release)\", \"version\": \"\"},\n        *old_r_versions[2:],\n    ]\nwith open(r_versions_path, 'w') as json_file:\n    json.dump(new_r_versions, json_file, indent=4)\n    json_file.write(\"\\n\")\n\n# Load the updated versions JSON file\nwith open(r_versions_path) as json_file:\n    data = json.load(json_file)\n\n# Write HTML to file\nwith open(r_html_path, 'w') as html_file:\n    html_file.write('<!DOCTYPE html>\\n<html>\\n<body>')\n    for i in data:\n        html_file.write(f'<p><a href=\"../{i[\"version\"]}r/\">{i[\"name\"]}</a></p>\\n')\n    html_file.write('</body>\\n</html>\\n')\n", "r/inst/demo_flight_server.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"\n    An example Flight Python server.\n    See https://github.com/apache/arrow/blob/main/python/examples/flight/server.py\n\"\"\"\n\nimport ast\nimport threading\nimport time\n\nimport pyarrow\nimport pyarrow.flight\n\n\nclass DemoFlightServer(pyarrow.flight.FlightServerBase):\n    def __init__(self, host=\"localhost\", port=5005):\n        if isinstance(port, float):\n            # Because R is looser with integer vs. float\n            port = int(port)\n        location = \"grpc+tcp://{}:{}\".format(host, port)\n        super(DemoFlightServer, self).__init__(location)\n        self.flights = {}\n        self.host = host\n\n    @classmethod\n    def descriptor_to_key(self, descriptor):\n        return (descriptor.descriptor_type.value, descriptor.command,\n                tuple(descriptor.path or tuple()))\n\n    def _make_flight_info(self, key, descriptor, table):\n        location = pyarrow.flight.Location.for_grpc_tcp(self.host, self.port)\n        endpoints = [pyarrow.flight.FlightEndpoint(repr(key), [location]), ]\n\n        mock_sink = pyarrow.MockOutputStream()\n        stream_writer = pyarrow.RecordBatchStreamWriter(\n            mock_sink, table.schema)\n        stream_writer.write_table(table)\n        stream_writer.close()\n        data_size = mock_sink.size()\n\n        return pyarrow.flight.FlightInfo(table.schema,\n                                         descriptor, endpoints,\n                                         table.num_rows, data_size)\n\n    def list_flights(self, context, criteria):\n        print(\"list_flights\")\n        for key, table in self.flights.items():\n            if key[1] is not None:\n                descriptor = \\\n                    pyarrow.flight.FlightDescriptor.for_command(key[1])\n            else:\n                descriptor = pyarrow.flight.FlightDescriptor.for_path(*key[2])\n\n            yield self._make_flight_info(key, descriptor, table)\n\n    def get_flight_info(self, context, descriptor):\n        print(\"get_flight_info\")\n        key = DemoFlightServer.descriptor_to_key(descriptor)\n        if key in self.flights:\n            table = self.flights[key]\n            return self._make_flight_info(key, descriptor, table)\n        raise KeyError('Flight not found.')\n\n    def do_put(self, context, descriptor, reader, writer):\n        print(\"do_put\")\n        key = DemoFlightServer.descriptor_to_key(descriptor)\n        print(key)\n        self.flights[key] = reader.read_all()\n        print(self.flights[key])\n\n    def do_get(self, context, ticket):\n        print(\"do_get\")\n        key = ast.literal_eval(ticket.ticket.decode())\n        if key not in self.flights:\n            return None\n        return pyarrow.flight.RecordBatchStream(self.flights[key])\n\n    def list_actions(self, context):\n        print(\"list_actions\")\n        return [\n            (\"clear\", \"Clear the stored flights.\"),\n            (\"shutdown\", \"Shut down this server.\"),\n        ]\n\n    def do_action(self, context, action):\n        print(\"do_action\")\n        if action.type == \"clear\":\n            raise NotImplementedError(\n                \"{} is not implemented.\".format(action.type))\n        elif action.type == \"healthcheck\":\n            pass\n        elif action.type == \"shutdown\":\n            yield pyarrow.flight.Result(pyarrow.py_buffer(b'Shutdown!'))\n            # Shut down on background thread to avoid blocking current\n            # request\n            threading.Thread(target=self._shutdown).start()\n        else:\n            raise KeyError(\"Unknown action {!r}\".format(action.type))\n\n    def _shutdown(self):\n        \"\"\"Shut down after a delay.\"\"\"\n        print(\"Server is shutting down...\")\n        time.sleep(2)\n        self.shutdown()\n", "go/arrow/cdata/test/test_export_to_cgo.py": "#!/usr/bin/env python3\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport gc\nimport os\nimport unittest\n\nimport pyarrow as pa\nfrom pyarrow.cffi import ffi\n\n\ndef load_cgotest():\n    # XXX what about Darwin?\n    libext = 'so'\n    if os.name == 'nt':\n        libext = 'dll'\n\n    ffi.cdef(\n        \"\"\"\n        long long totalAllocated();\n        void importSchema(uintptr_t ptr);\n        void importRecordBatch(uintptr_t scptr, uintptr_t rbptr);\n        void runGC();\n        void exportSchema(uintptr_t ptr);\n        void exportRecordBatch(uintptr_t schema, uintptr_t record);\n        void importThenExportSchema(uintptr_t input, uintptr_t output);\n        void importThenExportRecord(uintptr_t schemaIn, uintptr_t arrIn, \n                                    uintptr_t schemaOut, uintptr_t arrOut);\n        void roundtripArray(uintptr_t arrIn, uintptr_t schema, uintptr_t arrOut);\n        \"\"\")\n    return ffi.dlopen(f'./cgotest.{libext}')\n\n\ncgotest = load_cgotest()\n\nclass BaseTestGoPython(unittest.TestCase):\n    def setUp(self):\n        self.c_schema = ffi.new(\"struct ArrowSchema*\")\n        self.ptr_schema = int(ffi.cast(\"uintptr_t\", self.c_schema))\n        self.c_array = ffi.new(\"struct ArrowArray*\")\n        self.ptr_array = int(ffi.cast(\"uintptr_t\", self.c_array))\n\n    def make_schema(self):\n        return pa.schema([('ints', pa.list_(pa.int32()))],\n                         metadata={b'key1': b'value1'})\n\n    def make_batch(self):\n        return pa.record_batch([[[1], [], None, [2, 42]]],\n                               self.make_schema())\n\n    def run_gc(self):\n        # Several Go GC runs can be required to run all finalizers\n        for i in range(5):\n            cgotest.runGC()\n        gc.collect()\n\n    @contextlib.contextmanager\n    def assert_pyarrow_memory_released(self):\n        self.run_gc()\n        old_allocated = pa.total_allocated_bytes()\n        old_go_allocated = cgotest.totalAllocated()\n        yield\n        self.run_gc()\n        diff = pa.total_allocated_bytes() - old_allocated\n        godiff = cgotest.totalAllocated() - old_go_allocated\n        self.assertEqual(\n            pa.total_allocated_bytes(), old_allocated,\n            f\"PyArrow memory was not adequately released: {diff} bytes lost\")\n        self.assertEqual(\n            cgotest.totalAllocated(), old_go_allocated,\n            f\"Go memory was not properly released: {godiff} bytes lost\")\n        \n\nclass TestPythonToGo(BaseTestGoPython):\n    \n    def test_schema(self):\n        with self.assert_pyarrow_memory_released():\n            self.make_schema()._export_to_c(self.ptr_schema)\n            # Will panic if expectations are not met\n            cgotest.importSchema(self.ptr_schema)\n\n    def test_record_batch(self):\n        with self.assert_pyarrow_memory_released():\n            self.make_schema()._export_to_c(self.ptr_schema)\n            self.make_batch()._export_to_c(self.ptr_array)\n            # Will panic if expectations are not met\n            cgotest.importRecordBatch(self.ptr_schema, self.ptr_array)\n\n\nclass TestGoToPython(BaseTestGoPython):\n\n    def test_get_schema(self):\n        with self.assert_pyarrow_memory_released():\n            cgotest.exportSchema(self.ptr_schema)\n\n            sc = pa.Schema._import_from_c(self.ptr_schema)\n            assert sc == self.make_schema()\n    \n    def test_get_batch(self):\n        with self.assert_pyarrow_memory_released():\n            cgotest.exportRecordBatch(self.ptr_schema, self.ptr_array)\n            arrnew = pa.RecordBatch._import_from_c(self.ptr_array, self.ptr_schema)\n            assert arrnew == self.make_batch()\n            del arrnew\n    \nclass TestRoundTrip(BaseTestGoPython):\n\n    def test_schema_roundtrip(self):\n        with self.assert_pyarrow_memory_released():\n            # make sure that Python -> Go -> Python ends up with\n            # the same exact schema\n            schema = self.make_schema()\n            schema._export_to_c(self.ptr_schema)\n            del schema\n            \n            c_schema = ffi.new(\"struct ArrowSchema*\")\n            ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n            cgotest.importThenExportSchema(self.ptr_schema, ptr_schema)\n            schema_new = pa.Schema._import_from_c(ptr_schema)\n            assert schema_new == self.make_schema()\n            del c_schema\n\n    def test_batch_roundtrip(self):\n        with self.assert_pyarrow_memory_released():\n            # make sure that Python -> Go -> Python for record\n            # batches works correctly and gets the same data in the end\n            schema = self.make_schema()\n            batch = self.make_batch()\n            schema._export_to_c(self.ptr_schema)\n            batch._export_to_c(self.ptr_array)\n            del schema\n            del batch\n\n            c_schema = ffi.new(\"struct ArrowSchema*\")\n            c_batch = ffi.new(\"struct ArrowArray*\")\n            ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n            ptr_batch = int(ffi.cast(\"uintptr_t\", c_batch))\n\n            cgotest.importThenExportRecord(self.ptr_schema, self.ptr_array, \n                                           ptr_schema, ptr_batch)\n            batch_new = pa.RecordBatch._import_from_c(ptr_batch, ptr_schema)\n            assert batch_new == self.make_batch()\n            del batch_new\n            del c_schema\n            del c_batch\n\n    # commented out types can be uncommented after\n    # GH-14875 is addressed\n    _test_pyarrow_types = [\n        pa.null(),\n        pa.bool_(),\n        pa.int32(),\n        pa.time32(\"s\"),\n        pa.time64(\"us\"),\n        pa.date32(),\n        pa.timestamp(\"us\"),\n        pa.timestamp(\"us\", tz=\"UTC\"),\n        pa.timestamp(\"us\", tz=\"Europe/Paris\"),\n        pa.duration(\"s\"),\n        pa.duration(\"ms\"),\n        pa.duration(\"us\"),\n        pa.duration(\"ns\"),\n        pa.float16(),\n        pa.float32(),\n        pa.float64(),\n        pa.decimal128(19, 4),        \n        pa.string(),\n        pa.binary(),\n        pa.binary(10),\n        pa.large_string(),\n        pa.large_binary(),\n        pa.list_(pa.int32()),\n        pa.list_(pa.int32(), 2),\n        pa.large_list(pa.uint16()),\n        pa.struct([\n            pa.field(\"a\", pa.int32()),\n            pa.field(\"b\", pa.int8()),\n            pa.field(\"c\", pa.string()),\n        ]),\n        pa.struct([\n            pa.field(\"a\", pa.int32(), nullable=False),\n            pa.field(\"b\", pa.int8(), nullable=False),\n            pa.field(\"c\", pa.string()),\n        ]),\n        pa.dictionary(pa.int8(), pa.int64()),\n        pa.dictionary(pa.int8(), pa.string()),\n        pa.map_(pa.string(), pa.int32()),\n        pa.map_(pa.int64(), pa.int32()),\n        # pa.run_end_encoded(pa.int16(), pa.int64()),\n    ]\n\n    def test_empty_roundtrip(self):\n        for typ in self._test_pyarrow_types:\n            with self.subTest(typ=typ):\n                with self.assert_pyarrow_memory_released():\n                    a = pa.array([], typ)\n                    a._export_to_c(self.ptr_array)\n                    typ._export_to_c(self.ptr_schema)\n                    \n                    c_arr = ffi.new(\"struct ArrowArray*\")\n                    ptr_arr = int(ffi.cast(\"uintptr_t\", c_arr))\n\n                    cgotest.roundtripArray(self.ptr_array, self.ptr_schema, ptr_arr)\n                    b = pa.Array._import_from_c(ptr_arr, typ)\n                    b.validate(full=True)\n                    assert a.to_pylist() == b.to_pylist()\n                    assert a.type == b.type\n                    del a\n                    del b\n\nif __name__ == '__main__':\n    unittest.main(verbosity=2)\n", "c_glib/tool/generate-version-header.py": "#!/usr/bin/env python3\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nimport argparse\nfrom io import TextIOBase\nfrom pathlib import Path\nimport re\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n            description=\"Generate C header with version macros\")\n    parser.add_argument(\n            \"--library\",\n            required=True,\n            help=\"The library name to use in macro prefixes\")\n    parser.add_argument(\n            \"--version\",\n            required=True,\n            help=\"The library version number\")\n    parser.add_argument(\n            \"--input\",\n            type=Path,\n            required=True,\n            help=\"Path to the input template file\")\n    parser.add_argument(\n            \"--output\",\n            type=Path,\n            required=True,\n            help=\"Path to the output file to generate\")\n\n    args = parser.parse_args()\n\n    with open(args.input, \"r\", encoding=\"utf-8\") as input_file, \\\n            open(args.output, \"w\", encoding=\"utf-8\") as output_file:\n        write_header(\n                input_file, output_file, args.library, args.version)\n\n\ndef write_header(\n        input_file: TextIOBase,\n        output_file: TextIOBase,\n        library_name: str,\n        version: str):\n    if \"-\" in version:\n        version, version_tag = version.split(\"-\")\n    else:\n        version_tag = \"\"\n    version_major, version_minor, version_micro = [int(v) for v in version.split(\".\")]\n\n    encoded_versions = generate_encoded_versions(library_name)\n    visibility_macros = generate_visibility_macros(library_name)\n    availability_macros = generate_availability_macros(library_name)\n\n    replacements = {\n            \"VERSION_MAJOR\": str(version_major),\n            \"VERSION_MINOR\": str(version_minor),\n            \"VERSION_MICRO\": str(version_micro),\n            \"VERSION_TAG\": version_tag,\n            \"ENCODED_VERSIONS\": encoded_versions,\n            \"VISIBILITY_MACROS\": visibility_macros,\n            \"AVAILABILITY_MACROS\": availability_macros,\n    }\n\n    output_file.write(re.sub(\n        r\"@([A-Z_]+)@\", lambda match: replacements[match[1]], input_file.read()))\n\n\ndef generate_visibility_macros(library: str) -> str:\n    return f\"\"\"#if (defined(_WIN32) || defined(__CYGWIN__)) && defined(_MSC_VER) && \\\n  !defined({library}_STATIC_COMPILATION)\n#  define {library}_EXPORT __declspec(dllexport)\n#  define {library}_IMPORT __declspec(dllimport)\n#else\n#  define {library}_EXPORT\n#  define {library}_IMPORT\n#endif\n\n#ifdef {library}_COMPILATION\n#  define {library}_API {library}_EXPORT\n#else\n#  define {library}_API {library}_IMPORT\n#endif\n\n#define {library}_EXTERN {library}_API extern\"\"\"\n\n\ndef generate_encoded_versions(library: str) -> str:\n    macros = []\n\n    for major_version, minor_version in ALL_VERSIONS:\n        macros.append(f\"\"\"/**\n * {library}_VERSION_{major_version}_{minor_version}:\n *\n * You can use this macro value for compile time API version check.\n *\n * Since: {major_version}.{minor_version}.0\n */\n#define {library}_VERSION_{major_version}_{minor_version} G_ENCODE_VERSION({major_version}, {minor_version})\"\"\")  # noqa: E501\n\n    return \"\\n\\n\".join(macros)\n\n\ndef generate_availability_macros(library: str) -> str:\n    macros = [f\"\"\"#define {library}_AVAILABLE_IN_ALL {library}_EXTERN\"\"\"]\n\n    for major_version, minor_version in ALL_VERSIONS:\n        macros.append(f\"\"\"#if {library}_VERSION_MIN_REQUIRED >= {library}_VERSION_{major_version}_{minor_version}\n#  define {library}_DEPRECATED_IN_{major_version}_{minor_version}               {library}_DEPRECATED\n#  define {library}_DEPRECATED_IN_{major_version}_{minor_version}_FOR(function) {library}_DEPRECATED_FOR(function)\n#else\n#  define {library}_DEPRECATED_IN_{major_version}_{minor_version}\n#  define {library}_DEPRECATED_IN_{major_version}_{minor_version}_FOR(function)\n#endif\n\n#if {library}_VERSION_MAX_ALLOWED < {library}_VERSION_{major_version}_{minor_version}\n#  define {library}_AVAILABLE_IN_{major_version}_{minor_version} {library}_EXTERN {library}_UNAVAILABLE({major_version}, {minor_version})\n#else\n#  define {library}_AVAILABLE_IN_{major_version}_{minor_version} {library}_EXTERN\n#endif\"\"\")  # noqa: E501\n\n    return \"\\n\\n\".join(macros)\n\n\nALL_VERSIONS = [\n        (17, 0),\n        (16, 0),\n        (15, 0),\n        (14, 0),\n        (13, 0),\n        (12, 0),\n        (11, 0),\n        (10, 0),\n        (9, 0),\n        (8, 0),\n        (7, 0),\n        (6, 0),\n        (5, 0),\n        (4, 0),\n        (3, 0),\n        (2, 0),\n        (1, 0),\n        (0, 17),\n        (0, 16),\n        (0, 15),\n        (0, 14),\n        (0, 13),\n        (0, 12),\n        (0, 11),\n        (0, 10),\n]\n\n\nif __name__ == '__main__':\n    main()\n", "python/setup.py": "#!/usr/bin/env python\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport os\nimport os.path\nfrom os.path import join as pjoin\nimport re\nimport shlex\nimport sys\n\nif sys.version_info >= (3, 10):\n    import sysconfig\nelse:\n    # Get correct EXT_SUFFIX on Windows (https://bugs.python.org/issue39825)\n    from distutils import sysconfig\n\nimport pkg_resources\nfrom setuptools import setup, Extension, Distribution, find_namespace_packages\n\nfrom Cython.Distutils import build_ext as _build_ext\nimport Cython\n\n# Check if we're running 64-bit Python\nis_64_bit = sys.maxsize > 2**32\n\nif Cython.__version__ < '0.29.31':\n    raise Exception(\n        'Please update your Cython version. Supported Cython >= 0.29.31')\n\nsetup_dir = os.path.abspath(os.path.dirname(__file__))\n\next_suffix = sysconfig.get_config_var('EXT_SUFFIX')\n\n\n@contextlib.contextmanager\ndef changed_dir(dirname):\n    oldcwd = os.getcwd()\n    os.chdir(dirname)\n    try:\n        yield\n    finally:\n        os.chdir(oldcwd)\n\n\ndef strtobool(val):\n    \"\"\"Convert a string representation of truth to true (1) or false (0).\n\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n    'val' is anything else.\n    \"\"\"\n    # Copied from distutils\n    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))\n\n\nclass build_ext(_build_ext):\n    _found_names = ()\n\n    def build_extensions(self):\n        numpy_incl = pkg_resources.resource_filename('numpy', 'core/include')\n\n        self.extensions = [ext for ext in self.extensions\n                           if ext.name != '__dummy__']\n\n        for ext in self.extensions:\n            if (hasattr(ext, 'include_dirs') and\n                    numpy_incl not in ext.include_dirs):\n                ext.include_dirs.append(numpy_incl)\n        _build_ext.build_extensions(self)\n\n    def run(self):\n        self._run_cmake()\n        _build_ext.run(self)\n\n    # adapted from cmake_build_ext in dynd-python\n    # github.com/libdynd/dynd-python\n\n    description = \"Build the C-extensions for arrow\"\n    user_options = ([('cmake-generator=', None, 'CMake generator'),\n                     ('extra-cmake-args=', None, 'extra arguments for CMake'),\n                     ('build-type=', None,\n                      'build type (debug or release), default release'),\n                     ('boost-namespace=', None,\n                      'namespace of boost (default: boost)'),\n                     ('with-cuda', None, 'build the Cuda extension'),\n                     ('with-flight', None, 'build the Flight extension'),\n                     ('with-substrait', None, 'build the Substrait extension'),\n                     ('with-acero', None, 'build the Acero Engine extension'),\n                     ('with-dataset', None, 'build the Dataset extension'),\n                     ('with-parquet', None, 'build the Parquet extension'),\n                     ('with-parquet-encryption', None,\n                      'build the Parquet encryption extension'),\n                     ('with-azure', None,\n                      'build the Azure Blob Storage extension'),\n                     ('with-gcs', None,\n                      'build the Google Cloud Storage (GCS) extension'),\n                     ('with-s3', None, 'build the Amazon S3 extension'),\n                     ('with-static-parquet', None, 'link parquet statically'),\n                     ('with-static-boost', None, 'link boost statically'),\n                     ('with-orc', None, 'build the ORC extension'),\n                     ('with-gandiva', None, 'build the Gandiva extension'),\n                     ('generate-coverage', None,\n                      'enable Cython code coverage'),\n                     ('bundle-boost', None,\n                      'bundle the (shared) Boost libraries'),\n                     ('bundle-cython-cpp', None,\n                      'bundle generated Cython C++ code '\n                      '(used for code coverage)'),\n                     ('bundle-arrow-cpp', None,\n                      'bundle the Arrow C++ libraries'),\n                     ('bundle-arrow-cpp-headers', None,\n                      'bundle the Arrow C++ headers')] +\n                    _build_ext.user_options)\n\n    def initialize_options(self):\n        _build_ext.initialize_options(self)\n        self.cmake_generator = os.environ.get('PYARROW_CMAKE_GENERATOR')\n        if not self.cmake_generator and sys.platform == 'win32':\n            self.cmake_generator = 'Visual Studio 15 2017 Win64'\n        self.extra_cmake_args = os.environ.get('PYARROW_CMAKE_OPTIONS', '')\n        self.build_type = os.environ.get('PYARROW_BUILD_TYPE',\n                                         'release').lower()\n\n        self.cmake_cxxflags = os.environ.get('PYARROW_CXXFLAGS', '')\n\n        if sys.platform == 'win32':\n            # Cannot do debug builds in Windows unless Python itself is a debug\n            # build\n            if not hasattr(sys, 'gettotalrefcount'):\n                self.build_type = 'release'\n\n        self.with_azure = None\n        self.with_gcs = None\n        self.with_s3 = None\n        self.with_hdfs = None\n        self.with_cuda = None\n        self.with_substrait = None\n        self.with_flight = None\n        self.with_acero = None\n        self.with_dataset = None\n        self.with_parquet = None\n        self.with_parquet_encryption = None\n        self.with_orc = None\n        self.with_gandiva = None\n\n        self.generate_coverage = strtobool(\n            os.environ.get('PYARROW_GENERATE_COVERAGE', '0'))\n        self.bundle_arrow_cpp = strtobool(\n            os.environ.get('PYARROW_BUNDLE_ARROW_CPP', '0'))\n        self.bundle_cython_cpp = strtobool(\n            os.environ.get('PYARROW_BUNDLE_CYTHON_CPP', '0'))\n\n    CYTHON_MODULE_NAMES = [\n        'lib',\n        '_fs',\n        '_csv',\n        '_json',\n        '_compute',\n        '_cuda',\n        '_flight',\n        '_dataset',\n        '_dataset_orc',\n        '_dataset_parquet',\n        '_acero',\n        '_feather',\n        '_parquet',\n        '_parquet_encryption',\n        '_pyarrow_cpp_tests',\n        '_orc',\n        '_azurefs',\n        '_gcsfs',\n        '_s3fs',\n        '_substrait',\n        '_hdfs',\n        'gandiva']\n\n    def _run_cmake(self):\n        # check if build_type is correctly passed / set\n        if self.build_type.lower() not in ('release', 'debug',\n                                           'relwithdebinfo'):\n            raise ValueError(\"--build-type (or PYARROW_BUILD_TYPE) needs to \"\n                             \"be 'release', 'debug' or 'relwithdebinfo'\")\n\n        # The directory containing this setup.py\n        source = os.path.dirname(os.path.abspath(__file__))\n\n        # The staging directory for the module being built\n        build_cmd = self.get_finalized_command('build')\n        saved_cwd = os.getcwd()\n        build_temp = pjoin(saved_cwd, build_cmd.build_temp)\n        build_lib = pjoin(saved_cwd, build_cmd.build_lib)\n\n        if not os.path.isdir(build_temp):\n            self.mkpath(build_temp)\n\n        if self.inplace:\n            # a bit hacky\n            build_lib = saved_cwd\n\n        install_prefix = pjoin(build_lib, \"pyarrow\")\n\n        # Change to the build directory\n        with changed_dir(build_temp):\n            # Detect if we built elsewhere\n            if os.path.isfile('CMakeCache.txt'):\n                cachefile = open('CMakeCache.txt', 'r')\n                cachedir = re.search('CMAKE_CACHEFILE_DIR:INTERNAL=(.*)',\n                                     cachefile.read()).group(1)\n                cachefile.close()\n                if (cachedir != build_temp):\n                    build_base = pjoin(saved_cwd, build_cmd.build_base)\n                    print(f\"-- Skipping build. Temp build {build_temp} does \"\n                          f\"not match cached dir {cachedir}\")\n                    print(\"---- For a clean build you might want to delete \"\n                          f\"{build_base}.\")\n                    return\n\n            cmake_options = [\n                f'-DCMAKE_INSTALL_PREFIX={install_prefix}',\n                f'-DPYTHON_EXECUTABLE={sys.executable}',\n                f'-DPython3_EXECUTABLE={sys.executable}',\n                f'-DPYARROW_CXXFLAGS={self.cmake_cxxflags}',\n            ]\n\n            def append_cmake_bool(value, varname):\n                cmake_options.append('-D{0}={1}'.format(\n                    varname, 'on' if value else 'off'))\n\n            def append_cmake_component(flag, varname):\n                # only pass this to cmake is the user pass the --with-component\n                # flag to setup.py build_ext\n                if flag is not None:\n                    append_cmake_bool(flag, varname)\n\n            if self.cmake_generator:\n                cmake_options += ['-G', self.cmake_generator]\n\n            append_cmake_component(self.with_cuda, 'PYARROW_CUDA')\n            append_cmake_component(self.with_substrait, 'PYARROW_SUBSTRAIT')\n            append_cmake_component(self.with_flight, 'PYARROW_FLIGHT')\n            append_cmake_component(self.with_gandiva, 'PYARROW_GANDIVA')\n            append_cmake_component(self.with_acero, 'PYARROW_ACERO')\n            append_cmake_component(self.with_dataset, 'PYARROW_DATASET')\n            append_cmake_component(self.with_orc, 'PYARROW_ORC')\n            append_cmake_component(self.with_parquet, 'PYARROW_PARQUET')\n            append_cmake_component(self.with_parquet_encryption,\n                                   'PYARROW_PARQUET_ENCRYPTION')\n            append_cmake_component(self.with_azure, 'PYARROW_AZURE')\n            append_cmake_component(self.with_gcs, 'PYARROW_GCS')\n            append_cmake_component(self.with_s3, 'PYARROW_S3')\n            append_cmake_component(self.with_hdfs, 'PYARROW_HDFS')\n\n            append_cmake_bool(self.bundle_arrow_cpp,\n                              'PYARROW_BUNDLE_ARROW_CPP')\n            append_cmake_bool(self.bundle_cython_cpp,\n                              'PYARROW_BUNDLE_CYTHON_CPP')\n            append_cmake_bool(self.generate_coverage,\n                              'PYARROW_GENERATE_COVERAGE')\n\n            cmake_options.append(\n                f'-DCMAKE_BUILD_TYPE={self.build_type.lower()}')\n\n            extra_cmake_args = shlex.split(self.extra_cmake_args)\n\n            build_tool_args = []\n            if sys.platform == 'win32':\n                if not is_64_bit:\n                    raise RuntimeError('Not supported on 32-bit Windows')\n            else:\n                build_tool_args.append('--')\n                if os.environ.get('PYARROW_BUILD_VERBOSE', '0') == '1':\n                    cmake_options.append('-DCMAKE_VERBOSE_MAKEFILE=ON')\n                parallel = os.environ.get('PYARROW_PARALLEL')\n                if parallel:\n                    build_tool_args.append(f'-j{parallel}')\n\n            # Generate the build files\n            print(\"-- Running cmake for PyArrow\")\n            self.spawn(['cmake'] + extra_cmake_args + cmake_options + [source])\n            print(\"-- Finished cmake for PyArrow\")\n\n            print(\"-- Running cmake --build for PyArrow\")\n            self.spawn(['cmake', '--build', '.', '--config', self.build_type] +\n                       build_tool_args)\n            print(\"-- Finished cmake --build for PyArrow\")\n\n            print(\"-- Running cmake --build --target install for PyArrow\")\n            self.spawn(['cmake', '--build', '.', '--config', self.build_type] +\n                       ['--target', 'install'] + build_tool_args)\n            print(\"-- Finished cmake --build --target install for PyArrow\")\n\n            self._found_names = []\n            for name in self.CYTHON_MODULE_NAMES:\n                built_path = pjoin(install_prefix, name + ext_suffix)\n                if os.path.exists(built_path):\n                    self._found_names.append(name)\n\n    def _get_build_dir(self):\n        # Get the package directory from build_py\n        build_py = self.get_finalized_command('build_py')\n        return build_py.get_package_dir('pyarrow')\n\n    def _get_cmake_ext_path(self, name):\n        # This is the name of the arrow C-extension\n        filename = name + ext_suffix\n        return pjoin(self._get_build_dir(), filename)\n\n    def get_ext_generated_cpp_source(self, name):\n        if sys.platform == 'win32':\n            head, tail = os.path.split(name)\n            return pjoin(head, tail + \".cpp\")\n        else:\n            return pjoin(name + \".cpp\")\n\n    def get_ext_built_api_header(self, name):\n        if sys.platform == 'win32':\n            head, tail = os.path.split(name)\n            return pjoin(head, tail + \"_api.h\")\n        else:\n            return pjoin(name + \"_api.h\")\n\n    def get_names(self):\n        return self._found_names\n\n    def get_outputs(self):\n        # Just the C extensions\n        # regular_exts = _build_ext.get_outputs(self)\n        return [self._get_cmake_ext_path(name)\n                for name in self.get_names()]\n\n\nclass BinaryDistribution(Distribution):\n    def has_ext_modules(foo):\n        return True\n\n\nif strtobool(os.environ.get('PYARROW_INSTALL_TESTS', '1')):\n    packages = find_namespace_packages(include=['pyarrow*'])\n    exclude_package_data = {}\nelse:\n    packages = find_namespace_packages(include=['pyarrow*'],\n                                       exclude=[\"pyarrow.tests*\"])\n    # setuptools adds back importable packages even when excluded.\n    # https://github.com/pypa/setuptools/issues/3260\n    # https://github.com/pypa/setuptools/issues/3340#issuecomment-1219383976\n    exclude_package_data = {\"pyarrow\": [\"tests*\"]}\n\n\nsetup(\n    packages=packages,\n    exclude_package_data=exclude_package_data,\n    distclass=BinaryDistribution,\n    # Dummy extension to trigger build_ext\n    ext_modules=[Extension('__dummy__', sources=[])],\n    cmdclass={\n        'build_ext': build_ext\n    },\n)\n", "python/pyarrow/dataset.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"Dataset is currently unstable. APIs subject to change without notice.\"\"\"\n\nimport pyarrow as pa\nfrom pyarrow.util import _is_iterable, _stringify_path, _is_path_like\n\ntry:\n    from pyarrow._dataset import (  # noqa\n        CsvFileFormat,\n        CsvFragmentScanOptions,\n        JsonFileFormat,\n        JsonFragmentScanOptions,\n        Dataset,\n        DatasetFactory,\n        DirectoryPartitioning,\n        FeatherFileFormat,\n        FilenamePartitioning,\n        FileFormat,\n        FileFragment,\n        FileSystemDataset,\n        FileSystemDatasetFactory,\n        FileSystemFactoryOptions,\n        FileWriteOptions,\n        Fragment,\n        FragmentScanOptions,\n        HivePartitioning,\n        IpcFileFormat,\n        IpcFileWriteOptions,\n        InMemoryDataset,\n        Partitioning,\n        PartitioningFactory,\n        Scanner,\n        TaggedRecordBatch,\n        UnionDataset,\n        UnionDatasetFactory,\n        WrittenFile,\n        get_partition_keys,\n        get_partition_keys as _get_partition_keys,  # keep for backwards compatibility\n        _filesystemdataset_write,\n    )\nexcept ImportError as exc:\n    raise ImportError(\n        f\"The pyarrow installation is not built with support for 'dataset' ({str(exc)})\"\n    ) from None\n\n# keep Expression functionality exposed here for backwards compatibility\nfrom pyarrow.compute import Expression, scalar, field  # noqa\n\n\n_orc_available = False\n_orc_msg = (\n    \"The pyarrow installation is not built with support for the ORC file \"\n    \"format.\"\n)\n\ntry:\n    from pyarrow._dataset_orc import OrcFileFormat\n    _orc_available = True\nexcept ImportError:\n    pass\n\n_parquet_available = False\n_parquet_msg = (\n    \"The pyarrow installation is not built with support for the Parquet file \"\n    \"format.\"\n)\n\ntry:\n    from pyarrow._dataset_parquet import (  # noqa\n        ParquetDatasetFactory,\n        ParquetFactoryOptions,\n        ParquetFileFormat,\n        ParquetFileFragment,\n        ParquetFileWriteOptions,\n        ParquetFragmentScanOptions,\n        ParquetReadOptions,\n        RowGroupInfo,\n    )\n    _parquet_available = True\nexcept ImportError:\n    pass\n\n\ntry:\n    from pyarrow._dataset_parquet_encryption import (  # noqa\n        ParquetDecryptionConfig,\n        ParquetEncryptionConfig,\n    )\nexcept ImportError:\n    pass\n\n\ndef __getattr__(name):\n    if name == \"OrcFileFormat\" and not _orc_available:\n        raise ImportError(_orc_msg)\n\n    if name == \"ParquetFileFormat\" and not _parquet_available:\n        raise ImportError(_parquet_msg)\n\n    raise AttributeError(\n        \"module 'pyarrow.dataset' has no attribute '{0}'\".format(name)\n    )\n\n\ndef partitioning(schema=None, field_names=None, flavor=None,\n                 dictionaries=None):\n    \"\"\"\n    Specify a partitioning scheme.\n\n    The supported schemes include:\n\n    - \"DirectoryPartitioning\": this scheme expects one segment in the file path\n      for each field in the specified schema (all fields are required to be\n      present). For example given schema<year:int16, month:int8> the path\n      \"/2009/11\" would be parsed to (\"year\"_ == 2009 and \"month\"_ == 11).\n    - \"HivePartitioning\": a scheme for \"/$key=$value/\" nested directories as\n      found in Apache Hive. This is a multi-level, directory based partitioning\n      scheme. Data is partitioned by static values of a particular column in\n      the schema. Partition keys are represented in the form $key=$value in\n      directory names. Field order is ignored, as are missing or unrecognized\n      field names.\n      For example, given schema<year:int16, month:int8, day:int8>, a possible\n      path would be \"/year=2009/month=11/day=15\" (but the field order does not\n      need to match).\n    - \"FilenamePartitioning\": this scheme expects the partitions will have\n      filenames containing the field values separated by \"_\".\n      For example, given schema<year:int16, month:int8, day:int8>, a possible\n      partition filename \"2009_11_part-0.parquet\" would be parsed\n      to (\"year\"_ == 2009 and \"month\"_ == 11).\n\n    Parameters\n    ----------\n    schema : pyarrow.Schema, default None\n        The schema that describes the partitions present in the file path.\n        If not specified, and `field_names` and/or `flavor` are specified,\n        the schema will be inferred from the file path (and a\n        PartitioningFactory is returned).\n    field_names :  list of str, default None\n        A list of strings (field names). If specified, the schema's types are\n        inferred from the file paths (only valid for DirectoryPartitioning).\n    flavor : str, default None\n        The default is DirectoryPartitioning. Specify ``flavor=\"hive\"`` for\n        a HivePartitioning, and ``flavor=\"filename\"`` for a\n        FilenamePartitioning.\n    dictionaries : dict[str, Array]\n        If the type of any field of `schema` is a dictionary type, the\n        corresponding entry of `dictionaries` must be an array containing\n        every value which may be taken by the corresponding column or an\n        error will be raised in parsing. Alternatively, pass `infer` to have\n        Arrow discover the dictionary values, in which case a\n        PartitioningFactory is returned.\n\n    Returns\n    -------\n    Partitioning or PartitioningFactory\n        The partitioning scheme\n\n    Examples\n    --------\n\n    Specify the Schema for paths like \"/2009/June\":\n\n    >>> import pyarrow as pa\n    >>> import pyarrow.dataset as ds\n    >>> part = ds.partitioning(pa.schema([(\"year\", pa.int16()),\n    ...                                   (\"month\", pa.string())]))\n\n    or let the types be inferred by only specifying the field names:\n\n    >>> part =  ds.partitioning(field_names=[\"year\", \"month\"])\n\n    For paths like \"/2009/June\", the year will be inferred as int32 while month\n    will be inferred as string.\n\n    Specify a Schema with dictionary encoding, providing dictionary values:\n\n    >>> part = ds.partitioning(\n    ...     pa.schema([\n    ...         (\"year\", pa.int16()),\n    ...         (\"month\", pa.dictionary(pa.int8(), pa.string()))\n    ...     ]),\n    ...     dictionaries={\n    ...         \"month\": pa.array([\"January\", \"February\", \"March\"]),\n    ...     })\n\n    Alternatively, specify a Schema with dictionary encoding, but have Arrow\n    infer the dictionary values:\n\n    >>> part = ds.partitioning(\n    ...     pa.schema([\n    ...         (\"year\", pa.int16()),\n    ...         (\"month\", pa.dictionary(pa.int8(), pa.string()))\n    ...     ]),\n    ...     dictionaries=\"infer\")\n\n    Create a Hive scheme for a path like \"/year=2009/month=11\":\n\n    >>> part = ds.partitioning(\n    ...     pa.schema([(\"year\", pa.int16()), (\"month\", pa.int8())]),\n    ...     flavor=\"hive\")\n\n    A Hive scheme can also be discovered from the directory structure (and\n    types will be inferred):\n\n    >>> part = ds.partitioning(flavor=\"hive\")\n    \"\"\"\n    if flavor is None:\n        # default flavor\n        if schema is not None:\n            if field_names is not None:\n                raise ValueError(\n                    \"Cannot specify both 'schema' and 'field_names'\")\n            if dictionaries == 'infer':\n                return DirectoryPartitioning.discover(schema=schema)\n            return DirectoryPartitioning(schema, dictionaries)\n        elif field_names is not None:\n            if isinstance(field_names, list):\n                return DirectoryPartitioning.discover(field_names)\n            else:\n                raise ValueError(\n                    \"Expected list of field names, got {}\".format(\n                        type(field_names)))\n        else:\n            raise ValueError(\n                \"For the default directory flavor, need to specify \"\n                \"a Schema or a list of field names\")\n    if flavor == \"filename\":\n        if schema is not None:\n            if field_names is not None:\n                raise ValueError(\n                    \"Cannot specify both 'schema' and 'field_names'\")\n            if dictionaries == 'infer':\n                return FilenamePartitioning.discover(schema=schema)\n            return FilenamePartitioning(schema, dictionaries)\n        elif field_names is not None:\n            if isinstance(field_names, list):\n                return FilenamePartitioning.discover(field_names)\n            else:\n                raise ValueError(\n                    \"Expected list of field names, got {}\".format(\n                        type(field_names)))\n        else:\n            raise ValueError(\n                \"For the filename flavor, need to specify \"\n                \"a Schema or a list of field names\")\n    elif flavor == 'hive':\n        if field_names is not None:\n            raise ValueError(\"Cannot specify 'field_names' for flavor 'hive'\")\n        elif schema is not None:\n            if isinstance(schema, pa.Schema):\n                if dictionaries == 'infer':\n                    return HivePartitioning.discover(schema=schema)\n                return HivePartitioning(schema, dictionaries)\n            else:\n                raise ValueError(\n                    \"Expected Schema for 'schema', got {}\".format(\n                        type(schema)))\n        else:\n            return HivePartitioning.discover()\n    else:\n        raise ValueError(\"Unsupported flavor\")\n\n\ndef _ensure_partitioning(scheme):\n    \"\"\"\n    Validate input and return a Partitioning(Factory).\n\n    It passes None through if no partitioning scheme is defined.\n    \"\"\"\n    if scheme is None:\n        pass\n    elif isinstance(scheme, str):\n        scheme = partitioning(flavor=scheme)\n    elif isinstance(scheme, list):\n        scheme = partitioning(field_names=scheme)\n    elif isinstance(scheme, (Partitioning, PartitioningFactory)):\n        pass\n    else:\n        raise ValueError(\"Expected Partitioning or PartitioningFactory, got {}\"\n                         .format(type(scheme)))\n    return scheme\n\n\ndef _ensure_format(obj):\n    if isinstance(obj, FileFormat):\n        return obj\n    elif obj == \"parquet\":\n        if not _parquet_available:\n            raise ValueError(_parquet_msg)\n        return ParquetFileFormat()\n    elif obj in {\"ipc\", \"arrow\"}:\n        return IpcFileFormat()\n    elif obj == \"feather\":\n        return FeatherFileFormat()\n    elif obj == \"csv\":\n        return CsvFileFormat()\n    elif obj == \"orc\":\n        if not _orc_available:\n            raise ValueError(_orc_msg)\n        return OrcFileFormat()\n    elif obj == \"json\":\n        return JsonFileFormat()\n    else:\n        raise ValueError(\"format '{}' is not supported\".format(obj))\n\n\ndef _ensure_multiple_sources(paths, filesystem=None):\n    \"\"\"\n    Treat a list of paths as files belonging to a single file system\n\n    If the file system is local then also validates that all paths\n    are referencing existing *files* otherwise any non-file paths will be\n    silently skipped (for example on a remote filesystem).\n\n    Parameters\n    ----------\n    paths : list of path-like\n        Note that URIs are not allowed.\n    filesystem : FileSystem or str, optional\n        If an URI is passed, then its path component will act as a prefix for\n        the file paths.\n\n    Returns\n    -------\n    (FileSystem, list of str)\n        File system object and a list of normalized paths.\n\n    Raises\n    ------\n    TypeError\n        If the passed filesystem has wrong type.\n    IOError\n        If the file system is local and a referenced path is not available or\n        not a file.\n    \"\"\"\n    from pyarrow.fs import (\n        LocalFileSystem, SubTreeFileSystem, _MockFileSystem, FileType,\n        _ensure_filesystem\n    )\n\n    if filesystem is None:\n        # fall back to local file system as the default\n        filesystem = LocalFileSystem()\n    else:\n        # construct a filesystem if it is a valid URI\n        filesystem = _ensure_filesystem(filesystem)\n\n    is_local = (\n        isinstance(filesystem, (LocalFileSystem, _MockFileSystem)) or\n        (isinstance(filesystem, SubTreeFileSystem) and\n         isinstance(filesystem.base_fs, LocalFileSystem))\n    )\n\n    # allow normalizing irregular paths such as Windows local paths\n    paths = [filesystem.normalize_path(_stringify_path(p)) for p in paths]\n\n    # validate that all of the paths are pointing to existing *files*\n    # possible improvement is to group the file_infos by type and raise for\n    # multiple paths per error category\n    if is_local:\n        for info in filesystem.get_file_info(paths):\n            file_type = info.type\n            if file_type == FileType.File:\n                continue\n            elif file_type == FileType.NotFound:\n                raise FileNotFoundError(info.path)\n            elif file_type == FileType.Directory:\n                raise IsADirectoryError(\n                    'Path {} points to a directory, but only file paths are '\n                    'supported. To construct a nested or union dataset pass '\n                    'a list of dataset objects instead.'.format(info.path)\n                )\n            else:\n                raise IOError(\n                    'Path {} exists but its type is unknown (could be a '\n                    'special file such as a Unix socket or character device, '\n                    'or Windows NUL / CON / ...)'.format(info.path)\n                )\n\n    return filesystem, paths\n\n\ndef _ensure_single_source(path, filesystem=None):\n    \"\"\"\n    Treat path as either a recursively traversable directory or a single file.\n\n    Parameters\n    ----------\n    path : path-like\n    filesystem : FileSystem or str, optional\n        If an URI is passed, then its path component will act as a prefix for\n        the file paths.\n\n    Returns\n    -------\n    (FileSystem, list of str or fs.Selector)\n        File system object and either a single item list pointing to a file or\n        an fs.Selector object pointing to a directory.\n\n    Raises\n    ------\n    TypeError\n        If the passed filesystem has wrong type.\n    FileNotFoundError\n        If the referenced file or directory doesn't exist.\n    \"\"\"\n    from pyarrow.fs import FileType, FileSelector, _resolve_filesystem_and_path\n\n    # at this point we already checked that `path` is a path-like\n    filesystem, path = _resolve_filesystem_and_path(path, filesystem)\n\n    # ensure that the path is normalized before passing to dataset discovery\n    path = filesystem.normalize_path(path)\n\n    # retrieve the file descriptor\n    file_info = filesystem.get_file_info(path)\n\n    # depending on the path type either return with a recursive\n    # directory selector or as a list containing a single file\n    if file_info.type == FileType.Directory:\n        paths_or_selector = FileSelector(path, recursive=True)\n    elif file_info.type == FileType.File:\n        paths_or_selector = [path]\n    else:\n        raise FileNotFoundError(path)\n\n    return filesystem, paths_or_selector\n\n\ndef _filesystem_dataset(source, schema=None, filesystem=None,\n                        partitioning=None, format=None,\n                        partition_base_dir=None, exclude_invalid_files=None,\n                        selector_ignore_prefixes=None):\n    \"\"\"\n    Create a FileSystemDataset which can be used to build a Dataset.\n\n    Parameters are documented in the dataset function.\n\n    Returns\n    -------\n    FileSystemDataset\n    \"\"\"\n    from pyarrow.fs import LocalFileSystem, _ensure_filesystem, FileInfo\n\n    format = _ensure_format(format or 'parquet')\n    partitioning = _ensure_partitioning(partitioning)\n\n    if isinstance(source, (list, tuple)):\n        if source and isinstance(source[0], FileInfo):\n            if filesystem is None:\n                # fall back to local file system as the default\n                fs = LocalFileSystem()\n            else:\n                # construct a filesystem if it is a valid URI\n                fs = _ensure_filesystem(filesystem)\n            paths_or_selector = source\n        else:\n            fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\n    else:\n        fs, paths_or_selector = _ensure_single_source(source, filesystem)\n\n    options = FileSystemFactoryOptions(\n        partitioning=partitioning,\n        partition_base_dir=partition_base_dir,\n        exclude_invalid_files=exclude_invalid_files,\n        selector_ignore_prefixes=selector_ignore_prefixes\n    )\n    factory = FileSystemDatasetFactory(fs, paths_or_selector, format, options)\n\n    return factory.finish(schema)\n\n\ndef _in_memory_dataset(source, schema=None, **kwargs):\n    if any(v is not None for v in kwargs.values()):\n        raise ValueError(\n            \"For in-memory datasets, you cannot pass any additional arguments\")\n    return InMemoryDataset(source, schema)\n\n\ndef _union_dataset(children, schema=None, **kwargs):\n    if any(v is not None for v in kwargs.values()):\n        raise ValueError(\n            \"When passing a list of Datasets, you cannot pass any additional \"\n            \"arguments\"\n        )\n\n    if schema is None:\n        # unify the children datasets' schemas\n        schema = pa.unify_schemas([child.schema for child in children])\n\n    for child in children:\n        if getattr(child, \"_scan_options\", None):\n            raise ValueError(\n                \"Creating an UnionDataset from filtered or projected Datasets \"\n                \"is currently not supported. Union the unfiltered datasets \"\n                \"and apply the filter to the resulting union.\"\n            )\n\n    # create datasets with the requested schema\n    children = [child.replace_schema(schema) for child in children]\n\n    return UnionDataset(schema, children)\n\n\ndef parquet_dataset(metadata_path, schema=None, filesystem=None, format=None,\n                    partitioning=None, partition_base_dir=None):\n    \"\"\"\n    Create a FileSystemDataset from a `_metadata` file created via\n    `pyarrow.parquet.write_metadata`.\n\n    Parameters\n    ----------\n    metadata_path : path,\n        Path pointing to a single file parquet metadata file\n    schema : Schema, optional\n        Optionally provide the Schema for the Dataset, in which case it will\n        not be inferred from the source.\n    filesystem : FileSystem or URI string, default None\n        If a single path is given as source and filesystem is None, then the\n        filesystem will be inferred from the path.\n        If an URI string is passed, then a filesystem object is constructed\n        using the URI's optional path component as a directory prefix. See the\n        examples below.\n        Note that the URIs on Windows must follow 'file:///C:...' or\n        'file:/C:...' patterns.\n    format : ParquetFileFormat\n        An instance of a ParquetFileFormat if special options needs to be\n        passed.\n    partitioning : Partitioning, PartitioningFactory, str, list of str\n        The partitioning scheme specified with the ``partitioning()``\n        function. A flavor string can be used as shortcut, and with a list of\n        field names a DirectoryPartitioning will be inferred.\n    partition_base_dir : str, optional\n        For the purposes of applying the partitioning, paths will be\n        stripped of the partition_base_dir. Files not matching the\n        partition_base_dir prefix will be skipped for partitioning discovery.\n        The ignored files will still be part of the Dataset, but will not\n        have partition information.\n\n    Returns\n    -------\n    FileSystemDataset\n        The dataset corresponding to the given metadata\n    \"\"\"\n    from pyarrow.fs import LocalFileSystem, _ensure_filesystem\n\n    if format is None:\n        format = ParquetFileFormat()\n    elif not isinstance(format, ParquetFileFormat):\n        raise ValueError(\"format argument must be a ParquetFileFormat\")\n\n    if filesystem is None:\n        filesystem = LocalFileSystem()\n    else:\n        filesystem = _ensure_filesystem(filesystem)\n\n    metadata_path = filesystem.normalize_path(_stringify_path(metadata_path))\n    options = ParquetFactoryOptions(\n        partition_base_dir=partition_base_dir,\n        partitioning=_ensure_partitioning(partitioning)\n    )\n\n    factory = ParquetDatasetFactory(\n        metadata_path, filesystem, format, options=options)\n    return factory.finish(schema)\n\n\ndef dataset(source, schema=None, format=None, filesystem=None,\n            partitioning=None, partition_base_dir=None,\n            exclude_invalid_files=None, ignore_prefixes=None):\n    \"\"\"\n    Open a dataset.\n\n    Datasets provides functionality to efficiently work with tabular,\n    potentially larger than memory and multi-file dataset.\n\n    - A unified interface for different sources, like Parquet and Feather\n    - Discovery of sources (crawling directories, handle directory-based\n      partitioned datasets, basic schema normalization)\n    - Optimized reading with predicate pushdown (filtering rows), projection\n      (selecting columns), parallel reading or fine-grained managing of tasks.\n\n    Note that this is the high-level API, to have more control over the dataset\n    construction use the low-level API classes (FileSystemDataset,\n    FilesystemDatasetFactory, etc.)\n\n    Parameters\n    ----------\n    source : path, list of paths, dataset, list of datasets, (list of) \\\nRecordBatch or Table, iterable of RecordBatch, RecordBatchReader, or URI\n        Path pointing to a single file:\n            Open a FileSystemDataset from a single file.\n        Path pointing to a directory:\n            The directory gets discovered recursively according to a\n            partitioning scheme if given.\n        List of file paths:\n            Create a FileSystemDataset from explicitly given files. The files\n            must be located on the same filesystem given by the filesystem\n            parameter.\n            Note that in contrary of construction from a single file, passing\n            URIs as paths is not allowed.\n        List of datasets:\n            A nested UnionDataset gets constructed, it allows arbitrary\n            composition of other datasets.\n            Note that additional keyword arguments are not allowed.\n        (List of) batches or tables, iterable of batches, or RecordBatchReader:\n            Create an InMemoryDataset. If an iterable or empty list is given,\n            a schema must also be given. If an iterable or RecordBatchReader\n            is given, the resulting dataset can only be scanned once; further\n            attempts will raise an error.\n    schema : Schema, optional\n        Optionally provide the Schema for the Dataset, in which case it will\n        not be inferred from the source.\n    format : FileFormat or str\n        Currently \"parquet\", \"ipc\"/\"arrow\"/\"feather\", \"csv\", \"json\", and \"orc\" are\n        supported. For Feather, only version 2 files are supported.\n    filesystem : FileSystem or URI string, default None\n        If a single path is given as source and filesystem is None, then the\n        filesystem will be inferred from the path.\n        If an URI string is passed, then a filesystem object is constructed\n        using the URI's optional path component as a directory prefix. See the\n        examples below.\n        Note that the URIs on Windows must follow 'file:///C:...' or\n        'file:/C:...' patterns.\n    partitioning : Partitioning, PartitioningFactory, str, list of str\n        The partitioning scheme specified with the ``partitioning()``\n        function. A flavor string can be used as shortcut, and with a list of\n        field names a DirectoryPartitioning will be inferred.\n    partition_base_dir : str, optional\n        For the purposes of applying the partitioning, paths will be\n        stripped of the partition_base_dir. Files not matching the\n        partition_base_dir prefix will be skipped for partitioning discovery.\n        The ignored files will still be part of the Dataset, but will not\n        have partition information.\n    exclude_invalid_files : bool, optional (default True)\n        If True, invalid files will be excluded (file format specific check).\n        This will incur IO for each files in a serial and single threaded\n        fashion. Disabling this feature will skip the IO, but unsupported\n        files may be present in the Dataset (resulting in an error at scan\n        time).\n    ignore_prefixes : list, optional\n        Files matching any of these prefixes will be ignored by the\n        discovery process. This is matched to the basename of a path.\n        By default this is ['.', '_'].\n        Note that discovery happens only if a directory is passed as source.\n\n    Returns\n    -------\n    dataset : Dataset\n        Either a FileSystemDataset or a UnionDataset depending on the source\n        parameter.\n\n    Examples\n    --------\n    Creating an example Table:\n\n    >>> import pyarrow as pa\n    >>> import pyarrow.parquet as pq\n    >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n    ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n    ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n    ...                              \"Brittle stars\", \"Centipede\"]})\n    >>> pq.write_table(table, \"file.parquet\")\n\n    Opening a single file:\n\n    >>> import pyarrow.dataset as ds\n    >>> dataset = ds.dataset(\"file.parquet\", format=\"parquet\")\n    >>> dataset.to_table()\n    pyarrow.Table\n    year: int64\n    n_legs: int64\n    animal: string\n    ----\n    year: [[2020,2022,2021,2022,2019,2021]]\n    n_legs: [[2,2,4,4,5,100]]\n    animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n\n    Opening a single file with an explicit schema:\n\n    >>> myschema = pa.schema([\n    ...     ('n_legs', pa.int64()),\n    ...     ('animal', pa.string())])\n    >>> dataset = ds.dataset(\"file.parquet\", schema=myschema, format=\"parquet\")\n    >>> dataset.to_table()\n    pyarrow.Table\n    n_legs: int64\n    animal: string\n    ----\n    n_legs: [[2,2,4,4,5,100]]\n    animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n\n    Opening a dataset for a single directory:\n\n    >>> ds.write_dataset(table, \"partitioned_dataset\", format=\"parquet\",\n    ...                  partitioning=['year'])\n    >>> dataset = ds.dataset(\"partitioned_dataset\", format=\"parquet\")\n    >>> dataset.to_table()\n    pyarrow.Table\n    n_legs: int64\n    animal: string\n    ----\n    n_legs: [[5],[2],[4,100],[2,4]]\n    animal: [[\"Brittle stars\"],[\"Flamingo\"],...[\"Parrot\",\"Horse\"]]\n\n    For a single directory from a S3 bucket:\n\n    >>> ds.dataset(\"s3://mybucket/nyc-taxi/\",\n    ...            format=\"parquet\") # doctest: +SKIP\n\n    Opening a dataset from a list of relatives local paths:\n\n    >>> dataset = ds.dataset([\n    ...     \"partitioned_dataset/2019/part-0.parquet\",\n    ...     \"partitioned_dataset/2020/part-0.parquet\",\n    ...     \"partitioned_dataset/2021/part-0.parquet\",\n    ... ], format='parquet')\n    >>> dataset.to_table()\n    pyarrow.Table\n    n_legs: int64\n    animal: string\n    ----\n    n_legs: [[5],[2],[4,100]]\n    animal: [[\"Brittle stars\"],[\"Flamingo\"],[\"Dog\",\"Centipede\"]]\n\n    With filesystem provided:\n\n    >>> paths = [\n    ...     'part0/data.parquet',\n    ...     'part1/data.parquet',\n    ...     'part3/data.parquet',\n    ... ]\n    >>> ds.dataset(paths, filesystem='file:///directory/prefix,\n    ...            format='parquet') # doctest: +SKIP\n\n    Which is equivalent with:\n\n    >>> fs = SubTreeFileSystem(\"/directory/prefix\",\n    ...                        LocalFileSystem()) # doctest: +SKIP\n    >>> ds.dataset(paths, filesystem=fs, format='parquet') # doctest: +SKIP\n\n    With a remote filesystem URI:\n\n    >>> paths = [\n    ...     'nested/directory/part0/data.parquet',\n    ...     'nested/directory/part1/data.parquet',\n    ...     'nested/directory/part3/data.parquet',\n    ... ]\n    >>> ds.dataset(paths, filesystem='s3://bucket/',\n    ...            format='parquet') # doctest: +SKIP\n\n    Similarly to the local example, the directory prefix may be included in the\n    filesystem URI:\n\n    >>> ds.dataset(paths, filesystem='s3://bucket/nested/directory',\n    ...         format='parquet') # doctest: +SKIP\n\n    Construction of a nested dataset:\n\n    >>> ds.dataset([\n    ...     dataset(\"s3://old-taxi-data\", format=\"parquet\"),\n    ...     dataset(\"local/path/to/data\", format=\"ipc\")\n    ... ]) # doctest: +SKIP\n    \"\"\"\n    from pyarrow.fs import FileInfo\n    # collect the keyword arguments for later reuse\n    kwargs = dict(\n        schema=schema,\n        filesystem=filesystem,\n        partitioning=partitioning,\n        format=format,\n        partition_base_dir=partition_base_dir,\n        exclude_invalid_files=exclude_invalid_files,\n        selector_ignore_prefixes=ignore_prefixes\n    )\n\n    if _is_path_like(source):\n        return _filesystem_dataset(source, **kwargs)\n    elif isinstance(source, (tuple, list)):\n        if all(_is_path_like(elem) or isinstance(elem, FileInfo) for elem in source):\n            return _filesystem_dataset(source, **kwargs)\n        elif all(isinstance(elem, Dataset) for elem in source):\n            return _union_dataset(source, **kwargs)\n        elif all(isinstance(elem, (pa.RecordBatch, pa.Table))\n                 for elem in source):\n            return _in_memory_dataset(source, **kwargs)\n        else:\n            unique_types = set(type(elem).__name__ for elem in source)\n            type_names = ', '.join('{}'.format(t) for t in unique_types)\n            raise TypeError(\n                'Expected a list of path-like or dataset objects, or a list '\n                'of batches or tables. The given list contains the following '\n                'types: {}'.format(type_names)\n            )\n    elif isinstance(source, (pa.RecordBatch, pa.Table)):\n        return _in_memory_dataset(source, **kwargs)\n    else:\n        raise TypeError(\n            'Expected a path-like, list of path-likes or a list of Datasets '\n            'instead of the given type: {}'.format(type(source).__name__)\n        )\n\n\ndef _ensure_write_partitioning(part, schema, flavor):\n    if isinstance(part, PartitioningFactory):\n        raise ValueError(\"A PartitioningFactory cannot be used. \"\n                         \"Did you call the partitioning function \"\n                         \"without supplying a schema?\")\n\n    if isinstance(part, Partitioning) and flavor:\n        raise ValueError(\n            \"Providing a partitioning_flavor with \"\n            \"a Partitioning object is not supported\"\n        )\n    elif isinstance(part, (tuple, list)):\n        # Name of fields were provided instead of a partitioning object.\n        # Create a partitioning factory with those field names.\n        part = partitioning(\n            schema=pa.schema([schema.field(f) for f in part]),\n            flavor=flavor\n        )\n    elif part is None:\n        part = partitioning(pa.schema([]), flavor=flavor)\n\n    if not isinstance(part, Partitioning):\n        raise ValueError(\n            \"partitioning must be a Partitioning object or \"\n            \"a list of column names\"\n        )\n\n    return part\n\n\ndef write_dataset(data, base_dir, *, basename_template=None, format=None,\n                  partitioning=None, partitioning_flavor=None, schema=None,\n                  filesystem=None, file_options=None, use_threads=True,\n                  max_partitions=None, max_open_files=None,\n                  max_rows_per_file=None, min_rows_per_group=None,\n                  max_rows_per_group=None, file_visitor=None,\n                  existing_data_behavior='error', create_dir=True):\n    \"\"\"\n    Write a dataset to a given format and partitioning.\n\n    Parameters\n    ----------\n    data : Dataset, Table/RecordBatch, RecordBatchReader, list of \\\nTable/RecordBatch, or iterable of RecordBatch\n        The data to write. This can be a Dataset instance or\n        in-memory Arrow data. If an iterable is given, the schema must\n        also be given.\n    base_dir : str\n        The root directory where to write the dataset.\n    basename_template : str, optional\n        A template string used to generate basenames of written data files.\n        The token '{i}' will be replaced with an automatically incremented\n        integer. If not specified, it defaults to\n        \"part-{i}.\" + format.default_extname\n    format : FileFormat or str\n        The format in which to write the dataset. Currently supported:\n        \"parquet\", \"ipc\"/\"arrow\"/\"feather\", and \"csv\". If a FileSystemDataset\n        is being written and `format` is not specified, it defaults to the\n        same format as the specified FileSystemDataset. When writing a\n        Table or RecordBatch, this keyword is required.\n    partitioning : Partitioning or list[str], optional\n        The partitioning scheme specified with the ``partitioning()``\n        function or a list of field names. When providing a list of\n        field names, you can use ``partitioning_flavor`` to drive which\n        partitioning type should be used.\n    partitioning_flavor : str, optional\n        One of the partitioning flavors supported by\n        ``pyarrow.dataset.partitioning``. If omitted will use the\n        default of ``partitioning()`` which is directory partitioning.\n    schema : Schema, optional\n    filesystem : FileSystem, optional\n    file_options : pyarrow.dataset.FileWriteOptions, optional\n        FileFormat specific write options, created using the\n        ``FileFormat.make_write_options()`` function.\n    use_threads : bool, default True\n        Write files in parallel. If enabled, then maximum parallelism will be\n        used determined by the number of available CPU cores.\n    max_partitions : int, default 1024\n        Maximum number of partitions any batch may be written into.\n    max_open_files : int, default 1024\n        If greater than 0 then this will limit the maximum number of\n        files that can be left open. If an attempt is made to open\n        too many files then the least recently used file will be closed.\n        If this setting is set too low you may end up fragmenting your\n        data into many small files.\n    max_rows_per_file : int, default 0\n        Maximum number of rows per file. If greater than 0 then this will\n        limit how many rows are placed in any single file. Otherwise there\n        will be no limit and one file will be created in each output\n        directory unless files need to be closed to respect max_open_files\n    min_rows_per_group : int, default 0\n        Minimum number of rows per group. When the value is greater than 0,\n        the dataset writer will batch incoming data and only write the row\n        groups to the disk when sufficient rows have accumulated.\n    max_rows_per_group : int, default 1024 * 1024\n        Maximum number of rows per group. If the value is greater than 0,\n        then the dataset writer may split up large incoming batches into\n        multiple row groups.  If this value is set, then min_rows_per_group\n        should also be set. Otherwise it could end up with very small row\n        groups.\n    file_visitor : function\n        If set, this function will be called with a WrittenFile instance\n        for each file created during the call.  This object will have both\n        a path attribute and a metadata attribute.\n\n        The path attribute will be a string containing the path to\n        the created file.\n\n        The metadata attribute will be the parquet metadata of the file.\n        This metadata will have the file path attribute set and can be used\n        to build a _metadata file.  The metadata attribute will be None if\n        the format is not parquet.\n\n        Example visitor which simple collects the filenames created::\n\n            visited_paths = []\n\n            def file_visitor(written_file):\n                visited_paths.append(written_file.path)\n    existing_data_behavior : 'error' | 'overwrite_or_ignore' | \\\n'delete_matching'\n        Controls how the dataset will handle data that already exists in\n        the destination.  The default behavior ('error') is to raise an error\n        if any data exists in the destination.\n\n        'overwrite_or_ignore' will ignore any existing data and will\n        overwrite files with the same name as an output file.  Other\n        existing files will be ignored.  This behavior, in combination\n        with a unique basename_template for each write, will allow for\n        an append workflow.\n\n        'delete_matching' is useful when you are writing a partitioned\n        dataset.  The first time each partition directory is encountered\n        the entire directory will be deleted.  This allows you to overwrite\n        old partitions completely.\n    create_dir : bool, default True\n        If False, directories will not be created.  This can be useful for\n        filesystems that do not require directories.\n    \"\"\"\n    from pyarrow.fs import _resolve_filesystem_and_path\n\n    if isinstance(data, (list, tuple)):\n        schema = schema or data[0].schema\n        data = InMemoryDataset(data, schema=schema)\n    elif isinstance(data, (pa.RecordBatch, pa.Table)):\n        schema = schema or data.schema\n        data = InMemoryDataset(data, schema=schema)\n    elif isinstance(data, pa.ipc.RecordBatchReader) or _is_iterable(data):\n        data = Scanner.from_batches(data, schema=schema)\n        schema = None\n    elif not isinstance(data, (Dataset, Scanner)):\n        raise ValueError(\n            \"Only Dataset, Scanner, Table/RecordBatch, RecordBatchReader, \"\n            \"a list of Tables/RecordBatches, or iterable of batches are \"\n            \"supported.\"\n        )\n\n    if format is None and isinstance(data, FileSystemDataset):\n        format = data.format\n    else:\n        format = _ensure_format(format)\n\n    if file_options is None:\n        file_options = format.make_write_options()\n\n    if format != file_options.format:\n        raise TypeError(\"Supplied FileWriteOptions have format {}, \"\n                        \"which doesn't match supplied FileFormat {}\".format(\n                            format, file_options))\n\n    if basename_template is None:\n        basename_template = \"part-{i}.\" + format.default_extname\n\n    if max_partitions is None:\n        max_partitions = 1024\n\n    if max_open_files is None:\n        max_open_files = 1024\n\n    if max_rows_per_file is None:\n        max_rows_per_file = 0\n\n    if max_rows_per_group is None:\n        max_rows_per_group = 1 << 20\n\n    if min_rows_per_group is None:\n        min_rows_per_group = 0\n\n    # at this point data is a Scanner or a Dataset, anything else\n    # was converted to one of those two. So we can grab the schema\n    # to build the partitioning object from Dataset.\n    if isinstance(data, Scanner):\n        partitioning_schema = data.projected_schema\n    else:\n        partitioning_schema = data.schema\n    partitioning = _ensure_write_partitioning(partitioning,\n                                              schema=partitioning_schema,\n                                              flavor=partitioning_flavor)\n\n    filesystem, base_dir = _resolve_filesystem_and_path(base_dir, filesystem)\n\n    if isinstance(data, Dataset):\n        scanner = data.scanner(use_threads=use_threads)\n    else:\n        # scanner was passed directly by the user, in which case a schema\n        # cannot be passed\n        if schema is not None:\n            raise ValueError(\"Cannot specify a schema when writing a Scanner\")\n        scanner = data\n\n    _filesystemdataset_write(\n        scanner, base_dir, basename_template, filesystem, partitioning,\n        file_options, max_partitions, file_visitor, existing_data_behavior,\n        max_open_files, max_rows_per_file,\n        min_rows_per_group, max_rows_per_group, create_dir\n    )\n", "python/pyarrow/cuda.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# flake8: noqa\n\n\nfrom pyarrow._cuda import (Context, IpcMemHandle, CudaBuffer,\n                           HostBuffer, BufferReader, BufferWriter,\n                           new_host_buffer,\n                           serialize_record_batch, read_message,\n                           read_record_batch)\n", "python/pyarrow/ipc.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Arrow file and stream reader/writer classes, and other messaging tools\n\nimport os\n\nimport pyarrow as pa\n\nfrom pyarrow.lib import (IpcReadOptions, IpcWriteOptions, ReadStats, WriteStats,  # noqa\n                         Message, MessageReader,\n                         RecordBatchReader, _ReadPandasMixin,\n                         MetadataVersion,\n                         read_message, read_record_batch, read_schema,\n                         read_tensor, write_tensor,\n                         get_record_batch_size, get_tensor_size)\nimport pyarrow.lib as lib\n\n\nclass RecordBatchStreamReader(lib._RecordBatchStreamReader):\n    \"\"\"\n    Reader for the Arrow streaming binary format.\n\n    Parameters\n    ----------\n    source : bytes/buffer-like, pyarrow.NativeFile, or file-like Python object\n        Either an in-memory buffer, or a readable file object.\n        If you want to use memory map use MemoryMappedFile as source.\n    options : pyarrow.ipc.IpcReadOptions\n        Options for IPC deserialization.\n        If None, default values will be used.\n    memory_pool : MemoryPool, default None\n        If None, default memory pool is used.\n    \"\"\"\n\n    def __init__(self, source, *, options=None, memory_pool=None):\n        options = _ensure_default_ipc_read_options(options)\n        self._open(source, options=options, memory_pool=memory_pool)\n\n\n_ipc_writer_class_doc = \"\"\"\\\nParameters\n----------\nsink : str, pyarrow.NativeFile, or file-like Python object\n    Either a file path, or a writable file object.\nschema : pyarrow.Schema\n    The Arrow schema for data to be written to the file.\nuse_legacy_format : bool, default None\n    Deprecated in favor of setting options. Cannot be provided with\n    options.\n\n    If None, False will be used unless this default is overridden by\n    setting the environment variable ARROW_PRE_0_15_IPC_FORMAT=1\noptions : pyarrow.ipc.IpcWriteOptions\n    Options for IPC serialization.\n\n    If None, default values will be used: the legacy format will not\n    be used unless overridden by setting the environment variable\n    ARROW_PRE_0_15_IPC_FORMAT=1, and the V5 metadata version will be\n    used unless overridden by setting the environment variable\n    ARROW_PRE_1_0_METADATA_VERSION=1.\"\"\"\n\n\nclass RecordBatchStreamWriter(lib._RecordBatchStreamWriter):\n    __doc__ = \"\"\"Writer for the Arrow streaming binary format\n\n{}\"\"\".format(_ipc_writer_class_doc)\n\n    def __init__(self, sink, schema, *, use_legacy_format=None, options=None):\n        options = _get_legacy_format_default(use_legacy_format, options)\n        self._open(sink, schema, options=options)\n\n\nclass RecordBatchFileReader(lib._RecordBatchFileReader):\n    \"\"\"\n    Class for reading Arrow record batch data from the Arrow binary file format\n\n    Parameters\n    ----------\n    source : bytes/buffer-like, pyarrow.NativeFile, or file-like Python object\n        Either an in-memory buffer, or a readable file object.\n        If you want to use memory map use MemoryMappedFile as source.\n    footer_offset : int, default None\n        If the file is embedded in some larger file, this is the byte offset to\n        the very end of the file data\n    options : pyarrow.ipc.IpcReadOptions\n        Options for IPC serialization.\n        If None, default values will be used.\n    memory_pool : MemoryPool, default None\n        If None, default memory pool is used.\n    \"\"\"\n\n    def __init__(self, source, footer_offset=None, *, options=None,\n                 memory_pool=None):\n        options = _ensure_default_ipc_read_options(options)\n        self._open(source, footer_offset=footer_offset,\n                   options=options, memory_pool=memory_pool)\n\n\nclass RecordBatchFileWriter(lib._RecordBatchFileWriter):\n\n    __doc__ = \"\"\"Writer to create the Arrow binary file format\n\n{}\"\"\".format(_ipc_writer_class_doc)\n\n    def __init__(self, sink, schema, *, use_legacy_format=None, options=None):\n        options = _get_legacy_format_default(use_legacy_format, options)\n        self._open(sink, schema, options=options)\n\n\ndef _get_legacy_format_default(use_legacy_format, options):\n    if use_legacy_format is not None and options is not None:\n        raise ValueError(\n            \"Can provide at most one of options and use_legacy_format\")\n    elif options:\n        if not isinstance(options, IpcWriteOptions):\n            raise TypeError(\"expected IpcWriteOptions, got {}\"\n                            .format(type(options)))\n        return options\n\n    metadata_version = MetadataVersion.V5\n    if use_legacy_format is None:\n        use_legacy_format = \\\n            bool(int(os.environ.get('ARROW_PRE_0_15_IPC_FORMAT', '0')))\n    if bool(int(os.environ.get('ARROW_PRE_1_0_METADATA_VERSION', '0'))):\n        metadata_version = MetadataVersion.V4\n    return IpcWriteOptions(use_legacy_format=use_legacy_format,\n                           metadata_version=metadata_version)\n\n\ndef _ensure_default_ipc_read_options(options):\n    if options and not isinstance(options, IpcReadOptions):\n        raise TypeError(\n            \"expected IpcReadOptions, got {}\".format(type(options))\n        )\n    return options or IpcReadOptions()\n\n\ndef new_stream(sink, schema, *, use_legacy_format=None, options=None):\n    return RecordBatchStreamWriter(sink, schema,\n                                   use_legacy_format=use_legacy_format,\n                                   options=options)\n\n\nnew_stream.__doc__ = \"\"\"\\\nCreate an Arrow columnar IPC stream writer instance\n\n{}\n\nReturns\n-------\nwriter : RecordBatchStreamWriter\n    A writer for the given sink\n\"\"\".format(_ipc_writer_class_doc)\n\n\ndef open_stream(source, *, options=None, memory_pool=None):\n    \"\"\"\n    Create reader for Arrow streaming format.\n\n    Parameters\n    ----------\n    source : bytes/buffer-like, pyarrow.NativeFile, or file-like Python object\n        Either an in-memory buffer, or a readable file object.\n    options : pyarrow.ipc.IpcReadOptions\n        Options for IPC serialization.\n        If None, default values will be used.\n    memory_pool : MemoryPool, default None\n        If None, default memory pool is used.\n\n    Returns\n    -------\n    reader : RecordBatchStreamReader\n        A reader for the given source\n    \"\"\"\n    return RecordBatchStreamReader(source, options=options,\n                                   memory_pool=memory_pool)\n\n\ndef new_file(sink, schema, *, use_legacy_format=None, options=None):\n    return RecordBatchFileWriter(sink, schema,\n                                 use_legacy_format=use_legacy_format,\n                                 options=options)\n\n\nnew_file.__doc__ = \"\"\"\\\nCreate an Arrow columnar IPC file writer instance\n\n{}\n\nReturns\n-------\nwriter : RecordBatchFileWriter\n    A writer for the given sink\n\"\"\".format(_ipc_writer_class_doc)\n\n\ndef open_file(source, footer_offset=None, *, options=None, memory_pool=None):\n    \"\"\"\n    Create reader for Arrow file format.\n\n    Parameters\n    ----------\n    source : bytes/buffer-like, pyarrow.NativeFile, or file-like Python object\n        Either an in-memory buffer, or a readable file object.\n    footer_offset : int, default None\n        If the file is embedded in some larger file, this is the byte offset to\n        the very end of the file data.\n    options : pyarrow.ipc.IpcReadOptions\n        Options for IPC serialization.\n        If None, default values will be used.\n    memory_pool : MemoryPool, default None\n        If None, default memory pool is used.\n\n    Returns\n    -------\n    reader : RecordBatchFileReader\n        A reader for the given source\n    \"\"\"\n    return RecordBatchFileReader(\n        source, footer_offset=footer_offset,\n        options=options, memory_pool=memory_pool)\n\n\ndef serialize_pandas(df, *, nthreads=None, preserve_index=None):\n    \"\"\"\n    Serialize a pandas DataFrame into a buffer protocol compatible object.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n    nthreads : int, default None\n        Number of threads to use for conversion to Arrow, default all CPUs.\n    preserve_index : bool, default None\n        The default of None will store the index as a column, except for\n        RangeIndex which is stored as metadata only. If True, always\n        preserve the pandas index data as a column. If False, no index\n        information is saved and the result will have a default RangeIndex.\n\n    Returns\n    -------\n    buf : buffer\n        An object compatible with the buffer protocol.\n    \"\"\"\n    batch = pa.RecordBatch.from_pandas(df, nthreads=nthreads,\n                                       preserve_index=preserve_index)\n    sink = pa.BufferOutputStream()\n    with pa.RecordBatchStreamWriter(sink, batch.schema) as writer:\n        writer.write_batch(batch)\n    return sink.getvalue()\n\n\ndef deserialize_pandas(buf, *, use_threads=True):\n    \"\"\"Deserialize a buffer protocol compatible object into a pandas DataFrame.\n\n    Parameters\n    ----------\n    buf : buffer\n        An object compatible with the buffer protocol.\n    use_threads : bool, default True\n        Whether to parallelize the conversion using multiple threads.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The buffer deserialized as pandas DataFrame\n    \"\"\"\n    buffer_reader = pa.BufferReader(buf)\n    with pa.RecordBatchStreamReader(buffer_reader) as reader:\n        table = reader.read_all()\n    return table.to_pandas(use_threads=use_threads)\n", "python/pyarrow/_compute_docstrings.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"\nCustom documentation additions for compute functions.\n\"\"\"\n\nfunction_doc_additions = {}\n\nfunction_doc_additions[\"filter\"] = \"\"\"\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> arr = pa.array([\"a\", \"b\", \"c\", None, \"e\"])\n    >>> mask = pa.array([True, False, None, False, True])\n    >>> arr.filter(mask)\n    <pyarrow.lib.StringArray object at ...>\n    [\n      \"a\",\n      \"e\"\n    ]\n    >>> arr.filter(mask, null_selection_behavior='emit_null')\n    <pyarrow.lib.StringArray object at ...>\n    [\n      \"a\",\n      null,\n      \"e\"\n    ]\n    \"\"\"\n\nfunction_doc_additions[\"mode\"] = \"\"\"\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> import pyarrow.compute as pc\n    >>> arr = pa.array([1, 1, 2, 2, 3, 2, 2, 2])\n    >>> modes = pc.mode(arr, 2)\n    >>> modes[0]\n    <pyarrow.StructScalar: [('mode', 2), ('count', 5)]>\n    >>> modes[1]\n    <pyarrow.StructScalar: [('mode', 1), ('count', 2)]>\n    \"\"\"\n", "python/pyarrow/orc.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nfrom numbers import Integral\nimport warnings\n\nfrom pyarrow.lib import Table\nimport pyarrow._orc as _orc\nfrom pyarrow.fs import _resolve_filesystem_and_path\n\n\nclass ORCFile:\n    \"\"\"\n    Reader interface for a single ORC file\n\n    Parameters\n    ----------\n    source : str or pyarrow.NativeFile\n        Readable source. For passing Python file objects or byte buffers,\n        see pyarrow.io.PythonFileInterface or pyarrow.io.BufferReader.\n    \"\"\"\n\n    def __init__(self, source):\n        self.reader = _orc.ORCReader()\n        self.reader.open(source)\n\n    @property\n    def metadata(self):\n        \"\"\"The file metadata, as an arrow KeyValueMetadata\"\"\"\n        return self.reader.metadata()\n\n    @property\n    def schema(self):\n        \"\"\"The file schema, as an arrow schema\"\"\"\n        return self.reader.schema()\n\n    @property\n    def nrows(self):\n        \"\"\"The number of rows in the file\"\"\"\n        return self.reader.nrows()\n\n    @property\n    def nstripes(self):\n        \"\"\"The number of stripes in the file\"\"\"\n        return self.reader.nstripes()\n\n    @property\n    def file_version(self):\n        \"\"\"Format version of the ORC file, must be 0.11 or 0.12\"\"\"\n        return self.reader.file_version()\n\n    @property\n    def software_version(self):\n        \"\"\"Software instance and version that wrote this file\"\"\"\n        return self.reader.software_version()\n\n    @property\n    def compression(self):\n        \"\"\"Compression codec of the file\"\"\"\n        return self.reader.compression()\n\n    @property\n    def compression_size(self):\n        \"\"\"Number of bytes to buffer for the compression codec in the file\"\"\"\n        return self.reader.compression_size()\n\n    @property\n    def writer(self):\n        \"\"\"Name of the writer that wrote this file.\n        If the writer is unknown then its Writer ID\n        (a number) is returned\"\"\"\n        return self.reader.writer()\n\n    @property\n    def writer_version(self):\n        \"\"\"Version of the writer\"\"\"\n        return self.reader.writer_version()\n\n    @property\n    def row_index_stride(self):\n        \"\"\"Number of rows per an entry in the row index or 0\n        if there is no row index\"\"\"\n        return self.reader.row_index_stride()\n\n    @property\n    def nstripe_statistics(self):\n        \"\"\"Number of stripe statistics\"\"\"\n        return self.reader.nstripe_statistics()\n\n    @property\n    def content_length(self):\n        \"\"\"Length of the data stripes in the file in bytes\"\"\"\n        return self.reader.content_length()\n\n    @property\n    def stripe_statistics_length(self):\n        \"\"\"The number of compressed bytes in the file stripe statistics\"\"\"\n        return self.reader.stripe_statistics_length()\n\n    @property\n    def file_footer_length(self):\n        \"\"\"The number of compressed bytes in the file footer\"\"\"\n        return self.reader.file_footer_length()\n\n    @property\n    def file_postscript_length(self):\n        \"\"\"The number of bytes in the file postscript\"\"\"\n        return self.reader.file_postscript_length()\n\n    @property\n    def file_length(self):\n        \"\"\"The number of bytes in the file\"\"\"\n        return self.reader.file_length()\n\n    def _select_names(self, columns=None):\n        if columns is None:\n            return None\n\n        schema = self.schema\n        names = []\n        for col in columns:\n            if isinstance(col, Integral):\n                col = int(col)\n                if 0 <= col < len(schema):\n                    col = schema[col].name\n                    names.append(col)\n                else:\n                    raise ValueError(\"Column indices must be in 0 <= ind < %d,\"\n                                     \" got %d\" % (len(schema), col))\n            else:\n                return columns\n\n        return names\n\n    def read_stripe(self, n, columns=None):\n        \"\"\"Read a single stripe from the file.\n\n        Parameters\n        ----------\n        n : int\n            The stripe index\n        columns : list\n            If not None, only these columns will be read from the stripe. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'\n\n        Returns\n        -------\n        pyarrow.RecordBatch\n            Content of the stripe as a RecordBatch.\n        \"\"\"\n        columns = self._select_names(columns)\n        return self.reader.read_stripe(n, columns=columns)\n\n    def read(self, columns=None):\n        \"\"\"Read the whole file.\n\n        Parameters\n        ----------\n        columns : list\n            If not None, only these columns will be read from the file. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'. Output always follows the\n            ordering of the file and not the `columns` list.\n\n        Returns\n        -------\n        pyarrow.Table\n            Content of the file as a Table.\n        \"\"\"\n        columns = self._select_names(columns)\n        return self.reader.read(columns=columns)\n\n\n_orc_writer_args_docs = \"\"\"file_version : {\"0.11\", \"0.12\"}, default \"0.12\"\n    Determine which ORC file version to use.\n    `Hive 0.11 / ORC v0 <https://orc.apache.org/specification/ORCv0/>`_\n    is the older version\n    while `Hive 0.12 / ORC v1 <https://orc.apache.org/specification/ORCv1/>`_\n    is the newer one.\nbatch_size : int, default 1024\n    Number of rows the ORC writer writes at a time.\nstripe_size : int, default 64 * 1024 * 1024\n    Size of each ORC stripe in bytes.\ncompression : string, default 'uncompressed'\n    The compression codec.\n    Valid values: {'UNCOMPRESSED', 'SNAPPY', 'ZLIB', 'LZ4', 'ZSTD'}\n    Note that LZ0 is currently not supported.\ncompression_block_size : int, default 64 * 1024\n    Size of each compression block in bytes.\ncompression_strategy : string, default 'speed'\n    The compression strategy i.e. speed vs size reduction.\n    Valid values: {'SPEED', 'COMPRESSION'}\nrow_index_stride : int, default 10000\n    The row index stride i.e. the number of rows per\n    an entry in the row index.\npadding_tolerance : double, default 0.0\n    The padding tolerance.\ndictionary_key_size_threshold : double, default 0.0\n    The dictionary key size threshold. 0 to disable dictionary encoding.\n    1 to always enable dictionary encoding.\nbloom_filter_columns : None, set-like or list-like, default None\n    Columns that use the bloom filter.\nbloom_filter_fpp : double, default 0.05\n    Upper limit of the false-positive rate of the bloom filter.\n\"\"\"\n\n\nclass ORCWriter:\n    __doc__ = \"\"\"\nWriter interface for a single ORC file\n\nParameters\n----------\nwhere : str or pyarrow.io.NativeFile\n    Writable target. For passing Python file objects or byte buffers,\n    see pyarrow.io.PythonFileInterface, pyarrow.io.BufferOutputStream\n    or pyarrow.io.FixedSizeBufferWriter.\n{}\n\"\"\".format(_orc_writer_args_docs)\n\n    is_open = False\n\n    def __init__(self, where, *,\n                 file_version='0.12',\n                 batch_size=1024,\n                 stripe_size=64 * 1024 * 1024,\n                 compression='uncompressed',\n                 compression_block_size=65536,\n                 compression_strategy='speed',\n                 row_index_stride=10000,\n                 padding_tolerance=0.0,\n                 dictionary_key_size_threshold=0.0,\n                 bloom_filter_columns=None,\n                 bloom_filter_fpp=0.05,\n                 ):\n        self.writer = _orc.ORCWriter()\n        self.writer.open(\n            where,\n            file_version=file_version,\n            batch_size=batch_size,\n            stripe_size=stripe_size,\n            compression=compression,\n            compression_block_size=compression_block_size,\n            compression_strategy=compression_strategy,\n            row_index_stride=row_index_stride,\n            padding_tolerance=padding_tolerance,\n            dictionary_key_size_threshold=dictionary_key_size_threshold,\n            bloom_filter_columns=bloom_filter_columns,\n            bloom_filter_fpp=bloom_filter_fpp\n        )\n        self.is_open = True\n\n    def __del__(self):\n        self.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n    def write(self, table):\n        \"\"\"\n        Write the table into an ORC file. The schema of the table must\n        be equal to the schema used when opening the ORC file.\n\n        Parameters\n        ----------\n        table : pyarrow.Table\n            The table to be written into the ORC file\n        \"\"\"\n        assert self.is_open\n        self.writer.write(table)\n\n    def close(self):\n        \"\"\"\n        Close the ORC file\n        \"\"\"\n        if self.is_open:\n            self.writer.close()\n            self.is_open = False\n\n\ndef read_table(source, columns=None, filesystem=None):\n    filesystem, path = _resolve_filesystem_and_path(source, filesystem)\n    if filesystem is not None:\n        source = filesystem.open_input_file(path)\n\n    if columns is not None and len(columns) == 0:\n        result = ORCFile(source).read().select(columns)\n    else:\n        result = ORCFile(source).read(columns=columns)\n\n    return result\n\n\nread_table.__doc__ = \"\"\"\nRead a Table from an ORC file.\n\nParameters\n----------\nsource : str, pyarrow.NativeFile, or file-like object\n    If a string passed, can be a single file name. For file-like objects,\n    only read a single file. Use pyarrow.BufferReader to read a file\n    contained in a bytes or buffer-like object.\ncolumns : list\n    If not None, only these columns will be read from the file. A column\n    name may be a prefix of a nested field, e.g. 'a' will select 'a.b',\n    'a.c', and 'a.d.e'. Output always follows the ordering of the file and\n    not the `columns` list. If empty, no columns will be read. Note\n    that the table will still have the correct num_rows set despite having\n    no columns.\nfilesystem : FileSystem, default None\n    If nothing passed, will be inferred based on path.\n    Path will try to be found in the local on-disk filesystem otherwise\n    it will be parsed as an URI to determine the filesystem.\n\"\"\"\n\n\ndef write_table(table, where, *,\n                file_version='0.12',\n                batch_size=1024,\n                stripe_size=64 * 1024 * 1024,\n                compression='uncompressed',\n                compression_block_size=65536,\n                compression_strategy='speed',\n                row_index_stride=10000,\n                padding_tolerance=0.0,\n                dictionary_key_size_threshold=0.0,\n                bloom_filter_columns=None,\n                bloom_filter_fpp=0.05):\n    if isinstance(where, Table):\n        warnings.warn(\n            \"The order of the arguments has changed. Pass as \"\n            \"'write_table(table, where)' instead. The old order will raise \"\n            \"an error in the future.\", FutureWarning, stacklevel=2\n        )\n        table, where = where, table\n    with ORCWriter(\n        where,\n        file_version=file_version,\n        batch_size=batch_size,\n        stripe_size=stripe_size,\n        compression=compression,\n        compression_block_size=compression_block_size,\n        compression_strategy=compression_strategy,\n        row_index_stride=row_index_stride,\n        padding_tolerance=padding_tolerance,\n        dictionary_key_size_threshold=dictionary_key_size_threshold,\n        bloom_filter_columns=bloom_filter_columns,\n        bloom_filter_fpp=bloom_filter_fpp\n    ) as writer:\n        writer.write(table)\n\n\nwrite_table.__doc__ = \"\"\"\nWrite a table into an ORC file.\n\nParameters\n----------\ntable : pyarrow.lib.Table\n    The table to be written into the ORC file\nwhere : str or pyarrow.io.NativeFile\n    Writable target. For passing Python file objects or byte buffers,\n    see pyarrow.io.PythonFileInterface, pyarrow.io.BufferOutputStream\n    or pyarrow.io.FixedSizeBufferWriter.\n{}\n\"\"\".format(_orc_writer_args_docs)\n", "python/pyarrow/benchmark.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# flake8: noqa\n\n\nfrom pyarrow.lib import benchmark_PandasObjectIsNull\n", "python/pyarrow/flight.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\ntry:\n    from pyarrow._flight import (  # noqa:F401\n        connect,\n        Action,\n        ActionType,\n        BasicAuth,\n        CallInfo,\n        CertKeyPair,\n        ClientAuthHandler,\n        ClientMiddleware,\n        ClientMiddlewareFactory,\n        DescriptorType,\n        FlightCallOptions,\n        FlightCancelledError,\n        FlightClient,\n        FlightDataStream,\n        FlightDescriptor,\n        FlightEndpoint,\n        FlightError,\n        FlightInfo,\n        FlightInternalError,\n        FlightMetadataReader,\n        FlightMetadataWriter,\n        FlightMethod,\n        FlightServerBase,\n        FlightServerError,\n        FlightStreamChunk,\n        FlightStreamReader,\n        FlightStreamWriter,\n        FlightTimedOutError,\n        FlightUnauthenticatedError,\n        FlightUnauthorizedError,\n        FlightUnavailableError,\n        FlightWriteSizeExceededError,\n        GeneratorStream,\n        Location,\n        MetadataRecordBatchReader,\n        MetadataRecordBatchWriter,\n        RecordBatchStream,\n        Result,\n        SchemaResult,\n        ServerAuthHandler,\n        ServerCallContext,\n        ServerMiddleware,\n        ServerMiddlewareFactory,\n        Ticket,\n        TracingServerMiddlewareFactory,\n    )\nexcept ImportError as exc:\n    raise ImportError(\n        f\"The pyarrow installation is not built with support for 'flight' ({str(exc)})\"\n    ) from None\n", "python/pyarrow/feather.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nimport os\n\nfrom pyarrow.pandas_compat import _pandas_api  # noqa\nfrom pyarrow.lib import (Codec, Table,  # noqa\n                         concat_tables, schema)\nimport pyarrow.lib as ext\nfrom pyarrow import _feather\nfrom pyarrow._feather import FeatherError  # noqa: F401\n\n\nclass FeatherDataset:\n    \"\"\"\n    Encapsulates details of reading a list of Feather files.\n\n    Parameters\n    ----------\n    path_or_paths : List[str]\n        A list of file names\n    validate_schema : bool, default True\n        Check that individual file schemas are all the same / compatible\n    \"\"\"\n\n    def __init__(self, path_or_paths, validate_schema=True):\n        self.paths = path_or_paths\n        self.validate_schema = validate_schema\n\n    def read_table(self, columns=None):\n        \"\"\"\n        Read multiple feather files as a single pyarrow.Table\n\n        Parameters\n        ----------\n        columns : List[str]\n            Names of columns to read from the file\n\n        Returns\n        -------\n        pyarrow.Table\n            Content of the file as a table (of columns)\n        \"\"\"\n        _fil = read_table(self.paths[0], columns=columns)\n        self._tables = [_fil]\n        self.schema = _fil.schema\n\n        for path in self.paths[1:]:\n            table = read_table(path, columns=columns)\n            if self.validate_schema:\n                self.validate_schemas(path, table)\n            self._tables.append(table)\n        return concat_tables(self._tables)\n\n    def validate_schemas(self, piece, table):\n        if not self.schema.equals(table.schema):\n            raise ValueError('Schema in {!s} was different. \\n'\n                             '{!s}\\n\\nvs\\n\\n{!s}'\n                             .format(piece, self.schema,\n                                     table.schema))\n\n    def read_pandas(self, columns=None, use_threads=True):\n        \"\"\"\n        Read multiple Parquet files as a single pandas DataFrame\n\n        Parameters\n        ----------\n        columns : List[str]\n            Names of columns to read from the file\n        use_threads : bool, default True\n            Use multiple threads when converting to pandas\n\n        Returns\n        -------\n        pandas.DataFrame\n            Content of the file as a pandas DataFrame (of columns)\n        \"\"\"\n        return self.read_table(columns=columns).to_pandas(\n            use_threads=use_threads)\n\n\ndef check_chunked_overflow(name, col):\n    if col.num_chunks == 1:\n        return\n\n    if col.type in (ext.binary(), ext.string()):\n        raise ValueError(\"Column '{}' exceeds 2GB maximum capacity of \"\n                         \"a Feather binary column. This restriction may be \"\n                         \"lifted in the future\".format(name))\n    else:\n        # TODO(wesm): Not sure when else this might be reached\n        raise ValueError(\"Column '{}' of type {} was chunked on conversion \"\n                         \"to Arrow and cannot be currently written to \"\n                         \"Feather format\".format(name, str(col.type)))\n\n\n_FEATHER_SUPPORTED_CODECS = {'lz4', 'zstd', 'uncompressed'}\n\n\ndef write_feather(df, dest, compression=None, compression_level=None,\n                  chunksize=None, version=2):\n    \"\"\"\n    Write a pandas.DataFrame to Feather format.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pyarrow.Table\n        Data to write out as Feather format.\n    dest : str\n        Local destination path.\n    compression : string, default None\n        Can be one of {\"zstd\", \"lz4\", \"uncompressed\"}. The default of None uses\n        LZ4 for V2 files if it is available, otherwise uncompressed.\n    compression_level : int, default None\n        Use a compression level particular to the chosen compressor. If None\n        use the default compression level\n    chunksize : int, default None\n        For V2 files, the internal maximum size of Arrow RecordBatch chunks\n        when writing the Arrow IPC file format. None means use the default,\n        which is currently 64K\n    version : int, default 2\n        Feather file version. Version 2 is the current. Version 1 is the more\n        limited legacy format\n    \"\"\"\n    if _pandas_api.have_pandas:\n        if (_pandas_api.has_sparse and\n                isinstance(df, _pandas_api.pd.SparseDataFrame)):\n            df = df.to_dense()\n\n    if _pandas_api.is_data_frame(df):\n        # Feather v1 creates a new column in the resultant Table to\n        # store index information if index type is not RangeIndex\n\n        if version == 1:\n            preserve_index = False\n        elif version == 2:\n            preserve_index = None\n        else:\n            raise ValueError(\"Version value should either be 1 or 2\")\n\n        table = Table.from_pandas(df, preserve_index=preserve_index)\n\n        if version == 1:\n            # Version 1 does not chunking\n            for i, name in enumerate(table.schema.names):\n                col = table[i]\n                check_chunked_overflow(name, col)\n    else:\n        table = df\n\n    if version == 1:\n        if len(table.column_names) > len(set(table.column_names)):\n            raise ValueError(\"cannot serialize duplicate column names\")\n\n        if compression is not None:\n            raise ValueError(\"Feather V1 files do not support compression \"\n                             \"option\")\n\n        if chunksize is not None:\n            raise ValueError(\"Feather V1 files do not support chunksize \"\n                             \"option\")\n    else:\n        if compression is None and Codec.is_available('lz4_frame'):\n            compression = 'lz4'\n        elif (compression is not None and\n              compression not in _FEATHER_SUPPORTED_CODECS):\n            raise ValueError('compression=\"{}\" not supported, must be '\n                             'one of {}'.format(compression,\n                                                _FEATHER_SUPPORTED_CODECS))\n\n    try:\n        _feather.write_feather(table, dest, compression=compression,\n                               compression_level=compression_level,\n                               chunksize=chunksize, version=version)\n    except Exception:\n        if isinstance(dest, str):\n            try:\n                os.remove(dest)\n            except os.error:\n                pass\n        raise\n\n\ndef read_feather(source, columns=None, use_threads=True,\n                 memory_map=False, **kwargs):\n    \"\"\"\n    Read a pandas.DataFrame from Feather format. To read as pyarrow.Table use\n    feather.read_table.\n\n    Parameters\n    ----------\n    source : str file path, or file-like object\n        You can use MemoryMappedFile as source, for explicitly use memory map.\n    columns : sequence, optional\n        Only read a specific set of columns. If not provided, all columns are\n        read.\n    use_threads : bool, default True\n        Whether to parallelize reading using multiple threads. If false the\n        restriction is used in the conversion to Pandas as well as in the\n        reading from Feather format.\n    memory_map : boolean, default False\n        Use memory mapping when opening file on disk, when source is a str.\n    **kwargs\n        Additional keyword arguments passed on to `pyarrow.Table.to_pandas`.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The contents of the Feather file as a pandas.DataFrame\n    \"\"\"\n    return (read_table(\n        source, columns=columns, memory_map=memory_map,\n        use_threads=use_threads).to_pandas(use_threads=use_threads, **kwargs))\n\n\ndef read_table(source, columns=None, memory_map=False, use_threads=True):\n    \"\"\"\n    Read a pyarrow.Table from Feather format\n\n    Parameters\n    ----------\n    source : str file path, or file-like object\n        You can use MemoryMappedFile as source, for explicitly use memory map.\n    columns : sequence, optional\n        Only read a specific set of columns. If not provided, all columns are\n        read.\n    memory_map : boolean, default False\n        Use memory mapping when opening file on disk, when source is a str\n    use_threads : bool, default True\n        Whether to parallelize reading using multiple threads.\n\n    Returns\n    -------\n    table : pyarrow.Table\n        The contents of the Feather file as a pyarrow.Table\n    \"\"\"\n    reader = _feather.FeatherReader(\n        source, use_memory_map=memory_map, use_threads=use_threads)\n\n    if columns is None:\n        return reader.read()\n\n    column_types = [type(column) for column in columns]\n    if all(map(lambda t: t == int, column_types)):\n        table = reader.read_indices(columns)\n    elif all(map(lambda t: t == str, column_types)):\n        table = reader.read_names(columns)\n    else:\n        column_type_names = [t.__name__ for t in column_types]\n        raise TypeError(\"Columns must be indices or names. \"\n                        \"Got columns {} of types {}\"\n                        .format(columns, column_type_names))\n\n    # Feather v1 already respects the column selection\n    if reader.version < 3:\n        return table\n    # Feather v2 reads with sorted / deduplicated selection\n    elif sorted(set(columns)) == columns:\n        return table\n    else:\n        # follow exact order / selection of names\n        return table.select(columns)\n", "python/pyarrow/fs.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"\nFileSystem abstraction to interact with various local and remote filesystems.\n\"\"\"\n\nfrom pyarrow.util import _is_path_like, _stringify_path\n\nfrom pyarrow._fs import (  # noqa\n    FileSelector,\n    FileType,\n    FileInfo,\n    FileSystem,\n    LocalFileSystem,\n    SubTreeFileSystem,\n    _MockFileSystem,\n    FileSystemHandler,\n    PyFileSystem,\n    _copy_files,\n    _copy_files_selector,\n)\n\n# For backward compatibility.\nFileStats = FileInfo\n\n_not_imported = []\ntry:\n    from pyarrow._azurefs import AzureFileSystem  # noqa\nexcept ImportError:\n    _not_imported.append(\"AzureFileSystem\")\n\ntry:\n    from pyarrow._hdfs import HadoopFileSystem  # noqa\nexcept ImportError:\n    _not_imported.append(\"HadoopFileSystem\")\n\ntry:\n    from pyarrow._gcsfs import GcsFileSystem  # noqa\nexcept ImportError:\n    _not_imported.append(\"GcsFileSystem\")\n\ntry:\n    from pyarrow._s3fs import (  # noqa\n        AwsDefaultS3RetryStrategy, AwsStandardS3RetryStrategy,\n        S3FileSystem, S3LogLevel, S3RetryStrategy, ensure_s3_initialized,\n        finalize_s3, ensure_s3_finalized, initialize_s3, resolve_s3_region)\nexcept ImportError:\n    _not_imported.append(\"S3FileSystem\")\nelse:\n    # GH-38364: we don't initialize S3 eagerly as that could lead\n    # to crashes at shutdown even when S3 isn't used.\n    # Instead, S3 is initialized lazily using `ensure_s3_initialized`\n    # in assorted places.\n    import atexit\n    atexit.register(ensure_s3_finalized)\n\n\ndef __getattr__(name):\n    if name in _not_imported:\n        raise ImportError(\n            \"The pyarrow installation is not built with support for \"\n            \"'{0}'\".format(name)\n        )\n\n    raise AttributeError(\n        \"module 'pyarrow.fs' has no attribute '{0}'\".format(name)\n    )\n\n\ndef _filesystem_from_str(uri):\n    # instantiate the file system from an uri, if the uri has a path\n    # component then it will be treated as a path prefix\n    filesystem, prefix = FileSystem.from_uri(uri)\n    prefix = filesystem.normalize_path(prefix)\n    if prefix:\n        # validate that the prefix is pointing to a directory\n        prefix_info = filesystem.get_file_info([prefix])[0]\n        if prefix_info.type != FileType.Directory:\n            raise ValueError(\n                \"The path component of the filesystem URI must point to a \"\n                \"directory but it has a type: `{}`. The path component \"\n                \"is `{}` and the given filesystem URI is `{}`\".format(\n                    prefix_info.type.name, prefix_info.path, uri\n                )\n            )\n        filesystem = SubTreeFileSystem(prefix, filesystem)\n    return filesystem\n\n\ndef _ensure_filesystem(filesystem, *, use_mmap=False):\n    if isinstance(filesystem, FileSystem):\n        return filesystem\n    elif isinstance(filesystem, str):\n        if use_mmap:\n            raise ValueError(\n                \"Specifying to use memory mapping not supported for \"\n                \"filesystem specified as an URI string\"\n            )\n        return _filesystem_from_str(filesystem)\n\n    # handle fsspec-compatible filesystems\n    try:\n        import fsspec\n    except ImportError:\n        pass\n    else:\n        if isinstance(filesystem, fsspec.AbstractFileSystem):\n            if type(filesystem).__name__ == 'LocalFileSystem':\n                # In case its a simple LocalFileSystem, use native arrow one\n                return LocalFileSystem(use_mmap=use_mmap)\n            return PyFileSystem(FSSpecHandler(filesystem))\n\n    raise TypeError(\n        \"Unrecognized filesystem: {}. `filesystem` argument must be a \"\n        \"FileSystem instance or a valid file system URI'\".format(\n            type(filesystem))\n    )\n\n\ndef _resolve_filesystem_and_path(path, filesystem=None, *, memory_map=False):\n    \"\"\"\n    Return filesystem/path from path which could be an URI or a plain\n    filesystem path.\n    \"\"\"\n    if not _is_path_like(path):\n        if filesystem is not None:\n            raise ValueError(\n                \"'filesystem' passed but the specified path is file-like, so\"\n                \" there is nothing to open with 'filesystem'.\"\n            )\n        return filesystem, path\n\n    if filesystem is not None:\n        filesystem = _ensure_filesystem(filesystem, use_mmap=memory_map)\n        if isinstance(filesystem, LocalFileSystem):\n            path = _stringify_path(path)\n        elif not isinstance(path, str):\n            raise TypeError(\n                \"Expected string path; path-like objects are only allowed \"\n                \"with a local filesystem\"\n            )\n        path = filesystem.normalize_path(path)\n        return filesystem, path\n\n    path = _stringify_path(path)\n\n    # if filesystem is not given, try to automatically determine one\n    # first check if the file exists as a local (relative) file path\n    # if not then try to parse the path as an URI\n    filesystem = LocalFileSystem(use_mmap=memory_map)\n\n    try:\n        file_info = filesystem.get_file_info(path)\n    except ValueError:  # ValueError means path is likely an URI\n        file_info = None\n        exists_locally = False\n    else:\n        exists_locally = (file_info.type != FileType.NotFound)\n\n    # if the file or directory doesn't exists locally, then assume that\n    # the path is an URI describing the file system as well\n    if not exists_locally:\n        try:\n            filesystem, path = FileSystem.from_uri(path)\n        except ValueError as e:\n            # neither an URI nor a locally existing path, so assume that\n            # local path was given and propagate a nicer file not found error\n            # instead of a more confusing scheme parsing error\n            if \"empty scheme\" not in str(e) \\\n                    and \"Cannot parse URI\" not in str(e):\n                raise\n    else:\n        path = filesystem.normalize_path(path)\n\n    return filesystem, path\n\n\ndef copy_files(source, destination,\n               source_filesystem=None, destination_filesystem=None,\n               *, chunk_size=1024*1024, use_threads=True):\n    \"\"\"\n    Copy files between FileSystems.\n\n    This functions allows you to recursively copy directories of files from\n    one file system to another, such as from S3 to your local machine.\n\n    Parameters\n    ----------\n    source : string\n        Source file path or URI to a single file or directory.\n        If a directory, files will be copied recursively from this path.\n    destination : string\n        Destination file path or URI. If `source` is a file, `destination`\n        is also interpreted as the destination file (not directory).\n        Directories will be created as necessary.\n    source_filesystem : FileSystem, optional\n        Source filesystem, needs to be specified if `source` is not a URI,\n        otherwise inferred.\n    destination_filesystem : FileSystem, optional\n        Destination filesystem, needs to be specified if `destination` is not\n        a URI, otherwise inferred.\n    chunk_size : int, default 1MB\n        The maximum size of block to read before flushing to the\n        destination file. A larger chunk_size will use more memory while\n        copying but may help accommodate high latency FileSystems.\n    use_threads : bool, default True\n        Whether to use multiple threads to accelerate copying.\n\n    Examples\n    --------\n    Inspect an S3 bucket's files:\n\n    >>> s3, path = fs.FileSystem.from_uri(\n    ...            \"s3://registry.opendata.aws/roda/ndjson/\")\n    >>> selector = fs.FileSelector(path)\n    >>> s3.get_file_info(selector)\n    [<FileInfo for 'registry.opendata.aws/roda/ndjson/index.ndjson':...]\n\n    Copy one file from S3 bucket to a local directory:\n\n    >>> fs.copy_files(\"s3://registry.opendata.aws/roda/ndjson/index.ndjson\",\n    ...               \"file:///{}/index_copy.ndjson\".format(local_path))\n\n    >>> fs.LocalFileSystem().get_file_info(str(local_path)+\n    ...                                    '/index_copy.ndjson')\n    <FileInfo for '.../index_copy.ndjson': type=FileType.File, size=...>\n\n    Copy file using a FileSystem object:\n\n    >>> fs.copy_files(\"registry.opendata.aws/roda/ndjson/index.ndjson\",\n    ...               \"file:///{}/index_copy.ndjson\".format(local_path),\n    ...               source_filesystem=fs.S3FileSystem())\n    \"\"\"\n    source_fs, source_path = _resolve_filesystem_and_path(\n        source, source_filesystem\n    )\n    destination_fs, destination_path = _resolve_filesystem_and_path(\n        destination, destination_filesystem\n    )\n\n    file_info = source_fs.get_file_info(source_path)\n    if file_info.type == FileType.Directory:\n        source_sel = FileSelector(source_path, recursive=True)\n        _copy_files_selector(source_fs, source_sel,\n                             destination_fs, destination_path,\n                             chunk_size, use_threads)\n    else:\n        _copy_files(source_fs, source_path,\n                    destination_fs, destination_path,\n                    chunk_size, use_threads)\n\n\nclass FSSpecHandler(FileSystemHandler):\n    \"\"\"\n    Handler for fsspec-based Python filesystems.\n\n    https://filesystem-spec.readthedocs.io/en/latest/index.html\n\n    Parameters\n    ----------\n    fs : FSSpec-compliant filesystem instance\n\n    Examples\n    --------\n    >>> PyFileSystem(FSSpecHandler(fsspec_fs)) # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, fs):\n        self.fs = fs\n\n    def __eq__(self, other):\n        if isinstance(other, FSSpecHandler):\n            return self.fs == other.fs\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, FSSpecHandler):\n            return self.fs != other.fs\n        return NotImplemented\n\n    def get_type_name(self):\n        protocol = self.fs.protocol\n        if isinstance(protocol, list):\n            protocol = protocol[0]\n        return \"fsspec+{0}\".format(protocol)\n\n    def normalize_path(self, path):\n        return path\n\n    @staticmethod\n    def _create_file_info(path, info):\n        size = info[\"size\"]\n        if info[\"type\"] == \"file\":\n            ftype = FileType.File\n        elif info[\"type\"] == \"directory\":\n            ftype = FileType.Directory\n            # some fsspec filesystems include a file size for directories\n            size = None\n        else:\n            ftype = FileType.Unknown\n        return FileInfo(path, ftype, size=size, mtime=info.get(\"mtime\", None))\n\n    def get_file_info(self, paths):\n        infos = []\n        for path in paths:\n            try:\n                info = self.fs.info(path)\n            except FileNotFoundError:\n                infos.append(FileInfo(path, FileType.NotFound))\n            else:\n                infos.append(self._create_file_info(path, info))\n        return infos\n\n    def get_file_info_selector(self, selector):\n        if not self.fs.isdir(selector.base_dir):\n            if self.fs.exists(selector.base_dir):\n                raise NotADirectoryError(selector.base_dir)\n            else:\n                if selector.allow_not_found:\n                    return []\n                else:\n                    raise FileNotFoundError(selector.base_dir)\n\n        if selector.recursive:\n            maxdepth = None\n        else:\n            maxdepth = 1\n\n        infos = []\n        selected_files = self.fs.find(\n            selector.base_dir, maxdepth=maxdepth, withdirs=True, detail=True\n        )\n        for path, info in selected_files.items():\n            _path = path.strip(\"/\")\n            base_dir = selector.base_dir.strip(\"/\")\n            # Need to exclude base directory from selected files if present\n            # (fsspec filesystems, see GH-37555)\n            if _path != base_dir:\n                infos.append(self._create_file_info(path, info))\n\n        return infos\n\n    def create_dir(self, path, recursive):\n        # mkdir also raises FileNotFoundError when base directory is not found\n        try:\n            self.fs.mkdir(path, create_parents=recursive)\n        except FileExistsError:\n            pass\n\n    def delete_dir(self, path):\n        self.fs.rm(path, recursive=True)\n\n    def _delete_dir_contents(self, path, missing_dir_ok):\n        try:\n            subpaths = self.fs.listdir(path, detail=False)\n        except FileNotFoundError:\n            if missing_dir_ok:\n                return\n            raise\n        for subpath in subpaths:\n            if self.fs.isdir(subpath):\n                self.fs.rm(subpath, recursive=True)\n            elif self.fs.isfile(subpath):\n                self.fs.rm(subpath)\n\n    def delete_dir_contents(self, path, missing_dir_ok):\n        if path.strip(\"/\") == \"\":\n            raise ValueError(\n                \"delete_dir_contents called on path '\", path, \"'\")\n        self._delete_dir_contents(path, missing_dir_ok)\n\n    def delete_root_dir_contents(self):\n        self._delete_dir_contents(\"/\")\n\n    def delete_file(self, path):\n        # fs.rm correctly raises IsADirectoryError when `path` is a directory\n        # instead of a file and `recursive` is not set to True\n        if not self.fs.exists(path):\n            raise FileNotFoundError(path)\n        self.fs.rm(path)\n\n    def move(self, src, dest):\n        self.fs.mv(src, dest, recursive=True)\n\n    def copy_file(self, src, dest):\n        # fs.copy correctly raises IsADirectoryError when `src` is a directory\n        # instead of a file\n        self.fs.copy(src, dest)\n\n    # TODO can we read/pass metadata (e.g. Content-Type) in the methods below?\n\n    def open_input_stream(self, path):\n        from pyarrow import PythonFile\n\n        if not self.fs.isfile(path):\n            raise FileNotFoundError(path)\n\n        return PythonFile(self.fs.open(path, mode=\"rb\"), mode=\"r\")\n\n    def open_input_file(self, path):\n        from pyarrow import PythonFile\n\n        if not self.fs.isfile(path):\n            raise FileNotFoundError(path)\n\n        return PythonFile(self.fs.open(path, mode=\"rb\"), mode=\"r\")\n\n    def open_output_stream(self, path, metadata):\n        from pyarrow import PythonFile\n\n        return PythonFile(self.fs.open(path, mode=\"wb\"), mode=\"w\")\n\n    def open_append_stream(self, path, metadata):\n        from pyarrow import PythonFile\n\n        return PythonFile(self.fs.open(path, mode=\"ab\"), mode=\"w\")\n", "python/pyarrow/compute.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom pyarrow._compute import (  # noqa\n    Function,\n    FunctionOptions,\n    FunctionRegistry,\n    HashAggregateFunction,\n    HashAggregateKernel,\n    Kernel,\n    ScalarAggregateFunction,\n    ScalarAggregateKernel,\n    ScalarFunction,\n    ScalarKernel,\n    VectorFunction,\n    VectorKernel,\n    # Option classes\n    ArraySortOptions,\n    AssumeTimezoneOptions,\n    CastOptions,\n    CountOptions,\n    CumulativeOptions,\n    CumulativeSumOptions,\n    DayOfWeekOptions,\n    DictionaryEncodeOptions,\n    RunEndEncodeOptions,\n    ElementWiseAggregateOptions,\n    ExtractRegexOptions,\n    FilterOptions,\n    IndexOptions,\n    JoinOptions,\n    ListSliceOptions,\n    ListFlattenOptions,\n    MakeStructOptions,\n    MapLookupOptions,\n    MatchSubstringOptions,\n    ModeOptions,\n    NullOptions,\n    PadOptions,\n    PairwiseOptions,\n    PartitionNthOptions,\n    QuantileOptions,\n    RandomOptions,\n    RankOptions,\n    ReplaceSliceOptions,\n    ReplaceSubstringOptions,\n    RoundBinaryOptions,\n    RoundOptions,\n    RoundTemporalOptions,\n    RoundToMultipleOptions,\n    ScalarAggregateOptions,\n    SelectKOptions,\n    SetLookupOptions,\n    SliceOptions,\n    SortOptions,\n    SplitOptions,\n    SplitPatternOptions,\n    StrftimeOptions,\n    StrptimeOptions,\n    StructFieldOptions,\n    TakeOptions,\n    TDigestOptions,\n    TrimOptions,\n    Utf8NormalizeOptions,\n    VarianceOptions,\n    WeekOptions,\n    # Functions\n    call_function,\n    function_registry,\n    get_function,\n    list_functions,\n    # Udf\n    call_tabular_function,\n    register_scalar_function,\n    register_tabular_function,\n    register_aggregate_function,\n    register_vector_function,\n    UdfContext,\n    # Expressions\n    Expression,\n)\n\nfrom collections import namedtuple\nimport inspect\nfrom textwrap import dedent\nimport warnings\n\nimport pyarrow as pa\nfrom pyarrow import _compute_docstrings\nfrom pyarrow.vendored import docscrape\n\n\ndef _get_arg_names(func):\n    return func._doc.arg_names\n\n\n_OptionsClassDoc = namedtuple('_OptionsClassDoc', ('params',))\n\n\ndef _scrape_options_class_doc(options_class):\n    if not options_class.__doc__:\n        return None\n    doc = docscrape.NumpyDocString(options_class.__doc__)\n    return _OptionsClassDoc(doc['Parameters'])\n\n\ndef _decorate_compute_function(wrapper, exposed_name, func, options_class):\n    # Decorate the given compute function wrapper with useful metadata\n    # and documentation.\n    cpp_doc = func._doc\n\n    wrapper.__arrow_compute_function__ = dict(\n        name=func.name,\n        arity=func.arity,\n        options_class=cpp_doc.options_class,\n        options_required=cpp_doc.options_required)\n    wrapper.__name__ = exposed_name\n    wrapper.__qualname__ = exposed_name\n\n    doc_pieces = []\n\n    # 1. One-line summary\n    summary = cpp_doc.summary\n    if not summary:\n        arg_str = \"arguments\" if func.arity > 1 else \"argument\"\n        summary = (\"Call compute function {!r} with the given {}\"\n                   .format(func.name, arg_str))\n\n    doc_pieces.append(f\"{summary}.\\n\\n\")\n\n    # 2. Multi-line description\n    description = cpp_doc.description\n    if description:\n        doc_pieces.append(f\"{description}\\n\\n\")\n\n    doc_addition = _compute_docstrings.function_doc_additions.get(func.name)\n\n    # 3. Parameter description\n    doc_pieces.append(dedent(\"\"\"\\\n        Parameters\n        ----------\n        \"\"\"))\n\n    # 3a. Compute function parameters\n    arg_names = _get_arg_names(func)\n    for arg_name in arg_names:\n        if func.kind in ('vector', 'scalar_aggregate'):\n            arg_type = 'Array-like'\n        else:\n            arg_type = 'Array-like or scalar-like'\n        doc_pieces.append(f\"{arg_name} : {arg_type}\\n\")\n        doc_pieces.append(\"    Argument to compute function.\\n\")\n\n    # 3b. Compute function option values\n    if options_class is not None:\n        options_class_doc = _scrape_options_class_doc(options_class)\n        if options_class_doc:\n            for p in options_class_doc.params:\n                doc_pieces.append(f\"{p.name} : {p.type}\\n\")\n                for s in p.desc:\n                    doc_pieces.append(f\"    {s}\\n\")\n        else:\n            warnings.warn(f\"Options class {options_class.__name__} \"\n                          f\"does not have a docstring\", RuntimeWarning)\n            options_sig = inspect.signature(options_class)\n            for p in options_sig.parameters.values():\n                doc_pieces.append(dedent(\"\"\"\\\n                {0} : optional\n                    Parameter for {1} constructor. Either `options`\n                    or `{0}` can be passed, but not both at the same time.\n                \"\"\".format(p.name, options_class.__name__)))\n        doc_pieces.append(dedent(f\"\"\"\\\n            options : pyarrow.compute.{options_class.__name__}, optional\n                Alternative way of passing options.\n            \"\"\"))\n\n    doc_pieces.append(dedent(\"\"\"\\\n        memory_pool : pyarrow.MemoryPool, optional\n            If not passed, will allocate memory from the default memory pool.\n        \"\"\"))\n\n    # 4. Custom addition (e.g. examples)\n    if doc_addition is not None:\n        doc_pieces.append(\"\\n{}\\n\".format(dedent(doc_addition).strip(\"\\n\")))\n\n    wrapper.__doc__ = \"\".join(doc_pieces)\n    return wrapper\n\n\ndef _get_options_class(func):\n    class_name = func._doc.options_class\n    if not class_name:\n        return None\n    try:\n        return globals()[class_name]\n    except KeyError:\n        warnings.warn(\"Python binding for {} not exposed\"\n                      .format(class_name), RuntimeWarning)\n        return None\n\n\ndef _handle_options(name, options_class, options, args, kwargs):\n    if args or kwargs:\n        if options is not None:\n            raise TypeError(\n                \"Function {!r} called with both an 'options' argument \"\n                \"and additional arguments\"\n                .format(name))\n        return options_class(*args, **kwargs)\n\n    if options is not None:\n        if isinstance(options, dict):\n            return options_class(**options)\n        elif isinstance(options, options_class):\n            return options\n        raise TypeError(\n            \"Function {!r} expected a {} parameter, got {}\"\n            .format(name, options_class, type(options)))\n\n    return None\n\n\ndef _make_generic_wrapper(func_name, func, options_class, arity):\n    if options_class is None:\n        def wrapper(*args, memory_pool=None):\n            if arity is not Ellipsis and len(args) != arity:\n                raise TypeError(\n                    f\"{func_name} takes {arity} positional argument(s), \"\n                    f\"but {len(args)} were given\"\n                )\n            if args and isinstance(args[0], Expression):\n                return Expression._call(func_name, list(args))\n            return func.call(args, None, memory_pool)\n    else:\n        def wrapper(*args, memory_pool=None, options=None, **kwargs):\n            if arity is not Ellipsis:\n                if len(args) < arity:\n                    raise TypeError(\n                        f\"{func_name} takes {arity} positional argument(s), \"\n                        f\"but {len(args)} were given\"\n                    )\n                option_args = args[arity:]\n                args = args[:arity]\n            else:\n                option_args = ()\n            options = _handle_options(func_name, options_class, options,\n                                      option_args, kwargs)\n            if args and isinstance(args[0], Expression):\n                return Expression._call(func_name, list(args), options)\n            return func.call(args, options, memory_pool)\n    return wrapper\n\n\ndef _make_signature(arg_names, var_arg_names, options_class):\n    from inspect import Parameter\n    params = []\n    for name in arg_names:\n        params.append(Parameter(name, Parameter.POSITIONAL_ONLY))\n    for name in var_arg_names:\n        params.append(Parameter(name, Parameter.VAR_POSITIONAL))\n    if options_class is not None:\n        options_sig = inspect.signature(options_class)\n        for p in options_sig.parameters.values():\n            assert p.kind in (Parameter.POSITIONAL_OR_KEYWORD,\n                              Parameter.KEYWORD_ONLY)\n            if var_arg_names:\n                # Cannot have a positional argument after a *args\n                p = p.replace(kind=Parameter.KEYWORD_ONLY)\n            params.append(p)\n        params.append(Parameter(\"options\", Parameter.KEYWORD_ONLY,\n                                default=None))\n    params.append(Parameter(\"memory_pool\", Parameter.KEYWORD_ONLY,\n                            default=None))\n    return inspect.Signature(params)\n\n\ndef _wrap_function(name, func):\n    options_class = _get_options_class(func)\n    arg_names = _get_arg_names(func)\n    has_vararg = arg_names and arg_names[-1].startswith('*')\n    if has_vararg:\n        var_arg_names = [arg_names.pop().lstrip('*')]\n    else:\n        var_arg_names = []\n\n    wrapper = _make_generic_wrapper(\n        name, func, options_class, arity=func.arity)\n    wrapper.__signature__ = _make_signature(arg_names, var_arg_names,\n                                            options_class)\n    return _decorate_compute_function(wrapper, name, func, options_class)\n\n\ndef _make_global_functions():\n    \"\"\"\n    Make global functions wrapping each compute function.\n\n    Note that some of the automatically-generated wrappers may be overridden\n    by custom versions below.\n    \"\"\"\n    g = globals()\n    reg = function_registry()\n\n    # Avoid clashes with Python keywords\n    rewrites = {'and': 'and_',\n                'or': 'or_'}\n\n    for cpp_name in reg.list_functions():\n        name = rewrites.get(cpp_name, cpp_name)\n        func = reg.get_function(cpp_name)\n        if func.kind == \"hash_aggregate\":\n            # Hash aggregate functions are not callable,\n            # so let's not expose them at module level.\n            continue\n        if func.kind == \"scalar_aggregate\" and func.arity == 0:\n            # Nullary scalar aggregate functions are not callable\n            # directly so let's not expose them at module level.\n            continue\n        assert name not in g, name\n        g[cpp_name] = g[name] = _wrap_function(name, func)\n\n\n_make_global_functions()\n\n\ndef cast(arr, target_type=None, safe=None, options=None, memory_pool=None):\n    \"\"\"\n    Cast array values to another data type. Can also be invoked as an array\n    instance method.\n\n    Parameters\n    ----------\n    arr : Array-like\n    target_type : DataType or str\n        Type to cast to\n    safe : bool, default True\n        Check for overflows or other unsafe conversions\n    options : CastOptions, default None\n        Additional checks pass by CastOptions\n    memory_pool : MemoryPool, optional\n        memory pool to use for allocations during function execution.\n\n    Examples\n    --------\n    >>> from datetime import datetime\n    >>> import pyarrow as pa\n    >>> arr = pa.array([datetime(2010, 1, 1), datetime(2015, 1, 1)])\n    >>> arr.type\n    TimestampType(timestamp[us])\n\n    You can use ``pyarrow.DataType`` objects to specify the target type:\n\n    >>> cast(arr, pa.timestamp('ms'))\n    <pyarrow.lib.TimestampArray object at ...>\n    [\n      2010-01-01 00:00:00.000,\n      2015-01-01 00:00:00.000\n    ]\n\n    >>> cast(arr, pa.timestamp('ms')).type\n    TimestampType(timestamp[ms])\n\n    Alternatively, it is also supported to use the string aliases for these\n    types:\n\n    >>> arr.cast('timestamp[ms]')\n    <pyarrow.lib.TimestampArray object at ...>\n    [\n      2010-01-01 00:00:00.000,\n      2015-01-01 00:00:00.000\n    ]\n    >>> arr.cast('timestamp[ms]').type\n    TimestampType(timestamp[ms])\n\n    Returns\n    -------\n    casted : Array\n        The cast result as a new Array\n    \"\"\"\n    safe_vars_passed = (safe is not None) or (target_type is not None)\n\n    if safe_vars_passed and (options is not None):\n        raise ValueError(\"Must either pass values for 'target_type' and 'safe'\"\n                         \" or pass a value for 'options'\")\n\n    if options is None:\n        target_type = pa.types.lib.ensure_type(target_type)\n        if safe is False:\n            options = CastOptions.unsafe(target_type)\n        else:\n            options = CastOptions.safe(target_type)\n    return call_function(\"cast\", [arr], options, memory_pool)\n\n\ndef index(data, value, start=None, end=None, *, memory_pool=None):\n    \"\"\"\n    Find the index of the first occurrence of a given value.\n\n    Parameters\n    ----------\n    data : Array-like\n    value : Scalar-like object\n        The value to search for.\n    start : int, optional\n    end : int, optional\n    memory_pool : MemoryPool, optional\n        If not passed, will allocate memory from the default memory pool.\n\n    Returns\n    -------\n    index : int\n        the index, or -1 if not found\n    \"\"\"\n    if start is not None:\n        if end is not None:\n            data = data.slice(start, end - start)\n        else:\n            data = data.slice(start)\n    elif end is not None:\n        data = data.slice(0, end)\n\n    if not isinstance(value, pa.Scalar):\n        value = pa.scalar(value, type=data.type)\n    elif data.type != value.type:\n        value = pa.scalar(value.as_py(), type=data.type)\n    options = IndexOptions(value=value)\n    result = call_function('index', [data], options, memory_pool)\n    if start is not None and result.as_py() >= 0:\n        result = pa.scalar(result.as_py() + start, type=pa.int64())\n    return result\n\n\ndef take(data, indices, *, boundscheck=True, memory_pool=None):\n    \"\"\"\n    Select values (or records) from array- or table-like data given integer\n    selection indices.\n\n    The result will be of the same type(s) as the input, with elements taken\n    from the input array (or record batch / table fields) at the given\n    indices. If an index is null then the corresponding value in the output\n    will be null.\n\n    Parameters\n    ----------\n    data : Array, ChunkedArray, RecordBatch, or Table\n    indices : Array, ChunkedArray\n        Must be of integer type\n    boundscheck : boolean, default True\n        Whether to boundscheck the indices. If False and there is an out of\n        bounds index, will likely cause the process to crash.\n    memory_pool : MemoryPool, optional\n        If not passed, will allocate memory from the default memory pool.\n\n    Returns\n    -------\n    result : depends on inputs\n        Selected values for the given indices\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> arr = pa.array([\"a\", \"b\", \"c\", None, \"e\", \"f\"])\n    >>> indices = pa.array([0, None, 4, 3])\n    >>> arr.take(indices)\n    <pyarrow.lib.StringArray object at ...>\n    [\n      \"a\",\n      null,\n      \"e\",\n      null\n    ]\n    \"\"\"\n    options = TakeOptions(boundscheck=boundscheck)\n    return call_function('take', [data, indices], options, memory_pool)\n\n\ndef fill_null(values, fill_value):\n    \"\"\"Replace each null element in values with a corresponding\n    element from fill_value.\n\n    If fill_value is scalar-like, then every null element in values\n    will be replaced with fill_value. If fill_value is array-like,\n    then the i-th element in values will be replaced with the i-th\n    element in fill_value.\n\n    The fill_value's type must be the same as that of values, or it\n    must be able to be implicitly casted to the array's type.\n\n    This is an alias for :func:`coalesce`.\n\n    Parameters\n    ----------\n    values : Array, ChunkedArray, or Scalar-like object\n        Each null element is replaced with the corresponding value\n        from fill_value.\n    fill_value : Array, ChunkedArray, or Scalar-like object\n        If not same type as values, will attempt to cast.\n\n    Returns\n    -------\n    result : depends on inputs\n        Values with all null elements replaced\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> arr = pa.array([1, 2, None, 3], type=pa.int8())\n    >>> fill_value = pa.scalar(5, type=pa.int8())\n    >>> arr.fill_null(fill_value)\n    <pyarrow.lib.Int8Array object at ...>\n    [\n      1,\n      2,\n      5,\n      3\n    ]\n    >>> arr = pa.array([1, 2, None, 4, None])\n    >>> arr.fill_null(pa.array([10, 20, 30, 40, 50]))\n    <pyarrow.lib.Int64Array object at ...>\n    [\n      1,\n      2,\n      30,\n      4,\n      50\n    ]\n    \"\"\"\n    if not isinstance(fill_value, (pa.Array, pa.ChunkedArray, pa.Scalar)):\n        fill_value = pa.scalar(fill_value, type=values.type)\n    elif values.type != fill_value.type:\n        fill_value = pa.scalar(fill_value.as_py(), type=values.type)\n\n    return call_function(\"coalesce\", [values, fill_value])\n\n\ndef top_k_unstable(values, k, sort_keys=None, *, memory_pool=None):\n    \"\"\"\n    Select the indices of the top-k ordered elements from array- or table-like\n    data.\n\n    This is a specialization for :func:`select_k_unstable`. Output is not\n    guaranteed to be stable.\n\n    Parameters\n    ----------\n    values : Array, ChunkedArray, RecordBatch, or Table\n        Data to sort and get top indices from.\n    k : int\n        The number of `k` elements to keep.\n    sort_keys : List-like\n        Column key names to order by when input is table-like data.\n    memory_pool : MemoryPool, optional\n        If not passed, will allocate memory from the default memory pool.\n\n    Returns\n    -------\n    result : Array\n        Indices of the top-k ordered elements\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> import pyarrow.compute as pc\n    >>> arr = pa.array([\"a\", \"b\", \"c\", None, \"e\", \"f\"])\n    >>> pc.top_k_unstable(arr, k=3)\n    <pyarrow.lib.UInt64Array object at ...>\n    [\n      5,\n      4,\n      2\n    ]\n    \"\"\"\n    if sort_keys is None:\n        sort_keys = []\n    if isinstance(values, (pa.Array, pa.ChunkedArray)):\n        sort_keys.append((\"dummy\", \"descending\"))\n    else:\n        sort_keys = map(lambda key_name: (key_name, \"descending\"), sort_keys)\n    options = SelectKOptions(k, sort_keys)\n    return call_function(\"select_k_unstable\", [values], options, memory_pool)\n\n\ndef bottom_k_unstable(values, k, sort_keys=None, *, memory_pool=None):\n    \"\"\"\n    Select the indices of the bottom-k ordered elements from\n    array- or table-like data.\n\n    This is a specialization for :func:`select_k_unstable`. Output is not\n    guaranteed to be stable.\n\n    Parameters\n    ----------\n    values : Array, ChunkedArray, RecordBatch, or Table\n        Data to sort and get bottom indices from.\n    k : int\n        The number of `k` elements to keep.\n    sort_keys : List-like\n        Column key names to order by when input is table-like data.\n    memory_pool : MemoryPool, optional\n        If not passed, will allocate memory from the default memory pool.\n\n    Returns\n    -------\n    result : Array of indices\n        Indices of the bottom-k ordered elements\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> import pyarrow.compute as pc\n    >>> arr = pa.array([\"a\", \"b\", \"c\", None, \"e\", \"f\"])\n    >>> pc.bottom_k_unstable(arr, k=3)\n    <pyarrow.lib.UInt64Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    if sort_keys is None:\n        sort_keys = []\n    if isinstance(values, (pa.Array, pa.ChunkedArray)):\n        sort_keys.append((\"dummy\", \"ascending\"))\n    else:\n        sort_keys = map(lambda key_name: (key_name, \"ascending\"), sort_keys)\n    options = SelectKOptions(k, sort_keys)\n    return call_function(\"select_k_unstable\", [values], options, memory_pool)\n\n\ndef random(n, *, initializer='system', options=None, memory_pool=None):\n    \"\"\"\n    Generate numbers in the range [0, 1).\n\n    Generated values are uniformly-distributed, double-precision\n    in range [0, 1). Algorithm and seed can be changed via RandomOptions.\n\n    Parameters\n    ----------\n    n : int\n        Number of values to generate, must be greater than or equal to 0\n    initializer : int or str\n        How to initialize the underlying random generator.\n        If an integer is given, it is used as a seed.\n        If \"system\" is given, the random generator is initialized with\n        a system-specific source of (hopefully true) randomness.\n        Other values are invalid.\n    options : pyarrow.compute.RandomOptions, optional\n        Alternative way of passing options.\n    memory_pool : pyarrow.MemoryPool, optional\n        If not passed, will allocate memory from the default memory pool.\n    \"\"\"\n    options = RandomOptions(initializer=initializer)\n    return call_function(\"random\", [], options, memory_pool, length=n)\n\n\ndef field(*name_or_index):\n    \"\"\"Reference a column of the dataset.\n\n    Stores only the field's name. Type and other information is known only when\n    the expression is bound to a dataset having an explicit scheme.\n\n    Nested references are allowed by passing multiple names or a tuple of\n    names. For example ``('foo', 'bar')`` references the field named \"bar\"\n    inside the field named \"foo\".\n\n    Parameters\n    ----------\n    *name_or_index : string, multiple strings, tuple or int\n        The name or index of the (possibly nested) field the expression\n        references to.\n\n    Returns\n    -------\n    field_expr : Expression\n        Reference to the given field\n\n    Examples\n    --------\n    >>> import pyarrow.compute as pc\n    >>> pc.field(\"a\")\n    <pyarrow.compute.Expression a>\n    >>> pc.field(1)\n    <pyarrow.compute.Expression FieldPath(1)>\n    >>> pc.field((\"a\", \"b\"))\n    <pyarrow.compute.Expression FieldRef.Nested(FieldRef.Name(a) ...\n    >>> pc.field(\"a\", \"b\")\n    <pyarrow.compute.Expression FieldRef.Nested(FieldRef.Name(a) ...\n    \"\"\"\n    n = len(name_or_index)\n    if n == 1:\n        if isinstance(name_or_index[0], (str, int)):\n            return Expression._field(name_or_index[0])\n        elif isinstance(name_or_index[0], tuple):\n            return Expression._nested_field(name_or_index[0])\n        else:\n            raise TypeError(\n                \"field reference should be str, multiple str, tuple or \"\n                f\"integer, got {type(name_or_index[0])}\"\n            )\n    # In case of multiple strings not supplied in a tuple\n    else:\n        return Expression._nested_field(name_or_index)\n\n\ndef scalar(value):\n    \"\"\"Expression representing a scalar value.\n\n    Parameters\n    ----------\n    value : bool, int, float or string\n        Python value of the scalar. Note that only a subset of types are\n        currently supported.\n\n    Returns\n    -------\n    scalar_expr : Expression\n        An Expression representing the scalar value\n    \"\"\"\n    return Expression._scalar(value)\n", "python/pyarrow/util.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Miscellaneous utility code\n\nimport os\nimport contextlib\nimport functools\nimport gc\nimport socket\nimport sys\nimport textwrap\nimport types\nimport warnings\n\n\n_DEPR_MSG = (\n    \"pyarrow.{} is deprecated as of {}, please use pyarrow.{} instead.\"\n)\n\n\ndef doc(*docstrings, **params):\n    \"\"\"\n    A decorator that takes docstring templates, concatenates them, and finally\n    performs string substitution on them.\n    This decorator will add a variable \"_docstring_components\" to the wrapped\n    callable to keep track of the original docstring template for potential future use.\n    If the docstring is a template, it will be saved as a string.\n    Otherwise, it will be saved as a callable and the docstring will be obtained via\n    the __doc__ attribute.\n    This decorator cannot be used on Cython classes due to a CPython constraint,\n    which enforces the __doc__ attribute to be read-only.\n    See https://github.com/python/cpython/issues/91309\n\n    Parameters\n    ----------\n    *docstrings : None, str, or callable\n        The string / docstring / docstring template to be prepended in order\n        before the default docstring under the callable.\n    **params\n        The key/value pairs used to format the docstring template.\n    \"\"\"\n\n    def decorator(decorated):\n        docstring_components = []\n\n        # collect docstrings and docstring templates\n        for docstring in docstrings:\n            if docstring is None:\n                continue\n            if hasattr(docstring, \"_docstring_components\"):\n                docstring_components.extend(\n                    docstring._docstring_components\n                )\n            elif isinstance(docstring, str) or docstring.__doc__:\n                docstring_components.append(docstring)\n\n        # append the callable's docstring last\n        if decorated.__doc__:\n            docstring_components.append(textwrap.dedent(decorated.__doc__))\n\n        params_applied = [\n            component.format(**params)\n            if isinstance(component, str) and len(params) > 0\n            else component\n            for component in docstring_components\n        ]\n\n        decorated.__doc__ = \"\".join(\n            [\n                component\n                if isinstance(component, str)\n                else textwrap.dedent(component.__doc__ or \"\")\n                for component in params_applied\n            ]\n        )\n\n        decorated._docstring_components = (\n            docstring_components\n        )\n        return decorated\n\n    return decorator\n\n\ndef _deprecate_api(old_name, new_name, api, next_version, type=FutureWarning):\n    msg = _DEPR_MSG.format(old_name, next_version, new_name)\n\n    def wrapper(*args, **kwargs):\n        warnings.warn(msg, type)\n        return api(*args, **kwargs)\n    return wrapper\n\n\ndef _deprecate_class(old_name, new_class, next_version,\n                     instancecheck=True):\n    \"\"\"\n    Raise warning if a deprecated class is used in an isinstance check.\n    \"\"\"\n    class _DeprecatedMeta(type):\n        def __instancecheck__(self, other):\n            warnings.warn(\n                _DEPR_MSG.format(old_name, next_version, new_class.__name__),\n                FutureWarning,\n                stacklevel=2\n            )\n            return isinstance(other, new_class)\n\n    return _DeprecatedMeta(old_name, (new_class,), {})\n\n\ndef _is_iterable(obj):\n    try:\n        iter(obj)\n        return True\n    except TypeError:\n        return False\n\n\ndef _is_path_like(path):\n    return isinstance(path, str) or hasattr(path, '__fspath__')\n\n\ndef _stringify_path(path):\n    \"\"\"\n    Convert *path* to a string or unicode path if possible.\n    \"\"\"\n    if isinstance(path, str):\n        return os.path.expanduser(path)\n\n    # checking whether path implements the filesystem protocol\n    try:\n        return os.path.expanduser(path.__fspath__())\n    except AttributeError:\n        pass\n\n    raise TypeError(\"not a path-like object\")\n\n\ndef product(seq):\n    \"\"\"\n    Return a product of sequence items.\n    \"\"\"\n    return functools.reduce(lambda a, b: a*b, seq, 1)\n\n\ndef get_contiguous_span(shape, strides, itemsize):\n    \"\"\"\n    Return a contiguous span of N-D array data.\n\n    Parameters\n    ----------\n    shape : tuple\n    strides : tuple\n    itemsize : int\n      Specify array shape data\n\n    Returns\n    -------\n    start, end : int\n      The span end points.\n    \"\"\"\n    if not strides:\n        start = 0\n        end = itemsize * product(shape)\n    else:\n        start = 0\n        end = itemsize\n        for i, dim in enumerate(shape):\n            if dim == 0:\n                start = end = 0\n                break\n            stride = strides[i]\n            if stride > 0:\n                end += stride * (dim - 1)\n            elif stride < 0:\n                start += stride * (dim - 1)\n        if end - start != itemsize * product(shape):\n            raise ValueError('array data is non-contiguous')\n    return start, end\n\n\ndef find_free_port():\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    with contextlib.closing(sock) as sock:\n        sock.bind(('', 0))\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        return sock.getsockname()[1]\n\n\ndef guid():\n    from uuid import uuid4\n    return uuid4().hex\n\n\ndef _break_traceback_cycle_from_frame(frame):\n    # Clear local variables in all inner frames, so as to break the\n    # reference cycle.\n    this_frame = sys._getframe(0)\n    refs = gc.get_referrers(frame)\n    while refs:\n        for frame in refs:\n            if frame is not this_frame and isinstance(frame, types.FrameType):\n                break\n        else:\n            # No frame found in referrers (finished?)\n            break\n        refs = None\n        # Clear the frame locals, to try and break the cycle (it is\n        # somewhere along the chain of execution frames).\n        frame.clear()\n        # To visit the inner frame, we need to find it among the\n        # referrers of this frame (while `frame.f_back` would let\n        # us visit the outer frame).\n        refs = gc.get_referrers(frame)\n    refs = frame = this_frame = None\n\n\ndef download_tzdata_on_windows():\n    r\"\"\"\n    Download and extract latest IANA timezone database into the\n    location expected by Arrow which is %USERPROFILE%\\Downloads\\tzdata.\n    \"\"\"\n    if sys.platform != 'win32':\n        raise TypeError(f\"Timezone database is already provided by {sys.platform}\")\n\n    import tarfile\n\n    tzdata_path = os.path.expandvars(r\"%USERPROFILE%\\Downloads\\tzdata\")\n    tzdata_compressed = os.path.join(tzdata_path, \"tzdata.tar.gz\")\n    os.makedirs(tzdata_path, exist_ok=True)\n\n    from urllib.request import urlopen\n    with urlopen('https://data.iana.org/time-zones/tzdata-latest.tar.gz') as response:\n        with open(tzdata_compressed, 'wb') as f:\n            f.write(response.read())\n\n    assert os.path.exists(tzdata_compressed)\n\n    tarfile.open(tzdata_compressed).extractall(tzdata_path)\n\n    with urlopen('https://raw.githubusercontent.com/unicode-org/cldr/master/common/supplemental/windowsZones.xml') as response_zones:   # noqa\n        with open(os.path.join(tzdata_path, \"windowsZones.xml\"), 'wb') as f:\n            f.write(response_zones.read())\n", "python/pyarrow/substrait.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\ntry:\n    from pyarrow._substrait import (  # noqa\n        BoundExpressions,\n        get_supported_functions,\n        run_query,\n        deserialize_expressions,\n        serialize_expressions\n    )\nexcept ImportError as exc:\n    raise ImportError(\n        \"The pyarrow installation is not built with support \"\n        f\"for 'substrait' ({str(exc)})\"\n    ) from None\n", "python/pyarrow/json.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nfrom pyarrow._json import ReadOptions, ParseOptions, read_json  # noqa\n", "python/pyarrow/types.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Tools for dealing with Arrow type metadata in Python\n\n\nfrom pyarrow.lib import (is_boolean_value,  # noqa\n                         is_integer_value,\n                         is_float_value)\n\nimport pyarrow.lib as lib\nfrom pyarrow.util import doc\n\n\n_SIGNED_INTEGER_TYPES = {lib.Type_INT8, lib.Type_INT16, lib.Type_INT32,\n                         lib.Type_INT64}\n_UNSIGNED_INTEGER_TYPES = {lib.Type_UINT8, lib.Type_UINT16, lib.Type_UINT32,\n                           lib.Type_UINT64}\n_INTEGER_TYPES = _SIGNED_INTEGER_TYPES | _UNSIGNED_INTEGER_TYPES\n_FLOATING_TYPES = {lib.Type_HALF_FLOAT, lib.Type_FLOAT, lib.Type_DOUBLE}\n_DECIMAL_TYPES = {lib.Type_DECIMAL128, lib.Type_DECIMAL256}\n_DATE_TYPES = {lib.Type_DATE32, lib.Type_DATE64}\n_TIME_TYPES = {lib.Type_TIME32, lib.Type_TIME64}\n_INTERVAL_TYPES = {lib.Type_INTERVAL_MONTH_DAY_NANO}\n_TEMPORAL_TYPES = ({lib.Type_TIMESTAMP,\n                    lib.Type_DURATION} | _TIME_TYPES | _DATE_TYPES |\n                   _INTERVAL_TYPES)\n_UNION_TYPES = {lib.Type_SPARSE_UNION, lib.Type_DENSE_UNION}\n_NESTED_TYPES = {lib.Type_LIST, lib.Type_FIXED_SIZE_LIST, lib.Type_LARGE_LIST,\n                 lib.Type_LIST_VIEW, lib.Type_LARGE_LIST_VIEW,\n                 lib.Type_STRUCT, lib.Type_MAP} | _UNION_TYPES\n\n\n@doc(datatype=\"null\")\ndef is_null(t):\n    \"\"\"\n    Return True if value is an instance of type: {datatype}.\n\n    Parameters\n    ----------\n    t : DataType\n    \"\"\"\n    return t.id == lib.Type_NA\n\n\n@doc(is_null, datatype=\"boolean\")\ndef is_boolean(t):\n    return t.id == lib.Type_BOOL\n\n\n@doc(is_null, datatype=\"any integer\")\ndef is_integer(t):\n    return t.id in _INTEGER_TYPES\n\n\n@doc(is_null, datatype=\"signed integer\")\ndef is_signed_integer(t):\n    return t.id in _SIGNED_INTEGER_TYPES\n\n\n@doc(is_null, datatype=\"unsigned integer\")\ndef is_unsigned_integer(t):\n    return t.id in _UNSIGNED_INTEGER_TYPES\n\n\n@doc(is_null, datatype=\"int8\")\ndef is_int8(t):\n    return t.id == lib.Type_INT8\n\n\n@doc(is_null, datatype=\"int16\")\ndef is_int16(t):\n    return t.id == lib.Type_INT16\n\n\n@doc(is_null, datatype=\"int32\")\ndef is_int32(t):\n    return t.id == lib.Type_INT32\n\n\n@doc(is_null, datatype=\"int64\")\ndef is_int64(t):\n    return t.id == lib.Type_INT64\n\n\n@doc(is_null, datatype=\"uint8\")\ndef is_uint8(t):\n    return t.id == lib.Type_UINT8\n\n\n@doc(is_null, datatype=\"uint16\")\ndef is_uint16(t):\n    return t.id == lib.Type_UINT16\n\n\n@doc(is_null, datatype=\"uint32\")\ndef is_uint32(t):\n    return t.id == lib.Type_UINT32\n\n\n@doc(is_null, datatype=\"uint64\")\ndef is_uint64(t):\n    return t.id == lib.Type_UINT64\n\n\n@doc(is_null, datatype=\"floating point numeric\")\ndef is_floating(t):\n    return t.id in _FLOATING_TYPES\n\n\n@doc(is_null, datatype=\"float16 (half-precision)\")\ndef is_float16(t):\n    return t.id == lib.Type_HALF_FLOAT\n\n\n@doc(is_null, datatype=\"float32 (single precision)\")\ndef is_float32(t):\n    return t.id == lib.Type_FLOAT\n\n\n@doc(is_null, datatype=\"float64 (double precision)\")\ndef is_float64(t):\n    return t.id == lib.Type_DOUBLE\n\n\n@doc(is_null, datatype=\"list\")\ndef is_list(t):\n    return t.id == lib.Type_LIST\n\n\n@doc(is_null, datatype=\"large list\")\ndef is_large_list(t):\n    return t.id == lib.Type_LARGE_LIST\n\n\n@doc(is_null, datatype=\"fixed size list\")\ndef is_fixed_size_list(t):\n    return t.id == lib.Type_FIXED_SIZE_LIST\n\n\n@doc(is_null, datatype=\"list view\")\ndef is_list_view(t):\n    return t.id == lib.Type_LIST_VIEW\n\n\n@doc(is_null, datatype=\"large list view\")\ndef is_large_list_view(t):\n    return t.id == lib.Type_LARGE_LIST_VIEW\n\n\n@doc(is_null, datatype=\"struct\")\ndef is_struct(t):\n    return t.id == lib.Type_STRUCT\n\n\n@doc(is_null, datatype=\"union\")\ndef is_union(t):\n    return t.id in _UNION_TYPES\n\n\n@doc(is_null, datatype=\"nested type\")\ndef is_nested(t):\n    return t.id in _NESTED_TYPES\n\n\n@doc(is_null, datatype=\"run-end encoded\")\ndef is_run_end_encoded(t):\n    return t.id == lib.Type_RUN_END_ENCODED\n\n\n@doc(is_null, datatype=\"date, time, timestamp or duration\")\ndef is_temporal(t):\n    return t.id in _TEMPORAL_TYPES\n\n\n@doc(is_null, datatype=\"timestamp\")\ndef is_timestamp(t):\n    return t.id == lib.Type_TIMESTAMP\n\n\n@doc(is_null, datatype=\"duration\")\ndef is_duration(t):\n    return t.id == lib.Type_DURATION\n\n\n@doc(is_null, datatype=\"time\")\ndef is_time(t):\n    return t.id in _TIME_TYPES\n\n\n@doc(is_null, datatype=\"time32\")\ndef is_time32(t):\n    return t.id == lib.Type_TIME32\n\n\n@doc(is_null, datatype=\"time64\")\ndef is_time64(t):\n    return t.id == lib.Type_TIME64\n\n\n@doc(is_null, datatype=\"variable-length binary\")\ndef is_binary(t):\n    return t.id == lib.Type_BINARY\n\n\n@doc(is_null, datatype=\"large variable-length binary\")\ndef is_large_binary(t):\n    return t.id == lib.Type_LARGE_BINARY\n\n\n@doc(method=\"is_string\")\ndef is_unicode(t):\n    \"\"\"\n    Alias for {method}.\n\n    Parameters\n    ----------\n    t : DataType\n    \"\"\"\n    return is_string(t)\n\n\n@doc(is_null, datatype=\"string (utf8 unicode)\")\ndef is_string(t):\n    return t.id == lib.Type_STRING\n\n\n@doc(is_unicode, method=\"is_large_string\")\ndef is_large_unicode(t):\n    return is_large_string(t)\n\n\n@doc(is_null, datatype=\"large string (utf8 unicode)\")\ndef is_large_string(t):\n    return t.id == lib.Type_LARGE_STRING\n\n\n@doc(is_null, datatype=\"fixed size binary\")\ndef is_fixed_size_binary(t):\n    return t.id == lib.Type_FIXED_SIZE_BINARY\n\n\n@doc(is_null, datatype=\"variable-length binary view\")\ndef is_binary_view(t):\n    return t.id == lib.Type_BINARY_VIEW\n\n\n@doc(is_null, datatype=\"variable-length string (utf-8) view\")\ndef is_string_view(t):\n    return t.id == lib.Type_STRING_VIEW\n\n\n@doc(is_null, datatype=\"date\")\ndef is_date(t):\n    return t.id in _DATE_TYPES\n\n\n@doc(is_null, datatype=\"date32 (days)\")\ndef is_date32(t):\n    return t.id == lib.Type_DATE32\n\n\n@doc(is_null, datatype=\"date64 (milliseconds)\")\ndef is_date64(t):\n    return t.id == lib.Type_DATE64\n\n\n@doc(is_null, datatype=\"map\")\ndef is_map(t):\n    return t.id == lib.Type_MAP\n\n\n@doc(is_null, datatype=\"decimal\")\ndef is_decimal(t):\n    return t.id in _DECIMAL_TYPES\n\n\n@doc(is_null, datatype=\"decimal128\")\ndef is_decimal128(t):\n    return t.id == lib.Type_DECIMAL128\n\n\n@doc(is_null, datatype=\"decimal256\")\ndef is_decimal256(t):\n    return t.id == lib.Type_DECIMAL256\n\n\n@doc(is_null, datatype=\"dictionary-encoded\")\ndef is_dictionary(t):\n    return t.id == lib.Type_DICTIONARY\n\n\n@doc(is_null, datatype=\"interval\")\ndef is_interval(t):\n    return t.id == lib.Type_INTERVAL_MONTH_DAY_NANO\n\n\n@doc(is_null, datatype=\"primitive type\")\ndef is_primitive(t):\n    return lib._is_primitive(t.id)\n", "python/pyarrow/pandas_compat.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nimport ast\nfrom collections.abc import Sequence\nfrom concurrent import futures\n# import threading submodule upfront to avoid partially initialized\n# module bug (ARROW-11983)\nimport concurrent.futures.thread  # noqa\nfrom copy import deepcopy\nimport decimal\nfrom itertools import zip_longest\nimport json\nimport operator\nimport re\nimport warnings\n\nimport numpy as np\n\nimport pyarrow as pa\nfrom pyarrow.lib import _pandas_api, frombytes  # noqa\n\n\n_logical_type_map = {}\n\n\ndef get_logical_type_map():\n    global _logical_type_map\n\n    if not _logical_type_map:\n        _logical_type_map.update({\n            pa.lib.Type_NA: 'empty',\n            pa.lib.Type_BOOL: 'bool',\n            pa.lib.Type_INT8: 'int8',\n            pa.lib.Type_INT16: 'int16',\n            pa.lib.Type_INT32: 'int32',\n            pa.lib.Type_INT64: 'int64',\n            pa.lib.Type_UINT8: 'uint8',\n            pa.lib.Type_UINT16: 'uint16',\n            pa.lib.Type_UINT32: 'uint32',\n            pa.lib.Type_UINT64: 'uint64',\n            pa.lib.Type_HALF_FLOAT: 'float16',\n            pa.lib.Type_FLOAT: 'float32',\n            pa.lib.Type_DOUBLE: 'float64',\n            pa.lib.Type_DATE32: 'date',\n            pa.lib.Type_DATE64: 'date',\n            pa.lib.Type_TIME32: 'time',\n            pa.lib.Type_TIME64: 'time',\n            pa.lib.Type_BINARY: 'bytes',\n            pa.lib.Type_FIXED_SIZE_BINARY: 'bytes',\n            pa.lib.Type_STRING: 'unicode',\n        })\n    return _logical_type_map\n\n\ndef get_logical_type(arrow_type):\n    logical_type_map = get_logical_type_map()\n\n    try:\n        return logical_type_map[arrow_type.id]\n    except KeyError:\n        if isinstance(arrow_type, pa.lib.DictionaryType):\n            return 'categorical'\n        elif isinstance(arrow_type, pa.lib.ListType):\n            return 'list[{}]'.format(get_logical_type(arrow_type.value_type))\n        elif isinstance(arrow_type, pa.lib.TimestampType):\n            return 'datetimetz' if arrow_type.tz is not None else 'datetime'\n        elif isinstance(arrow_type, pa.lib.Decimal128Type):\n            return 'decimal'\n        return 'object'\n\n\n_numpy_logical_type_map = {\n    np.bool_: 'bool',\n    np.int8: 'int8',\n    np.int16: 'int16',\n    np.int32: 'int32',\n    np.int64: 'int64',\n    np.uint8: 'uint8',\n    np.uint16: 'uint16',\n    np.uint32: 'uint32',\n    np.uint64: 'uint64',\n    np.float32: 'float32',\n    np.float64: 'float64',\n    'datetime64[D]': 'date',\n    np.str_: 'string',\n    np.bytes_: 'bytes',\n}\n\n\ndef get_logical_type_from_numpy(pandas_collection):\n    try:\n        return _numpy_logical_type_map[pandas_collection.dtype.type]\n    except KeyError:\n        if hasattr(pandas_collection.dtype, 'tz'):\n            return 'datetimetz'\n        # See https://github.com/pandas-dev/pandas/issues/24739 (infer_dtype will\n        # result in \"datetime64\" without unit, while pandas astype requires a unit)\n        if str(pandas_collection.dtype).startswith('datetime64'):\n            return str(pandas_collection.dtype)\n        result = _pandas_api.infer_dtype(pandas_collection)\n        if result == 'string':\n            return 'unicode'\n        return result\n\n\ndef get_extension_dtype_info(column):\n    dtype = column.dtype\n    if str(dtype) == 'category':\n        cats = getattr(column, 'cat', column)\n        assert cats is not None\n        metadata = {\n            'num_categories': len(cats.categories),\n            'ordered': cats.ordered,\n        }\n        physical_dtype = str(cats.codes.dtype)\n    elif hasattr(dtype, 'tz'):\n        metadata = {'timezone': pa.lib.tzinfo_to_string(dtype.tz)}\n        physical_dtype = 'datetime64[ns]'\n    else:\n        metadata = None\n        physical_dtype = str(dtype)\n    return physical_dtype, metadata\n\n\ndef get_column_metadata(column, name, arrow_type, field_name):\n    \"\"\"Construct the metadata for a given column\n\n    Parameters\n    ----------\n    column : pandas.Series or pandas.Index\n    name : str\n    arrow_type : pyarrow.DataType\n    field_name : str\n        Equivalent to `name` when `column` is a `Series`, otherwise if `column`\n        is a pandas Index then `field_name` will not be the same as `name`.\n        This is the name of the field in the arrow Table's schema.\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    logical_type = get_logical_type(arrow_type)\n\n    string_dtype, extra_metadata = get_extension_dtype_info(column)\n    if logical_type == 'decimal':\n        extra_metadata = {\n            'precision': arrow_type.precision,\n            'scale': arrow_type.scale,\n        }\n        string_dtype = 'object'\n\n    if name is not None and not isinstance(name, str):\n        raise TypeError(\n            'Column name must be a string. Got column {} of type {}'.format(\n                name, type(name).__name__\n            )\n        )\n\n    assert field_name is None or isinstance(field_name, str), \\\n        str(type(field_name))\n    return {\n        'name': name,\n        'field_name': 'None' if field_name is None else field_name,\n        'pandas_type': logical_type,\n        'numpy_type': string_dtype,\n        'metadata': extra_metadata,\n    }\n\n\ndef construct_metadata(columns_to_convert, df, column_names, index_levels,\n                       index_descriptors, preserve_index, types):\n    \"\"\"Returns a dictionary containing enough metadata to reconstruct a pandas\n    DataFrame as an Arrow Table, including index columns.\n\n    Parameters\n    ----------\n    columns_to_convert : list[pd.Series]\n    df : pandas.DataFrame\n    index_levels : List[pd.Index]\n    index_descriptors : List[Dict]\n    preserve_index : bool\n    types : List[pyarrow.DataType]\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    num_serialized_index_levels = len([descr for descr in index_descriptors\n                                       if not isinstance(descr, dict)])\n    # Use ntypes instead of Python shorthand notation [:-len(x)] as [:-0]\n    # behaves differently to what we want.\n    ntypes = len(types)\n    df_types = types[:ntypes - num_serialized_index_levels]\n    index_types = types[ntypes - num_serialized_index_levels:]\n\n    column_metadata = []\n    for col, sanitized_name, arrow_type in zip(columns_to_convert,\n                                               column_names, df_types):\n        metadata = get_column_metadata(col, name=sanitized_name,\n                                       arrow_type=arrow_type,\n                                       field_name=sanitized_name)\n        column_metadata.append(metadata)\n\n    index_column_metadata = []\n    if preserve_index is not False:\n        non_str_index_names = []\n        for level, arrow_type, descriptor in zip(index_levels, index_types,\n                                                 index_descriptors):\n            if isinstance(descriptor, dict):\n                # The index is represented in a non-serialized fashion,\n                # e.g. RangeIndex\n                continue\n\n            if level.name is not None and not isinstance(level.name, str):\n                non_str_index_names.append(level.name)\n\n            metadata = get_column_metadata(\n                level,\n                name=_column_name_to_strings(level.name),\n                arrow_type=arrow_type,\n                field_name=descriptor,\n            )\n            index_column_metadata.append(metadata)\n\n        if len(non_str_index_names) > 0:\n            warnings.warn(\n                f\"The DataFrame has non-str index name `{non_str_index_names}`\"\n                \" which will be converted to string\"\n                \" and not roundtrip correctly.\",\n                UserWarning, stacklevel=4)\n\n        column_indexes = []\n\n        levels = getattr(df.columns, 'levels', [df.columns])\n        names = getattr(df.columns, 'names', [df.columns.name])\n        for level, name in zip(levels, names):\n            metadata = _get_simple_index_descriptor(level, name)\n            column_indexes.append(metadata)\n    else:\n        index_descriptors = index_column_metadata = column_indexes = []\n\n    return {\n        b'pandas': json.dumps({\n            'index_columns': index_descriptors,\n            'column_indexes': column_indexes,\n            'columns': column_metadata + index_column_metadata,\n            'creator': {\n                'library': 'pyarrow',\n                'version': pa.__version__\n            },\n            'pandas_version': _pandas_api.version\n        }).encode('utf8')\n    }\n\n\ndef _get_simple_index_descriptor(level, name):\n    string_dtype, extra_metadata = get_extension_dtype_info(level)\n    pandas_type = get_logical_type_from_numpy(level)\n    if 'mixed' in pandas_type:\n        warnings.warn(\n            \"The DataFrame has column names of mixed type. They will be \"\n            \"converted to strings and not roundtrip correctly.\",\n            UserWarning, stacklevel=4)\n    if pandas_type == 'unicode':\n        assert not extra_metadata\n        extra_metadata = {'encoding': 'UTF-8'}\n    return {\n        'name': name,\n        'field_name': name,\n        'pandas_type': pandas_type,\n        'numpy_type': string_dtype,\n        'metadata': extra_metadata,\n    }\n\n\ndef _column_name_to_strings(name):\n    \"\"\"Convert a column name (or level) to either a string or a recursive\n    collection of strings.\n\n    Parameters\n    ----------\n    name : str or tuple\n\n    Returns\n    -------\n    value : str or tuple\n\n    Examples\n    --------\n    >>> name = 'foo'\n    >>> _column_name_to_strings(name)\n    'foo'\n    >>> name = ('foo', 'bar')\n    >>> _column_name_to_strings(name)\n    \"('foo', 'bar')\"\n    >>> import pandas as pd\n    >>> name = (1, pd.Timestamp('2017-02-01 00:00:00'))\n    >>> _column_name_to_strings(name)\n    \"('1', '2017-02-01 00:00:00')\"\n    \"\"\"\n    if isinstance(name, str):\n        return name\n    elif isinstance(name, bytes):\n        # XXX: should we assume that bytes in Python 3 are UTF-8?\n        return name.decode('utf8')\n    elif isinstance(name, tuple):\n        return str(tuple(map(_column_name_to_strings, name)))\n    elif isinstance(name, Sequence):\n        raise TypeError(\"Unsupported type for MultiIndex level\")\n    elif name is None:\n        return None\n    return str(name)\n\n\ndef _index_level_name(index, i, column_names):\n    \"\"\"Return the name of an index level or a default name if `index.name` is\n    None or is already a column name.\n\n    Parameters\n    ----------\n    index : pandas.Index\n    i : int\n\n    Returns\n    -------\n    name : str\n    \"\"\"\n    if index.name is not None and index.name not in column_names:\n        return _column_name_to_strings(index.name)\n    else:\n        return '__index_level_{:d}__'.format(i)\n\n\ndef _get_columns_to_convert(df, schema, preserve_index, columns):\n    columns = _resolve_columns_of_interest(df, schema, columns)\n\n    if not df.columns.is_unique:\n        raise ValueError(\n            'Duplicate column names found: {}'.format(list(df.columns))\n        )\n\n    if schema is not None:\n        return _get_columns_to_convert_given_schema(df, schema, preserve_index)\n\n    column_names = []\n\n    index_levels = (\n        _get_index_level_values(df.index) if preserve_index is not False\n        else []\n    )\n\n    columns_to_convert = []\n    convert_fields = []\n\n    for name in columns:\n        col = df[name]\n        name = _column_name_to_strings(name)\n\n        if _pandas_api.is_sparse(col):\n            raise TypeError(\n                \"Sparse pandas data (column {}) not supported.\".format(name))\n\n        columns_to_convert.append(col)\n        convert_fields.append(None)\n        column_names.append(name)\n\n    index_descriptors = []\n    index_column_names = []\n    for i, index_level in enumerate(index_levels):\n        name = _index_level_name(index_level, i, column_names)\n        if (isinstance(index_level, _pandas_api.pd.RangeIndex) and\n                preserve_index is None):\n            descr = _get_range_index_descriptor(index_level)\n        else:\n            columns_to_convert.append(index_level)\n            convert_fields.append(None)\n            descr = name\n            index_column_names.append(name)\n        index_descriptors.append(descr)\n\n    all_names = column_names + index_column_names\n\n    # all_names : all of the columns in the resulting table including the data\n    # columns and serialized index columns\n    # column_names : the names of the data columns\n    # index_column_names : the names of the serialized index columns\n    # index_descriptors : descriptions of each index to be used for\n    # reconstruction\n    # index_levels : the extracted index level values\n    # columns_to_convert : assembled raw data (both data columns and indexes)\n    # to be converted to Arrow format\n    # columns_fields : specified column to use for coercion / casting\n    # during serialization, if a Schema was provided\n    return (all_names, column_names, index_column_names, index_descriptors,\n            index_levels, columns_to_convert, convert_fields)\n\n\ndef _get_columns_to_convert_given_schema(df, schema, preserve_index):\n    \"\"\"\n    Specialized version of _get_columns_to_convert in case a Schema is\n    specified.\n    In that case, the Schema is used as the single point of truth for the\n    table structure (types, which columns are included, order of columns, ...).\n    \"\"\"\n    column_names = []\n    columns_to_convert = []\n    convert_fields = []\n    index_descriptors = []\n    index_column_names = []\n    index_levels = []\n\n    for name in schema.names:\n        try:\n            col = df[name]\n            is_index = False\n        except KeyError:\n            try:\n                col = _get_index_level(df, name)\n            except (KeyError, IndexError):\n                # name not found as index level\n                raise KeyError(\n                    \"name '{}' present in the specified schema is not found \"\n                    \"in the columns or index\".format(name))\n            if preserve_index is False:\n                raise ValueError(\n                    \"name '{}' present in the specified schema corresponds \"\n                    \"to the index, but 'preserve_index=False' was \"\n                    \"specified\".format(name))\n            elif (preserve_index is None and\n                    isinstance(col, _pandas_api.pd.RangeIndex)):\n                raise ValueError(\n                    \"name '{}' is present in the schema, but it is a \"\n                    \"RangeIndex which will not be converted as a column \"\n                    \"in the Table, but saved as metadata-only not in \"\n                    \"columns. Specify 'preserve_index=True' to force it \"\n                    \"being added as a column, or remove it from the \"\n                    \"specified schema\".format(name))\n            is_index = True\n\n        name = _column_name_to_strings(name)\n\n        if _pandas_api.is_sparse(col):\n            raise TypeError(\n                \"Sparse pandas data (column {}) not supported.\".format(name))\n\n        field = schema.field(name)\n        columns_to_convert.append(col)\n        convert_fields.append(field)\n        column_names.append(name)\n\n        if is_index:\n            index_column_names.append(name)\n            index_descriptors.append(name)\n            index_levels.append(col)\n\n    all_names = column_names + index_column_names\n\n    return (all_names, column_names, index_column_names, index_descriptors,\n            index_levels, columns_to_convert, convert_fields)\n\n\ndef _get_index_level(df, name):\n    \"\"\"\n    Get the index level of a DataFrame given 'name' (column name in an arrow\n    Schema).\n    \"\"\"\n    key = name\n    if name not in df.index.names and _is_generated_index_name(name):\n        # we know we have an autogenerated name => extract number and get\n        # the index level positionally\n        key = int(name[len(\"__index_level_\"):-2])\n    return df.index.get_level_values(key)\n\n\ndef _level_name(name):\n    # preserve type when default serializable, otherwise str it\n    try:\n        json.dumps(name)\n        return name\n    except TypeError:\n        return str(name)\n\n\ndef _get_range_index_descriptor(level):\n    # public start/stop/step attributes added in pandas 0.25.0\n    return {\n        'kind': 'range',\n        'name': _level_name(level.name),\n        'start': _pandas_api.get_rangeindex_attribute(level, 'start'),\n        'stop': _pandas_api.get_rangeindex_attribute(level, 'stop'),\n        'step': _pandas_api.get_rangeindex_attribute(level, 'step')\n    }\n\n\ndef _get_index_level_values(index):\n    n = len(getattr(index, 'levels', [index]))\n    return [index.get_level_values(i) for i in range(n)]\n\n\ndef _resolve_columns_of_interest(df, schema, columns):\n    if schema is not None and columns is not None:\n        raise ValueError('Schema and columns arguments are mutually '\n                         'exclusive, pass only one of them')\n    elif schema is not None:\n        columns = schema.names\n    elif columns is not None:\n        columns = [c for c in columns if c in df.columns]\n    else:\n        columns = df.columns\n\n    return columns\n\n\ndef dataframe_to_types(df, preserve_index, columns=None):\n    (all_names,\n     column_names,\n     _,\n     index_descriptors,\n     index_columns,\n     columns_to_convert,\n     _) = _get_columns_to_convert(df, None, preserve_index, columns)\n\n    types = []\n    # If pandas knows type, skip conversion\n    for c in columns_to_convert:\n        values = c.values\n        if _pandas_api.is_categorical(values):\n            type_ = pa.array(c, from_pandas=True).type\n        elif _pandas_api.is_extension_array_dtype(values):\n            empty = c.head(0) if isinstance(\n                c, _pandas_api.pd.Series) else c[:0]\n            type_ = pa.array(empty, from_pandas=True).type\n        else:\n            values, type_ = get_datetimetz_type(values, c.dtype, None)\n            type_ = pa.lib._ndarray_to_arrow_type(values, type_)\n            if type_ is None:\n                type_ = pa.array(c, from_pandas=True).type\n        types.append(type_)\n\n    metadata = construct_metadata(\n        columns_to_convert, df, column_names, index_columns,\n        index_descriptors, preserve_index, types\n    )\n\n    return all_names, types, metadata\n\n\ndef dataframe_to_arrays(df, schema, preserve_index, nthreads=1, columns=None,\n                        safe=True):\n    (all_names,\n     column_names,\n     index_column_names,\n     index_descriptors,\n     index_columns,\n     columns_to_convert,\n     convert_fields) = _get_columns_to_convert(df, schema, preserve_index,\n                                               columns)\n\n    # NOTE(wesm): If nthreads=None, then we use a heuristic to decide whether\n    # using a thread pool is worth it. Currently the heuristic is whether the\n    # nrows > 100 * ncols and ncols > 1.\n    if nthreads is None:\n        nrows, ncols = len(df), len(df.columns)\n        if nrows > ncols * 100 and ncols > 1:\n            nthreads = pa.cpu_count()\n        else:\n            nthreads = 1\n\n    def convert_column(col, field):\n        if field is None:\n            field_nullable = True\n            type_ = None\n        else:\n            field_nullable = field.nullable\n            type_ = field.type\n\n        try:\n            result = pa.array(col, type=type_, from_pandas=True, safe=safe)\n        except (pa.ArrowInvalid,\n                pa.ArrowNotImplementedError,\n                pa.ArrowTypeError) as e:\n            e.args += (\"Conversion failed for column {!s} with type {!s}\"\n                       .format(col.name, col.dtype),)\n            raise e\n        if not field_nullable and result.null_count > 0:\n            raise ValueError(\"Field {} was non-nullable but pandas column \"\n                             \"had {} null values\".format(str(field),\n                                                         result.null_count))\n        return result\n\n    def _can_definitely_zero_copy(arr):\n        return (isinstance(arr, np.ndarray) and\n                arr.flags.contiguous and\n                issubclass(arr.dtype.type, np.integer))\n\n    if nthreads == 1:\n        arrays = [convert_column(c, f)\n                  for c, f in zip(columns_to_convert, convert_fields)]\n    else:\n        arrays = []\n        with futures.ThreadPoolExecutor(nthreads) as executor:\n            for c, f in zip(columns_to_convert, convert_fields):\n                if _can_definitely_zero_copy(c.values):\n                    arrays.append(convert_column(c, f))\n                else:\n                    arrays.append(executor.submit(convert_column, c, f))\n\n        for i, maybe_fut in enumerate(arrays):\n            if isinstance(maybe_fut, futures.Future):\n                arrays[i] = maybe_fut.result()\n\n    types = [x.type for x in arrays]\n\n    if schema is None:\n        fields = []\n        for name, type_ in zip(all_names, types):\n            name = name if name is not None else 'None'\n            fields.append(pa.field(name, type_))\n        schema = pa.schema(fields)\n\n    pandas_metadata = construct_metadata(\n        columns_to_convert, df, column_names, index_columns,\n        index_descriptors, preserve_index, types\n    )\n    metadata = deepcopy(schema.metadata) if schema.metadata else dict()\n    metadata.update(pandas_metadata)\n    schema = schema.with_metadata(metadata)\n\n    # If dataframe is empty but with RangeIndex ->\n    # remember the length of the indexes\n    n_rows = None\n    if len(arrays) == 0:\n        try:\n            kind = index_descriptors[0][\"kind\"]\n            if kind == \"range\":\n                start = index_descriptors[0][\"start\"]\n                stop = index_descriptors[0][\"stop\"]\n                step = index_descriptors[0][\"step\"]\n                n_rows = len(range(start, stop, step))\n        except IndexError:\n            pass\n\n    return arrays, schema, n_rows\n\n\ndef get_datetimetz_type(values, dtype, type_):\n    if values.dtype.type != np.datetime64:\n        return values, type_\n\n    if _pandas_api.is_datetimetz(dtype) and type_ is None:\n        # If no user type passed, construct a tz-aware timestamp type\n        tz = dtype.tz\n        unit = dtype.unit\n        type_ = pa.timestamp(unit, tz)\n    elif type_ is None:\n        # Trust the NumPy dtype\n        type_ = pa.from_numpy_dtype(values.dtype)\n\n    return values, type_\n\n# ----------------------------------------------------------------------\n# Converting pyarrow.Table efficiently to pandas.DataFrame\n\n\ndef _reconstruct_block(item, columns=None, extension_columns=None, return_block=True):\n    \"\"\"\n    Construct a pandas Block from the `item` dictionary coming from pyarrow's\n    serialization or returned by arrow::python::ConvertTableToPandas.\n\n    This function takes care of converting dictionary types to pandas\n    categorical, Timestamp-with-timezones to the proper pandas Block, and\n    conversion to pandas ExtensionBlock\n\n    Parameters\n    ----------\n    item : dict\n        For basic types, this is a dictionary in the form of\n        {'block': np.ndarray of values, 'placement': pandas block placement}.\n        Additional keys are present for other types (dictionary, timezone,\n        object).\n    columns :\n        Column names of the table being constructed, used for extension types\n    extension_columns : dict\n        Dictionary of {column_name: pandas_dtype} that includes all columns\n        and corresponding dtypes that will be converted to a pandas\n        ExtensionBlock.\n\n    Returns\n    -------\n    pandas Block\n\n    \"\"\"\n    import pandas.core.internals as _int\n\n    block_arr = item.get('block', None)\n    placement = item['placement']\n    if 'dictionary' in item:\n        arr = _pandas_api.categorical_type.from_codes(\n            block_arr, categories=item['dictionary'],\n            ordered=item['ordered'])\n    elif 'timezone' in item:\n        unit, _ = np.datetime_data(block_arr.dtype)\n        dtype = make_datetimetz(unit, item['timezone'])\n        if _pandas_api.is_ge_v21():\n            arr = _pandas_api.pd.array(\n                block_arr.view(\"int64\"), dtype=dtype, copy=False\n            )\n        else:\n            arr = block_arr\n            if return_block:\n                block = _int.make_block(block_arr, placement=placement,\n                                        klass=_int.DatetimeTZBlock,\n                                        dtype=dtype)\n                return block\n    elif 'py_array' in item:\n        # create ExtensionBlock\n        arr = item['py_array']\n        assert len(placement) == 1\n        name = columns[placement[0]]\n        pandas_dtype = extension_columns[name]\n        if not hasattr(pandas_dtype, '__from_arrow__'):\n            raise ValueError(\"This column does not support to be converted \"\n                             \"to a pandas ExtensionArray\")\n        arr = pandas_dtype.__from_arrow__(arr)\n    else:\n        arr = block_arr\n\n    if return_block:\n        return _int.make_block(arr, placement=placement)\n    else:\n        return arr, placement\n\n\ndef make_datetimetz(unit, tz):\n    if _pandas_api.is_v1():\n        unit = 'ns'  # ARROW-3789: Coerce date/timestamp types to datetime64[ns]\n    tz = pa.lib.string_to_tzinfo(tz)\n    return _pandas_api.datetimetz_type(unit, tz=tz)\n\n\ndef table_to_dataframe(\n    options, table, categories=None, ignore_metadata=False, types_mapper=None\n):\n    all_columns = []\n    column_indexes = []\n    pandas_metadata = table.schema.pandas_metadata\n\n    if not ignore_metadata and pandas_metadata is not None:\n        all_columns = pandas_metadata['columns']\n        column_indexes = pandas_metadata.get('column_indexes', [])\n        index_descriptors = pandas_metadata['index_columns']\n        table = _add_any_metadata(table, pandas_metadata)\n        table, index = _reconstruct_index(table, index_descriptors,\n                                          all_columns, types_mapper)\n        ext_columns_dtypes = _get_extension_dtypes(\n            table, all_columns, types_mapper)\n    else:\n        index = _pandas_api.pd.RangeIndex(table.num_rows)\n        ext_columns_dtypes = _get_extension_dtypes(table, [], types_mapper)\n\n    _check_data_column_metadata_consistency(all_columns)\n    columns = _deserialize_column_index(table, all_columns, column_indexes)\n\n    column_names = table.column_names\n    result = pa.lib.table_to_blocks(options, table, categories,\n                                    list(ext_columns_dtypes.keys()))\n    if _pandas_api.is_ge_v3():\n        from pandas.api.internals import create_dataframe_from_blocks\n\n        blocks = [\n            _reconstruct_block(\n                item, column_names, ext_columns_dtypes, return_block=False)\n            for item in result\n        ]\n        df = create_dataframe_from_blocks(blocks, index=index, columns=columns)\n        return df\n    else:\n        from pandas.core.internals import BlockManager\n        from pandas import DataFrame\n\n        blocks = [\n            _reconstruct_block(item, column_names, ext_columns_dtypes)\n            for item in result\n        ]\n        axes = [columns, index]\n        mgr = BlockManager(blocks, axes)\n        if _pandas_api.is_ge_v21():\n            df = DataFrame._from_mgr(mgr, mgr.axes)\n        else:\n            df = DataFrame(mgr)\n        return df\n\n\n# Set of the string repr of all numpy dtypes that can be stored in a pandas\n# dataframe (complex not included since not supported by Arrow)\n_pandas_supported_numpy_types = {\n    \"int8\", \"int16\", \"int32\", \"int64\",\n    \"uint8\", \"uint16\", \"uint32\", \"uint64\",\n    \"float16\", \"float32\", \"float64\",\n    \"object\", \"bool\"\n}\n\n\ndef _get_extension_dtypes(table, columns_metadata, types_mapper=None):\n    \"\"\"\n    Based on the stored column pandas metadata and the extension types\n    in the arrow schema, infer which columns should be converted to a\n    pandas extension dtype.\n\n    The 'numpy_type' field in the column metadata stores the string\n    representation of the original pandas dtype (and, despite its name,\n    not the 'pandas_type' field).\n    Based on this string representation, a pandas/numpy dtype is constructed\n    and then we can check if this dtype supports conversion from arrow.\n\n    \"\"\"\n    ext_columns = {}\n\n    # older pandas version that does not yet support extension dtypes\n    if _pandas_api.extension_dtype is None:\n        return ext_columns\n\n    # infer the extension columns from the pandas metadata\n    for col_meta in columns_metadata:\n        try:\n            name = col_meta['field_name']\n        except KeyError:\n            name = col_meta['name']\n        dtype = col_meta['numpy_type']\n\n        if dtype not in _pandas_supported_numpy_types:\n            # pandas_dtype is expensive, so avoid doing this for types\n            # that are certainly numpy dtypes\n            pandas_dtype = _pandas_api.pandas_dtype(dtype)\n            if isinstance(pandas_dtype, _pandas_api.extension_dtype):\n                if hasattr(pandas_dtype, \"__from_arrow__\"):\n                    ext_columns[name] = pandas_dtype\n\n    # infer from extension type in the schema\n    for field in table.schema:\n        typ = field.type\n        if isinstance(typ, pa.BaseExtensionType):\n            try:\n                pandas_dtype = typ.to_pandas_dtype()\n            except NotImplementedError:\n                pass\n            else:\n                ext_columns[field.name] = pandas_dtype\n\n    # use the specified mapping of built-in arrow types to pandas dtypes\n    if types_mapper:\n        for field in table.schema:\n            typ = field.type\n            pandas_dtype = types_mapper(typ)\n            if pandas_dtype is not None:\n                ext_columns[field.name] = pandas_dtype\n\n    return ext_columns\n\n\ndef _check_data_column_metadata_consistency(all_columns):\n    # It can never be the case in a released version of pyarrow that\n    # c['name'] is None *and* 'field_name' is not a key in the column metadata,\n    # because the change to allow c['name'] to be None and the change to add\n    # 'field_name' are in the same release (0.8.0)\n    assert all(\n        (c['name'] is None and 'field_name' in c) or c['name'] is not None\n        for c in all_columns\n    )\n\n\ndef _deserialize_column_index(block_table, all_columns, column_indexes):\n    if all_columns:\n        columns_name_dict = {\n            c.get('field_name', _column_name_to_strings(c['name'])): c['name']\n            for c in all_columns\n        }\n        columns_values = [\n            columns_name_dict.get(name, name) for name in block_table.column_names\n        ]\n    else:\n        columns_values = block_table.column_names\n\n    # Construct the base index\n    if len(column_indexes) > 1:\n        # If we're passed multiple column indexes then evaluate with\n        # ast.literal_eval, since the column index values show up as a list of\n        # tuples\n        columns = _pandas_api.pd.MultiIndex.from_tuples(\n            list(map(ast.literal_eval, columns_values)),\n            names=[col_index['name'] for col_index in column_indexes],\n        )\n    else:\n        columns = _pandas_api.pd.Index(\n            columns_values, name=column_indexes[0][\"name\"] if column_indexes else None\n        )\n\n    # if we're reconstructing the index\n    if len(column_indexes) > 0:\n        columns = _reconstruct_columns_from_metadata(columns, column_indexes)\n\n    return columns\n\n\ndef _reconstruct_index(table, index_descriptors, all_columns, types_mapper=None):\n    # 0. 'field_name' is the name of the column in the arrow Table\n    # 1. 'name' is the user-facing name of the column, that is, it came from\n    #    pandas\n    # 2. 'field_name' and 'name' differ for index columns\n    # 3. We fall back on c['name'] for backwards compatibility\n    field_name_to_metadata = {\n        c.get('field_name', c['name']): c\n        for c in all_columns\n    }\n\n    # Build up a list of index columns and names while removing those columns\n    # from the original table\n    index_arrays = []\n    index_names = []\n    result_table = table\n    for descr in index_descriptors:\n        if isinstance(descr, str):\n            result_table, index_level, index_name = _extract_index_level(\n                table, result_table, descr, field_name_to_metadata, types_mapper)\n            if index_level is None:\n                # ARROW-1883: the serialized index column was not found\n                continue\n        elif descr['kind'] == 'range':\n            index_name = descr['name']\n            index_level = _pandas_api.pd.RangeIndex(descr['start'],\n                                                    descr['stop'],\n                                                    step=descr['step'],\n                                                    name=index_name)\n            if len(index_level) != len(table):\n                # Possibly the result of munged metadata\n                continue\n        else:\n            raise ValueError(\"Unrecognized index kind: {}\"\n                             .format(descr['kind']))\n        index_arrays.append(index_level)\n        index_names.append(index_name)\n\n    pd = _pandas_api.pd\n\n    # Reconstruct the row index\n    if len(index_arrays) > 1:\n        index = pd.MultiIndex.from_arrays(index_arrays, names=index_names)\n    elif len(index_arrays) == 1:\n        index = index_arrays[0]\n        if not isinstance(index, pd.Index):\n            # Box anything that wasn't boxed above\n            index = pd.Index(index, name=index_names[0])\n    else:\n        index = pd.RangeIndex(table.num_rows)\n\n    return result_table, index\n\n\ndef _extract_index_level(table, result_table, field_name,\n                         field_name_to_metadata, types_mapper=None):\n    logical_name = field_name_to_metadata[field_name]['name']\n    index_name = _backwards_compatible_index_name(field_name, logical_name)\n    i = table.schema.get_field_index(field_name)\n\n    if i == -1:\n        # The serialized index column was removed by the user\n        return result_table, None, None\n\n    col = table.column(i)\n    index_level = col.to_pandas(types_mapper=types_mapper)\n    index_level.name = None\n    result_table = result_table.remove_column(\n        result_table.schema.get_field_index(field_name)\n    )\n    return result_table, index_level, index_name\n\n\ndef _backwards_compatible_index_name(raw_name, logical_name):\n    \"\"\"Compute the name of an index column that is compatible with older\n    versions of :mod:`pyarrow`.\n\n    Parameters\n    ----------\n    raw_name : str\n    logical_name : str\n\n    Returns\n    -------\n    result : str\n\n    Notes\n    -----\n    * Part of :func:`~pyarrow.pandas_compat.table_to_blockmanager`\n    \"\"\"\n    # Part of table_to_blockmanager\n    if raw_name == logical_name and _is_generated_index_name(raw_name):\n        return None\n    else:\n        return logical_name\n\n\ndef _is_generated_index_name(name):\n    pattern = r'^__index_level_\\d+__$'\n    return re.match(pattern, name) is not None\n\n\n_pandas_logical_type_map = {\n    'date': 'datetime64[D]',\n    'datetime': 'datetime64[ns]',\n    'datetimetz': 'datetime64[ns]',\n    'unicode': np.str_,\n    'bytes': np.bytes_,\n    'string': np.str_,\n    'integer': np.int64,\n    'floating': np.float64,\n    'decimal': np.object_,\n    'empty': np.object_,\n}\n\n\ndef _pandas_type_to_numpy_type(pandas_type):\n    \"\"\"Get the numpy dtype that corresponds to a pandas type.\n\n    Parameters\n    ----------\n    pandas_type : str\n        The result of a call to pandas.lib.infer_dtype.\n\n    Returns\n    -------\n    dtype : np.dtype\n        The dtype that corresponds to `pandas_type`.\n    \"\"\"\n    try:\n        return _pandas_logical_type_map[pandas_type]\n    except KeyError:\n        if 'mixed' in pandas_type:\n            # catching 'mixed', 'mixed-integer' and 'mixed-integer-float'\n            return np.object_\n        return np.dtype(pandas_type)\n\n\ndef _reconstruct_columns_from_metadata(columns, column_indexes):\n    \"\"\"Construct a pandas MultiIndex from `columns` and column index metadata\n    in `column_indexes`.\n\n    Parameters\n    ----------\n    columns : List[pd.Index]\n        The columns coming from a pyarrow.Table\n    column_indexes : List[Dict[str, str]]\n        The column index metadata deserialized from the JSON schema metadata\n        in a :class:`~pyarrow.Table`.\n\n    Returns\n    -------\n    result : MultiIndex\n        The index reconstructed using `column_indexes` metadata with levels of\n        the correct type.\n\n    Notes\n    -----\n    * Part of :func:`~pyarrow.pandas_compat.table_to_blockmanager`\n    \"\"\"\n    pd = _pandas_api.pd\n    # Get levels and labels, and provide sane defaults if the index has a\n    # single level to avoid if/else spaghetti.\n    levels = getattr(columns, 'levels', None) or [columns]\n    labels = getattr(columns, 'codes', None) or [None]\n\n    # Convert each level to the dtype provided in the metadata\n    levels_dtypes = [\n        (level, col_index.get('pandas_type', str(level.dtype)),\n         col_index.get('numpy_type', None))\n        for level, col_index in zip_longest(\n            levels, column_indexes, fillvalue={}\n        )\n    ]\n\n    new_levels = []\n    encoder = operator.methodcaller('encode', 'UTF-8')\n\n    for level, pandas_dtype, numpy_dtype in levels_dtypes:\n        dtype = _pandas_type_to_numpy_type(pandas_dtype)\n        # Since our metadata is UTF-8 encoded, Python turns things that were\n        # bytes into unicode strings when json.loads-ing them. We need to\n        # convert them back to bytes to preserve metadata.\n        if dtype == np.bytes_:\n            level = level.map(encoder)\n        # ARROW-13756: if index is timezone aware DataTimeIndex\n        if pandas_dtype == \"datetimetz\":\n            tz = pa.lib.string_to_tzinfo(\n                column_indexes[0]['metadata']['timezone'])\n            level = pd.to_datetime(level, utc=True).tz_convert(tz)\n            if _pandas_api.is_ge_v3():\n                # with pandas 3+, to_datetime returns a unit depending on the string\n                # data, so we restore it to the original unit from the metadata\n                level = level.as_unit(np.datetime_data(dtype)[0])\n        # GH-41503: if the column index was decimal, restore to decimal\n        elif pandas_dtype == \"decimal\":\n            level = _pandas_api.pd.Index([decimal.Decimal(i) for i in level])\n        elif level.dtype != dtype:\n            level = level.astype(dtype)\n        # ARROW-9096: if original DataFrame was upcast we keep that\n        if level.dtype != numpy_dtype and pandas_dtype != \"datetimetz\":\n            level = level.astype(numpy_dtype)\n\n        new_levels.append(level)\n\n    if len(new_levels) > 1:\n        return pd.MultiIndex(new_levels, labels, names=columns.names)\n    else:\n        return pd.Index(new_levels[0], dtype=new_levels[0].dtype, name=columns.name)\n\n\ndef _add_any_metadata(table, pandas_metadata):\n    modified_columns = {}\n    modified_fields = {}\n\n    schema = table.schema\n\n    index_columns = pandas_metadata['index_columns']\n    # only take index columns into account if they are an actual table column\n    index_columns = [idx_col for idx_col in index_columns\n                     if isinstance(idx_col, str)]\n    n_index_levels = len(index_columns)\n    n_columns = len(pandas_metadata['columns']) - n_index_levels\n\n    # Add time zones\n    for i, col_meta in enumerate(pandas_metadata['columns']):\n\n        raw_name = col_meta.get('field_name')\n        if not raw_name:\n            # deal with metadata written with arrow < 0.8 or fastparquet\n            raw_name = col_meta['name']\n            if i >= n_columns:\n                # index columns\n                raw_name = index_columns[i - n_columns]\n            if raw_name is None:\n                raw_name = 'None'\n\n        idx = schema.get_field_index(raw_name)\n        if idx != -1:\n            if col_meta['pandas_type'] == 'datetimetz':\n                col = table[idx]\n                if not isinstance(col.type, pa.lib.TimestampType):\n                    continue\n                metadata = col_meta['metadata']\n                if not metadata:\n                    continue\n                metadata_tz = metadata.get('timezone')\n                if metadata_tz and metadata_tz != col.type.tz:\n                    converted = col.to_pandas()\n                    tz_aware_type = pa.timestamp('ns', tz=metadata_tz)\n                    with_metadata = pa.Array.from_pandas(converted,\n                                                         type=tz_aware_type)\n\n                    modified_fields[idx] = pa.field(schema[idx].name,\n                                                    tz_aware_type)\n                    modified_columns[idx] = with_metadata\n\n    if len(modified_columns) > 0:\n        columns = []\n        fields = []\n        for i in range(len(table.schema)):\n            if i in modified_columns:\n                columns.append(modified_columns[i])\n                fields.append(modified_fields[i])\n            else:\n                columns.append(table[i])\n                fields.append(table.schema[i])\n        return pa.Table.from_arrays(columns, schema=pa.schema(fields))\n    else:\n        return table\n\n\n# ----------------------------------------------------------------------\n# Helper functions used in lib\n\n\ndef make_tz_aware(series, tz):\n    \"\"\"\n    Make a datetime64 Series timezone-aware for the given tz\n    \"\"\"\n    tz = pa.lib.string_to_tzinfo(tz)\n    series = (series.dt.tz_localize('utc')\n                    .dt.tz_convert(tz))\n    return series\n", "python/pyarrow/conftest.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\nimport pyarrow as pa\nfrom pyarrow import Codec\nfrom pyarrow import fs\n\nimport numpy as np\n\ngroups = [\n    'acero',\n    'azure',\n    'brotli',\n    'bz2',\n    'cython',\n    'dataset',\n    'hypothesis',\n    'fastparquet',\n    'gandiva',\n    'gcs',\n    'gdb',\n    'gzip',\n    'hdfs',\n    'large_memory',\n    'lz4',\n    'memory_leak',\n    'nopandas',\n    'orc',\n    'pandas',\n    'parquet',\n    'parquet_encryption',\n    's3',\n    'snappy',\n    'substrait',\n    'flight',\n    'slow',\n    'requires_testing_data',\n    'zstd',\n]\n\ndefaults = {\n    'acero': False,\n    'azure': False,\n    'brotli': Codec.is_available('brotli'),\n    'bz2': Codec.is_available('bz2'),\n    'cython': False,\n    'dataset': False,\n    'fastparquet': False,\n    'flight': False,\n    'gandiva': False,\n    'gcs': False,\n    'gdb': True,\n    'gzip': Codec.is_available('gzip'),\n    'hdfs': False,\n    'hypothesis': False,\n    'large_memory': False,\n    'lz4': Codec.is_available('lz4'),\n    'memory_leak': False,\n    'nopandas': False,\n    'orc': False,\n    'pandas': False,\n    'parquet': False,\n    'parquet_encryption': False,\n    'requires_testing_data': True,\n    's3': False,\n    'slow': False,\n    'snappy': Codec.is_available('snappy'),\n    'substrait': False,\n    'zstd': Codec.is_available('zstd'),\n}\n\ntry:\n    import cython  # noqa\n    defaults['cython'] = True\nexcept ImportError:\n    pass\n\ntry:\n    import fastparquet  # noqa\n    defaults['fastparquet'] = True\nexcept ImportError:\n    pass\n\ntry:\n    import pyarrow.gandiva  # noqa\n    defaults['gandiva'] = True\nexcept ImportError:\n    pass\n\ntry:\n    import pyarrow.acero  # noqa\n    defaults['acero'] = True\nexcept ImportError:\n    pass\n\ntry:\n    import pyarrow.dataset  # noqa\n    defaults['dataset'] = True\nexcept ImportError:\n    pass\n\ntry:\n    import pyarrow.orc  # noqa\n    defaults['orc'] = True\nexcept ImportError:\n    pass\n\ntry:\n    import pandas  # noqa\n    defaults['pandas'] = True\nexcept ImportError:\n    defaults['nopandas'] = True\n\ntry:\n    import pyarrow.parquet  # noqa\n    defaults['parquet'] = True\nexcept ImportError:\n    pass\n\ntry:\n    import pyarrow.parquet.encryption  # noqa\n    defaults['parquet_encryption'] = True\nexcept ImportError:\n    pass\n\ntry:\n    import pyarrow.flight  # noqa\n    defaults['flight'] = True\nexcept ImportError:\n    pass\n\ntry:\n    from pyarrow.fs import AzureFileSystem  # noqa\n    defaults['azure'] = True\nexcept ImportError:\n    pass\n\ntry:\n    from pyarrow.fs import GcsFileSystem  # noqa\n    defaults['gcs'] = True\nexcept ImportError:\n    pass\n\ntry:\n    from pyarrow.fs import S3FileSystem  # noqa\n    defaults['s3'] = True\nexcept ImportError:\n    pass\n\ntry:\n    from pyarrow.fs import HadoopFileSystem  # noqa\n    defaults['hdfs'] = True\nexcept ImportError:\n    pass\n\ntry:\n    import pyarrow.substrait  # noqa\n    defaults['substrait'] = True\nexcept ImportError:\n    pass\n\n\n# Doctest should ignore files for the modules that are not built\ndef pytest_ignore_collect(path, config):\n    if config.option.doctestmodules:\n        # don't try to run doctests on the /tests directory\n        if \"/pyarrow/tests/\" in str(path):\n            return True\n\n        doctest_groups = [\n            'dataset',\n            'orc',\n            'parquet',\n            'flight',\n            'substrait',\n        ]\n\n        # handle cuda, flight, etc\n        for group in doctest_groups:\n            if 'pyarrow/{}'.format(group) in str(path):\n                if not defaults[group]:\n                    return True\n\n        if 'pyarrow/parquet/encryption' in str(path):\n            if not defaults['parquet_encryption']:\n                return True\n\n        if 'pyarrow/cuda' in str(path):\n            try:\n                import pyarrow.cuda  # noqa\n                return False\n            except ImportError:\n                return True\n\n        if 'pyarrow/fs' in str(path):\n            try:\n                from pyarrow.fs import S3FileSystem  # noqa\n                return False\n            except ImportError:\n                return True\n\n    if getattr(config.option, \"doctest_cython\", False):\n        if \"/pyarrow/tests/\" in str(path):\n            return True\n        if \"/pyarrow/_parquet_encryption\" in str(path):\n            return True\n\n    return False\n\n\n# Save output files from doctest examples into temp dir\n@pytest.fixture(autouse=True)\ndef _docdir(request):\n\n    # Trigger ONLY for the doctests\n    doctest_m = request.config.option.doctestmodules\n    doctest_c = getattr(request.config.option, \"doctest_cython\", False)\n\n    if doctest_m or doctest_c:\n\n        # Get the fixture dynamically by its name.\n        tmpdir = request.getfixturevalue('tmpdir')\n\n        # Chdir only for the duration of the test.\n        with tmpdir.as_cwd():\n            yield\n\n    else:\n        yield\n\n\n# Define doctest_namespace for fs module docstring import\n@pytest.fixture(autouse=True)\ndef add_fs(doctest_namespace, request, tmp_path):\n\n    # Trigger ONLY for the doctests\n    doctest_m = request.config.option.doctestmodules\n    doctest_c = getattr(request.config.option, \"doctest_cython\", False)\n\n    if doctest_m or doctest_c:\n        # fs import\n        doctest_namespace[\"fs\"] = fs\n\n        # Creation of an object and file with data\n        local = fs.LocalFileSystem()\n        path = tmp_path / 'pyarrow-fs-example.dat'\n        with local.open_output_stream(str(path)) as stream:\n            stream.write(b'data')\n        doctest_namespace[\"local\"] = local\n        doctest_namespace[\"local_path\"] = str(tmp_path)\n        doctest_namespace[\"path\"] = str(path)\n    yield\n\n\n# Define udf fixture for test_udf.py and test_substrait.py\n@pytest.fixture(scope=\"session\")\ndef unary_func_fixture():\n    \"\"\"\n    Register a unary scalar function.\n    \"\"\"\n    from pyarrow import compute as pc\n\n    def unary_function(ctx, x):\n        return pc.call_function(\"add\", [x, 1],\n                                memory_pool=ctx.memory_pool)\n    func_name = \"y=x+1\"\n    unary_doc = {\"summary\": \"add function\",\n                 \"description\": \"test add function\"}\n    pc.register_scalar_function(unary_function,\n                                func_name,\n                                unary_doc,\n                                {\"array\": pa.int64()},\n                                pa.int64())\n    return unary_function, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef unary_agg_func_fixture():\n    \"\"\"\n    Register a unary aggregate function (mean)\n    \"\"\"\n    from pyarrow import compute as pc\n\n    def func(ctx, x):\n        return pa.scalar(np.nanmean(x))\n\n    func_name = \"mean_udf\"\n    func_doc = {\"summary\": \"y=avg(x)\",\n                \"description\": \"find mean of x\"}\n\n    pc.register_aggregate_function(func,\n                                   func_name,\n                                   func_doc,\n                                   {\n                                       \"x\": pa.float64(),\n                                   },\n                                   pa.float64()\n                                   )\n    return func, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef varargs_agg_func_fixture():\n    \"\"\"\n    Register a unary aggregate function\n    \"\"\"\n    from pyarrow import compute as pc\n\n    def func(ctx, *args):\n        sum = 0.0\n        for arg in args:\n            sum += np.nanmean(arg)\n        return pa.scalar(sum)\n\n    func_name = \"sum_mean\"\n    func_doc = {\"summary\": \"Varargs aggregate\",\n                \"description\": \"Varargs aggregate\"}\n\n    pc.register_aggregate_function(func,\n                                   func_name,\n                                   func_doc,\n                                   {\n                                       \"x\": pa.int64(),\n                                       \"y\": pa.float64()\n                                   },\n                                   pa.float64()\n                                   )\n    return func, func_name\n", "python/pyarrow/csv.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nfrom pyarrow._csv import (  # noqa\n    ReadOptions, ParseOptions, ConvertOptions, ISO8601,\n    open_csv, read_csv, CSVStreamingReader, write_csv,\n    WriteOptions, CSVWriter, InvalidRow)\n", "python/pyarrow/jvm.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nFunctions to interact with Arrow memory allocated by Arrow Java.\n\nThese functions convert the objects holding the metadata, the actual\ndata is not copied at all.\n\nThis will only work with a JVM running in the same process such as provided\nthrough jpype. Modules that talk to a remote JVM like py4j will not work as the\nmemory addresses reported by them are not reachable in the python process.\n\"\"\"\n\nimport pyarrow as pa\n\n\nclass _JvmBufferNanny:\n    \"\"\"\n    An object that keeps a org.apache.arrow.memory.ArrowBuf's underlying\n    memory alive.\n    \"\"\"\n    ref_manager = None\n\n    def __init__(self, jvm_buf):\n        ref_manager = jvm_buf.getReferenceManager()\n        # Will raise a java.lang.IllegalArgumentException if the buffer\n        # is already freed.  It seems that exception cannot easily be\n        # caught...\n        ref_manager.retain()\n        self.ref_manager = ref_manager\n\n    def __del__(self):\n        if self.ref_manager is not None:\n            self.ref_manager.release()\n\n\ndef jvm_buffer(jvm_buf):\n    \"\"\"\n    Construct an Arrow buffer from org.apache.arrow.memory.ArrowBuf\n\n    Parameters\n    ----------\n\n    jvm_buf: org.apache.arrow.memory.ArrowBuf\n        Arrow Buffer representation on the JVM.\n\n    Returns\n    -------\n    pyarrow.Buffer\n        Python Buffer that references the JVM memory.\n    \"\"\"\n    nanny = _JvmBufferNanny(jvm_buf)\n    address = jvm_buf.memoryAddress()\n    size = jvm_buf.capacity()\n    return pa.foreign_buffer(address, size, base=nanny)\n\n\ndef _from_jvm_int_type(jvm_type):\n    \"\"\"\n    Convert a JVM int type to its Python equivalent.\n\n    Parameters\n    ----------\n    jvm_type : org.apache.arrow.vector.types.pojo.ArrowType$Int\n\n    Returns\n    -------\n    typ : pyarrow.DataType\n    \"\"\"\n\n    bit_width = jvm_type.getBitWidth()\n    if jvm_type.getIsSigned():\n        if bit_width == 8:\n            return pa.int8()\n        elif bit_width == 16:\n            return pa.int16()\n        elif bit_width == 32:\n            return pa.int32()\n        elif bit_width == 64:\n            return pa.int64()\n    else:\n        if bit_width == 8:\n            return pa.uint8()\n        elif bit_width == 16:\n            return pa.uint16()\n        elif bit_width == 32:\n            return pa.uint32()\n        elif bit_width == 64:\n            return pa.uint64()\n\n\ndef _from_jvm_float_type(jvm_type):\n    \"\"\"\n    Convert a JVM float type to its Python equivalent.\n\n    Parameters\n    ----------\n    jvm_type: org.apache.arrow.vector.types.pojo.ArrowType$FloatingPoint\n\n    Returns\n    -------\n    typ: pyarrow.DataType\n    \"\"\"\n    precision = jvm_type.getPrecision().toString()\n    if precision == 'HALF':\n        return pa.float16()\n    elif precision == 'SINGLE':\n        return pa.float32()\n    elif precision == 'DOUBLE':\n        return pa.float64()\n\n\ndef _from_jvm_time_type(jvm_type):\n    \"\"\"\n    Convert a JVM time type to its Python equivalent.\n\n    Parameters\n    ----------\n    jvm_type: org.apache.arrow.vector.types.pojo.ArrowType$Time\n\n    Returns\n    -------\n    typ: pyarrow.DataType\n    \"\"\"\n    time_unit = jvm_type.getUnit().toString()\n    if time_unit == 'SECOND':\n        assert jvm_type.getBitWidth() == 32\n        return pa.time32('s')\n    elif time_unit == 'MILLISECOND':\n        assert jvm_type.getBitWidth() == 32\n        return pa.time32('ms')\n    elif time_unit == 'MICROSECOND':\n        assert jvm_type.getBitWidth() == 64\n        return pa.time64('us')\n    elif time_unit == 'NANOSECOND':\n        assert jvm_type.getBitWidth() == 64\n        return pa.time64('ns')\n\n\ndef _from_jvm_timestamp_type(jvm_type):\n    \"\"\"\n    Convert a JVM timestamp type to its Python equivalent.\n\n    Parameters\n    ----------\n    jvm_type: org.apache.arrow.vector.types.pojo.ArrowType$Timestamp\n\n    Returns\n    -------\n    typ: pyarrow.DataType\n    \"\"\"\n    time_unit = jvm_type.getUnit().toString()\n    timezone = jvm_type.getTimezone()\n    if timezone is not None:\n        timezone = str(timezone)\n    if time_unit == 'SECOND':\n        return pa.timestamp('s', tz=timezone)\n    elif time_unit == 'MILLISECOND':\n        return pa.timestamp('ms', tz=timezone)\n    elif time_unit == 'MICROSECOND':\n        return pa.timestamp('us', tz=timezone)\n    elif time_unit == 'NANOSECOND':\n        return pa.timestamp('ns', tz=timezone)\n\n\ndef _from_jvm_date_type(jvm_type):\n    \"\"\"\n    Convert a JVM date type to its Python equivalent\n\n    Parameters\n    ----------\n    jvm_type: org.apache.arrow.vector.types.pojo.ArrowType$Date\n\n    Returns\n    -------\n    typ: pyarrow.DataType\n    \"\"\"\n    day_unit = jvm_type.getUnit().toString()\n    if day_unit == 'DAY':\n        return pa.date32()\n    elif day_unit == 'MILLISECOND':\n        return pa.date64()\n\n\ndef field(jvm_field):\n    \"\"\"\n    Construct a Field from a org.apache.arrow.vector.types.pojo.Field\n    instance.\n\n    Parameters\n    ----------\n    jvm_field: org.apache.arrow.vector.types.pojo.Field\n\n    Returns\n    -------\n    pyarrow.Field\n    \"\"\"\n    name = str(jvm_field.getName())\n    jvm_type = jvm_field.getType()\n\n    typ = None\n    if not jvm_type.isComplex():\n        type_str = jvm_type.getTypeID().toString()\n        if type_str == 'Null':\n            typ = pa.null()\n        elif type_str == 'Int':\n            typ = _from_jvm_int_type(jvm_type)\n        elif type_str == 'FloatingPoint':\n            typ = _from_jvm_float_type(jvm_type)\n        elif type_str == 'Utf8':\n            typ = pa.string()\n        elif type_str == 'Binary':\n            typ = pa.binary()\n        elif type_str == 'FixedSizeBinary':\n            typ = pa.binary(jvm_type.getByteWidth())\n        elif type_str == 'Bool':\n            typ = pa.bool_()\n        elif type_str == 'Time':\n            typ = _from_jvm_time_type(jvm_type)\n        elif type_str == 'Timestamp':\n            typ = _from_jvm_timestamp_type(jvm_type)\n        elif type_str == 'Date':\n            typ = _from_jvm_date_type(jvm_type)\n        elif type_str == 'Decimal':\n            typ = pa.decimal128(jvm_type.getPrecision(), jvm_type.getScale())\n        else:\n            raise NotImplementedError(\n                \"Unsupported JVM type: {}\".format(type_str))\n    else:\n        # TODO: The following JVM types are not implemented:\n        #       Struct, List, FixedSizeList, Union, Dictionary\n        raise NotImplementedError(\n            \"JVM field conversion only implemented for primitive types.\")\n\n    nullable = jvm_field.isNullable()\n    jvm_metadata = jvm_field.getMetadata()\n    if jvm_metadata.isEmpty():\n        metadata = None\n    else:\n        metadata = {str(entry.getKey()): str(entry.getValue())\n                    for entry in jvm_metadata.entrySet()}\n    return pa.field(name, typ, nullable, metadata)\n\n\ndef schema(jvm_schema):\n    \"\"\"\n    Construct a Schema from a org.apache.arrow.vector.types.pojo.Schema\n    instance.\n\n    Parameters\n    ----------\n    jvm_schema: org.apache.arrow.vector.types.pojo.Schema\n\n    Returns\n    -------\n    pyarrow.Schema\n    \"\"\"\n    fields = jvm_schema.getFields()\n    fields = [field(f) for f in fields]\n    jvm_metadata = jvm_schema.getCustomMetadata()\n    if jvm_metadata.isEmpty():\n        metadata = None\n    else:\n        metadata = {str(entry.getKey()): str(entry.getValue())\n                    for entry in jvm_metadata.entrySet()}\n    return pa.schema(fields, metadata)\n\n\ndef array(jvm_array):\n    \"\"\"\n    Construct an (Python) Array from its JVM equivalent.\n\n    Parameters\n    ----------\n    jvm_array : org.apache.arrow.vector.ValueVector\n\n    Returns\n    -------\n    array : Array\n    \"\"\"\n    if jvm_array.getField().getType().isComplex():\n        minor_type_str = jvm_array.getMinorType().toString()\n        raise NotImplementedError(\n            \"Cannot convert JVM Arrow array of type {},\"\n            \" complex types not yet implemented.\".format(minor_type_str))\n    dtype = field(jvm_array.getField()).type\n    buffers = [jvm_buffer(buf)\n               for buf in list(jvm_array.getBuffers(False))]\n\n    # If JVM has an empty Vector, buffer list will be empty so create manually\n    if len(buffers) == 0:\n        return pa.array([], type=dtype)\n\n    length = jvm_array.getValueCount()\n    null_count = jvm_array.getNullCount()\n    return pa.Array.from_buffers(dtype, length, buffers, null_count)\n\n\ndef record_batch(jvm_vector_schema_root):\n    \"\"\"\n    Construct a (Python) RecordBatch from a JVM VectorSchemaRoot\n\n    Parameters\n    ----------\n    jvm_vector_schema_root : org.apache.arrow.vector.VectorSchemaRoot\n\n    Returns\n    -------\n    record_batch: pyarrow.RecordBatch\n    \"\"\"\n    pa_schema = schema(jvm_vector_schema_root.getSchema())\n\n    arrays = []\n    for name in pa_schema.names:\n        arrays.append(array(jvm_vector_schema_root.getVector(name)))\n\n    return pa.RecordBatch.from_arrays(\n        arrays,\n        pa_schema.names,\n        metadata=pa_schema.metadata\n    )\n", "python/pyarrow/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# flake8: noqa\n\n\"\"\"\nPyArrow is the python implementation of Apache Arrow.\n\nApache Arrow is a cross-language development platform for in-memory data.\nIt specifies a standardized language-independent columnar memory format for\nflat and hierarchical data, organized for efficient analytic operations on\nmodern hardware. It also provides computational libraries and zero-copy\nstreaming messaging and interprocess communication.\n\nFor more information see the official page at https://arrow.apache.org\n\"\"\"\n\nimport gc as _gc\nimport importlib as _importlib\nimport os as _os\nimport platform as _platform\nimport sys as _sys\nimport warnings as _warnings\n\ntry:\n    from ._generated_version import version as __version__\nexcept ImportError:\n    # Package is not installed, parse git tag at runtime\n    try:\n        import setuptools_scm\n        # Code duplicated from setup.py to avoid a dependency on each other\n\n        def parse_git(root, **kwargs):\n            \"\"\"\n            Parse function for setuptools_scm that ignores tags for non-C++\n            subprojects, e.g. apache-arrow-js-XXX tags.\n            \"\"\"\n            from setuptools_scm.git import parse\n            kwargs['describe_command'] = \\\n                \"git describe --dirty --tags --long --match 'apache-arrow-[0-9]*.*'\"\n            return parse(root, **kwargs)\n        __version__ = setuptools_scm.get_version('../',\n                                                 parse=parse_git)\n    except ImportError:\n        __version__ = None\n\n# ARROW-8684: Disable GC while initializing Cython extension module,\n# to workaround Cython bug in https://github.com/cython/cython/issues/3603\n_gc_enabled = _gc.isenabled()\n_gc.disable()\nimport pyarrow.lib as _lib\nif _gc_enabled:\n    _gc.enable()\n\nfrom pyarrow.lib import (BuildInfo, RuntimeInfo, set_timezone_db_path,\n                         MonthDayNano, VersionInfo, cpp_build_info,\n                         cpp_version, cpp_version_info, runtime_info,\n                         cpu_count, set_cpu_count, enable_signal_handlers,\n                         io_thread_count, set_io_thread_count)\n\n\ndef show_versions():\n    \"\"\"\n    Print various version information, to help with error reporting.\n    \"\"\"\n    def print_entry(label, value):\n        print(f\"{label: <26}: {value: <8}\")\n\n    print(\"pyarrow version info\\n--------------------\")\n    print_entry(\"Package kind\", cpp_build_info.package_kind\n                if len(cpp_build_info.package_kind) > 0\n                else \"not indicated\")\n    print_entry(\"Arrow C++ library version\", cpp_build_info.version)\n    print_entry(\"Arrow C++ compiler\",\n                f\"{cpp_build_info.compiler_id} {cpp_build_info.compiler_version}\")\n    print_entry(\"Arrow C++ compiler flags\", cpp_build_info.compiler_flags)\n    print_entry(\"Arrow C++ git revision\", cpp_build_info.git_id)\n    print_entry(\"Arrow C++ git description\", cpp_build_info.git_description)\n    print_entry(\"Arrow C++ build type\", cpp_build_info.build_type)\n\n\ndef _module_is_available(module):\n    try:\n        _importlib.import_module(f'pyarrow.{module}')\n    except ImportError:\n        return False\n    else:\n        return True\n\n\ndef _filesystem_is_available(fs):\n    try:\n        import pyarrow.fs\n    except ImportError:\n        return False\n\n    try:\n        getattr(pyarrow.fs, fs)\n    except (ImportError, AttributeError):\n        return False\n    else:\n        return True\n\n\ndef show_info():\n    \"\"\"\n    Print detailed version and platform information, for error reporting\n    \"\"\"\n    show_versions()\n\n    def print_entry(label, value):\n        print(f\"  {label: <20}: {value: <8}\")\n\n    print(\"\\nPlatform:\")\n    print_entry(\"OS / Arch\", f\"{_platform.system()} {_platform.machine()}\")\n    print_entry(\"SIMD Level\", runtime_info().simd_level)\n    print_entry(\"Detected SIMD Level\", runtime_info().detected_simd_level)\n\n    pool = default_memory_pool()\n    print(\"\\nMemory:\")\n    print_entry(\"Default backend\", pool.backend_name)\n    print_entry(\"Bytes allocated\", f\"{pool.bytes_allocated()} bytes\")\n    print_entry(\"Max memory\", f\"{pool.max_memory()} bytes\")\n    print_entry(\"Supported Backends\", ', '.join(supported_memory_backends()))\n\n    print(\"\\nOptional modules:\")\n    modules = [\"csv\", \"cuda\", \"dataset\", \"feather\", \"flight\", \"fs\", \"gandiva\", \"json\",\n               \"orc\", \"parquet\"]\n    for module in modules:\n        status = \"Enabled\" if _module_is_available(module) else \"-\"\n        print(f\"  {module: <20}: {status: <8}\")\n\n    print(\"\\nFilesystems:\")\n    filesystems = [\"AzureFileSystem\", \"GcsFileSystem\",\n                   \"HadoopFileSystem\", \"S3FileSystem\"]\n    for fs in filesystems:\n        status = \"Enabled\" if _filesystem_is_available(fs) else \"-\"\n        print(f\"  {fs: <20}: {status: <8}\")\n\n    print(\"\\nCompression Codecs:\")\n    codecs = [\"brotli\", \"bz2\", \"gzip\", \"lz4_frame\", \"lz4\", \"snappy\", \"zstd\"]\n    for codec in codecs:\n        status = \"Enabled\" if Codec.is_available(codec) else \"-\"\n        print(f\"  {codec: <20}: {status: <8}\")\n\n\nfrom pyarrow.lib import (null, bool_,\n                         int8, int16, int32, int64,\n                         uint8, uint16, uint32, uint64,\n                         time32, time64, timestamp, date32, date64, duration,\n                         month_day_nano_interval,\n                         float16, float32, float64,\n                         binary, string, utf8, binary_view, string_view,\n                         large_binary, large_string, large_utf8,\n                         decimal128, decimal256,\n                         list_, large_list, list_view, large_list_view,\n                         map_, struct,\n                         union, sparse_union, dense_union,\n                         dictionary,\n                         run_end_encoded,\n                         fixed_shape_tensor,\n                         field,\n                         type_for_alias,\n                         DataType, DictionaryType, StructType,\n                         ListType, LargeListType, FixedSizeListType,\n                         ListViewType, LargeListViewType,\n                         MapType, UnionType, SparseUnionType, DenseUnionType,\n                         TimestampType, Time32Type, Time64Type, DurationType,\n                         FixedSizeBinaryType, Decimal128Type, Decimal256Type,\n                         BaseExtensionType, ExtensionType,\n                         RunEndEncodedType, FixedShapeTensorType,\n                         PyExtensionType, UnknownExtensionType,\n                         register_extension_type, unregister_extension_type,\n                         DictionaryMemo,\n                         KeyValueMetadata,\n                         Field,\n                         Schema,\n                         schema,\n                         unify_schemas,\n                         Array, Tensor,\n                         array, chunked_array, record_batch, nulls, repeat,\n                         SparseCOOTensor, SparseCSRMatrix, SparseCSCMatrix,\n                         SparseCSFTensor,\n                         infer_type, from_numpy_dtype,\n                         NullArray,\n                         NumericArray, IntegerArray, FloatingPointArray,\n                         BooleanArray,\n                         Int8Array, UInt8Array,\n                         Int16Array, UInt16Array,\n                         Int32Array, UInt32Array,\n                         Int64Array, UInt64Array,\n                         HalfFloatArray, FloatArray, DoubleArray,\n                         ListArray, LargeListArray, FixedSizeListArray,\n                         ListViewArray, LargeListViewArray,\n                         MapArray, UnionArray,\n                         BinaryArray, StringArray,\n                         LargeBinaryArray, LargeStringArray,\n                         BinaryViewArray, StringViewArray,\n                         FixedSizeBinaryArray,\n                         DictionaryArray,\n                         Date32Array, Date64Array, TimestampArray,\n                         Time32Array, Time64Array, DurationArray,\n                         MonthDayNanoIntervalArray,\n                         Decimal128Array, Decimal256Array, StructArray, ExtensionArray,\n                         RunEndEncodedArray, FixedShapeTensorArray,\n                         scalar, NA, _NULL as NULL, Scalar,\n                         NullScalar, BooleanScalar,\n                         Int8Scalar, Int16Scalar, Int32Scalar, Int64Scalar,\n                         UInt8Scalar, UInt16Scalar, UInt32Scalar, UInt64Scalar,\n                         HalfFloatScalar, FloatScalar, DoubleScalar,\n                         Decimal128Scalar, Decimal256Scalar,\n                         ListScalar, LargeListScalar, FixedSizeListScalar,\n                         ListViewScalar, LargeListViewScalar,\n                         Date32Scalar, Date64Scalar,\n                         Time32Scalar, Time64Scalar,\n                         TimestampScalar, DurationScalar,\n                         MonthDayNanoIntervalScalar,\n                         BinaryScalar, LargeBinaryScalar, BinaryViewScalar,\n                         StringScalar, LargeStringScalar, StringViewScalar,\n                         FixedSizeBinaryScalar, DictionaryScalar,\n                         MapScalar, StructScalar, UnionScalar,\n                         RunEndEncodedScalar, ExtensionScalar)\n\n# Buffers, allocation\nfrom pyarrow.lib import (DeviceAllocationType, Device, MemoryManager,\n                         default_cpu_memory_manager)\n\nfrom pyarrow.lib import (Buffer, ResizableBuffer, foreign_buffer, py_buffer,\n                         Codec, compress, decompress, allocate_buffer)\n\nfrom pyarrow.lib import (MemoryPool, LoggingMemoryPool, ProxyMemoryPool,\n                         total_allocated_bytes, set_memory_pool,\n                         default_memory_pool, system_memory_pool,\n                         jemalloc_memory_pool, mimalloc_memory_pool,\n                         logging_memory_pool, proxy_memory_pool,\n                         log_memory_allocations, jemalloc_set_decay_ms,\n                         supported_memory_backends)\n\n# I/O\nfrom pyarrow.lib import (NativeFile, PythonFile,\n                         BufferedInputStream, BufferedOutputStream, CacheOptions,\n                         CompressedInputStream, CompressedOutputStream,\n                         TransformInputStream, transcoding_input_stream,\n                         FixedSizeBufferWriter,\n                         BufferReader, BufferOutputStream,\n                         OSFile, MemoryMappedFile, memory_map,\n                         create_memory_map, MockOutputStream,\n                         input_stream, output_stream,\n                         have_libhdfs)\n\nfrom pyarrow.lib import (ChunkedArray, RecordBatch, Table, table,\n                         concat_arrays, concat_tables, TableGroupBy,\n                         RecordBatchReader)\n\n# Exceptions\nfrom pyarrow.lib import (ArrowCancelled,\n                         ArrowCapacityError,\n                         ArrowException,\n                         ArrowKeyError,\n                         ArrowIndexError,\n                         ArrowInvalid,\n                         ArrowIOError,\n                         ArrowMemoryError,\n                         ArrowNotImplementedError,\n                         ArrowTypeError,\n                         ArrowSerializationError)\n\nfrom pyarrow.ipc import serialize_pandas, deserialize_pandas\nimport pyarrow.ipc as ipc\n\nimport pyarrow.types as types\n\n\n# ----------------------------------------------------------------------\n# Deprecations\n\nfrom pyarrow.util import _deprecate_api, _deprecate_class\n\n\n# TODO: Deprecate these somehow in the pyarrow namespace\nfrom pyarrow.ipc import (Message, MessageReader, MetadataVersion,\n                         RecordBatchFileReader, RecordBatchFileWriter,\n                         RecordBatchStreamReader, RecordBatchStreamWriter)\n\n# ----------------------------------------------------------------------\n# Returning absolute path to the pyarrow include directory (if bundled, e.g. in\n# wheels)\n\n\ndef get_include():\n    \"\"\"\n    Return absolute path to directory containing Arrow C++ include\n    headers. Similar to numpy.get_include\n    \"\"\"\n    return _os.path.join(_os.path.dirname(__file__), 'include')\n\n\ndef _get_pkg_config_executable():\n    return _os.environ.get('PKG_CONFIG', 'pkg-config')\n\n\ndef _has_pkg_config(pkgname):\n    import subprocess\n    try:\n        return subprocess.call([_get_pkg_config_executable(),\n                                '--exists', pkgname]) == 0\n    except FileNotFoundError:\n        return False\n\n\ndef _read_pkg_config_variable(pkgname, cli_args):\n    import subprocess\n    cmd = [_get_pkg_config_executable(), pkgname] + cli_args\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE)\n    out, err = proc.communicate()\n    if proc.returncode != 0:\n        raise RuntimeError(\"pkg-config failed: \" + err.decode('utf8'))\n    return out.rstrip().decode('utf8')\n\n\ndef get_libraries():\n    \"\"\"\n    Return list of library names to include in the `libraries` argument for C\n    or Cython extensions using pyarrow\n    \"\"\"\n    return ['arrow_python', 'arrow']\n\n\ndef create_library_symlinks():\n    \"\"\"\n    With Linux and macOS wheels, the bundled shared libraries have an embedded\n    ABI version like libarrow.so.17 or libarrow.17.dylib and so linking to them\n    with -larrow won't work unless we create symlinks at locations like\n    site-packages/pyarrow/libarrow.so. This unfortunate workaround addresses\n    prior problems we had with shipping two copies of the shared libraries to\n    permit third party projects like turbodbc to build their C++ extensions\n    against the pyarrow wheels.\n\n    This function must only be invoked once and only when the shared libraries\n    are bundled with the Python package, which should only apply to wheel-based\n    installs. It requires write access to the site-packages/pyarrow directory\n    and so depending on your system may need to be run with root.\n    \"\"\"\n    import glob\n    if _sys.platform == 'win32':\n        return\n    package_cwd = _os.path.dirname(__file__)\n\n    if _sys.platform == 'linux':\n        bundled_libs = glob.glob(_os.path.join(package_cwd, '*.so.*'))\n\n        def get_symlink_path(hard_path):\n            return hard_path.rsplit('.', 1)[0]\n    else:\n        bundled_libs = glob.glob(_os.path.join(package_cwd, '*.*.dylib'))\n\n        def get_symlink_path(hard_path):\n            return '.'.join((hard_path.rsplit('.', 2)[0], 'dylib'))\n\n    for lib_hard_path in bundled_libs:\n        symlink_path = get_symlink_path(lib_hard_path)\n        if _os.path.exists(symlink_path):\n            continue\n        try:\n            _os.symlink(lib_hard_path, symlink_path)\n        except PermissionError:\n            print(\"Tried creating symlink {}. If you need to link to \"\n                  \"bundled shared libraries, run \"\n                  \"pyarrow.create_library_symlinks() as root\")\n\n\ndef get_library_dirs():\n    \"\"\"\n    Return lists of directories likely to contain Arrow C++ libraries for\n    linking C or Cython extensions using pyarrow\n    \"\"\"\n    package_cwd = _os.path.dirname(__file__)\n    library_dirs = [package_cwd]\n\n    def append_library_dir(library_dir):\n        if library_dir not in library_dirs:\n            library_dirs.append(library_dir)\n\n    # Search library paths via pkg-config. This is necessary if the user\n    # installed libarrow and the other shared libraries manually and they\n    # are not shipped inside the pyarrow package (see also ARROW-2976).\n    pkg_config_executable = _os.environ.get('PKG_CONFIG') or 'pkg-config'\n    for pkgname in [\"arrow\", \"arrow_python\"]:\n        if _has_pkg_config(pkgname):\n            library_dir = _read_pkg_config_variable(pkgname,\n                                                    [\"--libs-only-L\"])\n            # pkg-config output could be empty if Arrow is installed\n            # as a system package.\n            if library_dir:\n                if not library_dir.startswith(\"-L\"):\n                    raise ValueError(\n                        \"pkg-config --libs-only-L returned unexpected \"\n                        \"value {!r}\".format(library_dir))\n                append_library_dir(library_dir[2:])\n\n    if _sys.platform == 'win32':\n        # TODO(wesm): Is this necessary, or does setuptools within a conda\n        # installation add Library\\lib to the linker path for MSVC?\n        python_base_install = _os.path.dirname(_sys.executable)\n        library_dir = _os.path.join(python_base_install, 'Library', 'lib')\n\n        if _os.path.exists(_os.path.join(library_dir, 'arrow.lib')):\n            append_library_dir(library_dir)\n\n    # ARROW-4074: Allow for ARROW_HOME to be set to some other directory\n    if _os.environ.get('ARROW_HOME'):\n        append_library_dir(_os.path.join(_os.environ['ARROW_HOME'], 'lib'))\n    else:\n        # Python wheels bundle the Arrow libraries in the pyarrow directory.\n        append_library_dir(_os.path.dirname(_os.path.abspath(__file__)))\n\n    return library_dirs\n", "python/pyarrow/acero.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# ---------------------------------------------------------------------\n# Implement Internal ExecPlan bindings\n\n# cython: profile=False\n# distutils: language = c++\n# cython: language_level = 3\n\nfrom pyarrow.lib import Table\nfrom pyarrow.compute import Expression, field\n\ntry:\n    from pyarrow._acero import (  # noqa\n        Declaration,\n        ExecNodeOptions,\n        TableSourceNodeOptions,\n        FilterNodeOptions,\n        ProjectNodeOptions,\n        AggregateNodeOptions,\n        OrderByNodeOptions,\n        HashJoinNodeOptions,\n        AsofJoinNodeOptions,\n    )\nexcept ImportError as exc:\n    raise ImportError(\n        f\"The pyarrow installation is not built with support for 'acero' ({str(exc)})\"\n    ) from None\n\n\ntry:\n    import pyarrow.dataset as ds\n    from pyarrow._dataset import ScanNodeOptions\nexcept ImportError:\n    class DatasetModuleStub:\n        class Dataset:\n            pass\n\n        class InMemoryDataset:\n            pass\n    ds = DatasetModuleStub\n\n\ndef _dataset_to_decl(dataset, use_threads=True):\n    decl = Declaration(\"scan\", ScanNodeOptions(dataset, use_threads=use_threads))\n\n    # Get rid of special dataset columns\n    # \"__fragment_index\", \"__batch_index\", \"__last_in_fragment\", \"__filename\"\n    projections = [field(f) for f in dataset.schema.names]\n    decl = Declaration.from_sequence(\n        [decl, Declaration(\"project\", ProjectNodeOptions(projections))]\n    )\n\n    filter_expr = dataset._scan_options.get(\"filter\")\n    if filter_expr is not None:\n        # Filters applied in CScanNodeOptions are \"best effort\" for the scan node itself\n        # so we always need to inject an additional Filter node to apply them for real.\n        decl = Declaration.from_sequence(\n            [decl, Declaration(\"filter\", FilterNodeOptions(filter_expr))]\n        )\n\n    return decl\n\n\ndef _perform_join(join_type, left_operand, left_keys,\n                  right_operand, right_keys,\n                  left_suffix=None, right_suffix=None,\n                  use_threads=True, coalesce_keys=False,\n                  output_type=Table):\n    \"\"\"\n    Perform join of two tables or datasets.\n\n    The result will be an output table with the result of the join operation\n\n    Parameters\n    ----------\n    join_type : str\n        One of supported join types.\n    left_operand : Table or Dataset\n        The left operand for the join operation.\n    left_keys : str or list[str]\n        The left key (or keys) on which the join operation should be performed.\n    right_operand : Table or Dataset\n        The right operand for the join operation.\n    right_keys : str or list[str]\n        The right key (or keys) on which the join operation should be performed.\n    left_suffix : str, default None\n        Which suffix to add to left column names. This prevents confusion\n        when the columns in left and right operands have colliding names.\n    right_suffix : str, default None\n        Which suffix to add to the right column names. This prevents confusion\n        when the columns in left and right operands have colliding names.\n    use_threads : bool, default True\n        Whether to use multithreading or not.\n    coalesce_keys : bool, default False\n        If the duplicated keys should be omitted from one of the sides\n        in the join result.\n    output_type: Table or InMemoryDataset\n        The output type for the exec plan result.\n\n    Returns\n    -------\n    result_table : Table or InMemoryDataset\n    \"\"\"\n    if not isinstance(left_operand, (Table, ds.Dataset)):\n        raise TypeError(f\"Expected Table or Dataset, got {type(left_operand)}\")\n    if not isinstance(right_operand, (Table, ds.Dataset)):\n        raise TypeError(f\"Expected Table or Dataset, got {type(right_operand)}\")\n\n    # Prepare left and right tables Keys to send them to the C++ function\n    left_keys_order = {}\n    if not isinstance(left_keys, (tuple, list)):\n        left_keys = [left_keys]\n    for idx, key in enumerate(left_keys):\n        left_keys_order[key] = idx\n\n    right_keys_order = {}\n    if not isinstance(right_keys, (list, tuple)):\n        right_keys = [right_keys]\n    for idx, key in enumerate(right_keys):\n        right_keys_order[key] = idx\n\n    # By default expose all columns on both left and right table\n    left_columns = left_operand.schema.names\n    right_columns = right_operand.schema.names\n\n    # Pick the join type\n    if join_type == \"left semi\" or join_type == \"left anti\":\n        right_columns = []\n    elif join_type == \"right semi\" or join_type == \"right anti\":\n        left_columns = []\n    elif join_type == \"inner\" or join_type == \"left outer\":\n        right_columns = [\n            col for col in right_columns if col not in right_keys_order\n        ]\n    elif join_type == \"right outer\":\n        left_columns = [\n            col for col in left_columns if col not in left_keys_order\n        ]\n\n    # Turn the columns to vectors of FieldRefs\n    # and set aside indices of keys.\n    left_column_keys_indices = {}\n    for idx, colname in enumerate(left_columns):\n        if colname in left_keys:\n            left_column_keys_indices[colname] = idx\n    right_column_keys_indices = {}\n    for idx, colname in enumerate(right_columns):\n        if colname in right_keys:\n            right_column_keys_indices[colname] = idx\n\n    # Add the join node to the execplan\n    if isinstance(left_operand, ds.Dataset):\n        left_source = _dataset_to_decl(left_operand, use_threads=use_threads)\n    else:\n        left_source = Declaration(\"table_source\", TableSourceNodeOptions(left_operand))\n    if isinstance(right_operand, ds.Dataset):\n        right_source = _dataset_to_decl(right_operand, use_threads=use_threads)\n    else:\n        right_source = Declaration(\n            \"table_source\", TableSourceNodeOptions(right_operand)\n        )\n\n    if coalesce_keys:\n        join_opts = HashJoinNodeOptions(\n            join_type, left_keys, right_keys, left_columns, right_columns,\n            output_suffix_for_left=left_suffix or \"\",\n            output_suffix_for_right=right_suffix or \"\",\n        )\n    else:\n        join_opts = HashJoinNodeOptions(\n            join_type, left_keys, right_keys,\n            output_suffix_for_left=left_suffix or \"\",\n            output_suffix_for_right=right_suffix or \"\",\n        )\n    decl = Declaration(\n        \"hashjoin\", options=join_opts, inputs=[left_source, right_source]\n    )\n\n    if coalesce_keys and join_type == \"full outer\":\n        # In case of full outer joins, the join operation will output all columns\n        # so that we can coalesce the keys and exclude duplicates in a subsequent\n        # projection.\n        left_columns_set = set(left_columns)\n        right_columns_set = set(right_columns)\n        # Where the right table columns start.\n        right_operand_index = len(left_columns)\n        projected_col_names = []\n        projections = []\n        for idx, col in enumerate(left_columns + right_columns):\n            if idx < len(left_columns) and col in left_column_keys_indices:\n                # Include keys only once and coalesce left+right table keys.\n                projected_col_names.append(col)\n                # Get the index of the right key that is being paired\n                # with this left key. We do so by retrieving the name\n                # of the right key that is in the same position in the provided keys\n                # and then looking up the index for that name in the right table.\n                right_key_index = right_column_keys_indices[\n                    right_keys[left_keys_order[col]]]\n                projections.append(\n                    Expression._call(\"coalesce\", [\n                        Expression._field(idx), Expression._field(\n                            right_operand_index+right_key_index)\n                    ])\n                )\n            elif idx >= right_operand_index and col in right_column_keys_indices:\n                # Do not include right table keys. As they would lead to duplicated keys\n                continue\n            else:\n                # For all the other columns include them as they are.\n                # Just recompute the suffixes that the join produced as the projection\n                # would lose them otherwise.\n                if (\n                    left_suffix and idx < right_operand_index\n                    and col in right_columns_set\n                ):\n                    col += left_suffix\n                if (\n                    right_suffix and idx >= right_operand_index\n                    and col in left_columns_set\n                ):\n                    col += right_suffix\n                projected_col_names.append(col)\n                projections.append(\n                    Expression._field(idx)\n                )\n        projection = Declaration(\n            \"project\", ProjectNodeOptions(projections, projected_col_names)\n        )\n        decl = Declaration.from_sequence([decl, projection])\n\n    result_table = decl.to_table(use_threads=use_threads)\n\n    if output_type == Table:\n        return result_table\n    elif output_type == ds.InMemoryDataset:\n        return ds.InMemoryDataset(result_table)\n    else:\n        raise TypeError(\"Unsupported output type\")\n\n\ndef _perform_join_asof(left_operand, left_on, left_by,\n                       right_operand, right_on, right_by,\n                       tolerance, use_threads=True,\n                       output_type=Table):\n    \"\"\"\n    Perform asof join of two tables or datasets.\n\n    The result will be an output table with the result of the join operation\n\n    Parameters\n    ----------\n    left_operand : Table or Dataset\n        The left operand for the join operation.\n    left_on : str\n        The left key (or keys) on which the join operation should be performed.\n    left_by: str or list[str]\n        The left key (or keys) on which the join operation should be performed.\n    right_operand : Table or Dataset\n        The right operand for the join operation.\n    right_on : str or list[str]\n        The right key (or keys) on which the join operation should be performed.\n    right_by: str or list[str]\n        The right key (or keys) on which the join operation should be performed.\n    tolerance : int\n        The tolerance to use for the asof join. The tolerance is interpreted in\n        the same units as the \"on\" key.\n    output_type: Table or InMemoryDataset\n        The output type for the exec plan result.\n\n    Returns\n    -------\n    result_table : Table or InMemoryDataset\n    \"\"\"\n    if not isinstance(left_operand, (Table, ds.Dataset)):\n        raise TypeError(f\"Expected Table or Dataset, got {type(left_operand)}\")\n    if not isinstance(right_operand, (Table, ds.Dataset)):\n        raise TypeError(f\"Expected Table or Dataset, got {type(right_operand)}\")\n\n    if not isinstance(left_by, (tuple, list)):\n        left_by = [left_by]\n    if not isinstance(right_by, (tuple, list)):\n        right_by = [right_by]\n\n    # AsofJoin does not return on or by columns for right_operand.\n    right_columns = [\n        col for col in right_operand.schema.names\n        if col not in [right_on] + right_by\n    ]\n    columns_collisions = set(left_operand.schema.names) & set(right_columns)\n    if columns_collisions:\n        raise ValueError(\n            \"Columns {} present in both tables. AsofJoin does not support \"\n            \"column collisions.\".format(columns_collisions),\n        )\n\n    # Add the join node to the execplan\n    if isinstance(left_operand, ds.Dataset):\n        left_source = _dataset_to_decl(left_operand, use_threads=use_threads)\n    else:\n        left_source = Declaration(\n            \"table_source\", TableSourceNodeOptions(left_operand),\n        )\n    if isinstance(right_operand, ds.Dataset):\n        right_source = _dataset_to_decl(right_operand, use_threads=use_threads)\n    else:\n        right_source = Declaration(\n            \"table_source\", TableSourceNodeOptions(right_operand)\n        )\n\n    join_opts = AsofJoinNodeOptions(\n        left_on, left_by, right_on, right_by, tolerance\n    )\n    decl = Declaration(\n        \"asofjoin\", options=join_opts, inputs=[left_source, right_source]\n    )\n\n    result_table = decl.to_table(use_threads=use_threads)\n\n    if output_type == Table:\n        return result_table\n    elif output_type == ds.InMemoryDataset:\n        return ds.InMemoryDataset(result_table)\n    else:\n        raise TypeError(\"Unsupported output type\")\n\n\ndef _filter_table(table, expression):\n    \"\"\"Filter rows of a table based on the provided expression.\n\n    The result will be an output table with only the rows matching\n    the provided expression.\n\n    Parameters\n    ----------\n    table : Table or Dataset\n        Table or Dataset that should be filtered.\n    expression : Expression\n        The expression on which rows should be filtered.\n\n    Returns\n    -------\n    Table\n    \"\"\"\n    decl = Declaration.from_sequence([\n        Declaration(\"table_source\", options=TableSourceNodeOptions(table)),\n        Declaration(\"filter\", options=FilterNodeOptions(expression))\n    ])\n    return decl.to_table(use_threads=True)\n\n\ndef _sort_source(table_or_dataset, sort_keys, output_type=Table, **kwargs):\n\n    if isinstance(table_or_dataset, ds.Dataset):\n        data_source = _dataset_to_decl(table_or_dataset, use_threads=True)\n    else:\n        data_source = Declaration(\n            \"table_source\", TableSourceNodeOptions(table_or_dataset)\n        )\n\n    order_by = Declaration(\"order_by\", OrderByNodeOptions(sort_keys, **kwargs))\n\n    decl = Declaration.from_sequence([data_source, order_by])\n    result_table = decl.to_table(use_threads=True)\n\n    if output_type == Table:\n        return result_table\n    elif output_type == ds.InMemoryDataset:\n        return ds.InMemoryDataset(result_table)\n    else:\n        raise TypeError(\"Unsupported output type\")\n\n\ndef _group_by(table, aggregates, keys, use_threads=True):\n\n    decl = Declaration.from_sequence([\n        Declaration(\"table_source\", TableSourceNodeOptions(table)),\n        Declaration(\"aggregate\", AggregateNodeOptions(aggregates, keys=keys))\n    ])\n    return decl.to_table(use_threads=use_threads)\n", "python/pyarrow/cffi.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import absolute_import\n\nimport cffi\n\nc_source = \"\"\"\n    struct ArrowSchema {\n      // Array type description\n      const char* format;\n      const char* name;\n      const char* metadata;\n      int64_t flags;\n      int64_t n_children;\n      struct ArrowSchema** children;\n      struct ArrowSchema* dictionary;\n\n      // Release callback\n      void (*release)(struct ArrowSchema*);\n      // Opaque producer-specific data\n      void* private_data;\n    };\n\n    struct ArrowArray {\n      // Array data description\n      int64_t length;\n      int64_t null_count;\n      int64_t offset;\n      int64_t n_buffers;\n      int64_t n_children;\n      const void** buffers;\n      struct ArrowArray** children;\n      struct ArrowArray* dictionary;\n\n      // Release callback\n      void (*release)(struct ArrowArray*);\n      // Opaque producer-specific data\n      void* private_data;\n    };\n\n    struct ArrowArrayStream {\n      int (*get_schema)(struct ArrowArrayStream*, struct ArrowSchema* out);\n      int (*get_next)(struct ArrowArrayStream*, struct ArrowArray* out);\n\n      const char* (*get_last_error)(struct ArrowArrayStream*);\n\n      // Release callback\n      void (*release)(struct ArrowArrayStream*);\n      // Opaque producer-specific data\n      void* private_data;\n    };\n\n    typedef int32_t ArrowDeviceType;\n\n    struct ArrowDeviceArray {\n      struct ArrowArray array;\n      int64_t device_id;\n      ArrowDeviceType device_type;\n      void* sync_event;\n      int64_t reserved[3];\n    };\n    \"\"\"\n\n# TODO use out-of-line mode for faster import and avoid C parsing\nffi = cffi.FFI()\nffi.cdef(c_source)\n", "python/pyarrow/interchange/dataframe.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import annotations\nfrom typing import (\n    Any,\n    Iterable,\n    Optional,\n    Sequence,\n)\n\nimport pyarrow as pa\n\nfrom pyarrow.interchange.column import _PyArrowColumn\n\n\nclass _PyArrowDataFrame:\n    \"\"\"\n    A data frame class, with only the methods required by the interchange\n    protocol defined.\n\n    A \"data frame\" represents an ordered collection of named columns.\n    A column's \"name\" must be a unique string.\n    Columns may be accessed by name or by position.\n\n    This could be a public data frame class, or an object with the methods and\n    attributes defined on this DataFrame class could be returned from the\n    ``__dataframe__`` method of a public data frame class in a library adhering\n    to the dataframe interchange protocol specification.\n    \"\"\"\n\n    def __init__(\n        self, df: pa.Table | pa.RecordBatch,\n        nan_as_null: bool = False,\n        allow_copy: bool = True\n    ) -> None:\n        \"\"\"\n        Constructor - an instance of this (private) class is returned from\n        `pa.Table.__dataframe__` or `pa.RecordBatch.__dataframe__`.\n        \"\"\"\n        self._df = df\n        # ``nan_as_null`` is a keyword intended for the consumer to tell the\n        # producer to overwrite null values in the data with ``NaN`` (or\n        # ``NaT``).\n        if nan_as_null is True:\n            raise RuntimeError(\n                \"nan_as_null=True currently has no effect, \"\n                \"use the default nan_as_null=False\"\n            )\n        self._nan_as_null = nan_as_null\n        self._allow_copy = allow_copy\n\n    def __dataframe__(\n        self, nan_as_null: bool = False, allow_copy: bool = True\n    ) -> _PyArrowDataFrame:\n        \"\"\"\n        Construct a new exchange object, potentially changing the parameters.\n        ``nan_as_null`` is a keyword intended for the consumer to tell the\n        producer to overwrite null values in the data with ``NaN``.\n        It is intended for cases where the consumer does not support the bit\n        mask or byte mask that is the producer's native representation.\n        ``allow_copy`` is a keyword that defines whether or not the library is\n        allowed to make a copy of the data. For example, copying data would be\n        necessary if a library supports strided buffers, given that this\n        protocol specifies contiguous buffers.\n        \"\"\"\n        return _PyArrowDataFrame(self._df, nan_as_null, allow_copy)\n\n    @property\n    def metadata(self) -> dict[str, Any]:\n        \"\"\"\n        The metadata for the data frame, as a dictionary with string keys. The\n        contents of `metadata` may be anything, they are meant for a library\n        to store information that it needs to, e.g., roundtrip losslessly or\n        for two implementations to share data that is not (yet) part of the\n        interchange protocol specification. For avoiding collisions with other\n        entries, please add name the keys with the name of the library\n        followed by a period and the desired name, e.g, ``pandas.indexcol``.\n        \"\"\"\n        # The metadata for the data frame, as a dictionary with string keys.\n        # Add schema metadata here (pandas metadata or custom metadata)\n        if self._df.schema.metadata:\n            schema_metadata = {\"pyarrow.\" + k.decode('utf8'): v.decode('utf8')\n                               for k, v in self._df.schema.metadata.items()}\n            return schema_metadata\n        else:\n            return {}\n\n    def num_columns(self) -> int:\n        \"\"\"\n        Return the number of columns in the DataFrame.\n        \"\"\"\n        return self._df.num_columns\n\n    def num_rows(self) -> int:\n        \"\"\"\n        Return the number of rows in the DataFrame, if available.\n        \"\"\"\n        return self._df.num_rows\n\n    def num_chunks(self) -> int:\n        \"\"\"\n        Return the number of chunks the DataFrame consists of.\n        \"\"\"\n        if isinstance(self._df, pa.RecordBatch):\n            return 1\n        else:\n            # pyarrow.Table can have columns with different number\n            # of chunks so we take the number of chunks that\n            # .to_batches() returns as it takes the min chunk size\n            # of all the columns (to_batches is a zero copy method)\n            batches = self._df.to_batches()\n            return len(batches)\n\n    def column_names(self) -> Iterable[str]:\n        \"\"\"\n        Return an iterator yielding the column names.\n        \"\"\"\n        return self._df.schema.names\n\n    def get_column(self, i: int) -> _PyArrowColumn:\n        \"\"\"\n        Return the column at the indicated position.\n        \"\"\"\n        return _PyArrowColumn(self._df.column(i),\n                              allow_copy=self._allow_copy)\n\n    def get_column_by_name(self, name: str) -> _PyArrowColumn:\n        \"\"\"\n        Return the column whose name is the indicated name.\n        \"\"\"\n        return _PyArrowColumn(self._df.column(name),\n                              allow_copy=self._allow_copy)\n\n    def get_columns(self) -> Iterable[_PyArrowColumn]:\n        \"\"\"\n        Return an iterator yielding the columns.\n        \"\"\"\n        return [\n            _PyArrowColumn(col, allow_copy=self._allow_copy)\n            for col in self._df.columns\n        ]\n\n    def select_columns(self, indices: Sequence[int]) -> _PyArrowDataFrame:\n        \"\"\"\n        Create a new DataFrame by selecting a subset of columns by index.\n        \"\"\"\n        return _PyArrowDataFrame(\n            self._df.select(list(indices)), self._nan_as_null, self._allow_copy\n        )\n\n    def select_columns_by_name(\n        self, names: Sequence[str]\n    ) -> _PyArrowDataFrame:\n        \"\"\"\n        Create a new DataFrame by selecting a subset of columns by name.\n        \"\"\"\n        return _PyArrowDataFrame(\n            self._df.select(list(names)), self._nan_as_null, self._allow_copy\n        )\n\n    def get_chunks(\n        self, n_chunks: Optional[int] = None\n    ) -> Iterable[_PyArrowDataFrame]:\n        \"\"\"\n        Return an iterator yielding the chunks.\n\n        By default (None), yields the chunks that the data is stored as by the\n        producer. If given, ``n_chunks`` must be a multiple of\n        ``self.num_chunks()``, meaning the producer must subdivide each chunk\n        before yielding it.\n\n        Note that the producer must ensure that all columns are chunked the\n        same way.\n        \"\"\"\n        # Subdivide chunks\n        if n_chunks and n_chunks > 1:\n            chunk_size = self.num_rows() // n_chunks\n            if self.num_rows() % n_chunks != 0:\n                chunk_size += 1\n            if isinstance(self._df, pa.Table):\n                batches = self._df.to_batches(max_chunksize=chunk_size)\n            else:\n                batches = []\n                for start in range(0, chunk_size * n_chunks, chunk_size):\n                    batches.append(self._df.slice(start, chunk_size))\n            # In case when the size of the chunk is such that the resulting\n            # list is one less chunk then n_chunks -> append an empty chunk\n            if len(batches) == n_chunks - 1:\n                batches.append(pa.record_batch([[]], schema=self._df.schema))\n        # yields the chunks that the data is stored as\n        else:\n            if isinstance(self._df, pa.Table):\n                batches = self._df.to_batches()\n            else:\n                batches = [self._df]\n\n        # Create an iterator of RecordBatches\n        iterator = [_PyArrowDataFrame(batch,\n                                      self._nan_as_null,\n                                      self._allow_copy)\n                    for batch in batches]\n        return iterator\n", "python/pyarrow/interchange/column.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import annotations\n\nimport enum\nfrom typing import (\n    Any,\n    Dict,\n    Iterable,\n    Optional,\n    Tuple,\n)\n\nimport sys\nif sys.version_info >= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nimport pyarrow as pa\nimport pyarrow.compute as pc\nfrom pyarrow.interchange.buffer import _PyArrowBuffer\n\n\nclass DtypeKind(enum.IntEnum):\n    \"\"\"\n    Integer enum for data types.\n\n    Attributes\n    ----------\n    INT : int\n        Matches to signed integer data type.\n    UINT : int\n        Matches to unsigned integer data type.\n    FLOAT : int\n        Matches to floating point data type.\n    BOOL : int\n        Matches to boolean data type.\n    STRING : int\n        Matches to string data type (UTF-8 encoded).\n    DATETIME : int\n        Matches to datetime data type.\n    CATEGORICAL : int\n        Matches to categorical data type.\n    \"\"\"\n\n    INT = 0\n    UINT = 1\n    FLOAT = 2\n    BOOL = 20\n    STRING = 21  # UTF-8\n    DATETIME = 22\n    CATEGORICAL = 23\n\n\nDtype = Tuple[DtypeKind, int, str, str]  # see Column.dtype\n\n\n_PYARROW_KINDS = {\n    pa.int8(): (DtypeKind.INT, \"c\"),\n    pa.int16(): (DtypeKind.INT, \"s\"),\n    pa.int32(): (DtypeKind.INT, \"i\"),\n    pa.int64(): (DtypeKind.INT, \"l\"),\n    pa.uint8(): (DtypeKind.UINT, \"C\"),\n    pa.uint16(): (DtypeKind.UINT, \"S\"),\n    pa.uint32(): (DtypeKind.UINT, \"I\"),\n    pa.uint64(): (DtypeKind.UINT, \"L\"),\n    pa.float16(): (DtypeKind.FLOAT, \"e\"),\n    pa.float32(): (DtypeKind.FLOAT, \"f\"),\n    pa.float64(): (DtypeKind.FLOAT, \"g\"),\n    pa.bool_(): (DtypeKind.BOOL, \"b\"),\n    pa.string(): (DtypeKind.STRING, \"u\"),\n    pa.large_string(): (DtypeKind.STRING, \"U\"),\n}\n\n\nclass ColumnNullType(enum.IntEnum):\n    \"\"\"\n    Integer enum for null type representation.\n\n    Attributes\n    ----------\n    NON_NULLABLE : int\n        Non-nullable column.\n    USE_NAN : int\n        Use explicit float NaN value.\n    USE_SENTINEL : int\n        Sentinel value besides NaN.\n    USE_BITMASK : int\n        The bit is set/unset representing a null on a certain position.\n    USE_BYTEMASK : int\n        The byte is set/unset representing a null on a certain position.\n    \"\"\"\n\n    NON_NULLABLE = 0\n    USE_NAN = 1\n    USE_SENTINEL = 2\n    USE_BITMASK = 3\n    USE_BYTEMASK = 4\n\n\nclass ColumnBuffers(TypedDict):\n    # first element is a buffer containing the column data;\n    # second element is the data buffer's associated dtype\n    data: Tuple[_PyArrowBuffer, Dtype]\n\n    # first element is a buffer containing mask values indicating missing data;\n    # second element is the mask value buffer's associated dtype.\n    # None if the null representation is not a bit or byte mask\n    validity: Optional[Tuple[_PyArrowBuffer, Dtype]]\n\n    # first element is a buffer containing the offset values for\n    # variable-size binary data (e.g., variable-length strings);\n    # second element is the offsets buffer's associated dtype.\n    # None if the data buffer does not have an associated offsets buffer\n    offsets: Optional[Tuple[_PyArrowBuffer, Dtype]]\n\n\nclass CategoricalDescription(TypedDict):\n    # whether the ordering of dictionary indices is semantically meaningful\n    is_ordered: bool\n    # whether a dictionary-style mapping of categorical values to other objects\n    # exists\n    is_dictionary: bool\n    # Python-level only (e.g. ``{int: str}``).\n    # None if not a dictionary-style categorical.\n    categories: Optional[_PyArrowColumn]\n\n\nclass Endianness:\n    \"\"\"Enum indicating the byte-order of a data-type.\"\"\"\n\n    LITTLE = \"<\"\n    BIG = \">\"\n    NATIVE = \"=\"\n    NA = \"|\"\n\n\nclass NoBufferPresent(Exception):\n    \"\"\"Exception to signal that there is no requested buffer.\"\"\"\n\n\nclass _PyArrowColumn:\n    \"\"\"\n    A column object, with only the methods and properties required by the\n    interchange protocol defined.\n\n    A column can contain one or more chunks. Each chunk can contain up to three\n    buffers - a data buffer, a mask buffer (depending on null representation),\n    and an offsets buffer (if variable-size binary; e.g., variable-length\n    strings).\n\n    TBD: Arrow has a separate \"null\" dtype, and has no separate mask concept.\n         Instead, it seems to use \"children\" for both columns with a bit mask,\n         and for nested dtypes. Unclear whether this is elegant or confusing.\n         This design requires checking the null representation explicitly.\n\n         The Arrow design requires checking:\n         1. the ARROW_FLAG_NULLABLE (for sentinel values)\n         2. if a column has two children, combined with one of those children\n            having a null dtype.\n\n         Making the mask concept explicit seems useful. One null dtype would\n         not be enough to cover both bit and byte masks, so that would mean\n         even more checking if we did it the Arrow way.\n\n    TBD: there's also the \"chunk\" concept here, which is implicit in Arrow as\n         multiple buffers per array (= column here). Semantically it may make\n         sense to have both: chunks were meant for example for lazy evaluation\n         of data which doesn't fit in memory, while multiple buffers per column\n         could also come from doing a selection operation on a single\n         contiguous buffer.\n\n         Given these concepts, one would expect chunks to be all of the same\n         size (say a 10,000 row dataframe could have 10 chunks of 1,000 rows),\n         while multiple buffers could have data-dependent lengths. Not an issue\n         in pandas if one column is backed by a single NumPy array, but in\n         Arrow it seems possible.\n         Are multiple chunks *and* multiple buffers per column necessary for\n         the purposes of this interchange protocol, or must producers either\n         reuse the chunk concept for this or copy the data?\n\n    Note: this Column object can only be produced by ``__dataframe__``, so\n          doesn't need its own version or ``__column__`` protocol.\n    \"\"\"\n\n    def __init__(\n        self, column: pa.Array | pa.ChunkedArray, allow_copy: bool = True\n    ) -> None:\n        \"\"\"\n        Handles PyArrow Arrays and ChunkedArrays.\n        \"\"\"\n        # Store the column as a private attribute\n        if isinstance(column, pa.ChunkedArray):\n            if column.num_chunks == 1:\n                column = column.chunk(0)\n            else:\n                if not allow_copy:\n                    raise RuntimeError(\n                        \"Chunks will be combined and a copy is required which \"\n                        \"is forbidden by allow_copy=False\"\n                    )\n                column = column.combine_chunks()\n\n        self._allow_copy = allow_copy\n\n        if pa.types.is_boolean(column.type):\n            if not allow_copy:\n                raise RuntimeError(\n                    \"Boolean column will be casted to uint8 and a copy \"\n                    \"is required which is forbidden by allow_copy=False\"\n                )\n            self._dtype = self._dtype_from_arrowdtype(column.type, 8)\n            self._col = pc.cast(column, pa.uint8())\n        else:\n            self._col = column\n            dtype = self._col.type\n            try:\n                bit_width = dtype.bit_width\n            except ValueError:\n                # in case of a variable-length strings, considered as array\n                # of bytes (8 bits)\n                bit_width = 8\n            self._dtype = self._dtype_from_arrowdtype(dtype, bit_width)\n\n    def size(self) -> int:\n        \"\"\"\n        Size of the column, in elements.\n\n        Corresponds to DataFrame.num_rows() if column is a single chunk;\n        equal to size of this current chunk otherwise.\n\n        Is a method rather than a property because it may cause a (potentially\n        expensive) computation for some dataframe implementations.\n        \"\"\"\n        return len(self._col)\n\n    @property\n    def offset(self) -> int:\n        \"\"\"\n        Offset of first element.\n\n        May be > 0 if using chunks; for example for a column with N chunks of\n        equal size M (only the last chunk may be shorter),\n        ``offset = n * M``, ``n = 0 .. N-1``.\n        \"\"\"\n        return self._col.offset\n\n    @property\n    def dtype(self) -> Tuple[DtypeKind, int, str, str]:\n        \"\"\"\n        Dtype description as a tuple ``(kind, bit-width, format string,\n        endianness)``.\n\n        Bit-width : the number of bits as an integer\n        Format string : data type description format string in Apache Arrow C\n                        Data Interface format.\n        Endianness : current only native endianness (``=``) is supported\n\n        Notes:\n            - Kind specifiers are aligned with DLPack where possible (hence the\n              jump to 20, leave enough room for future extension)\n            - Masks must be specified as boolean with either bit width 1 (for\n              bit masks) or 8 (for byte masks).\n            - Dtype width in bits was preferred over bytes\n            - Endianness isn't too useful, but included now in case in the\n              future we need to support non-native endianness\n            - Went with Apache Arrow format strings over NumPy format strings\n              because they're more complete from a dataframe perspective\n            - Format strings are mostly useful for datetime specification, and\n              for categoricals.\n            - For categoricals, the format string describes the type of the\n              categorical in the data buffer. In case of a separate encoding of\n              the categorical (e.g. an integer to string mapping), this can\n              be derived from ``self.describe_categorical``.\n            - Data types not included: complex, Arrow-style null, binary,\n              decimal, and nested (list, struct, map, union) dtypes.\n        \"\"\"\n        return self._dtype\n\n    def _dtype_from_arrowdtype(\n        self, dtype: pa.DataType, bit_width: int\n    ) -> Tuple[DtypeKind, int, str, str]:\n        \"\"\"\n        See `self.dtype` for details.\n        \"\"\"\n        # Note: 'c' (complex) not handled yet (not in array spec v1).\n        #       'b', 'B' (bytes), 'S', 'a', (old-style string) 'V' (void)\n        #       not handled datetime and timedelta both map to datetime\n        #       (is timedelta handled?)\n\n        if pa.types.is_timestamp(dtype):\n            kind = DtypeKind.DATETIME\n            ts = dtype.unit[0]\n            tz = dtype.tz if dtype.tz else \"\"\n            f_string = \"ts{ts}:{tz}\".format(ts=ts, tz=tz)\n            return kind, bit_width, f_string, Endianness.NATIVE\n        elif pa.types.is_dictionary(dtype):\n            kind = DtypeKind.CATEGORICAL\n            arr = self._col\n            indices_dtype = arr.indices.type\n            _, f_string = _PYARROW_KINDS.get(indices_dtype)\n            return kind, bit_width, f_string, Endianness.NATIVE\n        else:\n            kind, f_string = _PYARROW_KINDS.get(dtype, (None, None))\n            if kind is None:\n                raise ValueError(\n                    f\"Data type {dtype} not supported by interchange protocol\")\n\n            return kind, bit_width, f_string, Endianness.NATIVE\n\n    @property\n    def describe_categorical(self) -> CategoricalDescription:\n        \"\"\"\n        If the dtype is categorical, there are two options:\n        - There are only values in the data buffer.\n        - There is a separate non-categorical Column encoding categorical\n          values.\n\n        Raises TypeError if the dtype is not categorical\n\n        Returns the dictionary with description on how to interpret the\n        data buffer:\n            - \"is_ordered\" : bool, whether the ordering of dictionary indices\n                             is semantically meaningful.\n            - \"is_dictionary\" : bool, whether a mapping of\n                                categorical values to other objects exists\n            - \"categories\" : Column representing the (implicit) mapping of\n                             indices to category values (e.g. an array of\n                             cat1, cat2, ...). None if not a dictionary-style\n                             categorical.\n\n        TBD: are there any other in-memory representations that are needed?\n        \"\"\"\n        arr = self._col\n        if not pa.types.is_dictionary(arr.type):\n            raise TypeError(\n                \"describe_categorical only works on a column with \"\n                \"categorical dtype!\"\n            )\n\n        return {\n            \"is_ordered\": self._col.type.ordered,\n            \"is_dictionary\": True,\n            \"categories\": _PyArrowColumn(arr.dictionary),\n        }\n\n    @property\n    def describe_null(self) -> Tuple[ColumnNullType, Any]:\n        \"\"\"\n        Return the missing value (or \"null\") representation the column dtype\n        uses, as a tuple ``(kind, value)``.\n\n        Value : if kind is \"sentinel value\", the actual value. If kind is a bit\n        mask or a byte mask, the value (0 or 1) indicating a missing value.\n        None otherwise.\n        \"\"\"\n        # In case of no missing values, we need to set ColumnNullType to\n        # non nullable as in the current __dataframe__ protocol bit/byte masks\n        # cannot be None\n        if self.null_count == 0:\n            return ColumnNullType.NON_NULLABLE, None\n        else:\n            return ColumnNullType.USE_BITMASK, 0\n\n    @property\n    def null_count(self) -> int:\n        \"\"\"\n        Number of null elements, if known.\n\n        Note: Arrow uses -1 to indicate \"unknown\", but None seems cleaner.\n        \"\"\"\n        arrow_null_count = self._col.null_count\n        n = arrow_null_count if arrow_null_count != -1 else None\n        return n\n\n    @property\n    def metadata(self) -> Dict[str, Any]:\n        \"\"\"\n        The metadata for the column. See `DataFrame.metadata` for more details.\n        \"\"\"\n        pass\n\n    def num_chunks(self) -> int:\n        \"\"\"\n        Return the number of chunks the column consists of.\n        \"\"\"\n        return 1\n\n    def get_chunks(\n        self, n_chunks: Optional[int] = None\n    ) -> Iterable[_PyArrowColumn]:\n        \"\"\"\n        Return an iterator yielding the chunks.\n\n        See `DataFrame.get_chunks` for details on ``n_chunks``.\n        \"\"\"\n        if n_chunks and n_chunks > 1:\n            chunk_size = self.size() // n_chunks\n            if self.size() % n_chunks != 0:\n                chunk_size += 1\n\n            array = self._col\n            i = 0\n            for start in range(0, chunk_size * n_chunks, chunk_size):\n                yield _PyArrowColumn(\n                    array.slice(start, chunk_size), self._allow_copy\n                )\n                i += 1\n        else:\n            yield self\n\n    def get_buffers(self) -> ColumnBuffers:\n        \"\"\"\n        Return a dictionary containing the underlying buffers.\n\n        The returned dictionary has the following contents:\n\n            - \"data\": a two-element tuple whose first element is a buffer\n                      containing the data and whose second element is the data\n                      buffer's associated dtype.\n            - \"validity\": a two-element tuple whose first element is a buffer\n                          containing mask values indicating missing data and\n                          whose second element is the mask value buffer's\n                          associated dtype. None if the null representation is\n                          not a bit or byte mask.\n            - \"offsets\": a two-element tuple whose first element is a buffer\n                         containing the offset values for variable-size binary\n                         data (e.g., variable-length strings) and whose second\n                         element is the offsets buffer's associated dtype. None\n                         if the data buffer does not have an associated offsets\n                         buffer.\n        \"\"\"\n        buffers: ColumnBuffers = {\n            \"data\": self._get_data_buffer(),\n            \"validity\": None,\n            \"offsets\": None,\n        }\n\n        try:\n            buffers[\"validity\"] = self._get_validity_buffer()\n        except NoBufferPresent:\n            pass\n\n        try:\n            buffers[\"offsets\"] = self._get_offsets_buffer()\n        except NoBufferPresent:\n            pass\n\n        return buffers\n\n    def _get_data_buffer(\n        self,\n    ) -> Tuple[_PyArrowBuffer, Any]:  # Any is for self.dtype tuple\n        \"\"\"\n        Return the buffer containing the data and the buffer's\n        associated dtype.\n        \"\"\"\n        array = self._col\n        dtype = self.dtype\n\n        # In case of dictionary arrays, use indices\n        # to define a buffer, codes are transferred through\n        # describe_categorical()\n        if pa.types.is_dictionary(array.type):\n            array = array.indices\n            dtype = _PyArrowColumn(array).dtype\n\n        n = len(array.buffers())\n        if n == 2:\n            return _PyArrowBuffer(array.buffers()[1]), dtype\n        elif n == 3:\n            return _PyArrowBuffer(array.buffers()[2]), dtype\n\n    def _get_validity_buffer(self) -> Tuple[_PyArrowBuffer, Any]:\n        \"\"\"\n        Return the buffer containing the mask values indicating missing data\n        and the buffer's associated dtype.\n        Raises NoBufferPresent if null representation is not a bit or byte\n        mask.\n        \"\"\"\n        # Define the dtype of the returned buffer\n        dtype = (DtypeKind.BOOL, 1, \"b\", Endianness.NATIVE)\n        array = self._col\n        buff = array.buffers()[0]\n        if buff:\n            return _PyArrowBuffer(buff), dtype\n        else:\n            raise NoBufferPresent(\n                \"There are no missing values so \"\n                \"does not have a separate mask\")\n\n    def _get_offsets_buffer(self) -> Tuple[_PyArrowBuffer, Any]:\n        \"\"\"\n        Return the buffer containing the offset values for variable-size binary\n        data (e.g., variable-length strings) and the buffer's associated dtype.\n        Raises NoBufferPresent if the data buffer does not have an associated\n        offsets buffer.\n        \"\"\"\n        array = self._col\n        n = len(array.buffers())\n        if n == 2:\n            raise NoBufferPresent(\n                \"This column has a fixed-length dtype so \"\n                \"it does not have an offsets buffer\"\n            )\n        elif n == 3:\n            # Define the dtype of the returned buffer\n            dtype = self._col.type\n            if pa.types.is_large_string(dtype):\n                dtype = (DtypeKind.INT, 64, \"l\", Endianness.NATIVE)\n            else:\n                dtype = (DtypeKind.INT, 32, \"i\", Endianness.NATIVE)\n            return _PyArrowBuffer(array.buffers()[1]), dtype\n", "python/pyarrow/interchange/buffer.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import annotations\nimport enum\n\nimport pyarrow as pa\n\n\nclass DlpackDeviceType(enum.IntEnum):\n    \"\"\"Integer enum for device type codes matching DLPack.\"\"\"\n\n    CPU = 1\n    CUDA = 2\n    CPU_PINNED = 3\n    OPENCL = 4\n    VULKAN = 7\n    METAL = 8\n    VPI = 9\n    ROCM = 10\n\n\nclass _PyArrowBuffer:\n    \"\"\"\n    Data in the buffer is guaranteed to be contiguous in memory.\n\n    Note that there is no dtype attribute present, a buffer can be thought of\n    as simply a block of memory. However, if the column that the buffer is\n    attached to has a dtype that's supported by DLPack and ``__dlpack__`` is\n    implemented, then that dtype information will be contained in the return\n    value from ``__dlpack__``.\n\n    This distinction is useful to support both data exchange via DLPack on a\n    buffer and (b) dtypes like variable-length strings which do not have a\n    fixed number of bytes per element.\n    \"\"\"\n\n    def __init__(self, x: pa.Buffer, allow_copy: bool = True) -> None:\n        \"\"\"\n        Handle PyArrow Buffers.\n        \"\"\"\n        self._x = x\n\n    @property\n    def bufsize(self) -> int:\n        \"\"\"\n        Buffer size in bytes.\n        \"\"\"\n        return self._x.size\n\n    @property\n    def ptr(self) -> int:\n        \"\"\"\n        Pointer to start of the buffer as an integer.\n        \"\"\"\n        return self._x.address\n\n    def __dlpack__(self):\n        \"\"\"\n        Produce DLPack capsule (see array API standard).\n\n        Raises:\n            - TypeError : if the buffer contains unsupported dtypes.\n            - NotImplementedError : if DLPack support is not implemented\n\n        Useful to have to connect to array libraries. Support optional because\n        it's not completely trivial to implement for a Python-only library.\n        \"\"\"\n        raise NotImplementedError(\"__dlpack__\")\n\n    def __dlpack_device__(self) -> tuple[DlpackDeviceType, int | None]:\n        \"\"\"\n        Device type and device ID for where the data in the buffer resides.\n        Uses device type codes matching DLPack.\n        Note: must be implemented even if ``__dlpack__`` is not.\n        \"\"\"\n        if self._x.is_cpu:\n            return (DlpackDeviceType.CPU, None)\n        else:\n            raise NotImplementedError(\"__dlpack_device__\")\n\n    def __repr__(self) -> str:\n        return (\n            \"PyArrowBuffer(\" +\n            str(\n                {\n                    \"bufsize\": self.bufsize,\n                    \"ptr\": self.ptr,\n                    \"device\": self.__dlpack_device__()[0].name,\n                }\n            ) +\n            \")\"\n        )\n", "python/pyarrow/interchange/from_dataframe.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import annotations\n\nfrom typing import (\n    Any,\n    Tuple,\n)\n\nfrom pyarrow.interchange.column import (\n    DtypeKind,\n    ColumnBuffers,\n    ColumnNullType,\n)\n\nimport pyarrow as pa\nimport re\n\nimport pyarrow.compute as pc\nfrom pyarrow.interchange.column import Dtype\n\n\n# A typing protocol could be added later to let Mypy validate code using\n# `from_dataframe` better.\nDataFrameObject = Any\nColumnObject = Any\nBufferObject = Any\n\n\n_PYARROW_DTYPES: dict[DtypeKind, dict[int, Any]] = {\n    DtypeKind.INT: {8: pa.int8(),\n                    16: pa.int16(),\n                    32: pa.int32(),\n                    64: pa.int64()},\n    DtypeKind.UINT: {8: pa.uint8(),\n                     16: pa.uint16(),\n                     32: pa.uint32(),\n                     64: pa.uint64()},\n    DtypeKind.FLOAT: {16: pa.float16(),\n                      32: pa.float32(),\n                      64: pa.float64()},\n    DtypeKind.BOOL: {1: pa.bool_(),\n                     8: pa.uint8()},\n    DtypeKind.STRING: {8: pa.string()},\n}\n\n\ndef from_dataframe(df: DataFrameObject, allow_copy=True) -> pa.Table:\n    \"\"\"\n    Build a ``pa.Table`` from any DataFrame supporting the interchange protocol.\n\n    Parameters\n    ----------\n    df : DataFrameObject\n        Object supporting the interchange protocol, i.e. `__dataframe__`\n        method.\n    allow_copy : bool, default: True\n        Whether to allow copying the memory to perform the conversion\n        (if false then zero-copy approach is requested).\n\n    Returns\n    -------\n    pa.Table\n\n    Examples\n    --------\n    >>> import pyarrow\n    >>> from pyarrow.interchange import from_dataframe\n\n    Convert a pandas dataframe to a pyarrow table:\n\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({\n    ...         \"n_attendees\": [100, 10, 1],\n    ...         \"country\": [\"Italy\", \"Spain\", \"Slovenia\"],\n    ...     })\n    >>> df\n       n_attendees   country\n    0          100     Italy\n    1           10     Spain\n    2            1  Slovenia\n    >>> from_dataframe(df)\n    pyarrow.Table\n    n_attendees: int64\n    country: large_string\n    ----\n    n_attendees: [[100,10,1]]\n    country: [[\"Italy\",\"Spain\",\"Slovenia\"]]\n    \"\"\"\n    if isinstance(df, pa.Table):\n        return df\n    elif isinstance(df, pa.RecordBatch):\n        return pa.Table.from_batches([df])\n\n    if not hasattr(df, \"__dataframe__\"):\n        raise ValueError(\"`df` does not support __dataframe__\")\n\n    return _from_dataframe(df.__dataframe__(allow_copy=allow_copy),\n                           allow_copy=allow_copy)\n\n\ndef _from_dataframe(df: DataFrameObject, allow_copy=True):\n    \"\"\"\n    Build a ``pa.Table`` from the DataFrame interchange object.\n\n    Parameters\n    ----------\n    df : DataFrameObject\n        Object supporting the interchange protocol, i.e. `__dataframe__`\n        method.\n    allow_copy : bool, default: True\n        Whether to allow copying the memory to perform the conversion\n        (if false then zero-copy approach is requested).\n\n    Returns\n    -------\n    pa.Table\n    \"\"\"\n    batches = []\n    for chunk in df.get_chunks():\n        batch = protocol_df_chunk_to_pyarrow(chunk, allow_copy)\n        batches.append(batch)\n\n    if not batches:\n        batch = protocol_df_chunk_to_pyarrow(df)\n        batches.append(batch)\n\n    return pa.Table.from_batches(batches)\n\n\ndef protocol_df_chunk_to_pyarrow(\n    df: DataFrameObject,\n    allow_copy: bool = True\n) -> pa.RecordBatch:\n    \"\"\"\n    Convert interchange protocol chunk to ``pa.RecordBatch``.\n\n    Parameters\n    ----------\n    df : DataFrameObject\n        Object supporting the interchange protocol, i.e. `__dataframe__`\n        method.\n    allow_copy : bool, default: True\n        Whether to allow copying the memory to perform the conversion\n        (if false then zero-copy approach is requested).\n\n    Returns\n    -------\n    pa.RecordBatch\n    \"\"\"\n    # We need a dict of columns here, with each column being a pa.Array\n    columns: dict[str, pa.Array] = {}\n    for name in df.column_names():\n        if not isinstance(name, str):\n            raise ValueError(f\"Column {name} is not a string\")\n        if name in columns:\n            raise ValueError(f\"Column {name} is not unique\")\n        col = df.get_column_by_name(name)\n        dtype = col.dtype[0]\n        if dtype in (\n            DtypeKind.INT,\n            DtypeKind.UINT,\n            DtypeKind.FLOAT,\n            DtypeKind.STRING,\n            DtypeKind.DATETIME,\n        ):\n            columns[name] = column_to_array(col, allow_copy)\n        elif dtype == DtypeKind.BOOL:\n            columns[name] = bool_column_to_array(col, allow_copy)\n        elif dtype == DtypeKind.CATEGORICAL:\n            columns[name] = categorical_column_to_dictionary(col, allow_copy)\n        else:\n            raise NotImplementedError(f\"Data type {dtype} not handled yet\")\n\n    return pa.RecordBatch.from_pydict(columns)\n\n\ndef column_to_array(\n    col: ColumnObject,\n    allow_copy: bool = True,\n) -> pa.Array:\n    \"\"\"\n    Convert a column holding one of the primitive dtypes to a PyArrow array.\n    A primitive type is one of: int, uint, float, bool (1 bit).\n\n    Parameters\n    ----------\n    col : ColumnObject\n    allow_copy : bool, default: True\n        Whether to allow copying the memory to perform the conversion\n        (if false then zero-copy approach is requested).\n\n    Returns\n    -------\n    pa.Array\n    \"\"\"\n    buffers = col.get_buffers()\n    data_type = col.dtype\n    data = buffers_to_array(buffers, data_type,\n                            col.size(),\n                            col.describe_null,\n                            col.offset,\n                            allow_copy)\n    return data\n\n\ndef bool_column_to_array(\n    col: ColumnObject,\n    allow_copy: bool = True,\n) -> pa.Array:\n    \"\"\"\n    Convert a column holding boolean dtype to a PyArrow array.\n\n    Parameters\n    ----------\n    col : ColumnObject\n    allow_copy : bool, default: True\n        Whether to allow copying the memory to perform the conversion\n        (if false then zero-copy approach is requested).\n\n    Returns\n    -------\n    pa.Array\n    \"\"\"\n    buffers = col.get_buffers()\n    size = buffers[\"data\"][1][1]\n\n    # If booleans are byte-packed a copy to bit-packed will be made\n    if size == 8 and not allow_copy:\n        raise RuntimeError(\n            \"Boolean column will be casted from uint8 and a copy \"\n            \"is required which is forbidden by allow_copy=False\"\n        )\n\n    data_type = col.dtype\n    data = buffers_to_array(buffers, data_type,\n                            col.size(),\n                            col.describe_null,\n                            col.offset)\n    if size == 8:\n        data = pc.cast(data, pa.bool_())\n\n    return data\n\n\ndef categorical_column_to_dictionary(\n    col: ColumnObject,\n    allow_copy: bool = True,\n) -> pa.DictionaryArray:\n    \"\"\"\n    Convert a column holding categorical data to a pa.DictionaryArray.\n\n    Parameters\n    ----------\n    col : ColumnObject\n    allow_copy : bool, default: True\n        Whether to allow copying the memory to perform the conversion\n        (if false then zero-copy approach is requested).\n\n    Returns\n    -------\n    pa.DictionaryArray\n    \"\"\"\n    if not allow_copy:\n        raise RuntimeError(\n            \"Categorical column will be casted from uint8 and a copy \"\n            \"is required which is forbidden by allow_copy=False\"\n        )\n\n    categorical = col.describe_categorical\n\n    if not categorical[\"is_dictionary\"]:\n        raise NotImplementedError(\n            \"Non-dictionary categoricals not supported yet\")\n\n    # We need to first convert the dictionary column\n    cat_column = categorical[\"categories\"]\n    dictionary = column_to_array(cat_column)\n    # Then we need to convert the indices\n    # Here we need to use the buffer data type!\n    buffers = col.get_buffers()\n    _, data_type = buffers[\"data\"]\n    indices = buffers_to_array(buffers, data_type,\n                               col.size(),\n                               col.describe_null,\n                               col.offset)\n\n    # Constructing a pa.DictionaryArray\n    dict_array = pa.DictionaryArray.from_arrays(indices, dictionary)\n\n    return dict_array\n\n\ndef parse_datetime_format_str(format_str):\n    \"\"\"Parse datetime `format_str` to interpret the `data`.\"\"\"\n\n    # timestamp 'ts{unit}:tz'\n    timestamp_meta = re.match(r\"ts([smun]):(.*)\", format_str)\n    if timestamp_meta:\n        unit, tz = timestamp_meta.group(1), timestamp_meta.group(2)\n        if unit != \"s\":\n            # the format string describes only a first letter of the unit, so\n            # add one extra letter to convert the unit to numpy-style:\n            # 'm' -> 'ms', 'u' -> 'us', 'n' -> 'ns'\n            unit += \"s\"\n\n        return unit, tz\n\n    raise NotImplementedError(f\"DateTime kind is not supported: {format_str}\")\n\n\ndef map_date_type(data_type):\n    \"\"\"Map column date type to pyarrow date type. \"\"\"\n    kind, bit_width, f_string, _ = data_type\n\n    if kind == DtypeKind.DATETIME:\n        unit, tz = parse_datetime_format_str(f_string)\n        return pa.timestamp(unit, tz=tz)\n    else:\n        pa_dtype = _PYARROW_DTYPES.get(kind, {}).get(bit_width, None)\n\n        # Error if dtype is not supported\n        if pa_dtype:\n            return pa_dtype\n        else:\n            raise NotImplementedError(\n                f\"Conversion for {data_type} is not yet supported.\")\n\n\ndef buffers_to_array(\n    buffers: ColumnBuffers,\n    data_type: Tuple[DtypeKind, int, str, str],\n    length: int,\n    describe_null: ColumnNullType,\n    offset: int = 0,\n    allow_copy: bool = True,\n) -> pa.Array:\n    \"\"\"\n    Build a PyArrow array from the passed buffer.\n\n    Parameters\n    ----------\n    buffer : ColumnBuffers\n        Dictionary containing tuples of underlying buffers and\n        their associated dtype.\n    data_type : Tuple[DtypeKind, int, str, str],\n        Dtype description of the column as a tuple ``(kind, bit-width, format string,\n        endianness)``.\n    length : int\n        The number of values in the array.\n    describe_null: ColumnNullType\n        Null representation the column dtype uses,\n        as a tuple ``(kind, value)``\n    offset : int, default: 0\n        Number of elements to offset from the start of the buffer.\n    allow_copy : bool, default: True\n        Whether to allow copying the memory to perform the conversion\n        (if false then zero-copy approach is requested).\n\n    Returns\n    -------\n    pa.Array\n\n    Notes\n    -----\n    The returned array doesn't own the memory. The caller of this function\n    is responsible for keeping the memory owner object alive as long as\n    the returned PyArrow array is being used.\n    \"\"\"\n    data_buff, _ = buffers[\"data\"]\n    try:\n        validity_buff, validity_dtype = buffers[\"validity\"]\n    except TypeError:\n        validity_buff = None\n    try:\n        offset_buff, offset_dtype = buffers[\"offsets\"]\n    except TypeError:\n        offset_buff = None\n\n    # Construct a pyarrow Buffer\n    data_pa_buffer = pa.foreign_buffer(data_buff.ptr, data_buff.bufsize,\n                                       base=data_buff)\n\n    # Construct a validity pyarrow Buffer, if applicable\n    if validity_buff:\n        validity_pa_buff = validity_buffer_from_mask(validity_buff,\n                                                     validity_dtype,\n                                                     describe_null,\n                                                     length,\n                                                     offset,\n                                                     allow_copy)\n    else:\n        validity_pa_buff = validity_buffer_nan_sentinel(data_pa_buffer,\n                                                        data_type,\n                                                        describe_null,\n                                                        length,\n                                                        offset,\n                                                        allow_copy)\n\n    # Construct a pyarrow Array from buffers\n    data_dtype = map_date_type(data_type)\n\n    if offset_buff:\n        _, offset_bit_width, _, _ = offset_dtype\n        # If an offset buffer exists, construct an offset pyarrow Buffer\n        # and add it to the construction of an array\n        offset_pa_buffer = pa.foreign_buffer(offset_buff.ptr,\n                                             offset_buff.bufsize,\n                                             base=offset_buff)\n\n        if data_type[2] == 'U':\n            string_type = pa.large_string()\n        else:\n            if offset_bit_width == 64:\n                string_type = pa.large_string()\n            else:\n                string_type = pa.string()\n        array = pa.Array.from_buffers(\n            string_type,\n            length,\n            [validity_pa_buff, offset_pa_buffer, data_pa_buffer],\n            offset=offset,\n        )\n    else:\n        array = pa.Array.from_buffers(\n            data_dtype,\n            length,\n            [validity_pa_buff, data_pa_buffer],\n            offset=offset,\n        )\n\n    return array\n\n\ndef validity_buffer_from_mask(\n    validity_buff: BufferObject,\n    validity_dtype: Dtype,\n    describe_null: ColumnNullType,\n    length: int,\n    offset: int = 0,\n    allow_copy: bool = True,\n) -> pa.Buffer:\n    \"\"\"\n    Build a PyArrow buffer from the passed mask buffer.\n\n    Parameters\n    ----------\n    validity_buff : BufferObject\n        Tuple of underlying validity buffer and associated dtype.\n    validity_dtype : Dtype\n        Dtype description as a tuple ``(kind, bit-width, format string,\n        endianness)``.\n    describe_null : ColumnNullType\n        Null representation the column dtype uses,\n        as a tuple ``(kind, value)``\n    length : int\n        The number of values in the array.\n    offset : int, default: 0\n        Number of elements to offset from the start of the buffer.\n    allow_copy : bool, default: True\n        Whether to allow copying the memory to perform the conversion\n        (if false then zero-copy approach is requested).\n\n    Returns\n    -------\n    pa.Buffer\n    \"\"\"\n    null_kind, sentinel_val = describe_null\n    validity_kind, _, _, _ = validity_dtype\n    assert validity_kind == DtypeKind.BOOL\n\n    if null_kind == ColumnNullType.NON_NULLABLE:\n        # Sliced array can have a NON_NULLABLE ColumnNullType due\n        # to no missing values in that slice of an array though the bitmask\n        # exists and validity_buff must be set to None in this case\n        return None\n\n    elif null_kind == ColumnNullType.USE_BYTEMASK or (\n        null_kind == ColumnNullType.USE_BITMASK and sentinel_val == 1\n    ):\n        buff = pa.foreign_buffer(validity_buff.ptr,\n                                 validity_buff.bufsize,\n                                 base=validity_buff)\n\n        if null_kind == ColumnNullType.USE_BYTEMASK:\n            if not allow_copy:\n                raise RuntimeError(\n                    \"To create a bitmask a copy of the data is \"\n                    \"required which is forbidden by allow_copy=False\"\n                )\n            mask = pa.Array.from_buffers(pa.int8(), length,\n                                         [None, buff],\n                                         offset=offset)\n            mask_bool = pc.cast(mask, pa.bool_())\n        else:\n            mask_bool = pa.Array.from_buffers(pa.bool_(), length,\n                                              [None, buff],\n                                              offset=offset)\n\n        if sentinel_val == 1:\n            mask_bool = pc.invert(mask_bool)\n\n        return mask_bool.buffers()[1]\n\n    elif null_kind == ColumnNullType.USE_BITMASK and sentinel_val == 0:\n        return pa.foreign_buffer(validity_buff.ptr,\n                                 validity_buff.bufsize,\n                                 base=validity_buff)\n    else:\n        raise NotImplementedError(\n            f\"{describe_null} null representation is not yet supported.\")\n\n\ndef validity_buffer_nan_sentinel(\n    data_pa_buffer: BufferObject,\n    data_type: Dtype,\n    describe_null: ColumnNullType,\n    length: int,\n    offset: int = 0,\n    allow_copy: bool = True,\n) -> pa.Buffer:\n    \"\"\"\n    Build a PyArrow buffer from NaN or sentinel values.\n\n    Parameters\n    ----------\n    data_pa_buffer : pa.Buffer\n        PyArrow buffer for the column data.\n    data_type : Dtype\n        Dtype description as a tuple ``(kind, bit-width, format string,\n        endianness)``.\n    describe_null : ColumnNullType\n        Null representation the column dtype uses,\n        as a tuple ``(kind, value)``\n    length : int\n        The number of values in the array.\n    offset : int, default: 0\n        Number of elements to offset from the start of the buffer.\n    allow_copy : bool, default: True\n        Whether to allow copying the memory to perform the conversion\n        (if false then zero-copy approach is requested).\n\n    Returns\n    -------\n    pa.Buffer\n    \"\"\"\n    kind, bit_width, _, _ = data_type\n    data_dtype = map_date_type(data_type)\n    null_kind, sentinel_val = describe_null\n\n    # Check for float NaN values\n    if null_kind == ColumnNullType.USE_NAN:\n        if not allow_copy:\n            raise RuntimeError(\n                \"To create a bitmask a copy of the data is \"\n                \"required which is forbidden by allow_copy=False\"\n            )\n\n        if kind == DtypeKind.FLOAT and bit_width == 16:\n            # 'pyarrow.compute.is_nan' kernel not yet implemented\n            # for float16\n            raise NotImplementedError(\n                f\"{data_type} with {null_kind} is not yet supported.\")\n        else:\n            pyarrow_data = pa.Array.from_buffers(\n                data_dtype,\n                length,\n                [None, data_pa_buffer],\n                offset=offset,\n            )\n            mask = pc.is_nan(pyarrow_data)\n            mask = pc.invert(mask)\n            return mask.buffers()[1]\n\n    # Check for sentinel values\n    elif null_kind == ColumnNullType.USE_SENTINEL:\n        if not allow_copy:\n            raise RuntimeError(\n                \"To create a bitmask a copy of the data is \"\n                \"required which is forbidden by allow_copy=False\"\n            )\n\n        if kind == DtypeKind.DATETIME:\n            sentinel_dtype = pa.int64()\n        else:\n            sentinel_dtype = data_dtype\n        pyarrow_data = pa.Array.from_buffers(sentinel_dtype,\n                                             length,\n                                             [None, data_pa_buffer],\n                                             offset=offset)\n        sentinel_arr = pc.equal(pyarrow_data, sentinel_val)\n        mask_bool = pc.invert(sentinel_arr)\n        return mask_bool.buffers()[1]\n\n    elif null_kind == ColumnNullType.NON_NULLABLE:\n        pass\n    else:\n        raise NotImplementedError(\n            f\"{describe_null} null representation is not yet supported.\")\n", "python/pyarrow/interchange/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# flake8: noqa\n\nfrom .from_dataframe import from_dataframe\n", "python/pyarrow/tests/test_cython.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport shutil\nimport subprocess\nimport sys\n\nimport pytest\n\nimport pyarrow as pa\nimport pyarrow.tests.util as test_util\n\nhere = os.path.dirname(os.path.abspath(__file__))\ntest_ld_path = os.environ.get('PYARROW_TEST_LD_PATH', '')\nif os.name == 'posix':\n    compiler_opts = ['-std=c++17']\nelif os.name == 'nt':\n    compiler_opts = ['-D_ENABLE_EXTENDED_ALIGNED_STORAGE', '/std:c++17']\nelse:\n    compiler_opts = []\n\nsetup_template = \"\"\"if 1:\n    from setuptools import setup\n    from Cython.Build import cythonize\n\n    import numpy as np\n\n    import pyarrow as pa\n\n    ext_modules = cythonize({pyx_file!r})\n    compiler_opts = {compiler_opts!r}\n    custom_ld_path = {test_ld_path!r}\n\n    for ext in ext_modules:\n        # XXX required for numpy/numpyconfig.h,\n        # included from arrow/python/api.h\n        ext.include_dirs.append(np.get_include())\n        ext.include_dirs.append(pa.get_include())\n        ext.libraries.extend(pa.get_libraries())\n        ext.library_dirs.extend(pa.get_library_dirs())\n        if custom_ld_path:\n            ext.library_dirs.append(custom_ld_path)\n        ext.extra_compile_args.extend(compiler_opts)\n        print(\"Extension module:\",\n              ext, ext.include_dirs, ext.libraries, ext.library_dirs)\n\n    setup(\n        ext_modules=ext_modules,\n    )\n\"\"\"\n\n\ndef check_cython_example_module(mod):\n    arr = pa.array([1, 2, 3])\n    assert mod.get_array_length(arr) == 3\n    with pytest.raises(TypeError, match=\"not an array\"):\n        mod.get_array_length(None)\n\n    scal = pa.scalar(123)\n    cast_scal = mod.cast_scalar(scal, pa.utf8())\n    assert cast_scal == pa.scalar(\"123\")\n    with pytest.raises(NotImplementedError,\n                       match=\"Unsupported cast from int64 to list using function \"\n                             \"cast_list\"):\n        mod.cast_scalar(scal, pa.list_(pa.int64()))\n\n\n@pytest.mark.cython\ndef test_cython_api(tmpdir):\n    \"\"\"\n    Basic test for the Cython API.\n    \"\"\"\n    # Fail early if cython is not found\n    import cython  # noqa\n\n    with tmpdir.as_cwd():\n        # Set up temporary workspace\n        pyx_file = 'pyarrow_cython_example.pyx'\n        shutil.copyfile(os.path.join(here, pyx_file),\n                        os.path.join(str(tmpdir), pyx_file))\n        # Create setup.py file\n        setup_code = setup_template.format(pyx_file=pyx_file,\n                                           compiler_opts=compiler_opts,\n                                           test_ld_path=test_ld_path)\n        with open('setup.py', 'w') as f:\n            f.write(setup_code)\n\n        # ARROW-2263: Make environment with this pyarrow/ package first on the\n        # PYTHONPATH, for local dev environments\n        subprocess_env = test_util.get_modified_env_with_pythonpath()\n\n        # Compile extension module\n        subprocess.check_call([sys.executable, 'setup.py',\n                               'build_ext', '--inplace'],\n                              env=subprocess_env)\n\n        # Check basic functionality\n        orig_path = sys.path[:]\n        sys.path.insert(0, str(tmpdir))\n        try:\n            mod = __import__('pyarrow_cython_example')\n            check_cython_example_module(mod)\n        finally:\n            sys.path = orig_path\n\n        # Check the extension module is loadable from a subprocess without\n        # pyarrow imported first.\n        code = \"\"\"if 1:\n            import sys\n            import os\n\n            try:\n                # Add dll directory was added on python 3.8\n                # and is required in order to find extra DLLs\n                # only for win32\n                for dir in {library_dirs}:\n                    os.add_dll_directory(dir)\n            except AttributeError:\n                pass\n\n            mod = __import__({mod_name!r})\n            arr = mod.make_null_array(5)\n            assert mod.get_array_length(arr) == 5\n            assert arr.null_count == 5\n        \"\"\".format(mod_name='pyarrow_cython_example',\n                   library_dirs=pa.get_library_dirs())\n\n        path_var = None\n        if sys.platform == 'win32':\n            if not hasattr(os, 'add_dll_directory'):\n                # Python 3.8 onwards don't check extension module DLLs on path\n                # we have to use os.add_dll_directory instead.\n                delim, path_var = ';', 'PATH'\n        elif sys.platform == 'darwin':\n            delim, path_var = ':', 'DYLD_LIBRARY_PATH'\n        else:\n            delim, path_var = ':', 'LD_LIBRARY_PATH'\n\n        if path_var:\n            paths = sys.path\n            paths += pa.get_library_dirs()\n            paths += [subprocess_env.get(path_var, '')]\n            paths = [path for path in paths if path]\n            subprocess_env[path_var] = delim.join(paths)\n        subprocess.check_call([sys.executable, '-c', code],\n                              stdout=subprocess.PIPE,\n                              env=subprocess_env)\n\n\n@pytest.mark.cython\ndef test_visit_strings(tmpdir):\n    with tmpdir.as_cwd():\n        # Set up temporary workspace\n        pyx_file = 'bound_function_visit_strings.pyx'\n        shutil.copyfile(os.path.join(here, pyx_file),\n                        os.path.join(str(tmpdir), pyx_file))\n        # Create setup.py file\n        setup_code = setup_template.format(pyx_file=pyx_file,\n                                           compiler_opts=compiler_opts,\n                                           test_ld_path=test_ld_path)\n        with open('setup.py', 'w') as f:\n            f.write(setup_code)\n\n        subprocess_env = test_util.get_modified_env_with_pythonpath()\n\n        # Compile extension module\n        subprocess.check_call([sys.executable, 'setup.py',\n                               'build_ext', '--inplace'],\n                              env=subprocess_env)\n\n    sys.path.insert(0, str(tmpdir))\n    mod = __import__('bound_function_visit_strings')\n\n    strings = ['a', 'b', 'c']\n    visited = []\n    mod._visit_strings(strings, visited.append)\n\n    assert visited == strings\n\n    with pytest.raises(ValueError, match=\"wtf\"):\n        def raise_on_b(s):\n            if s == 'b':\n                raise ValueError('wtf')\n\n        mod._visit_strings(strings, raise_on_b)\n", "python/pyarrow/tests/test_dataset_encryption.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport base64\nfrom datetime import timedelta\nimport numpy as np\nimport pyarrow.fs as fs\nimport pyarrow as pa\n\nimport pytest\n\nencryption_unavailable = False\n\ntry:\n    import pyarrow.parquet as pq\n    import pyarrow.dataset as ds\nexcept ImportError:\n    pq = None\n    ds = None\n\ntry:\n    from pyarrow.tests.parquet.encryption import InMemoryKmsClient\n    import pyarrow.parquet.encryption as pe\nexcept ImportError:\n    encryption_unavailable = True\n\n\n# Marks all of the tests in this module\npytestmark = pytest.mark.dataset\n\n\nFOOTER_KEY = b\"0123456789112345\"\nFOOTER_KEY_NAME = \"footer_key\"\nCOL_KEY = b\"1234567890123450\"\nCOL_KEY_NAME = \"col_key\"\n\n\ndef create_sample_table():\n    return pa.table(\n        {\n            \"year\": [2020, 2022, 2021, 2022, 2019, 2021],\n            \"n_legs\": [2, 2, 4, 4, 5, 100],\n            \"animal\": [\n                \"Flamingo\",\n                \"Parrot\",\n                \"Dog\",\n                \"Horse\",\n                \"Brittle stars\",\n                \"Centipede\",\n            ],\n        }\n    )\n\n\ndef create_encryption_config():\n    return pe.EncryptionConfiguration(\n        footer_key=FOOTER_KEY_NAME,\n        plaintext_footer=False,\n        column_keys={COL_KEY_NAME: [\"n_legs\", \"animal\"]},\n        encryption_algorithm=\"AES_GCM_V1\",\n        # requires timedelta or an assertion is raised\n        cache_lifetime=timedelta(minutes=5.0),\n        data_key_length_bits=256,\n    )\n\n\ndef create_decryption_config():\n    return pe.DecryptionConfiguration(cache_lifetime=300)\n\n\ndef create_kms_connection_config():\n    return pe.KmsConnectionConfig(\n        custom_kms_conf={\n            FOOTER_KEY_NAME: FOOTER_KEY.decode(\"UTF-8\"),\n            COL_KEY_NAME: COL_KEY.decode(\"UTF-8\"),\n        }\n    )\n\n\ndef kms_factory(kms_connection_configuration):\n    return InMemoryKmsClient(kms_connection_configuration)\n\n\n@pytest.mark.skipif(\n    encryption_unavailable, reason=\"Parquet Encryption is not currently enabled\"\n)\ndef test_dataset_encryption_decryption():\n    table = create_sample_table()\n\n    encryption_config = create_encryption_config()\n    decryption_config = create_decryption_config()\n    kms_connection_config = create_kms_connection_config()\n\n    crypto_factory = pe.CryptoFactory(kms_factory)\n    parquet_encryption_cfg = ds.ParquetEncryptionConfig(\n        crypto_factory, kms_connection_config, encryption_config\n    )\n    parquet_decryption_cfg = ds.ParquetDecryptionConfig(\n        crypto_factory, kms_connection_config, decryption_config\n    )\n\n    # create write_options with dataset encryption config\n    pformat = pa.dataset.ParquetFileFormat()\n    write_options = pformat.make_write_options(encryption_config=parquet_encryption_cfg)\n\n    mockfs = fs._MockFileSystem()\n    mockfs.create_dir(\"/\")\n\n    ds.write_dataset(\n        data=table,\n        base_dir=\"sample_dataset\",\n        format=pformat,\n        file_options=write_options,\n        filesystem=mockfs,\n    )\n\n    # read without decryption config -> should error is dataset was properly encrypted\n    pformat = pa.dataset.ParquetFileFormat()\n    with pytest.raises(IOError, match=r\"no decryption\"):\n        ds.dataset(\"sample_dataset\", format=pformat, filesystem=mockfs)\n\n    # set decryption config for parquet fragment scan options\n    pq_scan_opts = ds.ParquetFragmentScanOptions(\n        decryption_config=parquet_decryption_cfg\n    )\n    pformat = pa.dataset.ParquetFileFormat(default_fragment_scan_options=pq_scan_opts)\n    dataset = ds.dataset(\"sample_dataset\", format=pformat, filesystem=mockfs)\n\n    assert table.equals(dataset.to_table())\n\n    # set decryption properties for parquet fragment scan options\n    decryption_properties = crypto_factory.file_decryption_properties(\n        kms_connection_config, decryption_config)\n    pq_scan_opts = ds.ParquetFragmentScanOptions(\n        decryption_properties=decryption_properties\n    )\n\n    pformat = pa.dataset.ParquetFileFormat(default_fragment_scan_options=pq_scan_opts)\n    dataset = ds.dataset(\"sample_dataset\", format=pformat, filesystem=mockfs)\n\n    assert table.equals(dataset.to_table())\n\n\n@pytest.mark.skipif(\n    not encryption_unavailable, reason=\"Parquet Encryption is currently enabled\"\n)\ndef test_write_dataset_parquet_without_encryption():\n    \"\"\"Test write_dataset with ParquetFileFormat and test if an exception is thrown\n    if you try to set encryption_config using make_write_options\"\"\"\n\n    # Set the encryption configuration using ParquetFileFormat\n    # and make_write_options\n    pformat = pa.dataset.ParquetFileFormat()\n\n    with pytest.raises(NotImplementedError):\n        _ = pformat.make_write_options(encryption_config=\"some value\")\n\n\n@pytest.mark.skipif(\n    encryption_unavailable, reason=\"Parquet Encryption is not currently enabled\"\n)\ndef test_large_row_encryption_decryption():\n    \"\"\"Test encryption and decryption of a large number of rows.\"\"\"\n\n    class NoOpKmsClient(pe.KmsClient):\n        def wrap_key(self, key_bytes: bytes, _: str) -> bytes:\n            b = base64.b64encode(key_bytes)\n            return b\n\n        def unwrap_key(self, wrapped_key: bytes, _: str) -> bytes:\n            b = base64.b64decode(wrapped_key)\n            return b\n\n    row_count = 2**15 + 1\n    table = pa.Table.from_arrays(\n        [pa.array(np.random.rand(row_count), type=pa.float32())], names=[\"foo\"]\n    )\n\n    kms_config = pe.KmsConnectionConfig()\n    crypto_factory = pe.CryptoFactory(lambda _: NoOpKmsClient())\n    encryption_config = pe.EncryptionConfiguration(\n        footer_key=\"UNIMPORTANT_KEY\",\n        column_keys={\"UNIMPORTANT_KEY\": [\"foo\"]},\n        double_wrapping=True,\n        plaintext_footer=False,\n        data_key_length_bits=128,\n    )\n    pqe_config = ds.ParquetEncryptionConfig(\n        crypto_factory, kms_config, encryption_config\n    )\n    pqd_config = ds.ParquetDecryptionConfig(\n        crypto_factory, kms_config, pe.DecryptionConfiguration()\n    )\n    scan_options = ds.ParquetFragmentScanOptions(decryption_config=pqd_config)\n    file_format = ds.ParquetFileFormat(default_fragment_scan_options=scan_options)\n    write_options = file_format.make_write_options(encryption_config=pqe_config)\n    file_decryption_properties = crypto_factory.file_decryption_properties(kms_config)\n\n    mockfs = fs._MockFileSystem()\n    mockfs.create_dir(\"/\")\n\n    path = \"large-row-test-dataset\"\n    ds.write_dataset(table, path, format=file_format,\n                     file_options=write_options, filesystem=mockfs)\n\n    file_path = path + \"/part-0.parquet\"\n    new_table = pq.ParquetFile(\n        file_path, decryption_properties=file_decryption_properties,\n        filesystem=mockfs\n    ).read()\n    assert table == new_table\n\n    dataset = ds.dataset(path, format=file_format, filesystem=mockfs)\n    new_table = dataset.to_table()\n    assert table == new_table\n", "python/pyarrow/tests/test_flight.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport ast\nimport base64\nimport itertools\nimport os\nimport pathlib\nimport signal\nimport struct\nimport tempfile\nimport threading\nimport time\nimport traceback\nimport json\n\nimport numpy as np\nimport pytest\nimport pyarrow as pa\n\nfrom pyarrow.lib import IpcReadOptions, tobytes\nfrom pyarrow.util import find_free_port\nfrom pyarrow.tests import util\n\ntry:\n    from pyarrow import flight\n    from pyarrow.flight import (\n        FlightClient, FlightServerBase,\n        ServerAuthHandler, ClientAuthHandler,\n        ServerMiddleware, ServerMiddlewareFactory,\n        ClientMiddleware, ClientMiddlewareFactory,\n    )\nexcept ImportError:\n    flight = None\n    FlightClient, FlightServerBase = object, object\n    ServerAuthHandler, ClientAuthHandler = object, object\n    ServerMiddleware, ServerMiddlewareFactory = object, object\n    ClientMiddleware, ClientMiddlewareFactory = object, object\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not flight'\npytestmark = pytest.mark.flight\n\n\ndef test_import():\n    # So we see the ImportError somewhere\n    import pyarrow.flight  # noqa\n\n\ndef resource_root():\n    \"\"\"Get the path to the test resources directory.\"\"\"\n    if not os.environ.get(\"ARROW_TEST_DATA\"):\n        raise RuntimeError(\"Test resources not found; set \"\n                           \"ARROW_TEST_DATA to <repo root>/testing/data\")\n    return pathlib.Path(os.environ[\"ARROW_TEST_DATA\"]) / \"flight\"\n\n\ndef read_flight_resource(path):\n    \"\"\"Get the contents of a test resource file.\"\"\"\n    root = resource_root()\n    if not root:\n        return None\n    try:\n        with (root / path).open(\"rb\") as f:\n            return f.read()\n    except FileNotFoundError:\n        raise RuntimeError(\n            \"Test resource {} not found; did you initialize the \"\n            \"test resource submodule?\\n{}\".format(root / path,\n                                                  traceback.format_exc()))\n\n\ndef example_tls_certs():\n    \"\"\"Get the paths to test TLS certificates.\"\"\"\n    return {\n        \"root_cert\": read_flight_resource(\"root-ca.pem\"),\n        \"certificates\": [\n            flight.CertKeyPair(\n                cert=read_flight_resource(\"cert0.pem\"),\n                key=read_flight_resource(\"cert0.key\"),\n            ),\n            flight.CertKeyPair(\n                cert=read_flight_resource(\"cert1.pem\"),\n                key=read_flight_resource(\"cert1.key\"),\n            ),\n        ]\n    }\n\n\ndef simple_ints_table():\n    data = [\n        pa.array([-10, -5, 0, 5, 10])\n    ]\n    return pa.Table.from_arrays(data, names=['some_ints'])\n\n\ndef simple_dicts_table():\n    dict_values = pa.array([\"foo\", \"baz\", \"quux\"], type=pa.utf8())\n    data = [\n        pa.chunked_array([\n            pa.DictionaryArray.from_arrays([1, 0, None], dict_values),\n            pa.DictionaryArray.from_arrays([2, 1], dict_values)\n        ])\n    ]\n    return pa.Table.from_arrays(data, names=['some_dicts'])\n\n\ndef multiple_column_table():\n    return pa.Table.from_arrays([pa.array(['foo', 'bar', 'baz', 'qux']),\n                                 pa.array([1, 2, 3, 4])],\n                                names=['a', 'b'])\n\n\nclass ConstantFlightServer(FlightServerBase):\n    \"\"\"A Flight server that always returns the same data.\n\n    See ARROW-4796: this server implementation will segfault if Flight\n    does not properly hold a reference to the Table object.\n    \"\"\"\n\n    CRITERIA = b\"the expected criteria\"\n\n    def __init__(self, location=None, options=None, **kwargs):\n        super().__init__(location, **kwargs)\n        # Ticket -> Table\n        self.table_factories = {\n            b'ints': simple_ints_table,\n            b'dicts': simple_dicts_table,\n            b'multi': multiple_column_table,\n        }\n        self.options = options\n\n    def list_flights(self, context, criteria):\n        if criteria == self.CRITERIA:\n            yield flight.FlightInfo(\n                pa.schema([]),\n                flight.FlightDescriptor.for_path('/foo'),\n                [],\n                -1, -1\n            )\n\n    def do_get(self, context, ticket):\n        # Return a fresh table, so that Flight is the only one keeping a\n        # reference.\n        table = self.table_factories[ticket.ticket]()\n        return flight.RecordBatchStream(table, options=self.options)\n\n\nclass MetadataFlightServer(FlightServerBase):\n    \"\"\"A Flight server that numbers incoming/outgoing data.\"\"\"\n\n    def __init__(self, options=None, **kwargs):\n        super().__init__(**kwargs)\n        self.options = options\n\n    def do_get(self, context, ticket):\n        data = [\n            pa.array([-10, -5, 0, 5, 10])\n        ]\n        table = pa.Table.from_arrays(data, names=['a'])\n        return flight.GeneratorStream(\n            table.schema,\n            self.number_batches(table),\n            options=self.options)\n\n    def do_put(self, context, descriptor, reader, writer):\n        counter = 0\n        expected_data = [-10, -5, 0, 5, 10]\n        while True:\n            try:\n                batch, buf = reader.read_chunk()\n                assert batch.equals(pa.RecordBatch.from_arrays(\n                    [pa.array([expected_data[counter]])],\n                    ['a']\n                ))\n                assert buf is not None\n                client_counter, = struct.unpack('<i', buf.to_pybytes())\n                assert counter == client_counter\n                writer.write(struct.pack('<i', counter))\n                counter += 1\n            except StopIteration:\n                return\n\n    @staticmethod\n    def number_batches(table):\n        for idx, batch in enumerate(table.to_batches()):\n            buf = struct.pack('<i', idx)\n            yield batch, buf\n\n\nclass EchoFlightServer(FlightServerBase):\n    \"\"\"A Flight server that returns the last data uploaded.\"\"\"\n\n    def __init__(self, location=None, expected_schema=None, **kwargs):\n        super().__init__(location, **kwargs)\n        self.last_message = None\n        self.expected_schema = expected_schema\n\n    def do_get(self, context, ticket):\n        return flight.RecordBatchStream(self.last_message)\n\n    def do_put(self, context, descriptor, reader, writer):\n        if self.expected_schema:\n            assert self.expected_schema == reader.schema\n        self.last_message = reader.read_all()\n\n    def do_exchange(self, context, descriptor, reader, writer):\n        for chunk in reader:\n            pass\n\n\nclass EchoStreamFlightServer(EchoFlightServer):\n    \"\"\"An echo server that streams individual record batches.\"\"\"\n\n    def do_get(self, context, ticket):\n        return flight.GeneratorStream(\n            self.last_message.schema,\n            self.last_message.to_batches(max_chunksize=1024))\n\n    def list_actions(self, context):\n        return []\n\n    def do_action(self, context, action):\n        if action.type == \"who-am-i\":\n            return [context.peer_identity(), context.peer().encode(\"utf-8\")]\n        raise NotImplementedError\n\n\nclass GetInfoFlightServer(FlightServerBase):\n    \"\"\"A Flight server that tests GetFlightInfo.\"\"\"\n\n    def get_flight_info(self, context, descriptor):\n        return flight.FlightInfo(\n            pa.schema([('a', pa.int32())]),\n            descriptor,\n            [\n                flight.FlightEndpoint(b'', ['grpc://test']),\n                flight.FlightEndpoint(\n                    b'',\n                    [flight.Location.for_grpc_tcp('localhost', 5005)],\n                ),\n            ],\n            -1,\n            -1,\n        )\n\n    def get_schema(self, context, descriptor):\n        info = self.get_flight_info(context, descriptor)\n        return flight.SchemaResult(info.schema)\n\n\nclass ListActionsFlightServer(FlightServerBase):\n    \"\"\"A Flight server that tests ListActions.\"\"\"\n\n    @classmethod\n    def expected_actions(cls):\n        return [\n            (\"action-1\", \"description\"),\n            (\"action-2\", \"\"),\n            flight.ActionType(\"action-3\", \"more detail\"),\n        ]\n\n    def list_actions(self, context):\n        yield from self.expected_actions()\n\n\nclass ListActionsErrorFlightServer(FlightServerBase):\n    \"\"\"A Flight server that tests ListActions.\"\"\"\n\n    def list_actions(self, context):\n        yield (\"action-1\", \"\")\n        yield \"foo\"\n\n\nclass CheckTicketFlightServer(FlightServerBase):\n    \"\"\"A Flight server that compares the given ticket to an expected value.\"\"\"\n\n    def __init__(self, expected_ticket, location=None, **kwargs):\n        super().__init__(location, **kwargs)\n        self.expected_ticket = expected_ticket\n\n    def do_get(self, context, ticket):\n        assert self.expected_ticket == ticket.ticket\n        data1 = [pa.array([-10, -5, 0, 5, 10], type=pa.int32())]\n        table = pa.Table.from_arrays(data1, names=['a'])\n        return flight.RecordBatchStream(table)\n\n    def do_put(self, context, descriptor, reader):\n        self.last_message = reader.read_all()\n\n\nclass InvalidStreamFlightServer(FlightServerBase):\n    \"\"\"A Flight server that tries to return messages with differing schemas.\"\"\"\n\n    schema = pa.schema([('a', pa.int32())])\n\n    def do_get(self, context, ticket):\n        data1 = [pa.array([-10, -5, 0, 5, 10], type=pa.int32())]\n        data2 = [pa.array([-10.0, -5.0, 0.0, 5.0, 10.0], type=pa.float64())]\n        assert data1.type != data2.type\n        table1 = pa.Table.from_arrays(data1, names=['a'])\n        table2 = pa.Table.from_arrays(data2, names=['a'])\n        assert table1.schema == self.schema\n\n        return flight.GeneratorStream(self.schema, [table1, table2])\n\n\nclass NeverSendsDataFlightServer(FlightServerBase):\n    \"\"\"A Flight server that never actually yields data.\"\"\"\n\n    schema = pa.schema([('a', pa.int32())])\n\n    def do_get(self, context, ticket):\n        if ticket.ticket == b'yield_data':\n            # Check that the server handler will ignore empty tables\n            # up to a certain extent\n            data = [\n                self.schema.empty_table(),\n                self.schema.empty_table(),\n                pa.RecordBatch.from_arrays([range(5)], schema=self.schema),\n            ]\n            return flight.GeneratorStream(self.schema, data)\n        return flight.GeneratorStream(\n            self.schema, itertools.repeat(self.schema.empty_table()))\n\n\nclass SlowFlightServer(FlightServerBase):\n    \"\"\"A Flight server that delays its responses to test timeouts.\"\"\"\n\n    def do_get(self, context, ticket):\n        return flight.GeneratorStream(pa.schema([('a', pa.int32())]),\n                                      self.slow_stream())\n\n    def do_action(self, context, action):\n        time.sleep(0.5)\n        return []\n\n    @staticmethod\n    def slow_stream():\n        data1 = [pa.array([-10, -5, 0, 5, 10], type=pa.int32())]\n        yield pa.Table.from_arrays(data1, names=['a'])\n        # The second message should never get sent; the client should\n        # cancel before we send this\n        time.sleep(10)\n        yield pa.Table.from_arrays(data1, names=['a'])\n\n\nclass ErrorFlightServer(FlightServerBase):\n    \"\"\"A Flight server that uses all the Flight-specific errors.\"\"\"\n\n    @staticmethod\n    def error_cases():\n        return {\n            \"internal\": flight.FlightInternalError,\n            \"timedout\": flight.FlightTimedOutError,\n            \"cancel\": flight.FlightCancelledError,\n            \"unauthenticated\": flight.FlightUnauthenticatedError,\n            \"unauthorized\": flight.FlightUnauthorizedError,\n            \"notimplemented\": NotImplementedError,\n            \"invalid\": pa.ArrowInvalid,\n            \"key\": KeyError,\n        }\n\n    def do_action(self, context, action):\n        error_cases = ErrorFlightServer.error_cases()\n        if action.type in error_cases:\n            raise error_cases[action.type](\"foo\")\n        elif action.type == \"protobuf\":\n            err_msg = b'this is an error message'\n            raise flight.FlightUnauthorizedError(\"foo\", err_msg)\n        raise NotImplementedError\n\n    def list_flights(self, context, criteria):\n        yield flight.FlightInfo(\n            pa.schema([]),\n            flight.FlightDescriptor.for_path('/foo'),\n            [],\n            -1, -1\n        )\n        raise flight.FlightInternalError(\"foo\")\n\n    def do_put(self, context, descriptor, reader, writer):\n        if descriptor.command == b\"internal\":\n            raise flight.FlightInternalError(\"foo\")\n        elif descriptor.command == b\"timedout\":\n            raise flight.FlightTimedOutError(\"foo\")\n        elif descriptor.command == b\"cancel\":\n            raise flight.FlightCancelledError(\"foo\")\n        elif descriptor.command == b\"unauthenticated\":\n            raise flight.FlightUnauthenticatedError(\"foo\")\n        elif descriptor.command == b\"unauthorized\":\n            raise flight.FlightUnauthorizedError(\"foo\")\n        elif descriptor.command == b\"protobuf\":\n            err_msg = b'this is an error message'\n            raise flight.FlightUnauthorizedError(\"foo\", err_msg)\n\n\nclass ExchangeFlightServer(FlightServerBase):\n    \"\"\"A server for testing DoExchange.\"\"\"\n\n    def __init__(self, options=None, **kwargs):\n        super().__init__(**kwargs)\n        self.options = options\n\n    def do_exchange(self, context, descriptor, reader, writer):\n        if descriptor.descriptor_type != flight.DescriptorType.CMD:\n            raise pa.ArrowInvalid(\"Must provide a command descriptor\")\n        elif descriptor.command == b\"echo\":\n            return self.exchange_echo(context, reader, writer)\n        elif descriptor.command == b\"get\":\n            return self.exchange_do_get(context, reader, writer)\n        elif descriptor.command == b\"put\":\n            return self.exchange_do_put(context, reader, writer)\n        elif descriptor.command == b\"transform\":\n            return self.exchange_transform(context, reader, writer)\n        else:\n            raise pa.ArrowInvalid(\n                \"Unknown command: {}\".format(descriptor.command))\n\n    def exchange_do_get(self, context, reader, writer):\n        \"\"\"Emulate DoGet with DoExchange.\"\"\"\n        data = pa.Table.from_arrays([\n            pa.array(range(0, 10 * 1024))\n        ], names=[\"a\"])\n        writer.begin(data.schema)\n        writer.write_table(data)\n\n    def exchange_do_put(self, context, reader, writer):\n        \"\"\"Emulate DoPut with DoExchange.\"\"\"\n        num_batches = 0\n        for chunk in reader:\n            if not chunk.data:\n                raise pa.ArrowInvalid(\"All chunks must have data.\")\n            num_batches += 1\n        writer.write_metadata(str(num_batches).encode(\"utf-8\"))\n\n    def exchange_echo(self, context, reader, writer):\n        \"\"\"Run a simple echo server.\"\"\"\n        started = False\n        for chunk in reader:\n            if not started and chunk.data:\n                writer.begin(chunk.data.schema, options=self.options)\n                started = True\n            if chunk.app_metadata and chunk.data:\n                writer.write_with_metadata(chunk.data, chunk.app_metadata)\n            elif chunk.app_metadata:\n                writer.write_metadata(chunk.app_metadata)\n            elif chunk.data:\n                writer.write_batch(chunk.data)\n            else:\n                assert False, \"Should not happen\"\n\n    def exchange_transform(self, context, reader, writer):\n        \"\"\"Sum rows in an uploaded table.\"\"\"\n        for field in reader.schema:\n            if not pa.types.is_integer(field.type):\n                raise pa.ArrowInvalid(\"Invalid field: \" + repr(field))\n        table = reader.read_all()\n        sums = [0] * table.num_rows\n        for column in table:\n            for row, value in enumerate(column):\n                sums[row] += value.as_py()\n        result = pa.Table.from_arrays([pa.array(sums)], names=[\"sum\"])\n        writer.begin(result.schema)\n        writer.write_table(result)\n\n\nclass HttpBasicServerAuthHandler(ServerAuthHandler):\n    \"\"\"An example implementation of HTTP basic authentication.\"\"\"\n\n    def __init__(self, creds):\n        super().__init__()\n        self.creds = creds\n\n    def authenticate(self, outgoing, incoming):\n        buf = incoming.read()\n        auth = flight.BasicAuth.deserialize(buf)\n        if auth.username not in self.creds:\n            raise flight.FlightUnauthenticatedError(\"unknown user\")\n        if self.creds[auth.username] != auth.password:\n            raise flight.FlightUnauthenticatedError(\"wrong password\")\n        outgoing.write(tobytes(auth.username))\n\n    def is_valid(self, token):\n        if not token:\n            raise flight.FlightUnauthenticatedError(\"token not provided\")\n        if token not in self.creds:\n            raise flight.FlightUnauthenticatedError(\"unknown user\")\n        return token\n\n\nclass HttpBasicClientAuthHandler(ClientAuthHandler):\n    \"\"\"An example implementation of HTTP basic authentication.\"\"\"\n\n    def __init__(self, username, password):\n        super().__init__()\n        self.basic_auth = flight.BasicAuth(username, password)\n        self.token = None\n\n    def authenticate(self, outgoing, incoming):\n        auth = self.basic_auth.serialize()\n        outgoing.write(auth)\n        self.token = incoming.read()\n\n    def get_token(self):\n        return self.token\n\n\nclass TokenServerAuthHandler(ServerAuthHandler):\n    \"\"\"An example implementation of authentication via handshake.\"\"\"\n\n    def __init__(self, creds):\n        super().__init__()\n        self.creds = creds\n\n    def authenticate(self, outgoing, incoming):\n        username = incoming.read()\n        password = incoming.read()\n        if username in self.creds and self.creds[username] == password:\n            outgoing.write(base64.b64encode(b'secret:' + username))\n        else:\n            raise flight.FlightUnauthenticatedError(\n                \"invalid username/password\")\n\n    def is_valid(self, token):\n        token = base64.b64decode(token)\n        if not token.startswith(b'secret:'):\n            raise flight.FlightUnauthenticatedError(\"invalid token\")\n        return token[7:]\n\n\nclass TokenClientAuthHandler(ClientAuthHandler):\n    \"\"\"An example implementation of authentication via handshake.\"\"\"\n\n    def __init__(self, username, password):\n        super().__init__()\n        self.username = username\n        self.password = password\n        self.token = b''\n\n    def authenticate(self, outgoing, incoming):\n        outgoing.write(self.username)\n        outgoing.write(self.password)\n        self.token = incoming.read()\n\n    def get_token(self):\n        return self.token\n\n\nclass NoopAuthHandler(ServerAuthHandler):\n    \"\"\"A no-op auth handler.\"\"\"\n\n    def authenticate(self, outgoing, incoming):\n        \"\"\"Do nothing.\"\"\"\n\n    def is_valid(self, token):\n        \"\"\"\n        Returning an empty string.\n        Returning None causes Type error.\n        \"\"\"\n        return \"\"\n\n\ndef case_insensitive_header_lookup(headers, lookup_key):\n    \"\"\"Lookup the value of given key in the given headers.\n       The key lookup is case-insensitive.\n    \"\"\"\n    for key in headers:\n        if key.lower() == lookup_key.lower():\n            return headers.get(key)\n\n\nclass ClientHeaderAuthMiddlewareFactory(ClientMiddlewareFactory):\n    \"\"\"ClientMiddlewareFactory that creates ClientAuthHeaderMiddleware.\"\"\"\n\n    def __init__(self):\n        self.call_credential = []\n\n    def start_call(self, info):\n        return ClientHeaderAuthMiddleware(self)\n\n    def set_call_credential(self, call_credential):\n        self.call_credential = call_credential\n\n\nclass ClientHeaderAuthMiddleware(ClientMiddleware):\n    \"\"\"\n    ClientMiddleware that extracts the authorization header\n    from the server.\n\n    This is an example of a ClientMiddleware that can extract\n    the bearer token authorization header from a HTTP header\n    authentication enabled server.\n\n    Parameters\n    ----------\n    factory : ClientHeaderAuthMiddlewareFactory\n        This factory is used to set call credentials if an\n        authorization header is found in the headers from the server.\n    \"\"\"\n\n    def __init__(self, factory):\n        self.factory = factory\n\n    def received_headers(self, headers):\n        auth_header = case_insensitive_header_lookup(headers, 'Authorization')\n        self.factory.set_call_credential([\n            b'authorization',\n            auth_header[0].encode(\"utf-8\")])\n\n\nclass HeaderAuthServerMiddlewareFactory(ServerMiddlewareFactory):\n    \"\"\"Validates incoming username and password.\"\"\"\n\n    def start_call(self, info, headers):\n        auth_header = case_insensitive_header_lookup(\n            headers,\n            'Authorization'\n        )\n        values = auth_header[0].split(' ')\n        token = ''\n        error_message = 'Invalid credentials'\n\n        if values[0] == 'Basic':\n            decoded = base64.b64decode(values[1])\n            pair = decoded.decode(\"utf-8\").split(':')\n            if not (pair[0] == 'test' and pair[1] == 'password'):\n                raise flight.FlightUnauthenticatedError(error_message)\n            token = 'token1234'\n        elif values[0] == 'Bearer':\n            token = values[1]\n            if not token == 'token1234':\n                raise flight.FlightUnauthenticatedError(error_message)\n        else:\n            raise flight.FlightUnauthenticatedError(error_message)\n\n        return HeaderAuthServerMiddleware(token)\n\n\nclass HeaderAuthServerMiddleware(ServerMiddleware):\n    \"\"\"A ServerMiddleware that transports incoming username and password.\"\"\"\n\n    def __init__(self, token):\n        self.token = token\n\n    def sending_headers(self):\n        return {'authorization': 'Bearer ' + self.token}\n\n\nclass HeaderAuthFlightServer(FlightServerBase):\n    \"\"\"A Flight server that tests with basic token authentication. \"\"\"\n\n    def do_action(self, context, action):\n        middleware = context.get_middleware(\"auth\")\n        if middleware:\n            auth_header = case_insensitive_header_lookup(\n                middleware.sending_headers(), 'Authorization')\n            values = auth_header.split(' ')\n            return [values[1].encode(\"utf-8\")]\n        raise flight.FlightUnauthenticatedError(\n            'No token auth middleware found.')\n\n\nclass ArbitraryHeadersServerMiddlewareFactory(ServerMiddlewareFactory):\n    \"\"\"A ServerMiddlewareFactory that transports arbitrary headers.\"\"\"\n\n    def start_call(self, info, headers):\n        return ArbitraryHeadersServerMiddleware(headers)\n\n\nclass ArbitraryHeadersServerMiddleware(ServerMiddleware):\n    \"\"\"A ServerMiddleware that transports arbitrary headers.\"\"\"\n\n    def __init__(self, incoming):\n        self.incoming = incoming\n\n    def sending_headers(self):\n        return self.incoming\n\n\nclass ArbitraryHeadersFlightServer(FlightServerBase):\n    \"\"\"A Flight server that tests multiple arbitrary headers.\"\"\"\n\n    def do_action(self, context, action):\n        middleware = context.get_middleware(\"arbitrary-headers\")\n        if middleware:\n            headers = middleware.sending_headers()\n            header_1 = case_insensitive_header_lookup(\n                headers,\n                'test-header-1'\n            )\n            header_2 = case_insensitive_header_lookup(\n                headers,\n                'test-header-2'\n            )\n            value1 = header_1[0].encode(\"utf-8\")\n            value2 = header_2[0].encode(\"utf-8\")\n            return [value1, value2]\n        raise flight.FlightServerError(\"No headers middleware found\")\n\n\nclass HeaderServerMiddleware(ServerMiddleware):\n    \"\"\"Expose a per-call value to the RPC method body.\"\"\"\n\n    def __init__(self, special_value):\n        self.special_value = special_value\n\n\nclass HeaderServerMiddlewareFactory(ServerMiddlewareFactory):\n    \"\"\"Expose a per-call hard-coded value to the RPC method body.\"\"\"\n\n    def start_call(self, info, headers):\n        return HeaderServerMiddleware(\"right value\")\n\n\nclass HeaderFlightServer(FlightServerBase):\n    \"\"\"Echo back the per-call hard-coded value.\"\"\"\n\n    def do_action(self, context, action):\n        middleware = context.get_middleware(\"test\")\n        if middleware:\n            return [middleware.special_value.encode()]\n        return [b\"\"]\n\n\nclass MultiHeaderFlightServer(FlightServerBase):\n    \"\"\"Test sending/receiving multiple (binary-valued) headers.\"\"\"\n\n    def do_action(self, context, action):\n        middleware = context.get_middleware(\"test\")\n        headers = repr(middleware.client_headers).encode(\"utf-8\")\n        return [headers]\n\n\nclass SelectiveAuthServerMiddlewareFactory(ServerMiddlewareFactory):\n    \"\"\"Deny access to certain methods based on a header.\"\"\"\n\n    def start_call(self, info, headers):\n        if info.method == flight.FlightMethod.LIST_ACTIONS:\n            # No auth needed\n            return\n\n        token = headers.get(\"x-auth-token\")\n        if not token:\n            raise flight.FlightUnauthenticatedError(\"No token\")\n\n        token = token[0]\n        if token != \"password\":\n            raise flight.FlightUnauthenticatedError(\"Invalid token\")\n\n        return HeaderServerMiddleware(token)\n\n\nclass SelectiveAuthClientMiddlewareFactory(ClientMiddlewareFactory):\n    def start_call(self, info):\n        return SelectiveAuthClientMiddleware()\n\n\nclass SelectiveAuthClientMiddleware(ClientMiddleware):\n    def sending_headers(self):\n        return {\n            \"x-auth-token\": \"password\",\n        }\n\n\nclass RecordingServerMiddlewareFactory(ServerMiddlewareFactory):\n    \"\"\"Record what methods were called.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.methods = []\n\n    def start_call(self, info, headers):\n        self.methods.append(info.method)\n        return None\n\n\nclass RecordingClientMiddlewareFactory(ClientMiddlewareFactory):\n    \"\"\"Record what methods were called.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.methods = []\n\n    def start_call(self, info):\n        self.methods.append(info.method)\n        return None\n\n\nclass MultiHeaderClientMiddlewareFactory(ClientMiddlewareFactory):\n    \"\"\"Test sending/receiving multiple (binary-valued) headers.\"\"\"\n\n    def __init__(self):\n        # Read in test_middleware_multi_header below.\n        # The middleware instance will update this value.\n        self.last_headers = {}\n\n    def start_call(self, info):\n        return MultiHeaderClientMiddleware(self)\n\n\nclass MultiHeaderClientMiddleware(ClientMiddleware):\n    \"\"\"Test sending/receiving multiple (binary-valued) headers.\"\"\"\n\n    EXPECTED = {\n        \"x-text\": [\"foo\", \"bar\"],\n        \"x-binary-bin\": [b\"\\x00\", b\"\\x01\"],\n        # ARROW-16606: ensure mixed-case headers are accepted\n        \"x-MIXED-case\": [\"baz\"],\n        b\"x-other-MIXED-case\": [\"baz\"],\n    }\n\n    def __init__(self, factory):\n        self.factory = factory\n\n    def sending_headers(self):\n        return self.EXPECTED\n\n    def received_headers(self, headers):\n        # Let the test code know what the last set of headers we\n        # received were.\n        self.factory.last_headers.update(headers)\n\n\nclass MultiHeaderServerMiddlewareFactory(ServerMiddlewareFactory):\n    \"\"\"Test sending/receiving multiple (binary-valued) headers.\"\"\"\n\n    def start_call(self, info, headers):\n        return MultiHeaderServerMiddleware(headers)\n\n\nclass MultiHeaderServerMiddleware(ServerMiddleware):\n    \"\"\"Test sending/receiving multiple (binary-valued) headers.\"\"\"\n\n    def __init__(self, client_headers):\n        self.client_headers = client_headers\n\n    def sending_headers(self):\n        return MultiHeaderClientMiddleware.EXPECTED\n\n\nclass LargeMetadataFlightServer(FlightServerBase):\n    \"\"\"Regression test for ARROW-13253.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._metadata = b' ' * (2 ** 31 + 1)\n\n    def do_get(self, context, ticket):\n        schema = pa.schema([('a', pa.int64())])\n        return flight.GeneratorStream(schema, [\n            (pa.record_batch([[1]], schema=schema), self._metadata),\n        ])\n\n    def do_exchange(self, context, descriptor, reader, writer):\n        writer.write_metadata(self._metadata)\n\n\ndef test_repr():\n    action_repr = \"<pyarrow.flight.Action type='foo' body=(0 bytes)>\"\n    action_type_repr = \"ActionType(type='foo', description='bar')\"\n    basic_auth_repr = \"<pyarrow.flight.BasicAuth username=b'user' password=(redacted)>\"\n    descriptor_repr = \"<pyarrow.flight.FlightDescriptor cmd=b'foo'>\"\n    endpoint_repr = (\"<pyarrow.flight.FlightEndpoint \"\n                     \"ticket=<pyarrow.flight.Ticket ticket=b'foo'> \"\n                     \"locations=[]>\")\n    info_repr = (\n        \"<pyarrow.flight.FlightInfo \"\n        \"schema= \"\n        \"descriptor=<pyarrow.flight.FlightDescriptor path=[]> \"\n        \"endpoints=[] \"\n        \"total_records=-1 \"\n        \"total_bytes=-1>\")\n    location_repr = \"<pyarrow.flight.Location b'grpc+tcp://localhost:1234'>\"\n    result_repr = \"<pyarrow.flight.Result body=(3 bytes)>\"\n    schema_result_repr = \"<pyarrow.flight.SchemaResult schema=()>\"\n    ticket_repr = \"<pyarrow.flight.Ticket ticket=b'foo'>\"\n\n    assert repr(flight.Action(\"foo\", b\"\")) == action_repr\n    assert repr(flight.ActionType(\"foo\", \"bar\")) == action_type_repr\n    assert repr(flight.BasicAuth(\"user\", \"pass\")) == basic_auth_repr\n    assert repr(flight.FlightDescriptor.for_command(\"foo\")) == descriptor_repr\n    assert repr(flight.FlightEndpoint(b\"foo\", [])) == endpoint_repr\n    info = flight.FlightInfo(\n        pa.schema([]), flight.FlightDescriptor.for_path(), [], -1, -1)\n    assert repr(info) == info_repr\n    assert repr(flight.Location(\"grpc+tcp://localhost:1234\")) == location_repr\n    assert repr(flight.Result(b\"foo\")) == result_repr\n    assert repr(flight.SchemaResult(pa.schema([]))) == schema_result_repr\n    assert repr(flight.SchemaResult(pa.schema([(\"int\", \"int64\")]))) == \\\n        \"<pyarrow.flight.SchemaResult schema=(int: int64)>\"\n    assert repr(flight.Ticket(b\"foo\")) == ticket_repr\n\n    with pytest.raises(TypeError):\n        flight.Action(\"foo\", None)\n\n\ndef test_eq():\n    items = [\n        lambda: (flight.Action(\"foo\", b\"\"), flight.Action(\"foo\", b\"bar\")),\n        lambda: (flight.ActionType(\"foo\", \"bar\"),\n                 flight.ActionType(\"foo\", \"baz\")),\n        lambda: (flight.BasicAuth(\"user\", \"pass\"),\n                 flight.BasicAuth(\"user2\", \"pass\")),\n        lambda: (flight.FlightDescriptor.for_command(\"foo\"),\n                 flight.FlightDescriptor.for_path(\"foo\")),\n        lambda: (flight.FlightEndpoint(b\"foo\", []),\n                 flight.FlightEndpoint(b\"\", [])),\n        lambda: (\n            flight.FlightInfo(\n                pa.schema([]),\n                flight.FlightDescriptor.for_path(), [], -1, -1),\n            flight.FlightInfo(\n                pa.schema([]),\n                flight.FlightDescriptor.for_command(b\"foo\"), [], -1, 42)),\n        lambda: (flight.Location(\"grpc+tcp://localhost:1234\"),\n                 flight.Location(\"grpc+tls://localhost:1234\")),\n        lambda: (flight.Result(b\"foo\"), flight.Result(b\"bar\")),\n        lambda: (flight.SchemaResult(pa.schema([])),\n                 flight.SchemaResult(pa.schema([(\"ints\", pa.int64())]))),\n        lambda: (flight.Ticket(b\"\"), flight.Ticket(b\"foo\")),\n    ]\n\n    for gen in items:\n        lhs1, rhs1 = gen()\n        lhs2, rhs2 = gen()\n        assert lhs1 == lhs2\n        assert rhs1 == rhs2\n        assert lhs1 != rhs1\n\n\ndef test_flight_server_location_argument():\n    locations = [\n        None,\n        'grpc://localhost:0',\n        ('localhost', find_free_port()),\n    ]\n    for location in locations:\n        with FlightServerBase(location) as server:\n            assert isinstance(server, FlightServerBase)\n\n\ndef test_server_exit_reraises_exception():\n    with pytest.raises(ValueError):\n        with FlightServerBase():\n            raise ValueError()\n\n\n@pytest.mark.slow\ndef test_client_wait_for_available():\n    location = ('localhost', find_free_port())\n    server = None\n\n    def serve():\n        global server\n        time.sleep(0.5)\n        server = FlightServerBase(location)\n        server.serve()\n\n    with FlightClient(location) as client:\n        thread = threading.Thread(target=serve, daemon=True)\n        thread.start()\n\n        started = time.time()\n        client.wait_for_available(timeout=5)\n        elapsed = time.time() - started\n        assert elapsed >= 0.5\n\n\ndef test_flight_list_flights():\n    \"\"\"Try a simple list_flights call.\"\"\"\n    with ConstantFlightServer() as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        assert list(client.list_flights()) == []\n        flights = client.list_flights(ConstantFlightServer.CRITERIA)\n        assert len(list(flights)) == 1\n\n\ndef test_flight_client_close():\n    with ConstantFlightServer() as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        assert list(client.list_flights()) == []\n        client.close()\n        client.close()  # Idempotent\n        with pytest.raises(pa.ArrowInvalid):\n            list(client.list_flights())\n\n\ndef test_flight_do_get_ints():\n    \"\"\"Try a simple do_get call.\"\"\"\n    table = simple_ints_table()\n\n    with ConstantFlightServer() as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        data = client.do_get(flight.Ticket(b'ints')).read_all()\n        assert data.equals(table)\n\n    options = pa.ipc.IpcWriteOptions(\n        metadata_version=pa.ipc.MetadataVersion.V4)\n    with ConstantFlightServer(options=options) as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        data = client.do_get(flight.Ticket(b'ints')).read_all()\n        assert data.equals(table)\n\n        # Also test via RecordBatchReader interface\n        data = client.do_get(flight.Ticket(b'ints')).to_reader().read_all()\n        assert data.equals(table)\n\n    with pytest.raises(flight.FlightServerError,\n                       match=\"expected IpcWriteOptions, got <class 'int'>\"):\n        with ConstantFlightServer(options=42) as server, \\\n                flight.connect(('localhost', server.port)) as client:\n            data = client.do_get(flight.Ticket(b'ints')).read_all()\n\n\n@pytest.mark.pandas\ndef test_do_get_ints_pandas():\n    \"\"\"Try a simple do_get call.\"\"\"\n    table = simple_ints_table()\n\n    with ConstantFlightServer() as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        data = client.do_get(flight.Ticket(b'ints')).read_pandas()\n        assert list(data['some_ints']) == table.column(0).to_pylist()\n\n\ndef test_flight_do_get_dicts():\n    table = simple_dicts_table()\n\n    with ConstantFlightServer() as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        data = client.do_get(flight.Ticket(b'dicts')).read_all()\n        assert data.equals(table)\n\n\ndef test_flight_do_get_ticket():\n    \"\"\"Make sure Tickets get passed to the server.\"\"\"\n    data1 = [pa.array([-10, -5, 0, 5, 10], type=pa.int32())]\n    table = pa.Table.from_arrays(data1, names=['a'])\n    with CheckTicketFlightServer(expected_ticket=b'the-ticket') as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        data = client.do_get(flight.Ticket(b'the-ticket')).read_all()\n        assert data.equals(table)\n\n\ndef test_flight_get_info():\n    \"\"\"Make sure FlightEndpoint accepts string and object URIs.\"\"\"\n    with GetInfoFlightServer() as server:\n        client = FlightClient(('localhost', server.port))\n        info = client.get_flight_info(flight.FlightDescriptor.for_command(b''))\n        assert info.total_records == -1\n        assert info.total_bytes == -1\n        assert info.schema == pa.schema([('a', pa.int32())])\n        assert len(info.endpoints) == 2\n        assert len(info.endpoints[0].locations) == 1\n        assert info.endpoints[0].locations[0] == flight.Location('grpc://test')\n        assert info.endpoints[1].locations[0] == \\\n            flight.Location.for_grpc_tcp('localhost', 5005)\n\n\ndef test_flight_get_schema():\n    \"\"\"Make sure GetSchema returns correct schema.\"\"\"\n    with GetInfoFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        info = client.get_schema(flight.FlightDescriptor.for_command(b''))\n        assert info.schema == pa.schema([('a', pa.int32())])\n\n\ndef test_list_actions():\n    \"\"\"Make sure the return type of ListActions is validated.\"\"\"\n    # ARROW-6392\n    with ListActionsErrorFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        with pytest.raises(\n                flight.FlightServerError,\n                match=(\"Results of list_actions must be \"\n                       \"ActionType or tuple\")\n        ):\n            list(client.list_actions())\n\n    with ListActionsFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        assert list(client.list_actions()) == \\\n            ListActionsFlightServer.expected_actions()\n\n\nclass ConvenienceServer(FlightServerBase):\n    \"\"\"\n    Server for testing various implementation conveniences (auto-boxing, etc.)\n    \"\"\"\n\n    @property\n    def simple_action_results(self):\n        return [b'foo', b'bar', b'baz']\n\n    def do_action(self, context, action):\n        if action.type == 'simple-action':\n            return self.simple_action_results\n        elif action.type == 'echo':\n            return [action.body]\n        elif action.type == 'bad-action':\n            return ['foo']\n        elif action.type == 'arrow-exception':\n            raise pa.ArrowMemoryError()\n        elif action.type == 'forever':\n            def gen():\n                while not context.is_cancelled():\n                    yield b'foo'\n            return gen()\n\n\ndef test_do_action_result_convenience():\n    with ConvenienceServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n\n        # do_action as action type without body\n        results = [x.body for x in client.do_action('simple-action')]\n        assert results == server.simple_action_results\n\n        # do_action with tuple of type and body\n        body = b'the-body'\n        results = [x.body for x in client.do_action(('echo', body))]\n        assert results == [body]\n\n\ndef test_nicer_server_exceptions():\n    with ConvenienceServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        with pytest.raises(flight.FlightServerError,\n                           match=\"a bytes-like object is required\"):\n            list(client.do_action('bad-action'))\n        # While Flight/C++ sends across the original status code, it\n        # doesn't get mapped to the equivalent code here, since we\n        # want to be able to distinguish between client- and server-\n        # side errors.\n        with pytest.raises(flight.FlightServerError,\n                           match=\"ArrowMemoryError\"):\n            list(client.do_action('arrow-exception'))\n\n\ndef test_get_port():\n    \"\"\"Make sure port() works.\"\"\"\n    server = GetInfoFlightServer(\"grpc://localhost:0\")\n    try:\n        assert server.port > 0\n    finally:\n        server.shutdown()\n\n\n@pytest.mark.skipif(os.name == 'nt',\n                    reason=\"Unix sockets can't be tested on Windows\")\ndef test_flight_domain_socket():\n    \"\"\"Try a simple do_get call over a Unix domain socket.\"\"\"\n    with tempfile.NamedTemporaryFile() as sock:\n        sock.close()\n        location = flight.Location.for_grpc_unix(sock.name)\n        with ConstantFlightServer(location=location), \\\n                FlightClient(location) as client:\n\n            reader = client.do_get(flight.Ticket(b'ints'))\n            table = simple_ints_table()\n            assert reader.schema.equals(table.schema)\n            data = reader.read_all()\n            assert data.equals(table)\n\n            reader = client.do_get(flight.Ticket(b'dicts'))\n            table = simple_dicts_table()\n            assert reader.schema.equals(table.schema)\n            data = reader.read_all()\n            assert data.equals(table)\n\n\n@pytest.mark.slow\ndef test_flight_large_message():\n    \"\"\"Try sending/receiving a large message via Flight.\n\n    See ARROW-4421: by default, gRPC won't allow us to send messages >\n    4MiB in size.\n    \"\"\"\n    data = pa.Table.from_arrays([\n        pa.array(range(0, 10 * 1024 * 1024))\n    ], names=['a'])\n\n    with EchoFlightServer(expected_schema=data.schema) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        writer, _ = client.do_put(flight.FlightDescriptor.for_path('test'),\n                                  data.schema)\n        # Write a single giant chunk\n        writer.write_table(data, 10 * 1024 * 1024)\n        writer.close()\n        result = client.do_get(flight.Ticket(b'')).read_all()\n        assert result.equals(data)\n\n\ndef test_flight_generator_stream():\n    \"\"\"Try downloading a flight of RecordBatches in a GeneratorStream.\"\"\"\n    data = pa.Table.from_arrays([\n        pa.array(range(0, 10 * 1024))\n    ], names=['a'])\n\n    with EchoStreamFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        writer, _ = client.do_put(flight.FlightDescriptor.for_path('test'),\n                                  data.schema)\n        writer.write_table(data)\n        writer.close()\n        result = client.do_get(flight.Ticket(b'')).read_all()\n        assert result.equals(data)\n\n\ndef test_flight_invalid_generator_stream():\n    \"\"\"Try streaming data with mismatched schemas.\"\"\"\n    with InvalidStreamFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        with pytest.raises(pa.ArrowException):\n            client.do_get(flight.Ticket(b'')).read_all()\n\n\ndef test_timeout_fires():\n    \"\"\"Make sure timeouts fire on slow requests.\"\"\"\n    # Do this in a separate thread so that if it fails, we don't hang\n    # the entire test process\n    with SlowFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        action = flight.Action(\"\", b\"\")\n        options = flight.FlightCallOptions(timeout=0.2)\n        # gRPC error messages change based on version, so don't look\n        # for a particular error\n        with pytest.raises(flight.FlightTimedOutError):\n            list(client.do_action(action, options=options))\n\n\ndef test_timeout_passes():\n    \"\"\"Make sure timeouts do not fire on fast requests.\"\"\"\n    with ConstantFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        options = flight.FlightCallOptions(timeout=5.0)\n        client.do_get(flight.Ticket(b'ints'), options=options).read_all()\n\n\ndef test_read_options():\n    \"\"\"Make sure ReadOptions can be used.\"\"\"\n    expected = pa.Table.from_arrays([pa.array([1, 2, 3, 4])], names=[\"b\"])\n    with ConstantFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        options = flight.FlightCallOptions(\n            read_options=IpcReadOptions(included_fields=[1]))\n        response1 = client.do_get(flight.Ticket(\n            b'multi'), options=options).read_all()\n        response2 = client.do_get(flight.Ticket(b'multi')).read_all()\n\n        assert response2.num_columns == 2\n        assert response1.num_columns == 1\n        assert response1 == expected\n        assert response2 == multiple_column_table()\n\n\nbasic_auth_handler = HttpBasicServerAuthHandler(creds={\n    b\"test\": b\"p4ssw0rd\",\n})\n\ntoken_auth_handler = TokenServerAuthHandler(creds={\n    b\"test\": b\"p4ssw0rd\",\n})\n\n\n@pytest.mark.slow\ndef test_http_basic_unauth():\n    \"\"\"Test that auth fails when not authenticated.\"\"\"\n    with EchoStreamFlightServer(auth_handler=basic_auth_handler) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        action = flight.Action(\"who-am-i\", b\"\")\n        with pytest.raises(flight.FlightUnauthenticatedError,\n                           match=\".*unauthenticated.*\"):\n            list(client.do_action(action))\n\n\n@pytest.mark.skipif(os.name == 'nt',\n                    reason=\"ARROW-10013: gRPC on Windows corrupts peer()\")\ndef test_http_basic_auth():\n    \"\"\"Test a Python implementation of HTTP basic authentication.\"\"\"\n    with EchoStreamFlightServer(auth_handler=basic_auth_handler) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        action = flight.Action(\"who-am-i\", b\"\")\n        client.authenticate(HttpBasicClientAuthHandler('test', 'p4ssw0rd'))\n        results = client.do_action(action)\n        identity = next(results)\n        assert identity.body.to_pybytes() == b'test'\n        peer_address = next(results)\n        assert peer_address.body.to_pybytes() != b''\n\n\ndef test_http_basic_auth_invalid_password():\n    \"\"\"Test that auth fails with the wrong password.\"\"\"\n    with EchoStreamFlightServer(auth_handler=basic_auth_handler) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        action = flight.Action(\"who-am-i\", b\"\")\n        with pytest.raises(flight.FlightUnauthenticatedError,\n                           match=\".*wrong password.*\"):\n            client.authenticate(HttpBasicClientAuthHandler('test', 'wrong'))\n            next(client.do_action(action))\n\n\ndef test_token_auth():\n    \"\"\"Test an auth mechanism that uses a handshake.\"\"\"\n    with EchoStreamFlightServer(auth_handler=token_auth_handler) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        action = flight.Action(\"who-am-i\", b\"\")\n        client.authenticate(TokenClientAuthHandler('test', 'p4ssw0rd'))\n        identity = next(client.do_action(action))\n        assert identity.body.to_pybytes() == b'test'\n\n\ndef test_token_auth_invalid():\n    \"\"\"Test an auth mechanism that uses a handshake.\"\"\"\n    with EchoStreamFlightServer(auth_handler=token_auth_handler) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        with pytest.raises(flight.FlightUnauthenticatedError):\n            client.authenticate(TokenClientAuthHandler('test', 'wrong'))\n\n\nheader_auth_server_middleware_factory = HeaderAuthServerMiddlewareFactory()\nno_op_auth_handler = NoopAuthHandler()\n\n\ndef test_authenticate_basic_token():\n    \"\"\"Test authenticate_basic_token with bearer token and auth headers.\"\"\"\n    with HeaderAuthFlightServer(auth_handler=no_op_auth_handler, middleware={\n        \"auth\": HeaderAuthServerMiddlewareFactory()\n    }) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        token_pair = client.authenticate_basic_token(b'test', b'password')\n        assert token_pair[0] == b'authorization'\n        assert token_pair[1] == b'Bearer token1234'\n\n\ndef test_authenticate_basic_token_invalid_password():\n    \"\"\"Test authenticate_basic_token with an invalid password.\"\"\"\n    with HeaderAuthFlightServer(auth_handler=no_op_auth_handler, middleware={\n        \"auth\": HeaderAuthServerMiddlewareFactory()\n    }) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        with pytest.raises(flight.FlightUnauthenticatedError):\n            client.authenticate_basic_token(b'test', b'badpassword')\n\n\ndef test_authenticate_basic_token_and_action():\n    \"\"\"Test authenticate_basic_token and doAction after authentication.\"\"\"\n    with HeaderAuthFlightServer(auth_handler=no_op_auth_handler, middleware={\n        \"auth\": HeaderAuthServerMiddlewareFactory()\n    }) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        token_pair = client.authenticate_basic_token(b'test', b'password')\n        assert token_pair[0] == b'authorization'\n        assert token_pair[1] == b'Bearer token1234'\n        options = flight.FlightCallOptions(headers=[token_pair])\n        result = list(client.do_action(\n            action=flight.Action('test-action', b''), options=options))\n        assert result[0].body.to_pybytes() == b'token1234'\n\n\ndef test_authenticate_basic_token_with_client_middleware():\n    \"\"\"Test authenticate_basic_token with client middleware\n       to intercept authorization header returned by the\n       HTTP header auth enabled server.\n    \"\"\"\n    with HeaderAuthFlightServer(auth_handler=no_op_auth_handler, middleware={\n        \"auth\": HeaderAuthServerMiddlewareFactory()\n    }) as server:\n        client_auth_middleware = ClientHeaderAuthMiddlewareFactory()\n        client = FlightClient(\n            ('localhost', server.port),\n            middleware=[client_auth_middleware]\n        )\n        encoded_credentials = base64.b64encode(b'test:password')\n        options = flight.FlightCallOptions(headers=[\n            (b'authorization', b'Basic ' + encoded_credentials)\n        ])\n        result = list(client.do_action(\n            action=flight.Action('test-action', b''), options=options))\n        assert result[0].body.to_pybytes() == b'token1234'\n        assert client_auth_middleware.call_credential[0] == b'authorization'\n        assert client_auth_middleware.call_credential[1] == \\\n            b'Bearer ' + b'token1234'\n        result2 = list(client.do_action(\n            action=flight.Action('test-action', b''), options=options))\n        assert result2[0].body.to_pybytes() == b'token1234'\n        assert client_auth_middleware.call_credential[0] == b'authorization'\n        assert client_auth_middleware.call_credential[1] == \\\n            b'Bearer ' + b'token1234'\n        client.close()\n\n\ndef test_arbitrary_headers_in_flight_call_options():\n    \"\"\"Test passing multiple arbitrary headers to the middleware.\"\"\"\n    with ArbitraryHeadersFlightServer(\n        auth_handler=no_op_auth_handler,\n        middleware={\n            \"auth\": HeaderAuthServerMiddlewareFactory(),\n            \"arbitrary-headers\": ArbitraryHeadersServerMiddlewareFactory()\n        }) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        token_pair = client.authenticate_basic_token(b'test', b'password')\n        assert token_pair[0] == b'authorization'\n        assert token_pair[1] == b'Bearer token1234'\n        options = flight.FlightCallOptions(headers=[\n            token_pair,\n            (b'test-header-1', b'value1'),\n            (b'test-header-2', b'value2')\n        ])\n        result = list(client.do_action(flight.Action(\n            \"test-action\", b\"\"), options=options))\n        assert result[0].body.to_pybytes() == b'value1'\n        assert result[1].body.to_pybytes() == b'value2'\n\n\ndef test_location_invalid():\n    \"\"\"Test constructing invalid URIs.\"\"\"\n    with pytest.raises(pa.ArrowInvalid, match=\".*Cannot parse URI:.*\"):\n        flight.connect(\"%\")\n\n    with pytest.raises(pa.ArrowInvalid, match=\".*Cannot parse URI:.*\"):\n        ConstantFlightServer(\"%\")\n\n\ndef test_location_unknown_scheme():\n    \"\"\"Test creating locations for unknown schemes.\"\"\"\n    assert flight.Location(\"s3://foo\").uri == b\"s3://foo\"\n    assert flight.Location(\"https://example.com/bar.parquet\").uri == \\\n        b\"https://example.com/bar.parquet\"\n\n\n@pytest.mark.slow\n@pytest.mark.requires_testing_data\ndef test_tls_fails():\n    \"\"\"Make sure clients cannot connect when cert verification fails.\"\"\"\n    certs = example_tls_certs()\n\n    # Ensure client doesn't connect when certificate verification\n    # fails (this is a slow test since gRPC does retry a few times)\n    with ConstantFlightServer(tls_certificates=certs[\"certificates\"]) as s, \\\n            FlightClient(\"grpc+tls://localhost:\" + str(s.port)) as client:\n        # gRPC error messages change based on version, so don't look\n        # for a particular error\n        with pytest.raises(flight.FlightUnavailableError):\n            client.do_get(flight.Ticket(b'ints')).read_all()\n\n\n@pytest.mark.requires_testing_data\ndef test_tls_do_get():\n    \"\"\"Try a simple do_get call over TLS.\"\"\"\n    table = simple_ints_table()\n    certs = example_tls_certs()\n\n    with ConstantFlightServer(tls_certificates=certs[\"certificates\"]) as s, \\\n        FlightClient(('localhost', s.port),\n                     tls_root_certs=certs[\"root_cert\"]) as client:\n        data = client.do_get(flight.Ticket(b'ints')).read_all()\n        assert data.equals(table)\n\n\n@pytest.mark.requires_testing_data\ndef test_tls_disable_server_verification():\n    \"\"\"Try a simple do_get call over TLS with server verification disabled.\"\"\"\n    table = simple_ints_table()\n    certs = example_tls_certs()\n\n    with ConstantFlightServer(tls_certificates=certs[\"certificates\"]) as s:\n        try:\n            client = FlightClient(('localhost', s.port),\n                                  disable_server_verification=True)\n        except NotImplementedError:\n            pytest.skip('disable_server_verification feature is not available')\n        data = client.do_get(flight.Ticket(b'ints')).read_all()\n        assert data.equals(table)\n        client.close()\n\n\n@pytest.mark.requires_testing_data\ndef test_tls_override_hostname():\n    \"\"\"Check that incorrectly overriding the hostname fails.\"\"\"\n    certs = example_tls_certs()\n\n    with ConstantFlightServer(tls_certificates=certs[\"certificates\"]) as s, \\\n        flight.connect(('localhost', s.port),\n                       tls_root_certs=certs[\"root_cert\"],\n                       override_hostname=\"fakehostname\") as client:\n        with pytest.raises(flight.FlightUnavailableError):\n            client.do_get(flight.Ticket(b'ints'))\n\n\ndef test_flight_do_get_metadata():\n    \"\"\"Try a simple do_get call with metadata.\"\"\"\n    data = [\n        pa.array([-10, -5, 0, 5, 10])\n    ]\n    table = pa.Table.from_arrays(data, names=['a'])\n\n    batches = []\n    with MetadataFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        reader = client.do_get(flight.Ticket(b''))\n        idx = 0\n        while True:\n            try:\n                batch, metadata = reader.read_chunk()\n                batches.append(batch)\n                server_idx, = struct.unpack('<i', metadata.to_pybytes())\n                assert idx == server_idx\n                idx += 1\n            except StopIteration:\n                break\n        data = pa.Table.from_batches(batches)\n        assert data.equals(table)\n\n\ndef test_flight_do_get_metadata_v4():\n    \"\"\"Try a simple do_get call with V4 metadata version.\"\"\"\n    table = pa.Table.from_arrays(\n        [pa.array([-10, -5, 0, 5, 10])], names=['a'])\n    options = pa.ipc.IpcWriteOptions(\n        metadata_version=pa.ipc.MetadataVersion.V4)\n    with MetadataFlightServer(options=options) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        reader = client.do_get(flight.Ticket(b''))\n        data = reader.read_all()\n        assert data.equals(table)\n\n\ndef test_flight_do_put_metadata():\n    \"\"\"Try a simple do_put call with metadata.\"\"\"\n    data = [\n        pa.array([-10, -5, 0, 5, 10])\n    ]\n    table = pa.Table.from_arrays(data, names=['a'])\n\n    with MetadataFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        writer, metadata_reader = client.do_put(\n            flight.FlightDescriptor.for_path(''),\n            table.schema)\n        with writer:\n            for idx, batch in enumerate(table.to_batches(max_chunksize=1)):\n                metadata = struct.pack('<i', idx)\n                writer.write_with_metadata(batch, metadata)\n                buf = metadata_reader.read()\n                assert buf is not None\n                server_idx, = struct.unpack('<i', buf.to_pybytes())\n                assert idx == server_idx\n\n\ndef test_flight_do_put_limit():\n    \"\"\"Try a simple do_put call with a size limit.\"\"\"\n    large_batch = pa.RecordBatch.from_arrays([\n        pa.array(np.ones(768, dtype=np.int64())),\n    ], names=['a'])\n\n    with EchoFlightServer() as server, \\\n        FlightClient(('localhost', server.port),\n                     write_size_limit_bytes=4096) as client:\n        writer, metadata_reader = client.do_put(\n            flight.FlightDescriptor.for_path(''),\n            large_batch.schema)\n        with writer:\n            with pytest.raises(flight.FlightWriteSizeExceededError,\n                               match=\"exceeded soft limit\") as excinfo:\n                writer.write_batch(large_batch)\n            assert excinfo.value.limit == 4096\n            smaller_batches = [\n                large_batch.slice(0, 384),\n                large_batch.slice(384),\n            ]\n            for batch in smaller_batches:\n                writer.write_batch(batch)\n        expected = pa.Table.from_batches([large_batch])\n        actual = client.do_get(flight.Ticket(b'')).read_all()\n        assert expected == actual\n\n\n@pytest.mark.slow\ndef test_cancel_do_get():\n    \"\"\"Test canceling a DoGet operation on the client side.\"\"\"\n    with ConstantFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        reader = client.do_get(flight.Ticket(b'ints'))\n        reader.cancel()\n        with pytest.raises(flight.FlightCancelledError,\n                           match=\"(?i).*cancel.*\"):\n            reader.read_chunk()\n\n\n@pytest.mark.slow\ndef test_cancel_do_get_threaded():\n    \"\"\"Test canceling a DoGet operation from another thread.\"\"\"\n    with SlowFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        reader = client.do_get(flight.Ticket(b'ints'))\n\n        read_first_message = threading.Event()\n        stream_canceled = threading.Event()\n        result_lock = threading.Lock()\n        raised_proper_exception = threading.Event()\n\n        def block_read():\n            reader.read_chunk()\n            read_first_message.set()\n            stream_canceled.wait(timeout=5)\n            try:\n                reader.read_chunk()\n            except flight.FlightCancelledError:\n                with result_lock:\n                    raised_proper_exception.set()\n\n        thread = threading.Thread(target=block_read, daemon=True)\n        thread.start()\n        read_first_message.wait(timeout=5)\n        reader.cancel()\n        stream_canceled.set()\n        thread.join(timeout=1)\n\n        with result_lock:\n            assert raised_proper_exception.is_set()\n\n\ndef test_streaming_do_action():\n    with ConvenienceServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        results = client.do_action(flight.Action('forever', b''))\n        assert next(results).body == b'foo'\n        # Implicit cancel when destructed\n        del results\n\n\ndef test_roundtrip_types():\n    \"\"\"Make sure serializable types round-trip.\"\"\"\n    action = flight.Action(\"action1\", b\"action1-body\")\n    assert action == flight.Action.deserialize(action.serialize())\n\n    ticket = flight.Ticket(\"foo\")\n    assert ticket == flight.Ticket.deserialize(ticket.serialize())\n\n    result = flight.Result(b\"result1\")\n    assert result == flight.Result.deserialize(result.serialize())\n\n    basic_auth = flight.BasicAuth(\"username1\", \"password1\")\n    assert basic_auth == flight.BasicAuth.deserialize(basic_auth.serialize())\n\n    schema_result = flight.SchemaResult(pa.schema([('a', pa.int32())]))\n    assert schema_result == flight.SchemaResult.deserialize(\n        schema_result.serialize())\n\n    desc = flight.FlightDescriptor.for_command(\"test\")\n    assert desc == flight.FlightDescriptor.deserialize(desc.serialize())\n\n    desc = flight.FlightDescriptor.for_path(\"a\", \"b\", \"test.arrow\")\n    assert desc == flight.FlightDescriptor.deserialize(desc.serialize())\n\n    info = flight.FlightInfo(\n        pa.schema([('a', pa.int32())]),\n        desc,\n        [\n            flight.FlightEndpoint(b'', ['grpc://test']),\n            flight.FlightEndpoint(\n                b'',\n                [flight.Location.for_grpc_tcp('localhost', 5005)],\n            ),\n        ],\n        -1,\n        -1,\n    )\n    info2 = flight.FlightInfo.deserialize(info.serialize())\n    assert info.schema == info2.schema\n    assert info.descriptor == info2.descriptor\n    assert info.total_bytes == info2.total_bytes\n    assert info.total_records == info2.total_records\n    assert info.endpoints == info2.endpoints\n\n    endpoint = flight.FlightEndpoint(\n        ticket,\n        ['grpc://test', flight.Location.for_grpc_tcp('localhost', 5005)]\n    )\n    assert endpoint == flight.FlightEndpoint.deserialize(endpoint.serialize())\n\n\ndef test_roundtrip_errors():\n    \"\"\"Ensure that Flight errors propagate from server to client.\"\"\"\n    with ErrorFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n\n        for arg, exc_type in ErrorFlightServer.error_cases().items():\n            with pytest.raises(exc_type, match=\".*foo.*\"):\n                list(client.do_action(flight.Action(arg, b\"\")))\n        with pytest.raises(flight.FlightInternalError, match=\".*foo.*\"):\n            list(client.list_flights())\n\n        data = [pa.array([-10, -5, 0, 5, 10])]\n        table = pa.Table.from_arrays(data, names=['a'])\n\n        exceptions = {\n            'internal': flight.FlightInternalError,\n            'timedout': flight.FlightTimedOutError,\n            'cancel': flight.FlightCancelledError,\n            'unauthenticated': flight.FlightUnauthenticatedError,\n            'unauthorized': flight.FlightUnauthorizedError,\n        }\n\n        for command, exception in exceptions.items():\n\n            with pytest.raises(exception, match=\".*foo.*\"):\n                writer, reader = client.do_put(\n                    flight.FlightDescriptor.for_command(command),\n                    table.schema)\n                writer.write_table(table)\n                writer.close()\n\n            with pytest.raises(exception, match=\".*foo.*\"):\n                writer, reader = client.do_put(\n                    flight.FlightDescriptor.for_command(command),\n                    table.schema)\n                writer.close()\n\n\ndef test_do_put_independent_read_write():\n    \"\"\"Ensure that separate threads can read/write on a DoPut.\"\"\"\n    # ARROW-6063: previously this would cause gRPC to abort when the\n    # writer was closed (due to simultaneous reads), or would hang\n    # forever.\n    data = [\n        pa.array([-10, -5, 0, 5, 10])\n    ]\n    table = pa.Table.from_arrays(data, names=['a'])\n\n    with MetadataFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        writer, metadata_reader = client.do_put(\n            flight.FlightDescriptor.for_path(''),\n            table.schema)\n\n        count = [0]\n\n        def _reader_thread():\n            while metadata_reader.read() is not None:\n                count[0] += 1\n\n        thread = threading.Thread(target=_reader_thread)\n        thread.start()\n\n        batches = table.to_batches(max_chunksize=1)\n        with writer:\n            for idx, batch in enumerate(batches):\n                metadata = struct.pack('<i', idx)\n                writer.write_with_metadata(batch, metadata)\n            # Causes the server to stop writing and end the call\n            writer.done_writing()\n            # Thus reader thread will break out of loop\n            thread.join()\n        # writer.close() won't segfault since reader thread has\n        # stopped\n        assert count[0] == len(batches)\n\n\ndef test_server_middleware_same_thread():\n    \"\"\"Ensure that server middleware run on the same thread as the RPC.\"\"\"\n    with HeaderFlightServer(middleware={\n        \"test\": HeaderServerMiddlewareFactory(),\n    }) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        results = list(client.do_action(flight.Action(b\"test\", b\"\")))\n        assert len(results) == 1\n        value = results[0].body.to_pybytes()\n        assert b\"right value\" == value\n\n\ndef test_middleware_reject():\n    \"\"\"Test rejecting an RPC with server middleware.\"\"\"\n    with HeaderFlightServer(middleware={\n        \"test\": SelectiveAuthServerMiddlewareFactory(),\n    }) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        # The middleware allows this through without auth.\n        with pytest.raises(pa.ArrowNotImplementedError):\n            list(client.list_actions())\n\n        # But not anything else.\n        with pytest.raises(flight.FlightUnauthenticatedError):\n            list(client.do_action(flight.Action(b\"\", b\"\")))\n\n        client = FlightClient(\n            ('localhost', server.port),\n            middleware=[SelectiveAuthClientMiddlewareFactory()]\n        )\n        response = next(client.do_action(flight.Action(b\"\", b\"\")))\n        assert b\"password\" == response.body.to_pybytes()\n\n\ndef test_middleware_mapping():\n    \"\"\"Test that middleware records methods correctly.\"\"\"\n    server_middleware = RecordingServerMiddlewareFactory()\n    client_middleware = RecordingClientMiddlewareFactory()\n    with FlightServerBase(middleware={\"test\": server_middleware}) as server, \\\n        FlightClient(\n            ('localhost', server.port),\n            middleware=[client_middleware]\n    ) as client:\n\n        descriptor = flight.FlightDescriptor.for_command(b\"\")\n        with pytest.raises(NotImplementedError):\n            list(client.list_flights())\n        with pytest.raises(NotImplementedError):\n            client.get_flight_info(descriptor)\n        with pytest.raises(NotImplementedError):\n            client.get_schema(descriptor)\n        with pytest.raises(NotImplementedError):\n            client.do_get(flight.Ticket(b\"\"))\n        with pytest.raises(NotImplementedError):\n            writer, _ = client.do_put(descriptor, pa.schema([]))\n            writer.close()\n        with pytest.raises(NotImplementedError):\n            list(client.do_action(flight.Action(b\"\", b\"\")))\n        with pytest.raises(NotImplementedError):\n            list(client.list_actions())\n        with pytest.raises(NotImplementedError):\n            writer, _ = client.do_exchange(descriptor)\n            writer.close()\n\n        expected = [\n            flight.FlightMethod.LIST_FLIGHTS,\n            flight.FlightMethod.GET_FLIGHT_INFO,\n            flight.FlightMethod.GET_SCHEMA,\n            flight.FlightMethod.DO_GET,\n            flight.FlightMethod.DO_PUT,\n            flight.FlightMethod.DO_ACTION,\n            flight.FlightMethod.LIST_ACTIONS,\n            flight.FlightMethod.DO_EXCHANGE,\n        ]\n        assert server_middleware.methods == expected\n        assert client_middleware.methods == expected\n\n\ndef test_extra_info():\n    with ErrorFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        try:\n            list(client.do_action(flight.Action(\"protobuf\", b\"\")))\n            assert False\n        except flight.FlightUnauthorizedError as e:\n            assert e.extra_info is not None\n            ei = e.extra_info\n            assert ei == b'this is an error message'\n\n\n@pytest.mark.requires_testing_data\ndef test_mtls():\n    \"\"\"Test mutual TLS (mTLS) with gRPC.\"\"\"\n    certs = example_tls_certs()\n    table = simple_ints_table()\n\n    with ConstantFlightServer(\n            tls_certificates=[certs[\"certificates\"][0]],\n            verify_client=True,\n            root_certificates=certs[\"root_cert\"]) as s, \\\n        FlightClient(\n            ('localhost', s.port),\n            tls_root_certs=certs[\"root_cert\"],\n            cert_chain=certs[\"certificates\"][0].cert,\n            private_key=certs[\"certificates\"][0].key) as client:\n        data = client.do_get(flight.Ticket(b'ints')).read_all()\n        assert data.equals(table)\n\n\ndef test_doexchange_get():\n    \"\"\"Emulate DoGet with DoExchange.\"\"\"\n    expected = pa.Table.from_arrays([\n        pa.array(range(0, 10 * 1024))\n    ], names=[\"a\"])\n\n    with ExchangeFlightServer() as server, \\\n            FlightClient((\"localhost\", server.port)) as client:\n        descriptor = flight.FlightDescriptor.for_command(b\"get\")\n        writer, reader = client.do_exchange(descriptor)\n        with writer:\n            table = reader.read_all()\n        assert expected == table\n\n\ndef test_doexchange_put():\n    \"\"\"Emulate DoPut with DoExchange.\"\"\"\n    data = pa.Table.from_arrays([\n        pa.array(range(0, 10 * 1024))\n    ], names=[\"a\"])\n    batches = data.to_batches(max_chunksize=512)\n\n    with ExchangeFlightServer() as server, \\\n            FlightClient((\"localhost\", server.port)) as client:\n        descriptor = flight.FlightDescriptor.for_command(b\"put\")\n        writer, reader = client.do_exchange(descriptor)\n        with writer:\n            writer.begin(data.schema)\n            for batch in batches:\n                writer.write_batch(batch)\n            writer.done_writing()\n            chunk = reader.read_chunk()\n            assert chunk.data is None\n            expected_buf = str(len(batches)).encode(\"utf-8\")\n            assert chunk.app_metadata == expected_buf\n\n\ndef test_doexchange_echo():\n    \"\"\"Try a DoExchange echo server.\"\"\"\n    data = pa.Table.from_arrays([\n        pa.array(range(0, 10 * 1024))\n    ], names=[\"a\"])\n    batches = data.to_batches(max_chunksize=512)\n\n    with ExchangeFlightServer() as server, \\\n            FlightClient((\"localhost\", server.port)) as client:\n        descriptor = flight.FlightDescriptor.for_command(b\"echo\")\n        writer, reader = client.do_exchange(descriptor)\n        with writer:\n            # Read/write metadata before starting data.\n            for i in range(10):\n                buf = str(i).encode(\"utf-8\")\n                writer.write_metadata(buf)\n                chunk = reader.read_chunk()\n                assert chunk.data is None\n                assert chunk.app_metadata == buf\n\n            # Now write data without metadata.\n            writer.begin(data.schema)\n            for batch in batches:\n                writer.write_batch(batch)\n                assert reader.schema == data.schema\n                chunk = reader.read_chunk()\n                assert chunk.data == batch\n                assert chunk.app_metadata is None\n\n            # And write data with metadata.\n            for i, batch in enumerate(batches):\n                buf = str(i).encode(\"utf-8\")\n                writer.write_with_metadata(batch, buf)\n                chunk = reader.read_chunk()\n                assert chunk.data == batch\n                assert chunk.app_metadata == buf\n\n\ndef test_doexchange_echo_v4():\n    \"\"\"Try a DoExchange echo server using the V4 metadata version.\"\"\"\n    data = pa.Table.from_arrays([\n        pa.array(range(0, 10 * 1024))\n    ], names=[\"a\"])\n    batches = data.to_batches(max_chunksize=512)\n\n    options = pa.ipc.IpcWriteOptions(\n        metadata_version=pa.ipc.MetadataVersion.V4)\n    with ExchangeFlightServer(options=options) as server, \\\n            FlightClient((\"localhost\", server.port)) as client:\n        descriptor = flight.FlightDescriptor.for_command(b\"echo\")\n        writer, reader = client.do_exchange(descriptor)\n        with writer:\n            # Now write data without metadata.\n            writer.begin(data.schema, options=options)\n            for batch in batches:\n                writer.write_batch(batch)\n                assert reader.schema == data.schema\n                chunk = reader.read_chunk()\n                assert chunk.data == batch\n                assert chunk.app_metadata is None\n\n\ndef test_doexchange_transform():\n    \"\"\"Transform a table with a service.\"\"\"\n    data = pa.Table.from_arrays([\n        pa.array(range(0, 1024)),\n        pa.array(range(1, 1025)),\n        pa.array(range(2, 1026)),\n    ], names=[\"a\", \"b\", \"c\"])\n    expected = pa.Table.from_arrays([\n        pa.array(range(3, 1024 * 3 + 3, 3)),\n    ], names=[\"sum\"])\n\n    with ExchangeFlightServer() as server, \\\n            FlightClient((\"localhost\", server.port)) as client:\n        descriptor = flight.FlightDescriptor.for_command(b\"transform\")\n        writer, reader = client.do_exchange(descriptor)\n        with writer:\n            writer.begin(data.schema)\n            writer.write_table(data)\n            writer.done_writing()\n            table = reader.read_all()\n        assert expected == table\n\n\ndef test_middleware_multi_header():\n    \"\"\"Test sending/receiving multiple (binary-valued) headers.\"\"\"\n    with MultiHeaderFlightServer(middleware={\n        \"test\": MultiHeaderServerMiddlewareFactory(),\n    }) as server:\n        headers = MultiHeaderClientMiddlewareFactory()\n        with FlightClient(\n                ('localhost', server.port),\n                middleware=[headers]) as client:\n            response = next(client.do_action(flight.Action(b\"\", b\"\")))\n            # The server echoes the headers it got back to us.\n            raw_headers = response.body.to_pybytes().decode(\"utf-8\")\n            client_headers = ast.literal_eval(raw_headers)\n            # Don't directly compare; gRPC may add headers like User-Agent.\n            for header, values in MultiHeaderClientMiddleware.EXPECTED.items():\n                header = header.lower()\n                if isinstance(header, bytes):\n                    header = header.decode(\"ascii\")\n                assert client_headers.get(header) == values\n                assert headers.last_headers.get(header) == values\n\n\n@pytest.mark.requires_testing_data\ndef test_generic_options():\n    \"\"\"Test setting generic client options.\"\"\"\n    certs = example_tls_certs()\n\n    with ConstantFlightServer(tls_certificates=certs[\"certificates\"]) as s:\n        # Try setting a string argument that will make requests fail\n        options = [(\"grpc.ssl_target_name_override\", \"fakehostname\")]\n        client = flight.connect(('localhost', s.port),\n                                tls_root_certs=certs[\"root_cert\"],\n                                generic_options=options)\n        with pytest.raises(flight.FlightUnavailableError):\n            client.do_get(flight.Ticket(b'ints'))\n        client.close()\n        # Try setting an int argument that will make requests fail\n        options = [(\"grpc.max_receive_message_length\", 32)]\n        client = flight.connect(('localhost', s.port),\n                                tls_root_certs=certs[\"root_cert\"],\n                                generic_options=options)\n        with pytest.raises((pa.ArrowInvalid, flight.FlightCancelledError)):\n            client.do_get(flight.Ticket(b'ints'))\n        client.close()\n\n\nclass CancelFlightServer(FlightServerBase):\n    \"\"\"A server for testing StopToken.\"\"\"\n\n    def do_get(self, context, ticket):\n        schema = pa.schema([])\n        rb = pa.RecordBatch.from_arrays([], schema=schema)\n        return flight.GeneratorStream(schema, itertools.repeat(rb))\n\n    def do_exchange(self, context, descriptor, reader, writer):\n        schema = pa.schema([])\n        rb = pa.RecordBatch.from_arrays([], schema=schema)\n        writer.begin(schema)\n        while not context.is_cancelled():\n            writer.write_batch(rb)\n            time.sleep(0.5)\n\n\ndef test_interrupt():\n    if threading.current_thread().ident != threading.main_thread().ident:\n        pytest.skip(\"test only works from main Python thread\")\n    # Skips test if not available\n    raise_signal = util.get_raise_signal()\n\n    def signal_from_thread():\n        time.sleep(0.5)\n        raise_signal(signal.SIGINT)\n\n    exc_types = (KeyboardInterrupt, pa.ArrowCancelled)\n\n    def test(read_all):\n        try:\n            try:\n                t = threading.Thread(target=signal_from_thread)\n                with pytest.raises(exc_types) as exc_info:\n                    t.start()\n                    read_all()\n            finally:\n                t.join()\n        except KeyboardInterrupt:\n            # In case KeyboardInterrupt didn't interrupt read_all\n            # above, at least prevent it from stopping the test suite\n            pytest.fail(\"KeyboardInterrupt didn't interrupt Flight read_all\")\n        # __context__ is sometimes None\n        e = exc_info.value\n        assert isinstance(e, (pa.ArrowCancelled, KeyboardInterrupt)) or \\\n            isinstance(e.__context__, (pa.ArrowCancelled, KeyboardInterrupt))\n\n    with CancelFlightServer() as server, \\\n            FlightClient((\"localhost\", server.port)) as client:\n\n        reader = client.do_get(flight.Ticket(b\"\"))\n        test(reader.read_all)\n\n        descriptor = flight.FlightDescriptor.for_command(b\"echo\")\n        writer, reader = client.do_exchange(descriptor)\n        test(reader.read_all)\n        try:\n            writer.close()\n        except (KeyboardInterrupt, flight.FlightCancelledError):\n            # Silence the Cancelled/Interrupt exception\n            pass\n\n\ndef test_never_sends_data():\n    # Regression test for ARROW-12779\n    match = \"application server implementation error\"\n    with NeverSendsDataFlightServer() as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        with pytest.raises(flight.FlightServerError, match=match):\n            client.do_get(flight.Ticket(b'')).read_all()\n\n        # Check that the server handler will ignore empty tables\n        # up to a certain extent\n        table = client.do_get(flight.Ticket(b'yield_data')).read_all()\n        assert table.num_rows == 5\n\n\n@pytest.mark.large_memory\n@pytest.mark.slow\ndef test_large_descriptor():\n    # Regression test for ARROW-13253. Placed here with appropriate marks\n    # since some CI pipelines can't run the C++ equivalent\n    large_descriptor = flight.FlightDescriptor.for_command(\n        b' ' * (2 ** 31 + 1))\n    with FlightServerBase() as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        with pytest.raises(OSError,\n                           match=\"Failed to serialize Flight descriptor\"):\n            writer, _ = client.do_put(large_descriptor, pa.schema([]))\n            writer.close()\n        with pytest.raises(pa.ArrowException,\n                           match=\"Failed to serialize Flight descriptor\"):\n            client.do_exchange(large_descriptor)\n\n\n@pytest.mark.large_memory\n@pytest.mark.slow\ndef test_large_metadata_client():\n    # Regression test for ARROW-13253\n    descriptor = flight.FlightDescriptor.for_command(b'')\n    metadata = b' ' * (2 ** 31 + 1)\n    with EchoFlightServer() as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        with pytest.raises(pa.ArrowCapacityError,\n                           match=\"app_metadata size overflow\"):\n            writer, _ = client.do_put(descriptor, pa.schema([]))\n            with writer:\n                writer.write_metadata(metadata)\n                writer.close()\n        with pytest.raises(pa.ArrowCapacityError,\n                           match=\"app_metadata size overflow\"):\n            writer, reader = client.do_exchange(descriptor)\n            with writer:\n                writer.write_metadata(metadata)\n\n    del metadata\n    with LargeMetadataFlightServer() as server, \\\n            flight.connect(('localhost', server.port)) as client:\n        with pytest.raises(flight.FlightServerError,\n                           match=\"app_metadata size overflow\"):\n            reader = client.do_get(flight.Ticket(b''))\n            reader.read_all()\n        with pytest.raises(pa.ArrowException,\n                           match=\"app_metadata size overflow\"):\n            writer, reader = client.do_exchange(descriptor)\n            with writer:\n                reader.read_all()\n\n\nclass ActionNoneFlightServer(EchoFlightServer):\n    \"\"\"A server that implements a side effect to a non iterable action.\"\"\"\n    VALUES = []\n\n    def do_action(self, context, action):\n        if action.type == \"get_value\":\n            return [json.dumps(self.VALUES).encode('utf-8')]\n        elif action.type == \"append\":\n            self.VALUES.append(True)\n            return None\n        raise NotImplementedError\n\n\ndef test_none_action_side_effect():\n    \"\"\"Ensure that actions are executed even when we don't consume iterator.\n\n    See https://issues.apache.org/jira/browse/ARROW-14255\n    \"\"\"\n\n    with ActionNoneFlightServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        client.do_action(flight.Action(\"append\", b\"\"))\n        r = client.do_action(flight.Action(\"get_value\", b\"\"))\n        assert json.loads(next(r).body.to_pybytes()) == [True]\n\n\n@pytest.mark.slow  # Takes a while for gRPC to \"realize\" writes fail\ndef test_write_error_propagation():\n    \"\"\"\n    Ensure that exceptions during writing preserve error context.\n\n    See https://issues.apache.org/jira/browse/ARROW-16592.\n    \"\"\"\n    expected_message = \"foo\"\n    expected_info = b\"bar\"\n    exc = flight.FlightCancelledError(\n        expected_message, extra_info=expected_info)\n    descriptor = flight.FlightDescriptor.for_command(b\"\")\n    schema = pa.schema([(\"int64\", pa.int64())])\n\n    class FailServer(flight.FlightServerBase):\n        def do_put(self, context, descriptor, reader, writer):\n            raise exc\n\n        def do_exchange(self, context, descriptor, reader, writer):\n            raise exc\n\n    with FailServer() as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        # DoPut\n        writer, reader = client.do_put(descriptor, schema)\n\n        # Set a concurrent reader - ensure this doesn't block the\n        # writer side from calling Close()\n        def _reader():\n            try:\n                while True:\n                    reader.read()\n            except flight.FlightError:\n                return\n\n        thread = threading.Thread(target=_reader, daemon=True)\n        thread.start()\n\n        with pytest.raises(flight.FlightCancelledError) as exc_info:\n            while True:\n                writer.write_batch(pa.record_batch([[1]], schema=schema))\n        assert exc_info.value.extra_info == expected_info\n\n        with pytest.raises(flight.FlightCancelledError) as exc_info:\n            writer.close()\n        assert exc_info.value.extra_info == expected_info\n        thread.join()\n\n        # DoExchange\n        writer, reader = client.do_exchange(descriptor)\n\n        def _reader():\n            try:\n                while True:\n                    reader.read_chunk()\n            except flight.FlightError:\n                return\n\n        thread = threading.Thread(target=_reader, daemon=True)\n        thread.start()\n        with pytest.raises(flight.FlightCancelledError) as exc_info:\n            while True:\n                writer.write_metadata(b\" \")\n        assert exc_info.value.extra_info == expected_info\n\n        with pytest.raises(flight.FlightCancelledError) as exc_info:\n            writer.close()\n        assert exc_info.value.extra_info == expected_info\n        thread.join()\n\n\ndef test_interpreter_shutdown():\n    \"\"\"\n    Ensure that the gRPC server is stopped at interpreter shutdown.\n\n    See https://issues.apache.org/jira/browse/ARROW-16597.\n    \"\"\"\n    util.invoke_script(\"arrow_16597.py\")\n\n\nclass TracingFlightServer(FlightServerBase):\n    \"\"\"A server that echoes back trace context values.\"\"\"\n\n    def do_action(self, context, action):\n        trace_context = context.get_middleware(\"tracing\").trace_context\n        # Don't turn this method into a generator since then\n        # trace_context will be evaluated after we've exited the scope\n        # of the OTel span (and so the value we want won't be present)\n        return ((f\"{key}: {value}\").encode(\"utf-8\")\n                for (key, value) in trace_context.items())\n\n\ndef test_tracing():\n    with TracingFlightServer(middleware={\n            \"tracing\": flight.TracingServerMiddlewareFactory(),\n    }) as server, \\\n            FlightClient(('localhost', server.port)) as client:\n        # We can't tell if Arrow was built with OpenTelemetry support,\n        # so we can't count on any particular values being there; we\n        # can only ensure things don't blow up either way.\n        options = flight.FlightCallOptions(headers=[\n            # Pretend we have an OTel implementation\n            (b\"traceparent\", b\"00-000ff00f00f0ff000f0f00ff0f00fff0-\"\n                             b\"000f0000f0f00000-00\"),\n            (b\"tracestate\", b\"\"),\n        ])\n        for value in client.do_action((b\"\", b\"\"), options=options):\n            pass\n\n\ndef test_do_put_does_not_crash_when_schema_is_none():\n    client = FlightClient('grpc+tls://localhost:9643',\n                          disable_server_verification=True)\n    msg = (\"Argument 'schema' has incorrect type \"\n           r\"\\(expected pyarrow.lib.Schema, got NoneType\\)\")\n    with pytest.raises(TypeError, match=msg):\n        client.do_put(flight.FlightDescriptor.for_command('foo'),\n                      schema=None)\n\n\ndef test_headers_trailers():\n    \"\"\"Ensure that server-sent headers/trailers make it through.\"\"\"\n\n    class HeadersTrailersFlightServer(FlightServerBase):\n        def get_flight_info(self, context, descriptor):\n            context.add_header(\"x-header\", \"header-value\")\n            context.add_header(\"x-header-bin\", \"header\\x01value\")\n            context.add_trailer(\"x-trailer\", \"trailer-value\")\n            context.add_trailer(\"x-trailer-bin\", \"trailer\\x01value\")\n            return flight.FlightInfo(\n                pa.schema([]),\n                descriptor,\n                [],\n                -1, -1\n            )\n\n    class HeadersTrailersMiddlewareFactory(ClientMiddlewareFactory):\n        def __init__(self):\n            self.headers = []\n\n        def start_call(self, info):\n            return HeadersTrailersMiddleware(self)\n\n    class HeadersTrailersMiddleware(ClientMiddleware):\n        def __init__(self, factory):\n            self.factory = factory\n\n        def received_headers(self, headers):\n            for key, values in headers.items():\n                for value in values:\n                    self.factory.headers.append((key, value))\n\n    factory = HeadersTrailersMiddlewareFactory()\n    with HeadersTrailersFlightServer() as server, \\\n            FlightClient((\"localhost\", server.port), middleware=[factory]) as client:\n        client.get_flight_info(flight.FlightDescriptor.for_path(\"\"))\n        assert (\"x-header\", \"header-value\") in factory.headers\n        assert (\"x-header-bin\", b\"header\\x01value\") in factory.headers\n        assert (\"x-trailer\", \"trailer-value\") in factory.headers\n        assert (\"x-trailer-bin\", b\"trailer\\x01value\") in factory.headers\n", "python/pyarrow/tests/strategies.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport datetime\nimport sys\n\nimport pytest\nimport hypothesis as h\nimport hypothesis.strategies as st\nimport hypothesis.extra.numpy as npst\ntry:\n    import hypothesis.extra.pytz as tzst\nexcept ImportError:\n    tzst = None\ntry:\n    import zoneinfo\nexcept ImportError:\n    zoneinfo = None\nif sys.platform == 'win32':\n    try:\n        import tzdata  # noqa:F401\n    except ImportError:\n        zoneinfo = None\nimport numpy as np\n\nimport pyarrow as pa\n\n\n# TODO(kszucs): alphanum_text, surrogate_text\ncustom_text = st.text(\n    alphabet=st.characters(\n        min_codepoint=0x41,\n        max_codepoint=0x7E\n    )\n)\n\nnull_type = st.just(pa.null())\nbool_type = st.just(pa.bool_())\n\nbinary_type = st.just(pa.binary())\nstring_type = st.just(pa.string())\nlarge_binary_type = st.just(pa.large_binary())\nlarge_string_type = st.just(pa.large_string())\nfixed_size_binary_type = st.builds(\n    pa.binary,\n    st.integers(min_value=0, max_value=16)\n)\nbinary_like_types = st.one_of(\n    binary_type,\n    string_type,\n    large_binary_type,\n    large_string_type,\n    fixed_size_binary_type\n)\n\nsigned_integer_types = st.sampled_from([\n    pa.int8(),\n    pa.int16(),\n    pa.int32(),\n    pa.int64()\n])\nunsigned_integer_types = st.sampled_from([\n    pa.uint8(),\n    pa.uint16(),\n    pa.uint32(),\n    pa.uint64()\n])\ninteger_types = st.one_of(signed_integer_types, unsigned_integer_types)\n\nfloating_types = st.sampled_from([\n    pa.float16(),\n    pa.float32(),\n    pa.float64()\n])\ndecimal128_type = st.builds(\n    pa.decimal128,\n    precision=st.integers(min_value=1, max_value=38),\n    scale=st.integers(min_value=1, max_value=38)\n)\ndecimal256_type = st.builds(\n    pa.decimal256,\n    precision=st.integers(min_value=1, max_value=76),\n    scale=st.integers(min_value=1, max_value=76)\n)\nnumeric_types = st.one_of(integer_types, floating_types,\n                          decimal128_type, decimal256_type)\n\ndate_types = st.sampled_from([\n    pa.date32(),\n    pa.date64()\n])\ntime_types = st.sampled_from([\n    pa.time32('s'),\n    pa.time32('ms'),\n    pa.time64('us'),\n    pa.time64('ns')\n])\n\nif tzst and zoneinfo:\n    timezones = st.one_of(st.none(), tzst.timezones(), st.timezones())\nelif tzst:\n    timezones = st.one_of(st.none(), tzst.timezones())\nelif zoneinfo:\n    timezones = st.one_of(st.none(), st.timezones())\nelse:\n    timezones = st.none()\ntimestamp_types = st.builds(\n    pa.timestamp,\n    unit=st.sampled_from(['s', 'ms', 'us', 'ns']),\n    tz=timezones\n)\nduration_types = st.builds(\n    pa.duration,\n    st.sampled_from(['s', 'ms', 'us', 'ns'])\n)\ninterval_types = st.just(pa.month_day_nano_interval())\ntemporal_types = st.one_of(\n    date_types,\n    time_types,\n    timestamp_types,\n    duration_types,\n    interval_types\n)\n\nprimitive_types = st.one_of(\n    null_type,\n    bool_type,\n    numeric_types,\n    temporal_types,\n    binary_like_types\n)\n\nmetadata = st.dictionaries(st.text(), st.text())\n\n\n@st.composite\ndef fields(draw, type_strategy=primitive_types):\n    name = draw(custom_text)\n    typ = draw(type_strategy)\n    if pa.types.is_null(typ):\n        nullable = True\n    else:\n        nullable = draw(st.booleans())\n    meta = draw(metadata)\n    return pa.field(name, type=typ, nullable=nullable, metadata=meta)\n\n\ndef list_types(item_strategy=primitive_types):\n    return (\n        st.builds(pa.list_, item_strategy) |\n        st.builds(pa.large_list, item_strategy) |\n        st.builds(\n            pa.list_,\n            item_strategy,\n            st.integers(min_value=0, max_value=16)\n        ) |\n        st.builds(pa.list_view, item_strategy) |\n        st.builds(pa.large_list_view, item_strategy)\n    )\n\n\n@st.composite\ndef struct_types(draw, item_strategy=primitive_types):\n    fields_strategy = st.lists(fields(item_strategy))\n    fields_rendered = draw(fields_strategy)\n    field_names = [field.name for field in fields_rendered]\n    # check that field names are unique, see ARROW-9997\n    h.assume(len(set(field_names)) == len(field_names))\n    return pa.struct(fields_rendered)\n\n\ndef dictionary_types(key_strategy=None, value_strategy=None):\n    if key_strategy is None:\n        key_strategy = signed_integer_types\n    if value_strategy is None:\n        value_strategy = st.one_of(\n            bool_type,\n            integer_types,\n            st.sampled_from([pa.float32(), pa.float64()]),\n            binary_type,\n            string_type,\n            fixed_size_binary_type,\n        )\n    return st.builds(pa.dictionary, key_strategy, value_strategy)\n\n\n@st.composite\ndef map_types(draw, key_strategy=primitive_types,\n              item_strategy=primitive_types):\n    key_type = draw(key_strategy)\n    h.assume(not pa.types.is_null(key_type))\n    value_type = draw(item_strategy)\n    return pa.map_(key_type, value_type)\n\n\n# union type\n# extension type\n\n\ndef schemas(type_strategy=primitive_types, max_fields=None):\n    children = st.lists(fields(type_strategy), max_size=max_fields)\n    return st.builds(pa.schema, children)\n\n\nall_types = st.deferred(\n    lambda: (\n        primitive_types |\n        list_types() |\n        struct_types() |\n        dictionary_types() |\n        map_types() |\n        list_types(all_types) |\n        struct_types(all_types)\n    )\n)\nall_fields = fields(all_types)\nall_schemas = schemas(all_types)\n\n\n_default_array_sizes = st.integers(min_value=0, max_value=20)\n\n\n@st.composite\ndef _pylist(draw, value_type, size, nullable=True):\n    arr = draw(arrays(value_type, size=size, nullable=False))\n    return arr.to_pylist()\n\n\n@st.composite\ndef _pymap(draw, key_type, value_type, size, nullable=True):\n    length = draw(size)\n    keys = draw(_pylist(key_type, size=length, nullable=False))\n    values = draw(_pylist(value_type, size=length, nullable=nullable))\n    return list(zip(keys, values))\n\n\n@st.composite\ndef arrays(draw, type, size=None, nullable=True):\n    if isinstance(type, st.SearchStrategy):\n        ty = draw(type)\n    elif isinstance(type, pa.DataType):\n        ty = type\n    else:\n        raise TypeError('Type must be a pyarrow DataType')\n\n    if isinstance(size, st.SearchStrategy):\n        size = draw(size)\n    elif size is None:\n        size = draw(_default_array_sizes)\n    elif not isinstance(size, int):\n        raise TypeError('Size must be an integer')\n\n    if pa.types.is_null(ty):\n        h.assume(nullable)\n        value = st.none()\n    elif pa.types.is_boolean(ty):\n        value = st.booleans()\n    elif pa.types.is_integer(ty):\n        values = draw(npst.arrays(ty.to_pandas_dtype(), shape=(size,)))\n        return pa.array(values, type=ty)\n    elif pa.types.is_floating(ty):\n        values = draw(npst.arrays(ty.to_pandas_dtype(), shape=(size,)))\n        # Workaround ARROW-4952: no easy way to assert array equality\n        # in a NaN-tolerant way.\n        values[np.isnan(values)] = -42.0\n        return pa.array(values, type=ty)\n    elif pa.types.is_decimal(ty):\n        # TODO(kszucs): properly limit the precision\n        # value = st.decimals(places=type.scale, allow_infinity=False)\n        h.reject()\n    elif pa.types.is_time(ty):\n        value = st.times()\n    elif pa.types.is_date(ty):\n        value = st.dates()\n    elif pa.types.is_timestamp(ty):\n        if zoneinfo is None:\n            pytest.skip('no module named zoneinfo (or tzdata on Windows)')\n        if ty.tz is None:\n            pytest.skip('requires timezone not None')\n        min_int64 = -(2**63)\n        max_int64 = 2**63 - 1\n        min_datetime = datetime.datetime.fromtimestamp(\n            min_int64 // 10**9) + datetime.timedelta(hours=12)\n        max_datetime = datetime.datetime.fromtimestamp(\n            max_int64 // 10**9) - datetime.timedelta(hours=12)\n        try:\n            offset = ty.tz.split(\":\")\n            offset_hours = int(offset[0])\n            offset_min = int(offset[1])\n            tz = datetime.timedelta(hours=offset_hours, minutes=offset_min)\n        except ValueError:\n            tz = zoneinfo.ZoneInfo(ty.tz)\n        value = st.datetimes(timezones=st.just(tz), min_value=min_datetime,\n                             max_value=max_datetime)\n    elif pa.types.is_duration(ty):\n        value = st.timedeltas()\n    elif pa.types.is_interval(ty):\n        value = st.timedeltas()\n    elif pa.types.is_binary(ty) or pa.types.is_large_binary(ty):\n        value = st.binary()\n    elif pa.types.is_string(ty) or pa.types.is_large_string(ty):\n        value = st.text()\n    elif pa.types.is_fixed_size_binary(ty):\n        value = st.binary(min_size=ty.byte_width, max_size=ty.byte_width)\n    elif pa.types.is_list(ty):\n        value = _pylist(ty.value_type, size=size, nullable=nullable)\n    elif pa.types.is_large_list(ty):\n        value = _pylist(ty.value_type, size=size, nullable=nullable)\n    elif pa.types.is_fixed_size_list(ty):\n        value = _pylist(ty.value_type, size=ty.list_size, nullable=nullable)\n    elif pa.types.is_list_view(ty):\n        value = _pylist(ty.value_type, size=size, nullable=nullable)\n    elif pa.types.is_large_list_view(ty):\n        value = _pylist(ty.value_type, size=size, nullable=nullable)\n    elif pa.types.is_dictionary(ty):\n        values = _pylist(ty.value_type, size=size, nullable=nullable)\n        return pa.array(draw(values), type=ty)\n    elif pa.types.is_map(ty):\n        value = _pymap(ty.key_type, ty.item_type, size=_default_array_sizes,\n                       nullable=nullable)\n    elif pa.types.is_struct(ty):\n        h.assume(len(ty) > 0)\n        fields, child_arrays = [], []\n        for field in ty:\n            fields.append(field)\n            child_arrays.append(draw(arrays(field.type, size=size)))\n        return pa.StructArray.from_arrays(child_arrays, fields=fields)\n    else:\n        raise NotImplementedError(ty)\n\n    if nullable:\n        value = st.one_of(st.none(), value)\n    values = st.lists(value, min_size=size, max_size=size)\n\n    return pa.array(draw(values), type=ty)\n\n\n@st.composite\ndef chunked_arrays(draw, type, min_chunks=0, max_chunks=None, chunk_size=None):\n    if isinstance(type, st.SearchStrategy):\n        type = draw(type)\n\n    # TODO(kszucs): remove it, field metadata is not kept\n    h.assume(not pa.types.is_struct(type))\n\n    chunk = arrays(type, size=chunk_size)\n    chunks = st.lists(chunk, min_size=min_chunks, max_size=max_chunks)\n\n    return pa.chunked_array(draw(chunks), type=type)\n\n\n@st.composite\ndef record_batches(draw, type, rows=None, max_fields=None):\n    if isinstance(rows, st.SearchStrategy):\n        rows = draw(rows)\n    elif rows is None:\n        rows = draw(_default_array_sizes)\n    elif not isinstance(rows, int):\n        raise TypeError('Rows must be an integer')\n\n    schema = draw(schemas(type, max_fields=max_fields))\n    children = [draw(arrays(field.type, size=rows)) for field in schema]\n    # TODO(kszucs): the names and schema arguments are not consistent with\n    #               Table.from_array's arguments\n    return pa.RecordBatch.from_arrays(children, schema=schema)\n\n\n@st.composite\ndef tables(draw, type, rows=None, max_fields=None):\n    if isinstance(rows, st.SearchStrategy):\n        rows = draw(rows)\n    elif rows is None:\n        rows = draw(_default_array_sizes)\n    elif not isinstance(rows, int):\n        raise TypeError('Rows must be an integer')\n\n    schema = draw(schemas(type, max_fields=max_fields))\n    children = [draw(arrays(field.type, size=rows)) for field in schema]\n    return pa.Table.from_arrays(children, schema=schema)\n\n\nall_arrays = arrays(all_types)\nall_chunked_arrays = chunked_arrays(all_types)\nall_record_batches = record_batches(all_types)\nall_tables = tables(all_types)\n\n\n# Define the same rules as above for pandas tests by excluding certain types\n# from the generation because of known issues.\n\npandas_compatible_primitive_types = st.one_of(\n    null_type,\n    bool_type,\n    integer_types,\n    st.sampled_from([pa.float32(), pa.float64()]),\n    decimal128_type,\n    date_types,\n    time_types,\n    # Need to exclude timestamp and duration types otherwise hypothesis\n    # discovers ARROW-10210\n    # timestamp_types,\n    # duration_types\n    interval_types,\n    binary_type,\n    string_type,\n    large_binary_type,\n    large_string_type,\n)\n\n# Need to exclude floating point types otherwise hypothesis discovers\n# ARROW-10211\npandas_compatible_dictionary_value_types = st.one_of(\n    bool_type,\n    integer_types,\n    binary_type,\n    string_type,\n    fixed_size_binary_type,\n)\n\n\ndef pandas_compatible_list_types(\n    item_strategy=pandas_compatible_primitive_types\n):\n    # Need to exclude fixed size list type otherwise hypothesis discovers\n    # ARROW-10194\n    return (\n        st.builds(pa.list_, item_strategy) |\n        st.builds(pa.large_list, item_strategy)\n    )\n\n\npandas_compatible_types = st.deferred(\n    lambda: st.one_of(\n        pandas_compatible_primitive_types,\n        pandas_compatible_list_types(pandas_compatible_primitive_types),\n        struct_types(pandas_compatible_primitive_types),\n        dictionary_types(\n            value_strategy=pandas_compatible_dictionary_value_types\n        ),\n        pandas_compatible_list_types(pandas_compatible_types),\n        struct_types(pandas_compatible_types)\n    )\n)\n", "python/pyarrow/tests/test_udf.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nimport pytest\n\nimport numpy as np\n\nimport pyarrow as pa\nfrom pyarrow import compute as pc\n\n# UDFs are all tested with a dataset scan\npytestmark = pytest.mark.dataset\n\n# For convenience, most of the test here doesn't care about udf func docs\nempty_udf_doc = {\"summary\": \"\", \"description\": \"\"}\n\ntry:\n    import pyarrow.dataset as ds\nexcept ImportError:\n    ds = None\n\n\ndef mock_udf_context(batch_length=10):\n    from pyarrow._compute import _get_udf_context\n    return _get_udf_context(pa.default_memory_pool(), batch_length)\n\n\nclass MyError(RuntimeError):\n    pass\n\n\n@pytest.fixture(scope=\"session\")\ndef sum_agg_func_fixture():\n    \"\"\"\n    Register a unary aggregate function (mean)\n    \"\"\"\n    def func(ctx, x, *args):\n        return pa.scalar(np.nansum(x))\n\n    func_name = \"sum_udf\"\n    func_doc = empty_udf_doc\n\n    pc.register_aggregate_function(func,\n                                   func_name,\n                                   func_doc,\n                                   {\n                                       \"x\": pa.float64(),\n                                   },\n                                   pa.float64()\n                                   )\n    return func, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef exception_agg_func_fixture():\n    def func(ctx, x):\n        raise RuntimeError(\"Oops\")\n        return pa.scalar(len(x))\n\n    func_name = \"y=exception_len(x)\"\n    func_doc = empty_udf_doc\n\n    pc.register_aggregate_function(func,\n                                   func_name,\n                                   func_doc,\n                                   {\n                                       \"x\": pa.int64(),\n                                   },\n                                   pa.int64()\n                                   )\n    return func, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef wrong_output_dtype_agg_func_fixture(scope=\"session\"):\n    def func(ctx, x):\n        return pa.scalar(len(x), pa.int32())\n\n    func_name = \"y=wrong_output_dtype(x)\"\n    func_doc = empty_udf_doc\n\n    pc.register_aggregate_function(func,\n                                   func_name,\n                                   func_doc,\n                                   {\n                                       \"x\": pa.int64(),\n                                   },\n                                   pa.int64()\n                                   )\n    return func, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef wrong_output_type_agg_func_fixture(scope=\"session\"):\n    def func(ctx, x):\n        return len(x)\n\n    func_name = \"y=wrong_output_type(x)\"\n    func_doc = empty_udf_doc\n\n    pc.register_aggregate_function(func,\n                                   func_name,\n                                   func_doc,\n                                   {\n                                       \"x\": pa.int64(),\n                                   },\n                                   pa.int64()\n                                   )\n    return func, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef binary_func_fixture():\n    \"\"\"\n    Register a binary scalar function.\n    \"\"\"\n    def binary_function(ctx, m, x):\n        return pc.call_function(\"multiply\", [m, x],\n                                memory_pool=ctx.memory_pool)\n    func_name = \"y=mx\"\n    binary_doc = {\"summary\": \"y=mx\",\n                  \"description\": \"find y from y = mx\"}\n    pc.register_scalar_function(binary_function,\n                                func_name,\n                                binary_doc,\n                                {\"m\": pa.int64(),\n                                 \"x\": pa.int64(),\n                                 },\n                                pa.int64())\n    return binary_function, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef ternary_func_fixture():\n    \"\"\"\n    Register a ternary scalar function.\n    \"\"\"\n    def ternary_function(ctx, m, x, c):\n        mx = pc.call_function(\"multiply\", [m, x],\n                              memory_pool=ctx.memory_pool)\n        return pc.call_function(\"add\", [mx, c],\n                                memory_pool=ctx.memory_pool)\n    ternary_doc = {\"summary\": \"y=mx+c\",\n                   \"description\": \"find y from y = mx + c\"}\n    func_name = \"y=mx+c\"\n    pc.register_scalar_function(ternary_function,\n                                func_name,\n                                ternary_doc,\n                                {\n                                    \"array1\": pa.int64(),\n                                    \"array2\": pa.int64(),\n                                    \"array3\": pa.int64(),\n                                },\n                                pa.int64())\n    return ternary_function, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef varargs_func_fixture():\n    \"\"\"\n    Register a varargs scalar function with at least two arguments.\n    \"\"\"\n    def varargs_function(ctx, first, *values):\n        acc = first\n        for val in values:\n            acc = pc.call_function(\"add\", [acc, val],\n                                   memory_pool=ctx.memory_pool)\n        return acc\n    func_name = \"z=ax+by+c\"\n    varargs_doc = {\"summary\": \"z=ax+by+c\",\n                   \"description\": \"find z from z = ax + by + c\"\n                   }\n    pc.register_scalar_function(varargs_function,\n                                func_name,\n                                varargs_doc,\n                                {\n                                    \"array1\": pa.int64(),\n                                    \"array2\": pa.int64(),\n                                },\n                                pa.int64())\n    return varargs_function, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef nullary_func_fixture():\n    \"\"\"\n    Register a nullary scalar function.\n    \"\"\"\n    def nullary_func(context):\n        return pa.array([42] * context.batch_length, type=pa.int64(),\n                        memory_pool=context.memory_pool)\n\n    func_doc = {\n        \"summary\": \"random function\",\n        \"description\": \"generates a random value\"\n    }\n    func_name = \"test_nullary_func\"\n    pc.register_scalar_function(nullary_func,\n                                func_name,\n                                func_doc,\n                                {},\n                                pa.int64())\n\n    return nullary_func, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef wrong_output_type_func_fixture():\n    \"\"\"\n    Register a scalar function which returns something that is neither\n    a Arrow scalar or array.\n    \"\"\"\n    def wrong_output_type(ctx):\n        return 42\n\n    func_name = \"test_wrong_output_type\"\n    in_types = {}\n    out_type = pa.int64()\n    doc = {\n        \"summary\": \"return wrong output type\",\n        \"description\": \"\"\n    }\n    pc.register_scalar_function(wrong_output_type, func_name, doc,\n                                in_types, out_type)\n    return wrong_output_type, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef wrong_output_datatype_func_fixture():\n    \"\"\"\n    Register a scalar function whose actual output DataType doesn't\n    match the declared output DataType.\n    \"\"\"\n    def wrong_output_datatype(ctx, array):\n        return pc.call_function(\"add\", [array, 1])\n    func_name = \"test_wrong_output_datatype\"\n    in_types = {\"array\": pa.int64()}\n    # The actual output DataType will be int64.\n    out_type = pa.int16()\n    doc = {\n        \"summary\": \"return wrong output datatype\",\n        \"description\": \"\"\n    }\n    pc.register_scalar_function(wrong_output_datatype, func_name, doc,\n                                in_types, out_type)\n    return wrong_output_datatype, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef wrong_signature_func_fixture():\n    \"\"\"\n    Register a scalar function with the wrong signature.\n    \"\"\"\n    # Missing the context argument\n    def wrong_signature():\n        return pa.scalar(1, type=pa.int64())\n\n    func_name = \"test_wrong_signature\"\n    in_types = {}\n    out_type = pa.int64()\n    doc = {\n        \"summary\": \"UDF with wrong signature\",\n        \"description\": \"\"\n    }\n    pc.register_scalar_function(wrong_signature, func_name, doc,\n                                in_types, out_type)\n    return wrong_signature, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef raising_func_fixture():\n    \"\"\"\n    Register a scalar function which raises a custom exception.\n    \"\"\"\n    def raising_func(ctx):\n        raise MyError(\"error raised by scalar UDF\")\n    func_name = \"test_raise\"\n    doc = {\n        \"summary\": \"raising function\",\n        \"description\": \"\"\n    }\n    pc.register_scalar_function(raising_func, func_name, doc,\n                                {}, pa.int64())\n    return raising_func, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef unary_vector_func_fixture():\n    \"\"\"\n    Register a vector function\n    \"\"\"\n    def pct_rank(ctx, x):\n        # copy here to get around pandas 1.0 issue\n        return pa.array(x.to_pandas().copy().rank(pct=True))\n\n    func_name = \"y=pct_rank(x)\"\n    doc = empty_udf_doc\n    pc.register_vector_function(pct_rank, func_name, doc, {\n                                'x': pa.float64()}, pa.float64())\n\n    return pct_rank, func_name\n\n\n@pytest.fixture(scope=\"session\")\ndef struct_vector_func_fixture():\n    \"\"\"\n    Register a vector function that returns a struct array\n    \"\"\"\n    def pivot(ctx, k, v, c):\n        df = pa.RecordBatch.from_arrays([k, v, c], names=['k', 'v', 'c']).to_pandas()\n        df_pivot = df.pivot(columns='c', values='v', index='k').reset_index()\n        return pa.RecordBatch.from_pandas(df_pivot).to_struct_array()\n\n    func_name = \"y=pivot(x)\"\n    doc = empty_udf_doc\n    pc.register_vector_function(\n        pivot, func_name, doc,\n        {'k': pa.int64(), 'v': pa.float64(), 'c': pa.utf8()},\n        pa.struct([('k', pa.int64()), ('v1', pa.float64()), ('v2', pa.float64())])\n    )\n\n    return pivot, func_name\n\n\ndef check_scalar_function(func_fixture,\n                          inputs, *,\n                          run_in_dataset=True,\n                          batch_length=None):\n    function, name = func_fixture\n    if batch_length is None:\n        all_scalar = True\n        for arg in inputs:\n            if isinstance(arg, pa.Array):\n                all_scalar = False\n                batch_length = len(arg)\n        if all_scalar:\n            batch_length = 1\n\n    func = pc.get_function(name)\n    assert func.name == name\n\n    result = pc.call_function(name, inputs, length=batch_length)\n    expected_output = function(mock_udf_context(batch_length), *inputs)\n    assert result == expected_output\n    # At the moment there is an issue when handling nullary functions.\n    # See: ARROW-15286 and ARROW-16290.\n    if run_in_dataset:\n        field_names = [f'field{index}' for index, in_arr in inputs]\n        table = pa.Table.from_arrays(inputs, field_names)\n        dataset = ds.dataset(table)\n        func_args = [ds.field(field_name) for field_name in field_names]\n        result_table = dataset.to_table(\n            columns={'result': ds.field('')._call(name, func_args)})\n        assert result_table.column(0).chunks[0] == expected_output\n\n\ndef test_udf_array_unary(unary_func_fixture):\n    check_scalar_function(unary_func_fixture,\n                          [\n                              pa.array([10, 20], pa.int64())\n                          ]\n                          )\n\n\ndef test_udf_array_binary(binary_func_fixture):\n    check_scalar_function(binary_func_fixture,\n                          [\n                              pa.array([10, 20], pa.int64()),\n                              pa.array([2, 4], pa.int64())\n                          ]\n                          )\n\n\ndef test_udf_array_ternary(ternary_func_fixture):\n    check_scalar_function(ternary_func_fixture,\n                          [\n                              pa.array([10, 20], pa.int64()),\n                              pa.array([2, 4], pa.int64()),\n                              pa.array([5, 10], pa.int64())\n                          ]\n                          )\n\n\ndef test_udf_array_varargs(varargs_func_fixture):\n    check_scalar_function(varargs_func_fixture,\n                          [\n                              pa.array([2, 3], pa.int64()),\n                              pa.array([10, 20], pa.int64()),\n                              pa.array([3, 7], pa.int64()),\n                              pa.array([20, 30], pa.int64()),\n                              pa.array([5, 10], pa.int64())\n                          ]\n                          )\n\n\ndef test_registration_errors():\n    # validate function name\n    doc = {\n        \"summary\": \"test udf input\",\n        \"description\": \"parameters are validated\"\n    }\n    in_types = {\"scalar\": pa.int64()}\n    out_type = pa.int64()\n\n    def test_reg_function(context):\n        return pa.array([10])\n\n    with pytest.raises(TypeError):\n        pc.register_scalar_function(test_reg_function,\n                                    None, doc, in_types,\n                                    out_type)\n\n    # validate function\n    with pytest.raises(TypeError, match=\"func must be a callable\"):\n        pc.register_scalar_function(None, \"test_none_function\", doc, in_types,\n                                    out_type)\n\n    # validate output type\n    expected_expr = \"DataType expected, got <class 'NoneType'>\"\n    with pytest.raises(TypeError, match=expected_expr):\n        pc.register_scalar_function(test_reg_function,\n                                    \"test_output_function\", doc, in_types,\n                                    None)\n\n    # validate input type\n    expected_expr = \"in_types must be a dictionary of DataType\"\n    with pytest.raises(TypeError, match=expected_expr):\n        pc.register_scalar_function(test_reg_function,\n                                    \"test_input_function\", doc, None,\n                                    out_type)\n\n    # register an already registered function\n    # first registration\n    pc.register_scalar_function(test_reg_function,\n                                \"test_reg_function\", doc, {},\n                                out_type)\n    # second registration\n    expected_expr = \"Already have a function registered with name:\" \\\n        + \" test_reg_function\"\n    with pytest.raises(KeyError, match=expected_expr):\n        pc.register_scalar_function(test_reg_function,\n                                    \"test_reg_function\", doc, {},\n                                    out_type)\n\n\ndef test_varargs_function_validation(varargs_func_fixture):\n    _, func_name = varargs_func_fixture\n\n    error_msg = r\"VarArgs function 'z=ax\\+by\\+c' needs at least 2 arguments\"\n\n    with pytest.raises(ValueError, match=error_msg):\n        pc.call_function(func_name, [42])\n\n\ndef test_function_doc_validation():\n    # validate arity\n    in_types = {\"scalar\": pa.int64()}\n    out_type = pa.int64()\n\n    # doc with no summary\n    func_doc = {\n        \"description\": \"desc\"\n    }\n\n    def add_const(ctx, scalar):\n        return pc.call_function(\"add\", [scalar, 1])\n\n    with pytest.raises(ValueError,\n                       match=\"Function doc must contain a summary\"):\n        pc.register_scalar_function(add_const, \"test_no_summary\",\n                                    func_doc, in_types,\n                                    out_type)\n\n    # doc with no description\n    func_doc = {\n        \"summary\": \"test summary\"\n    }\n\n    with pytest.raises(ValueError,\n                       match=\"Function doc must contain a description\"):\n        pc.register_scalar_function(add_const, \"test_no_desc\",\n                                    func_doc, in_types,\n                                    out_type)\n\n\ndef test_nullary_function(nullary_func_fixture):\n    # XXX the Python compute layer API doesn't let us override batch_length,\n    # so only test with the default value of 1.\n    check_scalar_function(nullary_func_fixture, [], run_in_dataset=False,\n                          batch_length=1)\n\n\ndef test_wrong_output_type(wrong_output_type_func_fixture):\n    _, func_name = wrong_output_type_func_fixture\n\n    with pytest.raises(TypeError,\n                       match=\"Unexpected output type: int\"):\n        pc.call_function(func_name, [], length=1)\n\n\ndef test_wrong_output_datatype(wrong_output_datatype_func_fixture):\n    _, func_name = wrong_output_datatype_func_fixture\n\n    expected_expr = (\"Expected output datatype int16, \"\n                     \"but function returned datatype int64\")\n\n    with pytest.raises(TypeError, match=expected_expr):\n        pc.call_function(func_name, [pa.array([20, 30])])\n\n\ndef test_wrong_signature(wrong_signature_func_fixture):\n    _, func_name = wrong_signature_func_fixture\n\n    expected_expr = (r\"wrong_signature\\(\\) takes 0 positional arguments \"\n                     \"but 1 was given\")\n\n    with pytest.raises(TypeError, match=expected_expr):\n        pc.call_function(func_name, [], length=1)\n\n\ndef test_wrong_datatype_declaration():\n    def identity(ctx, val):\n        return val\n\n    func_name = \"test_wrong_datatype_declaration\"\n    in_types = {\"array\": pa.int64()}\n    out_type = {}\n    doc = {\n        \"summary\": \"test output value\",\n        \"description\": \"test output\"\n    }\n    with pytest.raises(TypeError,\n                       match=\"DataType expected, got <class 'dict'>\"):\n        pc.register_scalar_function(identity, func_name,\n                                    doc, in_types, out_type)\n\n\ndef test_wrong_input_type_declaration():\n    def identity(ctx, val):\n        return val\n\n    func_name = \"test_wrong_input_type_declaration\"\n    in_types = {\"array\": None}\n    out_type = pa.int64()\n    doc = {\n        \"summary\": \"test invalid input type\",\n        \"description\": \"invalid input function\"\n    }\n    with pytest.raises(TypeError,\n                       match=\"DataType expected, got <class 'NoneType'>\"):\n        pc.register_scalar_function(identity, func_name, doc,\n                                    in_types, out_type)\n\n\ndef test_scalar_udf_context(unary_func_fixture):\n    # Check the memory_pool argument is properly propagated\n    proxy_pool = pa.proxy_memory_pool(pa.default_memory_pool())\n    _, func_name = unary_func_fixture\n\n    res = pc.call_function(func_name,\n                           [pa.array([1] * 1000, type=pa.int64())],\n                           memory_pool=proxy_pool)\n    assert res == pa.array([2] * 1000, type=pa.int64())\n    assert proxy_pool.bytes_allocated() == 1000 * 8\n    # Destroying Python array should destroy underlying C++ memory\n    res = None\n    assert proxy_pool.bytes_allocated() == 0\n\n\ndef test_raising_func(raising_func_fixture):\n    _, func_name = raising_func_fixture\n    with pytest.raises(MyError, match=\"error raised by scalar UDF\"):\n        pc.call_function(func_name, [], length=1)\n\n\ndef test_scalar_input(unary_func_fixture):\n    function, func_name = unary_func_fixture\n    res = pc.call_function(func_name, [pa.scalar(10)])\n    assert res == pa.scalar(11)\n\n\ndef test_input_lifetime(unary_func_fixture):\n    function, func_name = unary_func_fixture\n\n    proxy_pool = pa.proxy_memory_pool(pa.default_memory_pool())\n    assert proxy_pool.bytes_allocated() == 0\n\n    v = pa.array([1] * 1000, type=pa.int64(), memory_pool=proxy_pool)\n    assert proxy_pool.bytes_allocated() == 1000 * 8\n    pc.call_function(func_name, [v])\n    assert proxy_pool.bytes_allocated() == 1000 * 8\n    # Calling a UDF should not have kept `v` alive longer than required\n    v = None\n    assert proxy_pool.bytes_allocated() == 0\n\n\ndef _record_batch_from_iters(schema, *iters):\n    arrays = [pa.array(list(v), type=schema[i].type)\n              for i, v in enumerate(iters)]\n    return pa.RecordBatch.from_arrays(arrays=arrays, schema=schema)\n\n\ndef _record_batch_for_range(schema, n):\n    return _record_batch_from_iters(schema,\n                                    range(n, n + 10),\n                                    range(n + 1, n + 11))\n\n\ndef make_udt_func(schema, batch_gen):\n    def udf_func(ctx):\n        class UDT:\n            def __init__(self):\n                self.caller = None\n\n            def __call__(self, ctx):\n                try:\n                    if self.caller is None:\n                        self.caller, ctx = batch_gen(ctx).send, None\n                    batch = self.caller(ctx)\n                except StopIteration:\n                    arrays = [pa.array([], type=field.type)\n                              for field in schema]\n                    batch = pa.RecordBatch.from_arrays(\n                        arrays=arrays, schema=schema)\n                return batch.to_struct_array()\n        return UDT()\n    return udf_func\n\n\ndef datasource1_direct():\n    \"\"\"A short dataset\"\"\"\n    schema = datasource1_schema()\n\n    class Generator:\n        def __init__(self):\n            self.n = 3\n\n        def __call__(self, ctx):\n            if self.n == 0:\n                batch = _record_batch_from_iters(schema, [], [])\n            else:\n                self.n -= 1\n                batch = _record_batch_for_range(schema, self.n)\n            return batch.to_struct_array()\n    return lambda ctx: Generator()\n\n\ndef datasource1_generator():\n    schema = datasource1_schema()\n\n    def batch_gen(ctx):\n        for n in range(3, 0, -1):\n            # ctx =\n            yield _record_batch_for_range(schema, n - 1)\n    return make_udt_func(schema, batch_gen)\n\n\ndef datasource1_exception():\n    schema = datasource1_schema()\n\n    def batch_gen(ctx):\n        for n in range(3, 0, -1):\n            # ctx =\n            yield _record_batch_for_range(schema, n - 1)\n        raise RuntimeError(\"datasource1_exception\")\n    return make_udt_func(schema, batch_gen)\n\n\ndef datasource1_schema():\n    return pa.schema([('', pa.int32()), ('', pa.int32())])\n\n\ndef datasource1_args(func, func_name):\n    func_doc = {\"summary\": f\"{func_name} UDT\",\n                \"description\": \"test {func_name} UDT\"}\n    in_types = {}\n    out_type = pa.struct([(\"\", pa.int32()), (\"\", pa.int32())])\n    return func, func_name, func_doc, in_types, out_type\n\n\ndef _test_datasource1_udt(func_maker):\n    schema = datasource1_schema()\n    func = func_maker()\n    func_name = func_maker.__name__\n    func_args = datasource1_args(func, func_name)\n    pc.register_tabular_function(*func_args)\n    n = 3\n    for item in pc.call_tabular_function(func_name):\n        n -= 1\n        assert item == _record_batch_for_range(schema, n)\n\n\ndef test_udt_datasource1_direct():\n    _test_datasource1_udt(datasource1_direct)\n\n\ndef test_udt_datasource1_generator():\n    _test_datasource1_udt(datasource1_generator)\n\n\ndef test_udt_datasource1_exception():\n    with pytest.raises(RuntimeError, match='datasource1_exception'):\n        _test_datasource1_udt(datasource1_exception)\n\n\ndef test_scalar_agg_basic(unary_agg_func_fixture):\n    arr = pa.array([10.0, 20.0, 30.0, 40.0, 50.0], pa.float64())\n    result = pc.call_function(\"mean_udf\", [arr])\n    expected = pa.scalar(30.0)\n    assert result == expected\n\n\ndef test_scalar_agg_empty(unary_agg_func_fixture):\n    empty = pa.array([], pa.float64())\n\n    with pytest.raises(pa.ArrowInvalid, match='empty inputs'):\n        pc.call_function(\"mean_udf\", [empty])\n\n\ndef test_scalar_agg_wrong_output_dtype(wrong_output_dtype_agg_func_fixture):\n    arr = pa.array([10, 20, 30, 40, 50], pa.int64())\n    with pytest.raises(pa.ArrowTypeError, match=\"output datatype\"):\n        pc.call_function(\"y=wrong_output_dtype(x)\", [arr])\n\n\ndef test_scalar_agg_wrong_output_type(wrong_output_type_agg_func_fixture):\n    arr = pa.array([10, 20, 30, 40, 50], pa.int64())\n    with pytest.raises(pa.ArrowTypeError, match=\"output type\"):\n        pc.call_function(\"y=wrong_output_type(x)\", [arr])\n\n\ndef test_scalar_agg_varargs(varargs_agg_func_fixture):\n    arr1 = pa.array([10, 20, 30, 40, 50], pa.int64())\n    arr2 = pa.array([1.0, 2.0, 3.0, 4.0, 5.0], pa.float64())\n\n    result = pc.call_function(\n        \"sum_mean\", [arr1, arr2]\n    )\n    expected = pa.scalar(33.0)\n    assert result == expected\n\n\ndef test_scalar_agg_exception(exception_agg_func_fixture):\n    arr = pa.array([10, 20, 30, 40, 50, 60], pa.int64())\n\n    with pytest.raises(RuntimeError, match='Oops'):\n        pc.call_function(\"y=exception_len(x)\", [arr])\n\n\ndef test_hash_agg_basic(unary_agg_func_fixture):\n    arr1 = pa.array([10.0, 20.0, 30.0, 40.0, 50.0], pa.float64())\n    arr2 = pa.array([4, 2, 1, 2, 1], pa.int32())\n\n    arr3 = pa.array([60.0, 70.0, 80.0, 90.0, 100.0], pa.float64())\n    arr4 = pa.array([5, 1, 1, 4, 1], pa.int32())\n\n    table1 = pa.table([arr2, arr1], names=[\"id\", \"value\"])\n    table2 = pa.table([arr4, arr3], names=[\"id\", \"value\"])\n    table = pa.concat_tables([table1, table2])\n\n    result = table.group_by(\"id\").aggregate([(\"value\", \"mean_udf\")])\n    expected = table.group_by(\"id\").aggregate(\n        [(\"value\", \"mean\")]).rename_columns(['id', 'value_mean_udf'])\n\n    assert result.sort_by('id') == expected.sort_by('id')\n\n\ndef test_hash_agg_empty(unary_agg_func_fixture):\n    arr1 = pa.array([], pa.float64())\n    arr2 = pa.array([], pa.int32())\n    table = pa.table([arr2, arr1], names=[\"id\", \"value\"])\n\n    result = table.group_by(\"id\").aggregate([(\"value\", \"mean_udf\")])\n    expected = pa.table([pa.array([], pa.int32()), pa.array(\n        [], pa.float64())], names=['id', 'value_mean_udf'])\n\n    assert result == expected\n\n\ndef test_hash_agg_wrong_output_dtype(wrong_output_dtype_agg_func_fixture):\n    arr1 = pa.array([10, 20, 30, 40, 50], pa.int64())\n    arr2 = pa.array([4, 2, 1, 2, 1], pa.int32())\n\n    table = pa.table([arr2, arr1], names=[\"id\", \"value\"])\n    with pytest.raises(pa.ArrowTypeError, match=\"output datatype\"):\n        table.group_by(\"id\").aggregate([(\"value\", \"y=wrong_output_dtype(x)\")])\n\n\ndef test_hash_agg_wrong_output_type(wrong_output_type_agg_func_fixture):\n    arr1 = pa.array([10, 20, 30, 40, 50], pa.int64())\n    arr2 = pa.array([4, 2, 1, 2, 1], pa.int32())\n    table = pa.table([arr2, arr1], names=[\"id\", \"value\"])\n\n    with pytest.raises(pa.ArrowTypeError, match=\"output type\"):\n        table.group_by(\"id\").aggregate([(\"value\", \"y=wrong_output_type(x)\")])\n\n\ndef test_hash_agg_exception(exception_agg_func_fixture):\n    arr1 = pa.array([10, 20, 30, 40, 50], pa.int64())\n    arr2 = pa.array([4, 2, 1, 2, 1], pa.int32())\n    table = pa.table([arr2, arr1], names=[\"id\", \"value\"])\n\n    with pytest.raises(RuntimeError, match='Oops'):\n        table.group_by(\"id\").aggregate([(\"value\", \"y=exception_len(x)\")])\n\n\ndef test_hash_agg_random(sum_agg_func_fixture):\n    \"\"\"Test hash aggregate udf with randomly sampled data\"\"\"\n\n    value_num = 1000000\n    group_num = 1000\n\n    arr1 = pa.array(np.repeat(1, value_num), pa.float64())\n    arr2 = pa.array(np.random.choice(group_num, value_num), pa.int32())\n\n    table = pa.table([arr2, arr1], names=['id', 'value'])\n\n    result = table.group_by(\"id\").aggregate([(\"value\", \"sum_udf\")])\n    expected = table.group_by(\"id\").aggregate(\n        [(\"value\", \"sum\")]).rename_columns(['id', 'value_sum_udf'])\n\n    assert result.sort_by('id') == expected.sort_by('id')\n\n\n@pytest.mark.pandas\ndef test_vector_basic(unary_vector_func_fixture):\n    arr = pa.array([10.0, 20.0, 30.0, 40.0, 50.0], pa.float64())\n    result = pc.call_function(\"y=pct_rank(x)\", [arr])\n    expected = unary_vector_func_fixture[0](None, arr)\n    assert result == expected\n\n\n@pytest.mark.pandas\ndef test_vector_empty(unary_vector_func_fixture):\n    arr = pa.array([1], pa.float64())\n    result = pc.call_function(\"y=pct_rank(x)\", [arr])\n    expected = unary_vector_func_fixture[0](None, arr)\n    assert result == expected\n\n\n@pytest.mark.pandas\ndef test_vector_struct(struct_vector_func_fixture):\n    k = pa.array(\n        [1, 1, 2, 2], pa.int64()\n    )\n    v = pa.array(\n        [1.0, 2.0, 3.0, 4.0], pa.float64()\n    )\n    c = pa.array(\n        ['v1', 'v2', 'v1', 'v2']\n    )\n    result = pc.call_function(\"y=pivot(x)\", [k, v, c])\n    expected = struct_vector_func_fixture[0](None, k, v, c)\n    assert result == expected\n", "python/pyarrow/tests/test_gdb.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom functools import lru_cache\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\n\nimport pytest\n\nimport pyarrow as pa\n\n\npytestmark = pytest.mark.gdb\n\nhere = os.path.dirname(os.path.abspath(__file__))\n\n# The GDB script may be found in the source tree (if available)\n# or in another location given by the ARROW_GDB_SCRIPT environment variable.\ngdb_script = (os.environ.get('ARROW_GDB_SCRIPT') or\n              os.path.join(here, \"../../../cpp/gdb_arrow.py\"))\n\ngdb_command = [\"gdb\", \"--nx\"]\n\n\ndef environment_for_gdb():\n    env = {}\n    for var in ['PATH', 'LD_LIBRARY_PATH']:\n        try:\n            env[var] = os.environ[var]\n        except KeyError:\n            pass\n    return env\n\n\n@lru_cache()\ndef is_gdb_available():\n    try:\n        # Try to use the same arguments as in GdbSession so that the\n        # same error return gets propagated.\n        proc = subprocess.run(gdb_command + [\"--version\"],\n                              env=environment_for_gdb(), bufsize=0,\n                              stdin=subprocess.PIPE,\n                              stdout=subprocess.PIPE,\n                              stderr=subprocess.STDOUT)\n    except FileNotFoundError:\n        return False\n    return proc.returncode == 0\n\n\n@lru_cache()\ndef python_executable():\n    path = shutil.which(\"python3\")\n    assert path is not None, \"Couldn't find python3 executable\"\n    return path\n\n\ndef skip_if_gdb_unavailable():\n    if not is_gdb_available():\n        pytest.skip(\"gdb command unavailable\")\n\n\ndef skip_if_gdb_script_unavailable():\n    if not os.path.exists(gdb_script):\n        pytest.skip(\"gdb script not found\")\n\n\nclass GdbSession:\n    proc = None\n    verbose = True\n\n    def __init__(self, *args, **env):\n        # Let stderr through to let pytest display it separately on errors\n        gdb_env = environment_for_gdb()\n        gdb_env.update(env)\n        self.proc = subprocess.Popen(gdb_command + list(args),\n                                     env=gdb_env, bufsize=0,\n                                     stdin=subprocess.PIPE,\n                                     stdout=subprocess.PIPE)\n        self.last_stdout = []\n        self.last_stdout_line = b\"\"\n\n    def wait_until_ready(self):\n        \"\"\"\n        Record output until the gdb prompt displays.  Return recorded output.\n        \"\"\"\n        # TODO: add timeout?\n        while (not self.last_stdout_line.startswith(b\"(gdb) \") and\n               self.proc.poll() is None):\n            block = self.proc.stdout.read(4096)\n            if self.verbose:\n                sys.stdout.buffer.write(block)\n                sys.stdout.buffer.flush()\n            block, sep, last_line = block.rpartition(b\"\\n\")\n            if sep:\n                self.last_stdout.append(self.last_stdout_line)\n                self.last_stdout.append(block + sep)\n                self.last_stdout_line = last_line\n            else:\n                assert block == b\"\"\n                self.last_stdout_line += last_line\n\n        if self.proc.poll() is not None:\n            raise IOError(\"gdb session terminated unexpectedly\")\n\n        out = b\"\".join(self.last_stdout).decode('utf-8')\n        self.last_stdout = []\n        self.last_stdout_line = b\"\"\n        return out\n\n    def issue_command(self, line):\n        line = line.encode('utf-8') + b\"\\n\"\n        if self.verbose:\n            sys.stdout.buffer.write(line)\n            sys.stdout.buffer.flush()\n        self.proc.stdin.write(line)\n        self.proc.stdin.flush()\n\n    def run_command(self, line):\n        self.issue_command(line)\n        return self.wait_until_ready()\n\n    def print_value(self, expr):\n        \"\"\"\n        Ask gdb to print the value of an expression and return the result.\n        \"\"\"\n        out = self.run_command(f\"p {expr}\")\n        out, n = re.subn(r\"^\\$\\d+ = \", \"\", out)\n        assert n == 1, out\n        # gdb may add whitespace depending on result width, remove it\n        return out.strip()\n\n    def select_frame(self, func_name):\n        \"\"\"\n        Select the innermost frame with the given function name.\n        \"\"\"\n        # Ideally, we would use the \"frame function\" command,\n        # but it's not available on old GDB versions (such as 8.1.1),\n        # so instead parse the stack trace for a matching frame number.\n        out = self.run_command(\"info stack\")\n        pat = r\"(?mi)^#(\\d+)\\s+.* in \" + re.escape(func_name) + r\"\\b\"\n        m = re.search(pat, out)\n        if m is None:\n            pytest.fail(f\"Could not select frame for function {func_name}\")\n\n        frame_num = int(m[1])\n        out = self.run_command(f\"frame {frame_num}\")\n        assert f\"in {func_name}\" in out\n\n    def join(self):\n        if self.proc is not None:\n            self.proc.stdin.close()\n            self.proc.stdout.close()  # avoid ResourceWarning\n            self.proc.kill()\n            self.proc.wait()\n            self.proc = None\n\n    def __del__(self):\n        self.join()\n\n\n@pytest.fixture(scope='session')\ndef gdb():\n    skip_if_gdb_unavailable()\n    gdb = GdbSession(\"-q\", python_executable())\n    try:\n        gdb.wait_until_ready()\n        gdb.run_command(\"set confirm off\")\n        gdb.run_command(\"set print array-indexes on\")\n        # Make sure gdb formatting is not terminal-dependent\n        gdb.run_command(\"set width unlimited\")\n        gdb.run_command(\"set charset UTF-8\")\n        yield gdb\n    finally:\n        gdb.join()\n\n\n@pytest.fixture(scope='session')\ndef gdb_arrow(gdb):\n    if 'deb' not in pa.cpp_build_info.build_type:\n        pytest.skip(\"Arrow C++ debug symbols not available\")\n\n    skip_if_gdb_script_unavailable()\n    gdb.run_command(f\"source {gdb_script}\")\n\n    lib_path_var = 'PATH' if sys.platform == 'win32' else 'LD_LIBRARY_PATH'\n    lib_path = os.environ.get(lib_path_var)\n    if lib_path:\n        # GDB starts the inferior process in a pristine shell, need\n        # to propagate the library search path to find the Arrow DLL\n        gdb.run_command(f\"set env {lib_path_var} {lib_path}\")\n\n    code = \"from pyarrow.lib import _gdb_test_session; _gdb_test_session()\"\n    out = gdb.run_command(f\"run -c '{code}'\")\n    assert (\"Trace/breakpoint trap\" in out or\n            \"received signal\" in out), out\n    gdb.select_frame(\"arrow::gdb::TestSession\")\n    return gdb\n\n\ndef test_gdb_session(gdb):\n    out = gdb.run_command(\"show version\")\n    assert out.startswith(\"GNU gdb (\"), out\n\n\ndef test_gdb_arrow(gdb_arrow):\n    s = gdb_arrow.print_value(\"42 + 1\")\n    assert s == \"43\"\n\n\ndef check_stack_repr(gdb, expr, expected):\n    \"\"\"\n    Check printing a stack-located value.\n    \"\"\"\n    s = gdb.print_value(expr)\n    if isinstance(expected, re.Pattern):\n        assert expected.match(s), s\n    else:\n        assert s == expected\n\n\ndef check_heap_repr(gdb, expr, expected):\n    \"\"\"\n    Check printing a heap-located value, given its address.\n    \"\"\"\n    s = gdb.print_value(f\"*{expr}\")\n    # GDB may prefix the value with an address or type specification\n    if s != expected:\n        assert s.endswith(f\" {expected}\")\n\n\ndef test_status(gdb_arrow):\n    check_stack_repr(gdb_arrow, \"ok_status\", \"arrow::Status::OK()\")\n    check_stack_repr(gdb_arrow, \"error_status\",\n                     'arrow::Status::IOError(\"This is an error\")')\n    check_stack_repr(\n        gdb_arrow, \"error_detail_status\",\n        'arrow::Status::IOError(\"This is an error\", '\n        'detail=[custom-detail-id] \"This is a detail\")')\n\n    check_stack_repr(gdb_arrow, \"ok_result\", \"arrow::Result<int>(42)\")\n    check_stack_repr(\n        gdb_arrow, \"error_result\",\n        'arrow::Result<int>(arrow::Status::IOError(\"This is an error\"))')\n    check_stack_repr(\n        gdb_arrow, \"error_detail_result\",\n        'arrow::Result<int>(arrow::Status::IOError(\"This is an error\", '\n        'detail=[custom-detail-id] \"This is a detail\"))')\n\n\ndef test_buffer_stack(gdb_arrow):\n    check_stack_repr(gdb_arrow, \"buffer_null\",\n                     \"arrow::Buffer of size 0, read-only\")\n    check_stack_repr(gdb_arrow, \"buffer_abc\",\n                     'arrow::Buffer of size 3, read-only, \"abc\"')\n    check_stack_repr(\n        gdb_arrow, \"buffer_special_chars\",\n        r'arrow::Buffer of size 12, read-only, \"foo\\\"bar\\000\\r\\n\\t\\037\"')\n    check_stack_repr(gdb_arrow, \"buffer_mutable\",\n                     'arrow::MutableBuffer of size 3, mutable, \"abc\"')\n\n\ndef test_buffer_heap(gdb_arrow):\n    check_heap_repr(gdb_arrow, \"heap_buffer\",\n                    'arrow::Buffer of size 3, read-only, \"abc\"')\n    check_heap_repr(gdb_arrow, \"heap_buffer_mutable.get()\",\n                    'arrow::Buffer of size 3, mutable, \"abc\"')\n\n\ndef test_decimals(gdb_arrow):\n    v128 = \"98765432109876543210987654321098765432\"\n    check_stack_repr(gdb_arrow, \"decimal128_zero\", \"arrow::Decimal128(0)\")\n    check_stack_repr(gdb_arrow, \"decimal128_pos\",\n                     f\"arrow::Decimal128({v128})\")\n    check_stack_repr(gdb_arrow, \"decimal128_neg\",\n                     f\"arrow::Decimal128(-{v128})\")\n    check_stack_repr(gdb_arrow, \"basic_decimal128_zero\",\n                     \"arrow::BasicDecimal128(0)\")\n    check_stack_repr(gdb_arrow, \"basic_decimal128_pos\",\n                     f\"arrow::BasicDecimal128({v128})\")\n    check_stack_repr(gdb_arrow, \"basic_decimal128_neg\",\n                     f\"arrow::BasicDecimal128(-{v128})\")\n\n    v256 = (\"9876543210987654321098765432109876543210\"\n            \"987654321098765432109876543210987654\")\n    check_stack_repr(gdb_arrow, \"decimal256_zero\", \"arrow::Decimal256(0)\")\n    check_stack_repr(gdb_arrow, \"decimal256_pos\",\n                     f\"arrow::Decimal256({v256})\")\n    check_stack_repr(gdb_arrow, \"decimal256_neg\",\n                     f\"arrow::Decimal256(-{v256})\")\n    check_stack_repr(gdb_arrow, \"basic_decimal256_zero\",\n                     \"arrow::BasicDecimal256(0)\")\n    check_stack_repr(gdb_arrow, \"basic_decimal256_pos\",\n                     f\"arrow::BasicDecimal256({v256})\")\n    check_stack_repr(gdb_arrow, \"basic_decimal256_neg\",\n                     f\"arrow::BasicDecimal256(-{v256})\")\n\n\ndef test_metadata(gdb_arrow):\n    check_heap_repr(gdb_arrow, \"empty_metadata.get()\",\n                    \"arrow::KeyValueMetadata of size 0\")\n    check_heap_repr(\n        gdb_arrow, \"metadata.get()\",\n        ('arrow::KeyValueMetadata of size 2 = {'\n         '[\"key_text\"] = \"some value\", [\"key_binary\"] = \"z\\\\000\\\\037\\\\377\"}'))\n\n\ndef test_types_stack(gdb_arrow):\n    check_stack_repr(gdb_arrow, \"null_type\", \"arrow::null()\")\n    check_stack_repr(gdb_arrow, \"bool_type\", \"arrow::boolean()\")\n\n    check_stack_repr(gdb_arrow, \"date32_type\", \"arrow::date32()\")\n    check_stack_repr(gdb_arrow, \"date64_type\", \"arrow::date64()\")\n    check_stack_repr(gdb_arrow, \"time_type_s\",\n                     \"arrow::time32(arrow::TimeUnit::SECOND)\")\n    check_stack_repr(gdb_arrow, \"time_type_ms\",\n                     \"arrow::time32(arrow::TimeUnit::MILLI)\")\n    check_stack_repr(gdb_arrow, \"time_type_us\",\n                     \"arrow::time64(arrow::TimeUnit::MICRO)\")\n    check_stack_repr(gdb_arrow, \"time_type_ns\",\n                     \"arrow::time64(arrow::TimeUnit::NANO)\")\n    check_stack_repr(gdb_arrow, \"timestamp_type_s\",\n                     \"arrow::timestamp(arrow::TimeUnit::SECOND)\")\n    check_stack_repr(\n        gdb_arrow, \"timestamp_type_ms_timezone\",\n        'arrow::timestamp(arrow::TimeUnit::MILLI, \"Europe/Paris\")')\n    check_stack_repr(gdb_arrow, \"timestamp_type_us\",\n                     \"arrow::timestamp(arrow::TimeUnit::MICRO)\")\n    check_stack_repr(\n        gdb_arrow, \"timestamp_type_ns_timezone\",\n        'arrow::timestamp(arrow::TimeUnit::NANO, \"Europe/Paris\")')\n\n    check_stack_repr(gdb_arrow, \"day_time_interval_type\",\n                     \"arrow::day_time_interval()\")\n    check_stack_repr(gdb_arrow, \"month_interval_type\",\n                     \"arrow::month_interval()\")\n    check_stack_repr(gdb_arrow, \"month_day_nano_interval_type\",\n                     \"arrow::month_day_nano_interval()\")\n    check_stack_repr(gdb_arrow, \"duration_type_s\",\n                     \"arrow::duration(arrow::TimeUnit::SECOND)\")\n    check_stack_repr(gdb_arrow, \"duration_type_ns\",\n                     \"arrow::duration(arrow::TimeUnit::NANO)\")\n\n    check_stack_repr(gdb_arrow, \"decimal128_type\",\n                     \"arrow::decimal128(16, 5)\")\n    check_stack_repr(gdb_arrow, \"decimal256_type\",\n                     \"arrow::decimal256(42, 12)\")\n\n    check_stack_repr(gdb_arrow, \"binary_type\", \"arrow::binary()\")\n    check_stack_repr(gdb_arrow, \"string_type\", \"arrow::utf8()\")\n    check_stack_repr(gdb_arrow, \"large_binary_type\", \"arrow::large_binary()\")\n    check_stack_repr(gdb_arrow, \"large_string_type\", \"arrow::large_utf8()\")\n    check_stack_repr(gdb_arrow, \"fixed_size_binary_type\",\n                     \"arrow::fixed_size_binary(10)\")\n\n    check_stack_repr(gdb_arrow, \"list_type\",\n                     \"arrow::list(arrow::uint8())\")\n    check_stack_repr(gdb_arrow, \"large_list_type\",\n                     \"arrow::large_list(arrow::large_utf8())\")\n    check_stack_repr(gdb_arrow, \"fixed_size_list_type\",\n                     \"arrow::fixed_size_list(arrow::float64(), 3)\")\n    check_stack_repr(\n        gdb_arrow, \"map_type_unsorted\",\n        \"arrow::map(arrow::utf8(), arrow::binary(), keys_sorted=false)\")\n    check_stack_repr(\n        gdb_arrow, \"map_type_sorted\",\n        \"arrow::map(arrow::utf8(), arrow::binary(), keys_sorted=true)\")\n\n    check_stack_repr(gdb_arrow, \"struct_type_empty\",\n                     \"arrow::struct_({})\")\n    check_stack_repr(\n        gdb_arrow, \"struct_type\",\n        ('arrow::struct_({arrow::field(\"ints\", arrow::int8()), '\n         'arrow::field(\"strs\", arrow::utf8(), nullable=false)})'))\n\n    check_stack_repr(\n        gdb_arrow, \"sparse_union_type\",\n        ('arrow::sparse_union(fields={arrow::field(\"ints\", arrow::int8()), '\n         'arrow::field(\"strs\", arrow::utf8(), nullable=false)}, '\n         'type_codes={7, 42})'))\n    check_stack_repr(\n        gdb_arrow, \"dense_union_type\",\n        ('arrow::dense_union(fields={arrow::field(\"ints\", arrow::int8()), '\n         'arrow::field(\"strs\", arrow::utf8(), nullable=false)}, '\n         'type_codes={7, 42})'))\n\n    check_stack_repr(\n        gdb_arrow, \"dict_type_unordered\",\n        \"arrow::dictionary(arrow::int16(), arrow::utf8(), ordered=false)\")\n    check_stack_repr(\n        gdb_arrow, \"dict_type_ordered\",\n        \"arrow::dictionary(arrow::int16(), arrow::utf8(), ordered=true)\")\n\n    check_stack_repr(\n        gdb_arrow, \"uuid_type\",\n        ('arrow::ExtensionType \"extension<uuid>\" '\n         'with storage type arrow::fixed_size_binary(16)'))\n\n\ndef test_types_heap(gdb_arrow):\n    check_heap_repr(gdb_arrow, \"heap_null_type\", \"arrow::null()\")\n    check_heap_repr(gdb_arrow, \"heap_bool_type\", \"arrow::boolean()\")\n\n    check_heap_repr(gdb_arrow, \"heap_time_type_ns\",\n                    \"arrow::time64(arrow::TimeUnit::NANO)\")\n    check_heap_repr(\n        gdb_arrow, \"heap_timestamp_type_ns_timezone\",\n        'arrow::timestamp(arrow::TimeUnit::NANO, \"Europe/Paris\")')\n\n    check_heap_repr(gdb_arrow, \"heap_decimal128_type\",\n                    \"arrow::decimal128(16, 5)\")\n\n    check_heap_repr(gdb_arrow, \"heap_list_type\",\n                    \"arrow::list(arrow::uint8())\")\n    check_heap_repr(gdb_arrow, \"heap_large_list_type\",\n                    \"arrow::large_list(arrow::large_utf8())\")\n    check_heap_repr(gdb_arrow, \"heap_fixed_size_list_type\",\n                    \"arrow::fixed_size_list(arrow::float64(), 3)\")\n    check_heap_repr(\n        gdb_arrow, \"heap_map_type\",\n        \"arrow::map(arrow::utf8(), arrow::binary(), keys_sorted=false)\")\n\n    check_heap_repr(\n        gdb_arrow, \"heap_struct_type\",\n        ('arrow::struct_({arrow::field(\"ints\", arrow::int8()), '\n         'arrow::field(\"strs\", arrow::utf8(), nullable=false)})'))\n\n    check_heap_repr(\n        gdb_arrow, \"heap_dict_type\",\n        \"arrow::dictionary(arrow::int16(), arrow::utf8(), ordered=false)\")\n\n    check_heap_repr(\n        gdb_arrow, \"heap_uuid_type\",\n        ('arrow::ExtensionType \"extension<uuid>\" '\n         'with storage type arrow::fixed_size_binary(16)'))\n\n\ndef test_fields_stack(gdb_arrow):\n    check_stack_repr(gdb_arrow, \"int_field\",\n                     'arrow::field(\"ints\", arrow::int64())')\n    check_stack_repr(\n        gdb_arrow, \"float_field\",\n        'arrow::field(\"floats\", arrow::float32(), nullable=false)')\n\n\ndef test_fields_heap(gdb_arrow):\n    check_heap_repr(gdb_arrow, \"heap_int_field\",\n                    'arrow::field(\"ints\", arrow::int64())')\n\n\ndef test_scalars_stack(gdb_arrow):\n    check_stack_repr(gdb_arrow, \"null_scalar\", \"arrow::NullScalar\")\n    check_stack_repr(gdb_arrow, \"bool_scalar\",\n                     \"arrow::BooleanScalar of value true\")\n    check_stack_repr(gdb_arrow, \"bool_scalar_null\",\n                     \"arrow::BooleanScalar of null value\")\n    check_stack_repr(gdb_arrow, \"int8_scalar\",\n                     \"arrow::Int8Scalar of value -42\")\n    check_stack_repr(gdb_arrow, \"uint8_scalar\",\n                     \"arrow::UInt8Scalar of value 234\")\n    check_stack_repr(gdb_arrow, \"int64_scalar\",\n                     \"arrow::Int64Scalar of value -9223372036854775808\")\n    check_stack_repr(gdb_arrow, \"uint64_scalar\",\n                     \"arrow::UInt64Scalar of value 18446744073709551615\")\n    check_stack_repr(gdb_arrow, \"half_float_scalar\",\n                     \"arrow::HalfFloatScalar of value -1.5 [48640]\")\n    check_stack_repr(gdb_arrow, \"float_scalar\",\n                     \"arrow::FloatScalar of value 1.25\")\n    check_stack_repr(gdb_arrow, \"double_scalar\",\n                     \"arrow::DoubleScalar of value 2.5\")\n\n    check_stack_repr(gdb_arrow, \"time_scalar_s\",\n                     \"arrow::Time32Scalar of value 100s\")\n    check_stack_repr(gdb_arrow, \"time_scalar_ms\",\n                     \"arrow::Time32Scalar of value 1000ms\")\n    check_stack_repr(gdb_arrow, \"time_scalar_us\",\n                     \"arrow::Time64Scalar of value 10000us\")\n    check_stack_repr(gdb_arrow, \"time_scalar_ns\",\n                     \"arrow::Time64Scalar of value 100000ns\")\n    check_stack_repr(gdb_arrow, \"time_scalar_null\",\n                     \"arrow::Time64Scalar of null value [ns]\")\n\n    check_stack_repr(gdb_arrow, \"duration_scalar_s\",\n                     \"arrow::DurationScalar of value -100s\")\n    check_stack_repr(gdb_arrow, \"duration_scalar_ms\",\n                     \"arrow::DurationScalar of value -1000ms\")\n    check_stack_repr(gdb_arrow, \"duration_scalar_us\",\n                     \"arrow::DurationScalar of value -10000us\")\n    check_stack_repr(gdb_arrow, \"duration_scalar_ns\",\n                     \"arrow::DurationScalar of value -100000ns\")\n    check_stack_repr(gdb_arrow, \"duration_scalar_null\",\n                     \"arrow::DurationScalar of null value [ns]\")\n\n    check_stack_repr(\n        gdb_arrow, \"timestamp_scalar_s\",\n        \"arrow::TimestampScalar of value 12345s [no timezone]\")\n    check_stack_repr(\n        gdb_arrow, \"timestamp_scalar_ms\",\n        \"arrow::TimestampScalar of value -123456ms [no timezone]\")\n    check_stack_repr(\n        gdb_arrow, \"timestamp_scalar_us\",\n        \"arrow::TimestampScalar of value 1234567us [no timezone]\")\n    check_stack_repr(\n        gdb_arrow, \"timestamp_scalar_ns\",\n        \"arrow::TimestampScalar of value -12345678ns [no timezone]\")\n    check_stack_repr(\n        gdb_arrow, \"timestamp_scalar_null\",\n        \"arrow::TimestampScalar of null value [ns, no timezone]\")\n\n    check_stack_repr(\n        gdb_arrow, \"timestamp_scalar_s_tz\",\n        'arrow::TimestampScalar of value 12345s [\"Europe/Paris\"]')\n    check_stack_repr(\n        gdb_arrow, \"timestamp_scalar_ms_tz\",\n        'arrow::TimestampScalar of value -123456ms [\"Europe/Paris\"]')\n    check_stack_repr(\n        gdb_arrow, \"timestamp_scalar_us_tz\",\n        'arrow::TimestampScalar of value 1234567us [\"Europe/Paris\"]')\n    check_stack_repr(\n        gdb_arrow, \"timestamp_scalar_ns_tz\",\n        'arrow::TimestampScalar of value -12345678ns [\"Europe/Paris\"]')\n    check_stack_repr(\n        gdb_arrow, \"timestamp_scalar_null_tz\",\n        'arrow::TimestampScalar of null value [ns, \"Europe/Paris\"]')\n\n    check_stack_repr(gdb_arrow, \"month_interval_scalar\",\n                     \"arrow::MonthIntervalScalar of value 23M\")\n    check_stack_repr(gdb_arrow, \"month_interval_scalar_null\",\n                     \"arrow::MonthIntervalScalar of null value\")\n    check_stack_repr(gdb_arrow, \"day_time_interval_scalar\",\n                     \"arrow::DayTimeIntervalScalar of value 23d-456ms\")\n    check_stack_repr(gdb_arrow, \"day_time_interval_scalar_null\",\n                     \"arrow::DayTimeIntervalScalar of null value\")\n    check_stack_repr(\n        gdb_arrow, \"month_day_nano_interval_scalar\",\n        \"arrow::MonthDayNanoIntervalScalar of value 1M23d-456ns\")\n    check_stack_repr(\n        gdb_arrow, \"month_day_nano_interval_scalar_null\",\n        \"arrow::MonthDayNanoIntervalScalar of null value\")\n\n    check_stack_repr(gdb_arrow, \"date32_scalar\",\n                     \"arrow::Date32Scalar of value 23d [1970-01-24]\")\n    check_stack_repr(gdb_arrow, \"date32_scalar_null\",\n                     \"arrow::Date32Scalar of null value\")\n    check_stack_repr(gdb_arrow, \"date64_scalar\",\n                     \"arrow::Date64Scalar of value 3888000000ms [1970-02-15]\")\n    check_stack_repr(gdb_arrow, \"date64_scalar_null\",\n                     \"arrow::Date64Scalar of null value\")\n\n    check_stack_repr(\n        gdb_arrow, \"decimal128_scalar_null\",\n        \"arrow::Decimal128Scalar of null value [precision=10, scale=4]\")\n    check_stack_repr(\n        gdb_arrow, \"decimal128_scalar_pos_scale_pos\",\n        \"arrow::Decimal128Scalar of value 123.4567 [precision=10, scale=4]\")\n    check_stack_repr(\n        gdb_arrow, \"decimal128_scalar_pos_scale_neg\",\n        \"arrow::Decimal128Scalar of value -123.4567 [precision=10, scale=4]\")\n    check_stack_repr(\n        gdb_arrow, \"decimal128_scalar_neg_scale_pos\",\n        (\"arrow::Decimal128Scalar of value 1.234567e+10 \"\n         \"[precision=10, scale=-4]\"))\n    check_stack_repr(\n        gdb_arrow, \"decimal128_scalar_neg_scale_neg\",\n        (\"arrow::Decimal128Scalar of value -1.234567e+10 \"\n         \"[precision=10, scale=-4]\"))\n\n    check_stack_repr(\n        gdb_arrow, \"decimal256_scalar_null\",\n        \"arrow::Decimal256Scalar of null value [precision=50, scale=4]\")\n    check_stack_repr(\n        gdb_arrow, \"decimal256_scalar_pos_scale_pos\",\n        (\"arrow::Decimal256Scalar of value \"\n         \"123456789012345678901234567890123456789012.3456 \"\n         \"[precision=50, scale=4]\"))\n    check_stack_repr(\n        gdb_arrow, \"decimal256_scalar_pos_scale_neg\",\n        (\"arrow::Decimal256Scalar of value \"\n         \"-123456789012345678901234567890123456789012.3456 \"\n         \"[precision=50, scale=4]\"))\n    check_stack_repr(\n        gdb_arrow, \"decimal256_scalar_neg_scale_pos\",\n        (\"arrow::Decimal256Scalar of value \"\n         \"1.234567890123456789012345678901234567890123456e+49 \"\n         \"[precision=50, scale=-4]\"))\n    check_stack_repr(\n        gdb_arrow, \"decimal256_scalar_neg_scale_neg\",\n        (\"arrow::Decimal256Scalar of value \"\n         \"-1.234567890123456789012345678901234567890123456e+49 \"\n         \"[precision=50, scale=-4]\"))\n\n    check_stack_repr(\n        gdb_arrow, \"binary_scalar_null\",\n        \"arrow::BinaryScalar of null value\")\n    check_stack_repr(\n        gdb_arrow, \"binary_scalar_unallocated\",\n        \"arrow::BinaryScalar of value <unallocated>\")\n    check_stack_repr(\n        gdb_arrow, \"binary_scalar_empty\",\n        'arrow::BinaryScalar of size 0, value \"\"')\n    check_stack_repr(\n        gdb_arrow, \"binary_scalar_abc\",\n        'arrow::BinaryScalar of size 3, value \"abc\"')\n    check_stack_repr(\n        gdb_arrow, \"binary_scalar_bytes\",\n        r'arrow::BinaryScalar of size 3, value \"\\000\\037\\377\"')\n    check_stack_repr(\n        gdb_arrow, \"large_binary_scalar_abc\",\n        'arrow::LargeBinaryScalar of size 3, value \"abc\"')\n\n    check_stack_repr(\n        gdb_arrow, \"string_scalar_null\",\n        \"arrow::StringScalar of null value\")\n    check_stack_repr(\n        gdb_arrow, \"string_scalar_unallocated\",\n        \"arrow::StringScalar of value <unallocated>\")\n    check_stack_repr(\n        gdb_arrow, \"string_scalar_empty\",\n        'arrow::StringScalar of size 0, value \"\"')\n    check_stack_repr(\n        gdb_arrow, \"string_scalar_hehe\",\n        'arrow::StringScalar of size 6, value \"h\u00e9h\u00e9\"')\n    # FIXME: excessive escaping ('\\\\xff' vs. '\\x00')\n    check_stack_repr(\n        gdb_arrow, \"string_scalar_invalid_chars\",\n        r'arrow::StringScalar of size 11, value \"abc\\x00def\\\\xffghi\"')\n    check_stack_repr(\n        gdb_arrow, \"large_string_scalar_hehe\",\n        'arrow::LargeStringScalar of size 6, value \"h\u00e9h\u00e9\"')\n\n    check_stack_repr(\n        gdb_arrow, \"fixed_size_binary_scalar\",\n        'arrow::FixedSizeBinaryScalar of size 3, value \"abc\"')\n    check_stack_repr(\n        gdb_arrow, \"fixed_size_binary_scalar_null\",\n        'arrow::FixedSizeBinaryScalar of size 3, null with value \"   \"')\n\n    check_stack_repr(\n        gdb_arrow, \"dict_scalar\",\n        re.compile(\n            (r'^arrow::DictionaryScalar of index '\n             r'arrow::Int8Scalar of value 42, '\n             r'dictionary arrow::StringArray ')))\n    check_stack_repr(\n        gdb_arrow, \"dict_scalar_null\",\n        ('arrow::DictionaryScalar of type '\n         'arrow::dictionary(arrow::int8(), arrow::utf8(), ordered=false), '\n         'null value'))\n\n    check_stack_repr(\n        gdb_arrow, \"list_scalar\",\n        ('arrow::ListScalar of value arrow::Int32Array of '\n         'length 3, offset 0, null count 0 = {[0] = 4, [1] = 5, [2] = 6}'))\n    check_stack_repr(\n        gdb_arrow, \"list_scalar_null\",\n        'arrow::ListScalar of type arrow::list(arrow::int32()), null value')\n    check_stack_repr(\n        gdb_arrow, \"large_list_scalar\",\n        ('arrow::LargeListScalar of value arrow::Int32Array of '\n         'length 3, offset 0, null count 0 = {[0] = 4, [1] = 5, [2] = 6}'))\n    check_stack_repr(\n        gdb_arrow, \"large_list_scalar_null\",\n        ('arrow::LargeListScalar of type arrow::large_list(arrow::int32()), '\n         'null value'))\n    check_stack_repr(\n        gdb_arrow, \"fixed_size_list_scalar\",\n        ('arrow::FixedSizeListScalar of value arrow::Int32Array of '\n         'length 3, offset 0, null count 0 = {[0] = 4, [1] = 5, [2] = 6}'))\n    check_stack_repr(\n        gdb_arrow, \"fixed_size_list_scalar_null\",\n        ('arrow::FixedSizeListScalar of type '\n         'arrow::fixed_size_list(arrow::int32(), 3), null value'))\n\n    check_stack_repr(\n        gdb_arrow, \"struct_scalar\",\n        ('arrow::StructScalar = {[\"ints\"] = arrow::Int32Scalar of value 42, '\n         '[\"strs\"] = arrow::StringScalar of size 9, value \"some text\"}'))\n    check_stack_repr(\n        gdb_arrow, \"struct_scalar_null\",\n        ('arrow::StructScalar of type arrow::struct_('\n         '{arrow::field(\"ints\", arrow::int32()), '\n         'arrow::field(\"strs\", arrow::utf8())}), null value'))\n\n    check_stack_repr(\n        gdb_arrow, \"sparse_union_scalar\",\n        ('arrow::SparseUnionScalar of type code 7, '\n         'value arrow::Int32Scalar of value 43'))\n    check_stack_repr(\n        gdb_arrow, \"sparse_union_scalar_null\", re.compile(\n            r'^arrow::SparseUnionScalar of type arrow::sparse_union\\(.*\\), '\n            r'type code 7, null value$'))\n    check_stack_repr(\n        gdb_arrow, \"dense_union_scalar\",\n        ('arrow::DenseUnionScalar of type code 7, '\n         'value arrow::Int32Scalar of value 43'))\n    check_stack_repr(\n        gdb_arrow, \"dense_union_scalar_null\", re.compile(\n            r'^arrow::DenseUnionScalar of type arrow::dense_union\\(.*\\), '\n            r'type code 7, null value$'))\n\n    check_stack_repr(\n        gdb_arrow, \"extension_scalar\",\n        ('arrow::ExtensionScalar of type \"extension<uuid>\", '\n         'value arrow::FixedSizeBinaryScalar of size 16, '\n         'value \"0123456789abcdef\"'))\n    check_stack_repr(\n        gdb_arrow, \"extension_scalar_null\",\n        'arrow::ExtensionScalar of type \"extension<uuid>\", null value')\n\n\ndef test_scalars_heap(gdb_arrow):\n    check_heap_repr(gdb_arrow, \"heap_null_scalar\", \"arrow::NullScalar\")\n    check_heap_repr(gdb_arrow, \"heap_bool_scalar\",\n                    \"arrow::BooleanScalar of value true\")\n    check_heap_repr(\n        gdb_arrow, \"heap_decimal128_scalar\",\n        \"arrow::Decimal128Scalar of value 123.4567 [precision=10, scale=4]\")\n    check_heap_repr(\n        gdb_arrow, \"heap_decimal256_scalar\",\n        (\"arrow::Decimal256Scalar of value \"\n         \"123456789012345678901234567890123456789012.3456 \"\n         \"[precision=50, scale=4]\"))\n\n    check_heap_repr(\n        gdb_arrow, \"heap_map_scalar\",\n        ('arrow::MapScalar of type arrow::map(arrow::utf8(), arrow::int32(), '\n         'keys_sorted=false), value length 2, offset 0, null count 0'))\n    check_heap_repr(\n        gdb_arrow, \"heap_map_scalar_null\",\n        ('arrow::MapScalar of type arrow::map(arrow::utf8(), arrow::int32(), '\n         'keys_sorted=false), null value'))\n\n\ndef test_array_data(gdb_arrow):\n    check_stack_repr(\n        gdb_arrow, \"int32_array_data\",\n        (\"arrow::ArrayData of type arrow::int32(), length 4, offset 0, \"\n         \"null count 1 = {[0] = -5, [1] = 6, [2] = null, [3] = 42}\"))\n\n\ndef test_arrays_stack(gdb_arrow):\n    check_stack_repr(\n        gdb_arrow, \"int32_array\",\n        (\"arrow::Int32Array of length 4, offset 0, null count 1 = \"\n         \"{[0] = -5, [1] = 6, [2] = null, [3] = 42}\"))\n    check_stack_repr(\n        gdb_arrow, \"list_array\",\n        (\"arrow::ListArray of type arrow::list(arrow::int64()), \"\n         \"length 3, offset 0, null count 1\"))\n\n\ndef test_arrays_heap(gdb_arrow):\n    # Null\n    check_heap_repr(\n        gdb_arrow, \"heap_null_array\",\n        \"arrow::NullArray of length 2, offset 0, null count 2\")\n\n    # Primitive\n    check_heap_repr(\n        gdb_arrow, \"heap_int32_array\",\n        (\"arrow::Int32Array of length 4, offset 0, null count 1 = {\"\n         \"[0] = -5, [1] = 6, [2] = null, [3] = 42}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_int32_array_no_nulls\",\n        (\"arrow::Int32Array of length 4, offset 0, null count 0 = {\"\n         \"[0] = -5, [1] = 6, [2] = 3, [3] = 42}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_int32_array_sliced_1_9\",\n        (\"arrow::Int32Array of length 9, offset 1, unknown null count = {\"\n         \"[0] = 2, [1] = -3, [2] = 4, [3] = null, [4] = -5, [5] = 6, \"\n         \"[6] = -7, [7] = 8, [8] = null}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_int32_array_sliced_2_6\",\n        (\"arrow::Int32Array of length 6, offset 2, unknown null count = {\"\n         \"[0] = -3, [1] = 4, [2] = null, [3] = -5, [4] = 6, [5] = -7}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_int32_array_sliced_8_4\",\n        (\"arrow::Int32Array of length 4, offset 8, unknown null count = {\"\n         \"[0] = 8, [1] = null, [2] = -9, [3] = -10}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_int32_array_sliced_empty\",\n        \"arrow::Int32Array of length 0, offset 6, unknown null count\")\n\n    check_heap_repr(\n        gdb_arrow, \"heap_double_array\",\n        (\"arrow::DoubleArray of length 2, offset 0, null count 1 = {\"\n         \"[0] = -1.5, [1] = null}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_float16_array\",\n        (\"arrow::HalfFloatArray of length 2, offset 0, null count 0 = {\"\n         \"[0] = 0.0, [1] = -1.5}\"))\n\n    # Boolean\n    check_heap_repr(\n        gdb_arrow, \"heap_bool_array\",\n        (\"arrow::BooleanArray of length 18, offset 0, null count 6 = {\"\n         \"[0] = false, [1] = false, [2] = true, [3] = true, [4] = null, \"\n         \"[5] = null, [6] = false, [7] = false, [8] = true, [9] = true, \"\n         \"[10] = null, [11] = null, [12] = false, [13] = false, \"\n         \"[14] = true, [15] = true, [16] = null, [17] = null}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_bool_array_sliced_1_9\",\n        (\"arrow::BooleanArray of length 9, offset 1, unknown null count = {\"\n         \"[0] = false, [1] = true, [2] = true, [3] = null, [4] = null, \"\n         \"[5] = false, [6] = false, [7] = true, [8] = true}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_bool_array_sliced_2_6\",\n        (\"arrow::BooleanArray of length 6, offset 2, unknown null count = {\"\n         \"[0] = true, [1] = true, [2] = null, [3] = null, [4] = false, \"\n         \"[5] = false}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_bool_array_sliced_empty\",\n        \"arrow::BooleanArray of length 0, offset 6, unknown null count\")\n\n    # Temporal\n    check_heap_repr(\n        gdb_arrow, \"heap_date32_array\",\n        (\"arrow::Date32Array of length 6, offset 0, null count 1 = {\"\n         \"[0] = 0d [1970-01-01], [1] = null, [2] = 18336d [2020-03-15], \"\n         \"[3] = -9004d [1945-05-08], [4] = -719162d [0001-01-01], \"\n         \"[5] = -719163d [year <= 0]}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_date64_array\",\n        (\"arrow::Date64Array of length 5, offset 0, null count 0 = {\"\n         \"[0] = 1584230400000ms [2020-03-15], \"\n         \"[1] = -777945600000ms [1945-05-08], \"\n         \"[2] = -62135596800000ms [0001-01-01], \"\n         \"[3] = -62135683200000ms [year <= 0], \"\n         \"[4] = 123ms [non-multiple of 86400000]}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_time32_array_s\",\n        (\"arrow::Time32Array of type arrow::time32(arrow::TimeUnit::SECOND), \"\n         \"length 3, offset 0, null count 1 = {\"\n         \"[0] = null, [1] = -123s, [2] = 456s}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_time32_array_ms\",\n        (\"arrow::Time32Array of type arrow::time32(arrow::TimeUnit::MILLI), \"\n         \"length 3, offset 0, null count 1 = {\"\n         \"[0] = null, [1] = -123ms, [2] = 456ms}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_time64_array_us\",\n        (\"arrow::Time64Array of type arrow::time64(arrow::TimeUnit::MICRO), \"\n         \"length 3, offset 0, null count 1 = {\"\n         \"[0] = null, [1] = -123us, [2] = 456us}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_time64_array_ns\",\n        (\"arrow::Time64Array of type arrow::time64(arrow::TimeUnit::NANO), \"\n         \"length 3, offset 0, null count 1 = {\"\n         \"[0] = null, [1] = -123ns, [2] = 456ns}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_month_interval_array\",\n        (\"arrow::MonthIntervalArray of length 3, offset 0, null count 1 = {\"\n         \"[0] = 123M, [1] = -456M, [2] = null}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_day_time_interval_array\",\n        (\"arrow::DayTimeIntervalArray of length 2, offset 0, null count 1 = {\"\n         \"[0] = 1d-600ms, [1] = null}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_month_day_nano_interval_array\",\n        (\"arrow::MonthDayNanoIntervalArray of length 2, offset 0, \"\n         \"null count 1 = {[0] = 1M-600d5000ns, [1] = null}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_duration_array_s\",\n        (\"arrow::DurationArray of type arrow::duration\"\n         \"(arrow::TimeUnit::SECOND), length 2, offset 0, null count 1 = {\"\n         \"[0] = null, [1] = -1234567890123456789s}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_duration_array_ns\",\n        (\"arrow::DurationArray of type arrow::duration\"\n         \"(arrow::TimeUnit::NANO), length 2, offset 0, null count 1 = {\"\n         \"[0] = null, [1] = -1234567890123456789ns}\"))\n    if sys.maxsize > 2**32:\n        check_heap_repr(\n            gdb_arrow, \"heap_timestamp_array_s\",\n            (\"arrow::TimestampArray of type arrow::timestamp\"\n             \"(arrow::TimeUnit::SECOND), length 4, offset 0, null count 1 = {\"\n             \"[0] = null, [1] = 0s [1970-01-01 00:00:00], \"\n             \"[2] = -2203932304s [1900-02-28 12:34:56], \"\n             \"[3] = 63730281600s [3989-07-14 00:00:00]}\"))\n        check_heap_repr(\n            gdb_arrow, \"heap_timestamp_array_ms\",\n            (\"arrow::TimestampArray of type arrow::timestamp\"\n             \"(arrow::TimeUnit::MILLI), length 3, offset 0, null count 1 = {\"\n             \"[0] = null, [1] = -2203932303877ms [1900-02-28 12:34:56.123], \"\n             \"[2] = 63730281600789ms [3989-07-14 00:00:00.789]}\"))\n        check_heap_repr(\n            gdb_arrow, \"heap_timestamp_array_us\",\n            (\"arrow::TimestampArray of type arrow::timestamp\"\n             \"(arrow::TimeUnit::MICRO), length 3, offset 0, null count 1 = {\"\n             \"[0] = null, \"\n             \"[1] = -2203932303345679us [1900-02-28 12:34:56.654321], \"\n             \"[2] = 63730281600456789us [3989-07-14 00:00:00.456789]}\"))\n        check_heap_repr(\n            gdb_arrow, \"heap_timestamp_array_ns\",\n            (\"arrow::TimestampArray of type arrow::timestamp\"\n             \"(arrow::TimeUnit::NANO), length 2, offset 0, null count 1 = {\"\n             \"[0] = null, \"\n             \"[1] = -2203932303012345679ns [1900-02-28 12:34:56.987654321]}\"))\n    else:\n        # Python's datetime is limited to smaller timestamps on 32-bit platforms\n        check_heap_repr(\n            gdb_arrow, \"heap_timestamp_array_s\",\n            (\"arrow::TimestampArray of type arrow::timestamp\"\n             \"(arrow::TimeUnit::SECOND), length 4, offset 0, null count 1 = {\"\n             \"[0] = null, [1] = 0s [1970-01-01 00:00:00], \"\n             \"[2] = -2203932304s [too large to represent], \"\n             \"[3] = 63730281600s [too large to represent]}\"))\n        check_heap_repr(\n            gdb_arrow, \"heap_timestamp_array_ms\",\n            (\"arrow::TimestampArray of type arrow::timestamp\"\n             \"(arrow::TimeUnit::MILLI), length 3, offset 0, null count 1 = {\"\n             \"[0] = null, [1] = -2203932303877ms [too large to represent], \"\n             \"[2] = 63730281600789ms [too large to represent]}\"))\n        check_heap_repr(\n            gdb_arrow, \"heap_timestamp_array_us\",\n            (\"arrow::TimestampArray of type arrow::timestamp\"\n             \"(arrow::TimeUnit::MICRO), length 3, offset 0, null count 1 = {\"\n             \"[0] = null, \"\n             \"[1] = -2203932303345679us [too large to represent], \"\n             \"[2] = 63730281600456789us [too large to represent]}\"))\n        check_heap_repr(\n            gdb_arrow, \"heap_timestamp_array_ns\",\n            (\"arrow::TimestampArray of type arrow::timestamp\"\n             \"(arrow::TimeUnit::NANO), length 2, offset 0, null count 1 = {\"\n             \"[0] = null, \"\n             \"[1] = -2203932303012345679ns [too large to represent]}\"))\n\n    # Decimal\n    check_heap_repr(\n        gdb_arrow, \"heap_decimal128_array\",\n        (\"arrow::Decimal128Array of type arrow::decimal128(30, 6), \"\n         \"length 3, offset 0, null count 1 = {\"\n         \"[0] = null, [1] = -1234567890123456789.012345, \"\n         \"[2] = 1234567890123456789.012345}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_decimal256_array\",\n        (\"arrow::Decimal256Array of type arrow::decimal256(50, 6), \"\n         \"length 2, offset 0, null count 1 = {\"\n         \"[0] = null, \"\n         \"[1] = -123456789012345678901234567890123456789.012345}\"))\n    check_heap_repr(\n        gdb_arrow, \"heap_decimal128_array_sliced\",\n        (\"arrow::Decimal128Array of type arrow::decimal128(30, 6), \"\n         \"length 1, offset 1, unknown null count = {\"\n         \"[0] = -1234567890123456789.012345}\"))\n\n    # Binary-like\n    check_heap_repr(\n        gdb_arrow, \"heap_fixed_size_binary_array\",\n        (r'arrow::FixedSizeBinaryArray of type arrow::fixed_size_binary(3), '\n         r'length 3, offset 0, null count 1 = {'\n         r'[0] = null, [1] = \"abc\", [2] = \"\\000\\037\\377\"}'))\n    check_heap_repr(\n        gdb_arrow, \"heap_fixed_size_binary_array_zero_width\",\n        (r'arrow::FixedSizeBinaryArray of type arrow::fixed_size_binary(0), '\n         r'length 2, offset 0, null count 1 = {[0] = null, [1] = \"\"}'))\n    check_heap_repr(\n        gdb_arrow, \"heap_fixed_size_binary_array_sliced\",\n        (r'arrow::FixedSizeBinaryArray of type arrow::fixed_size_binary(3), '\n         r'length 1, offset 1, unknown null count = {[0] = \"abc\"}'))\n    check_heap_repr(\n        gdb_arrow, \"heap_binary_array\",\n        (r'arrow::BinaryArray of length 3, offset 0, null count 1 = {'\n         r'[0] = null, [1] = \"abcd\", [2] = \"\\000\\037\\377\"}'))\n    check_heap_repr(\n        gdb_arrow, \"heap_large_binary_array\",\n        (r'arrow::LargeBinaryArray of length 3, offset 0, null count 1 = {'\n         r'[0] = null, [1] = \"abcd\", [2] = \"\\000\\037\\377\"}'))\n    check_heap_repr(\n        gdb_arrow, \"heap_string_array\",\n        (r'arrow::StringArray of length 3, offset 0, null count 1 = {'\n         r'[0] = null, [1] = \"h\u00e9h\u00e9\", [2] = \"invalid \\\\xff char\"}'))\n    check_heap_repr(\n        gdb_arrow, \"heap_large_string_array\",\n        (r'arrow::LargeStringArray of length 3, offset 0, null count 1 = {'\n         r'[0] = null, [1] = \"h\u00e9h\u00e9\", [2] = \"invalid \\\\xff char\"}'))\n    check_heap_repr(\n        gdb_arrow, \"heap_binary_array_sliced\",\n        (r'arrow::BinaryArray of length 1, offset 1, unknown null count = '\n         r'{[0] = \"abcd\"}'))\n\n    # Nested\n    check_heap_repr(\n        gdb_arrow, \"heap_list_array\",\n        (\"arrow::ListArray of type arrow::list(arrow::int64()), \"\n         \"length 3, offset 0, null count 1\"))\n\n\ndef test_schema(gdb_arrow):\n    check_heap_repr(gdb_arrow, \"schema_empty\",\n                    \"arrow::Schema with 0 fields\")\n    check_heap_repr(\n        gdb_arrow, \"schema_non_empty\",\n        ('arrow::Schema with 2 fields = {[\"ints\"] = arrow::int8(), '\n         '[\"strs\"] = arrow::utf8()}'))\n    check_heap_repr(\n        gdb_arrow, \"schema_with_metadata\",\n        ('arrow::Schema with 2 fields and 2 metadata items = '\n         '{[\"ints\"] = arrow::int8(), [\"strs\"] = arrow::utf8()}'))\n\n\ndef test_chunked_array(gdb_arrow):\n    check_stack_repr(\n        gdb_arrow, \"chunked_array\",\n        (\"arrow::ChunkedArray of type arrow::int32(), length 5, null count 1 \"\n         \"with 2 chunks = {[0] = length 2, offset 0, null count 0, \"\n         \"[1] = length 3, offset 0, null count 1}\"))\n\n\ndef test_record_batch(gdb_arrow):\n    expected_prefix = 'arrow::RecordBatch with 2 columns, 3 rows'\n    expected_suffix = (\n        '{[\"ints\"] = arrow::ArrayData of type arrow::int32(), '\n        'length 3, offset 0, null count 0 = '\n        '{[0] = 1, [1] = 2, [2] = 3}, '\n        '[\"strs\"] = arrow::ArrayData of type arrow::utf8(), '\n        'length 3, offset 0, null count 1 = '\n        '{[0] = \"abc\", [1] = null, [2] = \"def\"}}')\n\n    expected = f\"{expected_prefix} = {expected_suffix}\"\n    # Representations may differ between those two because of\n    # RecordBatch (base class) vs. SimpleRecordBatch (concrete class).\n    check_heap_repr(gdb_arrow, \"batch\", expected)\n    check_heap_repr(gdb_arrow, \"batch.get()\", expected)\n\n    expected = f\"{expected_prefix}, 3 metadata items = {expected_suffix}\"\n    check_heap_repr(gdb_arrow, \"batch_with_metadata\", expected)\n\n\ndef test_table(gdb_arrow):\n    expected_table = (\n        'arrow::Table with 2 columns, 5 rows = {'\n        '[\"ints\"] = arrow::ChunkedArray of type arrow::int32(), '\n        'length 5, null count 0 with 2 chunks = '\n        '{[0] = length 3, offset 0, null count 0, '\n        '[1] = length 2, offset 0, null count 0}, '\n        '[\"strs\"] = arrow::ChunkedArray of type arrow::utf8(), '\n        'length 5, null count 1 with 3 chunks = '\n        '{[0] = length 2, offset 0, null count 1, '\n        '[1] = length 1, offset 0, null count 0, '\n        '[2] = length 2, offset 0, null count 0}}')\n\n    # Same as RecordBatch above (Table vs. SimpleTable)\n    check_heap_repr(gdb_arrow, \"table\", expected_table)\n    check_heap_repr(gdb_arrow, \"table.get()\", expected_table)\n\n\ndef test_datum(gdb_arrow):\n    check_stack_repr(gdb_arrow, \"empty_datum\", \"arrow::Datum (empty)\")\n    check_stack_repr(\n        gdb_arrow, \"scalar_datum\",\n        \"arrow::Datum of value arrow::BooleanScalar of null value\")\n    check_stack_repr(\n        gdb_arrow, \"array_datum\",\n        re.compile(r\"^arrow::Datum of value arrow::ArrayData of type \"))\n    check_stack_repr(\n        gdb_arrow, \"chunked_array_datum\",\n        re.compile(r\"^arrow::Datum of value arrow::ChunkedArray of type \"))\n    check_stack_repr(\n        gdb_arrow, \"batch_datum\",\n        re.compile(r\"^arrow::Datum of value arrow::RecordBatch \"\n                   r\"with 2 columns, 3 rows \"))\n    check_stack_repr(\n        gdb_arrow, \"table_datum\",\n        re.compile(r\"^arrow::Datum of value arrow::Table \"\n                   r\"with 2 columns, 5 rows \"))\n", "python/pyarrow/tests/test_pandas.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport gc\nimport decimal\nimport json\nimport multiprocessing as mp\nimport sys\nimport warnings\n\nfrom collections import OrderedDict\nfrom datetime import date, datetime, time, timedelta, timezone\n\nimport hypothesis as h\nimport hypothesis.strategies as st\nimport numpy as np\nimport numpy.testing as npt\nimport pytest\n\nfrom pyarrow.pandas_compat import get_logical_type, _pandas_api\nfrom pyarrow.tests.util import invoke_script, random_ascii, rands\nimport pyarrow.tests.strategies as past\nimport pyarrow.tests.util as test_util\nfrom pyarrow.vendored.version import Version\n\nimport pyarrow as pa\ntry:\n    from pyarrow import parquet as pq\nexcept ImportError:\n    pass\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n    from .pandas_examples import dataframe_with_arrays, dataframe_with_lists\nexcept ImportError:\n    pass\n\n\ntry:\n    _np_VisibleDeprecationWarning = np.VisibleDeprecationWarning\nexcept AttributeError:\n    from numpy.exceptions import (\n        VisibleDeprecationWarning as _np_VisibleDeprecationWarning\n    )\n\n\n# Marks all of the tests in this module\npytestmark = pytest.mark.pandas\n\n\ndef _alltypes_example(size=100):\n    return pd.DataFrame({\n        'uint8': np.arange(size, dtype=np.uint8),\n        'uint16': np.arange(size, dtype=np.uint16),\n        'uint32': np.arange(size, dtype=np.uint32),\n        'uint64': np.arange(size, dtype=np.uint64),\n        'int8': np.arange(size, dtype=np.int16),\n        'int16': np.arange(size, dtype=np.int16),\n        'int32': np.arange(size, dtype=np.int32),\n        'int64': np.arange(size, dtype=np.int64),\n        'float32': np.arange(size, dtype=np.float32),\n        'float64': np.arange(size, dtype=np.float64),\n        'bool': np.random.randn(size) > 0,\n        'datetime[s]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                 dtype='datetime64[s]'),\n        'datetime[ms]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                  dtype='datetime64[ms]'),\n        'datetime[us]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                  dtype='datetime64[us]'),\n        'datetime[ns]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                  dtype='datetime64[ns]'),\n        'timedelta64[s]': np.arange(0, size, dtype='timedelta64[s]'),\n        'timedelta64[ms]': np.arange(0, size, dtype='timedelta64[ms]'),\n        'timedelta64[us]': np.arange(0, size, dtype='timedelta64[us]'),\n        'timedelta64[ns]': np.arange(0, size, dtype='timedelta64[ns]'),\n        'str': [str(x) for x in range(size)],\n        'str_with_nulls': [None] + [str(x) for x in range(size - 2)] + [None],\n        'empty_str': [''] * size\n    })\n\n\ndef _check_pandas_roundtrip(df, expected=None, use_threads=False,\n                            expected_schema=None,\n                            check_dtype=True, schema=None,\n                            preserve_index=False,\n                            as_batch=False):\n    klass = pa.RecordBatch if as_batch else pa.Table\n    table = klass.from_pandas(df, schema=schema,\n                              preserve_index=preserve_index,\n                              nthreads=2 if use_threads else 1)\n    result = table.to_pandas(use_threads=use_threads)\n\n    if expected_schema:\n        # all occurrences of _check_pandas_roundtrip passes expected_schema\n        # without the pandas generated key-value metadata\n        assert table.schema.equals(expected_schema)\n\n    if expected is None:\n        expected = df\n\n        for col in expected.columns:\n            if expected[col].dtype == 'object':\n                expected[col] = expected[col].replace({np.nan: None})\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \"elementwise comparison failed\", DeprecationWarning)\n        tm.assert_frame_equal(result, expected, check_dtype=check_dtype,\n                              check_index_type=('equiv' if preserve_index\n                                                else False))\n\n\ndef _check_series_roundtrip(s, type_=None, expected_pa_type=None):\n    arr = pa.array(s, from_pandas=True, type=type_)\n\n    if type_ is not None and expected_pa_type is None:\n        expected_pa_type = type_\n\n    if expected_pa_type is not None:\n        assert arr.type == expected_pa_type\n\n    result = pd.Series(arr.to_pandas(), name=s.name)\n    tm.assert_series_equal(s, result)\n\n\ndef _check_array_roundtrip(values, expected=None, mask=None,\n                           type=None):\n    arr = pa.array(values, from_pandas=True, mask=mask, type=type)\n    result = arr.to_pandas()\n\n    values_nulls = pd.isnull(values)\n    if mask is None:\n        assert arr.null_count == values_nulls.sum()\n    else:\n        assert arr.null_count == (mask | values_nulls).sum()\n\n    if expected is None:\n        if mask is None:\n            expected = pd.Series(values)\n        else:\n            expected = pd.Series(values).copy()\n            expected[mask.copy()] = None\n\n        if expected.dtype == 'object':\n            expected = expected.replace({np.nan: None})\n\n    tm.assert_series_equal(pd.Series(result), expected, check_names=False)\n\n\ndef _check_array_from_pandas_roundtrip(np_array, type=None):\n    arr = pa.array(np_array, from_pandas=True, type=type)\n    result = arr.to_pandas()\n    npt.assert_array_equal(result, np_array)\n\n\nclass TestConvertMetadata:\n    \"\"\"\n    Conversion tests for Pandas metadata & indices.\n    \"\"\"\n\n    def test_non_string_columns(self):\n        df = pd.DataFrame({0: [1, 2, 3]})\n        table = pa.Table.from_pandas(df)\n        assert table.field(0).name == '0'\n\n    def test_non_string_columns_with_index(self):\n        df = pd.DataFrame({0: [1.0, 2.0, 3.0], 1: [4.0, 5.0, 6.0]})\n        df = df.set_index(0)\n\n        # assert that the from_pandas raises the warning\n        with pytest.warns(UserWarning):\n            table = pa.Table.from_pandas(df)\n            assert table.field(0).name == '1'\n\n        expected = df.copy()\n        # non-str index name will be converted to str\n        expected.index.name = str(expected.index.name)\n        with pytest.warns(UserWarning):\n            _check_pandas_roundtrip(df, expected=expected,\n                                    preserve_index=True)\n\n    def test_from_pandas_with_columns(self):\n        df = pd.DataFrame({0: [1, 2, 3], 1: [1, 3, 3], 2: [2, 4, 5]},\n                          columns=[1, 0])\n\n        table = pa.Table.from_pandas(df, columns=[0, 1])\n        expected = pa.Table.from_pandas(df[[0, 1]])\n        assert expected.equals(table)\n\n        record_batch_table = pa.RecordBatch.from_pandas(df, columns=[0, 1])\n        record_batch_expected = pa.RecordBatch.from_pandas(df[[0, 1]])\n        assert record_batch_expected.equals(record_batch_table)\n\n    def test_column_index_names_are_preserved(self):\n        df = pd.DataFrame({'data': [1, 2, 3]})\n        df.columns.names = ['a']\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_column_index_names_with_tz(self):\n        # ARROW-13756\n        # Bug if index is timezone aware DataTimeIndex\n\n        df = pd.DataFrame(\n            np.random.randn(5, 3),\n            columns=pd.date_range(\"2021-01-01\", periods=3, freq=\"50D\", tz=\"CET\")\n        )\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_column_index_names_with_decimal(self):\n        # GH-41503: Test valid roundtrip with decimal value in column index\n        df = pd.DataFrame(\n            [[decimal.Decimal(5), decimal.Decimal(6)]],\n            columns=pd.MultiIndex.from_product(\n                [[decimal.Decimal(1)], [decimal.Decimal(2), decimal.Decimal(3)]]\n            ),\n            index=[decimal.Decimal(4)],\n        )\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_range_index_shortcut(self):\n        # ARROW-1639\n        index_name = 'foo'\n        df = pd.DataFrame({'a': [1, 2, 3, 4]},\n                          index=pd.RangeIndex(0, 8, step=2, name=index_name))\n\n        df2 = pd.DataFrame({'a': [4, 5, 6, 7]},\n                           index=pd.RangeIndex(0, 4))\n\n        table = pa.Table.from_pandas(df)\n        table_no_index_name = pa.Table.from_pandas(df2)\n\n        # The RangeIndex is tracked in the metadata only\n        assert len(table.schema) == 1\n\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, df)\n        assert isinstance(result.index, pd.RangeIndex)\n        assert _pandas_api.get_rangeindex_attribute(result.index, 'step') == 2\n        assert result.index.name == index_name\n\n        result2 = table_no_index_name.to_pandas()\n        tm.assert_frame_equal(result2, df2)\n        assert isinstance(result2.index, pd.RangeIndex)\n        assert _pandas_api.get_rangeindex_attribute(result2.index, 'step') == 1\n        assert result2.index.name is None\n\n    def test_range_index_force_serialization(self):\n        # ARROW-5427: preserve_index=True will force the RangeIndex to\n        # be serialized as a column rather than tracked more\n        # efficiently as metadata\n        df = pd.DataFrame({'a': [1, 2, 3, 4]},\n                          index=pd.RangeIndex(0, 8, step=2, name='foo'))\n\n        table = pa.Table.from_pandas(df, preserve_index=True)\n        assert table.num_columns == 2\n        assert 'foo' in table.column_names\n\n        restored = table.to_pandas()\n        tm.assert_frame_equal(restored, df)\n\n    def test_rangeindex_doesnt_warn(self):\n        # ARROW-5606: pandas 0.25 deprecated private _start/stop/step\n        # attributes -> can be removed if support < pd 0.25 is dropped\n        df = pd.DataFrame(np.random.randn(4, 2), columns=['a', 'b'])\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(action=\"error\")\n            # make_block deprecation in pandas, still under discussion\n            # https://github.com/pandas-dev/pandas/pull/56422\n            # https://github.com/pandas-dev/pandas/issues/40226\n            warnings.filterwarnings(\n                \"ignore\", \"make_block is deprecated\", DeprecationWarning\n            )\n            _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_columns(self):\n        columns = pd.MultiIndex.from_arrays([\n            ['one', 'two'], ['X', 'Y']\n        ])\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_columns_with_dtypes(self):\n        columns = pd.MultiIndex.from_arrays(\n            [\n                ['one', 'two'],\n                pd.DatetimeIndex(['2017-08-01', '2017-08-02']),\n            ],\n            names=['level_1', 'level_2'],\n        )\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_with_column_dtype_object(self):\n        # ARROW-3651 & ARROW-9096\n        # Bug when dtype of the columns is object.\n\n        # uinderlying dtype: integer\n        df = pd.DataFrame([1], columns=pd.Index([1], dtype=object))\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n        # underlying dtype: floating\n        df = pd.DataFrame([1], columns=pd.Index([1.1], dtype=object))\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n        # underlying dtype: datetime\n        # ARROW-9096: a simple roundtrip now works\n        df = pd.DataFrame([1], columns=pd.Index(\n            [datetime(2018, 1, 1)], dtype=\"object\"))\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_columns_unicode(self):\n        columns = pd.MultiIndex.from_arrays([['\u3042', '\u3044'], ['X', 'Y']])\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_doesnt_warn(self):\n        # ARROW-3953: pandas 0.24 rename of MultiIndex labels to codes\n        columns = pd.MultiIndex.from_arrays([['one', 'two'], ['X', 'Y']])\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(action=\"error\")\n            # make_block deprecation in pandas, still under discussion\n            # https://github.com/pandas-dev/pandas/pull/56422\n            # https://github.com/pandas-dev/pandas/issues/40226\n            warnings.filterwarnings(\n                \"ignore\", \"make_block is deprecated\", DeprecationWarning\n            )\n            _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_integer_index_column(self):\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')])\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_index_metadata_field_name(self):\n        # test None case, and strangely named non-index columns\n        df = pd.DataFrame(\n            [(1, 'a', 3.1), (2, 'b', 2.2), (3, 'c', 1.3)],\n            index=pd.MultiIndex.from_arrays(\n                [['c', 'b', 'a'], [3, 2, 1]],\n                names=[None, 'foo']\n            ),\n            columns=['a', None, '__index_level_0__'],\n        )\n        with pytest.warns(UserWarning):\n            t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        col1, col2, col3, idx0, foo = js['columns']\n\n        assert col1['name'] == 'a'\n        assert col1['name'] == col1['field_name']\n\n        assert col2['name'] is None\n        assert col2['field_name'] == 'None'\n\n        assert col3['name'] == '__index_level_0__'\n        assert col3['name'] == col3['field_name']\n\n        idx0_descr, foo_descr = js['index_columns']\n        assert idx0_descr == '__index_level_0__'\n        assert idx0['field_name'] == idx0_descr\n        assert idx0['name'] is None\n\n        assert foo_descr == 'foo'\n        assert foo['field_name'] == foo_descr\n        assert foo['name'] == foo_descr\n\n    def test_categorical_column_index(self):\n        df = pd.DataFrame(\n            [(1, 'a', 2.0), (2, 'b', 3.0), (3, 'c', 4.0)],\n            columns=pd.Index(list('def'), dtype='category')\n        )\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        column_indexes, = js['column_indexes']\n        assert column_indexes['name'] is None\n        assert column_indexes['pandas_type'] == 'categorical'\n        assert column_indexes['numpy_type'] == 'int8'\n\n        md = column_indexes['metadata']\n        assert md['num_categories'] == 3\n        assert md['ordered'] is False\n\n    def test_string_column_index(self):\n        df = pd.DataFrame(\n            [(1, 'a', 2.0), (2, 'b', 3.0), (3, 'c', 4.0)],\n            columns=pd.Index(list('def'), name='stringz')\n        )\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        column_indexes, = js['column_indexes']\n        assert column_indexes['name'] == 'stringz'\n        assert column_indexes['name'] == column_indexes['field_name']\n        assert column_indexes['numpy_type'] == 'object'\n        assert column_indexes['pandas_type'] == 'unicode'\n\n        md = column_indexes['metadata']\n\n        assert len(md) == 1\n        assert md['encoding'] == 'UTF-8'\n\n    def test_datetimetz_column_index(self):\n        df = pd.DataFrame(\n            [(1, 'a', 2.0), (2, 'b', 3.0), (3, 'c', 4.0)],\n            columns=pd.date_range(\n                start='2017-01-01', periods=3, tz='America/New_York'\n            )\n        )\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        column_indexes, = js['column_indexes']\n        assert column_indexes['name'] is None\n        assert column_indexes['pandas_type'] == 'datetimetz'\n        assert column_indexes['numpy_type'] == 'datetime64[ns]'\n\n        md = column_indexes['metadata']\n        assert md['timezone'] == 'America/New_York'\n\n    def test_datetimetz_row_index(self):\n        df = pd.DataFrame({\n            'a': pd.date_range(\n                start='2017-01-01', periods=3, tz='America/New_York'\n            )\n        })\n        df = df.set_index('a')\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_categorical_row_index(self):\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': [1, 2, 3]})\n        df['a'] = df.a.astype('category')\n        df = df.set_index('a')\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_duplicate_column_names_does_not_crash(self):\n        df = pd.DataFrame([(1, 'a'), (2, 'b')], columns=list('aa'))\n        with pytest.raises(ValueError):\n            pa.Table.from_pandas(df)\n\n    def test_dictionary_indices_boundscheck(self):\n        # ARROW-1658. No validation of indices leads to segfaults in pandas\n        indices = [[0, 1], [0, -1]]\n\n        for inds in indices:\n            arr = pa.DictionaryArray.from_arrays(inds, ['a'], safe=False)\n            batch = pa.RecordBatch.from_arrays([arr], ['foo'])\n            table = pa.Table.from_batches([batch, batch, batch])\n\n            with pytest.raises(IndexError):\n                arr.to_pandas()\n\n            with pytest.raises(IndexError):\n                table.to_pandas()\n\n    def test_unicode_with_unicode_column_and_index(self):\n        df = pd.DataFrame({'\u3042': ['\u3044']}, index=['\u3046'])\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_mixed_column_names(self):\n        # mixed type column names are not reconstructed exactly\n        df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n\n        for cols in [['\u3042', b'a'], [1, '2'], [1, 1.5]]:\n            df.columns = pd.Index(cols, dtype=object)\n\n            # assert that the from_pandas raises the warning\n            with pytest.warns(UserWarning):\n                pa.Table.from_pandas(df)\n\n            expected = df.copy()\n            expected.columns = df.columns.values.astype(str)\n            with pytest.warns(UserWarning):\n                _check_pandas_roundtrip(df, expected=expected,\n                                        preserve_index=True)\n\n    def test_binary_column_name(self):\n        if Version(\"2.0.0\") <= Version(pd.__version__) < Version(\"3.0.0\"):\n            # TODO: regression in pandas, hopefully fixed in next version\n            # https://issues.apache.org/jira/browse/ARROW-18394\n            # https://github.com/pandas-dev/pandas/issues/50127\n            pytest.skip(\"Regression in pandas 2.0.0\")\n        column_data = ['\u3044']\n        key = '\u3042'.encode()\n        data = {key: column_data}\n        df = pd.DataFrame(data)\n\n        # we can't use _check_pandas_roundtrip here because our metadata\n        # is always decoded as utf8: even if binary goes in, utf8 comes out\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        df2 = t.to_pandas()\n        assert df.values[0] == df2.values[0]\n        assert df.index.values[0] == df2.index.values[0]\n        assert df.columns[0] == key\n\n    def test_multiindex_duplicate_values(self):\n        num_rows = 3\n        numbers = list(range(num_rows))\n        index = pd.MultiIndex.from_arrays(\n            [['foo', 'foo', 'bar'], numbers],\n            names=['foobar', 'some_numbers'],\n        )\n\n        df = pd.DataFrame({'numbers': numbers}, index=index)\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_metadata_with_mixed_types(self):\n        df = pd.DataFrame({'data': [b'some_bytes', 'some_unicode']})\n        table = pa.Table.from_pandas(df)\n        js = table.schema.pandas_metadata\n        assert 'mixed' not in js\n        data_column = js['columns'][0]\n        assert data_column['pandas_type'] == 'bytes'\n        assert data_column['numpy_type'] == 'object'\n\n    def test_ignore_metadata(self):\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': ['foo', 'bar', 'baz']},\n                          index=['one', 'two', 'three'])\n        table = pa.Table.from_pandas(df)\n\n        result = table.to_pandas(ignore_metadata=True)\n        expected = (table.cast(table.schema.remove_metadata())\n                    .to_pandas())\n\n        tm.assert_frame_equal(result, expected)\n\n    def test_list_metadata(self):\n        df = pd.DataFrame({'data': [[1], [2, 3, 4], [5] * 7]})\n        schema = pa.schema([pa.field('data', type=pa.list_(pa.int64()))])\n        table = pa.Table.from_pandas(df, schema=schema)\n        js = table.schema.pandas_metadata\n        assert 'mixed' not in js\n        data_column = js['columns'][0]\n        assert data_column['pandas_type'] == 'list[int64]'\n        assert data_column['numpy_type'] == 'object'\n\n    def test_struct_metadata(self):\n        df = pd.DataFrame({'dicts': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]})\n        table = pa.Table.from_pandas(df)\n        pandas_metadata = table.schema.pandas_metadata\n        assert pandas_metadata['columns'][0]['pandas_type'] == 'object'\n\n    def test_decimal_metadata(self):\n        expected = pd.DataFrame({\n            'decimals': [\n                decimal.Decimal('394092382910493.12341234678'),\n                -decimal.Decimal('314292388910493.12343437128'),\n            ]\n        })\n        table = pa.Table.from_pandas(expected)\n        js = table.schema.pandas_metadata\n        assert 'mixed' not in js\n        data_column = js['columns'][0]\n        assert data_column['pandas_type'] == 'decimal'\n        assert data_column['numpy_type'] == 'object'\n        assert data_column['metadata'] == {'precision': 26, 'scale': 11}\n\n    def test_table_column_subset_metadata(self):\n        # ARROW-1883\n        # non-default index\n        for index in [\n                pd.Index(['a', 'b', 'c'], name='index'),\n                pd.date_range(\"2017-01-01\", periods=3, tz='Europe/Brussels')]:\n            df = pd.DataFrame({'a': [1, 2, 3],\n                               'b': [.1, .2, .3]}, index=index)\n            table = pa.Table.from_pandas(df)\n\n            table_subset = table.remove_column(1)\n            result = table_subset.to_pandas()\n            expected = df[['a']]\n            if isinstance(df.index, pd.DatetimeIndex):\n                df.index.freq = None\n            tm.assert_frame_equal(result, expected)\n\n            table_subset2 = table_subset.remove_column(1)\n            result = table_subset2.to_pandas()\n            tm.assert_frame_equal(result, df[['a']].reset_index(drop=True))\n\n    def test_to_pandas_column_subset_multiindex(self):\n        # ARROW-10122\n        df = pd.DataFrame(\n            {\"first\": list(range(5)),\n             \"second\": list(range(5)),\n             \"value\": np.arange(5)}\n        )\n        table = pa.Table.from_pandas(df.set_index([\"first\", \"second\"]))\n\n        subset = table.select([\"first\", \"value\"])\n        result = subset.to_pandas()\n        expected = df[[\"first\", \"value\"]].set_index(\"first\")\n        tm.assert_frame_equal(result, expected)\n\n    def test_empty_list_metadata(self):\n        # Create table with array of empty lists, forced to have type\n        # list(string) in pyarrow\n        c1 = [[\"test\"], [\"a\", \"b\"], None]\n        c2 = [[], [], []]\n        arrays = OrderedDict([\n            ('c1', pa.array(c1, type=pa.list_(pa.string()))),\n            ('c2', pa.array(c2, type=pa.list_(pa.string()))),\n        ])\n        rb = pa.RecordBatch.from_arrays(\n            list(arrays.values()),\n            list(arrays.keys())\n        )\n        tbl = pa.Table.from_batches([rb])\n\n        # First roundtrip changes schema, because pandas cannot preserve the\n        # type of empty lists\n        df = tbl.to_pandas()\n        tbl2 = pa.Table.from_pandas(df)\n        md2 = tbl2.schema.pandas_metadata\n\n        # Second roundtrip\n        df2 = tbl2.to_pandas()\n        expected = pd.DataFrame(OrderedDict([('c1', c1), ('c2', c2)]))\n\n        tm.assert_frame_equal(df2, expected)\n\n        assert md2['columns'] == [\n            {\n                'name': 'c1',\n                'field_name': 'c1',\n                'metadata': None,\n                'numpy_type': 'object',\n                'pandas_type': 'list[unicode]',\n            },\n            {\n                'name': 'c2',\n                'field_name': 'c2',\n                'metadata': None,\n                'numpy_type': 'object',\n                'pandas_type': 'list[empty]',\n            }\n        ]\n\n    def test_metadata_pandas_version(self):\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': [1, 2, 3]})\n        table = pa.Table.from_pandas(df)\n        assert table.schema.pandas_metadata['pandas_version'] is not None\n\n    def test_mismatch_metadata_schema(self):\n        # ARROW-10511\n        # It is possible that the metadata and actual schema is not fully\n        # matching (eg no timezone information for tz-aware column)\n        # -> to_pandas() conversion should not fail on that\n        df = pd.DataFrame({\"datetime\": pd.date_range(\"2020-01-01\", periods=3)})\n\n        # OPTION 1: casting after conversion\n        table = pa.Table.from_pandas(df)\n        # cast the \"datetime\" column to be tz-aware\n        new_col = table[\"datetime\"].cast(pa.timestamp('ns', tz=\"UTC\"))\n        new_table1 = table.set_column(\n            0, pa.field(\"datetime\", new_col.type), new_col\n        )\n\n        # OPTION 2: specify schema during conversion\n        schema = pa.schema([(\"datetime\", pa.timestamp('ns', tz=\"UTC\"))])\n        new_table2 = pa.Table.from_pandas(df, schema=schema)\n\n        expected = df.copy()\n        expected[\"datetime\"] = expected[\"datetime\"].dt.tz_localize(\"UTC\")\n\n        for new_table in [new_table1, new_table2]:\n            # ensure the new table still has the pandas metadata\n            assert new_table.schema.pandas_metadata is not None\n            # convert to pandas\n            result = new_table.to_pandas()\n            tm.assert_frame_equal(result, expected)\n\n\nclass TestConvertPrimitiveTypes:\n    \"\"\"\n    Conversion tests for primitive (e.g. numeric) types.\n    \"\"\"\n\n    def test_float_no_nulls(self):\n        data = {}\n        fields = []\n        dtypes = [('f2', pa.float16()),\n                  ('f4', pa.float32()),\n                  ('f8', pa.float64())]\n        num_values = 100\n\n        for numpy_dtype, arrow_dtype in dtypes:\n            values = np.random.randn(num_values)\n            data[numpy_dtype] = values.astype(numpy_dtype)\n            fields.append(pa.field(numpy_dtype, arrow_dtype))\n\n        df = pd.DataFrame(data)\n        schema = pa.schema(fields)\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_float_nulls(self):\n        num_values = 100\n\n        null_mask = np.random.randint(0, 10, size=num_values) < 3\n        dtypes = [('f2', pa.float16()),\n                  ('f4', pa.float32()),\n                  ('f8', pa.float64())]\n        names = ['f2', 'f4', 'f8']\n        expected_cols = []\n\n        arrays = []\n        fields = []\n        for name, arrow_dtype in dtypes:\n            values = np.random.randn(num_values).astype(name)\n\n            arr = pa.array(values, from_pandas=True, mask=null_mask)\n            arrays.append(arr)\n            fields.append(pa.field(name, arrow_dtype))\n            values[null_mask] = np.nan\n\n            expected_cols.append(values)\n\n        ex_frame = pd.DataFrame(dict(zip(names, expected_cols)),\n                                columns=names)\n\n        table = pa.Table.from_arrays(arrays, names)\n        assert table.schema.equals(pa.schema(fields))\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, ex_frame)\n\n    def test_float_nulls_to_ints(self):\n        # ARROW-2135\n        df = pd.DataFrame({\"a\": [1.0, 2.0, np.nan]})\n        schema = pa.schema([pa.field(\"a\", pa.int16(), nullable=True)])\n        table = pa.Table.from_pandas(df, schema=schema, safe=False)\n        assert table[0].to_pylist() == [1, 2, None]\n        tm.assert_frame_equal(df, table.to_pandas())\n\n    def test_float_nulls_to_boolean(self):\n        s = pd.Series([0.0, 1.0, 2.0, None, -3.0])\n        expected = pd.Series([False, True, True, None, True])\n        _check_array_roundtrip(s, expected=expected, type=pa.bool_())\n\n    def test_series_from_pandas_false_respected(self):\n        # Check that explicit from_pandas=False is respected\n        s = pd.Series([0.0, np.nan])\n        arr = pa.array(s, from_pandas=False)\n        assert arr.null_count == 0\n        assert np.isnan(arr[1].as_py())\n\n    def test_integer_no_nulls(self):\n        data = OrderedDict()\n        fields = []\n\n        numpy_dtypes = [\n            ('i1', pa.int8()), ('i2', pa.int16()),\n            ('i4', pa.int32()), ('i8', pa.int64()),\n            ('u1', pa.uint8()), ('u2', pa.uint16()),\n            ('u4', pa.uint32()), ('u8', pa.uint64()),\n            ('longlong', pa.int64()), ('ulonglong', pa.uint64())\n        ]\n        num_values = 100\n\n        for dtype, arrow_dtype in numpy_dtypes:\n            info = np.iinfo(dtype)\n            values = np.random.randint(max(info.min, np.iinfo(np.int_).min),\n                                       min(info.max, np.iinfo(np.int_).max),\n                                       size=num_values, dtype=dtype)\n            data[dtype] = values.astype(dtype)\n            fields.append(pa.field(dtype, arrow_dtype))\n\n        df = pd.DataFrame(data)\n        schema = pa.schema(fields)\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_all_integer_types(self):\n        # Test all Numpy integer aliases\n        data = OrderedDict()\n        numpy_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8',\n                        'byte', 'ubyte', 'short', 'ushort', 'intc', 'uintc',\n                        'int_', 'uint', 'longlong', 'ulonglong']\n        for dtype in numpy_dtypes:\n            data[dtype] = np.arange(12, dtype=dtype)\n        df = pd.DataFrame(data)\n        _check_pandas_roundtrip(df)\n\n        # Do the same with pa.array()\n        # (for some reason, it doesn't use the same code paths at all)\n        for np_arr in data.values():\n            arr = pa.array(np_arr)\n            assert arr.to_pylist() == np_arr.tolist()\n\n    def test_integer_byteorder(self):\n        # Byteswapped arrays are not supported yet\n        int_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8']\n        for dt in int_dtypes:\n            for order in '=<>':\n                data = np.array([1, 2, 42], dtype=order + dt)\n                for np_arr in (data, data[::2]):\n                    if data.dtype.isnative:\n                        arr = pa.array(data)\n                        assert arr.to_pylist() == data.tolist()\n                    else:\n                        with pytest.raises(NotImplementedError):\n                            arr = pa.array(data)\n\n    def test_integer_with_nulls(self):\n        # pandas requires upcast to float dtype\n\n        int_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8']\n        num_values = 100\n\n        null_mask = np.random.randint(0, 10, size=num_values) < 3\n\n        expected_cols = []\n        arrays = []\n        for name in int_dtypes:\n            values = np.random.randint(0, 100, size=num_values)\n\n            arr = pa.array(values, mask=null_mask)\n            arrays.append(arr)\n\n            expected = values.astype('f8')\n            expected[null_mask] = np.nan\n\n            expected_cols.append(expected)\n\n        ex_frame = pd.DataFrame(dict(zip(int_dtypes, expected_cols)),\n                                columns=int_dtypes)\n\n        table = pa.Table.from_arrays(arrays, int_dtypes)\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, ex_frame)\n\n    def test_array_from_pandas_type_cast(self):\n        arr = np.arange(10, dtype='int64')\n\n        target_type = pa.int8()\n\n        result = pa.array(arr, type=target_type)\n        expected = pa.array(arr.astype('int8'))\n        assert result.equals(expected)\n\n    def test_boolean_no_nulls(self):\n        num_values = 100\n\n        np.random.seed(0)\n\n        df = pd.DataFrame({'bools': np.random.randn(num_values) > 0})\n        field = pa.field('bools', pa.bool_())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_boolean_nulls(self):\n        # pandas requires upcast to object dtype\n        num_values = 100\n        np.random.seed(0)\n\n        mask = np.random.randint(0, 10, size=num_values) < 3\n        values = np.random.randint(0, 10, size=num_values) < 5\n\n        arr = pa.array(values, mask=mask)\n\n        expected = values.astype(object)\n        expected[mask] = None\n\n        field = pa.field('bools', pa.bool_())\n        schema = pa.schema([field])\n        ex_frame = pd.DataFrame({'bools': expected})\n\n        table = pa.Table.from_arrays([arr], ['bools'])\n        assert table.schema.equals(schema)\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, ex_frame)\n\n    def test_boolean_to_int(self):\n        # test from dtype=bool\n        s = pd.Series([True, True, False, True, True] * 2)\n        expected = pd.Series([1, 1, 0, 1, 1] * 2)\n        _check_array_roundtrip(s, expected=expected, type=pa.int64())\n\n    def test_boolean_objects_to_int(self):\n        # test from dtype=object\n        s = pd.Series([True, True, False, True, True] * 2, dtype=object)\n        expected = pd.Series([1, 1, 0, 1, 1] * 2)\n        expected_msg = 'Expected integer, got bool'\n        with pytest.raises(pa.ArrowTypeError, match=expected_msg):\n            _check_array_roundtrip(s, expected=expected, type=pa.int64())\n\n    def test_boolean_nulls_to_float(self):\n        # test from dtype=object\n        s = pd.Series([True, True, False, None, True] * 2)\n        expected = pd.Series([1.0, 1.0, 0.0, None, 1.0] * 2)\n        _check_array_roundtrip(s, expected=expected, type=pa.float64())\n\n    def test_boolean_multiple_columns(self):\n        # ARROW-6325 (multiple columns resulting in strided conversion)\n        df = pd.DataFrame(np.ones((3, 2), dtype='bool'), columns=['a', 'b'])\n        _check_pandas_roundtrip(df)\n\n    def test_float_object_nulls(self):\n        arr = np.array([None, 1.5, np.float64(3.5)] * 5, dtype=object)\n        df = pd.DataFrame({'floats': arr})\n        expected = pd.DataFrame({'floats': pd.to_numeric(arr)})\n        field = pa.field('floats', pa.float64())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected=expected,\n                                expected_schema=schema)\n\n    def test_float_with_null_as_integer(self):\n        # ARROW-2298\n        s = pd.Series([np.nan, 1., 2., np.nan])\n\n        types = [pa.int8(), pa.int16(), pa.int32(), pa.int64(),\n                 pa.uint8(), pa.uint16(), pa.uint32(), pa.uint64()]\n        for ty in types:\n            result = pa.array(s, type=ty)\n            expected = pa.array([None, 1, 2, None], type=ty)\n            assert result.equals(expected)\n\n            df = pd.DataFrame({'has_nulls': s})\n            schema = pa.schema([pa.field('has_nulls', ty)])\n            result = pa.Table.from_pandas(df, schema=schema,\n                                          preserve_index=False)\n            assert result[0].chunk(0).equals(expected)\n\n    def test_int_object_nulls(self):\n        arr = np.array([None, 1, np.int64(3)] * 5, dtype=object)\n        df = pd.DataFrame({'ints': arr})\n        expected = pd.DataFrame({'ints': pd.to_numeric(arr)})\n        field = pa.field('ints', pa.int64())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected=expected,\n                                expected_schema=schema)\n\n    def test_boolean_object_nulls(self):\n        arr = np.array([False, None, True] * 100, dtype=object)\n        df = pd.DataFrame({'bools': arr})\n        field = pa.field('bools', pa.bool_())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_all_nulls_cast_numeric(self):\n        arr = np.array([None], dtype=object)\n\n        def _check_type(t):\n            a2 = pa.array(arr, type=t)\n            assert a2.type == t\n            assert a2[0].as_py() is None\n\n        _check_type(pa.int32())\n        _check_type(pa.float64())\n\n    def test_half_floats_from_numpy(self):\n        arr = np.array([1.5, np.nan], dtype=np.float16)\n        a = pa.array(arr, type=pa.float16())\n        x, y = a.to_pylist()\n        assert isinstance(x, np.float16)\n        assert x == 1.5\n        assert isinstance(y, np.float16)\n        assert np.isnan(y)\n\n        a = pa.array(arr, type=pa.float16(), from_pandas=True)\n        x, y = a.to_pylist()\n        assert isinstance(x, np.float16)\n        assert x == 1.5\n        assert y is None\n\n\n@pytest.mark.parametrize('dtype',\n                         ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8'])\ndef test_array_integer_object_nulls_option(dtype):\n    num_values = 100\n\n    null_mask = np.random.randint(0, 10, size=num_values) < 3\n    values = np.random.randint(0, 100, size=num_values, dtype=dtype)\n\n    array = pa.array(values, mask=null_mask)\n\n    if null_mask.any():\n        expected = values.astype('O')\n        expected[null_mask] = None\n    else:\n        expected = values\n\n    result = array.to_pandas(integer_object_nulls=True)\n\n    np.testing.assert_equal(result, expected)\n\n\n@pytest.mark.parametrize('dtype',\n                         ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8'])\ndef test_table_integer_object_nulls_option(dtype):\n    num_values = 100\n\n    null_mask = np.random.randint(0, 10, size=num_values) < 3\n    values = np.random.randint(0, 100, size=num_values, dtype=dtype)\n\n    array = pa.array(values, mask=null_mask)\n\n    if null_mask.any():\n        expected = values.astype('O')\n        expected[null_mask] = None\n    else:\n        expected = values\n\n    expected = pd.DataFrame({dtype: expected})\n\n    table = pa.Table.from_arrays([array], [dtype])\n    result = table.to_pandas(integer_object_nulls=True)\n\n    tm.assert_frame_equal(result, expected)\n\n\nclass TestConvertDateTimeLikeTypes:\n    \"\"\"\n    Conversion tests for datetime- and timestamp-like types (date64, etc.).\n    \"\"\"\n\n    def test_timestamps_notimezone_no_nulls(self):\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123456789',\n                '2006-01-13T12:34:56.432539784',\n                '2010-08-13T05:46:57.437699912'],\n                dtype='datetime64[ns]')\n        })\n        field = pa.field('datetime64', pa.timestamp('ns'))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    def test_timestamps_notimezone_nulls(self):\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123456789',\n                None,\n                '2010-08-13T05:46:57.437699912'],\n                dtype='datetime64[ns]')\n        })\n        field = pa.field('datetime64', pa.timestamp('ns'))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    @pytest.mark.parametrize('unit', ['s', 'ms', 'us', 'ns'])\n    def test_timestamps_with_timezone(self, unit):\n        if Version(pd.__version__) < Version(\"2.0.0\") and unit != 'ns':\n            pytest.skip(\"pandas < 2.0 only supports nanosecond datetime64\")\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123',\n                '2006-01-13T12:34:56.432',\n                '2010-08-13T05:46:57.437'],\n                dtype=f'datetime64[{unit}]')\n        })\n        df['datetime64'] = df['datetime64'].dt.tz_localize('US/Eastern')\n        _check_pandas_roundtrip(df)\n\n        _check_series_roundtrip(df['datetime64'])\n\n        # drop-in a null\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123456789',\n                None,\n                '2006-01-13T12:34:56.432539784',\n                '2010-08-13T05:46:57.437699912'],\n                dtype=f'datetime64[{unit}]')\n        })\n        df['datetime64'] = df['datetime64'].dt.tz_localize('US/Eastern')\n\n        _check_pandas_roundtrip(df)\n\n    def test_python_datetime(self):\n        # ARROW-2106\n        date_array = [datetime.today() + timedelta(days=x) for x in range(10)]\n        df = pd.DataFrame({\n            'datetime': pd.Series(date_array, dtype=object)\n        })\n\n        table = pa.Table.from_pandas(df)\n        assert isinstance(table[0].chunk(0), pa.TimestampArray)\n\n        result = table.to_pandas()\n        # Pandas v2 defaults to [ns], but Arrow defaults to [us] time units\n        # so we need to cast the pandas dtype. Pandas v1 will always silently\n        # coerce to [ns] due to lack of non-[ns] support.\n        expected_df = pd.DataFrame({\n            'datetime': pd.Series(date_array, dtype='datetime64[us]')\n        })\n        tm.assert_frame_equal(expected_df, result)\n\n    def test_python_datetime_with_pytz_tzinfo(self):\n        pytz = pytest.importorskip(\"pytz\")\n\n        for tz in [pytz.utc, pytz.timezone('US/Eastern'), pytz.FixedOffset(1)]:\n            values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz)]\n            df = pd.DataFrame({'datetime': values})\n            _check_pandas_roundtrip(df)\n\n    @h.given(st.none() | past.timezones)\n    @h.settings(deadline=None)\n    def test_python_datetime_with_pytz_timezone(self, tz):\n        if str(tz) in [\"build/etc/localtime\", \"Factory\"]:\n            pytest.skip(\"Localtime timezone not supported\")\n        values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz)]\n        df = pd.DataFrame({'datetime': values})\n        _check_pandas_roundtrip(df, check_dtype=False)\n\n    def test_python_datetime_with_timezone_tzinfo(self):\n        pytz = pytest.importorskip(\"pytz\")\n        from datetime import timezone\n\n        values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=timezone.utc)]\n        # also test with index to ensure both paths roundtrip (ARROW-9962)\n        df = pd.DataFrame({'datetime': values}, index=values)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n        # datetime.timezone is going to be pytz.FixedOffset\n        hours = 1\n        tz_timezone = timezone(timedelta(hours=hours))\n        tz_pytz = pytz.FixedOffset(hours * 60)\n        values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz_timezone)]\n        values_exp = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz_pytz)]\n        df = pd.DataFrame({'datetime': values}, index=values)\n        df_exp = pd.DataFrame({'datetime': values_exp}, index=values_exp)\n        _check_pandas_roundtrip(df, expected=df_exp, preserve_index=True)\n\n    def test_python_datetime_subclass(self):\n\n        class MyDatetime(datetime):\n            # see https://github.com/pandas-dev/pandas/issues/21142\n            nanosecond = 0.0\n\n        date_array = [MyDatetime(2000, 1, 1, 1, 1, 1)]\n        df = pd.DataFrame({\"datetime\": pd.Series(date_array, dtype=object)})\n\n        table = pa.Table.from_pandas(df)\n        assert isinstance(table[0].chunk(0), pa.TimestampArray)\n\n        result = table.to_pandas()\n\n        # Pandas v2 defaults to [ns], but Arrow defaults to [us] time units\n        # so we need to cast the pandas dtype. Pandas v1 will always silently\n        # coerce to [ns] due to lack of non-[ns] support.\n        expected_df = pd.DataFrame(\n            {\"datetime\": pd.Series(date_array, dtype='datetime64[us]')})\n\n        # https://github.com/pandas-dev/pandas/issues/21142\n        expected_df[\"datetime\"] = pd.to_datetime(expected_df[\"datetime\"])\n\n        tm.assert_frame_equal(expected_df, result)\n\n    def test_python_date_subclass(self):\n\n        class MyDate(date):\n            pass\n\n        date_array = [MyDate(2000, 1, 1)]\n        df = pd.DataFrame({\"date\": pd.Series(date_array, dtype=object)})\n\n        table = pa.Table.from_pandas(df)\n        assert isinstance(table[0].chunk(0), pa.Date32Array)\n\n        result = table.to_pandas()\n        expected_df = pd.DataFrame(\n            {\"date\": np.array([date(2000, 1, 1)], dtype=object)}\n        )\n        tm.assert_frame_equal(expected_df, result)\n\n    def test_datetime64_to_date32(self):\n        # ARROW-1718\n        arr = pa.array([date(2017, 10, 23), None])\n        c = pa.chunked_array([arr])\n        s = c.to_pandas()\n\n        arr2 = pa.Array.from_pandas(s, type=pa.date32())\n\n        assert arr2.equals(arr.cast('date32'))\n\n    @pytest.mark.parametrize('mask', [\n        None,\n        np.array([True, False, False, True, False, False]),\n    ])\n    def test_pandas_datetime_to_date64(self, mask):\n        s = pd.to_datetime([\n            '2018-05-10T00:00:00',\n            '2018-05-11T00:00:00',\n            '2018-05-12T00:00:00',\n            '2018-05-10T10:24:01',\n            '2018-05-11T10:24:01',\n            '2018-05-12T10:24:01',\n        ])\n        arr = pa.Array.from_pandas(s, type=pa.date64(), mask=mask)\n\n        data = np.array([\n            date(2018, 5, 10),\n            date(2018, 5, 11),\n            date(2018, 5, 12),\n            date(2018, 5, 10),\n            date(2018, 5, 11),\n            date(2018, 5, 12),\n        ])\n        expected = pa.array(data, mask=mask, type=pa.date64())\n\n        assert arr.equals(expected)\n\n    @pytest.mark.parametrize(\"coerce_to_ns,expected_dtype\",\n                             [(False, 'datetime64[ms]'),\n                              (True, 'datetime64[ns]')])\n    def test_array_types_date_as_object(self, coerce_to_ns, expected_dtype):\n        data = [date(2000, 1, 1),\n                None,\n                date(1970, 1, 1),\n                date(2040, 2, 26)]\n        expected_days = np.array(['2000-01-01', None, '1970-01-01',\n                                  '2040-02-26'], dtype='datetime64[D]')\n\n        if Version(pd.__version__) < Version(\"2.0.0\"):\n            # ARROW-3789: Coerce date/timestamp types to datetime64[ns]\n            expected_dtype = 'datetime64[ns]'\n\n        expected = np.array(['2000-01-01', None, '1970-01-01',\n                             '2040-02-26'], dtype=expected_dtype)\n\n        objects = [pa.array(data),\n                   pa.chunked_array([data])]\n\n        for obj in objects:\n            result = obj.to_pandas(coerce_temporal_nanoseconds=coerce_to_ns)\n            expected_obj = expected_days.astype(object)\n            assert result.dtype == expected_obj.dtype\n            npt.assert_array_equal(result, expected_obj)\n\n            result = obj.to_pandas(date_as_object=False,\n                                   coerce_temporal_nanoseconds=coerce_to_ns)\n            assert result.dtype == expected.dtype\n            npt.assert_array_equal(result, expected)\n\n    @pytest.mark.parametrize(\"coerce_to_ns,expected_type\",\n                             [(False, 'datetime64[ms]'),\n                              (True, 'datetime64[ns]')])\n    def test_table_convert_date_as_object(self, coerce_to_ns, expected_type):\n        df = pd.DataFrame({\n            'date': [date(2000, 1, 1),\n                     None,\n                     date(1970, 1, 1),\n                     date(2040, 2, 26)]})\n\n        table = pa.Table.from_pandas(df, preserve_index=False)\n\n        df_datetime = table.to_pandas(date_as_object=False,\n                                      coerce_temporal_nanoseconds=coerce_to_ns)\n        df_object = table.to_pandas()\n\n        tm.assert_frame_equal(df.astype(expected_type), df_datetime,\n                              check_dtype=True)\n        tm.assert_frame_equal(df, df_object, check_dtype=True)\n\n    @pytest.mark.parametrize(\"arrow_type\",\n                             [pa.date32(), pa.date64(), pa.timestamp('s'),\n                              pa.timestamp('ms'), pa.timestamp('us'),\n                              pa.timestamp('ns'), pa.timestamp('s', 'UTC'),\n                              pa.timestamp('ms', 'UTC'), pa.timestamp('us', 'UTC'),\n                              pa.timestamp('ns', 'UTC')])\n    def test_array_coerce_temporal_nanoseconds(self, arrow_type):\n        data = [date(2000, 1, 1), datetime(2001, 1, 1)]\n        expected = pd.Series(data)\n        arr = pa.array(data).cast(arrow_type)\n        result = arr.to_pandas(\n            coerce_temporal_nanoseconds=True, date_as_object=False)\n        expected_tz = None\n        if hasattr(arrow_type, 'tz') and arrow_type.tz is not None:\n            expected_tz = 'UTC'\n        expected_type = pa.timestamp('ns', expected_tz).to_pandas_dtype()\n        tm.assert_series_equal(result, expected.astype(expected_type))\n\n    @pytest.mark.parametrize(\"arrow_type\",\n                             [pa.date32(), pa.date64(), pa.timestamp('s'),\n                              pa.timestamp('ms'), pa.timestamp('us'),\n                              pa.timestamp('ns'), pa.timestamp('s', 'UTC'),\n                              pa.timestamp('ms', 'UTC'), pa.timestamp('us', 'UTC'),\n                              pa.timestamp('ns', 'UTC')])\n    def test_table_coerce_temporal_nanoseconds(self, arrow_type):\n        data = [date(2000, 1, 1), datetime(2001, 1, 1)]\n        schema = pa.schema([pa.field('date', arrow_type)])\n        expected_df = pd.DataFrame({'date': data})\n        table = pa.table([pa.array(data)], schema=schema)\n        result_df = table.to_pandas(\n            coerce_temporal_nanoseconds=True, date_as_object=False)\n        expected_tz = None\n        if hasattr(arrow_type, 'tz') and arrow_type.tz is not None:\n            expected_tz = 'UTC'\n        expected_type = pa.timestamp('ns', expected_tz).to_pandas_dtype()\n        tm.assert_frame_equal(result_df, expected_df.astype(expected_type))\n\n    def test_date_infer(self):\n        df = pd.DataFrame({\n            'date': [date(2000, 1, 1),\n                     None,\n                     date(1970, 1, 1),\n                     date(2040, 2, 26)]})\n        table = pa.Table.from_pandas(df, preserve_index=False)\n        field = pa.field('date', pa.date32())\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = pa.schema([field], metadata=table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, df)\n\n    def test_date_mask(self):\n        arr = np.array([date(2017, 4, 3), date(2017, 4, 4)],\n                       dtype='datetime64[D]')\n        mask = [True, False]\n        result = pa.array(arr, mask=np.array(mask))\n        expected = np.array([None, date(2017, 4, 4)], dtype='datetime64[D]')\n        expected = pa.array(expected, from_pandas=True)\n        assert expected.equals(result)\n\n    def test_date_objects_typed(self):\n        arr = np.array([\n            date(2017, 4, 3),\n            None,\n            date(2017, 4, 4),\n            date(2017, 4, 5)], dtype=object)\n\n        arr_i4 = np.array([17259, -1, 17260, 17261], dtype='int32')\n        arr_i8 = arr_i4.astype('int64') * 86400000\n        mask = np.array([False, True, False, False])\n\n        t32 = pa.date32()\n        t64 = pa.date64()\n\n        a32 = pa.array(arr, type=t32)\n        a64 = pa.array(arr, type=t64)\n\n        a32_expected = pa.array(arr_i4, mask=mask, type=t32)\n        a64_expected = pa.array(arr_i8, mask=mask, type=t64)\n\n        assert a32.equals(a32_expected)\n        assert a64.equals(a64_expected)\n\n        # Test converting back to pandas\n        colnames = ['date32', 'date64']\n        table = pa.Table.from_arrays([a32, a64], colnames)\n\n        ex_values = (np.array(['2017-04-03', '2017-04-04', '2017-04-04',\n                               '2017-04-05'],\n                              dtype='datetime64[D]'))\n        ex_values[1] = pd.NaT.value\n\n        # date32 and date64 convert to [ms] in pandas v2, but\n        # in pandas v1 they are silently coerced to [ns]\n        ex_datetime64ms = ex_values.astype('datetime64[ms]')\n        expected_pandas = pd.DataFrame({'date32': ex_datetime64ms,\n                                        'date64': ex_datetime64ms},\n                                       columns=colnames)\n        table_pandas = table.to_pandas(date_as_object=False)\n        tm.assert_frame_equal(table_pandas, expected_pandas)\n\n        table_pandas_objects = table.to_pandas()\n        ex_objects = ex_values.astype('object')\n        expected_pandas_objects = pd.DataFrame({'date32': ex_objects,\n                                                'date64': ex_objects},\n                                               columns=colnames)\n        tm.assert_frame_equal(table_pandas_objects,\n                              expected_pandas_objects)\n\n    def test_pandas_null_values(self):\n        # ARROW-842\n        pd_NA = getattr(pd, 'NA', None)\n        values = np.array([datetime(2000, 1, 1), pd.NaT, pd_NA], dtype=object)\n        values_with_none = np.array([datetime(2000, 1, 1), None, None],\n                                    dtype=object)\n        result = pa.array(values, from_pandas=True)\n        expected = pa.array(values_with_none, from_pandas=True)\n        assert result.equals(expected)\n        assert result.null_count == 2\n\n        # ARROW-9407\n        assert pa.array([pd.NaT], from_pandas=True).type == pa.null()\n        assert pa.array([pd_NA], from_pandas=True).type == pa.null()\n\n    def test_dates_from_integers(self):\n        t1 = pa.date32()\n        t2 = pa.date64()\n\n        arr = np.array([17259, 17260, 17261], dtype='int32')\n        arr2 = arr.astype('int64') * 86400000\n\n        a1 = pa.array(arr, type=t1)\n        a2 = pa.array(arr2, type=t2)\n\n        expected = date(2017, 4, 3)\n        assert a1[0].as_py() == expected\n        assert a2[0].as_py() == expected\n\n    def test_pytime_from_pandas(self):\n        pytimes = [time(1, 2, 3, 1356),\n                   time(4, 5, 6, 1356)]\n\n        # microseconds\n        t1 = pa.time64('us')\n\n        aobjs = np.array(pytimes + [None], dtype=object)\n        parr = pa.array(aobjs)\n        assert parr.type == t1\n        assert parr[0].as_py() == pytimes[0]\n        assert parr[1].as_py() == pytimes[1]\n        assert parr[2].as_py() is None\n\n        # DataFrame\n        df = pd.DataFrame({'times': aobjs})\n        batch = pa.RecordBatch.from_pandas(df)\n        assert batch[0].equals(parr)\n\n        # Test ndarray of int64 values\n        arr = np.array([_pytime_to_micros(v) for v in pytimes],\n                       dtype='int64')\n\n        a1 = pa.array(arr, type=pa.time64('us'))\n        assert a1[0].as_py() == pytimes[0]\n\n        a2 = pa.array(arr * 1000, type=pa.time64('ns'))\n        assert a2[0].as_py() == pytimes[0]\n\n        a3 = pa.array((arr / 1000).astype('i4'),\n                      type=pa.time32('ms'))\n        assert a3[0].as_py() == pytimes[0].replace(microsecond=1000)\n\n        a4 = pa.array((arr / 1000000).astype('i4'),\n                      type=pa.time32('s'))\n        assert a4[0].as_py() == pytimes[0].replace(microsecond=0)\n\n    def test_arrow_time_to_pandas(self):\n        pytimes = [time(1, 2, 3, 1356),\n                   time(4, 5, 6, 1356),\n                   time(0, 0, 0)]\n\n        expected = np.array(pytimes[:2] + [None])\n        expected_ms = np.array([x.replace(microsecond=1000)\n                                for x in pytimes[:2]] +\n                               [None])\n        expected_s = np.array([x.replace(microsecond=0)\n                               for x in pytimes[:2]] +\n                              [None])\n\n        arr = np.array([_pytime_to_micros(v) for v in pytimes],\n                       dtype='int64')\n        arr = np.array([_pytime_to_micros(v) for v in pytimes],\n                       dtype='int64')\n\n        null_mask = np.array([False, False, True], dtype=bool)\n\n        a1 = pa.array(arr, mask=null_mask, type=pa.time64('us'))\n        a2 = pa.array(arr * 1000, mask=null_mask,\n                      type=pa.time64('ns'))\n\n        a3 = pa.array((arr / 1000).astype('i4'), mask=null_mask,\n                      type=pa.time32('ms'))\n        a4 = pa.array((arr / 1000000).astype('i4'), mask=null_mask,\n                      type=pa.time32('s'))\n\n        names = ['time64[us]', 'time64[ns]', 'time32[ms]', 'time32[s]']\n        batch = pa.RecordBatch.from_arrays([a1, a2, a3, a4], names)\n\n        for arr, expected_values in [(a1, expected),\n                                     (a2, expected),\n                                     (a3, expected_ms),\n                                     (a4, expected_s)]:\n            result_pandas = arr.to_pandas()\n            assert (result_pandas.values == expected_values).all()\n\n        df = batch.to_pandas()\n        expected_df = pd.DataFrame({'time64[us]': expected,\n                                    'time64[ns]': expected,\n                                    'time32[ms]': expected_ms,\n                                    'time32[s]': expected_s},\n                                   columns=names)\n\n        tm.assert_frame_equal(df, expected_df)\n\n    def test_numpy_datetime64_columns(self):\n        datetime64_ns = np.array([\n            '2007-07-13T01:23:34.123456789',\n            None,\n            '2006-01-13T12:34:56.432539784',\n            '2010-08-13T05:46:57.437699912'],\n            dtype='datetime64[ns]')\n        _check_array_from_pandas_roundtrip(datetime64_ns)\n\n        datetime64_us = np.array([\n            '2007-07-13T01:23:34.123456',\n            None,\n            '2006-01-13T12:34:56.432539',\n            '2010-08-13T05:46:57.437699'],\n            dtype='datetime64[us]')\n        _check_array_from_pandas_roundtrip(datetime64_us)\n\n        datetime64_ms = np.array([\n            '2007-07-13T01:23:34.123',\n            None,\n            '2006-01-13T12:34:56.432',\n            '2010-08-13T05:46:57.437'],\n            dtype='datetime64[ms]')\n        _check_array_from_pandas_roundtrip(datetime64_ms)\n\n        datetime64_s = np.array([\n            '2007-07-13T01:23:34',\n            None,\n            '2006-01-13T12:34:56',\n            '2010-08-13T05:46:57'],\n            dtype='datetime64[s]')\n        _check_array_from_pandas_roundtrip(datetime64_s)\n\n    def test_timestamp_to_pandas_coerces_to_ns(self):\n        # non-ns timestamp gets cast to ns on conversion to pandas\n        if Version(pd.__version__) >= Version(\"2.0.0\"):\n            pytest.skip(\"pandas >= 2.0 supports non-nanosecond datetime64\")\n\n        arr = pa.array([1, 2, 3], pa.timestamp('ms'))\n        expected = pd.Series(pd.to_datetime([1, 2, 3], unit='ms'))\n        s = arr.to_pandas()\n        tm.assert_series_equal(s, expected)\n        arr = pa.chunked_array([arr])\n        s = arr.to_pandas()\n        tm.assert_series_equal(s, expected)\n\n    def test_timestamp_to_pandas_out_of_bounds(self):\n        # ARROW-7758 check for out of bounds timestamps for non-ns timestamps\n        # that end up getting coerced into ns timestamps.\n\n        for unit in ['s', 'ms', 'us']:\n            for tz in [None, 'America/New_York']:\n                arr = pa.array([datetime(1, 1, 1)], pa.timestamp(unit, tz=tz))\n                table = pa.table({'a': arr})\n\n                msg = \"would result in out of bounds timestamp\"\n                with pytest.raises(ValueError, match=msg):\n                    arr.to_pandas(coerce_temporal_nanoseconds=True)\n\n                with pytest.raises(ValueError, match=msg):\n                    table.to_pandas(coerce_temporal_nanoseconds=True)\n\n                with pytest.raises(ValueError, match=msg):\n                    # chunked array\n                    table.column('a').to_pandas(coerce_temporal_nanoseconds=True)\n\n                # just ensure those don't give an error, but do not\n                # check actual garbage output\n                arr.to_pandas(safe=False, coerce_temporal_nanoseconds=True)\n                table.to_pandas(safe=False, coerce_temporal_nanoseconds=True)\n                table.column('a').to_pandas(\n                    safe=False, coerce_temporal_nanoseconds=True)\n\n    def test_timestamp_to_pandas_empty_chunked(self):\n        # ARROW-7907 table with chunked array with 0 chunks\n        table = pa.table({'a': pa.chunked_array([], type=pa.timestamp('us'))})\n        result = table.to_pandas()\n        expected = pd.DataFrame({'a': pd.Series([], dtype=\"datetime64[us]\")})\n        tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize('dtype', [pa.date32(), pa.date64()])\n    def test_numpy_datetime64_day_unit(self, dtype):\n        datetime64_d = np.array([\n            '2007-07-13',\n            None,\n            '2006-01-15',\n            '2010-08-19'],\n            dtype='datetime64[D]')\n        _check_array_from_pandas_roundtrip(datetime64_d, type=dtype)\n\n    def test_array_from_pandas_date_with_mask(self):\n        m = np.array([True, False, True])\n        data = pd.Series([\n            date(1990, 1, 1),\n            date(1991, 1, 1),\n            date(1992, 1, 1)\n        ])\n\n        result = pa.Array.from_pandas(data, mask=m)\n\n        expected = pd.Series([None, date(1991, 1, 1), None])\n        assert pa.Array.from_pandas(expected).equals(result)\n\n    @pytest.mark.skipif(\n        Version('1.16.0') <= Version(np.__version__) < Version('1.16.1'),\n        reason='Until numpy/numpy#12745 is resolved')\n    def test_fixed_offset_timezone(self):\n        df = pd.DataFrame({\n            'a': [\n                pd.Timestamp('2012-11-11 00:00:00+01:00'),\n                pd.NaT\n            ]\n        })\n        # 'check_dtype=False' because pandas >= 2 uses datetime.timezone\n        # instead of pytz.FixedOffset, and thus the dtype is not exactly\n        # identical (pyarrow still defaults to pytz)\n        # TODO remove if https://github.com/apache/arrow/issues/15047 is fixed\n        _check_pandas_roundtrip(df, check_dtype=False)\n\n    @pytest.mark.parametrize(\"unit\", ['s', 'ms', 'us', 'ns'])\n    def test_timedeltas_no_nulls(self, unit):\n        if Version(pd.__version__) < Version(\"2.0.0\"):\n            unit = 'ns'\n        df = pd.DataFrame({\n            'timedelta64': np.array([0, 3600000000000, 7200000000000],\n                                    dtype=f'timedelta64[{unit}]')\n        })\n        field = pa.field('timedelta64', pa.duration(unit))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    @pytest.mark.parametrize(\"unit\", ['s', 'ms', 'us', 'ns'])\n    def test_timedeltas_nulls(self, unit):\n        if Version(pd.__version__) < Version(\"2.0.0\"):\n            unit = 'ns'\n        df = pd.DataFrame({\n            'timedelta64': np.array([0, None, 7200000000000],\n                                    dtype=f'timedelta64[{unit}]')\n        })\n        field = pa.field('timedelta64', pa.duration(unit))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    def test_month_day_nano_interval(self):\n        from pandas.tseries.offsets import DateOffset\n        df = pd.DataFrame({\n            'date_offset': [None,\n                            DateOffset(days=3600, months=3600, microseconds=3,\n                                       nanoseconds=600)]\n        })\n        schema = pa.schema([('date_offset', pa.month_day_nano_interval())])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema)\n\n\n# ----------------------------------------------------------------------\n# Conversion tests for string and binary types.\n\n\nclass TestConvertStringLikeTypes:\n\n    def test_pandas_unicode(self):\n        repeats = 1000\n        values = ['foo', None, 'bar', 'ma\u00f1ana', np.nan]\n        df = pd.DataFrame({'strings': values * repeats})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        ex_values = ['foo', None, 'bar', 'ma\u00f1ana', None]\n        expected = pd.DataFrame({'strings': ex_values * repeats})\n\n        _check_pandas_roundtrip(df, expected=expected, expected_schema=schema)\n\n    def test_bytes_to_binary(self):\n        values = ['qux', b'foo', None, bytearray(b'barz'), 'qux', np.nan]\n        df = pd.DataFrame({'strings': values})\n\n        table = pa.Table.from_pandas(df)\n        assert table[0].type == pa.binary()\n\n        values2 = [b'qux', b'foo', None, b'barz', b'qux', None]\n        expected = pd.DataFrame({'strings': values2})\n        _check_pandas_roundtrip(df, expected)\n\n    @pytest.mark.large_memory\n    def test_bytes_exceed_2gb(self):\n        v1 = b'x' * 100000000\n        v2 = b'x' * 147483646\n\n        # ARROW-2227, hit exactly 2GB on the nose\n        df = pd.DataFrame({\n            'strings': [v1] * 20 + [v2] + ['x'] * 20\n        })\n        arr = pa.array(df['strings'])\n        assert isinstance(arr, pa.ChunkedArray)\n        assert arr.num_chunks == 2\n        arr = None\n\n        table = pa.Table.from_pandas(df)\n        assert table[0].num_chunks == 2\n\n    @pytest.mark.large_memory\n    @pytest.mark.parametrize('char', ['x', b'x'])\n    def test_auto_chunking_pandas_series_of_strings(self, char):\n        # ARROW-2367\n        v1 = char * 100000000\n        v2 = char * 147483646\n\n        df = pd.DataFrame({\n            'strings': [[v1]] * 20 + [[v2]] + [[b'x']]\n        })\n        arr = pa.array(df['strings'], from_pandas=True)\n        arr.validate(full=True)\n        assert isinstance(arr, pa.ChunkedArray)\n        assert arr.num_chunks == 2\n        assert len(arr.chunk(0)) == 21\n        assert len(arr.chunk(1)) == 1\n\n    def test_fixed_size_bytes(self):\n        values = [b'foo', None, bytearray(b'bar'), None, None, b'hey']\n        df = pd.DataFrame({'strings': values})\n        schema = pa.schema([pa.field('strings', pa.binary(3))])\n        table = pa.Table.from_pandas(df, schema=schema)\n        assert table.schema[0].type == schema[0].type\n        assert table.schema[0].name == schema[0].name\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, df)\n\n    def test_fixed_size_bytes_does_not_accept_varying_lengths(self):\n        values = [b'foo', None, b'ba', None, None, b'hey']\n        df = pd.DataFrame({'strings': values})\n        schema = pa.schema([pa.field('strings', pa.binary(3))])\n        with pytest.raises(pa.ArrowInvalid):\n            pa.Table.from_pandas(df, schema=schema)\n\n    def test_variable_size_bytes(self):\n        s = pd.Series([b'123', b'', b'a', None])\n        _check_series_roundtrip(s, type_=pa.binary())\n\n    def test_binary_from_bytearray(self):\n        s = pd.Series([bytearray(b'123'), bytearray(b''), bytearray(b'a'),\n                       None])\n        # Explicitly set type\n        _check_series_roundtrip(s, type_=pa.binary())\n        # Infer type from bytearrays\n        _check_series_roundtrip(s, expected_pa_type=pa.binary())\n\n    def test_large_binary(self):\n        s = pd.Series([b'123', b'', b'a', None])\n        _check_series_roundtrip(s, type_=pa.large_binary())\n        df = pd.DataFrame({'a': s})\n        _check_pandas_roundtrip(\n            df, schema=pa.schema([('a', pa.large_binary())]))\n\n    def test_large_string(self):\n        s = pd.Series(['123', '', 'a', None])\n        _check_series_roundtrip(s, type_=pa.large_string())\n        df = pd.DataFrame({'a': s})\n        _check_pandas_roundtrip(\n            df, schema=pa.schema([('a', pa.large_string())]))\n\n    def test_binary_view(self):\n        s = pd.Series([b'123', b'', b'a', None])\n        _check_series_roundtrip(s, type_=pa.binary_view())\n        df = pd.DataFrame({'a': s})\n        _check_pandas_roundtrip(\n            df, schema=pa.schema([('a', pa.binary_view())]))\n\n    def test_string_view(self):\n        s = pd.Series(['123', '', 'a', None])\n        _check_series_roundtrip(s, type_=pa.string_view())\n        df = pd.DataFrame({'a': s})\n        _check_pandas_roundtrip(\n            df, schema=pa.schema([('a', pa.string_view())]))\n\n    def test_table_empty_str(self):\n        values = ['', '', '', '', '']\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n\n        result1 = table.to_pandas(strings_to_categorical=False)\n        expected1 = pd.DataFrame({'strings': values})\n        tm.assert_frame_equal(result1, expected1, check_dtype=True)\n\n        result2 = table.to_pandas(strings_to_categorical=True)\n        expected2 = pd.DataFrame({'strings': pd.Categorical(values)})\n        tm.assert_frame_equal(result2, expected2, check_dtype=True)\n\n    def test_selective_categoricals(self):\n        values = ['', '', '', '', '']\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n        expected_str = pd.DataFrame({'strings': values})\n        expected_cat = pd.DataFrame({'strings': pd.Categorical(values)})\n\n        result1 = table.to_pandas(categories=['strings'])\n        tm.assert_frame_equal(result1, expected_cat, check_dtype=True)\n        result2 = table.to_pandas(categories=[])\n        tm.assert_frame_equal(result2, expected_str, check_dtype=True)\n        result3 = table.to_pandas(categories=('strings',))\n        tm.assert_frame_equal(result3, expected_cat, check_dtype=True)\n        result4 = table.to_pandas(categories=tuple())\n        tm.assert_frame_equal(result4, expected_str, check_dtype=True)\n\n    def test_to_pandas_categorical_zero_length(self):\n        # ARROW-3586\n        array = pa.array([], type=pa.int32())\n        table = pa.Table.from_arrays(arrays=[array], names=['col'])\n        # This would segfault under 0.11.0\n        table.to_pandas(categories=['col'])\n\n    def test_to_pandas_categories_already_dictionary(self):\n        # Showed up in ARROW-6434, ARROW-6435\n        array = pa.array(['foo', 'foo', 'foo', 'bar']).dictionary_encode()\n        table = pa.Table.from_arrays(arrays=[array], names=['col'])\n        result = table.to_pandas(categories=['col'])\n        assert table.to_pandas().equals(result)\n\n    def test_table_str_to_categorical_without_na(self):\n        values = ['a', 'a', 'b', 'b', 'c']\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n\n        result = table.to_pandas(strings_to_categorical=True)\n        expected = pd.DataFrame({'strings': pd.Categorical(values)})\n        tm.assert_frame_equal(result, expected, check_dtype=True)\n\n        with pytest.raises(pa.ArrowInvalid):\n            table.to_pandas(strings_to_categorical=True,\n                            zero_copy_only=True)\n\n    def test_table_str_to_categorical_with_na(self):\n        values = [None, 'a', 'b', np.nan]\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n\n        result = table.to_pandas(strings_to_categorical=True)\n        expected = pd.DataFrame({'strings': pd.Categorical(values)})\n        tm.assert_frame_equal(result, expected, check_dtype=True)\n\n        with pytest.raises(pa.ArrowInvalid):\n            table.to_pandas(strings_to_categorical=True,\n                            zero_copy_only=True)\n\n    # Regression test for ARROW-2101\n    def test_array_of_bytes_to_strings(self):\n        converted = pa.array(np.array([b'x'], dtype=object), pa.string())\n        assert converted.type == pa.string()\n\n    # Make sure that if an ndarray of bytes is passed to the array\n    # constructor and the type is string, it will fail if those bytes\n    # cannot be converted to utf-8\n    def test_array_of_bytes_to_strings_bad_data(self):\n        with pytest.raises(\n                pa.lib.ArrowInvalid,\n                match=\"was not a utf8 string\"):\n            pa.array(np.array([b'\\x80\\x81'], dtype=object), pa.string())\n\n    def test_numpy_string_array_to_fixed_size_binary(self):\n        arr = np.array([b'foo', b'bar', b'baz'], dtype='|S3')\n\n        converted = pa.array(arr, type=pa.binary(3))\n        expected = pa.array(list(arr), type=pa.binary(3))\n        assert converted.equals(expected)\n\n        mask = np.array([False, True, False])\n        converted = pa.array(arr, type=pa.binary(3), mask=mask)\n        expected = pa.array([b'foo', None, b'baz'], type=pa.binary(3))\n        assert converted.equals(expected)\n\n        with pytest.raises(pa.lib.ArrowInvalid,\n                           match=r'Got bytestring of length 3 \\(expected 4\\)'):\n            arr = np.array([b'foo', b'bar', b'baz'], dtype='|S3')\n            pa.array(arr, type=pa.binary(4))\n\n        with pytest.raises(\n                pa.lib.ArrowInvalid,\n                match=r'Got bytestring of length 12 \\(expected 3\\)'):\n            arr = np.array([b'foo', b'bar', b'baz'], dtype='|U3')\n            pa.array(arr, type=pa.binary(3))\n\n\nclass TestConvertDecimalTypes:\n    \"\"\"\n    Conversion test for decimal types.\n    \"\"\"\n    decimal32 = [\n        decimal.Decimal('-1234.123'),\n        decimal.Decimal('1234.439')\n    ]\n    decimal64 = [\n        decimal.Decimal('-129934.123331'),\n        decimal.Decimal('129534.123731')\n    ]\n    decimal128 = [\n        decimal.Decimal('394092382910493.12341234678'),\n        decimal.Decimal('-314292388910493.12343437128')\n    ]\n\n    @pytest.mark.parametrize(('values', 'expected_type'), [\n        pytest.param(decimal32, pa.decimal128(7, 3), id='decimal32'),\n        pytest.param(decimal64, pa.decimal128(12, 6), id='decimal64'),\n        pytest.param(decimal128, pa.decimal128(26, 11), id='decimal128')\n    ])\n    def test_decimal_from_pandas(self, values, expected_type):\n        expected = pd.DataFrame({'decimals': values})\n        table = pa.Table.from_pandas(expected, preserve_index=False)\n        field = pa.field('decimals', expected_type)\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = pa.schema([field], metadata=table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n    @pytest.mark.parametrize('values', [\n        pytest.param(decimal32, id='decimal32'),\n        pytest.param(decimal64, id='decimal64'),\n        pytest.param(decimal128, id='decimal128')\n    ])\n    def test_decimal_to_pandas(self, values):\n        expected = pd.DataFrame({'decimals': values})\n        converted = pa.Table.from_pandas(expected)\n        df = converted.to_pandas()\n        tm.assert_frame_equal(df, expected)\n\n    def test_decimal_fails_with_truncation(self):\n        data1 = [decimal.Decimal('1.234')]\n        type1 = pa.decimal128(10, 2)\n        with pytest.raises(pa.ArrowInvalid):\n            pa.array(data1, type=type1)\n\n        data2 = [decimal.Decimal('1.2345')]\n        type2 = pa.decimal128(10, 3)\n        with pytest.raises(pa.ArrowInvalid):\n            pa.array(data2, type=type2)\n\n    def test_decimal_with_different_precisions(self):\n        data = [\n            decimal.Decimal('0.01'),\n            decimal.Decimal('0.001'),\n        ]\n        series = pd.Series(data)\n        array = pa.array(series)\n        assert array.to_pylist() == data\n        assert array.type == pa.decimal128(3, 3)\n\n        array = pa.array(data, type=pa.decimal128(12, 5))\n        expected = [decimal.Decimal('0.01000'), decimal.Decimal('0.00100')]\n        assert array.to_pylist() == expected\n\n    def test_decimal_with_None_explicit_type(self):\n        series = pd.Series([decimal.Decimal('3.14'), None])\n        _check_series_roundtrip(series, type_=pa.decimal128(12, 5))\n\n        # Test that having all None values still produces decimal array\n        series = pd.Series([None] * 2)\n        _check_series_roundtrip(series, type_=pa.decimal128(12, 5))\n\n    def test_decimal_with_None_infer_type(self):\n        series = pd.Series([decimal.Decimal('3.14'), None])\n        _check_series_roundtrip(series, expected_pa_type=pa.decimal128(3, 2))\n\n    def test_strided_objects(self, tmpdir):\n        # see ARROW-3053\n        data = {\n            'a': {0: 'a'},\n            'b': {0: decimal.Decimal('0.0')}\n        }\n\n        # This yields strided objects\n        df = pd.DataFrame.from_dict(data)\n        _check_pandas_roundtrip(df)\n\n\nclass TestConvertListTypes:\n    \"\"\"\n    Conversion tests for list<> types.\n    \"\"\"\n\n    def test_column_of_arrays(self):\n        df, schema = dataframe_with_arrays()\n        _check_pandas_roundtrip(df, schema=schema, expected_schema=schema)\n        table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = schema.with_metadata(table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n        for column in df.columns:\n            field = schema.field(column)\n            _check_array_roundtrip(df[column], type=field.type)\n\n    def test_column_of_arrays_to_py(self):\n        # Test regression in ARROW-1199 not caught in above test\n        dtype = 'i1'\n        arr = np.array([\n            np.arange(10, dtype=dtype),\n            np.arange(5, dtype=dtype),\n            None,\n            np.arange(1, dtype=dtype)\n        ], dtype=object)\n        type_ = pa.list_(pa.int8())\n        parr = pa.array(arr, type=type_)\n\n        assert parr[0].as_py() == list(range(10))\n        assert parr[1].as_py() == list(range(5))\n        assert parr[2].as_py() is None\n        assert parr[3].as_py() == [0]\n\n    def test_column_of_boolean_list(self):\n        # ARROW-4370: Table to pandas conversion fails for list of bool\n        array = pa.array([[True, False], [True]], type=pa.list_(pa.bool_()))\n        table = pa.Table.from_arrays([array], names=['col1'])\n        df = table.to_pandas()\n\n        expected_df = pd.DataFrame({'col1': [[True, False], [True]]})\n        tm.assert_frame_equal(df, expected_df)\n\n        s = table[0].to_pandas()\n        tm.assert_series_equal(pd.Series(s), df['col1'], check_names=False)\n\n    def test_column_of_decimal_list(self):\n        array = pa.array([[decimal.Decimal('1'), decimal.Decimal('2')],\n                          [decimal.Decimal('3.3')]],\n                         type=pa.list_(pa.decimal128(2, 1)))\n        table = pa.Table.from_arrays([array], names=['col1'])\n        df = table.to_pandas()\n\n        expected_df = pd.DataFrame(\n            {'col1': [[decimal.Decimal('1'), decimal.Decimal('2')],\n                      [decimal.Decimal('3.3')]]})\n        tm.assert_frame_equal(df, expected_df)\n\n    def test_nested_types_from_ndarray_null_entries(self):\n        # Root cause of ARROW-6435\n        s = pd.Series(np.array([np.nan, np.nan], dtype=object))\n\n        for ty in [pa.list_(pa.int64()),\n                   pa.large_list(pa.int64()),\n                   pa.struct([pa.field('f0', 'int32')])]:\n            result = pa.array(s, type=ty)\n            expected = pa.array([None, None], type=ty)\n            assert result.equals(expected)\n\n            with pytest.raises(TypeError):\n                pa.array(s.values, type=ty)\n\n    def test_column_of_lists(self):\n        df, schema = dataframe_with_lists()\n        _check_pandas_roundtrip(df, schema=schema, expected_schema=schema)\n        table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = schema.with_metadata(table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n        for column in df.columns:\n            field = schema.field(column)\n            _check_array_roundtrip(df[column], type=field.type)\n\n    def test_column_of_lists_first_empty(self):\n        # ARROW-2124\n        num_lists = [[], [2, 3, 4], [3, 6, 7, 8], [], [2]]\n        series = pd.Series([np.array(s, dtype=float) for s in num_lists])\n        arr = pa.array(series)\n        result = pd.Series(arr.to_pandas())\n        tm.assert_series_equal(result, series)\n\n    def test_column_of_lists_chunked(self):\n        # ARROW-1357\n        df = pd.DataFrame({\n            'lists': np.array([\n                [1, 2],\n                None,\n                [2, 3],\n                [4, 5],\n                [6, 7],\n                [8, 9]\n            ], dtype=object)\n        })\n\n        schema = pa.schema([\n            pa.field('lists', pa.list_(pa.int64()))\n        ])\n\n        t1 = pa.Table.from_pandas(df[:2], schema=schema)\n        t2 = pa.Table.from_pandas(df[2:], schema=schema)\n\n        table = pa.concat_tables([t1, t2])\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, df)\n\n    def test_empty_column_of_lists_chunked(self):\n        df = pd.DataFrame({\n            'lists': np.array([], dtype=object)\n        })\n\n        schema = pa.schema([\n            pa.field('lists', pa.list_(pa.int64()))\n        ])\n\n        table = pa.Table.from_pandas(df, schema=schema)\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, df)\n\n    def test_column_of_lists_chunked2(self):\n        data1 = [[0, 1], [2, 3], [4, 5], [6, 7], [10, 11],\n                 [12, 13], [14, 15], [16, 17]]\n        data2 = [[8, 9], [18, 19]]\n\n        a1 = pa.array(data1)\n        a2 = pa.array(data2)\n\n        t1 = pa.Table.from_arrays([a1], names=['a'])\n        t2 = pa.Table.from_arrays([a2], names=['a'])\n\n        concatenated = pa.concat_tables([t1, t2])\n\n        result = concatenated.to_pandas()\n        expected = pd.DataFrame({'a': data1 + data2})\n\n        tm.assert_frame_equal(result, expected)\n\n    def test_column_of_lists_strided(self):\n        df, schema = dataframe_with_lists()\n        df = pd.concat([df] * 6, ignore_index=True)\n\n        arr = df['int64'].values[::3]\n        assert arr.strides[0] != 8\n\n        _check_array_roundtrip(arr)\n\n    def test_nested_lists_all_none(self):\n        data = np.array([[None, None], None], dtype=object)\n\n        arr = pa.array(data)\n        expected = pa.array(list(data))\n        assert arr.equals(expected)\n        assert arr.type == pa.list_(pa.null())\n\n        data2 = np.array([None, None, [None, None],\n                          np.array([None, None], dtype=object)],\n                         dtype=object)\n        arr = pa.array(data2)\n        expected = pa.array([None, None, [None, None], [None, None]])\n        assert arr.equals(expected)\n\n    def test_nested_lists_all_empty(self):\n        # ARROW-2128\n        data = pd.Series([[], [], []])\n        arr = pa.array(data)\n        expected = pa.array(list(data))\n        assert arr.equals(expected)\n        assert arr.type == pa.list_(pa.null())\n\n    def test_nested_list_first_empty(self):\n        # ARROW-2711\n        data = pd.Series([[], [\"a\"]])\n        arr = pa.array(data)\n        expected = pa.array(list(data))\n        assert arr.equals(expected)\n        assert arr.type == pa.list_(pa.string())\n\n    def test_nested_smaller_ints(self):\n        # ARROW-1345, ARROW-2008, there were some type inference bugs happening\n        # before\n        data = pd.Series([np.array([1, 2, 3], dtype='i1'), None])\n        result = pa.array(data)\n        result2 = pa.array(data.values)\n        expected = pa.array([[1, 2, 3], None], type=pa.list_(pa.int8()))\n        assert result.equals(expected)\n        assert result2.equals(expected)\n\n        data3 = pd.Series([np.array([1, 2, 3], dtype='f4'), None])\n        result3 = pa.array(data3)\n        expected3 = pa.array([[1, 2, 3], None], type=pa.list_(pa.float32()))\n        assert result3.equals(expected3)\n\n    def test_infer_lists(self):\n        data = OrderedDict([\n            ('nan_ints', [[np.nan, 1], [2, 3]]),\n            ('ints', [[0, 1], [2, 3]]),\n            ('strs', [[None, 'b'], ['c', 'd']]),\n            ('nested_strs', [[[None, 'b'], ['c', 'd']], None])\n        ])\n        df = pd.DataFrame(data)\n\n        expected_schema = pa.schema([\n            pa.field('nan_ints', pa.list_(pa.int64())),\n            pa.field('ints', pa.list_(pa.int64())),\n            pa.field('strs', pa.list_(pa.string())),\n            pa.field('nested_strs', pa.list_(pa.list_(pa.string())))\n        ])\n\n        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n\n    def test_fixed_size_list(self):\n        # ARROW-7365\n        fixed_ty = pa.list_(pa.int64(), list_size=4)\n        variable_ty = pa.list_(pa.int64())\n\n        data = [[0, 1, 2, 3], None, [4, 5, 6, 7], [8, 9, 10, 11]]\n        fixed_arr = pa.array(data, type=fixed_ty)\n        variable_arr = pa.array(data, type=variable_ty)\n\n        result = fixed_arr.to_pandas()\n        expected = variable_arr.to_pandas()\n\n        for left, right in zip(result, expected):\n            if left is None:\n                assert right is None\n            npt.assert_array_equal(left, right)\n\n    def test_infer_numpy_array(self):\n        data = OrderedDict([\n            ('ints', [\n                np.array([0, 1], dtype=np.int64),\n                np.array([2, 3], dtype=np.int64)\n            ])\n        ])\n        df = pd.DataFrame(data)\n        expected_schema = pa.schema([\n            pa.field('ints', pa.list_(pa.int64()))\n        ])\n\n        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n\n    def test_to_list_of_structs_pandas(self):\n        ints = pa.array([1, 2, 3], pa.int32())\n        strings = pa.array([['a', 'b'], ['c', 'd'], ['e', 'f']],\n                           pa.list_(pa.string()))\n        structs = pa.StructArray.from_arrays([ints, strings], ['f1', 'f2'])\n        data = pa.ListArray.from_arrays([0, 1, 3], structs)\n\n        expected = pd.Series([\n            [{'f1': 1, 'f2': ['a', 'b']}],\n            [{'f1': 2, 'f2': ['c', 'd']},\n             {'f1': 3, 'f2': ['e', 'f']}]\n        ])\n\n        series = pd.Series(data.to_pandas())\n\n        # pandas.testing generates a\n        # DeprecationWarning: elementwise comparison failed\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(series, expected)\n\n    def test_to_list_of_maps_pandas(self):\n        if ((Version(np.__version__) >= Version(\"1.25.0.dev0\")) and\n                (Version(pd.__version__) < Version(\"2.0.0\"))):\n            # TODO: regression in pandas with numpy 1.25dev\n            # https://github.com/pandas-dev/pandas/issues/50360\n            pytest.skip(\"Regression in pandas with numpy 1.25\")\n        data = [\n            [[('foo', ['a', 'b']), ('bar', ['c', 'd'])]],\n            [[('baz', []), ('qux', None), ('quux', [None, 'e'])], [('quz', ['f', 'g'])]]\n        ]\n        arr = pa.array(data, pa.list_(pa.map_(pa.utf8(), pa.list_(pa.utf8()))))\n        series = arr.to_pandas()\n        expected = pd.Series(data)\n\n        # pandas.testing generates a\n        # DeprecationWarning: elementwise comparison failed\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(series, expected)\n\n    def test_to_list_of_maps_pandas_sliced(self):\n        \"\"\"\n        A slightly more rigorous test for chunk/slice combinations\n        \"\"\"\n\n        if ((Version(np.__version__) >= Version(\"1.25.0.dev0\")) and\n                (Version(pd.__version__) < Version(\"2.0.0\"))):\n            # TODO: regression in pandas with numpy 1.25dev\n            # https://github.com/pandas-dev/pandas/issues/50360\n            pytest.skip(\"Regression in pandas with numpy 1.25\")\n\n        keys = pa.array(['ignore', 'foo', 'bar', 'baz',\n                         'qux', 'quux', 'ignore']).slice(1, 5)\n        items = pa.array(\n            [['ignore'], ['ignore'], ['a', 'b'], ['c', 'd'], [], None, [None, 'e']],\n            pa.list_(pa.string()),\n        ).slice(2, 5)\n        map = pa.MapArray.from_arrays([0, 2, 4], keys, items)\n        arr = pa.ListArray.from_arrays([0, 1, 2], map)\n\n        series = arr.to_pandas()\n        expected = pd.Series([\n            [[('foo', ['a', 'b']), ('bar', ['c', 'd'])]],\n            [[('baz', []), ('qux', None)]],\n        ])\n\n        series_sliced = arr.slice(1, 2).to_pandas()\n        expected_sliced = pd.Series([\n            [[('baz', []), ('qux', None)]],\n        ])\n\n        # pandas.testing generates a\n        # DeprecationWarning: elementwise comparison failed\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(series, expected)\n            tm.assert_series_equal(series_sliced, expected_sliced)\n\n    @pytest.mark.parametrize('t,data,expected', [\n        (\n            pa.int64,\n            [[1, 2], [3], None],\n            [None, [3], None]\n        ),\n        (\n            pa.string,\n            [['aaa', 'bb'], ['c'], None],\n            [None, ['c'], None]\n        ),\n        (\n            pa.null,\n            [[None, None], [None], None],\n            [None, [None], None]\n        )\n    ])\n    def test_array_from_pandas_typed_array_with_mask(self, t, data, expected):\n        m = np.array([True, False, True])\n\n        s = pd.Series(data)\n        result = pa.Array.from_pandas(s, mask=m, type=pa.list_(t()))\n\n        assert pa.Array.from_pandas(expected,\n                                    type=pa.list_(t())).equals(result)\n\n    def test_empty_list_roundtrip(self):\n        empty_list_array = np.empty((3,), dtype=object)\n        empty_list_array.fill([])\n\n        df = pd.DataFrame({'a': np.array(['1', '2', '3']),\n                           'b': empty_list_array})\n        tbl = pa.Table.from_pandas(df)\n\n        result = tbl.to_pandas()\n\n        tm.assert_frame_equal(result, df)\n\n    def test_array_from_nested_arrays(self):\n        df, schema = dataframe_with_arrays()\n        for field in schema:\n            arr = df[field.name].values\n            expected = pa.array(list(arr), type=field.type)\n            result = pa.array(arr)\n            assert result.type == field.type  # == list<scalar>\n            assert result.equals(expected)\n\n    def test_nested_large_list(self):\n        s = (pa.array([[[1, 2, 3], [4]], None],\n                      type=pa.large_list(pa.large_list(pa.int64())))\n             .to_pandas())\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\",\n                                    \"Creating an ndarray from ragged nested\",\n                                    _np_VisibleDeprecationWarning)\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(\n                s, pd.Series([[[1, 2, 3], [4]], None], dtype=object),\n                check_names=False)\n\n    def test_large_binary_list(self):\n        for list_type_factory in (pa.list_, pa.large_list):\n            s = (pa.array([[\"aa\", \"bb\"], None, [\"cc\"], []],\n                          type=list_type_factory(pa.large_binary()))\n                 .to_pandas())\n            tm.assert_series_equal(\n                s, pd.Series([[b\"aa\", b\"bb\"], None, [b\"cc\"], []]),\n                check_names=False)\n            s = (pa.array([[\"aa\", \"bb\"], None, [\"cc\"], []],\n                          type=list_type_factory(pa.large_string()))\n                 .to_pandas())\n            tm.assert_series_equal(\n                s, pd.Series([[\"aa\", \"bb\"], None, [\"cc\"], []]),\n                check_names=False)\n\n    def test_list_of_dictionary(self):\n        child = pa.array([\"foo\", \"bar\", None, \"foo\"]).dictionary_encode()\n        arr = pa.ListArray.from_arrays([0, 1, 3, 3, 4], child)\n\n        # Expected a Series of lists\n        expected = pd.Series(arr.to_pylist())\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n        # Same but with nulls\n        arr = arr.take([0, 1, None, 3])\n        expected[2] = None\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n    @pytest.mark.large_memory\n    def test_auto_chunking_on_list_overflow(self):\n        # ARROW-9976\n        n = 2**21\n        df = pd.DataFrame.from_dict({\n            \"a\": list(np.zeros((n, 2**10), dtype='uint8')),\n            \"b\": range(n)\n        })\n        table = pa.Table.from_pandas(df)\n        table.validate(full=True)\n\n        column_a = table[0]\n        assert column_a.num_chunks == 2\n        assert len(column_a.chunk(0)) == 2**21 - 1\n        assert len(column_a.chunk(1)) == 1\n\n    def test_map_array_roundtrip(self):\n        data = [[(b'a', 1), (b'b', 2)],\n                [(b'c', 3)],\n                [(b'd', 4), (b'e', 5), (b'f', 6)],\n                [(b'g', 7)]]\n\n        df = pd.DataFrame({\"map\": data})\n        schema = pa.schema([(\"map\", pa.map_(pa.binary(), pa.int32()))])\n\n        _check_pandas_roundtrip(df, schema=schema)\n\n    def test_map_array_chunked(self):\n        data1 = [[(b'a', 1), (b'b', 2)],\n                 [(b'c', 3)],\n                 [(b'd', 4), (b'e', 5), (b'f', 6)],\n                 [(b'g', 7)]]\n        data2 = [[(k, v * 2) for k, v in row] for row in data1]\n\n        arr1 = pa.array(data1, type=pa.map_(pa.binary(), pa.int32()))\n        arr2 = pa.array(data2, type=pa.map_(pa.binary(), pa.int32()))\n        arr = pa.chunked_array([arr1, arr2])\n\n        expected = pd.Series(data1 + data2)\n        actual = arr.to_pandas()\n        tm.assert_series_equal(actual, expected, check_names=False)\n\n    def test_map_array_with_nulls(self):\n        data = [[(b'a', 1), (b'b', 2)],\n                None,\n                [(b'd', 4), (b'e', 5), (b'f', None)],\n                [(b'g', 7)]]\n\n        # None value in item array causes upcast to float\n        expected = [[(k, float(v) if v is not None else None) for k, v in row]\n                    if row is not None else None for row in data]\n        expected = pd.Series(expected)\n\n        arr = pa.array(data, type=pa.map_(pa.binary(), pa.int32()))\n        actual = arr.to_pandas()\n        tm.assert_series_equal(actual, expected, check_names=False)\n\n    def test_map_array_dictionary_encoded(self):\n        offsets = pa.array([0, 3, 5])\n        items = pa.array(['a', 'b', 'c', 'a', 'd']).dictionary_encode()\n        keys = pa.array(list(range(len(items))))\n        arr = pa.MapArray.from_arrays(offsets, keys, items)\n\n        # Dictionary encoded values converted to dense\n        expected = pd.Series(\n            [[(0, 'a'), (1, 'b'), (2, 'c')], [(3, 'a'), (4, 'd')]])\n\n        actual = arr.to_pandas()\n        tm.assert_series_equal(actual, expected, check_names=False)\n\n    def test_list_no_duplicate_base(self):\n        # ARROW-18400\n        arr = pa.array([[1, 2], [3, 4, 5], None, [6, None], [7, 8]])\n        chunked_arr = pa.chunked_array([arr.slice(0, 3), arr.slice(3, 1)])\n\n        np_arr = chunked_arr.to_numpy()\n\n        expected = np.array([[1., 2.], [3., 4., 5.], None,\n                            [6., np.nan]], dtype=\"object\")\n        for left, right in zip(np_arr, expected):\n            if right is None:\n                assert left == right\n            else:\n                npt.assert_array_equal(left, right)\n\n        expected_base = np.array([[1., 2., 3., 4., 5., 6., np.nan]])\n        npt.assert_array_equal(np_arr[0].base, expected_base)\n\n        np_arr_sliced = chunked_arr.slice(1, 3).to_numpy()\n\n        expected = np.array([[3, 4, 5], None, [6, np.nan]], dtype=\"object\")\n        for left, right in zip(np_arr_sliced, expected):\n            if right is None:\n                assert left == right\n            else:\n                npt.assert_array_equal(left, right)\n\n        expected_base = np.array([[3., 4., 5., 6., np.nan]])\n        npt.assert_array_equal(np_arr_sliced[0].base, expected_base)\n\n    def test_list_values_behind_null(self):\n        arr = pa.ListArray.from_arrays(\n            offsets=pa.array([0, 2, 4, 6]),\n            values=pa.array([1, 2, 99, 99, 3, None]),\n            mask=pa.array([False, True, False])\n        )\n        np_arr = arr.to_numpy(zero_copy_only=False)\n\n        expected = np.array([[1., 2.], None, [3., np.nan]], dtype=\"object\")\n        for left, right in zip(np_arr, expected):\n            if right is None:\n                assert left == right\n            else:\n                npt.assert_array_equal(left, right)\n\n    @pytest.mark.parametrize(\"klass\", [pa.ListViewArray, pa.LargeListViewArray])\n    def test_list_view_to_pandas_with_in_order_offsets(self, klass):\n        arr = klass.from_arrays(\n            offsets=pa.array([0, 2, 4]),\n            sizes=pa.array([2, 2, 2]),\n            values=pa.array([1, 2, 3, 4, 5, 6]),\n        )\n\n        actual = arr.to_pandas()\n        expected = pd.Series([[1, 2], [3, 4], [5, 6]])\n\n        tm.assert_series_equal(actual, expected)\n\n    @pytest.mark.parametrize(\"klass\", [pa.ListViewArray, pa.LargeListViewArray])\n    def test_list_view_to_pandas_with_out_of_order_offsets(self, klass):\n        arr = klass.from_arrays(\n            offsets=pa.array([2, 4, 0]),\n            sizes=pa.array([2, 2, 2]),\n            values=pa.array([1, 2, 3, 4, 5, 6]),\n        )\n\n        actual = arr.to_pandas()\n        expected = pd.Series([[3, 4], [5, 6], [1, 2]])\n\n        tm.assert_series_equal(actual, expected)\n\n    @pytest.mark.parametrize(\"klass\", [pa.ListViewArray, pa.LargeListViewArray])\n    def test_list_view_to_pandas_with_overlapping_offsets(self, klass):\n        arr = klass.from_arrays(\n            offsets=pa.array([0, 1, 2]),\n            sizes=pa.array([4, 4, 4]),\n            values=pa.array([1, 2, 3, 4, 5, 6]),\n        )\n\n        actual = arr.to_pandas()\n        expected = pd.Series([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])\n\n        tm.assert_series_equal(actual, expected)\n\n    @pytest.mark.parametrize(\"klass\", [pa.ListViewArray, pa.LargeListViewArray])\n    def test_list_view_to_pandas_with_null_values(self, klass):\n        arr = klass.from_arrays(\n            offsets=pa.array([0, 2, 2]),\n            sizes=pa.array([2, 0, 0]),\n            values=pa.array([1, None]),\n            mask=pa.array([False, False, True])\n        )\n\n        actual = arr.to_pandas()\n        expected = pd.Series([[1, np.nan], [], None])\n\n        tm.assert_series_equal(actual, expected)\n\n    @pytest.mark.parametrize(\"klass\", [pa.ListViewArray, pa.LargeListViewArray])\n    def test_list_view_to_pandas_multiple_chunks(self, klass):\n        gc.collect()\n        bytes_start = pa.total_allocated_bytes()\n        arr1 = klass.from_arrays(\n            offsets=pa.array([2, 1, 0]),\n            sizes=pa.array([2, 2, 2]),\n            values=pa.array([1, 2, 3, 4])\n        )\n        arr2 = klass.from_arrays(\n            offsets=pa.array([0, 1, 1]),\n            sizes=pa.array([3, 3, 0]),\n            values=pa.array([5, 6, 7, None]),\n            mask=pa.array([False, False, True])\n        )\n        arr = pa.chunked_array([arr1, arr2])\n\n        actual = arr.to_pandas()\n        expected = pd.Series([[3, 4], [2, 3], [1, 2], [5, 6, 7], [6, 7, np.nan], None])\n\n        tm.assert_series_equal(actual, expected)\n\n        del actual\n        del arr\n        del arr1\n        del arr2\n        bytes_end = pa.total_allocated_bytes()\n        assert bytes_end == bytes_start\n\n\nclass TestConvertStructTypes:\n    \"\"\"\n    Conversion tests for struct types.\n    \"\"\"\n\n    def test_pandas_roundtrip(self):\n        df = pd.DataFrame({'dicts': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]})\n\n        expected_schema = pa.schema([\n            ('dicts', pa.struct([('a', pa.int64()), ('b', pa.int64())])),\n        ])\n\n        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n\n        # specifying schema explicitly in from_pandas\n        _check_pandas_roundtrip(\n            df, schema=expected_schema, expected_schema=expected_schema)\n\n    def test_to_pandas(self):\n        ints = pa.array([None, 2, 3], type=pa.int64())\n        strs = pa.array(['a', None, 'c'], type=pa.string())\n        bools = pa.array([True, False, None], type=pa.bool_())\n        arr = pa.StructArray.from_arrays(\n            [ints, strs, bools],\n            ['ints', 'strs', 'bools'])\n\n        expected = pd.Series([\n            {'ints': None, 'strs': 'a', 'bools': True},\n            {'ints': 2, 'strs': None, 'bools': False},\n            {'ints': 3, 'strs': 'c', 'bools': None},\n        ])\n\n        series = pd.Series(arr.to_pandas())\n        tm.assert_series_equal(series, expected)\n\n    def test_to_pandas_multiple_chunks(self):\n        # ARROW-11855\n        gc.collect()\n        bytes_start = pa.total_allocated_bytes()\n        ints1 = pa.array([1], type=pa.int64())\n        ints2 = pa.array([2], type=pa.int64())\n        arr1 = pa.StructArray.from_arrays([ints1], ['ints'])\n        arr2 = pa.StructArray.from_arrays([ints2], ['ints'])\n        arr = pa.chunked_array([arr1, arr2])\n\n        expected = pd.Series([\n            {'ints': 1},\n            {'ints': 2}\n        ])\n\n        series = pd.Series(arr.to_pandas())\n        tm.assert_series_equal(series, expected)\n\n        del series\n        del arr\n        del arr1\n        del arr2\n        del ints1\n        del ints2\n        bytes_end = pa.total_allocated_bytes()\n        assert bytes_end == bytes_start\n\n    def test_from_numpy(self):\n        dt = np.dtype([('x', np.int32),\n                       (('y_title', 'y'), np.bool_)])\n        ty = pa.struct([pa.field('x', pa.int32()),\n                        pa.field('y', pa.bool_())])\n\n        data = np.array([], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == []\n\n        data = np.array([(42, True), (43, False)], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == [{'x': 42, 'y': True},\n                                   {'x': 43, 'y': False}]\n\n        # With mask\n        arr = pa.array(data, mask=np.bool_([False, True]), type=ty)\n        assert arr.to_pylist() == [{'x': 42, 'y': True}, None]\n\n        # Trivial struct type\n        dt = np.dtype([])\n        ty = pa.struct([])\n\n        data = np.array([], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == []\n\n        data = np.array([(), ()], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == [{}, {}]\n\n    def test_from_numpy_nested(self):\n        # Note: an object field inside a struct\n        dt = np.dtype([('x', np.dtype([('xx', np.int8),\n                                       ('yy', np.bool_)])),\n                       ('y', np.int16),\n                       ('z', np.object_)])\n        # Note: itemsize is not necessarily a multiple of sizeof(object)\n        # object_ is 8 bytes on 64-bit systems, 4 bytes on 32-bit systems\n        assert dt.itemsize == (12 if sys.maxsize > 2**32 else 8)\n        ty = pa.struct([pa.field('x', pa.struct([pa.field('xx', pa.int8()),\n                                                 pa.field('yy', pa.bool_())])),\n                        pa.field('y', pa.int16()),\n                        pa.field('z', pa.string())])\n\n        data = np.array([], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == []\n\n        data = np.array([\n            ((1, True), 2, 'foo'),\n            ((3, False), 4, 'bar')], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == [\n            {'x': {'xx': 1, 'yy': True}, 'y': 2, 'z': 'foo'},\n            {'x': {'xx': 3, 'yy': False}, 'y': 4, 'z': 'bar'}]\n\n    @pytest.mark.slow\n    @pytest.mark.large_memory\n    def test_from_numpy_large(self):\n        # Exercise rechunking + nulls\n        target_size = 3 * 1024**3  # 4GB\n        dt = np.dtype([('x', np.float64), ('y', 'object')])\n        bs = 65536 - dt.itemsize\n        block = b'.' * bs\n        n = target_size // (bs + dt.itemsize)\n        data = np.zeros(n, dtype=dt)\n        data['x'] = np.random.random_sample(n)\n        data['y'] = block\n        # Add implicit nulls\n        data['x'][data['x'] < 0.2] = np.nan\n\n        ty = pa.struct([pa.field('x', pa.float64()),\n                        pa.field('y', pa.binary())])\n        arr = pa.array(data, type=ty, from_pandas=True)\n        arr.validate(full=True)\n        assert arr.num_chunks == 2\n\n        def iter_chunked_array(arr):\n            for chunk in arr.iterchunks():\n                yield from chunk\n\n        def check(arr, data, mask=None):\n            assert len(arr) == len(data)\n            xs = data['x']\n            ys = data['y']\n            for i, obj in enumerate(iter_chunked_array(arr)):\n                try:\n                    d = obj.as_py()\n                    if mask is not None and mask[i]:\n                        assert d is None\n                    else:\n                        x = xs[i]\n                        if np.isnan(x):\n                            assert d['x'] is None\n                        else:\n                            assert d['x'] == x\n                        assert d['y'] == ys[i]\n                except Exception:\n                    print(\"Failed at index\", i)\n                    raise\n\n        check(arr, data)\n        del arr\n\n        # Now with explicit mask\n        mask = np.random.random_sample(n) < 0.2\n        arr = pa.array(data, type=ty, mask=mask, from_pandas=True)\n        arr.validate(full=True)\n        assert arr.num_chunks == 2\n\n        check(arr, data, mask)\n        del arr\n\n    def test_from_numpy_bad_input(self):\n        ty = pa.struct([pa.field('x', pa.int32()),\n                        pa.field('y', pa.bool_())])\n        dt = np.dtype([('x', np.int32),\n                       ('z', np.bool_)])\n\n        data = np.array([], dtype=dt)\n        with pytest.raises(ValueError,\n                           match=\"Missing field 'y'\"):\n            pa.array(data, type=ty)\n        data = np.int32([])\n        with pytest.raises(TypeError,\n                           match=\"Expected struct array\"):\n            pa.array(data, type=ty)\n\n    def test_from_tuples(self):\n        df = pd.DataFrame({'tuples': [(1, 2), (3, 4)]})\n        expected_df = pd.DataFrame(\n            {'tuples': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]})\n\n        # conversion from tuples works when specifying expected struct type\n        struct_type = pa.struct([('a', pa.int64()), ('b', pa.int64())])\n\n        arr = np.asarray(df['tuples'])\n        _check_array_roundtrip(\n            arr, expected=expected_df['tuples'], type=struct_type)\n\n        expected_schema = pa.schema([('tuples', struct_type)])\n        _check_pandas_roundtrip(\n            df, expected=expected_df, schema=expected_schema,\n            expected_schema=expected_schema)\n\n    def test_struct_of_dictionary(self):\n        names = ['ints', 'strs']\n        children = [pa.array([456, 789, 456]).dictionary_encode(),\n                    pa.array([\"foo\", \"foo\", None]).dictionary_encode()]\n        arr = pa.StructArray.from_arrays(children, names=names)\n\n        # Expected a Series of {field name: field value} dicts\n        rows_as_tuples = zip(*(child.to_pylist() for child in children))\n        rows_as_dicts = [dict(zip(names, row)) for row in rows_as_tuples]\n\n        expected = pd.Series(rows_as_dicts)\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n        # Same but with nulls\n        arr = arr.take([0, None, 2])\n        expected[1] = None\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n\nclass TestZeroCopyConversion:\n    \"\"\"\n    Tests that zero-copy conversion works with some types.\n    \"\"\"\n\n    def test_zero_copy_success(self):\n        result = pa.array([0, 1, 2]).to_pandas(zero_copy_only=True)\n        npt.assert_array_equal(result, [0, 1, 2])\n\n    def test_zero_copy_dictionaries(self):\n        arr = pa.DictionaryArray.from_arrays(\n            np.array([0, 0]),\n            np.array([5], dtype=\"int64\"),\n        )\n\n        result = arr.to_pandas(zero_copy_only=True)\n        values = pd.Categorical([5, 5])\n\n        tm.assert_series_equal(pd.Series(result), pd.Series(values),\n                               check_names=False)\n\n    def test_zero_copy_timestamp(self):\n        arr = np.array(['2007-07-13'], dtype='datetime64[ns]')\n        result = pa.array(arr).to_pandas(zero_copy_only=True)\n        npt.assert_array_equal(result, arr)\n\n    def test_zero_copy_duration(self):\n        arr = np.array([1], dtype='timedelta64[ns]')\n        result = pa.array(arr).to_pandas(zero_copy_only=True)\n        npt.assert_array_equal(result, arr)\n\n    def check_zero_copy_failure(self, arr):\n        with pytest.raises(pa.ArrowInvalid):\n            arr.to_pandas(zero_copy_only=True)\n\n    def test_zero_copy_failure_on_object_types(self):\n        self.check_zero_copy_failure(pa.array(['A', 'B', 'C']))\n\n    def test_zero_copy_failure_with_int_when_nulls(self):\n        self.check_zero_copy_failure(pa.array([0, 1, None]))\n\n    def test_zero_copy_failure_with_float_when_nulls(self):\n        self.check_zero_copy_failure(pa.array([0.0, 1.0, None]))\n\n    def test_zero_copy_failure_on_bool_types(self):\n        self.check_zero_copy_failure(pa.array([True, False]))\n\n    def test_zero_copy_failure_on_list_types(self):\n        arr = pa.array([[1, 2], [8, 9]], type=pa.list_(pa.int64()))\n        self.check_zero_copy_failure(arr)\n\n    def test_zero_copy_failure_on_timestamp_with_nulls(self):\n        arr = np.array([1, None], dtype='datetime64[ns]')\n        self.check_zero_copy_failure(pa.array(arr))\n\n    def test_zero_copy_failure_on_duration_with_nulls(self):\n        arr = np.array([1, None], dtype='timedelta64[ns]')\n        self.check_zero_copy_failure(pa.array(arr))\n\n\ndef _non_threaded_conversion():\n    df = _alltypes_example()\n    _check_pandas_roundtrip(df, use_threads=False)\n    _check_pandas_roundtrip(df, use_threads=False, as_batch=True)\n\n\ndef _threaded_conversion():\n    df = _alltypes_example()\n    _check_pandas_roundtrip(df, use_threads=True)\n    _check_pandas_roundtrip(df, use_threads=True, as_batch=True)\n\n\nclass TestConvertMisc:\n    \"\"\"\n    Miscellaneous conversion tests.\n    \"\"\"\n\n    type_pairs = [\n        (np.int8, pa.int8()),\n        (np.int16, pa.int16()),\n        (np.int32, pa.int32()),\n        (np.int64, pa.int64()),\n        (np.uint8, pa.uint8()),\n        (np.uint16, pa.uint16()),\n        (np.uint32, pa.uint32()),\n        (np.uint64, pa.uint64()),\n        (np.float16, pa.float16()),\n        (np.float32, pa.float32()),\n        (np.float64, pa.float64()),\n        # XXX unsupported\n        # (np.dtype([('a', 'i2')]), pa.struct([pa.field('a', pa.int16())])),\n        (np.object_, pa.string()),\n        (np.object_, pa.binary()),\n        (np.object_, pa.binary(10)),\n        (np.object_, pa.list_(pa.int64())),\n    ]\n\n    def test_all_none_objects(self):\n        df = pd.DataFrame({'a': [None, None, None]})\n        _check_pandas_roundtrip(df)\n\n    def test_all_none_category(self):\n        df = pd.DataFrame({'a': [None, None, None]})\n        df['a'] = df['a'].astype('category')\n        _check_pandas_roundtrip(df)\n\n    def test_empty_arrays(self):\n        for dtype, pa_type in self.type_pairs:\n            arr = np.array([], dtype=dtype)\n            _check_array_roundtrip(arr, type=pa_type)\n\n    def test_non_threaded_conversion(self):\n        _non_threaded_conversion()\n\n    def test_threaded_conversion_multiprocess(self):\n        # Parallel conversion should work from child processes too (ARROW-2963)\n        pool = mp.Pool(2)\n        try:\n            pool.apply(_threaded_conversion)\n        finally:\n            pool.close()\n            pool.join()\n\n    def test_category(self):\n        repeats = 5\n        v1 = ['foo', None, 'bar', 'qux', np.nan]\n        v2 = [4, 5, 6, 7, 8]\n        v3 = [b'foo', None, b'bar', b'qux', np.nan]\n\n        arrays = {\n            'cat_strings': pd.Categorical(v1 * repeats),\n            'cat_strings_with_na': pd.Categorical(v1 * repeats,\n                                                  categories=['foo', 'bar']),\n            'cat_ints': pd.Categorical(v2 * repeats),\n            'cat_binary': pd.Categorical(v3 * repeats),\n            'cat_strings_ordered': pd.Categorical(\n                v1 * repeats, categories=['bar', 'qux', 'foo'],\n                ordered=True),\n            'ints': v2 * repeats,\n            'ints2': v2 * repeats,\n            'strings': v1 * repeats,\n            'strings2': v1 * repeats,\n            'strings3': v3 * repeats}\n        df = pd.DataFrame(arrays)\n        _check_pandas_roundtrip(df)\n\n        for k in arrays:\n            _check_array_roundtrip(arrays[k])\n\n    def test_category_implicit_from_pandas(self):\n        # ARROW-3374\n        def _check(v):\n            arr = pa.array(v)\n            result = arr.to_pandas()\n            tm.assert_series_equal(pd.Series(result), pd.Series(v))\n\n        arrays = [\n            pd.Categorical(['a', 'b', 'c'], categories=['a', 'b']),\n            pd.Categorical(['a', 'b', 'c'], categories=['a', 'b'],\n                           ordered=True)\n        ]\n        for arr in arrays:\n            _check(arr)\n\n    def test_empty_category(self):\n        # ARROW-2443\n        df = pd.DataFrame({'cat': pd.Categorical([])})\n        _check_pandas_roundtrip(df)\n\n    def test_category_zero_chunks(self):\n        # ARROW-5952\n        for pa_type, dtype in [(pa.string(), 'object'), (pa.int64(), 'int64')]:\n            a = pa.chunked_array([], pa.dictionary(pa.int8(), pa_type))\n            result = a.to_pandas()\n            expected = pd.Categorical([], categories=np.array([], dtype=dtype))\n            tm.assert_series_equal(pd.Series(result), pd.Series(expected))\n\n            table = pa.table({'a': a})\n            result = table.to_pandas()\n            expected = pd.DataFrame({'a': expected})\n            tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"data,error_type\",\n        [\n            ({\"a\": [\"a\", 1, 2.0]}, pa.ArrowTypeError),\n            ({\"a\": [\"a\", 1, 2.0]}, pa.ArrowTypeError),\n            ({\"a\": [1, True]}, pa.ArrowTypeError),\n            ({\"a\": [True, \"a\"]}, pa.ArrowInvalid),\n            ({\"a\": [1, \"a\"]}, pa.ArrowInvalid),\n            ({\"a\": [1.0, \"a\"]}, pa.ArrowInvalid),\n        ],\n    )\n    def test_mixed_types_fails(self, data, error_type):\n        df = pd.DataFrame(data)\n        msg = \"Conversion failed for column a with type object\"\n        with pytest.raises(error_type, match=msg):\n            pa.Table.from_pandas(df)\n\n    def test_strided_data_import(self):\n        cases = []\n\n        columns = ['a', 'b', 'c']\n        N, K = 100, 3\n        random_numbers = np.random.randn(N, K).copy() * 100\n\n        numeric_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8',\n                          'f4', 'f8']\n\n        for type_name in numeric_dtypes:\n            # Casting np.float64 -> uint32 or uint64 throws a RuntimeWarning\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                cases.append(random_numbers.astype(type_name))\n\n        # strings\n        cases.append(np.array([random_ascii(10) for i in range(N * K)],\n                              dtype=object)\n                     .reshape(N, K).copy())\n\n        # booleans\n        boolean_objects = (np.array([True, False, True] * N, dtype=object)\n                           .reshape(N, K).copy())\n\n        # add some nulls, so dtype comes back as objects\n        boolean_objects[5] = None\n        cases.append(boolean_objects)\n\n        cases.append(np.arange(\"2016-01-01T00:00:00.001\", N * K,\n                               dtype='datetime64[ms]')\n                     .reshape(N, K).copy())\n\n        strided_mask = (random_numbers > 0).astype(bool)[:, 0]\n\n        for case in cases:\n            df = pd.DataFrame(case, columns=columns)\n            col = df['a']\n\n            _check_pandas_roundtrip(df)\n            _check_array_roundtrip(col)\n            _check_array_roundtrip(col, mask=strided_mask)\n\n    def test_all_nones(self):\n        def _check_series(s):\n            converted = pa.array(s)\n            assert isinstance(converted, pa.NullArray)\n            assert len(converted) == 3\n            assert converted.null_count == 3\n            for item in converted:\n                assert item is pa.NA\n\n        _check_series(pd.Series([None] * 3, dtype=object))\n        _check_series(pd.Series([np.nan] * 3, dtype=object))\n        _check_series(pd.Series([None, np.nan, None], dtype=object))\n\n    def test_partial_schema(self):\n        data = OrderedDict([\n            ('a', [0, 1, 2, 3, 4]),\n            ('b', np.array([-10, -5, 0, 5, 10], dtype=np.int32)),\n            ('c', [-10, -5, 0, 5, 10])\n        ])\n        df = pd.DataFrame(data)\n\n        partial_schema = pa.schema([\n            pa.field('c', pa.int64()),\n            pa.field('a', pa.int64())\n        ])\n\n        _check_pandas_roundtrip(df, schema=partial_schema,\n                                expected=df[['c', 'a']],\n                                expected_schema=partial_schema)\n\n    def test_table_batch_empty_dataframe(self):\n        df = pd.DataFrame({})\n        _check_pandas_roundtrip(df, preserve_index=None)\n        _check_pandas_roundtrip(df, preserve_index=None, as_batch=True)\n\n        expected = pd.DataFrame(columns=pd.Index([]))\n        _check_pandas_roundtrip(df, expected, preserve_index=False)\n        _check_pandas_roundtrip(df, expected, preserve_index=False, as_batch=True)\n\n        df2 = pd.DataFrame({}, index=[0, 1, 2])\n        _check_pandas_roundtrip(df2, preserve_index=True)\n        _check_pandas_roundtrip(df2, as_batch=True, preserve_index=True)\n\n    def test_convert_empty_table(self):\n        arr = pa.array([], type=pa.int64())\n        empty_objects = pd.Series(np.array([], dtype=object))\n        tm.assert_series_equal(arr.to_pandas(),\n                               pd.Series(np.array([], dtype=np.int64)))\n        arr = pa.array([], type=pa.string())\n        tm.assert_series_equal(arr.to_pandas(), empty_objects)\n        arr = pa.array([], type=pa.list_(pa.int64()))\n        tm.assert_series_equal(arr.to_pandas(), empty_objects)\n        arr = pa.array([], type=pa.struct([pa.field('a', pa.int64())]))\n        tm.assert_series_equal(arr.to_pandas(), empty_objects)\n\n    def test_non_natural_stride(self):\n        \"\"\"\n        ARROW-2172: converting from a Numpy array with a stride that's\n        not a multiple of itemsize.\n        \"\"\"\n        dtype = np.dtype([('x', np.int32), ('y', np.int16)])\n        data = np.array([(42, -1), (-43, 2)], dtype=dtype)\n        assert data.strides == (6,)\n        arr = pa.array(data['x'], type=pa.int32())\n        assert arr.to_pylist() == [42, -43]\n        arr = pa.array(data['y'], type=pa.int16())\n        assert arr.to_pylist() == [-1, 2]\n\n    def test_array_from_strided_numpy_array(self):\n        # ARROW-5651\n        np_arr = np.arange(0, 10, dtype=np.float32)[1:-1:2]\n        pa_arr = pa.array(np_arr, type=pa.float64())\n        expected = pa.array([1.0, 3.0, 5.0, 7.0], type=pa.float64())\n        pa_arr.equals(expected)\n\n    def test_safe_unsafe_casts(self):\n        # ARROW-2799\n        df = pd.DataFrame({\n            'A': list('abc'),\n            'B': np.linspace(0, 1, 3)\n        })\n\n        schema = pa.schema([\n            pa.field('A', pa.string()),\n            pa.field('B', pa.int32())\n        ])\n\n        with pytest.raises(ValueError):\n            pa.Table.from_pandas(df, schema=schema)\n\n        table = pa.Table.from_pandas(df, schema=schema, safe=False)\n        assert table.column('B').type == pa.int32()\n\n    def test_error_sparse(self):\n        # ARROW-2818\n        try:\n            df = pd.DataFrame({'a': pd.arrays.SparseArray([1, np.nan, 3])})\n        except AttributeError:\n            # pandas.arrays module introduced in pandas 0.24\n            df = pd.DataFrame({'a': pd.SparseArray([1, np.nan, 3])})\n        with pytest.raises(TypeError, match=\"Sparse pandas data\"):\n            pa.Table.from_pandas(df)\n\n\ndef test_safe_cast_from_float_with_nans_to_int():\n    # TODO(kszucs): write tests for creating Date32 and Date64 arrays, see\n    #               ARROW-4258 and https://github.com/apache/arrow/pull/3395\n    values = pd.Series([1, 2, None, 4])\n    arr = pa.Array.from_pandas(values, type=pa.int32(), safe=True)\n    expected = pa.array([1, 2, None, 4], type=pa.int32())\n    assert arr.equals(expected)\n\n\ndef _fully_loaded_dataframe_example():\n    index = pd.MultiIndex.from_arrays([\n        pd.date_range('2000-01-01', periods=5).repeat(2),\n        np.tile(np.array(['foo', 'bar'], dtype=object), 5)\n    ])\n\n    c1 = pd.date_range('2000-01-01', periods=10)\n    data = {\n        0: c1,\n        1: c1.tz_localize('utc'),\n        2: c1.tz_localize('US/Eastern'),\n        3: c1[::2].tz_localize('utc').repeat(2).astype('category'),\n        4: ['foo', 'bar'] * 5,\n        5: pd.Series(['foo', 'bar'] * 5).astype('category').values,\n        6: [True, False] * 5,\n        7: np.random.randn(10),\n        8: np.random.randint(0, 100, size=10),\n        9: pd.period_range('2013', periods=10, freq='M'),\n        10: pd.interval_range(start=1, freq=1, periods=10),\n    }\n    return pd.DataFrame(data, index=index)\n\n\n@pytest.mark.parametrize('columns', ([b'foo'], ['foo']))\ndef test_roundtrip_with_bytes_unicode(columns):\n    if Version(\"2.0.0\") <= Version(pd.__version__) < Version(\"3.0.0\"):\n        # TODO: regression in pandas, hopefully fixed in next version\n        # https://issues.apache.org/jira/browse/ARROW-18394\n        # https://github.com/pandas-dev/pandas/issues/50127\n        pytest.skip(\"Regression in pandas 2.0.0\")\n\n    df = pd.DataFrame(columns=columns)\n    table1 = pa.Table.from_pandas(df)\n    table2 = pa.Table.from_pandas(table1.to_pandas())\n    assert table1.equals(table2)\n    assert table1.schema.equals(table2.schema)\n    assert table1.schema.metadata == table2.schema.metadata\n\n\ndef _pytime_from_micros(val):\n    microseconds = val % 1000000\n    val //= 1000000\n    seconds = val % 60\n    val //= 60\n    minutes = val % 60\n    hours = val // 60\n    return time(hours, minutes, seconds, microseconds)\n\n\ndef _pytime_to_micros(pytime):\n    return (pytime.hour * 3600000000 +\n            pytime.minute * 60000000 +\n            pytime.second * 1000000 +\n            pytime.microsecond)\n\n\ndef test_convert_unsupported_type_error_message():\n    # ARROW-1454\n\n    # custom python objects\n    class A:\n        pass\n\n    df = pd.DataFrame({'a': [A(), A()]})\n\n    msg = 'Conversion failed for column a with type object'\n    with pytest.raises(ValueError, match=msg):\n        pa.Table.from_pandas(df)\n\n\n# ----------------------------------------------------------------------\n# Hypothesis tests\n\n\n@h.given(past.arrays(past.pandas_compatible_types))\ndef test_array_to_pandas_roundtrip(arr):\n    s = arr.to_pandas()\n    restored = pa.array(s, type=arr.type, from_pandas=True)\n    assert restored.equals(arr)\n\n\n# ----------------------------------------------------------------------\n# Test object deduplication in to_pandas\n\n\ndef _generate_dedup_example(nunique, repeats):\n    unique_values = [rands(10) for i in range(nunique)]\n    return unique_values * repeats\n\n\ndef _assert_nunique(obj, expected):\n    assert len({id(x) for x in obj}) == expected\n\n\ndef test_to_pandas_deduplicate_strings_array_types():\n    nunique = 100\n    repeats = 10\n    values = _generate_dedup_example(nunique, repeats)\n\n    for arr in [pa.array(values, type=pa.binary()),\n                pa.array(values, type=pa.utf8()),\n                pa.chunked_array([values, values])]:\n        _assert_nunique(arr.to_pandas(), nunique)\n        _assert_nunique(arr.to_pandas(deduplicate_objects=False), len(arr))\n\n\ndef test_to_pandas_deduplicate_strings_table_types():\n    nunique = 100\n    repeats = 10\n    values = _generate_dedup_example(nunique, repeats)\n\n    arr = pa.array(values)\n    rb = pa.RecordBatch.from_arrays([arr], ['foo'])\n    tbl = pa.Table.from_batches([rb])\n\n    for obj in [rb, tbl]:\n        _assert_nunique(obj.to_pandas()['foo'], nunique)\n        _assert_nunique(obj.to_pandas(deduplicate_objects=False)['foo'],\n                        len(obj))\n\n\ndef test_to_pandas_deduplicate_integers_as_objects():\n    nunique = 100\n    repeats = 10\n\n    # Python automatically interns smaller integers\n    unique_values = list(np.random.randint(10000000, 1000000000, size=nunique))\n    unique_values[nunique // 2] = None\n\n    arr = pa.array(unique_values * repeats)\n\n    _assert_nunique(arr.to_pandas(integer_object_nulls=True), nunique)\n    _assert_nunique(arr.to_pandas(integer_object_nulls=True,\n                                  deduplicate_objects=False),\n                    # Account for None\n                    (nunique - 1) * repeats + 1)\n\n\ndef test_to_pandas_deduplicate_date_time():\n    nunique = 100\n    repeats = 10\n\n    unique_values = list(range(nunique))\n\n    cases = [\n        # raw type, array type, to_pandas options\n        ('int32', 'date32', {'date_as_object': True}),\n        ('int64', 'date64', {'date_as_object': True}),\n        ('int32', 'time32[ms]', {}),\n        ('int64', 'time64[us]', {})\n    ]\n\n    for raw_type, array_type, pandas_options in cases:\n        raw_arr = pa.array(unique_values * repeats, type=raw_type)\n        casted_arr = raw_arr.cast(array_type)\n\n        _assert_nunique(casted_arr.to_pandas(**pandas_options),\n                        nunique)\n        _assert_nunique(casted_arr.to_pandas(deduplicate_objects=False,\n                                             **pandas_options),\n                        len(casted_arr))\n\n\n# ---------------------------------------------------------------------\n\ndef test_table_from_pandas_checks_field_nullability():\n    # ARROW-2136\n    df = pd.DataFrame({'a': [1.2, 2.1, 3.1],\n                       'b': [np.nan, 'string', 'foo']})\n    schema = pa.schema([pa.field('a', pa.float64(), nullable=False),\n                        pa.field('b', pa.utf8(), nullable=False)])\n\n    with pytest.raises(ValueError):\n        pa.Table.from_pandas(df, schema=schema)\n\n\ndef test_table_from_pandas_keeps_column_order_of_dataframe():\n    df1 = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n    df2 = df1[['floats', 'partition', 'arrays']]\n\n    schema1 = pa.schema([\n        ('partition', pa.int64()),\n        ('arrays', pa.list_(pa.int64())),\n        ('floats', pa.float64()),\n    ])\n    schema2 = pa.schema([\n        ('floats', pa.float64()),\n        ('partition', pa.int64()),\n        ('arrays', pa.list_(pa.int64()))\n    ])\n\n    table1 = pa.Table.from_pandas(df1, preserve_index=False)\n    table2 = pa.Table.from_pandas(df2, preserve_index=False)\n\n    assert table1.schema.equals(schema1)\n    assert table2.schema.equals(schema2)\n\n\ndef test_table_from_pandas_keeps_column_order_of_schema():\n    # ARROW-3766\n    df = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n\n    schema = pa.schema([\n        ('floats', pa.float64()),\n        ('arrays', pa.list_(pa.int32())),\n        ('partition', pa.int32())\n    ])\n\n    df1 = df[df.partition == 0]\n    df2 = df[df.partition == 1][['floats', 'partition', 'arrays']]\n\n    table1 = pa.Table.from_pandas(df1, schema=schema, preserve_index=False)\n    table2 = pa.Table.from_pandas(df2, schema=schema, preserve_index=False)\n\n    assert table1.schema.equals(schema)\n    assert table1.schema.equals(table2.schema)\n\n\ndef test_table_from_pandas_columns_argument_only_does_filtering():\n    df = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n\n    columns1 = ['arrays', 'floats', 'partition']\n    schema1 = pa.schema([\n        ('arrays', pa.list_(pa.int64())),\n        ('floats', pa.float64()),\n        ('partition', pa.int64())\n    ])\n\n    columns2 = ['floats', 'partition']\n    schema2 = pa.schema([\n        ('floats', pa.float64()),\n        ('partition', pa.int64())\n    ])\n\n    table1 = pa.Table.from_pandas(df, columns=columns1, preserve_index=False)\n    table2 = pa.Table.from_pandas(df, columns=columns2, preserve_index=False)\n\n    assert table1.schema.equals(schema1)\n    assert table2.schema.equals(schema2)\n\n\ndef test_table_from_pandas_columns_and_schema_are_mutually_exclusive():\n    df = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n    schema = pa.schema([\n        ('partition', pa.int32()),\n        ('arrays', pa.list_(pa.int32())),\n        ('floats', pa.float64()),\n    ])\n    columns = ['arrays', 'floats']\n\n    with pytest.raises(ValueError):\n        pa.Table.from_pandas(df, schema=schema, columns=columns)\n\n\ndef test_table_from_pandas_keeps_schema_nullability():\n    # ARROW-5169\n    df = pd.DataFrame({'a': [1, 2, 3, 4]})\n\n    schema = pa.schema([\n        pa.field('a', pa.int64(), nullable=False),\n    ])\n\n    table = pa.Table.from_pandas(df)\n    assert table.schema.field('a').nullable is True\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.field('a').nullable is False\n\n\ndef test_table_from_pandas_schema_index_columns():\n    # ARROW-5220\n    df = pd.DataFrame({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3]})\n\n    schema = pa.schema([\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n        ('index', pa.int64()),\n    ])\n\n    # schema includes index with name not in dataframe\n    with pytest.raises(KeyError, match=\"name 'index' present in the\"):\n        pa.Table.from_pandas(df, schema=schema)\n\n    df.index.name = 'index'\n\n    # schema includes correct index name -> roundtrip works\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n\n    # schema includes correct index name but preserve_index=False\n    with pytest.raises(ValueError, match=\"'preserve_index=False' was\"):\n        pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n\n    # in case of preserve_index=None -> RangeIndex serialized as metadata\n    # clashes with the index in the schema\n    with pytest.raises(ValueError, match=\"name 'index' is present in the \"\n                                         \"schema, but it is a RangeIndex\"):\n        pa.Table.from_pandas(df, schema=schema, preserve_index=None)\n\n    df.index = pd.Index([0, 1, 2], name='index')\n\n    # for non-RangeIndex, both preserve_index=None and True work\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n\n    # schema has different order (index column not at the end)\n    schema = pa.schema([\n        ('index', pa.int64()),\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n\n    # schema does not include the index -> index is not included as column\n    # even though preserve_index=True/None\n    schema = pa.schema([\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    expected = df.copy()\n    expected = expected.reset_index(drop=True)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema, expected=expected)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema, expected=expected)\n\n    # dataframe with a MultiIndex\n    df.index = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1)],\n                                         names=['level1', 'level2'])\n    schema = pa.schema([\n        ('level1', pa.string()),\n        ('level2', pa.int64()),\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema)\n\n    # only one of the levels of the MultiIndex is included\n    schema = pa.schema([\n        ('level2', pa.int64()),\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    expected = df.copy()\n    expected = expected.reset_index('level1', drop=True)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema, expected=expected)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema, expected=expected)\n\n\ndef test_table_from_pandas_schema_index_columns__unnamed_index():\n    # ARROW-6999 - unnamed indices in specified schema\n    df = pd.DataFrame({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3]})\n\n    expected_schema = pa.schema([\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n        ('__index_level_0__', pa.int64()),\n    ])\n\n    schema = pa.Schema.from_pandas(df, preserve_index=True)\n    table = pa.Table.from_pandas(df, preserve_index=True, schema=schema)\n    assert table.schema.remove_metadata().equals(expected_schema)\n\n    # non-RangeIndex (preserved by default)\n    df = pd.DataFrame({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3]}, index=[0, 1, 2])\n    schema = pa.Schema.from_pandas(df)\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.remove_metadata().equals(expected_schema)\n\n\ndef test_table_from_pandas_schema_with_custom_metadata():\n    # ARROW-7087 - metadata disappear from pandas\n    df = pd.DataFrame()\n    schema = pa.Schema.from_pandas(df).with_metadata({'meta': 'True'})\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.metadata.get(b'meta') == b'True'\n\n\ndef test_table_from_pandas_schema_field_order_metadata():\n    # ARROW-10532\n    # ensure that a different field order in specified schema doesn't\n    # mangle metadata\n    df = pd.DataFrame({\n        \"datetime\": pd.date_range(\"2020-01-01T00:00:00Z\", freq=\"h\", periods=2),\n        \"float\": np.random.randn(2)\n    })\n\n    schema = pa.schema([\n        pa.field(\"float\", pa.float32(), nullable=True),\n        pa.field(\"datetime\", pa.timestamp(\"s\", tz=\"UTC\"), nullable=False)\n    ])\n\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.equals(schema)\n    metadata_float = table.schema.pandas_metadata[\"columns\"][0]\n    assert metadata_float[\"name\"] == \"float\"\n    assert metadata_float[\"metadata\"] is None\n    metadata_datetime = table.schema.pandas_metadata[\"columns\"][1]\n    assert metadata_datetime[\"name\"] == \"datetime\"\n    assert metadata_datetime[\"metadata\"] == {'timezone': 'UTC'}\n\n    result = table.to_pandas()\n    coerce_cols_to_types = {\"float\": \"float32\"}\n    if Version(pd.__version__) >= Version(\"2.0.0\"):\n        # Pandas v2 now support non-nanosecond time units\n        coerce_cols_to_types[\"datetime\"] = \"datetime64[s, UTC]\"\n    expected = df[[\"float\", \"datetime\"]].astype(coerce_cols_to_types)\n\n    tm.assert_frame_equal(result, expected)\n\n\n# ----------------------------------------------------------------------\n# RecordBatch, Table\n\n\ndef test_recordbatch_from_to_pandas():\n    data = pd.DataFrame({\n        'c1': np.array([1, 2, 3, 4, 5], dtype='int64'),\n        'c2': np.array([1, 2, 3, 4, 5], dtype='uint32'),\n        'c3': np.random.randn(5),\n        'c4': ['foo', 'bar', None, 'baz', 'qux'],\n        'c5': [False, True, False, True, False]\n    })\n\n    batch = pa.RecordBatch.from_pandas(data)\n    result = batch.to_pandas()\n    tm.assert_frame_equal(data, result)\n\n\ndef test_recordbatchlist_to_pandas():\n    data1 = pd.DataFrame({\n        'c1': np.array([1, 1, 2], dtype='uint32'),\n        'c2': np.array([1.0, 2.0, 3.0], dtype='float64'),\n        'c3': [True, None, False],\n        'c4': ['foo', 'bar', None]\n    })\n\n    data2 = pd.DataFrame({\n        'c1': np.array([3, 5], dtype='uint32'),\n        'c2': np.array([4.0, 5.0], dtype='float64'),\n        'c3': [True, True],\n        'c4': ['baz', 'qux']\n    })\n\n    batch1 = pa.RecordBatch.from_pandas(data1)\n    batch2 = pa.RecordBatch.from_pandas(data2)\n\n    table = pa.Table.from_batches([batch1, batch2])\n    result = table.to_pandas()\n    data = pd.concat([data1, data2]).reset_index(drop=True)\n    tm.assert_frame_equal(data, result)\n\n\ndef test_recordbatch_table_pass_name_to_pandas():\n    rb = pa.record_batch([pa.array([1, 2, 3, 4])], names=['a0'])\n    t = pa.table([pa.array([1, 2, 3, 4])], names=['a0'])\n    assert rb[0].to_pandas().name == 'a0'\n    assert t[0].to_pandas().name == 'a0'\n\n\n# ----------------------------------------------------------------------\n# Metadata serialization\n\n\n@pytest.mark.parametrize(\n    ('type', 'expected'),\n    [\n        (pa.null(), 'empty'),\n        (pa.bool_(), 'bool'),\n        (pa.int8(), 'int8'),\n        (pa.int16(), 'int16'),\n        (pa.int32(), 'int32'),\n        (pa.int64(), 'int64'),\n        (pa.uint8(), 'uint8'),\n        (pa.uint16(), 'uint16'),\n        (pa.uint32(), 'uint32'),\n        (pa.uint64(), 'uint64'),\n        (pa.float16(), 'float16'),\n        (pa.float32(), 'float32'),\n        (pa.float64(), 'float64'),\n        (pa.date32(), 'date'),\n        (pa.date64(), 'date'),\n        (pa.binary(), 'bytes'),\n        (pa.binary(length=4), 'bytes'),\n        (pa.string(), 'unicode'),\n        (pa.list_(pa.list_(pa.int16())), 'list[list[int16]]'),\n        (pa.decimal128(18, 3), 'decimal'),\n        (pa.timestamp('ms'), 'datetime'),\n        (pa.timestamp('us', 'UTC'), 'datetimetz'),\n        (pa.time32('s'), 'time'),\n        (pa.time64('us'), 'time')\n    ]\n)\ndef test_logical_type(type, expected):\n    assert get_logical_type(type) == expected\n\n\n# ----------------------------------------------------------------------\n# to_pandas uses MemoryPool\n\ndef test_array_uses_memory_pool():\n    # ARROW-6570\n    N = 10000\n    arr = pa.array(np.arange(N, dtype=np.int64),\n                   mask=np.random.randint(0, 2, size=N).astype(np.bool_))\n\n    # In the case the gc is caught loading\n    gc.collect()\n\n    prior_allocation = pa.total_allocated_bytes()\n\n    x = arr.to_pandas()\n    assert pa.total_allocated_bytes() == (prior_allocation + N * 8)\n    x = None  # noqa\n    gc.collect()\n\n    assert pa.total_allocated_bytes() == prior_allocation\n\n    # zero copy does not allocate memory\n    arr = pa.array(np.arange(N, dtype=np.int64))\n\n    prior_allocation = pa.total_allocated_bytes()\n    x = arr.to_pandas()  # noqa\n    assert pa.total_allocated_bytes() == prior_allocation\n\n\ndef test_singleton_blocks_zero_copy():\n    # Part of ARROW-3789\n    t = pa.table([pa.array(np.arange(1000, dtype=np.int64))], ['f0'])\n\n    # Zero copy if split_blocks=True\n    _check_to_pandas_memory_unchanged(t, split_blocks=True)\n\n    prior_allocation = pa.total_allocated_bytes()\n    result = t.to_pandas()\n    # access private `_values` because the public `values` is made read-only by pandas\n    assert result['f0']._values.flags.writeable\n    assert pa.total_allocated_bytes() > prior_allocation\n\n\ndef _check_to_pandas_memory_unchanged(obj, **kwargs):\n    prior_allocation = pa.total_allocated_bytes()\n    x = obj.to_pandas(**kwargs)  # noqa\n\n    # Memory allocation unchanged -- either zero copy or self-destructing\n    assert pa.total_allocated_bytes() == prior_allocation\n\n\ndef test_to_pandas_split_blocks():\n    # ARROW-3789\n    t = pa.table([\n        pa.array([1, 2, 3, 4, 5], type='i1'),\n        pa.array([1, 2, 3, 4, 5], type='i4'),\n        pa.array([1, 2, 3, 4, 5], type='i8'),\n        pa.array([1, 2, 3, 4, 5], type='f4'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n    ], ['f{}'.format(i) for i in range(8)])\n\n    _check_blocks_created(t, 8)\n    _check_to_pandas_memory_unchanged(t, split_blocks=True)\n\n\ndef _get_mgr(df):\n    if Version(pd.__version__) < Version(\"1.1.0\"):\n        return df._data\n    else:\n        return df._mgr\n\n\ndef _check_blocks_created(t, number):\n    x = t.to_pandas(split_blocks=True)\n    assert len(_get_mgr(x).blocks) == number\n\n\ndef test_to_pandas_self_destruct():\n    K = 50\n\n    def _make_table():\n        return pa.table([\n            # Slice to force a copy\n            pa.array(np.random.randn(10000)[::2])\n            for i in range(K)\n        ], ['f{}'.format(i) for i in range(K)])\n\n    t = _make_table()\n    _check_to_pandas_memory_unchanged(t, split_blocks=True, self_destruct=True)\n\n    # Check non-split-block behavior\n    t = _make_table()\n    _check_to_pandas_memory_unchanged(t, self_destruct=True)\n\n\ndef test_table_uses_memory_pool():\n    N = 10000\n    arr = pa.array(np.arange(N, dtype=np.int64))\n    t = pa.table([arr, arr, arr], ['f0', 'f1', 'f2'])\n\n    prior_allocation = pa.total_allocated_bytes()\n    x = t.to_pandas()\n\n    assert pa.total_allocated_bytes() == (prior_allocation + 3 * N * 8)\n\n    # Check successful garbage collection\n    x = None  # noqa\n    gc.collect()\n    assert pa.total_allocated_bytes() == prior_allocation\n\n\ndef test_object_leak_in_numpy_array():\n    # ARROW-6876\n    arr = pa.array([{'a': 1}])\n    np_arr = arr.to_pandas()\n    assert np_arr.dtype == np.dtype('object')\n    obj = np_arr[0]\n    refcount = sys.getrefcount(obj)\n    assert sys.getrefcount(obj) == refcount\n    del np_arr\n    assert sys.getrefcount(obj) == refcount - 1\n\n\ndef test_object_leak_in_dataframe():\n    # ARROW-6876\n    arr = pa.array([{'a': 1}])\n    table = pa.table([arr], ['f0'])\n    col = table.to_pandas()['f0']\n    assert col.dtype == np.dtype('object')\n    obj = col[0]\n    refcount = sys.getrefcount(obj)\n    assert sys.getrefcount(obj) == refcount\n    del col\n    assert sys.getrefcount(obj) == refcount - 1\n\n\n# ----------------------------------------------------------------------\n# Some nested array tests array tests\n\n\ndef test_array_from_py_float32():\n    data = [[1.2, 3.4], [9.0, 42.0]]\n\n    t = pa.float32()\n\n    arr1 = pa.array(data[0], type=t)\n    arr2 = pa.array(data, type=pa.list_(t))\n\n    expected1 = np.array(data[0], dtype=np.float32)\n    expected2 = pd.Series([np.array(data[0], dtype=np.float32),\n                           np.array(data[1], dtype=np.float32)])\n\n    assert arr1.type == t\n    assert arr1.equals(pa.array(expected1))\n    assert arr2.equals(pa.array(expected2))\n\n\n# ----------------------------------------------------------------------\n# Timestamp tests\n\n\ndef test_cast_timestamp_unit():\n    # ARROW-1680\n    val = datetime.now()\n    s = pd.Series([val])\n    s_nyc = s.dt.tz_localize('tzlocal()').dt.tz_convert('America/New_York')\n\n    us_with_tz = pa.timestamp('us', tz='America/New_York')\n\n    arr = pa.Array.from_pandas(s_nyc, type=us_with_tz)\n\n    # ARROW-1906\n    assert arr.type == us_with_tz\n\n    arr2 = pa.Array.from_pandas(s, type=pa.timestamp('us'))\n\n    assert arr[0].as_py() == s_nyc[0].to_pydatetime()\n    assert arr2[0].as_py() == s[0].to_pydatetime()\n\n    # Disallow truncation\n    arr = pa.array([123123], type='int64').cast(pa.timestamp('ms'))\n    expected = pa.array([123], type='int64').cast(pa.timestamp('s'))\n\n    # sanity check that the cast worked right\n    assert arr.type == pa.timestamp('ms')\n\n    target = pa.timestamp('s')\n    with pytest.raises(ValueError):\n        arr.cast(target)\n\n    result = arr.cast(target, safe=False)\n    assert result.equals(expected)\n\n    # ARROW-1949\n    series = pd.Series([pd.Timestamp(1), pd.Timestamp(10), pd.Timestamp(1000)])\n    expected = pa.array([0, 0, 1], type=pa.timestamp('us'))\n\n    with pytest.raises(ValueError):\n        pa.array(series, type=pa.timestamp('us'))\n\n    with pytest.raises(ValueError):\n        pa.Array.from_pandas(series, type=pa.timestamp('us'))\n\n    result = pa.Array.from_pandas(series, type=pa.timestamp('us'), safe=False)\n    assert result.equals(expected)\n\n    result = pa.array(series, type=pa.timestamp('us'), safe=False)\n    assert result.equals(expected)\n\n\ndef test_nested_with_timestamp_tz_round_trip():\n    ts = pd.Timestamp.now()\n    ts_dt = ts.to_pydatetime()\n    arr = pa.array([ts_dt], type=pa.timestamp('us', tz='America/New_York'))\n    struct = pa.StructArray.from_arrays([arr, arr], ['start', 'stop'])\n\n    result = struct.to_pandas()\n    restored = pa.array(result)\n    assert restored.equals(struct)\n\n\ndef test_nested_with_timestamp_tz():\n    # ARROW-7723\n    ts = pd.Timestamp.now()\n    ts_dt = ts.to_pydatetime()\n\n    # XXX: Ensure that this data does not get promoted to nanoseconds (and thus\n    # integers) to preserve behavior in 0.15.1\n    for unit in ['s', 'ms', 'us']:\n        if unit in ['s', 'ms']:\n            # This is used for verifying timezone conversion to micros are not\n            # important\n            def truncate(x): return x.replace(microsecond=0)\n        else:\n            def truncate(x): return x\n        arr = pa.array([ts], type=pa.timestamp(unit))\n        arr2 = pa.array([ts], type=pa.timestamp(unit, tz='America/New_York'))\n\n        arr3 = pa.StructArray.from_arrays([arr, arr], ['start', 'stop'])\n        arr4 = pa.StructArray.from_arrays([arr2, arr2], ['start', 'stop'])\n\n        result = arr3.to_pandas()\n        assert isinstance(result[0]['start'], datetime)\n        assert result[0]['start'].tzinfo is None\n        assert isinstance(result[0]['stop'], datetime)\n        assert result[0]['stop'].tzinfo is None\n\n        result = arr4.to_pandas()\n        assert isinstance(result[0]['start'], datetime)\n        assert result[0]['start'].tzinfo is not None\n        utc_dt = result[0]['start'].astimezone(timezone.utc)\n        assert truncate(utc_dt).replace(tzinfo=None) == truncate(ts_dt)\n        assert isinstance(result[0]['stop'], datetime)\n        assert result[0]['stop'].tzinfo is not None\n\n        # same conversion for table\n        result = pa.table({'a': arr3}).to_pandas()\n        assert isinstance(result['a'][0]['start'], datetime)\n        assert result['a'][0]['start'].tzinfo is None\n        assert isinstance(result['a'][0]['stop'], datetime)\n        assert result['a'][0]['stop'].tzinfo is None\n\n        result = pa.table({'a': arr4}).to_pandas()\n        assert isinstance(result['a'][0]['start'], datetime)\n        assert result['a'][0]['start'].tzinfo is not None\n        assert isinstance(result['a'][0]['stop'], datetime)\n        assert result['a'][0]['stop'].tzinfo is not None\n\n\n# ----------------------------------------------------------------------\n# DictionaryArray tests\n\n\ndef test_dictionary_with_pandas():\n    src_indices = np.repeat([0, 1, 2], 2)\n    dictionary = np.array(['foo', 'bar', 'baz'], dtype=object)\n    mask = np.array([False, False, True, False, False, False])\n\n    for index_type in ['uint8', 'int8', 'uint16', 'int16', 'uint32', 'int32',\n                       'uint64', 'int64']:\n        indices = src_indices.astype(index_type)\n        d1 = pa.DictionaryArray.from_arrays(indices, dictionary)\n        d2 = pa.DictionaryArray.from_arrays(indices, dictionary, mask=mask)\n\n        if index_type[0] == 'u':\n            # TODO: unsigned dictionary indices to pandas\n            with pytest.raises(TypeError):\n                d1.to_pandas()\n            continue\n\n        pandas1 = d1.to_pandas()\n        ex_pandas1 = pd.Categorical.from_codes(indices, categories=dictionary)\n\n        tm.assert_series_equal(pd.Series(pandas1), pd.Series(ex_pandas1))\n\n        pandas2 = d2.to_pandas()\n        assert pandas2.isnull().sum() == 1\n\n        # Unsigned integers converted to signed\n        signed_indices = indices\n        if index_type[0] == 'u':\n            signed_indices = indices.astype(index_type[1:])\n        ex_pandas2 = pd.Categorical.from_codes(np.where(mask, -1,\n                                                        signed_indices),\n                                               categories=dictionary)\n\n        tm.assert_series_equal(pd.Series(pandas2), pd.Series(ex_pandas2))\n\n\ndef random_strings(n, item_size, pct_null=0, dictionary=None):\n    if dictionary is not None:\n        result = dictionary[np.random.randint(0, len(dictionary), size=n)]\n    else:\n        result = np.array([random_ascii(item_size) for i in range(n)],\n                          dtype=object)\n\n    if pct_null > 0:\n        result[np.random.rand(n) < pct_null] = None\n\n    return result\n\n\ndef test_variable_dictionary_to_pandas():\n    np.random.seed(12345)\n\n    d1 = pa.array(random_strings(100, 32), type='string')\n    d2 = pa.array(random_strings(100, 16), type='string')\n    d3 = pa.array(random_strings(10000, 10), type='string')\n\n    a1 = pa.DictionaryArray.from_arrays(\n        np.random.randint(0, len(d1), size=1000, dtype='i4'),\n        d1\n    )\n    a2 = pa.DictionaryArray.from_arrays(\n        np.random.randint(0, len(d2), size=1000, dtype='i4'),\n        d2\n    )\n\n    # With some nulls\n    a3 = pa.DictionaryArray.from_arrays(\n        np.random.randint(0, len(d3), size=1000, dtype='i4'), d3)\n\n    i4 = pa.array(\n        np.random.randint(0, len(d3), size=1000, dtype='i4'),\n        mask=np.random.rand(1000) < 0.1\n    )\n    a4 = pa.DictionaryArray.from_arrays(i4, d3)\n\n    expected_dict = pa.concat_arrays([d1, d2, d3])\n\n    a = pa.chunked_array([a1, a2, a3, a4])\n    a_dense = pa.chunked_array([a1.cast('string'),\n                                a2.cast('string'),\n                                a3.cast('string'),\n                                a4.cast('string')])\n\n    result = a.to_pandas()\n    result_dense = a_dense.to_pandas()\n\n    assert (result.cat.categories == expected_dict.to_pandas()).all()\n\n    expected_dense = result.astype('str')\n    expected_dense[result_dense.isnull()] = None\n    tm.assert_series_equal(result_dense, expected_dense)\n\n\ndef test_dictionary_encoded_nested_to_pandas():\n    # ARROW-6899\n    child = pa.array(['a', 'a', 'a', 'b', 'b']).dictionary_encode()\n\n    arr = pa.ListArray.from_arrays([0, 3, 5], child)\n\n    result = arr.to_pandas()\n    expected = pd.Series([np.array(['a', 'a', 'a'], dtype=object),\n                          np.array(['b', 'b'], dtype=object)])\n\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dictionary_from_pandas():\n    cat = pd.Categorical(['a', 'b', 'a'])\n    expected_type = pa.dictionary(pa.int8(), pa.string())\n\n    result = pa.array(cat)\n    assert result.to_pylist() == ['a', 'b', 'a']\n    assert result.type.equals(expected_type)\n\n    # with missing values in categorical\n    cat = pd.Categorical(['a', 'b', None, 'a'])\n\n    result = pa.array(cat)\n    assert result.to_pylist() == ['a', 'b', None, 'a']\n    assert result.type.equals(expected_type)\n\n    # with additional mask\n    result = pa.array(cat, mask=np.array([False, False, False, True]))\n    assert result.to_pylist() == ['a', 'b', None, None]\n    assert result.type.equals(expected_type)\n\n\ndef test_dictionary_from_pandas_specified_type():\n    # ARROW-7168 - ensure specified type is always respected\n\n    # the same as cat = pd.Categorical(['a', 'b']) but explicit about dtypes\n    cat = pd.Categorical.from_codes(\n        np.array([0, 1], dtype='int8'), np.array(['a', 'b'], dtype=object))\n\n    # different index type -> allow this\n    # (the type of the 'codes' in pandas is not part of the data type)\n    typ = pa.dictionary(index_type=pa.int16(), value_type=pa.string())\n    result = pa.array(cat, type=typ)\n    assert result.type.equals(typ)\n    assert result.to_pylist() == ['a', 'b']\n\n    # mismatching values type -> raise error\n    typ = pa.dictionary(index_type=pa.int8(), value_type=pa.int64())\n    with pytest.raises(pa.ArrowInvalid):\n        result = pa.array(cat, type=typ)\n\n    # mismatching order -> raise error\n    typ = pa.dictionary(\n        index_type=pa.int8(), value_type=pa.string(), ordered=True)\n    msg = \"The 'ordered' flag of the passed categorical values \"\n    with pytest.raises(ValueError, match=msg):\n        result = pa.array(cat, type=typ)\n    assert result.to_pylist() == ['a', 'b']\n\n    # with mask\n    typ = pa.dictionary(index_type=pa.int16(), value_type=pa.string())\n    result = pa.array(cat, type=typ, mask=np.array([False, True]))\n    assert result.type.equals(typ)\n    assert result.to_pylist() == ['a', None]\n\n    # empty categorical -> be flexible in values type to allow\n    cat = pd.Categorical([])\n\n    typ = pa.dictionary(index_type=pa.int8(), value_type=pa.string())\n    result = pa.array(cat, type=typ)\n    assert result.type.equals(typ)\n    assert result.to_pylist() == []\n    typ = pa.dictionary(index_type=pa.int8(), value_type=pa.int64())\n    result = pa.array(cat, type=typ)\n    assert result.type.equals(typ)\n    assert result.to_pylist() == []\n\n    # passing non-dictionary type\n    cat = pd.Categorical(['a', 'b'])\n    result = pa.array(cat, type=pa.string())\n    expected = pa.array(['a', 'b'], type=pa.string())\n    assert result.equals(expected)\n    assert result.to_pylist() == ['a', 'b']\n\n\ndef test_convert_categories_to_array_with_string_pyarrow_dtype():\n    # gh-33727: categories should be converted to pa.Array\n    if Version(pd.__version__) < Version(\"1.3.0\"):\n        pytest.skip(\"PyArrow backed string data type introduced in pandas 1.3.0\")\n\n    df = pd.DataFrame({\"x\": [\"foo\", \"bar\", \"foo\"]}, dtype=\"string[pyarrow]\")\n    df = df.astype(\"category\")\n    indices = pa.array(df['x'].cat.codes)\n    dictionary = pa.array(df[\"x\"].cat.categories.values)\n    assert isinstance(dictionary, pa.Array)\n\n    expected = pa.Array.from_pandas(df['x'])\n    result = pa.DictionaryArray.from_arrays(indices, dictionary)\n    assert result == expected\n\n\n# ----------------------------------------------------------------------\n# Array protocol in pandas conversions tests\n\n\ndef test_array_protocol():\n    df = pd.DataFrame({'a': pd.Series([1, 2, None], dtype='Int64')})\n\n    # __arrow_array__ added to pandas IntegerArray in 0.26.0.dev\n\n    # default conversion\n    result = pa.table(df)\n    expected = pa.array([1, 2, None], pa.int64())\n    assert result[0].chunk(0).equals(expected)\n\n    # with specifying schema\n    schema = pa.schema([('a', pa.float64())])\n    result = pa.table(df, schema=schema)\n    expected2 = pa.array([1, 2, None], pa.float64())\n    assert result[0].chunk(0).equals(expected2)\n\n    # pass Series to pa.array\n    result = pa.array(df['a'])\n    assert result.equals(expected)\n    result = pa.array(df['a'], type=pa.float64())\n    assert result.equals(expected2)\n\n    # pass actual ExtensionArray to pa.array\n    result = pa.array(df['a'].values)\n    assert result.equals(expected)\n    result = pa.array(df['a'].values, type=pa.float64())\n    assert result.equals(expected2)\n\n\nclass DummyExtensionType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.int64(),\n                         'pyarrow.tests.test_pandas.DummyExtensionType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        assert storage_type == pa.int64()\n        return cls()\n\n\ndef PandasArray__arrow_array__(self, type=None):\n    # hardcode dummy return regardless of self - we only want to check that\n    # this method is correctly called\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    return pa.ExtensionArray.from_storage(DummyExtensionType(), storage)\n\n\ndef test_array_protocol_pandas_extension_types(monkeypatch):\n    # ARROW-7022 - ensure protocol works for Period / Interval extension dtypes\n\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    expected = pa.ExtensionArray.from_storage(DummyExtensionType(), storage)\n\n    monkeypatch.setattr(pd.arrays.PeriodArray, \"__arrow_array__\",\n                        PandasArray__arrow_array__, raising=False)\n    monkeypatch.setattr(pd.arrays.IntervalArray, \"__arrow_array__\",\n                        PandasArray__arrow_array__, raising=False)\n    for arr in [pd.period_range(\"2012-01-01\", periods=3, freq=\"D\").array,\n                pd.interval_range(1, 4).array]:\n        result = pa.array(arr)\n        assert result.equals(expected)\n        result = pa.array(pd.Series(arr))\n        assert result.equals(expected)\n        result = pa.array(pd.Index(arr))\n        assert result.equals(expected)\n        result = pa.table(pd.DataFrame({'a': arr})).column('a').chunk(0)\n        assert result.equals(expected)\n\n\n# ----------------------------------------------------------------------\n# Pandas ExtensionArray support\n\n\ndef _Int64Dtype__from_arrow__(self, array):\n    # for test only deal with single chunk for now\n    # TODO: do we require handling of chunked arrays in the protocol?\n    if isinstance(array, pa.Array):\n        arr = array\n    else:\n        # ChunkedArray - here only deal with a single chunk for the test\n        arr = array.chunk(0)\n    buflist = arr.buffers()\n    data = np.frombuffer(buflist[-1], dtype='int64')[\n        arr.offset:arr.offset + len(arr)]\n    bitmask = buflist[0]\n    if bitmask is not None:\n        mask = pa.BooleanArray.from_buffers(\n            pa.bool_(), len(arr), [None, bitmask])\n        mask = np.asarray(mask)\n    else:\n        mask = np.ones(len(arr), dtype=bool)\n    int_arr = pd.arrays.IntegerArray(data.copy(), ~mask, copy=False)\n    return int_arr\n\n\ndef test_convert_to_extension_array(monkeypatch):\n    # table converted from dataframe with extension types (so pandas_metadata\n    # has this information)\n    df = pd.DataFrame(\n        {'a': [1, 2, 3], 'b': pd.array([2, 3, 4], dtype='Int64'),\n         'c': [4, 5, 6]})\n    table = pa.table(df)\n\n    # Int64Dtype is recognized -> convert to extension block by default\n    # for a proper roundtrip\n    result = table.to_pandas()\n    assert _get_mgr(result).blocks[0].values.dtype == np.dtype(\"int64\")\n    assert _get_mgr(result).blocks[1].values.dtype == pd.Int64Dtype()\n    tm.assert_frame_equal(result, df)\n\n    # test with missing values\n    df2 = pd.DataFrame({'a': pd.array([1, 2, None], dtype='Int64')})\n    table2 = pa.table(df2)\n    result = table2.to_pandas()\n    assert _get_mgr(result).blocks[0].values.dtype == pd.Int64Dtype()\n    tm.assert_frame_equal(result, df2)\n\n    # monkeypatch pandas Int64Dtype to *not* have the protocol method\n    if Version(pd.__version__) < Version(\"1.3.0.dev\"):\n        monkeypatch.delattr(\n            pd.core.arrays.integer._IntegerDtype, \"__from_arrow__\")\n    else:\n        monkeypatch.delattr(\n            pd.core.arrays.integer.NumericDtype, \"__from_arrow__\")\n    # Int64Dtype has no __from_arrow__ -> use normal conversion\n    result = table.to_pandas()\n    assert len(_get_mgr(result).blocks) == 1\n    assert _get_mgr(result).blocks[0].values.dtype == np.dtype(\"int64\")\n\n\nclass MyCustomIntegerType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.int64(),\n                         'pyarrow.tests.test_pandas.MyCustomIntegerType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    def to_pandas_dtype(self):\n        return pd.Int64Dtype()\n\n\ndef test_conversion_extensiontype_to_extensionarray(monkeypatch):\n    # converting extension type to linked pandas ExtensionDtype/Array\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(MyCustomIntegerType(), storage)\n    table = pa.table({'a': arr})\n\n    # extension type points to Int64Dtype, which knows how to create a\n    # pandas ExtensionArray\n    result = arr.to_pandas()\n    assert _get_mgr(result).blocks[0].values.dtype == pd.Int64Dtype()\n    expected = pd.Series([1, 2, 3, 4], dtype='Int64')\n    tm.assert_series_equal(result, expected)\n\n    result = table.to_pandas()\n    assert _get_mgr(result).blocks[0].values.dtype == pd.Int64Dtype()\n    expected = pd.DataFrame({'a': pd.array([1, 2, 3, 4], dtype='Int64')})\n    tm.assert_frame_equal(result, expected)\n\n    # monkeypatch pandas Int64Dtype to *not* have the protocol method\n    # (remove the version added above and the actual version for recent pandas)\n    if Version(pd.__version__) < Version(\"1.3.0.dev\"):\n        monkeypatch.delattr(\n            pd.core.arrays.integer._IntegerDtype, \"__from_arrow__\")\n    else:\n        monkeypatch.delattr(\n            pd.core.arrays.integer.NumericDtype, \"__from_arrow__\")\n\n    result = arr.to_pandas()\n    assert _get_mgr(result).blocks[0].values.dtype == np.dtype(\"int64\")\n    expected = pd.Series([1, 2, 3, 4])\n    tm.assert_series_equal(result, expected)\n\n    with pytest.raises(ValueError):\n        table.to_pandas()\n\n\ndef test_to_pandas_extension_dtypes_mapping():\n    table = pa.table({'a': pa.array([1, 2, 3], pa.int64())})\n\n    # default use numpy dtype\n    result = table.to_pandas()\n    assert result['a'].dtype == np.dtype('int64')\n\n    # specify to override the default\n    result = table.to_pandas(types_mapper={pa.int64(): pd.Int64Dtype()}.get)\n    assert isinstance(result['a'].dtype, pd.Int64Dtype)\n\n    # types that return None in function get normal conversion\n    table = pa.table({'a': pa.array([1, 2, 3], pa.int32())})\n    result = table.to_pandas(types_mapper={pa.int64(): pd.Int64Dtype()}.get)\n    assert result['a'].dtype == np.dtype('int32')\n\n    # `types_mapper` overrules the pandas metadata\n    table = pa.table(pd.DataFrame({'a': pd.array([1, 2, 3], dtype=\"Int64\")}))\n    result = table.to_pandas()\n    assert isinstance(result['a'].dtype, pd.Int64Dtype)\n    result = table.to_pandas(\n        types_mapper={pa.int64(): pd.PeriodDtype('D')}.get)\n    assert isinstance(result['a'].dtype, pd.PeriodDtype)\n\n\ndef test_array_to_pandas():\n    if Version(pd.__version__) < Version(\"1.1\"):\n        pytest.skip(\"ExtensionDtype to_pandas method missing\")\n\n    for arr in [pd.period_range(\"2012-01-01\", periods=3, freq=\"D\").array,\n                pd.interval_range(1, 4).array]:\n        result = pa.array(arr).to_pandas()\n        expected = pd.Series(arr)\n        tm.assert_series_equal(result, expected)\n\n        result = pa.table({\"col\": arr})[\"col\"].to_pandas()\n        expected = pd.Series(arr, name=\"col\")\n        tm.assert_series_equal(result, expected)\n\n\ndef test_roundtrip_empty_table_with_extension_dtype_index():\n    df = pd.DataFrame(index=pd.interval_range(start=0, end=3))\n    table = pa.table(df)\n    if Version(pd.__version__) > Version(\"1.0\"):\n        tm.assert_index_equal(table.to_pandas().index, df.index)\n    else:\n        tm.assert_index_equal(table.to_pandas().index,\n                              pd.Index([{'left': 0, 'right': 1},\n                                        {'left': 1, 'right': 2},\n                                        {'left': 2, 'right': 3}],\n                                       dtype='object'))\n\n\n@pytest.mark.parametrize(\"index\", [\"a\", [\"a\", \"b\"]])\ndef test_to_pandas_types_mapper_index(index):\n    if Version(pd.__version__) < Version(\"1.5.0\"):\n        pytest.skip(\"ArrowDtype missing\")\n    df = pd.DataFrame(\n        {\n            \"a\": [1, 2],\n            \"b\": [3, 4],\n            \"c\": [5, 6],\n        },\n        dtype=pd.ArrowDtype(pa.int64()),\n    ).set_index(index)\n    expected = df.copy()\n    table = pa.table(df)\n    result = table.to_pandas(types_mapper=pd.ArrowDtype)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_array_to_pandas_types_mapper():\n    # https://issues.apache.org/jira/browse/ARROW-9664\n    if Version(pd.__version__) < Version(\"1.2.0\"):\n        pytest.skip(\"Float64Dtype extension dtype missing\")\n\n    data = pa.array([1, 2, 3], pa.int64())\n\n    # Test with mapper function\n    types_mapper = {pa.int64(): pd.Int64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == pd.Int64Dtype()\n\n    # Test mapper function returning None\n    types_mapper = {pa.int64(): None}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n    # Test mapper function not containing the dtype\n    types_mapper = {pa.float64(): pd.Float64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n\n@pytest.mark.pandas\ndef test_chunked_array_to_pandas_types_mapper():\n    # https://issues.apache.org/jira/browse/ARROW-9664\n    if Version(pd.__version__) < Version(\"1.2.0\"):\n        pytest.skip(\"Float64Dtype extension dtype missing\")\n\n    data = pa.chunked_array([pa.array([1, 2, 3], pa.int64())])\n    assert isinstance(data, pa.ChunkedArray)\n\n    # Test with mapper function\n    types_mapper = {pa.int64(): pd.Int64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == pd.Int64Dtype()\n\n    # Test mapper function returning None\n    types_mapper = {pa.int64(): None}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n    # Test mapper function not containing the dtype\n    types_mapper = {pa.float64(): pd.Float64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n\n# ----------------------------------------------------------------------\n# Legacy metadata compatibility tests\n\n\ndef test_metadata_compat_range_index_pre_0_12():\n    # Forward compatibility for metadata created from pandas.RangeIndex\n    # prior to pyarrow 0.13.0\n    a_values = ['foo', 'bar', None, 'baz']\n    b_values = ['a', 'a', 'b', 'b']\n    a_arrow = pa.array(a_values, type='utf8')\n    b_arrow = pa.array(b_values, type='utf8')\n\n    rng_index_arrow = pa.array([0, 2, 4, 6], type='int64')\n\n    gen_name_0 = '__index_level_0__'\n    gen_name_1 = '__index_level_1__'\n\n    # Case 1: named RangeIndex\n    e1 = pd.DataFrame({\n        'a': a_values\n    }, index=pd.RangeIndex(0, 8, step=2, name='qux'))\n    t1 = pa.Table.from_arrays([a_arrow, rng_index_arrow],\n                              names=['a', 'qux'])\n    t1 = t1.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': ['qux'],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': 'qux',\n                          'field_name': 'qux',\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r1 = t1.to_pandas()\n    tm.assert_frame_equal(r1, e1)\n\n    # Case 2: named RangeIndex, but conflicts with an actual column\n    e2 = pd.DataFrame({\n        'qux': a_values\n    }, index=pd.RangeIndex(0, 8, step=2, name='qux'))\n    t2 = pa.Table.from_arrays([a_arrow, rng_index_arrow],\n                              names=['qux', gen_name_0])\n    t2 = t2.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': [gen_name_0],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': 'qux',\n                          'field_name': gen_name_0,\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r2 = t2.to_pandas()\n    tm.assert_frame_equal(r2, e2)\n\n    # Case 3: unnamed RangeIndex\n    e3 = pd.DataFrame({\n        'a': a_values\n    }, index=pd.RangeIndex(0, 8, step=2, name=None))\n    t3 = pa.Table.from_arrays([a_arrow, rng_index_arrow],\n                              names=['a', gen_name_0])\n    t3 = t3.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': [gen_name_0],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_0,\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r3 = t3.to_pandas()\n    tm.assert_frame_equal(r3, e3)\n\n    # Case 4: MultiIndex with named RangeIndex\n    e4 = pd.DataFrame({\n        'a': a_values\n    }, index=[pd.RangeIndex(0, 8, step=2, name='qux'), b_values])\n    t4 = pa.Table.from_arrays([a_arrow, rng_index_arrow, b_arrow],\n                              names=['a', 'qux', gen_name_1])\n    t4 = t4.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': ['qux', gen_name_1],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': 'qux',\n                          'field_name': 'qux',\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_1,\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r4 = t4.to_pandas()\n    tm.assert_frame_equal(r4, e4)\n\n    # Case 4: MultiIndex with unnamed RangeIndex\n    e5 = pd.DataFrame({\n        'a': a_values\n    }, index=[pd.RangeIndex(0, 8, step=2, name=None), b_values])\n    t5 = pa.Table.from_arrays([a_arrow, rng_index_arrow, b_arrow],\n                              names=['a', gen_name_0, gen_name_1])\n    t5 = t5.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': [gen_name_0, gen_name_1],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_0,\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_1,\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r5 = t5.to_pandas()\n    tm.assert_frame_equal(r5, e5)\n\n\ndef test_metadata_compat_missing_field_name():\n    # Combination of missing field name but with index column as metadata.\n    # This combo occurs in the latest versions of fastparquet (0.3.2), but not\n    # in pyarrow itself (since field_name was added in 0.8, index as metadata\n    # only added later)\n\n    a_values = [1, 2, 3, 4]\n    b_values = ['a', 'b', 'c', 'd']\n    a_arrow = pa.array(a_values, type='int64')\n    b_arrow = pa.array(b_values, type='utf8')\n\n    expected = pd.DataFrame({\n        'a': a_values,\n        'b': b_values,\n    }, index=pd.RangeIndex(0, 8, step=2, name='qux'))\n    table = pa.table({'a': a_arrow, 'b': b_arrow})\n\n    # metadata generated by fastparquet 0.3.2 with missing field_names\n    table = table.replace_schema_metadata({\n        b'pandas': json.dumps({\n            'column_indexes': [\n                {'field_name': None,\n                 'metadata': None,\n                 'name': None,\n                 'numpy_type': 'object',\n                 'pandas_type': 'mixed-integer'}\n            ],\n            'columns': [\n                {'metadata': None,\n                 'name': 'a',\n                 'numpy_type': 'int64',\n                 'pandas_type': 'int64'},\n                {'metadata': None,\n                 'name': 'b',\n                 'numpy_type': 'object',\n                 'pandas_type': 'unicode'}\n            ],\n            'index_columns': [\n                {'kind': 'range',\n                 'name': 'qux',\n                 'start': 0,\n                 'step': 2,\n                 'stop': 8}\n            ],\n            'pandas_version': '0.25.0'}\n\n        )})\n    result = table.to_pandas()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_metadata_index_name_not_json_serializable():\n    name = np.int64(6)  # not json serializable by default\n    table = pa.table(pd.DataFrame(index=pd.RangeIndex(0, 4, name=name)))\n    metadata = table.schema.pandas_metadata\n    assert metadata['index_columns'][0]['name'] == '6'\n\n\ndef test_metadata_index_name_is_json_serializable():\n    name = 6  # json serializable by default\n    table = pa.table(pd.DataFrame(index=pd.RangeIndex(0, 4, name=name)))\n    metadata = table.schema.pandas_metadata\n    assert metadata['index_columns'][0]['name'] == 6\n\n\ndef make_df_with_timestamps():\n    # Some of the milliseconds timestamps deliberately don't fit in the range\n    # that is possible with nanosecond timestamps.\n    df = pd.DataFrame({\n        'dateTimeMs': [\n            np.datetime64('0001-01-01 00:00', 'ms'),\n            np.datetime64('2012-05-02 12:35', 'ms'),\n            np.datetime64('2012-05-03 15:42', 'ms'),\n            np.datetime64('3000-05-03 15:42', 'ms'),\n        ],\n        'dateTimeNs': [\n            np.datetime64('1991-01-01 00:00', 'ns'),\n            np.datetime64('2012-05-02 12:35', 'ns'),\n            np.datetime64('2012-05-03 15:42', 'ns'),\n            np.datetime64('2050-05-03 15:42', 'ns'),\n        ],\n    })\n    df['dateTimeMs'] = df['dateTimeMs'].astype('object')\n    # Not part of what we're testing, just ensuring that the inputs are what we\n    # expect.\n    assert (df.dateTimeMs.dtype, df.dateTimeNs.dtype) == (\n        # O == object, M8[ns] == timestamp64[ns]\n        np.dtype(\"O\"), np.dtype(\"M8[ns]\")\n    )\n    return df\n\n\n@pytest.mark.parquet\n@pytest.mark.filterwarnings(\"ignore:Parquet format '2.0':FutureWarning\")\ndef test_timestamp_as_object_parquet(tempdir):\n    # Timestamps can be stored as Parquet and reloaded into Pandas with no loss\n    # of information if the timestamp_as_object option is True.\n    df = make_df_with_timestamps()\n    table = pa.Table.from_pandas(df)\n    filename = tempdir / \"timestamps_from_pandas.parquet\"\n    pq.write_table(table, filename, version=\"2.0\")\n    result = pq.read_table(filename)\n    df2 = result.to_pandas(timestamp_as_object=True)\n    tm.assert_frame_equal(df, df2)\n\n\ndef test_timestamp_as_object_out_of_range():\n    # Out of range timestamps can be converted Arrow and reloaded into Pandas\n    # with no loss of information if the timestamp_as_object option is True.\n    df = make_df_with_timestamps()\n    table = pa.Table.from_pandas(df)\n    df2 = table.to_pandas(timestamp_as_object=True)\n    tm.assert_frame_equal(df, df2)\n\n\n@pytest.mark.parametrize(\"resolution\", [\"s\", \"ms\", \"us\"])\n@pytest.mark.parametrize(\"tz\", [None, \"America/New_York\"])\n# One datetime outside nanosecond range, one inside nanosecond range:\n@pytest.mark.parametrize(\"dt\", [datetime(1553, 1, 1), datetime(2020, 1, 1)])\ndef test_timestamp_as_object_non_nanosecond(resolution, tz, dt):\n    # Timestamps can be converted Arrow and reloaded into Pandas with no loss\n    # of information if the timestamp_as_object option is True.\n    arr = pa.array([dt], type=pa.timestamp(resolution, tz=tz))\n    table = pa.table({'a': arr})\n\n    for result in [\n        arr.to_pandas(timestamp_as_object=True),\n        table.to_pandas(timestamp_as_object=True)['a']\n    ]:\n        assert result.dtype == object\n        assert isinstance(result[0], datetime)\n        if tz:\n            assert result[0].tzinfo is not None\n            expected = result[0].tzinfo.fromutc(dt)\n        else:\n            assert result[0].tzinfo is None\n            expected = dt\n        assert result[0] == expected\n\n\ndef test_timestamp_as_object_fixed_offset():\n    # ARROW-16547 to_pandas with timestamp_as_object=True and FixedOffset\n    pytz = pytest.importorskip(\"pytz\")\n    import datetime\n    timezone = pytz.FixedOffset(120)\n    dt = timezone.localize(datetime.datetime(2022, 5, 12, 16, 57))\n\n    table = pa.table({\"timestamp_col\": pa.array([dt])})\n    result = table.to_pandas(timestamp_as_object=True)\n    assert pa.table(result) == table\n\n\ndef test_threaded_pandas_import():\n    invoke_script(\"pandas_threaded_import.py\")\n\n\ndef test_does_not_mutate_timedelta_dtype():\n    expected = np.dtype('m8')\n\n    assert np.dtype(np.timedelta64) == expected\n\n    df = pd.DataFrame({\"a\": [np.timedelta64()]})\n    t = pa.Table.from_pandas(df)\n    t.to_pandas()\n\n    assert np.dtype(np.timedelta64) == expected\n\n\ndef test_does_not_mutate_timedelta_nested():\n    # ARROW-17893: dataframe with timedelta and a list of dictionary\n    # also with timedelta produces wrong result with to_pandas\n\n    from datetime import timedelta\n    timedelta_1 = [{\"timedelta_1\": timedelta(seconds=12, microseconds=1)}]\n    timedelta_2 = [timedelta(hours=3, minutes=40, seconds=23)]\n    table = pa.table({\"timedelta_1\": timedelta_1, \"timedelta_2\": timedelta_2})\n    df = table.to_pandas()\n\n    assert df[\"timedelta_2\"][0].to_pytimedelta() == timedelta_2[0]\n\n\ndef test_roundtrip_nested_map_table_with_pydicts():\n    schema = pa.schema([\n        pa.field(\n            \"a\",\n            pa.list_(\n                pa.map_(pa.int8(), pa.struct([pa.field(\"b\", pa.binary())]))\n            )\n        )\n    ])\n    table = pa.table([[\n        [[(1, None)]],\n        None,\n        [\n            [(2, {\"b\": b\"abc\"})],\n            [(3, {\"b\": None}), (4, {\"b\": b\"def\"})],\n        ]\n    ]],\n        schema=schema,\n    )\n\n    expected_default_df = pd.DataFrame(\n        {\"a\": [[[(1, None)]], None, [[(2, {\"b\": b\"abc\"})],\n                                     [(3, {\"b\": None}), (4, {\"b\": b\"def\"})]]]}\n    )\n    expected_as_pydicts_df = pd.DataFrame(\n        {\"a\": [\n            [{1: None}],\n            None,\n            [{2: {\"b\": b\"abc\"}}, {3: {\"b\": None}, 4: {\"b\": b\"def\"}}],\n        ]}\n    )\n\n    default_df = table.to_pandas()\n    as_pydicts_df = table.to_pandas(maps_as_pydicts=\"strict\")\n\n    tm.assert_frame_equal(default_df, expected_default_df)\n    tm.assert_frame_equal(as_pydicts_df, expected_as_pydicts_df)\n\n    table_default_roundtrip = pa.Table.from_pandas(default_df, schema=schema)\n    assert table.equals(table_default_roundtrip)\n\n    table_as_pydicts_roundtrip = pa.Table.from_pandas(as_pydicts_df, schema=schema)\n    assert table.equals(table_as_pydicts_roundtrip)\n\n\ndef test_roundtrip_nested_map_array_with_pydicts_sliced():\n    \"\"\"\n    Slightly more robust test with chunking and slicing\n    \"\"\"\n    keys_1 = pa.array(['foo', 'bar'])\n    keys_2 = pa.array(['baz', 'qux', 'quux', 'quz'])\n    keys_3 = pa.array([], pa.string())\n\n    items_1 = pa.array(\n        [['a', 'b'], ['c', 'd']],\n        pa.list_(pa.string()),\n    )\n    items_2 = pa.array(\n        [[], None, [None, 'e'], ['f', 'g']],\n        pa.list_(pa.string()),\n    )\n    items_3 = pa.array(\n        [],\n        pa.list_(pa.string()),\n    )\n\n    map_chunk_1 = pa.MapArray.from_arrays([0, 2], keys_1, items_1)\n    map_chunk_2 = pa.MapArray.from_arrays([0, 3, 4], keys_2, items_2)\n    map_chunk_3 = pa.MapArray.from_arrays([0, 0], keys_3, items_3)\n    chunked_array = pa.chunked_array([\n        pa.ListArray.from_arrays([0, 1], map_chunk_1).slice(0),\n        pa.ListArray.from_arrays([0, 1], map_chunk_2.slice(1)).slice(0),\n        pa.ListArray.from_arrays([0, 0], map_chunk_3).slice(0),\n    ])\n\n    series_default = chunked_array.to_pandas()\n    expected_series_default = pd.Series([\n        [[('foo', ['a', 'b']), ('bar', ['c', 'd'])]],\n        [[('quz', ['f', 'g'])]],\n        [],\n    ])\n\n    series_pydicts = chunked_array.to_pandas(maps_as_pydicts=\"strict\")\n    expected_series_pydicts = pd.Series([\n        [{'foo': ['a', 'b'], 'bar': ['c', 'd']}],\n        [{'quz': ['f', 'g']}],\n        [],\n    ])\n\n    sliced = chunked_array.slice(1, 3)\n    series_default_sliced = sliced.to_pandas()\n    expected_series_default_sliced = pd.Series([\n        [[('quz', ['f', 'g'])]],\n        [],\n    ])\n\n    series_pydicts_sliced = sliced.to_pandas(maps_as_pydicts=\"strict\")\n    expected_series_pydicts_sliced = pd.Series([\n        [{'quz': ['f', 'g']}],\n        [],\n    ])\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                DeprecationWarning)\n        tm.assert_series_equal(series_default, expected_series_default)\n        tm.assert_series_equal(series_pydicts, expected_series_pydicts)\n        tm.assert_series_equal(series_default_sliced, expected_series_default_sliced)\n        tm.assert_series_equal(series_pydicts_sliced, expected_series_pydicts_sliced)\n\n    ty = pa.list_(pa.map_(pa.string(), pa.list_(pa.string())))\n\n    def assert_roundtrip(series: pd.Series, data) -> None:\n        array_roundtrip = pa.chunked_array(pa.Array.from_pandas(series, type=ty))\n        array_roundtrip.validate(full=True)\n        assert data.equals(array_roundtrip)\n\n    assert_roundtrip(series_default, chunked_array)\n    assert_roundtrip(series_pydicts, chunked_array)\n    assert_roundtrip(series_default_sliced, sliced)\n    assert_roundtrip(series_pydicts_sliced, sliced)\n\n\ndef test_roundtrip_map_array_with_pydicts_duplicate_keys():\n    keys = pa.array(['foo', 'bar', 'foo'])\n    items = pa.array(\n        [['a', 'b'], ['c', 'd'], ['1', '2']],\n        pa.list_(pa.string()),\n    )\n    offsets = [0, 3]\n    maps = pa.MapArray.from_arrays(offsets, keys, items)\n    ty = pa.map_(pa.string(), pa.list_(pa.string()))\n\n    # ------------------------\n    # With maps as pydicts\n    with pytest.raises(pa.lib.ArrowException):\n        # raises because of duplicate keys\n        maps.to_pandas(maps_as_pydicts=\"strict\")\n    series_pydicts = maps.to_pandas(maps_as_pydicts=\"lossy\")\n    # some data loss occurs for duplicate keys\n    expected_series_pydicts = pd.Series([\n        {'foo': ['1', '2'], 'bar': ['c', 'd']},\n    ])\n    # roundtrip is not possible because of data loss\n    assert not maps.equals(pa.Array.from_pandas(series_pydicts, type=ty))\n\n    # ------------------------\n    # With default assoc list of tuples\n    series_default = maps.to_pandas()\n    expected_series_default = pd.Series([\n        [('foo', ['a', 'b']), ('bar', ['c', 'd']), ('foo', ['1', '2'])],\n    ])\n    assert maps.equals(pa.Array.from_pandas(series_default, type=ty))\n\n    # custom comparison for compatibility w/ Pandas 1.0.0\n    # would otherwise run:\n    #   tm.assert_series_equal(series_pydicts, expected_series_pydicts)\n    assert len(series_pydicts) == len(expected_series_pydicts)\n    for row1, row2 in zip(series_pydicts, expected_series_pydicts):\n        assert len(row1) == len(row2)\n        for tup1, tup2 in zip(row1.items(), row2.items()):\n            assert tup1[0] == tup2[0]\n            assert np.array_equal(tup1[1], tup2[1])\n\n    # custom comparison for compatibility w/ Pandas 1.0.0\n    # would otherwise run:\n    #   tm.assert_series_equal(series_default, expected_series_default)\n    assert len(series_default) == len(expected_series_default)\n    for row1, row2 in zip(series_default, expected_series_default):\n        assert len(row1) == len(row2)\n        for tup1, tup2 in zip(row1, row2):\n            assert tup1[0] == tup2[0]\n            assert np.array_equal(tup1[1], tup2[1])\n\n\ndef test_unhashable_map_keys_with_pydicts():\n    keys = pa.array(\n        [['a', 'b'], ['c', 'd'], [], ['e'], [None, 'f'], ['g', 'h']],\n        pa.list_(pa.string()),\n    )\n    items = pa.array(['foo', 'bar', 'baz', 'qux', 'quux', 'quz'])\n    offsets = [0, 2, 6]\n    maps = pa.MapArray.from_arrays(offsets, keys, items)\n\n    # ------------------------\n    # With maps as pydicts\n    with pytest.raises(TypeError):\n        maps.to_pandas(maps_as_pydicts=\"lossy\")\n\n    # ------------------------\n    # With default assoc list of tuples\n    series = maps.to_pandas()\n    expected_series_default = pd.Series([\n        [(['a', 'b'], 'foo'), (['c', 'd'], 'bar')],\n        [([], 'baz'), (['e'], 'qux'), ([None, 'f'], 'quux'), (['g', 'h'], 'quz')],\n    ])\n\n    # custom comparison for compatibility w/ Pandas 1.0.0\n    # would otherwise run:\n    #   tm.assert_series_equal(series, expected_series_default)\n    assert len(series) == len(expected_series_default)\n    for row1, row2 in zip(series, expected_series_default):\n        assert len(row1) == len(row2)\n        for tup1, tup2 in zip(row1, row2):\n            assert np.array_equal(tup1[0], tup2[0])\n            assert tup1[1] == tup2[1]\n\n\ndef test_table_column_conversion_for_datetime():\n    # GH-35235\n    # pandas implemented __from_arrow__ for DatetimeTZDtype,\n    # but we choose to do the conversion in Arrow instead.\n    # https://github.com/pandas-dev/pandas/pull/52201\n    series = pd.Series(pd.date_range(\"2012\", periods=2, tz=\"Europe/Brussels\"),\n                       name=\"datetime_column\")\n    table = pa.table({\"datetime_column\": pa.array(series)})\n    table_col = table.column(\"datetime_column\")\n\n    result = table_col.to_pandas()\n    assert result.name == \"datetime_column\"\n    tm.assert_series_equal(result, series)\n\n\ndef test_array_conversion_for_datetime():\n    # GH-35235\n    # pandas implemented __from_arrow__ for DatetimeTZDtype,\n    # but we choose to do the conversion in Arrow instead.\n    # https://github.com/pandas-dev/pandas/pull/52201\n    series = pd.Series(pd.date_range(\"2012\", periods=2, tz=\"Europe/Brussels\"))\n    arr = pa.array(series)\n\n    result = arr.to_pandas()\n    tm.assert_series_equal(result, series)\n\n\n@pytest.mark.large_memory\ndef test_nested_chunking_valid():\n    # GH-32439: Chunking can cause arrays to be in invalid state\n    # when nested types are involved.\n    # Here we simply ensure we validate correctly.\n\n    def roundtrip(df, schema=None):\n        tab = pa.Table.from_pandas(df, schema=schema)\n        tab.validate(full=True)\n        # we expect to trigger chunking internally\n        # an assertion failure here may just mean this threshold has changed\n        num_chunks = tab.column(0).num_chunks\n        assert num_chunks > 1\n        tm.assert_frame_equal(tab.to_pandas(self_destruct=True,\n                                            maps_as_pydicts=\"strict\"), df)\n\n    x = b\"0\" * 720000000\n    roundtrip(pd.DataFrame({\"strings\": [x, x, x]}))\n\n    struct = {\"struct_field\": x}\n    roundtrip(pd.DataFrame({\"structs\": [struct, struct, struct]}))\n\n    lists = [x]\n    roundtrip(pd.DataFrame({\"lists\": [lists, lists, lists]}))\n\n    los = [struct]\n    roundtrip(pd.DataFrame({\"los\": [los, los, los]}))\n\n    sol = {\"struct_field\": lists}\n    roundtrip(pd.DataFrame({\"sol\": [sol, sol, sol]}))\n\n    map_of_los = {\"a\": los}\n    map_type = pa.map_(pa.string(),\n                       pa.list_(pa.struct([(\"struct_field\", pa.binary())])))\n    schema = pa.schema([(\"maps\", map_type)])\n    roundtrip(pd.DataFrame({\"maps\": [map_of_los, map_of_los, map_of_los]}),\n              schema=schema)\n\n\ndef test_is_data_frame_race_condition():\n    # See https://github.com/apache/arrow/issues/39313\n    test_util.invoke_script('arrow_39313.py')\n", "python/pyarrow/tests/test_builder.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport weakref\n\nimport numpy as np\n\nimport pyarrow as pa\nfrom pyarrow.lib import StringBuilder, StringViewBuilder\n\n\ndef test_weakref():\n    sbuilder = StringBuilder()\n    wr = weakref.ref(sbuilder)\n    assert wr() is not None\n    del sbuilder\n    assert wr() is None\n\n\ndef test_string_builder_append():\n    sbuilder = StringBuilder()\n    sbuilder.append(b\"a byte string\")\n    sbuilder.append(\"a string\")\n    sbuilder.append(np.nan)\n    sbuilder.append(None)\n    assert len(sbuilder) == 4\n    assert sbuilder.null_count == 2\n    arr = sbuilder.finish()\n    assert len(sbuilder) == 0\n    assert isinstance(arr, pa.Array)\n    assert arr.null_count == 2\n    assert arr.type == 'str'\n    expected = [\"a byte string\", \"a string\", None, None]\n    assert arr.to_pylist() == expected\n\n\ndef test_string_builder_append_values():\n    sbuilder = StringBuilder()\n    sbuilder.append_values([np.nan, None, \"text\", None, \"other text\"])\n    assert sbuilder.null_count == 3\n    arr = sbuilder.finish()\n    assert arr.null_count == 3\n    expected = [None, None, \"text\", None, \"other text\"]\n    assert arr.to_pylist() == expected\n\n\ndef test_string_builder_append_after_finish():\n    sbuilder = StringBuilder()\n    sbuilder.append_values([np.nan, None, \"text\", None, \"other text\"])\n    arr = sbuilder.finish()\n    sbuilder.append(\"No effect\")\n    expected = [None, None, \"text\", None, \"other text\"]\n    assert arr.to_pylist() == expected\n\n\ndef test_string_view_builder():\n    builder = StringViewBuilder()\n    builder.append(b\"a byte string\")\n    builder.append(\"a string\")\n    builder.append(\"a longer not-inlined string\")\n    builder.append(np.nan)\n    builder.append_values([None, \"text\"])\n    assert len(builder) == 6\n    assert builder.null_count == 2\n    arr = builder.finish()\n    assert isinstance(arr, pa.Array)\n    assert arr.null_count == 2\n    assert arr.type == 'string_view'\n    expected = [\n        \"a byte string\", \"a string\", \"a longer not-inlined string\", None, None, \"text\"\n    ]\n    assert arr.to_pylist() == expected\n", "python/pyarrow/tests/test_io.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport bz2\nfrom contextlib import contextmanager\nfrom io import (BytesIO, StringIO, TextIOWrapper, BufferedIOBase, IOBase)\nimport itertools\nimport gc\nimport gzip\nimport math\nimport os\nimport pathlib\nimport pytest\nimport sys\nimport tempfile\nimport weakref\n\nimport numpy as np\n\nfrom pyarrow.util import guid\nfrom pyarrow import Codec\nimport pyarrow as pa\n\n\ndef check_large_seeks(file_factory, expected_error=None):\n    if sys.platform in ('win32', 'darwin'):\n        pytest.skip(\"need sparse file support\")\n    try:\n        filename = tempfile.mktemp(prefix='test_io')\n        with open(filename, 'wb') as f:\n            f.truncate(2 ** 32 + 10)\n            f.seek(2 ** 32 + 5)\n            f.write(b'mark\\n')\n        if expected_error:\n            with expected_error:\n                file_factory(filename)\n        else:\n            with file_factory(filename) as f:\n                assert f.size() == 2 ** 32 + 10\n                assert f.seek(2 ** 32 + 5) == 2 ** 32 + 5\n                assert f.tell() == 2 ** 32 + 5\n                assert f.read(5) == b'mark\\n'\n                assert f.tell() == 2 ** 32 + 10\n    finally:\n        os.unlink(filename)\n\n\n@contextmanager\ndef assert_file_not_found():\n    with pytest.raises(FileNotFoundError):\n        yield\n\n\n# ----------------------------------------------------------------------\n# Python file-like objects\n\n\ndef test_python_file_write():\n    buf = BytesIO()\n\n    f = pa.PythonFile(buf)\n\n    assert f.tell() == 0\n\n    s1 = b'enga\\xc3\\xb1ado'\n    s2 = b'foobar'\n\n    f.write(s1)\n    assert f.tell() == len(s1)\n\n    f.write(s2)\n\n    expected = s1 + s2\n\n    result = buf.getvalue()\n    assert result == expected\n\n    assert not f.closed\n    f.close()\n    assert f.closed\n\n    with pytest.raises(TypeError, match=\"binary file expected\"):\n        pa.PythonFile(StringIO())\n\n\ndef test_python_file_read():\n    data = b'some sample data'\n\n    buf = BytesIO(data)\n    f = pa.PythonFile(buf, mode='r')\n\n    assert f.size() == len(data)\n\n    assert f.tell() == 0\n\n    assert f.read(4) == b'some'\n    assert f.tell() == 4\n\n    f.seek(0)\n    assert f.tell() == 0\n\n    f.seek(5)\n    assert f.tell() == 5\n\n    v = f.read(50)\n    assert v == b'sample data'\n    assert len(v) == 11\n\n    assert f.size() == len(data)\n\n    assert not f.closed\n    f.close()\n    assert f.closed\n\n    with pytest.raises(TypeError, match=\"binary file expected\"):\n        pa.PythonFile(StringIO(), mode='r')\n\n\n@pytest.mark.parametrize(\"nbytes\", (-1, 0, 1, 5, 100))\n@pytest.mark.parametrize(\"file_offset\", (-1, 0, 5, 100))\ndef test_python_file_get_stream(nbytes, file_offset):\n\n    data = b'data1data2data3data4data5'\n\n    f = pa.PythonFile(BytesIO(data), mode='r')\n\n    # negative nbytes or offsets don't make sense here, raise ValueError\n    if nbytes < 0 or file_offset < 0:\n        with pytest.raises(pa.ArrowInvalid,\n                           match=\"should be a positive value\"):\n            f.get_stream(file_offset=file_offset, nbytes=nbytes)\n        f.close()\n        return\n    else:\n        stream = f.get_stream(file_offset=file_offset, nbytes=nbytes)\n\n    # Subsequent calls to 'read' should match behavior if same\n    # data passed to BytesIO where get_stream should handle if\n    # nbytes/file_offset results in no bytes b/c out of bounds.\n    start = min(file_offset, len(data))\n    end = min(file_offset + nbytes, len(data))\n    buf = BytesIO(data[start:end])\n\n    # read some chunks\n    assert stream.read(nbytes=4) == buf.read(4)\n    assert stream.read(nbytes=6) == buf.read(6)\n\n    # Read to end of each stream\n    assert stream.read() == buf.read()\n\n    # Try reading past the stream\n    n = len(data) * 2\n    assert stream.read(n) == buf.read(n)\n\n    # NativeFile[CInputStream] is not seekable\n    with pytest.raises(OSError, match=\"seekable\"):\n        stream.seek(0)\n\n    stream.close()\n    assert stream.closed\n\n\ndef test_python_file_read_at():\n    data = b'some sample data'\n\n    buf = BytesIO(data)\n    f = pa.PythonFile(buf, mode='r')\n\n    # test simple read at\n    v = f.read_at(nbytes=5, offset=3)\n    assert v == b'e sam'\n    assert len(v) == 5\n\n    # test reading entire file when nbytes > len(file)\n    w = f.read_at(nbytes=50, offset=0)\n    assert w == data\n    assert len(w) == 16\n\n\ndef test_python_file_readall():\n    data = b'some sample data'\n\n    buf = BytesIO(data)\n    with pa.PythonFile(buf, mode='r') as f:\n        assert f.readall() == data\n\n\ndef test_python_file_readinto():\n    length = 10\n    data = b'some sample data longer than 10'\n    dst_buf = bytearray(length)\n    src_buf = BytesIO(data)\n\n    with pa.PythonFile(src_buf, mode='r') as f:\n        assert f.readinto(dst_buf) == 10\n\n        assert dst_buf[:length] == data[:length]\n        assert len(dst_buf) == length\n\n\ndef test_python_file_read_buffer():\n    length = 10\n    data = b'0123456798'\n    dst_buf = bytearray(data)\n\n    class DuckReader:\n        def close(self):\n            pass\n\n        @property\n        def closed(self):\n            return False\n\n        def read_buffer(self, nbytes):\n            assert nbytes == length\n            return memoryview(dst_buf)[:nbytes]\n\n    duck_reader = DuckReader()\n    with pa.PythonFile(duck_reader, mode='r') as f:\n        buf = f.read_buffer(length)\n        assert len(buf) == length\n        assert memoryview(buf).tobytes() == dst_buf[:length]\n        # buf should point to the same memory, so modifying it\n        memoryview(buf)[0] = ord(b'x')\n        # should modify the original\n        assert dst_buf[0] == ord(b'x')\n\n\ndef test_python_file_correct_abc():\n    with pa.PythonFile(BytesIO(b''), mode='r') as f:\n        assert isinstance(f, BufferedIOBase)\n        assert isinstance(f, IOBase)\n\n\ndef test_python_file_iterable():\n    data = b'''line1\n    line2\n    line3\n    '''\n\n    buf = BytesIO(data)\n    buf2 = BytesIO(data)\n\n    with pa.PythonFile(buf, mode='r') as f:\n        for read, expected in zip(f, buf2):\n            assert read == expected\n\n\ndef test_python_file_large_seeks():\n    def factory(filename):\n        return pa.PythonFile(open(filename, 'rb'))\n\n    check_large_seeks(factory)\n\n\ndef test_bytes_reader():\n    # Like a BytesIO, but zero-copy underneath for C++ consumers\n    data = b'some sample data'\n    f = pa.BufferReader(data)\n    assert f.tell() == 0\n\n    assert f.size() == len(data)\n\n    assert f.read(4) == b'some'\n    assert f.tell() == 4\n\n    f.seek(0)\n    assert f.tell() == 0\n\n    f.seek(0, 2)\n    assert f.tell() == len(data)\n\n    f.seek(5)\n    assert f.tell() == 5\n\n    assert f.read(50) == b'sample data'\n\n    assert not f.closed\n    f.close()\n    assert f.closed\n\n\ndef test_bytes_reader_non_bytes():\n    with pytest.raises(TypeError):\n        pa.BufferReader('some sample data')\n\n\ndef test_bytes_reader_retains_parent_reference():\n    import gc\n\n    # ARROW-421\n    def get_buffer():\n        data = b'some sample data' * 1000\n        reader = pa.BufferReader(data)\n        reader.seek(5)\n        return reader.read_buffer(6)\n\n    buf = get_buffer()\n    gc.collect()\n    assert buf.to_pybytes() == b'sample'\n    assert buf.parent is not None\n\n\ndef test_python_file_implicit_mode(tmpdir):\n    path = os.path.join(str(tmpdir), 'foo.txt')\n    with open(path, 'wb') as f:\n        pf = pa.PythonFile(f)\n        assert pf.writable()\n        assert not pf.readable()\n        assert not pf.seekable()  # PyOutputStream isn't seekable\n        f.write(b'foobar\\n')\n\n    with open(path, 'rb') as f:\n        pf = pa.PythonFile(f)\n        assert pf.readable()\n        assert not pf.writable()\n        assert pf.seekable()\n        assert pf.read() == b'foobar\\n'\n\n    bio = BytesIO()\n    pf = pa.PythonFile(bio)\n    assert pf.writable()\n    assert not pf.readable()\n    assert not pf.seekable()\n    pf.write(b'foobar\\n')\n    assert bio.getvalue() == b'foobar\\n'\n\n\ndef test_python_file_writelines(tmpdir):\n    lines = [b'line1\\n', b'line2\\n' b'line3']\n    path = os.path.join(str(tmpdir), 'foo.txt')\n    with open(path, 'wb') as f:\n        try:\n            f = pa.PythonFile(f, mode='w')\n            assert f.writable()\n            f.writelines(lines)\n        finally:\n            f.close()\n\n    with open(path, 'rb') as f:\n        try:\n            f = pa.PythonFile(f, mode='r')\n            assert f.readable()\n            assert f.read() == b''.join(lines)\n        finally:\n            f.close()\n\n\ndef test_python_file_closing():\n    bio = BytesIO()\n    pf = pa.PythonFile(bio)\n    wr = weakref.ref(pf)\n    del pf\n    assert wr() is None  # object was destroyed\n    assert not bio.closed\n    pf = pa.PythonFile(bio)\n    pf.close()\n    assert bio.closed\n\n\n# ----------------------------------------------------------------------\n# Buffers\n\n\ndef check_buffer_pickling(buf, pickler):\n    # Check that buffer survives a pickle roundtrip\n    for protocol in range(0, pickler.HIGHEST_PROTOCOL + 1):\n        result = pickler.loads(pickler.dumps(buf, protocol=protocol))\n        assert len(result) == len(buf)\n        assert memoryview(result) == memoryview(buf)\n        assert result.to_pybytes() == buf.to_pybytes()\n        assert result.is_mutable == buf.is_mutable\n\n\ndef test_buffer_bytes(pickle_module):\n    val = b'some data'\n\n    buf = pa.py_buffer(val)\n    assert isinstance(buf, pa.Buffer)\n    assert not buf.is_mutable\n    assert buf.is_cpu\n\n    result = buf.to_pybytes()\n    assert result == val\n\n    check_buffer_pickling(buf, pickle_module)\n\n\ndef test_buffer_null_data(pickle_module):\n    null_buff = pa.foreign_buffer(address=0, size=0)\n    assert null_buff.to_pybytes() == b\"\"\n    assert null_buff.address == 0\n    # ARROW-16048: we shouldn't expose a NULL address through the Python\n    # buffer protocol.\n    m = memoryview(null_buff)\n    assert m.tobytes() == b\"\"\n    assert pa.py_buffer(m).address != 0\n\n    check_buffer_pickling(null_buff, pickle_module)\n\n\ndef test_buffer_memoryview(pickle_module):\n    val = b'some data'\n\n    buf = pa.py_buffer(val)\n    assert isinstance(buf, pa.Buffer)\n    assert not buf.is_mutable\n    assert buf.is_cpu\n\n    result = memoryview(buf)\n    assert result == val\n\n    check_buffer_pickling(buf, pickle_module)\n\n\ndef test_buffer_bytearray(pickle_module):\n    val = bytearray(b'some data')\n\n    buf = pa.py_buffer(val)\n    assert isinstance(buf, pa.Buffer)\n    assert buf.is_mutable\n    assert buf.is_cpu\n\n    result = bytearray(buf)\n    assert result == val\n\n    check_buffer_pickling(buf, pickle_module)\n\n\ndef test_buffer_invalid():\n    with pytest.raises(TypeError,\n                       match=\"(bytes-like object|buffer interface)\"):\n        pa.py_buffer(None)\n\n\ndef test_buffer_weakref():\n    buf = pa.py_buffer(b'some data')\n    wr = weakref.ref(buf)\n    assert wr() is not None\n    del buf\n    assert wr() is None\n\n\n@pytest.mark.parametrize('val, expected_hex_buffer',\n                         [(b'check', b'636865636B'),\n                          (b'\\a0', b'0730'),\n                          (b'', b'')])\ndef test_buffer_hex(val, expected_hex_buffer):\n    buf = pa.py_buffer(val)\n    assert buf.hex() == expected_hex_buffer\n\n\ndef test_buffer_to_numpy():\n    # Make sure creating a numpy array from an arrow buffer works\n    byte_array = bytearray(20)\n    byte_array[0] = 42\n    buf = pa.py_buffer(byte_array)\n    array = np.frombuffer(buf, dtype=\"uint8\")\n    assert array[0] == byte_array[0]\n    byte_array[0] += 1\n    assert array[0] == byte_array[0]\n    assert array.base == buf\n\n\ndef test_buffer_from_numpy():\n    # C-contiguous\n    arr = np.arange(12, dtype=np.int8).reshape((3, 4))\n    buf = pa.py_buffer(arr)\n    assert buf.is_cpu\n    assert buf.is_mutable\n    assert buf.to_pybytes() == arr.tobytes()\n    # F-contiguous; note strides information is lost\n    buf = pa.py_buffer(arr.T)\n    assert buf.is_cpu\n    assert buf.is_mutable\n    assert buf.to_pybytes() == arr.tobytes()\n    # Non-contiguous\n    with pytest.raises(ValueError, match=\"not contiguous\"):\n        buf = pa.py_buffer(arr.T[::2])\n\n\ndef test_buffer_address():\n    b1 = b'some data!'\n    b2 = bytearray(b1)\n    b3 = bytearray(b1)\n\n    buf1 = pa.py_buffer(b1)\n    buf2 = pa.py_buffer(b1)\n    buf3 = pa.py_buffer(b2)\n    buf4 = pa.py_buffer(b3)\n\n    assert buf1.address > 0\n    assert buf1.address == buf2.address\n    assert buf3.address != buf2.address\n    assert buf4.address != buf3.address\n\n    arr = np.arange(5)\n    buf = pa.py_buffer(arr)\n    assert buf.address == arr.ctypes.data\n\n\ndef test_buffer_equals():\n    # Buffer.equals() returns true iff the buffers have the same contents\n    def eq(a, b):\n        assert a.equals(b)\n        assert a == b\n        assert not (a != b)\n\n    def ne(a, b):\n        assert not a.equals(b)\n        assert not (a == b)\n        assert a != b\n\n    b1 = b'some data!'\n    b2 = bytearray(b1)\n    b3 = bytearray(b1)\n    b3[0] = 42\n    buf1 = pa.py_buffer(b1)\n    buf2 = pa.py_buffer(b2)\n    buf3 = pa.py_buffer(b2)\n    buf4 = pa.py_buffer(b3)\n    buf5 = pa.py_buffer(np.frombuffer(b2, dtype=np.int16))\n    eq(buf1, buf1)\n    eq(buf1, buf2)\n    eq(buf2, buf3)\n    ne(buf2, buf4)\n    # Data type is indifferent\n    eq(buf2, buf5)\n\n\ndef test_buffer_eq_bytes():\n    buf = pa.py_buffer(b'some data')\n    assert buf == b'some data'\n    assert buf == bytearray(b'some data')\n    assert buf != b'some dat1'\n\n    with pytest.raises(TypeError):\n        buf == 'some data'\n\n\ndef test_buffer_getitem():\n    data = bytearray(b'some data!')\n    buf = pa.py_buffer(data)\n\n    n = len(data)\n    for ix in range(-n, n - 1):\n        assert buf[ix] == data[ix]\n\n    with pytest.raises(IndexError):\n        buf[n]\n\n    with pytest.raises(IndexError):\n        buf[-n - 1]\n\n\ndef test_buffer_slicing():\n    data = b'some data!'\n    buf = pa.py_buffer(data)\n\n    sliced = buf.slice(2)\n    expected = pa.py_buffer(b'me data!')\n    assert sliced.equals(expected)\n\n    sliced2 = buf.slice(2, 4)\n    expected2 = pa.py_buffer(b'me d')\n    assert sliced2.equals(expected2)\n\n    # 0 offset\n    assert buf.slice(0).equals(buf)\n\n    # Slice past end of buffer\n    assert len(buf.slice(len(buf))) == 0\n\n    with pytest.raises(IndexError):\n        buf.slice(-1)\n\n    with pytest.raises(IndexError):\n        buf.slice(len(buf) + 1)\n    assert buf[11:].to_pybytes() == b\"\"\n\n    # Slice stop exceeds buffer length\n    with pytest.raises(IndexError):\n        buf.slice(1, len(buf))\n    assert buf[1:11].to_pybytes() == buf.to_pybytes()[1:]\n\n    # Negative length\n    with pytest.raises(IndexError):\n        buf.slice(1, -1)\n\n    # Test slice notation\n    assert buf[2:].equals(buf.slice(2))\n    assert buf[2:5].equals(buf.slice(2, 3))\n    assert buf[-5:].equals(buf.slice(len(buf) - 5))\n    assert buf[-5:-2].equals(buf.slice(len(buf) - 5, 3))\n\n    with pytest.raises(IndexError):\n        buf[::-1]\n    with pytest.raises(IndexError):\n        buf[::2]\n\n    n = len(buf)\n    for start in range(-n * 2, n * 2):\n        for stop in range(-n * 2, n * 2):\n            assert buf[start:stop].to_pybytes() == buf.to_pybytes()[start:stop]\n\n\ndef test_buffer_hashing():\n    # Buffers are unhashable\n    with pytest.raises(TypeError, match=\"unhashable\"):\n        hash(pa.py_buffer(b'123'))\n\n\ndef test_buffer_protocol_respects_immutability():\n    # ARROW-3228; NumPy's frombuffer ctor determines whether a buffer-like\n    # object is mutable by first attempting to get a mutable buffer using\n    # PyObject_FromBuffer. If that fails, it assumes that the object is\n    # immutable\n    a = b'12345'\n    arrow_ref = pa.py_buffer(a)\n    numpy_ref = np.frombuffer(arrow_ref, dtype=np.uint8)\n    assert not numpy_ref.flags.writeable\n\n\ndef test_foreign_buffer():\n    obj = np.array([1, 2], dtype=np.int32)\n    addr = obj.__array_interface__[\"data\"][0]\n    size = obj.nbytes\n    buf = pa.foreign_buffer(addr, size, obj)\n    wr = weakref.ref(obj)\n    del obj\n    assert np.frombuffer(buf, dtype=np.int32).tolist() == [1, 2]\n    assert wr() is not None\n    del buf\n    assert wr() is None\n\n\ndef test_allocate_buffer():\n    buf = pa.allocate_buffer(100)\n    assert buf.size == 100\n    assert buf.is_mutable\n    assert buf.parent is None\n\n    bit = b'abcde'\n    writer = pa.FixedSizeBufferWriter(buf)\n    writer.write(bit)\n\n    assert buf.to_pybytes()[:5] == bit\n\n\ndef test_allocate_buffer_resizable():\n    buf = pa.allocate_buffer(100, resizable=True)\n    assert isinstance(buf, pa.ResizableBuffer)\n\n    buf.resize(200)\n    assert buf.size == 200\n\n\ndef test_non_cpu_buffer(pickle_module):\n    cuda = pytest.importorskip(\"pyarrow.cuda\")\n    ctx = cuda.Context(0)\n\n    data = np.array([b'testing'])\n    cuda_buf = ctx.buffer_from_data(data)\n    arr = pa.FixedSizeBinaryArray.from_buffers(pa.binary(7), 1, [None, cuda_buf])\n    buf_on_gpu = arr.buffers()[1]\n\n    assert buf_on_gpu.size == cuda_buf.size\n    assert buf_on_gpu.address == cuda_buf.address\n    assert buf_on_gpu.is_cpu == cuda_buf.is_cpu\n    assert buf_on_gpu.is_mutable\n\n    repr1 = \"<pyarrow.Buffer address=\"\n    repr2 = \"size=7 is_cpu=False is_mutable=True>\"\n    assert repr1 in repr(buf_on_gpu)\n    assert repr2 in repr(buf_on_gpu)\n\n    buf_on_gpu_sliced = buf_on_gpu.slice(2)\n    cuda_sliced = cuda.CudaBuffer.from_buffer(buf_on_gpu_sliced)\n    assert cuda_sliced.to_pybytes() == b'sting'\n\n    buf_on_gpu_sliced = buf_on_gpu[2:4]\n    cuda_sliced = cuda.CudaBuffer.from_buffer(buf_on_gpu_sliced)\n    assert cuda_sliced.to_pybytes() == b'st'\n\n    # Sliced buffers with same address\n    assert buf_on_gpu_sliced.equals(cuda_buf[2:4])\n\n    # Buffers on different devices\n    msg_device = \"Device on which the data resides differs between buffers\"\n    with pytest.raises(ValueError, match=msg_device):\n        buf_on_gpu.equals(pa.py_buffer(data))\n\n    msg = \"Implemented only for data on CPU device\"\n    # Buffers with different addresses\n    arr_short = np.array([b'sting'])\n    cuda_buf_short = ctx.buffer_from_data(arr_short)\n    with pytest.raises(NotImplementedError, match=msg):\n        buf_on_gpu_sliced.equals(cuda_buf_short)\n    arr_short = pa.FixedSizeBinaryArray.from_buffers(\n        pa.binary(5), 1, [None, cuda_buf_short]\n    )\n    buf_on_gpu_short = arr_short.buffers()[1]\n    with pytest.raises(NotImplementedError, match=msg):\n        buf_on_gpu_sliced.equals(buf_on_gpu_short)\n\n    with pytest.raises(NotImplementedError, match=msg):\n        buf_on_gpu.hex()\n\n    with pytest.raises(NotImplementedError, match=msg):\n        cuda_buf.hex()\n\n    with pytest.raises(NotImplementedError, match=msg):\n        buf_on_gpu[1]\n\n    with pytest.raises(NotImplementedError, match=msg):\n        buf_on_gpu.to_pybytes()\n\n    with pytest.raises(NotImplementedError, match=msg):\n        pickle_module.dumps(buf_on_gpu, protocol=4)\n\n    with pytest.raises(NotImplementedError, match=msg):\n        pickle_module.dumps(cuda_buf, protocol=4)\n\n    with pytest.raises(NotImplementedError, match=msg):\n        memoryview(buf_on_gpu)\n\n\ndef test_cache_options():\n    opts1 = pa.CacheOptions()\n    opts2 = pa.CacheOptions(hole_size_limit=1024)\n    opts3 = pa.CacheOptions(hole_size_limit=4096, range_size_limit=8192)\n    opts4 = pa.CacheOptions(hole_size_limit=4096,\n                            range_size_limit=8192, prefetch_limit=5)\n    opts5 = pa.CacheOptions(hole_size_limit=4096,\n                            range_size_limit=8192, lazy=False)\n    opts6 = pa.CacheOptions.from_network_metrics(time_to_first_byte_millis=100,\n                                                 transfer_bandwidth_mib_per_sec=200,\n                                                 ideal_bandwidth_utilization_frac=0.9,\n                                                 max_ideal_request_size_mib=64)\n\n    assert opts1.hole_size_limit == 8192\n    assert opts1.range_size_limit == 32 * 1024 * 1024\n    assert opts1.lazy is True\n    assert opts1.prefetch_limit == 0\n\n    assert opts2.hole_size_limit == 1024\n    assert opts2.range_size_limit == 32 * 1024 * 1024\n    assert opts2.lazy is True\n    assert opts2.prefetch_limit == 0\n\n    assert opts3.hole_size_limit == 4096\n    assert opts3.range_size_limit == 8192\n    assert opts3.lazy is True\n    assert opts3.prefetch_limit == 0\n\n    assert opts4.hole_size_limit == 4096\n    assert opts4.range_size_limit == 8192\n    assert opts4.lazy is True\n    assert opts4.prefetch_limit == 5\n\n    assert opts5.hole_size_limit == 4096\n    assert opts5.range_size_limit == 8192\n    assert opts5.lazy is False\n    assert opts5.prefetch_limit == 0\n\n    assert opts6.lazy is False\n\n    assert opts1 == opts1\n    assert opts1 != opts2\n    assert opts2 != opts3\n    assert opts3 != opts4\n    assert opts4 != opts5\n    assert opts6 != opts1\n\n\ndef test_cache_options_pickling(pickle_module):\n    options = [\n        pa.CacheOptions(),\n        pa.CacheOptions(hole_size_limit=4096, range_size_limit=8192,\n                        lazy=True, prefetch_limit=5),\n    ]\n\n    for option in options:\n        assert pickle_module.loads(pickle_module.dumps(option)) == option\n\n\n@pytest.mark.parametrize(\"compression\", [\n    pytest.param(\n        \"bz2\", marks=pytest.mark.xfail(raises=pa.lib.ArrowNotImplementedError)\n    ),\n    \"brotli\",\n    \"gzip\",\n    \"lz4\",\n    \"zstd\",\n    \"snappy\"\n])\ndef test_compress_decompress(compression):\n    if not Codec.is_available(compression):\n        pytest.skip(\"{} support is not built\".format(compression))\n\n    INPUT_SIZE = 10000\n    test_data = (np.random.randint(0, 255, size=INPUT_SIZE)\n                 .astype(np.uint8)\n                 .tobytes())\n    test_buf = pa.py_buffer(test_data)\n\n    compressed_buf = pa.compress(test_buf, codec=compression)\n    compressed_bytes = pa.compress(test_data, codec=compression,\n                                   asbytes=True)\n\n    assert isinstance(compressed_bytes, bytes)\n\n    decompressed_buf = pa.decompress(compressed_buf, INPUT_SIZE,\n                                     codec=compression)\n    decompressed_bytes = pa.decompress(compressed_bytes, INPUT_SIZE,\n                                       codec=compression, asbytes=True)\n\n    assert isinstance(decompressed_bytes, bytes)\n\n    assert decompressed_buf.equals(test_buf)\n    assert decompressed_bytes == test_data\n\n    with pytest.raises(ValueError):\n        pa.decompress(compressed_bytes, codec=compression)\n\n\n@pytest.mark.parametrize(\"compression\", [\n    pytest.param(\n        \"bz2\", marks=pytest.mark.xfail(raises=pa.lib.ArrowNotImplementedError)\n    ),\n    \"brotli\",\n    \"gzip\",\n    \"lz4\",\n    \"zstd\",\n    \"snappy\"\n])\ndef test_compression_level(compression):\n    if not Codec.is_available(compression):\n        pytest.skip(\"{} support is not built\".format(compression))\n\n    codec = Codec(compression)\n    if codec.name == \"snappy\":\n        assert codec.compression_level is None\n    else:\n        assert isinstance(codec.compression_level, int)\n\n    # These codecs do not support a compression level\n    no_level = ['snappy']\n    if compression in no_level:\n        assert not Codec.supports_compression_level(compression)\n        with pytest.raises(ValueError):\n            Codec(compression, 0)\n        with pytest.raises(ValueError):\n            Codec.minimum_compression_level(compression)\n        with pytest.raises(ValueError):\n            Codec.maximum_compression_level(compression)\n        with pytest.raises(ValueError):\n            Codec.default_compression_level(compression)\n        return\n\n    INPUT_SIZE = 10000\n    test_data = (np.random.randint(0, 255, size=INPUT_SIZE)\n                 .astype(np.uint8)\n                 .tobytes())\n    test_buf = pa.py_buffer(test_data)\n\n    min_level = Codec.minimum_compression_level(compression)\n    max_level = Codec.maximum_compression_level(compression)\n    default_level = Codec.default_compression_level(compression)\n\n    assert min_level < max_level\n    assert default_level >= min_level\n    assert default_level <= max_level\n\n    for compression_level in range(min_level, max_level+1):\n        codec = Codec(compression, compression_level)\n        compressed_buf = codec.compress(test_buf)\n        compressed_bytes = codec.compress(test_data, asbytes=True)\n        assert isinstance(compressed_bytes, bytes)\n        decompressed_buf = codec.decompress(compressed_buf, INPUT_SIZE)\n        decompressed_bytes = codec.decompress(compressed_bytes, INPUT_SIZE,\n                                              asbytes=True)\n\n        assert isinstance(decompressed_bytes, bytes)\n\n        assert decompressed_buf.equals(test_buf)\n        assert decompressed_bytes == test_data\n\n        with pytest.raises(ValueError):\n            codec.decompress(compressed_bytes)\n\n    # The ability to set a seed this way is not present on older versions of\n    # numpy (currently in our python 3.6 CI build).  Some inputs might just\n    # happen to compress the same between the two levels so using seeded\n    # random numbers is necessary to help get more reliable results\n    #\n    # The goal of this part is to ensure the compression_level is being\n    # passed down to the C++ layer, not to verify the compression algs\n    # themselves\n    if not hasattr(np.random, 'default_rng'):\n        pytest.skip('Requires newer version of numpy')\n    rng = np.random.default_rng(seed=42)\n    values = rng.integers(0, 100, 1000)\n    arr = pa.array(values)\n    hard_to_compress_buffer = arr.buffers()[1]\n\n    weak_codec = Codec(compression, min_level)\n    weakly_compressed_buf = weak_codec.compress(hard_to_compress_buffer)\n\n    strong_codec = Codec(compression, max_level)\n    strongly_compressed_buf = strong_codec.compress(hard_to_compress_buffer)\n\n    assert len(weakly_compressed_buf) > len(strongly_compressed_buf)\n\n\ndef test_buffer_memoryview_is_immutable():\n    val = b'some data'\n\n    buf = pa.py_buffer(val)\n    assert not buf.is_mutable\n    assert isinstance(buf, pa.Buffer)\n\n    result = memoryview(buf)\n    assert result.readonly\n\n    with pytest.raises(TypeError) as exc:\n        result[0] = b'h'\n        assert 'cannot modify read-only' in str(exc.value)\n\n    b = bytes(buf)\n    with pytest.raises(TypeError) as exc:\n        b[0] = b'h'\n        assert 'cannot modify read-only' in str(exc.value)\n\n\ndef test_uninitialized_buffer():\n    # ARROW-2039: calling Buffer() directly creates an uninitialized object\n    # ARROW-2638: prevent calling extension class constructors directly\n    with pytest.raises(TypeError):\n        pa.Buffer()\n\n\ndef test_memory_output_stream():\n    # 10 bytes\n    val = b'dataabcdef'\n    f = pa.BufferOutputStream()\n\n    K = 1000\n    for i in range(K):\n        f.write(val)\n\n    buf = f.getvalue()\n    assert len(buf) == len(val) * K\n    assert buf.to_pybytes() == val * K\n\n\ndef test_inmemory_write_after_closed():\n    f = pa.BufferOutputStream()\n    f.write(b'ok')\n    assert not f.closed\n    f.getvalue()\n    assert f.closed\n\n    with pytest.raises(ValueError):\n        f.write(b'not ok')\n\n\ndef test_buffer_protocol_ref_counting():\n    def make_buffer(bytes_obj):\n        return bytearray(pa.py_buffer(bytes_obj))\n\n    buf = make_buffer(b'foo')\n    gc.collect()\n    assert buf == b'foo'\n\n    # ARROW-1053\n    val = b'foo'\n    refcount_before = sys.getrefcount(val)\n    for i in range(10):\n        make_buffer(val)\n    gc.collect()\n    assert refcount_before == sys.getrefcount(val)\n\n\ndef test_nativefile_write_memoryview():\n    f = pa.BufferOutputStream()\n    data = b'ok'\n\n    arr = np.frombuffer(data, dtype='S1')\n\n    f.write(arr)\n    f.write(bytearray(data))\n    f.write(pa.py_buffer(data))\n    with pytest.raises(TypeError):\n        f.write(data.decode('utf8'))\n\n    buf = f.getvalue()\n\n    assert buf.to_pybytes() == data * 3\n\n\n# ----------------------------------------------------------------------\n# Mock output stream\n\n\ndef test_mock_output_stream():\n    # Make sure that the MockOutputStream and the BufferOutputStream record the\n    # same size\n\n    # 10 bytes\n    val = b'dataabcdef'\n\n    f1 = pa.MockOutputStream()\n    f2 = pa.BufferOutputStream()\n\n    K = 1000\n    for i in range(K):\n        f1.write(val)\n        f2.write(val)\n\n    assert f1.size() == len(f2.getvalue())\n\n    # Do the same test with a table\n    record_batch = pa.RecordBatch.from_arrays([pa.array([1, 2, 3])], ['a'])\n\n    f1 = pa.MockOutputStream()\n    f2 = pa.BufferOutputStream()\n\n    stream_writer1 = pa.RecordBatchStreamWriter(f1, record_batch.schema)\n    stream_writer2 = pa.RecordBatchStreamWriter(f2, record_batch.schema)\n\n    stream_writer1.write_batch(record_batch)\n    stream_writer2.write_batch(record_batch)\n    stream_writer1.close()\n    stream_writer2.close()\n\n    assert f1.size() == len(f2.getvalue())\n\n\n# ----------------------------------------------------------------------\n# OS files and memory maps\n\n\n@pytest.fixture\ndef sample_disk_data(request, tmpdir):\n    SIZE = 4096\n    arr = np.random.randint(0, 256, size=SIZE).astype('u1')\n    data = arr.tobytes()[:SIZE]\n\n    path = os.path.join(str(tmpdir), guid())\n\n    with open(path, 'wb') as f:\n        f.write(data)\n\n    def teardown():\n        _try_delete(path)\n\n    request.addfinalizer(teardown)\n    return path, data\n\n\ndef _check_native_file_reader(FACTORY, sample_data,\n                              allow_read_out_of_bounds=True):\n    path, data = sample_data\n\n    f = FACTORY(path, mode='r')\n\n    assert f.read(10) == data[:10]\n    assert f.read(0) == b''\n    assert f.tell() == 10\n\n    assert f.read() == data[10:]\n\n    assert f.size() == len(data)\n\n    f.seek(0)\n    assert f.tell() == 0\n\n    # Seeking past end of file not supported in memory maps\n    if allow_read_out_of_bounds:\n        f.seek(len(data) + 1)\n        assert f.tell() == len(data) + 1\n        assert f.read(5) == b''\n\n    # Test whence argument of seek, ARROW-1287\n    assert f.seek(3) == 3\n    assert f.seek(3, os.SEEK_CUR) == 6\n    assert f.tell() == 6\n\n    ex_length = len(data) - 2\n    assert f.seek(-2, os.SEEK_END) == ex_length\n    assert f.tell() == ex_length\n\n\ndef test_memory_map_reader(sample_disk_data):\n    _check_native_file_reader(pa.memory_map, sample_disk_data,\n                              allow_read_out_of_bounds=False)\n\n\ndef test_memory_map_retain_buffer_reference(sample_disk_data):\n    path, data = sample_disk_data\n\n    cases = []\n    with pa.memory_map(path, 'rb') as f:\n        cases.append((f.read_buffer(100), data[:100]))\n        cases.append((f.read_buffer(100), data[100:200]))\n        cases.append((f.read_buffer(100), data[200:300]))\n\n    # Call gc.collect() for good measure\n    gc.collect()\n\n    for buf, expected in cases:\n        assert buf.to_pybytes() == expected\n\n\ndef test_os_file_reader(sample_disk_data):\n    _check_native_file_reader(pa.OSFile, sample_disk_data)\n\n\ndef test_os_file_large_seeks():\n    check_large_seeks(pa.OSFile)\n\n\ndef _try_delete(path):\n    try:\n        os.remove(path)\n    except os.error:\n        pass\n\n\ndef test_memory_map_writer(tmpdir):\n    SIZE = 4096\n    arr = np.random.randint(0, 256, size=SIZE).astype('u1')\n    data = arr.tobytes()[:SIZE]\n\n    path = os.path.join(str(tmpdir), guid())\n    with open(path, 'wb') as f:\n        f.write(data)\n\n    f = pa.memory_map(path, mode='r+b')\n\n    f.seek(10)\n    f.write(b'peekaboo')\n    assert f.tell() == 18\n\n    f.seek(10)\n    assert f.read(8) == b'peekaboo'\n\n    f2 = pa.memory_map(path, mode='r+b')\n\n    f2.seek(10)\n    f2.write(b'booapeak')\n    f2.seek(10)\n\n    f.seek(10)\n    assert f.read(8) == b'booapeak'\n\n    # Does not truncate file\n    f3 = pa.memory_map(path, mode='w')\n    f3.write(b'foo')\n\n    with pa.memory_map(path) as f4:\n        assert f4.size() == SIZE\n\n    with pytest.raises(IOError):\n        f3.read(5)\n\n    f.seek(0)\n    assert f.read(3) == b'foo'\n\n\ndef test_memory_map_resize(tmpdir):\n    SIZE = 4096\n    arr = np.random.randint(0, 256, size=SIZE).astype(np.uint8)\n    data1 = arr.tobytes()[:(SIZE // 2)]\n    data2 = arr.tobytes()[(SIZE // 2):]\n\n    path = os.path.join(str(tmpdir), guid())\n\n    mmap = pa.create_memory_map(path, SIZE / 2)\n    mmap.write(data1)\n\n    mmap.resize(SIZE)\n    mmap.write(data2)\n\n    mmap.close()\n\n    with open(path, 'rb') as f:\n        assert f.read() == arr.tobytes()\n\n\ndef test_memory_zero_length(tmpdir):\n    path = os.path.join(str(tmpdir), guid())\n    f = open(path, 'wb')\n    f.close()\n    with pa.memory_map(path, mode='r+b') as memory_map:\n        assert memory_map.size() == 0\n\n\ndef test_memory_map_large_seeks():\n    if sys.maxsize >= 2**32:\n        expected_error = None\n    else:\n        expected_error = pytest.raises(\n            pa.ArrowCapacityError,\n            match=\"Requested memory map length 4294967306 \"\n                  \"does not fit in a C size_t\")\n    check_large_seeks(pa.memory_map, expected_error=expected_error)\n\n\ndef test_memory_map_close_remove(tmpdir):\n    # ARROW-6740: should be able to delete closed memory-mapped file (Windows)\n    path = os.path.join(str(tmpdir), guid())\n    mmap = pa.create_memory_map(path, 4096)\n    mmap.close()\n    assert mmap.closed\n    os.remove(path)  # Shouldn't fail\n\n\ndef test_memory_map_deref_remove(tmpdir):\n    path = os.path.join(str(tmpdir), guid())\n    pa.create_memory_map(path, 4096)\n    os.remove(path)  # Shouldn't fail\n\n\ndef test_os_file_writer(tmpdir):\n    SIZE = 4096\n    arr = np.random.randint(0, 256, size=SIZE).astype('u1')\n    data = arr.tobytes()[:SIZE]\n\n    path = os.path.join(str(tmpdir), guid())\n    with open(path, 'wb') as f:\n        f.write(data)\n\n    # Truncates file\n    f2 = pa.OSFile(path, mode='w')\n    f2.write(b'foo')\n\n    with pa.OSFile(path) as f3:\n        assert f3.size() == 3\n\n    with pytest.raises(IOError):\n        f2.read(5)\n    f2.close()\n\n    # Append\n    with pa.OSFile(path, mode='ab') as f4:\n        f4.write(b'bar')\n    with pa.OSFile(path) as f5:\n        assert f5.size() == 6  # foo + bar\n\n\ndef test_native_file_write_reject_unicode():\n    # ARROW-3227\n    nf = pa.BufferOutputStream()\n    with pytest.raises(TypeError):\n        nf.write('foo')\n\n\ndef test_native_file_modes(tmpdir):\n    path = os.path.join(str(tmpdir), guid())\n    with open(path, 'wb') as f:\n        f.write(b'foooo')\n\n    with pa.OSFile(path, mode='r') as f:\n        assert f.mode == 'rb'\n        assert f.readable()\n        assert not f.writable()\n        assert f.seekable()\n\n    with pa.OSFile(path, mode='rb') as f:\n        assert f.mode == 'rb'\n        assert f.readable()\n        assert not f.writable()\n        assert f.seekable()\n\n    with pa.OSFile(path, mode='w') as f:\n        assert f.mode == 'wb'\n        assert not f.readable()\n        assert f.writable()\n        assert not f.seekable()\n\n    with pa.OSFile(path, mode='wb') as f:\n        assert f.mode == 'wb'\n        assert not f.readable()\n        assert f.writable()\n        assert not f.seekable()\n\n    with pa.OSFile(path, mode='ab') as f:\n        assert f.mode == 'ab'\n        assert not f.readable()\n        assert f.writable()\n        assert not f.seekable()\n\n    with pa.OSFile(path, mode='a') as f:\n        assert f.mode == 'ab'\n        assert not f.readable()\n        assert f.writable()\n        assert not f.seekable()\n\n    with open(path, 'wb') as f:\n        f.write(b'foooo')\n\n    with pa.memory_map(path, 'r') as f:\n        assert f.mode == 'rb'\n        assert f.readable()\n        assert not f.writable()\n        assert f.seekable()\n\n    with pa.memory_map(path, 'r+') as f:\n        assert f.mode == 'rb+'\n        assert f.readable()\n        assert f.writable()\n        assert f.seekable()\n\n    with pa.memory_map(path, 'r+b') as f:\n        assert f.mode == 'rb+'\n        assert f.readable()\n        assert f.writable()\n        assert f.seekable()\n\n\ndef test_native_file_permissions(tmpdir):\n    # ARROW-10124: permissions of created files should follow umask\n    cur_umask = os.umask(0o002)\n    os.umask(cur_umask)\n\n    path = os.path.join(str(tmpdir), guid())\n    with pa.OSFile(path, mode='w'):\n        pass\n    assert os.stat(path).st_mode & 0o777 == 0o666 & ~cur_umask\n\n    path = os.path.join(str(tmpdir), guid())\n    with pa.memory_map(path, 'w'):\n        pass\n    assert os.stat(path).st_mode & 0o777 == 0o666 & ~cur_umask\n\n\ndef test_native_file_raises_ValueError_after_close(tmpdir):\n    path = os.path.join(str(tmpdir), guid())\n    with open(path, 'wb') as f:\n        f.write(b'foooo')\n\n    with pa.OSFile(path, mode='rb') as os_file:\n        assert not os_file.closed\n    assert os_file.closed\n\n    with pa.memory_map(path, mode='rb') as mmap_file:\n        assert not mmap_file.closed\n    assert mmap_file.closed\n\n    files = [os_file,\n             mmap_file]\n\n    methods = [('tell', ()),\n               ('seek', (0,)),\n               ('size', ()),\n               ('flush', ()),\n               ('readable', ()),\n               ('writable', ()),\n               ('seekable', ())]\n\n    for f in files:\n        for method, args in methods:\n            with pytest.raises(ValueError):\n                getattr(f, method)(*args)\n\n\ndef test_native_file_TextIOWrapper(tmpdir):\n    data = ('foooo\\n'\n            'barrr\\n'\n            'bazzz\\n')\n\n    path = os.path.join(str(tmpdir), guid())\n    with open(path, 'wb') as f:\n        f.write(data.encode('utf-8'))\n\n    with TextIOWrapper(pa.OSFile(path, mode='rb')) as fil:\n        assert fil.readable()\n        res = fil.read()\n        assert res == data\n    assert fil.closed\n\n    with TextIOWrapper(pa.OSFile(path, mode='rb')) as fil:\n        # Iteration works\n        lines = list(fil)\n        assert ''.join(lines) == data\n\n    # Writing\n    path2 = os.path.join(str(tmpdir), guid())\n    with TextIOWrapper(pa.OSFile(path2, mode='wb')) as fil:\n        assert fil.writable()\n        fil.write(data)\n\n    with TextIOWrapper(pa.OSFile(path2, mode='rb')) as fil:\n        res = fil.read()\n        assert res == data\n\n\ndef test_native_file_TextIOWrapper_perf(tmpdir):\n    # ARROW-16272: TextIOWrapper.readline() shouldn't exhaust a large\n    # Arrow input stream.\n    data = b'foo\\nquux\\n'\n    path = str(tmpdir / 'largefile.txt')\n    with open(path, 'wb') as f:\n        f.write(data * 100_000)\n\n    binary_file = pa.OSFile(path, mode='rb')\n    with TextIOWrapper(binary_file) as f:\n        assert binary_file.tell() == 0\n        nbytes = 20_000\n        lines = f.readlines(nbytes)\n        assert len(lines) == math.ceil(2 * nbytes / len(data))\n        assert nbytes <= binary_file.tell() <= nbytes * 2\n\n\ndef test_native_file_read1(tmpdir):\n    # ARROW-16272: read1() should not exhaust the input stream if there\n    # is a large amount of data remaining.\n    data = b'123\\n' * 1_000_000\n    path = str(tmpdir / 'largefile.txt')\n    with open(path, 'wb') as f:\n        f.write(data)\n\n    chunks = []\n    with pa.OSFile(path, mode='rb') as f:\n        while True:\n            b = f.read1()\n            assert len(b) < len(data)\n            chunks.append(b)\n            b = f.read1(30_000)\n            assert len(b) <= 30_000\n            chunks.append(b)\n            if not b:\n                break\n\n    assert b\"\".join(chunks) == data\n\n\n@pytest.mark.pandas\ndef test_native_file_pandas_text_reader(tmpdir):\n    # ARROW-16272: Pandas' read_csv() should not exhaust an Arrow\n    # input stream when a small nrows is passed.\n    import pandas as pd\n    import pandas.testing as tm\n    data = b'a,b\\n' * 10_000_000\n    path = str(tmpdir / 'largefile.txt')\n    with open(path, 'wb') as f:\n        f.write(data)\n\n    with pa.OSFile(path, mode='rb') as f:\n        df = pd.read_csv(f, nrows=10)\n        expected = pd.DataFrame({'a': ['a'] * 10, 'b': ['b'] * 10})\n        tm.assert_frame_equal(df, expected)\n        # Some readahead occurred, but not up to the end of file\n        assert f.tell() <= 256 * 1024\n\n\ndef test_native_file_open_error():\n    with assert_file_not_found():\n        pa.OSFile('non_existent_file', 'rb')\n    with assert_file_not_found():\n        pa.memory_map('non_existent_file', 'rb')\n\n\n# ----------------------------------------------------------------------\n# Buffered streams\n\ndef test_buffered_input_stream():\n    raw = pa.BufferReader(b\"123456789\")\n    f = pa.BufferedInputStream(raw, buffer_size=4)\n    assert f.read(2) == b\"12\"\n    assert raw.tell() == 4\n    f.close()\n    assert f.closed\n    assert raw.closed\n\n\ndef test_buffered_input_stream_detach_seekable():\n    # detach() to a seekable file (io::RandomAccessFile in C++)\n    f = pa.BufferedInputStream(pa.BufferReader(b\"123456789\"), buffer_size=4)\n    assert f.read(2) == b\"12\"\n    raw = f.detach()\n    assert f.closed\n    assert not raw.closed\n    assert raw.seekable()\n    assert raw.read(4) == b\"5678\"\n    raw.seek(2)\n    assert raw.read(4) == b\"3456\"\n\n\ndef test_buffered_input_stream_detach_non_seekable():\n    # detach() to a non-seekable file (io::InputStream in C++)\n    f = pa.BufferedInputStream(\n        pa.BufferedInputStream(pa.BufferReader(b\"123456789\"), buffer_size=4),\n        buffer_size=4)\n    assert f.read(2) == b\"12\"\n    raw = f.detach()\n    assert f.closed\n    assert not raw.closed\n    assert not raw.seekable()\n    assert raw.read(4) == b\"5678\"\n    with pytest.raises(EnvironmentError):\n        raw.seek(2)\n\n\ndef test_buffered_output_stream():\n    np_buf = np.zeros(100, dtype=np.int8)  # zero-initialized buffer\n    buf = pa.py_buffer(np_buf)\n\n    raw = pa.FixedSizeBufferWriter(buf)\n    f = pa.BufferedOutputStream(raw, buffer_size=4)\n    f.write(b\"12\")\n    assert np_buf[:4].tobytes() == b'\\0\\0\\0\\0'\n    f.flush()\n    assert np_buf[:4].tobytes() == b'12\\0\\0'\n    f.write(b\"3456789\")\n    f.close()\n    assert f.closed\n    assert raw.closed\n    assert np_buf[:10].tobytes() == b'123456789\\0'\n\n\ndef test_buffered_output_stream_detach():\n    np_buf = np.zeros(100, dtype=np.int8)  # zero-initialized buffer\n    buf = pa.py_buffer(np_buf)\n\n    f = pa.BufferedOutputStream(pa.FixedSizeBufferWriter(buf), buffer_size=4)\n    f.write(b\"12\")\n    assert np_buf[:4].tobytes() == b'\\0\\0\\0\\0'\n    raw = f.detach()\n    assert f.closed\n    assert not raw.closed\n    assert np_buf[:4].tobytes() == b'12\\0\\0'\n\n\n# ----------------------------------------------------------------------\n# Compressed input and output streams\n\ndef check_compressed_input(data, fn, compression):\n    raw = pa.OSFile(fn, mode=\"rb\")\n    with pa.CompressedInputStream(raw, compression) as compressed:\n        assert not compressed.closed\n        assert compressed.readable()\n        assert not compressed.writable()\n        assert not compressed.seekable()\n        got = compressed.read()\n        assert got == data\n    assert compressed.closed\n    assert raw.closed\n\n    # Same with read_buffer()\n    raw = pa.OSFile(fn, mode=\"rb\")\n    with pa.CompressedInputStream(raw, compression) as compressed:\n        buf = compressed.read_buffer()\n        assert isinstance(buf, pa.Buffer)\n        assert buf.to_pybytes() == data\n\n\n@pytest.mark.gzip\ndef test_compressed_input_gzip(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    fn = str(tmpdir / \"compressed_input_test.gz\")\n    with gzip.open(fn, \"wb\") as f:\n        f.write(data)\n    check_compressed_input(data, fn, \"gzip\")\n\n\ndef test_compressed_input_bz2(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    fn = str(tmpdir / \"compressed_input_test.bz2\")\n    with bz2.BZ2File(fn, \"w\") as f:\n        f.write(data)\n    try:\n        check_compressed_input(data, fn, \"bz2\")\n    except NotImplementedError as e:\n        pytest.skip(str(e))\n\n\n@pytest.mark.gzip\ndef test_compressed_input_openfile(tmpdir):\n    if not Codec.is_available(\"gzip\"):\n        pytest.skip(\"gzip support is not built\")\n\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    fn = str(tmpdir / \"test_compressed_input_openfile.gz\")\n    with gzip.open(fn, \"wb\") as f:\n        f.write(data)\n\n    with pa.CompressedInputStream(fn, \"gzip\") as compressed:\n        buf = compressed.read_buffer()\n        assert buf.to_pybytes() == data\n    assert compressed.closed\n\n    with pa.CompressedInputStream(pathlib.Path(fn), \"gzip\") as compressed:\n        buf = compressed.read_buffer()\n        assert buf.to_pybytes() == data\n    assert compressed.closed\n\n    f = open(fn, \"rb\")\n    with pa.CompressedInputStream(f, \"gzip\") as compressed:\n        buf = compressed.read_buffer()\n        assert buf.to_pybytes() == data\n    assert f.closed\n\n\ndef check_compressed_concatenated(data, fn, compression):\n    raw = pa.OSFile(fn, mode=\"rb\")\n    with pa.CompressedInputStream(raw, compression) as compressed:\n        got = compressed.read()\n        assert got == data\n\n\n@pytest.mark.gzip\ndef test_compressed_concatenated_gzip(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    fn = str(tmpdir / \"compressed_input_test2.gz\")\n    with gzip.open(fn, \"wb\") as f:\n        f.write(data[:50])\n    with gzip.open(fn, \"ab\") as f:\n        f.write(data[50:])\n    check_compressed_concatenated(data, fn, \"gzip\")\n\n\n@pytest.mark.gzip\ndef test_compressed_input_invalid():\n    data = b\"foo\" * 10\n    raw = pa.BufferReader(data)\n    with pytest.raises(ValueError):\n        pa.CompressedInputStream(raw, \"unknown_compression\")\n    with pytest.raises(TypeError):\n        pa.CompressedInputStream(raw, None)\n\n    with pa.CompressedInputStream(raw, \"gzip\") as compressed:\n        with pytest.raises(IOError, match=\"zlib inflate failed\"):\n            compressed.read()\n\n\ndef make_compressed_output(data, fn, compression):\n    raw = pa.BufferOutputStream()\n    with pa.CompressedOutputStream(raw, compression) as compressed:\n        assert not compressed.closed\n        assert not compressed.readable()\n        assert compressed.writable()\n        assert not compressed.seekable()\n        compressed.write(data)\n    assert compressed.closed\n    assert raw.closed\n    with open(fn, \"wb\") as f:\n        f.write(raw.getvalue())\n\n\n@pytest.mark.gzip\ndef test_compressed_output_gzip(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    fn = str(tmpdir / \"compressed_output_test.gz\")\n    make_compressed_output(data, fn, \"gzip\")\n    with gzip.open(fn, \"rb\") as f:\n        got = f.read()\n        assert got == data\n\n\ndef test_compressed_output_bz2(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    fn = str(tmpdir / \"compressed_output_test.bz2\")\n    try:\n        make_compressed_output(data, fn, \"bz2\")\n    except NotImplementedError as e:\n        pytest.skip(str(e))\n    with bz2.BZ2File(fn, \"r\") as f:\n        got = f.read()\n        assert got == data\n\n\ndef test_output_stream_constructor(tmpdir):\n    if not Codec.is_available(\"gzip\"):\n        pytest.skip(\"gzip support is not built\")\n    with pa.CompressedOutputStream(tmpdir / \"ctor.gz\", \"gzip\") as stream:\n        stream.write(b\"test\")\n    with (tmpdir / \"ctor2.gz\").open(\"wb\") as f:\n        with pa.CompressedOutputStream(f, \"gzip\") as stream:\n            stream.write(b\"test\")\n\n\n@pytest.mark.parametrize((\"path\", \"expected_compression\"), [\n    (\"file.bz2\", \"bz2\"),\n    (\"file.lz4\", \"lz4\"),\n    (pathlib.Path(\"file.gz\"), \"gzip\"),\n    (pathlib.Path(\"path/to/file.zst\"), \"zstd\"),\n])\ndef test_compression_detection(path, expected_compression):\n    if not Codec.is_available(expected_compression):\n        with pytest.raises(pa.lib.ArrowNotImplementedError):\n            Codec.detect(path)\n    else:\n        codec = Codec.detect(path)\n        assert isinstance(codec, Codec)\n        assert codec.name == expected_compression\n\n\ndef test_unknown_compression_raises():\n    with pytest.raises(ValueError):\n        Codec.is_available('unknown')\n    with pytest.raises(TypeError):\n        Codec(None)\n    with pytest.raises(ValueError):\n        Codec('unknown')\n\n\n@pytest.mark.parametrize(\"compression\", [\n    \"bz2\",\n    \"brotli\",\n    \"gzip\",\n    \"lz4\",\n    \"zstd\",\n    pytest.param(\n        \"snappy\",\n        marks=pytest.mark.xfail(raises=pa.lib.ArrowNotImplementedError)\n    )\n])\ndef test_compressed_roundtrip(compression):\n    if not Codec.is_available(compression):\n        pytest.skip(\"{} support is not built\".format(compression))\n\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    raw = pa.BufferOutputStream()\n    with pa.CompressedOutputStream(raw, compression) as compressed:\n        compressed.write(data)\n\n    cdata = raw.getvalue()\n    assert len(cdata) < len(data)\n    raw = pa.BufferReader(cdata)\n    with pa.CompressedInputStream(raw, compression) as compressed:\n        got = compressed.read()\n        assert got == data\n\n\n@pytest.mark.parametrize(\n    \"compression\",\n    [\"bz2\", \"brotli\", \"gzip\", \"lz4\", \"zstd\"]\n)\ndef test_compressed_recordbatch_stream(compression):\n    if not Codec.is_available(compression):\n        pytest.skip(\"{} support is not built\".format(compression))\n\n    # ARROW-4836: roundtrip a RecordBatch through a compressed stream\n    table = pa.Table.from_arrays([pa.array([1, 2, 3, 4, 5])], ['a'])\n    raw = pa.BufferOutputStream()\n    stream = pa.CompressedOutputStream(raw, compression)\n    writer = pa.RecordBatchStreamWriter(stream, table.schema)\n    writer.write_table(table, max_chunksize=3)\n    writer.close()\n    stream.close()  # Flush data\n    buf = raw.getvalue()\n    stream = pa.CompressedInputStream(pa.BufferReader(buf), compression)\n    got_table = pa.RecordBatchStreamReader(stream).read_all()\n    assert got_table == table\n\n\n# ----------------------------------------------------------------------\n# Transform input streams\n\nunicode_transcoding_example = (\n    \"D\u00e8s No\u00ebl o\u00f9 un z\u00e9phyr ha\u00ef me v\u00eat de gla\u00e7ons w\u00fcrmiens \"\n    \"je d\u00eene d\u2019exquis r\u00f4tis de b\u0153uf au kir \u00e0 l\u2019a\u00ff d\u2019\u00e2ge m\u00fbr & c\u00e6tera !\"\n)\n\n\ndef check_transcoding(data, src_encoding, dest_encoding, chunk_sizes):\n    chunk_sizes = iter(chunk_sizes)\n    stream = pa.transcoding_input_stream(\n        pa.BufferReader(data.encode(src_encoding)),\n        src_encoding, dest_encoding)\n    out = []\n    while True:\n        buf = stream.read(next(chunk_sizes))\n        out.append(buf)\n        if not buf:\n            break\n    out = b''.join(out)\n    assert out.decode(dest_encoding) == data\n\n\n@pytest.mark.parametrize('src_encoding, dest_encoding',\n                         [('utf-8', 'utf-16'),\n                          ('utf-16', 'utf-8'),\n                          ('utf-8', 'utf-32-le'),\n                          ('utf-8', 'utf-32-be'),\n                          ])\ndef test_transcoding_input_stream(src_encoding, dest_encoding):\n    # All at once\n    check_transcoding(unicode_transcoding_example,\n                      src_encoding, dest_encoding, [1000, 0])\n    # Incremental\n    check_transcoding(unicode_transcoding_example,\n                      src_encoding, dest_encoding,\n                      itertools.cycle([1, 2, 3, 5]))\n\n\n@pytest.mark.parametrize('src_encoding, dest_encoding',\n                         [('utf-8', 'utf-8'),\n                          ('utf-8', 'UTF8')])\ndef test_transcoding_no_ops(src_encoding, dest_encoding):\n    # No indirection is wasted when a trivial transcoding is requested\n    stream = pa.BufferReader(b\"abc123\")\n    assert pa.transcoding_input_stream(\n        stream, src_encoding, dest_encoding) is stream\n\n\n@pytest.mark.parametrize('src_encoding, dest_encoding',\n                         [('utf-8', 'ascii'),\n                          ('utf-8', 'latin-1'),\n                          ])\ndef test_transcoding_encoding_error(src_encoding, dest_encoding):\n    # Character \\u0100 cannot be represented in the destination encoding\n    stream = pa.transcoding_input_stream(\n        pa.BufferReader(\"\\u0100\".encode(src_encoding)),\n        src_encoding,\n        dest_encoding)\n    with pytest.raises(UnicodeEncodeError):\n        stream.read(1)\n\n\n@pytest.mark.parametrize('src_encoding, dest_encoding',\n                         [('utf-8', 'utf-16'),\n                          ('utf-16', 'utf-8'),\n                          ])\ndef test_transcoding_decoding_error(src_encoding, dest_encoding):\n    # The given bytestring is not valid in the source encoding\n    stream = pa.transcoding_input_stream(\n        pa.BufferReader(b\"\\xff\\xff\\xff\\xff\"),\n        src_encoding,\n        dest_encoding)\n    with pytest.raises(UnicodeError):\n        stream.read(1)\n\n\n# ----------------------------------------------------------------------\n# High-level API\n\n@pytest.mark.gzip\ndef test_input_stream_buffer():\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    for arg in [pa.py_buffer(data), memoryview(data)]:\n        stream = pa.input_stream(arg)\n        assert stream.read() == data\n\n    gz_data = gzip.compress(data)\n    stream = pa.input_stream(memoryview(gz_data))\n    assert stream.read() == gz_data\n    stream = pa.input_stream(memoryview(gz_data), compression='gzip')\n    assert stream.read() == data\n\n\ndef test_input_stream_duck_typing():\n    # Accept objects having the right file-like methods...\n    class DuckReader:\n\n        def close(self):\n            pass\n\n        @property\n        def closed(self):\n            return False\n\n        def read(self, nbytes=None):\n            return b'hello'\n\n    stream = pa.input_stream(DuckReader())\n    assert stream.read(5) == b'hello'\n\n\ndef test_input_stream_file_path(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    file_path = tmpdir / 'input_stream'\n    with open(str(file_path), 'wb') as f:\n        f.write(data)\n\n    stream = pa.input_stream(file_path)\n    assert stream.read() == data\n    stream = pa.input_stream(str(file_path))\n    assert stream.read() == data\n    stream = pa.input_stream(pathlib.Path(str(file_path)))\n    assert stream.read() == data\n\n\n@pytest.mark.gzip\ndef test_input_stream_file_path_compressed(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    gz_data = gzip.compress(data)\n    file_path = tmpdir / 'input_stream.gz'\n    with open(str(file_path), 'wb') as f:\n        f.write(gz_data)\n\n    stream = pa.input_stream(file_path)\n    assert stream.read() == data\n    stream = pa.input_stream(str(file_path))\n    assert stream.read() == data\n    stream = pa.input_stream(pathlib.Path(str(file_path)))\n    assert stream.read() == data\n\n    stream = pa.input_stream(file_path, compression='gzip')\n    assert stream.read() == data\n    stream = pa.input_stream(file_path, compression=None)\n    assert stream.read() == gz_data\n\n\ndef test_input_stream_file_path_buffered(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    file_path = tmpdir / 'input_stream.buffered'\n    with open(str(file_path), 'wb') as f:\n        f.write(data)\n\n    stream = pa.input_stream(file_path, buffer_size=32)\n    assert isinstance(stream, pa.BufferedInputStream)\n    assert stream.read() == data\n    stream = pa.input_stream(str(file_path), buffer_size=64)\n    assert isinstance(stream, pa.BufferedInputStream)\n    assert stream.read() == data\n    stream = pa.input_stream(pathlib.Path(str(file_path)), buffer_size=1024)\n    assert isinstance(stream, pa.BufferedInputStream)\n    assert stream.read() == data\n\n    unbuffered_stream = pa.input_stream(file_path, buffer_size=0)\n    assert isinstance(unbuffered_stream, pa.OSFile)\n\n    msg = 'Buffer size must be larger than zero'\n    with pytest.raises(ValueError, match=msg):\n        pa.input_stream(file_path, buffer_size=-1)\n    with pytest.raises(TypeError):\n        pa.input_stream(file_path, buffer_size='million')\n\n\n@pytest.mark.gzip\ndef test_input_stream_file_path_compressed_and_buffered(tmpdir):\n    data = b\"some test data\\n\" * 100 + b\"eof\\n\"\n    gz_data = gzip.compress(data)\n    file_path = tmpdir / 'input_stream_compressed_and_buffered.gz'\n    with open(str(file_path), 'wb') as f:\n        f.write(gz_data)\n\n    stream = pa.input_stream(file_path, buffer_size=32, compression='gzip')\n    assert stream.read() == data\n    stream = pa.input_stream(str(file_path), buffer_size=64)\n    assert stream.read() == data\n    stream = pa.input_stream(pathlib.Path(str(file_path)), buffer_size=1024)\n    assert stream.read() == data\n\n\n@pytest.mark.gzip\ndef test_input_stream_python_file(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    bio = BytesIO(data)\n\n    stream = pa.input_stream(bio)\n    assert stream.read() == data\n\n    gz_data = gzip.compress(data)\n    bio = BytesIO(gz_data)\n    stream = pa.input_stream(bio)\n    assert stream.read() == gz_data\n    bio.seek(0)\n    stream = pa.input_stream(bio, compression='gzip')\n    assert stream.read() == data\n\n    file_path = tmpdir / 'input_stream'\n    with open(str(file_path), 'wb') as f:\n        f.write(data)\n    with open(str(file_path), 'rb') as f:\n        stream = pa.input_stream(f)\n        assert stream.read() == data\n\n\n@pytest.mark.gzip\ndef test_input_stream_native_file():\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    gz_data = gzip.compress(data)\n    reader = pa.BufferReader(gz_data)\n    stream = pa.input_stream(reader)\n    assert stream is reader\n    reader = pa.BufferReader(gz_data)\n    stream = pa.input_stream(reader, compression='gzip')\n    assert stream.read() == data\n\n\ndef test_input_stream_errors(tmpdir):\n    buf = memoryview(b\"\")\n    with pytest.raises(ValueError):\n        pa.input_stream(buf, compression=\"foo\")\n\n    for arg in [bytearray(), StringIO()]:\n        with pytest.raises(TypeError):\n            pa.input_stream(arg)\n\n    with assert_file_not_found():\n        pa.input_stream(\"non_existent_file\")\n\n    with open(str(tmpdir / 'new_file'), 'wb') as f:\n        with pytest.raises(TypeError, match=\"readable file expected\"):\n            pa.input_stream(f)\n\n\ndef test_output_stream_buffer():\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    buf = bytearray(len(data))\n    stream = pa.output_stream(pa.py_buffer(buf))\n    stream.write(data)\n    assert buf == data\n\n    buf = bytearray(len(data))\n    stream = pa.output_stream(memoryview(buf))\n    stream.write(data)\n    assert buf == data\n\n\ndef test_output_stream_duck_typing():\n    # Accept objects having the right file-like methods...\n    class DuckWriter:\n        def __init__(self):\n            self.buf = pa.BufferOutputStream()\n\n        def close(self):\n            pass\n\n        @property\n        def closed(self):\n            return False\n\n        def write(self, data):\n            self.buf.write(data)\n\n    duck_writer = DuckWriter()\n    stream = pa.output_stream(duck_writer)\n    assert stream.write(b'hello')\n    assert duck_writer.buf.getvalue().to_pybytes() == b'hello'\n\n\ndef test_output_stream_file_path(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    file_path = tmpdir / 'output_stream'\n\n    def check_data(file_path, data):\n        with pa.output_stream(file_path) as stream:\n            stream.write(data)\n        with open(str(file_path), 'rb') as f:\n            assert f.read() == data\n\n    check_data(file_path, data)\n    check_data(str(file_path), data)\n    check_data(pathlib.Path(str(file_path)), data)\n\n\n@pytest.mark.gzip\ndef test_output_stream_file_path_compressed(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    file_path = tmpdir / 'output_stream.gz'\n\n    def check_data(file_path, data, **kwargs):\n        with pa.output_stream(file_path, **kwargs) as stream:\n            stream.write(data)\n        with open(str(file_path), 'rb') as f:\n            return f.read()\n\n    assert gzip.decompress(check_data(file_path, data)) == data\n    assert gzip.decompress(check_data(str(file_path), data)) == data\n    assert gzip.decompress(\n        check_data(pathlib.Path(str(file_path)), data)) == data\n\n    assert gzip.decompress(\n        check_data(file_path, data, compression='gzip')) == data\n    assert check_data(file_path, data, compression=None) == data\n\n    with pytest.raises(ValueError, match='Invalid value for compression'):\n        assert check_data(file_path, data, compression='rabbit') == data\n\n\ndef test_output_stream_file_path_buffered(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n    file_path = tmpdir / 'output_stream.buffered'\n\n    def check_data(file_path, data, **kwargs):\n        with pa.output_stream(file_path, **kwargs) as stream:\n            if kwargs.get('buffer_size', 0) > 0:\n                assert isinstance(stream, pa.BufferedOutputStream)\n            stream.write(data)\n        with open(str(file_path), 'rb') as f:\n            return f.read()\n\n    unbuffered_stream = pa.output_stream(file_path, buffer_size=0)\n    assert isinstance(unbuffered_stream, pa.OSFile)\n\n    msg = 'Buffer size must be larger than zero'\n    with pytest.raises(ValueError, match=msg):\n        assert check_data(file_path, data, buffer_size=-128) == data\n\n    assert check_data(file_path, data, buffer_size=32) == data\n    assert check_data(file_path, data, buffer_size=1024) == data\n    assert check_data(str(file_path), data, buffer_size=32) == data\n\n    result = check_data(pathlib.Path(str(file_path)), data, buffer_size=32)\n    assert result == data\n\n\n@pytest.mark.gzip\ndef test_output_stream_file_path_compressed_and_buffered(tmpdir):\n    data = b\"some test data\\n\" * 100 + b\"eof\\n\"\n    file_path = tmpdir / 'output_stream_compressed_and_buffered.gz'\n\n    def check_data(file_path, data, **kwargs):\n        with pa.output_stream(file_path, **kwargs) as stream:\n            stream.write(data)\n        with open(str(file_path), 'rb') as f:\n            return f.read()\n\n    result = check_data(file_path, data, buffer_size=32)\n    assert gzip.decompress(result) == data\n\n    result = check_data(file_path, data, buffer_size=1024)\n    assert gzip.decompress(result) == data\n\n    result = check_data(file_path, data, buffer_size=1024, compression='gzip')\n    assert gzip.decompress(result) == data\n\n\ndef test_output_stream_destructor(tmpdir):\n    # The wrapper returned by pa.output_stream() should respect Python\n    # file semantics, i.e. destroying it should close the underlying\n    # file cleanly.\n    data = b\"some test data\\n\"\n    file_path = tmpdir / 'output_stream.buffered'\n\n    def check_data(file_path, data, **kwargs):\n        stream = pa.output_stream(file_path, **kwargs)\n        stream.write(data)\n        del stream\n        gc.collect()\n        with open(str(file_path), 'rb') as f:\n            return f.read()\n\n    assert check_data(file_path, data, buffer_size=0) == data\n    assert check_data(file_path, data, buffer_size=1024) == data\n\n\n@pytest.mark.gzip\ndef test_output_stream_python_file(tmpdir):\n    data = b\"some test data\\n\" * 10 + b\"eof\\n\"\n\n    def check_data(data, **kwargs):\n        # XXX cannot use BytesIO because stream.close() is necessary\n        # to finish writing compressed data, but it will also close the\n        # underlying BytesIO\n        fn = str(tmpdir / 'output_stream_file')\n        with open(fn, 'wb') as f:\n            with pa.output_stream(f, **kwargs) as stream:\n                stream.write(data)\n        with open(fn, 'rb') as f:\n            return f.read()\n\n    assert check_data(data) == data\n    assert gzip.decompress(check_data(data, compression='gzip')) == data\n\n\ndef test_output_stream_errors(tmpdir):\n    buf = memoryview(bytearray())\n    with pytest.raises(ValueError):\n        pa.output_stream(buf, compression=\"foo\")\n\n    for arg in [bytearray(), StringIO()]:\n        with pytest.raises(TypeError):\n            pa.output_stream(arg)\n\n    fn = str(tmpdir / 'new_file')\n    with open(fn, 'wb') as f:\n        pass\n    with open(fn, 'rb') as f:\n        with pytest.raises(TypeError, match=\"writable file expected\"):\n            pa.output_stream(f)\n", "python/pyarrow/tests/test_memory.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport os\nimport signal\nimport subprocess\nimport sys\nimport weakref\n\nimport pyarrow as pa\n\nimport pytest\n\n\npossible_backends = [\"system\", \"jemalloc\", \"mimalloc\"]\n\nshould_have_jemalloc = sys.platform == \"linux\"\nshould_have_mimalloc = sys.platform == \"win32\"\n\n\ndef supported_factories():\n    yield pa.default_memory_pool\n    for backend in pa.supported_memory_backends():\n        yield getattr(pa, f\"{backend}_memory_pool\")\n\n\n@contextlib.contextmanager\ndef allocate_bytes(pool, nbytes):\n    \"\"\"\n    Temporarily allocate *nbytes* from the given *pool*.\n    \"\"\"\n    arr = pa.array([b\"x\" * nbytes], type=pa.binary(), memory_pool=pool)\n    # Fetch the values buffer from the varbinary array and release the rest,\n    # to get the desired allocation amount\n    buf = arr.buffers()[2]\n    arr = None\n    assert len(buf) == nbytes\n    try:\n        yield\n    finally:\n        buf = None\n\n\ndef check_allocated_bytes(pool):\n    \"\"\"\n    Check allocation stats on *pool*.\n    \"\"\"\n    allocated_before = pool.bytes_allocated()\n    max_mem_before = pool.max_memory()\n    with allocate_bytes(pool, 512):\n        assert pool.bytes_allocated() == allocated_before + 512\n        new_max_memory = pool.max_memory()\n        assert pool.max_memory() >= max_mem_before\n    assert pool.bytes_allocated() == allocated_before\n    assert pool.max_memory() == new_max_memory\n\n\ndef test_default_allocated_bytes():\n    pool = pa.default_memory_pool()\n    with allocate_bytes(pool, 1024):\n        check_allocated_bytes(pool)\n        assert pool.bytes_allocated() == pa.total_allocated_bytes()\n\n\ndef test_proxy_memory_pool():\n    pool = pa.proxy_memory_pool(pa.default_memory_pool())\n    check_allocated_bytes(pool)\n    wr = weakref.ref(pool)\n    assert wr() is not None\n    del pool\n    assert wr() is None\n\n\ndef test_logging_memory_pool(capfd):\n    pool = pa.logging_memory_pool(pa.default_memory_pool())\n    check_allocated_bytes(pool)\n    out, err = capfd.readouterr()\n    assert err == \"\"\n    assert out.count(\"Allocate:\") > 0\n    assert out.count(\"Allocate:\") == out.count(\"Free:\")\n\n\ndef test_set_memory_pool():\n    old_pool = pa.default_memory_pool()\n    pool = pa.proxy_memory_pool(old_pool)\n    pa.set_memory_pool(pool)\n    try:\n        allocated_before = pool.bytes_allocated()\n        with allocate_bytes(None, 512):\n            assert pool.bytes_allocated() == allocated_before + 512\n        assert pool.bytes_allocated() == allocated_before\n    finally:\n        pa.set_memory_pool(old_pool)\n\n\ndef test_default_backend_name():\n    pool = pa.default_memory_pool()\n    assert pool.backend_name in possible_backends\n\n\ndef test_release_unused():\n    pool = pa.default_memory_pool()\n    pool.release_unused()\n\n\ndef check_env_var(name, expected, *, expect_warning=False):\n    code = f\"\"\"if 1:\n        import pyarrow as pa\n\n        pool = pa.default_memory_pool()\n        assert pool.backend_name in {expected!r}, pool.backend_name\n        \"\"\"\n    env = dict(os.environ)\n    env['ARROW_DEFAULT_MEMORY_POOL'] = name\n    res = subprocess.run([sys.executable, \"-c\", code], env=env,\n                         universal_newlines=True, stderr=subprocess.PIPE)\n    if res.returncode != 0:\n        print(res.stderr, file=sys.stderr)\n        res.check_returncode()  # fail\n    errlines = res.stderr.splitlines()\n    if expect_warning:\n        assert len(errlines) in (1, 2)\n        if len(errlines) == 1:\n            # ARROW_USE_GLOG=OFF\n            assert f\"Unsupported backend '{name}'\" in errlines[0]\n        else:\n            # ARROW_USE_GLOG=ON\n            assert \"InitGoogleLogging()\" in errlines[0]\n            assert f\"Unsupported backend '{name}'\" in errlines[1]\n    else:\n        assert len(errlines) == 0\n\n\ndef test_env_var():\n    check_env_var(\"system\", [\"system\"])\n    if should_have_jemalloc:\n        check_env_var(\"jemalloc\", [\"jemalloc\"])\n    if should_have_mimalloc:\n        check_env_var(\"mimalloc\", [\"mimalloc\"])\n    check_env_var(\"nonexistent\", possible_backends, expect_warning=True)\n\n\ndef test_specific_memory_pools():\n    specific_pools = set()\n\n    def check(factory, name, *, can_fail=False):\n        if can_fail:\n            try:\n                pool = factory()\n            except NotImplementedError:\n                return\n        else:\n            pool = factory()\n        assert pool.backend_name == name\n        specific_pools.add(pool)\n\n    check(pa.system_memory_pool, \"system\")\n    check(pa.jemalloc_memory_pool, \"jemalloc\",\n          can_fail=not should_have_jemalloc)\n    check(pa.mimalloc_memory_pool, \"mimalloc\",\n          can_fail=not should_have_mimalloc)\n\n\ndef test_supported_memory_backends():\n    backends = pa.supported_memory_backends()\n\n    assert \"system\" in backends\n    if should_have_jemalloc:\n        assert \"jemalloc\" in backends\n    if should_have_mimalloc:\n        assert \"mimalloc\" in backends\n\n\ndef run_debug_memory_pool(pool_factory, env_value):\n    \"\"\"\n    Run a piece of code making an invalid memory write with the\n    ARROW_DEBUG_MEMORY_POOL environment variable set to a specific value.\n    \"\"\"\n    code = f\"\"\"if 1:\n        import ctypes\n        import pyarrow as pa\n        # ARROW-16873: some Python installs enable faulthandler by default,\n        # which could dump a spurious stack trace if the following crashes\n        import faulthandler\n        faulthandler.disable()\n\n        pool = pa.{pool_factory}()\n        buf = pa.allocate_buffer(64, memory_pool=pool)\n\n        # Write memory out of bounds\n        ptr = ctypes.cast(buf.address, ctypes.POINTER(ctypes.c_ubyte))\n        ptr[64] = 0\n\n        del buf\n        \"\"\"\n    env = dict(os.environ)\n    env['ARROW_DEBUG_MEMORY_POOL'] = env_value\n    res = subprocess.run([sys.executable, \"-c\", code], env=env,\n                         universal_newlines=True, stderr=subprocess.PIPE)\n    print(res.stderr, file=sys.stderr)\n    return res\n\n\n@pytest.mark.parametrize('pool_factory', supported_factories())\ndef test_debug_memory_pool_abort(pool_factory):\n    res = run_debug_memory_pool(pool_factory.__name__, \"abort\")\n    if os.name == \"posix\":\n        assert res.returncode == -signal.SIGABRT\n    else:\n        assert res.returncode != 0\n    assert \"Wrong size on deallocation\" in res.stderr\n\n\n@pytest.mark.parametrize('pool_factory', supported_factories())\ndef test_debug_memory_pool_trap(pool_factory):\n    res = run_debug_memory_pool(pool_factory.__name__, \"trap\")\n    if os.name == \"posix\":\n        assert res.returncode == -signal.SIGTRAP\n    else:\n        assert res.returncode != 0\n    assert \"Wrong size on deallocation\" in res.stderr\n\n\n@pytest.mark.parametrize('pool_factory', supported_factories())\ndef test_debug_memory_pool_warn(pool_factory):\n    res = run_debug_memory_pool(pool_factory.__name__, \"warn\")\n    res.check_returncode()\n    assert \"Wrong size on deallocation\" in res.stderr\n\n\ndef check_debug_memory_pool_disabled(pool_factory, env_value, msg):\n    res = run_debug_memory_pool(pool_factory.__name__, env_value)\n    # The subprocess either returned successfully or was killed by a signal\n    # (due to writing out of bounds), depending on the underlying allocator.\n    if os.name == \"posix\":\n        assert res.returncode <= 0\n    else:\n        res.check_returncode()\n    if msg == \"\":\n        assert res.stderr == \"\"\n    else:\n        assert msg in res.stderr\n\n\n@pytest.mark.parametrize('pool_factory', supported_factories())\ndef test_debug_memory_pool_none(pool_factory):\n    check_debug_memory_pool_disabled(pool_factory, \"none\", \"\")\n\n\n@pytest.mark.parametrize('pool_factory', supported_factories())\ndef test_debug_memory_pool_empty(pool_factory):\n    check_debug_memory_pool_disabled(pool_factory, \"\", \"\")\n\n\n@pytest.mark.parametrize('pool_factory', supported_factories())\ndef test_debug_memory_pool_unknown(pool_factory):\n    env_value = \"some_arbitrary_value\"\n    msg = (\n        f\"Invalid value for ARROW_DEBUG_MEMORY_POOL: '{env_value}'. \"\n        \"Valid values are 'abort', 'trap', 'warn', 'none'.\"\n    )\n    check_debug_memory_pool_disabled(pool_factory, env_value, msg)\n", "python/pyarrow/tests/test_util.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport gc\nimport os\nimport signal\nimport shutil\nimport sys\nimport textwrap\nimport weakref\n\nimport pytest\n\nfrom pyarrow.util import (doc, _break_traceback_cycle_from_frame,\n                          download_tzdata_on_windows)\nfrom pyarrow.tests.util import disabled_gc\n\n\n@doc(method=\"func_a\", operation=\"A\")\ndef func_a(whatever):\n    \"\"\"\n    This is the {method} method.\n\n    It computes {operation}.\n    \"\"\"\n    pass\n\n\n@doc(\n    func_a,\n    textwrap.dedent(\n        \"\"\"\n        Examples\n        --------\n\n        >>> func_b()\n        B\n        \"\"\"\n    ),\n    method=\"func_b\",\n    operation=\"B\",\n)\ndef func_b(whatever):\n    pass\n\n\n@doc(\n    func_a,\n    method=\"func_c\",\n    operation=\"C\",\n)\ndef func_c(whatever):\n    \"\"\"\n    Examples\n    --------\n\n    >>> func_c()\n    C\n    \"\"\"\n    pass\n\n\n@doc(func_a, method=\"func_d\", operation=\"D\")\ndef func_d(whatever):\n    pass\n\n\n@doc(func_d, method=\"func_e\", operation=\"E\")\ndef func_e(whatever):\n    pass\n\n\n@doc(method=\"func_f\")\ndef func_f(whatever):\n    \"\"\"\n    This is the {method} method.\n\n    {{ We can escape curly braces like this. }}\n\n    Examples\n    --------\n    We should replace curly brace usage in doctests.\n\n    >>> dict(x = \"x\", y = \"y\")\n    >>> set((1, 2, 3))\n    \"\"\"\n    pass\n\n\ndef test_docstring_formatting():\n    docstr = textwrap.dedent(\n        \"\"\"\n        This is the func_a method.\n\n        It computes A.\n        \"\"\"\n    )\n    assert func_a.__doc__ == docstr\n\n\ndef test_docstring_concatenation():\n    docstr = textwrap.dedent(\n        \"\"\"\n        This is the func_b method.\n\n        It computes B.\n\n        Examples\n        --------\n\n        >>> func_b()\n        B\n        \"\"\"\n    )\n    assert func_b.__doc__ == docstr\n\n\ndef test_docstring_append():\n    docstr = textwrap.dedent(\n        \"\"\"\n        This is the func_c method.\n\n        It computes C.\n\n        Examples\n        --------\n\n        >>> func_c()\n        C\n        \"\"\"\n    )\n    assert func_c.__doc__ == docstr\n\n\ndef test_docstring_template_from_callable():\n    docstr = textwrap.dedent(\n        \"\"\"\n        This is the func_d method.\n\n        It computes D.\n        \"\"\"\n    )\n    assert func_d.__doc__ == docstr\n\n\ndef test_inherit_docstring_template_from_callable():\n    docstr = textwrap.dedent(\n        \"\"\"\n        This is the func_e method.\n\n        It computes E.\n        \"\"\"\n    )\n    assert func_e.__doc__ == docstr\n\n\ndef test_escaping_in_docstring():\n    docstr = textwrap.dedent(\n        \"\"\"\n        This is the func_f method.\n\n        { We can escape curly braces like this. }\n\n        Examples\n        --------\n        We should replace curly brace usage in doctests.\n\n        >>> dict(x = \"x\", y = \"y\")\n        >>> set((1, 2, 3))\n        \"\"\"\n    )\n    assert func_f.__doc__ == docstr\n\n\ndef exhibit_signal_refcycle():\n    # Put an object in the frame locals and return a weakref to it.\n    # If `signal.getsignal` has a bug where it creates a reference cycle\n    # keeping alive the current execution frames, `obj` will not be\n    # destroyed immediately when this function returns.\n    obj = set()\n    signal.getsignal(signal.SIGINT)\n    return weakref.ref(obj)\n\n\ndef test_signal_refcycle():\n    # Test possible workaround for https://bugs.python.org/issue42248\n    with disabled_gc():\n        wr = exhibit_signal_refcycle()\n        if wr() is None:\n            pytest.skip(\n                \"Python version does not have the bug we're testing for\")\n\n    gc.collect()\n    with disabled_gc():\n        wr = exhibit_signal_refcycle()\n        assert wr() is not None\n        _break_traceback_cycle_from_frame(sys._getframe(0))\n        assert wr() is None\n\n\n@pytest.mark.skipif(sys.platform != \"win32\",\n                    reason=\"Timezone database is already provided.\")\ndef test_download_tzdata_on_windows():\n    tzdata_path = os.path.expandvars(r\"%USERPROFILE%\\Downloads\\tzdata\")\n\n    # Download timezone database and remove data in case it already exists\n    if (os.path.exists(tzdata_path)):\n        shutil.rmtree(tzdata_path)\n    download_tzdata_on_windows()\n\n    # Inspect the folder\n    assert os.path.exists(tzdata_path)\n    assert os.path.exists(os.path.join(tzdata_path, \"windowsZones.xml\"))\n    assert os.path.exists(os.path.join(tzdata_path, \"europe\"))\n    assert 'version' in os.listdir(tzdata_path)\n", "python/pyarrow/tests/test_fs.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom datetime import datetime, timezone, timedelta\nimport gzip\nimport os\nimport pathlib\nimport subprocess\nimport sys\n\nimport pytest\nimport weakref\n\nimport pyarrow as pa\nfrom pyarrow.tests.test_io import assert_file_not_found\nfrom pyarrow.tests.util import (_filesystem_uri, ProxyHandler,\n                                _configure_s3_limited_user)\n\nfrom pyarrow.fs import (FileType, FileInfo, FileSelector, FileSystem,\n                        LocalFileSystem, SubTreeFileSystem, _MockFileSystem,\n                        FileSystemHandler, PyFileSystem, FSSpecHandler,\n                        copy_files)\n\n\nclass DummyHandler(FileSystemHandler):\n    def __init__(self, value=42):\n        self._value = value\n\n    def __eq__(self, other):\n        if isinstance(other, FileSystemHandler):\n            return self._value == other._value\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, FileSystemHandler):\n            return self._value != other._value\n        return NotImplemented\n\n    def get_type_name(self):\n        return \"dummy\"\n\n    def normalize_path(self, path):\n        return path\n\n    def get_file_info(self, paths):\n        info = []\n        for path in paths:\n            if \"file\" in path:\n                info.append(FileInfo(path, FileType.File))\n            elif \"dir\" in path:\n                info.append(FileInfo(path, FileType.Directory))\n            elif \"notfound\" in path:\n                info.append(FileInfo(path, FileType.NotFound))\n            elif \"badtype\" in path:\n                # Will raise when converting\n                info.append(object())\n            else:\n                raise IOError\n        return info\n\n    def get_file_info_selector(self, selector):\n        if selector.base_dir != \"somedir\":\n            if selector.allow_not_found:\n                return []\n            else:\n                raise FileNotFoundError(selector.base_dir)\n        infos = [\n            FileInfo(\"somedir/file1\", FileType.File, size=123),\n            FileInfo(\"somedir/subdir1\", FileType.Directory),\n        ]\n        if selector.recursive:\n            infos += [\n                FileInfo(\"somedir/subdir1/file2\", FileType.File, size=456),\n            ]\n        return infos\n\n    def create_dir(self, path, recursive):\n        if path == \"recursive\":\n            assert recursive is True\n        elif path == \"non-recursive\":\n            assert recursive is False\n        else:\n            raise IOError\n\n    def delete_dir(self, path):\n        assert path == \"delete_dir\"\n\n    def delete_dir_contents(self, path, missing_dir_ok):\n        if not path.strip(\"/\"):\n            raise ValueError\n        assert path == \"delete_dir_contents\"\n\n    def delete_root_dir_contents(self):\n        pass\n\n    def delete_file(self, path):\n        assert path == \"delete_file\"\n\n    def move(self, src, dest):\n        assert src == \"move_from\"\n        assert dest == \"move_to\"\n\n    def copy_file(self, src, dest):\n        assert src == \"copy_file_from\"\n        assert dest == \"copy_file_to\"\n\n    def open_input_stream(self, path):\n        if \"notfound\" in path:\n            raise FileNotFoundError(path)\n        data = \"{0}:input_stream\".format(path).encode('utf8')\n        return pa.BufferReader(data)\n\n    def open_input_file(self, path):\n        if \"notfound\" in path:\n            raise FileNotFoundError(path)\n        data = \"{0}:input_file\".format(path).encode('utf8')\n        return pa.BufferReader(data)\n\n    def open_output_stream(self, path, metadata):\n        if \"notfound\" in path:\n            raise FileNotFoundError(path)\n        return pa.BufferOutputStream()\n\n    def open_append_stream(self, path, metadata):\n        if \"notfound\" in path:\n            raise FileNotFoundError(path)\n        return pa.BufferOutputStream()\n\n\n@pytest.fixture\ndef localfs(request, tempdir):\n    return dict(\n        fs=LocalFileSystem(),\n        pathfn=lambda p: (tempdir / p).as_posix(),\n        allow_move_dir=True,\n        allow_append_to_file=True,\n    )\n\n\n@pytest.fixture\ndef py_localfs(request, tempdir):\n    return dict(\n        fs=PyFileSystem(ProxyHandler(LocalFileSystem())),\n        pathfn=lambda p: (tempdir / p).as_posix(),\n        allow_move_dir=True,\n        allow_append_to_file=True,\n    )\n\n\n@pytest.fixture\ndef mockfs(request):\n    return dict(\n        fs=_MockFileSystem(),\n        pathfn=lambda p: p,\n        allow_move_dir=True,\n        allow_append_to_file=True,\n    )\n\n\n@pytest.fixture\ndef py_mockfs(request):\n    return dict(\n        fs=PyFileSystem(ProxyHandler(_MockFileSystem())),\n        pathfn=lambda p: p,\n        allow_move_dir=True,\n        allow_append_to_file=True,\n    )\n\n\n@pytest.fixture\ndef localfs_with_mmap(request, tempdir):\n    return dict(\n        fs=LocalFileSystem(use_mmap=True),\n        pathfn=lambda p: (tempdir / p).as_posix(),\n        allow_move_dir=True,\n        allow_append_to_file=True,\n    )\n\n\n@pytest.fixture\ndef subtree_localfs(request, tempdir, localfs):\n    return dict(\n        fs=SubTreeFileSystem(str(tempdir), localfs['fs']),\n        pathfn=lambda p: p,\n        allow_move_dir=True,\n        allow_append_to_file=True,\n    )\n\n\n@pytest.fixture\ndef gcsfs(request, gcs_server):\n    request.config.pyarrow.requires('gcs')\n    from pyarrow.fs import GcsFileSystem\n\n    host, port = gcs_server['connection']\n    bucket = 'pyarrow-filesystem/'\n\n    fs = GcsFileSystem(\n        endpoint_override=f'{host}:{port}',\n        scheme='http',\n        # Mock endpoint doesn't check credentials.\n        anonymous=True,\n        retry_time_limit=timedelta(seconds=45),\n        project_id='test-project-id'\n    )\n    try:\n        fs.create_dir(bucket)\n    except OSError as e:\n        pytest.skip(f\"Could not create directory in {fs}: {e}\")\n\n    yield dict(\n        fs=fs,\n        pathfn=bucket.__add__,\n        allow_move_dir=False,\n        allow_append_to_file=False,\n    )\n    fs.delete_dir(bucket)\n\n\n@pytest.fixture\ndef s3fs(request, s3_server):\n    request.config.pyarrow.requires('s3')\n    from pyarrow.fs import S3FileSystem\n\n    host, port, access_key, secret_key = s3_server['connection']\n    bucket = 'pyarrow-filesystem/'\n\n    fs = S3FileSystem(\n        access_key=access_key,\n        secret_key=secret_key,\n        endpoint_override='{}:{}'.format(host, port),\n        scheme='http',\n        allow_bucket_creation=True,\n        allow_bucket_deletion=True\n    )\n    fs.create_dir(bucket)\n\n    yield dict(\n        fs=fs,\n        pathfn=bucket.__add__,\n        allow_move_dir=False,\n        allow_append_to_file=False,\n    )\n    fs.delete_dir(bucket)\n\n\n@pytest.fixture\ndef subtree_s3fs(request, s3fs):\n    prefix = 'pyarrow-filesystem/prefix/'\n    return dict(\n        fs=SubTreeFileSystem(prefix, s3fs['fs']),\n        pathfn=prefix.__add__,\n        allow_move_dir=False,\n        allow_append_to_file=False,\n    )\n\n\n_minio_limited_policy = \"\"\"{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\",\n                \"s3:PutObjectTagging\",\n                \"s3:DeleteObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        }\n    ]\n}\"\"\"\n\n\n@pytest.fixture\ndef azurefs(request, azure_server):\n    request.config.pyarrow.requires('azure')\n    from pyarrow.fs import AzureFileSystem\n\n    host, port, account_name, account_key = azure_server['connection']\n    azurite_authority = f\"{host}:{port}\"\n    azurite_scheme = \"http\"\n\n    container = 'pyarrow-filesystem/'\n\n    fs = AzureFileSystem(account_name=account_name,\n                         account_key=account_key,\n                         blob_storage_authority=azurite_authority,\n                         dfs_storage_authority=azurite_authority,\n                         blob_storage_scheme=azurite_scheme,\n                         dfs_storage_scheme=azurite_scheme)\n\n    fs.create_dir(container)\n\n    yield dict(\n        fs=fs,\n        pathfn=container.__add__,\n        allow_move_dir=True,\n        allow_append_to_file=True,\n    )\n    fs.delete_dir(container)\n\n\n@pytest.fixture\ndef hdfs(request, hdfs_connection):\n    request.config.pyarrow.requires('hdfs')\n    if not pa.have_libhdfs():\n        pytest.skip('Cannot locate libhdfs')\n\n    from pyarrow.fs import HadoopFileSystem\n\n    host, port, user = hdfs_connection\n    fs = HadoopFileSystem(host, port=port, user=user)\n\n    return dict(\n        fs=fs,\n        pathfn=lambda p: p,\n        allow_move_dir=True,\n        allow_append_to_file=True,\n    )\n\n\n@pytest.fixture\ndef py_fsspec_localfs(request, tempdir):\n    fsspec = pytest.importorskip(\"fsspec\")\n    fs = fsspec.filesystem('file')\n    return dict(\n        fs=PyFileSystem(FSSpecHandler(fs)),\n        pathfn=lambda p: (tempdir / p).as_posix(),\n        allow_move_dir=True,\n        allow_append_to_file=True,\n    )\n\n\n@pytest.fixture\ndef py_fsspec_memoryfs(request, tempdir):\n    fsspec = pytest.importorskip(\"fsspec\", minversion=\"0.7.5\")\n    if fsspec.__version__ == \"0.8.5\":\n        # see https://issues.apache.org/jira/browse/ARROW-10934\n        pytest.skip(\"Bug in fsspec 0.8.5 for in-memory filesystem\")\n    fs = fsspec.filesystem('memory')\n    return dict(\n        fs=PyFileSystem(FSSpecHandler(fs)),\n        pathfn=lambda p: p,\n        allow_move_dir=True,\n        allow_append_to_file=True,\n    )\n\n\n@pytest.fixture\ndef py_fsspec_s3fs(request, s3_server):\n    s3fs = pytest.importorskip(\"s3fs\")\n    host, port, access_key, secret_key = s3_server['connection']\n    bucket = 'pyarrow-filesystem/'\n\n    fs = s3fs.S3FileSystem(\n        key=access_key,\n        secret=secret_key,\n        client_kwargs=dict(endpoint_url='http://{}:{}'.format(host, port))\n    )\n    fs = PyFileSystem(FSSpecHandler(fs))\n    fs.create_dir(bucket)\n\n    yield dict(\n        fs=fs,\n        pathfn=bucket.__add__,\n        allow_move_dir=False,\n        allow_append_to_file=True,\n    )\n    fs.delete_dir(bucket)\n\n\n@pytest.fixture(params=[\n    pytest.param(\n        'localfs',\n        id='LocalFileSystem()'\n    ),\n    pytest.param(\n        'localfs_with_mmap',\n        id='LocalFileSystem(use_mmap=True)'\n    ),\n    pytest.param(\n        'subtree_localfs',\n        id='SubTreeFileSystem(LocalFileSystem())'\n    ),\n    pytest.param(\n        's3fs',\n        id='S3FileSystem',\n        marks=pytest.mark.s3\n    ),\n    pytest.param(\n        'gcsfs',\n        id='GcsFileSystem',\n        marks=pytest.mark.gcs\n    ),\n    pytest.param(\n        'azurefs',\n        id='AzureFileSystem',\n        marks=pytest.mark.azure\n    ),\n    pytest.param(\n        'hdfs',\n        id='HadoopFileSystem',\n        marks=pytest.mark.hdfs\n    ),\n    pytest.param(\n        'mockfs',\n        id='_MockFileSystem()'\n    ),\n    pytest.param(\n        'py_localfs',\n        id='PyFileSystem(ProxyHandler(LocalFileSystem()))'\n    ),\n    pytest.param(\n        'py_mockfs',\n        id='PyFileSystem(ProxyHandler(_MockFileSystem()))'\n    ),\n    pytest.param(\n        'py_fsspec_localfs',\n        id='PyFileSystem(FSSpecHandler(fsspec.LocalFileSystem()))'\n    ),\n    pytest.param(\n        'py_fsspec_memoryfs',\n        id='PyFileSystem(FSSpecHandler(fsspec.filesystem(\"memory\")))'\n    ),\n    pytest.param(\n        'py_fsspec_s3fs',\n        id='PyFileSystem(FSSpecHandler(s3fs.S3FileSystem()))',\n        marks=pytest.mark.s3\n    ),\n])\ndef filesystem_config(request):\n    return request.getfixturevalue(request.param)\n\n\n@pytest.fixture\ndef fs(filesystem_config):\n    return filesystem_config['fs']\n\n\n@pytest.fixture\ndef pathfn(filesystem_config):\n    return filesystem_config['pathfn']\n\n\n@pytest.fixture\ndef allow_move_dir(filesystem_config):\n    return filesystem_config['allow_move_dir']\n\n\n@pytest.fixture\ndef allow_append_to_file(filesystem_config):\n    return filesystem_config['allow_append_to_file']\n\n\ndef check_mtime(file_info):\n    assert isinstance(file_info.mtime, datetime)\n    assert isinstance(file_info.mtime_ns, int)\n    assert file_info.mtime_ns >= 0\n    assert file_info.mtime_ns == pytest.approx(\n        file_info.mtime.timestamp() * 1e9)\n    # It's an aware UTC datetime\n    tzinfo = file_info.mtime.tzinfo\n    assert tzinfo is not None\n    assert tzinfo.utcoffset(None) == timedelta(0)\n\n\ndef check_mtime_absent(file_info):\n    assert file_info.mtime is None\n    assert file_info.mtime_ns is None\n\n\ndef check_mtime_or_absent(file_info):\n    if file_info.mtime is None:\n        check_mtime_absent(file_info)\n    else:\n        check_mtime(file_info)\n\n\ndef skip_fsspec_s3fs(fs):\n    if fs.type_name == \"py::fsspec+('s3', 's3a')\":\n        pytest.xfail(reason=\"Not working with fsspec's s3fs\")\n\n\ndef skip_azure(fs, reason):\n    if fs.type_name == \"abfs\":\n        pytest.skip(reason=reason)\n\n\n@pytest.mark.s3\ndef test_s3fs_limited_permissions_create_bucket(s3_server):\n    from pyarrow.fs import S3FileSystem\n    _configure_s3_limited_user(s3_server, _minio_limited_policy)\n    host, port, _, _ = s3_server['connection']\n\n    fs = S3FileSystem(\n        access_key='limited',\n        secret_key='limited123',\n        endpoint_override='{}:{}'.format(host, port),\n        scheme='http'\n    )\n    fs.create_dir('existing-bucket/test')\n\n    with pytest.raises(pa.ArrowIOError, match=\"Bucket 'new-bucket' not found\"):\n        fs.create_dir('new-bucket')\n\n    with pytest.raises(pa.ArrowIOError, match=\"Would delete bucket\"):\n        fs.delete_dir('existing-bucket')\n\n\ndef test_file_info_constructor():\n    dt = datetime.fromtimestamp(1568799826, timezone.utc)\n\n    info = FileInfo(\"foo/bar\")\n    assert info.path == \"foo/bar\"\n    assert info.base_name == \"bar\"\n    assert info.type == FileType.Unknown\n    assert info.size is None\n    check_mtime_absent(info)\n\n    info = FileInfo(\"foo/baz.txt\", type=FileType.File, size=123,\n                    mtime=1568799826.5)\n    assert info.path == \"foo/baz.txt\"\n    assert info.base_name == \"baz.txt\"\n    assert info.type == FileType.File\n    assert info.size == 123\n    assert info.mtime_ns == 1568799826500000000\n    check_mtime(info)\n\n    info = FileInfo(\"foo\", type=FileType.Directory, mtime=dt)\n    assert info.path == \"foo\"\n    assert info.base_name == \"foo\"\n    assert info.type == FileType.Directory\n    assert info.size is None\n    assert info.mtime == dt\n    assert info.mtime_ns == 1568799826000000000\n    check_mtime(info)\n\n\ndef test_cannot_instantiate_base_filesystem():\n    with pytest.raises(TypeError):\n        FileSystem()\n\n\ndef test_filesystem_equals():\n    fs0 = LocalFileSystem()\n    fs1 = LocalFileSystem()\n    fs2 = _MockFileSystem()\n\n    assert fs0.equals(fs0)\n    assert fs0.equals(fs1)\n    with pytest.raises(TypeError):\n        fs0.equals('string')\n    assert fs0 == fs0 == fs1\n    assert fs0 != 4\n\n    assert fs2 == fs2\n    assert fs2 != _MockFileSystem()\n\n    assert SubTreeFileSystem('/base', fs0) == SubTreeFileSystem('/base', fs0)\n    assert SubTreeFileSystem('/base', fs0) != SubTreeFileSystem('/base', fs2)\n    assert SubTreeFileSystem('/base', fs0) != SubTreeFileSystem('/other', fs0)\n\n\ndef test_filesystem_equals_none(fs):\n    with pytest.raises(TypeError, match=\"got NoneType\"):\n        fs.equals(None)\n\n    assert fs is not None\n\n\ndef test_subtree_filesystem():\n    localfs = LocalFileSystem()\n\n    subfs = SubTreeFileSystem('/base', localfs)\n    assert subfs.base_path == '/base/'\n    assert subfs.base_fs == localfs\n    assert repr(subfs).startswith('SubTreeFileSystem(base_path=/base/, '\n                                  'base_fs=<pyarrow._fs.LocalFileSystem')\n\n    subfs = SubTreeFileSystem('/another/base/', LocalFileSystem())\n    assert subfs.base_path == '/another/base/'\n    assert subfs.base_fs == localfs\n    assert repr(subfs).startswith('SubTreeFileSystem(base_path=/another/base/,'\n                                  ' base_fs=<pyarrow._fs.LocalFileSystem')\n\n\ndef test_filesystem_pickling(fs, pickle_module):\n    if fs.type_name.split('::')[-1] == 'mock':\n        pytest.xfail(reason='MockFileSystem is not serializable')\n\n    serialized = pickle_module.dumps(fs)\n    restored = pickle_module.loads(serialized)\n    assert isinstance(restored, FileSystem)\n    assert restored.equals(fs)\n\n\ndef test_filesystem_is_functional_after_pickling(fs, pathfn, pickle_module):\n    if fs.type_name.split('::')[-1] == 'mock':\n        pytest.xfail(reason='MockFileSystem is not serializable')\n    skip_fsspec_s3fs(fs)\n\n    aaa = pathfn('a/aa/aaa/')\n    bb = pathfn('a/bb')\n    c = pathfn('c.txt')\n\n    fs.create_dir(aaa)\n    with fs.open_output_stream(bb):\n        pass  # touch\n    with fs.open_output_stream(c) as fp:\n        fp.write(b'test')\n\n    restored = pickle_module.loads(pickle_module.dumps(fs))\n    aaa_info, bb_info, c_info = restored.get_file_info([aaa, bb, c])\n    assert aaa_info.type == FileType.Directory\n    assert bb_info.type == FileType.File\n    assert c_info.type == FileType.File\n\n\ndef test_type_name():\n    fs = LocalFileSystem()\n    assert fs.type_name == \"local\"\n    fs = _MockFileSystem()\n    assert fs.type_name == \"mock\"\n\n\ndef test_normalize_path(fs):\n    # Trivial path names (without separators) should generally be\n    # already normalized.  Just a sanity check.\n    assert fs.normalize_path(\"foo\") == \"foo\"\n\n\ndef test_non_path_like_input_raises(fs):\n    class Path:\n        pass\n\n    invalid_paths = [1, 1.1, Path(), tuple(), {}, [], lambda: 1,\n                     pathlib.Path()]\n    for path in invalid_paths:\n        with pytest.raises(TypeError):\n            fs.create_dir(path)\n\n\ndef test_get_file_info(fs, pathfn):\n    aaa = pathfn('a/aa/aaa/')\n    bb = pathfn('a/bb')\n    c = pathfn('c.txt')\n    zzz = pathfn('zzz')\n\n    fs.create_dir(aaa)\n    with fs.open_output_stream(bb):\n        pass  # touch\n    with fs.open_output_stream(c) as fp:\n        fp.write(b'test')\n\n    aaa_info, bb_info, c_info, zzz_info = fs.get_file_info([aaa, bb, c, zzz])\n\n    assert aaa_info.path == aaa\n    assert 'aaa' in repr(aaa_info)\n    assert aaa_info.extension == ''\n    if fs.type_name == \"py::fsspec+('s3', 's3a')\":\n        # s3fs doesn't create empty directories\n        assert aaa_info.type == FileType.NotFound\n    else:\n        assert aaa_info.type == FileType.Directory\n        assert 'FileType.Directory' in repr(aaa_info)\n    assert aaa_info.size is None\n    check_mtime_or_absent(aaa_info)\n\n    assert bb_info.path == str(bb)\n    assert bb_info.base_name == 'bb'\n    assert bb_info.extension == ''\n    assert bb_info.type == FileType.File\n    assert 'FileType.File' in repr(bb_info)\n    assert bb_info.size == 0\n    if fs.type_name not in [\"py::fsspec+memory\", \"py::fsspec+('s3', 's3a')\"]:\n        check_mtime(bb_info)\n\n    assert c_info.path == str(c)\n    assert c_info.base_name == 'c.txt'\n    assert c_info.extension == 'txt'\n    assert c_info.type == FileType.File\n    assert 'FileType.File' in repr(c_info)\n    assert c_info.size == 4\n    if fs.type_name not in [\"py::fsspec+memory\", \"py::fsspec+('s3', 's3a')\"]:\n        check_mtime(c_info)\n\n    assert zzz_info.path == str(zzz)\n    assert zzz_info.base_name == 'zzz'\n    assert zzz_info.extension == ''\n    assert zzz_info.type == FileType.NotFound\n    assert zzz_info.size is None\n    assert zzz_info.mtime is None\n    assert 'FileType.NotFound' in repr(zzz_info)\n    check_mtime_absent(zzz_info)\n\n    # with single path\n    aaa_info2 = fs.get_file_info(aaa)\n    assert aaa_info.path == aaa_info2.path\n    assert aaa_info.type == aaa_info2.type\n\n\ndef test_get_file_info_with_selector(fs, pathfn):\n    base_dir = pathfn('selector-dir/')\n    file_a = pathfn('selector-dir/test_file_a')\n    file_b = pathfn('selector-dir/test_file_b')\n    dir_a = pathfn('selector-dir/test_dir_a')\n    file_c = pathfn('selector-dir/test_dir_a/test_file_c')\n    dir_b = pathfn('selector-dir/test_dir_b')\n\n    try:\n        fs.create_dir(base_dir)\n        with fs.open_output_stream(file_a):\n            pass\n        with fs.open_output_stream(file_b):\n            pass\n        fs.create_dir(dir_a)\n        with fs.open_output_stream(file_c):\n            pass\n        fs.create_dir(dir_b)\n\n        # recursive selector\n        selector = FileSelector(base_dir, allow_not_found=False,\n                                recursive=True)\n        assert selector.base_dir == base_dir\n\n        infos = fs.get_file_info(selector)\n        if fs.type_name == \"py::fsspec+('s3', 's3a')\":\n            # s3fs only lists directories if they are not empty\n            len(infos) == 4\n        else:\n            assert len(infos) == 5\n\n        for info in infos:\n            if (info.path.endswith(file_a) or info.path.endswith(file_b) or\n                    info.path.endswith(file_c)):\n                assert info.type == FileType.File\n            elif (info.path.rstrip(\"/\").endswith(dir_a) or\n                  info.path.rstrip(\"/\").endswith(dir_b)):\n                assert info.type == FileType.Directory\n            else:\n                raise ValueError('unexpected path {}'.format(info.path))\n            check_mtime_or_absent(info)\n\n        # non-recursive selector -> not selecting the nested file_c\n        selector = FileSelector(base_dir, recursive=False)\n\n        infos = fs.get_file_info(selector)\n        if fs.type_name == \"py::fsspec+('s3', 's3a')\":\n            # s3fs only lists directories if they are not empty\n            assert len(infos) == 3\n        else:\n            assert len(infos) == 4\n\n    finally:\n        fs.delete_dir(base_dir)\n\n\ndef test_create_dir(fs, pathfn):\n    # s3fs fails deleting dir fails if it is empty\n    # (https://github.com/dask/s3fs/issues/317)\n    skip_fsspec_s3fs(fs)\n    d = pathfn('test-directory/')\n\n    with pytest.raises(pa.ArrowIOError):\n        fs.delete_dir(d)\n\n    fs.create_dir(d)\n    fs.delete_dir(d)\n\n    d = pathfn('deeply/nested/test-directory/')\n    fs.create_dir(d, recursive=True)\n    fs.delete_dir(d)\n\n\ndef test_delete_dir(fs, pathfn):\n    skip_fsspec_s3fs(fs)\n\n    d = pathfn('directory/')\n    nd = pathfn('directory/nested/')\n\n    fs.create_dir(nd)\n    fs.delete_dir(d)\n    with pytest.raises(pa.ArrowIOError):\n        fs.delete_dir(nd)\n    with pytest.raises(pa.ArrowIOError):\n        fs.delete_dir(d)\n\n\ndef test_delete_dir_with_explicit_subdir(fs, pathfn):\n    # GH-38618: regression with AWS failing to delete directories,\n    # depending on whether they were created explicitly. Note that\n    # Minio doesn't reproduce the issue, so this test is not a regression\n    # test in itself.\n    skip_fsspec_s3fs(fs)\n\n    d = pathfn('directory/')\n    nd = pathfn('directory/nested/')\n\n    # deleting dir with explicit subdir\n    fs.create_dir(d)\n    fs.create_dir(nd)\n    fs.delete_dir(d)\n    dir_info = fs.get_file_info(d)\n    assert dir_info.type == FileType.NotFound\n\n    # deleting dir with blob in explicit subdir\n    d = pathfn('directory2')\n    nd = pathfn('directory2/nested')\n    f = pathfn('directory2/nested/target-file')\n\n    fs.create_dir(d)\n    fs.create_dir(nd)\n    with fs.open_output_stream(f) as s:\n        s.write(b'data')\n\n    fs.delete_dir(d)\n    dir_info = fs.get_file_info(d)\n    assert dir_info.type == FileType.NotFound\n\n\ndef test_delete_dir_contents(fs, pathfn):\n    skip_fsspec_s3fs(fs)\n\n    d = pathfn('directory/')\n    nd = pathfn('directory/nested/')\n\n    fs.create_dir(nd)\n    fs.delete_dir_contents(d)\n    with pytest.raises(pa.ArrowIOError):\n        fs.delete_dir(nd)\n    fs.delete_dir_contents(nd, missing_dir_ok=True)\n    with pytest.raises(pa.ArrowIOError):\n        fs.delete_dir_contents(nd)\n    fs.delete_dir(d)\n    with pytest.raises(pa.ArrowIOError):\n        fs.delete_dir(d)\n\n\ndef _check_root_dir_contents(config):\n    fs = config['fs']\n    pathfn = config['pathfn']\n\n    d = pathfn('directory/')\n    nd = pathfn('directory/nested/')\n\n    fs.create_dir(nd)\n    with pytest.raises(pa.ArrowInvalid):\n        fs.delete_dir_contents(\"\")\n    with pytest.raises(pa.ArrowInvalid):\n        fs.delete_dir_contents(\"/\")\n    with pytest.raises(pa.ArrowInvalid):\n        fs.delete_dir_contents(\"//\")\n\n    fs.delete_dir_contents(\"\", accept_root_dir=True)\n    fs.delete_dir_contents(\"/\", accept_root_dir=True)\n    fs.delete_dir_contents(\"//\", accept_root_dir=True)\n    with pytest.raises(pa.ArrowIOError):\n        fs.delete_dir(d)\n\n\ndef test_delete_root_dir_contents(mockfs, py_mockfs):\n    _check_root_dir_contents(mockfs)\n    _check_root_dir_contents(py_mockfs)\n\n\ndef test_copy_file(fs, pathfn):\n    s = pathfn('test-copy-source-file')\n    t = pathfn('test-copy-target-file')\n\n    with fs.open_output_stream(s):\n        pass\n\n    fs.copy_file(s, t)\n    fs.delete_file(s)\n    fs.delete_file(t)\n\n\ndef test_move_directory(fs, pathfn, allow_move_dir):\n    # TODO(GH-40025): Stop skipping this test\n    skip_azure(fs, \"Not implemented yet in for Azure. See GH-40025\")\n\n    # move directory (doesn't work with S3)\n    s = pathfn('source-dir/')\n    t = pathfn('target-dir/')\n\n    fs.create_dir(s)\n\n    if allow_move_dir:\n        fs.move(s, t)\n        with pytest.raises(pa.ArrowIOError):\n            fs.delete_dir(s)\n        fs.delete_dir(t)\n    else:\n        with pytest.raises(pa.ArrowIOError):\n            fs.move(s, t)\n\n\ndef test_move_file(fs, pathfn):\n    # s3fs moving a file with recursive=True on latest 0.5 version\n    # (https://github.com/dask/s3fs/issues/394)\n    skip_fsspec_s3fs(fs)\n\n    # TODO(GH-40025): Stop skipping this test\n    skip_azure(fs, \"Not implemented yet in for Azure. See GH-40025\")\n\n    s = pathfn('test-move-source-file')\n    t = pathfn('test-move-target-file')\n\n    with fs.open_output_stream(s):\n        pass\n\n    fs.move(s, t)\n    with pytest.raises(pa.ArrowIOError):\n        fs.delete_file(s)\n    fs.delete_file(t)\n\n\ndef test_delete_file(fs, pathfn):\n    p = pathfn('test-delete-target-file')\n    with fs.open_output_stream(p):\n        pass\n\n    fs.delete_file(p)\n    with pytest.raises(pa.ArrowIOError):\n        fs.delete_file(p)\n\n    d = pathfn('test-delete-nested')\n    fs.create_dir(d)\n    f = pathfn('test-delete-nested/target-file')\n    with fs.open_output_stream(f) as s:\n        s.write(b'data')\n\n    fs.delete_dir(d)\n\n\ndef identity(v):\n    return v\n\n\n@pytest.mark.gzip\n@pytest.mark.parametrize(\n    ('compression', 'buffer_size', 'compressor'),\n    [\n        (None, None, identity),\n        (None, 64, identity),\n        ('gzip', None, gzip.compress),\n        ('gzip', 256, gzip.compress),\n    ]\n)\ndef test_open_input_stream(fs, pathfn, compression, buffer_size, compressor):\n    p = pathfn('open-input-stream')\n\n    data = b'some data for reading\\n' * 512\n    with fs.open_output_stream(p) as s:\n        s.write(compressor(data))\n\n    with fs.open_input_stream(p, compression, buffer_size) as s:\n        result = s.read()\n\n    assert result == data\n\n\ndef test_open_input_file(fs, pathfn):\n    p = pathfn('open-input-file')\n\n    data = b'some data' * 1024\n    with fs.open_output_stream(p) as s:\n        s.write(data)\n\n    read_from = len(b'some data') * 512\n    with fs.open_input_file(p) as f:\n        result = f.read()\n    assert result == data\n\n    with fs.open_input_file(p) as f:\n        f.seek(read_from)\n        result = f.read()\n\n    assert result == data[read_from:]\n\n\ndef test_open_input_stream_not_found(fs, pathfn):\n    # The proper exception should be raised for this common case (ARROW-15896)\n    p = pathfn('open-input-stream-not-found')\n    with pytest.raises(FileNotFoundError):\n        fs.open_input_stream(p)\n\n\n@pytest.mark.gzip\n@pytest.mark.parametrize(\n    ('compression', 'buffer_size', 'decompressor'),\n    [\n        (None, None, identity),\n        (None, 64, identity),\n        ('gzip', None, gzip.decompress),\n        ('gzip', 256, gzip.decompress),\n    ]\n)\ndef test_open_output_stream(fs, pathfn, compression, buffer_size,\n                            decompressor):\n    p = pathfn('open-output-stream')\n\n    data = b'some data for writing' * 1024\n    with fs.open_output_stream(p, compression, buffer_size) as f:\n        f.write(data)\n\n    with fs.open_input_stream(p, compression, buffer_size) as f:\n        assert f.read(len(data)) == data\n\n\n@pytest.mark.gzip\n@pytest.mark.parametrize(\n    ('compression', 'buffer_size', 'compressor', 'decompressor'),\n    [\n        (None, None, identity, identity),\n        (None, 64, identity, identity),\n        ('gzip', None, gzip.compress, gzip.decompress),\n        ('gzip', 256, gzip.compress, gzip.decompress),\n    ]\n)\ndef test_open_append_stream(fs, pathfn, compression, buffer_size, compressor,\n                            decompressor, allow_append_to_file):\n    p = pathfn('open-append-stream')\n\n    initial = compressor(b'already existing')\n    with fs.open_output_stream(p) as s:\n        s.write(initial)\n\n    if allow_append_to_file:\n        with fs.open_append_stream(p, compression=compression,\n                                   buffer_size=buffer_size) as f:\n            f.write(b'\\nnewly added')\n\n        with fs.open_input_stream(p) as f:\n            result = f.read()\n\n        result = decompressor(result)\n        assert result == b'already existing\\nnewly added'\n    else:\n        with pytest.raises(pa.ArrowNotImplementedError):\n            fs.open_append_stream(p, compression=compression,\n                                  buffer_size=buffer_size)\n\n\ndef test_open_output_stream_metadata(fs, pathfn):\n    p = pathfn('open-output-stream-metadata')\n    metadata = {'Content-Type': 'x-pyarrow/test'}\n\n    data = b'some data'\n    with fs.open_output_stream(p, metadata=metadata) as f:\n        f.write(data)\n\n    with fs.open_input_stream(p) as f:\n        assert f.read() == data\n        got_metadata = f.metadata()\n\n    if fs.type_name in ['s3', 'gcs', 'abfs'] or 'mock' in fs.type_name:\n        # TODO(GH-40026): Stop skipping this test\n        skip_azure(\n            fs, \"Azure filesystem currently only returns system metadata not user \"\n            \"metadata. See GH-40026\")\n        for k, v in metadata.items():\n            assert got_metadata[k] == v.encode()\n    else:\n        assert got_metadata == {}\n\n\ndef test_localfs_options():\n    # LocalFileSystem instantiation\n    LocalFileSystem(use_mmap=False)\n\n    with pytest.raises(TypeError):\n        LocalFileSystem(xxx=False)\n\n\ndef test_localfs_errors(localfs):\n    # Local filesystem errors should raise the right Python exceptions\n    # (e.g. FileNotFoundError)\n    fs = localfs['fs']\n    with assert_file_not_found():\n        fs.open_input_stream('/non/existent/file')\n    with assert_file_not_found():\n        fs.open_output_stream('/non/existent/file')\n    with assert_file_not_found():\n        fs.create_dir('/non/existent/dir', recursive=False)\n    with assert_file_not_found():\n        fs.delete_dir('/non/existent/dir')\n    with assert_file_not_found():\n        fs.delete_file('/non/existent/dir')\n    with assert_file_not_found():\n        fs.move('/non/existent', '/xxx')\n    with assert_file_not_found():\n        fs.copy_file('/non/existent', '/xxx')\n\n\ndef test_localfs_file_info(localfs):\n    fs = localfs['fs']\n\n    file_path = pathlib.Path(__file__)\n    dir_path = file_path.parent\n    [file_info, dir_info] = fs.get_file_info([file_path.as_posix(),\n                                              dir_path.as_posix()])\n    assert file_info.size == file_path.stat().st_size\n    assert file_info.mtime_ns == file_path.stat().st_mtime_ns\n    check_mtime(file_info)\n    assert dir_info.mtime_ns == dir_path.stat().st_mtime_ns\n    check_mtime(dir_info)\n\n\ndef test_mockfs_mtime_roundtrip(mockfs):\n    dt = datetime.fromtimestamp(1568799826, timezone.utc)\n    fs = _MockFileSystem(dt)\n\n    with fs.open_output_stream('foo'):\n        pass\n    [info] = fs.get_file_info(['foo'])\n    assert info.mtime == dt\n\n\n@pytest.mark.gcs\ndef test_gcs_options(pickle_module):\n    from pyarrow.fs import GcsFileSystem\n    dt = datetime.now()\n    fs = GcsFileSystem(access_token='abc',\n                       target_service_account='service_account@apache',\n                       credential_token_expiration=dt,\n                       default_bucket_location='us-west2',\n                       scheme='https', endpoint_override='localhost:8999',\n                       project_id='test-project-id')\n    assert isinstance(fs, GcsFileSystem)\n    assert fs.default_bucket_location == 'us-west2'\n    assert fs.project_id == 'test-project-id'\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs = GcsFileSystem()\n    assert isinstance(fs, GcsFileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs = GcsFileSystem(anonymous=True)\n    assert isinstance(fs, GcsFileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs = GcsFileSystem(default_metadata={\"ACL\": \"authenticated-read\",\n                                         \"Content-Type\": \"text/plain\"})\n    assert isinstance(fs, GcsFileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    with pytest.raises(ValueError):\n        GcsFileSystem(access_token='access')\n    with pytest.raises(ValueError):\n        GcsFileSystem(anonymous=True, access_token='secret')\n    with pytest.raises(ValueError):\n        GcsFileSystem(anonymous=True, target_service_account='acct')\n    with pytest.raises(ValueError):\n        GcsFileSystem(credential_token_expiration=datetime.now())\n\n\n@pytest.mark.s3\ndef test_s3_options(pickle_module):\n    from pyarrow.fs import (AwsDefaultS3RetryStrategy,\n                            AwsStandardS3RetryStrategy, S3FileSystem,\n                            S3RetryStrategy)\n\n    fs = S3FileSystem(access_key='access', secret_key='secret',\n                      session_token='token', region='us-east-2',\n                      scheme='https', endpoint_override='localhost:8999')\n    assert isinstance(fs, S3FileSystem)\n    assert fs.region == 'us-east-2'\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs = S3FileSystem(role_arn='role', session_name='session',\n                      external_id='id', load_frequency=100)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    # Note that the retry strategy won't survive pickling for now\n    fs = S3FileSystem(\n        retry_strategy=AwsStandardS3RetryStrategy(max_attempts=5))\n    assert isinstance(fs, S3FileSystem)\n\n    fs = S3FileSystem(\n        retry_strategy=AwsDefaultS3RetryStrategy(max_attempts=5))\n    assert isinstance(fs, S3FileSystem)\n\n    fs2 = S3FileSystem(role_arn='role')\n    assert isinstance(fs2, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs2)) == fs2\n    assert fs2 != fs\n\n    fs = S3FileSystem(anonymous=True)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs = S3FileSystem(background_writes=True)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs2 = S3FileSystem(background_writes=True,\n                       default_metadata={\"ACL\": \"authenticated-read\",\n                                         \"Content-Type\": \"text/plain\"})\n    assert isinstance(fs2, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs2)) == fs2\n    assert fs2 != fs\n\n    fs = S3FileSystem(allow_bucket_creation=True, allow_bucket_deletion=True)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs = S3FileSystem(allow_bucket_creation=True, allow_bucket_deletion=True,\n                      check_directory_existence_before_creation=True)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs = S3FileSystem(request_timeout=0.5, connect_timeout=0.25)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs2 = S3FileSystem(request_timeout=0.25, connect_timeout=0.5)\n    assert isinstance(fs2, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs2)) == fs2\n    assert fs2 != fs\n\n    fs = S3FileSystem(endpoint_override='localhost:8999', force_virtual_addressing=True)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    with pytest.raises(ValueError):\n        S3FileSystem(access_key='access')\n    with pytest.raises(ValueError):\n        S3FileSystem(secret_key='secret')\n    with pytest.raises(ValueError):\n        S3FileSystem(access_key='access', session_token='token')\n    with pytest.raises(ValueError):\n        S3FileSystem(secret_key='secret', session_token='token')\n    with pytest.raises(ValueError):\n        S3FileSystem(\n            access_key='access', secret_key='secret', role_arn='arn'\n        )\n    with pytest.raises(ValueError):\n        S3FileSystem(\n            access_key='access', secret_key='secret', anonymous=True\n        )\n    with pytest.raises(ValueError):\n        S3FileSystem(role_arn=\"arn\", anonymous=True)\n    with pytest.raises(ValueError):\n        S3FileSystem(default_metadata=[\"foo\", \"bar\"])\n    with pytest.raises(ValueError):\n        S3FileSystem(retry_strategy=S3RetryStrategy())\n\n\n@pytest.mark.s3\ndef test_s3_proxy_options(monkeypatch, pickle_module):\n    from pyarrow.fs import S3FileSystem\n\n    # The following two are equivalent:\n    proxy_opts_1_dict = {'scheme': 'http', 'host': 'localhost', 'port': 8999}\n    proxy_opts_1_str = 'http://localhost:8999'\n    # The following two are equivalent:\n    proxy_opts_2_dict = {'scheme': 'https', 'host': 'localhost', 'port': 8080}\n    proxy_opts_2_str = 'https://localhost:8080'\n\n    # Check dict case for 'proxy_options'\n    fs = S3FileSystem(proxy_options=proxy_opts_1_dict)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs = S3FileSystem(proxy_options=proxy_opts_2_dict)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    # Check str case for 'proxy_options'\n    fs = S3FileSystem(proxy_options=proxy_opts_1_str)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    fs = S3FileSystem(proxy_options=proxy_opts_2_str)\n    assert isinstance(fs, S3FileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    # Check that two FSs using the same proxy_options dict are equal\n    fs1 = S3FileSystem(proxy_options=proxy_opts_1_dict)\n    fs2 = S3FileSystem(proxy_options=proxy_opts_1_dict)\n    assert fs1 == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) == fs1\n\n    fs1 = S3FileSystem(proxy_options=proxy_opts_2_dict)\n    fs2 = S3FileSystem(proxy_options=proxy_opts_2_dict)\n    assert fs1 == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) == fs1\n\n    # Check that two FSs using the same proxy_options str are equal\n    fs1 = S3FileSystem(proxy_options=proxy_opts_1_str)\n    fs2 = S3FileSystem(proxy_options=proxy_opts_1_str)\n    assert fs1 == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) == fs1\n\n    fs1 = S3FileSystem(proxy_options=proxy_opts_2_str)\n    fs2 = S3FileSystem(proxy_options=proxy_opts_2_str)\n    assert fs1 == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) == fs1\n\n    # Check that two FSs using equivalent proxy_options\n    # (one dict, one str) are equal\n    fs1 = S3FileSystem(proxy_options=proxy_opts_1_dict)\n    fs2 = S3FileSystem(proxy_options=proxy_opts_1_str)\n    assert fs1 == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) == fs1\n\n    fs1 = S3FileSystem(proxy_options=proxy_opts_2_dict)\n    fs2 = S3FileSystem(proxy_options=proxy_opts_2_str)\n    assert fs1 == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) == fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) == fs1\n\n    # Check that two FSs using nonequivalent proxy_options are not equal\n    fs1 = S3FileSystem(proxy_options=proxy_opts_1_dict)\n    fs2 = S3FileSystem(proxy_options=proxy_opts_2_dict)\n    assert fs1 != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) != fs1\n\n    fs1 = S3FileSystem(proxy_options=proxy_opts_1_dict)\n    fs2 = S3FileSystem(proxy_options=proxy_opts_2_str)\n    assert fs1 != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) != fs1\n\n    fs1 = S3FileSystem(proxy_options=proxy_opts_1_str)\n    fs2 = S3FileSystem(proxy_options=proxy_opts_2_dict)\n    assert fs1 != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) != fs1\n\n    fs1 = S3FileSystem(proxy_options=proxy_opts_1_str)\n    fs2 = S3FileSystem(proxy_options=proxy_opts_2_str)\n    assert fs1 != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) != fs1\n\n    # Check that two FSs (one using proxy_options and the other not)\n    # are not equal\n    fs1 = S3FileSystem(proxy_options=proxy_opts_1_dict)\n    fs2 = S3FileSystem()\n    assert fs1 != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) != fs1\n\n    fs1 = S3FileSystem(proxy_options=proxy_opts_1_str)\n    fs2 = S3FileSystem()\n    assert fs1 != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) != fs1\n\n    fs1 = S3FileSystem(proxy_options=proxy_opts_2_dict)\n    fs2 = S3FileSystem()\n    assert fs1 != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) != fs1\n\n    fs1 = S3FileSystem(proxy_options=proxy_opts_2_str)\n    fs2 = S3FileSystem()\n    assert fs1 != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs1)) != fs2\n    assert pickle_module.loads(pickle_module.dumps(fs2)) != fs1\n\n    # Only dict and str are supported\n    with pytest.raises(TypeError):\n        S3FileSystem(proxy_options=('http', 'localhost', 9090))\n    # Missing scheme\n    with pytest.raises(KeyError):\n        S3FileSystem(proxy_options={'host': 'localhost', 'port': 9090})\n    # Missing host\n    with pytest.raises(KeyError):\n        S3FileSystem(proxy_options={'scheme': 'https', 'port': 9090})\n    # Missing port\n    with pytest.raises(KeyError):\n        S3FileSystem(proxy_options={'scheme': 'http', 'host': 'localhost'})\n    # Invalid proxy URI (invalid scheme httpsB)\n    with pytest.raises(pa.ArrowInvalid):\n        S3FileSystem(proxy_options='httpsB://localhost:9000')\n    # Invalid proxy_options dict (invalid scheme httpA)\n    with pytest.raises(pa.ArrowInvalid):\n        S3FileSystem(proxy_options={'scheme': 'httpA', 'host': 'localhost',\n                                    'port': 8999})\n\n\n@pytest.mark.s3\ndef test_s3fs_wrong_region():\n    from pyarrow.fs import S3FileSystem\n\n    # wrong region for bucket\n    # anonymous=True incase CI/etc has invalid credentials\n    fs = S3FileSystem(region='eu-north-1', anonymous=True)\n\n    msg = (\"When getting information for bucket 'voltrondata-labs-datasets': \"\n           r\"AWS Error UNKNOWN \\(HTTP status 301\\) during HeadBucket \"\n           \"operation: No response body. Looks like the configured region is \"\n           \"'eu-north-1' while the bucket is located in 'us-east-2'.\"\n           \"|NETWORK_CONNECTION\")\n    with pytest.raises(OSError, match=msg) as exc:\n        fs.get_file_info(\"voltrondata-labs-datasets\")\n\n    # Sometimes fails on unrelated network error, so next call would also fail.\n    if 'NETWORK_CONNECTION' in str(exc.value):\n        return\n\n    fs = S3FileSystem(region='us-east-2', anonymous=True)\n    fs.get_file_info(\"voltrondata-labs-datasets\")\n\n\n@pytest.mark.azure\ndef test_azurefs_options(pickle_module):\n    from pyarrow.fs import AzureFileSystem\n\n    fs1 = AzureFileSystem(account_name='fake-account-name')\n    assert isinstance(fs1, AzureFileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs1)) == fs1\n\n    fs2 = AzureFileSystem(account_name='fake-account-name',\n                          account_key='fakeaccountkey')\n    assert isinstance(fs2, AzureFileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs2)) == fs2\n    assert fs2 != fs1\n\n    fs3 = AzureFileSystem(account_name='fake-account', account_key='fakeaccount',\n                          blob_storage_authority='fake-blob-authority',\n                          dfs_storage_authority='fake-dfs-authority',\n                          blob_storage_scheme='https',\n                          dfs_storage_scheme='https')\n    assert isinstance(fs3, AzureFileSystem)\n    assert pickle_module.loads(pickle_module.dumps(fs3)) == fs3\n    assert fs3 != fs2\n\n    with pytest.raises(TypeError):\n        AzureFileSystem()\n\n\n@pytest.mark.hdfs\ndef test_hdfs_options(hdfs_connection, pickle_module):\n    from pyarrow.fs import HadoopFileSystem\n    if not pa.have_libhdfs():\n        pytest.skip('Cannot locate libhdfs')\n\n    host, port, user = hdfs_connection\n\n    replication = 2\n    buffer_size = 64*1024\n    default_block_size = 128*1024**2\n    uri = ('hdfs://{}:{}/?user={}&replication={}&buffer_size={}'\n           '&default_block_size={}')\n\n    hdfs1 = HadoopFileSystem(host, port, user='libhdfs',\n                             replication=replication, buffer_size=buffer_size,\n                             default_block_size=default_block_size)\n    hdfs2 = HadoopFileSystem.from_uri(uri.format(\n        host, port, 'libhdfs', replication, buffer_size, default_block_size\n    ))\n    hdfs3 = HadoopFileSystem.from_uri(uri.format(\n        host, port, 'me', replication, buffer_size, default_block_size\n    ))\n    hdfs4 = HadoopFileSystem.from_uri(uri.format(\n        host, port, 'me', replication + 1, buffer_size, default_block_size\n    ))\n    hdfs5 = HadoopFileSystem(host, port)\n    hdfs6 = HadoopFileSystem.from_uri('hdfs://{}:{}'.format(host, port))\n    hdfs7 = HadoopFileSystem(host, port, user='localuser')\n    hdfs8 = HadoopFileSystem(host, port, user='localuser',\n                             kerb_ticket=\"cache_path\")\n    hdfs9 = HadoopFileSystem(host, port, user='localuser',\n                             kerb_ticket=pathlib.Path(\"cache_path\"))\n    hdfs10 = HadoopFileSystem(host, port, user='localuser',\n                              kerb_ticket=\"cache_path2\")\n    hdfs11 = HadoopFileSystem(host, port, user='localuser',\n                              kerb_ticket=\"cache_path\",\n                              extra_conf={'hdfs_token': 'abcd'})\n\n    assert hdfs1 == hdfs2\n    assert hdfs5 == hdfs6\n    assert hdfs6 != hdfs7\n    assert hdfs2 != hdfs3\n    assert hdfs3 != hdfs4\n    assert hdfs7 != hdfs5\n    assert hdfs2 != hdfs3\n    assert hdfs3 != hdfs4\n    assert hdfs7 != hdfs8\n    assert hdfs8 == hdfs9\n    assert hdfs10 != hdfs9\n    assert hdfs11 != hdfs8\n\n    with pytest.raises(TypeError):\n        HadoopFileSystem()\n    with pytest.raises(TypeError):\n        HadoopFileSystem.from_uri(3)\n\n    for fs in [hdfs1, hdfs2, hdfs3, hdfs4, hdfs5, hdfs6, hdfs7, hdfs8,\n               hdfs9, hdfs10, hdfs11]:\n        assert pickle_module.loads(pickle_module.dumps(fs)) == fs\n\n    host, port, user = hdfs_connection\n\n    hdfs = HadoopFileSystem(host, port, user=user)\n    assert hdfs.get_file_info(FileSelector('/'))\n\n    hdfs = HadoopFileSystem.from_uri(\n        \"hdfs://{}:{}/?user={}\".format(host, port, user)\n    )\n    assert hdfs.get_file_info(FileSelector('/'))\n\n\n@pytest.mark.parametrize(('uri', 'expected_klass', 'expected_path'), [\n    # leading slashes are removed intentionally, because MockFileSystem doesn't\n    # have a distinction between relative and absolute paths\n    ('mock:', _MockFileSystem, ''),\n    ('mock:foo/bar', _MockFileSystem, 'foo/bar'),\n    ('mock:/foo/bar', _MockFileSystem, 'foo/bar'),\n    ('mock:///foo/bar', _MockFileSystem, 'foo/bar'),\n    ('mock:///some%20path/%C3%A9', _MockFileSystem, 'some path/\u00e9'),\n    ('file:/', LocalFileSystem, '/'),\n    ('file:///', LocalFileSystem, '/'),\n    ('file:/foo/bar', LocalFileSystem, '/foo/bar'),\n    ('file:///foo/bar', LocalFileSystem, '/foo/bar'),\n    ('file:///some%20path/%C3%A9', LocalFileSystem, '/some path/\u00e9'),\n    # no %-decoding for non-URI inputs\n    ('/', LocalFileSystem, '/'),\n    ('/foo/bar', LocalFileSystem, '/foo/bar'),\n    ('/some path/%20\u00e9', LocalFileSystem, '/some path/%20\u00e9'),\n])\ndef test_filesystem_from_uri(uri, expected_klass, expected_path):\n    fs, path = FileSystem.from_uri(uri)\n    assert isinstance(fs, expected_klass)\n    assert path == expected_path\n\n\n@pytest.mark.parametrize(\n    'path',\n    ['', '/', 'foo/bar', '/foo/bar', __file__]\n)\ndef test_filesystem_from_path_object(path):\n    p = pathlib.Path(path)\n    fs, path = FileSystem.from_uri(p)\n    assert isinstance(fs, LocalFileSystem)\n    assert path == p.resolve().absolute().as_posix()\n\n\n@pytest.mark.s3\ndef test_filesystem_from_uri_s3(s3_server):\n    from pyarrow.fs import S3FileSystem\n\n    host, port, access_key, secret_key = s3_server['connection']\n\n    uri = \"s3://{}:{}@mybucket/foo/bar?scheme=http&endpoint_override={}:{}\"\\\n          \"&allow_bucket_creation=True\" \\\n          .format(access_key, secret_key, host, port)\n\n    fs, path = FileSystem.from_uri(uri)\n    assert isinstance(fs, S3FileSystem)\n    assert path == \"mybucket/foo/bar\"\n\n    fs.create_dir(path)\n    [info] = fs.get_file_info([path])\n    assert info.path == path\n    assert info.type == FileType.Directory\n\n\n@pytest.mark.gcs\ndef test_filesystem_from_uri_gcs(gcs_server):\n    from pyarrow.fs import GcsFileSystem\n\n    host, port = gcs_server['connection']\n\n    uri = (\"gs://anonymous@\" +\n           f\"mybucket/foo/bar?scheme=http&endpoint_override={host}:{port}&\" +\n           \"retry_limit_seconds=5&project_id=test-project-id\")\n\n    fs, path = FileSystem.from_uri(uri)\n    assert isinstance(fs, GcsFileSystem)\n    assert path == \"mybucket/foo/bar\"\n\n    fs.create_dir(path)\n    [info] = fs.get_file_info([path])\n    assert info.path == path\n    assert info.type == FileType.Directory\n\n\ndef test_py_filesystem():\n    handler = DummyHandler()\n    fs = PyFileSystem(handler)\n    assert isinstance(fs, PyFileSystem)\n    assert fs.type_name == \"py::dummy\"\n    assert fs.handler is handler\n\n    with pytest.raises(TypeError):\n        PyFileSystem(None)\n\n\ndef test_py_filesystem_equality():\n    handler1 = DummyHandler(1)\n    handler2 = DummyHandler(2)\n    handler3 = DummyHandler(2)\n    fs1 = PyFileSystem(handler1)\n    fs2 = PyFileSystem(handler1)\n    fs3 = PyFileSystem(handler2)\n    fs4 = PyFileSystem(handler3)\n\n    assert fs2 is not fs1\n    assert fs3 is not fs2\n    assert fs4 is not fs3\n    assert fs2 == fs1  # Same handler\n    assert fs3 != fs2  # Unequal handlers\n    assert fs4 == fs3  # Equal handlers\n\n    assert fs1 != LocalFileSystem()\n    assert fs1 != object()\n\n\ndef test_py_filesystem_pickling(pickle_module):\n    handler = DummyHandler()\n    fs = PyFileSystem(handler)\n\n    serialized = pickle_module.dumps(fs)\n    restored = pickle_module.loads(serialized)\n    assert isinstance(restored, FileSystem)\n    assert restored == fs\n    assert restored.handler == handler\n    assert restored.type_name == \"py::dummy\"\n\n\ndef test_py_filesystem_lifetime():\n    handler = DummyHandler()\n    fs = PyFileSystem(handler)\n    assert isinstance(fs, PyFileSystem)\n    wr = weakref.ref(handler)\n    handler = None\n    assert wr() is not None\n    fs = None\n    assert wr() is None\n\n    # Taking the .handler attribute doesn't wreck reference counts\n    handler = DummyHandler()\n    fs = PyFileSystem(handler)\n    wr = weakref.ref(handler)\n    handler = None\n    assert wr() is fs.handler\n    assert wr() is not None\n    fs = None\n    assert wr() is None\n\n\ndef test_py_filesystem_get_file_info():\n    handler = DummyHandler()\n    fs = PyFileSystem(handler)\n\n    [info] = fs.get_file_info(['some/dir'])\n    assert info.path == 'some/dir'\n    assert info.type == FileType.Directory\n\n    [info] = fs.get_file_info(['some/file'])\n    assert info.path == 'some/file'\n    assert info.type == FileType.File\n\n    [info] = fs.get_file_info(['notfound'])\n    assert info.path == 'notfound'\n    assert info.type == FileType.NotFound\n\n    with pytest.raises(TypeError):\n        fs.get_file_info(['badtype'])\n\n    with pytest.raises(IOError):\n        fs.get_file_info(['xxx'])\n\n\ndef test_py_filesystem_get_file_info_selector():\n    handler = DummyHandler()\n    fs = PyFileSystem(handler)\n\n    selector = FileSelector(base_dir=\"somedir\")\n    infos = fs.get_file_info(selector)\n    assert len(infos) == 2\n    assert infos[0].path == \"somedir/file1\"\n    assert infos[0].type == FileType.File\n    assert infos[0].size == 123\n    assert infos[1].path == \"somedir/subdir1\"\n    assert infos[1].type == FileType.Directory\n    assert infos[1].size is None\n\n    selector = FileSelector(base_dir=\"somedir\", recursive=True)\n    infos = fs.get_file_info(selector)\n    assert len(infos) == 3\n    assert infos[0].path == \"somedir/file1\"\n    assert infos[1].path == \"somedir/subdir1\"\n    assert infos[2].path == \"somedir/subdir1/file2\"\n\n    selector = FileSelector(base_dir=\"notfound\")\n    with pytest.raises(FileNotFoundError):\n        fs.get_file_info(selector)\n\n    selector = FileSelector(base_dir=\"notfound\", allow_not_found=True)\n    assert fs.get_file_info(selector) == []\n\n\ndef test_py_filesystem_ops():\n    handler = DummyHandler()\n    fs = PyFileSystem(handler)\n\n    fs.create_dir(\"recursive\", recursive=True)\n    fs.create_dir(\"non-recursive\", recursive=False)\n    with pytest.raises(IOError):\n        fs.create_dir(\"foobar\")\n\n    fs.delete_dir(\"delete_dir\")\n    fs.delete_dir_contents(\"delete_dir_contents\")\n    for path in (\"\", \"/\", \"//\"):\n        with pytest.raises(ValueError):\n            fs.delete_dir_contents(path)\n        fs.delete_dir_contents(path, accept_root_dir=True)\n    fs.delete_file(\"delete_file\")\n    fs.move(\"move_from\", \"move_to\")\n    fs.copy_file(\"copy_file_from\", \"copy_file_to\")\n\n\ndef test_py_open_input_stream():\n    fs = PyFileSystem(DummyHandler())\n\n    with fs.open_input_stream(\"somefile\") as f:\n        assert f.read() == b\"somefile:input_stream\"\n    with pytest.raises(FileNotFoundError):\n        fs.open_input_stream(\"notfound\")\n\n\ndef test_py_open_input_file():\n    fs = PyFileSystem(DummyHandler())\n\n    with fs.open_input_file(\"somefile\") as f:\n        assert f.read() == b\"somefile:input_file\"\n    with pytest.raises(FileNotFoundError):\n        fs.open_input_file(\"notfound\")\n\n\ndef test_py_open_output_stream():\n    fs = PyFileSystem(DummyHandler())\n\n    with fs.open_output_stream(\"somefile\") as f:\n        f.write(b\"data\")\n\n\ndef test_py_open_append_stream():\n    fs = PyFileSystem(DummyHandler())\n\n    with fs.open_append_stream(\"somefile\") as f:\n        f.write(b\"data\")\n\n\n@pytest.mark.s3\ndef test_s3_real_aws():\n    # Exercise connection code with an AWS-backed S3 bucket.\n    # This is a minimal integration check for ARROW-9261 and similar issues.\n    from pyarrow.fs import S3FileSystem\n    default_region = (os.environ.get('PYARROW_TEST_S3_REGION') or\n                      'us-east-1')\n    fs = S3FileSystem(anonymous=True)\n    assert fs.region == default_region\n\n    fs = S3FileSystem(anonymous=True, region='us-east-2')\n    entries = fs.get_file_info(FileSelector(\n        'voltrondata-labs-datasets/nyc-taxi'))\n    assert len(entries) > 0\n    key = 'voltrondata-labs-datasets/nyc-taxi/year=2019/month=6/part-0.parquet'\n    with fs.open_input_stream(key) as f:\n        md = f.metadata()\n        assert 'Content-Type' in md\n        assert md['Last-Modified'] == b'2022-07-12T23:32:00Z'\n        # For some reason, the header value is quoted\n        # (both with AWS and Minio)\n        assert md['ETag'] == b'\"4c6a76826a695c6ac61592bc30cda3df-16\"'\n\n\n@pytest.mark.s3\ndef test_s3_real_aws_region_selection():\n    # Taken from a registry of open S3-hosted datasets\n    # at https://github.com/awslabs/open-data-registry\n    fs, path = FileSystem.from_uri('s3://mf-nwp-models/README.txt')\n    assert fs.region == 'eu-west-1'\n    with fs.open_input_stream(path) as f:\n        assert b\"Meteo-France Atmospheric models on AWS\" in f.read(50)\n\n    # Passing an explicit region disables auto-selection\n    fs, path = FileSystem.from_uri(\n        's3://mf-nwp-models/README.txt?region=us-east-2')\n    assert fs.region == 'us-east-2'\n    # Reading from the wrong region may still work for public buckets...\n\n    # Nonexistent bucket (hopefully, otherwise need to fix this test)\n    with pytest.raises(IOError, match=\"Bucket '.*' not found\"):\n        FileSystem.from_uri('s3://x-arrow-nonexistent-bucket')\n    fs, path = FileSystem.from_uri(\n        's3://x-arrow-nonexistent-bucket?region=us-east-3')\n    assert fs.region == 'us-east-3'\n\n\n@pytest.mark.s3\ndef test_resolve_s3_region():\n    from pyarrow.fs import resolve_s3_region\n    assert resolve_s3_region('voltrondata-labs-datasets') == 'us-east-2'\n    assert resolve_s3_region('mf-nwp-models') == 'eu-west-1'\n\n    with pytest.raises(ValueError, match=\"Not a valid bucket name\"):\n        resolve_s3_region('foo/bar')\n    with pytest.raises(ValueError, match=\"Not a valid bucket name\"):\n        resolve_s3_region('s3:bucket')\n\n\n@pytest.mark.s3\ndef test_copy_files(s3_connection, s3fs, tempdir):\n    fs = s3fs[\"fs\"]\n    pathfn = s3fs[\"pathfn\"]\n\n    # create test file on S3 filesystem\n    path = pathfn('c.txt')\n    with fs.open_output_stream(path) as f:\n        f.write(b'test')\n\n    # create URI for created file\n    host, port, access_key, secret_key = s3_connection\n    source_uri = (\n        f\"s3://{access_key}:{secret_key}@{path}\"\n        f\"?scheme=http&endpoint_override={host}:{port}\"\n    )\n    # copy from S3 URI to local file\n    local_path1 = str(tempdir / \"c_copied1.txt\")\n    copy_files(source_uri, local_path1)\n\n    localfs = LocalFileSystem()\n    with localfs.open_input_stream(local_path1) as f:\n        assert f.read() == b\"test\"\n\n    # copy from S3 path+filesystem to local file\n    local_path2 = str(tempdir / \"c_copied2.txt\")\n    copy_files(path, local_path2, source_filesystem=fs)\n    with localfs.open_input_stream(local_path2) as f:\n        assert f.read() == b\"test\"\n\n    # copy to local file with URI\n    local_path3 = str(tempdir / \"c_copied3.txt\")\n    destination_uri = _filesystem_uri(local_path3)  # file://\n    copy_files(source_uri, destination_uri)\n\n    with localfs.open_input_stream(local_path3) as f:\n        assert f.read() == b\"test\"\n\n    # copy to local file with path+filesystem\n    local_path4 = str(tempdir / \"c_copied4.txt\")\n    copy_files(source_uri, local_path4, destination_filesystem=localfs)\n\n    with localfs.open_input_stream(local_path4) as f:\n        assert f.read() == b\"test\"\n\n    # copy with additional options\n    local_path5 = str(tempdir / \"c_copied5.txt\")\n    copy_files(source_uri, local_path5, chunk_size=1, use_threads=False)\n\n    with localfs.open_input_stream(local_path5) as f:\n        assert f.read() == b\"test\"\n\n\ndef test_copy_files_directory(tempdir):\n    localfs = LocalFileSystem()\n\n    # create source directory with 2 files\n    source_dir = tempdir / \"source\"\n    source_dir.mkdir()\n    with localfs.open_output_stream(str(source_dir / \"file1\")) as f:\n        f.write(b'test1')\n    with localfs.open_output_stream(str(source_dir / \"file2\")) as f:\n        f.write(b'test2')\n\n    def check_copied_files(destination_dir):\n        with localfs.open_input_stream(str(destination_dir / \"file1\")) as f:\n            assert f.read() == b\"test1\"\n        with localfs.open_input_stream(str(destination_dir / \"file2\")) as f:\n            assert f.read() == b\"test2\"\n\n    # Copy directory with local file paths\n    destination_dir1 = tempdir / \"destination1\"\n    # TODO need to create?\n    destination_dir1.mkdir()\n    copy_files(str(source_dir), str(destination_dir1))\n    check_copied_files(destination_dir1)\n\n    # Copy directory with path+filesystem\n    destination_dir2 = tempdir / \"destination2\"\n    destination_dir2.mkdir()\n    copy_files(str(source_dir), str(destination_dir2),\n               source_filesystem=localfs, destination_filesystem=localfs)\n    check_copied_files(destination_dir2)\n\n    # Copy directory with URI\n    destination_dir3 = tempdir / \"destination3\"\n    destination_dir3.mkdir()\n    source_uri = _filesystem_uri(str(source_dir))  # file://\n    destination_uri = _filesystem_uri(str(destination_dir3))\n    copy_files(source_uri, destination_uri)\n    check_copied_files(destination_dir3)\n\n    # Copy directory with Path objects\n    destination_dir4 = tempdir / \"destination4\"\n    destination_dir4.mkdir()\n    copy_files(source_dir, destination_dir4)\n    check_copied_files(destination_dir4)\n\n    # copy with additional non-default options\n    destination_dir5 = tempdir / \"destination5\"\n    destination_dir5.mkdir()\n    copy_files(source_dir, destination_dir5, chunk_size=1, use_threads=False)\n    check_copied_files(destination_dir5)\n\n\n@pytest.mark.s3\ndef test_s3_finalize():\n    # Once finalize_s3() was called, most/all operations on S3 filesystems\n    # should raise.\n    code = \"\"\"if 1:\n        import pytest\n        from pyarrow.fs import (FileSystem, S3FileSystem,\n                                ensure_s3_initialized, finalize_s3)\n\n        fs, path = FileSystem.from_uri('s3://mf-nwp-models/README.txt')\n        assert fs.region == 'eu-west-1'\n        f = fs.open_input_stream(path)\n        f.read(50)\n\n        finalize_s3()\n\n        with pytest.raises(ValueError, match=\"S3 .* finalized\"):\n            f.read(50)\n        with pytest.raises(ValueError, match=\"S3 .* finalized\"):\n            fs.open_input_stream(path)\n        with pytest.raises(ValueError, match=\"S3 .* finalized\"):\n            S3FileSystem(anonymous=True)\n        with pytest.raises(ValueError, match=\"S3 .* finalized\"):\n            FileSystem.from_uri('s3://mf-nwp-models/README.txt')\n        \"\"\"\n    subprocess.check_call([sys.executable, \"-c\", code])\n\n\n@pytest.mark.s3\ndef test_s3_finalize_region_resolver():\n    # Same as test_s3_finalize(), but exercising region resolution\n    code = \"\"\"if 1:\n        import pytest\n        from pyarrow.fs import resolve_s3_region, ensure_s3_initialized, finalize_s3\n\n        resolve_s3_region('mf-nwp-models')\n\n        finalize_s3()\n\n        # Testing both cached and uncached accesses\n        with pytest.raises(ValueError, match=\"S3 .* finalized\"):\n            resolve_s3_region('mf-nwp-models')\n        with pytest.raises(ValueError, match=\"S3 .* finalized\"):\n            resolve_s3_region('voltrondata-labs-datasets')\n        \"\"\"\n    subprocess.check_call([sys.executable, \"-c\", code])\n\n\n@pytest.mark.s3\ndef test_concurrent_s3fs_init():\n    # GH-39897: lazy concurrent initialization of S3 subsystem should not crash\n    code = \"\"\"if 1:\n        import threading\n        import pytest\n        from pyarrow.fs import (FileSystem, S3FileSystem,\n                                ensure_s3_initialized, finalize_s3)\n        threads = []\n        fn = lambda: FileSystem.from_uri('s3://mf-nwp-models/README.txt')\n        for i in range(4):\n            thread = threading.Thread(target = fn)\n            threads.append(thread)\n            thread.start()\n\n        for thread in threads:\n            thread.join()\n\n        finalize_s3()\n        \"\"\"\n    subprocess.check_call([sys.executable, \"-c\", code])\n", "python/pyarrow/tests/test_flight_async.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport asyncio\n\nimport pytest\n\nimport pyarrow\n\nflight = pytest.importorskip(\"pyarrow.flight\")\npytestmark = pytest.mark.flight\n\n\nclass ExampleServer(flight.FlightServerBase):\n    simple_info = flight.FlightInfo(\n        pyarrow.schema([(\"a\", \"int32\")]),\n        flight.FlightDescriptor.for_command(b\"simple\"),\n        [],\n        -1,\n        -1,\n    )\n\n    def get_flight_info(self, context, descriptor):\n        if descriptor.command == b\"simple\":\n            return self.simple_info\n        elif descriptor.command == b\"unknown\":\n            raise NotImplementedError(\"Unknown command\")\n\n        raise NotImplementedError(\"Unknown descriptor\")\n\n\ndef async_or_skip(client):\n    if not client.supports_async:\n        # Use async error message as skip message\n        with pytest.raises(NotImplementedError) as e:\n            client.as_async()\n        pytest.skip(str(e.value))\n\n\n@pytest.fixture(scope=\"module\")\ndef flight_client():\n    with ExampleServer() as server:\n        with flight.connect(f\"grpc://localhost:{server.port}\") as client:\n            yield client\n\n\n@pytest.fixture(scope=\"module\")\ndef async_client(flight_client):\n    async_or_skip(flight_client)\n    yield flight_client.as_async()\n\n\ndef test_async_support_property(flight_client):\n    assert isinstance(flight_client.supports_async, bool)\n    if flight_client.supports_async:\n        flight_client.as_async()\n    else:\n        with pytest.raises(NotImplementedError):\n            flight_client.as_async()\n\n\ndef test_get_flight_info(async_client):\n    async def _test():\n        descriptor = flight.FlightDescriptor.for_command(b\"simple\")\n        info = await async_client.get_flight_info(descriptor)\n        assert info == ExampleServer.simple_info\n\n    asyncio.run(_test())\n\n\ndef test_get_flight_info_error(async_client):\n    async def _test():\n        descriptor = flight.FlightDescriptor.for_command(b\"unknown\")\n        with pytest.raises(NotImplementedError) as excinfo:\n            await async_client.get_flight_info(descriptor)\n\n        assert \"Unknown command\" in repr(excinfo.value)\n\n    asyncio.run(_test())\n", "python/pyarrow/tests/pandas_threaded_import.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# This file is called from a test in test_pandas.py.\n\nfrom concurrent.futures import ThreadPoolExecutor\nimport faulthandler\nimport sys\n\nimport pyarrow as pa\n\nnum_threads = 60\ntimeout = 10  # seconds\n\n\ndef thread_func(i):\n    pa.array([i]).to_pandas()\n\n\ndef main():\n    # In case of import deadlock, crash after a finite timeout\n    faulthandler.dump_traceback_later(timeout, exit=True)\n    with ThreadPoolExecutor(num_threads) as pool:\n        assert \"pandas\" not in sys.modules  # pandas is imported lazily\n        list(pool.map(thread_func, range(num_threads)))\n        assert \"pandas\" in sys.modules\n\n\nif __name__ == \"__main__\":\n    main()\n", "python/pyarrow/tests/test_array.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections.abc import Iterable\nimport datetime\nimport decimal\nimport hypothesis as h\nimport hypothesis.strategies as st\nimport itertools\nimport pytest\nimport struct\nimport subprocess\nimport sys\nimport weakref\n\nimport numpy as np\n\nimport pyarrow as pa\nimport pyarrow.tests.strategies as past\nfrom pyarrow.vendored.version import Version\n\n\ndef test_total_bytes_allocated():\n    code = \"\"\"if 1:\n    import pyarrow as pa\n\n    assert pa.total_allocated_bytes() == 0\n    \"\"\"\n    res = subprocess.run([sys.executable, \"-c\", code],\n                         universal_newlines=True, stderr=subprocess.PIPE)\n    if res.returncode != 0:\n        print(res.stderr, file=sys.stderr)\n        res.check_returncode()  # fail\n    assert len(res.stderr.splitlines()) == 0\n\n\ndef test_weakref():\n    arr = pa.array([1, 2, 3])\n    wr = weakref.ref(arr)\n    assert wr() is not None\n    del arr\n    assert wr() is None\n\n\ndef test_getitem_NULL():\n    arr = pa.array([1, None, 2])\n    assert arr[1].as_py() is None\n    assert arr[1].is_valid is False\n    assert isinstance(arr[1], pa.Int64Scalar)\n\n\ndef test_constructor_raises():\n    # This could happen by wrong capitalization.\n    # ARROW-2638: prevent calling extension class constructors directly\n    with pytest.raises(TypeError):\n        pa.Array([1, 2])\n\n\ndef test_list_format():\n    arr = pa.array([[1], None, [2, 3, None]])\n    result = arr.to_string()\n    expected = \"\"\"\\\n[\n  [\n    1\n  ],\n  null,\n  [\n    2,\n    3,\n    null\n  ]\n]\"\"\"\n    assert result == expected\n\n\ndef test_string_format():\n    arr = pa.array(['', None, 'foo'])\n    result = arr.to_string()\n    expected = \"\"\"\\\n[\n  \"\",\n  null,\n  \"foo\"\n]\"\"\"\n    assert result == expected\n\n\ndef test_long_array_format():\n    arr = pa.array(range(100))\n    result = arr.to_string(window=2)\n    expected = \"\"\"\\\n[\n  0,\n  1,\n  ...\n  98,\n  99\n]\"\"\"\n    assert result == expected\n\n\ndef test_indented_string_format():\n    arr = pa.array(['', None, 'foo'])\n    result = arr.to_string(indent=1)\n    expected = '[\\n \"\",\\n null,\\n \"foo\"\\n]'\n\n    assert result == expected\n\n\ndef test_top_level_indented_string_format():\n    arr = pa.array(['', None, 'foo'])\n    result = arr.to_string(top_level_indent=1)\n    expected = ' [\\n   \"\",\\n   null,\\n   \"foo\"\\n ]'\n\n    assert result == expected\n\n\ndef test_binary_format():\n    arr = pa.array([b'\\x00', b'', None, b'\\x01foo', b'\\x80\\xff'])\n    result = arr.to_string()\n    expected = \"\"\"\\\n[\n  00,\n  ,\n  null,\n  01666F6F,\n  80FF\n]\"\"\"\n    assert result == expected\n\n\ndef test_binary_total_values_length():\n    arr = pa.array([b'0000', None, b'11111', b'222222', b'3333333'],\n                   type='binary')\n    large_arr = pa.array([b'0000', None, b'11111', b'222222', b'3333333'],\n                         type='large_binary')\n\n    assert arr.total_values_length == 22\n    assert arr.slice(1, 3).total_values_length == 11\n    assert large_arr.total_values_length == 22\n    assert large_arr.slice(1, 3).total_values_length == 11\n\n\ndef test_to_numpy_zero_copy():\n    arr = pa.array(range(10))\n\n    np_arr = arr.to_numpy()\n\n    # check for zero copy (both arrays using same memory)\n    arrow_buf = arr.buffers()[1]\n    assert arrow_buf.address == np_arr.ctypes.data\n\n    arr = None\n    import gc\n    gc.collect()\n\n    # Ensure base is still valid\n    assert np_arr.base is not None\n    expected = np.arange(10)\n    np.testing.assert_array_equal(np_arr, expected)\n\n\ndef test_chunked_array_to_numpy_zero_copy():\n    elements = [[2, 2, 4], [4, 5, 100]]\n\n    chunked_arr = pa.chunked_array(elements)\n\n    msg = \"zero_copy_only must be False for pyarrow.ChunkedArray.to_numpy\"\n\n    with pytest.raises(ValueError, match=msg):\n        chunked_arr.to_numpy(zero_copy_only=True)\n\n    np_arr = chunked_arr.to_numpy()\n    expected = [2, 2, 4, 4, 5, 100]\n    np.testing.assert_array_equal(np_arr, expected)\n\n\ndef test_to_numpy_unsupported_types():\n    # ARROW-2871: Some primitive types are not yet supported in to_numpy\n    bool_arr = pa.array([True, False, True])\n\n    with pytest.raises(ValueError):\n        bool_arr.to_numpy()\n\n    result = bool_arr.to_numpy(zero_copy_only=False)\n    expected = np.array([True, False, True])\n    np.testing.assert_array_equal(result, expected)\n\n    null_arr = pa.array([None, None, None])\n\n    with pytest.raises(ValueError):\n        null_arr.to_numpy()\n\n    result = null_arr.to_numpy(zero_copy_only=False)\n    expected = np.array([None, None, None], dtype=object)\n    np.testing.assert_array_equal(result, expected)\n\n    arr = pa.array([1, 2, None])\n\n    with pytest.raises(ValueError, match=\"with 1 nulls\"):\n        arr.to_numpy()\n\n\ndef test_to_numpy_writable():\n    arr = pa.array(range(10))\n    np_arr = arr.to_numpy()\n\n    # by default not writable for zero-copy conversion\n    with pytest.raises(ValueError):\n        np_arr[0] = 10\n\n    np_arr2 = arr.to_numpy(zero_copy_only=False, writable=True)\n    np_arr2[0] = 10\n    assert arr[0].as_py() == 0\n\n    # when asking for writable, cannot do zero-copy\n    with pytest.raises(ValueError):\n        arr.to_numpy(zero_copy_only=True, writable=True)\n\n\n@pytest.mark.parametrize('unit', ['s', 'ms', 'us', 'ns'])\n@pytest.mark.parametrize('tz', [None, \"UTC\"])\ndef test_to_numpy_datetime64(unit, tz):\n    arr = pa.array([1, 2, 3], pa.timestamp(unit, tz=tz))\n    expected = np.array([1, 2, 3], dtype=\"datetime64[{}]\".format(unit))\n    np_arr = arr.to_numpy()\n    np.testing.assert_array_equal(np_arr, expected)\n\n\n@pytest.mark.parametrize('unit', ['s', 'ms', 'us', 'ns'])\ndef test_to_numpy_timedelta64(unit):\n    arr = pa.array([1, 2, 3], pa.duration(unit))\n    expected = np.array([1, 2, 3], dtype=\"timedelta64[{}]\".format(unit))\n    np_arr = arr.to_numpy()\n    np.testing.assert_array_equal(np_arr, expected)\n\n\ndef test_to_numpy_dictionary():\n    # ARROW-7591\n    arr = pa.array([\"a\", \"b\", \"a\"]).dictionary_encode()\n    expected = np.array([\"a\", \"b\", \"a\"], dtype=object)\n    np_arr = arr.to_numpy(zero_copy_only=False)\n    np.testing.assert_array_equal(np_arr, expected)\n\n\n@pytest.mark.pandas\ndef test_to_pandas_zero_copy():\n    import gc\n\n    arr = pa.array(range(10))\n\n    for i in range(10):\n        series = arr.to_pandas()\n        assert sys.getrefcount(series) == 2\n        series = None  # noqa\n\n    assert sys.getrefcount(arr) == 2\n\n    for i in range(10):\n        arr = pa.array(range(10))\n        series = arr.to_pandas()\n        arr = None\n        gc.collect()\n\n        # Ensure base is still valid\n\n        # Because of py.test's assert inspection magic, if you put getrefcount\n        # on the line being examined, it will be 1 higher than you expect\n        base_refcount = sys.getrefcount(series.values.base)\n        assert base_refcount == 2\n        series.sum()\n\n\n@pytest.mark.nopandas\n@pytest.mark.pandas\ndef test_asarray():\n    # ensure this is tested both when pandas is present or not (ARROW-6564)\n\n    arr = pa.array(range(4))\n\n    # The iterator interface gives back an array of Int64Value's\n    np_arr = np.asarray([_ for _ in arr])\n    assert np_arr.tolist() == [0, 1, 2, 3]\n    assert np_arr.dtype == np.dtype('O')\n    assert isinstance(np_arr[0], pa.lib.Int64Value)\n\n    # Calling with the arrow array gives back an array with 'int64' dtype\n    np_arr = np.asarray(arr)\n    assert np_arr.tolist() == [0, 1, 2, 3]\n    assert np_arr.dtype == np.dtype('int64')\n\n    # An optional type can be specified when calling np.asarray\n    np_arr = np.asarray(arr, dtype='str')\n    assert np_arr.tolist() == ['0', '1', '2', '3']\n\n    # If PyArrow array has null values, numpy type will be changed as needed\n    # to support nulls.\n    arr = pa.array([0, 1, 2, None])\n    assert arr.type == pa.int64()\n    np_arr = np.asarray(arr)\n    elements = np_arr.tolist()\n    assert elements[:3] == [0., 1., 2.]\n    assert np.isnan(elements[3])\n    assert np_arr.dtype == np.dtype('float64')\n\n    # DictionaryType data will be converted to dense numpy array\n    arr = pa.DictionaryArray.from_arrays(\n        pa.array([0, 1, 2, 0, 1]), pa.array(['a', 'b', 'c']))\n    np_arr = np.asarray(arr)\n    assert np_arr.dtype == np.dtype('object')\n    assert np_arr.tolist() == ['a', 'b', 'c', 'a', 'b']\n\n\n@pytest.mark.parametrize('ty', [\n    None,\n    pa.null(),\n    pa.int8(),\n    pa.string()\n])\ndef test_nulls(ty):\n    arr = pa.nulls(3, type=ty)\n    expected = pa.array([None, None, None], type=ty)\n\n    assert len(arr) == 3\n    assert arr.equals(expected)\n\n    if ty is None:\n        assert arr.type == pa.null()\n    else:\n        assert arr.type == ty\n\n\ndef test_array_from_scalar():\n    pytz = pytest.importorskip(\"pytz\")\n\n    today = datetime.date.today()\n    now = datetime.datetime.now()\n    now_utc = now.replace(tzinfo=pytz.utc)\n    now_with_tz = now_utc.astimezone(pytz.timezone('US/Eastern'))\n    oneday = datetime.timedelta(days=1)\n\n    cases = [\n        (None, 1, pa.array([None])),\n        (None, 10, pa.nulls(10)),\n        (-1, 3, pa.array([-1, -1, -1], type=pa.int64())),\n        (2.71, 2, pa.array([2.71, 2.71], type=pa.float64())),\n        (\"string\", 4, pa.array([\"string\"] * 4)),\n        (\n            pa.scalar(8, type=pa.uint8()),\n            17,\n            pa.array([8] * 17, type=pa.uint8())\n        ),\n        (pa.scalar(None), 3, pa.array([None, None, None])),\n        (pa.scalar(True), 11, pa.array([True] * 11)),\n        (today, 2, pa.array([today] * 2)),\n        (now, 10, pa.array([now] * 10)),\n        (\n            now_with_tz,\n            2,\n            pa.array(\n                [now_utc] * 2,\n                type=pa.timestamp('us', tz=pytz.timezone('US/Eastern'))\n            )\n        ),\n        (now.time(), 9, pa.array([now.time()] * 9)),\n        (oneday, 4, pa.array([oneday] * 4)),\n        (False, 9, pa.array([False] * 9)),\n        ([1, 2], 2, pa.array([[1, 2], [1, 2]])),\n        (\n            pa.scalar([-1, 3], type=pa.large_list(pa.int8())),\n            5,\n            pa.array([[-1, 3]] * 5, type=pa.large_list(pa.int8()))\n        ),\n        ({'a': 1, 'b': 2}, 3, pa.array([{'a': 1, 'b': 2}] * 3))\n    ]\n\n    for value, size, expected in cases:\n        arr = pa.repeat(value, size)\n        assert len(arr) == size\n        assert arr.type.equals(expected.type)\n        assert arr.equals(expected)\n        if expected.type == pa.null():\n            assert arr.null_count == size\n        else:\n            assert arr.null_count == 0\n\n\ndef test_array_from_dictionary_scalar():\n    dictionary = ['foo', 'bar', 'baz']\n    arr = pa.DictionaryArray.from_arrays([2, 1, 2, 0], dictionary=dictionary)\n\n    result = pa.repeat(arr[0], 5)\n    expected = pa.DictionaryArray.from_arrays([2] * 5, dictionary=dictionary)\n    assert result.equals(expected)\n\n    result = pa.repeat(arr[3], 5)\n    expected = pa.DictionaryArray.from_arrays([0] * 5, dictionary=dictionary)\n    assert result.equals(expected)\n\n\ndef test_array_getitem():\n    arr = pa.array(range(10, 15))\n    lst = arr.to_pylist()\n\n    for idx in range(-len(arr), len(arr)):\n        assert arr[idx].as_py() == lst[idx]\n    for idx in range(-2 * len(arr), -len(arr)):\n        with pytest.raises(IndexError):\n            arr[idx]\n    for idx in range(len(arr), 2 * len(arr)):\n        with pytest.raises(IndexError):\n            arr[idx]\n\n    # check that numpy scalars are supported\n    for idx in range(-len(arr), len(arr)):\n        assert arr[np.int32(idx)].as_py() == lst[idx]\n\n\ndef test_array_slice():\n    arr = pa.array(range(10))\n\n    sliced = arr.slice(2)\n    expected = pa.array(range(2, 10))\n    assert sliced.equals(expected)\n\n    sliced2 = arr.slice(2, 4)\n    expected2 = pa.array(range(2, 6))\n    assert sliced2.equals(expected2)\n\n    # 0 offset\n    assert arr.slice(0).equals(arr)\n\n    # Slice past end of array\n    assert len(arr.slice(len(arr))) == 0\n    assert len(arr.slice(len(arr) + 2)) == 0\n    assert len(arr.slice(len(arr) + 2, 100)) == 0\n\n    with pytest.raises(IndexError):\n        arr.slice(-1)\n\n    with pytest.raises(ValueError):\n        arr.slice(2, -1)\n\n    # Test slice notation\n    assert arr[2:].equals(arr.slice(2))\n    assert arr[2:5].equals(arr.slice(2, 3))\n    assert arr[-5:].equals(arr.slice(len(arr) - 5))\n\n    n = len(arr)\n    for start in range(-n * 2, n * 2):\n        for stop in range(-n * 2, n * 2):\n            res = arr[start:stop]\n            res.validate()\n            expected = arr.to_pylist()[start:stop]\n            assert res.to_pylist() == expected\n            assert res.to_numpy().tolist() == expected\n\n\ndef test_array_slice_negative_step():\n    # ARROW-2714\n    np_arr = np.arange(20)\n    arr = pa.array(np_arr)\n    chunked_arr = pa.chunked_array([arr])\n\n    cases = [\n        slice(None, None, -1),\n        slice(None, 6, -2),\n        slice(10, 6, -2),\n        slice(8, None, -2),\n        slice(2, 10, -2),\n        slice(10, 2, -2),\n        slice(None, None, 2),\n        slice(0, 10, 2),\n        slice(15, -25, -1),  # GH-38768\n        slice(-22, -22, -1),  # GH-40642\n    ]\n\n    for case in cases:\n        result = arr[case]\n        expected = pa.array(np_arr[case])\n        assert result.equals(expected)\n\n        result = pa.record_batch([arr], names=['f0'])[case]\n        expected = pa.record_batch([expected], names=['f0'])\n        assert result.equals(expected)\n\n        result = chunked_arr[case]\n        expected = pa.chunked_array([np_arr[case]])\n        assert result.equals(expected)\n\n\ndef test_array_diff():\n    # ARROW-6252\n    arr1 = pa.array(['foo'], type=pa.utf8())\n    arr2 = pa.array(['foo', 'bar', None], type=pa.utf8())\n    arr3 = pa.array([1, 2, 3])\n    arr4 = pa.array([[], [1], None], type=pa.list_(pa.int64()))\n\n    assert arr1.diff(arr1) == ''\n    assert arr1.diff(arr2) == '''\n@@ -1, +1 @@\n+\"bar\"\n+null\n'''\n    assert arr1.diff(arr3).strip() == '# Array types differed: string vs int64'\n    assert arr1.diff(arr3).strip() == '# Array types differed: string vs int64'\n    assert arr1.diff(arr4).strip() == ('# Array types differed: string vs '\n                                       'list<item: int64>')\n\n\ndef test_array_iter():\n    arr = pa.array(range(10))\n\n    for i, j in zip(range(10), arr):\n        assert i == j.as_py()\n\n    assert isinstance(arr, Iterable)\n\n\ndef test_struct_array_slice():\n    # ARROW-2311: slicing nested arrays needs special care\n    ty = pa.struct([pa.field('a', pa.int8()),\n                    pa.field('b', pa.float32())])\n    arr = pa.array([(1, 2.5), (3, 4.5), (5, 6.5)], type=ty)\n    assert arr[1:].to_pylist() == [{'a': 3, 'b': 4.5},\n                                   {'a': 5, 'b': 6.5}]\n\n\ndef test_array_factory_invalid_type():\n\n    class MyObject:\n        pass\n\n    arr = np.array([MyObject()])\n    with pytest.raises(ValueError):\n        pa.array(arr)\n\n\ndef test_array_ref_to_ndarray_base():\n    arr = np.array([1, 2, 3])\n\n    refcount = sys.getrefcount(arr)\n    arr2 = pa.array(arr)  # noqa\n    assert sys.getrefcount(arr) == (refcount + 1)\n\n\ndef test_array_eq():\n    # ARROW-2150 / ARROW-9445: we define the __eq__ behavior to be\n    # data equality (not element-wise equality)\n    arr1 = pa.array([1, 2, 3], type=pa.int32())\n    arr2 = pa.array([1, 2, 3], type=pa.int32())\n    arr3 = pa.array([1, 2, 3], type=pa.int64())\n\n    assert (arr1 == arr2) is True\n    assert (arr1 != arr2) is False\n    assert (arr1 == arr3) is False\n    assert (arr1 != arr3) is True\n\n    assert (arr1 == 1) is False\n    assert (arr1 == None) is False  # noqa: E711\n\n\ndef test_array_from_buffers():\n    values_buf = pa.py_buffer(np.int16([4, 5, 6, 7]))\n    nulls_buf = pa.py_buffer(np.uint8([0b00001101]))\n    arr = pa.Array.from_buffers(pa.int16(), 4, [nulls_buf, values_buf])\n    assert arr.type == pa.int16()\n    assert arr.to_pylist() == [4, None, 6, 7]\n\n    arr = pa.Array.from_buffers(pa.int16(), 4, [None, values_buf])\n    assert arr.type == pa.int16()\n    assert arr.to_pylist() == [4, 5, 6, 7]\n\n    arr = pa.Array.from_buffers(pa.int16(), 3, [nulls_buf, values_buf],\n                                offset=1)\n    assert arr.type == pa.int16()\n    assert arr.to_pylist() == [None, 6, 7]\n\n    with pytest.raises(TypeError):\n        pa.Array.from_buffers(pa.int16(), 3, ['', ''], offset=1)\n\n\ndef test_string_binary_from_buffers():\n    array = pa.array([\"a\", None, \"b\", \"c\"])\n\n    buffers = array.buffers()\n    copied = pa.StringArray.from_buffers(\n        len(array), buffers[1], buffers[2], buffers[0], array.null_count,\n        array.offset)\n    assert copied.to_pylist() == [\"a\", None, \"b\", \"c\"]\n\n    binary_copy = pa.Array.from_buffers(pa.binary(), len(array),\n                                        array.buffers(), array.null_count,\n                                        array.offset)\n    assert binary_copy.to_pylist() == [b\"a\", None, b\"b\", b\"c\"]\n\n    copied = pa.StringArray.from_buffers(\n        len(array), buffers[1], buffers[2], buffers[0])\n    assert copied.to_pylist() == [\"a\", None, \"b\", \"c\"]\n\n    sliced = array[1:]\n    buffers = sliced.buffers()\n    copied = pa.StringArray.from_buffers(\n        len(sliced), buffers[1], buffers[2], buffers[0], -1, sliced.offset)\n    assert copied.to_pylist() == [None, \"b\", \"c\"]\n    assert copied.null_count == 1\n\n    # Slice but exclude all null entries so that we don't need to pass\n    # the null bitmap.\n    sliced = array[2:]\n    buffers = sliced.buffers()\n    copied = pa.StringArray.from_buffers(\n        len(sliced), buffers[1], buffers[2], None, -1, sliced.offset)\n    assert copied.to_pylist() == [\"b\", \"c\"]\n    assert copied.null_count == 0\n\n\n@pytest.mark.parametrize('list_type_factory', [\n    pa.list_, pa.large_list, pa.list_view, pa.large_list_view])\ndef test_list_from_buffers(list_type_factory):\n    ty = list_type_factory(pa.int16())\n    array = pa.array([[0, 1, 2], None, [], [3, 4, 5]], type=ty)\n    assert array.type == ty\n\n    buffers = array.buffers()\n\n    with pytest.raises(ValueError):\n        # No children\n        pa.Array.from_buffers(ty, 4, buffers[:ty.num_buffers])\n\n    child = pa.Array.from_buffers(pa.int16(), 6, buffers[ty.num_buffers:])\n    copied = pa.Array.from_buffers(ty, 4, buffers[:ty.num_buffers], children=[child])\n    assert copied.equals(array)\n\n    with pytest.raises(ValueError):\n        # too many children\n        pa.Array.from_buffers(ty, 4, buffers[:ty.num_buffers],\n                              children=[child, child])\n\n\ndef test_struct_from_buffers():\n    ty = pa.struct([pa.field('a', pa.int16()), pa.field('b', pa.utf8())])\n    array = pa.array([{'a': 0, 'b': 'foo'}, None, {'a': 5, 'b': ''}],\n                     type=ty)\n    buffers = array.buffers()\n\n    with pytest.raises(ValueError):\n        # No children\n        pa.Array.from_buffers(ty, 3, [None, buffers[1]])\n\n    children = [pa.Array.from_buffers(pa.int16(), 3, buffers[1:3]),\n                pa.Array.from_buffers(pa.utf8(), 3, buffers[3:])]\n    copied = pa.Array.from_buffers(ty, 3, buffers[:1], children=children)\n    assert copied.equals(array)\n\n    with pytest.raises(ValueError):\n        # not enough many children\n        pa.Array.from_buffers(ty, 3, [buffers[0]],\n                              children=children[:1])\n\n\ndef test_struct_from_arrays():\n    a = pa.array([4, 5, 6], type=pa.int64())\n    b = pa.array([\"bar\", None, \"\"])\n    c = pa.array([[1, 2], None, [3, None]])\n    expected_list = [\n        {'a': 4, 'b': 'bar', 'c': [1, 2]},\n        {'a': 5, 'b': None, 'c': None},\n        {'a': 6, 'b': '', 'c': [3, None]},\n    ]\n\n    # From field names\n    arr = pa.StructArray.from_arrays([a, b, c], [\"a\", \"b\", \"c\"])\n    assert arr.type == pa.struct(\n        [(\"a\", a.type), (\"b\", b.type), (\"c\", c.type)])\n    assert arr.to_pylist() == expected_list\n\n    with pytest.raises(ValueError):\n        pa.StructArray.from_arrays([a, b, c], [\"a\", \"b\"])\n\n    arr = pa.StructArray.from_arrays([], [])\n    assert arr.type == pa.struct([])\n    assert arr.to_pylist() == []\n\n    # From fields\n    fa = pa.field(\"a\", a.type, nullable=False)\n    fb = pa.field(\"b\", b.type)\n    fc = pa.field(\"c\", c.type)\n    arr = pa.StructArray.from_arrays([a, b, c], fields=[fa, fb, fc])\n    assert arr.type == pa.struct([fa, fb, fc])\n    assert not arr.type[0].nullable\n    assert arr.to_pylist() == expected_list\n\n    with pytest.raises(ValueError):\n        pa.StructArray.from_arrays([a, b, c], fields=[fa, fb])\n\n    arr = pa.StructArray.from_arrays([], fields=[])\n    assert arr.type == pa.struct([])\n    assert arr.to_pylist() == []\n\n    # Inconsistent fields\n    fa2 = pa.field(\"a\", pa.int32())\n    with pytest.raises(ValueError, match=\"int64 vs int32\"):\n        pa.StructArray.from_arrays([a, b, c], fields=[fa2, fb, fc])\n\n    arrays = [a, b, c]\n    fields = [fa, fb, fc]\n    # With mask\n    mask = pa.array([True, False, False])\n    arr = pa.StructArray.from_arrays(arrays, fields=fields, mask=mask)\n    assert arr.to_pylist() == [None] + expected_list[1:]\n\n    arr = pa.StructArray.from_arrays(arrays, names=['a', 'b', 'c'], mask=mask)\n    assert arr.to_pylist() == [None] + expected_list[1:]\n\n    # Bad masks\n    with pytest.raises(TypeError, match='Mask must be'):\n        pa.StructArray.from_arrays(arrays, fields, mask=[True, False, False])\n\n    with pytest.raises(ValueError, match='not contain nulls'):\n        pa.StructArray.from_arrays(\n            arrays, fields, mask=pa.array([True, False, None]))\n\n    with pytest.raises(TypeError, match='Mask must be'):\n        pa.StructArray.from_arrays(\n            arrays, fields, mask=pa.chunked_array([mask]))\n\n    # Non-empty array with no fields https://github.com/apache/arrow/issues/15109\n    arr = pa.StructArray.from_arrays([], [], mask=mask)\n    assert arr.is_null() == mask\n    assert arr.to_pylist() == [None, {}, {}]\n\n\ndef test_struct_array_from_chunked():\n    # ARROW-11780\n    # Check that we don't segfault when trying to build\n    # a StructArray from a chunked array.\n    chunked_arr = pa.chunked_array([[1, 2, 3], [4, 5, 6]])\n\n    with pytest.raises(TypeError, match=\"Expected Array\"):\n        pa.StructArray.from_arrays([chunked_arr], [\"foo\"])\n\n\n@pytest.mark.parametrize(\"offset\", (0, 1))\ndef test_dictionary_from_buffers(offset):\n    a = pa.array([\"one\", \"two\", \"three\", \"two\", \"one\"]).dictionary_encode()\n    b = pa.DictionaryArray.from_buffers(a.type, len(a)-offset,\n                                        a.indices.buffers(), a.dictionary,\n                                        offset=offset)\n    assert a[offset:] == b\n\n\ndef test_dictionary_from_numpy():\n    indices = np.repeat([0, 1, 2], 2)\n    dictionary = np.array(['foo', 'bar', 'baz'], dtype=object)\n    mask = np.array([False, False, True, False, False, False])\n\n    d1 = pa.DictionaryArray.from_arrays(indices, dictionary)\n    d2 = pa.DictionaryArray.from_arrays(indices, dictionary, mask=mask)\n\n    assert d1.indices.to_pylist() == indices.tolist()\n    assert d1.indices.to_pylist() == indices.tolist()\n    assert d1.dictionary.to_pylist() == dictionary.tolist()\n    assert d2.dictionary.to_pylist() == dictionary.tolist()\n\n    for i in range(len(indices)):\n        assert d1[i].as_py() == dictionary[indices[i]]\n\n        if mask[i]:\n            assert d2[i].as_py() is None\n        else:\n            assert d2[i].as_py() == dictionary[indices[i]]\n\n\ndef test_dictionary_to_numpy():\n    expected = pa.array(\n        [\"foo\", \"bar\", None, \"foo\"]\n    ).to_numpy(zero_copy_only=False)\n    a = pa.DictionaryArray.from_arrays(\n        pa.array([0, 1, None, 0]),\n        pa.array(['foo', 'bar'])\n    )\n    np.testing.assert_array_equal(a.to_numpy(zero_copy_only=False),\n                                  expected)\n\n    with pytest.raises(pa.ArrowInvalid):\n        # If this would be changed to no longer raise in the future,\n        # ensure to test the actual result because, currently, to_numpy takes\n        # for granted that when zero_copy_only=True there will be no nulls\n        # (it's the decoding of the DictionaryArray that handles the nulls and\n        # this is only activated with zero_copy_only=False)\n        a.to_numpy(zero_copy_only=True)\n\n    anonulls = pa.DictionaryArray.from_arrays(\n        pa.array([0, 1, 1, 0]),\n        pa.array(['foo', 'bar'])\n    )\n    expected = pa.array(\n        [\"foo\", \"bar\", \"bar\", \"foo\"]\n    ).to_numpy(zero_copy_only=False)\n    np.testing.assert_array_equal(anonulls.to_numpy(zero_copy_only=False),\n                                  expected)\n\n    with pytest.raises(pa.ArrowInvalid):\n        anonulls.to_numpy(zero_copy_only=True)\n\n    afloat = pa.DictionaryArray.from_arrays(\n        pa.array([0, 1, 1, 0]),\n        pa.array([13.7, 11.0])\n    )\n    expected = pa.array([13.7, 11.0, 11.0, 13.7]).to_numpy()\n    np.testing.assert_array_equal(afloat.to_numpy(zero_copy_only=True),\n                                  expected)\n    np.testing.assert_array_equal(afloat.to_numpy(zero_copy_only=False),\n                                  expected)\n\n    afloat2 = pa.DictionaryArray.from_arrays(\n        pa.array([0, 1, None, 0]),\n        pa.array([13.7, 11.0])\n    )\n    expected = pa.array(\n        [13.7, 11.0, None, 13.7]\n    ).to_numpy(zero_copy_only=False)\n    np.testing.assert_allclose(\n        afloat2.to_numpy(zero_copy_only=False),\n        expected,\n        equal_nan=True\n    )\n\n    # Testing for integers can reveal problems related to dealing\n    # with None values, as a numpy array of int dtype\n    # can't contain NaN nor None.\n    aints = pa.DictionaryArray.from_arrays(\n        pa.array([0, 1, None, 0]),\n        pa.array([7, 11])\n    )\n    expected = pa.array([7, 11, None, 7]).to_numpy(zero_copy_only=False)\n    np.testing.assert_allclose(\n        aints.to_numpy(zero_copy_only=False),\n        expected,\n        equal_nan=True\n    )\n\n\ndef test_dictionary_from_boxed_arrays():\n    indices = np.repeat([0, 1, 2], 2)\n    dictionary = np.array(['foo', 'bar', 'baz'], dtype=object)\n\n    iarr = pa.array(indices)\n    darr = pa.array(dictionary)\n\n    d1 = pa.DictionaryArray.from_arrays(iarr, darr)\n\n    assert d1.indices.to_pylist() == indices.tolist()\n    assert d1.dictionary.to_pylist() == dictionary.tolist()\n\n    for i in range(len(indices)):\n        assert d1[i].as_py() == dictionary[indices[i]]\n\n\ndef test_dictionary_from_arrays_boundscheck():\n    indices1 = pa.array([0, 1, 2, 0, 1, 2])\n    indices2 = pa.array([0, -1, 2])\n    indices3 = pa.array([0, 1, 2, 3])\n\n    dictionary = pa.array(['foo', 'bar', 'baz'])\n\n    # Works fine\n    pa.DictionaryArray.from_arrays(indices1, dictionary)\n\n    with pytest.raises(pa.ArrowException):\n        pa.DictionaryArray.from_arrays(indices2, dictionary)\n\n    with pytest.raises(pa.ArrowException):\n        pa.DictionaryArray.from_arrays(indices3, dictionary)\n\n    # If we are confident that the indices are \"safe\" we can pass safe=False to\n    # disable the boundschecking\n    pa.DictionaryArray.from_arrays(indices2, dictionary, safe=False)\n\n\ndef test_dictionary_indices():\n    # https://issues.apache.org/jira/browse/ARROW-6882\n    indices = pa.array([0, 1, 2, 0, 1, 2])\n    dictionary = pa.array(['foo', 'bar', 'baz'])\n    arr = pa.DictionaryArray.from_arrays(indices, dictionary)\n    arr.indices.validate(full=True)\n\n\n@pytest.mark.parametrize(('list_array_type', 'list_type_factory'),\n                         [(pa.ListArray, pa.list_),\n                          (pa.LargeListArray, pa.large_list)])\ndef test_list_from_arrays(list_array_type, list_type_factory):\n    offsets_arr = np.array([0, 2, 5, 8], dtype='i4')\n    offsets = pa.array(offsets_arr, type='int32')\n    pyvalues = [b'a', b'b', b'c', b'd', b'e', b'f', b'g', b'h']\n    values = pa.array(pyvalues, type='binary')\n\n    result = list_array_type.from_arrays(offsets, values)\n    expected = pa.array([pyvalues[:2], pyvalues[2:5], pyvalues[5:8]],\n                        type=list_type_factory(pa.binary()))\n\n    assert result.equals(expected)\n\n    # With specified type\n    typ = list_type_factory(pa.field(\"name\", pa.binary()))\n    result = list_array_type.from_arrays(offsets, values, typ)\n    assert result.type == typ\n    assert result.type.value_field.name == \"name\"\n\n    # With nulls\n    offsets = [0, None, 2, 6]\n    values = [b'a', b'b', b'c', b'd', b'e', b'f']\n\n    result = list_array_type.from_arrays(offsets, values)\n    expected = pa.array([values[:2], None, values[2:]],\n                        type=list_type_factory(pa.binary()))\n\n    assert result.equals(expected)\n\n    # Another edge case\n    offsets2 = [0, 2, None, 6]\n    result = list_array_type.from_arrays(offsets2, values)\n    expected = pa.array([values[:2], values[2:], None],\n                        type=list_type_factory(pa.binary()))\n    assert result.equals(expected)\n\n    # raise on invalid array\n    offsets = [1, 3, 10]\n    values = np.arange(5)\n    with pytest.raises(ValueError):\n        list_array_type.from_arrays(offsets, values)\n\n    # Non-monotonic offsets\n    offsets = [0, 3, 2, 6]\n    values = list(range(6))\n    result = list_array_type.from_arrays(offsets, values)\n    with pytest.raises(ValueError):\n        result.validate(full=True)\n\n    # mismatching type\n    typ = list_type_factory(pa.binary())\n    with pytest.raises(TypeError):\n        list_array_type.from_arrays(offsets, values, type=typ)\n\n\n@pytest.mark.parametrize(('list_array_type', 'list_type_factory'), (\n    (pa.ListArray, pa.list_),\n    (pa.LargeListArray, pa.large_list)\n))\n@pytest.mark.parametrize(\"arr\", (\n    [None, [0]],\n    [None, [0, None], [0]],\n    [[0], [1]],\n))\ndef test_list_array_types_from_arrays(\n    list_array_type, list_type_factory, arr\n):\n    arr = pa.array(arr, list_type_factory(pa.int8()))\n    reconstructed_arr = list_array_type.from_arrays(\n        arr.offsets, arr.values, mask=arr.is_null())\n    assert arr == reconstructed_arr\n\n\n@pytest.mark.parametrize(('list_array_type', 'list_type_factory'), (\n    (pa.ListArray, pa.list_),\n    (pa.LargeListArray, pa.large_list)\n))\ndef test_list_array_types_from_arrays_fail(list_array_type, list_type_factory):\n    # Fail when manual offsets include nulls and mask passed\n    # ListArray.offsets doesn't report nulls.\n\n    # This test case arr.offsets == [0, 1, 1, 3, 4]\n    arr = pa.array([[0], None, [0, None], [0]], list_type_factory(pa.int8()))\n    offsets = pa.array([0, None, 1, 3, 4])\n\n    # Using array's offset has no nulls; gives empty lists on top level\n    reconstructed_arr = list_array_type.from_arrays(arr.offsets, arr.values)\n    assert reconstructed_arr.to_pylist() == [[0], [], [0, None], [0]]\n\n    # Manually specifying offsets (with nulls) is same as mask at top level\n    reconstructed_arr = list_array_type.from_arrays(offsets, arr.values)\n    assert arr == reconstructed_arr\n    reconstructed_arr = list_array_type.from_arrays(arr.offsets,\n                                                    arr.values,\n                                                    mask=arr.is_null())\n    assert arr == reconstructed_arr\n\n    # But using both is ambiguous, in this case `offsets` has nulls\n    with pytest.raises(ValueError, match=\"Ambiguous to specify both \"):\n        list_array_type.from_arrays(offsets, arr.values, mask=arr.is_null())\n\n    # Not supported to reconstruct from a slice.\n    arr_slice = arr[1:]\n    msg = \"Null bitmap with offsets slice not supported.\"\n    with pytest.raises(NotImplementedError, match=msg):\n        list_array_type.from_arrays(\n            arr_slice.offsets, arr_slice.values, mask=arr_slice.is_null())\n\n\ndef test_map_cast():\n    # GH-38553\n    t = pa.map_(pa.int64(), pa.int64())\n    arr = pa.array([{1: 2}], type=t)\n    result = arr.cast(pa.map_(pa.int32(), pa.int64()))\n\n    t_expected = pa.map_(pa.int32(), pa.int64())\n    expected = pa.array([{1: 2}], type=t_expected)\n\n    assert result.equals(expected)\n\n\ndef test_map_labelled():\n    #  ARROW-13735\n    t = pa.map_(pa.field(\"name\", \"string\", nullable=False), \"int64\")\n    arr = pa.array([[('a', 1), ('b', 2)], [('c', 3)]], type=t)\n    assert arr.type.key_field == pa.field(\"name\", pa.utf8(), nullable=False)\n    assert arr.type.item_field == pa.field(\"value\", pa.int64())\n    assert len(arr) == 2\n\n\ndef test_map_from_dict():\n    # ARROW-17832\n    tup_arr = pa.array([[('a', 1), ('b', 2)], [('c', 3)]],\n                       pa.map_(pa.string(), pa.int64()))\n    dict_arr = pa.array([{'a': 1, 'b': 2}, {'c': 3}],\n                        pa.map_(pa.string(), pa.int64()))\n\n    assert tup_arr.equals(dict_arr)\n\n\ndef test_map_from_arrays():\n    offsets_arr = np.array([0, 2, 5, 8], dtype='i4')\n    offsets = pa.array(offsets_arr, type='int32')\n    pykeys = [b'a', b'b', b'c', b'd', b'e', b'f', b'g', b'h']\n    pyitems = list(range(len(pykeys)))\n    pypairs = list(zip(pykeys, pyitems))\n    pyentries = [pypairs[:2], pypairs[2:5], pypairs[5:8]]\n    keys = pa.array(pykeys, type='binary')\n    items = pa.array(pyitems, type='i4')\n\n    result = pa.MapArray.from_arrays(offsets, keys, items)\n    expected = pa.array(pyentries, type=pa.map_(pa.binary(), pa.int32()))\n\n    assert result.equals(expected)\n\n    # With nulls\n    offsets = [0, None, 2, 6]\n    pykeys = [b'a', b'b', b'c', b'd', b'e', b'f']\n    pyitems = [1, 2, 3, None, 4, 5]\n    pypairs = list(zip(pykeys, pyitems))\n    pyentries = [pypairs[:2], None, pypairs[2:]]\n    keys = pa.array(pykeys, type='binary')\n    items = pa.array(pyitems, type='i4')\n\n    result = pa.MapArray.from_arrays(offsets, keys, items)\n    expected = pa.array(pyentries, type=pa.map_(pa.binary(), pa.int32()))\n\n    assert result.equals(expected)\n\n    # pass in the type explicitly\n    result = pa.MapArray.from_arrays(offsets, keys, items, pa.map_(\n        keys.type,\n        items.type\n    ))\n    assert result.equals(expected)\n\n    # pass in invalid types\n    with pytest.raises(pa.ArrowTypeError, match='Expected map type, got string'):\n        pa.MapArray.from_arrays(offsets, keys, items, pa.string())\n\n    with pytest.raises(pa.ArrowTypeError, match='Mismatching map items type'):\n        pa.MapArray.from_arrays(offsets, keys, items, pa.map_(\n            keys.type,\n            # Larger than the original i4\n            pa.int64()\n        ))\n\n    # pass in null bitmap with type\n    result = pa.MapArray.from_arrays([0, 2, 2, 6], keys, items, pa.map_(\n        keys.type,\n        items.type),\n        mask=pa.array([False, True, False], type=pa.bool_())\n    )\n    assert result.null_count == 1\n    assert result.equals(expected)\n\n    # pass in null bitmap without the type\n    result = pa.MapArray.from_arrays([0, 2, 2, 6], keys, items,\n                                     mask=pa.array([False, True, False],\n                                                   type=pa.bool_())\n                                     )\n    assert result.equals(expected)\n\n    # pass in null bitmap with two nulls\n    offsets = [0, None, None, 6]\n    pyentries = [None, None, pypairs[2:]]\n\n    result = pa.MapArray.from_arrays([0, 2, 2, 6], keys, items, pa.map_(\n        keys.type,\n        items.type),\n        mask=pa.array([True, True, False], type=pa.bool_())\n    )\n    expected = pa.array(pyentries, type=pa.map_(pa.binary(), pa.int32()))\n    assert result.null_count == 2\n    assert result.equals(expected)\n\n    # error if null bitmap and offsets with nulls passed\n    msg1 = 'Ambiguous to specify both validity map and offsets with nulls'\n    with pytest.raises(pa.ArrowInvalid, match=msg1):\n        pa.MapArray.from_arrays(offsets, keys, items, pa.map_(\n            keys.type,\n            items.type),\n            mask=pa.array([False, True, False], type=pa.bool_())\n        )\n\n    # error if null bitmap passed to sliced offset\n    msg2 = 'Null bitmap with offsets slice not supported.'\n    offsets = pa.array([0, 2, 2, 6], pa.int32())\n    with pytest.raises(pa.ArrowNotImplementedError, match=msg2):\n        pa.MapArray.from_arrays(offsets.slice(2), keys, items, pa.map_(\n            keys.type,\n            items.type),\n            mask=pa.array([False, True, False], type=pa.bool_())\n        )\n\n    # check invalid usage\n    offsets = [0, 1, 3, 5]\n    keys = np.arange(5)\n    items = np.arange(5)\n    _ = pa.MapArray.from_arrays(offsets, keys, items)\n\n    # raise on invalid offsets\n    with pytest.raises(ValueError):\n        pa.MapArray.from_arrays(offsets + [6], keys, items)\n\n    # raise on length of keys != items\n    with pytest.raises(ValueError):\n        pa.MapArray.from_arrays(offsets, keys, np.concatenate([items, items]))\n\n    # raise on keys with null\n    keys_with_null = list(keys)[:-1] + [None]\n    assert len(keys_with_null) == len(items)\n    with pytest.raises(ValueError):\n        pa.MapArray.from_arrays(offsets, keys_with_null, items)\n\n    # Check if offset in offsets > 0\n    offsets = pa.array(offsets, pa.int32())\n    result = pa.MapArray.from_arrays(offsets.slice(1), keys, items)\n    expected = pa.MapArray.from_arrays([1, 3, 5], keys, items)\n\n    assert result.equals(expected)\n    assert result.offset == 1\n    assert expected.offset == 0\n\n    offsets = pa.array([0, 0, 0, 0, 0, 0], pa.int32())\n    result = pa.MapArray.from_arrays(\n        offsets.slice(1),\n        pa.array([], pa.string()),\n        pa.array([], pa.string()),\n    )\n    expected = pa.MapArray.from_arrays(\n        [0, 0, 0, 0, 0],\n        pa.array([], pa.string()),\n        pa.array([], pa.string()),\n    )\n    assert result.equals(expected)\n    assert result.offset == 1\n    assert expected.offset == 0\n\n\ndef test_fixed_size_list_from_arrays():\n    values = pa.array(range(12), pa.int64())\n    result = pa.FixedSizeListArray.from_arrays(values, 4)\n    assert result.to_pylist() == [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]\n    assert result.type.equals(pa.list_(pa.int64(), 4))\n\n    typ = pa.list_(pa.field(\"name\", pa.int64()), 4)\n    result = pa.FixedSizeListArray.from_arrays(values, type=typ)\n    assert result.to_pylist() == [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]\n    assert result.type.equals(typ)\n    assert result.type.value_field.name == \"name\"\n\n    result = pa.FixedSizeListArray.from_arrays(values,\n                                               type=typ,\n                                               mask=pa.array([False, True, False]))\n    assert result.to_pylist() == [[0, 1, 2, 3], None, [8, 9, 10, 11]]\n\n    result = pa.FixedSizeListArray.from_arrays(values,\n                                               list_size=4,\n                                               mask=pa.array([False, True, False]))\n    assert result.to_pylist() == [[0, 1, 2, 3], None, [8, 9, 10, 11]]\n\n    # raise on invalid values / list_size\n    with pytest.raises(ValueError):\n        pa.FixedSizeListArray.from_arrays(values, -4)\n\n    with pytest.raises(ValueError):\n        # array with list size 0 cannot be constructed with from_arrays\n        pa.FixedSizeListArray.from_arrays(pa.array([], pa.int64()), 0)\n\n    with pytest.raises(ValueError):\n        # length of values not multiple of 5\n        pa.FixedSizeListArray.from_arrays(values, 5)\n\n    typ = pa.list_(pa.int64(), 5)\n    with pytest.raises(ValueError):\n        pa.FixedSizeListArray.from_arrays(values, type=typ)\n\n    # raise on mismatching values type\n    typ = pa.list_(pa.float64(), 4)\n    with pytest.raises(TypeError):\n        pa.FixedSizeListArray.from_arrays(values, type=typ)\n\n    # raise on specifying none or both of list_size / type\n    with pytest.raises(ValueError):\n        pa.FixedSizeListArray.from_arrays(values)\n\n    typ = pa.list_(pa.int64(), 4)\n    with pytest.raises(ValueError):\n        pa.FixedSizeListArray.from_arrays(values, list_size=4, type=typ)\n\n\ndef test_variable_list_from_arrays():\n    values = pa.array([1, 2, 3, 4], pa.int64())\n    offsets = pa.array([0, 2, 4])\n    result = pa.ListArray.from_arrays(offsets, values)\n    assert result.to_pylist() == [[1, 2], [3, 4]]\n    assert result.type.equals(pa.list_(pa.int64()))\n\n    offsets = pa.array([0, None, 2, 4])\n    result = pa.ListArray.from_arrays(offsets, values)\n    assert result.to_pylist() == [[1, 2], None, [3, 4]]\n\n    # raise if offset out of bounds\n    with pytest.raises(ValueError):\n        pa.ListArray.from_arrays(pa.array([-1, 2, 4]), values)\n\n    with pytest.raises(ValueError):\n        pa.ListArray.from_arrays(pa.array([0, 2, 5]), values)\n\n\ndef test_union_from_dense():\n    binary = pa.array([b'a', b'b', b'c', b'd'], type='binary')\n    int64 = pa.array([1, 2, 3], type='int64')\n    types = pa.array([0, 1, 0, 0, 1, 1, 0], type='int8')\n    logical_types = pa.array([11, 13, 11, 11, 13, 13, 11], type='int8')\n    value_offsets = pa.array([0, 0, 1, 2, 1, 2, 3], type='int32')\n    py_value = [b'a', 1, b'b', b'c', 2, 3, b'd']\n\n    def check_result(result, expected_field_names, expected_type_codes,\n                     expected_type_code_values):\n        result.validate(full=True)\n        actual_field_names = [result.type[i].name\n                              for i in range(result.type.num_fields)]\n        assert actual_field_names == expected_field_names\n        assert result.type.mode == \"dense\"\n        assert result.type.type_codes == expected_type_codes\n        assert result.to_pylist() == py_value\n        assert expected_type_code_values.equals(result.type_codes)\n        assert value_offsets.equals(result.offsets)\n        assert result.field(0).equals(binary)\n        assert result.field(1).equals(int64)\n        with pytest.raises(KeyError):\n            result.field(-1)\n        with pytest.raises(KeyError):\n            result.field(2)\n\n    # without field names and type codes\n    check_result(pa.UnionArray.from_dense(types, value_offsets,\n                                          [binary, int64]),\n                 expected_field_names=['0', '1'],\n                 expected_type_codes=[0, 1],\n                 expected_type_code_values=types)\n\n    # with field names\n    check_result(pa.UnionArray.from_dense(types, value_offsets,\n                                          [binary, int64],\n                                          ['bin', 'int']),\n                 expected_field_names=['bin', 'int'],\n                 expected_type_codes=[0, 1],\n                 expected_type_code_values=types)\n\n    # with type codes\n    check_result(pa.UnionArray.from_dense(logical_types, value_offsets,\n                                          [binary, int64],\n                                          type_codes=[11, 13]),\n                 expected_field_names=['0', '1'],\n                 expected_type_codes=[11, 13],\n                 expected_type_code_values=logical_types)\n\n    # with field names and type codes\n    check_result(pa.UnionArray.from_dense(logical_types, value_offsets,\n                                          [binary, int64],\n                                          ['bin', 'int'], [11, 13]),\n                 expected_field_names=['bin', 'int'],\n                 expected_type_codes=[11, 13],\n                 expected_type_code_values=logical_types)\n\n    # Bad type ids\n    arr = pa.UnionArray.from_dense(logical_types, value_offsets,\n                                   [binary, int64])\n    with pytest.raises(pa.ArrowInvalid):\n        arr.validate(full=True)\n    arr = pa.UnionArray.from_dense(types, value_offsets, [binary, int64],\n                                   type_codes=[11, 13])\n    with pytest.raises(pa.ArrowInvalid):\n        arr.validate(full=True)\n\n    # Offset larger than child size\n    bad_offsets = pa.array([0, 0, 1, 2, 1, 2, 4], type='int32')\n    arr = pa.UnionArray.from_dense(types, bad_offsets, [binary, int64])\n    with pytest.raises(pa.ArrowInvalid):\n        arr.validate(full=True)\n\n\ndef test_union_from_sparse():\n    binary = pa.array([b'a', b' ', b'b', b'c', b' ', b' ', b'd'],\n                      type='binary')\n    int64 = pa.array([0, 1, 0, 0, 2, 3, 0], type='int64')\n    types = pa.array([0, 1, 0, 0, 1, 1, 0], type='int8')\n    logical_types = pa.array([11, 13, 11, 11, 13, 13, 11], type='int8')\n    py_value = [b'a', 1, b'b', b'c', 2, 3, b'd']\n\n    def check_result(result, expected_field_names, expected_type_codes,\n                     expected_type_code_values):\n        result.validate(full=True)\n        assert result.to_pylist() == py_value\n        actual_field_names = [result.type[i].name\n                              for i in range(result.type.num_fields)]\n        assert actual_field_names == expected_field_names\n        assert result.type.mode == \"sparse\"\n        assert result.type.type_codes == expected_type_codes\n        assert expected_type_code_values.equals(result.type_codes)\n        assert result.field(0).equals(binary)\n        assert result.field(1).equals(int64)\n        with pytest.raises(pa.ArrowTypeError):\n            result.offsets\n        with pytest.raises(KeyError):\n            result.field(-1)\n        with pytest.raises(KeyError):\n            result.field(2)\n\n    # without field names and type codes\n    check_result(pa.UnionArray.from_sparse(types, [binary, int64]),\n                 expected_field_names=['0', '1'],\n                 expected_type_codes=[0, 1],\n                 expected_type_code_values=types)\n\n    # with field names\n    check_result(pa.UnionArray.from_sparse(types, [binary, int64],\n                                           ['bin', 'int']),\n                 expected_field_names=['bin', 'int'],\n                 expected_type_codes=[0, 1],\n                 expected_type_code_values=types)\n\n    # with type codes\n    check_result(pa.UnionArray.from_sparse(logical_types, [binary, int64],\n                                           type_codes=[11, 13]),\n                 expected_field_names=['0', '1'],\n                 expected_type_codes=[11, 13],\n                 expected_type_code_values=logical_types)\n\n    # with field names and type codes\n    check_result(pa.UnionArray.from_sparse(logical_types, [binary, int64],\n                                           ['bin', 'int'],\n                                           [11, 13]),\n                 expected_field_names=['bin', 'int'],\n                 expected_type_codes=[11, 13],\n                 expected_type_code_values=logical_types)\n\n    # Bad type ids\n    arr = pa.UnionArray.from_sparse(logical_types, [binary, int64])\n    with pytest.raises(pa.ArrowInvalid):\n        arr.validate(full=True)\n    arr = pa.UnionArray.from_sparse(types, [binary, int64],\n                                    type_codes=[11, 13])\n    with pytest.raises(pa.ArrowInvalid):\n        arr.validate(full=True)\n\n    # Invalid child length\n    with pytest.raises(pa.ArrowInvalid):\n        arr = pa.UnionArray.from_sparse(logical_types, [binary, int64[1:]])\n\n\ndef test_union_array_to_pylist_with_nulls():\n    # ARROW-9556\n    arr = pa.UnionArray.from_sparse(\n        pa.array([0, 1, 0, 0, 1], type=pa.int8()),\n        [\n            pa.array([0.0, 1.1, None, 3.3, 4.4]),\n            pa.array([True, None, False, True, False]),\n        ]\n    )\n    assert arr.to_pylist() == [0.0, None, None, 3.3, False]\n\n    arr = pa.UnionArray.from_dense(\n        pa.array([0, 1, 0, 0, 0, 1, 1], type=pa.int8()),\n        pa.array([0, 0, 1, 2, 3, 1, 2], type=pa.int32()),\n        [\n            pa.array([0.0, 1.1, None, 3.3]),\n            pa.array([True, None, False])\n        ]\n    )\n    assert arr.to_pylist() == [0.0, True, 1.1, None, 3.3, None, False]\n\n\ndef test_union_array_slice():\n    # ARROW-2314\n    arr = pa.UnionArray.from_sparse(pa.array([0, 0, 1, 1], type=pa.int8()),\n                                    [pa.array([\"a\", \"b\", \"c\", \"d\"]),\n                                     pa.array([1, 2, 3, 4])])\n    assert arr[1:].to_pylist() == [\"b\", 3, 4]\n\n    binary = pa.array([b'a', b'b', b'c', b'd'], type='binary')\n    int64 = pa.array([1, 2, 3], type='int64')\n    types = pa.array([0, 1, 0, 0, 1, 1, 0], type='int8')\n    value_offsets = pa.array([0, 0, 2, 1, 1, 2, 3], type='int32')\n\n    arr = pa.UnionArray.from_dense(types, value_offsets, [binary, int64])\n    lst = arr.to_pylist()\n    for i in range(len(arr)):\n        for j in range(i, len(arr)):\n            assert arr[i:j].to_pylist() == lst[i:j]\n\n\ndef _check_cast_case(case, *, safe=True, check_array_construction=True):\n    in_data, in_type, out_data, out_type = case\n    if isinstance(out_data, pa.Array):\n        assert out_data.type == out_type\n        expected = out_data\n    else:\n        expected = pa.array(out_data, type=out_type)\n\n    # check casting an already created array\n    if isinstance(in_data, pa.Array):\n        assert in_data.type == in_type\n        in_arr = in_data\n    else:\n        in_arr = pa.array(in_data, type=in_type)\n    casted = in_arr.cast(out_type, safe=safe)\n    casted.validate(full=True)\n    assert casted.equals(expected)\n\n    # constructing an array with out type which optionally involves casting\n    # for more see ARROW-1949\n    if check_array_construction:\n        in_arr = pa.array(in_data, type=out_type, safe=safe)\n        assert in_arr.equals(expected)\n\n\ndef test_cast_integers_safe():\n    safe_cases = [\n        (np.array([0, 1, 2, 3], dtype='i1'), 'int8',\n         np.array([0, 1, 2, 3], dtype='i4'), pa.int32()),\n        (np.array([0, 1, 2, 3], dtype='i1'), 'int8',\n         np.array([0, 1, 2, 3], dtype='u4'), pa.uint16()),\n        (np.array([0, 1, 2, 3], dtype='i1'), 'int8',\n         np.array([0, 1, 2, 3], dtype='u1'), pa.uint8()),\n        (np.array([0, 1, 2, 3], dtype='i1'), 'int8',\n         np.array([0, 1, 2, 3], dtype='f8'), pa.float64())\n    ]\n\n    for case in safe_cases:\n        _check_cast_case(case)\n\n    unsafe_cases = [\n        (np.array([50000], dtype='i4'), 'int32', 'int16'),\n        (np.array([70000], dtype='i4'), 'int32', 'uint16'),\n        (np.array([-1], dtype='i4'), 'int32', 'uint16'),\n        (np.array([50000], dtype='u2'), 'uint16', 'int16')\n    ]\n    for in_data, in_type, out_type in unsafe_cases:\n        in_arr = pa.array(in_data, type=in_type)\n\n        with pytest.raises(pa.ArrowInvalid):\n            in_arr.cast(out_type)\n\n\ndef test_cast_none():\n    # ARROW-3735: Ensure that calling cast(None) doesn't segfault.\n    arr = pa.array([1, 2, 3])\n\n    with pytest.raises(TypeError):\n        arr.cast(None)\n\n\ndef test_cast_list_to_primitive():\n    # ARROW-8070: cast segfaults on unsupported cast from list<binary> to utf8\n    arr = pa.array([[1, 2], [3, 4]])\n    with pytest.raises(NotImplementedError):\n        arr.cast(pa.int8())\n\n    arr = pa.array([[b\"a\", b\"b\"], [b\"c\"]], pa.list_(pa.binary()))\n    with pytest.raises(NotImplementedError):\n        arr.cast(pa.binary())\n\n\ndef test_slice_chunked_array_zero_chunks():\n    # ARROW-8911\n    arr = pa.chunked_array([], type='int8')\n    assert arr.num_chunks == 0\n\n    result = arr[:]\n    assert result.equals(arr)\n\n    # Do not crash\n    arr[:5]\n\n\ndef test_cast_chunked_array():\n    arrays = [pa.array([1, 2, 3]), pa.array([4, 5, 6])]\n    carr = pa.chunked_array(arrays)\n\n    target = pa.float64()\n    casted = carr.cast(target)\n    expected = pa.chunked_array([x.cast(target) for x in arrays])\n    assert casted.equals(expected)\n\n\ndef test_cast_chunked_array_empty():\n    # ARROW-8142\n    for typ1, typ2 in [(pa.dictionary(pa.int8(), pa.string()), pa.string()),\n                       (pa.int64(), pa.int32())]:\n\n        arr = pa.chunked_array([], type=typ1)\n        result = arr.cast(typ2)\n        expected = pa.chunked_array([], type=typ2)\n        assert result.equals(expected)\n\n\ndef test_chunked_array_data_warns():\n    with pytest.warns(FutureWarning):\n        res = pa.chunked_array([[]]).data\n    assert isinstance(res, pa.ChunkedArray)\n\n\ndef test_cast_integers_unsafe():\n    # We let NumPy do the unsafe casting.\n    # Note that NEP50 in the NumPy spec no longer allows\n    # the np.array() constructor to pass the dtype directly\n    # if it results in an unsafe cast.\n    unsafe_cases = [\n        (np.array([50000], dtype='i4'), 'int32',\n         np.array([50000]).astype(dtype='i2'), pa.int16()),\n        (np.array([70000], dtype='i4'), 'int32',\n         np.array([70000]).astype(dtype='u2'), pa.uint16()),\n        (np.array([-1], dtype='i4'), 'int32',\n         np.array([-1]).astype(dtype='u2'), pa.uint16()),\n        (np.array([50000], dtype='u2'), pa.uint16(),\n         np.array([50000]).astype(dtype='i2'), pa.int16())\n    ]\n\n    for case in unsafe_cases:\n        _check_cast_case(case, safe=False)\n\n\ndef test_floating_point_truncate_safe():\n    safe_cases = [\n        (np.array([1.0, 2.0, 3.0], dtype='float32'), 'float32',\n         np.array([1, 2, 3], dtype='i4'), pa.int32()),\n        (np.array([1.0, 2.0, 3.0], dtype='float64'), 'float64',\n         np.array([1, 2, 3], dtype='i4'), pa.int32()),\n        (np.array([-10.0, 20.0, -30.0], dtype='float64'), 'float64',\n         np.array([-10, 20, -30], dtype='i4'), pa.int32()),\n    ]\n    for case in safe_cases:\n        _check_cast_case(case, safe=True)\n\n\ndef test_floating_point_truncate_unsafe():\n    unsafe_cases = [\n        (np.array([1.1, 2.2, 3.3], dtype='float32'), 'float32',\n         np.array([1, 2, 3], dtype='i4'), pa.int32()),\n        (np.array([1.1, 2.2, 3.3], dtype='float64'), 'float64',\n         np.array([1, 2, 3], dtype='i4'), pa.int32()),\n        (np.array([-10.1, 20.2, -30.3], dtype='float64'), 'float64',\n         np.array([-10, 20, -30], dtype='i4'), pa.int32()),\n    ]\n    for case in unsafe_cases:\n        # test safe casting raises\n        with pytest.raises(pa.ArrowInvalid, match='truncated'):\n            _check_cast_case(case, safe=True)\n\n        # test unsafe casting truncates\n        _check_cast_case(case, safe=False)\n\n\ndef test_decimal_to_int_safe():\n    safe_cases = [\n        (\n            [decimal.Decimal(\"123456\"), None, decimal.Decimal(\"-912345\")],\n            pa.decimal128(32, 5),\n            [123456, None, -912345],\n            pa.int32()\n        ),\n        (\n            [decimal.Decimal(\"1234\"), None, decimal.Decimal(\"-9123\")],\n            pa.decimal128(19, 10),\n            [1234, None, -9123],\n            pa.int16()\n        ),\n        (\n            [decimal.Decimal(\"123\"), None, decimal.Decimal(\"-91\")],\n            pa.decimal128(19, 10),\n            [123, None, -91],\n            pa.int8()\n        ),\n    ]\n    for case in safe_cases:\n        _check_cast_case(case)\n        _check_cast_case(case, safe=True)\n\n\ndef test_decimal_to_int_value_out_of_bounds():\n    out_of_bounds_cases = [\n        (\n            np.array([\n                decimal.Decimal(\"1234567890123\"),\n                None,\n                decimal.Decimal(\"-912345678901234\")\n            ]),\n            pa.decimal128(32, 5),\n            [1912276171, None, -135950322],\n            pa.int32()\n        ),\n        (\n            [decimal.Decimal(\"123456\"), None, decimal.Decimal(\"-912345678\")],\n            pa.decimal128(32, 5),\n            [-7616, None, -19022],\n            pa.int16()\n        ),\n        (\n            [decimal.Decimal(\"1234\"), None, decimal.Decimal(\"-9123\")],\n            pa.decimal128(32, 5),\n            [-46, None, 93],\n            pa.int8()\n        ),\n    ]\n\n    for case in out_of_bounds_cases:\n        # test safe casting raises\n        with pytest.raises(pa.ArrowInvalid,\n                           match='Integer value out of bounds'):\n            _check_cast_case(case)\n\n        # XXX `safe=False` can be ignored when constructing an array\n        # from a sequence of Python objects (ARROW-8567)\n        _check_cast_case(case, safe=False, check_array_construction=False)\n\n\ndef test_decimal_to_int_non_integer():\n    non_integer_cases = [\n        (\n            [\n                decimal.Decimal(\"123456.21\"),\n                None,\n                decimal.Decimal(\"-912345.13\")\n            ],\n            pa.decimal128(32, 5),\n            [123456, None, -912345],\n            pa.int32()\n        ),\n        (\n            [decimal.Decimal(\"1234.134\"), None, decimal.Decimal(\"-9123.1\")],\n            pa.decimal128(19, 10),\n            [1234, None, -9123],\n            pa.int16()\n        ),\n        (\n            [decimal.Decimal(\"123.1451\"), None, decimal.Decimal(\"-91.21\")],\n            pa.decimal128(19, 10),\n            [123, None, -91],\n            pa.int8()\n        ),\n    ]\n\n    for case in non_integer_cases:\n        # test safe casting raises\n        msg_regexp = 'Rescaling Decimal128 value would cause data loss'\n        with pytest.raises(pa.ArrowInvalid, match=msg_regexp):\n            _check_cast_case(case)\n\n        _check_cast_case(case, safe=False)\n\n\ndef test_decimal_to_decimal():\n    arr = pa.array(\n        [decimal.Decimal(\"1234.12\"), None],\n        type=pa.decimal128(19, 10)\n    )\n    result = arr.cast(pa.decimal128(15, 6))\n    expected = pa.array(\n        [decimal.Decimal(\"1234.12\"), None],\n        type=pa.decimal128(15, 6)\n    )\n    assert result.equals(expected)\n\n    msg_regexp = 'Rescaling Decimal128 value would cause data loss'\n    with pytest.raises(pa.ArrowInvalid, match=msg_regexp):\n        result = arr.cast(pa.decimal128(9, 1))\n\n    result = arr.cast(pa.decimal128(9, 1), safe=False)\n    expected = pa.array(\n        [decimal.Decimal(\"1234.1\"), None],\n        type=pa.decimal128(9, 1)\n    )\n    assert result.equals(expected)\n\n    with pytest.raises(pa.ArrowInvalid,\n                       match='Decimal value does not fit in precision'):\n        result = arr.cast(pa.decimal128(5, 2))\n\n\ndef test_safe_cast_nan_to_int_raises():\n    arr = pa.array([np.nan, 1.])\n\n    with pytest.raises(pa.ArrowInvalid, match='truncated'):\n        arr.cast(pa.int64(), safe=True)\n\n\ndef test_cast_signed_to_unsigned():\n    safe_cases = [\n        (np.array([0, 1, 2, 3], dtype='i1'), pa.uint8(),\n         np.array([0, 1, 2, 3], dtype='u1'), pa.uint8()),\n        (np.array([0, 1, 2, 3], dtype='i2'), pa.uint16(),\n         np.array([0, 1, 2, 3], dtype='u2'), pa.uint16())\n    ]\n\n    for case in safe_cases:\n        _check_cast_case(case)\n\n\ndef test_cast_from_null():\n    in_data = [None] * 3\n    in_type = pa.null()\n    out_types = [\n        pa.null(),\n        pa.uint8(),\n        pa.float16(),\n        pa.utf8(),\n        pa.binary(),\n        pa.binary(10),\n        pa.list_(pa.int16()),\n        pa.list_(pa.int32(), 4),\n        pa.large_list(pa.uint8()),\n        pa.decimal128(19, 4),\n        pa.timestamp('us'),\n        pa.timestamp('us', tz='UTC'),\n        pa.timestamp('us', tz='Europe/Paris'),\n        pa.duration('us'),\n        pa.month_day_nano_interval(),\n        pa.struct([pa.field('a', pa.int32()),\n                   pa.field('b', pa.list_(pa.int8())),\n                   pa.field('c', pa.string())]),\n        pa.dictionary(pa.int32(), pa.string()),\n    ]\n    for out_type in out_types:\n        _check_cast_case((in_data, in_type, in_data, out_type))\n\n    out_types = [\n\n        pa.union([pa.field('a', pa.binary(10)),\n                  pa.field('b', pa.string())], mode=pa.lib.UnionMode_DENSE),\n        pa.union([pa.field('a', pa.binary(10)),\n                  pa.field('b', pa.string())], mode=pa.lib.UnionMode_SPARSE),\n    ]\n    in_arr = pa.array(in_data, type=pa.null())\n    for out_type in out_types:\n        with pytest.raises(NotImplementedError):\n            in_arr.cast(out_type)\n\n\ndef test_cast_string_to_number_roundtrip():\n    cases = [\n        (pa.array([\"1\", \"127\", \"-128\"]),\n         pa.array([1, 127, -128], type=pa.int8())),\n        (pa.array([None, \"18446744073709551615\"]),\n         pa.array([None, 18446744073709551615], type=pa.uint64())),\n    ]\n    for in_arr, expected in cases:\n        casted = in_arr.cast(expected.type, safe=True)\n        casted.validate(full=True)\n        assert casted.equals(expected)\n        casted_back = casted.cast(in_arr.type, safe=True)\n        casted_back.validate(full=True)\n        assert casted_back.equals(in_arr)\n\n\ndef test_cast_dictionary():\n    # cast to the value type\n    arr = pa.array(\n        [\"foo\", \"bar\", None],\n        type=pa.dictionary(pa.int64(), pa.string())\n    )\n    expected = pa.array([\"foo\", \"bar\", None])\n    assert arr.type == pa.dictionary(pa.int64(), pa.string())\n    assert arr.cast(pa.string()) == expected\n\n    # cast to a different key type\n    for key_type in [pa.int8(), pa.int16(), pa.int32()]:\n        typ = pa.dictionary(key_type, pa.string())\n        expected = pa.array(\n            [\"foo\", \"bar\", None],\n            type=pa.dictionary(key_type, pa.string())\n        )\n        assert arr.cast(typ) == expected\n\n    # shouldn't crash (ARROW-7077)\n    with pytest.raises(pa.ArrowInvalid):\n        arr.cast(pa.int32())\n\n\ndef test_view():\n    # ARROW-5992\n    arr = pa.array(['foo', 'bar', 'baz'], type=pa.utf8())\n    expected = pa.array(['foo', 'bar', 'baz'], type=pa.binary())\n\n    assert arr.view(pa.binary()).equals(expected)\n    assert arr.view('binary').equals(expected)\n\n\ndef test_unique_simple():\n    cases = [\n        (pa.array([1, 2, 3, 1, 2, 3]), pa.array([1, 2, 3])),\n        (pa.array(['foo', None, 'bar', 'foo']),\n         pa.array(['foo', None, 'bar'])),\n        (pa.array(['foo', None, 'bar', 'foo'], pa.large_binary()),\n         pa.array(['foo', None, 'bar'], pa.large_binary())),\n    ]\n    for arr, expected in cases:\n        result = arr.unique()\n        assert result.equals(expected)\n        result = pa.chunked_array([arr]).unique()\n        assert result.equals(expected)\n\n\ndef test_value_counts_simple():\n    cases = [\n        (pa.array([1, 2, 3, 1, 2, 3]),\n         pa.array([1, 2, 3]),\n         pa.array([2, 2, 2], type=pa.int64())),\n        (pa.array(['foo', None, 'bar', 'foo']),\n         pa.array(['foo', None, 'bar']),\n         pa.array([2, 1, 1], type=pa.int64())),\n        (pa.array(['foo', None, 'bar', 'foo'], pa.large_binary()),\n         pa.array(['foo', None, 'bar'], pa.large_binary()),\n         pa.array([2, 1, 1], type=pa.int64())),\n    ]\n    for arr, expected_values, expected_counts in cases:\n        for arr_in in (arr, pa.chunked_array([arr])):\n            result = arr_in.value_counts()\n            assert result.type.equals(\n                pa.struct([pa.field(\"values\", arr.type),\n                           pa.field(\"counts\", pa.int64())]))\n            assert result.field(\"values\").equals(expected_values)\n            assert result.field(\"counts\").equals(expected_counts)\n\n\ndef test_unique_value_counts_dictionary_type():\n    indices = pa.array([3, 0, 0, 0, 1, 1, 3, 0, 1, 3, 0, 1])\n    dictionary = pa.array(['foo', 'bar', 'baz', 'qux'])\n\n    arr = pa.DictionaryArray.from_arrays(indices, dictionary)\n\n    unique_result = arr.unique()\n    expected = pa.DictionaryArray.from_arrays(indices.unique(), dictionary)\n    assert unique_result.equals(expected)\n\n    result = arr.value_counts()\n    assert result.field('values').equals(unique_result)\n    assert result.field('counts').equals(pa.array([3, 5, 4], type='int64'))\n\n    arr = pa.DictionaryArray.from_arrays(\n        pa.array([], type='int64'), dictionary)\n    unique_result = arr.unique()\n    expected = pa.DictionaryArray.from_arrays(pa.array([], type='int64'),\n                                              pa.array([], type='utf8'))\n    assert unique_result.equals(expected)\n\n    result = arr.value_counts()\n    assert result.field('values').equals(unique_result)\n    assert result.field('counts').equals(pa.array([], type='int64'))\n\n\ndef test_dictionary_encode_simple():\n    cases = [\n        (pa.array([1, 2, 3, None, 1, 2, 3]),\n         pa.DictionaryArray.from_arrays(\n             pa.array([0, 1, 2, None, 0, 1, 2], type='int32'),\n             [1, 2, 3])),\n        (pa.array(['foo', None, 'bar', 'foo']),\n         pa.DictionaryArray.from_arrays(\n             pa.array([0, None, 1, 0], type='int32'),\n             ['foo', 'bar'])),\n        (pa.array(['foo', None, 'bar', 'foo'], type=pa.large_binary()),\n         pa.DictionaryArray.from_arrays(\n             pa.array([0, None, 1, 0], type='int32'),\n             pa.array(['foo', 'bar'], type=pa.large_binary()))),\n    ]\n    for arr, expected in cases:\n        result = arr.dictionary_encode()\n        assert result.equals(expected)\n        result = pa.chunked_array([arr]).dictionary_encode()\n        assert result.num_chunks == 1\n        assert result.chunk(0).equals(expected)\n        result = pa.chunked_array([], type=arr.type).dictionary_encode()\n        assert result.num_chunks == 0\n        assert result.type == expected.type\n\n\ndef test_dictionary_encode_sliced():\n    cases = [\n        (pa.array([1, 2, 3, None, 1, 2, 3])[1:-1],\n         pa.DictionaryArray.from_arrays(\n             pa.array([0, 1, None, 2, 0], type='int32'),\n             [2, 3, 1])),\n        (pa.array([None, 'foo', 'bar', 'foo', 'xyzzy'])[1:-1],\n         pa.DictionaryArray.from_arrays(\n             pa.array([0, 1, 0], type='int32'),\n             ['foo', 'bar'])),\n        (pa.array([None, 'foo', 'bar', 'foo', 'xyzzy'],\n                  type=pa.large_string())[1:-1],\n         pa.DictionaryArray.from_arrays(\n             pa.array([0, 1, 0], type='int32'),\n             pa.array(['foo', 'bar'], type=pa.large_string()))),\n    ]\n    for arr, expected in cases:\n        result = arr.dictionary_encode()\n        assert result.equals(expected)\n        result = pa.chunked_array([arr]).dictionary_encode()\n        assert result.num_chunks == 1\n        assert result.type == expected.type\n        assert result.chunk(0).equals(expected)\n        result = pa.chunked_array([], type=arr.type).dictionary_encode()\n        assert result.num_chunks == 0\n        assert result.type == expected.type\n\n    # ARROW-9143 dictionary_encode after slice was segfaulting\n    array = pa.array(['foo', 'bar', 'baz'])\n    array.slice(1).dictionary_encode()\n\n\ndef test_dictionary_encode_zero_length():\n    # User-facing experience of ARROW-7008\n    arr = pa.array([], type=pa.string())\n    encoded = arr.dictionary_encode()\n    assert len(encoded.dictionary) == 0\n    encoded.validate(full=True)\n\n\ndef test_dictionary_decode():\n    cases = [\n        (pa.array([1, 2, 3, None, 1, 2, 3]),\n         pa.DictionaryArray.from_arrays(\n             pa.array([0, 1, 2, None, 0, 1, 2], type='int32'),\n             [1, 2, 3])),\n        (pa.array(['foo', None, 'bar', 'foo']),\n         pa.DictionaryArray.from_arrays(\n             pa.array([0, None, 1, 0], type='int32'),\n             ['foo', 'bar'])),\n        (pa.array(['foo', None, 'bar', 'foo'], type=pa.large_binary()),\n         pa.DictionaryArray.from_arrays(\n             pa.array([0, None, 1, 0], type='int32'),\n             pa.array(['foo', 'bar'], type=pa.large_binary()))),\n    ]\n    for expected, arr in cases:\n        result = arr.dictionary_decode()\n        assert result.equals(expected)\n\n\ndef test_cast_time32_to_int():\n    arr = pa.array(np.array([0, 1, 2], dtype='int32'),\n                   type=pa.time32('s'))\n    expected = pa.array([0, 1, 2], type='i4')\n\n    result = arr.cast('i4')\n    assert result.equals(expected)\n\n\ndef test_cast_time64_to_int():\n    arr = pa.array(np.array([0, 1, 2], dtype='int64'),\n                   type=pa.time64('us'))\n    expected = pa.array([0, 1, 2], type='i8')\n\n    result = arr.cast('i8')\n    assert result.equals(expected)\n\n\ndef test_cast_timestamp_to_int():\n    arr = pa.array(np.array([0, 1, 2], dtype='int64'),\n                   type=pa.timestamp('us'))\n    expected = pa.array([0, 1, 2], type='i8')\n\n    result = arr.cast('i8')\n    assert result.equals(expected)\n\n\ndef test_cast_date32_to_int():\n    arr = pa.array([0, 1, 2], type='i4')\n\n    result1 = arr.cast('date32')\n    result2 = result1.cast('i4')\n\n    expected1 = pa.array([\n        datetime.date(1970, 1, 1),\n        datetime.date(1970, 1, 2),\n        datetime.date(1970, 1, 3)\n    ]).cast('date32')\n\n    assert result1.equals(expected1)\n    assert result2.equals(arr)\n\n\ndef test_cast_duration_to_int():\n    arr = pa.array(np.array([0, 1, 2], dtype='int64'),\n                   type=pa.duration('us'))\n    expected = pa.array([0, 1, 2], type='i8')\n\n    result = arr.cast('i8')\n    assert result.equals(expected)\n\n\ndef test_cast_binary_to_utf8():\n    binary_arr = pa.array([b'foo', b'bar', b'baz'], type=pa.binary())\n    utf8_arr = binary_arr.cast(pa.utf8())\n    expected = pa.array(['foo', 'bar', 'baz'], type=pa.utf8())\n\n    assert utf8_arr.equals(expected)\n\n    non_utf8_values = [('ma\u00f1ana').encode('utf-16-le')]\n    non_utf8_binary = pa.array(non_utf8_values)\n    assert non_utf8_binary.type == pa.binary()\n    with pytest.raises(ValueError):\n        non_utf8_binary.cast(pa.string())\n\n    non_utf8_all_null = pa.array(non_utf8_values, mask=np.array([True]),\n                                 type=pa.binary())\n    # No error\n    casted = non_utf8_all_null.cast(pa.string())\n    assert casted.null_count == 1\n\n\ndef test_cast_date64_to_int():\n    arr = pa.array(np.array([0, 1, 2], dtype='int64'),\n                   type=pa.date64())\n    expected = pa.array([0, 1, 2], type='i8')\n\n    result = arr.cast('i8')\n\n    assert result.equals(expected)\n\n\ndef test_date64_from_builtin_datetime():\n    val1 = datetime.datetime(2000, 1, 1, 12, 34, 56, 123456)\n    val2 = datetime.datetime(2000, 1, 1)\n    result = pa.array([val1, val2], type='date64')\n    result2 = pa.array([val1.date(), val2.date()], type='date64')\n\n    assert result.equals(result2)\n\n    as_i8 = result.view('int64')\n    assert as_i8[0].as_py() == as_i8[1].as_py()\n\n\n@pytest.mark.parametrize(('ty', 'values'), [\n    ('bool', [True, False, True]),\n    ('uint8', range(0, 255)),\n    ('int8', range(0, 128)),\n    ('uint16', range(0, 10)),\n    ('int16', range(0, 10)),\n    ('uint32', range(0, 10)),\n    ('int32', range(0, 10)),\n    ('uint64', range(0, 10)),\n    ('int64', range(0, 10)),\n    ('float', [0.0, 0.1, 0.2]),\n    ('double', [0.0, 0.1, 0.2]),\n    ('string', ['a', 'b', 'c']),\n    ('binary', [b'a', b'b', b'c']),\n    (pa.binary(3), [b'abc', b'bcd', b'cde'])\n])\ndef test_cast_identities(ty, values):\n    arr = pa.array(values, type=ty)\n    assert arr.cast(ty).equals(arr)\n\n\npickle_test_parametrize = pytest.mark.parametrize(\n    ('data', 'typ'),\n    [\n        ([True, False, True, True], pa.bool_()),\n        ([1, 2, 4, 6], pa.int64()),\n        ([1.0, 2.5, None], pa.float64()),\n        (['a', None, 'b'], pa.string()),\n        ([], None),\n        ([[1, 2], [3]], pa.list_(pa.int64())),\n        ([[4, 5], [6]], pa.large_list(pa.int16())),\n        ([['a'], None, ['b', 'c']], pa.list_(pa.string())),\n        ([[1, 2], [3]], pa.list_view(pa.int64())),\n        ([[4, 5], [6]], pa.large_list_view(pa.int16())),\n        ([['a'], None, ['b', 'c']], pa.list_view(pa.string())),\n        ([(1, 'a'), (2, 'c'), None],\n            pa.struct([pa.field('a', pa.int64()), pa.field('b', pa.string())]))\n    ]\n)\n\n\n@pickle_test_parametrize\ndef test_array_pickle(data, typ, pickle_module):\n    # Allocate here so that we don't have any Arrow data allocated.\n    # This is needed to ensure that allocator tests can be reliable.\n    array = pa.array(data, type=typ)\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        result = pickle_module.loads(pickle_module.dumps(array, proto))\n        assert array.equals(result)\n\n\ndef test_array_pickle_dictionary(pickle_module):\n    # not included in the above as dictionary array cannot be created with\n    # the pa.array function\n    array = pa.DictionaryArray.from_arrays([0, 1, 2, 0, 1], ['a', 'b', 'c'])\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        result = pickle_module.loads(pickle_module.dumps(array, proto))\n        assert array.equals(result)\n\n\n@h.settings(suppress_health_check=(h.HealthCheck.too_slow,))\n@h.given(\n    past.arrays(\n        past.all_types,\n        size=st.integers(min_value=0, max_value=10)\n    )\n)\ndef test_pickling(pickle_module, arr):\n    data = pickle_module.dumps(arr)\n    restored = pickle_module.loads(data)\n    assert arr.equals(restored)\n\n\n@pickle_test_parametrize\ndef test_array_pickle_protocol5(data, typ, pickle_module):\n    # Test zero-copy pickling with protocol 5 (PEP 574)\n    array = pa.array(data, type=typ)\n    addresses = [buf.address if buf is not None else 0\n                 for buf in array.buffers()]\n\n    for proto in range(5, pickle_module.HIGHEST_PROTOCOL + 1):\n        buffers = []\n        pickled = pickle_module.dumps(array, proto, buffer_callback=buffers.append)\n        result = pickle_module.loads(pickled, buffers=buffers)\n        assert array.equals(result)\n\n        result_addresses = [buf.address if buf is not None else 0\n                            for buf in result.buffers()]\n        assert result_addresses == addresses\n\n\n@pytest.mark.parametrize(\n    'narr',\n    [\n        np.arange(10, dtype=np.int64),\n        np.arange(10, dtype=np.int32),\n        np.arange(10, dtype=np.int16),\n        np.arange(10, dtype=np.int8),\n        np.arange(10, dtype=np.uint64),\n        np.arange(10, dtype=np.uint32),\n        np.arange(10, dtype=np.uint16),\n        np.arange(10, dtype=np.uint8),\n        np.arange(10, dtype=np.float64),\n        np.arange(10, dtype=np.float32),\n        np.arange(10, dtype=np.float16),\n    ]\n)\ndef test_to_numpy_roundtrip(narr):\n    arr = pa.array(narr)\n    assert narr.dtype == arr.to_numpy().dtype\n    np.testing.assert_array_equal(narr, arr.to_numpy())\n    np.testing.assert_array_equal(narr[:6], arr[:6].to_numpy())\n    np.testing.assert_array_equal(narr[2:], arr[2:].to_numpy())\n    np.testing.assert_array_equal(narr[2:6], arr[2:6].to_numpy())\n\n\ndef test_array_uint64_from_py_over_range():\n    arr = pa.array([2 ** 63], type=pa.uint64())\n    expected = pa.array(np.array([2 ** 63], dtype='u8'))\n    assert arr.equals(expected)\n\n\ndef test_array_conversions_no_sentinel_values():\n    arr = np.array([1, 2, 3, 4], dtype='int8')\n    refcount = sys.getrefcount(arr)\n    arr2 = pa.array(arr)  # noqa\n    assert sys.getrefcount(arr) == (refcount + 1)\n\n    assert arr2.type == 'int8'\n\n    arr3 = pa.array(np.array([1, np.nan, 2, 3, np.nan, 4], dtype='float32'),\n                    type='float32')\n    assert arr3.type == 'float32'\n    assert arr3.null_count == 0\n\n\ndef test_time32_time64_from_integer():\n    # ARROW-4111\n    result = pa.array([1, 2, None], type=pa.time32('s'))\n    expected = pa.array([datetime.time(second=1),\n                         datetime.time(second=2), None],\n                        type=pa.time32('s'))\n    assert result.equals(expected)\n\n    result = pa.array([1, 2, None], type=pa.time32('ms'))\n    expected = pa.array([datetime.time(microsecond=1000),\n                         datetime.time(microsecond=2000), None],\n                        type=pa.time32('ms'))\n    assert result.equals(expected)\n\n    result = pa.array([1, 2, None], type=pa.time64('us'))\n    expected = pa.array([datetime.time(microsecond=1),\n                         datetime.time(microsecond=2), None],\n                        type=pa.time64('us'))\n    assert result.equals(expected)\n\n    result = pa.array([1000, 2000, None], type=pa.time64('ns'))\n    expected = pa.array([datetime.time(microsecond=1),\n                         datetime.time(microsecond=2), None],\n                        type=pa.time64('ns'))\n    assert result.equals(expected)\n\n\ndef test_binary_string_pandas_null_sentinels():\n    # ARROW-6227\n    def _check_case(ty):\n        arr = pa.array(['string', np.nan], type=ty, from_pandas=True)\n        expected = pa.array(['string', None], type=ty)\n        assert arr.equals(expected)\n    _check_case('binary')\n    _check_case('utf8')\n\n\ndef test_pandas_null_sentinels_raise_error():\n    # ARROW-6227\n    cases = [\n        ([None, np.nan], 'null'),\n        (['string', np.nan], 'binary'),\n        (['string', np.nan], 'utf8'),\n        (['string', np.nan], 'large_binary'),\n        (['string', np.nan], 'large_utf8'),\n        ([b'string', np.nan], pa.binary(6)),\n        ([True, np.nan], pa.bool_()),\n        ([decimal.Decimal('0'), np.nan], pa.decimal128(12, 2)),\n        ([0, np.nan], pa.date32()),\n        ([0, np.nan], pa.date32()),\n        ([0, np.nan], pa.date64()),\n        ([0, np.nan], pa.time32('s')),\n        ([0, np.nan], pa.time64('us')),\n        ([0, np.nan], pa.timestamp('us')),\n        ([0, np.nan], pa.duration('us')),\n    ]\n    for case, ty in cases:\n        # Both types of exceptions are raised. May want to clean that up\n        with pytest.raises((ValueError, TypeError)):\n            pa.array(case, type=ty)\n\n        # from_pandas option suppresses failure\n        result = pa.array(case, type=ty, from_pandas=True)\n        assert result.null_count == (1 if ty != 'null' else 2)\n\n\n@pytest.mark.pandas\ndef test_pandas_null_sentinels_index():\n    # ARROW-7023 - ensure that when passing a pandas Index, \"from_pandas\"\n    # semantics are used\n    import pandas as pd\n    idx = pd.Index([1, 2, np.nan], dtype=object)\n    result = pa.array(idx)\n    expected = pa.array([1, 2, np.nan], from_pandas=True)\n    assert result.equals(expected)\n\n\ndef test_array_roundtrip_from_numpy_datetimeD():\n    arr = np.array([None, datetime.date(2017, 4, 4)], dtype='datetime64[D]')\n\n    result = pa.array(arr)\n    expected = pa.array([None, datetime.date(2017, 4, 4)], type=pa.date32())\n    assert result.equals(expected)\n    result = result.to_numpy(zero_copy_only=False)\n    np.testing.assert_array_equal(result, arr)\n    assert result.dtype == arr.dtype\n\n\ndef test_array_from_naive_datetimes():\n    arr = pa.array([\n        None,\n        datetime.datetime(2017, 4, 4, 12, 11, 10),\n        datetime.datetime(2018, 1, 1, 0, 2, 0)\n    ])\n    assert arr.type == pa.timestamp('us', tz=None)\n\n\n@pytest.mark.parametrize(('dtype', 'type'), [\n    ('datetime64[s]', pa.timestamp('s')),\n    ('datetime64[ms]', pa.timestamp('ms')),\n    ('datetime64[us]', pa.timestamp('us')),\n    ('datetime64[ns]', pa.timestamp('ns'))\n])\ndef test_array_from_numpy_datetime(dtype, type):\n    data = [\n        None,\n        datetime.datetime(2017, 4, 4, 12, 11, 10),\n        datetime.datetime(2018, 1, 1, 0, 2, 0)\n    ]\n\n    # from numpy array\n    arr = pa.array(np.array(data, dtype=dtype))\n    expected = pa.array(data, type=type)\n    assert arr.equals(expected)\n\n    # from list of numpy scalars\n    arr = pa.array(list(np.array(data, dtype=dtype)))\n    assert arr.equals(expected)\n\n\ndef test_array_from_different_numpy_datetime_units_raises():\n    data = [\n        None,\n        datetime.datetime(2017, 4, 4, 12, 11, 10),\n        datetime.datetime(2018, 1, 1, 0, 2, 0)\n    ]\n    s = np.array(data, dtype='datetime64[s]')\n    ms = np.array(data, dtype='datetime64[ms]')\n    data = list(s[:2]) + list(ms[2:])\n\n    with pytest.raises(pa.ArrowNotImplementedError):\n        pa.array(data)\n\n\n@pytest.mark.parametrize('unit', ['ns', 'us', 'ms', 's'])\ndef test_array_from_list_of_timestamps(unit):\n    n = np.datetime64('NaT', unit)\n    x = np.datetime64('2017-01-01 01:01:01.111111111', unit)\n    y = np.datetime64('2018-11-22 12:24:48.111111111', unit)\n\n    a1 = pa.array([n, x, y])\n    a2 = pa.array([n, x, y], type=pa.timestamp(unit))\n\n    assert a1.type == a2.type\n    assert a1.type.unit == unit\n    assert a1[0] == a2[0]\n\n\ndef test_array_from_timestamp_with_generic_unit():\n    n = np.datetime64('NaT')\n    x = np.datetime64('2017-01-01 01:01:01.111111111')\n    y = np.datetime64('2018-11-22 12:24:48.111111111')\n\n    with pytest.raises(pa.ArrowNotImplementedError,\n                       match='Unbound or generic datetime64 time unit'):\n        pa.array([n, x, y])\n\n\n@pytest.mark.parametrize(('dtype', 'type'), [\n    ('timedelta64[s]', pa.duration('s')),\n    ('timedelta64[ms]', pa.duration('ms')),\n    ('timedelta64[us]', pa.duration('us')),\n    ('timedelta64[ns]', pa.duration('ns'))\n])\ndef test_array_from_numpy_timedelta(dtype, type):\n    data = [\n        None,\n        datetime.timedelta(1),\n        datetime.timedelta(0, 1)\n    ]\n\n    # from numpy array\n    np_arr = np.array(data, dtype=dtype)\n    arr = pa.array(np_arr)\n    assert isinstance(arr, pa.DurationArray)\n    assert arr.type == type\n    expected = pa.array(data, type=type)\n    assert arr.equals(expected)\n    assert arr.to_pylist() == data\n\n    # from list of numpy scalars\n    arr = pa.array(list(np.array(data, dtype=dtype)))\n    assert arr.equals(expected)\n    assert arr.to_pylist() == data\n\n\ndef test_array_from_numpy_timedelta_incorrect_unit():\n    # generic (no unit)\n    td = np.timedelta64(1)\n\n    for data in [[td], np.array([td])]:\n        with pytest.raises(NotImplementedError):\n            pa.array(data)\n\n    # unsupported unit\n    td = np.timedelta64(1, 'M')\n    for data in [[td], np.array([td])]:\n        with pytest.raises(NotImplementedError):\n            pa.array(data)\n\n\ndef test_array_from_numpy_ascii():\n    arr = np.array(['abcde', 'abc', ''], dtype='|S5')\n\n    arrow_arr = pa.array(arr)\n    assert arrow_arr.type == 'binary'\n    expected = pa.array(['abcde', 'abc', ''], type='binary')\n    assert arrow_arr.equals(expected)\n\n    mask = np.array([False, True, False])\n    arrow_arr = pa.array(arr, mask=mask)\n    expected = pa.array(['abcde', None, ''], type='binary')\n    assert arrow_arr.equals(expected)\n\n    # Strided variant\n    arr = np.array(['abcde', 'abc', ''] * 5, dtype='|S5')[::2]\n    mask = np.array([False, True, False] * 5)[::2]\n    arrow_arr = pa.array(arr, mask=mask)\n\n    expected = pa.array(['abcde', '', None, 'abcde', '', None, 'abcde', ''],\n                        type='binary')\n    assert arrow_arr.equals(expected)\n\n    # 0 itemsize\n    arr = np.array(['', '', ''], dtype='|S0')\n    arrow_arr = pa.array(arr)\n    expected = pa.array(['', '', ''], type='binary')\n    assert arrow_arr.equals(expected)\n\n\ndef test_interval_array_from_timedelta():\n    data = [\n        None,\n        datetime.timedelta(days=1, seconds=1, microseconds=1,\n                           milliseconds=1, minutes=1, hours=1, weeks=1)]\n\n    # From timedelta (explicit type required)\n    arr = pa.array(data, pa.month_day_nano_interval())\n    assert isinstance(arr, pa.MonthDayNanoIntervalArray)\n    assert arr.type == pa.month_day_nano_interval()\n    expected_list = [\n        None,\n        pa.MonthDayNano([0, 8,\n                         (datetime.timedelta(seconds=1, microseconds=1,\n                                             milliseconds=1, minutes=1,\n                                             hours=1) //\n                          datetime.timedelta(microseconds=1)) * 1000])]\n    expected = pa.array(expected_list)\n    assert arr.equals(expected)\n    assert arr.to_pylist() == expected_list\n\n\n@pytest.mark.pandas\ndef test_interval_array_from_relativedelta():\n    # dateutil is dependency of pandas\n    from dateutil.relativedelta import relativedelta\n    from pandas import DateOffset\n    data = [\n        None,\n        relativedelta(years=1, months=1,\n                      days=1, seconds=1, microseconds=1,\n                      minutes=1, hours=1, weeks=1, leapdays=1)]\n    # Note leapdays are ignored.\n\n    # From relativedelta\n    arr = pa.array(data)\n    assert isinstance(arr, pa.MonthDayNanoIntervalArray)\n    assert arr.type == pa.month_day_nano_interval()\n    expected_list = [\n        None,\n        pa.MonthDayNano([13, 8,\n                         (datetime.timedelta(seconds=1, microseconds=1,\n                                             minutes=1, hours=1) //\n                          datetime.timedelta(microseconds=1)) * 1000])]\n    expected = pa.array(expected_list)\n    assert arr.equals(expected)\n    assert arr.to_pandas().tolist() == [\n        None, DateOffset(months=13, days=8,\n                         microseconds=(\n                             datetime.timedelta(seconds=1, microseconds=1,\n                                                minutes=1, hours=1) //\n                             datetime.timedelta(microseconds=1)),\n                         nanoseconds=0)]\n    with pytest.raises(ValueError):\n        pa.array([DateOffset(years=((1 << 32) // 12), months=100)])\n    with pytest.raises(ValueError):\n        pa.array([DateOffset(weeks=((1 << 32) // 7), days=100)])\n    with pytest.raises(ValueError):\n        pa.array([DateOffset(seconds=((1 << 64) // 1000000000),\n                             nanoseconds=1)])\n    with pytest.raises(ValueError):\n        pa.array([DateOffset(microseconds=((1 << 64) // 100))])\n\n\ndef test_interval_array_from_tuple():\n    data = [None, (1, 2, -3)]\n\n    # From timedelta (explicit type required)\n    arr = pa.array(data, pa.month_day_nano_interval())\n    assert isinstance(arr, pa.MonthDayNanoIntervalArray)\n    assert arr.type == pa.month_day_nano_interval()\n    expected_list = [\n        None,\n        pa.MonthDayNano([1, 2, -3])]\n    expected = pa.array(expected_list)\n    assert arr.equals(expected)\n    assert arr.to_pylist() == expected_list\n\n\n@pytest.mark.pandas\ndef test_interval_array_from_dateoffset():\n    from pandas.tseries.offsets import DateOffset\n    data = [\n        None,\n        DateOffset(years=1, months=1,\n                   days=1, seconds=1, microseconds=1,\n                   minutes=1, hours=1, weeks=1, nanoseconds=1),\n        DateOffset()]\n\n    arr = pa.array(data)\n    assert isinstance(arr, pa.MonthDayNanoIntervalArray)\n    assert arr.type == pa.month_day_nano_interval()\n    expected_list = [\n        None,\n        pa.MonthDayNano([13, 8, 3661000001001]),\n        pa.MonthDayNano([0, 0, 0])]\n    expected = pa.array(expected_list)\n    assert arr.equals(expected)\n    expected_from_pandas = [\n        None, DateOffset(months=13, days=8,\n                         microseconds=(\n                             datetime.timedelta(seconds=1, microseconds=1,\n                                                minutes=1, hours=1) //\n                             datetime.timedelta(microseconds=1)),\n                         nanoseconds=1),\n        DateOffset(months=0, days=0, microseconds=0, nanoseconds=0)]\n\n    assert arr.to_pandas().tolist() == expected_from_pandas\n\n    # nested list<interval> array conversion\n    actual_list = pa.array([data]).to_pandas().tolist()\n    assert len(actual_list) == 1\n    assert list(actual_list[0]) == expected_from_pandas\n\n\ndef test_array_from_numpy_unicode():\n    dtypes = ['<U5', '>U5']\n\n    for dtype in dtypes:\n        arr = np.array(['abcde', 'abc', ''], dtype=dtype)\n\n        arrow_arr = pa.array(arr)\n        assert arrow_arr.type == 'utf8'\n        expected = pa.array(['abcde', 'abc', ''], type='utf8')\n        assert arrow_arr.equals(expected)\n\n        mask = np.array([False, True, False])\n        arrow_arr = pa.array(arr, mask=mask)\n        expected = pa.array(['abcde', None, ''], type='utf8')\n        assert arrow_arr.equals(expected)\n\n        # Strided variant\n        arr = np.array(['abcde', 'abc', ''] * 5, dtype=dtype)[::2]\n        mask = np.array([False, True, False] * 5)[::2]\n        arrow_arr = pa.array(arr, mask=mask)\n\n        expected = pa.array(['abcde', '', None, 'abcde', '', None,\n                             'abcde', ''], type='utf8')\n        assert arrow_arr.equals(expected)\n\n    # 0 itemsize\n    arr = np.array(['', '', ''], dtype='<U0')\n    arrow_arr = pa.array(arr)\n    expected = pa.array(['', '', ''], type='utf8')\n    assert arrow_arr.equals(expected)\n\n\ndef test_array_string_from_non_string():\n    # ARROW-5682 - when converting to string raise on non string-like dtype\n    with pytest.raises(TypeError):\n        pa.array(np.array([1, 2, 3]), type=pa.string())\n\n\ndef test_array_string_from_all_null():\n    # ARROW-5682\n    vals = np.array([None, None], dtype=object)\n    arr = pa.array(vals, type=pa.string())\n    assert arr.null_count == 2\n\n    vals = np.array([np.nan, np.nan], dtype='float64')\n    # by default raises, but accept as all-null when from_pandas=True\n    with pytest.raises(TypeError):\n        pa.array(vals, type=pa.string())\n    arr = pa.array(vals, type=pa.string(), from_pandas=True)\n    assert arr.null_count == 2\n\n\ndef test_array_from_masked():\n    ma = np.ma.array([1, 2, 3, 4], dtype='int64',\n                     mask=[False, False, True, False])\n    result = pa.array(ma)\n    expected = pa.array([1, 2, None, 4], type='int64')\n    assert expected.equals(result)\n\n    with pytest.raises(ValueError, match=\"Cannot pass a numpy masked array\"):\n        pa.array(ma, mask=np.array([True, False, False, False]))\n\n\ndef test_array_from_shrunken_masked():\n    ma = np.ma.array([0], dtype='int64')\n    result = pa.array(ma)\n    expected = pa.array([0], type='int64')\n    assert expected.equals(result)\n\n\ndef test_array_from_invalid_dim_raises():\n    msg = \"only handle 1-dimensional arrays\"\n    arr2d = np.array([[1, 2, 3], [4, 5, 6]])\n    with pytest.raises(ValueError, match=msg):\n        pa.array(arr2d)\n\n    arr0d = np.array(0)\n    with pytest.raises(ValueError, match=msg):\n        pa.array(arr0d)\n\n\ndef test_array_from_strided_bool():\n    # ARROW-6325\n    arr = np.ones((3, 2), dtype=bool)\n    result = pa.array(arr[:, 0])\n    expected = pa.array([True, True, True])\n    assert result.equals(expected)\n    result = pa.array(arr[0, :])\n    expected = pa.array([True, True])\n    assert result.equals(expected)\n\n\ndef test_array_from_strided():\n    pydata = [\n        ([b\"ab\", b\"cd\", b\"ef\"], (pa.binary(), pa.binary(2))),\n        ([1, 2, 3], (pa.int8(), pa.int16(), pa.int32(), pa.int64())),\n        ([1.0, 2.0, 3.0], (pa.float32(), pa.float64())),\n        ([\"ab\", \"cd\", \"ef\"], (pa.utf8(), ))\n    ]\n\n    for values, dtypes in pydata:\n        nparray = np.array(values)\n        for patype in dtypes:\n            for mask in (None, np.array([False, False])):\n                arrow_array = pa.array(nparray[::2], patype,\n                                       mask=mask)\n                assert values[::2] == arrow_array.to_pylist()\n\n\ndef test_boolean_true_count_false_count():\n    # ARROW-9145\n    arr = pa.array([True, True, None, False, None, True] * 1000)\n    assert arr.true_count == 3000\n    assert arr.false_count == 1000\n\n\ndef test_buffers_primitive():\n    a = pa.array([1, 2, None, 4], type=pa.int16())\n    buffers = a.buffers()\n    assert len(buffers) == 2\n    null_bitmap = buffers[0].to_pybytes()\n    assert 1 <= len(null_bitmap) <= 64  # XXX this is varying\n    assert bytearray(null_bitmap)[0] == 0b00001011\n\n    # Slicing does not affect the buffers but the offset\n    a_sliced = a[1:]\n    buffers = a_sliced.buffers()\n    a_sliced.offset == 1\n    assert len(buffers) == 2\n    null_bitmap = buffers[0].to_pybytes()\n    assert 1 <= len(null_bitmap) <= 64  # XXX this is varying\n    assert bytearray(null_bitmap)[0] == 0b00001011\n\n    assert struct.unpack('hhxxh', buffers[1].to_pybytes()) == (1, 2, 4)\n\n    a = pa.array(np.int8([4, 5, 6]))\n    buffers = a.buffers()\n    assert len(buffers) == 2\n    # No null bitmap from Numpy int array\n    assert buffers[0] is None\n    assert struct.unpack('3b', buffers[1].to_pybytes()) == (4, 5, 6)\n\n    a = pa.array([b'foo!', None, b'bar!!'])\n    buffers = a.buffers()\n    assert len(buffers) == 3\n    null_bitmap = buffers[0].to_pybytes()\n    assert bytearray(null_bitmap)[0] == 0b00000101\n    offsets = buffers[1].to_pybytes()\n    assert struct.unpack('4i', offsets) == (0, 4, 4, 9)\n    values = buffers[2].to_pybytes()\n    assert values == b'foo!bar!!'\n\n\ndef test_buffers_nested():\n    a = pa.array([[1, 2], None, [3, None, 4, 5]], type=pa.list_(pa.int64()))\n    buffers = a.buffers()\n    assert len(buffers) == 4\n    # The parent buffers\n    null_bitmap = buffers[0].to_pybytes()\n    assert bytearray(null_bitmap)[0] == 0b00000101\n    offsets = buffers[1].to_pybytes()\n    assert struct.unpack('4i', offsets) == (0, 2, 2, 6)\n    # The child buffers\n    null_bitmap = buffers[2].to_pybytes()\n    assert bytearray(null_bitmap)[0] == 0b00110111\n    values = buffers[3].to_pybytes()\n    assert struct.unpack('qqq8xqq', values) == (1, 2, 3, 4, 5)\n\n    a = pa.array([(42, None), None, (None, 43)],\n                 type=pa.struct([pa.field('a', pa.int8()),\n                                 pa.field('b', pa.int16())]))\n    buffers = a.buffers()\n    assert len(buffers) == 5\n    # The parent buffer\n    null_bitmap = buffers[0].to_pybytes()\n    assert bytearray(null_bitmap)[0] == 0b00000101\n    # The child buffers: 'a'\n    null_bitmap = buffers[1].to_pybytes()\n    assert bytearray(null_bitmap)[0] == 0b00000011\n    values = buffers[2].to_pybytes()\n    assert struct.unpack('bxx', values) == (42,)\n    # The child buffers: 'b'\n    null_bitmap = buffers[3].to_pybytes()\n    assert bytearray(null_bitmap)[0] == 0b00000110\n    values = buffers[4].to_pybytes()\n    assert struct.unpack('4xh', values) == (43,)\n\n\ndef test_total_buffer_size():\n    a = pa.array(np.array([4, 5, 6], dtype='int64'))\n    assert a.nbytes == 8 * 3\n    assert a.get_total_buffer_size() == 8 * 3\n    assert sys.getsizeof(a) >= object.__sizeof__(a) + a.nbytes\n    a = pa.array([1, None, 3], type='int64')\n    assert a.nbytes == 8*3 + 1\n    assert a.get_total_buffer_size() == 8*3 + 1\n    assert sys.getsizeof(a) >= object.__sizeof__(a) + a.nbytes\n    a = pa.array([[1, 2], None, [3, None, 4, 5]], type=pa.list_(pa.int64()))\n    assert a.nbytes == 62\n    assert a.get_total_buffer_size() == 1 + 4 * 4 + 1 + 6 * 8\n    assert sys.getsizeof(a) >= object.__sizeof__(a) + a.nbytes\n    a = pa.array([[[5, 6, 7]], [[9, 10]]], type=pa.list_(pa.list_(pa.int8())))\n    assert a.get_total_buffer_size() == (4 * 3) + (4 * 3) + (1 * 5)\n    assert a.nbytes == 21\n    a = pa.array([[[1, 2], [3, 4]], [[5, 6, 7], None, [8]], [[9, 10]]],\n                 type=pa.list_(pa.list_(pa.int8())))\n    a1 = a.slice(1, 2)\n    assert a1.nbytes == (4 * 2) + 1 + (4 * 4) + (1 * 6)\n    assert a1.get_total_buffer_size() == (4 * 4) + 1 + (4 * 7) + (1 * 10)\n\n\ndef test_nbytes_size():\n    a = pa.chunked_array([pa.array([1, None, 3], type=pa.int16()),\n                          pa.array([4, 5, 6], type=pa.int16())])\n    assert a.nbytes == 13\n\n\ndef test_invalid_tensor_constructor_repr():\n    # ARROW-2638: prevent calling extension class constructors directly\n    with pytest.raises(TypeError):\n        repr(pa.Tensor([1]))\n\n\ndef test_invalid_tensor_construction():\n    with pytest.raises(TypeError):\n        pa.Tensor()\n\n\n@pytest.mark.parametrize(('offset_type', 'list_type_factory'),\n                         [(pa.int32(), pa.list_), (pa.int64(), pa.large_list)])\ndef test_list_array_flatten(offset_type, list_type_factory):\n    typ2 = list_type_factory(\n        list_type_factory(\n            pa.int64()\n        )\n    )\n    arr2 = pa.array([\n        None,\n        [\n            [1, None, 2],\n            None,\n            [3, 4]\n        ],\n        [],\n        [\n            [],\n            [5, 6],\n            None\n        ],\n        [\n            [7, 8]\n        ]\n    ], type=typ2)\n    offsets2 = pa.array([0, 0, 3, 3, 6, 7], type=offset_type)\n\n    typ1 = list_type_factory(pa.int64())\n    arr1 = pa.array([\n        [1, None, 2],\n        None,\n        [3, 4],\n        [],\n        [5, 6],\n        None,\n        [7, 8]\n    ], type=typ1)\n    offsets1 = pa.array([0, 3, 3, 5, 5, 7, 7, 9], type=offset_type)\n\n    arr0 = pa.array([\n        1, None, 2,\n        3, 4,\n        5, 6,\n        7, 8\n    ], type=pa.int64())\n\n    assert arr2.flatten().equals(arr1)\n    assert arr2.offsets.equals(offsets2)\n    assert arr2.values.equals(arr1)\n    assert arr1.flatten().equals(arr0)\n    assert arr1.offsets.equals(offsets1)\n    assert arr1.values.equals(arr0)\n    assert arr2.flatten().flatten().equals(arr0)\n    assert arr2.values.values.equals(arr0)\n    assert arr2.flatten(True).equals(arr0)\n\n\n@pytest.mark.parametrize('list_type', [\n    pa.list_(pa.int32()),\n    pa.list_(pa.int32(), list_size=2),\n    pa.large_list(pa.int32())])\ndef test_list_value_parent_indices(list_type):\n    arr = pa.array(\n        [\n            [0, 1],\n            None,\n            [None, None],\n            [3, 4]\n        ], type=list_type)\n    expected = pa.array([0, 0, 2, 2, 3, 3], type=pa.int64())\n    assert arr.value_parent_indices().equals(expected)\n\n\n@pytest.mark.parametrize(('offset_type', 'list_type'),\n                         [(pa.int32(), pa.list_(pa.int32())),\n                          (pa.int32(), pa.list_(pa.int32(), list_size=2)),\n                          (pa.int64(), pa.large_list(pa.int32())),\n                          (pa.int32(), pa.list_view(pa.int32())),\n                          (pa.int64(), pa.large_list_view(pa.int32()))])\ndef test_list_value_lengths(offset_type, list_type):\n\n    # FixedSizeListArray needs fixed list sizes\n    if getattr(list_type, \"list_size\", None):\n        arr = pa.array(\n            [\n                [0, 1],\n                None,\n                [None, None],\n                [3, 4]\n            ], type=list_type)\n        expected = pa.array([2, None, 2, 2], type=offset_type)\n\n    # Otherwise create variable list sizes\n    else:\n        arr = pa.array(\n            [\n                [0, 1, 2],\n                None,\n                [],\n                [3, 4]\n            ], type=list_type)\n        expected = pa.array([3, None, 0, 2], type=offset_type)\n    assert arr.value_lengths().equals(expected)\n\n\n@pytest.mark.parametrize('list_type_factory', [pa.list_, pa.large_list])\ndef test_list_array_flatten_non_canonical(list_type_factory):\n    # Non-canonical list array (null elements backed by non-empty sublists)\n    typ = list_type_factory(pa.int64())\n    arr = pa.array([[1], [2, 3], [4, 5, 6]], type=typ)\n    buffers = arr.buffers()[:2]\n    buffers[0] = pa.py_buffer(b\"\\x05\")  # validity bitmap\n    arr = arr.from_buffers(arr.type, len(arr), buffers, children=[arr.values])\n    assert arr.to_pylist() == [[1], None, [4, 5, 6]]\n    assert arr.offsets.to_pylist() == [0, 1, 3, 6]\n\n    flattened = arr.flatten()\n    flattened.validate(full=True)\n    assert flattened.type == typ.value_type\n    assert flattened.to_pylist() == [1, 4, 5, 6]\n\n    # .values is the physical values array (including masked elements)\n    assert arr.values.to_pylist() == [1, 2, 3, 4, 5, 6]\n\n\n@pytest.mark.parametrize('klass', [pa.ListArray, pa.LargeListArray])\ndef test_list_array_values_offsets_sliced(klass):\n    # ARROW-7301\n    arr = klass.from_arrays(offsets=[0, 3, 4, 6], values=[1, 2, 3, 4, 5, 6])\n    assert arr.values.to_pylist() == [1, 2, 3, 4, 5, 6]\n    assert arr.offsets.to_pylist() == [0, 3, 4, 6]\n\n    # sliced -> values keeps referring to full values buffer, but offsets is\n    # sliced as well so the offsets correctly point into the full values array\n    # sliced -> flatten() will return the sliced value array.\n    arr2 = arr[1:]\n    assert arr2.values.to_pylist() == [1, 2, 3, 4, 5, 6]\n    assert arr2.offsets.to_pylist() == [3, 4, 6]\n    assert arr2.flatten().to_pylist() == [4, 5, 6]\n    i = arr2.offsets[0].as_py()\n    j = arr2.offsets[1].as_py()\n    assert arr2[0].as_py() == arr2.values[i:j].to_pylist() == [4]\n\n\ndef test_fixed_size_list_array_flatten():\n    typ2 = pa.list_(pa.list_(pa.int64(), 2), 3)\n    arr2 = pa.array([\n        [\n            [1, 2],\n            [3, 4],\n            [5, 6],\n        ],\n        None,\n        [\n            [7, None],\n            None,\n            [8, 9]\n        ],\n    ], type=typ2)\n    assert arr2.type.equals(typ2)\n\n    typ1 = pa.list_(pa.int64(), 2)\n    arr1 = pa.array([\n        [1, 2], [3, 4], [5, 6],\n        [7, None], None, [8, 9]\n    ], type=typ1)\n    assert arr1.type.equals(typ1)\n    assert arr2.flatten().equals(arr1)\n\n    typ0 = pa.int64()\n    arr0 = pa.array([\n        1, 2, 3, 4, 5, 6, 7, None, 8, 9,\n    ], type=typ0)\n    assert arr0.type.equals(typ0)\n    assert arr1.flatten().equals(arr0)\n    assert arr2.flatten().flatten().equals(arr0)\n    assert arr2.flatten().equals(arr1)\n    assert arr2.flatten(True).equals(arr0)\n\n\ndef test_fixed_size_list_array_flatten_with_slice():\n    array = pa.array([[1], [2], [3]],\n                     type=pa.list_(pa.float64(), list_size=1))\n    assert array[2:].flatten() == pa.array([3], type=pa.float64())\n\n\ndef test_map_array_values_offsets():\n    ty = pa.map_(pa.utf8(), pa.int32())\n    ty_values = pa.struct([pa.field(\"key\", pa.utf8(), nullable=False),\n                           pa.field(\"value\", pa.int32())])\n    a = pa.array([[('a', 1), ('b', 2)], [('c', 3)]], type=ty)\n\n    assert a.values.type.equals(ty_values)\n    assert a.values == pa.array([\n        {'key': 'a', 'value': 1},\n        {'key': 'b', 'value': 2},\n        {'key': 'c', 'value': 3},\n    ], type=ty_values)\n    assert a.keys.equals(pa.array(['a', 'b', 'c']))\n    assert a.items.equals(pa.array([1, 2, 3], type=pa.int32()))\n\n    assert pa.ListArray.from_arrays(a.offsets, a.keys).equals(\n        pa.array([['a', 'b'], ['c']]))\n    assert pa.ListArray.from_arrays(a.offsets, a.items).equals(\n        pa.array([[1, 2], [3]], type=pa.list_(pa.int32())))\n\n    with pytest.raises(NotImplementedError):\n        a.flatten()\n\n\ndef test_struct_array_flatten():\n    ty = pa.struct([pa.field('x', pa.int16()),\n                    pa.field('y', pa.float32())])\n    a = pa.array([(1, 2.5), (3, 4.5), (5, 6.5)], type=ty)\n    xs, ys = a.flatten()\n    assert xs.type == pa.int16()\n    assert ys.type == pa.float32()\n    assert xs.to_pylist() == [1, 3, 5]\n    assert ys.to_pylist() == [2.5, 4.5, 6.5]\n    xs, ys = a[1:].flatten()\n    assert xs.to_pylist() == [3, 5]\n    assert ys.to_pylist() == [4.5, 6.5]\n\n    a = pa.array([(1, 2.5), None, (3, 4.5)], type=ty)\n    xs, ys = a.flatten()\n    assert xs.to_pylist() == [1, None, 3]\n    assert ys.to_pylist() == [2.5, None, 4.5]\n    xs, ys = a[1:].flatten()\n    assert xs.to_pylist() == [None, 3]\n    assert ys.to_pylist() == [None, 4.5]\n\n    a = pa.array([(1, None), (2, 3.5), (None, 4.5)], type=ty)\n    xs, ys = a.flatten()\n    assert xs.to_pylist() == [1, 2, None]\n    assert ys.to_pylist() == [None, 3.5, 4.5]\n    xs, ys = a[1:].flatten()\n    assert xs.to_pylist() == [2, None]\n    assert ys.to_pylist() == [3.5, 4.5]\n\n    a = pa.array([(1, None), None, (None, 2.5)], type=ty)\n    xs, ys = a.flatten()\n    assert xs.to_pylist() == [1, None, None]\n    assert ys.to_pylist() == [None, None, 2.5]\n    xs, ys = a[1:].flatten()\n    assert xs.to_pylist() == [None, None]\n    assert ys.to_pylist() == [None, 2.5]\n\n\ndef test_struct_array_field():\n    ty = pa.struct([pa.field('x', pa.int16()),\n                    pa.field('y', pa.float32())])\n    a = pa.array([(1, 2.5), (3, 4.5), (5, 6.5)], type=ty)\n\n    x0 = a.field(0)\n    y0 = a.field(1)\n    x1 = a.field(-2)\n    y1 = a.field(-1)\n    x2 = a.field('x')\n    y2 = a.field('y')\n\n    assert isinstance(x0, pa.lib.Int16Array)\n    assert isinstance(y1, pa.lib.FloatArray)\n    assert x0.equals(pa.array([1, 3, 5], type=pa.int16()))\n    assert y0.equals(pa.array([2.5, 4.5, 6.5], type=pa.float32()))\n    assert x0.equals(x1)\n    assert x0.equals(x2)\n    assert y0.equals(y1)\n    assert y0.equals(y2)\n\n    for invalid_index in [None, pa.int16()]:\n        with pytest.raises(TypeError):\n            a.field(invalid_index)\n\n    for invalid_index in [3, -3]:\n        with pytest.raises(IndexError):\n            a.field(invalid_index)\n\n    for invalid_name in ['z', '']:\n        with pytest.raises(KeyError):\n            a.field(invalid_name)\n\n\ndef test_struct_array_flattened_field():\n    ty = pa.struct([pa.field('x', pa.int16()),\n                    pa.field('y', pa.float32())])\n    a = pa.array([(1, 2.5), (3, 4.5), (5, 6.5)], type=ty,\n                 mask=pa.array([False, True, False]))\n\n    x0 = a._flattened_field(0)\n    y0 = a._flattened_field(1)\n    x1 = a._flattened_field(-2)\n    y1 = a._flattened_field(-1)\n    x2 = a._flattened_field('x')\n    y2 = a._flattened_field('y')\n\n    assert isinstance(x0, pa.lib.Int16Array)\n    assert isinstance(y1, pa.lib.FloatArray)\n    assert x0.equals(pa.array([1, None, 5], type=pa.int16()))\n    assert y0.equals(pa.array([2.5, None, 6.5], type=pa.float32()))\n    assert x0.equals(x1)\n    assert x0.equals(x2)\n    assert y0.equals(y1)\n    assert y0.equals(y2)\n\n    for invalid_index in [None, pa.int16()]:\n        with pytest.raises(TypeError):\n            a._flattened_field(invalid_index)\n\n    for invalid_index in [3, -3]:\n        with pytest.raises(IndexError):\n            a._flattened_field(invalid_index)\n\n    for invalid_name in ['z', '']:\n        with pytest.raises(KeyError):\n            a._flattened_field(invalid_name)\n\n\ndef test_empty_cast():\n    types = [\n        pa.null(),\n        pa.bool_(),\n        pa.int8(),\n        pa.int16(),\n        pa.int32(),\n        pa.int64(),\n        pa.uint8(),\n        pa.uint16(),\n        pa.uint32(),\n        pa.uint64(),\n        pa.float16(),\n        pa.float32(),\n        pa.float64(),\n        pa.date32(),\n        pa.date64(),\n        pa.binary(),\n        pa.binary(length=4),\n        pa.string(),\n    ]\n\n    for (t1, t2) in itertools.product(types, types):\n        try:\n            # ARROW-4766: Ensure that supported types conversion don't segfault\n            # on empty arrays of common types\n            pa.array([], type=t1).cast(t2)\n        except (pa.lib.ArrowNotImplementedError, pa.ArrowInvalid):\n            continue\n\n\ndef test_nested_dictionary_array():\n    dict_arr = pa.DictionaryArray.from_arrays([0, 1, 0], ['a', 'b'])\n    list_arr = pa.ListArray.from_arrays([0, 2, 3], dict_arr)\n    assert list_arr.to_pylist() == [['a', 'b'], ['a']]\n\n    dict_arr = pa.DictionaryArray.from_arrays([0, 1, 0], ['a', 'b'])\n    dict_arr2 = pa.DictionaryArray.from_arrays([0, 1, 2, 1, 0], dict_arr)\n    assert dict_arr2.to_pylist() == ['a', 'b', 'a', 'b', 'a']\n\n\ndef test_array_from_numpy_str_utf8():\n    # ARROW-3890 -- in Python 3, NPY_UNICODE arrays are produced, but in Python\n    # 2 they are NPY_STRING (binary), so we must do UTF-8 validation\n    vec = np.array([\"toto\", \"tata\"])\n    vec2 = np.array([\"toto\", \"tata\"], dtype=object)\n\n    arr = pa.array(vec, pa.string())\n    arr2 = pa.array(vec2, pa.string())\n    expected = pa.array([\"toto\", \"tata\"])\n    assert arr.equals(expected)\n    assert arr2.equals(expected)\n\n    # with mask, separate code path\n    mask = np.array([False, False], dtype=bool)\n    arr = pa.array(vec, pa.string(), mask=mask)\n    assert arr.equals(expected)\n\n    # UTF8 validation failures\n    vec = np.array([('ma\u00f1ana').encode('utf-16-le')])\n    with pytest.raises(ValueError):\n        pa.array(vec, pa.string())\n\n    with pytest.raises(ValueError):\n        pa.array(vec, pa.string(), mask=np.array([False]))\n\n\n@pytest.mark.slow\n@pytest.mark.large_memory\ndef test_numpy_binary_overflow_to_chunked():\n    # ARROW-3762, ARROW-5966\n\n    # 2^31 + 1 bytes\n    values = [b'x']\n    unicode_values = ['x']\n\n    # Make 10 unique 1MB strings then repeat then 2048 times\n    unique_strings = {\n        i: b'x' * ((1 << 20) - 1) + str(i % 10).encode('utf8')\n        for i in range(10)\n    }\n    unicode_unique_strings = {i: x.decode('utf8')\n                              for i, x in unique_strings.items()}\n    values += [unique_strings[i % 10] for i in range(1 << 11)]\n    unicode_values += [unicode_unique_strings[i % 10]\n                       for i in range(1 << 11)]\n\n    for case, ex_type in [(values, pa.binary()),\n                          (unicode_values, pa.utf8())]:\n        arr = np.array(case)\n        arrow_arr = pa.array(arr)\n        arr = None\n\n        assert isinstance(arrow_arr, pa.ChunkedArray)\n        assert arrow_arr.type == ex_type\n\n        # Split up into 16MB chunks. 128 * 16 = 2048, so 129\n        assert arrow_arr.num_chunks == 129\n\n        value_index = 0\n        for i in range(arrow_arr.num_chunks):\n            chunk = arrow_arr.chunk(i)\n            for val in chunk:\n                assert val.as_py() == case[value_index]\n                value_index += 1\n\n\n@pytest.mark.large_memory\ndef test_list_child_overflow_to_chunked():\n    kilobyte_string = 'x' * 1024\n    two_mega = 2**21\n\n    vals = [[kilobyte_string]] * (two_mega - 1)\n    arr = pa.array(vals)\n    assert isinstance(arr, pa.Array)\n    assert len(arr) == two_mega - 1\n\n    vals = [[kilobyte_string]] * two_mega\n    arr = pa.array(vals)\n    assert isinstance(arr, pa.ChunkedArray)\n    assert len(arr) == two_mega\n    assert len(arr.chunk(0)) == two_mega - 1\n    assert len(arr.chunk(1)) == 1\n\n\ndef test_infer_type_masked():\n    # ARROW-5208\n    ty = pa.infer_type(['foo', 'bar', None, 2],\n                       mask=[False, False, False, True])\n    assert ty == pa.utf8()\n\n    # all masked\n    ty = pa.infer_type(['foo', 'bar', None, 2],\n                       mask=np.array([True, True, True, True]))\n    assert ty == pa.null()\n\n    # length 0\n    assert pa.infer_type([], mask=[]) == pa.null()\n\n\ndef test_array_masked():\n    # ARROW-5208\n    arr = pa.array([4, None, 4, 3.],\n                   mask=np.array([False, True, False, True]))\n    assert arr.type == pa.int64()\n\n    # ndarray dtype=object argument\n    arr = pa.array(np.array([4, None, 4, 3.], dtype=\"O\"),\n                   mask=np.array([False, True, False, True]))\n    assert arr.type == pa.int64()\n\n\ndef test_array_supported_masks():\n    # ARROW-13883\n    arr = pa.array([4, None, 4, 3.],\n                   mask=np.array([False, True, False, True]))\n    assert arr.to_pylist() == [4, None, 4, None]\n\n    arr = pa.array([4, None, 4, 3],\n                   mask=pa.array([False, True, False, True]))\n    assert arr.to_pylist() == [4, None, 4, None]\n\n    arr = pa.array([4, None, 4, 3],\n                   mask=[False, True, False, True])\n    assert arr.to_pylist() == [4, None, 4, None]\n\n    arr = pa.array([4, 3, None, 3],\n                   mask=[False, True, False, True])\n    assert arr.to_pylist() == [4, None, None, None]\n\n    # Non boolean values\n    with pytest.raises(pa.ArrowTypeError):\n        arr = pa.array([4, None, 4, 3],\n                       mask=pa.array([1.0, 2.0, 3.0, 4.0]))\n\n    with pytest.raises(pa.ArrowTypeError):\n        arr = pa.array([4, None, 4, 3],\n                       mask=[1.0, 2.0, 3.0, 4.0])\n\n    with pytest.raises(pa.ArrowTypeError):\n        arr = pa.array([4, None, 4, 3],\n                       mask=np.array([1.0, 2.0, 3.0, 4.0]))\n\n    with pytest.raises(pa.ArrowTypeError):\n        arr = pa.array([4, None, 4, 3],\n                       mask=pa.array([False, True, False, True],\n                                     mask=pa.array([True, True, True, True])))\n\n    with pytest.raises(pa.ArrowTypeError):\n        arr = pa.array([4, None, 4, 3],\n                       mask=pa.array([False, None, False, True]))\n\n    # Numpy arrays only accepts numpy masks\n    with pytest.raises(TypeError):\n        arr = pa.array(np.array([4, None, 4, 3.]),\n                       mask=[True, False, True, False])\n\n    with pytest.raises(TypeError):\n        arr = pa.array(np.array([4, None, 4, 3.]),\n                       mask=pa.array([True, False, True, False]))\n\n\n@pytest.mark.pandas\ndef test_array_supported_pandas_masks():\n    import pandas\n    arr = pa.array(pandas.Series([0, 1], name=\"a\", dtype=\"int64\"),\n                   mask=pandas.Series([True, False], dtype='bool'))\n    assert arr.to_pylist() == [None, 1]\n\n\ndef test_binary_array_masked():\n    # ARROW-12431\n    masked_basic = pa.array([b'\\x05'], type=pa.binary(1),\n                            mask=np.array([False]))\n    assert [b'\\x05'] == masked_basic.to_pylist()\n\n    # Fixed Length Binary\n    masked = pa.array(np.array([b'\\x05']), type=pa.binary(1),\n                      mask=np.array([False]))\n    assert [b'\\x05'] == masked.to_pylist()\n\n    masked_nulls = pa.array(np.array([b'\\x05']), type=pa.binary(1),\n                            mask=np.array([True]))\n    assert [None] == masked_nulls.to_pylist()\n\n    # Variable Length Binary\n    masked = pa.array(np.array([b'\\x05']), type=pa.binary(),\n                      mask=np.array([False]))\n    assert [b'\\x05'] == masked.to_pylist()\n\n    masked_nulls = pa.array(np.array([b'\\x05']), type=pa.binary(),\n                            mask=np.array([True]))\n    assert [None] == masked_nulls.to_pylist()\n\n    # Fixed Length Binary, copy\n    npa = np.array([b'aaa', b'bbb', b'ccc']*10)\n    arrow_array = pa.array(npa, type=pa.binary(3),\n                           mask=np.array([False, False, False]*10))\n    npa[npa == b\"bbb\"] = b\"XXX\"\n    assert ([b'aaa', b'bbb', b'ccc']*10) == arrow_array.to_pylist()\n\n\ndef test_binary_array_strided():\n    # Masked\n    nparray = np.array([b\"ab\", b\"cd\", b\"ef\"])\n    arrow_array = pa.array(nparray[::2], pa.binary(2),\n                           mask=np.array([False, False]))\n    assert [b\"ab\", b\"ef\"] == arrow_array.to_pylist()\n\n    # Unmasked\n    nparray = np.array([b\"ab\", b\"cd\", b\"ef\"])\n    arrow_array = pa.array(nparray[::2], pa.binary(2))\n    assert [b\"ab\", b\"ef\"] == arrow_array.to_pylist()\n\n\ndef test_array_invalid_mask_raises():\n    # ARROW-10742\n    cases = [\n        ([1, 2], np.array([False, False], dtype=\"O\"),\n         TypeError, \"must be boolean dtype\"),\n\n        ([1, 2], np.array([[False], [False]]),\n         pa.ArrowInvalid, \"must be 1D array\"),\n\n        ([1, 2, 3], np.array([False, False]),\n         pa.ArrowInvalid, \"different length\"),\n\n        (np.array([1, 2]), np.array([False, False], dtype=\"O\"),\n         TypeError, \"must be boolean dtype\"),\n\n        (np.array([1, 2]), np.array([[False], [False]]),\n         ValueError, \"must be 1D array\"),\n\n        (np.array([1, 2, 3]), np.array([False, False]),\n         ValueError, \"different length\"),\n    ]\n    for obj, mask, ex, msg in cases:\n        with pytest.raises(ex, match=msg):\n            pa.array(obj, mask=mask)\n\n\ndef test_array_from_large_pyints():\n    # ARROW-5430\n    with pytest.raises(OverflowError):\n        # too large for int64 so dtype must be explicitly provided\n        pa.array([int(2 ** 63)])\n\n\ndef test_numpy_array_protocol():\n    # test the __array__ method on pyarrow.Array\n    arr = pa.array([1, 2, 3])\n    result = np.asarray(arr)\n    expected = np.array([1, 2, 3], dtype=\"int64\")\n    np.testing.assert_array_equal(result, expected)\n\n    # this should not raise a deprecation warning with numpy 2.0+\n    result = np.array(arr, copy=False)\n    np.testing.assert_array_equal(result, expected)\n\n    result = np.array(arr, dtype=\"int64\", copy=False)\n    np.testing.assert_array_equal(result, expected)\n\n    # no zero-copy is possible\n    arr = pa.array([1, 2, None])\n    expected = np.array([1, 2, np.nan], dtype=\"float64\")\n    result = np.asarray(arr)\n    np.testing.assert_array_equal(result, expected)\n\n    if Version(np.__version__) < Version(\"2.0.0.dev0\"):\n        # copy keyword is not strict and not passed down to __array__\n        result = np.array(arr, copy=False)\n        np.testing.assert_array_equal(result, expected)\n\n        result = np.array(arr, dtype=\"float64\", copy=False)\n        np.testing.assert_array_equal(result, expected)\n    else:\n        # starting with numpy 2.0, the copy=False keyword is assumed to be strict\n        with pytest.raises(ValueError, match=\"Unable to avoid a copy\"):\n            np.array(arr, copy=False)\n\n        arr = pa.array([1, 2, 3])\n        with pytest.raises(ValueError):\n            np.array(arr, dtype=\"float64\", copy=False)\n\n    # copy=True -> not yet passed by numpy, so we have to call this directly to test\n    arr = pa.array([1, 2, 3])\n    result = arr.__array__(copy=True)\n    assert result.flags.writeable\n\n    arr = pa.array([1, 2, 3])\n    result = arr.__array__(dtype=np.dtype(\"float64\"), copy=True)\n    assert result.dtype == \"float64\"\n\n\ndef test_array_protocol():\n\n    class MyArray:\n        def __init__(self, data):\n            self.data = data\n\n        def __arrow_array__(self, type=None):\n            return pa.array(self.data, type=type)\n\n    arr = MyArray(np.array([1, 2, 3], dtype='int64'))\n    result = pa.array(arr)\n    expected = pa.array([1, 2, 3], type=pa.int64())\n    assert result.equals(expected)\n    result = pa.array(arr, type=pa.int64())\n    expected = pa.array([1, 2, 3], type=pa.int64())\n    assert result.equals(expected)\n    result = pa.array(arr, type=pa.float64())\n    expected = pa.array([1, 2, 3], type=pa.float64())\n    assert result.equals(expected)\n\n    # raise error when passing size or mask keywords\n    with pytest.raises(ValueError):\n        pa.array(arr, mask=np.array([True, False, True]))\n    with pytest.raises(ValueError):\n        pa.array(arr, size=3)\n\n    # ensure the return value is an Array\n    class MyArrayInvalid:\n        def __init__(self, data):\n            self.data = data\n\n        def __arrow_array__(self, type=None):\n            return np.array(self.data)\n\n    arr = MyArrayInvalid(np.array([1, 2, 3], dtype='int64'))\n    with pytest.raises(TypeError):\n        pa.array(arr)\n\n    # ARROW-7066 - allow ChunkedArray output\n    # GH-33727 - if num_chunks=1 return Array\n    class MyArray2:\n        def __init__(self, data):\n            self.data = data\n\n        def __arrow_array__(self, type=None):\n            return pa.chunked_array([self.data], type=type)\n\n    arr = MyArray2(np.array([1, 2, 3], dtype='int64'))\n    result = pa.array(arr)\n    expected = pa.array([1, 2, 3], type=pa.int64())\n    assert result.equals(expected)\n\n    class MyArray3:\n        def __init__(self, data1, data2):\n            self.data1 = data1\n            self.data2 = data2\n\n        def __arrow_array__(self, type=None):\n            return pa.chunked_array([self.data1, self.data2], type=type)\n\n    np_arr = np.array([1, 2, 3], dtype='int64')\n    arr = MyArray3(np_arr, np_arr)\n    result = pa.array(arr)\n    expected = pa.chunked_array([[1, 2, 3], [1, 2, 3]], type=pa.int64())\n    assert result.equals(expected)\n\n\ndef test_c_array_protocol():\n    class ArrayWrapper:\n        def __init__(self, data):\n            self.data = data\n\n        def __arrow_c_array__(self, requested_schema=None):\n            return self.data.__arrow_c_array__(requested_schema)\n\n    # Can roundtrip through the C array protocol\n    arr = ArrayWrapper(pa.array([1, 2, 3], type=pa.int64()))\n    result = pa.array(arr)\n    assert result == arr.data\n\n    # Will cast to requested type\n    result = pa.array(arr, type=pa.int32())\n    assert result == pa.array([1, 2, 3], type=pa.int32())\n\n\ndef test_concat_array():\n    concatenated = pa.concat_arrays(\n        [pa.array([1, 2]), pa.array([3, 4])])\n    assert concatenated.equals(pa.array([1, 2, 3, 4]))\n\n\ndef test_concat_array_different_types():\n    with pytest.raises(pa.ArrowInvalid):\n        pa.concat_arrays([pa.array([1]), pa.array([2.])])\n\n\ndef test_concat_array_invalid_type():\n    # ARROW-9920 - do not segfault on non-array input\n\n    with pytest.raises(TypeError, match=\"should contain Array objects\"):\n        pa.concat_arrays([None])\n\n    arr = pa.chunked_array([[0, 1], [3, 4]])\n    with pytest.raises(TypeError, match=\"should contain Array objects\"):\n        pa.concat_arrays(arr)\n\n\n@pytest.mark.pandas\ndef test_to_pandas_timezone():\n    # https://issues.apache.org/jira/browse/ARROW-6652\n    arr = pa.array([1, 2, 3], type=pa.timestamp('s', tz='Europe/Brussels'))\n    s = arr.to_pandas()\n    assert s.dt.tz is not None\n    arr = pa.chunked_array([arr])\n    s = arr.to_pandas()\n    assert s.dt.tz is not None\n\n\n@pytest.mark.pandas\ndef test_to_pandas_float16_list():\n    # https://github.com/apache/arrow/issues/36168\n    expected = [[np.float16(1)], [np.float16(2)], [np.float16(3)]]\n    arr = pa.array(expected)\n    result = arr.to_pandas()\n    assert result[0].dtype == \"float16\"\n    assert result.tolist() == expected\n\n\ndef test_array_sort():\n    arr = pa.array([5, 7, 35], type=pa.int64())\n    sorted_arr = arr.sort(\"descending\")\n    assert sorted_arr.to_pylist() == [35, 7, 5]\n\n    arr = pa.chunked_array([[1, 2, 3], [4, 5, 6]])\n    sorted_arr = arr.sort(\"descending\")\n    assert sorted_arr.to_pylist() == [6, 5, 4, 3, 2, 1]\n\n    arr = pa.array([5, 7, 35, None], type=pa.int64())\n    sorted_arr = arr.sort(\"descending\", null_placement=\"at_end\")\n    assert sorted_arr.to_pylist() == [35, 7, 5, None]\n    sorted_arr = arr.sort(\"descending\", null_placement=\"at_start\")\n    assert sorted_arr.to_pylist() == [None, 35, 7, 5]\n\n\ndef test_struct_array_sort():\n    arr = pa.StructArray.from_arrays([\n        pa.array([5, 7, 7, 35], type=pa.int64()),\n        pa.array([\"foo\", \"car\", \"bar\", \"foobar\"])\n    ], names=[\"a\", \"b\"])\n\n    sorted_arr = arr.sort(\"descending\", by=\"a\")\n    assert sorted_arr.to_pylist() == [\n        {\"a\": 35, \"b\": \"foobar\"},\n        {\"a\": 7, \"b\": \"car\"},\n        {\"a\": 7, \"b\": \"bar\"},\n        {\"a\": 5, \"b\": \"foo\"},\n    ]\n\n    sorted_arr = arr.sort()\n    assert sorted_arr.to_pylist() == [\n        {\"a\": 5, \"b\": \"foo\"},\n        {\"a\": 7, \"b\": \"bar\"},\n        {\"a\": 7, \"b\": \"car\"},\n        {\"a\": 35, \"b\": \"foobar\"},\n    ]\n\n    arr_with_nulls = pa.StructArray.from_arrays([\n        pa.array([5, 7, 7, 35], type=pa.int64()),\n        pa.array([\"foo\", \"car\", \"bar\", \"foobar\"])\n    ], names=[\"a\", \"b\"], mask=pa.array([False, False, True, False]))\n\n    sorted_arr = arr_with_nulls.sort(\n        \"descending\", by=\"a\", null_placement=\"at_start\")\n    assert sorted_arr.to_pylist() == [\n        None,\n        {\"a\": 35, \"b\": \"foobar\"},\n        {\"a\": 7, \"b\": \"car\"},\n        {\"a\": 5, \"b\": \"foo\"},\n    ]\n\n    sorted_arr = arr_with_nulls.sort(\n        \"descending\", by=\"a\", null_placement=\"at_end\")\n    assert sorted_arr.to_pylist() == [\n        {\"a\": 35, \"b\": \"foobar\"},\n        {\"a\": 7, \"b\": \"car\"},\n        {\"a\": 5, \"b\": \"foo\"},\n        None\n    ]\n\n\ndef test_array_accepts_pyarrow_array():\n    arr = pa.array([1, 2, 3])\n    result = pa.array(arr)\n    assert arr == result\n\n    # Test casting to a different type\n    result = pa.array(arr, type=pa.uint8())\n    expected = pa.array([1, 2, 3], type=pa.uint8())\n    assert expected == result\n    assert expected.type == pa.uint8()\n\n    # Test casting with safe keyword\n    arr = pa.array([2 ** 63 - 1], type=pa.int64())\n\n    with pytest.raises(pa.ArrowInvalid):\n        pa.array(arr, type=pa.int32())\n\n    expected = pa.array([-1], type=pa.int32())\n    result = pa.array(arr, type=pa.int32(), safe=False)\n    assert result == expected\n\n    # Test memory_pool keyword is accepted\n    result = pa.array(arr, memory_pool=pa.default_memory_pool())\n    assert arr == result\n\n\ndef check_run_end_encoded(ree_array, run_ends, values, logical_length, physical_length,\n                          physical_offset):\n    assert ree_array.run_ends.to_pylist() == run_ends\n    assert ree_array.values.to_pylist() == values\n    assert len(ree_array) == logical_length\n    assert ree_array.find_physical_length() == physical_length\n    assert ree_array.find_physical_offset() == physical_offset\n\n\ndef check_run_end_encoded_from_arrays_with_type(ree_type=None):\n    run_ends = [3, 5, 10, 19]\n    values = [1, 2, 1, 3]\n    ree_array = pa.RunEndEncodedArray.from_arrays(run_ends, values, ree_type)\n    check_run_end_encoded(ree_array, run_ends, values, 19, 4, 0)\n\n\ndef check_run_end_encoded_from_typed_arrays(ree_type):\n    run_ends = [3, 5, 10, 19]\n    values = [1, 2, 1, 3]\n    typed_run_ends = pa.array(run_ends, ree_type.run_end_type)\n    typed_values = pa.array(values, ree_type.value_type)\n    ree_array = pa.RunEndEncodedArray.from_arrays(typed_run_ends, typed_values)\n    assert ree_array.type == ree_type\n    check_run_end_encoded(ree_array, run_ends, values, 19, 4, 0)\n\n\ndef test_run_end_encoded_from_arrays():\n    check_run_end_encoded_from_arrays_with_type()\n    for run_end_type in [pa.int16(), pa.int32(), pa.int64()]:\n        for value_type in [pa.uint32(), pa.int32(), pa.uint64(), pa.int64()]:\n            ree_type = pa.run_end_encoded(run_end_type, value_type)\n            check_run_end_encoded_from_arrays_with_type(ree_type)\n            check_run_end_encoded_from_typed_arrays(ree_type)\n\n\ndef test_run_end_encoded_from_buffers():\n    run_ends = [3, 5, 10, 19]\n    values = [1, 2, 1, 3]\n\n    ree_type = pa.run_end_encoded(run_end_type=pa.int32(), value_type=pa.uint8())\n    length = 19\n    buffers = [None]\n    null_count = 0\n    offset = 0\n    children = [run_ends, values]\n\n    ree_array = pa.RunEndEncodedArray.from_buffers(ree_type, length, buffers,\n                                                   null_count, offset,\n                                                   children)\n    check_run_end_encoded(ree_array, run_ends, values, 19, 4, 0)\n    # buffers = []\n    ree_array = pa.RunEndEncodedArray.from_buffers(ree_type, length, [],\n                                                   null_count, offset,\n                                                   children)\n    check_run_end_encoded(ree_array, run_ends, values, 19, 4, 0)\n    # null_count = -1\n    ree_array = pa.RunEndEncodedArray.from_buffers(ree_type, length, buffers,\n                                                   -1, offset,\n                                                   children)\n    check_run_end_encoded(ree_array, run_ends, values, 19, 4, 0)\n    # offset = 4\n    ree_array = pa.RunEndEncodedArray.from_buffers(ree_type, length - 4, buffers,\n                                                   null_count, 4, children)\n    check_run_end_encoded(ree_array, run_ends, values, length - 4, 3, 1)\n    # buffers = [None, None]\n    with pytest.raises(ValueError):\n        pa.RunEndEncodedArray.from_buffers(ree_type, length, [None, None],\n                                           null_count, offset, children)\n    # children = None\n    with pytest.raises(ValueError):\n        pa.RunEndEncodedArray.from_buffers(ree_type, length, buffers,\n                                           null_count, offset, None)\n    # len(children) == 1\n    with pytest.raises(ValueError):\n        pa.RunEndEncodedArray.from_buffers(ree_type, length, buffers,\n                                           null_count, offset, [run_ends])\n    # null_count = 1\n    with pytest.raises(ValueError):\n        pa.RunEndEncodedArray.from_buffers(ree_type, length, buffers,\n                                           1, offset, children)\n\n\ndef test_run_end_encoded_from_array_with_type():\n    run_ends = [1, 3, 6]\n    values = [1, 2, 3]\n    ree_type = pa.run_end_encoded(pa.int32(), pa.int64())\n    expected = pa.RunEndEncodedArray.from_arrays(run_ends, values,\n                                                 ree_type)\n\n    arr = [1, 2, 2, 3, 3, 3]\n    result = pa.array(arr, type=ree_type)\n    assert result.equals(expected)\n    result = pa.array(np.array(arr), type=ree_type)\n    assert result.equals(expected)\n\n    ree_type_2 = pa.run_end_encoded(pa.int16(), pa.float32())\n    result = pa.array(arr, type=ree_type_2)\n    assert not result.equals(expected)\n    expected_2 = pa.RunEndEncodedArray.from_arrays(run_ends, values,\n                                                   ree_type_2)\n    assert result.equals(expected_2)\n\n    run_ends = [1, 3, 5, 6]\n    values = [1, 2, 3, None]\n    expected = pa.RunEndEncodedArray.from_arrays(run_ends, values,\n                                                 ree_type)\n\n    arr = [1, 2, 2, 3, 3, None]\n    result = pa.array(arr, type=ree_type)\n    assert result.equals(expected)\n\n    run_ends = [1, 3, 4, 5, 6]\n    values = [1, 2, None, 3, None]\n    expected = pa.RunEndEncodedArray.from_arrays(run_ends, values,\n                                                 ree_type)\n\n    mask = pa.array([False, False, False, True, False, True])\n    result = pa.array(arr, type=ree_type, mask=mask)\n    assert result.equals(expected)\n\n\ndef test_run_end_encoded_to_numpy():\n    arr = [1, 2, 2, 3, 3, 3]\n    ree_array = pa.array(arr, pa.run_end_encoded(pa.int32(), pa.int64()))\n    expected = np.array(arr)\n\n    np.testing.assert_array_equal(ree_array.to_numpy(zero_copy_only=False), expected)\n\n    with pytest.raises(pa.ArrowInvalid):\n        ree_array.to_numpy()\n\n\n@pytest.mark.pandas\ndef test_run_end_encoded_to_pandas():\n    arr = [1, 2, 2, 3, 3, 3]\n    ree_array = pa.array(arr, pa.run_end_encoded(pa.int32(), pa.int64()))\n\n    assert ree_array.to_pandas().tolist() == arr\n\n    with pytest.raises(pa.ArrowInvalid):\n        ree_array.to_pandas(zero_copy_only=True)\n\n\n@pytest.mark.parametrize(('list_array_type', 'list_type_factory'),\n                         [(pa.ListViewArray, pa.list_view),\n                          (pa.LargeListViewArray, pa.large_list_view)])\ndef test_list_view_from_arrays(list_array_type, list_type_factory):\n    # test in order offsets, similar to ListArray representation\n    values = [1, 2, 3, 4, 5, 6, None, 7]\n    offsets = [0, 2, 4, 6]\n    sizes = [2, 2, 2, 2]\n    array = list_array_type.from_arrays(offsets, sizes, values)\n\n    assert array.to_pylist() == [[1, 2], [3, 4], [5, 6], [None, 7]]\n    assert array.values.to_pylist() == values\n    assert array.offsets.to_pylist() == offsets\n    assert array.sizes.to_pylist() == sizes\n\n    # with specified type\n    typ = list_type_factory(pa.field(\"name\", pa.int64()))\n    result = list_array_type.from_arrays(offsets, sizes, values, typ)\n    assert result.type == typ\n    assert result.type.value_field.name == \"name\"\n\n    # with mismatching type\n    typ = list_type_factory(pa.binary())\n    with pytest.raises(TypeError):\n        list_array_type.from_arrays(offsets, sizes, values, type=typ)\n\n    # test out of order offsets with overlapping values\n    values = [1, 2, 3, 4]\n    offsets = [2, 1, 0]\n    sizes = [2, 2, 2]\n    array = list_array_type.from_arrays(offsets, sizes, values)\n\n    assert array.to_pylist() == [[3, 4], [2, 3], [1, 2]]\n    assert array.values.to_pylist() == values\n    assert array.offsets.to_pylist() == offsets\n    assert array.sizes.to_pylist() == sizes\n\n    # test null offsets and empty list values\n    values = []\n    offsets = [0, None]\n    sizes = [0, 0]\n    array = list_array_type.from_arrays(offsets, sizes, values)\n\n    assert array.to_pylist() == [[], None]\n    assert array.values.to_pylist() == values\n    assert array.offsets.to_pylist() == [0, 0]\n    assert array.sizes.to_pylist() == sizes\n\n    # test null sizes and empty list values\n    values = []\n    offsets = [0, 0]\n    sizes = [None, 0]\n    array = list_array_type.from_arrays(offsets, sizes, values)\n\n    assert array.to_pylist() == [None, []]\n    assert array.values.to_pylist() == values\n    assert array.offsets.to_pylist() == offsets\n    assert array.sizes.to_pylist() == [0, 0]\n\n    # test null bitmask\n    values = [1, 2]\n    offsets = [0, 0, 1]\n    sizes = [1, 0, 1]\n    mask = pa.array([False, True, False])\n    array = list_array_type.from_arrays(offsets, sizes, values, mask=mask)\n\n    assert array.to_pylist() == [[1], None, [2]]\n    assert array.values.to_pylist() == values\n    assert array.offsets.to_pylist() == offsets\n    assert array.sizes.to_pylist() == sizes\n\n\n@pytest.mark.parametrize(('list_array_type', 'list_type_factory'),\n                         [(pa.ListViewArray, pa.list_view),\n                          (pa.LargeListViewArray, pa.large_list_view)])\ndef test_list_view_from_arrays_fails(list_array_type, list_type_factory):\n    values = [1, 2]\n    offsets = [0, 1, None]\n    sizes = [1, 1, 0]\n    mask = pa.array([False, False, True])\n\n    # Ambiguous to specify both validity map and offsets or sizes with nulls\n    with pytest.raises(pa.lib.ArrowInvalid):\n        list_array_type.from_arrays(offsets, sizes, values, mask=mask)\n\n    offsets = [0, 1, 1]\n    array = list_array_type.from_arrays(offsets, sizes, values, mask=mask)\n    array_slice = array[1:]\n\n    # List offsets and sizes must not be slices if a validity map is specified\n    with pytest.raises(pa.lib.ArrowInvalid):\n        list_array_type.from_arrays(\n            array_slice.offsets, array_slice.sizes,\n            array_slice.values, mask=array_slice.is_null())\n\n\n@pytest.mark.parametrize(('list_array_type', 'list_type_factory', 'offset_type'),\n                         [(pa.ListViewArray, pa.list_view, pa.int32()),\n                          (pa.LargeListViewArray, pa.large_list_view, pa.int64())])\ndef test_list_view_flatten(list_array_type, list_type_factory, offset_type):\n    arr0 = pa.array([\n        1, None, 2,\n        3, 4,\n        5, 6,\n        7, 8\n    ], type=pa.int64())\n\n    typ1 = list_type_factory(pa.int64())\n    arr1 = pa.array([\n        [1, None, 2],\n        None,\n        [3, 4],\n        [],\n        [5, 6],\n        None,\n        [7, 8]\n    ], type=typ1)\n    offsets1 = pa.array([0, 3, 3, 5, 5, 7, 7], type=offset_type)\n    sizes1 = pa.array([3, 0, 2, 0, 2, 0, 2], type=offset_type)\n\n    typ2 = list_type_factory(\n        list_type_factory(\n            pa.int64()\n        )\n    )\n    arr2 = pa.array([\n        None,\n        [\n            [1, None, 2],\n            None,\n            [3, 4]\n        ],\n        [],\n        [\n            [],\n            [5, 6],\n            None\n        ],\n        [\n            [7, 8]\n        ]\n    ], type=typ2)\n    offsets2 = pa.array([0, 0, 3, 3, 6], type=offset_type)\n    sizes2 = pa.array([0, 3, 0, 3, 1], type=offset_type)\n\n    assert arr1.flatten().equals(arr0)\n    assert arr1.offsets.equals(offsets1)\n    assert arr1.sizes.equals(sizes1)\n    assert arr1.values.equals(arr0)\n    assert arr2.flatten().equals(arr1)\n    assert arr2.offsets.equals(offsets2)\n    assert arr2.sizes.equals(sizes2)\n    assert arr2.values.equals(arr1)\n    assert arr2.flatten().flatten().equals(arr0)\n    assert arr2.values.values.equals(arr0)\n    assert arr2.flatten(True).equals(arr0)\n\n    # test out of order offsets\n    values = [1, 2, 3, 4]\n    offsets = [3, 2, 1, 0]\n    sizes = [1, 1, 1, 1]\n    array = list_array_type.from_arrays(offsets, sizes, values)\n\n    assert array.flatten().to_pylist() == [4, 3, 2, 1]\n\n    # test null elements backed by non-empty sublists\n    mask = pa.array([False, False, False, True])\n    array = list_array_type.from_arrays(offsets, sizes, values, mask=mask)\n\n    assert array.flatten().to_pylist() == [4, 3, 2]\n    assert array.values.to_pylist() == [1, 2, 3, 4]\n\n\n@pytest.mark.parametrize('list_view_type', [pa.ListViewArray, pa.LargeListViewArray])\ndef test_list_view_slice(list_view_type):\n    # sliced -> values keeps referring to full values buffer, but offsets is\n    # sliced as well so the offsets correctly point into the full values array\n    # sliced -> flatten() will return the sliced value array.\n\n    array = list_view_type.from_arrays(offsets=[0, 3, 4], sizes=[\n                                       3, 1, 2], values=[1, 2, 3, 4, 5, 6])\n    sliced_array = array[1:]\n\n    assert sliced_array.values.to_pylist() == [1, 2, 3, 4, 5, 6]\n    assert sliced_array.offsets.to_pylist() == [3, 4]\n    assert sliced_array.flatten().to_pylist() == [4, 5, 6]\n\n    i = sliced_array.offsets[0].as_py()\n    j = sliced_array.offsets[1].as_py()\n\n    assert sliced_array[0].as_py() == sliced_array.values[i:j].to_pylist() == [4]\n\n\n@pytest.mark.parametrize('numpy_native_dtype', ['u2', 'i4', 'f8'])\ndef test_swapped_byte_order_fails(numpy_native_dtype):\n    # ARROW-39129\n\n    numpy_swapped_dtype = np.dtype(numpy_native_dtype).newbyteorder()\n    np_arr = np.arange(10, dtype=numpy_swapped_dtype)\n\n    # Primitive type array, type is inferred from the numpy array\n    with pytest.raises(pa.ArrowNotImplementedError):\n        pa.array(np_arr)\n\n    # Primitive type array, type is explicitly provided\n    with pytest.raises(pa.ArrowNotImplementedError):\n        pa.array(np_arr, type=pa.float64())\n\n    # List type array\n    with pytest.raises(pa.ArrowNotImplementedError):\n        pa.array([np_arr])\n\n    # Struct type array\n    with pytest.raises(pa.ArrowNotImplementedError):\n        pa.StructArray.from_arrays([np_arr], names=['a'])\n", "python/pyarrow/tests/test_tensor.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport sys\nimport pytest\nimport warnings\nimport weakref\n\nimport numpy as np\nimport pyarrow as pa\n\n\ntensor_type_pairs = [\n    ('i1', pa.int8()),\n    ('i2', pa.int16()),\n    ('i4', pa.int32()),\n    ('i8', pa.int64()),\n    ('u1', pa.uint8()),\n    ('u2', pa.uint16()),\n    ('u4', pa.uint32()),\n    ('u8', pa.uint64()),\n    ('f2', pa.float16()),\n    ('f4', pa.float32()),\n    ('f8', pa.float64())\n]\n\n\ndef test_tensor_attrs():\n    data = np.random.randn(10, 4)\n\n    tensor = pa.Tensor.from_numpy(data)\n\n    assert tensor.ndim == 2\n    assert tensor.dim_names == []\n    assert tensor.size == 40\n    assert tensor.shape == data.shape\n    assert tensor.strides == data.strides\n\n    assert tensor.is_contiguous\n    assert tensor.is_mutable\n\n    # not writeable\n    data2 = data.copy()\n    data2.flags.writeable = False\n    tensor = pa.Tensor.from_numpy(data2)\n    assert not tensor.is_mutable\n\n    # With dim_names\n    tensor = pa.Tensor.from_numpy(data, dim_names=('x', 'y'))\n    assert tensor.ndim == 2\n    assert tensor.dim_names == ['x', 'y']\n    assert tensor.dim_name(0) == 'x'\n    assert tensor.dim_name(1) == 'y'\n\n    wr = weakref.ref(tensor)\n    assert wr() is not None\n    del tensor\n    assert wr() is None\n\n\ndef test_tensor_base_object():\n    tensor = pa.Tensor.from_numpy(np.random.randn(10, 4))\n    n = sys.getrefcount(tensor)\n    array = tensor.to_numpy()  # noqa\n    assert sys.getrefcount(tensor) == n + 1\n\n\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_tensor_numpy_roundtrip(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    # Casting np.float64 -> uint32 or uint64 throws a RuntimeWarning\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        data = (100 * np.random.randn(10, 4)).astype(dtype)\n    tensor = pa.Tensor.from_numpy(data)\n    assert tensor.type == arrow_type\n\n    repr(tensor)\n\n    result = tensor.to_numpy()\n    assert (data == result).all()\n\n\ndef test_tensor_ipc_roundtrip(tmpdir):\n    data = np.random.randn(10, 4)\n    tensor = pa.Tensor.from_numpy(data)\n\n    path = os.path.join(str(tmpdir), 'pyarrow-tensor-ipc-roundtrip')\n    mmap = pa.create_memory_map(path, 1024)\n\n    pa.ipc.write_tensor(tensor, mmap)\n\n    mmap.seek(0)\n    result = pa.ipc.read_tensor(mmap)\n\n    assert result.equals(tensor)\n\n\n@pytest.mark.gzip\ndef test_tensor_ipc_read_from_compressed(tempdir):\n    # ARROW-5910\n    data = np.random.randn(10, 4)\n    tensor = pa.Tensor.from_numpy(data)\n\n    path = tempdir / 'tensor-compressed-file'\n\n    out_stream = pa.output_stream(path, compression='gzip')\n    pa.ipc.write_tensor(tensor, out_stream)\n    out_stream.close()\n\n    result = pa.ipc.read_tensor(pa.input_stream(path, compression='gzip'))\n    assert result.equals(tensor)\n\n\ndef test_tensor_ipc_strided(tmpdir):\n    data1 = np.random.randn(10, 4)\n    tensor1 = pa.Tensor.from_numpy(data1[::2])\n\n    data2 = np.random.randn(10, 6, 4)\n    tensor2 = pa.Tensor.from_numpy(data2[::, ::2, ::])\n\n    path = os.path.join(str(tmpdir), 'pyarrow-tensor-ipc-strided')\n    mmap = pa.create_memory_map(path, 2048)\n\n    for tensor in [tensor1, tensor2]:\n        mmap.seek(0)\n        pa.ipc.write_tensor(tensor, mmap)\n\n        mmap.seek(0)\n        result = pa.ipc.read_tensor(mmap)\n\n        assert result.equals(tensor)\n\n\ndef test_tensor_equals():\n    def eq(a, b):\n        assert a.equals(b)\n        assert a == b\n        assert not (a != b)\n\n    def ne(a, b):\n        assert not a.equals(b)\n        assert not (a == b)\n        assert a != b\n\n    data = np.random.randn(10, 6, 4)[::, ::2, ::]\n    tensor1 = pa.Tensor.from_numpy(data)\n    tensor2 = pa.Tensor.from_numpy(np.ascontiguousarray(data))\n    eq(tensor1, tensor2)\n    data = data.copy()\n    data[9, 0, 0] = 1.0\n    tensor2 = pa.Tensor.from_numpy(np.ascontiguousarray(data))\n    ne(tensor1, tensor2)\n\n\ndef test_tensor_hashing():\n    # Tensors are unhashable\n    with pytest.raises(TypeError, match=\"unhashable\"):\n        hash(pa.Tensor.from_numpy(np.arange(10)))\n\n\ndef test_tensor_size():\n    data = np.random.randn(10, 4)\n    tensor = pa.Tensor.from_numpy(data)\n    assert pa.ipc.get_tensor_size(tensor) > (data.size * 8)\n\n\ndef test_read_tensor(tmpdir):\n    # Create and write tensor tensor\n    data = np.random.randn(10, 4)\n    tensor = pa.Tensor.from_numpy(data)\n    data_size = pa.ipc.get_tensor_size(tensor)\n    path = os.path.join(str(tmpdir), 'pyarrow-tensor-ipc-read-tensor')\n    write_mmap = pa.create_memory_map(path, data_size)\n    pa.ipc.write_tensor(tensor, write_mmap)\n    # Try to read tensor\n    read_mmap = pa.memory_map(path, mode='r')\n    array = pa.ipc.read_tensor(read_mmap).to_numpy()\n    np.testing.assert_equal(data, array)\n\n\ndef test_tensor_memoryview():\n    # Tensors support the PEP 3118 buffer protocol\n    for dtype, expected_format in [(np.int8, '=b'),\n                                   (np.int64, '=q'),\n                                   (np.uint64, '=Q'),\n                                   (np.float16, 'e'),\n                                   (np.float64, 'd'),\n                                   ]:\n        data = np.arange(10, dtype=dtype)\n        dtype = data.dtype\n        lst = data.tolist()\n        tensor = pa.Tensor.from_numpy(data)\n        m = memoryview(tensor)\n        assert m.format == expected_format\n        assert m.shape == data.shape\n        assert m.strides == data.strides\n        assert m.ndim == 1\n        assert m.nbytes == data.nbytes\n        assert m.itemsize == data.itemsize\n        assert m.itemsize * 8 == tensor.type.bit_width\n        assert np.frombuffer(m, dtype).tolist() == lst\n        del tensor, data\n        assert np.frombuffer(m, dtype).tolist() == lst\n", "python/pyarrow/tests/test_deprecations.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Check that various deprecation warnings are raised\n\n# flake8: noqa\n\nimport pyarrow as pa\nimport pytest\n", "python/pyarrow/tests/test_cuda_numba_interop.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\nimport pyarrow as pa\nimport numpy as np\n\ndtypes = ['uint8', 'int16', 'float32']\ncuda = pytest.importorskip(\"pyarrow.cuda\")\nnb_cuda = pytest.importorskip(\"numba.cuda\")\n\nfrom numba.cuda.cudadrv.devicearray import DeviceNDArray  # noqa: E402\n\n\ncontext_choices = None\ncontext_choice_ids = ['pyarrow.cuda', 'numba.cuda']\n\n\ndef setup_module(module):\n    np.random.seed(1234)\n    ctx1 = cuda.Context()\n    nb_ctx1 = ctx1.to_numba()\n    nb_ctx2 = nb_cuda.current_context()\n    ctx2 = cuda.Context.from_numba(nb_ctx2)\n    module.context_choices = [(ctx1, nb_ctx1), (ctx2, nb_ctx2)]\n\n\ndef teardown_module(module):\n    del module.context_choices\n\n\n@pytest.mark.parametrize(\"c\", range(len(context_choice_ids)),\n                         ids=context_choice_ids)\ndef test_context(c):\n    ctx, nb_ctx = context_choices[c]\n    assert ctx.handle == nb_ctx.handle.value\n    assert ctx.handle == ctx.to_numba().handle.value\n    ctx2 = cuda.Context.from_numba(nb_ctx)\n    assert ctx.handle == ctx2.handle\n    size = 10\n    buf = ctx.new_buffer(size)\n    assert ctx.handle == buf.context.handle\n\n\ndef make_random_buffer(size, target='host', dtype='uint8', ctx=None):\n    \"\"\"Return a host or device buffer with random data.\n    \"\"\"\n    dtype = np.dtype(dtype)\n    if target == 'host':\n        assert size >= 0\n        buf = pa.allocate_buffer(size*dtype.itemsize)\n        arr = np.frombuffer(buf, dtype=dtype)\n        arr[:] = np.random.randint(low=0, high=255, size=size,\n                                   dtype=np.uint8)\n        return arr, buf\n    elif target == 'device':\n        arr, buf = make_random_buffer(size, target='host', dtype=dtype)\n        dbuf = ctx.new_buffer(size * dtype.itemsize)\n        dbuf.copy_from_host(buf, position=0, nbytes=buf.size)\n        return arr, dbuf\n    raise ValueError('invalid target value')\n\n\n@pytest.mark.parametrize(\"c\", range(len(context_choice_ids)),\n                         ids=context_choice_ids)\n@pytest.mark.parametrize(\"dtype\", dtypes, ids=dtypes)\n@pytest.mark.parametrize(\"size\", [0, 1, 8, 1000])\ndef test_from_object(c, dtype, size):\n    ctx, nb_ctx = context_choices[c]\n    arr, cbuf = make_random_buffer(size, target='device', dtype=dtype, ctx=ctx)\n\n    # Creating device buffer from numba DeviceNDArray:\n    darr = nb_cuda.to_device(arr)\n    cbuf2 = ctx.buffer_from_object(darr)\n    assert cbuf2.size == cbuf.size\n    arr2 = np.frombuffer(cbuf2.copy_to_host(), dtype=dtype)\n    np.testing.assert_equal(arr, arr2)\n\n    # Creating device buffer from a slice of numba DeviceNDArray:\n    if size >= 8:\n        # 1-D arrays\n        for s in [slice(size//4, None, None),\n                  slice(size//4, -(size//4), None)]:\n            cbuf2 = ctx.buffer_from_object(darr[s])\n            arr2 = np.frombuffer(cbuf2.copy_to_host(), dtype=dtype)\n            np.testing.assert_equal(arr[s], arr2)\n\n        # cannot test negative strides due to numba bug, see its issue 3705\n        if 0:\n            rdarr = darr[::-1]\n            cbuf2 = ctx.buffer_from_object(rdarr)\n            assert cbuf2.size == cbuf.size\n            arr2 = np.frombuffer(cbuf2.copy_to_host(), dtype=dtype)\n            np.testing.assert_equal(arr, arr2)\n\n        with pytest.raises(ValueError,\n                           match=('array data is non-contiguous')):\n            ctx.buffer_from_object(darr[::2])\n\n        # a rectangular 2-D array\n        s1 = size//4\n        s2 = size//s1\n        assert s1 * s2 == size\n        cbuf2 = ctx.buffer_from_object(darr.reshape(s1, s2))\n        assert cbuf2.size == cbuf.size\n        arr2 = np.frombuffer(cbuf2.copy_to_host(), dtype=dtype)\n        np.testing.assert_equal(arr, arr2)\n\n        with pytest.raises(ValueError,\n                           match=('array data is non-contiguous')):\n            ctx.buffer_from_object(darr.reshape(s1, s2)[:, ::2])\n\n        # a 3-D array\n        s1 = 4\n        s2 = size//8\n        s3 = size//(s1*s2)\n        assert s1 * s2 * s3 == size\n        cbuf2 = ctx.buffer_from_object(darr.reshape(s1, s2, s3))\n        assert cbuf2.size == cbuf.size\n        arr2 = np.frombuffer(cbuf2.copy_to_host(), dtype=dtype)\n        np.testing.assert_equal(arr, arr2)\n\n        with pytest.raises(ValueError,\n                           match=('array data is non-contiguous')):\n            ctx.buffer_from_object(darr.reshape(s1, s2, s3)[::2])\n\n    # Creating device buffer from am object implementing cuda array\n    # interface:\n    class MyObj:\n        def __init__(self, darr):\n            self.darr = darr\n\n        @property\n        def __cuda_array_interface__(self):\n            return self.darr.__cuda_array_interface__\n\n    cbuf2 = ctx.buffer_from_object(MyObj(darr))\n    assert cbuf2.size == cbuf.size\n    arr2 = np.frombuffer(cbuf2.copy_to_host(), dtype=dtype)\n    np.testing.assert_equal(arr, arr2)\n\n\n@pytest.mark.parametrize(\"c\", range(len(context_choice_ids)),\n                         ids=context_choice_ids)\n@pytest.mark.parametrize(\"dtype\", dtypes, ids=dtypes)\ndef test_numba_memalloc(c, dtype):\n    ctx, nb_ctx = context_choices[c]\n    dtype = np.dtype(dtype)\n    # Allocate memory using numba context\n    # Warning: this will not be reflected in pyarrow context manager\n    # (e.g bytes_allocated does not change)\n    size = 10\n    mem = nb_ctx.memalloc(size * dtype.itemsize)\n    darr = DeviceNDArray((size,), (dtype.itemsize,), dtype, gpu_data=mem)\n    darr[:5] = 99\n    darr[5:] = 88\n    np.testing.assert_equal(darr.copy_to_host()[:5], 99)\n    np.testing.assert_equal(darr.copy_to_host()[5:], 88)\n\n    # wrap numba allocated memory with CudaBuffer\n    cbuf = cuda.CudaBuffer.from_numba(mem)\n    arr2 = np.frombuffer(cbuf.copy_to_host(), dtype=dtype)\n    np.testing.assert_equal(arr2, darr.copy_to_host())\n\n\n@pytest.mark.parametrize(\"c\", range(len(context_choice_ids)),\n                         ids=context_choice_ids)\n@pytest.mark.parametrize(\"dtype\", dtypes, ids=dtypes)\ndef test_pyarrow_memalloc(c, dtype):\n    ctx, nb_ctx = context_choices[c]\n    size = 10\n    arr, cbuf = make_random_buffer(size, target='device', dtype=dtype, ctx=ctx)\n\n    # wrap CudaBuffer with numba device array\n    mem = cbuf.to_numba()\n    darr = DeviceNDArray(arr.shape, arr.strides, arr.dtype, gpu_data=mem)\n    np.testing.assert_equal(darr.copy_to_host(), arr)\n\n\n@pytest.mark.parametrize(\"c\", range(len(context_choice_ids)),\n                         ids=context_choice_ids)\n@pytest.mark.parametrize(\"dtype\", dtypes, ids=dtypes)\ndef test_numba_context(c, dtype):\n    ctx, nb_ctx = context_choices[c]\n    size = 10\n    with nb_cuda.gpus[0]:\n        arr, cbuf = make_random_buffer(size, target='device',\n                                       dtype=dtype, ctx=ctx)\n        assert cbuf.context.handle == nb_ctx.handle.value\n        mem = cbuf.to_numba()\n        darr = DeviceNDArray(arr.shape, arr.strides, arr.dtype, gpu_data=mem)\n        np.testing.assert_equal(darr.copy_to_host(), arr)\n        darr[0] = 99\n        cbuf.context.synchronize()\n        arr2 = np.frombuffer(cbuf.copy_to_host(), dtype=dtype)\n        assert arr2[0] == 99\n\n\n@pytest.mark.parametrize(\"c\", range(len(context_choice_ids)),\n                         ids=context_choice_ids)\n@pytest.mark.parametrize(\"dtype\", dtypes, ids=dtypes)\ndef test_pyarrow_jit(c, dtype):\n    ctx, nb_ctx = context_choices[c]\n\n    @nb_cuda.jit\n    def increment_by_one(an_array):\n        pos = nb_cuda.grid(1)\n        if pos < an_array.size:\n            an_array[pos] += 1\n\n    # applying numba.cuda kernel to memory hold by CudaBuffer\n    size = 10\n    arr, cbuf = make_random_buffer(size, target='device', dtype=dtype, ctx=ctx)\n    threadsperblock = 32\n    blockspergrid = (arr.size + (threadsperblock - 1)) // threadsperblock\n    mem = cbuf.to_numba()\n    darr = DeviceNDArray(arr.shape, arr.strides, arr.dtype, gpu_data=mem)\n    increment_by_one[blockspergrid, threadsperblock](darr)\n    cbuf.context.synchronize()\n    arr1 = np.frombuffer(cbuf.copy_to_host(), dtype=arr.dtype)\n    np.testing.assert_equal(arr1, arr + 1)\n", "python/pyarrow/tests/test_sparse_tensor.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\nimport sys\nimport weakref\n\nimport numpy as np\nimport pyarrow as pa\n\ntry:\n    from scipy.sparse import csr_matrix, coo_matrix\nexcept ImportError:\n    coo_matrix = None\n    csr_matrix = None\n\ntry:\n    import sparse\nexcept ImportError:\n    sparse = None\n\n\ntensor_type_pairs = [\n    ('i1', pa.int8()),\n    ('i2', pa.int16()),\n    ('i4', pa.int32()),\n    ('i8', pa.int64()),\n    ('u1', pa.uint8()),\n    ('u2', pa.uint16()),\n    ('u4', pa.uint32()),\n    ('u8', pa.uint64()),\n    ('f2', pa.float16()),\n    ('f4', pa.float32()),\n    ('f8', pa.float64())\n]\n\n\n@pytest.mark.parametrize('sparse_tensor_type', [\n    pa.SparseCSRMatrix,\n    pa.SparseCSCMatrix,\n    pa.SparseCOOTensor,\n    pa.SparseCSFTensor,\n])\ndef test_sparse_tensor_attrs(sparse_tensor_type):\n    data = np.array([\n        [8, 0, 2, 0, 0, 0],\n        [0, 0, 0, 0, 0, 5],\n        [3, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 4, 6],\n    ])\n    dim_names = ('x', 'y')\n    sparse_tensor = sparse_tensor_type.from_dense_numpy(data, dim_names)\n\n    assert sparse_tensor.ndim == 2\n    assert sparse_tensor.size == 24\n    assert sparse_tensor.shape == data.shape\n    assert sparse_tensor.is_mutable\n    assert sparse_tensor.dim_name(0) == dim_names[0]\n    assert sparse_tensor.dim_names == dim_names\n    assert sparse_tensor.non_zero_length == 6\n\n    wr = weakref.ref(sparse_tensor)\n    assert wr() is not None\n    del sparse_tensor\n    assert wr() is None\n\n\ndef test_sparse_coo_tensor_base_object():\n    expected_data = np.array([[8, 2, 5, 3, 4, 6]]).T\n    expected_coords = np.array([\n        [0, 0, 1, 2, 3, 3],\n        [0, 2, 5, 0, 4, 5],\n    ]).T\n    array = np.array([\n        [8, 0, 2, 0, 0, 0],\n        [0, 0, 0, 0, 0, 5],\n        [3, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 4, 6],\n    ])\n    sparse_tensor = pa.SparseCOOTensor.from_dense_numpy(array)\n    n = sys.getrefcount(sparse_tensor)\n    result_data, result_coords = sparse_tensor.to_numpy()\n    assert sparse_tensor.has_canonical_format\n    assert sys.getrefcount(sparse_tensor) == n + 2\n\n    sparse_tensor = None\n    assert np.array_equal(expected_data, result_data)\n    assert np.array_equal(expected_coords, result_coords)\n    assert result_coords.flags.c_contiguous  # row-major\n\n\ndef test_sparse_csr_matrix_base_object():\n    data = np.array([[8, 2, 5, 3, 4, 6]]).T\n    indptr = np.array([0, 2, 3, 4, 6])\n    indices = np.array([0, 2, 5, 0, 4, 5])\n    array = np.array([\n        [8, 0, 2, 0, 0, 0],\n        [0, 0, 0, 0, 0, 5],\n        [3, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 4, 6],\n    ])\n    sparse_tensor = pa.SparseCSRMatrix.from_dense_numpy(array)\n    n = sys.getrefcount(sparse_tensor)\n    result_data, result_indptr, result_indices = sparse_tensor.to_numpy()\n    assert sys.getrefcount(sparse_tensor) == n + 3\n\n    sparse_tensor = None\n    assert np.array_equal(data, result_data)\n    assert np.array_equal(indptr, result_indptr)\n    assert np.array_equal(indices, result_indices)\n\n\ndef test_sparse_csf_tensor_base_object():\n    data = np.array([[8, 2, 5, 3, 4, 6]]).T\n    indptr = [np.array([0, 2, 3, 4, 6])]\n    indices = [\n        np.array([0, 1, 2, 3]),\n        np.array([0, 2, 5, 0, 4, 5])\n    ]\n    array = np.array([\n        [8, 0, 2, 0, 0, 0],\n        [0, 0, 0, 0, 0, 5],\n        [3, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 4, 6],\n    ])\n    sparse_tensor = pa.SparseCSFTensor.from_dense_numpy(array)\n    n = sys.getrefcount(sparse_tensor)\n    result_data, result_indptr, result_indices = sparse_tensor.to_numpy()\n    assert sys.getrefcount(sparse_tensor) == n + 4\n\n    sparse_tensor = None\n    assert np.array_equal(data, result_data)\n    assert np.array_equal(indptr[0], result_indptr[0])\n    assert np.array_equal(indices[0], result_indices[0])\n    assert np.array_equal(indices[1], result_indices[1])\n\n\n@pytest.mark.parametrize('sparse_tensor_type', [\n    pa.SparseCSRMatrix,\n    pa.SparseCSCMatrix,\n    pa.SparseCOOTensor,\n    pa.SparseCSFTensor,\n])\ndef test_sparse_tensor_equals(sparse_tensor_type):\n    def eq(a, b):\n        assert a.equals(b)\n        assert a == b\n        assert not (a != b)\n\n    def ne(a, b):\n        assert not a.equals(b)\n        assert not (a == b)\n        assert a != b\n\n    data = np.random.randn(10, 6)[::, ::2]\n    sparse_tensor1 = sparse_tensor_type.from_dense_numpy(data)\n    sparse_tensor2 = sparse_tensor_type.from_dense_numpy(\n        np.ascontiguousarray(data))\n    eq(sparse_tensor1, sparse_tensor2)\n    data = data.copy()\n    data[9, 0] = 1.0\n    sparse_tensor2 = sparse_tensor_type.from_dense_numpy(\n        np.ascontiguousarray(data))\n    ne(sparse_tensor1, sparse_tensor2)\n\n\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_sparse_coo_tensor_from_dense(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    expected_data = np.array([[8, 2, 5, 3, 4, 6]]).T.astype(dtype)\n    expected_coords = np.array([\n        [0, 0, 1, 2, 3, 3],\n        [0, 2, 5, 0, 4, 5],\n    ]).T\n    array = np.array([\n        [8, 0, 2, 0, 0, 0],\n        [0, 0, 0, 0, 0, 5],\n        [3, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 4, 6],\n    ]).astype(dtype)\n    tensor = pa.Tensor.from_numpy(array)\n\n    # Test from numpy array\n    sparse_tensor = pa.SparseCOOTensor.from_dense_numpy(array)\n    repr(sparse_tensor)\n    result_data, result_coords = sparse_tensor.to_numpy()\n    assert sparse_tensor.type == arrow_type\n    assert np.array_equal(expected_data, result_data)\n    assert np.array_equal(expected_coords, result_coords)\n\n    # Test from Tensor\n    sparse_tensor = pa.SparseCOOTensor.from_tensor(tensor)\n    repr(sparse_tensor)\n    result_data, result_coords = sparse_tensor.to_numpy()\n    assert sparse_tensor.type == arrow_type\n    assert np.array_equal(expected_data, result_data)\n    assert np.array_equal(expected_coords, result_coords)\n\n\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_sparse_csr_matrix_from_dense(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    data = np.array([[8, 2, 5, 3, 4, 6]]).T.astype(dtype)\n    indptr = np.array([0, 2, 3, 4, 6])\n    indices = np.array([0, 2, 5, 0, 4, 5])\n    array = np.array([\n        [8, 0, 2, 0, 0, 0],\n        [0, 0, 0, 0, 0, 5],\n        [3, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 4, 6],\n    ]).astype(dtype)\n    tensor = pa.Tensor.from_numpy(array)\n\n    # Test from numpy array\n    sparse_tensor = pa.SparseCSRMatrix.from_dense_numpy(array)\n    repr(sparse_tensor)\n    result_data, result_indptr, result_indices = sparse_tensor.to_numpy()\n    assert sparse_tensor.type == arrow_type\n    assert np.array_equal(data, result_data)\n    assert np.array_equal(indptr, result_indptr)\n    assert np.array_equal(indices, result_indices)\n\n    # Test from Tensor\n    sparse_tensor = pa.SparseCSRMatrix.from_tensor(tensor)\n    repr(sparse_tensor)\n    result_data, result_indptr, result_indices = sparse_tensor.to_numpy()\n    assert sparse_tensor.type == arrow_type\n    assert np.array_equal(data, result_data)\n    assert np.array_equal(indptr, result_indptr)\n    assert np.array_equal(indices, result_indices)\n\n\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_sparse_csf_tensor_from_dense_numpy(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    data = np.array([[8, 2, 5, 3, 4, 6]]).T.astype(dtype)\n    indptr = [np.array([0, 2, 3, 4, 6])]\n    indices = [\n        np.array([0, 1, 2, 3]),\n        np.array([0, 2, 5, 0, 4, 5])\n    ]\n    array = np.array([\n        [8, 0, 2, 0, 0, 0],\n        [0, 0, 0, 0, 0, 5],\n        [3, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 4, 6],\n    ]).astype(dtype)\n\n    # Test from numpy array\n    sparse_tensor = pa.SparseCSFTensor.from_dense_numpy(array)\n    repr(sparse_tensor)\n    result_data, result_indptr, result_indices = sparse_tensor.to_numpy()\n    assert sparse_tensor.type == arrow_type\n    assert np.array_equal(data, result_data)\n    assert np.array_equal(indptr[0], result_indptr[0])\n    assert np.array_equal(indices[0], result_indices[0])\n    assert np.array_equal(indices[1], result_indices[1])\n\n\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_sparse_csf_tensor_from_dense_tensor(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    data = np.array([[8, 2, 5, 3, 4, 6]]).T.astype(dtype)\n    indptr = [np.array([0, 2, 3, 4, 6])]\n    indices = [\n        np.array([0, 1, 2, 3]),\n        np.array([0, 2, 5, 0, 4, 5])\n    ]\n    array = np.array([\n        [8, 0, 2, 0, 0, 0],\n        [0, 0, 0, 0, 0, 5],\n        [3, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 4, 6],\n    ]).astype(dtype)\n    tensor = pa.Tensor.from_numpy(array)\n\n    # Test from Tensor\n    sparse_tensor = pa.SparseCSFTensor.from_tensor(tensor)\n    repr(sparse_tensor)\n    result_data, result_indptr, result_indices = sparse_tensor.to_numpy()\n    assert sparse_tensor.type == arrow_type\n    assert np.array_equal(data, result_data)\n    assert np.array_equal(indptr[0], result_indptr[0])\n    assert np.array_equal(indices[0], result_indices[0])\n    assert np.array_equal(indices[1], result_indices[1])\n\n\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_sparse_coo_tensor_numpy_roundtrip(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    data = np.array([[1, 2, 3, 4, 5, 6]]).T.astype(dtype)\n    coords = np.array([\n        [0, 0, 2, 3, 1, 3],\n        [0, 2, 0, 4, 5, 5],\n    ]).T\n    shape = (4, 6)\n    dim_names = ('x', 'y')\n\n    sparse_tensor = pa.SparseCOOTensor.from_numpy(data, coords, shape,\n                                                  dim_names)\n    repr(sparse_tensor)\n    result_data, result_coords = sparse_tensor.to_numpy()\n    assert sparse_tensor.type == arrow_type\n    assert np.array_equal(data, result_data)\n    assert np.array_equal(coords, result_coords)\n    assert sparse_tensor.dim_names == dim_names\n\n\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_sparse_csr_matrix_numpy_roundtrip(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    data = np.array([[8, 2, 5, 3, 4, 6]]).T.astype(dtype)\n    indptr = np.array([0, 2, 3, 4, 6])\n    indices = np.array([0, 2, 5, 0, 4, 5])\n    shape = (4, 6)\n    dim_names = ('x', 'y')\n\n    sparse_tensor = pa.SparseCSRMatrix.from_numpy(data, indptr, indices,\n                                                  shape, dim_names)\n    repr(sparse_tensor)\n    result_data, result_indptr, result_indices = sparse_tensor.to_numpy()\n    assert sparse_tensor.type == arrow_type\n    assert np.array_equal(data, result_data)\n    assert np.array_equal(indptr, result_indptr)\n    assert np.array_equal(indices, result_indices)\n    assert sparse_tensor.dim_names == dim_names\n\n\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_sparse_csf_tensor_numpy_roundtrip(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    data = np.array([[8, 2, 5, 3, 4, 6]]).T.astype(dtype)\n    indptr = [np.array([0, 2, 3, 4, 6])]\n    indices = [\n        np.array([0, 1, 2, 3]),\n        np.array([0, 2, 5, 0, 4, 5])\n    ]\n    axis_order = (0, 1)\n    shape = (4, 6)\n    dim_names = ('x', 'y')\n\n    sparse_tensor = pa.SparseCSFTensor.from_numpy(data, indptr, indices,\n                                                  shape, axis_order,\n                                                  dim_names)\n    repr(sparse_tensor)\n    result_data, result_indptr, result_indices = sparse_tensor.to_numpy()\n    assert sparse_tensor.type == arrow_type\n    assert np.array_equal(data, result_data)\n    assert np.array_equal(indptr[0], result_indptr[0])\n    assert np.array_equal(indices[0], result_indices[0])\n    assert np.array_equal(indices[1], result_indices[1])\n    assert sparse_tensor.dim_names == dim_names\n\n\n@pytest.mark.parametrize('sparse_tensor_type', [\n    pa.SparseCSRMatrix,\n    pa.SparseCSCMatrix,\n    pa.SparseCOOTensor,\n    pa.SparseCSFTensor,\n])\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_dense_to_sparse_tensor(dtype_str, arrow_type, sparse_tensor_type):\n    dtype = np.dtype(dtype_str)\n    array = np.array([[4, 0, 9, 0],\n                      [0, 7, 0, 0],\n                      [0, 0, 0, 0],\n                      [0, 0, 0, 5]]).astype(dtype)\n    dim_names = ('x', 'y')\n\n    sparse_tensor = sparse_tensor_type.from_dense_numpy(array, dim_names)\n    tensor = sparse_tensor.to_tensor()\n    result_array = tensor.to_numpy()\n\n    assert sparse_tensor.type == arrow_type\n    assert tensor.type == arrow_type\n    assert sparse_tensor.dim_names == dim_names\n    assert np.array_equal(array, result_array)\n\n\n@pytest.mark.skipif(not coo_matrix, reason=\"requires scipy\")\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_sparse_coo_tensor_scipy_roundtrip(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    data = np.array([1, 2, 3, 4, 5, 6]).astype(dtype)\n    row = np.array([0, 0, 2, 3, 1, 3])\n    col = np.array([0, 2, 0, 4, 5, 5])\n    shape = (4, 6)\n    dim_names = ('x', 'y')\n\n    # non-canonical sparse coo matrix\n    scipy_matrix = coo_matrix((data, (row, col)), shape=shape)\n    sparse_tensor = pa.SparseCOOTensor.from_scipy(scipy_matrix,\n                                                  dim_names=dim_names)\n    out_scipy_matrix = sparse_tensor.to_scipy()\n\n    assert not scipy_matrix.has_canonical_format\n    assert not sparse_tensor.has_canonical_format\n    assert not out_scipy_matrix.has_canonical_format\n    assert sparse_tensor.type == arrow_type\n    assert sparse_tensor.dim_names == dim_names\n    assert scipy_matrix.dtype == out_scipy_matrix.dtype\n    assert np.array_equal(scipy_matrix.data, out_scipy_matrix.data)\n    assert np.array_equal(scipy_matrix.row, out_scipy_matrix.row)\n    assert np.array_equal(scipy_matrix.col, out_scipy_matrix.col)\n\n    if dtype_str == 'f2':\n        dense_array = \\\n            scipy_matrix.astype(np.float32).toarray().astype(np.float16)\n    else:\n        dense_array = scipy_matrix.toarray()\n    assert np.array_equal(dense_array, sparse_tensor.to_tensor().to_numpy())\n\n    # canonical sparse coo matrix\n    scipy_matrix.sum_duplicates()\n    sparse_tensor = pa.SparseCOOTensor.from_scipy(scipy_matrix,\n                                                  dim_names=dim_names)\n    out_scipy_matrix = sparse_tensor.to_scipy()\n\n    assert scipy_matrix.has_canonical_format\n    assert sparse_tensor.has_canonical_format\n    assert out_scipy_matrix.has_canonical_format\n\n\n@pytest.mark.skipif(not csr_matrix, reason=\"requires scipy\")\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_sparse_csr_matrix_scipy_roundtrip(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    data = np.array([8, 2, 5, 3, 4, 6]).astype(dtype)\n    indptr = np.array([0, 2, 3, 4, 6])\n    indices = np.array([0, 2, 5, 0, 4, 5])\n    shape = (4, 6)\n    dim_names = ('x', 'y')\n\n    sparse_array = csr_matrix((data, indices, indptr), shape=shape)\n    sparse_tensor = pa.SparseCSRMatrix.from_scipy(sparse_array,\n                                                  dim_names=dim_names)\n    out_sparse_array = sparse_tensor.to_scipy()\n\n    assert sparse_tensor.type == arrow_type\n    assert sparse_tensor.dim_names == dim_names\n    assert sparse_array.dtype == out_sparse_array.dtype\n    assert np.array_equal(sparse_array.data, out_sparse_array.data)\n    assert np.array_equal(sparse_array.indptr, out_sparse_array.indptr)\n    assert np.array_equal(sparse_array.indices, out_sparse_array.indices)\n\n    if dtype_str == 'f2':\n        dense_array = \\\n            sparse_array.astype(np.float32).toarray().astype(np.float16)\n    else:\n        dense_array = sparse_array.toarray()\n    assert np.array_equal(dense_array, sparse_tensor.to_tensor().to_numpy())\n\n\n@pytest.mark.skipif(not sparse, reason=\"requires pydata/sparse\")\n@pytest.mark.parametrize('dtype_str,arrow_type', tensor_type_pairs)\ndef test_pydata_sparse_sparse_coo_tensor_roundtrip(dtype_str, arrow_type):\n    dtype = np.dtype(dtype_str)\n    data = np.array([1, 2, 3, 4, 5, 6]).astype(dtype)\n    coords = np.array([\n        [0, 0, 2, 3, 1, 3],\n        [0, 2, 0, 4, 5, 5],\n    ])\n    shape = (4, 6)\n    dim_names = (\"x\", \"y\")\n\n    sparse_array = sparse.COO(data=data, coords=coords, shape=shape)\n    sparse_tensor = pa.SparseCOOTensor.from_pydata_sparse(sparse_array,\n                                                          dim_names=dim_names)\n    out_sparse_array = sparse_tensor.to_pydata_sparse()\n\n    assert sparse_tensor.type == arrow_type\n    assert sparse_tensor.dim_names == dim_names\n    assert sparse_array.dtype == out_sparse_array.dtype\n    assert np.array_equal(sparse_array.data, out_sparse_array.data)\n    assert np.array_equal(sparse_array.coords, out_sparse_array.coords)\n    assert np.array_equal(sparse_array.todense(),\n                          sparse_tensor.to_tensor().to_numpy())\n", "python/pyarrow/tests/test_extension_type.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport os\nimport shutil\nimport subprocess\nimport weakref\nfrom uuid import uuid4, UUID\nimport sys\n\nimport numpy as np\nimport pyarrow as pa\nfrom pyarrow.vendored.version import Version\n\nimport pytest\n\n\n@contextlib.contextmanager\ndef registered_extension_type(ext_type):\n    pa.register_extension_type(ext_type)\n    try:\n        yield\n    finally:\n        pa.unregister_extension_type(ext_type.extension_name)\n\n\n@contextlib.contextmanager\ndef enabled_auto_load():\n    pa.PyExtensionType.set_auto_load(True)\n    try:\n        yield\n    finally:\n        pa.PyExtensionType.set_auto_load(False)\n\n\nclass TinyIntType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.int8(), 'pyarrow.tests.TinyIntType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        assert storage_type == pa.int8()\n        return cls()\n\n\nclass IntegerType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.int64(), 'pyarrow.tests.IntegerType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        assert storage_type == pa.int64()\n        return cls()\n\n\nclass IntegerEmbeddedType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(IntegerType(), 'pyarrow.tests.IntegerType')\n\n    def __arrow_ext_serialize__(self):\n        # XXX pa.BaseExtensionType should expose C++ serialization method\n        return self.storage_type.__arrow_ext_serialize__()\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        deserialized_storage_type = storage_type.__arrow_ext_deserialize__(\n            serialized)\n        assert deserialized_storage_type == storage_type\n        return cls()\n\n\nclass UuidScalarType(pa.ExtensionScalar):\n    def as_py(self):\n        return None if self.value is None else UUID(bytes=self.value.as_py())\n\n\nclass UuidType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.binary(16), 'pyarrow.tests.UuidType')\n\n    def __arrow_ext_scalar_class__(self):\n        return UuidScalarType\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        return cls()\n\n\nclass UuidType2(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.binary(16), 'pyarrow.tests.UuidType2')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        return cls()\n\n\nclass LabelType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.string(), 'pyarrow.tests.LabelType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        return cls()\n\n\nclass ParamExtType(pa.ExtensionType):\n\n    def __init__(self, width):\n        self._width = width\n        super().__init__(pa.binary(width), 'pyarrow.tests.ParamExtType')\n\n    @property\n    def width(self):\n        return self._width\n\n    def __arrow_ext_serialize__(self):\n        return str(self._width).encode()\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        width = int(serialized.decode())\n        assert storage_type == pa.binary(width)\n        return cls(width)\n\n\nclass MyStructType(pa.ExtensionType):\n    storage_type = pa.struct([('left', pa.int64()),\n                              ('right', pa.int64())])\n\n    def __init__(self):\n        super().__init__(self.storage_type, 'pyarrow.tests.MyStructType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        assert storage_type == cls.storage_type\n        return cls()\n\n\nclass MyListType(pa.ExtensionType):\n\n    def __init__(self, storage_type):\n        assert isinstance(storage_type, pa.ListType)\n        super().__init__(storage_type, 'pyarrow.tests.MyListType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        return cls(storage_type)\n\n\nclass MyFixedListType(pa.ExtensionType):\n\n    def __init__(self, storage_type):\n        assert isinstance(storage_type, pa.FixedSizeListType)\n        super().__init__(storage_type, 'pyarrow.tests.MyFixedListType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        return cls(storage_type)\n\n\nclass AnnotatedType(pa.ExtensionType):\n    \"\"\"\n    Generic extension type that can store any storage type.\n    \"\"\"\n\n    def __init__(self, storage_type, annotation):\n        self.annotation = annotation\n        super().__init__(storage_type, 'pyarrow.tests.AnnotatedType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        return cls(storage_type)\n\n\nclass LegacyIntType(pa.PyExtensionType):\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, pa.int8())\n\n    def __reduce__(self):\n        return LegacyIntType, ()\n\n\ndef ipc_write_batch(batch):\n    stream = pa.BufferOutputStream()\n    writer = pa.RecordBatchStreamWriter(stream, batch.schema)\n    writer.write_batch(batch)\n    writer.close()\n    return stream.getvalue()\n\n\ndef ipc_read_batch(buf):\n    reader = pa.RecordBatchStreamReader(buf)\n    return reader.read_next_batch()\n\n\ndef test_ext_type_basics():\n    ty = UuidType()\n    assert ty.extension_name == \"pyarrow.tests.UuidType\"\n\n\ndef test_ext_type_str():\n    ty = IntegerType()\n    expected = \"extension<pyarrow.tests.IntegerType<IntegerType>>\"\n    assert str(ty) == expected\n    assert pa.DataType.__str__(ty) == expected\n\n\ndef test_ext_type_repr():\n    ty = IntegerType()\n    assert repr(ty) == \"IntegerType(DataType(int64))\"\n\n\ndef test_ext_type_lifetime():\n    ty = UuidType()\n    wr = weakref.ref(ty)\n    del ty\n    assert wr() is None\n\n\ndef test_ext_type_storage_type():\n    ty = UuidType()\n    assert ty.storage_type == pa.binary(16)\n    assert ty.__class__ is UuidType\n    ty = ParamExtType(5)\n    assert ty.storage_type == pa.binary(5)\n    assert ty.__class__ is ParamExtType\n\n\ndef test_ext_type_byte_width():\n    # Test for fixed-size binary types\n    ty = UuidType()\n    assert ty.byte_width == 16\n    ty = ParamExtType(5)\n    assert ty.byte_width == 5\n\n    # Test for non fixed-size binary types\n    ty = LabelType()\n    with pytest.raises(ValueError, match=\"Non-fixed width type\"):\n        _ = ty.byte_width\n\n\ndef test_ext_type_bit_width():\n    # Test for fixed-size binary types\n    ty = UuidType()\n    assert ty.bit_width == 128\n    ty = ParamExtType(5)\n    assert ty.bit_width == 40\n\n    # Test for non fixed-size binary types\n    ty = LabelType()\n    with pytest.raises(ValueError, match=\"Non-fixed width type\"):\n        _ = ty.bit_width\n\n\ndef test_ext_type_as_py():\n    ty = UuidType()\n    expected = uuid4()\n    scalar = pa.ExtensionScalar.from_storage(ty, expected.bytes)\n    assert scalar.as_py() == expected\n\n    # test array\n    uuids = [uuid4() for _ in range(3)]\n    storage = pa.array([uuid.bytes for uuid in uuids], type=pa.binary(16))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n\n    # Works for __get_item__\n    for i, expected in enumerate(uuids):\n        assert arr[i].as_py() == expected\n\n    # Works for __iter__\n    for result, expected in zip(arr, uuids):\n        assert result.as_py() == expected\n\n    # test chunked array\n    data = [\n        pa.ExtensionArray.from_storage(ty, storage),\n        pa.ExtensionArray.from_storage(ty, storage)\n    ]\n    carr = pa.chunked_array(data)\n    for i, expected in enumerate(uuids + uuids):\n        assert carr[i].as_py() == expected\n\n    for result, expected in zip(carr, uuids + uuids):\n        assert result.as_py() == expected\n\n\ndef test_uuid_type_pickle(pickle_module):\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        ty = UuidType()\n        ser = pickle_module.dumps(ty, protocol=proto)\n        del ty\n        ty = pickle_module.loads(ser)\n        wr = weakref.ref(ty)\n        assert ty.extension_name == \"pyarrow.tests.UuidType\"\n        del ty\n        assert wr() is None\n\n\ndef test_ext_type_equality():\n    a = ParamExtType(5)\n    b = ParamExtType(6)\n    c = ParamExtType(6)\n    assert a != b\n    assert b == c\n    d = UuidType()\n    e = UuidType()\n    assert a != d\n    assert d == e\n\n\ndef test_ext_array_basics():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n    arr.validate()\n    assert arr.type is ty\n    assert arr.storage.equals(storage)\n\n\ndef test_ext_array_lifetime():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n\n    refs = [weakref.ref(ty), weakref.ref(arr), weakref.ref(storage)]\n    del ty, storage, arr\n    for ref in refs:\n        assert ref() is None\n\n\ndef test_ext_array_to_pylist():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\", None], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n\n    assert arr.to_pylist() == [b\"foo\", b\"bar\", None]\n\n\ndef test_ext_array_errors():\n    ty = ParamExtType(4)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    with pytest.raises(TypeError, match=\"Incompatible storage type\"):\n        pa.ExtensionArray.from_storage(ty, storage)\n\n\ndef test_ext_array_equality():\n    storage1 = pa.array([b\"0123456789abcdef\"], type=pa.binary(16))\n    storage2 = pa.array([b\"0123456789abcdef\"], type=pa.binary(16))\n    storage3 = pa.array([], type=pa.binary(16))\n    ty1 = UuidType()\n    ty2 = ParamExtType(16)\n\n    a = pa.ExtensionArray.from_storage(ty1, storage1)\n    b = pa.ExtensionArray.from_storage(ty1, storage2)\n    assert a.equals(b)\n    c = pa.ExtensionArray.from_storage(ty1, storage3)\n    assert not a.equals(c)\n    d = pa.ExtensionArray.from_storage(ty2, storage1)\n    assert not a.equals(d)\n    e = pa.ExtensionArray.from_storage(ty2, storage2)\n    assert d.equals(e)\n    f = pa.ExtensionArray.from_storage(ty2, storage3)\n    assert not d.equals(f)\n\n\ndef test_ext_array_wrap_array():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\", None], type=pa.binary(3))\n    arr = ty.wrap_array(storage)\n    arr.validate(full=True)\n    assert isinstance(arr, pa.ExtensionArray)\n    assert arr.type == ty\n    assert arr.storage == storage\n\n    storage = pa.chunked_array([[b\"abc\", b\"def\"], [b\"ghi\"]],\n                               type=pa.binary(3))\n    arr = ty.wrap_array(storage)\n    arr.validate(full=True)\n    assert isinstance(arr, pa.ChunkedArray)\n    assert arr.type == ty\n    assert arr.chunk(0).storage == storage.chunk(0)\n    assert arr.chunk(1).storage == storage.chunk(1)\n\n    # Wrong storage type\n    storage = pa.array([b\"foo\", b\"bar\", None])\n    with pytest.raises(TypeError, match=\"Incompatible storage type\"):\n        ty.wrap_array(storage)\n\n    # Not an array or chunked array\n    with pytest.raises(TypeError, match=\"Expected array or chunked array\"):\n        ty.wrap_array(None)\n\n\ndef test_ext_scalar_from_array():\n    data = [b\"0123456789abcdef\", b\"0123456789abcdef\",\n            b\"zyxwvutsrqponmlk\", None]\n    storage = pa.array(data, type=pa.binary(16))\n    ty1 = UuidType()\n    ty2 = ParamExtType(16)\n    ty3 = UuidType2()\n\n    a = pa.ExtensionArray.from_storage(ty1, storage)\n    b = pa.ExtensionArray.from_storage(ty2, storage)\n    c = pa.ExtensionArray.from_storage(ty3, storage)\n\n    scalars_a = list(a)\n    assert len(scalars_a) == 4\n\n    assert ty1.__arrow_ext_scalar_class__() == UuidScalarType\n    assert isinstance(a[0], UuidScalarType)\n    assert isinstance(scalars_a[0], UuidScalarType)\n\n    for s, val in zip(scalars_a, data):\n        assert isinstance(s, pa.ExtensionScalar)\n        assert s.is_valid == (val is not None)\n        assert s.type == ty1\n        if val is not None:\n            assert s.value == pa.scalar(val, storage.type)\n            assert s.as_py() == UUID(bytes=val)\n        else:\n            assert s.value is None\n\n    scalars_b = list(b)\n    assert len(scalars_b) == 4\n\n    for sa, sb in zip(scalars_a, scalars_b):\n        assert isinstance(sb, pa.ExtensionScalar)\n        assert sa.is_valid == sb.is_valid\n        if sa.as_py() is None:\n            assert sa.as_py() == sb.as_py()\n        else:\n            assert sa.as_py().bytes == sb.as_py()\n        assert sa != sb\n\n    scalars_c = list(c)\n    assert len(scalars_c) == 4\n\n    for s, val in zip(scalars_c, data):\n        assert isinstance(s, pa.ExtensionScalar)\n        assert s.is_valid == (val is not None)\n        assert s.type == ty3\n        if val is not None:\n            assert s.value == pa.scalar(val, storage.type)\n            assert s.as_py() == val\n        else:\n            assert s.value is None\n\n    assert a.to_pylist() == [UUID(bytes=x) if x else None for x in data]\n\n\ndef test_ext_scalar_from_storage():\n    ty = UuidType()\n\n    s = pa.ExtensionScalar.from_storage(ty, None)\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is False\n    assert s.value is None\n\n    s = pa.ExtensionScalar.from_storage(ty, b\"0123456789abcdef\")\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is True\n    assert s.value == pa.scalar(b\"0123456789abcdef\", ty.storage_type)\n\n    s = pa.ExtensionScalar.from_storage(ty, pa.scalar(None, ty.storage_type))\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is False\n    assert s.value is None\n\n    s = pa.ExtensionScalar.from_storage(\n        ty, pa.scalar(b\"0123456789abcdef\", ty.storage_type))\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is True\n    assert s.value == pa.scalar(b\"0123456789abcdef\", ty.storage_type)\n\n\ndef test_ext_array_pickling(pickle_module):\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        ty = ParamExtType(3)\n        storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n        arr = pa.ExtensionArray.from_storage(ty, storage)\n        ser = pickle_module.dumps(arr, protocol=proto)\n        del ty, storage, arr\n        arr = pickle_module.loads(ser)\n        arr.validate()\n        assert isinstance(arr, pa.ExtensionArray)\n        assert arr.type == ParamExtType(3)\n        assert arr.type.storage_type == pa.binary(3)\n        assert arr.storage.type == pa.binary(3)\n        assert arr.storage.to_pylist() == [b\"foo\", b\"bar\"]\n\n\ndef test_ext_array_conversion_to_numpy():\n    storage1 = pa.array([1, 2, 3], type=pa.int64())\n    storage2 = pa.array([b\"123\", b\"456\", b\"789\"], type=pa.binary(3))\n    ty1 = IntegerType()\n    ty2 = ParamExtType(3)\n\n    arr1 = pa.ExtensionArray.from_storage(ty1, storage1)\n    arr2 = pa.ExtensionArray.from_storage(ty2, storage2)\n\n    result = arr1.to_numpy()\n    expected = np.array([1, 2, 3], dtype=\"int64\")\n    np.testing.assert_array_equal(result, expected)\n\n    with pytest.raises(ValueError, match=\"zero_copy_only was True\"):\n        arr2.to_numpy()\n    result = arr2.to_numpy(zero_copy_only=False)\n    expected = np.array([b\"123\", b\"456\", b\"789\"])\n    np.testing.assert_array_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_ext_array_conversion_to_pandas():\n    import pandas as pd\n\n    storage1 = pa.array([1, 2, 3], type=pa.int64())\n    storage2 = pa.array([b\"123\", b\"456\", b\"789\"], type=pa.binary(3))\n    ty1 = IntegerType()\n    ty2 = ParamExtType(3)\n\n    arr1 = pa.ExtensionArray.from_storage(ty1, storage1)\n    arr2 = pa.ExtensionArray.from_storage(ty2, storage2)\n\n    result = arr1.to_pandas()\n    expected = pd.Series([1, 2, 3], dtype=\"int64\")\n    pd.testing.assert_series_equal(result, expected)\n\n    result = arr2.to_pandas()\n    expected = pd.Series([b\"123\", b\"456\", b\"789\"], dtype=object)\n    pd.testing.assert_series_equal(result, expected)\n\n\n@pytest.fixture\ndef struct_w_ext_data():\n    storage1 = pa.array([1, 2, 3], type=pa.int64())\n    storage2 = pa.array([b\"123\", b\"456\", b\"789\"], type=pa.binary(3))\n    ty1 = IntegerType()\n    ty2 = ParamExtType(3)\n\n    arr1 = pa.ExtensionArray.from_storage(ty1, storage1)\n    arr2 = pa.ExtensionArray.from_storage(ty2, storage2)\n\n    sarr1 = pa.StructArray.from_arrays([arr1], [\"f0\"])\n    sarr2 = pa.StructArray.from_arrays([arr2], [\"f1\"])\n\n    return [sarr1, sarr2]\n\n\ndef test_struct_w_ext_array_to_numpy(struct_w_ext_data):\n    # ARROW-15291\n    # Check that we don't segfault when trying to build\n    # a numpy array from a StructArray with a field being\n    # an ExtensionArray\n\n    result = struct_w_ext_data[0].to_numpy(zero_copy_only=False)\n    expected = np.array([{'f0': 1}, {'f0': 2},\n                         {'f0': 3}], dtype=object)\n    np.testing.assert_array_equal(result, expected)\n\n    result = struct_w_ext_data[1].to_numpy(zero_copy_only=False)\n    expected = np.array([{'f1': b'123'}, {'f1': b'456'},\n                         {'f1': b'789'}], dtype=object)\n    np.testing.assert_array_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_struct_w_ext_array_to_pandas(struct_w_ext_data):\n    # ARROW-15291\n    # Check that we don't segfault when trying to build\n    # a Pandas dataframe from a StructArray with a field\n    # being an ExtensionArray\n    import pandas as pd\n\n    result = struct_w_ext_data[0].to_pandas()\n    expected = pd.Series([{'f0': 1}, {'f0': 2},\n                         {'f0': 3}], dtype=object)\n    pd.testing.assert_series_equal(result, expected)\n\n    result = struct_w_ext_data[1].to_pandas()\n    expected = pd.Series([{'f1': b'123'}, {'f1': b'456'},\n                         {'f1': b'789'}], dtype=object)\n    pd.testing.assert_series_equal(result, expected)\n\n\ndef test_cast_kernel_on_extension_arrays():\n    # test array casting\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(IntegerType(), storage)\n\n    # test that no allocation happens during identity cast\n    allocated_before_cast = pa.total_allocated_bytes()\n    casted = arr.cast(pa.int64())\n    assert pa.total_allocated_bytes() == allocated_before_cast\n\n    cases = [\n        (pa.int64(), pa.Int64Array),\n        (pa.int32(), pa.Int32Array),\n        (pa.int16(), pa.Int16Array),\n        (pa.uint64(), pa.UInt64Array),\n        (pa.uint32(), pa.UInt32Array),\n        (pa.uint16(), pa.UInt16Array)\n    ]\n    for typ, klass in cases:\n        casted = arr.cast(typ)\n        assert casted.type == typ\n        assert isinstance(casted, klass)\n\n    # test chunked array casting\n    arr = pa.chunked_array([arr, arr])\n    casted = arr.cast(pa.int16())\n    assert casted.type == pa.int16()\n    assert isinstance(casted, pa.ChunkedArray)\n\n\n@pytest.mark.parametrize(\"data,ty\", (\n    ([1, 2], pa.int32),\n    ([1, 2], pa.int64),\n    ([\"1\", \"2\"], pa.string),\n    ([b\"1\", b\"2\"], pa.binary),\n    ([1.0, 2.0], pa.float32),\n    ([1.0, 2.0], pa.float64)\n))\ndef test_casting_to_extension_type(data, ty):\n    arr = pa.array(data, ty())\n    out = arr.cast(IntegerType())\n    assert isinstance(out, pa.ExtensionArray)\n    assert out.type == IntegerType()\n    assert out.to_pylist() == [1, 2]\n\n\ndef test_cast_between_extension_types():\n    array = pa.array([1, 2, 3], pa.int8())\n\n    tiny_int_arr = array.cast(TinyIntType())\n    assert tiny_int_arr.type == TinyIntType()\n\n    # Casting between extension types w/ different storage types not okay.\n    msg = (\"Casting from 'extension<.*?<TinyIntType>>' \"\n           \"to different extension type \"\n           \"'extension<.*?<IntegerType>>' not permitted. \"\n           \"One can first cast to the storage type, \"\n           \"then to the extension type.\"\n           )\n    with pytest.raises(TypeError, match=msg):\n        tiny_int_arr.cast(IntegerType())\n    tiny_int_arr.cast(pa.int64()).cast(IntegerType())\n\n    # Between the same extension types is okay\n    array = pa.array([b'1' * 16, b'2' * 16], pa.binary(16)).cast(UuidType())\n    out = array.cast(UuidType())\n    assert out.type == UuidType()\n\n    # Will still fail casting between extensions who share storage type,\n    # can only cast between exactly the same extension types.\n    with pytest.raises(TypeError, match='Casting from *'):\n        array.cast(UuidType2())\n\n\ndef test_cast_to_extension_with_extension_storage():\n    # Test casting directly, and IntegerType -> IntegerEmbeddedType\n    array = pa.array([1, 2, 3], pa.int64())\n    array.cast(IntegerEmbeddedType())\n    array.cast(IntegerType()).cast(IntegerEmbeddedType())\n\n\n@pytest.mark.parametrize(\"data,type_factory\", (\n    # list<extension>\n    ([[1, 2, 3]], lambda: pa.list_(IntegerType())),\n    # struct<extension>\n    ([{\"foo\": 1}], lambda: pa.struct([(\"foo\", IntegerType())])),\n    # list<struct<extension>>\n    ([[{\"foo\": 1}]], lambda: pa.list_(pa.struct([(\"foo\", IntegerType())]))),\n    # struct<list<extension>>\n    ([{\"foo\": [1, 2, 3]}], lambda: pa.struct(\n        [(\"foo\", pa.list_(IntegerType()))])),\n))\ndef test_cast_nested_extension_types(data, type_factory):\n    ty = type_factory()\n    a = pa.array(data)\n    b = a.cast(ty)\n    assert b.type == ty  # casted to target extension\n    assert b.cast(a.type)  # and can cast back\n\n\ndef test_casting_dict_array_to_extension_type():\n    storage = pa.array([b\"0123456789abcdef\"], type=pa.binary(16))\n    arr = pa.ExtensionArray.from_storage(UuidType(), storage)\n    dict_arr = pa.DictionaryArray.from_arrays(pa.array([0, 0], pa.int32()),\n                                              arr)\n    out = dict_arr.cast(UuidType())\n    assert isinstance(out, pa.ExtensionArray)\n    assert out.to_pylist() == [UUID('30313233-3435-3637-3839-616263646566'),\n                               UUID('30313233-3435-3637-3839-616263646566')]\n\n\ndef test_cast_to_extension_with_nested_storage():\n    # https://github.com/apache/arrow/issues/37669\n\n    # With fixed-size list\n    array = pa.array([[1, 2], [3, 4], [5, 6]], pa.list_(pa.float64(), 2))\n    result = array.cast(MyFixedListType(pa.list_(pa.float64(), 2)))\n    expected = pa.ExtensionArray.from_storage(MyFixedListType(array.type), array)\n    assert result.equals(expected)\n\n    ext_type = MyFixedListType(pa.list_(pa.float32(), 2))\n    result = array.cast(ext_type)\n    expected = pa.ExtensionArray.from_storage(\n        ext_type, array.cast(ext_type.storage_type)\n    )\n    assert result.equals(expected)\n\n    # With variable-size list\n    array = pa.array([[1, 2], [3], [4, 5, 6]], pa.list_(pa.float64()))\n    result = array.cast(MyListType(pa.list_(pa.float64())))\n    expected = pa.ExtensionArray.from_storage(MyListType(array.type), array)\n    assert result.equals(expected)\n\n    ext_type = MyListType(pa.list_(pa.float32()))\n    result = array.cast(ext_type)\n    expected = pa.ExtensionArray.from_storage(\n        ext_type, array.cast(ext_type.storage_type)\n    )\n    assert result.equals(expected)\n\n\ndef test_concat():\n    arr1 = pa.array([1, 2, 3], IntegerType())\n    arr2 = pa.array([4, 5, 6], IntegerType())\n\n    result = pa.concat_arrays([arr1, arr2])\n    expected = pa.array([1, 2, 3, 4, 5, 6], IntegerType())\n    assert result.equals(expected)\n\n    # nested in a struct\n    struct_arr1 = pa.StructArray.from_arrays([arr1], names=[\"a\"])\n    struct_arr2 = pa.StructArray.from_arrays([arr2], names=[\"a\"])\n    result = pa.concat_arrays([struct_arr1, struct_arr2])\n    expected = pa.StructArray.from_arrays([expected], names=[\"a\"])\n    assert result.equals(expected)\n\n\ndef test_null_storage_type():\n    ext_type = AnnotatedType(pa.null(), {\"key\": \"value\"})\n    storage = pa.array([None] * 10, pa.null())\n    arr = pa.ExtensionArray.from_storage(ext_type, storage)\n    assert arr.null_count == 10\n    arr.validate(full=True)\n\n\ndef example_batch():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n    return pa.RecordBatch.from_arrays([arr], [\"exts\"])\n\n\ndef check_example_batch(batch, *, expect_extension):\n    arr = batch.column(0)\n    if expect_extension:\n        assert isinstance(arr, pa.ExtensionArray)\n        assert arr.type.storage_type == pa.binary(3)\n        assert arr.storage.to_pylist() == [b\"foo\", b\"bar\"]\n    else:\n        assert arr.type == pa.binary(3)\n        assert arr.to_pylist() == [b\"foo\", b\"bar\"]\n    return arr\n\n\ndef test_ipc_unregistered():\n    batch = example_batch()\n    buf = ipc_write_batch(batch)\n    del batch\n\n    batch = ipc_read_batch(buf)\n    batch.validate(full=True)\n    check_example_batch(batch, expect_extension=False)\n\n\ndef test_ipc_registered():\n    with registered_extension_type(ParamExtType(1)):\n        batch = example_batch()\n        buf = ipc_write_batch(batch)\n        del batch\n\n        batch = ipc_read_batch(buf)\n        batch.validate(full=True)\n        arr = check_example_batch(batch, expect_extension=True)\n        assert arr.type == ParamExtType(3)\n\n\nclass PeriodArray(pa.ExtensionArray):\n    pass\n\n\nclass PeriodType(pa.ExtensionType):\n    def __init__(self, freq):\n        # attributes need to be set first before calling\n        # super init (as that calls serialize)\n        self._freq = freq\n        pa.ExtensionType.__init__(self, pa.int64(), 'test.period')\n\n    @property\n    def freq(self):\n        return self._freq\n\n    def __arrow_ext_serialize__(self):\n        return \"freq={}\".format(self.freq).encode()\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        serialized = serialized.decode()\n        assert serialized.startswith(\"freq=\")\n        freq = serialized.split('=')[1]\n        return PeriodType(freq)\n\n    def __eq__(self, other):\n        if isinstance(other, pa.BaseExtensionType):\n            return (isinstance(self, type(other)) and\n                    self.freq == other.freq)\n        else:\n            return NotImplemented\n\n\nclass PeriodTypeWithClass(PeriodType):\n    def __init__(self, freq):\n        PeriodType.__init__(self, freq)\n\n    def __arrow_ext_class__(self):\n        return PeriodArray\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        freq = PeriodType.__arrow_ext_deserialize__(\n            storage_type, serialized).freq\n        return PeriodTypeWithClass(freq)\n\n\nclass PeriodTypeWithToPandasDtype(PeriodType):\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        freq = PeriodType.__arrow_ext_deserialize__(\n            storage_type, serialized).freq\n        return PeriodTypeWithToPandasDtype(freq)\n\n    def to_pandas_dtype(self):\n        import pandas as pd\n        return pd.PeriodDtype(freq=self.freq)\n\n\n@pytest.fixture(params=[PeriodType('D'),\n                        PeriodTypeWithClass('D'),\n                        PeriodTypeWithToPandasDtype('D')])\ndef registered_period_type(request):\n    # setup\n    period_type = request.param\n    period_class = period_type.__arrow_ext_class__()\n    pa.register_extension_type(period_type)\n    yield period_type, period_class\n    # teardown\n    try:\n        pa.unregister_extension_type('test.period')\n    except KeyError:\n        pass\n\n\ndef test_generic_ext_type():\n    period_type = PeriodType('D')\n    assert period_type.extension_name == \"test.period\"\n    assert period_type.storage_type == pa.int64()\n    # default ext_class expected.\n    assert period_type.__arrow_ext_class__() == pa.ExtensionArray\n\n\ndef test_generic_ext_type_ipc(registered_period_type):\n    period_type, period_class = registered_period_type\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n    # check the built array has exactly the expected clss\n    assert isinstance(arr, period_class)\n\n    buf = ipc_write_batch(batch)\n    del batch\n    batch = ipc_read_batch(buf)\n\n    result = batch.column(0)\n    # check the deserialized array class is the expected one\n    assert isinstance(result, period_class)\n    assert result.type.extension_name == \"test.period\"\n    assert arr.storage.to_pylist() == [1, 2, 3, 4]\n\n    # we get back an actual PeriodType\n    assert isinstance(result.type, PeriodType)\n    assert result.type.freq == 'D'\n    assert result.type == period_type\n\n    # using different parametrization as how it was registered\n    period_type_H = period_type.__class__('H')\n    assert period_type_H.extension_name == \"test.period\"\n    assert period_type_H.freq == 'H'\n\n    arr = pa.ExtensionArray.from_storage(period_type_H, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n\n    buf = ipc_write_batch(batch)\n    del batch\n    batch = ipc_read_batch(buf)\n    result = batch.column(0)\n    assert isinstance(result.type, PeriodType)\n    assert result.type.freq == 'H'\n    assert isinstance(result, period_class)\n\n\ndef test_generic_ext_type_ipc_unknown(registered_period_type):\n    period_type, _ = registered_period_type\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n\n    buf = ipc_write_batch(batch)\n    del batch\n\n    # unregister type before loading again => reading unknown extension type\n    # as plain array (but metadata in schema's field are preserved)\n    pa.unregister_extension_type('test.period')\n\n    batch = ipc_read_batch(buf)\n    result = batch.column(0)\n\n    assert isinstance(result, pa.Int64Array)\n    ext_field = batch.schema.field('ext')\n    assert ext_field.metadata == {\n        b'ARROW:extension:metadata': b'freq=D',\n        b'ARROW:extension:name': b'test.period'\n    }\n\n\ndef test_generic_ext_type_equality():\n    period_type = PeriodType('D')\n    assert period_type.extension_name == \"test.period\"\n\n    period_type2 = PeriodType('D')\n    period_type3 = PeriodType('H')\n    assert period_type == period_type2\n    assert not period_type == period_type3\n\n\ndef test_generic_ext_type_pickling(registered_period_type, pickle_module):\n    # GH-36038\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        period_type, _ = registered_period_type\n        ser = pickle_module.dumps(period_type, protocol=proto)\n        period_type_pickled = pickle_module.loads(ser)\n        assert period_type == period_type_pickled\n\n\ndef test_generic_ext_array_pickling(registered_period_type, pickle_module):\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        period_type, _ = registered_period_type\n        storage = pa.array([1, 2, 3, 4], pa.int64())\n        arr = pa.ExtensionArray.from_storage(period_type, storage)\n        ser = pickle_module.dumps(arr, protocol=proto)\n        del storage, arr\n        arr = pickle_module.loads(ser)\n        arr.validate()\n        assert isinstance(arr, pa.ExtensionArray)\n        assert arr.type == period_type\n        assert arr.type.storage_type == pa.int64()\n        assert arr.storage.type == pa.int64()\n        assert arr.storage.to_pylist() == [1, 2, 3, 4]\n\n\ndef test_generic_ext_type_register(registered_period_type):\n    # test that trying to register other type does not segfault\n    with pytest.raises(TypeError):\n        pa.register_extension_type(pa.string())\n\n    # register second time raises KeyError\n    period_type = PeriodType('D')\n    with pytest.raises(KeyError):\n        pa.register_extension_type(period_type)\n\n\n@pytest.mark.parquet\ndef test_parquet_period(tmpdir, registered_period_type):\n    # Parquet support for primitive extension types\n    period_type, period_class = registered_period_type\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n    table = pa.table([arr], names=[\"ext\"])\n\n    import pyarrow.parquet as pq\n\n    filename = tmpdir / 'period_extension_type.parquet'\n    pq.write_table(table, filename)\n\n    # Stored in parquet as storage type but with extension metadata saved\n    # in the serialized arrow schema\n    meta = pq.read_metadata(filename)\n    assert meta.schema.column(0).physical_type == \"INT64\"\n    assert b\"ARROW:schema\" in meta.metadata\n\n    import base64\n    decoded_schema = base64.b64decode(meta.metadata[b\"ARROW:schema\"])\n    schema = pa.ipc.read_schema(pa.BufferReader(decoded_schema))\n    # Since the type could be reconstructed, the extension type metadata is\n    # absent.\n    assert schema.field(\"ext\").metadata == {}\n\n    # When reading in, properly create extension type if it is registered\n    result = pq.read_table(filename)\n    result.validate(full=True)\n    assert result.schema.field(\"ext\").type == period_type\n    assert result.schema.field(\"ext\").metadata == {}\n    # Get the exact array class defined by the registered type.\n    result_array = result.column(\"ext\").chunk(0)\n    assert type(result_array) is period_class\n\n    # When the type is not registered, read in as storage type\n    pa.unregister_extension_type(period_type.extension_name)\n    result = pq.read_table(filename)\n    result.validate(full=True)\n    assert result.schema.field(\"ext\").type == pa.int64()\n    # The extension metadata is present for roundtripping.\n    assert result.schema.field(\"ext\").metadata == {\n        b'ARROW:extension:metadata': b'freq=D',\n        b'ARROW:extension:name': b'test.period'\n    }\n\n\n@pytest.mark.parquet\ndef test_parquet_extension_with_nested_storage(tmpdir):\n    # Parquet support for extension types with nested storage type\n    import pyarrow.parquet as pq\n\n    struct_array = pa.StructArray.from_arrays(\n        [pa.array([0, 1], type=\"int64\"), pa.array([4, 5], type=\"int64\")],\n        names=[\"left\", \"right\"])\n    list_array = pa.array([[1, 2, 3], [4, 5]], type=pa.list_(pa.int32()))\n\n    mystruct_array = pa.ExtensionArray.from_storage(MyStructType(),\n                                                    struct_array)\n    mylist_array = pa.ExtensionArray.from_storage(\n        MyListType(list_array.type), list_array)\n\n    orig_table = pa.table({'structs': mystruct_array,\n                           'lists': mylist_array})\n    filename = tmpdir / 'nested_extension_storage.parquet'\n    pq.write_table(orig_table, filename)\n\n    # Unregistered\n    table = pq.read_table(filename)\n    table.validate(full=True)\n    assert table.column('structs').type == struct_array.type\n    assert table.column('structs').combine_chunks() == struct_array\n    assert table.column('lists').type == list_array.type\n    assert table.column('lists').combine_chunks() == list_array\n\n    # Registered\n    with registered_extension_type(mystruct_array.type):\n        with registered_extension_type(mylist_array.type):\n            table = pq.read_table(filename)\n            table.validate(full=True)\n            assert table.column('structs').type == mystruct_array.type\n            assert table.column('lists').type == mylist_array.type\n            assert table == orig_table\n\n            # Cannot select a subfield of an extension type with\n            # a struct storage type.\n            with pytest.raises(pa.ArrowInvalid,\n                               match='without all of its fields'):\n                pq.ParquetFile(filename).read(columns=['structs.left'])\n\n\n@pytest.mark.parquet\ndef test_parquet_nested_extension(tmpdir):\n    # Parquet support for extension types nested in struct or list\n    import pyarrow.parquet as pq\n\n    ext_type = IntegerType()\n    storage = pa.array([4, 5, 6, 7], type=pa.int64())\n    ext_array = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    # Struct of extensions\n    struct_array = pa.StructArray.from_arrays(\n        [storage, ext_array],\n        names=['ints', 'exts'])\n\n    orig_table = pa.table({'structs': struct_array})\n    filename = tmpdir / 'struct_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    table.validate(full=True)\n    assert table.column(0).type == pa.struct({'ints': pa.int64(),\n                                              'exts': pa.int64()})\n    with registered_extension_type(ext_type):\n        table = pq.read_table(filename)\n        table.validate(full=True)\n        assert table.column(0).type == struct_array.type\n        assert table == orig_table\n\n    # List of extensions\n    list_array = pa.ListArray.from_arrays([0, 1, None, 3], ext_array)\n\n    orig_table = pa.table({'lists': list_array})\n    filename = tmpdir / 'list_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    table.validate(full=True)\n    assert table.column(0).type == pa.list_(pa.int64())\n    with registered_extension_type(ext_type):\n        table = pq.read_table(filename)\n        table.validate(full=True)\n        assert table.column(0).type == list_array.type\n        assert table == orig_table\n\n    # Large list of extensions\n    list_array = pa.LargeListArray.from_arrays([0, 1, None, 3], ext_array)\n\n    orig_table = pa.table({'lists': list_array})\n    filename = tmpdir / 'list_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    table.validate(full=True)\n    assert table.column(0).type == pa.large_list(pa.int64())\n    with registered_extension_type(ext_type):\n        table = pq.read_table(filename)\n        table.validate(full=True)\n        assert table.column(0).type == list_array.type\n        assert table == orig_table\n\n\n@pytest.mark.parquet\ndef test_parquet_extension_nested_in_extension(tmpdir):\n    # Parquet support for extension<list<extension>>\n    import pyarrow.parquet as pq\n\n    inner_ext_type = IntegerType()\n    inner_storage = pa.array([4, 5, 6, 7], type=pa.int64())\n    inner_ext_array = pa.ExtensionArray.from_storage(inner_ext_type,\n                                                     inner_storage)\n\n    list_array = pa.ListArray.from_arrays([0, 1, None, 3], inner_ext_array)\n    mylist_array = pa.ExtensionArray.from_storage(\n        MyListType(list_array.type), list_array)\n\n    orig_table = pa.table({'lists': mylist_array})\n    filename = tmpdir / 'ext_of_list_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    assert table.column(0).type == pa.list_(pa.int64())\n    with registered_extension_type(mylist_array.type):\n        with registered_extension_type(inner_ext_array.type):\n            table = pq.read_table(filename)\n            assert table.column(0).type == mylist_array.type\n            assert table == orig_table\n\n\ndef test_to_numpy():\n    period_type = PeriodType('D')\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n\n    expected = storage.to_numpy()\n    result = arr.to_numpy()\n    np.testing.assert_array_equal(result, expected)\n\n    result = np.asarray(arr)\n    np.testing.assert_array_equal(result, expected)\n\n    # chunked array\n    a1 = pa.chunked_array([arr, arr])\n    a2 = pa.chunked_array([arr, arr], type=period_type)\n    expected = np.hstack([expected, expected])\n\n    for charr in [a1, a2]:\n        assert charr.type == period_type\n        for result in [np.asarray(charr), charr.to_numpy()]:\n            assert result.dtype == np.int64\n            np.testing.assert_array_equal(result, expected)\n\n    # zero chunks\n    charr = pa.chunked_array([], type=period_type)\n    assert charr.type == period_type\n\n    for result in [np.asarray(charr), charr.to_numpy()]:\n        assert result.dtype == np.int64\n        np.testing.assert_array_equal(result, np.array([], dtype='int64'))\n\n\ndef test_empty_take():\n    # https://issues.apache.org/jira/browse/ARROW-13474\n    ext_type = IntegerType()\n    storage = pa.array([], type=pa.int64())\n    empty_arr = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    result = empty_arr.filter(pa.array([], pa.bool_()))\n    assert len(result) == 0\n    assert result.equals(empty_arr)\n\n    result = empty_arr.take(pa.array([], pa.int32()))\n    assert len(result) == 0\n    assert result.equals(empty_arr)\n\n\n@pytest.mark.parametrize(\"data,ty\", (\n    ([1, 2, 3], IntegerType),\n    ([\"cat\", \"dog\", \"horse\"], LabelType)\n))\n@pytest.mark.parametrize(\n    \"into\", [\"to_numpy\", pytest.param(\"to_pandas\", marks=pytest.mark.pandas)])\ndef test_extension_array_to_numpy_pandas(data, ty, into):\n    storage = pa.array(data)\n    ext_arr = pa.ExtensionArray.from_storage(ty(), storage)\n    offsets = pa.array([0, 1, 2, 3])\n    list_arr = pa.ListArray.from_arrays(offsets, ext_arr)\n    result = getattr(list_arr, into)(zero_copy_only=False)\n\n    list_arr_storage_type = list_arr.cast(pa.list_(ext_arr.type.storage_type))\n    expected = getattr(list_arr_storage_type, into)(zero_copy_only=False)\n    if into == \"to_pandas\":\n        assert result.equals(expected)\n    else:\n        assert np.array_equal(result, expected)\n\n\ndef test_array_constructor():\n    ext_type = IntegerType()\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    expected = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    result = pa.array([1, 2, 3], type=IntegerType())\n    assert result.equals(expected)\n\n    result = pa.array(np.array([1, 2, 3]), type=IntegerType())\n    assert result.equals(expected)\n\n    result = pa.array(np.array([1.0, 2.0, 3.0]), type=IntegerType())\n    assert result.equals(expected)\n\n\n@pytest.mark.pandas\ndef test_array_constructor_from_pandas():\n    import pandas as pd\n\n    ext_type = IntegerType()\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    expected = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    result = pa.array(pd.Series([1, 2, 3]), type=IntegerType())\n    assert result.equals(expected)\n\n    result = pa.array(\n        pd.Series([1, 2, 3], dtype=\"category\"), type=IntegerType()\n    )\n    assert result.equals(expected)\n\n\n@pytest.mark.cython\ndef test_cpp_extension_in_python(tmpdir):\n    from .test_cython import (\n        setup_template, compiler_opts, test_ld_path, test_util, here)\n    with tmpdir.as_cwd():\n        # Set up temporary workspace\n        pyx_file = 'extensions.pyx'\n        shutil.copyfile(os.path.join(here, pyx_file),\n                        os.path.join(str(tmpdir), pyx_file))\n        # Create setup.py file\n        setup_code = setup_template.format(pyx_file=pyx_file,\n                                           compiler_opts=compiler_opts,\n                                           test_ld_path=test_ld_path)\n        with open('setup.py', 'w') as f:\n            f.write(setup_code)\n\n        subprocess_env = test_util.get_modified_env_with_pythonpath()\n\n        # Compile extension module\n        subprocess.check_call([sys.executable, 'setup.py',\n                               'build_ext', '--inplace'],\n                              env=subprocess_env)\n\n    sys.path.insert(0, str(tmpdir))\n    mod = __import__('extensions')\n\n    uuid_type = mod._make_uuid_type()\n    assert uuid_type.extension_name == \"uuid\"\n    assert uuid_type.storage_type == pa.binary(16)\n\n    array = mod._make_uuid_array()\n    assert array.type == uuid_type\n    assert array.to_pylist() == [b'abcdefghijklmno0', b'0onmlkjihgfedcba']\n    assert array[0].as_py() == b'abcdefghijklmno0'\n    assert array[1].as_py() == b'0onmlkjihgfedcba'\n\n    buf = ipc_write_batch(pa.RecordBatch.from_arrays([array], [\"uuid\"]))\n\n    batch = ipc_read_batch(buf)\n    reconstructed_array = batch.column(0)\n    assert reconstructed_array.type == uuid_type\n    assert reconstructed_array == array\n\n\ndef test_tensor_type():\n    tensor_type = pa.fixed_shape_tensor(pa.int8(), [2, 3])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert tensor_type.storage_type == pa.list_(pa.int8(), 6)\n    assert tensor_type.shape == [2, 3]\n    assert tensor_type.dim_names is None\n    assert tensor_type.permutation is None\n\n    tensor_type = pa.fixed_shape_tensor(pa.float64(), [2, 2, 3],\n                                        permutation=[0, 2, 1])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert tensor_type.storage_type == pa.list_(pa.float64(), 12)\n    assert tensor_type.shape == [2, 2, 3]\n    assert tensor_type.dim_names is None\n    assert tensor_type.permutation == [0, 2, 1]\n\n    tensor_type = pa.fixed_shape_tensor(pa.bool_(), [2, 2, 3],\n                                        dim_names=['C', 'H', 'W'])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert tensor_type.storage_type == pa.list_(pa.bool_(), 12)\n    assert tensor_type.shape == [2, 2, 3]\n    assert tensor_type.dim_names == ['C', 'H', 'W']\n    assert tensor_type.permutation is None\n\n\n@pytest.mark.parametrize(\"value_type\", (np.int8(), np.int64(), np.float32()))\ndef test_tensor_class_methods(value_type):\n    from numpy.lib.stride_tricks import as_strided\n    arrow_type = pa.from_numpy_dtype(value_type)\n\n    tensor_type = pa.fixed_shape_tensor(arrow_type, [2, 3])\n    storage = pa.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]],\n                       pa.list_(arrow_type, 6))\n    arr = pa.ExtensionArray.from_storage(tensor_type, storage)\n    expected = np.array(\n        [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]], dtype=value_type)\n    np.testing.assert_array_equal(arr.to_tensor(), expected)\n    np.testing.assert_array_equal(arr.to_numpy_ndarray(), expected)\n\n    expected = np.array([[[7, 8, 9], [10, 11, 12]]], dtype=value_type)\n    result = arr[1:].to_numpy_ndarray()\n    np.testing.assert_array_equal(result, expected)\n\n    values = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]\n    flat_arr = np.array(values[0], dtype=value_type)\n    bw = value_type.itemsize\n    storage = pa.array(values, pa.list_(arrow_type, 12))\n\n    tensor_type = pa.fixed_shape_tensor(arrow_type, [2, 2, 3], permutation=[0, 1, 2])\n    result = pa.ExtensionArray.from_storage(tensor_type, storage)\n    expected = np.array(\n        [[[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]], dtype=value_type)\n    np.testing.assert_array_equal(result.to_numpy_ndarray(), expected)\n\n    result = flat_arr.reshape(1, 2, 3, 2)\n    expected = np.array(\n        [[[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]]]], dtype=value_type)\n    np.testing.assert_array_equal(result, expected)\n\n    tensor_type = pa.fixed_shape_tensor(arrow_type, [2, 2, 3], permutation=[0, 2, 1])\n    result = pa.ExtensionArray.from_storage(tensor_type, storage)\n    expected = as_strided(flat_arr, shape=(1, 2, 3, 2),\n                          strides=(bw * 12, bw * 6, bw, bw * 3))\n    np.testing.assert_array_equal(result.to_numpy_ndarray(), expected)\n\n    tensor_type = pa.fixed_shape_tensor(arrow_type, [2, 2, 3], permutation=[2, 0, 1])\n    result = pa.ExtensionArray.from_storage(tensor_type, storage)\n    expected = as_strided(flat_arr, shape=(1, 3, 2, 2),\n                          strides=(bw * 12, bw, bw * 6, bw * 2))\n    np.testing.assert_array_equal(result.to_numpy_ndarray(), expected)\n\n    assert result.type.permutation == [2, 0, 1]\n    assert result.type.shape == [2, 2, 3]\n    assert result.to_tensor().shape == (1, 3, 2, 2)\n    assert result.to_tensor().strides == (12 * bw, 1 * bw, 6 * bw, 2 * bw)\n\n\n@pytest.mark.parametrize(\"value_type\", (np.int8(), np.int64(), np.float32()))\ndef test_tensor_array_from_numpy(value_type):\n    from numpy.lib.stride_tricks import as_strided\n    arrow_type = pa.from_numpy_dtype(value_type)\n\n    arr = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]],\n                   dtype=value_type, order=\"C\")\n    tensor_array_from_numpy = pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n    assert isinstance(tensor_array_from_numpy.type, pa.FixedShapeTensorType)\n    assert tensor_array_from_numpy.type.value_type == arrow_type\n    assert tensor_array_from_numpy.type.shape == [2, 3]\n\n    arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]],\n                   dtype=value_type, order=\"F\")\n    with pytest.raises(ValueError, match=\"First stride needs to be largest\"):\n        pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n\n    flat_arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=value_type)\n    bw = value_type.itemsize\n\n    arr = flat_arr.reshape(1, 3, 4)\n    tensor_array_from_numpy = pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n    assert tensor_array_from_numpy.type.shape == [3, 4]\n    assert tensor_array_from_numpy.type.permutation == [0, 1]\n    assert tensor_array_from_numpy.to_tensor() == pa.Tensor.from_numpy(arr)\n\n    arr = as_strided(flat_arr, shape=(1, 2, 3, 2),\n                     strides=(bw * 12, bw * 6, bw, bw * 3))\n    tensor_array_from_numpy = pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n    assert tensor_array_from_numpy.type.shape == [2, 2, 3]\n    assert tensor_array_from_numpy.type.permutation == [0, 2, 1]\n    assert tensor_array_from_numpy.to_tensor() == pa.Tensor.from_numpy(arr)\n\n    arr = flat_arr.reshape(1, 2, 3, 2)\n    result = pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n    expected = np.array(\n        [[[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]]]], dtype=value_type)\n    np.testing.assert_array_equal(result.to_numpy_ndarray(), expected)\n\n    arr = np.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]], dtype=value_type)\n    expected = arr[1:]\n    result = pa.FixedShapeTensorArray.from_numpy_ndarray(arr)[1:].to_numpy_ndarray()\n    np.testing.assert_array_equal(result, expected)\n\n    arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=value_type)\n    with pytest.raises(ValueError, match=\"Cannot convert 1D array or scalar to fixed\"):\n        pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n\n    arr = np.array(1, dtype=value_type)\n    with pytest.raises(ValueError, match=\"Cannot convert 1D array or scalar to fixed\"):\n        pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n\n    arr = np.array([], dtype=value_type)\n\n    with pytest.raises(ValueError, match=\"Cannot convert 1D array or scalar to fixed\"):\n        pa.FixedShapeTensorArray.from_numpy_ndarray(arr.reshape((0)))\n\n    with pytest.raises(ValueError, match=\"Expected a non-empty ndarray\"):\n        pa.FixedShapeTensorArray.from_numpy_ndarray(arr.reshape((0, 3, 2)))\n\n    with pytest.raises(ValueError, match=\"Expected a non-empty ndarray\"):\n        pa.FixedShapeTensorArray.from_numpy_ndarray(arr.reshape((3, 0, 2)))\n\n\n@pytest.mark.parametrize(\"tensor_type\", (\n    pa.fixed_shape_tensor(pa.int8(), [2, 2, 3]),\n    pa.fixed_shape_tensor(pa.int8(), [2, 2, 3], permutation=[0, 2, 1]),\n    pa.fixed_shape_tensor(pa.int8(), [2, 2, 3], dim_names=['C', 'H', 'W'])\n))\ndef test_tensor_type_ipc(tensor_type):\n    storage = pa.array([[1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]], pa.list_(pa.int8(), 12))\n    arr = pa.ExtensionArray.from_storage(tensor_type, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n\n    # check the built array has exactly the expected clss\n    tensor_class = tensor_type.__arrow_ext_class__()\n    assert isinstance(arr, tensor_class)\n\n    buf = ipc_write_batch(batch)\n    del batch\n    batch = ipc_read_batch(buf)\n\n    result = batch.column(0)\n    # check the deserialized array class is the expected one\n    assert isinstance(result, tensor_class)\n    assert result.type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert arr.storage.to_pylist() == [[1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]]\n\n    # we get back an actual TensorType\n    assert isinstance(result.type, pa.FixedShapeTensorType)\n    assert result.type.value_type == pa.int8()\n    assert result.type.shape == [2, 2, 3]\n\n\ndef test_tensor_type_equality():\n    tensor_type = pa.fixed_shape_tensor(pa.int8(), [2, 2, 3])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n\n    tensor_type2 = pa.fixed_shape_tensor(pa.int8(), [2, 2, 3])\n    tensor_type3 = pa.fixed_shape_tensor(pa.uint8(), [2, 2, 3])\n    assert tensor_type == tensor_type2\n    assert not tensor_type == tensor_type3\n\n\ndef test_tensor_type_cast():\n    tensor_type = pa.fixed_shape_tensor(pa.int8(), [2, 3])\n    inner = pa.array(range(18), pa.int8())\n    storage = pa.FixedSizeListArray.from_arrays(inner, 6)\n\n    # cast storage -> extension type\n    result = storage.cast(tensor_type)\n    expected = pa.ExtensionArray.from_storage(tensor_type, storage)\n    assert result.equals(expected)\n\n    # cast extension type -> storage type\n    storage_result = result.cast(storage.type)\n    assert storage_result.equals(storage)\n\n\n@pytest.mark.pandas\ndef test_extension_to_pandas_storage_type(registered_period_type):\n    period_type, _ = registered_period_type\n    np_arr = np.array([1, 2, 3, 4], dtype='i8')\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n\n    if isinstance(period_type, PeriodTypeWithToPandasDtype):\n        pandas_dtype = period_type.to_pandas_dtype()\n    else:\n        pandas_dtype = np_arr.dtype\n\n    # Test arrays\n    result = arr.to_pandas()\n    assert result.dtype == pandas_dtype\n\n    # Test chunked arrays\n    chunked_arr = pa.chunked_array([arr])\n    result = chunked_arr.to_numpy()\n    assert result.dtype == np_arr.dtype\n\n    result = chunked_arr.to_pandas()\n    assert result.dtype == pandas_dtype\n\n    # Test Table.to_pandas\n    data = [\n        pa.array([1, 2, 3, 4]),\n        pa.array(['foo', 'bar', None, None]),\n        pa.array([True, None, True, False]),\n        arr\n    ]\n    my_schema = pa.schema([('f0', pa.int8()),\n                           ('f1', pa.string()),\n                           ('f2', pa.bool_()),\n                           ('ext', period_type)])\n    table = pa.Table.from_arrays(data, schema=my_schema)\n    result = table.to_pandas()\n    assert result[\"ext\"].dtype == pandas_dtype\n\n    import pandas as pd\n    # Skip tests for 2.0.x, See: GH-35821\n    if (\n        Version(pd.__version__) >= Version(\"2.1.0\")\n    ):\n        # Check the usage of types_mapper\n        result = table.to_pandas(types_mapper=pd.ArrowDtype)\n        assert isinstance(result[\"ext\"].dtype, pd.ArrowDtype)\n\n\ndef test_tensor_type_is_picklable(pickle_module):\n    # GH-35599\n\n    expected_type = pa.fixed_shape_tensor(pa.int32(), (2, 2))\n    result = pickle_module.loads(pickle_module.dumps(expected_type))\n\n    assert result == expected_type\n\n    arr = [[1, 2, 3, 4], [10, 20, 30, 40], [100, 200, 300, 400]]\n    storage = pa.array(arr, pa.list_(pa.int32(), 4))\n    expected_arr = pa.ExtensionArray.from_storage(expected_type, storage)\n    result = pickle_module.loads(pickle_module.dumps(expected_arr))\n\n    assert result == expected_arr\n\n\n@pytest.mark.parametrize((\"tensor_type\", \"text\"), [\n    (\n        pa.fixed_shape_tensor(pa.int8(), [2, 2, 3]),\n        'fixed_shape_tensor[value_type=int8, shape=[2,2,3]]'\n    ),\n    (\n        pa.fixed_shape_tensor(pa.int32(), [2, 2, 3], permutation=[0, 2, 1]),\n        'fixed_shape_tensor[value_type=int32, shape=[2,2,3], permutation=[0,2,1]]'\n    ),\n    (\n        pa.fixed_shape_tensor(pa.int64(), [2, 2, 3], dim_names=['C', 'H', 'W']),\n        'fixed_shape_tensor[value_type=int64, shape=[2,2,3], dim_names=[C,H,W]]'\n    )\n])\ndef test_tensor_type_str(tensor_type, text):\n    tensor_type_str = tensor_type.__str__()\n    assert text in tensor_type_str\n\n\ndef test_legacy_int_type():\n    with pytest.warns(FutureWarning, match=\"PyExtensionType is deprecated\"):\n        ext_ty = LegacyIntType()\n    arr = pa.array([1, 2, 3], type=ext_ty.storage_type)\n    ext_arr = pa.ExtensionArray.from_storage(ext_ty, arr)\n    batch = pa.RecordBatch.from_arrays([ext_arr], names=['ext'])\n    buf = ipc_write_batch(batch)\n\n    with pytest.warns((RuntimeWarning, FutureWarning)):\n        batch = ipc_read_batch(buf)\n        assert isinstance(batch.column(0).type, pa.UnknownExtensionType)\n\n    with enabled_auto_load():\n        with pytest.warns(FutureWarning, match=\"PyExtensionType is deprecated\"):\n            batch = ipc_read_batch(buf)\n            assert isinstance(batch.column(0).type, LegacyIntType)\n            assert batch.column(0) == ext_arr\n", "python/pyarrow/tests/test_cffi.py": "# -*- coding: utf-8 -*-\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport ctypes\nimport gc\n\nimport pyarrow as pa\ntry:\n    from pyarrow.cffi import ffi\nexcept ImportError:\n    ffi = None\n\nimport pytest\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\nexcept ImportError:\n    pd = tm = None\n\n\nneeds_cffi = pytest.mark.skipif(ffi is None,\n                                reason=\"test needs cffi package installed\")\n\nassert_schema_released = pytest.raises(\n    ValueError, match=\"Cannot import released ArrowSchema\")\n\nassert_array_released = pytest.raises(\n    ValueError, match=\"Cannot import released ArrowArray\")\n\nassert_stream_released = pytest.raises(\n    ValueError, match=\"Cannot import released Arrow Stream\")\n\n\ndef PyCapsule_IsValid(capsule, name):\n    return ctypes.pythonapi.PyCapsule_IsValid(ctypes.py_object(capsule), name) == 1\n\n\n@contextlib.contextmanager\ndef registered_extension_type(ext_type):\n    pa.register_extension_type(ext_type)\n    try:\n        yield\n    finally:\n        pa.unregister_extension_type(ext_type.extension_name)\n\n\nclass ParamExtType(pa.ExtensionType):\n\n    def __init__(self, width):\n        self._width = width\n        super().__init__(pa.binary(width),\n                         \"pyarrow.tests.test_cffi.ParamExtType\")\n\n    @property\n    def width(self):\n        return self._width\n\n    def __arrow_ext_serialize__(self):\n        return str(self.width).encode()\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        width = int(serialized.decode())\n        return cls(width)\n\n\ndef make_schema():\n    return pa.schema([('ints', pa.list_(pa.int32()))],\n                     metadata={b'key1': b'value1'})\n\n\ndef make_extension_schema():\n    return pa.schema([('ext', ParamExtType(3))],\n                     metadata={b'key1': b'value1'})\n\n\ndef make_extension_storage_schema():\n    # Should be kept in sync with make_extension_schema\n    return pa.schema([('ext', ParamExtType(3).storage_type)],\n                     metadata={b'key1': b'value1'})\n\n\ndef make_batch():\n    return pa.record_batch([[[1], [2, 42]]], make_schema())\n\n\ndef make_extension_batch():\n    schema = make_extension_schema()\n    ext_col = schema[0].type.wrap_array(pa.array([b\"foo\", b\"bar\"],\n                                                 type=pa.binary(3)))\n    return pa.record_batch([ext_col], schema)\n\n\ndef make_batches():\n    schema = make_schema()\n    return [\n        pa.record_batch([[[1], [2, 42]]], schema),\n        pa.record_batch([[None, [], [5, 6]]], schema),\n    ]\n\n\ndef make_serialized(schema, batches):\n    with pa.BufferOutputStream() as sink:\n        with pa.ipc.new_stream(sink, schema) as out:\n            for batch in batches:\n                out.write(batch)\n        return sink.getvalue()\n\n\n@needs_cffi\ndef test_export_import_type():\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    typ = pa.list_(pa.int32())\n    typ._export_to_c(ptr_schema)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    del typ\n    assert pa.total_allocated_bytes() > old_allocated\n    typ_new = pa.DataType._import_from_c(ptr_schema)\n    assert typ_new == pa.list_(pa.int32())\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        pa.DataType._import_from_c(ptr_schema)\n\n    # Invalid format string\n    pa.int32()._export_to_c(ptr_schema)\n    bad_format = ffi.new(\"char[]\", b\"zzz\")\n    c_schema.format = bad_format\n    with pytest.raises(ValueError,\n                       match=\"Invalid or unsupported format string\"):\n        pa.DataType._import_from_c(ptr_schema)\n    # Now released\n    with assert_schema_released:\n        pa.DataType._import_from_c(ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_field():\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    field = pa.field(\"test\", pa.list_(pa.int32()), nullable=True)\n    field._export_to_c(ptr_schema)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    del field\n    assert pa.total_allocated_bytes() > old_allocated\n\n    field_new = pa.Field._import_from_c(ptr_schema)\n    assert field_new == pa.field(\"test\", pa.list_(pa.int32()), nullable=True)\n    assert pa.total_allocated_bytes() == old_allocated\n\n    # Now released\n    with assert_schema_released:\n        pa.Field._import_from_c(ptr_schema)\n\n\ndef check_export_import_array(array_type, exporter, importer):\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n    c_array = ffi.new(f\"struct {array_type}*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    # Type is known up front\n    typ = pa.list_(pa.int32())\n    arr = pa.array([[1], [2, 42]], type=typ)\n    py_value = arr.to_pylist()\n    exporter(arr, ptr_array)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete recreate C++ object from exported pointer\n    del arr\n    arr_new = importer(ptr_array, typ)\n    assert arr_new.to_pylist() == py_value\n    assert arr_new.type == pa.list_(pa.int32())\n    assert pa.total_allocated_bytes() > old_allocated\n    del arr_new, typ\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_array_released:\n        importer(ptr_array, pa.list_(pa.int32()))\n\n    # Type is exported and imported at the same time\n    arr = pa.array([[1], [2, 42]], type=pa.list_(pa.int32()))\n    py_value = arr.to_pylist()\n    exporter(arr, ptr_array, ptr_schema)\n    # Delete and recreate C++ objects from exported pointers\n    del arr\n    arr_new = importer(ptr_array, ptr_schema)\n    assert arr_new.to_pylist() == py_value\n    assert arr_new.type == pa.list_(pa.int32())\n    assert pa.total_allocated_bytes() > old_allocated\n    del arr_new\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        importer(ptr_array, ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_array():\n    check_export_import_array(\n        \"ArrowArray\",\n        pa.Array._export_to_c,\n        pa.Array._import_from_c,\n    )\n\n\n@needs_cffi\ndef test_export_import_device_array():\n    check_export_import_array(\n        \"ArrowDeviceArray\",\n        pa.Array._export_to_c_device,\n        pa.Array._import_from_c_device,\n    )\n\n    # verify exported struct\n    c_array = ffi.new(\"struct ArrowDeviceArray*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n    arr = pa.array([[1], [2, 42]], type=pa.list_(pa.int32()))\n    arr._export_to_c_device(ptr_array)\n\n    assert c_array.device_type == 1  # ARROW_DEVICE_CPU 1\n    assert c_array.device_id == -1\n    assert c_array.array.length == 2\n\n\ndef check_export_import_schema(schema_factory, expected_schema_factory=None):\n    if expected_schema_factory is None:\n        expected_schema_factory = schema_factory\n\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    schema_factory()._export_to_c(ptr_schema)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    schema_new = pa.Schema._import_from_c(ptr_schema)\n    assert schema_new == expected_schema_factory()\n    assert pa.total_allocated_bytes() == old_allocated\n    del schema_new\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        pa.Schema._import_from_c(ptr_schema)\n\n    # Not a struct type\n    pa.int32()._export_to_c(ptr_schema)\n    with pytest.raises(ValueError,\n                       match=\"ArrowSchema describes non-struct type\"):\n        pa.Schema._import_from_c(ptr_schema)\n    # Now released\n    with assert_schema_released:\n        pa.Schema._import_from_c(ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_schema():\n    check_export_import_schema(make_schema)\n\n\n@needs_cffi\ndef test_export_import_schema_with_extension():\n    # Extension type is unregistered => the storage type is imported\n    check_export_import_schema(make_extension_schema,\n                               make_extension_storage_schema)\n\n    # Extension type is registered => the extension type is imported\n    with registered_extension_type(ParamExtType(1)):\n        check_export_import_schema(make_extension_schema)\n\n\n@needs_cffi\ndef test_export_import_schema_float_pointer():\n    # Previous versions of the R Arrow library used to pass pointer\n    # values as a double.\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    match = \"Passing a pointer value as a float is unsafe\"\n    with pytest.warns(UserWarning, match=match):\n        make_schema()._export_to_c(float(ptr_schema))\n    with pytest.warns(UserWarning, match=match):\n        schema_new = pa.Schema._import_from_c(float(ptr_schema))\n    assert schema_new == make_schema()\n\n\ndef check_export_import_batch(array_type, exporter, importer, batch_factory):\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n    c_array = ffi.new(f\"struct {array_type}*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    # Schema is known up front\n    batch = batch_factory()\n    schema = batch.schema\n    py_value = batch.to_pydict()\n    exporter(batch, ptr_array)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    del batch\n    batch_new = importer(ptr_array, schema)\n    assert batch_new.to_pydict() == py_value\n    assert batch_new.schema == schema\n    assert pa.total_allocated_bytes() > old_allocated\n    del batch_new, schema\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_array_released:\n        importer(ptr_array, make_schema())\n\n    # Type is exported and imported at the same time\n    batch = batch_factory()\n    py_value = batch.to_pydict()\n    batch._export_to_c(ptr_array, ptr_schema)\n    # Delete and recreate C++ objects from exported pointers\n    del batch\n    batch_new = importer(ptr_array, ptr_schema)\n    assert batch_new.to_pydict() == py_value\n    assert batch_new.schema == batch_factory().schema\n    assert pa.total_allocated_bytes() > old_allocated\n    del batch_new\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        importer(ptr_array, ptr_schema)\n\n    # Not a struct type\n    pa.int32()._export_to_c(ptr_schema)\n    batch_factory()._export_to_c(ptr_array)\n    with pytest.raises(ValueError,\n                       match=\"ArrowSchema describes non-struct type\"):\n        importer(ptr_array, ptr_schema)\n    # Now released\n    with assert_schema_released:\n        importer(ptr_array, ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_batch():\n    check_export_import_batch(\n        \"ArrowArray\",\n        pa.RecordBatch._export_to_c,\n        pa.RecordBatch._import_from_c,\n        make_batch,\n    )\n\n\n@needs_cffi\ndef test_export_import_batch_with_extension():\n    with registered_extension_type(ParamExtType(1)):\n        check_export_import_batch(\n            \"ArrowArray\",\n            pa.RecordBatch._export_to_c,\n            pa.RecordBatch._import_from_c,\n            make_extension_batch,\n        )\n\n\n@needs_cffi\ndef test_export_import_device_batch():\n    check_export_import_batch(\n        \"ArrowDeviceArray\",\n        pa.RecordBatch._export_to_c_device,\n        pa.RecordBatch._import_from_c_device,\n        make_batch,\n    )\n\n    # verify exported struct\n    c_array = ffi.new(\"struct ArrowDeviceArray*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n    batch = make_batch()\n    batch._export_to_c_device(ptr_array)\n    assert c_array.device_type == 1  # ARROW_DEVICE_CPU 1\n    assert c_array.device_id == -1\n    assert c_array.array.length == 2\n\n\ndef _export_import_batch_reader(ptr_stream, reader_factory):\n    # Prepare input\n    batches = make_batches()\n    schema = batches[0].schema\n\n    reader = reader_factory(schema, batches)\n    reader._export_to_c(ptr_stream)\n    # Delete and recreate C++ object from exported pointer\n    del reader, batches\n\n    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n    assert reader_new.schema == schema\n    got_batches = list(reader_new)\n    del reader_new\n    assert got_batches == make_batches()\n\n    # Test read_pandas()\n    if pd is not None:\n        batches = make_batches()\n        schema = batches[0].schema\n        expected_df = pa.Table.from_batches(batches).to_pandas()\n\n        reader = reader_factory(schema, batches)\n        reader._export_to_c(ptr_stream)\n        del reader, batches\n\n        reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n        got_df = reader_new.read_pandas()\n        del reader_new\n        tm.assert_frame_equal(expected_df, got_df)\n\n\ndef make_ipc_stream_reader(schema, batches):\n    return pa.ipc.open_stream(make_serialized(schema, batches))\n\n\ndef make_py_record_batch_reader(schema, batches):\n    return pa.RecordBatchReader.from_batches(schema, batches)\n\n\n@needs_cffi\n@pytest.mark.parametrize('reader_factory',\n                         [make_ipc_stream_reader,\n                          make_py_record_batch_reader])\ndef test_export_import_batch_reader(reader_factory):\n    c_stream = ffi.new(\"struct ArrowArrayStream*\")\n    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    _export_import_batch_reader(ptr_stream, reader_factory)\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    # Now released\n    with assert_stream_released:\n        pa.RecordBatchReader._import_from_c(ptr_stream)\n\n\n@needs_cffi\ndef test_export_import_exception_reader():\n    # See: https://github.com/apache/arrow/issues/37164\n    c_stream = ffi.new(\"struct ArrowArrayStream*\")\n    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    def gen():\n        if True:\n            try:\n                raise ValueError('foo')\n            except ValueError as e:\n                raise NotImplementedError('bar') from e\n        else:\n            yield from make_batches()\n\n    original = pa.RecordBatchReader.from_batches(make_schema(), gen())\n    original._export_to_c(ptr_stream)\n\n    reader = pa.RecordBatchReader._import_from_c(ptr_stream)\n    with pytest.raises(OSError) as exc_info:\n        reader.read_next_batch()\n\n    # inner *and* outer exception should be present\n    assert 'ValueError: foo' in str(exc_info.value)\n    assert 'NotImplementedError: bar' in str(exc_info.value)\n    # Stacktrace containing line of the raise statement\n    assert 'raise ValueError(\\'foo\\')' in str(exc_info.value)\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n\n@needs_cffi\ndef test_imported_batch_reader_error():\n    c_stream = ffi.new(\"struct ArrowArrayStream*\")\n    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))\n\n    schema = pa.schema([('foo', pa.int32())])\n    batches = [pa.record_batch([[1, 2, 3]], schema=schema),\n               pa.record_batch([[4, 5, 6]], schema=schema)]\n    buf = make_serialized(schema, batches)\n\n    # Open a corrupt/incomplete stream and export it\n    reader = pa.ipc.open_stream(buf[:-16])\n    reader._export_to_c(ptr_stream)\n    del reader\n\n    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n    batch = reader_new.read_next_batch()\n    assert batch == batches[0]\n    with pytest.raises(OSError,\n                       match=\"Expected to be able to read 16 bytes \"\n                             \"for message body, got 8\"):\n        reader_new.read_next_batch()\n\n    # Again, but call read_all()\n    reader = pa.ipc.open_stream(buf[:-16])\n    reader._export_to_c(ptr_stream)\n    del reader\n\n    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n    with pytest.raises(OSError,\n                       match=\"Expected to be able to read 16 bytes \"\n                             \"for message body, got 8\"):\n        reader_new.read_all()\n\n\n@pytest.mark.parametrize('obj', [pa.int32(), pa.field('foo', pa.int32()),\n                                 pa.schema({'foo': pa.int32()})],\n                         ids=['type', 'field', 'schema'])\ndef test_roundtrip_schema_capsule(obj):\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    capsule = obj.__arrow_c_schema__()\n    assert PyCapsule_IsValid(capsule, b\"arrow_schema\") == 1\n    assert pa.total_allocated_bytes() > old_allocated\n    obj_out = type(obj)._import_from_c_capsule(capsule)\n    assert obj_out == obj\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    capsule = obj.__arrow_c_schema__()\n\n    assert pa.total_allocated_bytes() > old_allocated\n    del capsule\n    assert pa.total_allocated_bytes() == old_allocated\n\n\n@pytest.mark.parametrize('arr,schema_accessor,bad_type,good_type', [\n    (pa.array(['a', 'b', 'c']), lambda x: x.type, pa.int32(), pa.string()),\n    (\n        pa.record_batch([pa.array(['a', 'b', 'c'])], names=['x']),\n        lambda x: x.schema,\n        pa.schema({'x': pa.int32()}),\n        pa.schema({'x': pa.string()})\n    ),\n], ids=['array', 'record_batch'])\ndef test_roundtrip_array_capsule(arr, schema_accessor, bad_type, good_type):\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    import_array = type(arr)._import_from_c_capsule\n\n    schema_capsule, capsule = arr.__arrow_c_array__()\n    assert PyCapsule_IsValid(schema_capsule, b\"arrow_schema\") == 1\n    assert PyCapsule_IsValid(capsule, b\"arrow_array\") == 1\n    arr_out = import_array(schema_capsule, capsule)\n    assert arr_out.equals(arr)\n\n    assert pa.total_allocated_bytes() > old_allocated\n    del arr_out\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    capsule = arr.__arrow_c_array__()\n\n    assert pa.total_allocated_bytes() > old_allocated\n    del capsule\n    assert pa.total_allocated_bytes() == old_allocated\n\n    with pytest.raises(ValueError,\n                       match=r\"Could not cast.* string to requested .* int32\"):\n        arr.__arrow_c_array__(bad_type.__arrow_c_schema__())\n\n    schema_capsule, array_capsule = arr.__arrow_c_array__(\n        good_type.__arrow_c_schema__())\n    arr_out = import_array(schema_capsule, array_capsule)\n    assert schema_accessor(arr_out) == good_type\n\n\n# TODO: implement requested_schema for stream\n@pytest.mark.parametrize('constructor', [\n    pa.RecordBatchReader.from_batches,\n    # Use a lambda because we need to re-order the parameters\n    lambda schema, batches: pa.Table.from_batches(batches, schema),\n], ids=['recordbatchreader', 'table'])\ndef test_roundtrip_reader_capsule(constructor):\n    batches = make_batches()\n    schema = batches[0].schema\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    obj = constructor(schema, batches)\n\n    capsule = obj.__arrow_c_stream__()\n    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1\n    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)\n    assert imported_reader.schema == schema\n    imported_batches = list(imported_reader)\n    assert len(imported_batches) == len(batches)\n    for batch, expected in zip(imported_batches, batches):\n        assert batch.equals(expected)\n\n    del obj, imported_reader, batch, expected, imported_batches\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    obj = constructor(schema, batches)\n\n    bad_schema = pa.schema({'ints': pa.int32()})\n    with pytest.raises(pa.lib.ArrowTypeError, match=\"Field 0 cannot be cast\"):\n        obj.__arrow_c_stream__(bad_schema.__arrow_c_schema__())\n\n    # Can work with matching schema\n    matching_schema = pa.schema({'ints': pa.list_(pa.int32())})\n    capsule = obj.__arrow_c_stream__(matching_schema.__arrow_c_schema__())\n    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)\n    assert imported_reader.schema == matching_schema\n    for batch, expected in zip(imported_reader, batches):\n        assert batch.equals(expected)\n\n\ndef test_roundtrip_batch_reader_capsule_requested_schema():\n    batch = make_batch()\n    requested_schema = pa.schema([('ints', pa.list_(pa.int64()))])\n    requested_capsule = requested_schema.__arrow_c_schema__()\n    batch_as_requested = batch.cast(requested_schema)\n\n    capsule = batch.__arrow_c_stream__(requested_capsule)\n    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1\n    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)\n    assert imported_reader.schema == requested_schema\n    assert imported_reader.read_next_batch().equals(batch_as_requested)\n    with pytest.raises(StopIteration):\n        imported_reader.read_next_batch()\n\n\ndef test_roundtrip_batch_reader_capsule():\n    batch = make_batch()\n\n    capsule = batch.__arrow_c_stream__()\n    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1\n    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)\n    assert imported_reader.schema == batch.schema\n    assert imported_reader.read_next_batch().equals(batch)\n    with pytest.raises(StopIteration):\n        imported_reader.read_next_batch()\n\n\ndef test_roundtrip_chunked_array_capsule():\n    chunked = pa.chunked_array([pa.array([\"a\", \"b\", \"c\"])])\n\n    capsule = chunked.__arrow_c_stream__()\n    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1\n    imported_chunked = pa.ChunkedArray._import_from_c_capsule(capsule)\n    assert imported_chunked.type == chunked.type\n    assert imported_chunked == chunked\n\n\ndef test_roundtrip_chunked_array_capsule_requested_schema():\n    chunked = pa.chunked_array([pa.array([\"a\", \"b\", \"c\"])])\n\n    # Requesting the same type should work\n    requested_capsule = chunked.type.__arrow_c_schema__()\n    capsule = chunked.__arrow_c_stream__(requested_capsule)\n    imported_chunked = pa.ChunkedArray._import_from_c_capsule(capsule)\n    assert imported_chunked == chunked\n\n    # Casting to something else should error if not possible\n    requested_type = pa.binary()\n    requested_capsule = requested_type.__arrow_c_schema__()\n    capsule = chunked.__arrow_c_stream__(requested_capsule)\n    imported_chunked = pa.ChunkedArray._import_from_c_capsule(capsule)\n    assert imported_chunked == chunked.cast(pa.binary())\n\n    requested_type = pa.int64()\n    requested_capsule = requested_type.__arrow_c_schema__()\n    with pytest.raises(\n        ValueError, match=\"Could not cast string to requested type int64\"\n    ):\n        chunked.__arrow_c_stream__(requested_capsule)\n\n\ndef test_import_device_no_cuda():\n    try:\n        import pyarrow.cuda  # noqa\n    except ImportError:\n        pass\n    else:\n        pytest.skip(\"pyarrow.cuda is available\")\n\n    c_array = ffi.new(\"struct ArrowDeviceArray*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n    arr = pa.array([1, 2, 3], type=pa.int64())\n    arr._export_to_c_device(ptr_array)\n\n    # patch the device type of the struct, this results in an invalid ArrowDeviceArray\n    # but this is just to test we raise am error before actually importing buffers\n    c_array.device_type = 2  # ARROW_DEVICE_CUDA\n\n    with pytest.raises(ImportError, match=\"Trying to import data on a CUDA device\"):\n        pa.Array._import_from_c_device(ptr_array, arr.type)\n", "python/pyarrow/tests/test_cpp_internals.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os.path\nfrom os.path import join as pjoin\n\nfrom pyarrow._pyarrow_cpp_tests import get_cpp_tests\n\n\ndef inject_cpp_tests(ns):\n    \"\"\"\n    Inject C++ tests as Python functions into namespace `ns` (a dict).\n    \"\"\"\n    for case in get_cpp_tests():\n        def wrapper(case=case):\n            case()\n        wrapper.__name__ = wrapper.__qualname__ = case.name\n        wrapper.__module__ = ns['__name__']\n        ns[case.name] = wrapper\n\n\ninject_cpp_tests(globals())\n\n\ndef test_pyarrow_include():\n    # We need to make sure that pyarrow/include is always\n    # created. Either with PyArrow C++ header files or with\n    # Arrow C++ and PyArrow C++ header files together\n\n    source = os.path.dirname(os.path.abspath(__file__))\n    pyarrow_dir = pjoin(source, '..')\n    pyarrow_include = pjoin(pyarrow_dir, 'include')\n    pyarrow_cpp_include = pjoin(pyarrow_include, 'arrow', 'python')\n\n    assert os.path.exists(pyarrow_include)\n    assert os.path.exists(pyarrow_cpp_include)\n", "python/pyarrow/tests/arrow_16597.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# This file is called from a test in test_flight.py.\nimport time\n\nimport pyarrow as pa\nimport pyarrow.flight as flight\n\n\nclass Server(flight.FlightServerBase):\n    def do_put(self, context, descriptor, reader, writer):\n        time.sleep(1)\n        raise flight.FlightCancelledError(\"\")\n\n\nif __name__ == \"__main__\":\n    server = Server(\"grpc://localhost:0\")\n    client = flight.connect(f\"grpc://localhost:{server.port}\")\n    schema = pa.schema([])\n    writer, reader = client.do_put(\n        flight.FlightDescriptor.for_command(b\"\"), schema)\n    writer.done_writing()\n", "python/pyarrow/tests/test_misc.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport subprocess\nimport sys\n\nimport pytest\n\nimport pyarrow as pa\nfrom pyarrow.lib import ArrowInvalid\n\n\ndef test_get_include():\n    include_dir = pa.get_include()\n    assert os.path.exists(os.path.join(include_dir, 'arrow', 'api.h'))\n\n\n@pytest.mark.skipif('sys.platform != \"win32\"')\ndef test_get_library_dirs_win32():\n    assert any(os.path.exists(os.path.join(directory, 'arrow.lib'))\n               for directory in pa.get_library_dirs())\n\n\ndef test_cpu_count():\n    n = pa.cpu_count()\n    assert n > 0\n    try:\n        pa.set_cpu_count(n + 5)\n        assert pa.cpu_count() == n + 5\n    finally:\n        pa.set_cpu_count(n)\n\n\ndef test_io_thread_count():\n    n = pa.io_thread_count()\n    assert n > 0\n    try:\n        pa.set_io_thread_count(n + 5)\n        assert pa.io_thread_count() == n + 5\n    finally:\n        pa.set_io_thread_count(n)\n\n\ndef test_env_var_io_thread_count():\n    # Test that the number of IO threads can be overridden with the\n    # ARROW_IO_THREADS environment variable.\n    code = \"\"\"if 1:\n        import pyarrow as pa\n        print(pa.io_thread_count())\n        \"\"\"\n\n    def run_with_env_var(env_var):\n        env = os.environ.copy()\n        env['ARROW_IO_THREADS'] = env_var\n        res = subprocess.run([sys.executable, \"-c\", code], env=env,\n                             capture_output=True)\n        res.check_returncode()\n        return res.stdout.decode(), res.stderr.decode()\n\n    out, err = run_with_env_var('17')\n    assert out.strip() == '17'\n    assert err == ''\n\n    for v in ('-1', 'z'):\n        out, err = run_with_env_var(v)\n        assert out.strip() == '8'  # default value\n        assert (\"ARROW_IO_THREADS does not contain a valid number of threads\"\n                in err.strip())\n\n\ndef test_build_info():\n    assert isinstance(pa.cpp_build_info, pa.BuildInfo)\n    assert isinstance(pa.cpp_version_info, pa.VersionInfo)\n    assert isinstance(pa.cpp_version, str)\n    assert isinstance(pa.__version__, str)\n    assert pa.cpp_build_info.version_info == pa.cpp_version_info\n\n    assert pa.cpp_build_info.build_type in (\n        'debug', 'release', 'minsizerel', 'relwithdebinfo')\n\n    # assert pa.version == pa.__version__  # XXX currently false\n\n\ndef test_runtime_info():\n    info = pa.runtime_info()\n    assert isinstance(info, pa.RuntimeInfo)\n    possible_simd_levels = ('none', 'sse4_2', 'avx', 'avx2', 'avx512')\n    assert info.simd_level in possible_simd_levels\n    assert info.detected_simd_level in possible_simd_levels\n\n    if info.simd_level != 'none':\n        env = os.environ.copy()\n        env['ARROW_USER_SIMD_LEVEL'] = 'none'\n        code = f\"\"\"if 1:\n            import pyarrow as pa\n\n            info = pa.runtime_info()\n            assert info.simd_level == 'none', info.simd_level\n            assert info.detected_simd_level == {info.detected_simd_level!r},\\\n                info.detected_simd_level\n            \"\"\"\n        subprocess.check_call([sys.executable, \"-c\", code], env=env)\n\n\ndef test_import_at_shutdown():\n    # GH-38626: importing PyArrow at interpreter shutdown would crash\n    code = \"\"\"if 1:\n        import atexit\n\n        def import_arrow():\n            import pyarrow\n\n        atexit.register(import_arrow)\n        \"\"\"\n    subprocess.check_call([sys.executable, \"-c\", code])\n\n\n@pytest.mark.skipif(sys.platform == \"win32\",\n                    reason=\"Path to timezone database is not configurable \"\n                           \"on non-Windows platforms\")\ndef test_set_timezone_db_path_non_windows():\n    # set_timezone_db_path raises an error on non-Windows platforms\n    with pytest.raises(ArrowInvalid,\n                       match=\"Arrow was set to use OS timezone \"\n                             \"database at compile time\"):\n        pa.set_timezone_db_path(\"path\")\n\n\n@pytest.mark.parametrize('klass', [\n    pa.Field,\n    pa.Schema,\n    pa.ChunkedArray,\n    pa.RecordBatch,\n    pa.Table,\n    pa.Buffer,\n    pa.Array,\n    pa.Tensor,\n    pa.DataType,\n    pa.ListType,\n    pa.LargeListType,\n    pa.FixedSizeListType,\n    pa.ListViewType,\n    pa.LargeListViewType,\n    pa.UnionType,\n    pa.SparseUnionType,\n    pa.DenseUnionType,\n    pa.StructType,\n    pa.Time32Type,\n    pa.Time64Type,\n    pa.TimestampType,\n    pa.Decimal128Type,\n    pa.Decimal256Type,\n    pa.DictionaryType,\n    pa.FixedSizeBinaryType,\n    pa.NullArray,\n    pa.NumericArray,\n    pa.IntegerArray,\n    pa.FloatingPointArray,\n    pa.BooleanArray,\n    pa.Int8Array,\n    pa.Int16Array,\n    pa.Int32Array,\n    pa.Int64Array,\n    pa.UInt8Array,\n    pa.UInt16Array,\n    pa.UInt32Array,\n    pa.UInt64Array,\n    pa.ListArray,\n    pa.LargeListArray,\n    pa.MapArray,\n    pa.FixedSizeListArray,\n    pa.UnionArray,\n    pa.BinaryArray,\n    pa.StringArray,\n    pa.BinaryViewArray,\n    pa.StringViewArray,\n    pa.FixedSizeBinaryArray,\n    pa.DictionaryArray,\n    pa.Date32Array,\n    pa.Date64Array,\n    pa.TimestampArray,\n    pa.Time32Array,\n    pa.Time64Array,\n    pa.DurationArray,\n    pa.Decimal128Array,\n    pa.Decimal256Array,\n    pa.StructArray,\n    pa.RunEndEncodedArray,\n    pa.Scalar,\n    pa.BooleanScalar,\n    pa.Int8Scalar,\n    pa.Int16Scalar,\n    pa.Int32Scalar,\n    pa.Int64Scalar,\n    pa.UInt8Scalar,\n    pa.UInt16Scalar,\n    pa.UInt32Scalar,\n    pa.UInt64Scalar,\n    pa.HalfFloatScalar,\n    pa.FloatScalar,\n    pa.DoubleScalar,\n    pa.Decimal128Scalar,\n    pa.Decimal256Scalar,\n    pa.Date32Scalar,\n    pa.Date64Scalar,\n    pa.Time32Scalar,\n    pa.Time64Scalar,\n    pa.TimestampScalar,\n    pa.DurationScalar,\n    pa.StringScalar,\n    pa.BinaryScalar,\n    pa.FixedSizeBinaryScalar,\n    pa.BinaryViewScalar,\n    pa.StringViewScalar,\n    pa.ListScalar,\n    pa.LargeListScalar,\n    pa.ListViewScalar,\n    pa.LargeListViewScalar,\n    pa.MapScalar,\n    pa.FixedSizeListScalar,\n    pa.UnionScalar,\n    pa.StructScalar,\n    pa.DictionaryScalar,\n    pa.RunEndEncodedScalar,\n    pa.RecordBatchReader,\n    pa.ipc.Message,\n    pa.ipc.MessageReader,\n    pa.MemoryPool,\n    pa.LoggingMemoryPool,\n    pa.ProxyMemoryPool,\n    pa.Device,\n    pa.MemoryManager,\n])\ndef test_extension_type_constructor_errors(klass):\n    # ARROW-2638: prevent calling extension class constructors directly\n    msg = \"Do not call {cls}'s constructor directly, use .* instead.\"\n    with pytest.raises(TypeError, match=msg.format(cls=klass.__name__)):\n        klass()\n", "python/pyarrow/tests/read_record_batch.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# This file is called from a test in test_ipc.py.\n\nimport sys\n\nimport pyarrow as pa\n\nwith open(sys.argv[1], 'rb') as f:\n    pa.ipc.open_file(f).read_all().to_pandas()\n", "python/pyarrow/tests/test_adhoc_memory_leak.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\n\nimport numpy as np\nimport pyarrow as pa\n\nimport pyarrow.tests.util as test_util\n\ntry:\n    import pandas as pd\nexcept ImportError:\n    pass\n\n\n@pytest.mark.memory_leak\n@pytest.mark.pandas\ndef test_deserialize_pandas_arrow_7956():\n    df = pd.DataFrame({'a': np.arange(10000),\n                       'b': [test_util.rands(5) for _ in range(10000)]})\n\n    def action():\n        df_bytes = pa.ipc.serialize_pandas(df).to_pybytes()\n        buf = pa.py_buffer(df_bytes)\n        pa.ipc.deserialize_pandas(buf)\n\n    # Abort at 128MB threshold\n    test_util.memory_leak_check(action, threshold=1 << 27, iterations=100)\n", "python/pyarrow/tests/util.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"\nUtility functions for testing\n\"\"\"\n\nimport contextlib\nimport decimal\nimport gc\nimport numpy as np\nimport os\nimport random\nimport re\nimport shutil\nimport signal\nimport socket\nimport string\nimport subprocess\nimport sys\nimport time\n\nimport pytest\n\nimport pyarrow as pa\nimport pyarrow.fs\n\n\ndef randsign():\n    \"\"\"Randomly choose either 1 or -1.\n\n    Returns\n    -------\n    sign : int\n    \"\"\"\n    return random.choice((-1, 1))\n\n\n@contextlib.contextmanager\ndef random_seed(seed):\n    \"\"\"Set the random seed inside of a context manager.\n\n    Parameters\n    ----------\n    seed : int\n        The seed to set\n\n    Notes\n    -----\n    This function is useful when you want to set a random seed but not affect\n    the random state of other functions using the random module.\n    \"\"\"\n    original_state = random.getstate()\n    random.seed(seed)\n    try:\n        yield\n    finally:\n        random.setstate(original_state)\n\n\ndef randdecimal(precision, scale):\n    \"\"\"Generate a random decimal value with specified precision and scale.\n\n    Parameters\n    ----------\n    precision : int\n        The maximum number of digits to generate. Must be an integer between 1\n        and 38 inclusive.\n    scale : int\n        The maximum number of digits following the decimal point.  Must be an\n        integer greater than or equal to 0.\n\n    Returns\n    -------\n    decimal_value : decimal.Decimal\n        A random decimal.Decimal object with the specified precision and scale.\n    \"\"\"\n    assert 1 <= precision <= 38, 'precision must be between 1 and 38 inclusive'\n    if scale < 0:\n        raise ValueError(\n            'randdecimal does not yet support generating decimals with '\n            'negative scale'\n        )\n    max_whole_value = 10 ** (precision - scale) - 1\n    whole = random.randint(-max_whole_value, max_whole_value)\n\n    if not scale:\n        return decimal.Decimal(whole)\n\n    max_fractional_value = 10 ** scale - 1\n    fractional = random.randint(0, max_fractional_value)\n\n    return decimal.Decimal(\n        '{}.{}'.format(whole, str(fractional).rjust(scale, '0'))\n    )\n\n\ndef random_ascii(length):\n    return bytes(np.random.randint(65, 123, size=length, dtype='i1'))\n\n\ndef rands(nchars):\n    \"\"\"\n    Generate one random string.\n    \"\"\"\n    RANDS_CHARS = np.array(\n        list(string.ascii_letters + string.digits), dtype=(np.str_, 1))\n    return \"\".join(np.random.choice(RANDS_CHARS, nchars))\n\n\ndef make_dataframe():\n    import pandas as pd\n\n    N = 30\n    df = pd.DataFrame(\n        {col: np.random.randn(N) for col in string.ascii_uppercase[:4]},\n        index=pd.Index([rands(10) for _ in range(N)])\n    )\n    return df\n\n\ndef memory_leak_check(f, metric='rss', threshold=1 << 17, iterations=10,\n                      check_interval=1):\n    \"\"\"\n    Execute the function and try to detect a clear memory leak either internal\n    to Arrow or caused by a reference counting problem in the Python binding\n    implementation. Raises exception if a leak detected\n\n    Parameters\n    ----------\n    f : callable\n        Function to invoke on each iteration\n    metric : {'rss', 'vms', 'shared'}, default 'rss'\n        Attribute of psutil.Process.memory_info to use for determining current\n        memory use\n    threshold : int, default 128K\n        Threshold in number of bytes to consider a leak\n    iterations : int, default 10\n        Total number of invocations of f\n    check_interval : int, default 1\n        Number of invocations of f in between each memory use check\n    \"\"\"\n    import psutil\n    proc = psutil.Process()\n\n    def _get_use():\n        gc.collect()\n        return getattr(proc.memory_info(), metric)\n\n    baseline_use = _get_use()\n\n    def _leak_check():\n        current_use = _get_use()\n        if current_use - baseline_use > threshold:\n            raise Exception(\"Memory leak detected. \"\n                            \"Departure from baseline {} after {} iterations\"\n                            .format(current_use - baseline_use, i))\n\n    for i in range(iterations):\n        f()\n        if i % check_interval == 0:\n            _leak_check()\n\n\ndef get_modified_env_with_pythonpath():\n    # Prepend pyarrow root directory to PYTHONPATH\n    env = os.environ.copy()\n    existing_pythonpath = env.get('PYTHONPATH', '')\n\n    module_path = os.path.abspath(\n        os.path.dirname(os.path.dirname(pa.__file__)))\n\n    if existing_pythonpath:\n        new_pythonpath = os.pathsep.join((module_path, existing_pythonpath))\n    else:\n        new_pythonpath = module_path\n    env['PYTHONPATH'] = new_pythonpath\n    return env\n\n\ndef invoke_script(script_name, *args):\n    subprocess_env = get_modified_env_with_pythonpath()\n\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    python_file = os.path.join(dir_path, script_name)\n\n    cmd = [sys.executable, python_file]\n    cmd.extend(args)\n\n    subprocess.check_call(cmd, env=subprocess_env)\n\n\n@contextlib.contextmanager\ndef changed_environ(name, value):\n    \"\"\"\n    Temporarily set environment variable *name* to *value*.\n    \"\"\"\n    orig_value = os.environ.get(name)\n    os.environ[name] = value\n    try:\n        yield\n    finally:\n        if orig_value is None:\n            del os.environ[name]\n        else:\n            os.environ[name] = orig_value\n\n\n@contextlib.contextmanager\ndef change_cwd(path):\n    curdir = os.getcwd()\n    os.chdir(str(path))\n    try:\n        yield\n    finally:\n        os.chdir(curdir)\n\n\n@contextlib.contextmanager\ndef disabled_gc():\n    gc.disable()\n    try:\n        yield\n    finally:\n        gc.enable()\n\n\ndef _filesystem_uri(path):\n    # URIs on Windows must follow 'file:///C:...' or 'file:/C:...' patterns.\n    if os.name == 'nt':\n        uri = 'file:///{}'.format(path)\n    else:\n        uri = 'file://{}'.format(path)\n    return uri\n\n\nclass FSProtocolClass:\n    def __init__(self, path):\n        self._path = path\n\n    def __fspath__(self):\n        return str(self._path)\n\n\nclass ProxyHandler(pyarrow.fs.FileSystemHandler):\n    \"\"\"\n    A dataset handler that proxies to an underlying filesystem.  Useful\n    to partially wrap an existing filesystem with partial changes.\n    \"\"\"\n\n    def __init__(self, fs):\n        self._fs = fs\n\n    def __eq__(self, other):\n        if isinstance(other, ProxyHandler):\n            return self._fs == other._fs\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, ProxyHandler):\n            return self._fs != other._fs\n        return NotImplemented\n\n    def get_type_name(self):\n        return \"proxy::\" + self._fs.type_name\n\n    def normalize_path(self, path):\n        return self._fs.normalize_path(path)\n\n    def get_file_info(self, paths):\n        return self._fs.get_file_info(paths)\n\n    def get_file_info_selector(self, selector):\n        return self._fs.get_file_info(selector)\n\n    def create_dir(self, path, recursive):\n        return self._fs.create_dir(path, recursive=recursive)\n\n    def delete_dir(self, path):\n        return self._fs.delete_dir(path)\n\n    def delete_dir_contents(self, path, missing_dir_ok):\n        return self._fs.delete_dir_contents(path,\n                                            missing_dir_ok=missing_dir_ok)\n\n    def delete_root_dir_contents(self):\n        return self._fs.delete_dir_contents(\"\", accept_root_dir=True)\n\n    def delete_file(self, path):\n        return self._fs.delete_file(path)\n\n    def move(self, src, dest):\n        return self._fs.move(src, dest)\n\n    def copy_file(self, src, dest):\n        return self._fs.copy_file(src, dest)\n\n    def open_input_stream(self, path):\n        return self._fs.open_input_stream(path)\n\n    def open_input_file(self, path):\n        return self._fs.open_input_file(path)\n\n    def open_output_stream(self, path, metadata):\n        return self._fs.open_output_stream(path, metadata=metadata)\n\n    def open_append_stream(self, path, metadata):\n        return self._fs.open_append_stream(path, metadata=metadata)\n\n\ndef get_raise_signal():\n    if sys.version_info >= (3, 8):\n        return signal.raise_signal\n    elif os.name == 'nt':\n        # On Windows, os.kill() doesn't actually send a signal,\n        # it just terminates the process with the given exit code.\n        pytest.skip(\"test requires Python 3.8+ on Windows\")\n    else:\n        # On Unix, emulate raise_signal() with os.kill().\n        def raise_signal(signum):\n            os.kill(os.getpid(), signum)\n        return raise_signal\n\n\n@contextlib.contextmanager\ndef signal_wakeup_fd(*, warn_on_full_buffer=False):\n    # Use a socket pair, rather a self-pipe, so that select() can be used\n    # on Windows.\n    r, w = socket.socketpair()\n    old_fd = None\n    try:\n        r.setblocking(False)\n        w.setblocking(False)\n        old_fd = signal.set_wakeup_fd(\n            w.fileno(), warn_on_full_buffer=warn_on_full_buffer)\n        yield r\n    finally:\n        if old_fd is not None:\n            signal.set_wakeup_fd(old_fd)\n        r.close()\n        w.close()\n\n\ndef _ensure_minio_component_version(component, minimum_year):\n    full_args = [component, '--version']\n    with subprocess.Popen(full_args, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE, encoding='utf-8') as proc:\n        if proc.wait(10) != 0:\n            return False\n        stdout = proc.stdout.read()\n        pattern = component + r' version RELEASE\\.(\\d+)-.*'\n        version_match = re.search(pattern, stdout)\n        if version_match:\n            version_year = version_match.group(1)\n            return int(version_year) >= minimum_year\n        else:\n            raise FileNotFoundError(\n                \"minio component older than the minimum year\")\n\n\ndef _wait_for_minio_startup(mcdir, address, access_key, secret_key):\n    start = time.time()\n    while time.time() - start < 10:\n        try:\n            _run_mc_command(mcdir, 'alias', 'set', 'myminio',\n                            f'http://{address}', access_key, secret_key)\n            return\n        except ChildProcessError:\n            time.sleep(1)\n    raise Exception(\"mc command could not connect to local minio\")\n\n\ndef _run_mc_command(mcdir, *args):\n    full_args = ['mc', '-C', mcdir] + list(args)\n    with subprocess.Popen(full_args, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE, encoding='utf-8') as proc:\n        retval = proc.wait(10)\n        cmd_str = ' '.join(full_args)\n        print(f'Cmd: {cmd_str}')\n        print(f'  Return: {retval}')\n        print(f'  Stdout: {proc.stdout.read()}')\n        print(f'  Stderr: {proc.stderr.read()}')\n        if retval != 0:\n            raise ChildProcessError(\"Could not run mc\")\n\n\ndef _configure_s3_limited_user(s3_server, policy):\n    \"\"\"\n    Attempts to use the mc command to configure the minio server\n    with a special user limited:limited123 which does not have\n    permission to create buckets.  This mirrors some real life S3\n    configurations where users are given strict permissions.\n\n    Arrow S3 operations should still work in such a configuration\n    (e.g. see ARROW-13685)\n    \"\"\"\n\n    if sys.platform == 'win32':\n        # Can't rely on FileNotFound check because\n        # there is sometimes an mc command on Windows\n        # which is unrelated to the minio mc\n        pytest.skip('The mc command is not installed on Windows')\n\n    try:\n        # ensuring version of mc and minio for the capabilities we need\n        _ensure_minio_component_version('mc', 2021)\n        _ensure_minio_component_version('minio', 2021)\n\n        tempdir = s3_server['tempdir']\n        host, port, access_key, secret_key = s3_server['connection']\n        address = '{}:{}'.format(host, port)\n\n        mcdir = os.path.join(tempdir, 'mc')\n        if os.path.exists(mcdir):\n            shutil.rmtree(mcdir)\n        os.mkdir(mcdir)\n        policy_path = os.path.join(tempdir, 'limited-buckets-policy.json')\n        with open(policy_path, mode='w') as policy_file:\n            policy_file.write(policy)\n        # The s3_server fixture starts the minio process but\n        # it takes a few moments for the process to become available\n        _wait_for_minio_startup(mcdir, address, access_key, secret_key)\n        # These commands create a limited user with a specific\n        # policy and creates a sample bucket for that user to\n        # write to\n        _run_mc_command(mcdir, 'admin', 'policy', 'add',\n                        'myminio/', 'no-create-buckets', policy_path)\n        _run_mc_command(mcdir, 'admin', 'user', 'add',\n                        'myminio/', 'limited', 'limited123')\n        _run_mc_command(mcdir, 'admin', 'policy', 'set',\n                        'myminio', 'no-create-buckets', 'user=limited')\n        _run_mc_command(mcdir, 'mb', 'myminio/existing-bucket',\n                        '--ignore-existing')\n\n    except FileNotFoundError:\n        pytest.skip(\"Configuring limited s3 user failed\")\n\n\ndef windows_has_tzdata():\n    \"\"\"\n    This is the default location where tz.cpp will look for (until we make\n    this configurable at run-time)\n    \"\"\"\n    tzdata_bool = False\n    if \"PYARROW_TZDATA_PATH\" in os.environ:\n        tzdata_bool = os.path.exists(os.environ['PYARROW_TZDATA_PATH'])\n    if not tzdata_bool:\n        tzdata_path = os.path.expandvars(r\"%USERPROFILE%\\Downloads\\tzdata\")\n        tzdata_bool = os.path.exists(tzdata_path)\n\n    return tzdata_bool\n", "python/pyarrow/tests/test_csv.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport abc\nimport bz2\nfrom datetime import date, datetime\nfrom decimal import Decimal\nimport gc\nimport gzip\nimport io\nimport itertools\nimport os\nimport select\nimport shutil\nimport signal\nimport string\nimport tempfile\nimport threading\nimport time\nimport unittest\nimport weakref\n\nimport pytest\n\nimport numpy as np\n\nimport pyarrow as pa\nfrom pyarrow.csv import (\n    open_csv, read_csv, ReadOptions, ParseOptions, ConvertOptions, ISO8601,\n    write_csv, WriteOptions, CSVWriter, InvalidRow)\nfrom pyarrow.tests import util\n\n\ndef generate_col_names():\n    # 'a', 'b'... 'z', then 'aa', 'ab'...\n    letters = string.ascii_lowercase\n    yield from letters\n    for first in letters:\n        for second in letters:\n            yield first + second\n\n\ndef make_random_csv(num_cols=2, num_rows=10, linesep='\\r\\n', write_names=True):\n    arr = np.random.RandomState(42).randint(0, 1000, size=(num_cols, num_rows))\n    csv = io.StringIO()\n    col_names = list(itertools.islice(generate_col_names(), num_cols))\n    if write_names:\n        csv.write(\",\".join(col_names))\n        csv.write(linesep)\n    for row in arr.T:\n        csv.write(\",\".join(map(str, row)))\n        csv.write(linesep)\n    csv = csv.getvalue().encode()\n    columns = [pa.array(a, type=pa.int64()) for a in arr]\n    expected = pa.Table.from_arrays(columns, col_names)\n    return csv, expected\n\n\ndef make_empty_csv(column_names):\n    csv = io.StringIO()\n    csv.write(\",\".join(column_names))\n    csv.write(\"\\n\")\n    return csv.getvalue().encode()\n\n\ndef check_options_class(cls, **attr_values):\n    \"\"\"\n    Check setting and getting attributes of an *Options class.\n    \"\"\"\n    opts = cls()\n\n    for name, values in attr_values.items():\n        assert getattr(opts, name) == values[0], \\\n            \"incorrect default value for \" + name\n        for v in values:\n            setattr(opts, name, v)\n            assert getattr(opts, name) == v, \"failed setting value\"\n\n    with pytest.raises(AttributeError):\n        opts.zzz_non_existent = True\n\n    # Check constructor named arguments\n    non_defaults = {name: values[1] for name, values in attr_values.items()}\n    opts = cls(**non_defaults)\n    for name, value in non_defaults.items():\n        assert getattr(opts, name) == value\n\n\n# The various options classes need to be picklable for dataset\ndef check_options_class_pickling(cls, pickler, **attr_values):\n    opts = cls(**attr_values)\n    new_opts = pickler.loads(pickler.dumps(opts,\n                                           protocol=pickler.HIGHEST_PROTOCOL))\n    for name, value in attr_values.items():\n        assert getattr(new_opts, name) == value\n\n\nclass InvalidRowHandler:\n    def __init__(self, result):\n        self.result = result\n        self.rows = []\n\n    def __call__(self, row):\n        self.rows.append(row)\n        return self.result\n\n    def __eq__(self, other):\n        return (isinstance(other, InvalidRowHandler) and\n                other.result == self.result)\n\n    def __ne__(self, other):\n        return (not isinstance(other, InvalidRowHandler) or\n                other.result != self.result)\n\n\ndef test_read_options(pickle_module):\n    cls = ReadOptions\n    opts = cls()\n\n    check_options_class(cls, use_threads=[True, False],\n                        skip_rows=[0, 3],\n                        column_names=[[], [\"ab\", \"cd\"]],\n                        autogenerate_column_names=[False, True],\n                        encoding=['utf8', 'utf16'],\n                        skip_rows_after_names=[0, 27])\n\n    check_options_class_pickling(cls, pickler=pickle_module,\n                                 use_threads=True,\n                                 skip_rows=3,\n                                 column_names=[\"ab\", \"cd\"],\n                                 autogenerate_column_names=False,\n                                 encoding='utf16',\n                                 skip_rows_after_names=27)\n\n    assert opts.block_size > 0\n    opts.block_size = 12345\n    assert opts.block_size == 12345\n\n    opts = cls(block_size=1234)\n    assert opts.block_size == 1234\n\n    opts.validate()\n\n    match = \"ReadOptions: block_size must be at least 1: 0\"\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.block_size = 0\n        opts.validate()\n\n    match = \"ReadOptions: skip_rows cannot be negative: -1\"\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.skip_rows = -1\n        opts.validate()\n\n    match = \"ReadOptions: skip_rows_after_names cannot be negative: -1\"\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.skip_rows_after_names = -1\n        opts.validate()\n\n    match = \"ReadOptions: autogenerate_column_names cannot be true when\" \\\n            \" column_names are provided\"\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.autogenerate_column_names = True\n        opts.column_names = ('a', 'b')\n        opts.validate()\n\n\ndef test_parse_options(pickle_module):\n    cls = ParseOptions\n    skip_handler = InvalidRowHandler('skip')\n\n    check_options_class(cls, delimiter=[',', 'x'],\n                        escape_char=[False, 'y'],\n                        quote_char=['\"', 'z', False],\n                        double_quote=[True, False],\n                        newlines_in_values=[False, True],\n                        ignore_empty_lines=[True, False],\n                        invalid_row_handler=[None, skip_handler])\n\n    check_options_class_pickling(cls, pickler=pickle_module,\n                                 delimiter='x',\n                                 escape_char='y',\n                                 quote_char=False,\n                                 double_quote=False,\n                                 newlines_in_values=True,\n                                 ignore_empty_lines=False,\n                                 invalid_row_handler=skip_handler)\n\n    cls().validate()\n    opts = cls()\n    opts.delimiter = \"\\t\"\n    opts.validate()\n\n    match = \"ParseOptions: delimiter cannot be \\\\\\\\r or \\\\\\\\n\"\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.delimiter = \"\\n\"\n        opts.validate()\n\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.delimiter = \"\\r\"\n        opts.validate()\n\n    match = \"ParseOptions: quote_char cannot be \\\\\\\\r or \\\\\\\\n\"\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.quote_char = \"\\n\"\n        opts.validate()\n\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.quote_char = \"\\r\"\n        opts.validate()\n\n    match = \"ParseOptions: escape_char cannot be \\\\\\\\r or \\\\\\\\n\"\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.escape_char = \"\\n\"\n        opts.validate()\n\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.escape_char = \"\\r\"\n        opts.validate()\n\n\ndef test_convert_options(pickle_module):\n    cls = ConvertOptions\n    opts = cls()\n\n    check_options_class(\n        cls, check_utf8=[True, False],\n        strings_can_be_null=[False, True],\n        quoted_strings_can_be_null=[True, False],\n        decimal_point=['.', ','],\n        include_columns=[[], ['def', 'abc']],\n        include_missing_columns=[False, True],\n        auto_dict_encode=[False, True],\n        timestamp_parsers=[[], [ISO8601, '%y-%m']])\n\n    check_options_class_pickling(\n        cls, pickler=pickle_module,\n        check_utf8=False,\n        strings_can_be_null=True,\n        quoted_strings_can_be_null=False,\n        decimal_point=',',\n        include_columns=['def', 'abc'],\n        include_missing_columns=False,\n        auto_dict_encode=True,\n        timestamp_parsers=[ISO8601, '%y-%m'])\n\n    with pytest.raises(ValueError):\n        opts.decimal_point = '..'\n\n    assert opts.auto_dict_max_cardinality > 0\n    opts.auto_dict_max_cardinality = 99999\n    assert opts.auto_dict_max_cardinality == 99999\n\n    assert opts.column_types == {}\n    # Pass column_types as mapping\n    opts.column_types = {'b': pa.int16(), 'c': pa.float32()}\n    assert opts.column_types == {'b': pa.int16(), 'c': pa.float32()}\n    opts.column_types = {'v': 'int16', 'w': 'null'}\n    assert opts.column_types == {'v': pa.int16(), 'w': pa.null()}\n    # Pass column_types as schema\n    schema = pa.schema([('a', pa.int32()), ('b', pa.string())])\n    opts.column_types = schema\n    assert opts.column_types == {'a': pa.int32(), 'b': pa.string()}\n    # Pass column_types as sequence\n    opts.column_types = [('x', pa.binary())]\n    assert opts.column_types == {'x': pa.binary()}\n\n    with pytest.raises(TypeError, match='DataType expected'):\n        opts.column_types = {'a': None}\n    with pytest.raises(TypeError):\n        opts.column_types = 0\n\n    assert isinstance(opts.null_values, list)\n    assert '' in opts.null_values\n    assert 'N/A' in opts.null_values\n    opts.null_values = ['xxx', 'yyy']\n    assert opts.null_values == ['xxx', 'yyy']\n\n    assert isinstance(opts.true_values, list)\n    opts.true_values = ['xxx', 'yyy']\n    assert opts.true_values == ['xxx', 'yyy']\n\n    assert isinstance(opts.false_values, list)\n    opts.false_values = ['xxx', 'yyy']\n    assert opts.false_values == ['xxx', 'yyy']\n\n    assert opts.timestamp_parsers == []\n    opts.timestamp_parsers = [ISO8601]\n    assert opts.timestamp_parsers == [ISO8601]\n\n    opts = cls(column_types={'a': pa.null()},\n               null_values=['N', 'nn'], true_values=['T', 'tt'],\n               false_values=['F', 'ff'], auto_dict_max_cardinality=999,\n               timestamp_parsers=[ISO8601, '%Y-%m-%d'])\n    assert opts.column_types == {'a': pa.null()}\n    assert opts.null_values == ['N', 'nn']\n    assert opts.false_values == ['F', 'ff']\n    assert opts.true_values == ['T', 'tt']\n    assert opts.auto_dict_max_cardinality == 999\n    assert opts.timestamp_parsers == [ISO8601, '%Y-%m-%d']\n\n\ndef test_write_options():\n    cls = WriteOptions\n    opts = cls()\n\n    check_options_class(\n        cls, include_header=[True, False], delimiter=[',', '\\t', '|'],\n        quoting_style=['needed', 'none', 'all_valid'])\n\n    assert opts.batch_size > 0\n    opts.batch_size = 12345\n    assert opts.batch_size == 12345\n\n    opts = cls(batch_size=9876)\n    assert opts.batch_size == 9876\n\n    opts.validate()\n\n    match = \"WriteOptions: batch_size must be at least 1: 0\"\n    with pytest.raises(pa.ArrowInvalid, match=match):\n        opts = cls()\n        opts.batch_size = 0\n        opts.validate()\n\n\nclass BaseTestCSV(abc.ABC):\n    \"\"\"Common tests which are shared by streaming and non streaming readers\"\"\"\n\n    @abc.abstractmethod\n    def read_bytes(self, b, **kwargs):\n        \"\"\"\n        :param b: bytes to be parsed\n        :param kwargs: arguments passed on to open the csv file\n        :return: b parsed as a single RecordBatch\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    @abc.abstractmethod\n    def use_threads(self):\n        \"\"\"Whether this test is multi-threaded\"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def check_names(table, names):\n        assert table.num_columns == len(names)\n        assert table.column_names == names\n\n    def test_header_skip_rows(self):\n        rows = b\"ab,cd\\nef,gh\\nij,kl\\nmn,op\\n\"\n\n        opts = ReadOptions()\n        opts.skip_rows = 1\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"ef\", \"gh\"])\n        assert table.to_pydict() == {\n            \"ef\": [\"ij\", \"mn\"],\n            \"gh\": [\"kl\", \"op\"],\n        }\n\n        opts.skip_rows = 3\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"mn\", \"op\"])\n        assert table.to_pydict() == {\n            \"mn\": [],\n            \"op\": [],\n        }\n\n        opts.skip_rows = 4\n        with pytest.raises(pa.ArrowInvalid):\n            # Not enough rows\n            table = self.read_bytes(rows, read_options=opts)\n\n        # Can skip rows with a different number of columns\n        rows = b\"abcd\\n,,,,,\\nij,kl\\nmn,op\\n\"\n        opts.skip_rows = 2\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"ij\", \"kl\"])\n        assert table.to_pydict() == {\n            \"ij\": [\"mn\"],\n            \"kl\": [\"op\"],\n        }\n\n        # Can skip all rows exactly when columns are given\n        opts.skip_rows = 4\n        opts.column_names = ['ij', 'kl']\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"ij\", \"kl\"])\n        assert table.to_pydict() == {\n            \"ij\": [],\n            \"kl\": [],\n        }\n\n    def test_skip_rows_after_names(self):\n        rows = b\"ab,cd\\nef,gh\\nij,kl\\nmn,op\\n\"\n\n        opts = ReadOptions()\n        opts.skip_rows_after_names = 1\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"ab\", \"cd\"])\n        assert table.to_pydict() == {\n            \"ab\": [\"ij\", \"mn\"],\n            \"cd\": [\"kl\", \"op\"],\n        }\n\n        # Can skip exact number of rows\n        opts.skip_rows_after_names = 3\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"ab\", \"cd\"])\n        assert table.to_pydict() == {\n            \"ab\": [],\n            \"cd\": [],\n        }\n\n        # Can skip beyond all rows\n        opts.skip_rows_after_names = 4\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"ab\", \"cd\"])\n        assert table.to_pydict() == {\n            \"ab\": [],\n            \"cd\": [],\n        }\n\n        # Can skip rows with a different number of columns\n        rows = b\"abcd\\n,,,,,\\nij,kl\\nmn,op\\n\"\n        opts.skip_rows_after_names = 2\n        opts.column_names = [\"f0\", \"f1\"]\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"f0\", \"f1\"])\n        assert table.to_pydict() == {\n            \"f0\": [\"ij\", \"mn\"],\n            \"f1\": [\"kl\", \"op\"],\n        }\n        opts = ReadOptions()\n\n        # Can skip rows with new lines in the value\n        rows = b'ab,cd\\n\"e\\nf\",\"g\\n\\nh\"\\n\"ij\",\"k\\nl\"\\nmn,op'\n        opts.skip_rows_after_names = 2\n        parse_opts = ParseOptions()\n        parse_opts.newlines_in_values = True\n        table = self.read_bytes(rows, read_options=opts,\n                                parse_options=parse_opts)\n        self.check_names(table, [\"ab\", \"cd\"])\n        assert table.to_pydict() == {\n            \"ab\": [\"mn\"],\n            \"cd\": [\"op\"],\n        }\n\n        # Can skip rows when block ends in middle of quoted value\n        opts.skip_rows_after_names = 2\n        opts.block_size = 26\n        table = self.read_bytes(rows, read_options=opts,\n                                parse_options=parse_opts)\n        self.check_names(table, [\"ab\", \"cd\"])\n        assert table.to_pydict() == {\n            \"ab\": [\"mn\"],\n            \"cd\": [\"op\"],\n        }\n        opts = ReadOptions()\n\n        # Can skip rows that are beyond the first block without lexer\n        rows, expected = make_random_csv(num_cols=5, num_rows=1000)\n        opts.skip_rows_after_names = 900\n        opts.block_size = len(rows) / 11\n        table = self.read_bytes(rows, read_options=opts)\n        assert table.schema == expected.schema\n        assert table.num_rows == 100\n        table_dict = table.to_pydict()\n        for name, values in expected.to_pydict().items():\n            assert values[900:] == table_dict[name]\n\n        # Can skip rows that are beyond the first block with lexer\n        table = self.read_bytes(rows, read_options=opts,\n                                parse_options=parse_opts)\n        assert table.schema == expected.schema\n        assert table.num_rows == 100\n        table_dict = table.to_pydict()\n        for name, values in expected.to_pydict().items():\n            assert values[900:] == table_dict[name]\n\n        # Skip rows and skip rows after names\n        rows, expected = make_random_csv(num_cols=5, num_rows=200,\n                                         write_names=False)\n        opts = ReadOptions()\n        opts.skip_rows = 37\n        opts.skip_rows_after_names = 41\n        opts.column_names = expected.schema.names\n        table = self.read_bytes(rows, read_options=opts,\n                                parse_options=parse_opts)\n        assert table.schema == expected.schema\n        assert (table.num_rows ==\n                expected.num_rows - opts.skip_rows -\n                opts.skip_rows_after_names)\n        table_dict = table.to_pydict()\n        for name, values in expected.to_pydict().items():\n            assert (values[opts.skip_rows + opts.skip_rows_after_names:] ==\n                    table_dict[name])\n\n    def test_row_number_offset_in_errors(self):\n        # Row numbers are only correctly counted in serial reads\n        def format_msg(msg_format, row, *args):\n            if self.use_threads:\n                row_info = \"\"\n            else:\n                row_info = \"Row #{}: \".format(row)\n            return msg_format.format(row_info, *args)\n\n        csv, _ = make_random_csv(4, 100, write_names=True)\n\n        read_options = ReadOptions()\n        read_options.block_size = len(csv) / 3\n        convert_options = ConvertOptions()\n        convert_options.column_types = {\"a\": pa.int32()}\n\n        # Test without skip_rows and column names in the csv\n        csv_bad_columns = csv + b\"1,2\\r\\n\"\n        message_columns = format_msg(\"{}Expected 4 columns, got 2\", 102)\n        with pytest.raises(pa.ArrowInvalid, match=message_columns):\n            self.read_bytes(csv_bad_columns,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n        csv_bad_type = csv + b\"a,b,c,d\\r\\n\"\n        message_value = format_msg(\n            \"In CSV column #0: {}\"\n            \"CSV conversion error to int32: invalid value 'a'\",\n            102, csv)\n        with pytest.raises(pa.ArrowInvalid, match=message_value):\n            self.read_bytes(csv_bad_type,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n        long_row = (b\"this is a long row\" * 15) + b\",3\\r\\n\"\n        csv_bad_columns_long = csv + long_row\n        message_long = format_msg(\"{}Expected 4 columns, got 2: {} ...\", 102,\n                                  long_row[0:96].decode(\"utf-8\"))\n        with pytest.raises(pa.ArrowInvalid, match=message_long):\n            self.read_bytes(csv_bad_columns_long,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n        # Test skipping rows after the names\n        read_options.skip_rows_after_names = 47\n\n        with pytest.raises(pa.ArrowInvalid, match=message_columns):\n            self.read_bytes(csv_bad_columns,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n        with pytest.raises(pa.ArrowInvalid, match=message_value):\n            self.read_bytes(csv_bad_type,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n        with pytest.raises(pa.ArrowInvalid, match=message_long):\n            self.read_bytes(csv_bad_columns_long,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n        read_options.skip_rows_after_names = 0\n\n        # Test without skip_rows and column names not in the csv\n        csv, _ = make_random_csv(4, 100, write_names=False)\n        read_options.column_names = [\"a\", \"b\", \"c\", \"d\"]\n        csv_bad_columns = csv + b\"1,2\\r\\n\"\n        message_columns = format_msg(\"{}Expected 4 columns, got 2\", 101)\n        with pytest.raises(pa.ArrowInvalid, match=message_columns):\n            self.read_bytes(csv_bad_columns,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n        csv_bad_columns_long = csv + long_row\n        message_long = format_msg(\"{}Expected 4 columns, got 2: {} ...\", 101,\n                                  long_row[0:96].decode(\"utf-8\"))\n        with pytest.raises(pa.ArrowInvalid, match=message_long):\n            self.read_bytes(csv_bad_columns_long,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n        csv_bad_type = csv + b\"a,b,c,d\\r\\n\"\n        message_value = format_msg(\n            \"In CSV column #0: {}\"\n            \"CSV conversion error to int32: invalid value 'a'\",\n            101)\n        message_value = message_value.format(len(csv))\n        with pytest.raises(pa.ArrowInvalid, match=message_value):\n            self.read_bytes(csv_bad_type,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n        # Test with skip_rows and column names not in the csv\n        read_options.skip_rows = 23\n        with pytest.raises(pa.ArrowInvalid, match=message_columns):\n            self.read_bytes(csv_bad_columns,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n        with pytest.raises(pa.ArrowInvalid, match=message_value):\n            self.read_bytes(csv_bad_type,\n                            read_options=read_options,\n                            convert_options=convert_options)\n\n    def test_invalid_row_handler(self, pickle_module):\n        rows = b\"a,b\\nc\\nd,e\\nf,g,h\\ni,j\\n\"\n        parse_opts = ParseOptions()\n        with pytest.raises(\n                ValueError,\n                match=\"Expected 2 columns, got 1: c\"):\n            self.read_bytes(rows, parse_options=parse_opts)\n\n        # Skip requested\n        parse_opts.invalid_row_handler = InvalidRowHandler('skip')\n        table = self.read_bytes(rows, parse_options=parse_opts)\n        assert table.to_pydict() == {\n            'a': [\"d\", \"i\"],\n            'b': [\"e\", \"j\"],\n        }\n\n        def row_num(x):\n            return None if self.use_threads else x\n        expected_rows = [\n            InvalidRow(2, 1, row_num(2), \"c\"),\n            InvalidRow(2, 3, row_num(4), \"f,g,h\"),\n        ]\n        assert parse_opts.invalid_row_handler.rows == expected_rows\n\n        # Error requested\n        parse_opts.invalid_row_handler = InvalidRowHandler('error')\n        with pytest.raises(\n                ValueError,\n                match=\"Expected 2 columns, got 1: c\"):\n            self.read_bytes(rows, parse_options=parse_opts)\n        expected_rows = [InvalidRow(2, 1, row_num(2), \"c\")]\n        assert parse_opts.invalid_row_handler.rows == expected_rows\n\n        # Test ser/de\n        parse_opts.invalid_row_handler = InvalidRowHandler('skip')\n        parse_opts = pickle_module.loads(pickle_module.dumps(parse_opts))\n\n        table = self.read_bytes(rows, parse_options=parse_opts)\n        assert table.to_pydict() == {\n            'a': [\"d\", \"i\"],\n            'b': [\"e\", \"j\"],\n        }\n\n    def test_chunker_out_of_sync(self):\n        # GH-39892: if there are newlines in values, the parser may become\n        # out of sync with the chunker. In this case, we try to produce an\n        # informative error message.\n        rows = b\"\"\"a,b,c\\nd,e,\"f\\n\"\\ng,h,i\\n\"\"\"\n        expected = {\n            'a': [\"d\", \"g\"],\n            'b': [\"e\", \"h\"],\n            'c': [\"f\\n\", \"i\"],\n        }\n        for block_size in range(8, 15):\n            # Sanity check: parsing works with newlines_in_values=True\n            d = self.read_bytes(\n                rows, parse_options=ParseOptions(newlines_in_values=True),\n                read_options=ReadOptions(block_size=block_size)).to_pydict()\n            assert d == expected\n        # With these block sizes, a block would end on the physical newline\n        # inside the quoted cell value, leading to a mismatch between\n        # CSV chunker and parser.\n        for block_size in range(8, 11):\n            with pytest.raises(ValueError,\n                               match=\"cell values spanning multiple lines\"):\n                self.read_bytes(\n                    rows, read_options=ReadOptions(block_size=block_size))\n\n\nclass BaseCSVTableRead(BaseTestCSV):\n\n    def read_csv(self, csv, *args, validate_full=True, **kwargs):\n        \"\"\"\n        Reads the CSV file into memory using pyarrow's read_csv\n        csv The CSV bytes\n        args Positional arguments to be forwarded to pyarrow's read_csv\n        validate_full Whether or not to fully validate the resulting table\n        kwargs Keyword arguments to be forwarded to pyarrow's read_csv\n        \"\"\"\n        assert isinstance(self.use_threads, bool)  # sanity check\n        read_options = kwargs.setdefault('read_options', ReadOptions())\n        read_options.use_threads = self.use_threads\n        table = read_csv(csv, *args, **kwargs)\n        table.validate(full=validate_full)\n        return table\n\n    def read_bytes(self, b, **kwargs):\n        return self.read_csv(pa.py_buffer(b), **kwargs)\n\n    def test_file_object(self):\n        data = b\"a,b\\n1,2\\n\"\n        expected_data = {'a': [1], 'b': [2]}\n        bio = io.BytesIO(data)\n        table = self.read_csv(bio)\n        assert table.to_pydict() == expected_data\n        # Text files not allowed\n        sio = io.StringIO(data.decode())\n        with pytest.raises(TypeError):\n            self.read_csv(sio)\n\n    def test_header(self):\n        rows = b\"abc,def,gh\\n\"\n        table = self.read_bytes(rows)\n        assert isinstance(table, pa.Table)\n        self.check_names(table, [\"abc\", \"def\", \"gh\"])\n        assert table.num_rows == 0\n\n    def test_bom(self):\n        rows = b\"\\xef\\xbb\\xbfa,b\\n1,2\\n\"\n        expected_data = {'a': [1], 'b': [2]}\n        table = self.read_bytes(rows)\n        assert table.to_pydict() == expected_data\n\n    def test_one_chunk(self):\n        # ARROW-7661: lack of newline at end of file should not produce\n        # an additional chunk.\n        rows = [b\"a,b\", b\"1,2\", b\"3,4\", b\"56,78\"]\n        for line_ending in [b'\\n', b'\\r', b'\\r\\n']:\n            for file_ending in [b'', line_ending]:\n                data = line_ending.join(rows) + file_ending\n                table = self.read_bytes(data)\n                assert len(table.to_batches()) == 1\n                assert table.to_pydict() == {\n                    \"a\": [1, 3, 56],\n                    \"b\": [2, 4, 78],\n                }\n\n    def test_header_column_names(self):\n        rows = b\"ab,cd\\nef,gh\\nij,kl\\nmn,op\\n\"\n\n        opts = ReadOptions()\n        opts.column_names = [\"x\", \"y\"]\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"x\", \"y\"])\n        assert table.to_pydict() == {\n            \"x\": [\"ab\", \"ef\", \"ij\", \"mn\"],\n            \"y\": [\"cd\", \"gh\", \"kl\", \"op\"],\n        }\n\n        opts.skip_rows = 3\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"x\", \"y\"])\n        assert table.to_pydict() == {\n            \"x\": [\"mn\"],\n            \"y\": [\"op\"],\n        }\n\n        opts.skip_rows = 4\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"x\", \"y\"])\n        assert table.to_pydict() == {\n            \"x\": [],\n            \"y\": [],\n        }\n\n        opts.skip_rows = 5\n        with pytest.raises(pa.ArrowInvalid):\n            # Not enough rows\n            table = self.read_bytes(rows, read_options=opts)\n\n        # Unexpected number of columns\n        opts.skip_rows = 0\n        opts.column_names = [\"x\", \"y\", \"z\"]\n        with pytest.raises(pa.ArrowInvalid,\n                           match=\"Expected 3 columns, got 2\"):\n            table = self.read_bytes(rows, read_options=opts)\n\n        # Can skip rows with a different number of columns\n        rows = b\"abcd\\n,,,,,\\nij,kl\\nmn,op\\n\"\n        opts.skip_rows = 2\n        opts.column_names = [\"x\", \"y\"]\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"x\", \"y\"])\n        assert table.to_pydict() == {\n            \"x\": [\"ij\", \"mn\"],\n            \"y\": [\"kl\", \"op\"],\n        }\n\n    def test_header_autogenerate_column_names(self):\n        rows = b\"ab,cd\\nef,gh\\nij,kl\\nmn,op\\n\"\n\n        opts = ReadOptions()\n        opts.autogenerate_column_names = True\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"f0\", \"f1\"])\n        assert table.to_pydict() == {\n            \"f0\": [\"ab\", \"ef\", \"ij\", \"mn\"],\n            \"f1\": [\"cd\", \"gh\", \"kl\", \"op\"],\n        }\n\n        opts.skip_rows = 3\n        table = self.read_bytes(rows, read_options=opts)\n        self.check_names(table, [\"f0\", \"f1\"])\n        assert table.to_pydict() == {\n            \"f0\": [\"mn\"],\n            \"f1\": [\"op\"],\n        }\n\n        # Not enough rows, impossible to infer number of columns\n        opts.skip_rows = 4\n        with pytest.raises(pa.ArrowInvalid):\n            table = self.read_bytes(rows, read_options=opts)\n\n    def test_include_columns(self):\n        rows = b\"ab,cd\\nef,gh\\nij,kl\\nmn,op\\n\"\n\n        convert_options = ConvertOptions()\n        convert_options.include_columns = ['ab']\n        table = self.read_bytes(rows, convert_options=convert_options)\n        self.check_names(table, [\"ab\"])\n        assert table.to_pydict() == {\n            \"ab\": [\"ef\", \"ij\", \"mn\"],\n        }\n\n        # Order of include_columns is respected, regardless of CSV order\n        convert_options.include_columns = ['cd', 'ab']\n        table = self.read_bytes(rows, convert_options=convert_options)\n        schema = pa.schema([('cd', pa.string()),\n                            ('ab', pa.string())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            \"cd\": [\"gh\", \"kl\", \"op\"],\n            \"ab\": [\"ef\", \"ij\", \"mn\"],\n        }\n\n        # Include a column not in the CSV file => raises by default\n        convert_options.include_columns = ['xx', 'ab', 'yy']\n        with pytest.raises(KeyError,\n                           match=\"Column 'xx' in include_columns \"\n                                 \"does not exist in CSV file\"):\n            self.read_bytes(rows, convert_options=convert_options)\n\n    def test_include_missing_columns(self):\n        rows = b\"ab,cd\\nef,gh\\nij,kl\\nmn,op\\n\"\n\n        read_options = ReadOptions()\n        convert_options = ConvertOptions()\n        convert_options.include_columns = ['xx', 'ab', 'yy']\n        convert_options.include_missing_columns = True\n        table = self.read_bytes(rows, read_options=read_options,\n                                convert_options=convert_options)\n        schema = pa.schema([('xx', pa.null()),\n                            ('ab', pa.string()),\n                            ('yy', pa.null())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            \"xx\": [None, None, None],\n            \"ab\": [\"ef\", \"ij\", \"mn\"],\n            \"yy\": [None, None, None],\n        }\n\n        # Combining with `column_names`\n        read_options.column_names = [\"xx\", \"yy\"]\n        convert_options.include_columns = [\"yy\", \"cd\"]\n        table = self.read_bytes(rows, read_options=read_options,\n                                convert_options=convert_options)\n        schema = pa.schema([('yy', pa.string()),\n                            ('cd', pa.null())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            \"yy\": [\"cd\", \"gh\", \"kl\", \"op\"],\n            \"cd\": [None, None, None, None],\n        }\n\n        # And with `column_types` as well\n        convert_options.column_types = {\"yy\": pa.binary(),\n                                        \"cd\": pa.int32()}\n        table = self.read_bytes(rows, read_options=read_options,\n                                convert_options=convert_options)\n        schema = pa.schema([('yy', pa.binary()),\n                            ('cd', pa.int32())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            \"yy\": [b\"cd\", b\"gh\", b\"kl\", b\"op\"],\n            \"cd\": [None, None, None, None],\n        }\n\n    def test_simple_ints(self):\n        # Infer integer columns\n        rows = b\"a,b,c\\n1,2,3\\n4,5,6\\n\"\n        table = self.read_bytes(rows)\n        schema = pa.schema([('a', pa.int64()),\n                            ('b', pa.int64()),\n                            ('c', pa.int64())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [1, 4],\n            'b': [2, 5],\n            'c': [3, 6],\n        }\n\n    def test_simple_varied(self):\n        # Infer various kinds of data\n        rows = b\"a,b,c,d\\n1,2,3,0\\n4.0,-5,foo,True\\n\"\n        table = self.read_bytes(rows)\n        schema = pa.schema([('a', pa.float64()),\n                            ('b', pa.int64()),\n                            ('c', pa.string()),\n                            ('d', pa.bool_())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [1.0, 4.0],\n            'b': [2, -5],\n            'c': [\"3\", \"foo\"],\n            'd': [False, True],\n        }\n\n    def test_simple_nulls(self):\n        # Infer various kinds of data, with nulls\n        rows = (b\"a,b,c,d,e,f\\n\"\n                b\"1,2,,,3,N/A\\n\"\n                b\"nan,-5,foo,,nan,TRUE\\n\"\n                b\"4.5,#N/A,nan,,\\xff,false\\n\")\n        table = self.read_bytes(rows)\n        schema = pa.schema([('a', pa.float64()),\n                            ('b', pa.int64()),\n                            ('c', pa.string()),\n                            ('d', pa.null()),\n                            ('e', pa.binary()),\n                            ('f', pa.bool_())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [1.0, None, 4.5],\n            'b': [2, -5, None],\n            'c': [\"\", \"foo\", \"nan\"],\n            'd': [None, None, None],\n            'e': [b\"3\", b\"nan\", b\"\\xff\"],\n            'f': [None, True, False],\n        }\n\n    def test_decimal_point(self):\n        # Infer floats with a custom decimal point\n        parse_options = ParseOptions(delimiter=';')\n        rows = b\"a;b\\n1.25;2,5\\nNA;-3\\n-4;NA\"\n\n        table = self.read_bytes(rows, parse_options=parse_options)\n        schema = pa.schema([('a', pa.float64()),\n                            ('b', pa.string())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [1.25, None, -4.0],\n            'b': [\"2,5\", \"-3\", \"NA\"],\n        }\n\n        convert_options = ConvertOptions(decimal_point=',')\n        table = self.read_bytes(rows, parse_options=parse_options,\n                                convert_options=convert_options)\n        schema = pa.schema([('a', pa.string()),\n                            ('b', pa.float64())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [\"1.25\", \"NA\", \"-4\"],\n            'b': [2.5, -3.0, None],\n        }\n\n    def test_simple_timestamps(self):\n        # Infer a timestamp column\n        rows = (b\"a,b,c\\n\"\n                b\"1970,1970-01-01 00:00:00,1970-01-01 00:00:00.123\\n\"\n                b\"1989,1989-07-14 01:00:00,1989-07-14 01:00:00.123456\\n\")\n        table = self.read_bytes(rows)\n        schema = pa.schema([('a', pa.int64()),\n                            ('b', pa.timestamp('s')),\n                            ('c', pa.timestamp('ns'))])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [1970, 1989],\n            'b': [datetime(1970, 1, 1), datetime(1989, 7, 14, 1)],\n            'c': [datetime(1970, 1, 1, 0, 0, 0, 123000),\n                  datetime(1989, 7, 14, 1, 0, 0, 123456)],\n        }\n\n    def test_timestamp_parsers(self):\n        # Infer timestamps with custom parsers\n        rows = b\"a,b\\n1970/01/01,1980-01-01 00\\n1970/01/02,1980-01-02 00\\n\"\n        opts = ConvertOptions()\n\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.string()),\n                            ('b', pa.timestamp('s'))])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': ['1970/01/01', '1970/01/02'],\n            'b': [datetime(1980, 1, 1), datetime(1980, 1, 2)],\n        }\n\n        opts.timestamp_parsers = ['%Y/%m/%d']\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.timestamp('s')),\n                            ('b', pa.string())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [datetime(1970, 1, 1), datetime(1970, 1, 2)],\n            'b': ['1980-01-01 00', '1980-01-02 00'],\n        }\n\n        opts.timestamp_parsers = ['%Y/%m/%d', ISO8601]\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.timestamp('s')),\n                            ('b', pa.timestamp('s'))])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [datetime(1970, 1, 1), datetime(1970, 1, 2)],\n            'b': [datetime(1980, 1, 1), datetime(1980, 1, 2)],\n        }\n\n    def test_dates(self):\n        # Dates are inferred as date32 by default\n        rows = b\"a,b\\n1970-01-01,1970-01-02\\n1971-01-01,1971-01-02\\n\"\n        table = self.read_bytes(rows)\n        schema = pa.schema([('a', pa.date32()),\n                            ('b', pa.date32())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [date(1970, 1, 1), date(1971, 1, 1)],\n            'b': [date(1970, 1, 2), date(1971, 1, 2)],\n        }\n\n        # Can ask for date types explicitly\n        opts = ConvertOptions()\n        opts.column_types = {'a': pa.date32(), 'b': pa.date64()}\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.date32()),\n                            ('b', pa.date64())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [date(1970, 1, 1), date(1971, 1, 1)],\n            'b': [date(1970, 1, 2), date(1971, 1, 2)],\n        }\n\n        # Can ask for timestamp types explicitly\n        opts = ConvertOptions()\n        opts.column_types = {'a': pa.timestamp('s'), 'b': pa.timestamp('ms')}\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.timestamp('s')),\n                            ('b', pa.timestamp('ms'))])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [datetime(1970, 1, 1), datetime(1971, 1, 1)],\n            'b': [datetime(1970, 1, 2), datetime(1971, 1, 2)],\n        }\n\n    def test_times(self):\n        # Times are inferred as time32[s] by default\n        from datetime import time\n\n        rows = b\"a,b\\n12:34:56,12:34:56.789\\n23:59:59,23:59:59.999\\n\"\n        table = self.read_bytes(rows)\n        # Column 'b' has subseconds, so cannot be inferred as time32[s]\n        schema = pa.schema([('a', pa.time32('s')),\n                            ('b', pa.string())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [time(12, 34, 56), time(23, 59, 59)],\n            'b': [\"12:34:56.789\", \"23:59:59.999\"],\n        }\n\n        # Can ask for time types explicitly\n        opts = ConvertOptions()\n        opts.column_types = {'a': pa.time64('us'), 'b': pa.time32('ms')}\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.time64('us')),\n                            ('b', pa.time32('ms'))])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [time(12, 34, 56), time(23, 59, 59)],\n            'b': [time(12, 34, 56, 789000), time(23, 59, 59, 999000)],\n        }\n\n    def test_auto_dict_encode(self):\n        opts = ConvertOptions(auto_dict_encode=True)\n        rows = \"a,b\\nab,1\\ncd\u00e9,2\\ncd\u00e9,3\\nab,4\".encode()\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.dictionary(pa.int32(), pa.string())),\n                            ('b', pa.int64())])\n        expected = {\n            'a': [\"ab\", \"cd\u00e9\", \"cd\u00e9\", \"ab\"],\n            'b': [1, 2, 3, 4],\n        }\n        assert table.schema == schema\n        assert table.to_pydict() == expected\n\n        opts.auto_dict_max_cardinality = 2\n        table = self.read_bytes(rows, convert_options=opts)\n        assert table.schema == schema\n        assert table.to_pydict() == expected\n\n        # Cardinality above max => plain-encoded\n        opts.auto_dict_max_cardinality = 1\n        table = self.read_bytes(rows, convert_options=opts)\n        assert table.schema == pa.schema([('a', pa.string()),\n                                          ('b', pa.int64())])\n        assert table.to_pydict() == expected\n\n        # With invalid UTF8, not checked\n        opts.auto_dict_max_cardinality = 50\n        opts.check_utf8 = False\n        rows = b\"a,b\\nab,1\\ncd\\xff,2\\nab,3\"\n        table = self.read_bytes(rows, convert_options=opts,\n                                validate_full=False)\n        assert table.schema == schema\n        dict_values = table['a'].chunk(0).dictionary\n        assert len(dict_values) == 2\n        assert dict_values[0].as_py() == \"ab\"\n        assert dict_values[1].as_buffer() == b\"cd\\xff\"\n\n        # With invalid UTF8, checked\n        opts.check_utf8 = True\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.dictionary(pa.int32(), pa.binary())),\n                            ('b', pa.int64())])\n        expected = {\n            'a': [b\"ab\", b\"cd\\xff\", b\"ab\"],\n            'b': [1, 2, 3],\n        }\n        assert table.schema == schema\n        assert table.to_pydict() == expected\n\n    def test_custom_nulls(self):\n        # Infer nulls with custom values\n        opts = ConvertOptions(null_values=['Xxx', 'Zzz'])\n        rows = b\"\"\"a,b,c,d\\nZzz,\"Xxx\",1,2\\nXxx,#N/A,,Zzz\\n\"\"\"\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.null()),\n                            ('b', pa.string()),\n                            ('c', pa.string()),\n                            ('d', pa.int64())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [None, None],\n            'b': [\"Xxx\", \"#N/A\"],\n            'c': [\"1\", \"\"],\n            'd': [2, None],\n        }\n\n        opts = ConvertOptions(null_values=['Xxx', 'Zzz'],\n                              strings_can_be_null=True)\n        table = self.read_bytes(rows, convert_options=opts)\n        assert table.to_pydict() == {\n            'a': [None, None],\n            'b': [None, \"#N/A\"],\n            'c': [\"1\", \"\"],\n            'd': [2, None],\n        }\n        opts.quoted_strings_can_be_null = False\n        table = self.read_bytes(rows, convert_options=opts)\n        assert table.to_pydict() == {\n            'a': [None, None],\n            'b': [\"Xxx\", \"#N/A\"],\n            'c': [\"1\", \"\"],\n            'd': [2, None],\n        }\n\n        opts = ConvertOptions(null_values=[])\n        rows = b\"a,b\\n#N/A,\\n\"\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.string()),\n                            ('b', pa.string())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [\"#N/A\"],\n            'b': [\"\"],\n        }\n\n    def test_custom_bools(self):\n        # Infer booleans with custom values\n        opts = ConvertOptions(true_values=['T', 'yes'],\n                              false_values=['F', 'no'])\n        rows = (b\"a,b,c\\n\"\n                b\"True,T,t\\n\"\n                b\"False,F,f\\n\"\n                b\"True,yes,yes\\n\"\n                b\"False,no,no\\n\"\n                b\"N/A,N/A,N/A\\n\")\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.string()),\n                            ('b', pa.bool_()),\n                            ('c', pa.string())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [\"True\", \"False\", \"True\", \"False\", \"N/A\"],\n            'b': [True, False, True, False, None],\n            'c': [\"t\", \"f\", \"yes\", \"no\", \"N/A\"],\n        }\n\n    def test_column_types(self):\n        # Ask for specific column types in ConvertOptions\n        opts = ConvertOptions(column_types={'b': 'float32',\n                                            'c': 'string',\n                                            'd': 'boolean',\n                                            'e': pa.decimal128(11, 2),\n                                            'zz': 'null'})\n        rows = b\"a,b,c,d,e\\n1,2,3,true,1.0\\n4,-5,6,false,0\\n\"\n        table = self.read_bytes(rows, convert_options=opts)\n        schema = pa.schema([('a', pa.int64()),\n                            ('b', pa.float32()),\n                            ('c', pa.string()),\n                            ('d', pa.bool_()),\n                            ('e', pa.decimal128(11, 2))])\n        expected = {\n            'a': [1, 4],\n            'b': [2.0, -5.0],\n            'c': [\"3\", \"6\"],\n            'd': [True, False],\n            'e': [Decimal(\"1.00\"), Decimal(\"0.00\")]\n        }\n        assert table.schema == schema\n        assert table.to_pydict() == expected\n        # Pass column_types as schema\n        opts = ConvertOptions(\n            column_types=pa.schema([('b', pa.float32()),\n                                    ('c', pa.string()),\n                                    ('d', pa.bool_()),\n                                    ('e', pa.decimal128(11, 2)),\n                                    ('zz', pa.bool_())]))\n        table = self.read_bytes(rows, convert_options=opts)\n        assert table.schema == schema\n        assert table.to_pydict() == expected\n        # One of the columns in column_types fails converting\n        rows = b\"a,b,c,d,e\\n1,XXX,3,true,5\\n4,-5,6,false,7\\n\"\n        with pytest.raises(pa.ArrowInvalid) as exc:\n            self.read_bytes(rows, convert_options=opts)\n        err = str(exc.value)\n        assert \"In CSV column #1: \" in err\n        assert \"CSV conversion error to float: invalid value 'XXX'\" in err\n\n    def test_column_types_dict(self):\n        # Ask for dict-encoded column types in ConvertOptions\n        column_types = [\n            ('a', pa.dictionary(pa.int32(), pa.utf8())),\n            ('b', pa.dictionary(pa.int32(), pa.int64())),\n            ('c', pa.dictionary(pa.int32(), pa.decimal128(11, 2))),\n            ('d', pa.dictionary(pa.int32(), pa.large_utf8()))]\n\n        opts = ConvertOptions(column_types=dict(column_types))\n        rows = (b\"a,b,c,d\\n\"\n                b\"abc,123456,1.0,zz\\n\"\n                b\"defg,123456,0.5,xx\\n\"\n                b\"abc,N/A,1.0,xx\\n\")\n        table = self.read_bytes(rows, convert_options=opts)\n\n        schema = pa.schema(column_types)\n        expected = {\n            'a': [\"abc\", \"defg\", \"abc\"],\n            'b': [123456, 123456, None],\n            'c': [Decimal(\"1.00\"), Decimal(\"0.50\"), Decimal(\"1.00\")],\n            'd': [\"zz\", \"xx\", \"xx\"],\n        }\n        assert table.schema == schema\n        assert table.to_pydict() == expected\n\n        # Unsupported index type\n        column_types[0] = ('a', pa.dictionary(pa.int8(), pa.utf8()))\n\n        opts = ConvertOptions(column_types=dict(column_types))\n        with pytest.raises(NotImplementedError):\n            table = self.read_bytes(rows, convert_options=opts)\n\n    def test_column_types_with_column_names(self):\n        # When both `column_names` and `column_types` are given, names\n        # in `column_types` should refer to names in `column_names`\n        rows = b\"a,b\\nc,d\\ne,f\\n\"\n        read_options = ReadOptions(column_names=['x', 'y'])\n        convert_options = ConvertOptions(column_types={'x': pa.binary()})\n        table = self.read_bytes(rows, read_options=read_options,\n                                convert_options=convert_options)\n        schema = pa.schema([('x', pa.binary()),\n                            ('y', pa.string())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'x': [b'a', b'c', b'e'],\n            'y': ['b', 'd', 'f'],\n        }\n\n    def test_no_ending_newline(self):\n        # No \\n after last line\n        rows = b\"a,b,c\\n1,2,3\\n4,5,6\"\n        table = self.read_bytes(rows)\n        assert table.to_pydict() == {\n            'a': [1, 4],\n            'b': [2, 5],\n            'c': [3, 6],\n        }\n\n    def test_trivial(self):\n        # A bit pointless, but at least it shouldn't crash\n        rows = b\",\\n\\n\"\n        table = self.read_bytes(rows)\n        assert table.to_pydict() == {'': []}\n\n    def test_empty_lines(self):\n        rows = b\"a,b\\n\\r1,2\\r\\n\\r\\n3,4\\r\\n\"\n        table = self.read_bytes(rows)\n        assert table.to_pydict() == {\n            'a': [1, 3],\n            'b': [2, 4],\n        }\n        parse_options = ParseOptions(ignore_empty_lines=False)\n        table = self.read_bytes(rows, parse_options=parse_options)\n        assert table.to_pydict() == {\n            'a': [None, 1, None, 3],\n            'b': [None, 2, None, 4],\n        }\n        read_options = ReadOptions(skip_rows=2)\n        table = self.read_bytes(rows, parse_options=parse_options,\n                                read_options=read_options)\n        assert table.to_pydict() == {\n            '1': [None, 3],\n            '2': [None, 4],\n        }\n\n    def test_invalid_csv(self):\n        # Various CSV errors\n        rows = b\"a,b,c\\n1,2\\n4,5,6\\n\"\n        with pytest.raises(pa.ArrowInvalid, match=\"Expected 3 columns, got 2\"):\n            self.read_bytes(rows)\n        rows = b\"a,b,c\\n1,2,3\\n4\"\n        with pytest.raises(pa.ArrowInvalid, match=\"Expected 3 columns, got 1\"):\n            self.read_bytes(rows)\n        for rows in [b\"\", b\"\\n\", b\"\\r\\n\", b\"\\r\", b\"\\n\\n\"]:\n            with pytest.raises(pa.ArrowInvalid, match=\"Empty CSV file\"):\n                self.read_bytes(rows)\n\n    def test_options_delimiter(self):\n        rows = b\"a;b,c\\nde,fg;eh\\n\"\n        table = self.read_bytes(rows)\n        assert table.to_pydict() == {\n            'a;b': ['de'],\n            'c': ['fg;eh'],\n        }\n        opts = ParseOptions(delimiter=';')\n        table = self.read_bytes(rows, parse_options=opts)\n        assert table.to_pydict() == {\n            'a': ['de,fg'],\n            'b,c': ['eh'],\n        }\n\n    def test_small_random_csv(self):\n        csv, expected = make_random_csv(num_cols=2, num_rows=10)\n        table = self.read_bytes(csv)\n        assert table.schema == expected.schema\n        assert table.equals(expected)\n        assert table.to_pydict() == expected.to_pydict()\n\n    def test_stress_block_sizes(self):\n        # Test a number of small block sizes to stress block stitching\n        csv_base, expected = make_random_csv(num_cols=2, num_rows=500)\n        block_sizes = [11, 12, 13, 17, 37, 111]\n        csvs = [csv_base, csv_base.rstrip(b'\\r\\n')]\n        for csv in csvs:\n            for block_size in block_sizes:\n                read_options = ReadOptions(block_size=block_size)\n                table = self.read_bytes(csv, read_options=read_options)\n                assert table.schema == expected.schema\n                if not table.equals(expected):\n                    # Better error output\n                    assert table.to_pydict() == expected.to_pydict()\n\n    def test_stress_convert_options_blowup(self):\n        # ARROW-6481: A convert_options with a very large number of columns\n        # should not blow memory and CPU time.\n        try:\n            clock = time.thread_time\n        except AttributeError:\n            clock = time.time\n        num_columns = 10000\n        col_names = [\"K{}\".format(i) for i in range(num_columns)]\n        csv = make_empty_csv(col_names)\n        t1 = clock()\n        convert_options = ConvertOptions(\n            column_types={k: pa.string() for k in col_names[::2]})\n        table = self.read_bytes(csv, convert_options=convert_options)\n        dt = clock() - t1\n        # Check that processing time didn't blow up.\n        # This is a conservative check (it takes less than 300 ms\n        # in debug mode on my local machine).\n        assert dt <= 10.0\n        # Check result\n        assert table.num_columns == num_columns\n        assert table.num_rows == 0\n        assert table.column_names == col_names\n\n    def test_cancellation(self):\n        if (threading.current_thread().ident !=\n                threading.main_thread().ident):\n            pytest.skip(\"test only works from main Python thread\")\n        # Skips test if not available\n        raise_signal = util.get_raise_signal()\n        signum = signal.SIGINT\n\n        def signal_from_thread():\n            # Give our workload a chance to start up\n            time.sleep(0.2)\n            raise_signal(signum)\n\n        # We start with a small CSV reading workload and increase its size\n        # until it's large enough to get an interruption during it, even in\n        # release mode on fast machines.\n        last_duration = 0.0\n        workload_size = 100_000\n        attempts = 0\n\n        while last_duration < 5.0 and attempts < 10:\n            print(\"workload size:\", workload_size)\n            large_csv = b\"a,b,c\\n\" + b\"1,2,3\\n\" * workload_size\n            exc_info = None\n\n            try:\n                # We use a signal fd to reliably ensure that the signal\n                # has been delivered to Python, regardless of how exactly\n                # it was caught.\n                with util.signal_wakeup_fd() as sigfd:\n                    try:\n                        t = threading.Thread(target=signal_from_thread)\n                        t.start()\n                        t1 = time.time()\n                        try:\n                            self.read_bytes(large_csv)\n                        except KeyboardInterrupt as e:\n                            exc_info = e\n                            last_duration = time.time() - t1\n                    finally:\n                        # Wait for signal to arrive if it didn't already,\n                        # to avoid getting a KeyboardInterrupt after the\n                        # `except` block below.\n                        select.select([sigfd], [], [sigfd], 10.0)\n\n            except KeyboardInterrupt:\n                # KeyboardInterrupt didn't interrupt `read_bytes` above.\n                pass\n\n            if exc_info is not None:\n                # We managed to get `self.read_bytes` interrupted, see if it\n                # was actually interrupted inside Arrow C++ or in the Python\n                # scaffolding.\n                if exc_info.__context__ is not None:\n                    # Interrupted inside Arrow C++, we're satisfied now\n                    break\n\n            # Increase workload size to get a better chance\n            workload_size = workload_size * 3\n\n        if exc_info is None:\n            pytest.fail(\"Failed to get an interruption during CSV reading\")\n\n        # Interruption should have arrived timely\n        assert last_duration <= 2.0\n        e = exc_info.__context__\n        assert isinstance(e, pa.ArrowCancelled)\n        assert e.signum == signum\n\n    def test_cancellation_disabled(self):\n        # ARROW-12622: reader would segfault when the cancelling signal\n        # handler was not enabled (e.g. if disabled, or if not on the\n        # main thread)\n        t = threading.Thread(\n            target=lambda: self.read_bytes(b\"f64\\n0.1\"))\n        t.start()\n        t.join()\n\n\nclass TestSerialCSVTableRead(BaseCSVTableRead):\n    @property\n    def use_threads(self):\n        return False\n\n\nclass TestThreadedCSVTableRead(BaseCSVTableRead):\n    @property\n    def use_threads(self):\n        return True\n\n\nclass BaseStreamingCSVRead(BaseTestCSV):\n\n    def open_csv(self, csv, *args, **kwargs):\n        \"\"\"\n        Reads the CSV file into memory using pyarrow's open_csv\n        csv The CSV bytes\n        args Positional arguments to be forwarded to pyarrow's open_csv\n        kwargs Keyword arguments to be forwarded to pyarrow's open_csv\n        \"\"\"\n        read_options = kwargs.setdefault('read_options', ReadOptions())\n        read_options.use_threads = self.use_threads\n        return open_csv(csv, *args, **kwargs)\n\n    def open_bytes(self, b, **kwargs):\n        return self.open_csv(pa.py_buffer(b), **kwargs)\n\n    def check_reader(self, reader, expected_schema, expected_data):\n        assert reader.schema == expected_schema\n        batches = list(reader)\n        assert len(batches) == len(expected_data)\n        for batch, expected_batch in zip(batches, expected_data):\n            batch.validate(full=True)\n            assert batch.schema == expected_schema\n            assert batch.to_pydict() == expected_batch\n\n    def read_bytes(self, b, **kwargs):\n        return self.open_bytes(b, **kwargs).read_all()\n\n    def test_file_object(self):\n        data = b\"a,b\\n1,2\\n3,4\\n\"\n        expected_data = {'a': [1, 3], 'b': [2, 4]}\n        bio = io.BytesIO(data)\n        reader = self.open_csv(bio)\n        expected_schema = pa.schema([('a', pa.int64()),\n                                     ('b', pa.int64())])\n        self.check_reader(reader, expected_schema, [expected_data])\n\n    def test_header(self):\n        rows = b\"abc,def,gh\\n\"\n        reader = self.open_bytes(rows)\n        expected_schema = pa.schema([('abc', pa.null()),\n                                     ('def', pa.null()),\n                                     ('gh', pa.null())])\n        self.check_reader(reader, expected_schema, [])\n\n    def test_inference(self):\n        # Inference is done on first block\n        rows = b\"a,b\\n123,456\\nabc,de\\xff\\ngh,ij\\n\"\n        expected_schema = pa.schema([('a', pa.string()),\n                                     ('b', pa.binary())])\n\n        read_options = ReadOptions()\n        read_options.block_size = len(rows)\n        reader = self.open_bytes(rows, read_options=read_options)\n        self.check_reader(reader, expected_schema,\n                          [{'a': ['123', 'abc', 'gh'],\n                            'b': [b'456', b'de\\xff', b'ij']}])\n\n        read_options.block_size = len(rows) - 1\n        reader = self.open_bytes(rows, read_options=read_options)\n        self.check_reader(reader, expected_schema,\n                          [{'a': ['123', 'abc'],\n                            'b': [b'456', b'de\\xff']},\n                           {'a': ['gh'],\n                            'b': [b'ij']}])\n\n    def test_inference_failure(self):\n        # Inference on first block, then conversion failure on second block\n        rows = b\"a,b\\n123,456\\nabc,de\\xff\\ngh,ij\\n\"\n        read_options = ReadOptions()\n        read_options.block_size = len(rows) - 7\n        reader = self.open_bytes(rows, read_options=read_options)\n        expected_schema = pa.schema([('a', pa.int64()),\n                                     ('b', pa.int64())])\n        assert reader.schema == expected_schema\n        assert reader.read_next_batch().to_pydict() == {\n            'a': [123], 'b': [456]\n        }\n        # Second block\n        with pytest.raises(ValueError,\n                           match=\"CSV conversion error to int64\"):\n            reader.read_next_batch()\n        # EOF\n        with pytest.raises(StopIteration):\n            reader.read_next_batch()\n\n    def test_invalid_csv(self):\n        # CSV errors on first block\n        rows = b\"a,b\\n1,2,3\\n4,5\\n6,7\\n\"\n        read_options = ReadOptions()\n        read_options.block_size = 10\n        with pytest.raises(pa.ArrowInvalid,\n                           match=\"Expected 2 columns, got 3\"):\n            reader = self.open_bytes(\n                rows, read_options=read_options)\n\n        # CSV errors on second block\n        rows = b\"a,b\\n1,2\\n3,4,5\\n6,7\\n\"\n        read_options.block_size = 8\n        reader = self.open_bytes(rows, read_options=read_options)\n        assert reader.read_next_batch().to_pydict() == {'a': [1], 'b': [2]}\n        with pytest.raises(pa.ArrowInvalid,\n                           match=\"Expected 2 columns, got 3\"):\n            reader.read_next_batch()\n        # Cannot continue after a parse error\n        with pytest.raises(StopIteration):\n            reader.read_next_batch()\n\n    def test_options_delimiter(self):\n        rows = b\"a;b,c\\nde,fg;eh\\n\"\n        reader = self.open_bytes(rows)\n        expected_schema = pa.schema([('a;b', pa.string()),\n                                     ('c', pa.string())])\n        self.check_reader(reader, expected_schema,\n                          [{'a;b': ['de'],\n                            'c': ['fg;eh']}])\n\n        opts = ParseOptions(delimiter=';')\n        reader = self.open_bytes(rows, parse_options=opts)\n        expected_schema = pa.schema([('a', pa.string()),\n                                     ('b,c', pa.string())])\n        self.check_reader(reader, expected_schema,\n                          [{'a': ['de,fg'],\n                            'b,c': ['eh']}])\n\n    def test_no_ending_newline(self):\n        # No \\n after last line\n        rows = b\"a,b,c\\n1,2,3\\n4,5,6\"\n        reader = self.open_bytes(rows)\n        expected_schema = pa.schema([('a', pa.int64()),\n                                     ('b', pa.int64()),\n                                     ('c', pa.int64())])\n        self.check_reader(reader, expected_schema,\n                          [{'a': [1, 4],\n                            'b': [2, 5],\n                            'c': [3, 6]}])\n\n    def test_empty_file(self):\n        with pytest.raises(ValueError, match=\"Empty CSV file\"):\n            self.open_bytes(b\"\")\n\n    def test_column_options(self):\n        # With column_names\n        rows = b\"1,2,3\\n4,5,6\"\n        read_options = ReadOptions()\n        read_options.column_names = ['d', 'e', 'f']\n        reader = self.open_bytes(rows, read_options=read_options)\n        expected_schema = pa.schema([('d', pa.int64()),\n                                     ('e', pa.int64()),\n                                     ('f', pa.int64())])\n        self.check_reader(reader, expected_schema,\n                          [{'d': [1, 4],\n                            'e': [2, 5],\n                            'f': [3, 6]}])\n\n        # With include_columns\n        convert_options = ConvertOptions()\n        convert_options.include_columns = ['f', 'e']\n        reader = self.open_bytes(rows, read_options=read_options,\n                                 convert_options=convert_options)\n        expected_schema = pa.schema([('f', pa.int64()),\n                                     ('e', pa.int64())])\n        self.check_reader(reader, expected_schema,\n                          [{'e': [2, 5],\n                            'f': [3, 6]}])\n\n        # With column_types\n        convert_options.column_types = {'e': pa.string()}\n        reader = self.open_bytes(rows, read_options=read_options,\n                                 convert_options=convert_options)\n        expected_schema = pa.schema([('f', pa.int64()),\n                                     ('e', pa.string())])\n        self.check_reader(reader, expected_schema,\n                          [{'e': [\"2\", \"5\"],\n                            'f': [3, 6]}])\n\n        # Missing columns in include_columns\n        convert_options.include_columns = ['g', 'f', 'e']\n        with pytest.raises(\n                KeyError,\n                match=\"Column 'g' in include_columns does not exist\"):\n            reader = self.open_bytes(rows, read_options=read_options,\n                                     convert_options=convert_options)\n\n        convert_options.include_missing_columns = True\n        reader = self.open_bytes(rows, read_options=read_options,\n                                 convert_options=convert_options)\n        expected_schema = pa.schema([('g', pa.null()),\n                                     ('f', pa.int64()),\n                                     ('e', pa.string())])\n        self.check_reader(reader, expected_schema,\n                          [{'g': [None, None],\n                            'e': [\"2\", \"5\"],\n                            'f': [3, 6]}])\n\n        convert_options.column_types = {'e': pa.string(), 'g': pa.float64()}\n        reader = self.open_bytes(rows, read_options=read_options,\n                                 convert_options=convert_options)\n        expected_schema = pa.schema([('g', pa.float64()),\n                                     ('f', pa.int64()),\n                                     ('e', pa.string())])\n        self.check_reader(reader, expected_schema,\n                          [{'g': [None, None],\n                            'e': [\"2\", \"5\"],\n                            'f': [3, 6]}])\n\n    def test_encoding(self):\n        # latin-1 (invalid utf-8)\n        rows = b\"a,b\\nun,\\xe9l\\xe9phant\"\n        read_options = ReadOptions()\n        reader = self.open_bytes(rows, read_options=read_options)\n        expected_schema = pa.schema([('a', pa.string()),\n                                     ('b', pa.binary())])\n        self.check_reader(reader, expected_schema,\n                          [{'a': [\"un\"],\n                            'b': [b\"\\xe9l\\xe9phant\"]}])\n\n        read_options.encoding = 'latin1'\n        reader = self.open_bytes(rows, read_options=read_options)\n        expected_schema = pa.schema([('a', pa.string()),\n                                     ('b', pa.string())])\n        self.check_reader(reader, expected_schema,\n                          [{'a': [\"un\"],\n                            'b': [\"\u00e9l\u00e9phant\"]}])\n\n        # utf-16\n        rows = (b'\\xff\\xfea\\x00,\\x00b\\x00\\n\\x00u\\x00n\\x00,'\n                b'\\x00\\xe9\\x00l\\x00\\xe9\\x00p\\x00h\\x00a\\x00n\\x00t\\x00')\n        read_options.encoding = 'utf16'\n        reader = self.open_bytes(rows, read_options=read_options)\n        expected_schema = pa.schema([('a', pa.string()),\n                                     ('b', pa.string())])\n        self.check_reader(reader, expected_schema,\n                          [{'a': [\"un\"],\n                            'b': [\"\u00e9l\u00e9phant\"]}])\n\n    def test_small_random_csv(self):\n        csv, expected = make_random_csv(num_cols=2, num_rows=10)\n        reader = self.open_bytes(csv)\n        table = reader.read_all()\n        assert table.schema == expected.schema\n        assert table.equals(expected)\n        assert table.to_pydict() == expected.to_pydict()\n\n    def test_stress_block_sizes(self):\n        # Test a number of small block sizes to stress block stitching\n        csv_base, expected = make_random_csv(num_cols=2, num_rows=500)\n        block_sizes = [19, 21, 23, 26, 37, 111]\n        csvs = [csv_base, csv_base.rstrip(b'\\r\\n')]\n        for csv in csvs:\n            for block_size in block_sizes:\n                # Need at least two lines for type inference\n                assert csv[:block_size].count(b'\\n') >= 2\n                read_options = ReadOptions(block_size=block_size)\n                reader = self.open_bytes(\n                    csv, read_options=read_options)\n                table = reader.read_all()\n                assert table.schema == expected.schema\n                if not table.equals(expected):\n                    # Better error output\n                    assert table.to_pydict() == expected.to_pydict()\n\n    def test_batch_lifetime(self):\n        gc.collect()\n        old_allocated = pa.total_allocated_bytes()\n\n        # Memory occupation should not grow with CSV file size\n        def check_one_batch(reader, expected):\n            batch = reader.read_next_batch()\n            assert batch.to_pydict() == expected\n\n        rows = b\"10,11\\n12,13\\n14,15\\n16,17\\n\"\n        read_options = ReadOptions()\n        read_options.column_names = ['a', 'b']\n        read_options.block_size = 6\n        reader = self.open_bytes(rows, read_options=read_options)\n        check_one_batch(reader, {'a': [10], 'b': [11]})\n        allocated_after_first_batch = pa.total_allocated_bytes()\n        check_one_batch(reader, {'a': [12], 'b': [13]})\n        assert pa.total_allocated_bytes() <= allocated_after_first_batch\n        check_one_batch(reader, {'a': [14], 'b': [15]})\n        assert pa.total_allocated_bytes() <= allocated_after_first_batch\n        check_one_batch(reader, {'a': [16], 'b': [17]})\n        assert pa.total_allocated_bytes() <= allocated_after_first_batch\n        with pytest.raises(StopIteration):\n            reader.read_next_batch()\n        assert pa.total_allocated_bytes() == old_allocated\n        reader = None\n        assert pa.total_allocated_bytes() == old_allocated\n\n    def test_header_skip_rows(self):\n        super().test_header_skip_rows()\n\n        rows = b\"ab,cd\\nef,gh\\nij,kl\\nmn,op\\n\"\n\n        # Skipping all rows immediately results in end of iteration\n        opts = ReadOptions()\n        opts.skip_rows = 4\n        opts.column_names = ['ab', 'cd']\n        reader = self.open_bytes(rows, read_options=opts)\n        with pytest.raises(StopIteration):\n            assert reader.read_next_batch()\n\n    def test_skip_rows_after_names(self):\n        super().test_skip_rows_after_names()\n\n        rows = b\"ab,cd\\nef,gh\\nij,kl\\nmn,op\\n\"\n\n        # Skipping all rows immediately results in end of iteration\n        opts = ReadOptions()\n        opts.skip_rows_after_names = 3\n        reader = self.open_bytes(rows, read_options=opts)\n        with pytest.raises(StopIteration):\n            assert reader.read_next_batch()\n\n        # Skipping beyond all rows immediately results in end of iteration\n        opts.skip_rows_after_names = 99999\n        reader = self.open_bytes(rows, read_options=opts)\n        with pytest.raises(StopIteration):\n            assert reader.read_next_batch()\n\n\nclass TestSerialStreamingCSVRead(BaseStreamingCSVRead):\n    @property\n    def use_threads(self):\n        return False\n\n\nclass TestThreadedStreamingCSVRead(BaseStreamingCSVRead):\n    @property\n    def use_threads(self):\n        return True\n\n\nclass BaseTestCompressedCSVRead:\n\n    def setUp(self):\n        self.tmpdir = tempfile.mkdtemp(prefix='arrow-csv-test-')\n\n    def tearDown(self):\n        shutil.rmtree(self.tmpdir)\n\n    def read_csv(self, csv_path):\n        try:\n            return read_csv(csv_path)\n        except pa.ArrowNotImplementedError as e:\n            pytest.skip(str(e))\n\n    def test_random_csv(self):\n        csv, expected = make_random_csv(num_cols=2, num_rows=100)\n        csv_path = os.path.join(self.tmpdir, self.csv_filename)\n        self.write_file(csv_path, csv)\n        table = self.read_csv(csv_path)\n        table.validate(full=True)\n        assert table.schema == expected.schema\n        assert table.equals(expected)\n        assert table.to_pydict() == expected.to_pydict()\n\n\nclass TestGZipCSVRead(BaseTestCompressedCSVRead, unittest.TestCase):\n    csv_filename = \"compressed.csv.gz\"\n\n    def write_file(self, path, contents):\n        with gzip.open(path, 'wb', 3) as f:\n            f.write(contents)\n\n    def test_concatenated(self):\n        # ARROW-5974\n        csv_path = os.path.join(self.tmpdir, self.csv_filename)\n        with gzip.open(csv_path, 'wb', 3) as f:\n            f.write(b\"ab,cd\\nef,gh\\n\")\n        with gzip.open(csv_path, 'ab', 3) as f:\n            f.write(b\"ij,kl\\nmn,op\\n\")\n        table = self.read_csv(csv_path)\n        assert table.to_pydict() == {\n            'ab': ['ef', 'ij', 'mn'],\n            'cd': ['gh', 'kl', 'op'],\n        }\n\n\nclass TestBZ2CSVRead(BaseTestCompressedCSVRead, unittest.TestCase):\n    csv_filename = \"compressed.csv.bz2\"\n\n    def write_file(self, path, contents):\n        with bz2.BZ2File(path, 'w') as f:\n            f.write(contents)\n\n\ndef test_read_csv_does_not_close_passed_file_handles():\n    # ARROW-4823\n    buf = io.BytesIO(b\"a,b,c\\n1,2,3\\n4,5,6\")\n    read_csv(buf)\n    assert not buf.closed\n\n\ndef test_write_read_round_trip():\n    t = pa.Table.from_arrays([[1, 2, 3], [\"a\", \"b\", \"c\"]], [\"c1\", \"c2\"])\n    record_batch = t.to_batches(max_chunksize=4)[0]\n    for data in [t, record_batch]:\n        # Test with header\n        buf = io.BytesIO()\n        write_csv(data, buf, WriteOptions(include_header=True))\n        buf.seek(0)\n        assert t == read_csv(buf)\n\n        # Test without header\n        buf = io.BytesIO()\n        write_csv(data, buf, WriteOptions(include_header=False))\n        buf.seek(0)\n\n        read_options = ReadOptions(column_names=t.column_names)\n        assert t == read_csv(buf, read_options=read_options)\n\n    # Test with writer\n    for read_options, parse_options, write_options in [\n        (None, None, WriteOptions(include_header=True)),\n        (ReadOptions(column_names=t.column_names), None,\n         WriteOptions(include_header=False)),\n        (None, ParseOptions(delimiter='|'),\n         WriteOptions(include_header=True, delimiter='|')),\n        (ReadOptions(column_names=t.column_names),\n         ParseOptions(delimiter='\\t'),\n         WriteOptions(include_header=False, delimiter='\\t')),\n    ]:\n        buf = io.BytesIO()\n        with CSVWriter(buf, t.schema, write_options=write_options) as writer:\n            writer.write_table(t)\n        buf.seek(0)\n        assert t == read_csv(buf, read_options=read_options,\n                             parse_options=parse_options)\n        buf = io.BytesIO()\n        with CSVWriter(buf, t.schema, write_options=write_options) as writer:\n            for batch in t.to_batches(max_chunksize=1):\n                writer.write_batch(batch)\n        buf.seek(0)\n        assert t == read_csv(buf, read_options=read_options,\n                             parse_options=parse_options)\n\n\ndef test_write_quoting_style():\n    t = pa.Table.from_arrays([[1, 2, None], [\"a\", None, \"c\"]], [\"c1\", \"c2\"])\n    buf = io.BytesIO()\n    for write_options, res in [\n        (WriteOptions(quoting_style='none'), b'\"c1\",\"c2\"\\n1,a\\n2,\\n,c\\n'),\n        (WriteOptions(), b'\"c1\",\"c2\"\\n1,\"a\"\\n2,\\n,\"c\"\\n'),\n        (WriteOptions(quoting_style='all_valid'),\n         b'\"c1\",\"c2\"\\n\"1\",\"a\"\\n\"2\",\\n,\"c\"\\n'),\n    ]:\n        with CSVWriter(buf, t.schema, write_options=write_options) as writer:\n            writer.write_table(t)\n        assert buf.getvalue() == res\n        buf.seek(0)\n\n    # Test writing special characters with different quoting styles\n    t = pa.Table.from_arrays([[\",\", \"\\\"\"]], [\"c1\"])\n    buf = io.BytesIO()\n    for write_options, res in [\n        (WriteOptions(quoting_style='needed'), b'\"c1\"\\n\",\"\\n\"\"\"\"\\n'),\n        (WriteOptions(quoting_style='none'), pa.lib.ArrowInvalid),\n    ]:\n        with CSVWriter(buf, t.schema, write_options=write_options) as writer:\n            try:\n                writer.write_table(t)\n            except Exception as e:\n                # This will trigger when we try to write a comma (,)\n                # without quotes, which is invalid\n                assert isinstance(e, res)\n                break\n        assert buf.getvalue() == res\n        buf.seek(0)\n\n\ndef test_read_csv_reference_cycle():\n    # ARROW-13187\n    def inner():\n        buf = io.BytesIO(b\"a,b,c\\n1,2,3\\n4,5,6\")\n        table = read_csv(buf)\n        return weakref.ref(table)\n\n    with util.disabled_gc():\n        wr = inner()\n        assert wr() is None\n\n\n@pytest.mark.parametrize(\"type_factory\", (\n    lambda: pa.decimal128(20, 1),\n    lambda: pa.decimal128(38, 15),\n    lambda: pa.decimal256(20, 1),\n    lambda: pa.decimal256(76, 10),\n))\ndef test_write_csv_decimal(tmpdir, type_factory):\n    type = type_factory()\n    table = pa.table({\"col\": pa.array([1, 2]).cast(type)})\n\n    write_csv(table, tmpdir / \"out.csv\")\n    out = read_csv(tmpdir / \"out.csv\")\n\n    assert out.column('col').cast(type) == table.column('col')\n\n\ndef test_read_csv_gil_deadlock():\n    # GH-38676\n    # This test depends on several preconditions:\n    # - the CSV input is a Python file object\n    # - reading the CSV file produces an error\n    data = b\"a,b,c\"\n\n    class MyBytesIO(io.BytesIO):\n        def read(self, *args):\n            time.sleep(0.001)\n            return super().read(*args)\n\n        def readinto(self, *args):\n            time.sleep(0.001)\n            return super().readinto(*args)\n\n    for i in range(20):\n        with pytest.raises(pa.ArrowInvalid):\n            read_csv(MyBytesIO(data))\n", "python/pyarrow/tests/test_substrait.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport pathlib\n\nimport pytest\n\nimport pyarrow as pa\nimport pyarrow.compute as pc\nfrom pyarrow.lib import tobytes\nfrom pyarrow.lib import ArrowInvalid, ArrowNotImplementedError\n\ntry:\n    import pyarrow.substrait as substrait\nexcept ImportError:\n    substrait = None\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not substrait'\npytestmark = pytest.mark.substrait\n\n\ndef mock_udf_context(batch_length=10):\n    from pyarrow._compute import _get_udf_context\n    return _get_udf_context(pa.default_memory_pool(), batch_length)\n\n\ndef _write_dummy_data_to_disk(tmpdir, file_name, table):\n    path = os.path.join(str(tmpdir), file_name)\n    with pa.ipc.RecordBatchFileWriter(path, schema=table.schema) as writer:\n        writer.write_table(table)\n    return path\n\n\n@pytest.mark.parametrize(\"use_threads\", [True, False])\ndef test_run_serialized_query(tmpdir, use_threads):\n    substrait_query = \"\"\"\n    {\n        \"version\": { \"major\": 9999 },\n        \"relations\": [\n        {\"rel\": {\n            \"read\": {\n            \"base_schema\": {\n                \"struct\": {\n                \"types\": [\n                            {\"i64\": {}}\n                        ]\n                },\n                \"names\": [\n                        \"foo\"\n                        ]\n            },\n            \"local_files\": {\n                \"items\": [\n                {\n                    \"uri_file\": \"FILENAME_PLACEHOLDER\",\n                    \"arrow\": {}\n                }\n                ]\n            }\n            }\n        }}\n        ]\n    }\n    \"\"\"\n\n    file_name = \"read_data.arrow\"\n    table = pa.table([[1, 2, 3, 4, 5]], names=['foo'])\n    path = _write_dummy_data_to_disk(tmpdir, file_name, table)\n    query = tobytes(substrait_query.replace(\n        \"FILENAME_PLACEHOLDER\", pathlib.Path(path).as_uri()))\n\n    buf = pa._substrait._parse_json_plan(query)\n\n    reader = substrait.run_query(buf, use_threads=use_threads)\n    res_tb = reader.read_all()\n\n    assert table.select([\"foo\"]) == res_tb.select([\"foo\"])\n\n\n@pytest.mark.parametrize(\"query\", (pa.py_buffer(b'buffer'), b\"bytes\", 1))\ndef test_run_query_input_types(tmpdir, query):\n\n    # Passing unsupported type, like int, will not segfault.\n    if not isinstance(query, (pa.Buffer, bytes)):\n        msg = f\"Expected 'pyarrow.Buffer' or bytes, got '{type(query)}'\"\n        with pytest.raises(TypeError, match=msg):\n            substrait.run_query(query)\n        return\n\n    # Otherwise error for invalid query\n    msg = \"ParseFromZeroCopyStream failed for substrait.Plan\"\n    with pytest.raises(OSError, match=msg):\n        substrait.run_query(query)\n\n\ndef test_invalid_plan():\n    query = \"\"\"\n    {\n        \"relations\": [\n        ]\n    }\n    \"\"\"\n    buf = pa._substrait._parse_json_plan(tobytes(query))\n    exec_message = \"Plan has no relations\"\n    with pytest.raises(ArrowInvalid, match=exec_message):\n        substrait.run_query(buf)\n\n\n@pytest.mark.parametrize(\"use_threads\", [True, False])\ndef test_binary_conversion_with_json_options(tmpdir, use_threads):\n    substrait_query = \"\"\"\n    {\n        \"version\": { \"major\": 9999 },\n        \"relations\": [\n        {\"rel\": {\n            \"read\": {\n            \"base_schema\": {\n                \"struct\": {\n                \"types\": [\n                            {\"i64\": {}}\n                        ]\n                },\n                \"names\": [\n                        \"bar\"\n                        ]\n            },\n            \"local_files\": {\n                \"items\": [\n                {\n                    \"uri_file\": \"FILENAME_PLACEHOLDER\",\n                    \"arrow\": {},\n                    \"metadata\" : {\n                      \"created_by\" : {},\n                    }\n                }\n                ]\n            }\n            }\n        }}\n        ]\n    }\n    \"\"\"\n\n    file_name = \"binary_json_data.arrow\"\n    table = pa.table([[1, 2, 3, 4, 5]], names=['bar'])\n    path = _write_dummy_data_to_disk(tmpdir, file_name, table)\n    query = tobytes(substrait_query.replace(\n        \"FILENAME_PLACEHOLDER\", pathlib.Path(path).as_uri()))\n    buf = pa._substrait._parse_json_plan(tobytes(query))\n\n    reader = substrait.run_query(buf, use_threads=use_threads)\n    res_tb = reader.read_all()\n\n    assert table.select([\"bar\"]) == res_tb.select([\"bar\"])\n\n\n# Substrait has not finalized what the URI should be for standard functions\n# In the meantime, lets just check the suffix\ndef has_function(fns, ext_file, fn_name):\n    suffix = f'{ext_file}#{fn_name}'\n    for fn in fns:\n        if fn.endswith(suffix):\n            return True\n    return False\n\n\ndef test_get_supported_functions():\n    supported_functions = pa._substrait.get_supported_functions()\n    # It probably doesn't make sense to exhaustively verify this list but\n    # we can check a sample aggregate and a sample non-aggregate entry\n    assert has_function(supported_functions,\n                        'functions_arithmetic.yaml', 'add')\n    assert has_function(supported_functions,\n                        'functions_arithmetic.yaml', 'sum')\n\n\n@pytest.mark.parametrize(\"use_threads\", [True, False])\ndef test_named_table(use_threads):\n    test_table_1 = pa.Table.from_pydict({\"x\": [1, 2, 3]})\n    test_table_2 = pa.Table.from_pydict({\"x\": [4, 5, 6]})\n    schema_1 = pa.schema([pa.field(\"x\", pa.int64())])\n\n    def table_provider(names, schema):\n        if not names:\n            raise Exception(\"No names provided\")\n        elif names[0] == \"t1\":\n            assert schema == schema_1\n            return test_table_1\n        elif names[1] == \"t2\":\n            return test_table_2\n        else:\n            raise Exception(\"Unrecognized table name\")\n\n    substrait_query = \"\"\"\n    {\n        \"version\": { \"major\": 9999 },\n        \"relations\": [\n        {\"rel\": {\n            \"read\": {\n            \"base_schema\": {\n                \"struct\": {\n                \"types\": [\n                            {\"i64\": {}}\n                        ]\n                },\n                \"names\": [\n                        \"x\"\n                        ]\n            },\n            \"namedTable\": {\n                    \"names\": [\"t1\"]\n            }\n            }\n        }}\n        ]\n    }\n    \"\"\"\n\n    buf = pa._substrait._parse_json_plan(tobytes(substrait_query))\n    reader = pa.substrait.run_query(\n        buf, table_provider=table_provider, use_threads=use_threads)\n    res_tb = reader.read_all()\n    assert res_tb == test_table_1\n\n\ndef test_named_table_invalid_table_name():\n    test_table_1 = pa.Table.from_pydict({\"x\": [1, 2, 3]})\n\n    def table_provider(names, _):\n        if not names:\n            raise Exception(\"No names provided\")\n        elif names[0] == \"t1\":\n            return test_table_1\n        else:\n            raise Exception(\"Unrecognized table name\")\n\n    substrait_query = \"\"\"\n    {\n        \"version\": { \"major\": 9999 },\n        \"relations\": [\n        {\"rel\": {\n            \"read\": {\n            \"base_schema\": {\n                \"struct\": {\n                \"types\": [\n                            {\"i64\": {}}\n                        ]\n                },\n                \"names\": [\n                        \"x\"\n                        ]\n            },\n            \"namedTable\": {\n                    \"names\": [\"t3\"]\n            }\n            }\n        }}\n        ]\n    }\n    \"\"\"\n\n    buf = pa._substrait._parse_json_plan(tobytes(substrait_query))\n    exec_message = \"Invalid NamedTable Source\"\n    with pytest.raises(ArrowInvalid, match=exec_message):\n        substrait.run_query(buf, table_provider=table_provider)\n\n\ndef test_named_table_empty_names():\n    test_table_1 = pa.Table.from_pydict({\"x\": [1, 2, 3]})\n\n    def table_provider(names, _):\n        if not names:\n            raise Exception(\"No names provided\")\n        elif names[0] == \"t1\":\n            return test_table_1\n        else:\n            raise Exception(\"Unrecognized table name\")\n\n    substrait_query = \"\"\"\n    {\n        \"version\": { \"major\": 9999 },\n        \"relations\": [\n        {\"rel\": {\n            \"read\": {\n            \"base_schema\": {\n                \"struct\": {\n                \"types\": [\n                            {\"i64\": {}}\n                        ]\n                },\n                \"names\": [\n                        \"x\"\n                        ]\n            },\n            \"namedTable\": {\n                    \"names\": []\n            }\n            }\n        }}\n        ]\n    }\n    \"\"\"\n    query = tobytes(substrait_query)\n    buf = pa._substrait._parse_json_plan(tobytes(query))\n    exec_message = \"names for NamedTable not provided\"\n    with pytest.raises(ArrowInvalid, match=exec_message):\n        substrait.run_query(buf, table_provider=table_provider)\n\n\n@pytest.mark.parametrize(\"use_threads\", [True, False])\ndef test_udf_via_substrait(unary_func_fixture, use_threads):\n    test_table = pa.Table.from_pydict({\"x\": [1, 2, 3]})\n\n    def table_provider(names, _):\n        if not names:\n            raise Exception(\"No names provided\")\n        elif names[0] == \"t1\":\n            return test_table\n        else:\n            raise Exception(\"Unrecognized table name\")\n\n    substrait_query = b\"\"\"\n    {\n  \"extensionUris\": [\n    {\n      \"extensionUriAnchor\": 1\n    },\n    {\n      \"extensionUriAnchor\": 2,\n      \"uri\": \"urn:arrow:substrait_simple_extension_function\"\n    }\n  ],\n  \"extensions\": [\n    {\n      \"extensionFunction\": {\n        \"extensionUriReference\": 2,\n        \"functionAnchor\": 1,\n        \"name\": \"y=x+1\"\n      }\n    }\n  ],\n  \"relations\": [\n    {\n      \"root\": {\n        \"input\": {\n          \"project\": {\n            \"common\": {\n              \"emit\": {\n                \"outputMapping\": [\n                  1,\n                  2,\n                ]\n              }\n            },\n            \"input\": {\n              \"read\": {\n                \"baseSchema\": {\n                  \"names\": [\n                    \"t\",\n                  ],\n                  \"struct\": {\n                    \"types\": [\n                      {\n                        \"i64\": {\n                          \"nullability\": \"NULLABILITY_REQUIRED\"\n                        }\n                      },\n                    ],\n                    \"nullability\": \"NULLABILITY_REQUIRED\"\n                  }\n                },\n                \"namedTable\": {\n                  \"names\": [\n                    \"t1\"\n                  ]\n                }\n              }\n            },\n            \"expressions\": [\n              {\n                \"selection\": {\n                  \"directReference\": {\n                    \"structField\": {}\n                  },\n                  \"rootReference\": {}\n                }\n              },\n              {\n                \"scalarFunction\": {\n                  \"functionReference\": 1,\n                  \"outputType\": {\n                    \"i64\": {\n                      \"nullability\": \"NULLABILITY_NULLABLE\"\n                    }\n                  },\n                  \"arguments\": [\n                    {\n                      \"value\": {\n                        \"selection\": {\n                          \"directReference\": {\n                            \"structField\": {}\n                          },\n                          \"rootReference\": {}\n                        }\n                      }\n                    }\n                  ]\n                }\n              }\n            ]\n          }\n        },\n        \"names\": [\n          \"x\",\n          \"y\",\n        ]\n      }\n    }\n  ]\n}\n    \"\"\"\n\n    buf = pa._substrait._parse_json_plan(substrait_query)\n    reader = pa.substrait.run_query(\n        buf, table_provider=table_provider, use_threads=use_threads)\n    res_tb = reader.read_all()\n\n    function, name = unary_func_fixture\n    expected_tb = test_table.add_column(1, 'y', function(\n        mock_udf_context(10), test_table['x']))\n    assert res_tb == expected_tb\n\n\ndef test_udf_via_substrait_wrong_udf_name():\n    test_table = pa.Table.from_pydict({\"x\": [1, 2, 3]})\n\n    def table_provider(names, _):\n        if not names:\n            raise Exception(\"No names provided\")\n        elif names[0] == \"t1\":\n            return test_table\n        else:\n            raise Exception(\"Unrecognized table name\")\n\n    substrait_query = b\"\"\"\n    {\n  \"extensionUris\": [\n    {\n      \"extensionUriAnchor\": 1\n    },\n    {\n      \"extensionUriAnchor\": 2,\n      \"uri\": \"urn:arrow:substrait_simple_extension_function\"\n    }\n  ],\n  \"extensions\": [\n    {\n      \"extensionFunction\": {\n        \"extensionUriReference\": 2,\n        \"functionAnchor\": 1,\n        \"name\": \"wrong_udf_name\"\n      }\n    }\n  ],\n  \"relations\": [\n    {\n      \"root\": {\n        \"input\": {\n          \"project\": {\n            \"common\": {\n              \"emit\": {\n                \"outputMapping\": [\n                  1,\n                  2,\n                ]\n              }\n            },\n            \"input\": {\n              \"read\": {\n                \"baseSchema\": {\n                  \"names\": [\n                    \"t\",\n                  ],\n                  \"struct\": {\n                    \"types\": [\n                      {\n                        \"i64\": {\n                          \"nullability\": \"NULLABILITY_REQUIRED\"\n                        }\n                      },\n                    ],\n                    \"nullability\": \"NULLABILITY_REQUIRED\"\n                  }\n                },\n                \"namedTable\": {\n                  \"names\": [\n                    \"t1\"\n                  ]\n                }\n              }\n            },\n            \"expressions\": [\n              {\n                \"selection\": {\n                  \"directReference\": {\n                    \"structField\": {}\n                  },\n                  \"rootReference\": {}\n                }\n              },\n              {\n                \"scalarFunction\": {\n                  \"functionReference\": 1,\n                  \"outputType\": {\n                    \"i64\": {\n                      \"nullability\": \"NULLABILITY_NULLABLE\"\n                    }\n                  },\n                  \"arguments\": [\n                    {\n                      \"value\": {\n                        \"selection\": {\n                          \"directReference\": {\n                            \"structField\": {}\n                          },\n                          \"rootReference\": {}\n                        }\n                      }\n                    }\n                  ]\n                }\n              }\n            ]\n          }\n        },\n        \"names\": [\n          \"x\",\n          \"y\",\n        ]\n      }\n    }\n  ]\n}\n    \"\"\"\n\n    buf = pa._substrait._parse_json_plan(substrait_query)\n    with pytest.raises(pa.ArrowKeyError) as excinfo:\n        pa.substrait.run_query(buf, table_provider=table_provider)\n    assert \"No function registered\" in str(excinfo.value)\n\n\n@pytest.mark.parametrize(\"use_threads\", [True, False])\ndef test_output_field_names(use_threads):\n    in_table = pa.Table.from_pydict({\"x\": [1, 2, 3]})\n\n    def table_provider(names, schema):\n        return in_table\n\n    substrait_query = \"\"\"\n    {\n      \"version\": { \"major\": 9999 },\n      \"relations\": [\n        {\n          \"root\": {\n            \"input\": {\n              \"read\": {\n                \"base_schema\": {\n                  \"struct\": {\n                    \"types\": [{\"i64\": {}}]\n                  },\n                  \"names\": [\"x\"]\n                },\n                \"namedTable\": {\n                  \"names\": [\"t1\"]\n                }\n              }\n            },\n            \"names\": [\"out\"]\n          }\n        }\n      ]\n    }\n    \"\"\"\n\n    buf = pa._substrait._parse_json_plan(tobytes(substrait_query))\n    reader = pa.substrait.run_query(\n        buf, table_provider=table_provider, use_threads=use_threads)\n    res_tb = reader.read_all()\n\n    expected = pa.Table.from_pydict({\"out\": [1, 2, 3]})\n\n    assert res_tb == expected\n\n\ndef test_scalar_aggregate_udf_basic(varargs_agg_func_fixture):\n\n    test_table = pa.Table.from_pydict(\n        {\"k\": [1, 1, 2, 2], \"v1\": [1, 2, 3, 4],\n         \"v2\": [1.0, 1.0, 1.0, 1.0]}\n    )\n\n    def table_provider(names, _):\n        return test_table\n\n    substrait_query = b\"\"\"\n{\n  \"extensionUris\": [\n    {\n      \"extensionUriAnchor\": 1,\n      \"uri\": \"urn:arrow:substrait_simple_extension_function\"\n    },\n  ],\n  \"extensions\": [\n    {\n      \"extensionFunction\": {\n        \"extensionUriReference\": 1,\n        \"functionAnchor\": 1,\n        \"name\": \"sum_mean\"\n      }\n    }\n  ],\n  \"relations\": [\n    {\n      \"root\": {\n        \"input\": {\n          \"extensionSingle\": {\n            \"common\": {\n              \"emit\": {\n                \"outputMapping\": [\n                  0,\n                  1\n                ]\n              }\n            },\n            \"input\": {\n              \"read\": {\n                \"baseSchema\": {\n                  \"names\": [\n                    \"k\",\n                    \"v1\",\n                    \"v2\",\n                  ],\n                  \"struct\": {\n                    \"types\": [\n                      {\n                        \"i64\": {\n                          \"nullability\": \"NULLABILITY_REQUIRED\"\n                        }\n                      },\n                      {\n                        \"i64\": {\n                          \"nullability\": \"NULLABILITY_NULLABLE\"\n                        }\n                      },\n                      {\n                        \"fp64\": {\n                          \"nullability\": \"NULLABILITY_NULLABLE\"\n                        }\n                      }\n                    ],\n                    \"nullability\": \"NULLABILITY_REQUIRED\"\n                  }\n                },\n                \"namedTable\": {\n                  \"names\": [\"t1\"]\n                }\n              }\n            },\n            \"detail\": {\n              \"@type\": \"/arrow.substrait_ext.SegmentedAggregateRel\",\n              \"segmentKeys\": [\n                {\n                  \"directReference\": {\n                    \"structField\": {}\n                  },\n                  \"rootReference\": {}\n                }\n              ],\n              \"measures\": [\n                {\n                  \"measure\": {\n                    \"functionReference\": 1,\n                    \"phase\": \"AGGREGATION_PHASE_INITIAL_TO_RESULT\",\n                    \"outputType\": {\n                      \"fp64\": {\n                        \"nullability\": \"NULLABILITY_NULLABLE\"\n                      }\n                    },\n                    \"arguments\": [\n                      {\n                        \"value\": {\n                          \"selection\": {\n                            \"directReference\": {\n                              \"structField\": {\n                                \"field\": 1\n                              }\n                            },\n                            \"rootReference\": {}\n                          }\n                        }\n                      },\n                      {\n                        \"value\": {\n                          \"selection\": {\n                            \"directReference\": {\n                              \"structField\": {\n                                \"field\": 2\n                              }\n                            },\n                            \"rootReference\": {}\n                          }\n                        }\n                      }\n                    ]\n                  }\n                }\n              ]\n            }\n          }\n        },\n        \"names\": [\n          \"k\",\n          \"v_avg\"\n        ]\n      }\n    }\n  ],\n}\n\"\"\"\n    buf = pa._substrait._parse_json_plan(substrait_query)\n    reader = pa.substrait.run_query(\n        buf, table_provider=table_provider, use_threads=False)\n    res_tb = reader.read_all()\n\n    expected_tb = pa.Table.from_pydict({\n        'k': [1, 2],\n        'v_avg': [2.5, 4.5]\n    })\n\n    assert res_tb == expected_tb\n\n\ndef test_hash_aggregate_udf_basic(varargs_agg_func_fixture):\n\n    test_table = pa.Table.from_pydict(\n        {\"t\": [1, 1, 1, 1, 2, 2, 2, 2],\n         \"k\": [1, 0, 0, 1, 0, 1, 0, 1],\n         \"v1\": [1, 2, 3, 4, 5, 6, 7, 8],\n         \"v2\": [1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 4.0, 5.0]}\n    )\n\n    def table_provider(names, _):\n        return test_table\n\n    substrait_query = b\"\"\"\n{\n  \"extensionUris\": [\n    {\n      \"extensionUriAnchor\": 1,\n      \"uri\": \"urn:arrow:substrait_simple_extension_function\"\n    },\n  ],\n  \"extensions\": [\n    {\n      \"extensionFunction\": {\n        \"extensionUriReference\": 1,\n        \"functionAnchor\": 1,\n        \"name\": \"sum_mean\"\n      }\n    }\n  ],\n  \"relations\": [\n    {\n      \"root\": {\n        \"input\": {\n          \"extensionSingle\": {\n            \"common\": {\n              \"emit\": {\n                \"outputMapping\": [\n                  0,\n                  1,\n                  2\n                ]\n              }\n            },\n            \"input\": {\n              \"read\": {\n                \"baseSchema\": {\n                  \"names\": [\n                    \"t\",\n                    \"k\",\n                    \"v1\",\n                    \"v2\",\n                  ],\n                  \"struct\": {\n                    \"types\": [\n                      {\n                        \"i64\": {\n                          \"nullability\": \"NULLABILITY_REQUIRED\"\n                        }\n                      },\n                      {\n                        \"i64\": {\n                          \"nullability\": \"NULLABILITY_REQUIRED\"\n                        }\n                      },\n                      {\n                        \"i64\": {\n                          \"nullability\": \"NULLABILITY_NULLABLE\"\n                        }\n                      },\n                      {\n                        \"fp64\": {\n                          \"nullability\": \"NULLABILITY_NULLABLE\"\n                        }\n                      }\n                    ],\n                    \"nullability\": \"NULLABILITY_REQUIRED\"\n                  }\n                },\n                \"namedTable\": {\n                  \"names\": [\"t1\"]\n                }\n              }\n            },\n            \"detail\": {\n              \"@type\": \"/arrow.substrait_ext.SegmentedAggregateRel\",\n              \"groupingKeys\": [\n                {\n                  \"directReference\": {\n                    \"structField\": {\n                      \"field\": 1\n                    }\n                  },\n                  \"rootReference\": {}\n                }\n              ],\n              \"segmentKeys\": [\n                {\n                  \"directReference\": {\n                    \"structField\": {}\n                  },\n                  \"rootReference\": {}\n                }\n              ],\n              \"measures\": [\n                {\n                  \"measure\": {\n                    \"functionReference\": 1,\n                    \"phase\": \"AGGREGATION_PHASE_INITIAL_TO_RESULT\",\n                    \"outputType\": {\n                      \"fp64\": {\n                        \"nullability\": \"NULLABILITY_NULLABLE\"\n                      }\n                    },\n                    \"arguments\": [\n                      {\n                        \"value\": {\n                          \"selection\": {\n                            \"directReference\": {\n                              \"structField\": {\n                                \"field\": 2\n                              }\n                            },\n                            \"rootReference\": {}\n                          }\n                        }\n                      },\n                      {\n                        \"value\": {\n                          \"selection\": {\n                            \"directReference\": {\n                              \"structField\": {\n                                \"field\": 3\n                              }\n                            },\n                            \"rootReference\": {}\n                          }\n                        }\n                      }\n                    ]\n                  }\n                }\n              ]\n            }\n          }\n        },\n        \"names\": [\n          \"t\",\n          \"k\",\n          \"v_avg\"\n        ]\n      }\n    }\n  ],\n}\n\"\"\"\n    buf = pa._substrait._parse_json_plan(substrait_query)\n    reader = pa.substrait.run_query(\n        buf, table_provider=table_provider, use_threads=False)\n    res_tb = reader.read_all()\n\n    expected_tb = pa.Table.from_pydict({\n        't': [1, 1, 2, 2],\n        'k': [1, 0, 0, 1],\n        'v_avg': [3.5, 3.5, 9.0, 11.0]\n    })\n\n    # Ordering of k is deterministic because this is running with serial execution\n    assert res_tb == expected_tb\n\n\n@pytest.mark.parametrize(\"expr\", [\n    pc.equal(pc.field(\"x\"), 7),\n    pc.equal(pc.field(\"x\"), pc.field(\"y\")),\n    pc.field(\"x\") > 50\n])\ndef test_serializing_expressions(expr):\n    schema = pa.schema([\n        pa.field(\"x\", pa.int32()),\n        pa.field(\"y\", pa.int32())\n    ])\n\n    buf = pa.substrait.serialize_expressions([expr], [\"test_expr\"], schema)\n    returned = pa.substrait.deserialize_expressions(buf)\n    assert schema == returned.schema\n    assert len(returned.expressions) == 1\n    assert \"test_expr\" in returned.expressions\n\n\ndef test_arrow_specific_types():\n    fields = {\n        \"time_seconds\": (pa.time32(\"s\"), 0),\n        \"time_millis\": (pa.time32(\"ms\"), 0),\n        \"time_nanos\": (pa.time64(\"ns\"), 0),\n        \"date_millis\": (pa.date64(), 0),\n        \"large_string\": (pa.large_string(), \"test_string\"),\n        \"large_binary\": (pa.large_binary(), b\"test_string\"),\n    }\n    schema = pa.schema([pa.field(name, typ) for name, (typ, _) in fields.items()])\n\n    def check_round_trip(expr):\n        buf = pa.substrait.serialize_expressions([expr], [\"test_expr\"], schema)\n        returned = pa.substrait.deserialize_expressions(buf)\n        assert schema == returned.schema\n\n    for name, (typ, val) in fields.items():\n        check_round_trip(pc.field(name) == pa.scalar(val, type=typ))\n\n\ndef test_arrow_one_way_types():\n    schema = pa.schema(\n        [\n            pa.field(\"binary_view\", pa.binary_view()),\n            pa.field(\"string_view\", pa.string_view()),\n            pa.field(\"dictionary\", pa.dictionary(pa.int32(), pa.string())),\n            pa.field(\"ree\", pa.run_end_encoded(pa.int32(), pa.string())),\n        ]\n    )\n    alt_schema = pa.schema(\n        [\n            pa.field(\"binary_view\", pa.binary()),\n            pa.field(\"string_view\", pa.string()),\n            pa.field(\"dictionary\", pa.string()),\n            pa.field(\"ree\", pa.string())\n        ]\n    )\n\n    def check_one_way(field):\n        expr = pc.is_null(pc.field(field.name))\n        buf = pa.substrait.serialize_expressions([expr], [\"test_expr\"], schema)\n        returned = pa.substrait.deserialize_expressions(buf)\n        assert alt_schema == returned.schema\n\n    for field in schema:\n        check_one_way(field)\n\n\ndef test_invalid_expression_ser_des():\n    schema = pa.schema([\n        pa.field(\"x\", pa.int32()),\n        pa.field(\"y\", pa.int32())\n    ])\n    expr = pc.equal(pc.field(\"x\"), 7)\n    bad_expr = pc.equal(pc.field(\"z\"), 7)\n    # Invalid number of names\n    with pytest.raises(ValueError) as excinfo:\n        pa.substrait.serialize_expressions([expr], [], schema)\n    assert 'need to have the same length' in str(excinfo.value)\n    with pytest.raises(ValueError) as excinfo:\n        pa.substrait.serialize_expressions([expr], [\"foo\", \"bar\"], schema)\n    assert 'need to have the same length' in str(excinfo.value)\n    # Expression doesn't match schema\n    with pytest.raises(ValueError) as excinfo:\n        pa.substrait.serialize_expressions([bad_expr], [\"expr\"], schema)\n    assert 'No match for FieldRef' in str(excinfo.value)\n\n\ndef test_serializing_multiple_expressions():\n    schema = pa.schema([\n        pa.field(\"x\", pa.int32()),\n        pa.field(\"y\", pa.int32())\n    ])\n    exprs = [pc.equal(pc.field(\"x\"), 7), pc.equal(pc.field(\"x\"), pc.field(\"y\"))]\n    buf = pa.substrait.serialize_expressions(exprs, [\"first\", \"second\"], schema)\n    returned = pa.substrait.deserialize_expressions(buf)\n    assert schema == returned.schema\n    assert len(returned.expressions) == 2\n\n    norm_exprs = [pc.equal(pc.field(0), 7), pc.equal(pc.field(0), pc.field(1))]\n    assert str(returned.expressions[\"first\"]) == str(norm_exprs[0])\n    assert str(returned.expressions[\"second\"]) == str(norm_exprs[1])\n\n\ndef test_serializing_with_compute():\n    schema = pa.schema([\n        pa.field(\"x\", pa.int32()),\n        pa.field(\"y\", pa.int32())\n    ])\n    expr = pc.equal(pc.field(\"x\"), 7)\n    expr_norm = pc.equal(pc.field(0), 7)\n    buf = expr.to_substrait(schema)\n    returned = pa.substrait.deserialize_expressions(buf)\n\n    assert schema == returned.schema\n    assert len(returned.expressions) == 1\n\n    assert str(returned.expressions[\"expression\"]) == str(expr_norm)\n\n    # Compute can't deserialize messages with multiple expressions\n    buf = pa.substrait.serialize_expressions([expr, expr], [\"first\", \"second\"], schema)\n    with pytest.raises(ValueError) as excinfo:\n        pc.Expression.from_substrait(buf)\n    assert 'contained multiple expressions' in str(excinfo.value)\n\n    # Deserialization should be possible regardless of the expression name\n    buf = pa.substrait.serialize_expressions([expr], [\"weirdname\"], schema)\n    expr2 = pc.Expression.from_substrait(buf)\n    assert str(expr2) == str(expr_norm)\n\n\ndef test_serializing_udfs():\n    # Note, UDF in this context means a function that is not\n    # recognized by Substrait.  It might still be a builtin pyarrow\n    # function.\n    schema = pa.schema([\n        pa.field(\"x\", pa.uint32())\n    ])\n    a = pc.scalar(10)\n    b = pc.scalar(4)\n    exprs = [pc.shift_left(a, b)]\n\n    with pytest.raises(ArrowNotImplementedError):\n        pa.substrait.serialize_expressions(exprs, [\"expr\"], schema)\n\n    buf = pa.substrait.serialize_expressions(\n        exprs, [\"expr\"], schema, allow_arrow_extensions=True)\n    returned = pa.substrait.deserialize_expressions(buf)\n    assert schema == returned.schema\n    assert len(returned.expressions) == 1\n    assert str(returned.expressions[\"expr\"]) == str(exprs[0])\n", "python/pyarrow/tests/test_convert_builtin.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport collections\nimport datetime\nimport decimal\nimport itertools\nimport math\nimport re\nimport sys\n\nimport hypothesis as h\nimport numpy as np\nimport pytest\n\nfrom pyarrow.pandas_compat import _pandas_api  # noqa\nimport pyarrow as pa\nfrom pyarrow.tests import util\nimport pyarrow.tests.strategies as past\n\n\nint_type_pairs = [\n    (np.int8, pa.int8()),\n    (np.int16, pa.int16()),\n    (np.int32, pa.int32()),\n    (np.int64, pa.int64()),\n    (np.uint8, pa.uint8()),\n    (np.uint16, pa.uint16()),\n    (np.uint32, pa.uint32()),\n    (np.uint64, pa.uint64())]\n\n\nnp_int_types, pa_int_types = zip(*int_type_pairs)\n\n\nclass StrangeIterable:\n    def __init__(self, lst):\n        self.lst = lst\n\n    def __iter__(self):\n        return self.lst.__iter__()\n\n\nclass MyInt:\n    def __init__(self, value):\n        self.value = value\n\n    def __int__(self):\n        return self.value\n\n\nclass MyBrokenInt:\n    def __int__(self):\n        1/0  # MARKER\n\n\ndef check_struct_type(ty, expected):\n    \"\"\"\n    Check a struct type is as expected, but not taking order into account.\n    \"\"\"\n    assert pa.types.is_struct(ty)\n    assert set(ty) == set(expected)\n\n\ndef test_iterable_types():\n    arr1 = pa.array(StrangeIterable([0, 1, 2, 3]))\n    arr2 = pa.array((0, 1, 2, 3))\n\n    assert arr1.equals(arr2)\n\n\ndef test_empty_iterable():\n    arr = pa.array(StrangeIterable([]))\n    assert len(arr) == 0\n    assert arr.null_count == 0\n    assert arr.type == pa.null()\n    assert arr.to_pylist() == []\n\n\ndef test_limited_iterator_types():\n    arr1 = pa.array(iter(range(3)), type=pa.int64(), size=3)\n    arr2 = pa.array((0, 1, 2))\n    assert arr1.equals(arr2)\n\n\ndef test_limited_iterator_size_overflow():\n    arr1 = pa.array(iter(range(3)), type=pa.int64(), size=2)\n    arr2 = pa.array((0, 1))\n    assert arr1.equals(arr2)\n\n\ndef test_limited_iterator_size_underflow():\n    arr1 = pa.array(iter(range(3)), type=pa.int64(), size=10)\n    arr2 = pa.array((0, 1, 2))\n    assert arr1.equals(arr2)\n\n\ndef test_iterator_without_size():\n    expected = pa.array((0, 1, 2))\n    arr1 = pa.array(iter(range(3)))\n    assert arr1.equals(expected)\n    # Same with explicit type\n    arr1 = pa.array(iter(range(3)), type=pa.int64())\n    assert arr1.equals(expected)\n\n\ndef test_infinite_iterator():\n    expected = pa.array((0, 1, 2))\n    arr1 = pa.array(itertools.count(0), size=3)\n    assert arr1.equals(expected)\n    # Same with explicit type\n    arr1 = pa.array(itertools.count(0), type=pa.int64(), size=3)\n    assert arr1.equals(expected)\n\n\ndef test_failing_iterator():\n    with pytest.raises(ZeroDivisionError):\n        pa.array((1 // 0 for x in range(10)))\n    # ARROW-17253\n    with pytest.raises(ZeroDivisionError):\n        pa.array((1 // 0 for x in range(10)), size=10)\n\n\nclass ObjectWithOnlyGetitem:\n    def __getitem__(self, key):\n        return 3\n\n\ndef test_object_with_getitem():\n    # https://github.com/apache/arrow/issues/34944\n    # considered as sequence because of __getitem__, but has no length\n    with pytest.raises(TypeError, match=\"has no len()\"):\n        pa.array(ObjectWithOnlyGetitem())\n\n\ndef _as_list(xs):\n    return xs\n\n\ndef _as_tuple(xs):\n    return tuple(xs)\n\n\ndef _as_deque(xs):\n    # deque is a sequence while neither tuple nor list\n    return collections.deque(xs)\n\n\ndef _as_dict_values(xs):\n    # a dict values object is not a sequence, just a regular iterable\n    dct = {k: v for k, v in enumerate(xs)}\n    return dct.values()\n\n\ndef _as_numpy_array(xs):\n    arr = np.empty(len(xs), dtype=object)\n    arr[:] = xs\n    return arr\n\n\ndef _as_set(xs):\n    return set(xs)\n\n\nSEQUENCE_TYPES = [_as_list, _as_tuple, _as_numpy_array]\nITERABLE_TYPES = [_as_set, _as_dict_values] + SEQUENCE_TYPES\nCOLLECTIONS_TYPES = [_as_deque] + ITERABLE_TYPES\n\nparametrize_with_iterable_types = pytest.mark.parametrize(\n    \"seq\", ITERABLE_TYPES\n)\n\nparametrize_with_sequence_types = pytest.mark.parametrize(\n    \"seq\", SEQUENCE_TYPES\n)\n\nparametrize_with_collections_types = pytest.mark.parametrize(\n    \"seq\", COLLECTIONS_TYPES\n)\n\n\n@parametrize_with_collections_types\ndef test_sequence_types(seq):\n    arr1 = pa.array(seq([1, 2, 3]))\n    arr2 = pa.array([1, 2, 3])\n\n    assert arr1.equals(arr2)\n\n\n@parametrize_with_iterable_types\ndef test_nested_sequence_types(seq):\n    arr1 = pa.array([seq([1, 2, 3])])\n    arr2 = pa.array([[1, 2, 3]])\n\n    assert arr1.equals(arr2)\n\n\n@parametrize_with_sequence_types\ndef test_sequence_boolean(seq):\n    expected = [True, None, False, None]\n    arr = pa.array(seq(expected))\n    assert len(arr) == 4\n    assert arr.null_count == 2\n    assert arr.type == pa.bool_()\n    assert arr.to_pylist() == expected\n\n\n@parametrize_with_sequence_types\ndef test_sequence_numpy_boolean(seq):\n    expected = [np.bool_(True), None, np.bool_(False), None]\n    arr = pa.array(seq(expected))\n    assert arr.type == pa.bool_()\n    assert arr.to_pylist() == [True, None, False, None]\n\n\n@parametrize_with_sequence_types\ndef test_sequence_mixed_numpy_python_bools(seq):\n    values = np.array([True, False])\n    arr = pa.array(seq([values[0], None, values[1], True, False]))\n    assert arr.type == pa.bool_()\n    assert arr.to_pylist() == [True, None, False, True, False]\n\n\n@parametrize_with_collections_types\ndef test_empty_list(seq):\n    arr = pa.array(seq([]))\n    assert len(arr) == 0\n    assert arr.null_count == 0\n    assert arr.type == pa.null()\n    assert arr.to_pylist() == []\n\n\n@parametrize_with_sequence_types\ndef test_nested_lists(seq):\n    data = [[], [1, 2], None]\n    arr = pa.array(seq(data))\n    assert len(arr) == 3\n    assert arr.null_count == 1\n    assert arr.type == pa.list_(pa.int64())\n    assert arr.to_pylist() == data\n\n\n@parametrize_with_sequence_types\n@pytest.mark.parametrize(\"factory\", [\n    pa.list_, pa.large_list, pa.list_view, pa.large_list_view])\ndef test_nested_lists_with_explicit_type(seq, factory):\n    data = [[], [1, 2], None]\n    arr = pa.array(seq(data), type=factory(pa.int16()))\n    assert len(arr) == 3\n    assert arr.null_count == 1\n    assert arr.type == factory(pa.int16())\n    assert arr.to_pylist() == data\n\n\n@parametrize_with_collections_types\ndef test_list_with_non_list(seq):\n    # List types don't accept non-sequences\n    with pytest.raises(TypeError):\n        pa.array(seq([[], [1, 2], 3]), type=pa.list_(pa.int64()))\n    with pytest.raises(TypeError):\n        pa.array(seq([[], [1, 2], 3]), type=pa.large_list(pa.int64()))\n    with pytest.raises(TypeError):\n        pa.array(seq([[], [1, 2], 3]), type=pa.list_view(pa.int64()))\n    with pytest.raises(TypeError):\n        pa.array(seq([[], [1, 2], 3]), type=pa.large_list_view(pa.int64()))\n\n\n@parametrize_with_sequence_types\n@pytest.mark.parametrize(\"factory\", [\n    pa.list_, pa.large_list, pa.list_view, pa.large_list_view])\ndef test_nested_arrays(seq, factory):\n    arr = pa.array(seq([np.array([], dtype=np.int64),\n                        np.array([1, 2], dtype=np.int64), None]),\n                   type=factory(pa.int64()))\n    assert len(arr) == 3\n    assert arr.null_count == 1\n    assert arr.type == factory(pa.int64())\n    assert arr.to_pylist() == [[], [1, 2], None]\n\n\n@parametrize_with_sequence_types\ndef test_nested_fixed_size_list(seq):\n    # sequence of lists\n    data = [[1, 2], [3, None], None]\n    arr = pa.array(seq(data), type=pa.list_(pa.int64(), 2))\n    assert len(arr) == 3\n    assert arr.null_count == 1\n    assert arr.type == pa.list_(pa.int64(), 2)\n    assert arr.to_pylist() == data\n\n    # sequence of numpy arrays\n    data = [np.array([1, 2], dtype='int64'), np.array([3, 4], dtype='int64'),\n            None]\n    arr = pa.array(seq(data), type=pa.list_(pa.int64(), 2))\n    assert len(arr) == 3\n    assert arr.null_count == 1\n    assert arr.type == pa.list_(pa.int64(), 2)\n    assert arr.to_pylist() == [[1, 2], [3, 4], None]\n\n    # incorrect length of the lists or arrays\n    data = [[1, 2, 4], [3, None], None]\n    for data in [[[1, 2, 3]], [np.array([1, 2, 4], dtype='int64')]]:\n        with pytest.raises(\n                ValueError, match=\"Length of item not correct: expected 2\"):\n            pa.array(seq(data), type=pa.list_(pa.int64(), 2))\n\n    # with list size of 0\n    data = [[], [], None]\n    arr = pa.array(seq(data), type=pa.list_(pa.int64(), 0))\n    assert len(arr) == 3\n    assert arr.null_count == 1\n    assert arr.type == pa.list_(pa.int64(), 0)\n    assert arr.to_pylist() == [[], [], None]\n\n\n@parametrize_with_sequence_types\ndef test_sequence_all_none(seq):\n    arr = pa.array(seq([None, None]))\n    assert len(arr) == 2\n    assert arr.null_count == 2\n    assert arr.type == pa.null()\n    assert arr.to_pylist() == [None, None]\n\n\n@parametrize_with_sequence_types\n@pytest.mark.parametrize(\"np_scalar_pa_type\", int_type_pairs)\ndef test_sequence_integer(seq, np_scalar_pa_type):\n    np_scalar, pa_type = np_scalar_pa_type\n    expected = [1, None, 3, None,\n                np.iinfo(np_scalar).min, np.iinfo(np_scalar).max]\n    arr = pa.array(seq(expected), type=pa_type)\n    assert len(arr) == 6\n    assert arr.null_count == 2\n    assert arr.type == pa_type\n    assert arr.to_pylist() == expected\n\n\n@parametrize_with_collections_types\n@pytest.mark.parametrize(\"np_scalar_pa_type\", int_type_pairs)\ndef test_sequence_integer_np_nan(seq, np_scalar_pa_type):\n    # ARROW-2806: numpy.nan is a double value and thus should produce\n    # a double array.\n    _, pa_type = np_scalar_pa_type\n    with pytest.raises(ValueError):\n        pa.array(seq([np.nan]), type=pa_type, from_pandas=False)\n\n    arr = pa.array(seq([np.nan]), type=pa_type, from_pandas=True)\n    expected = [None]\n    assert len(arr) == 1\n    assert arr.null_count == 1\n    assert arr.type == pa_type\n    assert arr.to_pylist() == expected\n\n\n@parametrize_with_sequence_types\n@pytest.mark.parametrize(\"np_scalar_pa_type\", int_type_pairs)\ndef test_sequence_integer_nested_np_nan(seq, np_scalar_pa_type):\n    # ARROW-2806: numpy.nan is a double value and thus should produce\n    # a double array.\n    _, pa_type = np_scalar_pa_type\n    with pytest.raises(ValueError):\n        pa.array(seq([[np.nan]]), type=pa.list_(pa_type), from_pandas=False)\n\n    arr = pa.array(seq([[np.nan]]), type=pa.list_(pa_type), from_pandas=True)\n    expected = [[None]]\n    assert len(arr) == 1\n    assert arr.null_count == 0\n    assert arr.type == pa.list_(pa_type)\n    assert arr.to_pylist() == expected\n\n\n@parametrize_with_sequence_types\ndef test_sequence_integer_inferred(seq):\n    expected = [1, None, 3, None]\n    arr = pa.array(seq(expected))\n    assert len(arr) == 4\n    assert arr.null_count == 2\n    assert arr.type == pa.int64()\n    assert arr.to_pylist() == expected\n\n\n@parametrize_with_sequence_types\n@pytest.mark.parametrize(\"np_scalar_pa_type\", int_type_pairs)\ndef test_sequence_numpy_integer(seq, np_scalar_pa_type):\n    np_scalar, pa_type = np_scalar_pa_type\n    expected = [np_scalar(1), None, np_scalar(3), None,\n                np_scalar(np.iinfo(np_scalar).min),\n                np_scalar(np.iinfo(np_scalar).max)]\n    arr = pa.array(seq(expected), type=pa_type)\n    assert len(arr) == 6\n    assert arr.null_count == 2\n    assert arr.type == pa_type\n    assert arr.to_pylist() == expected\n\n\n@parametrize_with_sequence_types\n@pytest.mark.parametrize(\"np_scalar_pa_type\", int_type_pairs)\ndef test_sequence_numpy_integer_inferred(seq, np_scalar_pa_type):\n    np_scalar, pa_type = np_scalar_pa_type\n    expected = [np_scalar(1), None, np_scalar(3), None]\n    expected += [np_scalar(np.iinfo(np_scalar).min),\n                 np_scalar(np.iinfo(np_scalar).max)]\n    arr = pa.array(seq(expected))\n    assert len(arr) == 6\n    assert arr.null_count == 2\n    assert arr.type == pa_type\n    assert arr.to_pylist() == expected\n\n\n@parametrize_with_sequence_types\ndef test_sequence_custom_integers(seq):\n    expected = [0, 42, 2**33 + 1, -2**63]\n    data = list(map(MyInt, expected))\n    arr = pa.array(seq(data), type=pa.int64())\n    assert arr.to_pylist() == expected\n\n\n@parametrize_with_collections_types\ndef test_broken_integers(seq):\n    data = [MyBrokenInt()]\n    with pytest.raises(pa.ArrowInvalid, match=\"tried to convert to int\"):\n        pa.array(seq(data), type=pa.int64())\n\n\ndef test_numpy_scalars_mixed_type():\n    # ARROW-4324\n    data = [np.int32(10), np.float32(0.5)]\n    arr = pa.array(data)\n    expected = pa.array([10, 0.5], type=\"float64\")\n    assert arr.equals(expected)\n\n    # ARROW-9490\n    data = [np.int8(10), np.float32(0.5)]\n    arr = pa.array(data)\n    expected = pa.array([10, 0.5], type=\"float32\")\n    assert arr.equals(expected)\n\n\n@pytest.mark.xfail(reason=\"Type inference for uint64 not implemented\",\n                   raises=OverflowError)\ndef test_uint64_max_convert():\n    data = [0, np.iinfo(np.uint64).max]\n\n    arr = pa.array(data, type=pa.uint64())\n    expected = pa.array(np.array(data, dtype='uint64'))\n    assert arr.equals(expected)\n\n    arr_inferred = pa.array(data)\n    assert arr_inferred.equals(expected)\n\n\n@pytest.mark.parametrize(\"bits\", [8, 16, 32, 64])\ndef test_signed_integer_overflow(bits):\n    ty = getattr(pa, \"int%d\" % bits)()\n    # XXX ideally would always raise OverflowError\n    with pytest.raises((OverflowError, pa.ArrowInvalid)):\n        pa.array([2 ** (bits - 1)], ty)\n    with pytest.raises((OverflowError, pa.ArrowInvalid)):\n        pa.array([-2 ** (bits - 1) - 1], ty)\n\n\n@pytest.mark.parametrize(\"bits\", [8, 16, 32, 64])\ndef test_unsigned_integer_overflow(bits):\n    ty = getattr(pa, \"uint%d\" % bits)()\n    # XXX ideally would always raise OverflowError\n    with pytest.raises((OverflowError, pa.ArrowInvalid)):\n        pa.array([2 ** bits], ty)\n    with pytest.raises((OverflowError, pa.ArrowInvalid)):\n        pa.array([-1], ty)\n\n\n@parametrize_with_collections_types\n@pytest.mark.parametrize(\"typ\", pa_int_types)\ndef test_integer_from_string_error(seq, typ):\n    # ARROW-9451: pa.array(['1'], type=pa.uint32()) should not succeed\n    with pytest.raises(pa.ArrowInvalid):\n        pa.array(seq(['1']), type=typ)\n\n\ndef test_convert_with_mask():\n    data = [1, 2, 3, 4, 5]\n    mask = np.array([False, True, False, False, True])\n\n    result = pa.array(data, mask=mask)\n    expected = pa.array([1, None, 3, 4, None])\n\n    assert result.equals(expected)\n\n    # Mask wrong length\n    with pytest.raises(ValueError):\n        pa.array(data, mask=mask[1:])\n\n\ndef test_garbage_collection():\n    import gc\n\n    # Force the cyclic garbage collector to run\n    gc.collect()\n\n    bytes_before = pa.total_allocated_bytes()\n    pa.array([1, None, 3, None])\n    gc.collect()\n    assert pa.total_allocated_bytes() == bytes_before\n\n\ndef test_sequence_double():\n    data = [1.5, 1., None, 2.5, None, None]\n    arr = pa.array(data)\n    assert len(arr) == 6\n    assert arr.null_count == 3\n    assert arr.type == pa.float64()\n    assert arr.to_pylist() == data\n\n\ndef test_double_auto_coerce_from_integer():\n    # Done as part of ARROW-2814\n    data = [1.5, 1., None, 2.5, None, None]\n    arr = pa.array(data)\n\n    data2 = [1.5, 1, None, 2.5, None, None]\n    arr2 = pa.array(data2)\n\n    assert arr.equals(arr2)\n\n    data3 = [1, 1.5, None, 2.5, None, None]\n    arr3 = pa.array(data3)\n\n    data4 = [1., 1.5, None, 2.5, None, None]\n    arr4 = pa.array(data4)\n\n    assert arr3.equals(arr4)\n\n\ndef test_double_integer_coerce_representable_range():\n    valid_values = [1.5, 1, 2, None, 1 << 53, -(1 << 53)]\n    invalid_values = [1.5, 1, 2, None, (1 << 53) + 1]\n    invalid_values2 = [1.5, 1, 2, None, -((1 << 53) + 1)]\n\n    # it works\n    pa.array(valid_values)\n\n    # it fails\n    with pytest.raises(ValueError):\n        pa.array(invalid_values)\n\n    with pytest.raises(ValueError):\n        pa.array(invalid_values2)\n\n\ndef test_float32_integer_coerce_representable_range():\n    f32 = np.float32\n    valid_values = [f32(1.5), 1 << 24, -(1 << 24)]\n    invalid_values = [f32(1.5), (1 << 24) + 1]\n    invalid_values2 = [f32(1.5), -((1 << 24) + 1)]\n\n    # it works\n    pa.array(valid_values, type=pa.float32())\n\n    # it fails\n    with pytest.raises(ValueError):\n        pa.array(invalid_values, type=pa.float32())\n\n    with pytest.raises(ValueError):\n        pa.array(invalid_values2, type=pa.float32())\n\n\ndef test_mixed_sequence_errors():\n    with pytest.raises(ValueError, match=\"tried to convert to boolean\"):\n        pa.array([True, 'foo'], type=pa.bool_())\n\n    with pytest.raises(ValueError, match=\"tried to convert to float32\"):\n        pa.array([1.5, 'foo'], type=pa.float32())\n\n    with pytest.raises(ValueError, match=\"tried to convert to double\"):\n        pa.array([1.5, 'foo'])\n\n\n@parametrize_with_sequence_types\n@pytest.mark.parametrize(\"np_scalar,pa_type\", [\n    (np.float16, pa.float16()),\n    (np.float32, pa.float32()),\n    (np.float64, pa.float64())\n])\n@pytest.mark.parametrize(\"from_pandas\", [True, False])\ndef test_sequence_numpy_double(seq, np_scalar, pa_type, from_pandas):\n    data = [np_scalar(1.5), np_scalar(1), None, np_scalar(2.5), None, np.nan]\n    arr = pa.array(seq(data), from_pandas=from_pandas)\n    assert len(arr) == 6\n    if from_pandas:\n        assert arr.null_count == 3\n    else:\n        assert arr.null_count == 2\n    if from_pandas:\n        # The NaN is skipped in type inference, otherwise it forces a\n        # float64 promotion\n        assert arr.type == pa_type\n    else:\n        assert arr.type == pa.float64()\n\n    assert arr.to_pylist()[:4] == data[:4]\n    if from_pandas:\n        assert arr.to_pylist()[5] is None\n    else:\n        assert np.isnan(arr.to_pylist()[5])\n\n\n@pytest.mark.parametrize(\"from_pandas\", [True, False])\n@pytest.mark.parametrize(\"inner_seq\", [np.array, list])\ndef test_ndarray_nested_numpy_double(from_pandas, inner_seq):\n    # ARROW-2806\n    data = np.array([\n        inner_seq([1., 2.]),\n        inner_seq([1., 2., 3.]),\n        inner_seq([np.nan]),\n        None\n    ], dtype=object)\n    arr = pa.array(data, from_pandas=from_pandas)\n    assert len(arr) == 4\n    assert arr.null_count == 1\n    assert arr.type == pa.list_(pa.float64())\n    if from_pandas:\n        assert arr.to_pylist() == [[1.0, 2.0], [1.0, 2.0, 3.0], [None], None]\n    else:\n        np.testing.assert_equal(arr.to_pylist(),\n                                [[1., 2.], [1., 2., 3.], [np.nan], None])\n\n\ndef test_nested_ndarray_in_object_array():\n    # ARROW-4350\n    arr = np.empty(2, dtype=object)\n    arr[:] = [np.array([1, 2], dtype=np.int64),\n              np.array([2, 3], dtype=np.int64)]\n\n    arr2 = np.empty(2, dtype=object)\n    arr2[0] = [3, 4]\n    arr2[1] = [5, 6]\n\n    expected_type = pa.list_(pa.list_(pa.int64()))\n    assert pa.infer_type([arr]) == expected_type\n\n    result = pa.array([arr, arr2])\n    expected = pa.array([[[1, 2], [2, 3]], [[3, 4], [5, 6]]],\n                        type=expected_type)\n\n    assert result.equals(expected)\n\n    # test case for len-1 arrays to ensure they are interpreted as\n    # sublists and not scalars\n    arr = np.empty(2, dtype=object)\n    arr[:] = [np.array([1]), np.array([2])]\n    result = pa.array([arr, arr])\n    assert result.to_pylist() == [[[1], [2]], [[1], [2]]]\n\n\n@pytest.mark.xfail(reason=(\"Type inference for multidimensional ndarray \"\n                           \"not yet implemented\"),\n                   raises=AssertionError)\ndef test_multidimensional_ndarray_as_nested_list():\n    # TODO(wesm): see ARROW-5645\n    arr = np.array([[1, 2], [2, 3]], dtype=np.int64)\n    arr2 = np.array([[3, 4], [5, 6]], dtype=np.int64)\n\n    expected_type = pa.list_(pa.list_(pa.int64()))\n    assert pa.infer_type([arr]) == expected_type\n\n    result = pa.array([arr, arr2])\n    expected = pa.array([[[1, 2], [2, 3]], [[3, 4], [5, 6]]],\n                        type=expected_type)\n\n    assert result.equals(expected)\n\n\n@pytest.mark.parametrize(('data', 'value_type'), [\n    ([True, False], pa.bool_()),\n    ([None, None], pa.null()),\n    ([1, 2, None], pa.int8()),\n    ([1, 2., 3., None], pa.float32()),\n    ([datetime.date.today(), None], pa.date32()),\n    ([None, datetime.date.today()], pa.date64()),\n    ([datetime.time(1, 1, 1), None], pa.time32('s')),\n    ([None, datetime.time(2, 2, 2)], pa.time64('us')),\n    ([datetime.datetime.now(), None], pa.timestamp('us')),\n    ([datetime.timedelta(seconds=10)], pa.duration('s')),\n    ([b\"a\", b\"b\"], pa.binary()),\n    ([b\"aaa\", b\"bbb\", b\"ccc\"], pa.binary(3)),\n    ([b\"a\", b\"b\", b\"c\"], pa.large_binary()),\n    ([\"a\", \"b\", \"c\"], pa.string()),\n    ([\"a\", \"b\", \"c\"], pa.large_string()),\n    (\n        [{\"a\": 1, \"b\": 2}, None, {\"a\": 5, \"b\": None}],\n        pa.struct([('a', pa.int8()), ('b', pa.int16())])\n    )\n])\ndef test_list_array_from_object_ndarray(data, value_type):\n    ty = pa.list_(value_type)\n    ndarray = np.array(data, dtype=object)\n    arr = pa.array([ndarray], type=ty)\n    assert arr.type.equals(ty)\n    assert arr.to_pylist() == [data]\n\n\n@pytest.mark.parametrize(('data', 'value_type'), [\n    ([[1, 2], [3]], pa.list_(pa.int64())),\n    ([[1, 2], [3, 4]], pa.list_(pa.int64(), 2)),\n    ([[1], [2, 3]], pa.large_list(pa.int64()))\n])\ndef test_nested_list_array_from_object_ndarray(data, value_type):\n    ndarray = np.empty(len(data), dtype=object)\n    ndarray[:] = [np.array(item, dtype=object) for item in data]\n\n    ty = pa.list_(value_type)\n    arr = pa.array([ndarray], type=ty)\n    assert arr.type.equals(ty)\n    assert arr.to_pylist() == [data]\n\n\ndef test_array_ignore_nan_from_pandas():\n    # See ARROW-4324, this reverts logic that was introduced in\n    # ARROW-2240\n    with pytest.raises(ValueError):\n        pa.array([np.nan, 'str'])\n\n    arr = pa.array([np.nan, 'str'], from_pandas=True)\n    expected = pa.array([None, 'str'])\n    assert arr.equals(expected)\n\n\ndef test_nested_ndarray_different_dtypes():\n    data = [\n        np.array([1, 2, 3], dtype='int64'),\n        None,\n        np.array([4, 5, 6], dtype='uint32')\n    ]\n\n    arr = pa.array(data)\n    expected = pa.array([[1, 2, 3], None, [4, 5, 6]],\n                        type=pa.list_(pa.int64()))\n    assert arr.equals(expected)\n\n    t2 = pa.list_(pa.uint32())\n    arr2 = pa.array(data, type=t2)\n    expected2 = expected.cast(t2)\n    assert arr2.equals(expected2)\n\n\ndef test_sequence_unicode():\n    data = ['foo', 'bar', None, 'ma\u00f1ana']\n    arr = pa.array(data)\n    assert len(arr) == 4\n    assert arr.null_count == 1\n    assert arr.type == pa.string()\n    assert arr.to_pylist() == data\n\n\n@pytest.mark.parametrize(\"ty\", [pa.string(), pa.large_string(), pa.string_view()])\ndef test_sequence_unicode_explicit_type(ty):\n    data = ['foo', 'bar', None, 'ma\u00f1ana']\n    arr = pa.array(data, type=ty)\n    assert len(arr) == 4\n    assert arr.null_count == 1\n    assert arr.type == ty\n    assert arr.to_pylist() == data\n\n\ndef check_array_mixed_unicode_bytes(binary_type, string_type):\n    values = ['qux', b'foo', bytearray(b'barz')]\n    b_values = [b'qux', b'foo', b'barz']\n    u_values = ['qux', 'foo', 'barz']\n\n    arr = pa.array(values)\n    expected = pa.array(b_values, type=pa.binary())\n    assert arr.type == pa.binary()\n    assert arr.equals(expected)\n\n    arr = pa.array(values, type=binary_type)\n    expected = pa.array(b_values, type=binary_type)\n    assert arr.type == binary_type\n    assert arr.equals(expected)\n\n    arr = pa.array(values, type=string_type)\n    expected = pa.array(u_values, type=string_type)\n    assert arr.type == string_type\n    assert arr.equals(expected)\n\n\ndef test_array_mixed_unicode_bytes():\n    check_array_mixed_unicode_bytes(pa.binary(), pa.string())\n    check_array_mixed_unicode_bytes(pa.large_binary(), pa.large_string())\n    check_array_mixed_unicode_bytes(pa.binary_view(), pa.string_view())\n\n\n@pytest.mark.large_memory\n@pytest.mark.parametrize(\"ty\", [pa.large_binary(), pa.large_string()])\ndef test_large_binary_array(ty):\n    # Construct a large binary array with more than 4GB of data\n    s = b\"0123456789abcdefghijklmnopqrstuvwxyz\" * 10\n    nrepeats = math.ceil((2**32 + 5) / len(s))\n    data = [s] * nrepeats\n    arr = pa.array(data, type=ty)\n    assert isinstance(arr, pa.Array)\n    assert arr.type == ty\n    assert len(arr) == nrepeats\n\n\n@pytest.mark.slow\n@pytest.mark.large_memory\n@pytest.mark.parametrize(\"ty\", [pa.large_binary(), pa.large_string()])\ndef test_large_binary_value(ty):\n    # Construct a large binary array with a single value larger than 4GB\n    s = b\"0123456789abcdefghijklmnopqrstuvwxyz\"\n    nrepeats = math.ceil((2**32 + 5) / len(s))\n    arr = pa.array([b\"foo\", s * nrepeats, None, b\"bar\"], type=ty)\n    assert isinstance(arr, pa.Array)\n    assert arr.type == ty\n    assert len(arr) == 4\n    buf = arr[1].as_buffer()\n    assert len(buf) == len(s) * nrepeats\n\n\n@pytest.mark.large_memory\n@pytest.mark.parametrize(\"ty\", [pa.binary(), pa.string(), pa.string_view()])\ndef test_string_too_large(ty):\n    # Construct a binary array with a single value larger than 4GB\n    s = b\"0123456789abcdefghijklmnopqrstuvwxyz\"\n    nrepeats = math.ceil((2**32 + 5) / len(s))\n    with pytest.raises(pa.ArrowCapacityError):\n        pa.array([b\"foo\", s * nrepeats, None, b\"bar\"], type=ty)\n\n\ndef test_sequence_bytes():\n    u1 = b'ma\\xc3\\xb1ana'\n\n    data = [b'foo',\n            memoryview(b'dada'),\n            memoryview(b'd-a-t-a')[::2],  # non-contiguous is made contiguous\n            u1.decode('utf-8'),  # unicode gets encoded,\n            bytearray(b'bar'),\n            None]\n    for ty in [None, pa.binary(), pa.large_binary(), pa.binary_view()]:\n        arr = pa.array(data, type=ty)\n        assert len(arr) == 6\n        assert arr.null_count == 1\n        assert arr.type == ty or pa.binary()\n        assert arr.to_pylist() == [b'foo', b'dada', b'data', u1, b'bar', None]\n\n\n@pytest.mark.parametrize(\"ty\", [pa.string(), pa.large_string(), pa.string_view()])\ndef test_sequence_utf8_to_unicode(ty):\n    # ARROW-1225\n    data = [b'foo', None, b'bar']\n    arr = pa.array(data, type=ty)\n    assert arr.type == ty\n    assert arr[0].as_py() == 'foo'\n\n    # test a non-utf8 unicode string\n    val = ('ma\u00f1ana').encode('utf-16-le')\n    with pytest.raises(pa.ArrowInvalid):\n        pa.array([val], type=ty)\n\n\ndef test_sequence_fixed_size_bytes():\n    data = [b'foof', None, bytearray(b'barb'), b'2346']\n    arr = pa.array(data, type=pa.binary(4))\n    assert len(arr) == 4\n    assert arr.null_count == 1\n    assert arr.type == pa.binary(4)\n    assert arr.to_pylist() == [b'foof', None, b'barb', b'2346']\n\n\ndef test_fixed_size_bytes_does_not_accept_varying_lengths():\n    data = [b'foo', None, b'barb', b'2346']\n    with pytest.raises(pa.ArrowInvalid):\n        pa.array(data, type=pa.binary(4))\n\n\ndef test_fixed_size_binary_length_check():\n    # ARROW-10193\n    data = [b'\\x19h\\r\\x9e\\x00\\x00\\x00\\x00\\x01\\x9b\\x9fA']\n    assert len(data[0]) == 12\n    ty = pa.binary(12)\n    arr = pa.array(data, type=ty)\n    assert arr.to_pylist() == data\n\n\ndef test_sequence_date():\n    data = [datetime.date(2000, 1, 1), None, datetime.date(1970, 1, 1),\n            datetime.date(2040, 2, 26)]\n    arr = pa.array(data)\n    assert len(arr) == 4\n    assert arr.type == pa.date32()\n    assert arr.null_count == 1\n    assert arr[0].as_py() == datetime.date(2000, 1, 1)\n    assert arr[1].as_py() is None\n    assert arr[2].as_py() == datetime.date(1970, 1, 1)\n    assert arr[3].as_py() == datetime.date(2040, 2, 26)\n\n\n@pytest.mark.parametrize('input',\n                         [(pa.date32(), [10957, None]),\n                          (pa.date64(), [10957 * 86400000, None])])\ndef test_sequence_explicit_types(input):\n    t, ex_values = input\n    data = [datetime.date(2000, 1, 1), None]\n    arr = pa.array(data, type=t)\n    arr2 = pa.array(ex_values, type=t)\n\n    for x in [arr, arr2]:\n        assert len(x) == 2\n        assert x.type == t\n        assert x.null_count == 1\n        assert x[0].as_py() == datetime.date(2000, 1, 1)\n        assert x[1].as_py() is None\n\n\ndef test_date32_overflow():\n    # Overflow\n    data3 = [2**32, None]\n    with pytest.raises((OverflowError, pa.ArrowException)):\n        pa.array(data3, type=pa.date32())\n\n\n@pytest.mark.parametrize(('time_type', 'unit', 'int_type'), [\n    (pa.time32, 's', 'int32'),\n    (pa.time32, 'ms', 'int32'),\n    (pa.time64, 'us', 'int64'),\n    (pa.time64, 'ns', 'int64'),\n])\ndef test_sequence_time_with_timezone(time_type, unit, int_type):\n    def expected_integer_value(t):\n        # only use with utc time object because it doesn't adjust with the\n        # offset\n        units = ['s', 'ms', 'us', 'ns']\n        multiplier = 10**(units.index(unit) * 3)\n        if t is None:\n            return None\n        seconds = (\n            t.hour * 3600 +\n            t.minute * 60 +\n            t.second +\n            t.microsecond * 10**-6\n        )\n        return int(seconds * multiplier)\n\n    def expected_time_value(t):\n        # only use with utc time object because it doesn't adjust with the\n        # time objects tzdata\n        if unit == 's':\n            return t.replace(microsecond=0)\n        elif unit == 'ms':\n            return t.replace(microsecond=(t.microsecond // 1000) * 1000)\n        else:\n            return t\n\n    # only timezone naive times are supported in arrow\n    data = [\n        datetime.time(8, 23, 34, 123456),\n        datetime.time(5, 0, 0, 1000),\n        None,\n        datetime.time(1, 11, 56, 432539),\n        datetime.time(23, 10, 0, 437699)\n    ]\n\n    ty = time_type(unit)\n    arr = pa.array(data, type=ty)\n    assert len(arr) == 5\n    assert arr.type == ty\n    assert arr.null_count == 1\n\n    # test that the underlying integers are UTC values\n    values = arr.cast(int_type)\n    expected = list(map(expected_integer_value, data))\n    assert values.to_pylist() == expected\n\n    # test that the scalars are datetime.time objects with UTC timezone\n    assert arr[0].as_py() == expected_time_value(data[0])\n    assert arr[1].as_py() == expected_time_value(data[1])\n    assert arr[2].as_py() is None\n    assert arr[3].as_py() == expected_time_value(data[3])\n    assert arr[4].as_py() == expected_time_value(data[4])\n\n    def tz(hours, minutes=0):\n        offset = datetime.timedelta(hours=hours, minutes=minutes)\n        return datetime.timezone(offset)\n\n\ndef test_sequence_timestamp():\n    data = [\n        datetime.datetime(2007, 7, 13, 1, 23, 34, 123456),\n        None,\n        datetime.datetime(2006, 1, 13, 12, 34, 56, 432539),\n        datetime.datetime(2010, 8, 13, 5, 46, 57, 437699)\n    ]\n    arr = pa.array(data)\n    assert len(arr) == 4\n    assert arr.type == pa.timestamp('us')\n    assert arr.null_count == 1\n    assert arr[0].as_py() == datetime.datetime(2007, 7, 13, 1,\n                                               23, 34, 123456)\n    assert arr[1].as_py() is None\n    assert arr[2].as_py() == datetime.datetime(2006, 1, 13, 12,\n                                               34, 56, 432539)\n    assert arr[3].as_py() == datetime.datetime(2010, 8, 13, 5,\n                                               46, 57, 437699)\n\n\n@pytest.mark.parametrize('timezone', [\n    None,\n    'UTC',\n    'Etc/GMT-1',\n    'Europe/Budapest',\n])\n@pytest.mark.parametrize('unit', [\n    's',\n    'ms',\n    'us',\n    'ns'\n])\ndef test_sequence_timestamp_with_timezone(timezone, unit):\n    pytz = pytest.importorskip(\"pytz\")\n\n    def expected_integer_value(dt):\n        units = ['s', 'ms', 'us', 'ns']\n        multiplier = 10**(units.index(unit) * 3)\n        if dt is None:\n            return None\n        else:\n            # avoid float precision issues\n            ts = decimal.Decimal(str(dt.timestamp()))\n            return int(ts * multiplier)\n\n    def expected_datetime_value(dt):\n        if dt is None:\n            return None\n\n        if unit == 's':\n            dt = dt.replace(microsecond=0)\n        elif unit == 'ms':\n            dt = dt.replace(microsecond=(dt.microsecond // 1000) * 1000)\n\n        # adjust the timezone\n        if timezone is None:\n            # make datetime timezone unaware\n            return dt.replace(tzinfo=None)\n        else:\n            # convert to the expected timezone\n            return dt.astimezone(pytz.timezone(timezone))\n\n    data = [\n        datetime.datetime(2007, 7, 13, 8, 23, 34, 123456),  # naive\n        pytz.utc.localize(\n            datetime.datetime(2008, 1, 5, 5, 0, 0, 1000)\n        ),\n        None,\n        pytz.timezone('US/Eastern').localize(\n            datetime.datetime(2006, 1, 13, 12, 34, 56, 432539)\n        ),\n        pytz.timezone('Europe/Moscow').localize(\n            datetime.datetime(2010, 8, 13, 5, 0, 0, 437699)\n        ),\n    ]\n    utcdata = [\n        pytz.utc.localize(data[0]),\n        data[1],\n        None,\n        data[3].astimezone(pytz.utc),\n        data[4].astimezone(pytz.utc),\n    ]\n\n    ty = pa.timestamp(unit, tz=timezone)\n    arr = pa.array(data, type=ty)\n    assert len(arr) == 5\n    assert arr.type == ty\n    assert arr.null_count == 1\n\n    # test that the underlying integers are UTC values\n    values = arr.cast('int64')\n    expected = list(map(expected_integer_value, utcdata))\n    assert values.to_pylist() == expected\n\n    # test that the scalars are datetimes with the correct timezone\n    for i in range(len(arr)):\n        assert arr[i].as_py() == expected_datetime_value(utcdata[i])\n\n\n@pytest.mark.parametrize('timezone', [\n    None,\n    'UTC',\n    'Etc/GMT-1',\n    'Europe/Budapest',\n])\ndef test_pyarrow_ignore_timezone_environment_variable(monkeypatch, timezone):\n    # note that any non-empty value will evaluate to true\n    pytest.importorskip(\"pytz\")\n    import pytz\n\n    monkeypatch.setenv(\"PYARROW_IGNORE_TIMEZONE\", \"1\")\n    data = [\n        datetime.datetime(2007, 7, 13, 8, 23, 34, 123456),  # naive\n        pytz.utc.localize(\n            datetime.datetime(2008, 1, 5, 5, 0, 0, 1000)\n        ),\n        pytz.timezone('US/Eastern').localize(\n            datetime.datetime(2006, 1, 13, 12, 34, 56, 432539)\n        ),\n        pytz.timezone('Europe/Moscow').localize(\n            datetime.datetime(2010, 8, 13, 5, 0, 0, 437699)\n        ),\n    ]\n\n    expected = [dt.replace(tzinfo=None) for dt in data]\n    if timezone is not None:\n        tzinfo = pytz.timezone(timezone)\n        expected = [tzinfo.fromutc(dt) for dt in expected]\n\n    ty = pa.timestamp('us', tz=timezone)\n    arr = pa.array(data, type=ty)\n    assert arr.to_pylist() == expected\n\n\ndef test_sequence_timestamp_with_timezone_inference():\n    pytest.importorskip(\"pytz\")\n    import pytz\n\n    data = [\n        datetime.datetime(2007, 7, 13, 8, 23, 34, 123456),  # naive\n        pytz.utc.localize(\n            datetime.datetime(2008, 1, 5, 5, 0, 0, 1000)\n        ),\n        None,\n        pytz.timezone('US/Eastern').localize(\n            datetime.datetime(2006, 1, 13, 12, 34, 56, 432539)\n        ),\n        pytz.timezone('Europe/Moscow').localize(\n            datetime.datetime(2010, 8, 13, 5, 0, 0, 437699)\n        ),\n    ]\n    expected = [\n        pa.timestamp('us', tz=None),\n        pa.timestamp('us', tz='UTC'),\n        pa.timestamp('us', tz=None),\n        pa.timestamp('us', tz='US/Eastern'),\n        pa.timestamp('us', tz='Europe/Moscow')\n    ]\n    for dt, expected_type in zip(data, expected):\n        prepended = [dt] + data\n        arr = pa.array(prepended)\n        assert arr.type == expected_type\n\n\ndef test_sequence_timestamp_with_zoneinfo_timezone_inference():\n    pytest.importorskip(\"zoneinfo\")\n    import zoneinfo\n\n    data = [\n        datetime.datetime(2007, 7, 13, 8, 23, 34, 123456),  # naive\n        datetime.datetime(2008, 1, 5, 5, 0, 0, 1000,\n                          tzinfo=datetime.timezone.utc),\n        None,\n        datetime.datetime(2006, 1, 13, 12, 34, 56, 432539,\n                          tzinfo=zoneinfo.ZoneInfo(key='US/Eastern')),\n        datetime.datetime(2010, 8, 13, 5, 0, 0, 437699,\n                          tzinfo=zoneinfo.ZoneInfo(key='Europe/Moscow')),\n    ]\n    expected = [\n        pa.timestamp('us', tz=None),\n        pa.timestamp('us', tz='UTC'),\n        pa.timestamp('us', tz=None),\n        pa.timestamp('us', tz='US/Eastern'),\n        pa.timestamp('us', tz='Europe/Moscow')\n    ]\n    for dt, expected_type in zip(data, expected):\n        prepended = [dt] + data\n        arr = pa.array(prepended)\n        assert arr.type == expected_type\n\n\n@pytest.mark.pandas\ndef test_sequence_timestamp_from_mixed_builtin_and_pandas_datetimes():\n    pytest.importorskip(\"pytz\")\n    import pytz\n    import pandas as pd\n\n    data = [\n        pd.Timestamp(1184307814123456123, tz=pytz.timezone('US/Eastern'),\n                     unit='ns'),\n        datetime.datetime(2007, 7, 13, 8, 23, 34, 123456),  # naive\n        pytz.utc.localize(\n            datetime.datetime(2008, 1, 5, 5, 0, 0, 1000)\n        ),\n        None,\n    ]\n    utcdata = [\n        data[0].astimezone(pytz.utc),\n        pytz.utc.localize(data[1]),\n        data[2].astimezone(pytz.utc),\n        None,\n    ]\n\n    arr = pa.array(data)\n    assert arr.type == pa.timestamp('us', tz='US/Eastern')\n\n    values = arr.cast('int64')\n    expected = [int(dt.timestamp() * 10**6) if dt else None for dt in utcdata]\n    assert values.to_pylist() == expected\n\n\ndef test_sequence_timestamp_out_of_bounds_nanosecond():\n    # https://issues.apache.org/jira/browse/ARROW-9768\n    # datetime outside of range supported for nanosecond resolution\n    data = [datetime.datetime(2262, 4, 12)]\n    with pytest.raises(ValueError, match=\"out of bounds\"):\n        pa.array(data, type=pa.timestamp('ns'))\n\n    # with microsecond resolution it works fine\n    arr = pa.array(data, type=pa.timestamp('us'))\n    assert arr.to_pylist() == data\n\n    # case where the naive is within bounds, but converted to UTC not\n    tz = datetime.timezone(datetime.timedelta(hours=-1))\n    data = [datetime.datetime(2262, 4, 11, 23, tzinfo=tz)]\n    with pytest.raises(ValueError, match=\"out of bounds\"):\n        pa.array(data, type=pa.timestamp('ns'))\n\n    arr = pa.array(data, type=pa.timestamp('us'))\n    assert arr.to_pylist()[0] == datetime.datetime(2262, 4, 12)\n\n\ndef test_sequence_numpy_timestamp():\n    data = [\n        np.datetime64(datetime.datetime(2007, 7, 13, 1, 23, 34, 123456)),\n        None,\n        np.datetime64(datetime.datetime(2006, 1, 13, 12, 34, 56, 432539)),\n        np.datetime64(datetime.datetime(2010, 8, 13, 5, 46, 57, 437699))\n    ]\n    arr = pa.array(data)\n    assert len(arr) == 4\n    assert arr.type == pa.timestamp('us')\n    assert arr.null_count == 1\n    assert arr[0].as_py() == datetime.datetime(2007, 7, 13, 1,\n                                               23, 34, 123456)\n    assert arr[1].as_py() is None\n    assert arr[2].as_py() == datetime.datetime(2006, 1, 13, 12,\n                                               34, 56, 432539)\n    assert arr[3].as_py() == datetime.datetime(2010, 8, 13, 5,\n                                               46, 57, 437699)\n\n\nclass MyDate(datetime.date):\n    pass\n\n\nclass MyDatetime(datetime.datetime):\n    pass\n\n\nclass MyTimedelta(datetime.timedelta):\n    pass\n\n\ndef test_datetime_subclassing():\n    data = [\n        MyDate(2007, 7, 13),\n    ]\n    date_type = pa.date32()\n    arr_date = pa.array(data, type=date_type)\n    assert len(arr_date) == 1\n    assert arr_date.type == date_type\n    assert arr_date[0].as_py() == datetime.date(2007, 7, 13)\n\n    data = [\n        MyDatetime(2007, 7, 13, 1, 23, 34, 123456),\n    ]\n\n    s = pa.timestamp('s')\n    ms = pa.timestamp('ms')\n    us = pa.timestamp('us')\n\n    arr_s = pa.array(data, type=s)\n    assert len(arr_s) == 1\n    assert arr_s.type == s\n    assert arr_s[0].as_py() == datetime.datetime(2007, 7, 13, 1,\n                                                 23, 34, 0)\n\n    arr_ms = pa.array(data, type=ms)\n    assert len(arr_ms) == 1\n    assert arr_ms.type == ms\n    assert arr_ms[0].as_py() == datetime.datetime(2007, 7, 13, 1,\n                                                  23, 34, 123000)\n\n    arr_us = pa.array(data, type=us)\n    assert len(arr_us) == 1\n    assert arr_us.type == us\n    assert arr_us[0].as_py() == datetime.datetime(2007, 7, 13, 1,\n                                                  23, 34, 123456)\n\n    data = [\n        MyTimedelta(123, 456, 1002),\n    ]\n\n    s = pa.duration('s')\n    ms = pa.duration('ms')\n    us = pa.duration('us')\n\n    arr_s = pa.array(data)\n    assert len(arr_s) == 1\n    assert arr_s.type == us\n    assert arr_s[0].as_py() == datetime.timedelta(123, 456, 1002)\n\n    arr_s = pa.array(data, type=s)\n    assert len(arr_s) == 1\n    assert arr_s.type == s\n    assert arr_s[0].as_py() == datetime.timedelta(123, 456)\n\n    arr_ms = pa.array(data, type=ms)\n    assert len(arr_ms) == 1\n    assert arr_ms.type == ms\n    assert arr_ms[0].as_py() == datetime.timedelta(123, 456, 1000)\n\n    arr_us = pa.array(data, type=us)\n    assert len(arr_us) == 1\n    assert arr_us.type == us\n    assert arr_us[0].as_py() == datetime.timedelta(123, 456, 1002)\n\n\n@pytest.mark.xfail(not _pandas_api.have_pandas,\n                   reason=\"pandas required for nanosecond conversion\")\ndef test_sequence_timestamp_nanoseconds():\n    inputs = [\n        [datetime.datetime(2007, 7, 13, 1, 23, 34, 123456)],\n        [MyDatetime(2007, 7, 13, 1, 23, 34, 123456)]\n    ]\n\n    for data in inputs:\n        ns = pa.timestamp('ns')\n        arr_ns = pa.array(data, type=ns)\n        assert len(arr_ns) == 1\n        assert arr_ns.type == ns\n        assert arr_ns[0].as_py() == datetime.datetime(2007, 7, 13, 1,\n                                                      23, 34, 123456)\n\n\n@pytest.mark.pandas\n@pytest.mark.skipif(sys.platform == \"win32\" and not util.windows_has_tzdata(),\n                    reason=\"Timezone database is not installed on Windows\")\ndef test_sequence_timestamp_from_int_with_unit():\n    # TODO(wesm): This test might be rewritten to assert the actual behavior\n    # when pandas is not installed\n\n    data = [1]\n\n    s = pa.timestamp('s')\n    ms = pa.timestamp('ms')\n    us = pa.timestamp('us')\n    ns = pa.timestamp('ns')\n\n    arr_s = pa.array(data, type=s)\n    assert len(arr_s) == 1\n    assert arr_s.type == s\n    assert repr(arr_s[0]) == (\n        \"<pyarrow.TimestampScalar: '1970-01-01T00:00:01'>\"\n    )\n    assert str(arr_s[0]) == \"1970-01-01 00:00:01\"\n\n    arr_ms = pa.array(data, type=ms)\n    assert len(arr_ms) == 1\n    assert arr_ms.type == ms\n    assert repr(arr_ms[0].as_py()) == (\n        \"datetime.datetime(1970, 1, 1, 0, 0, 0, 1000)\"\n    )\n    assert str(arr_ms[0]) == \"1970-01-01 00:00:00.001000\"\n\n    arr_us = pa.array(data, type=us)\n    assert len(arr_us) == 1\n    assert arr_us.type == us\n    assert repr(arr_us[0].as_py()) == (\n        \"datetime.datetime(1970, 1, 1, 0, 0, 0, 1)\"\n    )\n    assert str(arr_us[0]) == \"1970-01-01 00:00:00.000001\"\n\n    arr_ns = pa.array(data, type=ns)\n    assert len(arr_ns) == 1\n    assert arr_ns.type == ns\n    assert repr(arr_ns[0].as_py()) == (\n        \"Timestamp('1970-01-01 00:00:00.000000001')\"\n    )\n    assert str(arr_ns[0]) == \"1970-01-01 00:00:00.000000001\"\n\n    expected_exc = TypeError\n\n    class CustomClass():\n        pass\n\n    for ty in [ns, pa.date32(), pa.date64()]:\n        with pytest.raises(expected_exc):\n            pa.array([1, CustomClass()], type=ty)\n\n\n@pytest.mark.parametrize('np_scalar', [True, False])\ndef test_sequence_duration(np_scalar):\n    td1 = datetime.timedelta(2, 3601, 1)\n    td2 = datetime.timedelta(1, 100, 1000)\n    if np_scalar:\n        data = [np.timedelta64(td1), None, np.timedelta64(td2)]\n    else:\n        data = [td1, None, td2]\n\n    arr = pa.array(data)\n    assert len(arr) == 3\n    assert arr.type == pa.duration('us')\n    assert arr.null_count == 1\n    assert arr[0].as_py() == td1\n    assert arr[1].as_py() is None\n    assert arr[2].as_py() == td2\n\n\n@pytest.mark.parametrize('unit', ['s', 'ms', 'us', 'ns'])\ndef test_sequence_duration_with_unit(unit):\n    data = [\n        datetime.timedelta(3, 22, 1001),\n    ]\n    expected = {'s': datetime.timedelta(3, 22),\n                'ms': datetime.timedelta(3, 22, 1000),\n                'us': datetime.timedelta(3, 22, 1001),\n                'ns': datetime.timedelta(3, 22, 1001)}\n\n    ty = pa.duration(unit)\n\n    arr_s = pa.array(data, type=ty)\n    assert len(arr_s) == 1\n    assert arr_s.type == ty\n    assert arr_s[0].as_py() == expected[unit]\n\n\n@pytest.mark.parametrize('unit', ['s', 'ms', 'us', 'ns'])\ndef test_sequence_duration_from_int_with_unit(unit):\n    data = [5]\n\n    ty = pa.duration(unit)\n    arr = pa.array(data, type=ty)\n    assert len(arr) == 1\n    assert arr.type == ty\n    assert arr[0].value == 5\n\n\ndef test_sequence_duration_nested_lists():\n    td1 = datetime.timedelta(1, 1, 1000)\n    td2 = datetime.timedelta(1, 100)\n\n    data = [[td1, None], [td1, td2]]\n\n    arr = pa.array(data)\n    assert len(arr) == 2\n    assert arr.type == pa.list_(pa.duration('us'))\n    assert arr.to_pylist() == data\n\n\n@pytest.mark.parametrize(\"factory\", [\n    pa.list_, pa.large_list, pa.list_view, pa.large_list_view])\ndef test_sequence_duration_nested_lists_with_explicit_type(factory):\n    td1 = datetime.timedelta(1, 1, 1000)\n    td2 = datetime.timedelta(1, 100)\n\n    data = [[td1, None], [td1, td2]]\n\n    arr = pa.array(data, type=factory(pa.duration('ms')))\n    assert len(arr) == 2\n    assert arr.type == factory(pa.duration('ms'))\n    assert arr.to_pylist() == data\n\n\ndef test_sequence_duration_nested_lists_numpy():\n    td1 = datetime.timedelta(1, 1, 1000)\n    td2 = datetime.timedelta(1, 100)\n\n    data = [[np.timedelta64(td1), None],\n            [np.timedelta64(td1), np.timedelta64(td2)]]\n\n    arr = pa.array(data)\n    assert len(arr) == 2\n    assert arr.type == pa.list_(pa.duration('us'))\n    assert arr.to_pylist() == [[td1, None], [td1, td2]]\n\n    data = [np.array([np.timedelta64(td1), None], dtype='timedelta64[us]'),\n            np.array([np.timedelta64(td1), np.timedelta64(td2)])]\n\n    arr = pa.array(data)\n    assert len(arr) == 2\n    assert arr.type == pa.list_(pa.duration('us'))\n    assert arr.to_pylist() == [[td1, None], [td1, td2]]\n\n\ndef test_sequence_nesting_levels():\n    data = [1, 2, None]\n    arr = pa.array(data)\n    assert arr.type == pa.int64()\n    assert arr.to_pylist() == data\n\n    data = [[1], [2], None]\n    arr = pa.array(data)\n    assert arr.type == pa.list_(pa.int64())\n    assert arr.to_pylist() == data\n\n    data = [[1], [2, 3, 4], [None]]\n    arr = pa.array(data)\n    assert arr.type == pa.list_(pa.int64())\n    assert arr.to_pylist() == data\n\n    data = [None, [[None, 1]], [[2, 3, 4], None], [None]]\n    arr = pa.array(data)\n    assert arr.type == pa.list_(pa.list_(pa.int64()))\n    assert arr.to_pylist() == data\n\n    exceptions = (pa.ArrowInvalid, pa.ArrowTypeError)\n\n    # Mixed nesting levels are rejected\n    with pytest.raises(exceptions):\n        pa.array([1, 2, [1]])\n\n    with pytest.raises(exceptions):\n        pa.array([1, 2, []])\n\n    with pytest.raises(exceptions):\n        pa.array([[1], [2], [None, [1]]])\n\n\ndef test_sequence_mixed_types_fails():\n    data = ['a', 1, 2.0]\n    with pytest.raises(pa.ArrowTypeError):\n        pa.array(data)\n\n\ndef test_sequence_mixed_types_with_specified_type_fails():\n    data = ['-10', '-5', {'a': 1}, '0', '5', '10']\n\n    type = pa.string()\n    with pytest.raises(TypeError):\n        pa.array(data, type=type)\n\n\ndef test_sequence_decimal():\n    data = [decimal.Decimal('1234.183'), decimal.Decimal('8094.234')]\n    for type in [pa.decimal128, pa.decimal256]:\n        arr = pa.array(data, type=type(precision=7, scale=3))\n        assert arr.to_pylist() == data\n\n\ndef test_sequence_decimal_different_precisions():\n    data = [\n        decimal.Decimal('1234234983.183'), decimal.Decimal('80943244.234')\n    ]\n    for type in [pa.decimal128, pa.decimal256]:\n        arr = pa.array(data, type=type(precision=13, scale=3))\n        assert arr.to_pylist() == data\n\n\ndef test_sequence_decimal_no_scale():\n    data = [decimal.Decimal('1234234983'), decimal.Decimal('8094324')]\n    for type in [pa.decimal128, pa.decimal256]:\n        arr = pa.array(data, type=type(precision=10))\n        assert arr.to_pylist() == data\n\n\ndef test_sequence_decimal_negative():\n    data = [decimal.Decimal('-1234.234983'), decimal.Decimal('-8.094324')]\n    for type in [pa.decimal128, pa.decimal256]:\n        arr = pa.array(data, type=type(precision=10, scale=6))\n        assert arr.to_pylist() == data\n\n\ndef test_sequence_decimal_no_whole_part():\n    data = [decimal.Decimal('-.4234983'), decimal.Decimal('.0103943')]\n    for type in [pa.decimal128, pa.decimal256]:\n        arr = pa.array(data, type=type(precision=7, scale=7))\n        assert arr.to_pylist() == data\n\n\ndef test_sequence_decimal_large_integer():\n    data = [decimal.Decimal('-394029506937548693.42983'),\n            decimal.Decimal('32358695912932.01033')]\n    for type in [pa.decimal128, pa.decimal256]:\n        arr = pa.array(data, type=type(precision=23, scale=5))\n        assert arr.to_pylist() == data\n\n\ndef test_sequence_decimal_from_integers():\n    data = [0, 1, -39402950693754869342983]\n    expected = [decimal.Decimal(x) for x in data]\n    for type in [pa.decimal128, pa.decimal256]:\n        arr = pa.array(data, type=type(precision=28, scale=5))\n        assert arr.to_pylist() == expected\n\n\ndef test_sequence_decimal_too_high_precision():\n    # ARROW-6989 python decimal has too high precision\n    with pytest.raises(ValueError, match=\"precision out of range\"):\n        pa.array([decimal.Decimal('1' * 80)])\n\n\ndef test_sequence_decimal_infer():\n    for data, typ in [\n        # simple case\n        (decimal.Decimal('1.234'), pa.decimal128(4, 3)),\n        # trailing zeros\n        (decimal.Decimal('12300'), pa.decimal128(5, 0)),\n        (decimal.Decimal('12300.0'), pa.decimal128(6, 1)),\n        # scientific power notation\n        (decimal.Decimal('1.23E+4'), pa.decimal128(5, 0)),\n        (decimal.Decimal('123E+2'), pa.decimal128(5, 0)),\n        (decimal.Decimal('123E+4'), pa.decimal128(7, 0)),\n        # leading zeros\n        (decimal.Decimal('0.0123'), pa.decimal128(4, 4)),\n        (decimal.Decimal('0.01230'), pa.decimal128(5, 5)),\n        (decimal.Decimal('1.230E-2'), pa.decimal128(5, 5)),\n    ]:\n        assert pa.infer_type([data]) == typ\n        arr = pa.array([data])\n        assert arr.type == typ\n        assert arr.to_pylist()[0] == data\n\n\ndef test_sequence_decimal_infer_mixed():\n    # ARROW-12150 - ensure mixed precision gets correctly inferred to\n    # common type that can hold all input values\n    cases = [\n        ([decimal.Decimal('1.234'), decimal.Decimal('3.456')],\n         pa.decimal128(4, 3)),\n        ([decimal.Decimal('1.234'), decimal.Decimal('456.7')],\n         pa.decimal128(6, 3)),\n        ([decimal.Decimal('123.4'), decimal.Decimal('4.567')],\n         pa.decimal128(6, 3)),\n        ([decimal.Decimal('123e2'), decimal.Decimal('4567e3')],\n         pa.decimal128(7, 0)),\n        ([decimal.Decimal('123e4'), decimal.Decimal('4567e2')],\n         pa.decimal128(7, 0)),\n        ([decimal.Decimal('0.123'), decimal.Decimal('0.04567')],\n         pa.decimal128(5, 5)),\n        ([decimal.Decimal('0.001'), decimal.Decimal('1.01E5')],\n         pa.decimal128(9, 3)),\n    ]\n    for data, typ in cases:\n        assert pa.infer_type(data) == typ\n        arr = pa.array(data)\n        assert arr.type == typ\n        assert arr.to_pylist() == data\n\n\ndef test_sequence_decimal_given_type():\n    for data, typs, wrong_typs in [\n        # simple case\n        (\n            decimal.Decimal('1.234'),\n            [pa.decimal128(4, 3), pa.decimal128(5, 3), pa.decimal128(5, 4)],\n            [pa.decimal128(4, 2), pa.decimal128(4, 4)]\n        ),\n        # trailing zeros\n        (\n            decimal.Decimal('12300'),\n            [pa.decimal128(5, 0), pa.decimal128(6, 0), pa.decimal128(3, -2)],\n            [pa.decimal128(4, 0), pa.decimal128(3, -3)]\n        ),\n        # scientific power notation\n        (\n            decimal.Decimal('1.23E+4'),\n            [pa.decimal128(5, 0), pa.decimal128(6, 0), pa.decimal128(3, -2)],\n            [pa.decimal128(4, 0), pa.decimal128(3, -3)]\n        ),\n    ]:\n        for typ in typs:\n            arr = pa.array([data], type=typ)\n            assert arr.type == typ\n            assert arr.to_pylist()[0] == data\n        for typ in wrong_typs:\n            with pytest.raises(ValueError):\n                pa.array([data], type=typ)\n\n\ndef test_range_types():\n    arr1 = pa.array(range(3))\n    arr2 = pa.array((0, 1, 2))\n    assert arr1.equals(arr2)\n\n\ndef test_empty_range():\n    arr = pa.array(range(0))\n    assert len(arr) == 0\n    assert arr.null_count == 0\n    assert arr.type == pa.null()\n    assert arr.to_pylist() == []\n\n\ndef test_structarray():\n    arr = pa.StructArray.from_arrays([], names=[])\n    assert arr.type == pa.struct([])\n    assert len(arr) == 0\n    assert arr.to_pylist() == []\n\n    ints = pa.array([None, 2, 3], type=pa.int64())\n    strs = pa.array(['a', None, 'c'], type=pa.string())\n    bools = pa.array([True, False, None], type=pa.bool_())\n    arr = pa.StructArray.from_arrays(\n        [ints, strs, bools],\n        ['ints', 'strs', 'bools'])\n\n    expected = [\n        {'ints': None, 'strs': 'a', 'bools': True},\n        {'ints': 2, 'strs': None, 'bools': False},\n        {'ints': 3, 'strs': 'c', 'bools': None},\n    ]\n\n    pylist = arr.to_pylist()\n    assert pylist == expected, (pylist, expected)\n\n    # len(names) != len(arrays)\n    with pytest.raises(ValueError):\n        pa.StructArray.from_arrays([ints], ['ints', 'strs'])\n\n\ndef test_struct_from_dicts():\n    ty = pa.struct([pa.field('a', pa.int32()),\n                    pa.field('b', pa.string()),\n                    pa.field('c', pa.bool_())])\n    arr = pa.array([], type=ty)\n    assert arr.to_pylist() == []\n\n    data = [{'a': 5, 'b': 'foo', 'c': True},\n            {'a': 6, 'b': 'bar', 'c': False}]\n    arr = pa.array(data, type=ty)\n    assert arr.to_pylist() == data\n\n    # With omitted values\n    data = [{'a': 5, 'c': True},\n            None,\n            {},\n            {'a': None, 'b': 'bar'}]\n    arr = pa.array(data, type=ty)\n    expected = [{'a': 5, 'b': None, 'c': True},\n                None,\n                {'a': None, 'b': None, 'c': None},\n                {'a': None, 'b': 'bar', 'c': None}]\n    assert arr.to_pylist() == expected\n\n\ndef test_struct_from_dicts_bytes_keys():\n    # ARROW-6878\n    ty = pa.struct([pa.field('a', pa.int32()),\n                    pa.field('b', pa.string()),\n                    pa.field('c', pa.bool_())])\n    arr = pa.array([], type=ty)\n    assert arr.to_pylist() == []\n\n    data = [{b'a': 5, b'b': 'foo'},\n            {b'a': 6, b'c': False}]\n    arr = pa.array(data, type=ty)\n    assert arr.to_pylist() == [\n        {'a': 5, 'b': 'foo', 'c': None},\n        {'a': 6, 'b': None, 'c': False},\n    ]\n\n\ndef test_struct_from_tuples():\n    ty = pa.struct([pa.field('a', pa.int32()),\n                    pa.field('b', pa.string()),\n                    pa.field('c', pa.bool_())])\n\n    data = [(5, 'foo', True),\n            (6, 'bar', False)]\n    expected = [{'a': 5, 'b': 'foo', 'c': True},\n                {'a': 6, 'b': 'bar', 'c': False}]\n    arr = pa.array(data, type=ty)\n\n    data_as_ndarray = np.empty(len(data), dtype=object)\n    data_as_ndarray[:] = data\n    arr2 = pa.array(data_as_ndarray, type=ty)\n    assert arr.to_pylist() == expected\n\n    assert arr.equals(arr2)\n\n    # With omitted values\n    data = [(5, 'foo', None),\n            None,\n            (6, None, False)]\n    expected = [{'a': 5, 'b': 'foo', 'c': None},\n                None,\n                {'a': 6, 'b': None, 'c': False}]\n    arr = pa.array(data, type=ty)\n    assert arr.to_pylist() == expected\n\n    # Invalid tuple size\n    for tup in [(5, 'foo'), (), ('5', 'foo', True, None)]:\n        with pytest.raises(ValueError, match=\"(?i)tuple size\"):\n            pa.array([tup], type=ty)\n\n\ndef test_struct_from_list_of_pairs():\n    ty = pa.struct([\n        pa.field('a', pa.int32()),\n        pa.field('b', pa.string()),\n        pa.field('c', pa.bool_())\n    ])\n    data = [\n        [('a', 5), ('b', 'foo'), ('c', True)],\n        [('a', 6), ('b', 'bar'), ('c', False)],\n        None\n    ]\n    arr = pa.array(data, type=ty)\n    assert arr.to_pylist() == [\n        {'a': 5, 'b': 'foo', 'c': True},\n        {'a': 6, 'b': 'bar', 'c': False},\n        None\n    ]\n\n    # test with duplicated field names\n    ty = pa.struct([\n        pa.field('a', pa.int32()),\n        pa.field('a', pa.string()),\n        pa.field('b', pa.bool_())\n    ])\n    data = [\n        [('a', 5), ('a', 'foo'), ('b', True)],\n        [('a', 6), ('a', 'bar'), ('b', False)],\n    ]\n    arr = pa.array(data, type=ty)\n    with pytest.raises(ValueError):\n        # TODO(kszucs): ARROW-9997\n        arr.to_pylist()\n\n    # test with empty elements\n    ty = pa.struct([\n        pa.field('a', pa.int32()),\n        pa.field('b', pa.string()),\n        pa.field('c', pa.bool_())\n    ])\n    data = [\n        [],\n        [('a', 5), ('b', 'foo'), ('c', True)],\n        [('a', 2), ('b', 'baz')],\n        [('a', 1), ('b', 'bar'), ('c', False), ('d', 'julia')],\n    ]\n    expected = [\n        {'a': None, 'b': None, 'c': None},\n        {'a': 5, 'b': 'foo', 'c': True},\n        {'a': 2, 'b': 'baz', 'c': None},\n        {'a': 1, 'b': 'bar', 'c': False},\n    ]\n    arr = pa.array(data, type=ty)\n    assert arr.to_pylist() == expected\n\n\ndef test_struct_from_list_of_pairs_errors():\n    ty = pa.struct([\n        pa.field('a', pa.int32()),\n        pa.field('b', pa.string()),\n        pa.field('c', pa.bool_())\n    ])\n\n    # test that it raises if the key doesn't match the expected field name\n    data = [\n        [],\n        [('a', 5), ('c', True), ('b', None)],\n    ]\n    msg = \"The expected field name is `b` but `c` was given\"\n    with pytest.raises(ValueError, match=msg):\n        pa.array(data, type=ty)\n\n    # test various errors both at the first position and after because of key\n    # type inference\n    template = (\n        r\"Could not convert {} with type {}: was expecting tuple of \"\n        r\"(key, value) pair\"\n    )\n    cases = [\n        tuple(),  # empty key-value pair\n        tuple('a',),  # missing value\n        tuple('unknown-key',),  # not known field name\n        'string',  # not a tuple\n    ]\n    for key_value_pair in cases:\n        msg = re.escape(template.format(\n            repr(key_value_pair), type(key_value_pair).__name__\n        ))\n\n        with pytest.raises(TypeError, match=msg):\n            pa.array([\n                [key_value_pair],\n                [('a', 5), ('b', 'foo'), ('c', None)],\n            ], type=ty)\n\n        with pytest.raises(TypeError, match=msg):\n            pa.array([\n                [('a', 5), ('b', 'foo'), ('c', None)],\n                [key_value_pair],\n            ], type=ty)\n\n\ndef test_struct_from_mixed_sequence():\n    # It is forbidden to mix dicts and tuples when initializing a struct array\n    ty = pa.struct([pa.field('a', pa.int32()),\n                    pa.field('b', pa.string()),\n                    pa.field('c', pa.bool_())])\n    data = [(5, 'foo', True),\n            {'a': 6, 'b': 'bar', 'c': False}]\n    with pytest.raises(TypeError):\n        pa.array(data, type=ty)\n\n\ndef test_struct_from_dicts_inference():\n    expected_type = pa.struct([pa.field('a', pa.int64()),\n                               pa.field('b', pa.string()),\n                               pa.field('c', pa.bool_())])\n    data = [{'a': 5, 'b': 'foo', 'c': True},\n            {'a': 6, 'b': 'bar', 'c': False}]\n\n    arr = pa.array(data)\n    check_struct_type(arr.type, expected_type)\n    assert arr.to_pylist() == data\n\n    # With omitted values\n    data = [{'a': 5, 'c': True},\n            None,\n            {},\n            {'a': None, 'b': 'bar'}]\n    expected = [{'a': 5, 'b': None, 'c': True},\n                None,\n                {'a': None, 'b': None, 'c': None},\n                {'a': None, 'b': 'bar', 'c': None}]\n\n    arr = pa.array(data)\n    data_as_ndarray = np.empty(len(data), dtype=object)\n    data_as_ndarray[:] = data\n    arr2 = pa.array(data)\n\n    check_struct_type(arr.type, expected_type)\n    assert arr.to_pylist() == expected\n    assert arr.equals(arr2)\n\n    # Nested\n    expected_type = pa.struct([\n        pa.field('a', pa.struct([pa.field('aa', pa.list_(pa.int64())),\n                                 pa.field('ab', pa.bool_())])),\n        pa.field('b', pa.string())])\n    data = [{'a': {'aa': [5, 6], 'ab': True}, 'b': 'foo'},\n            {'a': {'aa': None, 'ab': False}, 'b': None},\n            {'a': None, 'b': 'bar'}]\n    arr = pa.array(data)\n\n    assert arr.to_pylist() == data\n\n    # Edge cases\n    arr = pa.array([{}])\n    assert arr.type == pa.struct([])\n    assert arr.to_pylist() == [{}]\n\n    # Mixing structs and scalars is rejected\n    with pytest.raises((pa.ArrowInvalid, pa.ArrowTypeError)):\n        pa.array([1, {'a': 2}])\n\n\ndef test_structarray_from_arrays_coerce():\n    # ARROW-1706\n    ints = [None, 2, 3]\n    strs = ['a', None, 'c']\n    bools = [True, False, None]\n    ints_nonnull = [1, 2, 3]\n\n    arrays = [ints, strs, bools, ints_nonnull]\n    result = pa.StructArray.from_arrays(arrays,\n                                        ['ints', 'strs', 'bools',\n                                         'int_nonnull'])\n    expected = pa.StructArray.from_arrays(\n        [pa.array(ints, type='int64'),\n         pa.array(strs, type='utf8'),\n         pa.array(bools),\n         pa.array(ints_nonnull, type='int64')],\n        ['ints', 'strs', 'bools', 'int_nonnull'])\n\n    with pytest.raises(ValueError):\n        pa.StructArray.from_arrays(arrays)\n\n    assert result.equals(expected)\n\n\ndef test_decimal_array_with_none_and_nan():\n    values = [decimal.Decimal('1.234'), None, np.nan, decimal.Decimal('nan')]\n\n    with pytest.raises(TypeError):\n        # ARROW-6227: Without from_pandas=True, NaN is considered a float\n        array = pa.array(values)\n\n    array = pa.array(values, from_pandas=True)\n    assert array.type == pa.decimal128(4, 3)\n    assert array.to_pylist() == values[:2] + [None, None]\n\n    array = pa.array(values, type=pa.decimal128(10, 4), from_pandas=True)\n    assert array.to_pylist() == [decimal.Decimal('1.2340'), None, None, None]\n\n\ndef test_map_from_dicts():\n    data = [[{'key': b'a', 'value': 1}, {'key': b'b', 'value': 2}],\n            [{'key': b'c', 'value': 3}],\n            [{'key': b'd', 'value': 4}, {'key': b'e', 'value': 5},\n             {'key': b'f', 'value': None}],\n            [{'key': b'g', 'value': 7}]]\n    expected = [[(d['key'], d['value']) for d in entry] for entry in data]\n\n    arr = pa.array(expected, type=pa.map_(pa.binary(), pa.int32()))\n\n    assert arr.to_pylist() == expected\n\n    # With omitted values\n    data[1] = None\n    expected[1] = None\n\n    arr = pa.array(expected, type=pa.map_(pa.binary(), pa.int32()))\n\n    assert arr.to_pylist() == expected\n\n    # Invalid dictionary\n    for entry in [[{'value': 5}], [{}], [{'k': 1, 'v': 2}]]:\n        with pytest.raises(ValueError, match=\"Invalid Map\"):\n            pa.array([entry], type=pa.map_('i4', 'i4'))\n\n    # Invalid dictionary types\n    for entry in [[{'key': '1', 'value': 5}], [{'key': {'value': 2}}]]:\n        with pytest.raises(pa.ArrowInvalid, match=\"tried to convert to int\"):\n            pa.array([entry], type=pa.map_('i4', 'i4'))\n\n\ndef test_map_from_tuples():\n    expected = [[(b'a', 1), (b'b', 2)],\n                [(b'c', 3)],\n                [(b'd', 4), (b'e', 5), (b'f', None)],\n                [(b'g', 7)]]\n\n    arr = pa.array(expected, type=pa.map_(pa.binary(), pa.int32()))\n\n    assert arr.to_pylist() == expected\n\n    # With omitted values\n    expected[1] = None\n\n    arr = pa.array(expected, type=pa.map_(pa.binary(), pa.int32()))\n\n    assert arr.to_pylist() == expected\n\n    # Invalid tuple size\n    for entry in [[(5,)], [()], [('5', 'foo', True)]]:\n        with pytest.raises(ValueError, match=\"(?i)tuple size\"):\n            pa.array([entry], type=pa.map_('i4', 'i4'))\n\n\ndef test_dictionary_from_boolean():\n    typ = pa.dictionary(pa.int8(), value_type=pa.bool_())\n    a = pa.array([False, False, True, False, True], type=typ)\n    assert isinstance(a.type, pa.DictionaryType)\n    assert a.type.equals(typ)\n\n    expected_indices = pa.array([0, 0, 1, 0, 1], type=pa.int8())\n    expected_dictionary = pa.array([False, True], type=pa.bool_())\n    assert a.indices.equals(expected_indices)\n    assert a.dictionary.equals(expected_dictionary)\n\n\n@pytest.mark.parametrize('value_type', [\n    pa.int8(),\n    pa.int16(),\n    pa.int32(),\n    pa.int64(),\n    pa.uint8(),\n    pa.uint16(),\n    pa.uint32(),\n    pa.uint64(),\n    pa.float32(),\n    pa.float64(),\n])\ndef test_dictionary_from_integers(value_type):\n    typ = pa.dictionary(pa.int8(), value_type=value_type)\n    a = pa.array([1, 2, 1, 1, 2, 3], type=typ)\n    assert isinstance(a.type, pa.DictionaryType)\n    assert a.type.equals(typ)\n\n    expected_indices = pa.array([0, 1, 0, 0, 1, 2], type=pa.int8())\n    expected_dictionary = pa.array([1, 2, 3], type=value_type)\n    assert a.indices.equals(expected_indices)\n    assert a.dictionary.equals(expected_dictionary)\n\n\n@pytest.mark.parametrize('input_index_type', [\n    pa.int8(),\n    pa.int16(),\n    pa.int32(),\n    pa.int64()\n])\ndef test_dictionary_index_type(input_index_type):\n    # dictionary array is constructed using adaptive index type builder,\n    # but the input index type is considered as the minimal width type to use\n\n    typ = pa.dictionary(input_index_type, value_type=pa.int64())\n    arr = pa.array(range(10), type=typ)\n    assert arr.type.equals(typ)\n\n\ndef test_dictionary_is_always_adaptive():\n    # dictionary array is constructed using adaptive index type builder,\n    # meaning that the output index type may be wider than the given index type\n    # since it depends on the input data\n    typ = pa.dictionary(pa.int8(), value_type=pa.int64())\n\n    a = pa.array(range(2**7), type=typ)\n    expected = pa.dictionary(pa.int8(), pa.int64())\n    assert a.type.equals(expected)\n\n    a = pa.array(range(2**7 + 1), type=typ)\n    expected = pa.dictionary(pa.int16(), pa.int64())\n    assert a.type.equals(expected)\n\n\ndef test_dictionary_from_strings():\n    for value_type in [pa.binary(), pa.string()]:\n        typ = pa.dictionary(pa.int8(), value_type)\n        a = pa.array([\"\", \"a\", \"bb\", \"a\", \"bb\", \"ccc\"], type=typ)\n\n        assert isinstance(a.type, pa.DictionaryType)\n\n        expected_indices = pa.array([0, 1, 2, 1, 2, 3], type=pa.int8())\n        expected_dictionary = pa.array([\"\", \"a\", \"bb\", \"ccc\"], type=value_type)\n        assert a.indices.equals(expected_indices)\n        assert a.dictionary.equals(expected_dictionary)\n\n    # fixed size binary type\n    typ = pa.dictionary(pa.int8(), pa.binary(3))\n    a = pa.array([\"aaa\", \"aaa\", \"bbb\", \"ccc\", \"bbb\"], type=typ)\n    assert isinstance(a.type, pa.DictionaryType)\n\n    expected_indices = pa.array([0, 0, 1, 2, 1], type=pa.int8())\n    expected_dictionary = pa.array([\"aaa\", \"bbb\", \"ccc\"], type=pa.binary(3))\n    assert a.indices.equals(expected_indices)\n    assert a.dictionary.equals(expected_dictionary)\n\n\n@pytest.mark.parametrize(('unit', 'expected'), [\n    ('s', datetime.timedelta(seconds=-2147483000)),\n    ('ms', datetime.timedelta(milliseconds=-2147483000)),\n    ('us', datetime.timedelta(microseconds=-2147483000)),\n    ('ns', datetime.timedelta(microseconds=-2147483))\n])\ndef test_duration_array_roundtrip_corner_cases(unit, expected):\n    # Corner case discovered by hypothesis: there were implicit conversions to\n    # unsigned values resulting wrong values with wrong signs.\n    ty = pa.duration(unit)\n    arr = pa.array([-2147483000], type=ty)\n    restored = pa.array(arr.to_pylist(), type=ty)\n    assert arr.equals(restored)\n\n    expected_list = [expected]\n    if unit == 'ns':\n        # if pandas is available then a pandas Timedelta is returned\n        try:\n            import pandas as pd\n        except ImportError:\n            pass\n        else:\n            expected_list = [pd.Timedelta(-2147483000, unit='ns')]\n\n    assert restored.to_pylist() == expected_list\n\n\n@pytest.mark.pandas\ndef test_roundtrip_nanosecond_resolution_pandas_temporal_objects():\n    # corner case discovered by hypothesis: preserving the nanoseconds on\n    # conversion from a list of Timedelta and Timestamp objects\n    import pandas as pd\n\n    ty = pa.duration('ns')\n    arr = pa.array([9223371273709551616], type=ty)\n    data = arr.to_pylist()\n    assert isinstance(data[0], pd.Timedelta)\n    restored = pa.array(data, type=ty)\n    assert arr.equals(restored)\n    assert restored.to_pylist() == [\n        pd.Timedelta(9223371273709551616, unit='ns')\n    ]\n\n    ty = pa.timestamp('ns')\n    arr = pa.array([9223371273709551616], type=ty)\n    data = arr.to_pylist()\n    assert isinstance(data[0], pd.Timestamp)\n    restored = pa.array(data, type=ty)\n    assert arr.equals(restored)\n    assert restored.to_pylist() == [\n        pd.Timestamp(9223371273709551616, unit='ns')\n    ]\n\n    ty = pa.timestamp('ns', tz='US/Eastern')\n    value = 1604119893000000000\n    arr = pa.array([value], type=ty)\n    data = arr.to_pylist()\n    assert isinstance(data[0], pd.Timestamp)\n    restored = pa.array(data, type=ty)\n    assert arr.equals(restored)\n    assert restored.to_pylist() == [\n        pd.Timestamp(value, unit='ns').tz_localize(\n            \"UTC\").tz_convert('US/Eastern')\n    ]\n\n\n@h.given(past.all_arrays)\ndef test_array_to_pylist_roundtrip(arr):\n    seq = arr.to_pylist()\n    restored = pa.array(seq, type=arr.type)\n    assert restored.equals(arr)\n\n\n@pytest.mark.large_memory\ndef test_auto_chunking_binary_like():\n    # single chunk\n    v1 = b'x' * 100000000\n    v2 = b'x' * 147483646\n\n    # single chunk\n    one_chunk_data = [v1] * 20 + [b'', None, v2]\n    arr = pa.array(one_chunk_data, type=pa.binary())\n    assert isinstance(arr, pa.Array)\n    assert len(arr) == 23\n    assert arr[20].as_py() == b''\n    assert arr[21].as_py() is None\n    assert arr[22].as_py() == v2\n\n    # two chunks\n    two_chunk_data = one_chunk_data + [b'two']\n    arr = pa.array(two_chunk_data, type=pa.binary())\n    assert isinstance(arr, pa.ChunkedArray)\n    assert arr.num_chunks == 2\n    assert len(arr.chunk(0)) == 23\n    assert len(arr.chunk(1)) == 1\n    assert arr.chunk(0)[20].as_py() == b''\n    assert arr.chunk(0)[21].as_py() is None\n    assert arr.chunk(0)[22].as_py() == v2\n    assert arr.chunk(1).to_pylist() == [b'two']\n\n    # three chunks\n    three_chunk_data = one_chunk_data * 2 + [b'three', b'three']\n    arr = pa.array(three_chunk_data, type=pa.binary())\n    assert isinstance(arr, pa.ChunkedArray)\n    assert arr.num_chunks == 3\n    assert len(arr.chunk(0)) == 23\n    assert len(arr.chunk(1)) == 23\n    assert len(arr.chunk(2)) == 2\n    for i in range(2):\n        assert arr.chunk(i)[20].as_py() == b''\n        assert arr.chunk(i)[21].as_py() is None\n        assert arr.chunk(i)[22].as_py() == v2\n    assert arr.chunk(2).to_pylist() == [b'three', b'three']\n\n\n@pytest.mark.large_memory\ndef test_auto_chunking_list_of_binary():\n    # ARROW-6281\n    vals = [['x' * 1024]] * ((2 << 20) + 1)\n    arr = pa.array(vals)\n    assert isinstance(arr, pa.ChunkedArray)\n    assert arr.num_chunks == 2\n    assert len(arr.chunk(0)) == 2**21 - 1\n    assert len(arr.chunk(1)) == 2\n    assert arr.chunk(1).to_pylist() == [['x' * 1024]] * 2\n\n\n@pytest.mark.large_memory\ndef test_auto_chunking_list_like():\n    item = np.ones((2**28,), dtype='uint8')\n    data = [item] * (2**3 - 1)\n    arr = pa.array(data, type=pa.list_(pa.uint8()))\n    assert isinstance(arr, pa.Array)\n    assert len(arr) == 7\n\n    item = np.ones((2**28,), dtype='uint8')\n    data = [item] * 2**3\n    arr = pa.array(data, type=pa.list_(pa.uint8()))\n    assert isinstance(arr, pa.ChunkedArray)\n    assert arr.num_chunks == 2\n    assert len(arr.chunk(0)) == 7\n    assert len(arr.chunk(1)) == 1\n    chunk = arr.chunk(1)\n    scalar = chunk[0]\n    assert isinstance(scalar, pa.ListScalar)\n    expected = pa.array(item, type=pa.uint8())\n    assert scalar.values == expected\n\n\n@pytest.mark.slow\n@pytest.mark.large_memory\ndef test_auto_chunking_map_type():\n    # takes ~20 minutes locally\n    ty = pa.map_(pa.int8(), pa.int8())\n    item = [(1, 1)] * 2**28\n    data = [item] * 2**3\n    arr = pa.array(data, type=ty)\n    assert isinstance(arr, pa.ChunkedArray)\n    assert len(arr.chunk(0)) == 7\n    assert len(arr.chunk(1)) == 1\n\n\n@pytest.mark.large_memory\n@pytest.mark.parametrize(('ty', 'char'), [\n    (pa.string(), 'x'),\n    (pa.binary(), b'x'),\n])\ndef test_nested_auto_chunking(ty, char):\n    v1 = char * 100000000\n    v2 = char * 147483646\n\n    struct_type = pa.struct([\n        pa.field('bool', pa.bool_()),\n        pa.field('integer', pa.int64()),\n        pa.field('string-like', ty),\n    ])\n\n    data = [{'bool': True, 'integer': 1, 'string-like': v1}] * 20\n    data.append({'bool': True, 'integer': 1, 'string-like': v2})\n    arr = pa.array(data, type=struct_type)\n    assert isinstance(arr, pa.Array)\n\n    data.append({'bool': True, 'integer': 1, 'string-like': char})\n    arr = pa.array(data, type=struct_type)\n    assert isinstance(arr, pa.ChunkedArray)\n    assert arr.num_chunks == 2\n    assert len(arr.chunk(0)) == 21\n    assert len(arr.chunk(1)) == 1\n    assert arr.chunk(1)[0].as_py() == {\n        'bool': True,\n        'integer': 1,\n        'string-like': char\n    }\n\n\n@pytest.mark.large_memory\ndef test_array_from_pylist_data_overflow():\n    # Regression test for ARROW-12983\n    # Data buffer overflow - should result in chunked array\n    items = [b'a' * 4096] * (2 ** 19)\n    arr = pa.array(items, type=pa.string())\n    assert isinstance(arr, pa.ChunkedArray)\n    assert len(arr) == 2**19\n    assert len(arr.chunks) > 1\n\n    mask = np.zeros(2**19, bool)\n    arr = pa.array(items, mask=mask, type=pa.string())\n    assert isinstance(arr, pa.ChunkedArray)\n    assert len(arr) == 2**19\n    assert len(arr.chunks) > 1\n\n    arr = pa.array(items, type=pa.binary())\n    assert isinstance(arr, pa.ChunkedArray)\n    assert len(arr) == 2**19\n    assert len(arr.chunks) > 1\n\n\n@pytest.mark.slow\n@pytest.mark.large_memory\ndef test_array_from_pylist_offset_overflow():\n    # Regression test for ARROW-12983\n    # Offset buffer overflow - should result in chunked array\n    # Note this doesn't apply to primitive arrays\n    items = [b'a'] * (2 ** 31)\n    arr = pa.array(items, type=pa.string())\n    assert isinstance(arr, pa.ChunkedArray)\n    assert len(arr) == 2**31\n    assert len(arr.chunks) > 1\n\n    mask = np.zeros(2**31, bool)\n    arr = pa.array(items, mask=mask, type=pa.string())\n    assert isinstance(arr, pa.ChunkedArray)\n    assert len(arr) == 2**31\n    assert len(arr.chunks) > 1\n\n    arr = pa.array(items, type=pa.binary())\n    assert isinstance(arr, pa.ChunkedArray)\n    assert len(arr) == 2**31\n    assert len(arr.chunks) > 1\n\n\n@parametrize_with_collections_types\n@pytest.mark.parametrize(('data', 'scalar_data', 'value_type'), [\n    ([True, False, None], [pa.scalar(True), pa.scalar(False), None], pa.bool_()),\n    (\n        [1, 2, None],\n        [pa.scalar(1), pa.scalar(2), pa.scalar(None, pa.int64())],\n        pa.int64()\n    ),\n    ([1, None, None], [pa.scalar(1), None, pa.scalar(None, pa.int64())], pa.int64()),\n    ([None, None], [pa.scalar(None), pa.scalar(None)], pa.null()),\n    ([1., 2., None], [pa.scalar(1.), pa.scalar(2.), None], pa.float64()),\n    (\n        [None, datetime.date.today()],\n        [None, pa.scalar(datetime.date.today())],\n        pa.date32()\n    ),\n    (\n        [None, datetime.date.today()],\n        [None, pa.scalar(datetime.date.today(), pa.date64())],\n        pa.date64()\n    ),\n    (\n        [datetime.time(1, 1, 1), None],\n        [pa.scalar(datetime.time(1, 1, 1)), None],\n        pa.time64('us')\n    ),\n    (\n        [datetime.timedelta(seconds=10)],\n        [pa.scalar(datetime.timedelta(seconds=10))],\n        pa.duration('us')\n    ),\n    (\n        [None, datetime.datetime(2014, 1, 1)],\n        [None, pa.scalar(datetime.datetime(2014, 1, 1))],\n        pa.timestamp('us')\n    ),\n    (\n        [pa.MonthDayNano([1, -1, -10100])],\n        [pa.scalar(pa.MonthDayNano([1, -1, -10100]))],\n        pa.month_day_nano_interval()\n    ),\n    ([\"a\", \"b\"], [pa.scalar(\"a\"), pa.scalar(\"b\")], pa.string()),\n    ([b\"a\", b\"b\"], [pa.scalar(b\"a\"), pa.scalar(b\"b\")], pa.binary()),\n    (\n        [b\"a\", b\"b\"],\n        [pa.scalar(b\"a\", pa.binary(1)), pa.scalar(b\"b\", pa.binary(1))],\n        pa.binary(1)\n    ),\n    ([[1, 2, 3]], [pa.scalar([1, 2, 3])], pa.list_(pa.int64())),\n    ([[\"a\", \"b\"]], [pa.scalar([\"a\", \"b\"])], pa.list_(pa.string())),\n    ([[1, 2, 3]], [pa.scalar([1, 2, 3], type=pa.list_view(pa.int64()))],\n     pa.list_view(pa.int64())),\n    ([[\"a\", \"b\"]], [pa.scalar([\"a\", \"b\"], type=pa.list_view(pa.string()))],\n     pa.list_view(pa.string())),\n    (\n        [1, 2, None],\n        [pa.scalar(1, type=pa.int8()), pa.scalar(2, type=pa.int8()), None],\n        pa.int8()\n    ),\n    ([1, None], [pa.scalar(1.0, type=pa.int32()), None], pa.int32()),\n    (\n        [\"aaa\", \"bbb\"],\n        [pa.scalar(\"aaa\", type=pa.binary(3)), pa.scalar(\"bbb\", type=pa.binary(3))],\n        pa.binary(3)),\n    ([b\"a\"], [pa.scalar(\"a\", type=pa.large_binary())], pa.large_binary()),\n    ([\"a\"], [pa.scalar(\"a\", type=pa.large_string())], pa.large_string()),\n    ([b\"a\"], [pa.scalar(\"a\", type=pa.binary_view())], pa.binary_view()),\n    ([\"a\"], [pa.scalar(\"a\", type=pa.string_view())], pa.string_view()),\n    (\n        [\"a\"],\n        [pa.scalar(\"a\", type=pa.dictionary(pa.int64(), pa.string()))],\n        pa.dictionary(pa.int64(), pa.string())\n    ),\n    (\n        [\"a\", \"b\"],\n        [pa.scalar(\"a\", pa.dictionary(pa.int64(), pa.string())),\n         pa.scalar(\"b\", pa.dictionary(pa.int64(), pa.string()))],\n        pa.dictionary(pa.int64(), pa.string())\n    ),\n    (\n        [1],\n        [pa.scalar(1, type=pa.dictionary(pa.int64(), pa.int32()))],\n        pa.dictionary(pa.int64(), pa.int32())\n    ),\n    (\n        [(1, 2)],\n        [pa.scalar([('a', 1), ('b', 2)], type=pa.struct(\n            [('a', pa.int8()), ('b', pa.int8())]))],\n        pa.struct([('a', pa.int8()), ('b', pa.int8())])\n    ),\n    (\n        [(1, 'bar')],\n        [pa.scalar([('a', 1), ('b', 'bar')], type=pa.struct(\n            [('a', pa.int8()), ('b', pa.string())]))],\n        pa.struct([('a', pa.int8()), ('b', pa.string())])\n    )\n])\ndef test_array_accepts_pyarrow_scalar(seq, data, scalar_data, value_type):\n    if type(seq(scalar_data)) == set:\n        pytest.skip(\"The elements in the set get reordered.\")\n    expect = pa.array(data, type=value_type)\n    result = pa.array(seq(scalar_data))\n    assert expect.equals(result)\n\n    result = pa.array(seq(scalar_data), type=value_type)\n    assert expect.equals(result)\n\n\n@parametrize_with_collections_types\ndef test_array_accepts_pyarrow_scalar_errors(seq):\n    sequence = seq([pa.scalar(1), pa.scalar(\"a\"), pa.scalar(3.0)])\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"cannot mix scalars with different types\"):\n        pa.array(sequence)\n\n    sequence = seq([1, pa.scalar(\"a\"), None])\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"pyarrow scalars cannot be mixed with other \"\n                             \"Python scalar values currently\"):\n        pa.array(sequence)\n\n    sequence = seq([np.float16(\"0.1\"), pa.scalar(\"a\"), None])\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"pyarrow scalars cannot be mixed with other \"\n                             \"Python scalar values currently\"):\n        pa.array(sequence)\n\n    sequence = seq([pa.scalar(\"a\"), np.float16(\"0.1\"), None])\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"pyarrow scalars cannot be mixed with other \"\n                             \"Python scalar values currently\"):\n        pa.array(sequence)\n\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"Cannot append scalar of type string \"\n                             \"to builder for type int32\"):\n        pa.array([pa.scalar(\"a\")], type=pa.int32())\n\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"Cannot append scalar of type int64 \"\n                             \"to builder for type null\"):\n        pa.array([pa.scalar(1)], type=pa.null())\n", "python/pyarrow/tests/test_scalars.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport datetime\nimport decimal\nimport pytest\nimport sys\nimport weakref\n\nimport numpy as np\n\nimport pyarrow as pa\nimport pyarrow.compute as pc\nfrom pyarrow.tests import util\n\n\n@pytest.mark.parametrize(['value', 'ty', 'klass'], [\n    (False, None, pa.BooleanScalar),\n    (True, None, pa.BooleanScalar),\n    (1, None, pa.Int64Scalar),\n    (-1, None, pa.Int64Scalar),\n    (1, pa.int8(), pa.Int8Scalar),\n    (1, pa.uint8(), pa.UInt8Scalar),\n    (1, pa.int16(), pa.Int16Scalar),\n    (1, pa.uint16(), pa.UInt16Scalar),\n    (1, pa.int32(), pa.Int32Scalar),\n    (1, pa.uint32(), pa.UInt32Scalar),\n    (1, pa.int64(), pa.Int64Scalar),\n    (1, pa.uint64(), pa.UInt64Scalar),\n    (1.0, None, pa.DoubleScalar),\n    (np.float16(1.0), pa.float16(), pa.HalfFloatScalar),\n    (1.0, pa.float32(), pa.FloatScalar),\n    (decimal.Decimal(\"1.123\"), None, pa.Decimal128Scalar),\n    (decimal.Decimal(\"1.1234567890123456789012345678901234567890\"),\n     None, pa.Decimal256Scalar),\n    (\"string\", None, pa.StringScalar),\n    (b\"bytes\", None, pa.BinaryScalar),\n    (\"largestring\", pa.large_string(), pa.LargeStringScalar),\n    (b\"largebytes\", pa.large_binary(), pa.LargeBinaryScalar),\n    (\"string_view\", pa.string_view(), pa.StringViewScalar),\n    (b\"bytes_view\", pa.binary_view(), pa.BinaryViewScalar),\n    (b\"abc\", pa.binary(3), pa.FixedSizeBinaryScalar),\n    ([1, 2, 3], None, pa.ListScalar),\n    ([1, 2, 3, 4], pa.large_list(pa.int8()), pa.LargeListScalar),\n    ([1, 2, 3, 4, 5], pa.list_(pa.int8(), 5), pa.FixedSizeListScalar),\n    ([1, 2, 3], pa.list_view(pa.int8()), pa.ListViewScalar),\n    ([1, 2, 3, 4], pa.large_list_view(pa.int8()), pa.LargeListViewScalar),\n    (datetime.date.today(), None, pa.Date32Scalar),\n    (datetime.date.today(), pa.date64(), pa.Date64Scalar),\n    (datetime.datetime.now(), None, pa.TimestampScalar),\n    (datetime.datetime.now().time().replace(microsecond=0), pa.time32('s'),\n     pa.Time32Scalar),\n    (datetime.datetime.now().time(), None, pa.Time64Scalar),\n    (datetime.timedelta(days=1), None, pa.DurationScalar),\n    (pa.MonthDayNano([1, -1, -10100]), None,\n     pa.MonthDayNanoIntervalScalar),\n    ({'a': 1, 'b': [1, 2]}, None, pa.StructScalar),\n    ([('a', 1), ('b', 2)], pa.map_(pa.string(), pa.int8()), pa.MapScalar),\n])\ndef test_basics(value, ty, klass, pickle_module):\n    s = pa.scalar(value, type=ty)\n    s.validate()\n    s.validate(full=True)\n    assert isinstance(s, klass)\n    assert s.as_py() == value\n    assert s == pa.scalar(value, type=ty)\n    assert s != value\n    assert s != \"else\"\n    assert hash(s) == hash(s)\n    assert s.is_valid is True\n    assert s != None  # noqa: E711\n\n    s = pa.scalar(None, type=s.type)\n    assert s.is_valid is False\n    assert s.as_py() is None\n    assert s != pa.scalar(value, type=ty)\n\n    # test pickle roundtrip\n    restored = pickle_module.loads(pickle_module.dumps(s))\n    assert s.equals(restored)\n\n    # test that scalars are weak-referenceable\n    wr = weakref.ref(s)\n    assert wr() is not None\n    del s\n    assert wr() is None\n\n\ndef test_invalid_scalar():\n    s = pc.cast(pa.scalar(b\"\\xff\"), pa.string(), safe=False)\n    s.validate()\n    with pytest.raises(ValueError,\n                       match=\"string scalar contains invalid UTF8 data\"):\n        s.validate(full=True)\n\n\ndef test_null_singleton():\n    with pytest.raises(RuntimeError):\n        pa.NullScalar()\n\n\ndef test_nulls(pickle_module):\n    null = pa.scalar(None)\n    assert null is pa.NA\n    assert null.as_py() is None\n    assert null != \"something\"\n    assert (null == pa.scalar(None)) is True\n    assert (null == 0) is False\n    assert pa.NA == pa.NA\n    assert pa.NA not in [5]\n\n    arr = pa.array([None, None])\n    for v in arr:\n        assert v is pa.NA\n        assert v.as_py() is None\n\n    # test pickle roundtrip\n    restored = pickle_module.loads(pickle_module.dumps(null))\n    assert restored.equals(null)\n\n    # test that scalars are weak-referenceable\n    wr = weakref.ref(null)\n    assert wr() is not None\n    del null\n    assert wr() is not None  # singleton\n\n\ndef test_hashing():\n    # ARROW-640\n    values = list(range(500))\n    arr = pa.array(values + values)\n    set_from_array = set(arr)\n    assert isinstance(set_from_array, set)\n    assert len(set_from_array) == 500\n\n\ndef test_hashing_struct_scalar():\n    # GH-35360\n    a = pa.array([[{'a': 5}, {'a': 6}], [{'a': 7}, None]])\n    b = pa.array([[{'a': 7}, None]])\n    hash1 = hash(a[1])\n    hash2 = hash(b[0])\n    assert hash1 == hash2\n\n\n@pytest.mark.skipif(sys.platform == \"win32\" and not util.windows_has_tzdata(),\n                    reason=\"Timezone database is not installed on Windows\")\ndef test_timestamp_scalar():\n    a = repr(pa.scalar(\"0000-01-01\").cast(pa.timestamp(\"s\")))\n    assert a == \"<pyarrow.TimestampScalar: '0000-01-01T00:00:00'>\"\n    b = repr(pa.scalar(datetime.datetime(2015, 1, 1), type=pa.timestamp('s', tz='UTC')))\n    assert b == \"<pyarrow.TimestampScalar: '2015-01-01T00:00:00+0000'>\"\n    c = repr(pa.scalar(datetime.datetime(2015, 1, 1), type=pa.timestamp('us')))\n    assert c == \"<pyarrow.TimestampScalar: '2015-01-01T00:00:00.000000'>\"\n    d = repr(pc.assume_timezone(\n        pa.scalar(\"2000-01-01\").cast(pa.timestamp(\"s\")), \"America/New_York\"))\n    assert d == \"<pyarrow.TimestampScalar: '2000-01-01T00:00:00-0500'>\"\n\n\ndef test_bool():\n    false = pa.scalar(False)\n    true = pa.scalar(True)\n\n    assert isinstance(false, pa.BooleanScalar)\n    assert isinstance(true, pa.BooleanScalar)\n\n    assert repr(true) == \"<pyarrow.BooleanScalar: True>\"\n    assert str(true) == \"True\"\n    assert repr(false) == \"<pyarrow.BooleanScalar: False>\"\n    assert str(false) == \"False\"\n\n    assert true.as_py() is True\n    assert false.as_py() is False\n\n\ndef test_numerics():\n    # int64\n    s = pa.scalar(1)\n    assert isinstance(s, pa.Int64Scalar)\n    assert repr(s) == \"<pyarrow.Int64Scalar: 1>\"\n    assert str(s) == \"1\"\n    assert s.as_py() == 1\n\n    with pytest.raises(OverflowError):\n        pa.scalar(-1, type='uint8')\n\n    # float64\n    s = pa.scalar(1.5)\n    assert isinstance(s, pa.DoubleScalar)\n    assert repr(s) == \"<pyarrow.DoubleScalar: 1.5>\"\n    assert str(s) == \"1.5\"\n    assert s.as_py() == 1.5\n\n    # float16\n    s = pa.scalar(np.float16(0.5), type='float16')\n    assert isinstance(s, pa.HalfFloatScalar)\n    # on numpy2 repr(np.float16(0.5)) == \"np.float16(0.5)\"\n    # on numpy1 repr(np.float16(0.5)) == \"0.5\"\n    assert repr(s) == f\"<pyarrow.HalfFloatScalar: {np.float16(0.5)!r}>\"\n    assert str(s) == \"0.5\"\n    assert s.as_py() == 0.5\n\n\ndef test_decimal128():\n    v = decimal.Decimal(\"1.123\")\n    s = pa.scalar(v)\n    assert isinstance(s, pa.Decimal128Scalar)\n    assert s.as_py() == v\n    assert s.type == pa.decimal128(4, 3)\n\n    v = decimal.Decimal(\"1.1234\")\n    with pytest.raises(pa.ArrowInvalid):\n        pa.scalar(v, type=pa.decimal128(4, scale=3))\n    with pytest.raises(pa.ArrowInvalid):\n        pa.scalar(v, type=pa.decimal128(5, scale=3))\n\n    s = pa.scalar(v, type=pa.decimal128(5, scale=4))\n    assert isinstance(s, pa.Decimal128Scalar)\n    assert s.as_py() == v\n\n\ndef test_decimal256():\n    v = decimal.Decimal(\"1234567890123456789012345678901234567890.123\")\n    s = pa.scalar(v)\n    assert isinstance(s, pa.Decimal256Scalar)\n    assert s.as_py() == v\n    assert s.type == pa.decimal256(43, 3)\n\n    v = decimal.Decimal(\"1.1234\")\n    with pytest.raises(pa.ArrowInvalid):\n        pa.scalar(v, type=pa.decimal256(4, scale=3))\n    with pytest.raises(pa.ArrowInvalid):\n        pa.scalar(v, type=pa.decimal256(5, scale=3))\n\n    s = pa.scalar(v, type=pa.decimal256(5, scale=4))\n    assert isinstance(s, pa.Decimal256Scalar)\n    assert s.as_py() == v\n\n\ndef test_date():\n    # ARROW-5125\n    d1 = datetime.date(3200, 1, 1)\n    d2 = datetime.date(1960, 1, 1)\n\n    for ty in [pa.date32(), pa.date64()]:\n        for d in [d1, d2]:\n            s = pa.scalar(d, type=ty)\n            assert s.as_py() == d\n\n\ndef test_date_cast():\n    # ARROW-10472 - casting fo scalars doesn't segfault\n    scalar = pa.scalar(datetime.datetime(2012, 1, 1), type=pa.timestamp(\"us\"))\n    expected = datetime.date(2012, 1, 1)\n    for ty in [pa.date32(), pa.date64()]:\n        result = scalar.cast(ty)\n        assert result.as_py() == expected\n\n\ndef test_time_from_datetime_time():\n    t1 = datetime.time(18, 0)\n    t2 = datetime.time(21, 0)\n\n    types = [pa.time32('s'), pa.time32('ms'), pa.time64('us'), pa.time64('ns')]\n    for ty in types:\n        for t in [t1, t2]:\n            s = pa.scalar(t, type=ty)\n            assert s.as_py() == t\n\n\n@pytest.mark.parametrize(['value', 'time_type'], [\n    (1, pa.time32(\"s\")),\n    (2**30, pa.time32(\"s\")),\n    (None, pa.time32(\"s\")),\n    (1, pa.time32(\"ms\")),\n    (2**30, pa.time32(\"ms\")),\n    (None, pa.time32(\"ms\")),\n    (1, pa.time64(\"us\")),\n    (2**62, pa.time64(\"us\")),\n    (None, pa.time64(\"us\")),\n    (1, pa.time64(\"ns\")),\n    (2**62, pa.time64(\"ns\")),\n    (None, pa.time64(\"ns\")),\n    (1, pa.date32()),\n    (2**30, pa.date32()),\n    (None, pa.date32()),\n    (1, pa.date64()),\n    (2**62, pa.date64()),\n    (None, pa.date64()),\n    (1, pa.timestamp(\"ns\")),\n    (2**62, pa.timestamp(\"ns\")),\n    (None, pa.timestamp(\"ns\")),\n    (1, pa.duration(\"ns\")),\n    (2**62, pa.duration(\"ns\")),\n    (None, pa.duration(\"ns\")),\n    ((1, 2, -3), pa.month_day_nano_interval()),\n    (None, pa.month_day_nano_interval()),\n])\ndef test_temporal_values(value, time_type: pa.DataType):\n    time_scalar = pa.scalar(value, type=time_type)\n    time_scalar.validate(full=True)\n    assert time_scalar.value == value\n\n\ndef test_cast():\n    val = pa.scalar(5, type='int8')\n    assert val.cast('int64') == pa.scalar(5, type='int64')\n    assert val.cast('uint32') == pa.scalar(5, type='uint32')\n    assert val.cast('string') == pa.scalar('5', type='string')\n    with pytest.raises(ValueError):\n        pa.scalar('foo').cast('int32')\n\n\n@pytest.mark.skipif(sys.platform == \"win32\" and not util.windows_has_tzdata(),\n                    reason=\"Timezone database is not installed on Windows\")\ndef test_cast_timestamp_to_string():\n    # GH-35370\n    pytest.importorskip(\"pytz\")\n    import pytz\n    dt = datetime.datetime(2000, 1, 1, 0, 0, 0, tzinfo=pytz.utc)\n    ts = pa.scalar(dt, type=pa.timestamp(\"ns\", tz=\"UTC\"))\n    assert ts.cast(pa.string()) == pa.scalar('2000-01-01 00:00:00.000000000Z')\n\n\ndef test_cast_float_to_int():\n    # GH-35040\n    float_scalar = pa.scalar(1.5, type=pa.float64())\n    unsafe_cast = float_scalar.cast(pa.int64(), safe=False)\n    expected_unsafe_cast = pa.scalar(1, type=pa.int64())\n    assert unsafe_cast == expected_unsafe_cast\n    with pytest.raises(pa.ArrowInvalid):\n        float_scalar.cast(pa.int64())  # verify default is safe cast\n\n\ndef test_cast_int_to_float():\n    # GH-34901\n    int_scalar = pa.scalar(18014398509481983, type=pa.int64())\n    unsafe_cast = int_scalar.cast(pa.float64(), safe=False)\n    expected_unsafe_cast = pa.scalar(18014398509481983.0, type=pa.float64())\n    assert unsafe_cast == expected_unsafe_cast\n    with pytest.raises(pa.ArrowInvalid):\n        int_scalar.cast(pa.float64())  # verify default is safe cast\n\n\n@pytest.mark.parametrize(\"typ\", [pa.date32(), pa.date64()])\ndef test_cast_string_to_date(typ):\n    scalar = pa.scalar('2021-01-01')\n    result = scalar.cast(typ)\n    assert result == pa.scalar(datetime.date(2021, 1, 1), type=typ)\n\n\n@pytest.mark.pandas\ndef test_timestamp():\n    import pandas as pd\n    arr = pd.date_range('2000-01-01 12:34:56', periods=10).values\n\n    units = ['ns', 'us', 'ms', 's']\n\n    for i, unit in enumerate(units):\n        dtype = 'datetime64[{}]'.format(unit)\n        arrow_arr = pa.Array.from_pandas(arr.astype(dtype))\n        expected = pd.Timestamp('2000-01-01 12:34:56')\n\n        assert arrow_arr[0].as_py() == expected\n        assert arrow_arr[0].value * 1000**i == expected.value\n\n        tz = 'America/New_York'\n        arrow_type = pa.timestamp(unit, tz=tz)\n\n        dtype = 'datetime64[{}]'.format(unit)\n        arrow_arr = pa.Array.from_pandas(arr.astype(dtype), type=arrow_type)\n        expected = (pd.Timestamp('2000-01-01 12:34:56')\n                    .tz_localize('utc')\n                    .tz_convert(tz))\n\n        assert arrow_arr[0].as_py() == expected\n        assert arrow_arr[0].value * 1000**i == expected.value\n\n\n@pytest.mark.nopandas\ndef test_timestamp_nanos_nopandas():\n    # ARROW-5450\n    pytest.importorskip(\"pytz\")\n    import pytz\n    tz = 'America/New_York'\n    ty = pa.timestamp('ns', tz=tz)\n\n    # 2000-01-01 00:00:00 + 1 microsecond\n    s = pa.scalar(946684800000000000 + 1000, type=ty)\n\n    tzinfo = pytz.timezone(tz)\n    expected = datetime.datetime(2000, 1, 1, microsecond=1, tzinfo=tzinfo)\n    expected = tzinfo.fromutc(expected)\n    result = s.as_py()\n    assert result == expected\n    assert result.year == 1999\n    assert result.hour == 19\n\n    # Non-zero nanos yields ValueError\n    s = pa.scalar(946684800000000001, type=ty)\n    with pytest.raises(ValueError):\n        s.as_py()\n\n\ndef test_timestamp_no_overflow():\n    # ARROW-5450\n    pytest.importorskip(\"pytz\")\n    import pytz\n\n    timestamps = [\n        datetime.datetime(1, 1, 1, 0, 0, 0, tzinfo=pytz.utc),\n        datetime.datetime(9999, 12, 31, 23, 59, 59, 999999, tzinfo=pytz.utc),\n        datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=pytz.utc),\n    ]\n    for ts in timestamps:\n        s = pa.scalar(ts, type=pa.timestamp(\"us\", tz=\"UTC\"))\n        assert s.as_py() == ts\n\n\ndef test_timestamp_fixed_offset_print():\n    # ARROW-13896\n    pytest.importorskip(\"pytz\")\n    arr = pa.array([0], pa.timestamp('s', tz='+02:00'))\n    assert str(arr[0]) == \"1970-01-01 02:00:00+02:00\"\n\n\ndef test_duration():\n    arr = np.array([0, 3600000000000], dtype='timedelta64[ns]')\n\n    units = ['us', 'ms', 's']\n\n    for i, unit in enumerate(units):\n        dtype = 'timedelta64[{}]'.format(unit)\n        arrow_arr = pa.array(arr.astype(dtype))\n        expected = datetime.timedelta(seconds=60*60)\n        assert isinstance(arrow_arr[1].as_py(), datetime.timedelta)\n        assert arrow_arr[1].as_py() == expected\n        assert (arrow_arr[1].value * 1000**(i+1) ==\n                expected.total_seconds() * 1e9)\n\n\n@pytest.mark.pandas\ndef test_duration_nanos_pandas():\n    import pandas as pd\n    arr = pa.array([0, 3600000000000], type=pa.duration('ns'))\n    expected = pd.Timedelta('1 hour')\n    assert isinstance(arr[1].as_py(), pd.Timedelta)\n    assert arr[1].as_py() == expected\n    assert arr[1].value == expected.value\n\n    # Non-zero nanos work fine\n    arr = pa.array([946684800000000001], type=pa.duration('ns'))\n    assert arr[0].as_py() == pd.Timedelta(946684800000000001, unit='ns')\n\n\n@pytest.mark.nopandas\ndef test_duration_nanos_nopandas():\n    arr = pa.array([0, 3600000000000], pa.duration('ns'))\n    expected = datetime.timedelta(seconds=60*60)\n    assert isinstance(arr[1].as_py(), datetime.timedelta)\n    assert arr[1].as_py() == expected\n    assert arr[1].value == expected.total_seconds() * 1e9\n\n    # Non-zero nanos yields ValueError\n    arr = pa.array([946684800000000001], type=pa.duration('ns'))\n    with pytest.raises(ValueError):\n        arr[0].as_py()\n\n\ndef test_month_day_nano_interval():\n    triple = pa.MonthDayNano([-3600, 1800, -50])\n    arr = pa.array([triple])\n    assert isinstance(arr[0].as_py(), pa.MonthDayNano)\n    assert arr[0].as_py() == triple\n    assert arr[0].value == triple\n\n\n@pytest.mark.parametrize('value', ['foo', 'ma\u00f1ana'])\n@pytest.mark.parametrize(('ty', 'scalar_typ'), [\n    (pa.string(), pa.StringScalar),\n    (pa.large_string(), pa.LargeStringScalar),\n    (pa.string_view(), pa.StringViewScalar),\n])\ndef test_string(value, ty, scalar_typ):\n    s = pa.scalar(value, type=ty)\n    assert isinstance(s, scalar_typ)\n    assert s.as_py() == value\n    assert s.as_py() != 'something'\n    assert repr(value) in repr(s)\n    assert str(s) == str(value)\n\n    buf = s.as_buffer()\n    assert isinstance(buf, pa.Buffer)\n    assert buf.to_pybytes() == value.encode()\n\n\n@pytest.mark.parametrize('value', [b'foo', b'bar'])\n@pytest.mark.parametrize(('ty', 'scalar_typ'), [\n    (pa.binary(), pa.BinaryScalar),\n    (pa.large_binary(), pa.LargeBinaryScalar),\n    (pa.binary_view(), pa.BinaryViewScalar),\n])\ndef test_binary(value, ty, scalar_typ):\n    s = pa.scalar(value, type=ty)\n    assert isinstance(s, scalar_typ)\n    assert s.as_py() == value\n    assert str(s) == str(value)\n    assert repr(value) in repr(s)\n    assert s.as_py() == value\n    assert s != b'xxxxx'\n\n    buf = s.as_buffer()\n    assert isinstance(buf, pa.Buffer)\n    assert buf.to_pybytes() == value\n\n\ndef test_fixed_size_binary():\n    s = pa.scalar(b'foof', type=pa.binary(4))\n    assert isinstance(s, pa.FixedSizeBinaryScalar)\n    assert s.as_py() == b'foof'\n\n    with pytest.raises(pa.ArrowInvalid):\n        pa.scalar(b'foof5', type=pa.binary(4))\n\n\n@pytest.mark.parametrize(('ty', 'klass'), [\n    (pa.list_(pa.string()), pa.ListScalar),\n    (pa.large_list(pa.string()), pa.LargeListScalar),\n    (pa.list_view(pa.string()), pa.ListViewScalar),\n    (pa.large_list_view(pa.string()), pa.LargeListViewScalar)\n])\ndef test_list(ty, klass):\n    v = ['foo', None]\n    s = pa.scalar(v, type=ty)\n    assert s.type == ty\n    assert len(s) == 2\n    assert isinstance(s.values, pa.Array)\n    assert s.values.to_pylist() == v\n    assert isinstance(s, klass)\n    assert repr(v) in repr(s)\n    assert s.as_py() == v\n    assert s[0].as_py() == 'foo'\n    assert s[1].as_py() is None\n    assert s[-1] == s[1]\n    assert s[-2] == s[0]\n    with pytest.raises(IndexError):\n        s[-3]\n    with pytest.raises(IndexError):\n        s[2]\n\n\n@pytest.mark.parametrize('ty', [\n    pa.list_(pa.int64()),\n    pa.large_list(pa.int64()),\n    pa.list_view(pa.int64()),\n    pa.large_list_view(pa.int64()),\n    None\n])\ndef test_list_from_numpy(ty):\n    s = pa.scalar(np.array([1, 2, 3], dtype=np.int64()), type=ty)\n    if ty is None:\n        ty = pa.list_(pa.int64())  # expected inferred type\n    assert s.type == ty\n    assert s.as_py() == [1, 2, 3]\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('factory', [\n    pa.list_,\n    pa.large_list,\n    pa.list_view,\n    pa.large_list_view\n])\ndef test_list_from_pandas(factory):\n    import pandas as pd\n\n    s = pa.scalar(pd.Series([1, 2, 3]))\n    assert s.as_py() == [1, 2, 3]\n\n    cases = [\n        (np.nan, 'null'),\n        (['string', np.nan], factory(pa.binary())),\n        (['string', np.nan], factory(pa.utf8())),\n        ([b'string', np.nan], factory(pa.binary(6))),\n        ([True, np.nan], factory(pa.bool_())),\n        ([decimal.Decimal('0'), np.nan], factory(pa.decimal128(12, 2))),\n    ]\n    for case, ty in cases:\n        # Both types of exceptions are raised. May want to clean that up\n        with pytest.raises((ValueError, TypeError)):\n            pa.scalar(case, type=ty)\n\n        # from_pandas option suppresses failure\n        s = pa.scalar(case, type=ty, from_pandas=True)\n\n\ndef test_fixed_size_list():\n    s = pa.scalar([1, None, 3], type=pa.list_(pa.int64(), 3))\n\n    assert len(s) == 3\n    assert isinstance(s, pa.FixedSizeListScalar)\n    assert repr(s) == \"<pyarrow.FixedSizeListScalar: [1, None, 3]>\"\n    assert s.as_py() == [1, None, 3]\n    assert s[0].as_py() == 1\n    assert s[1].as_py() is None\n    assert s[-1] == s[2]\n    with pytest.raises(IndexError):\n        s[-4]\n    with pytest.raises(IndexError):\n        s[3]\n\n\ndef test_struct():\n    ty = pa.struct([\n        pa.field('x', pa.int16()),\n        pa.field('y', pa.float32())\n    ])\n\n    v = {'x': 2, 'y': 3.5}\n    s = pa.scalar(v, type=ty)\n    assert list(s) == list(s.keys()) == ['x', 'y']\n    assert list(s.values()) == [\n        pa.scalar(2, type=pa.int16()),\n        pa.scalar(3.5, type=pa.float32())\n    ]\n    assert list(s.items()) == [\n        ('x', pa.scalar(2, type=pa.int16())),\n        ('y', pa.scalar(3.5, type=pa.float32()))\n    ]\n    assert 'x' in s\n    assert 'y' in s\n    assert 'z' not in s\n    assert 0 not in s\n\n    assert s.as_py() == v\n    assert repr(s) != repr(v)\n    assert repr(s.as_py()) == repr(v)\n    assert len(s) == 2\n    assert isinstance(s['x'], pa.Int16Scalar)\n    assert isinstance(s['y'], pa.FloatScalar)\n    assert s['x'].as_py() == 2\n    assert s['y'].as_py() == 3.5\n\n    with pytest.raises(KeyError):\n        s['nonexistent']\n\n    s = pa.scalar(None, type=ty)\n    assert list(s) == list(s.keys()) == ['x', 'y']\n    assert s.as_py() is None\n    assert 'x' in s\n    assert 'y' in s\n    assert isinstance(s['x'], pa.Int16Scalar)\n    assert isinstance(s['y'], pa.FloatScalar)\n    assert s['x'].is_valid is False\n    assert s['y'].is_valid is False\n    assert s['x'].as_py() is None\n    assert s['y'].as_py() is None\n\n\ndef test_struct_duplicate_fields():\n    ty = pa.struct([\n        pa.field('x', pa.int16()),\n        pa.field('y', pa.float32()),\n        pa.field('x', pa.int64()),\n    ])\n    s = pa.scalar([('x', 1), ('y', 2.0), ('x', 3)], type=ty)\n\n    assert list(s) == list(s.keys()) == ['x', 'y', 'x']\n    assert len(s) == 3\n    assert s == s\n    assert list(s.items()) == [\n        ('x', pa.scalar(1, pa.int16())),\n        ('y', pa.scalar(2.0, pa.float32())),\n        ('x', pa.scalar(3, pa.int64()))\n    ]\n\n    assert 'x' in s\n    assert 'y' in s\n    assert 'z' not in s\n    assert 0 not in s\n\n    # getitem with field names fails for duplicate fields, works for others\n    with pytest.raises(KeyError):\n        s['x']\n\n    assert isinstance(s['y'], pa.FloatScalar)\n    assert s['y'].as_py() == 2.0\n\n    # getitem with integer index works for all fields\n    assert isinstance(s[0], pa.Int16Scalar)\n    assert s[0].as_py() == 1\n    assert isinstance(s[1], pa.FloatScalar)\n    assert s[1].as_py() == 2.0\n    assert isinstance(s[2], pa.Int64Scalar)\n    assert s[2].as_py() == 3\n\n    assert \"pyarrow.StructScalar\" in repr(s)\n\n    with pytest.raises(ValueError, match=\"duplicate field names\"):\n        s.as_py()\n\n\ndef test_map(pickle_module):\n    ty = pa.map_(pa.string(), pa.int8())\n    v = [('a', 1), ('b', 2)]\n    s = pa.scalar(v, type=ty)\n\n    assert len(s) == 2\n    assert isinstance(s, pa.MapScalar)\n    assert isinstance(s.values, pa.Array)\n    assert repr(s) == \"<pyarrow.MapScalar: [('a', 1), ('b', 2)]>\"\n    assert s.values.to_pylist() == [\n        {'key': 'a', 'value': 1},\n        {'key': 'b', 'value': 2}\n    ]\n\n    # test iteration\n    for i, j in zip(s, v):\n        assert i == j\n\n    # test iteration with missing values\n    for _ in pa.scalar(None, type=ty):\n        pass\n\n    assert s.as_py() == v\n    assert s[1] == (\n        pa.scalar('b', type=pa.string()),\n        pa.scalar(2, type=pa.int8())\n    )\n    assert s[-1] == s[1]\n    assert s[-2] == s[0]\n    with pytest.raises(IndexError):\n        s[-3]\n    with pytest.raises(IndexError):\n        s[2]\n\n    restored = pickle_module.loads(pickle_module.dumps(s))\n    assert restored.equals(s)\n\n\ndef test_dictionary(pickle_module):\n    indices = pa.array([2, None, 1, 2, 0, None])\n    dictionary = pa.array(['foo', 'bar', 'baz'])\n\n    arr = pa.DictionaryArray.from_arrays(indices, dictionary)\n    expected = ['baz', None, 'bar', 'baz', 'foo', None]\n    assert arr.to_pylist() == expected\n\n    for j, (i, v) in enumerate(zip(indices, expected)):\n        s = arr[j]\n\n        assert s.as_py() == v\n        assert s.value.as_py() == v\n        assert s.index.equals(i)\n        assert s.dictionary.equals(dictionary)\n\n        restored = pickle_module.loads(pickle_module.dumps(s))\n        assert restored.equals(s)\n\n\ndef test_run_end_encoded():\n    run_ends = [3, 5, 10, 12, 19]\n    values = [1, 2, 1, None, 3]\n    arr = pa.RunEndEncodedArray.from_arrays(run_ends, values)\n\n    scalar = arr[0]\n    assert isinstance(scalar, pa.RunEndEncodedScalar)\n    assert isinstance(scalar.value, pa.Int64Scalar)\n    assert scalar.value == pa.array(values)[0]\n    assert scalar.as_py() == 1\n\n    # null -> .value is still a scalar, as_py returns None\n    scalar = arr[10]\n    assert isinstance(scalar.value, pa.Int64Scalar)\n    assert scalar.as_py() is None\n\n    # constructing a scalar directly doesn't work yet\n    with pytest.raises(NotImplementedError):\n        pa.scalar(1, pa.run_end_encoded(pa.int64(), pa.int64()))\n\n\ndef test_union(pickle_module):\n    # sparse\n    arr = pa.UnionArray.from_sparse(\n        pa.array([0, 0, 1, 1], type=pa.int8()),\n        [\n            pa.array([\"a\", \"b\", \"c\", \"d\"]),\n            pa.array([1, 2, 3, 4])\n        ]\n    )\n    for s in arr:\n        s.validate(full=True)\n        assert isinstance(s, pa.UnionScalar)\n        assert s.type.equals(arr.type)\n        assert s.is_valid is True\n        with pytest.raises(pa.ArrowNotImplementedError):\n            pickle_module.loads(pickle_module.dumps(s))\n\n    assert arr[0].type_code == 0\n    assert arr[0].as_py() == \"a\"\n    assert arr[1].type_code == 0\n    assert arr[1].as_py() == \"b\"\n    assert arr[2].type_code == 1\n    assert arr[2].as_py() == 3\n    assert arr[3].type_code == 1\n    assert arr[3].as_py() == 4\n\n    # dense\n    arr = pa.UnionArray.from_dense(\n        types=pa.array([0, 1, 0, 0, 1, 1, 0], type='int8'),\n        value_offsets=pa.array([0, 0, 2, 1, 1, 2, 3], type='int32'),\n        children=[\n            pa.array([b'a', b'b', b'c', b'd'], type='binary'),\n            pa.array([1, 2, 3], type='int64')\n        ]\n    )\n    for s in arr:\n        s.validate(full=True)\n        assert isinstance(s, pa.UnionScalar)\n        assert s.type.equals(arr.type)\n        assert s.is_valid is True\n        with pytest.raises(pa.ArrowNotImplementedError):\n            pickle_module.loads(pickle_module.dumps(s))\n\n    assert arr[0].type_code == 0\n    assert arr[0].as_py() == b'a'\n    assert arr[5].type_code == 1\n    assert arr[5].as_py() == 3\n\n\ndef test_map_scalar_as_py_with_custom_field_name():\n    \"\"\"\n    Check we can call `MapScalar.as_py` with custom field names\n\n    See https://github.com/apache/arrow/issues/36809\n    \"\"\"\n    assert pa.scalar(\n        [(\"foo\", \"bar\")],\n        pa.map_(\n            pa.string(),\n            pa.string()\n        ),\n    ).as_py() == [(\"foo\", \"bar\")]\n\n    assert pa.scalar(\n        [(\"foo\", \"bar\")],\n        pa.map_(\n            pa.field(\"custom_key\", pa.string(), nullable=False),\n            pa.field(\"custom_value\", pa.string()),\n        ),\n    ).as_py() == [(\"foo\", \"bar\")]\n", "python/pyarrow/tests/test_dataset.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport datetime\nimport os\nimport pathlib\nimport posixpath\nimport sys\nimport tempfile\nimport textwrap\nimport threading\nimport time\nfrom shutil import copytree\nfrom urllib.parse import quote\n\nimport numpy as np\nimport pytest\n\nimport pyarrow as pa\nimport pyarrow.compute as pc\nimport pyarrow.csv\nimport pyarrow.feather\nimport pyarrow.fs as fs\nimport pyarrow.json\nfrom pyarrow.tests.util import (FSProtocolClass, ProxyHandler,\n                                _configure_s3_limited_user, _filesystem_uri,\n                                change_cwd)\n\ntry:\n    import pandas as pd\nexcept ImportError:\n    pd = None\n\ntry:\n    import pyarrow.dataset as ds\nexcept ImportError:\n    ds = None\n\ntry:\n    import pyarrow.parquet as pq\nexcept ImportError:\n    pq = None\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not dataset'\npytestmark = pytest.mark.dataset\n\n\ndef _generate_data(n):\n    import datetime\n    import itertools\n\n    day = datetime.datetime(2000, 1, 1)\n    interval = datetime.timedelta(days=5)\n    colors = itertools.cycle(['green', 'blue', 'yellow', 'red', 'orange'])\n\n    data = []\n    for i in range(n):\n        data.append((day, i, float(i), next(colors)))\n        day += interval\n\n    return pd.DataFrame(data, columns=['date', 'index', 'value', 'color'])\n\n\ndef _table_from_pandas(df):\n    schema = pa.schema([\n        pa.field('date', pa.date32()),\n        pa.field('index', pa.int64()),\n        pa.field('value', pa.float64()),\n        pa.field('color', pa.string()),\n    ])\n    table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n    return table.replace_schema_metadata()\n\n\ndef assert_dataset_fragment_convenience_methods(dataset):\n    # FileFragment convenience methods\n    for fragment in dataset.get_fragments():\n        with fragment.open() as nf:\n            assert isinstance(nf, pa.NativeFile)\n            assert not nf.closed\n            assert nf.seekable()\n            assert nf.readable()\n            assert not nf.writable()\n\n\n@pytest.fixture\ndef mockfs():\n    mockfs = fs._MockFileSystem()\n\n    directories = [\n        'subdir/1/xxx',\n        'subdir/2/yyy',\n    ]\n\n    for i, directory in enumerate(directories):\n        path = '{}/file{}.parquet'.format(directory, i)\n        mockfs.create_dir(directory)\n        with mockfs.open_output_stream(path) as out:\n            data = [\n                list(range(5)),\n                list(map(float, range(5))),\n                list(map(str, range(5))),\n                [i] * 5,\n                [{'a': j % 3, 'b': str(j % 3)} for j in range(5)],\n            ]\n            schema = pa.schema([\n                ('i64', pa.int64()),\n                ('f64', pa.float64()),\n                ('str', pa.string()),\n                ('const', pa.int64()),\n                ('struct', pa.struct({'a': pa.int64(), 'b': pa.string()})),\n            ])\n            batch = pa.record_batch(data, schema=schema)\n            table = pa.Table.from_batches([batch])\n\n            pq.write_table(table, out)\n\n    return mockfs\n\n\n@pytest.fixture\ndef open_logging_fs(monkeypatch):\n    from pyarrow.fs import LocalFileSystem, PyFileSystem\n\n    from .test_fs import ProxyHandler\n\n    localfs = LocalFileSystem()\n\n    def normalized(paths):\n        return {localfs.normalize_path(str(p)) for p in paths}\n\n    opened = set()\n\n    def open_input_file(self, path):\n        path = localfs.normalize_path(str(path))\n        opened.add(path)\n        return self._fs.open_input_file(path)\n\n    # patch proxyhandler to log calls to open_input_file\n    monkeypatch.setattr(ProxyHandler, \"open_input_file\", open_input_file)\n    fs = PyFileSystem(ProxyHandler(localfs))\n\n    @contextlib.contextmanager\n    def assert_opens(expected_opened):\n        opened.clear()\n        try:\n            yield\n        finally:\n            assert normalized(opened) == normalized(expected_opened)\n\n    return fs, assert_opens\n\n\n@pytest.fixture(scope='module')\ndef multisourcefs(request):\n    request.config.pyarrow.requires('pandas')\n    request.config.pyarrow.requires('parquet')\n\n    df = _generate_data(1000)\n    mockfs = fs._MockFileSystem()\n\n    # simply split the dataframe into four chunks to construct a data source\n    # from each chunk into its own directory\n    n = len(df)\n    df_a, df_b, df_c, df_d = [df.iloc[i:i+n//4] for i in range(0, n, n//4)]\n\n    # create a directory containing a flat sequence of parquet files without\n    # any partitioning involved\n    mockfs.create_dir('plain')\n    n = len(df_a)\n    for i, chunk in enumerate([df_a.iloc[i:i+n//10] for i in range(0, n, n//10)]):\n        path = 'plain/chunk-{}.parquet'.format(i)\n        with mockfs.open_output_stream(path) as out:\n            pq.write_table(_table_from_pandas(chunk), out)\n\n    # create one with schema partitioning by weekday and color\n    mockfs.create_dir('schema')\n    for part, chunk in df_b.groupby([df_b.date.dt.dayofweek, df_b.color]):\n        folder = 'schema/{}/{}'.format(*part)\n        path = '{}/chunk.parquet'.format(folder)\n        mockfs.create_dir(folder)\n        with mockfs.open_output_stream(path) as out:\n            pq.write_table(_table_from_pandas(chunk), out)\n\n    # create one with hive partitioning by year and month\n    mockfs.create_dir('hive')\n    for part, chunk in df_c.groupby([df_c.date.dt.year, df_c.date.dt.month]):\n        folder = 'hive/year={}/month={}'.format(*part)\n        path = '{}/chunk.parquet'.format(folder)\n        mockfs.create_dir(folder)\n        with mockfs.open_output_stream(path) as out:\n            pq.write_table(_table_from_pandas(chunk), out)\n\n    # create one with hive partitioning by color\n    mockfs.create_dir('hive_color')\n    for part, chunk in df_d.groupby(\"color\"):\n        folder = 'hive_color/color={}'.format(part)\n        path = '{}/chunk.parquet'.format(folder)\n        mockfs.create_dir(folder)\n        with mockfs.open_output_stream(path) as out:\n            pq.write_table(_table_from_pandas(chunk), out)\n\n    return mockfs\n\n\n@pytest.fixture\ndef dataset(mockfs):\n    format = ds.ParquetFileFormat()\n    selector = fs.FileSelector('subdir', recursive=True)\n    options = ds.FileSystemFactoryOptions('subdir')\n    options.partitioning = ds.DirectoryPartitioning(\n        pa.schema([\n            pa.field('group', pa.int32()),\n            pa.field('key', pa.string())\n        ])\n    )\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    return factory.finish()\n\n\n@pytest.fixture(params=[\n    (True),\n    (False)\n], ids=['threaded', 'serial'])\ndef dataset_reader(request):\n    '''\n    Fixture which allows dataset scanning operations to be\n    run with/without threads\n    '''\n    use_threads = request.param\n\n    class reader:\n\n        def __init__(self):\n            self.use_threads = use_threads\n\n        def _patch_kwargs(self, kwargs):\n            if 'use_threads' in kwargs:\n                raise Exception(\n                    ('Invalid use of dataset_reader, do not specify'\n                     ' use_threads'))\n            kwargs['use_threads'] = use_threads\n\n        def to_table(self, dataset, **kwargs):\n            self._patch_kwargs(kwargs)\n            return dataset.to_table(**kwargs)\n\n        def to_batches(self, dataset, **kwargs):\n            self._patch_kwargs(kwargs)\n            return dataset.to_batches(**kwargs)\n\n        def scanner(self, dataset, **kwargs):\n            self._patch_kwargs(kwargs)\n            return dataset.scanner(**kwargs)\n\n        def head(self, dataset, num_rows, **kwargs):\n            self._patch_kwargs(kwargs)\n            return dataset.head(num_rows, **kwargs)\n\n        def take(self, dataset, indices, **kwargs):\n            self._patch_kwargs(kwargs)\n            return dataset.take(indices, **kwargs)\n\n        def count_rows(self, dataset, **kwargs):\n            self._patch_kwargs(kwargs)\n            return dataset.count_rows(**kwargs)\n\n    return reader()\n\n\n@pytest.mark.parquet\ndef test_filesystem_dataset(mockfs):\n    schema = pa.schema([\n        pa.field('const', pa.int64())\n    ])\n    file_format = ds.ParquetFileFormat()\n    paths = ['subdir/1/xxx/file0.parquet', 'subdir/2/yyy/file1.parquet']\n    partitions = [ds.field('part') == x for x in range(1, 3)]\n    fragments = [file_format.make_fragment(path, mockfs, part)\n                 for path, part in zip(paths, partitions)]\n    root_partition = ds.field('level') == ds.scalar(1337)\n\n    dataset_from_fragments = ds.FileSystemDataset(\n        fragments, schema=schema, format=file_format,\n        filesystem=mockfs, root_partition=root_partition,\n    )\n    dataset_from_paths = ds.FileSystemDataset.from_paths(\n        paths, schema=schema, format=file_format, filesystem=mockfs,\n        partitions=partitions, root_partition=root_partition,\n    )\n\n    for dataset in [dataset_from_fragments, dataset_from_paths]:\n        assert isinstance(dataset, ds.FileSystemDataset)\n        assert isinstance(dataset.format, ds.ParquetFileFormat)\n        assert dataset.partition_expression.equals(root_partition)\n        assert set(dataset.files) == set(paths)\n\n        fragments = list(dataset.get_fragments())\n        for fragment, partition, path in zip(fragments, partitions, paths):\n            assert fragment.partition_expression.equals(partition)\n            assert fragment.path == path\n            assert isinstance(fragment.format, ds.ParquetFileFormat)\n            assert isinstance(fragment, ds.ParquetFileFragment)\n            assert fragment.row_groups == [0]\n            assert fragment.num_row_groups == 1\n\n            row_group_fragments = list(fragment.split_by_row_group())\n            assert fragment.num_row_groups == len(row_group_fragments) == 1\n            assert isinstance(row_group_fragments[0], ds.ParquetFileFragment)\n            assert row_group_fragments[0].path == path\n            assert row_group_fragments[0].row_groups == [0]\n            assert row_group_fragments[0].num_row_groups == 1\n\n        fragments = list(dataset.get_fragments(filter=ds.field(\"const\") == 0))\n        assert len(fragments) == 2\n\n    # the root_partition keyword has a default\n    dataset = ds.FileSystemDataset(\n        fragments, schema=schema, format=file_format, filesystem=mockfs\n    )\n    assert dataset.partition_expression.equals(ds.scalar(True))\n\n    # from_paths partitions have defaults\n    dataset = ds.FileSystemDataset.from_paths(\n        paths, schema=schema, format=file_format, filesystem=mockfs\n    )\n    assert dataset.partition_expression.equals(ds.scalar(True))\n    for fragment in dataset.get_fragments():\n        assert fragment.partition_expression.equals(ds.scalar(True))\n\n    # validation of required arguments\n    with pytest.raises(TypeError, match=\"incorrect type\"):\n        ds.FileSystemDataset(fragments, file_format, schema)\n    # validation of root_partition\n    with pytest.raises(TypeError, match=\"incorrect type\"):\n        ds.FileSystemDataset(fragments, schema=schema,\n                             format=file_format, root_partition=1)\n    # missing required argument in from_paths\n    with pytest.raises(TypeError, match=\"incorrect type\"):\n        ds.FileSystemDataset.from_paths(fragments, format=file_format)\n\n\ndef test_filesystem_dataset_no_filesystem_interaction(dataset_reader):\n    # ARROW-8283\n    schema = pa.schema([\n        pa.field('f1', pa.int64())\n    ])\n    file_format = ds.IpcFileFormat()\n    paths = ['nonexistingfile.arrow']\n\n    # creating the dataset itself doesn't raise\n    dataset = ds.FileSystemDataset.from_paths(\n        paths, schema=schema, format=file_format,\n        filesystem=fs.LocalFileSystem(),\n    )\n\n    # getting fragments also doesn't raise\n    dataset.get_fragments()\n\n    # scanning does raise\n    with pytest.raises(FileNotFoundError):\n        dataset_reader.to_table(dataset)\n\n\n@pytest.mark.parquet\ndef test_dataset(dataset, dataset_reader):\n    assert isinstance(dataset, ds.Dataset)\n    assert isinstance(dataset.schema, pa.Schema)\n\n    # TODO(kszucs): test non-boolean Exprs for filter do raise\n    expected_i64 = pa.array([0, 1, 2, 3, 4], type=pa.int64())\n    expected_f64 = pa.array([0, 1, 2, 3, 4], type=pa.float64())\n\n    for batch in dataset_reader.to_batches(dataset):\n        assert isinstance(batch, pa.RecordBatch)\n        assert batch.column(0).equals(expected_i64)\n        assert batch.column(1).equals(expected_f64)\n\n    for batch in dataset_reader.scanner(dataset).scan_batches():\n        assert isinstance(batch, ds.TaggedRecordBatch)\n        assert isinstance(batch.fragment, ds.Fragment)\n\n    table = dataset_reader.to_table(dataset)\n    assert isinstance(table, pa.Table)\n    assert len(table) == 10\n\n    condition = ds.field('i64') == 1\n    result = dataset.to_table(use_threads=True, filter=condition)\n    # Don't rely on the scanning order\n    result = result.sort_by('group').to_pydict()\n\n    assert result['i64'] == [1, 1]\n    assert result['f64'] == [1., 1.]\n    assert sorted(result['group']) == [1, 2]\n    assert sorted(result['key']) == ['xxx', 'yyy']\n\n    # Filtering on a nested field ref\n    condition = ds.field(('struct', 'b')) == '1'\n    result = dataset.to_table(use_threads=True, filter=condition)\n    result = result.sort_by('group').to_pydict()\n\n    assert result['i64'] == [1, 4, 1, 4]\n    assert result['f64'] == [1.0, 4.0, 1.0, 4.0]\n    assert result['group'] == [1, 1, 2, 2]\n    assert result['key'] == ['xxx', 'xxx', 'yyy', 'yyy']\n\n    # Projecting on a nested field ref expression\n    projection = {\n        'i64': ds.field('i64'),\n        'f64': ds.field('f64'),\n        'new': ds.field(('struct', 'b')) == '1',\n    }\n    result = dataset.to_table(use_threads=True, columns=projection)\n    result = result.sort_by('i64').to_pydict()\n\n    assert list(result) == ['i64', 'f64', 'new']\n    assert result['i64'] == [0, 0, 1, 1, 2, 2, 3, 3, 4, 4]\n    assert result['f64'] == [0.0, 0.0, 1.0, 1.0,\n                             2.0, 2.0, 3.0, 3.0, 4.0, 4.0]\n    assert result['new'] == [False, False, True, True, False, False,\n                             False, False, True, True]\n    assert_dataset_fragment_convenience_methods(dataset)\n\n\n@pytest.mark.parquet\ndef test_scanner_options(dataset):\n    scanner = dataset.to_batches(fragment_readahead=16, batch_readahead=8)\n    batch = next(scanner)\n    assert batch.num_columns == 7\n\n\n@pytest.mark.parquet\ndef test_scanner(dataset, dataset_reader):\n    scanner = dataset_reader.scanner(\n        dataset, memory_pool=pa.default_memory_pool())\n    assert isinstance(scanner, ds.Scanner)\n\n    with pytest.raises(pa.ArrowInvalid):\n        dataset_reader.scanner(dataset, columns=['unknown'])\n\n    scanner = dataset_reader.scanner(dataset, columns=['i64'],\n                                     memory_pool=pa.default_memory_pool())\n    assert scanner.dataset_schema == dataset.schema\n    assert scanner.projected_schema == pa.schema([(\"i64\", pa.int64())])\n\n    assert isinstance(scanner, ds.Scanner)\n    table = scanner.to_table()\n    for batch in scanner.to_batches():\n        assert batch.schema == scanner.projected_schema\n        assert batch.num_columns == 1\n    assert table == scanner.to_reader().read_all()\n\n    assert table.schema == scanner.projected_schema\n    for i in range(table.num_rows):\n        indices = pa.array([i])\n        assert table.take(indices) == scanner.take(indices)\n    with pytest.raises(pa.ArrowIndexError):\n        scanner.take(pa.array([table.num_rows]))\n\n    assert table.num_rows == scanner.count_rows()\n\n    scanner = dataset_reader.scanner(dataset, columns=['__filename',\n                                                       '__fragment_index',\n                                                       '__batch_index',\n                                                       '__last_in_fragment'],\n                                     memory_pool=pa.default_memory_pool())\n    table = scanner.to_table()\n    expected_names = ['__filename', '__fragment_index',\n                      '__batch_index', '__last_in_fragment']\n    assert table.column_names == expected_names\n\n    sorted_table = table.sort_by('__fragment_index')\n    assert sorted_table['__filename'].to_pylist() == (\n        ['subdir/1/xxx/file0.parquet'] * 5 +\n        ['subdir/2/yyy/file1.parquet'] * 5)\n    assert sorted_table['__fragment_index'].to_pylist() == ([0] * 5 + [1] * 5)\n    assert sorted_table['__batch_index'].to_pylist() == [0] * 10\n    assert sorted_table['__last_in_fragment'].to_pylist() == [True] * 10\n\n\n@pytest.mark.parquet\ndef test_scanner_memory_pool(dataset):\n    # honor default pool - https://issues.apache.org/jira/browse/ARROW-18164\n    old_pool = pa.default_memory_pool()\n    # TODO(ARROW-18293) we should be able to use the proxy memory pool for\n    # for testing, but this crashes\n    # pool = pa.proxy_memory_pool(old_pool)\n    pool = pa.system_memory_pool()\n    pa.set_memory_pool(pool)\n\n    try:\n        allocated_before = pool.bytes_allocated()\n        scanner = ds.Scanner.from_dataset(dataset)\n        _ = scanner.to_table()\n        assert pool.bytes_allocated() > allocated_before\n    finally:\n        pa.set_memory_pool(old_pool)\n\n\n@pytest.mark.parquet\ndef test_head(dataset, dataset_reader):\n    result = dataset_reader.head(dataset, 0)\n    assert result == pa.Table.from_batches([], schema=dataset.schema)\n\n    result = dataset_reader.head(dataset, 1, columns=['i64']).to_pydict()\n    assert result == {'i64': [0]}\n\n    result = dataset_reader.head(dataset, 2, columns=['i64'],\n                                 filter=ds.field('i64') > 1).to_pydict()\n    assert result == {'i64': [2, 3]}\n\n    result = dataset_reader.head(dataset, 1024, columns=['i64']).to_pydict()\n    assert result == {'i64': list(range(5)) * 2}\n\n    fragment = next(dataset.get_fragments())\n    result = fragment.head(1, columns=['i64']).to_pydict()\n    assert result == {'i64': [0]}\n\n    result = fragment.head(1024, columns=['i64']).to_pydict()\n    assert result == {'i64': list(range(5))}\n\n\n@pytest.mark.parquet\ndef test_take(dataset, dataset_reader):\n    fragment = next(dataset.get_fragments())\n    for indices in [[1, 3], pa.array([1, 3])]:\n        expected = dataset_reader.to_table(fragment).take(indices)\n        assert dataset_reader.take(fragment, indices) == expected\n    with pytest.raises(IndexError):\n        dataset_reader.take(fragment, pa.array([5]))\n\n    for indices in [[1, 7], pa.array([1, 7])]:\n        assert dataset_reader.take(\n            dataset, indices) == dataset_reader.to_table(dataset).take(indices)\n    with pytest.raises(IndexError):\n        dataset_reader.take(dataset, pa.array([10]))\n\n\n@pytest.mark.parquet\ndef test_count_rows(dataset, dataset_reader):\n    fragment = next(dataset.get_fragments())\n    assert dataset_reader.count_rows(fragment) == 5\n    assert dataset_reader.count_rows(\n        fragment, filter=ds.field(\"i64\") == 4) == 1\n\n    assert dataset_reader.count_rows(dataset) == 10\n    # Filter on partition key\n    assert dataset_reader.count_rows(\n        dataset, filter=ds.field(\"group\") == 1) == 5\n    # Filter on data\n    assert dataset_reader.count_rows(dataset, filter=ds.field(\"i64\") >= 3) == 4\n    assert dataset_reader.count_rows(dataset, filter=ds.field(\"i64\") < 0) == 0\n\n\ndef test_abstract_classes():\n    classes = [\n        ds.FileFormat,\n        ds.Scanner,\n        ds.Partitioning,\n    ]\n    for klass in classes:\n        with pytest.raises(TypeError):\n            klass()\n\n\ndef test_partitioning():\n    schema = pa.schema([\n        pa.field('i64', pa.int64()),\n        pa.field('f64', pa.float64())\n    ])\n    for klass in [ds.DirectoryPartitioning, ds.HivePartitioning,\n                  ds.FilenamePartitioning]:\n        partitioning = klass(schema)\n        assert isinstance(partitioning, ds.Partitioning)\n        assert partitioning == klass(schema)\n        assert partitioning != \"other object\"\n\n    schema = pa.schema([\n        pa.field('group', pa.int64()),\n        pa.field('key', pa.float64())\n    ])\n    partitioning = ds.DirectoryPartitioning(schema)\n    assert len(partitioning.dictionaries) == 2\n    assert all(x is None for x in partitioning.dictionaries)\n    expr = partitioning.parse('/3/3.14/')\n    assert isinstance(expr, ds.Expression)\n\n    expected = (ds.field('group') == 3) & (ds.field('key') == 3.14)\n    assert expr.equals(expected)\n\n    with pytest.raises(pa.ArrowInvalid):\n        partitioning.parse('/prefix/3/aaa')\n\n    expr = partitioning.parse('/3/')\n    expected = ds.field('group') == 3\n    assert expr.equals(expected)\n\n    assert partitioning != ds.DirectoryPartitioning(schema, segment_encoding=\"none\")\n\n    schema = pa.schema([\n        pa.field('alpha', pa.int64()),\n        pa.field('beta', pa.int64())\n    ])\n    partitioning = ds.HivePartitioning(schema, null_fallback='xyz')\n    assert len(partitioning.dictionaries) == 2\n    assert all(x is None for x in partitioning.dictionaries)\n    expr = partitioning.parse('/alpha=0/beta=3/')\n    expected = (\n        (ds.field('alpha') == ds.scalar(0)) &\n        (ds.field('beta') == ds.scalar(3))\n    )\n    assert expr.equals(expected)\n\n    expr = partitioning.parse('/alpha=xyz/beta=3/')\n    expected = (\n        (ds.field('alpha').is_null() & (ds.field('beta') == ds.scalar(3)))\n    )\n    assert expr.equals(expected)\n\n    for shouldfail in ['/alpha=one/beta=2/', '/alpha=one/', '/beta=two/']:\n        with pytest.raises(pa.ArrowInvalid):\n            partitioning.parse(shouldfail)\n\n    assert partitioning != ds.HivePartitioning(schema, null_fallback='other')\n\n    schema = pa.schema([\n        pa.field('group', pa.int64()),\n        pa.field('key', pa.float64())\n    ])\n    partitioning = ds.FilenamePartitioning(schema)\n    assert len(partitioning.dictionaries) == 2\n    assert all(x is None for x in partitioning.dictionaries)\n    expr = partitioning.parse('3_3.14_')\n    assert isinstance(expr, ds.Expression)\n\n    expected = (ds.field('group') == 3) & (ds.field('key') == 3.14)\n    assert expr.equals(expected)\n\n    with pytest.raises(pa.ArrowInvalid):\n        partitioning.parse('prefix_3_aaa_')\n\n    assert partitioning != ds.FilenamePartitioning(schema, segment_encoding=\"none\")\n\n    schema = pa.schema([\n        pa.field('group', pa.int64()),\n        pa.field('key', pa.dictionary(pa.int8(), pa.string()))\n    ])\n    partitioning = ds.DirectoryPartitioning(\n        schema, dictionaries={\"key\": pa.array([\"first\", \"second\", \"third\"])}\n    )\n    assert partitioning.dictionaries[0] is None\n    assert partitioning.dictionaries[1].to_pylist() == [\n        \"first\", \"second\", \"third\"]\n    assert partitioning != ds.DirectoryPartitioning(schema, dictionaries=None)\n\n    partitioning = ds.FilenamePartitioning(\n        pa.schema([\n            pa.field('group', pa.int64()),\n            pa.field('key', pa.dictionary(pa.int8(), pa.string()))\n        ]),\n        dictionaries={\n            \"key\": pa.array([\"first\", \"second\", \"third\"]),\n        })\n    assert partitioning.dictionaries[0] is None\n    assert partitioning.dictionaries[1].to_pylist() == [\n        \"first\", \"second\", \"third\"]\n\n    # test partitioning roundtrip\n    table = pa.table([\n        pa.array(range(20)), pa.array(np.random.randn(20)),\n        pa.array(np.repeat(['a', 'b'], 10))],\n        names=[\"f1\", \"f2\", \"part\"]\n    )\n    partitioning_schema = pa.schema([(\"part\", pa.string())])\n    for klass in [ds.DirectoryPartitioning, ds.HivePartitioning,\n                  ds.FilenamePartitioning]:\n        with tempfile.TemporaryDirectory() as tempdir:\n            partitioning = klass(partitioning_schema)\n            ds.write_dataset(table, tempdir,\n                             format='ipc', partitioning=partitioning)\n            load_back = ds.dataset(tempdir, format='ipc',\n                                   partitioning=partitioning)\n            load_back_table = load_back.to_table()\n            assert load_back_table.equals(table)\n\n    # test invalid partitioning input\n    with tempfile.TemporaryDirectory() as tempdir:\n        partitioning = ds.DirectoryPartitioning(partitioning_schema)\n        ds.write_dataset(table, tempdir,\n                         format='ipc', partitioning=partitioning)\n        load_back = None\n        with pytest.raises(ValueError,\n                           match=\"Expected Partitioning or PartitioningFactory\"):\n            load_back = ds.dataset(tempdir, format='ipc', partitioning=int(0))\n        assert load_back is None\n\n\ndef test_partitioning_pickling(pickle_module):\n    schema = pa.schema([\n        pa.field('i64', pa.int64()),\n        pa.field('f64', pa.float64())\n    ])\n    parts = [\n        ds.DirectoryPartitioning(schema),\n        ds.HivePartitioning(schema),\n        ds.FilenamePartitioning(schema),\n        ds.DirectoryPartitioning(schema, segment_encoding=\"none\"),\n        ds.FilenamePartitioning(schema, segment_encoding=\"none\"),\n        ds.HivePartitioning(schema, segment_encoding=\"none\", null_fallback=\"xyz\"),\n    ]\n\n    for part in parts:\n        assert pickle_module.loads(pickle_module.dumps(part)) == part\n\n\ndef test_expression_arithmetic_operators():\n    dataset = ds.dataset(pa.table({'a': [1, 2, 3], 'b': [2, 2, 2]}))\n    a = ds.field(\"a\")\n    b = ds.field(\"b\")\n    result = dataset.to_table(columns={\n        \"a+1\": a + 1,\n        \"b-a\": b - a,\n        \"a*2\": a * 2,\n        \"a/b\": a.cast(\"float64\") / b,\n    })\n    expected = pa.table({\n        \"a+1\": [2, 3, 4], \"b-a\": [1, 0, -1],\n        \"a*2\": [2, 4, 6], \"a/b\": [0.5, 1.0, 1.5],\n    })\n    assert result.equals(expected)\n\n\ndef test_partition_keys():\n    a, b, c = [ds.field(f) == f for f in 'abc']\n    assert ds.get_partition_keys(a) == {'a': 'a'}\n    assert ds.get_partition_keys(a) == ds._get_partition_keys(a)\n    assert ds.get_partition_keys(a & b & c) == {f: f for f in 'abc'}\n\n    nope = ds.field('d') >= 3\n    assert ds.get_partition_keys(nope) == {}\n    assert ds.get_partition_keys(a & nope) == {'a': 'a'}\n\n    null = ds.field('a').is_null()\n    assert ds.get_partition_keys(null) == {'a': None}\n\n\n@pytest.mark.parquet\ndef test_parquet_read_options():\n    opts1 = ds.ParquetReadOptions()\n    opts2 = ds.ParquetReadOptions(dictionary_columns=['a', 'b'])\n    opts3 = ds.ParquetReadOptions(coerce_int96_timestamp_unit=\"ms\")\n\n    assert opts1.dictionary_columns == set()\n\n    assert opts2.dictionary_columns == {'a', 'b'}\n\n    assert opts1.coerce_int96_timestamp_unit == \"ns\"\n    assert opts3.coerce_int96_timestamp_unit == \"ms\"\n\n    assert opts1 == opts1\n    assert opts1 != opts2\n    assert opts1 != opts3\n\n\n@pytest.mark.parquet\ndef test_parquet_file_format_read_options():\n    pff1 = ds.ParquetFileFormat()\n    pff2 = ds.ParquetFileFormat(dictionary_columns={'a'})\n    pff3 = ds.ParquetFileFormat(coerce_int96_timestamp_unit=\"s\")\n\n    assert pff1.read_options == ds.ParquetReadOptions()\n    assert pff2.read_options == ds.ParquetReadOptions(dictionary_columns=['a'])\n    assert pff3.read_options == ds.ParquetReadOptions(\n        coerce_int96_timestamp_unit=\"s\")\n\n\n@pytest.mark.parquet\ndef test_parquet_scan_options():\n    opts1 = ds.ParquetFragmentScanOptions()\n    opts2 = ds.ParquetFragmentScanOptions(buffer_size=4096)\n    opts3 = ds.ParquetFragmentScanOptions(\n        buffer_size=2**13, use_buffered_stream=True)\n    opts4 = ds.ParquetFragmentScanOptions(buffer_size=2**13, pre_buffer=False)\n    opts5 = ds.ParquetFragmentScanOptions(\n        thrift_string_size_limit=123456,\n        thrift_container_size_limit=987654,)\n    opts6 = ds.ParquetFragmentScanOptions(\n        page_checksum_verification=True)\n    cache_opts = pa.CacheOptions(\n        hole_size_limit=2**10, range_size_limit=8*2**10, lazy=True)\n    opts7 = ds.ParquetFragmentScanOptions(pre_buffer=True, cache_options=cache_opts)\n\n    assert opts1.use_buffered_stream is False\n    assert opts1.buffer_size == 2**13\n    assert opts1.pre_buffer is True\n    assert opts1.thrift_string_size_limit == 100_000_000  # default in C++\n    assert opts1.thrift_container_size_limit == 1_000_000  # default in C++\n    assert opts1.page_checksum_verification is False\n\n    assert opts2.use_buffered_stream is False\n    assert opts2.buffer_size == 2**12\n    assert opts2.pre_buffer is True\n\n    assert opts3.use_buffered_stream is True\n    assert opts3.buffer_size == 2**13\n    assert opts3.pre_buffer is True\n\n    assert opts4.use_buffered_stream is False\n    assert opts4.buffer_size == 2**13\n    assert opts4.pre_buffer is False\n\n    assert opts5.thrift_string_size_limit == 123456\n    assert opts5.thrift_container_size_limit == 987654\n\n    assert opts6.page_checksum_verification is True\n\n    assert opts7.pre_buffer is True\n    assert opts7.cache_options == cache_opts\n    assert opts7.cache_options != opts1.cache_options\n\n    assert opts1 == opts1\n    assert opts1 != opts2\n    assert opts2 != opts3\n    assert opts3 != opts4\n    assert opts5 != opts1\n    assert opts6 != opts1\n    assert opts7 != opts1\n\n\ndef test_file_format_pickling(pickle_module):\n    formats = [\n        ds.IpcFileFormat(),\n        ds.CsvFileFormat(),\n        ds.CsvFileFormat(pa.csv.ParseOptions(delimiter='\\t',\n                                             ignore_empty_lines=True)),\n        ds.CsvFileFormat(read_options=pa.csv.ReadOptions(\n            skip_rows=3, column_names=['foo'])),\n        ds.CsvFileFormat(read_options=pa.csv.ReadOptions(\n            skip_rows=3, block_size=2**20)),\n        ds.JsonFileFormat(),\n        ds.JsonFileFormat(\n            parse_options=pa.json.ParseOptions(newlines_in_values=True,\n                                               unexpected_field_behavior=\"ignore\")),\n        ds.JsonFileFormat(read_options=pa.json.ReadOptions(\n            use_threads=False, block_size=14)),\n    ]\n    try:\n        formats.append(ds.OrcFileFormat())\n    except ImportError:\n        pass\n\n    if pq is not None:\n        formats.extend([\n            ds.ParquetFileFormat(),\n            ds.ParquetFileFormat(dictionary_columns={'a'}),\n            ds.ParquetFileFormat(use_buffered_stream=True),\n            ds.ParquetFileFormat(\n                use_buffered_stream=True,\n                buffer_size=4096,\n                thrift_string_size_limit=123,\n                thrift_container_size_limit=456,\n            ),\n        ])\n\n    for file_format in formats:\n        assert pickle_module.loads(pickle_module.dumps(file_format)) == file_format\n\n\ndef test_fragment_scan_options_pickling(pickle_module):\n    options = [\n        ds.CsvFragmentScanOptions(),\n        ds.CsvFragmentScanOptions(\n            convert_options=pa.csv.ConvertOptions(strings_can_be_null=True)),\n        ds.CsvFragmentScanOptions(\n            read_options=pa.csv.ReadOptions(block_size=2**16)),\n        ds.JsonFragmentScanOptions(),\n        ds.JsonFragmentScanOptions(\n            pa.json.ParseOptions(newlines_in_values=False,\n                                 unexpected_field_behavior=\"error\")),\n        ds.JsonFragmentScanOptions(\n            read_options=pa.json.ReadOptions(use_threads=True, block_size=512)),\n    ]\n\n    if pq is not None:\n        options.extend([\n            ds.ParquetFragmentScanOptions(buffer_size=4096),\n            ds.ParquetFragmentScanOptions(pre_buffer=True),\n        ])\n\n    for option in options:\n        assert pickle_module.loads(pickle_module.dumps(option)) == option\n\n\n@pytest.mark.parametrize('paths_or_selector', [\n    fs.FileSelector('subdir', recursive=True),\n    [\n        'subdir/1/xxx/file0.parquet',\n        'subdir/2/yyy/file1.parquet',\n    ]\n])\n@pytest.mark.parametrize('pre_buffer', [False, True])\n@pytest.mark.parquet\ndef test_filesystem_factory(mockfs, paths_or_selector, pre_buffer):\n    format = ds.ParquetFileFormat(\n        read_options=ds.ParquetReadOptions(dictionary_columns={\"str\"}),\n        pre_buffer=pre_buffer\n    )\n\n    options = ds.FileSystemFactoryOptions('subdir')\n    options.partitioning = ds.DirectoryPartitioning(\n        pa.schema([\n            pa.field('group', pa.int32()),\n            pa.field('key', pa.string())\n        ])\n    )\n    assert options.partition_base_dir == 'subdir'\n    assert options.selector_ignore_prefixes == ['.', '_']\n    assert options.exclude_invalid_files is False\n\n    factory = ds.FileSystemDatasetFactory(\n        mockfs, paths_or_selector, format, options\n    )\n    inspected_schema = factory.inspect()\n\n    assert factory.inspect().equals(pa.schema([\n        pa.field('i64', pa.int64()),\n        pa.field('f64', pa.float64()),\n        pa.field('str', pa.dictionary(pa.int32(), pa.string())),\n        pa.field('const', pa.int64()),\n        pa.field('struct', pa.struct({'a': pa.int64(),\n                                      'b': pa.string()})),\n        pa.field('group', pa.int32()),\n        pa.field('key', pa.string()),\n    ]), check_metadata=False)\n\n    assert isinstance(factory.inspect_schemas(), list)\n    assert isinstance(factory.finish(inspected_schema),\n                      ds.FileSystemDataset)\n    assert factory.root_partition.equals(ds.scalar(True))\n\n    dataset = factory.finish()\n    assert isinstance(dataset, ds.FileSystemDataset)\n\n    scanner = dataset.scanner()\n    expected_i64 = pa.array([0, 1, 2, 3, 4], type=pa.int64())\n    expected_f64 = pa.array([0, 1, 2, 3, 4], type=pa.float64())\n    expected_str = pa.DictionaryArray.from_arrays(\n        pa.array([0, 1, 2, 3, 4], type=pa.int32()),\n        pa.array(\"0 1 2 3 4\".split(), type=pa.string())\n    )\n    expected_struct = pa.array([{'a': i % 3, 'b': str(i % 3)}\n                                for i in range(5)])\n    iterator = scanner.scan_batches()\n    for (batch, fragment), group, key in zip(iterator, [1, 2], ['xxx', 'yyy']):\n        expected_group = pa.array([group] * 5, type=pa.int32())\n        expected_key = pa.array([key] * 5, type=pa.string())\n        expected_const = pa.array([group - 1] * 5, type=pa.int64())\n        # Can't compare or really introspect expressions from Python\n        assert fragment.partition_expression is not None\n        assert batch.num_columns == 7\n        assert batch[0].equals(expected_i64)\n        assert batch[1].equals(expected_f64)\n        assert batch[2].equals(expected_str)\n        assert batch[3].equals(expected_const)\n        assert batch[4].equals(expected_struct)\n        assert batch[5].equals(expected_group)\n        assert batch[6].equals(expected_key)\n\n    table = dataset.to_table()\n    assert isinstance(table, pa.Table)\n    assert len(table) == 10\n    assert table.num_columns == 7\n\n\n@pytest.mark.parquet\ndef test_make_fragment(multisourcefs):\n    parquet_format = ds.ParquetFileFormat()\n    dataset = ds.dataset('/plain', filesystem=multisourcefs,\n                         format=parquet_format)\n\n    for path in dataset.files:\n        fragment = parquet_format.make_fragment(path, multisourcefs)\n        assert fragment.row_groups == [0]\n\n        row_group_fragment = parquet_format.make_fragment(path, multisourcefs,\n                                                          row_groups=[0])\n        for f in [fragment, row_group_fragment]:\n            assert isinstance(f, ds.ParquetFileFragment)\n            assert f.path == path\n            assert isinstance(f.filesystem, type(multisourcefs))\n        assert row_group_fragment.row_groups == [0]\n\n\n@pytest.mark.parquet\n@pytest.mark.s3\ndef test_make_fragment_with_size(s3_example_simple):\n    \"\"\"\n    Test passing file_size to make_fragment. Not all FS implementations make use\n    of the file size (by implementing an OpenInputFile that takes a FileInfo), but\n    s3 does, which is why it's used here.\n    \"\"\"\n    table, path, fs, uri, host, port, access_key, secret_key = s3_example_simple\n\n    file_format = ds.ParquetFileFormat()\n    paths = [path]\n\n    fragments = [file_format.make_fragment(path, fs)\n                 for path in paths]\n    dataset = ds.FileSystemDataset(\n        fragments, format=file_format, schema=table.schema, filesystem=fs\n    )\n\n    tbl = dataset.to_table()\n    assert tbl.equals(table)\n\n    # true sizes -> works\n    sizes_true = [dataset.filesystem.get_file_info(x).size for x in dataset.files]\n    fragments_with_size = [file_format.make_fragment(path, fs, file_size=size)\n                           for path, size in zip(paths, sizes_true)]\n    dataset_with_size = ds.FileSystemDataset(\n        fragments_with_size, format=file_format, schema=table.schema, filesystem=fs\n    )\n    tbl = dataset.to_table()\n    assert tbl.equals(table)\n\n    # too small sizes -> error\n    sizes_toosmall = [1 for path in paths]\n    fragments_with_size = [file_format.make_fragment(path, fs, file_size=size)\n                           for path, size in zip(paths, sizes_toosmall)]\n\n    dataset_with_size = ds.FileSystemDataset(\n        fragments_with_size, format=file_format, schema=table.schema, filesystem=fs\n    )\n\n    with pytest.raises(pyarrow.lib.ArrowInvalid, match='Parquet file size is 1 bytes'):\n        table = dataset_with_size.to_table()\n\n    # too large sizes -> error\n    sizes_toolarge = [1000000 for path in paths]\n    fragments_with_size = [file_format.make_fragment(path, fs, file_size=size)\n                           for path, size in zip(paths, sizes_toolarge)]\n\n    dataset_with_size = ds.FileSystemDataset(\n        fragments_with_size, format=file_format, schema=table.schema, filesystem=fs\n    )\n\n    # invalid range\n    with pytest.raises(OSError, match='HTTP status 416'):\n        table = dataset_with_size.to_table()\n\n\ndef test_make_csv_fragment_from_buffer(dataset_reader, pickle_module):\n    content = textwrap.dedent(\"\"\"\n        alpha,num,animal\n        a,12,dog\n        b,11,cat\n        c,10,rabbit\n    \"\"\")\n    buffer = pa.py_buffer(content.encode('utf-8'))\n\n    csv_format = ds.CsvFileFormat()\n    fragment = csv_format.make_fragment(buffer)\n\n    # When buffer, fragment open returns a BufferReader, not NativeFile\n    assert isinstance(fragment.open(), pa.BufferReader)\n\n    expected = pa.table([['a', 'b', 'c'],\n                         [12, 11, 10],\n                         ['dog', 'cat', 'rabbit']],\n                        names=['alpha', 'num', 'animal'])\n    assert dataset_reader.to_table(fragment).equals(expected)\n\n    pickled = pickle_module.loads(pickle_module.dumps(fragment))\n    assert dataset_reader.to_table(pickled).equals(fragment.to_table())\n\n\ndef test_make_json_fragment_from_buffer(dataset_reader, pickle_module):\n    content = '{\"alpha\" : \"a\", \"num\": 12, \"animal\" : \"dog\"}\\n' + \\\n        '{\"alpha\" : \"b\", \"num\": 11, \"animal\" : \"cat\"}\\n' + \\\n        '{\"alpha\" : \"c\", \"num\": 10, \"animal\" : \"rabbit\"}\\n'\n    buffer = pa.py_buffer(content.encode('utf-8'))\n\n    json_format = ds.JsonFileFormat()\n    fragment = json_format.make_fragment(buffer)\n\n    # When buffer, fragment open returns a BufferReader, not NativeFile\n    assert isinstance(fragment.open(), pa.BufferReader)\n\n    expected = pa.table([['a', 'b', 'c'],\n                         [12, 11, 10],\n                         ['dog', 'cat', 'rabbit']],\n                        names=['alpha', 'num', 'animal'])\n    assert dataset_reader.to_table(fragment).equals(expected)\n\n    pickled = pickle_module.loads(pickle_module.dumps(fragment))\n    assert dataset_reader.to_table(pickled).equals(fragment.to_table())\n\n\n@pytest.mark.parquet\ndef test_make_parquet_fragment_from_buffer(dataset_reader, pickle_module):\n    arrays = [\n        pa.array(['a', 'b', 'c']),\n        pa.array([12, 11, 10]),\n        pa.array(['dog', 'cat', 'rabbit'])\n    ]\n    dictionary_arrays = [\n        arrays[0].dictionary_encode(),\n        arrays[1],\n        arrays[2].dictionary_encode()\n    ]\n    dictionary_format = ds.ParquetFileFormat(\n        read_options=ds.ParquetReadOptions(\n            dictionary_columns=['alpha', 'animal']\n        ),\n        use_buffered_stream=True,\n        buffer_size=4096,\n    )\n\n    cases = [\n        (arrays, ds.ParquetFileFormat()),\n        (dictionary_arrays, dictionary_format)\n    ]\n    for arrays, format_ in cases:\n        table = pa.table(arrays, names=['alpha', 'num', 'animal'])\n\n        out = pa.BufferOutputStream()\n        pq.write_table(table, out)\n        buffer = out.getvalue()\n\n        fragment = format_.make_fragment(buffer)\n        assert dataset_reader.to_table(fragment).equals(table)\n\n        pickled = pickle_module.loads(pickle_module.dumps(fragment))\n        assert dataset_reader.to_table(pickled).equals(table)\n\n\n@pytest.mark.parquet\ndef _create_dataset_for_fragments(tempdir, chunk_size=None, filesystem=None):\n    table = pa.table(\n        [range(8), [1] * 8, ['a'] * 4 + ['b'] * 4],\n        names=['f1', 'f2', 'part']\n    )\n\n    path = str(tempdir / \"test_parquet_dataset\")\n\n    pq.write_to_dataset(table, path,\n                        partition_cols=[\"part\"], chunk_size=chunk_size)\n    dataset = ds.dataset(\n        path, format=\"parquet\", partitioning=\"hive\", filesystem=filesystem\n    )\n\n    return table, dataset\n\n\n@pytest.mark.parquet\ndef test_fragments(tempdir, dataset_reader):\n    table, dataset = _create_dataset_for_fragments(tempdir)\n\n    # list fragments\n    fragments = list(dataset.get_fragments())\n    assert len(fragments) == 2\n    f = fragments[0]\n\n    physical_names = ['f1', 'f2']\n    # file's schema does not include partition column\n    assert f.physical_schema.names == physical_names\n    assert f.format.inspect(f.path, f.filesystem) == f.physical_schema\n    assert f.partition_expression.equals(ds.field('part') == 'a')\n\n    # By default, the partition column is not part of the schema.\n    result = dataset_reader.to_table(f)\n    assert result.column_names == physical_names\n    assert result.equals(table.remove_column(2).slice(0, 4))\n\n    # scanning fragment includes partition columns when given the proper\n    # schema.\n    result = dataset_reader.to_table(f, schema=dataset.schema)\n    assert result.column_names == ['f1', 'f2', 'part']\n    assert result.equals(table.slice(0, 4))\n    assert f.physical_schema == result.schema.remove(2)\n\n    # scanning fragments follow filter predicate\n    result = dataset_reader.to_table(\n        f, schema=dataset.schema, filter=ds.field('f1') < 2)\n    assert result.column_names == ['f1', 'f2', 'part']\n\n\n@pytest.mark.pandas\n@pytest.mark.parquet\ndef test_fragments_implicit_cast(tempdir):\n    # ARROW-8693\n    table = pa.table([range(8), [1] * 4 + [2] * 4], names=['col', 'part'])\n    path = str(tempdir / \"test_parquet_dataset\")\n    pq.write_to_dataset(table, path, partition_cols=[\"part\"])\n\n    part = ds.partitioning(pa.schema([('part', 'int8')]), flavor=\"hive\")\n    dataset = ds.dataset(path, format=\"parquet\", partitioning=part)\n    fragments = dataset.get_fragments(filter=ds.field(\"part\") >= 2)\n    assert len(list(fragments)) == 1\n\n\n@pytest.mark.parquet\ndef test_fragments_reconstruct(tempdir, dataset_reader, pickle_module):\n    table, dataset = _create_dataset_for_fragments(tempdir)\n\n    def assert_yields_projected(fragment, row_slice,\n                                columns=None, filter=None):\n        actual = fragment.to_table(\n            schema=table.schema, columns=columns, filter=filter)\n        column_names = columns if columns else table.column_names\n        assert actual.column_names == column_names\n\n        expected = table.slice(*row_slice).select(column_names)\n        assert actual.equals(expected)\n\n    fragment = list(dataset.get_fragments())[0]\n    parquet_format = fragment.format\n\n    # test pickle roundtrip\n    pickled_fragment = pickle_module.loads(pickle_module.dumps(fragment))\n    assert dataset_reader.to_table(\n        pickled_fragment) == dataset_reader.to_table(fragment)\n\n    # manually re-construct a fragment, with explicit schema\n    new_fragment = parquet_format.make_fragment(\n        fragment.path, fragment.filesystem,\n        partition_expression=fragment.partition_expression)\n    assert dataset_reader.to_table(new_fragment).equals(\n        dataset_reader.to_table(fragment))\n    assert_yields_projected(new_fragment, (0, 4))\n\n    # filter / column projection, inspected schema\n    new_fragment = parquet_format.make_fragment(\n        fragment.path, fragment.filesystem,\n        partition_expression=fragment.partition_expression)\n    assert_yields_projected(new_fragment, (0, 2), filter=ds.field('f1') < 2)\n\n    # filter requiring cast / column projection, inspected schema\n    new_fragment = parquet_format.make_fragment(\n        fragment.path, fragment.filesystem,\n        partition_expression=fragment.partition_expression)\n    assert_yields_projected(new_fragment, (0, 2),\n                            columns=['f1'], filter=ds.field('f1') < 2.0)\n\n    # filter on the partition column\n    new_fragment = parquet_format.make_fragment(\n        fragment.path, fragment.filesystem,\n        partition_expression=fragment.partition_expression)\n    assert_yields_projected(new_fragment, (0, 4),\n                            filter=ds.field('part') == 'a')\n\n    # Fragments don't contain the partition's columns if not provided to the\n    # `to_table(schema=...)` method.\n    pattern = (r'No match for FieldRef.Name\\(part\\) in ' +\n               fragment.physical_schema.to_string(False, False, False))\n    with pytest.raises(ValueError, match=pattern):\n        new_fragment = parquet_format.make_fragment(\n            fragment.path, fragment.filesystem,\n            partition_expression=fragment.partition_expression)\n        dataset_reader.to_table(new_fragment, filter=ds.field('part') == 'a')\n\n\n@pytest.mark.parquet\ndef test_fragments_parquet_row_groups(tempdir, dataset_reader):\n    table, dataset = _create_dataset_for_fragments(tempdir, chunk_size=2)\n\n    fragment = list(dataset.get_fragments())[0]\n\n    # list and scan row group fragments\n    row_group_fragments = list(fragment.split_by_row_group())\n    assert len(row_group_fragments) == fragment.num_row_groups == 2\n    result = dataset_reader.to_table(\n        row_group_fragments[0], schema=dataset.schema)\n    assert result.column_names == ['f1', 'f2', 'part']\n    assert len(result) == 2\n    assert result.equals(table.slice(0, 2))\n\n    assert row_group_fragments[0].row_groups is not None\n    assert row_group_fragments[0].num_row_groups == 1\n    assert row_group_fragments[0].row_groups[0].statistics == {\n        'f1': {'min': 0, 'max': 1},\n        'f2': {'min': 1, 'max': 1},\n    }\n\n    fragment = list(dataset.get_fragments(filter=ds.field('f1') < 1))[0]\n    row_group_fragments = list(fragment.split_by_row_group(ds.field('f1') < 1))\n    assert len(row_group_fragments) == 1\n    result = dataset_reader.to_table(\n        row_group_fragments[0], filter=ds.field('f1') < 1)\n    assert len(result) == 1\n\n\n@pytest.mark.parquet\ndef test_fragments_parquet_num_row_groups(tempdir):\n    table = pa.table({'a': range(8)})\n    pq.write_table(table, tempdir / \"test.parquet\", row_group_size=2)\n    dataset = ds.dataset(tempdir / \"test.parquet\", format=\"parquet\")\n    original_fragment = list(dataset.get_fragments())[0]\n\n    # create fragment with subset of row groups\n    fragment = original_fragment.format.make_fragment(\n        original_fragment.path, original_fragment.filesystem,\n        row_groups=[1, 3])\n    assert fragment.num_row_groups == 2\n    # ensure that parsing metadata preserves correct number of row groups\n    fragment.ensure_complete_metadata()\n    assert fragment.num_row_groups == 2\n    assert len(fragment.row_groups) == 2\n\n\n@pytest.mark.pandas\n@pytest.mark.parquet\ndef test_fragments_parquet_row_groups_dictionary(tempdir, dataset_reader):\n    df = pd.DataFrame(dict(col1=['a', 'b'], col2=[1, 2]))\n    df['col1'] = df['col1'].astype(\"category\")\n\n    pq.write_table(pa.table(df), tempdir / \"test_filter_dictionary.parquet\")\n\n    import pyarrow.dataset as ds\n    dataset = ds.dataset(tempdir / 'test_filter_dictionary.parquet')\n    result = dataset_reader.to_table(dataset, filter=ds.field(\"col1\") == \"a\")\n\n    assert (df.iloc[0] == result.to_pandas()).all().all()\n\n\n@pytest.mark.parquet\ndef test_fragments_parquet_ensure_metadata(tempdir, open_logging_fs, pickle_module):\n    fs, assert_opens = open_logging_fs\n    _, dataset = _create_dataset_for_fragments(\n        tempdir, chunk_size=2, filesystem=fs\n    )\n    fragment = list(dataset.get_fragments())[0]\n\n    # with default discovery, no metadata loaded\n    with assert_opens([fragment.path]):\n        fragment.ensure_complete_metadata()\n    assert fragment.row_groups == [0, 1]\n\n    # second time -> use cached / no file IO\n    with assert_opens([]):\n        fragment.ensure_complete_metadata()\n\n    assert isinstance(fragment.metadata, pq.FileMetaData)\n\n    # recreate fragment with row group ids\n    new_fragment = fragment.format.make_fragment(\n        fragment.path, fragment.filesystem, row_groups=[0, 1]\n    )\n    assert new_fragment.row_groups == fragment.row_groups\n\n    # collect metadata\n    new_fragment.ensure_complete_metadata()\n    row_group = new_fragment.row_groups[0]\n    assert row_group.id == 0\n    assert row_group.num_rows == 2\n    assert row_group.statistics is not None\n\n    # pickling preserves row group ids\n    pickled_fragment = pickle_module.loads(pickle_module.dumps(new_fragment))\n    with assert_opens([fragment.path]):\n        assert pickled_fragment.row_groups == [0, 1]\n        row_group = pickled_fragment.row_groups[0]\n        assert row_group.id == 0\n        assert row_group.statistics is not None\n\n\n@pytest.mark.parquet\ndef test_fragments_parquet_pickle_no_metadata(tempdir, open_logging_fs, pickle_module):\n    # https://issues.apache.org/jira/browse/ARROW-15796\n    fs, assert_opens = open_logging_fs\n    _, dataset = _create_dataset_for_fragments(tempdir, filesystem=fs)\n    fragment = list(dataset.get_fragments())[1]\n\n    # second fragment hasn't yet loaded the metadata,\n    # and pickling it also should not read the metadata\n    with assert_opens([]):\n        pickled_fragment = pickle_module.loads(pickle_module.dumps(fragment))\n\n    # then accessing the row group info reads the metadata\n    with assert_opens([pickled_fragment.path]):\n        row_groups = pickled_fragment.row_groups\n    assert row_groups == [0]\n\n\ndef _create_dataset_all_types(tempdir, chunk_size=None):\n    table = pa.table(\n        [\n            pa.array([True, None, False], pa.bool_()),\n            pa.array([1, 10, 42], pa.int8()),\n            pa.array([1, 10, 42], pa.uint8()),\n            pa.array([1, 10, 42], pa.int16()),\n            pa.array([1, 10, 42], pa.uint16()),\n            pa.array([1, 10, 42], pa.int32()),\n            pa.array([1, 10, 42], pa.uint32()),\n            pa.array([1, 10, 42], pa.int64()),\n            pa.array([1, 10, 42], pa.uint64()),\n            pa.array([1.0, 10.0, 42.0], pa.float32()),\n            pa.array([1.0, 10.0, 42.0], pa.float64()),\n            pa.array(['a', None, 'z'], pa.utf8()),\n            pa.array(['a', None, 'z'], pa.binary()),\n            pa.array([1, 10, 42], pa.timestamp('s')),\n            pa.array([1, 10, 42], pa.timestamp('ms')),\n            pa.array([1, 10, 42], pa.timestamp('us')),\n            pa.array([1, 10, 42], pa.date32()),\n            pa.array([1, 10, 4200000000], pa.date64()),\n            pa.array([1, 10, 42], pa.time32('s')),\n            pa.array([1, 10, 42], pa.time64('us')),\n        ],\n        names=[\n            'boolean',\n            'int8',\n            'uint8',\n            'int16',\n            'uint16',\n            'int32',\n            'uint32',\n            'int64',\n            'uint64',\n            'float',\n            'double',\n            'utf8',\n            'binary',\n            'ts[s]',\n            'ts[ms]',\n            'ts[us]',\n            'date32',\n            'date64',\n            'time32',\n            'time64',\n        ]\n    )\n\n    path = str(tempdir / \"test_parquet_dataset_all_types\")\n\n    # write_to_dataset currently requires pandas\n    pq.write_to_dataset(table, path, chunk_size=chunk_size)\n\n    return table, ds.dataset(path, format=\"parquet\", partitioning=\"hive\")\n\n\n@pytest.mark.pandas\n@pytest.mark.parquet\ndef test_parquet_fragment_statistics(tempdir):\n    table, dataset = _create_dataset_all_types(tempdir)\n\n    fragment = list(dataset.get_fragments())[0]\n\n    import datetime\n    def dt_s(x): return datetime.datetime(1970, 1, 1, 0, 0, x)\n    def dt_ms(x): return datetime.datetime(1970, 1, 1, 0, 0, 0, x*1000)\n    def dt_us(x): return datetime.datetime(1970, 1, 1, 0, 0, 0, x)\n    date = datetime.date\n    time = datetime.time\n\n    # list and scan row group fragments\n    row_group_fragments = list(fragment.split_by_row_group())\n    assert row_group_fragments[0].row_groups is not None\n    row_group = row_group_fragments[0].row_groups[0]\n    assert row_group.num_rows == 3\n    assert row_group.total_byte_size > 1000\n    assert row_group.statistics == {\n        'boolean': {'min': False, 'max': True},\n        'int8': {'min': 1, 'max': 42},\n        'uint8': {'min': 1, 'max': 42},\n        'int16': {'min': 1, 'max': 42},\n        'uint16': {'min': 1, 'max': 42},\n        'int32': {'min': 1, 'max': 42},\n        'uint32': {'min': 1, 'max': 42},\n        'int64': {'min': 1, 'max': 42},\n        'uint64': {'min': 1, 'max': 42},\n        'float': {'min': 1.0, 'max': 42.0},\n        'double': {'min': 1.0, 'max': 42.0},\n        'utf8': {'min': 'a', 'max': 'z'},\n        'binary': {'min': b'a', 'max': b'z'},\n        'ts[s]': {'min': dt_s(1), 'max': dt_s(42)},\n        'ts[ms]': {'min': dt_ms(1), 'max': dt_ms(42)},\n        'ts[us]': {'min': dt_us(1), 'max': dt_us(42)},\n        'date32': {'min': date(1970, 1, 2), 'max': date(1970, 2, 12)},\n        'date64': {'min': date(1970, 1, 1), 'max': date(1970, 2, 18)},\n        'time32': {'min': time(0, 0, 1), 'max': time(0, 0, 42)},\n        'time64': {'min': time(0, 0, 0, 1), 'max': time(0, 0, 0, 42)},\n    }\n\n\n@pytest.mark.parquet\ndef test_parquet_fragment_statistics_nulls(tempdir):\n    table = pa.table({'a': [0, 1, None, None], 'b': ['a', 'b', None, None]})\n    pq.write_table(table, tempdir / \"test.parquet\", row_group_size=2)\n\n    dataset = ds.dataset(tempdir / \"test.parquet\", format=\"parquet\")\n    fragments = list(dataset.get_fragments())[0].split_by_row_group()\n    # second row group has all nulls -> no statistics\n    assert fragments[1].row_groups[0].statistics == {}\n\n\n@pytest.mark.pandas\n@pytest.mark.parquet\ndef test_parquet_empty_row_group_statistics(tempdir):\n    df = pd.DataFrame({\"a\": [\"a\", \"b\", \"b\"], \"b\": [4, 5, 6]})[:0]\n    df.to_parquet(tempdir / \"test.parquet\", engine=\"pyarrow\")\n\n    dataset = ds.dataset(tempdir / \"test.parquet\", format=\"parquet\")\n    fragments = list(dataset.get_fragments())[0].split_by_row_group()\n    # Only row group is empty\n    assert fragments[0].row_groups[0].statistics == {}\n\n\n@pytest.mark.parquet\ndef test_fragments_parquet_row_groups_predicate(tempdir):\n    table, dataset = _create_dataset_for_fragments(tempdir, chunk_size=2)\n\n    fragment = list(dataset.get_fragments())[0]\n    assert fragment.partition_expression.equals(ds.field('part') == 'a')\n\n    # predicate may reference a partition field not present in the\n    # physical_schema if an explicit schema is provided to split_by_row_group\n\n    # filter matches partition_expression: all row groups\n    row_group_fragments = list(\n        fragment.split_by_row_group(filter=ds.field('part') == 'a',\n                                    schema=dataset.schema))\n    assert len(row_group_fragments) == 2\n\n    # filter contradicts partition_expression: no row groups\n    row_group_fragments = list(\n        fragment.split_by_row_group(filter=ds.field('part') == 'b',\n                                    schema=dataset.schema))\n    assert len(row_group_fragments) == 0\n\n\n@pytest.mark.parquet\ndef test_fragments_parquet_row_groups_reconstruct(tempdir, dataset_reader,\n                                                  pickle_module):\n    table, dataset = _create_dataset_for_fragments(tempdir, chunk_size=2)\n\n    fragment = list(dataset.get_fragments())[0]\n    parquet_format = fragment.format\n    row_group_fragments = list(fragment.split_by_row_group())\n\n    # test pickle roundtrip\n    pickled_fragment = pickle_module.loads(pickle_module.dumps(fragment))\n    assert dataset_reader.to_table(\n        pickled_fragment) == dataset_reader.to_table(fragment)\n\n    # manually re-construct row group fragments\n    new_fragment = parquet_format.make_fragment(\n        fragment.path, fragment.filesystem,\n        partition_expression=fragment.partition_expression,\n        row_groups=[0])\n    result = dataset_reader.to_table(new_fragment)\n    assert result.equals(dataset_reader.to_table(row_group_fragments[0]))\n\n    # manually re-construct a row group fragment with filter/column projection\n    new_fragment = parquet_format.make_fragment(\n        fragment.path, fragment.filesystem,\n        partition_expression=fragment.partition_expression,\n        row_groups={1})\n    result = dataset_reader.to_table(\n        new_fragment, schema=table.schema, columns=['f1', 'part'],\n        filter=ds.field('f1') < 3, )\n    assert result.column_names == ['f1', 'part']\n    assert len(result) == 1\n\n    # out of bounds row group index\n    new_fragment = parquet_format.make_fragment(\n        fragment.path, fragment.filesystem,\n        partition_expression=fragment.partition_expression,\n        row_groups={2})\n    with pytest.raises(IndexError, match=\"references row group 2\"):\n        dataset_reader.to_table(new_fragment)\n\n\n@pytest.mark.parquet\ndef test_fragments_parquet_subset_ids(tempdir, open_logging_fs,\n                                      dataset_reader):\n    fs, assert_opens = open_logging_fs\n    table, dataset = _create_dataset_for_fragments(tempdir, chunk_size=1,\n                                                   filesystem=fs)\n    fragment = list(dataset.get_fragments())[0]\n\n    # select with row group ids\n    subfrag = fragment.subset(row_group_ids=[0, 3])\n    with assert_opens([]):\n        assert subfrag.num_row_groups == 2\n        assert subfrag.row_groups == [0, 3]\n        assert subfrag.row_groups[0].statistics is not None\n\n    # check correct scan result of subset\n    result = dataset_reader.to_table(subfrag)\n    assert result.to_pydict() == {\"f1\": [0, 3], \"f2\": [1, 1]}\n\n    # empty list of ids\n    subfrag = fragment.subset(row_group_ids=[])\n    assert subfrag.num_row_groups == 0\n    assert subfrag.row_groups == []\n    result = dataset_reader.to_table(subfrag, schema=dataset.schema)\n    assert result.num_rows == 0\n    assert result.equals(table[:0])\n\n\n@pytest.mark.parquet\ndef test_fragments_parquet_subset_filter(tempdir, open_logging_fs,\n                                         dataset_reader):\n    fs, assert_opens = open_logging_fs\n    table, dataset = _create_dataset_for_fragments(tempdir, chunk_size=1,\n                                                   filesystem=fs)\n    fragment = list(dataset.get_fragments())[0]\n\n    # select with filter\n    subfrag = fragment.subset(ds.field(\"f1\") >= 1)\n    with assert_opens([]):\n        assert subfrag.num_row_groups == 3\n        assert len(subfrag.row_groups) == 3\n        assert subfrag.row_groups[0].statistics is not None\n\n    # check correct scan result of subset\n    result = dataset_reader.to_table(subfrag)\n    assert result.to_pydict() == {\"f1\": [1, 2, 3], \"f2\": [1, 1, 1]}\n\n    # filter that results in empty selection\n    subfrag = fragment.subset(ds.field(\"f1\") > 5)\n    assert subfrag.num_row_groups == 0\n    assert subfrag.row_groups == []\n    result = dataset_reader.to_table(subfrag, schema=dataset.schema)\n    assert result.num_rows == 0\n    assert result.equals(table[:0])\n\n    # passing schema to ensure filter on partition expression works\n    subfrag = fragment.subset(ds.field(\"part\") == \"a\", schema=dataset.schema)\n    assert subfrag.num_row_groups == 4\n\n\n@pytest.mark.parquet\ndef test_fragments_parquet_subset_invalid(tempdir):\n    _, dataset = _create_dataset_for_fragments(tempdir, chunk_size=1)\n    fragment = list(dataset.get_fragments())[0]\n\n    # passing none or both of filter / row_group_ids\n    with pytest.raises(ValueError):\n        fragment.subset(ds.field(\"f1\") >= 1, row_group_ids=[1, 2])\n\n    with pytest.raises(ValueError):\n        fragment.subset()\n\n\n@pytest.mark.parquet\ndef test_fragments_parquet_subset_with_nested_fields(tempdir):\n    # ensure row group filtering with nested field works\n    f1 = pa.array([0, 1, 2, 3])\n    f21 = pa.array([0.1, 0.2, 0.3, 0.4])\n    f22 = pa.array([1, 2, 3, 4])\n    f2 = pa.StructArray.from_arrays([f21, f22], names=[\"f21\", \"f22\"])\n    struct_col = pa.StructArray.from_arrays([f1, f2], names=[\"f1\", \"f2\"])\n    table = pa.table({\"col\": struct_col})\n    pq.write_table(table, tempdir / \"data_struct.parquet\", row_group_size=2)\n\n    dataset = ds.dataset(tempdir / \"data_struct.parquet\", format=\"parquet\")\n    fragment = list(dataset.get_fragments())[0]\n    assert fragment.num_row_groups == 2\n\n    subfrag = fragment.subset(ds.field(\"col\", \"f1\") > 2)\n    assert subfrag.num_row_groups == 1\n    subfrag = fragment.subset(ds.field(\"col\", \"f1\") > 5)\n    assert subfrag.num_row_groups == 0\n\n    subfrag = fragment.subset(ds.field(\"col\", \"f2\", \"f21\") > 0)\n    assert subfrag.num_row_groups == 2\n    subfrag = fragment.subset(ds.field(\"col\", \"f2\", \"f22\") <= 2)\n    assert subfrag.num_row_groups == 1\n\n    # nonexisting field ref\n    with pytest.raises(pa.ArrowInvalid, match=\"No match for FieldRef.Nested\"):\n        fragment.subset(ds.field(\"col\", \"f3\") > 0)\n\n    # comparison with struct field is not implemented\n    with pytest.raises(\n        NotImplementedError, match=\"Function 'greater' has no kernel matching\"\n    ):\n        fragment.subset(ds.field(\"col\", \"f2\") > 0)\n\n\n@pytest.mark.pandas\n@pytest.mark.parquet\ndef test_fragments_repr(tempdir, dataset):\n    # partitioned parquet dataset\n    fragment = list(dataset.get_fragments())[0]\n    assert (\n        # Ordering of partition items is non-deterministic\n        repr(fragment) ==\n        \"<pyarrow.dataset.ParquetFileFragment path=subdir/1/xxx/file0.parquet \"\n        \"partition=[key=xxx, group=1]>\" or\n        repr(fragment) ==\n        \"<pyarrow.dataset.ParquetFileFragment path=subdir/1/xxx/file0.parquet \"\n        \"partition=[group=1, key=xxx]>\"\n    )\n\n    # single-file parquet dataset (no partition information in repr)\n    table, path = _create_single_file(tempdir)\n    dataset = ds.dataset(path, format=\"parquet\")\n    fragment = list(dataset.get_fragments())[0]\n    assert (\n        repr(fragment) ==\n        \"<pyarrow.dataset.ParquetFileFragment path={}>\".format(\n            dataset.filesystem.normalize_path(str(path)))\n    )\n\n    # non-parquet format\n    path = tempdir / \"data.feather\"\n    pa.feather.write_feather(table, path)\n    dataset = ds.dataset(path, format=\"feather\")\n    fragment = list(dataset.get_fragments())[0]\n    assert (\n        repr(fragment) ==\n        \"<pyarrow.dataset.FileFragment type=ipc path={}>\".format(\n            dataset.filesystem.normalize_path(str(path)))\n    )\n\n\n@pytest.mark.parquet\n@pytest.mark.parametrize(\n    \"pickled\", [lambda x, m: x, lambda x, m: m.loads(m.dumps(x))])\ndef test_partitioning_factory(mockfs, pickled, pickle_module):\n    paths_or_selector = fs.FileSelector('subdir', recursive=True)\n    format = ds.ParquetFileFormat()\n\n    options = ds.FileSystemFactoryOptions('subdir')\n    partitioning_factory = ds.DirectoryPartitioning.discover(['group', 'key'])\n    partitioning_factory = pickled(partitioning_factory, pickle_module)\n    assert isinstance(partitioning_factory, ds.PartitioningFactory)\n    options.partitioning_factory = partitioning_factory\n\n    factory = ds.FileSystemDatasetFactory(\n        mockfs, paths_or_selector, format, options\n    )\n    inspected_schema = factory.inspect()\n    # i64/f64 from data, group/key from \"/1/xxx\" and \"/2/yyy\" paths\n    expected_schema = pa.schema([\n        (\"i64\", pa.int64()),\n        (\"f64\", pa.float64()),\n        (\"str\", pa.string()),\n        (\"const\", pa.int64()),\n        (\"struct\", pa.struct({'a': pa.int64(), 'b': pa.string()})),\n        (\"group\", pa.int32()),\n        (\"key\", pa.string()),\n    ])\n    assert inspected_schema.equals(expected_schema)\n\n    hive_partitioning_factory = ds.HivePartitioning.discover()\n    assert isinstance(hive_partitioning_factory, ds.PartitioningFactory)\n\n\n@pytest.mark.parquet\n@pytest.mark.parametrize('infer_dictionary', [False, True])\n@pytest.mark.parametrize(\n    \"pickled\", [lambda x, m: x, lambda x, m: m.loads(m.dumps(x))])\ndef test_partitioning_factory_dictionary(mockfs, infer_dictionary, pickled,\n                                         pickle_module):\n    paths_or_selector = fs.FileSelector('subdir', recursive=True)\n    format = ds.ParquetFileFormat()\n    options = ds.FileSystemFactoryOptions('subdir')\n\n    partitioning_factory = ds.DirectoryPartitioning.discover(\n        ['group', 'key'], infer_dictionary=infer_dictionary)\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n\n    factory = ds.FileSystemDatasetFactory(\n        mockfs, paths_or_selector, format, options)\n\n    inferred_schema = factory.inspect()\n    if infer_dictionary:\n        expected_type = pa.dictionary(pa.int32(), pa.string())\n        assert inferred_schema.field('key').type == expected_type\n\n        table = factory.finish().to_table().combine_chunks()\n        actual = table.column('key').chunk(0)\n        expected = pa.array(['xxx'] * 5 + ['yyy'] * 5).dictionary_encode()\n        assert actual.equals(expected)\n\n        # ARROW-9345 ensure filtering on the partition field works\n        table = factory.finish().to_table(filter=ds.field('key') == 'xxx')\n        actual = table.column('key').chunk(0)\n        expected = expected.slice(0, 5)\n        assert actual.equals(expected)\n    else:\n        assert inferred_schema.field('key').type == pa.string()\n\n\n@pytest.mark.parametrize(\n    \"pickled\", [lambda x, m: x, lambda x, m: m.loads(m.dumps(x))])\ndef test_partitioning_factory_segment_encoding(pickled, pickle_module):\n    mockfs = fs._MockFileSystem()\n    format = ds.IpcFileFormat()\n    schema = pa.schema([(\"i64\", pa.int64())])\n    table = pa.table([pa.array(range(10))], schema=schema)\n    partition_schema = pa.schema(\n        [(\"date\", pa.timestamp(\"s\")), (\"string\", pa.string())])\n    string_partition_schema = pa.schema(\n        [(\"date\", pa.string()), (\"string\", pa.string())])\n    full_schema = pa.schema(list(schema) + list(partition_schema))\n    for directory in [\n            \"directory/2021-05-04 00%3A00%3A00/%24\",\n            \"hive/date=2021-05-04 00%3A00%3A00/string=%24\",\n    ]:\n        mockfs.create_dir(directory)\n        with mockfs.open_output_stream(directory + \"/0.feather\") as sink:\n            with pa.ipc.new_file(sink, schema) as writer:\n                writer.write_table(table)\n                writer.close()\n\n    # Directory\n    selector = fs.FileSelector(\"directory\", recursive=True)\n    options = ds.FileSystemFactoryOptions(\"directory\")\n    partitioning_factory = ds.DirectoryPartitioning.discover(\n        schema=partition_schema)\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    inferred_schema = factory.inspect()\n    assert inferred_schema == full_schema\n    actual = factory.finish().to_table(columns={\n        \"date_int\": ds.field(\"date\").cast(pa.int64()),\n    })\n    assert actual[0][0].as_py() == 1620086400\n\n    partitioning_factory = ds.DirectoryPartitioning.discover(\n        [\"date\", \"string\"], segment_encoding=\"none\")\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    fragments = list(factory.finish().get_fragments())\n    assert fragments[0].partition_expression.equals(\n        (ds.field(\"date\") == \"2021-05-04 00%3A00%3A00\") &\n        (ds.field(\"string\") == \"%24\"))\n\n    partitioning = ds.DirectoryPartitioning(\n        string_partition_schema, segment_encoding=\"none\")\n    options.partitioning = pickled(partitioning, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    fragments = list(factory.finish().get_fragments())\n    assert fragments[0].partition_expression.equals(\n        (ds.field(\"date\") == \"2021-05-04 00%3A00%3A00\") &\n        (ds.field(\"string\") == \"%24\"))\n\n    partitioning_factory = ds.DirectoryPartitioning.discover(\n        schema=partition_schema, segment_encoding=\"none\")\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"Could not cast segments for partition field\"):\n        inferred_schema = factory.inspect()\n\n    # Hive\n    selector = fs.FileSelector(\"hive\", recursive=True)\n    options = ds.FileSystemFactoryOptions(\"hive\")\n    partitioning_factory = ds.HivePartitioning.discover(\n        schema=partition_schema)\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    inferred_schema = factory.inspect()\n    assert inferred_schema == full_schema\n    actual = factory.finish().to_table(columns={\n        \"date_int\": ds.field(\"date\").cast(pa.int64()),\n    })\n    assert actual[0][0].as_py() == 1620086400\n\n    partitioning_factory = ds.HivePartitioning.discover(\n        segment_encoding=\"none\")\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    fragments = list(factory.finish().get_fragments())\n    assert fragments[0].partition_expression.equals(\n        (ds.field(\"date\") == \"2021-05-04 00%3A00%3A00\") &\n        (ds.field(\"string\") == \"%24\"))\n\n    options.partitioning = ds.HivePartitioning(\n        string_partition_schema, segment_encoding=\"none\")\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    fragments = list(factory.finish().get_fragments())\n    assert fragments[0].partition_expression.equals(\n        (ds.field(\"date\") == \"2021-05-04 00%3A00%3A00\") &\n        (ds.field(\"string\") == \"%24\"))\n\n    partitioning_factory = ds.HivePartitioning.discover(\n        schema=partition_schema, segment_encoding=\"none\")\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"Could not cast segments for partition field\"):\n        inferred_schema = factory.inspect()\n\n\n@pytest.mark.parametrize(\n    \"pickled\", [lambda x, m: x, lambda x, m: m.loads(m.dumps(x))])\ndef test_partitioning_factory_hive_segment_encoding_key_encoded(pickled, pickle_module):\n    mockfs = fs._MockFileSystem()\n    format = ds.IpcFileFormat()\n    schema = pa.schema([(\"i64\", pa.int64())])\n    table = pa.table([pa.array(range(10))], schema=schema)\n    partition_schema = pa.schema(\n        [(\"test'; date\", pa.timestamp(\"s\")), (\"test';[ string'\", pa.string())])\n    string_partition_schema = pa.schema(\n        [(\"test'; date\", pa.string()), (\"test';[ string'\", pa.string())])\n    full_schema = pa.schema(list(schema) + list(partition_schema))\n\n    partition_schema_en = pa.schema(\n        [(\"test%27%3B%20date\", pa.timestamp(\"s\")),\n         (\"test%27%3B%5B%20string%27\", pa.string())])\n    string_partition_schema_en = pa.schema(\n        [(\"test%27%3B%20date\", pa.string()),\n         (\"test%27%3B%5B%20string%27\", pa.string())])\n\n    directory = (\"hive/test%27%3B%20date=2021-05-04 00%3A00%3A00/\"\n                 \"test%27%3B%5B%20string%27=%24\")\n    mockfs.create_dir(directory)\n    with mockfs.open_output_stream(directory + \"/0.feather\") as sink:\n        with pa.ipc.new_file(sink, schema) as writer:\n            writer.write_table(table)\n            writer.close()\n\n    # Hive\n    selector = fs.FileSelector(\"hive\", recursive=True)\n    options = ds.FileSystemFactoryOptions(\"hive\")\n    partitioning_factory = ds.HivePartitioning.discover(\n        schema=partition_schema)\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    inferred_schema = factory.inspect()\n    assert inferred_schema == full_schema\n    actual = factory.finish().to_table(columns={\n        \"date_int\": ds.field(\"test'; date\").cast(pa.int64()),\n    })\n    assert actual[0][0].as_py() == 1620086400\n\n    partitioning_factory = ds.HivePartitioning.discover(\n        segment_encoding=\"uri\")\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    fragments = list(factory.finish().get_fragments())\n    assert fragments[0].partition_expression.equals(\n        (ds.field(\"test'; date\") == \"2021-05-04 00:00:00\") &\n        (ds.field(\"test';[ string'\") == \"$\"))\n\n    partitioning = ds.HivePartitioning(\n        string_partition_schema, segment_encoding=\"uri\")\n    options.partitioning = pickled(partitioning, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    fragments = list(factory.finish().get_fragments())\n    assert fragments[0].partition_expression.equals(\n        (ds.field(\"test'; date\") == \"2021-05-04 00:00:00\") &\n        (ds.field(\"test';[ string'\") == \"$\"))\n\n    partitioning_factory = ds.HivePartitioning.discover(\n        segment_encoding=\"none\")\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    fragments = list(factory.finish().get_fragments())\n    assert fragments[0].partition_expression.equals(\n        (ds.field(\"test%27%3B%20date\") == \"2021-05-04 00%3A00%3A00\") &\n        (ds.field(\"test%27%3B%5B%20string%27\") == \"%24\"))\n\n    partitioning = ds.HivePartitioning(\n        string_partition_schema_en, segment_encoding=\"none\")\n    options.partitioning = pickled(partitioning, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    fragments = list(factory.finish().get_fragments())\n    assert fragments[0].partition_expression.equals(\n        (ds.field(\"test%27%3B%20date\") == \"2021-05-04 00%3A00%3A00\") &\n        (ds.field(\"test%27%3B%5B%20string%27\") == \"%24\"))\n\n    partitioning_factory = ds.HivePartitioning.discover(\n        schema=partition_schema_en, segment_encoding=\"none\")\n    options.partitioning_factory = pickled(partitioning_factory, pickle_module)\n    factory = ds.FileSystemDatasetFactory(mockfs, selector, format, options)\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"Could not cast segments for partition field\"):\n        inferred_schema = factory.inspect()\n\n\ndef test_dictionary_partitioning_outer_nulls_raises(tempdir):\n    table = pa.table({'a': ['x', 'y', None], 'b': ['x', 'y', 'z']})\n    part = ds.partitioning(\n        pa.schema([pa.field('a', pa.string()), pa.field('b', pa.string())]))\n    with pytest.raises(pa.ArrowInvalid):\n        ds.write_dataset(table, tempdir, format='ipc', partitioning=part)\n\n\ndef test_positional_keywords_raises(tempdir):\n    table = pa.table({'a': ['x', 'y', None], 'b': ['x', 'y', 'z']})\n    with pytest.raises(TypeError):\n        ds.write_dataset(table, tempdir, \"basename-{i}.arrow\")\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_read_partition_keys_only(tempdir):\n    BATCH_SIZE = 2 ** 15\n    # This is a regression test for ARROW-15318 which saw issues\n    # reading only the partition keys from files with batches larger\n    # than the default batch size (e.g. so we need to return two chunks)\n    table = pa.table({\n        'key': pa.repeat(0, BATCH_SIZE + 1),\n        'value': np.arange(BATCH_SIZE + 1)})\n    pq.write_to_dataset(\n        table[:BATCH_SIZE],\n        tempdir / 'one', partition_cols=['key'])\n    pq.write_to_dataset(\n        table[:BATCH_SIZE + 1],\n        tempdir / 'two', partition_cols=['key'])\n\n    table = pq.read_table(tempdir / 'one', columns=['key'])\n    assert table['key'].num_chunks == 1\n\n    table = pq.read_table(tempdir / 'two', columns=['key', 'value'])\n    assert table['key'].num_chunks == 2\n\n    table = pq.read_table(tempdir / 'two', columns=['key'])\n    assert table['key'].num_chunks == 2\n\n\ndef _has_subdirs(basedir):\n    elements = os.listdir(basedir)\n    return any([os.path.isdir(os.path.join(basedir, el)) for el in elements])\n\n\ndef _do_list_all_dirs(basedir, path_so_far, result):\n    for f in os.listdir(basedir):\n        true_nested = os.path.join(basedir, f)\n        if os.path.isdir(true_nested):\n            norm_nested = posixpath.join(path_so_far, f)\n            if _has_subdirs(true_nested):\n                _do_list_all_dirs(true_nested, norm_nested, result)\n            else:\n                result.append(norm_nested)\n\n\ndef _list_all_dirs(basedir):\n    result = []\n    _do_list_all_dirs(basedir, '', result)\n    return result\n\n\ndef _check_dataset_directories(tempdir, expected_directories):\n    actual_directories = set(_list_all_dirs(tempdir))\n    assert actual_directories == set(expected_directories)\n\n\ndef test_dictionary_partitioning_inner_nulls(tempdir):\n    table = pa.table({'a': ['x', 'y', 'z'], 'b': ['x', 'y', None]})\n    part = ds.partitioning(\n        pa.schema([pa.field('a', pa.string()), pa.field('b', pa.string())]))\n    ds.write_dataset(table, tempdir, format='ipc', partitioning=part)\n    _check_dataset_directories(tempdir, ['x/x', 'y/y', 'z'])\n\n\ndef test_hive_partitioning_nulls(tempdir):\n    table = pa.table({'a': ['x', None, 'z'], 'b': ['x', 'y', None]})\n    part = ds.HivePartitioning(pa.schema(\n        [pa.field('a', pa.string()), pa.field('b', pa.string())]), None, 'xyz')\n    ds.write_dataset(table, tempdir, format='ipc', partitioning=part)\n    _check_dataset_directories(tempdir, ['a=x/b=x', 'a=xyz/b=y', 'a=z/b=xyz'])\n\n\ndef test_partitioning_function():\n    schema = pa.schema([(\"year\", pa.int16()), (\"month\", pa.int8())])\n    names = [\"year\", \"month\"]\n\n    # default DirectoryPartitioning\n    part = ds.partitioning(schema)\n    assert isinstance(part, ds.DirectoryPartitioning)\n    part = ds.partitioning(schema, dictionaries=\"infer\")\n    assert isinstance(part, ds.PartitioningFactory)\n    part = ds.partitioning(field_names=names)\n    assert isinstance(part, ds.PartitioningFactory)\n    # needs schema or list of names\n    with pytest.raises(ValueError):\n        ds.partitioning()\n    with pytest.raises(ValueError, match=\"Expected list\"):\n        ds.partitioning(field_names=schema)\n    with pytest.raises(ValueError, match=\"Cannot specify both\"):\n        ds.partitioning(schema, field_names=schema)\n\n    # Hive partitioning\n    part = ds.partitioning(schema, flavor=\"hive\")\n    assert isinstance(part, ds.HivePartitioning)\n    part = ds.partitioning(schema, dictionaries=\"infer\", flavor=\"hive\")\n    assert isinstance(part, ds.PartitioningFactory)\n    part = ds.partitioning(flavor=\"hive\")\n    assert isinstance(part, ds.PartitioningFactory)\n    # cannot pass list of names\n    with pytest.raises(ValueError):\n        ds.partitioning(names, flavor=\"hive\")\n    with pytest.raises(ValueError, match=\"Cannot specify 'field_names'\"):\n        ds.partitioning(field_names=names, flavor=\"hive\")\n\n    # unsupported flavor\n    with pytest.raises(ValueError):\n        ds.partitioning(schema, flavor=\"unsupported\")\n\n\n@pytest.mark.parquet\ndef test_directory_partitioning_dictionary_key(mockfs):\n    # ARROW-8088 specifying partition key as dictionary type\n    schema = pa.schema([\n        pa.field('group', pa.dictionary(pa.int8(), pa.int32())),\n        pa.field('key', pa.dictionary(pa.int8(), pa.string()))\n    ])\n    part = ds.DirectoryPartitioning.discover(schema=schema)\n\n    dataset = ds.dataset(\n        \"subdir\", format=\"parquet\", filesystem=mockfs, partitioning=part\n    )\n    assert dataset.partitioning.schema == schema\n    table = dataset.to_table()\n\n    assert table.column('group').type.equals(schema.types[0])\n    assert table.column('group').to_pylist() == [1] * 5 + [2] * 5\n    assert table.column('key').type.equals(schema.types[1])\n    assert table.column('key').to_pylist() == ['xxx'] * 5 + ['yyy'] * 5\n\n\ndef test_hive_partitioning_dictionary_key(multisourcefs):\n    # ARROW-8088 specifying partition key as dictionary type\n    schema = pa.schema([\n        pa.field('year', pa.dictionary(pa.int8(), pa.int16())),\n        pa.field('month', pa.dictionary(pa.int8(), pa.int16()))\n    ])\n    part = ds.HivePartitioning.discover(schema=schema)\n\n    dataset = ds.dataset(\n        \"hive\", format=\"parquet\", filesystem=multisourcefs, partitioning=part\n    )\n    assert dataset.partitioning.schema == schema\n    table = dataset.to_table()\n\n    year_dictionary = list(range(2006, 2011))\n    month_dictionary = list(range(1, 13))\n    assert table.column('year').type.equals(schema.types[0])\n    for chunk in table.column('year').chunks:\n        actual = chunk.dictionary.to_pylist()\n        actual.sort()\n        assert actual == year_dictionary\n    assert table.column('month').type.equals(schema.types[1])\n    for chunk in table.column('month').chunks:\n        actual = chunk.dictionary.to_pylist()\n        actual.sort()\n        assert actual == month_dictionary\n\n\ndef _create_single_file(base_dir, table=None, row_group_size=None):\n    if table is None:\n        table = pa.table({'a': range(9), 'b': [0.] * 4 + [1.] * 5})\n    path = base_dir / \"test.parquet\"\n    pq.write_table(table, path, row_group_size=row_group_size)\n    return table, path\n\n\ndef _create_directory_of_files(base_dir):\n    table1 = pa.table({'a': range(9), 'b': [0.] * 4 + [1.] * 5})\n    path1 = base_dir / \"test1.parquet\"\n    pq.write_table(table1, path1)\n    table2 = pa.table({'a': range(9, 18), 'b': [0.] * 4 + [1.] * 5})\n    path2 = base_dir / \"test2.parquet\"\n    pq.write_table(table2, path2)\n    return (table1, table2), (path1, path2)\n\n\ndef _check_dataset(dataset, table, dataset_reader, pickler):\n    # also test that pickle roundtrip keeps the functionality\n    for d in [dataset, pickler.loads(pickler.dumps(dataset))]:\n        assert dataset.schema.equals(table.schema)\n        assert dataset_reader.to_table(dataset).equals(table)\n\n\ndef _check_dataset_from_path(path, table, dataset_reader, pickler, **kwargs):\n    # pathlib object\n    assert isinstance(path, pathlib.Path)\n\n    # accept Path, str, List[Path], List[str]\n    for p in [path, str(path), [path], [str(path)]]:\n        dataset = ds.dataset(path, **kwargs)\n        assert isinstance(dataset, ds.FileSystemDataset)\n        _check_dataset(dataset, table, dataset_reader, pickler)\n\n    # relative string path\n    with change_cwd(path.parent):\n        dataset = ds.dataset(path.name, **kwargs)\n        assert isinstance(dataset, ds.FileSystemDataset)\n        _check_dataset(dataset, table, dataset_reader, pickler)\n\n\n@pytest.mark.parquet\ndef test_open_dataset_single_file(tempdir, dataset_reader, pickle_module):\n    table, path = _create_single_file(tempdir)\n    _check_dataset_from_path(path, table, dataset_reader, pickle_module)\n\n\n@pytest.mark.parquet\ndef test_deterministic_row_order(tempdir, dataset_reader, pickle_module):\n    # ARROW-8447 Ensure that dataset.to_table (and Scanner::ToTable) returns a\n    # deterministic row ordering. This is achieved by constructing a single\n    # parquet file with one row per RowGroup.\n    table, path = _create_single_file(tempdir, row_group_size=1)\n    _check_dataset_from_path(path, table, dataset_reader, pickle_module)\n\n\n@pytest.mark.parquet\ndef test_open_dataset_directory(tempdir, dataset_reader, pickle_module):\n    tables, _ = _create_directory_of_files(tempdir)\n    table = pa.concat_tables(tables)\n    _check_dataset_from_path(tempdir, table, dataset_reader, pickle_module)\n\n\n@pytest.mark.parquet\ndef test_open_dataset_list_of_files(tempdir, dataset_reader, pickle_module):\n    tables, (path1, path2) = _create_directory_of_files(tempdir)\n    table = pa.concat_tables(tables)\n\n    datasets = [\n        ds.dataset([path1, path2]),\n        ds.dataset([str(path1), str(path2)])\n    ]\n    datasets += [\n        pickle_module.loads(pickle_module.dumps(d)) for d in datasets\n    ]\n\n    for dataset in datasets:\n        assert dataset.schema.equals(table.schema)\n        result = dataset_reader.to_table(dataset)\n        assert result.equals(table)\n\n\n@pytest.mark.parquet\ndef test_open_dataset_filesystem_fspath(tempdir):\n    # single file\n    table, path = _create_single_file(tempdir)\n\n    fspath = FSProtocolClass(path)\n\n    # filesystem inferred from path\n    dataset1 = ds.dataset(fspath)\n    assert dataset1.schema.equals(table.schema)\n\n    # filesystem specified\n    dataset2 = ds.dataset(fspath, filesystem=fs.LocalFileSystem())\n    assert dataset2.schema.equals(table.schema)\n\n    # passing different filesystem\n    with pytest.raises(TypeError):\n        ds.dataset(fspath, filesystem=fs._MockFileSystem())\n\n\n@pytest.mark.parquet\ndef test_construct_from_single_file(tempdir, dataset_reader, pickle_module):\n    directory = tempdir / 'single-file'\n    directory.mkdir()\n    table, path = _create_single_file(directory)\n    relative_path = path.relative_to(directory)\n\n    # instantiate from a single file\n    d1 = ds.dataset(path)\n    # instantiate from a single file with a filesystem object\n    d2 = ds.dataset(path, filesystem=fs.LocalFileSystem())\n    # instantiate from a single file with prefixed filesystem URI\n    d3 = ds.dataset(str(relative_path), filesystem=_filesystem_uri(directory))\n    # pickle roundtrip\n    d4 = pickle_module.loads(pickle_module.dumps(d1))\n\n    assert dataset_reader.to_table(d1) == dataset_reader.to_table(\n        d2) == dataset_reader.to_table(d3) == dataset_reader.to_table(d4)\n\n\n@pytest.mark.parquet\ndef test_construct_from_single_directory(tempdir, dataset_reader, pickle_module):\n    directory = tempdir / 'single-directory'\n    directory.mkdir()\n    tables, paths = _create_directory_of_files(directory)\n\n    d1 = ds.dataset(directory)\n    d2 = ds.dataset(directory, filesystem=fs.LocalFileSystem())\n    d3 = ds.dataset(directory.name, filesystem=_filesystem_uri(tempdir))\n    t1 = dataset_reader.to_table(d1)\n    t2 = dataset_reader.to_table(d2)\n    t3 = dataset_reader.to_table(d3)\n    assert t1 == t2 == t3\n\n    # test pickle roundtrip\n    for d in [d1, d2, d3]:\n        restored = pickle_module.loads(pickle_module.dumps(d))\n        assert dataset_reader.to_table(restored) == t1\n\n\n@pytest.mark.parquet\ndef test_construct_from_list_of_files(tempdir, dataset_reader):\n    # instantiate from a list of files\n    directory = tempdir / 'list-of-files'\n    directory.mkdir()\n    tables, paths = _create_directory_of_files(directory)\n\n    relative_paths = [p.relative_to(tempdir) for p in paths]\n    with change_cwd(tempdir):\n        d1 = ds.dataset(relative_paths)\n        t1 = dataset_reader.to_table(d1)\n        assert len(t1) == sum(map(len, tables))\n\n    d2 = ds.dataset(relative_paths, filesystem=_filesystem_uri(tempdir))\n    t2 = dataset_reader.to_table(d2)\n    d3 = ds.dataset(paths)\n    t3 = dataset_reader.to_table(d3)\n    d4 = ds.dataset(paths, filesystem=fs.LocalFileSystem())\n    t4 = dataset_reader.to_table(d4)\n\n    assert t1 == t2 == t3 == t4\n\n\n@pytest.mark.parquet\ndef test_construct_from_list_of_mixed_paths_fails(mockfs):\n    # instantiate from a list of mixed paths\n    files = [\n        'subdir/1/xxx/file0.parquet',\n        'subdir/1/xxx/doesnt-exist.parquet',\n    ]\n    with pytest.raises(FileNotFoundError, match='doesnt-exist'):\n        ds.dataset(files, filesystem=mockfs)\n\n\n@pytest.mark.parquet\ndef test_construct_from_mixed_child_datasets(mockfs):\n    # instantiate from a list of mixed paths\n    a = ds.dataset(['subdir/1/xxx/file0.parquet',\n                    'subdir/2/yyy/file1.parquet'], filesystem=mockfs)\n    b = ds.dataset('subdir', filesystem=mockfs)\n\n    dataset = ds.dataset([a, b])\n\n    assert isinstance(dataset, ds.UnionDataset)\n    assert len(list(dataset.get_fragments())) == 4\n\n    table = dataset.to_table()\n    assert len(table) == 20\n    assert table.num_columns == 5\n\n    assert len(dataset.children) == 2\n    for child in dataset.children:\n        assert child.files == ['subdir/1/xxx/file0.parquet',\n                               'subdir/2/yyy/file1.parquet']\n\n\ndef test_construct_empty_dataset():\n    empty = ds.dataset([], format='ipc')\n    table = empty.to_table()\n    assert table.num_rows == 0\n    assert table.num_columns == 0\n\n\ndef test_construct_dataset_with_invalid_schema():\n    empty = ds.dataset([], format='ipc', schema=pa.schema([\n        ('a', pa.int64()),\n        ('a', pa.string())\n    ]))\n    with pytest.raises(ValueError, match='Multiple matches for .*a.* in '):\n        empty.to_table()\n\n\ndef test_construct_from_invalid_sources_raise(multisourcefs):\n    child1 = ds.FileSystemDatasetFactory(\n        multisourcefs,\n        fs.FileSelector('/plain'),\n        format=ds.ParquetFileFormat()\n    )\n    child2 = ds.FileSystemDatasetFactory(\n        multisourcefs,\n        fs.FileSelector('/schema'),\n        format=ds.ParquetFileFormat()\n    )\n    batch1 = pa.RecordBatch.from_arrays([pa.array(range(10))], names=[\"a\"])\n    batch2 = pa.RecordBatch.from_arrays([pa.array(range(10))], names=[\"b\"])\n\n    with pytest.raises(TypeError, match='Expected.*FileSystemDatasetFactory'):\n        ds.dataset([child1, child2])\n\n    expected = (\n        \"Expected a list of path-like or dataset objects, or a list \"\n        \"of batches or tables. The given list contains the following \"\n        \"types: int\"\n    )\n    with pytest.raises(TypeError, match=expected):\n        ds.dataset([1, 2, 3])\n\n    expected = (\n        \"Expected a path-like, list of path-likes or a list of Datasets \"\n        \"instead of the given type: NoneType\"\n    )\n    with pytest.raises(TypeError, match=expected):\n        ds.dataset(None)\n\n    expected = (\n        \"Expected a path-like, list of path-likes or a list of Datasets \"\n        \"instead of the given type: generator\"\n    )\n    with pytest.raises(TypeError, match=expected):\n        ds.dataset((batch1 for _ in range(3)))\n\n    expected = (\n        \"Must provide schema to construct in-memory dataset from an empty list\"\n    )\n    with pytest.raises(ValueError, match=expected):\n        ds.InMemoryDataset([])\n\n    expected = (\n        \"Item has schema\\nb: int64\\nwhich does not match expected schema\\n\"\n        \"a: int64\"\n    )\n    with pytest.raises(TypeError, match=expected):\n        ds.dataset([batch1, batch2])\n\n    expected = (\n        \"Expected a list of path-like or dataset objects, or a list of \"\n        \"batches or tables. The given list contains the following types:\"\n    )\n    with pytest.raises(TypeError, match=expected):\n        ds.dataset([batch1, 0])\n\n    expected = (\n        \"Expected a list of tables or batches. The given list contains a int\"\n    )\n    with pytest.raises(TypeError, match=expected):\n        ds.InMemoryDataset([batch1, 0])\n\n\ndef test_construct_in_memory(dataset_reader):\n    batch = pa.RecordBatch.from_arrays([pa.array(range(10))], names=[\"a\"])\n    table = pa.Table.from_batches([batch])\n\n    dataset_table = ds.dataset([], format='ipc', schema=pa.schema([])\n                               ).to_table()\n    assert dataset_table == pa.table([])\n\n    for source in (batch, table, [batch], [table]):\n        dataset = ds.dataset(source)\n        assert dataset_reader.to_table(dataset) == table\n        assert len(list(dataset.get_fragments())) == 1\n        assert next(dataset.get_fragments()).to_table() == table\n        assert pa.Table.from_batches(list(dataset.to_batches())) == table\n\n\n@pytest.mark.parametrize('use_threads', [False, True])\ndef test_scan_iterator(use_threads):\n    batch = pa.RecordBatch.from_arrays([pa.array(range(10))], names=[\"a\"])\n    table = pa.Table.from_batches([batch])\n    # When constructed from readers/iterators, should be one-shot\n    match = \"OneShotFragment was already scanned\"\n    for factory, schema in (\n            (lambda: pa.RecordBatchReader.from_batches(\n                batch.schema, [batch]), None),\n            (lambda: (batch for _ in range(1)), batch.schema),\n    ):\n        # Scanning the fragment consumes the underlying iterator\n        scanner = ds.Scanner.from_batches(\n            factory(), schema=schema, use_threads=use_threads)\n        assert scanner.to_table() == table\n        with pytest.raises(pa.ArrowInvalid, match=match):\n            scanner.to_table()\n\n\ndef _create_partitioned_dataset(basedir):\n    table = pa.table({'a': range(9), 'b': [0.] * 4 + [1.] * 5})\n\n    path = basedir / \"dataset-partitioned\"\n    path.mkdir()\n\n    for i in range(3):\n        part = path / \"part={}\".format(i)\n        part.mkdir()\n        pq.write_table(table.slice(3*i, 3), part / \"test.parquet\")\n\n    full_table = table.append_column(\n        \"part\", pa.array(np.repeat([0, 1, 2], 3), type=pa.int32()))\n\n    return full_table, path\n\n\n@pytest.mark.parquet\ndef test_open_dataset_partitioned_directory(tempdir, dataset_reader, pickle_module):\n    full_table, path = _create_partitioned_dataset(tempdir)\n\n    # no partitioning specified, just read all individual files\n    table = full_table.select(['a', 'b'])\n    _check_dataset_from_path(path, table, dataset_reader, pickle_module)\n\n    # specify partition scheme with discovery\n    dataset = ds.dataset(\n        str(path), partitioning=ds.partitioning(flavor=\"hive\"))\n    assert dataset.schema.equals(full_table.schema)\n\n    # specify partition scheme with discovery and relative path\n    with change_cwd(tempdir):\n        dataset = ds.dataset(\"dataset-partitioned/\",\n                             partitioning=ds.partitioning(flavor=\"hive\"))\n        assert dataset.schema.equals(full_table.schema)\n\n    # specify partition scheme with string short-cut\n    dataset = ds.dataset(str(path), partitioning=\"hive\")\n    assert dataset.schema.equals(full_table.schema)\n\n    # specify partition scheme with explicit scheme\n    dataset = ds.dataset(\n        str(path),\n        partitioning=ds.partitioning(\n            pa.schema([(\"part\", pa.int8())]), flavor=\"hive\"))\n    expected_schema = table.schema.append(pa.field(\"part\", pa.int8()))\n    assert dataset.schema.equals(expected_schema)\n\n    result = dataset.to_table()\n    expected = table.append_column(\n        \"part\", pa.array(np.repeat([0, 1, 2], 3), type=pa.int8()))\n    assert result.equals(expected)\n\n\n@pytest.mark.parquet\ndef test_open_dataset_filesystem(tempdir):\n    # single file\n    table, path = _create_single_file(tempdir)\n\n    # filesystem inferred from path\n    dataset1 = ds.dataset(str(path))\n    assert dataset1.schema.equals(table.schema)\n\n    # filesystem specified\n    dataset2 = ds.dataset(str(path), filesystem=fs.LocalFileSystem())\n    assert dataset2.schema.equals(table.schema)\n\n    # local filesystem specified with relative path\n    with change_cwd(tempdir):\n        dataset3 = ds.dataset(\"test.parquet\", filesystem=fs.LocalFileSystem())\n    assert dataset3.schema.equals(table.schema)\n\n    # passing different filesystem\n    with pytest.raises(FileNotFoundError):\n        ds.dataset(str(path), filesystem=fs._MockFileSystem())\n\n\n@pytest.mark.parquet\ndef test_open_dataset_unsupported_format(tempdir):\n    _, path = _create_single_file(tempdir)\n    with pytest.raises(ValueError, match=\"format 'blabla' is not supported\"):\n        ds.dataset([path], format=\"blabla\")\n\n\n@pytest.mark.parquet\ndef test_open_union_dataset(tempdir, dataset_reader, pickle_module):\n    _, path = _create_single_file(tempdir)\n    dataset = ds.dataset(path)\n\n    union = ds.dataset([dataset, dataset])\n    assert isinstance(union, ds.UnionDataset)\n\n    pickled = pickle_module.loads(pickle_module.dumps(union))\n    assert dataset_reader.to_table(pickled) == dataset_reader.to_table(union)\n\n\ndef test_open_union_dataset_with_additional_kwargs(multisourcefs):\n    child = ds.dataset('/plain', filesystem=multisourcefs, format='parquet')\n    with pytest.raises(ValueError, match=\"cannot pass any additional\"):\n        ds.dataset([child], format=\"parquet\")\n\n\ndef test_open_dataset_non_existing_file():\n    # ARROW-8213: Opening a dataset with a local incorrect path gives confusing\n    #             error message\n    with pytest.raises(FileNotFoundError):\n        ds.dataset('i-am-not-existing.arrow', format='ipc')\n\n    with pytest.raises(pa.ArrowInvalid, match='cannot be relative'):\n        ds.dataset('file:i-am-not-existing.arrow', format='ipc')\n\n\n@pytest.mark.parquet\n@pytest.mark.parametrize('partitioning', [\"directory\", \"hive\"])\n@pytest.mark.parametrize('null_fallback', ['xyz', None])\n@pytest.mark.parametrize('infer_dictionary', [False, True])\n@pytest.mark.parametrize('partition_keys', [\n    ([\"A\", \"B\", \"C\"], [1, 2, 3]),\n    ([1, 2, 3], [\"A\", \"B\", \"C\"]),\n    ([\"A\", \"B\", \"C\"], [\"D\", \"E\", \"F\"]),\n    ([1, 2, 3], [4, 5, 6]),\n    ([1, None, 3], [\"A\", \"B\", \"C\"]),\n    ([1, 2, 3], [\"A\", None, \"C\"]),\n    ([None, 2, 3], [None, 2, 3]),\n])\ndef test_partition_discovery(\n    tempdir, partitioning, null_fallback, infer_dictionary, partition_keys\n):\n    # ARROW-9288 / ARROW-9476\n    table = pa.table({'a': range(9), 'b': [0.0] * 4 + [1.0] * 5})\n\n    has_null = None in partition_keys[0] or None in partition_keys[1]\n    if partitioning == \"directory\" and has_null:\n        # Directory partitioning can't handle the first part being null\n        return\n\n    if partitioning == \"directory\":\n        partitioning = ds.DirectoryPartitioning.discover(\n            [\"part1\", \"part2\"], infer_dictionary=infer_dictionary)\n        fmt = \"{0}/{1}\"\n        null_value = None\n    else:\n        if null_fallback:\n            partitioning = ds.HivePartitioning.discover(\n                infer_dictionary=infer_dictionary, null_fallback=null_fallback\n            )\n        else:\n            partitioning = ds.HivePartitioning.discover(\n                infer_dictionary=infer_dictionary)\n        fmt = \"part1={0}/part2={1}\"\n        if null_fallback:\n            null_value = null_fallback\n        else:\n            null_value = \"__HIVE_DEFAULT_PARTITION__\"\n\n    basepath = tempdir / \"dataset\"\n    basepath.mkdir()\n\n    part_keys1, part_keys2 = partition_keys\n    for part1 in part_keys1:\n        for part2 in part_keys2:\n            path = basepath / \\\n                fmt.format(part1 or null_value, part2 or null_value)\n            path.mkdir(parents=True)\n            pq.write_table(table, path / \"test.parquet\")\n\n    dataset = ds.dataset(str(basepath), partitioning=partitioning)\n\n    def expected_type(key):\n        if infer_dictionary:\n            value_type = pa.string() if isinstance(key, str) else pa.int32()\n            return pa.dictionary(pa.int32(), value_type)\n        else:\n            return pa.string() if isinstance(key, str) else pa.int32()\n    expected_schema = table.schema.append(\n        pa.field(\"part1\", expected_type(part_keys1[0]))\n    ).append(\n        pa.field(\"part2\", expected_type(part_keys2[0]))\n    )\n    assert dataset.schema.equals(expected_schema)\n\n\n@pytest.mark.pandas\ndef test_dataset_partitioned_dictionary_type_reconstruct(tempdir, pickle_module):\n    # https://issues.apache.org/jira/browse/ARROW-11400\n    table = pa.table({'part': np.repeat(['A', 'B'], 5), 'col': range(10)})\n    part = ds.partitioning(table.select(['part']).schema, flavor=\"hive\")\n    ds.write_dataset(table, tempdir, partitioning=part, format=\"feather\")\n\n    dataset = ds.dataset(\n        tempdir, format=\"feather\",\n        partitioning=ds.HivePartitioning.discover(infer_dictionary=True)\n    )\n    expected = pa.table(\n        {'col': table['col'], 'part': table['part'].dictionary_encode()}\n    )\n    assert dataset.to_table().equals(expected)\n    fragment = list(dataset.get_fragments())[0]\n    assert fragment.to_table(schema=dataset.schema).equals(expected[:5])\n    part_expr = fragment.partition_expression\n\n    restored = pickle_module.loads(pickle_module.dumps(dataset))\n    assert restored.to_table().equals(expected)\n\n    restored = pickle_module.loads(pickle_module.dumps(fragment))\n    assert restored.to_table(schema=dataset.schema).equals(expected[:5])\n    # to_pandas call triggers computation of the actual dictionary values\n    assert restored.to_table(schema=dataset.schema).to_pandas().equals(\n        expected[:5].to_pandas()\n    )\n    assert restored.partition_expression.equals(part_expr)\n\n\n@pytest.fixture\ndef s3_example_simple(s3_server):\n    from pyarrow.fs import FileSystem\n\n    host, port, access_key, secret_key = s3_server['connection']\n    uri = (\n        \"s3://{}:{}@mybucket/data.parquet?scheme=http&endpoint_override={}:{}\"\n        \"&allow_bucket_creation=True\"\n        .format(access_key, secret_key, host, port)\n    )\n\n    fs, path = FileSystem.from_uri(uri)\n\n    fs.create_dir(\"mybucket\")\n    table = pa.table({'a': [1, 2, 3]})\n    with fs.open_output_stream(\"mybucket/data.parquet\") as out:\n        pq.write_table(table, out)\n\n    return table, path, fs, uri, host, port, access_key, secret_key\n\n\n@pytest.mark.parquet\n@pytest.mark.s3\ndef test_open_dataset_from_uri_s3(s3_example_simple, dataset_reader):\n    # open dataset from non-localfs string path\n    table, path, fs, uri, _, _, _, _ = s3_example_simple\n\n    # full string URI\n    dataset = ds.dataset(uri, format=\"parquet\")\n    assert dataset_reader.to_table(dataset).equals(table)\n\n    # passing filesystem object\n    dataset = ds.dataset(path, format=\"parquet\", filesystem=fs)\n    assert dataset_reader.to_table(dataset).equals(table)\n\n\n@pytest.mark.parquet\n@pytest.mark.s3\ndef test_open_dataset_from_fileinfos(s3_example_simple, dataset_reader):\n    table, path, filesystem, uri, _, _, _, _ = s3_example_simple\n    selector = fs.FileSelector(\"mybucket\")\n    finfos = filesystem.get_file_info(selector)\n    dataset = ds.dataset(finfos, format=\"parquet\", filesystem=filesystem)\n    assert dataset_reader.to_table(dataset).equals(table)\n\n\n@pytest.mark.parquet\n@pytest.mark.s3  # still needed to create the data\ndef test_open_dataset_from_uri_s3_fsspec(s3_example_simple):\n    table, path, _, _, host, port, access_key, secret_key = s3_example_simple\n    s3fs = pytest.importorskip(\"s3fs\")\n\n    from pyarrow.fs import FSSpecHandler, PyFileSystem\n\n    fs = s3fs.S3FileSystem(\n        key=access_key,\n        secret=secret_key,\n        client_kwargs={\n            'endpoint_url': 'http://{}:{}'.format(host, port)\n        }\n    )\n\n    # passing as fsspec filesystem\n    dataset = ds.dataset(path, format=\"parquet\", filesystem=fs)\n    assert dataset.to_table().equals(table)\n\n    # directly passing the fsspec-handler\n    fs = PyFileSystem(FSSpecHandler(fs))\n    dataset = ds.dataset(path, format=\"parquet\", filesystem=fs)\n    assert dataset.to_table().equals(table)\n\n\n@pytest.mark.parquet\n@pytest.mark.s3\ndef test_open_dataset_from_s3_with_filesystem_uri(s3_server):\n    from pyarrow.fs import FileSystem\n\n    host, port, access_key, secret_key = s3_server['connection']\n    bucket = 'theirbucket'\n    path = 'nested/folder/data.parquet'\n    uri = \"s3://{}:{}@{}/{}?scheme=http&endpoint_override={}:{}\"\\\n        \"&allow_bucket_creation=true\".format(\n            access_key, secret_key, bucket, path, host, port\n        )\n\n    fs, path = FileSystem.from_uri(uri)\n    assert path == 'theirbucket/nested/folder/data.parquet'\n\n    fs.create_dir(bucket)\n\n    table = pa.table({'a': [1, 2, 3]})\n    with fs.open_output_stream(path) as out:\n        pq.write_table(table, out)\n\n    # full string URI\n    dataset = ds.dataset(uri, format=\"parquet\")\n    assert dataset.to_table().equals(table)\n\n    # passing filesystem as an uri\n    template = (\n        \"s3://{}:{}@{{}}?scheme=http&endpoint_override={}:{}\".format(\n            access_key, secret_key, host, port\n        )\n    )\n    cases = [\n        ('theirbucket/nested/folder/', '/data.parquet'),\n        ('theirbucket/nested/folder', 'data.parquet'),\n        ('theirbucket/nested/', 'folder/data.parquet'),\n        ('theirbucket/nested', 'folder/data.parquet'),\n        ('theirbucket', '/nested/folder/data.parquet'),\n        ('theirbucket', 'nested/folder/data.parquet'),\n    ]\n    for prefix, path in cases:\n        uri = template.format(prefix)\n        dataset = ds.dataset(path, filesystem=uri, format=\"parquet\")\n        assert dataset.to_table().equals(table)\n\n    with pytest.raises(pa.ArrowInvalid, match='Missing bucket name'):\n        uri = template.format('/')\n        ds.dataset('/theirbucket/nested/folder/data.parquet', filesystem=uri)\n\n    error = (\n        \"The path component of the filesystem URI must point to a directory \"\n        \"but it has a type: `{}`. The path component is `{}` and the given \"\n        \"filesystem URI is `{}`\"\n    )\n\n    path = 'theirbucket/doesnt/exist'\n    uri = template.format(path)\n    with pytest.raises(ValueError) as exc:\n        ds.dataset('data.parquet', filesystem=uri)\n    assert str(exc.value) == error.format('NotFound', path, uri)\n\n    path = 'theirbucket/nested/folder/data.parquet'\n    uri = template.format(path)\n    with pytest.raises(ValueError) as exc:\n        ds.dataset('data.parquet', filesystem=uri)\n    assert str(exc.value) == error.format('File', path, uri)\n\n\n@pytest.mark.parquet\ndef test_open_dataset_from_fsspec(tempdir):\n    table, path = _create_single_file(tempdir)\n\n    fsspec = pytest.importorskip(\"fsspec\")\n\n    localfs = fsspec.filesystem(\"file\")\n    dataset = ds.dataset(path, filesystem=localfs)\n    assert dataset.schema.equals(table.schema)\n\n\n@pytest.mark.parquet\ndef test_file_format_inspect_fsspec(tempdir):\n    # https://issues.apache.org/jira/browse/ARROW-16413\n    fsspec = pytest.importorskip(\"fsspec\")\n\n    # create bucket + file with pyarrow\n    table = pa.table({'a': [1, 2, 3]})\n    path = tempdir / \"data.parquet\"\n    pq.write_table(table, path)\n\n    # read using fsspec filesystem\n    fsspec_fs = fsspec.filesystem(\"file\")\n    assert fsspec_fs.ls(tempdir)[0].endswith(\"data.parquet\")\n\n    # inspect using dataset file format\n    format = ds.ParquetFileFormat()\n    # manually creating a PyFileSystem instead of using fs._ensure_filesystem\n    # which would convert an fsspec local filesystem to a native one\n    filesystem = fs.PyFileSystem(fs.FSSpecHandler(fsspec_fs))\n    schema = format.inspect(path, filesystem)\n    assert schema.equals(table.schema)\n\n    fragment = format.make_fragment(path, filesystem)\n    assert fragment.physical_schema.equals(table.schema)\n\n\n@pytest.mark.pandas\ndef test_filter_timestamp(tempdir, dataset_reader):\n    # ARROW-11379\n    path = tempdir / \"test_partition_timestamps\"\n\n    table = pa.table({\n        \"dates\": ['2012-01-01', '2012-01-02'] * 5,\n        \"id\": range(10)})\n\n    # write dataset partitioned on dates (as strings)\n    part = ds.partitioning(table.select(['dates']).schema, flavor=\"hive\")\n    ds.write_dataset(table, path, partitioning=part, format=\"feather\")\n\n    # read dataset partitioned on dates (as timestamps)\n    part = ds.partitioning(pa.schema([(\"dates\", pa.timestamp(\"s\"))]),\n                           flavor=\"hive\")\n    dataset = ds.dataset(path, format=\"feather\", partitioning=part)\n\n    condition = ds.field(\"dates\") > pd.Timestamp(\"2012-01-01\")\n    table = dataset_reader.to_table(dataset, filter=condition)\n    assert table.column('id').to_pylist() == [1, 3, 5, 7, 9]\n\n    import datetime\n    condition = ds.field(\"dates\") > datetime.datetime(2012, 1, 1)\n    table = dataset_reader.to_table(dataset, filter=condition)\n    assert table.column('id').to_pylist() == [1, 3, 5, 7, 9]\n\n\n@pytest.mark.parquet\ndef test_filter_implicit_cast(tempdir, dataset_reader):\n    # ARROW-7652\n    table = pa.table({'a': pa.array([0, 1, 2, 3, 4, 5], type=pa.int8())})\n    _, path = _create_single_file(tempdir, table)\n    dataset = ds.dataset(str(path))\n\n    filter_ = ds.field('a') > 2\n    assert len(dataset_reader.to_table(dataset, filter=filter_)) == 3\n\n\n@pytest.mark.parquet\ndef test_filter_equal_null(tempdir, dataset_reader):\n    # ARROW-12066 equality with null, although not useful, should not crash\n    table = pa.table({\"A\": [\"a\", \"b\", None]})\n    _, path = _create_single_file(tempdir, table)\n    dataset = ds.dataset(str(path))\n\n    table = dataset_reader.to_table(\n        dataset, filter=ds.field(\"A\") == ds.scalar(None)\n    )\n    assert table.num_rows == 0\n\n\n@pytest.mark.parquet\ndef test_filter_compute_expression(tempdir, dataset_reader):\n    table = pa.table({\n        \"A\": [\"a\", \"b\", None, \"a\", \"c\"],\n        \"B\": [datetime.datetime(2022, 1, 1, i) for i in range(5)],\n        \"C\": [datetime.datetime(2022, 1, i) for i in range(1, 6)],\n    })\n    _, path = _create_single_file(tempdir, table)\n    dataset = ds.dataset(str(path))\n\n    filter_ = pc.is_in(ds.field('A'), pa.array([\"a\", \"b\"]))\n    assert dataset_reader.to_table(dataset, filter=filter_).num_rows == 3\n\n    filter_ = pc.hour(ds.field('B')) >= 3\n    assert dataset_reader.to_table(dataset, filter=filter_).num_rows == 2\n\n    days = pc.days_between(ds.field('B'), ds.field(\"C\"))\n    result = dataset_reader.to_table(dataset, columns={\"days\": days})\n    assert result[\"days\"].to_pylist() == [0, 1, 2, 3, 4]\n\n\ndef test_dataset_union(multisourcefs):\n    child = ds.FileSystemDatasetFactory(\n        multisourcefs, fs.FileSelector('/plain'),\n        format=ds.ParquetFileFormat()\n    )\n    factory = ds.UnionDatasetFactory([child])\n\n    # TODO(bkietz) reintroduce factory.children property\n    assert len(factory.inspect_schemas()) == 1\n    assert all(isinstance(s, pa.Schema) for s in factory.inspect_schemas())\n    assert factory.inspect_schemas()[0].equals(child.inspect())\n    assert factory.inspect().equals(child.inspect())\n    assert isinstance(factory.finish(), ds.Dataset)\n\n\ndef test_union_dataset_from_other_datasets(tempdir, multisourcefs):\n    child1 = ds.dataset('/plain', filesystem=multisourcefs, format='parquet')\n    child2 = ds.dataset('/schema', filesystem=multisourcefs, format='parquet',\n                        partitioning=['week', 'color'])\n    child3 = ds.dataset('/hive', filesystem=multisourcefs, format='parquet',\n                        partitioning='hive')\n\n    assert child1.schema != child2.schema != child3.schema\n\n    assembled = ds.dataset([child1, child2, child3])\n    assert isinstance(assembled, ds.UnionDataset)\n\n    msg = 'cannot pass any additional arguments'\n    with pytest.raises(ValueError, match=msg):\n        ds.dataset([child1, child2], filesystem=multisourcefs)\n\n    expected_schema = pa.schema([\n        ('date', pa.date32()),\n        ('index', pa.int64()),\n        ('value', pa.float64()),\n        ('color', pa.string()),\n        ('week', pa.int32()),\n        ('year', pa.int32()),\n        ('month', pa.int32()),\n    ])\n    assert assembled.schema.equals(expected_schema)\n    assert assembled.to_table().schema.equals(expected_schema)\n\n    assembled = ds.dataset([child1, child3])\n    expected_schema = pa.schema([\n        ('date', pa.date32()),\n        ('index', pa.int64()),\n        ('value', pa.float64()),\n        ('color', pa.string()),\n        ('year', pa.int32()),\n        ('month', pa.int32()),\n    ])\n    assert assembled.schema.equals(expected_schema)\n    assert assembled.to_table().schema.equals(expected_schema)\n\n    expected_schema = pa.schema([\n        ('month', pa.int32()),\n        ('color', pa.string()),\n        ('date', pa.date32()),\n    ])\n    assembled = ds.dataset([child1, child3], schema=expected_schema)\n    assert assembled.to_table().schema.equals(expected_schema)\n\n    expected_schema = pa.schema([\n        ('month', pa.int32()),\n        ('color', pa.string()),\n        ('unknown', pa.string())  # fill with nulls\n    ])\n    assembled = ds.dataset([child1, child3], schema=expected_schema)\n    assert assembled.to_table().schema.equals(expected_schema)\n\n    # incompatible schemas, date and index columns have conflicting types\n    table = pa.table([range(9), [0.] * 4 + [1.] * 5, 'abcdefghj'],\n                     names=['date', 'value', 'index'])\n    _, path = _create_single_file(tempdir, table=table)\n    child4 = ds.dataset(path)\n\n    with pytest.raises(pa.ArrowTypeError, match='Unable to merge'):\n        ds.dataset([child1, child4])\n\n\ndef test_dataset_from_a_list_of_local_directories_raises(multisourcefs):\n    msg = 'points to a directory, but only file paths are supported'\n    with pytest.raises(IsADirectoryError, match=msg):\n        ds.dataset(['/plain', '/schema', '/hive'], filesystem=multisourcefs)\n\n\ndef test_union_dataset_filesystem_datasets(multisourcefs):\n    # without partitioning\n    dataset = ds.dataset([\n        ds.dataset('/plain', filesystem=multisourcefs),\n        ds.dataset('/schema', filesystem=multisourcefs),\n        ds.dataset('/hive', filesystem=multisourcefs),\n    ])\n    expected_schema = pa.schema([\n        ('date', pa.date32()),\n        ('index', pa.int64()),\n        ('value', pa.float64()),\n        ('color', pa.string()),\n    ])\n    assert dataset.schema.equals(expected_schema)\n\n    # with hive partitioning for two hive sources\n    dataset = ds.dataset([\n        ds.dataset('/plain', filesystem=multisourcefs),\n        ds.dataset('/schema', filesystem=multisourcefs),\n        ds.dataset('/hive', filesystem=multisourcefs, partitioning='hive')\n    ])\n    expected_schema = pa.schema([\n        ('date', pa.date32()),\n        ('index', pa.int64()),\n        ('value', pa.float64()),\n        ('color', pa.string()),\n        ('year', pa.int32()),\n        ('month', pa.int32()),\n    ])\n    assert dataset.schema.equals(expected_schema)\n\n\n@pytest.mark.parquet\ndef test_specified_schema(tempdir, dataset_reader):\n    table = pa.table({'a': [1, 2, 3], 'b': [.1, .2, .3]})\n    pq.write_table(table, tempdir / \"data.parquet\")\n\n    def _check_dataset(schema, expected, expected_schema=None):\n        dataset = ds.dataset(str(tempdir / \"data.parquet\"), schema=schema)\n        if expected_schema is not None:\n            assert dataset.schema.equals(expected_schema)\n        else:\n            assert dataset.schema.equals(schema)\n        result = dataset_reader.to_table(dataset)\n        assert result.equals(expected)\n\n    # no schema specified\n    schema = None\n    expected = table\n    _check_dataset(schema, expected, expected_schema=table.schema)\n\n    # identical schema specified\n    schema = table.schema\n    expected = table\n    _check_dataset(schema, expected)\n\n    # Specifying schema with change column order\n    schema = pa.schema([('b', 'float64'), ('a', 'int64')])\n    expected = pa.table([[.1, .2, .3], [1, 2, 3]], names=['b', 'a'])\n    _check_dataset(schema, expected)\n\n    # Specifying schema with missing column\n    schema = pa.schema([('a', 'int64')])\n    expected = pa.table([[1, 2, 3]], names=['a'])\n    _check_dataset(schema, expected)\n\n    # Specifying schema with additional column\n    schema = pa.schema([('a', 'int64'), ('c', 'int32')])\n    expected = pa.table([[1, 2, 3],\n                         pa.array([None, None, None], type='int32')],\n                        names=['a', 'c'])\n    _check_dataset(schema, expected)\n\n    # Specifying with differing field types\n    schema = pa.schema([('a', 'int32'), ('b', 'float64')])\n    dataset = ds.dataset(str(tempdir / \"data.parquet\"), schema=schema)\n    expected = pa.table([table['a'].cast('int32'),\n                         table['b']],\n                        names=['a', 'b'])\n    _check_dataset(schema, expected)\n\n    # Specifying with incompatible schema\n    schema = pa.schema([('a', pa.list_(pa.int32())), ('b', 'float64')])\n    dataset = ds.dataset(str(tempdir / \"data.parquet\"), schema=schema)\n    assert dataset.schema.equals(schema)\n    with pytest.raises(NotImplementedError,\n                       match='Unsupported cast from int64 to list'):\n        dataset_reader.to_table(dataset)\n\n\n@pytest.mark.parquet\ndef test_incompatible_schema_hang(tempdir, dataset_reader):\n    # ARROW-13480: deadlock when reading past an errored fragment\n\n    fn = tempdir / \"data.parquet\"\n    table = pa.table({'a': [1, 2, 3]})\n    pq.write_table(table, fn)\n\n    schema = pa.schema([('a', pa.null())])\n    dataset = ds.dataset([str(fn)] * 100, schema=schema)\n    assert dataset.schema.equals(schema)\n    scanner = dataset_reader.scanner(dataset)\n    with pytest.raises(NotImplementedError,\n                       match='Unsupported cast from int64 to null'):\n        reader = scanner.to_reader()\n        reader.read_all()\n\n\ndef test_ipc_format(tempdir, dataset_reader):\n    table = pa.table({'a': pa.array([1, 2, 3], type=\"int8\"),\n                      'b': pa.array([.1, .2, .3], type=\"float64\")})\n\n    path = str(tempdir / 'test.arrow')\n    with pa.output_stream(path) as sink:\n        writer = pa.RecordBatchFileWriter(sink, table.schema)\n        writer.write_batch(table.to_batches()[0])\n        writer.close()\n\n    dataset = ds.dataset(path, format=ds.IpcFileFormat())\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n    assert_dataset_fragment_convenience_methods(dataset)\n\n    for format_str in [\"ipc\", \"arrow\"]:\n        dataset = ds.dataset(path, format=format_str)\n        result = dataset_reader.to_table(dataset)\n        assert result.equals(table)\n\n\n@pytest.mark.orc\ndef test_orc_format(tempdir, dataset_reader):\n    from pyarrow import orc\n    table = pa.table({'a': pa.array([1, 2, 3], type=\"int8\"),\n                      'b': pa.array([.1, .2, .3], type=\"float64\")})\n\n    path = str(tempdir / 'test.orc')\n    orc.write_table(table, path)\n\n    dataset = ds.dataset(path, format=ds.OrcFileFormat())\n    fragments = list(dataset.get_fragments())\n    assert isinstance(fragments[0], ds.FileFragment)\n    result = dataset_reader.to_table(dataset)\n    result.validate(full=True)\n    assert result.equals(table)\n\n    assert_dataset_fragment_convenience_methods(dataset)\n\n    dataset = ds.dataset(path, format=\"orc\")\n    result = dataset_reader.to_table(dataset)\n    result.validate(full=True)\n    assert result.equals(table)\n\n    result = dataset_reader.to_table(dataset, columns=[\"b\"])\n    result.validate(full=True)\n    assert result.equals(table.select([\"b\"]))\n\n    result = dataset_reader.to_table(\n        dataset, columns={\"b2\": ds.field(\"b\") * 2}\n    )\n    result.validate(full=True)\n    assert result.equals(\n        pa.table({'b2': pa.array([.2, .4, .6], type=\"float64\")})\n    )\n\n    assert dataset_reader.count_rows(dataset) == 3\n    assert dataset_reader.count_rows(dataset, filter=ds.field(\"a\") > 2) == 1\n\n\n@pytest.mark.orc\ndef test_orc_scan_options(tempdir, dataset_reader):\n    from pyarrow import orc\n    table = pa.table({'a': pa.array([1, 2, 3], type=\"int8\"),\n                      'b': pa.array([.1, .2, .3], type=\"float64\")})\n\n    path = str(tempdir / 'test.orc')\n    orc.write_table(table, path)\n\n    dataset = ds.dataset(path, format=\"orc\")\n    result = list(dataset_reader.to_batches(dataset))\n    assert len(result) == 1\n    assert result[0].num_rows == 3\n    assert result[0].equals(table.to_batches()[0])\n    # TODO batch_size is not yet supported (ARROW-14153)\n    # result = list(dataset_reader.to_batches(dataset, batch_size=2))\n    # assert len(result) == 2\n    # assert result[0].num_rows == 2\n    # assert result[0].equals(table.slice(0, 2).to_batches()[0])\n    # assert result[1].num_rows == 1\n    # assert result[1].equals(table.slice(2, 1).to_batches()[0])\n\n\ndef test_orc_format_not_supported():\n    try:\n        from pyarrow.dataset import OrcFileFormat  # noqa\n    except ImportError:\n        # ORC is not available, test error message\n        with pytest.raises(\n            ValueError, match=\"not built with support for the ORC file\"\n        ):\n            ds.dataset(\".\", format=\"orc\")\n\n\n@pytest.mark.orc\ndef test_orc_writer_not_implemented_for_dataset():\n    with pytest.raises(\n        NotImplementedError,\n        match=\"Writing datasets not yet implemented for this file format\"\n    ):\n        ds.write_dataset(\n            pa.table({\"a\": range(10)}), format='orc', base_dir='/tmp'\n        )\n\n    of = ds.OrcFileFormat()\n    with pytest.raises(\n        NotImplementedError,\n        match=\"Writing datasets not yet implemented for this file format\"\n    ):\n        of.make_write_options()\n\n\n@pytest.mark.pandas\ndef test_csv_format(tempdir, dataset_reader):\n    table = pa.table({'a': pa.array([1, 2, 3], type=\"int64\"),\n                      'b': pa.array([.1, .2, .3], type=\"float64\")})\n\n    path = str(tempdir / 'test.csv')\n    table.to_pandas().to_csv(path, index=False)\n\n    dataset = ds.dataset(path, format=ds.CsvFileFormat())\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n    assert_dataset_fragment_convenience_methods(dataset)\n\n    dataset = ds.dataset(path, format='csv')\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\"compression\", [\n    \"bz2\",\n    \"gzip\",\n    \"lz4\",\n    \"zstd\",\n])\ndef test_csv_format_compressed(tempdir, compression, dataset_reader):\n    if not pyarrow.Codec.is_available(compression):\n        pytest.skip(\"{} support is not built\".format(compression))\n    table = pa.table({'a': pa.array([1, 2, 3], type=\"int64\"),\n                      'b': pa.array([.1, .2, .3], type=\"float64\")})\n    filesystem = fs.LocalFileSystem()\n    suffix = compression if compression != 'gzip' else 'gz'\n    path = str(tempdir / f'test.csv.{suffix}')\n    with filesystem.open_output_stream(path, compression=compression) as sink:\n        # https://github.com/pandas-dev/pandas/issues/23854\n        # With CI version of Pandas (anything < 1.2), Pandas tries to write\n        # str to the sink\n        csv_str = table.to_pandas().to_csv(index=False)\n        sink.write(csv_str.encode('utf-8'))\n\n    dataset = ds.dataset(path, format=ds.CsvFileFormat())\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n\ndef test_csv_format_options(tempdir, dataset_reader):\n    path = str(tempdir / 'test.csv')\n    with open(path, 'w') as sink:\n        sink.write('skipped\\ncol0\\nfoo\\nbar\\n')\n    dataset = ds.dataset(path, format='csv')\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(\n        pa.table({'skipped': pa.array(['col0', 'foo', 'bar'])}))\n\n    dataset = ds.dataset(path, format=ds.CsvFileFormat(\n        read_options=pa.csv.ReadOptions(skip_rows=1)))\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(pa.table({'col0': pa.array(['foo', 'bar'])}))\n\n    dataset = ds.dataset(path, format=ds.CsvFileFormat(\n        read_options=pa.csv.ReadOptions(column_names=['foo'])))\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(\n        pa.table({'foo': pa.array(['skipped', 'col0', 'foo', 'bar'])}))\n\n\ndef test_csv_format_options_generate_columns(tempdir, dataset_reader):\n    path = str(tempdir / 'test.csv')\n    with open(path, 'w') as sink:\n        sink.write('1,a,true,1\\n')\n\n    dataset = ds.dataset(path, format=ds.CsvFileFormat(\n        read_options=pa.csv.ReadOptions(autogenerate_column_names=True)))\n    result = dataset_reader.to_table(dataset)\n    expected_column_names = [\"f0\", \"f1\", \"f2\", \"f3\"]\n    assert result.column_names == expected_column_names\n    assert result.equals(pa.table({'f0': pa.array([1]),\n                                   'f1': pa.array([\"a\"]),\n                                   'f2': pa.array([True]),\n                                   'f3': pa.array([1])}))\n\n\ndef test_csv_fragment_options(tempdir, dataset_reader):\n    path = str(tempdir / 'test.csv')\n    with open(path, 'w') as sink:\n        sink.write('col0\\nfoo\\nspam\\nMYNULL\\n')\n    dataset = ds.dataset(path, format='csv')\n    convert_options = pyarrow.csv.ConvertOptions(null_values=['MYNULL'],\n                                                 strings_can_be_null=True)\n    options = ds.CsvFragmentScanOptions(\n        convert_options=convert_options,\n        read_options=pa.csv.ReadOptions(block_size=2**16))\n    result = dataset_reader.to_table(dataset, fragment_scan_options=options)\n    assert result.equals(pa.table({'col0': pa.array(['foo', 'spam', None])}))\n\n    csv_format = ds.CsvFileFormat(convert_options=convert_options)\n    dataset = ds.dataset(path, format=csv_format)\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(pa.table({'col0': pa.array(['foo', 'spam', None])}))\n\n    options = ds.CsvFragmentScanOptions()\n    result = dataset_reader.to_table(dataset, fragment_scan_options=options)\n    assert result.equals(\n        pa.table({'col0': pa.array(['foo', 'spam', 'MYNULL'])}))\n\n\n@pytest.mark.pandas\ndef test_json_format(tempdir, dataset_reader):\n    table = pa.table({'a': pa.array([1, 2, 3], type=\"int64\"),\n                      'b': pa.array([.1, .2, .3], type=\"float64\")})\n\n    path = str(tempdir / 'test.json')\n    out = table.to_pandas().to_json(orient='records')[1:-1].replace('},{', '}\\n{')\n    with open(path, 'w') as f:\n        f.write(out)\n\n    dataset = ds.dataset(path, format=ds.JsonFileFormat())\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n    assert_dataset_fragment_convenience_methods(dataset)\n\n    dataset = ds.dataset(path, format='json')\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n\n@pytest.mark.pandas\ndef test_json_format_options(tempdir, dataset_reader):\n    table = pa.table({'a': pa.array([1, 2, 3], type=\"int64\"),\n                      'b': pa.array([.1, .2, .3], type=\"float64\")})\n\n    path = str(tempdir / 'test.json')\n    out = table.to_pandas().to_json(orient='records')[1:-1].replace('},{', '}\\n{')\n    with open(path, 'w') as f:\n        f.write(out)\n\n    with pytest.raises(ValueError,\n                       match=\"try to increase block size\"):\n        dataset = ds.dataset(path, format=ds.JsonFileFormat(\n            read_options=pa.json.ReadOptions(block_size=4)))\n\n    dataset = ds.dataset(path, format=ds.JsonFileFormat(\n        read_options=pa.json.ReadOptions(block_size=64)))\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n\n@pytest.mark.pandas\ndef test_json_fragment_options(tempdir, dataset_reader):\n    table = pa.table({'a': pa.array([1, 2, 3], type=\"int64\"),\n                      'b': pa.array([.1, .2, .3], type=\"float64\")})\n\n    path = str(tempdir / 'test.json')\n    out = table.to_pandas().to_json(orient='records')[1:-1].replace('},{', '}\\n{')\n    with open(path, 'w') as f:\n        f.write(out)\n\n    with pytest.raises(ValueError,\n                       match=\"try to increase block size\"):\n        options = ds.JsonFragmentScanOptions(\n            read_options=pa.json.ReadOptions(block_size=4))\n        dataset = ds.dataset(path, format=ds.JsonFileFormat(options))\n\n    options = ds.JsonFragmentScanOptions(\n        read_options=pa.json.ReadOptions(block_size=64))\n    dataset = ds.dataset(path, format=ds.JsonFileFormat(options))\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n\ndef test_encoding(tempdir, dataset_reader):\n    path = str(tempdir / 'test.csv')\n\n    for encoding, input_rows in [\n        ('latin-1', b\"a,b\\nun,\\xe9l\\xe9phant\"),\n        ('utf16', b'\\xff\\xfea\\x00,\\x00b\\x00\\n\\x00u\\x00n\\x00,'\n         b'\\x00\\xe9\\x00l\\x00\\xe9\\x00p\\x00h\\x00a\\x00n\\x00t\\x00'),\n    ]:\n\n        with open(path, 'wb') as sink:\n            sink.write(input_rows)\n\n        # Interpret as utf8:\n        expected_schema = pa.schema([(\"a\", pa.string()), (\"b\", pa.string())])\n        expected_table = pa.table({'a': [\"un\"],\n                                   'b': [\"\u00e9l\u00e9phant\"]}, schema=expected_schema)\n\n        read_options = pa.csv.ReadOptions(encoding=encoding)\n        file_format = ds.CsvFileFormat(read_options=read_options)\n        dataset_transcoded = ds.dataset(path, format=file_format)\n        assert dataset_transcoded.schema.equals(expected_schema)\n        assert dataset_transcoded.to_table().equals(expected_table)\n\n\n# Test if a dataset with non-utf8 chars in the column names is properly handled\ndef test_column_names_encoding(tempdir, dataset_reader):\n    path = str(tempdir / 'test.csv')\n\n    with open(path, 'wb') as sink:\n        sink.write(b\"\\xe9,b\\nun,\\xe9l\\xe9phant\")\n\n    # Interpret as utf8:\n    expected_schema = pa.schema([(\"\u00e9\", pa.string()), (\"b\", pa.string())])\n    expected_table = pa.table({'\u00e9': [\"un\"],\n                               'b': [\"\u00e9l\u00e9phant\"]}, schema=expected_schema)\n\n    # Reading as string without specifying encoding should produce an error\n    dataset = ds.dataset(path, format='csv', schema=expected_schema)\n    with pytest.raises(pyarrow.lib.ArrowInvalid, match=\"invalid UTF8\"):\n        dataset_reader.to_table(dataset)\n\n    # Setting the encoding in the read_options should transcode the data\n    read_options = pa.csv.ReadOptions(encoding='latin-1')\n    file_format = ds.CsvFileFormat(read_options=read_options)\n    dataset_transcoded = ds.dataset(path, format=file_format)\n    assert dataset_transcoded.schema.equals(expected_schema)\n    assert dataset_transcoded.to_table().equals(expected_table)\n\n\ndef test_feather_format(tempdir, dataset_reader):\n    from pyarrow.feather import write_feather\n\n    table = pa.table({'a': pa.array([1, 2, 3], type=\"int8\"),\n                      'b': pa.array([.1, .2, .3], type=\"float64\")})\n\n    basedir = tempdir / \"feather_dataset\"\n    basedir.mkdir()\n    write_feather(table, str(basedir / \"data.feather\"))\n\n    dataset = ds.dataset(basedir, format=ds.IpcFileFormat())\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n    assert_dataset_fragment_convenience_methods(dataset)\n\n    dataset = ds.dataset(basedir, format=\"feather\")\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n    # ARROW-8641 - column selection order\n    result = dataset_reader.to_table(dataset, columns=[\"b\", \"a\"])\n    assert result.column_names == [\"b\", \"a\"]\n    result = dataset_reader.to_table(dataset, columns=[\"a\", \"a\"])\n    assert result.column_names == [\"a\", \"a\"]\n\n    # error with Feather v1 files\n    write_feather(table, str(basedir / \"data1.feather\"), version=1)\n    with pytest.raises(ValueError):\n        dataset_reader.to_table(ds.dataset(basedir, format=\"feather\"))\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\"compression\", [\n    \"lz4\",\n    \"zstd\",\n    \"brotli\"  # not supported\n])\ndef test_feather_format_compressed(tempdir, compression, dataset_reader):\n    table = pa.table({'a': pa.array([0]*300, type=\"int8\"),\n                      'b': pa.array([.1, .2, .3]*100, type=\"float64\")})\n    if not pa.Codec.is_available(compression):\n        pytest.skip()\n\n    basedir = tempdir / \"feather_dataset_compressed\"\n    basedir.mkdir()\n    file_format = ds.IpcFileFormat()\n\n    uncompressed_basedir = tempdir / \"feather_dataset_uncompressed\"\n    uncompressed_basedir.mkdir()\n    ds.write_dataset(\n        table,\n        str(uncompressed_basedir / \"data.arrow\"),\n        format=file_format,\n        file_options=file_format.make_write_options(compression=None)\n    )\n\n    if compression == \"brotli\":\n        with pytest.raises(ValueError, match=\"Compression type\"):\n            write_options = file_format.make_write_options(\n                compression=compression)\n        with pytest.raises(ValueError, match=\"Compression type\"):\n            codec = pa.Codec(compression)\n            write_options = file_format.make_write_options(compression=codec)\n        return\n\n    write_options = file_format.make_write_options(compression=compression)\n    ds.write_dataset(\n        table,\n        str(basedir / \"data.arrow\"),\n        format=file_format,\n        file_options=write_options\n    )\n\n    dataset = ds.dataset(basedir, format=ds.IpcFileFormat())\n    result = dataset_reader.to_table(dataset)\n    assert result.equals(table)\n\n    compressed_file = basedir / \"data.arrow\" / \"part-0.arrow\"\n    compressed_size = compressed_file.stat().st_size\n    uncompressed_file = uncompressed_basedir / \"data.arrow\" / \"part-0.arrow\"\n    uncompressed_size = uncompressed_file.stat().st_size\n    assert compressed_size < uncompressed_size\n\n\ndef _create_parquet_dataset_simple(root_path):\n    \"\"\"\n    Creates a simple (flat files, no nested partitioning) Parquet dataset\n    \"\"\"\n\n    metadata_collector = []\n\n    for i in range(4):\n        table = pa.table({'f1': [i] * 10, 'f2': np.random.randn(10)})\n        pq.write_to_dataset(\n            table, str(root_path), metadata_collector=metadata_collector\n        )\n\n    metadata_path = str(root_path / '_metadata')\n    # write _metadata file\n    pq.write_metadata(\n        table.schema, metadata_path,\n        metadata_collector=metadata_collector\n    )\n    return metadata_path, table\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas  # write_to_dataset currently requires pandas\ndef test_parquet_dataset_factory(tempdir):\n    root_path = tempdir / \"test_parquet_dataset\"\n    metadata_path, table = _create_parquet_dataset_simple(root_path)\n    dataset = ds.parquet_dataset(metadata_path)\n    assert dataset.schema.equals(table.schema)\n    assert len(dataset.files) == 4\n    result = dataset.to_table()\n    assert result.num_rows == 40\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas  # write_to_dataset currently requires pandas\n@pytest.mark.skipif(sys.platform == 'win32',\n                    reason=\"Results in FileNotFoundError on Windows\")\ndef test_parquet_dataset_factory_fsspec(tempdir):\n    # https://issues.apache.org/jira/browse/ARROW-16413\n    fsspec = pytest.importorskip(\"fsspec\")\n\n    # create dataset with pyarrow\n    root_path = tempdir / \"test_parquet_dataset\"\n    metadata_path, table = _create_parquet_dataset_simple(root_path)\n\n    # read using fsspec filesystem\n    fsspec_fs = fsspec.filesystem(\"file\")\n    # manually creating a PyFileSystem, because passing the local fsspec\n    # filesystem would internally be converted to native LocalFileSystem\n    filesystem = fs.PyFileSystem(fs.FSSpecHandler(fsspec_fs))\n    dataset = ds.parquet_dataset(metadata_path, filesystem=filesystem)\n    assert dataset.schema.equals(table.schema)\n    assert len(dataset.files) == 4\n    result = dataset.to_table()\n    assert result.num_rows == 40\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas  # write_to_dataset currently requires pandas\ndef test_parquet_dataset_factory_roundtrip(tempdir):\n    # Simple test to ensure we can roundtrip dataset to\n    # _metadata/common_metadata and back.  A more complex test\n    # using partitioning will have to wait for ARROW-13269.  The\n    # above test (test_parquet_dataset_factory) will not work\n    # when legacy is False as there is no \"append\" equivalent in\n    # the new dataset until ARROW-12358\n    root_path = tempdir / \"test_parquet_dataset\"\n    table = pa.table({'f1': [0] * 10, 'f2': np.random.randn(10)})\n    metadata_collector = []\n    pq.write_to_dataset(\n        table, str(root_path), metadata_collector=metadata_collector,\n    )\n    metadata_path = str(root_path / '_metadata')\n    # write _metadata file\n    pq.write_metadata(\n        table.schema, metadata_path,\n        metadata_collector=metadata_collector\n    )\n    dataset = ds.parquet_dataset(metadata_path)\n    assert dataset.schema.equals(table.schema)\n    result = dataset.to_table()\n    assert result.num_rows == 10\n\n\n@pytest.mark.parquet\ndef test_parquet_dataset_factory_order(tempdir):\n    # The order of the fragments in the dataset should match the order of the\n    # row groups in the _metadata file.\n    metadatas = []\n    # Create a dataset where f1 is incrementing from 0 to 100 spread across\n    # 10 files.  Put the row groups in the correct order in _metadata\n    for i in range(10):\n        table = pa.table(\n            {'f1': list(range(i*10, (i+1)*10))})\n        table_path = tempdir / f'{i}.parquet'\n        pq.write_table(table, table_path, metadata_collector=metadatas)\n        metadatas[-1].set_file_path(f'{i}.parquet')\n    metadata_path = str(tempdir / '_metadata')\n    pq.write_metadata(table.schema, metadata_path, metadatas)\n    dataset = ds.parquet_dataset(metadata_path)\n    # Ensure the table contains values from 0-100 in the right order\n    scanned_table = dataset.to_table()\n    scanned_col = scanned_table.column('f1').to_pylist()\n    assert scanned_col == list(range(0, 100))\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_parquet_dataset_factory_invalid(tempdir):\n    root_path = tempdir / \"test_parquet_dataset_invalid\"\n    metadata_path, table = _create_parquet_dataset_simple(root_path)\n    # remove one of the files\n    list(root_path.glob(\"*.parquet\"))[0].unlink()\n    dataset = ds.parquet_dataset(metadata_path)\n    assert dataset.schema.equals(table.schema)\n    assert len(dataset.files) == 4\n    with pytest.raises(FileNotFoundError):\n        dataset.to_table()\n\n\ndef _create_metadata_file(root_path):\n    # create _metadata file from existing parquet dataset\n    parquet_paths = list(sorted(root_path.rglob(\"*.parquet\")))\n    schema = pq.ParquetFile(parquet_paths[0]).schema.to_arrow_schema()\n\n    metadata_collector = []\n    for path in parquet_paths:\n        metadata = pq.ParquetFile(path).metadata\n        metadata.set_file_path(str(path.relative_to(root_path)))\n        metadata_collector.append(metadata)\n\n    metadata_path = root_path / \"_metadata\"\n    pq.write_metadata(\n        schema, metadata_path, metadata_collector=metadata_collector\n    )\n    return metadata_path\n\n\ndef _create_parquet_dataset_partitioned(root_path):\n    table = pa.table([\n        pa.array(range(20)), pa.array(np.random.randn(20)),\n        pa.array(np.repeat(['a', 'b'], 10))],\n        names=[\"f1\", \"f2\", \"part\"]\n    )\n    table = table.replace_schema_metadata({\"key\": \"value\"})\n    pq.write_to_dataset(table, str(root_path), partition_cols=['part'])\n    return _create_metadata_file(root_path), table\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_parquet_dataset_factory_partitioned(tempdir):\n    root_path = tempdir / \"test_parquet_dataset_factory_partitioned\"\n    metadata_path, table = _create_parquet_dataset_partitioned(root_path)\n\n    partitioning = ds.partitioning(flavor=\"hive\")\n    dataset = ds.parquet_dataset(metadata_path, partitioning=partitioning)\n\n    assert dataset.schema.equals(table.schema)\n    assert len(dataset.files) == 2\n    result = dataset.to_table()\n    assert result.num_rows == 20\n\n    # the partitioned dataset does not preserve order\n    result = result.to_pandas().sort_values(\"f1\").reset_index(drop=True)\n    expected = table.to_pandas()\n    pd.testing.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_parquet_dataset_factory_metadata(tempdir):\n    # ensure ParquetDatasetFactory preserves metadata (ARROW-9363)\n    root_path = tempdir / \"test_parquet_dataset_factory_metadata\"\n    metadata_path, table = _create_parquet_dataset_partitioned(root_path)\n\n    dataset = ds.parquet_dataset(metadata_path, partitioning=\"hive\")\n    assert dataset.schema.equals(table.schema)\n    assert b\"key\" in dataset.schema.metadata\n\n    fragments = list(dataset.get_fragments())\n    assert b\"key\" in fragments[0].physical_schema.metadata\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_parquet_dataset_lazy_filtering(tempdir, open_logging_fs):\n    fs, assert_opens = open_logging_fs\n\n    # Test to ensure that no IO happens when filtering a dataset\n    # created with ParquetDatasetFactory from a _metadata file\n\n    root_path = tempdir / \"test_parquet_dataset_lazy_filtering\"\n    metadata_path, _ = _create_parquet_dataset_simple(root_path)\n\n    # creating the dataset should only open the metadata file\n    with assert_opens([metadata_path]):\n        dataset = ds.parquet_dataset(\n            metadata_path,\n            partitioning=ds.partitioning(flavor=\"hive\"),\n            filesystem=fs)\n\n    # materializing fragments should not open any file\n    with assert_opens([]):\n        fragments = list(dataset.get_fragments())\n\n    # filtering fragments should not open any file\n    with assert_opens([]):\n        list(dataset.get_fragments(ds.field(\"f1\") > 15))\n\n    # splitting by row group should still not open any file\n    with assert_opens([]):\n        fragments[0].split_by_row_group(ds.field(\"f1\") > 15)\n\n    # ensuring metadata of split fragment should also not open any file\n    with assert_opens([]):\n        rg_fragments = fragments[0].split_by_row_group()\n        rg_fragments[0].ensure_complete_metadata()\n\n    # FIXME(bkietz) on Windows this results in FileNotFoundErrors.\n    # but actually scanning does open files\n    # with assert_opens([f.path for f in fragments]):\n    #    dataset.to_table()\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_dataset_schema_metadata(tempdir, dataset_reader):\n    # ARROW-8802\n    df = pd.DataFrame({'a': [1, 2, 3]})\n    path = tempdir / \"test.parquet\"\n    df.to_parquet(path)\n    dataset = ds.dataset(path)\n\n    schema = dataset_reader.to_table(dataset).schema\n    projected_schema = dataset_reader.to_table(dataset, columns=[\"a\"]).schema\n\n    # ensure the pandas metadata is included in the schema\n    assert b\"pandas\" in schema.metadata\n    # ensure it is still there in a projected schema (with column selection)\n    assert schema.equals(projected_schema, check_metadata=True)\n\n\n@pytest.mark.parquet\ndef test_filter_mismatching_schema(tempdir, dataset_reader):\n    # ARROW-9146\n    table = pa.table({\"col\": pa.array([1, 2, 3, 4], type='int32')})\n    pq.write_table(table, str(tempdir / \"data.parquet\"))\n\n    # specifying explicit schema, but that mismatches the schema of the data\n    schema = pa.schema([(\"col\", pa.int64())])\n    dataset = ds.dataset(\n        tempdir / \"data.parquet\", format=\"parquet\", schema=schema)\n\n    # filtering on a column with such type mismatch should implicitly\n    # cast the column\n    filtered = dataset_reader.to_table(dataset, filter=ds.field(\"col\") > 2)\n    assert filtered[\"col\"].equals(table[\"col\"].cast('int64').slice(2))\n\n    fragment = list(dataset.get_fragments())[0]\n    filtered = dataset_reader.to_table(\n        fragment, filter=ds.field(\"col\") > 2, schema=schema)\n    assert filtered[\"col\"].equals(table[\"col\"].cast('int64').slice(2))\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_dataset_project_only_partition_columns(tempdir, dataset_reader):\n    # ARROW-8729\n    table = pa.table({'part': 'a a b b'.split(), 'col': list(range(4))})\n\n    path = str(tempdir / 'test_dataset')\n    pq.write_to_dataset(table, path, partition_cols=['part'])\n    dataset = ds.dataset(path, partitioning='hive')\n\n    all_cols = dataset_reader.to_table(dataset)\n    part_only = dataset_reader.to_table(dataset, columns=['part'])\n\n    assert all_cols.column('part').equals(part_only.column('part'))\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_dataset_project_null_column(tempdir, dataset_reader):\n    df = pd.DataFrame({\"col\": np.array([None, None, None], dtype='object')})\n\n    f = tempdir / \"test_dataset_project_null_column.parquet\"\n    df.to_parquet(f, engine=\"pyarrow\")\n\n    dataset = ds.dataset(f, format=\"parquet\",\n                         schema=pa.schema([(\"col\", pa.int64())]))\n    expected = pa.table({'col': pa.array([None, None, None], pa.int64())})\n    assert dataset_reader.to_table(dataset).equals(expected)\n\n\ndef test_dataset_project_columns(tempdir, dataset_reader):\n    # basic column re-projection with expressions\n    from pyarrow import feather\n    table = pa.table({\"A\": [1, 2, 3], \"B\": [1., 2., 3.], \"C\": [\"a\", \"b\", \"c\"]})\n    feather.write_feather(table, tempdir / \"data.feather\")\n\n    dataset = ds.dataset(tempdir / \"data.feather\", format=\"feather\")\n    result = dataset_reader.to_table(dataset, columns={\n        'A_renamed': ds.field('A'),\n        'B_as_int': ds.field('B').cast(\"int32\", safe=False),\n        'C_is_a': ds.field('C') == 'a'\n    })\n    expected = pa.table({\n        \"A_renamed\": [1, 2, 3],\n        \"B_as_int\": pa.array([1, 2, 3], type=\"int32\"),\n        \"C_is_a\": [True, False, False],\n    })\n    assert result.equals(expected)\n\n    # raise proper error when not passing an expression\n    with pytest.raises(TypeError, match=\"Expected an Expression\"):\n        dataset_reader.to_table(dataset, columns={\"A\": \"A\"})\n\n\n@pytest.mark.pandas\n@pytest.mark.parquet\ndef test_dataset_preserved_partitioning(tempdir):\n    # ARROW-8655\n\n    # through discovery, but without partitioning\n    _, path = _create_single_file(tempdir)\n    dataset = ds.dataset(path)\n    assert isinstance(dataset.partitioning, ds.DirectoryPartitioning)\n    # TODO(GH-34884) partitioning attribute not preserved in pickling\n    # dataset_ = ds.dataset(path)\n    # for dataset in [dataset_, pickle_module.loads(pickle_module.dumps(dataset_))]:\n    #     assert isinstance(dataset.partitioning, ds.DirectoryPartitioning)\n\n    # through discovery, with hive partitioning but not specified\n    full_table, path = _create_partitioned_dataset(tempdir)\n    dataset = ds.dataset(path)\n    assert isinstance(dataset.partitioning, ds.DirectoryPartitioning)\n\n    # through discovery, with hive partitioning (from a partitioning factory)\n    dataset = ds.dataset(path, partitioning=\"hive\")\n    part = dataset.partitioning\n    assert part is not None\n    assert isinstance(part, ds.HivePartitioning)\n    assert part.schema == pa.schema([(\"part\", pa.int32())])\n    assert len(part.dictionaries) == 1\n    assert part.dictionaries[0] == pa.array([0, 1, 2], pa.int32())\n\n    # through discovery, with hive partitioning (from a partitioning object)\n    part = ds.partitioning(pa.schema([(\"part\", pa.int32())]), flavor=\"hive\")\n    assert isinstance(part, ds.HivePartitioning)  # not a factory\n    assert len(part.dictionaries) == 1\n    assert all(x is None for x in part.dictionaries)\n    dataset = ds.dataset(path, partitioning=part)\n    part = dataset.partitioning\n    assert isinstance(part, ds.HivePartitioning)\n    assert part.schema == pa.schema([(\"part\", pa.int32())])\n    # TODO is this expected?\n    assert len(part.dictionaries) == 1\n    assert all(x is None for x in part.dictionaries)\n\n    # through manual creation -> not available\n    dataset = ds.dataset(path, partitioning=\"hive\")\n    dataset2 = ds.FileSystemDataset(\n        list(dataset.get_fragments()), schema=dataset.schema,\n        format=dataset.format, filesystem=dataset.filesystem\n    )\n    assert dataset2.partitioning is None\n\n    # through discovery with ParquetDatasetFactory\n    root_path = tempdir / \"data-partitioned-metadata\"\n    metadata_path, _ = _create_parquet_dataset_partitioned(root_path)\n    dataset = ds.parquet_dataset(metadata_path, partitioning=\"hive\")\n    part = dataset.partitioning\n    assert part is not None\n    assert isinstance(part, ds.HivePartitioning)\n    assert part.schema == pa.schema([(\"part\", pa.string())])\n    assert len(part.dictionaries) == 1\n    # will be fixed by ARROW-13153 (order is not preserved at the moment)\n    # assert part.dictionaries[0] == pa.array([\"a\", \"b\"], pa.string())\n    assert set(part.dictionaries[0].to_pylist()) == {\"a\", \"b\"}\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_write_to_dataset_given_null_just_works(tempdir):\n    schema = pa.schema([\n        pa.field('col', pa.int64()),\n        pa.field('part', pa.dictionary(pa.int32(), pa.string()))\n    ])\n    table = pa.table({'part': [None, None, 'a', 'a'],\n                      'col': list(range(4))}, schema=schema)\n\n    path = str(tempdir / 'test_dataset')\n    pq.write_to_dataset(table, path, partition_cols=['part'])\n\n    actual_table = pq.read_table(tempdir / 'test_dataset')\n    # column.equals can handle the difference in chunking but not the fact\n    # that `part` will have different dictionaries for the two chunks\n    assert actual_table.column('part').to_pylist(\n    ) == table.column('part').to_pylist()\n    assert actual_table.column('col').equals(table.column('col'))\n\n\ndef _sort_table(tab, sort_col):\n    import pyarrow.compute as pc\n    sorted_indices = pc.sort_indices(\n        tab, options=pc.SortOptions([(sort_col, 'ascending')]))\n    return pc.take(tab, sorted_indices)\n\n\ndef _check_dataset_roundtrip(dataset, base_dir, expected_files, sort_col,\n                             base_dir_path=None, partitioning=None):\n    base_dir_path = base_dir_path or base_dir\n\n    ds.write_dataset(dataset, base_dir, format=\"arrow\",\n                     partitioning=partitioning, use_threads=False)\n\n    # check that all files are present\n    file_paths = list(base_dir_path.rglob(\"*\"))\n    assert set(file_paths) == set(expected_files)\n\n    # check that reading back in as dataset gives the same result\n    dataset2 = ds.dataset(\n        base_dir_path, format=\"arrow\", partitioning=partitioning)\n\n    assert _sort_table(dataset2.to_table(), sort_col).equals(\n        _sort_table(dataset.to_table(), sort_col))\n\n\n@pytest.mark.parquet\ndef test_write_dataset(tempdir):\n    # manually create a written dataset and read as dataset object\n    directory = tempdir / 'single-file'\n    directory.mkdir()\n    _ = _create_single_file(directory)\n    dataset = ds.dataset(directory)\n\n    # full string path\n    target = tempdir / 'single-file-target'\n    expected_files = [target / \"part-0.arrow\"]\n    _check_dataset_roundtrip(dataset, str(target), expected_files, 'a', target)\n\n    # pathlib path object\n    target = tempdir / 'single-file-target2'\n    expected_files = [target / \"part-0.arrow\"]\n    _check_dataset_roundtrip(dataset, target, expected_files, 'a', target)\n\n    # TODO\n    # # relative path\n    # target = tempdir / 'single-file-target3'\n    # expected_files = [target / \"part-0.ipc\"]\n    # _check_dataset_roundtrip(\n    #     dataset, './single-file-target3', expected_files, target)\n\n    # Directory of files\n    directory = tempdir / 'single-directory'\n    directory.mkdir()\n    _ = _create_directory_of_files(directory)\n    dataset = ds.dataset(directory)\n\n    target = tempdir / 'single-directory-target'\n    expected_files = [target / \"part-0.arrow\"]\n    _check_dataset_roundtrip(dataset, str(target), expected_files, 'a', target)\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_write_dataset_partitioned(tempdir):\n    directory = tempdir / \"partitioned\"\n    _ = _create_parquet_dataset_partitioned(directory)\n    partitioning = ds.partitioning(flavor=\"hive\")\n    dataset = ds.dataset(directory, partitioning=partitioning)\n\n    # hive partitioning\n    target = tempdir / 'partitioned-hive-target'\n    expected_paths = [\n        target / \"part=a\", target / \"part=a\" / \"part-0.arrow\",\n        target / \"part=b\", target / \"part=b\" / \"part-0.arrow\"\n    ]\n    partitioning_schema = ds.partitioning(\n        pa.schema([(\"part\", pa.string())]), flavor=\"hive\")\n    _check_dataset_roundtrip(\n        dataset, str(target), expected_paths, 'f1', target,\n        partitioning=partitioning_schema)\n\n    # directory partitioning\n    target = tempdir / 'partitioned-dir-target'\n    expected_paths = [\n        target / \"a\", target / \"a\" / \"part-0.arrow\",\n        target / \"b\", target / \"b\" / \"part-0.arrow\"\n    ]\n    partitioning_schema = ds.partitioning(\n        pa.schema([(\"part\", pa.string())]))\n    _check_dataset_roundtrip(\n        dataset, str(target), expected_paths, 'f1', target,\n        partitioning=partitioning_schema)\n\n\ndef test_write_dataset_with_field_names(tempdir):\n    table = pa.table({'a': ['x', 'y', None], 'b': ['x', 'y', 'z']})\n\n    ds.write_dataset(table, tempdir, format='ipc',\n                     partitioning=[\"b\"])\n\n    load_back = ds.dataset(tempdir, format='ipc', partitioning=[\"b\"])\n    files = load_back.files\n    partitioning_dirs = {\n        str(pathlib.Path(f).relative_to(tempdir).parent) for f in files\n    }\n    assert partitioning_dirs == {\"x\", \"y\", \"z\"}\n\n    load_back_table = load_back.to_table()\n    assert load_back_table.equals(table)\n\n\ndef test_write_dataset_with_field_names_hive(tempdir):\n    table = pa.table({'a': ['x', 'y', None], 'b': ['x', 'y', 'z']})\n\n    ds.write_dataset(table, tempdir, format='ipc',\n                     partitioning=[\"b\"], partitioning_flavor=\"hive\")\n\n    load_back = ds.dataset(tempdir, format='ipc', partitioning=\"hive\")\n    files = load_back.files\n    partitioning_dirs = {\n        str(pathlib.Path(f).relative_to(tempdir).parent) for f in files\n    }\n    assert partitioning_dirs == {\"b=x\", \"b=y\", \"b=z\"}\n\n    load_back_table = load_back.to_table()\n    assert load_back_table.equals(table)\n\n\ndef test_write_dataset_with_scanner(tempdir):\n    table = pa.table({'a': ['x', 'y', None], 'b': ['x', 'y', 'z'],\n                      'c': [1, 2, 3]})\n\n    ds.write_dataset(table, tempdir, format='ipc',\n                     partitioning=[\"b\"])\n\n    dataset = ds.dataset(tempdir, format='ipc', partitioning=[\"b\"])\n\n    with tempfile.TemporaryDirectory() as tempdir2:\n        ds.write_dataset(dataset.scanner(columns=[\"b\", \"c\"]),\n                         tempdir2, format='ipc', partitioning=[\"b\"])\n\n        load_back = ds.dataset(tempdir2, format='ipc', partitioning=[\"b\"])\n        load_back_table = load_back.to_table()\n        assert dict(load_back_table.to_pydict()\n                    ) == table.drop_columns(\"a\").to_pydict()\n\n\n@pytest.mark.parquet\ndef test_write_dataset_with_backpressure(tempdir):\n    consumer_gate = threading.Event()\n\n    # A filesystem that blocks all writes so that we can build\n    # up backpressure.  The writes are released at the end of\n    # the test.\n    class GatingFs(ProxyHandler):\n        def open_output_stream(self, path, metadata):\n            # Block until the end of the test\n            consumer_gate.wait()\n            return self._fs.open_output_stream(path, metadata=metadata)\n    gating_fs = fs.PyFileSystem(GatingFs(fs.LocalFileSystem()))\n\n    schema = pa.schema([pa.field('data', pa.int32())])\n    # The scanner should queue ~ 8Mi rows (~8 batches) but due to ARROW-16258\n    # it always queues 32 batches.\n    batch = pa.record_batch([pa.array(list(range(1_000_000)))], schema=schema)\n    batches_read = 0\n    min_backpressure = 32\n    end = 200\n    keep_going = True\n\n    def counting_generator():\n        nonlocal batches_read\n        while batches_read < end:\n            if not keep_going:\n                return\n            time.sleep(0.01)\n            batches_read += 1\n            yield batch\n\n    scanner = ds.Scanner.from_batches(\n        counting_generator(), schema=schema, use_threads=True)\n\n    write_thread = threading.Thread(\n        target=lambda: ds.write_dataset(\n            scanner, str(tempdir), format='parquet', filesystem=gating_fs))\n    write_thread.start()\n\n    try:\n        start = time.time()\n\n        def duration():\n            return time.time() - start\n\n        # This test is timing dependent.  There is no signal from the C++\n        # when backpressure has been hit.  We don't know exactly when\n        # backpressure will be hit because it may take some time for the\n        # signal to get from the sink to the scanner.\n        #\n        # The test may emit false positives on slow systems.  It could\n        # theoretically emit a false negative if the scanner managed to read\n        # and emit all 200 batches before the backpressure signal had a chance\n        # to propagate but the 0.01s delay in the generator should make that\n        # scenario unlikely.\n        last_value = 0\n        backpressure_probably_hit = False\n        while duration() < 10:\n            if batches_read > min_backpressure:\n                if batches_read == last_value:\n                    backpressure_probably_hit = True\n                    break\n                last_value = batches_read\n            time.sleep(0.5)\n\n        assert backpressure_probably_hit\n\n    finally:\n        # If any batches remain to be generated go ahead and\n        # skip them\n        keep_going = False\n        consumer_gate.set()\n        write_thread.join()\n\n\ndef test_write_dataset_with_dataset(tempdir):\n    table = pa.table({'b': ['x', 'y', 'z'], 'c': [1, 2, 3]})\n\n    ds.write_dataset(table, tempdir, format='ipc',\n                     partitioning=[\"b\"])\n\n    dataset = ds.dataset(tempdir, format='ipc', partitioning=[\"b\"])\n\n    with tempfile.TemporaryDirectory() as tempdir2:\n        ds.write_dataset(dataset, tempdir2,\n                         format='ipc', partitioning=[\"b\"])\n\n        load_back = ds.dataset(tempdir2, format='ipc', partitioning=[\"b\"])\n        load_back_table = load_back.to_table()\n        assert dict(load_back_table.to_pydict()) == table.to_pydict()\n\n\n@pytest.mark.pandas\ndef test_write_dataset_existing_data(tempdir):\n    directory = tempdir / 'ds'\n    table = pa.table({'b': ['x', 'y', 'z'], 'c': [1, 2, 3]})\n    partitioning = ds.partitioning(schema=pa.schema(\n        [pa.field('c', pa.int64())]), flavor='hive')\n\n    def compare_tables_ignoring_order(t1, t2):\n        df1 = t1.to_pandas().sort_values('b').reset_index(drop=True)\n        df2 = t2.to_pandas().sort_values('b').reset_index(drop=True)\n        assert df1.equals(df2)\n\n    # First write is ok\n    ds.write_dataset(table, directory, partitioning=partitioning, format='ipc')\n\n    table = pa.table({'b': ['a', 'b', 'c'], 'c': [2, 3, 4]})\n\n    # Second write should fail\n    with pytest.raises(pa.ArrowInvalid):\n        ds.write_dataset(table, directory,\n                         partitioning=partitioning, format='ipc')\n\n    extra_table = pa.table({'b': ['e']})\n    extra_file = directory / 'c=2' / 'foo.arrow'\n    pyarrow.feather.write_feather(extra_table, extra_file)\n\n    # Should be ok and overwrite with overwrite behavior\n    ds.write_dataset(table, directory, partitioning=partitioning,\n                     format='ipc',\n                     existing_data_behavior='overwrite_or_ignore')\n\n    overwritten = pa.table(\n        {'b': ['e', 'x', 'a', 'b', 'c'], 'c': [2, 1, 2, 3, 4]})\n    readback = ds.dataset(tempdir, format='ipc',\n                          partitioning=partitioning).to_table()\n    compare_tables_ignoring_order(readback, overwritten)\n    assert extra_file.exists()\n\n    # Should be ok and delete matching with delete_matching\n    ds.write_dataset(table, directory, partitioning=partitioning,\n                     format='ipc', existing_data_behavior='delete_matching')\n\n    overwritten = pa.table({'b': ['x', 'a', 'b', 'c'], 'c': [1, 2, 3, 4]})\n    readback = ds.dataset(tempdir, format='ipc',\n                          partitioning=partitioning).to_table()\n    compare_tables_ignoring_order(readback, overwritten)\n    assert not extra_file.exists()\n\n\ndef _generate_random_int_array(size=4, min=1, max=10):\n    return np.random.randint(min, max, size)\n\n\ndef _generate_data_and_columns(num_of_columns, num_of_records):\n    data = []\n    column_names = []\n    for i in range(num_of_columns):\n        data.append(_generate_random_int_array(size=num_of_records,\n                                               min=1,\n                                               max=num_of_records))\n        column_names.append(\"c\" + str(i))\n    record_batch = pa.record_batch(data=data, names=column_names)\n    return record_batch\n\n\ndef _get_num_of_files_generated(base_directory, file_format):\n    return len(list(pathlib.Path(base_directory).glob(f'**/*.{file_format}')))\n\n\n@pytest.mark.parquet\ndef test_write_dataset_max_rows_per_file(tempdir):\n    directory = tempdir / 'ds'\n    max_rows_per_file = 10\n    max_rows_per_group = 10\n    num_of_columns = 2\n    num_of_records = 35\n\n    record_batch = _generate_data_and_columns(num_of_columns,\n                                              num_of_records)\n\n    ds.write_dataset(record_batch, directory, format=\"parquet\",\n                     max_rows_per_file=max_rows_per_file,\n                     max_rows_per_group=max_rows_per_group)\n\n    files_in_dir = os.listdir(directory)\n\n    # number of partitions with max_rows and the partition with the remainder\n    expected_partitions = num_of_records // max_rows_per_file + 1\n\n    # test whether the expected amount of files are written\n    assert len(files_in_dir) == expected_partitions\n\n    # compute the number of rows per each file written\n    result_row_combination = []\n    for _, f_file in enumerate(files_in_dir):\n        f_path = directory / str(f_file)\n        dataset = ds.dataset(f_path, format=\"parquet\")\n        result_row_combination.append(dataset.to_table().shape[0])\n\n    # test whether the generated files have the expected number of rows\n    assert expected_partitions == len(result_row_combination)\n    assert num_of_records == sum(result_row_combination)\n    assert all(file_rowcount <= max_rows_per_file\n               for file_rowcount in result_row_combination)\n\n\n@pytest.mark.parquet\ndef test_write_dataset_min_rows_per_group(tempdir):\n    directory = tempdir / 'ds'\n    min_rows_per_group = 6\n    max_rows_per_group = 8\n    num_of_columns = 2\n\n    record_sizes = [5, 5, 5, 5, 5, 4, 4, 4, 4, 4]\n\n    record_batches = [_generate_data_and_columns(num_of_columns,\n                                                 num_of_records)\n                      for num_of_records in record_sizes]\n\n    data_source = directory / \"min_rows_group\"\n\n    ds.write_dataset(data=record_batches, base_dir=data_source,\n                     min_rows_per_group=min_rows_per_group,\n                     max_rows_per_group=max_rows_per_group,\n                     format=\"parquet\")\n\n    files_in_dir = os.listdir(data_source)\n    for _, f_file in enumerate(files_in_dir):\n        f_path = data_source / str(f_file)\n        dataset = ds.dataset(f_path, format=\"parquet\")\n        table = dataset.to_table()\n        batches = table.to_batches()\n\n        for id, batch in enumerate(batches):\n            rows_per_batch = batch.num_rows\n            if id < len(batches) - 1:\n                assert rows_per_batch >= min_rows_per_group and \\\n                    rows_per_batch <= max_rows_per_group\n            else:\n                assert rows_per_batch <= max_rows_per_group\n\n\n@pytest.mark.parquet\ndef test_write_dataset_max_rows_per_group(tempdir):\n    directory = tempdir / 'ds'\n    max_rows_per_group = 18\n    num_of_columns = 2\n    num_of_records = 30\n\n    record_batch = _generate_data_and_columns(num_of_columns,\n                                              num_of_records)\n\n    data_source = directory / \"max_rows_group\"\n\n    ds.write_dataset(data=record_batch, base_dir=data_source,\n                     max_rows_per_group=max_rows_per_group,\n                     format=\"parquet\")\n\n    files_in_dir = os.listdir(data_source)\n    batched_data = []\n    for f_file in files_in_dir:\n        f_path = data_source / str(f_file)\n        dataset = ds.dataset(f_path, format=\"parquet\")\n        table = dataset.to_table()\n        batches = table.to_batches()\n        for batch in batches:\n            batched_data.append(batch.num_rows)\n\n    assert batched_data == [18, 12]\n\n\n@pytest.mark.parquet\ndef test_write_dataset_max_open_files(tempdir):\n    directory = tempdir / 'ds'\n    file_format = \"parquet\"\n    partition_column_id = 1\n    column_names = ['c1', 'c2']\n    record_batch_1 = pa.record_batch(data=[[1, 2, 3, 4, 0, 10],\n                                           ['a', 'b', 'c', 'd', 'e', 'a']],\n                                     names=column_names)\n    record_batch_2 = pa.record_batch(data=[[5, 6, 7, 8, 0, 1],\n                                           ['a', 'b', 'c', 'd', 'e', 'c']],\n                                     names=column_names)\n    record_batch_3 = pa.record_batch(data=[[9, 10, 11, 12, 0, 1],\n                                           ['a', 'b', 'c', 'd', 'e', 'd']],\n                                     names=column_names)\n    record_batch_4 = pa.record_batch(data=[[13, 14, 15, 16, 0, 1],\n                                           ['a', 'b', 'c', 'd', 'e', 'b']],\n                                     names=column_names)\n\n    table = pa.Table.from_batches([record_batch_1, record_batch_2,\n                                   record_batch_3, record_batch_4])\n\n    partitioning = ds.partitioning(\n        pa.schema([(column_names[partition_column_id], pa.string())]),\n        flavor=\"hive\")\n\n    data_source_1 = directory / \"default\"\n\n    ds.write_dataset(data=table, base_dir=data_source_1,\n                     partitioning=partitioning, format=file_format)\n\n    # Here we consider the number of unique partitions created when\n    # partitioning column contains duplicate records.\n    #   Returns: (number_of_files_generated, number_of_partitions)\n    def _get_compare_pair(data_source, record_batch, file_format, col_id):\n        num_of_files_generated = _get_num_of_files_generated(\n            base_directory=data_source, file_format=file_format)\n        number_of_partitions = len(pa.compute.unique(record_batch[col_id]))\n        return num_of_files_generated, number_of_partitions\n\n    # CASE 1: when max_open_files=default & max_open_files >= num_of_partitions\n    #         In case of a writing to disk via partitioning based on a\n    #         particular column (considering row labels in that column),\n    #         the number of unique rows must be equal\n    #         to the number of files generated\n\n    num_of_files_generated, number_of_partitions \\\n        = _get_compare_pair(data_source_1, record_batch_1, file_format,\n                            partition_column_id)\n    assert num_of_files_generated == number_of_partitions\n\n    # CASE 2: when max_open_files > 0 & max_open_files < num_of_partitions\n    #         the number of files generated must be greater than the number of\n    #         partitions\n\n    data_source_2 = directory / \"max_1\"\n\n    max_open_files = 3\n\n    ds.write_dataset(data=table, base_dir=data_source_2,\n                     partitioning=partitioning, format=file_format,\n                     max_open_files=max_open_files, use_threads=False)\n\n    num_of_files_generated, number_of_partitions \\\n        = _get_compare_pair(data_source_2, record_batch_1, file_format,\n                            partition_column_id)\n    assert num_of_files_generated > number_of_partitions\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_write_dataset_partitioned_dict(tempdir):\n    directory = tempdir / \"partitioned\"\n    _ = _create_parquet_dataset_partitioned(directory)\n\n    # directory partitioning, dictionary partition columns\n    dataset = ds.dataset(\n        directory,\n        partitioning=ds.HivePartitioning.discover(infer_dictionary=True))\n    target = tempdir / 'partitioned-dir-target'\n    expected_paths = [\n        target / \"a\", target / \"a\" / \"part-0.arrow\",\n        target / \"b\", target / \"b\" / \"part-0.arrow\"\n    ]\n    partitioning = ds.partitioning(pa.schema([\n        dataset.schema.field('part')]),\n        dictionaries={'part': pa.array(['a', 'b'])})\n    # NB: dictionaries required here since we use partitioning to parse\n    # directories in _check_dataset_roundtrip (not currently required for\n    # the formatting step)\n    _check_dataset_roundtrip(\n        dataset, str(target), expected_paths, 'f1', target,\n        partitioning=partitioning)\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_write_dataset_use_threads(tempdir):\n    directory = tempdir / \"partitioned\"\n    _ = _create_parquet_dataset_partitioned(directory)\n    dataset = ds.dataset(directory, partitioning=\"hive\")\n\n    partitioning = ds.partitioning(\n        pa.schema([(\"part\", pa.string())]), flavor=\"hive\")\n\n    target1 = tempdir / 'partitioned1'\n    paths_written = []\n\n    def file_visitor(written_file):\n        paths_written.append(written_file.path)\n\n    ds.write_dataset(\n        dataset, target1, format=\"feather\", partitioning=partitioning,\n        use_threads=True, file_visitor=file_visitor\n    )\n\n    expected_paths = {\n        target1 / 'part=a' / 'part-0.feather',\n        target1 / 'part=b' / 'part-0.feather'\n    }\n    paths_written_set = set(map(pathlib.Path, paths_written))\n    assert paths_written_set == expected_paths\n\n    target2 = tempdir / 'partitioned2'\n    ds.write_dataset(\n        dataset, target2, format=\"feather\", partitioning=partitioning,\n        use_threads=False\n    )\n\n    # check that reading in gives same result\n    result1 = ds.dataset(target1, format=\"feather\", partitioning=partitioning)\n    result2 = ds.dataset(target2, format=\"feather\", partitioning=partitioning)\n    assert result1.to_table().equals(result2.to_table())\n\n\ndef test_write_table(tempdir):\n    table = pa.table([\n        pa.array(range(20)), pa.array(np.random.randn(20)),\n        pa.array(np.repeat(['a', 'b'], 10))\n    ], names=[\"f1\", \"f2\", \"part\"])\n\n    base_dir = tempdir / 'single'\n    ds.write_dataset(table, base_dir,\n                     basename_template='dat_{i}.arrow', format=\"feather\")\n    # check that all files are present\n    file_paths = list(base_dir.rglob(\"*\"))\n    expected_paths = [base_dir / \"dat_0.arrow\"]\n    assert set(file_paths) == set(expected_paths)\n    # check Table roundtrip\n    result = ds.dataset(base_dir, format=\"ipc\").to_table()\n    assert result.equals(table)\n\n    # with partitioning\n    base_dir = tempdir / 'partitioned'\n    expected_paths = [\n        base_dir / \"part=a\", base_dir / \"part=a\" / \"dat_0.arrow\",\n        base_dir / \"part=b\", base_dir / \"part=b\" / \"dat_0.arrow\"\n    ]\n\n    visited_paths = []\n    visited_sizes = []\n\n    def file_visitor(written_file):\n        visited_paths.append(written_file.path)\n        visited_sizes.append(written_file.size)\n\n    partitioning = ds.partitioning(\n        pa.schema([(\"part\", pa.string())]), flavor=\"hive\")\n    ds.write_dataset(table, base_dir, format=\"feather\",\n                     basename_template='dat_{i}.arrow',\n                     partitioning=partitioning, file_visitor=file_visitor)\n    file_paths = list(base_dir.rglob(\"*\"))\n    assert set(file_paths) == set(expected_paths)\n    actual_sizes = [os.path.getsize(path) for path in visited_paths]\n    assert visited_sizes == actual_sizes\n    result = ds.dataset(base_dir, format=\"ipc\", partitioning=partitioning)\n    assert result.to_table().equals(table)\n    assert len(visited_paths) == 2\n    for visited_path in visited_paths:\n        assert pathlib.Path(visited_path) in expected_paths\n\n\ndef test_write_table_multiple_fragments(tempdir):\n    table = pa.table([\n        pa.array(range(10)), pa.array(np.random.randn(10)),\n        pa.array(np.repeat(['a', 'b'], 5))\n    ], names=[\"f1\", \"f2\", \"part\"])\n    table = pa.concat_tables([table]*2)\n\n    # Table with multiple batches written as single Fragment by default\n    base_dir = tempdir / 'single'\n    ds.write_dataset(table, base_dir, format=\"feather\")\n    assert set(base_dir.rglob(\"*\")) == set([base_dir / \"part-0.feather\"])\n    assert ds.dataset(base_dir, format=\"ipc\").to_table().equals(table)\n\n    # Same for single-element list of Table\n    base_dir = tempdir / 'single-list'\n    ds.write_dataset([table], base_dir, format=\"feather\")\n    assert set(base_dir.rglob(\"*\")) == set([base_dir / \"part-0.feather\"])\n    assert ds.dataset(base_dir, format=\"ipc\").to_table().equals(table)\n\n    # Provide list of batches to write multiple fragments\n    base_dir = tempdir / 'multiple'\n    ds.write_dataset(table.to_batches(), base_dir, format=\"feather\")\n    assert set(base_dir.rglob(\"*\")) == set(\n        [base_dir / \"part-0.feather\"])\n    assert ds.dataset(base_dir, format=\"ipc\").to_table().equals(table)\n\n    # Provide list of tables to write multiple fragments\n    base_dir = tempdir / 'multiple-table'\n    ds.write_dataset([table, table], base_dir, format=\"feather\")\n    assert set(base_dir.rglob(\"*\")) == set(\n        [base_dir / \"part-0.feather\"])\n    assert ds.dataset(base_dir, format=\"ipc\").to_table().equals(\n        pa.concat_tables([table]*2)\n    )\n\n\ndef test_write_iterable(tempdir):\n    table = pa.table([\n        pa.array(range(20)), pa.array(np.random.randn(20)),\n        pa.array(np.repeat(['a', 'b'], 10))\n    ], names=[\"f1\", \"f2\", \"part\"])\n\n    base_dir = tempdir / 'inmemory_iterable'\n    ds.write_dataset((batch for batch in table.to_batches()), base_dir,\n                     schema=table.schema,\n                     basename_template='dat_{i}.arrow', format=\"feather\")\n    result = ds.dataset(base_dir, format=\"ipc\").to_table()\n    assert result.equals(table)\n\n    base_dir = tempdir / 'inmemory_reader'\n    reader = pa.RecordBatchReader.from_batches(table.schema,\n                                               table.to_batches())\n    ds.write_dataset(reader, base_dir,\n                     basename_template='dat_{i}.arrow', format=\"feather\")\n    result = ds.dataset(base_dir, format=\"ipc\").to_table()\n    assert result.equals(table)\n\n\ndef test_write_scanner(tempdir, dataset_reader):\n    table = pa.table([\n        pa.array(range(20)), pa.array(np.random.randn(20)),\n        pa.array(np.repeat(['a', 'b'], 10))\n    ], names=[\"f1\", \"f2\", \"part\"])\n    dataset = ds.dataset(table)\n\n    base_dir = tempdir / 'dataset_from_scanner'\n    ds.write_dataset(dataset_reader.scanner(\n        dataset), base_dir, format=\"feather\")\n    result = dataset_reader.to_table(ds.dataset(base_dir, format=\"ipc\"))\n    assert result.equals(table)\n\n    # scanner with different projected_schema\n    base_dir = tempdir / 'dataset_from_scanner2'\n    ds.write_dataset(dataset_reader.scanner(dataset, columns=[\"f1\"]),\n                     base_dir, format=\"feather\")\n    result = dataset_reader.to_table(ds.dataset(base_dir, format=\"ipc\"))\n    assert result.equals(table.select([\"f1\"]))\n\n    # schema not allowed when writing a scanner\n    with pytest.raises(ValueError, match=\"Cannot specify a schema\"):\n        ds.write_dataset(dataset_reader.scanner(dataset), base_dir,\n                         schema=table.schema, format=\"feather\")\n\n\ndef test_write_table_partitioned_dict(tempdir):\n    # ensure writing table partitioned on a dictionary column works without\n    # specifying the dictionary values explicitly\n    table = pa.table([\n        pa.array(range(20)),\n        pa.array(np.repeat(['a', 'b'], 10)).dictionary_encode(),\n    ], names=['col', 'part'])\n\n    partitioning = ds.partitioning(table.select([\"part\"]).schema)\n\n    base_dir = tempdir / \"dataset\"\n    ds.write_dataset(\n        table, base_dir, format=\"feather\", partitioning=partitioning\n    )\n\n    # check roundtrip\n    partitioning_read = ds.DirectoryPartitioning.discover(\n        [\"part\"], infer_dictionary=True)\n    result = ds.dataset(\n        base_dir, format=\"ipc\", partitioning=partitioning_read\n    ).to_table()\n    assert result.equals(table)\n\n\n@pytest.mark.parquet\ndef test_write_dataset_parquet(tempdir):\n    table = pa.table([\n        pa.array(range(20), type=\"uint32\"),\n        pa.array(np.arange(\"2012-01-01\", 20, dtype=\"datetime64[D]\").astype(\n            \"datetime64[ns]\")),\n        pa.array(np.repeat(['a', 'b'], 10))\n    ], names=[\"f1\", \"f2\", \"part\"])\n\n    # using default \"parquet\" format string\n\n    base_dir = tempdir / 'parquet_dataset'\n    ds.write_dataset(table, base_dir, format=\"parquet\")\n    # check that all files are present\n    file_paths = list(base_dir.rglob(\"*\"))\n    expected_paths = [base_dir / \"part-0.parquet\"]\n    assert set(file_paths) == set(expected_paths)\n    # check Table roundtrip with default version\n    result = ds.dataset(base_dir, format=\"parquet\").to_table()\n    assert result.equals(table)\n\n    # using custom options\n    for version in [\"1.0\", \"2.4\", \"2.6\"]:\n        format = ds.ParquetFileFormat()\n        opts = format.make_write_options(version=version)\n        assert \"<pyarrow.dataset.ParquetFileWriteOptions\" in repr(opts)\n        base_dir = tempdir / 'parquet_dataset_version{0}'.format(version)\n        ds.write_dataset(table, base_dir, format=format, file_options=opts)\n        meta = pq.read_metadata(base_dir / \"part-0.parquet\")\n        expected_version = \"1.0\" if version == \"1.0\" else \"2.6\"\n        assert meta.format_version == expected_version\n\n        # ensure version is actually honored based on supported datatypes\n        result = ds.dataset(base_dir, format=\"parquet\").to_table()\n        schema = table.schema\n        if version == \"1.0\":\n            # uint32 is written as int64\n            schema = schema.set(0, schema.field(0).with_type(pa.int64()))\n        if version in (\"1.0\", \"2.4\"):\n            schema = schema.set(1, schema.field(1).with_type(pa.timestamp(\"us\")))\n        expected = table.cast(schema)\n        assert result.equals(expected)\n\n\ndef test_write_dataset_csv(tempdir):\n    table = pa.table([\n        pa.array(range(20)), pa.array(np.random.randn(20)),\n        pa.array(np.repeat(['a', 'b'], 10))\n    ], names=[\"f1\", \"f2\", \"chr1\"])\n\n    base_dir = tempdir / 'csv_dataset'\n    ds.write_dataset(table, base_dir, format=\"csv\")\n    # check that all files are present\n    file_paths = list(base_dir.rglob(\"*\"))\n    expected_paths = [base_dir / \"part-0.csv\"]\n    assert set(file_paths) == set(expected_paths)\n    # check Table roundtrip\n    result = ds.dataset(base_dir, format=\"csv\").to_table()\n    assert result.equals(table)\n\n    # using custom options\n    format = ds.CsvFileFormat(read_options=pyarrow.csv.ReadOptions(\n        column_names=table.schema.names))\n    opts = format.make_write_options(include_header=False)\n    base_dir = tempdir / 'csv_dataset_noheader'\n    ds.write_dataset(table, base_dir, format=format, file_options=opts)\n    result = ds.dataset(base_dir, format=format).to_table()\n    assert result.equals(table)\n\n\n@pytest.mark.parquet\ndef test_write_dataset_parquet_file_visitor(tempdir):\n    table = pa.table([\n        pa.array(range(20)), pa.array(np.random.randn(20)),\n        pa.array(np.repeat(['a', 'b'], 10))\n    ], names=[\"f1\", \"f2\", \"part\"])\n\n    visitor_called = False\n\n    def file_visitor(written_file):\n        nonlocal visitor_called\n        if (written_file.metadata is not None and\n                written_file.metadata.num_columns == 3):\n            visitor_called = True\n\n    base_dir = tempdir / 'parquet_dataset'\n    ds.write_dataset(table, base_dir, format=\"parquet\",\n                     file_visitor=file_visitor)\n\n    assert visitor_called\n\n\n@pytest.mark.parquet\ndef test_partition_dataset_parquet_file_visitor(tempdir):\n    f1_vals = [item for chunk in range(4) for item in [chunk] * 10]\n    f2_vals = [item*10 for chunk in range(4) for item in [chunk] * 10]\n    table = pa.table({'f1': f1_vals, 'f2': f2_vals,\n                      'part': np.repeat(['a', 'b'], 20)})\n\n    root_path = tempdir / 'partitioned'\n    partitioning = ds.partitioning(\n        pa.schema([(\"part\", pa.string())]), flavor=\"hive\")\n\n    paths_written = []\n\n    sample_metadata = None\n\n    def file_visitor(written_file):\n        nonlocal sample_metadata\n        if written_file.metadata:\n            sample_metadata = written_file.metadata\n        paths_written.append(written_file.path)\n\n    ds.write_dataset(\n        table, root_path, format=\"parquet\", partitioning=partitioning,\n        use_threads=True, file_visitor=file_visitor\n    )\n\n    expected_paths = {\n        root_path / 'part=a' / 'part-0.parquet',\n        root_path / 'part=b' / 'part-0.parquet'\n    }\n    paths_written_set = set(map(pathlib.Path, paths_written))\n    assert paths_written_set == expected_paths\n    assert sample_metadata is not None\n    assert sample_metadata.num_columns == 2\n\n\n@pytest.mark.parquet\n@pytest.mark.pandas\ndef test_write_dataset_arrow_schema_metadata(tempdir):\n    # ensure we serialize ARROW schema in the parquet metadata, to have a\n    # correct roundtrip (e.g. preserve non-UTC timezone)\n    table = pa.table({\"a\": [pd.Timestamp(\"2012-01-01\", tz=\"Europe/Brussels\")]})\n    assert table[\"a\"].type.tz == \"Europe/Brussels\"\n\n    ds.write_dataset(table, tempdir, format=\"parquet\")\n    result = pq.read_table(tempdir / \"part-0.parquet\")\n    assert result[\"a\"].type.tz == \"Europe/Brussels\"\n\n\ndef test_write_dataset_schema_metadata(tempdir):\n    # ensure that schema metadata gets written\n    from pyarrow import feather\n\n    table = pa.table({'a': [1, 2, 3]})\n    table = table.replace_schema_metadata({b'key': b'value'})\n    ds.write_dataset(table, tempdir, format=\"feather\")\n\n    schema = feather.read_table(tempdir / \"part-0.feather\").schema\n    assert schema.metadata == {b'key': b'value'}\n\n\n@pytest.mark.parquet\ndef test_write_dataset_schema_metadata_parquet(tempdir):\n    # ensure that schema metadata gets written\n    table = pa.table({'a': [1, 2, 3]})\n    table = table.replace_schema_metadata({b'key': b'value'})\n    ds.write_dataset(table, tempdir, format=\"parquet\")\n\n    schema = pq.read_table(tempdir / \"part-0.parquet\").schema\n    assert schema.metadata == {b'key': b'value'}\n\n\n@pytest.mark.parquet\n@pytest.mark.s3\ndef test_write_dataset_s3(s3_example_simple):\n    # write dataset with s3 filesystem\n    _, _, fs, _, host, port, access_key, secret_key = s3_example_simple\n    uri_template = (\n        \"s3://{}:{}@{{}}?scheme=http&endpoint_override={}:{}\".format(\n            access_key, secret_key, host, port)\n    )\n\n    table = pa.table([\n        pa.array(range(20)), pa.array(np.random.randn(20)),\n        pa.array(np.repeat(['a', 'b'], 10))],\n        names=[\"f1\", \"f2\", \"part\"]\n    )\n    part = ds.partitioning(pa.schema([(\"part\", pa.string())]), flavor=\"hive\")\n\n    # writing with filesystem object\n    ds.write_dataset(\n        table, \"mybucket/dataset\", filesystem=fs, format=\"feather\",\n        partitioning=part\n    )\n    # check roundtrip\n    result = ds.dataset(\n        \"mybucket/dataset\", filesystem=fs, format=\"ipc\", partitioning=\"hive\"\n    ).to_table()\n    assert result.equals(table)\n\n    # writing with URI\n    uri = uri_template.format(\"mybucket/dataset2\")\n    ds.write_dataset(table, uri, format=\"feather\", partitioning=part)\n    # check roundtrip\n    result = ds.dataset(\n        \"mybucket/dataset2\", filesystem=fs, format=\"ipc\", partitioning=\"hive\"\n    ).to_table()\n    assert result.equals(table)\n\n    # writing with path + URI as filesystem\n    uri = uri_template.format(\"mybucket\")\n    ds.write_dataset(\n        table, \"dataset3\", filesystem=uri, format=\"feather\", partitioning=part\n    )\n    # check roundtrip\n    result = ds.dataset(\n        \"mybucket/dataset3\", filesystem=fs, format=\"ipc\", partitioning=\"hive\"\n    ).to_table()\n    assert result.equals(table)\n\n\n_minio_put_only_policy = \"\"\"{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:ListBucket\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        }\n    ]\n}\"\"\"\n\n\n@pytest.mark.parquet\n@pytest.mark.s3\ndef test_write_dataset_s3_put_only(s3_server):\n    # [ARROW-15892] Testing the create_dir flag which will restrict\n    # creating a new directory for writing a dataset. This is\n    # required while writing a dataset in s3 where we have very\n    # limited permissions and thus we can directly write the dataset\n    # without creating a directory.\n    from pyarrow.fs import S3FileSystem\n\n    # write dataset with s3 filesystem\n    host, port, _, _ = s3_server['connection']\n    fs = S3FileSystem(\n        access_key='limited',\n        secret_key='limited123',\n        endpoint_override='{}:{}'.format(host, port),\n        scheme='http'\n    )\n    _configure_s3_limited_user(s3_server, _minio_put_only_policy)\n\n    table = pa.table([\n        pa.array(range(20)), pa.array(np.random.randn(20)),\n        pa.array(np.repeat(['a', 'b'], 10))],\n        names=[\"f1\", \"f2\", \"part\"]\n    )\n    part = ds.partitioning(pa.schema([(\"part\", pa.string())]), flavor=\"hive\")\n\n    # writing with filesystem object with create_dir flag set to false\n    ds.write_dataset(\n        table, \"existing-bucket\", filesystem=fs,\n        format=\"feather\", create_dir=False, partitioning=part,\n        existing_data_behavior='overwrite_or_ignore'\n    )\n    # check roundtrip\n    result = ds.dataset(\n        \"existing-bucket\", filesystem=fs, format=\"ipc\", partitioning=\"hive\"\n    ).to_table()\n    assert result.equals(table)\n\n    # Passing create_dir is fine if the bucket already exists\n    ds.write_dataset(\n        table, \"existing-bucket\", filesystem=fs,\n        format=\"feather\", create_dir=True, partitioning=part,\n        existing_data_behavior='overwrite_or_ignore'\n    )\n    # check roundtrip\n    result = ds.dataset(\n        \"existing-bucket\", filesystem=fs, format=\"ipc\", partitioning=\"hive\"\n    ).to_table()\n    assert result.equals(table)\n\n    # Error enforced by filesystem\n    with pytest.raises(OSError,\n                       match=\"Bucket 'non-existing-bucket' not found\"):\n        ds.write_dataset(\n            table, \"non-existing-bucket\", filesystem=fs,\n            format=\"feather\", create_dir=True,\n            existing_data_behavior='overwrite_or_ignore'\n        )\n\n    # Error enforced by minio / S3 service\n    fs = S3FileSystem(\n        access_key='limited',\n        secret_key='limited123',\n        endpoint_override='{}:{}'.format(host, port),\n        scheme='http',\n        allow_bucket_creation=True,\n    )\n    with pytest.raises(OSError, match=\"Access Denied\"):\n        ds.write_dataset(\n            table, \"non-existing-bucket\", filesystem=fs,\n            format=\"feather\", create_dir=True,\n            existing_data_behavior='overwrite_or_ignore'\n        )\n\n\n@pytest.mark.parquet\ndef test_dataset_null_to_dictionary_cast(tempdir, dataset_reader):\n    # ARROW-12420\n    table = pa.table({\"a\": [None, None]})\n    pq.write_table(table, tempdir / \"test.parquet\")\n\n    schema = pa.schema([\n        pa.field(\"a\", pa.dictionary(pa.int32(), pa.string()))\n    ])\n    fsds = ds.FileSystemDataset.from_paths(\n        paths=[tempdir / \"test.parquet\"],\n        schema=schema,\n        format=ds.ParquetFileFormat(),\n        filesystem=fs.LocalFileSystem(),\n    )\n    table = dataset_reader.to_table(fsds)\n    assert table.schema == schema\n\n\n@pytest.mark.dataset\ndef test_dataset_join(tempdir):\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"]\n    })\n    ds.write_dataset(t1, tempdir / \"t1\", format=\"ipc\")\n    ds1 = ds.dataset(tempdir / \"t1\", format=\"ipc\")\n\n    t2 = pa.table({\n        \"colB\": [99, 2, 1],\n        \"col3\": [\"Z\", \"B\", \"A\"]\n    })\n    ds.write_dataset(t2, tempdir / \"t2\", format=\"ipc\")\n    ds2 = ds.dataset(tempdir / \"t2\", format=\"ipc\")\n\n    result = ds1.join(ds2, \"colA\", \"colB\")\n    assert result.to_table() == pa.table({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"],\n        \"col3\": [\"A\", \"B\", None]\n    })\n\n    result = ds1.join(ds2, \"colA\", \"colB\", join_type=\"full outer\")\n    assert result.to_table().sort_by(\"colA\") == pa.table({\n        \"colA\": [1, 2, 6, 99],\n        \"col2\": [\"a\", \"b\", \"f\", None],\n        \"col3\": [\"A\", \"B\", None, \"Z\"]\n    })\n\n\n@pytest.mark.dataset\ndef test_dataset_join_unique_key(tempdir):\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"]\n    })\n    ds.write_dataset(t1, tempdir / \"t1\", format=\"ipc\")\n    ds1 = ds.dataset(tempdir / \"t1\", format=\"ipc\")\n\n    t2 = pa.table({\n        \"colA\": [99, 2, 1],\n        \"col3\": [\"Z\", \"B\", \"A\"]\n    })\n    ds.write_dataset(t2, tempdir / \"t2\", format=\"ipc\")\n    ds2 = ds.dataset(tempdir / \"t2\", format=\"ipc\")\n\n    result = ds1.join(ds2, \"colA\")\n    assert result.to_table() == pa.table({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"],\n        \"col3\": [\"A\", \"B\", None]\n    })\n\n    result = ds1.join(ds2, \"colA\", join_type=\"full outer\", right_suffix=\"_r\")\n    assert result.to_table().sort_by(\"colA\") == pa.table({\n        \"colA\": [1, 2, 6, 99],\n        \"col2\": [\"a\", \"b\", \"f\", None],\n        \"col3\": [\"A\", \"B\", None, \"Z\"]\n    })\n\n\n@pytest.mark.dataset\ndef test_dataset_join_collisions(tempdir):\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"colVals\": [\"a\", \"b\", \"f\"]\n    })\n    ds.write_dataset(t1, tempdir / \"t1\", format=\"ipc\")\n    ds1 = ds.dataset(tempdir / \"t1\", format=\"ipc\")\n\n    t2 = pa.table({\n        \"colA\": [99, 2, 1],\n        \"colB\": [99, 20, 10],\n        \"colVals\": [\"Z\", \"B\", \"A\"]\n    })\n    ds.write_dataset(t2, tempdir / \"t2\", format=\"ipc\")\n    ds2 = ds.dataset(tempdir / \"t2\", format=\"ipc\")\n\n    result = ds1.join(ds2, \"colA\", join_type=\"full outer\", right_suffix=\"_r\")\n    assert result.to_table().sort_by(\"colA\") == pa.table([\n        [1, 2, 6, 99],\n        [10, 20, 60, None],\n        [\"a\", \"b\", \"f\", None],\n        [10, 20, None, 99],\n        [\"A\", \"B\", None, \"Z\"],\n    ], names=[\"colA\", \"colB\", \"colVals\", \"colB_r\", \"colVals_r\"])\n\n\n@pytest.mark.dataset\ndef test_dataset_join_asof(tempdir):\n    t1 = pa.Table.from_pydict({\n        \"colA\": [1, 1, 5, 6, 7],\n        \"col2\": [\"a\", \"b\", \"a\", \"b\", \"f\"]\n    })\n    ds.write_dataset(t1, tempdir / \"t1\", format=\"ipc\")\n    ds1 = ds.dataset(tempdir / \"t1\", format=\"ipc\")\n\n    t2 = pa.Table.from_pydict({\n        \"colB\": [2, 9, 15],\n        \"col3\": [\"a\", \"b\", \"g\"],\n        \"colC\": [1., 3., 5.]\n    })\n    ds.write_dataset(t2, tempdir / \"t2\", format=\"ipc\")\n    ds2 = ds.dataset(tempdir / \"t2\", format=\"ipc\")\n\n    result = ds1.join_asof(\n        ds2, on=\"colA\", by=\"col2\", tolerance=1,\n        right_on=\"colB\", right_by=\"col3\",\n    )\n    assert result.to_table().sort_by(\"colA\") == pa.table({\n        \"colA\": [1, 1, 5, 6, 7],\n        \"col2\": [\"a\", \"b\", \"a\", \"b\", \"f\"],\n        \"colC\": [1., None, None, None, None],\n    })\n\n\n@pytest.mark.dataset\ndef test_dataset_join_asof_multiple_by(tempdir):\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"on\": [1, 2, 3],\n    })\n    ds.write_dataset(t1, tempdir / \"t1\", format=\"ipc\")\n    ds1 = ds.dataset(tempdir / \"t1\", format=\"ipc\")\n\n    t2 = pa.table({\n        \"colB\": [99, 20, 10],\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"colA\": [99, 2, 1],\n        \"on\": [2, 3, 4],\n    })\n    ds.write_dataset(t2, tempdir / \"t2\", format=\"ipc\")\n    ds2 = ds.dataset(tempdir / \"t2\", format=\"ipc\")\n\n    result = ds1.join_asof(\n        ds2, on=\"on\", by=[\"colA\", \"colB\"], tolerance=1\n    )\n    assert result.to_table().sort_by(\"colA\") == pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"on\": [1, 2, 3],\n        \"colVals\": [None, \"B\", None],\n    })\n\n\n@pytest.mark.dataset\ndef test_dataset_join_asof_empty_by(tempdir):\n    t1 = pa.table({\n        \"on\": [1, 2, 3],\n    })\n    ds.write_dataset(t1, tempdir / \"t1\", format=\"ipc\")\n    ds1 = ds.dataset(tempdir / \"t1\", format=\"ipc\")\n\n    t2 = pa.table({\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"on\": [2, 3, 4],\n    })\n    ds.write_dataset(t2, tempdir / \"t2\", format=\"ipc\")\n    ds2 = ds.dataset(tempdir / \"t2\", format=\"ipc\")\n\n    result = ds1.join_asof(\n        ds2, on=\"on\", by=[], tolerance=1\n    )\n    assert result.to_table() == pa.table({\n        \"on\": [1, 2, 3],\n        \"colVals\": [\"Z\", \"Z\", \"B\"],\n    })\n\n\n@pytest.mark.dataset\ndef test_dataset_join_asof_collisions(tempdir):\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"on\": [1, 2, 3],\n        \"colVals\": [\"a\", \"b\", \"f\"]\n    })\n    ds.write_dataset(t1, tempdir / \"t1\", format=\"ipc\")\n    ds1 = ds.dataset(tempdir / \"t1\", format=\"ipc\")\n\n    t2 = pa.table({\n        \"colB\": [99, 20, 10],\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"colUniq\": [100, 200, 300],\n        \"colA\": [99, 2, 1],\n        \"on\": [2, 3, 4],\n    })\n    ds.write_dataset(t2, tempdir / \"t2\", format=\"ipc\")\n    ds2 = ds.dataset(tempdir / \"t2\", format=\"ipc\")\n\n    msg = (\n        \"Columns {'colVals'} present in both tables. \"\n        \"AsofJoin does not support column collisions.\"\n    )\n    with pytest.raises(ValueError, match=msg):\n        ds1.join_asof(\n            ds2, on=\"on\", by=[\"colA\", \"colB\"], tolerance=1,\n            right_on=\"on\", right_by=[\"colA\", \"colB\"],\n        )\n\n\n@pytest.mark.parametrize('dstype', [\n    \"fs\", \"mem\"\n])\ndef test_dataset_filter(tempdir, dstype):\n    t1 = pa.table({\n        \"colA\": [1, 2, 6, 8],\n        \"col2\": [\"a\", \"b\", \"f\", \"g\"]\n    })\n    if dstype == \"fs\":\n        ds.write_dataset(t1, tempdir / \"t1\", format=\"ipc\")\n        ds1 = ds.dataset(tempdir / \"t1\", format=\"ipc\")\n    elif dstype == \"mem\":\n        ds1 = ds.dataset(t1)\n    else:\n        raise NotImplementedError\n\n    # Ensure chained filtering works.\n    result = ds1.filter(pc.field(\"colA\") < 3).filter(pc.field(\"col2\") == \"a\")\n    expected = ds.FileSystemDataset if dstype == \"fs\" else ds.InMemoryDataset\n    assert isinstance(result, expected)\n\n    assert result.to_table() == pa.table({\n        \"colA\": [1],\n        \"col2\": [\"a\"]\n    })\n\n    assert result.head(5) == pa.table({\n        \"colA\": [1],\n        \"col2\": [\"a\"]\n    })\n\n    # Ensure that further filtering with scanners works too\n    r2 = ds1.filter(pc.field(\"colA\") < 8).filter(\n        pc.field(\"colA\") > 1).scanner(filter=pc.field(\"colA\") != 6)\n    assert r2.to_table() == pa.table({\n        \"colA\": [2],\n        \"col2\": [\"b\"]\n    })\n\n    # Ensure that writing back to disk works.\n    ds.write_dataset(result, tempdir / \"filtered\", format=\"ipc\")\n    filtered = ds.dataset(tempdir / \"filtered\", format=\"ipc\")\n    assert filtered.to_table() == pa.table({\n        \"colA\": [1],\n        \"col2\": [\"a\"]\n    })\n\n    # Ensure that joining to a filtered Dataset works.\n    joined = result.join(ds.dataset(pa.table({\n        \"colB\": [10, 20],\n        \"col2\": [\"a\", \"b\"]\n    })), keys=\"col2\", join_type=\"right outer\")\n    assert joined.to_table().sort_by(\"colB\") == pa.table({\n        \"colA\": [1, None],\n        \"colB\": [10, 20],\n        \"col2\": [\"a\", \"b\"]\n    })\n\n    # Filter with None doesn't work for now\n    with pytest.raises(TypeError):\n        ds1.filter(None)\n\n    # Can't get fragments of a filtered dataset\n    with pytest.raises(ValueError):\n        result.get_fragments()\n\n    # Ensure replacing schema preserves the filter.\n    schema_without_col2 = ds1.schema.remove(1)\n    newschema = ds1.filter(\n        pc.field(\"colA\") < 3\n    ).replace_schema(schema_without_col2)\n    assert newschema.to_table() == pa.table({\n        \"colA\": [1, 2],\n    })\n    with pytest.raises(pa.ArrowInvalid):\n        # The schema might end up being replaced with\n        # something that makes the filter invalid.\n        # Let's make sure we error nicely.\n        result.replace_schema(schema_without_col2).to_table()\n\n\n@pytest.mark.parametrize('dstype', [\n    \"fs\", \"mem\"\n])\ndef test_union_dataset_filter(tempdir, dstype):\n    t1 = pa.table({\n        \"colA\": [1, 2, 6, 8],\n        \"col2\": [\"a\", \"b\", \"f\", \"g\"]\n    })\n    t2 = pa.table({\n        \"colA\": [9, 10, 11],\n        \"col2\": [\"h\", \"i\", \"l\"]\n    })\n    if dstype == \"fs\":\n        ds.write_dataset(t1, tempdir / \"t1\", format=\"ipc\")\n        ds1 = ds.dataset(tempdir / \"t1\", format=\"ipc\")\n        ds.write_dataset(t2, tempdir / \"t2\", format=\"ipc\")\n        ds2 = ds.dataset(tempdir / \"t2\", format=\"ipc\")\n    elif dstype == \"mem\":\n        ds1 = ds.dataset(t1)\n        ds2 = ds.dataset(t2)\n    else:\n        raise NotImplementedError\n\n    filtered_union_ds = ds.dataset((ds1, ds2)).filter(\n        (pc.field(\"colA\") < 3) | (pc.field(\"colA\") == 9)\n    )\n    assert filtered_union_ds.to_table() == pa.table({\n        \"colA\": [1, 2, 9],\n        \"col2\": [\"a\", \"b\", \"h\"]\n    })\n\n    joined = filtered_union_ds.join(ds.dataset(pa.table({\n        \"colB\": [10, 20],\n        \"col2\": [\"a\", \"b\"]\n    })), keys=\"col2\", join_type=\"left outer\")\n    assert joined.to_table().sort_by(\"colA\") == pa.table({\n        \"colA\": [1, 2, 9],\n        \"col2\": [\"a\", \"b\", \"h\"],\n        \"colB\": [10, 20, None]\n    })\n\n    filtered_ds1 = ds1.filter(pc.field(\"colA\") < 3)\n    filtered_ds2 = ds2.filter(pc.field(\"colA\") < 10)\n\n    with pytest.raises(ValueError, match=\"currently not supported\"):\n        ds.dataset((filtered_ds1, filtered_ds2))\n\n\ndef test_parquet_dataset_filter(tempdir):\n    root_path = tempdir / \"test_parquet_dataset_filter\"\n    metadata_path, _ = _create_parquet_dataset_simple(root_path)\n    dataset = ds.parquet_dataset(metadata_path)\n\n    result = dataset.to_table()\n    assert result.num_rows == 40\n\n    filtered_ds = dataset.filter(pc.field(\"f1\") < 2)\n    assert filtered_ds.to_table().num_rows == 20\n\n    with pytest.raises(ValueError):\n        filtered_ds.get_fragments()\n\n\ndef test_write_dataset_with_scanner_use_projected_schema(tempdir):\n    \"\"\"\n    Ensure the projected schema is used to validate partitions for scanner\n\n    https://issues.apache.org/jira/browse/ARROW-17228\n    \"\"\"\n    table = pa.table([pa.array(range(20))], names=[\"original_column\"])\n    table_dataset = ds.dataset(table)\n    columns = {\n        \"renamed_column\": ds.field(\"original_column\"),\n    }\n    scanner = table_dataset.scanner(columns=columns)\n\n    ds.write_dataset(\n        scanner, tempdir, partitioning=[\"renamed_column\"], format=\"ipc\")\n    with (\n        pytest.raises(\n            KeyError, match=r\"'Column original_column does not exist in schema\"\n        )\n    ):\n        ds.write_dataset(\n            scanner, tempdir, partitioning=[\"original_column\"], format=\"ipc\"\n        )\n\n\n@pytest.mark.parametrize(\"format\", (\"ipc\", \"parquet\"))\ndef test_read_table_nested_columns(tempdir, format):\n    if format == \"parquet\":\n        pytest.importorskip(\"pyarrow.parquet\")\n\n    table = pa.table({\"user_id\": [\"abc123\", \"qrs456\"],\n                      \"a.dotted.field\": [1, 2],\n                      \"interaction\": [\n        {\"type\": None, \"element\": \"button\",\n         \"values\": [1, 2], \"structs\": [{\"foo\": \"bar\"}, None]},\n        {\"type\": \"scroll\", \"element\": \"window\",\n         \"values\": [None, 3, 4], \"structs\": [{\"fizz\": \"buzz\"}]}\n    ]})\n    ds.write_dataset(table, tempdir / \"table\", format=format)\n    ds1 = ds.dataset(tempdir / \"table\", format=format)\n\n    # Dot path to read subsets of nested data\n    table = ds1.to_table(\n        columns=[\"user_id\", \"interaction.type\", \"interaction.values\",\n                 \"interaction.structs\", \"a.dotted.field\"])\n    assert table.to_pylist() == [\n        {'user_id': 'abc123', 'type': None, 'values': [1, 2],\n         'structs': [{'fizz': None, 'foo': 'bar'}, None], 'a.dotted.field': 1},\n        {'user_id': 'qrs456', 'type': 'scroll', 'values': [None, 3, 4],\n         'structs': [{'fizz': 'buzz', 'foo': None}], 'a.dotted.field': 2}\n    ]\n\n\ndef test_dataset_partition_with_slash(tmpdir):\n    from pyarrow import dataset as ds\n\n    path = tmpdir / \"slash-writer-x\"\n\n    dt_table = pa.Table.from_arrays([\n        pa.array([1, 2, 3, 4, 5], pa.int32()),\n        pa.array([\"experiment/A/f.csv\", \"experiment/B/f.csv\",\n                  \"experiment/A/f.csv\", \"experiment/C/k.csv\",\n                  \"experiment/M/i.csv\"], pa.utf8())], [\"exp_id\", \"exp_meta\"])\n\n    ds.write_dataset(\n        data=dt_table,\n        base_dir=path,\n        format='ipc',\n        partitioning=['exp_meta'],\n        partitioning_flavor='hive',\n    )\n\n    read_table = ds.dataset(\n        source=path,\n        format='ipc',\n        partitioning='hive',\n        schema=pa.schema([pa.field(\"exp_id\", pa.int32()),\n                          pa.field(\"exp_meta\", pa.utf8())])\n    ).to_table().combine_chunks()\n\n    assert dt_table == read_table.sort_by(\"exp_id\")\n\n    exp_meta = dt_table.column(1).to_pylist()\n    exp_meta = sorted(set(exp_meta))  # take unique\n    encoded_paths = [\"exp_meta=\" + quote(path, safe='') for path in exp_meta]\n    file_paths = sorted(os.listdir(path))\n\n    assert encoded_paths == file_paths\n\n\n@pytest.mark.parquet\ndef test_write_dataset_preserve_nullability(tempdir):\n    # GH-35730\n    schema_nullable = pa.schema([\n        pa.field(\"x\", pa.int64(), nullable=False),\n        pa.field(\"y\", pa.int64(), nullable=True)])\n\n    arrays = [[1, 2, 3], [None, 5, None]]\n    table = pa.Table.from_arrays(arrays, schema=schema_nullable)\n\n    pq.write_to_dataset(table, tempdir / \"nulltest1\")\n    dataset = ds.dataset(tempdir / \"nulltest1\", format=\"parquet\")\n    # nullability of field is preserved\n    assert dataset.to_table().schema.equals(schema_nullable)\n\n    ds.write_dataset(table, tempdir / \"nulltest2\", format=\"parquet\")\n    dataset = ds.dataset(tempdir / \"nulltest2\", format=\"parquet\")\n    assert dataset.to_table().schema.equals(schema_nullable)\n\n    ds.write_dataset([table, table], tempdir / \"nulltest3\", format=\"parquet\")\n    dataset = ds.dataset(tempdir / \"nulltest3\", format=\"parquet\")\n    assert dataset.to_table().schema.equals(schema_nullable)\n\n\ndef test_write_dataset_preserve_field_metadata(tempdir):\n    schema_metadata = pa.schema([\n        pa.field(\"x\", pa.int64(), metadata={b'foo': b'bar'}),\n        pa.field(\"y\", pa.int64())])\n\n    schema_no_meta = pa.schema([\n        pa.field(\"x\", pa.int64()),\n        pa.field(\"y\", pa.int64())])\n\n    arrays = [[1, 2, 3], [None, 5, None]]\n    table = pa.Table.from_arrays(arrays, schema=schema_metadata)\n    table_no_meta = pa.Table.from_arrays(arrays, schema=schema_no_meta)\n\n    # If no schema is provided the schema of the first table will be used\n    ds.write_dataset([table, table_no_meta], tempdir / \"test1\", format=\"parquet\")\n    dataset = ds.dataset(tempdir / \"test1\", format=\"parquet\")\n    assert dataset.to_table().schema.equals(schema_metadata, check_metadata=True)\n\n    ds.write_dataset([table_no_meta, table], tempdir / \"test2\", format=\"parquet\")\n    dataset = ds.dataset(tempdir / \"test2\", format=\"parquet\")\n    assert dataset.to_table().schema.equals(schema_no_meta, check_metadata=True)\n\n    # If a schema is provided it will override the schema of the input\n    ds.write_dataset([table_no_meta, table], tempdir / \"test3\", format=\"parquet\",\n                     schema=schema_metadata)\n    dataset = ds.dataset(tempdir / \"test3\", format=\"parquet\")\n    assert dataset.to_table().schema.equals(schema_metadata, check_metadata=True)\n\n\ndef test_write_dataset_write_page_index(tempdir):\n    for write_statistics in [True, False]:\n        for write_page_index in [True, False]:\n            schema = pa.schema([\n                pa.field(\"x\", pa.int64()),\n                pa.field(\"y\", pa.int64())])\n\n            arrays = [[1, 2, 3], [None, 5, None]]\n            table = pa.Table.from_arrays(arrays, schema=schema)\n\n            file_format = ds.ParquetFileFormat()\n            base_dir = tempdir / f\"write_page_index_{write_page_index}\"\n            ds.write_dataset(\n                table,\n                base_dir,\n                format=\"parquet\",\n                file_options=file_format.make_write_options(\n                    write_statistics=write_statistics,\n                    write_page_index=write_page_index,\n                ),\n                existing_data_behavior='overwrite_or_ignore',\n            )\n            ds1 = ds.dataset(base_dir, format=\"parquet\")\n\n            for file in ds1.files:\n                # Can retrieve sorting columns from metadata\n                metadata = pq.read_metadata(file)\n                cc = metadata.row_group(0).column(0)\n                assert cc.has_offset_index is write_page_index\n                assert cc.has_column_index is write_page_index & write_statistics\n\n\n@pytest.mark.parametrize('dstype', [\n    \"fs\", \"mem\"\n])\ndef test_dataset_sort_by(tempdir, dstype):\n    table = pa.table([\n        pa.array([3, 1, 4, 2, 5]),\n        pa.array([\"b\", \"a\", \"b\", \"a\", \"c\"]),\n    ], names=[\"values\", \"keys\"])\n\n    if dstype == \"fs\":\n        ds.write_dataset(table, tempdir / \"t1\", format=\"ipc\")\n        dt = ds.dataset(tempdir / \"t1\", format=\"ipc\")\n    elif dstype == \"mem\":\n        dt = ds.dataset(table)\n    else:\n        raise NotImplementedError\n\n    assert dt.sort_by(\"values\").to_table().to_pydict() == {\n        \"keys\": [\"a\", \"a\", \"b\", \"b\", \"c\"],\n        \"values\": [1, 2, 3, 4, 5]\n    }\n\n    assert dt.sort_by([(\"values\", \"descending\")]).to_table().to_pydict() == {\n        \"keys\": [\"c\", \"b\", \"b\", \"a\", \"a\"],\n        \"values\": [5, 4, 3, 2, 1]\n    }\n\n    assert dt.filter((pc.field(\"values\") < 4)).sort_by(\n        \"values\"\n    ).to_table().to_pydict() == {\n        \"keys\": [\"a\", \"a\", \"b\"],\n        \"values\": [1, 2, 3]\n    }\n\n    table = pa.Table.from_arrays([\n        pa.array([5, 7, 7, 35], type=pa.int64()),\n        pa.array([\"foo\", \"car\", \"bar\", \"foobar\"])\n    ], names=[\"a\", \"b\"])\n    dt = ds.dataset(table)\n\n    sorted_tab = dt.sort_by([(\"a\", \"descending\")])\n    sorted_tab_dict = sorted_tab.to_table().to_pydict()\n    assert sorted_tab_dict[\"a\"] == [35, 7, 7, 5]\n    assert sorted_tab_dict[\"b\"] == [\"foobar\", \"car\", \"bar\", \"foo\"]\n\n    sorted_tab = dt.sort_by([(\"a\", \"ascending\")])\n    sorted_tab_dict = sorted_tab.to_table().to_pydict()\n    assert sorted_tab_dict[\"a\"] == [5, 7, 7, 35]\n    assert sorted_tab_dict[\"b\"] == [\"foo\", \"car\", \"bar\", \"foobar\"]\n\n\ndef test_checksum_write_dataset_read_dataset_to_table(tempdir):\n    \"\"\"Check that checksum verification works for datasets created with\n    ds.write_dataset and read with ds.dataset.to_table\"\"\"\n\n    table_orig = pa.table({'a': [1, 2, 3, 4]})\n\n    # Write a sample dataset with page checksum enabled\n    pq_write_format = pa.dataset.ParquetFileFormat()\n    write_options = pq_write_format.make_write_options(\n        write_page_checksum=True)\n\n    original_dir_path = tempdir / 'correct_dir'\n    ds.write_dataset(\n        data=table_orig,\n        base_dir=original_dir_path,\n        format=pq_write_format,\n        file_options=write_options,\n    )\n\n    # Open dataset and verify that the data is correct\n    pq_scan_opts_crc = ds.ParquetFragmentScanOptions(\n        page_checksum_verification=True)\n    pq_read_format_crc = pa.dataset.ParquetFileFormat(\n        default_fragment_scan_options=pq_scan_opts_crc)\n    table_check = ds.dataset(\n        original_dir_path,\n        format=pq_read_format_crc\n    ).to_table()\n    assert table_orig == table_check\n\n    # Copy dataset dir (which should be just one file)\n    corrupted_dir_path = tempdir / 'corrupted_dir'\n    copytree(original_dir_path, corrupted_dir_path)\n\n    # Read the only file in the path as binary and swap the 31-th and 36-th\n    # bytes. This should be equivalent to storing the following data:\n    #    pa.table({'a': [1, 3, 2, 4]})\n    corrupted_file_path_list = list(corrupted_dir_path.iterdir())\n    assert len(corrupted_file_path_list) == 1\n    corrupted_file_path = corrupted_file_path_list[0]\n    bin_data = bytearray(corrupted_file_path.read_bytes())\n\n    # Swap two bytes to emulate corruption. Also, check that the two bytes are\n    # different, otherwise no corruption occurs\n    assert bin_data[31] != bin_data[36]\n    bin_data[31], bin_data[36] = bin_data[36], bin_data[31]\n\n    # Write the corrupted data to the parquet file\n    corrupted_file_path.write_bytes(bin_data)\n\n    # Case 1: Reading the corrupted file with dataset().to_table() and without\n    # page checksum verification succeeds but yields corrupted data\n    pq_scan_opts_no_crc = ds.ParquetFragmentScanOptions(\n        page_checksum_verification=False)\n    pq_read_format_no_crc = pa.dataset.ParquetFileFormat(\n        default_fragment_scan_options=pq_scan_opts_no_crc)\n    table_corrupt = ds.dataset(\n        corrupted_dir_path, format=pq_read_format_no_crc).to_table()\n\n    # The read should complete without error, but the table has different\n    # content than the original file!\n    assert table_corrupt != table_orig\n    assert table_corrupt == pa.table({'a': [1, 3, 2, 4]})\n\n    # Case 2: Reading the corrupted file with read_table() and with page\n    # checksum verification enabled raises an exception\n    with pytest.raises(OSError, match=\"CRC checksum verification\"):\n        _ = ds.dataset(\n            corrupted_dir_path,\n            format=pq_read_format_crc\n        ).to_table()\n\n\ndef test_make_write_options_error():\n    # GH-39440: calling make_write_options as a static class method\n    msg_1 = (\"make_write_options() should be called on an \"\n             \"instance of ParquetFileFormat\")\n    # GH-41043: In Cython2 all Cython methods were \"regular\" C extension methods\n    # see: https://github.com/cython/cython/issues/6127#issuecomment-2038153359\n    msg_2 = (\"descriptor 'make_write_options' for \"\n             \"'pyarrow._dataset_parquet.ParquetFileFormat' objects \"\n             \"doesn't apply to a 'int'\")\n    with pytest.raises(TypeError) as excinfo:\n        pa.dataset.ParquetFileFormat.make_write_options(43)\n    assert msg_1 in str(excinfo.value) or msg_2 in str(excinfo.value)\n\n    pformat = pa.dataset.ParquetFileFormat()\n    msg = \"make_write_options\\\\(\\\\) takes exactly 0 positional arguments\"\n    with pytest.raises(TypeError, match=msg):\n        pformat.make_write_options(43)\n", "python/pyarrow/tests/test_ipc.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import UserList\nimport datetime\nimport io\nimport pathlib\nimport pytest\nimport socket\nimport threading\nimport weakref\n\nimport numpy as np\n\nimport pyarrow as pa\nfrom pyarrow.tests.util import changed_environ, invoke_script\n\n\ntry:\n    from pandas.testing import assert_frame_equal\n    import pandas as pd\nexcept ImportError:\n    pass\n\n\nclass IpcFixture:\n    write_stats = None\n\n    def __init__(self, sink_factory=lambda: io.BytesIO()):\n        self._sink_factory = sink_factory\n        self.sink = self.get_sink()\n\n    def get_sink(self):\n        return self._sink_factory()\n\n    def get_source(self):\n        return self.sink.getvalue()\n\n    def write_batches(self, num_batches=5, as_table=False):\n        nrows = 5\n        schema = pa.schema([('one', pa.float64()), ('two', pa.utf8())])\n\n        writer = self._get_writer(self.sink, schema)\n\n        batches = []\n        for i in range(num_batches):\n            batch = pa.record_batch(\n                [np.random.randn(nrows),\n                 ['foo', None, 'bar', 'bazbaz', 'qux']],\n                schema=schema)\n            batches.append(batch)\n\n        if as_table:\n            table = pa.Table.from_batches(batches)\n            writer.write_table(table)\n        else:\n            for batch in batches:\n                writer.write_batch(batch)\n\n        self.write_stats = writer.stats\n        writer.close()\n        return batches\n\n\nclass FileFormatFixture(IpcFixture):\n\n    is_file = True\n    options = None\n\n    def _get_writer(self, sink, schema):\n        return pa.ipc.new_file(sink, schema, options=self.options)\n\n    def _check_roundtrip(self, as_table=False):\n        batches = self.write_batches(as_table=as_table)\n        file_contents = pa.BufferReader(self.get_source())\n\n        reader = pa.ipc.open_file(file_contents)\n\n        assert reader.num_record_batches == len(batches)\n\n        for i, batch in enumerate(batches):\n            # it works. Must convert back to DataFrame\n            batch = reader.get_batch(i)\n            assert batches[i].equals(batch)\n            assert reader.schema.equals(batches[0].schema)\n\n        assert isinstance(reader.stats, pa.ipc.ReadStats)\n        assert isinstance(self.write_stats, pa.ipc.WriteStats)\n        assert tuple(reader.stats) == tuple(self.write_stats)\n\n\nclass StreamFormatFixture(IpcFixture):\n\n    # ARROW-6474, for testing writing old IPC protocol with 4-byte prefix\n    use_legacy_ipc_format = False\n    # ARROW-9395, for testing writing old metadata version\n    options = None\n    is_file = False\n\n    def _get_writer(self, sink, schema):\n        return pa.ipc.new_stream(\n            sink,\n            schema,\n            use_legacy_format=self.use_legacy_ipc_format,\n            options=self.options,\n        )\n\n\nclass MessageFixture(IpcFixture):\n\n    def _get_writer(self, sink, schema):\n        return pa.RecordBatchStreamWriter(sink, schema)\n\n\n@pytest.fixture\ndef ipc_fixture():\n    return IpcFixture()\n\n\n@pytest.fixture\ndef file_fixture():\n    return FileFormatFixture()\n\n\n@pytest.fixture\ndef stream_fixture():\n    return StreamFormatFixture()\n\n\n@pytest.fixture(params=[\n    pytest.param(\n        'file_fixture',\n        id='File Format'\n    ),\n    pytest.param(\n        'stream_fixture',\n        id='Stream Format'\n    )\n])\ndef format_fixture(request):\n    return request.getfixturevalue(request.param)\n\n\ndef test_empty_file():\n    buf = b''\n    with pytest.raises(pa.ArrowInvalid):\n        pa.ipc.open_file(pa.BufferReader(buf))\n\n\ndef test_file_simple_roundtrip(file_fixture):\n    file_fixture._check_roundtrip(as_table=False)\n\n\ndef test_file_write_table(file_fixture):\n    file_fixture._check_roundtrip(as_table=True)\n\n\n@pytest.mark.parametrize(\"sink_factory\", [\n    lambda: io.BytesIO(),\n    lambda: pa.BufferOutputStream()\n])\ndef test_file_read_all(sink_factory):\n    fixture = FileFormatFixture(sink_factory)\n\n    batches = fixture.write_batches()\n    file_contents = pa.BufferReader(fixture.get_source())\n\n    reader = pa.ipc.open_file(file_contents)\n\n    result = reader.read_all()\n    expected = pa.Table.from_batches(batches)\n    assert result.equals(expected)\n\n\ndef test_open_file_from_buffer(file_fixture):\n    # ARROW-2859; APIs accept the buffer protocol\n    file_fixture.write_batches()\n    source = file_fixture.get_source()\n\n    reader1 = pa.ipc.open_file(source)\n    reader2 = pa.ipc.open_file(pa.BufferReader(source))\n    reader3 = pa.RecordBatchFileReader(source)\n\n    result1 = reader1.read_all()\n    result2 = reader2.read_all()\n    result3 = reader3.read_all()\n\n    assert result1.equals(result2)\n    assert result1.equals(result3)\n\n    st1 = reader1.stats\n    assert st1.num_messages == 6\n    assert st1.num_record_batches == 5\n    assert reader2.stats == st1\n    assert reader3.stats == st1\n\n\n@pytest.mark.pandas\ndef test_file_read_pandas(file_fixture):\n    frames = [batch.to_pandas() for batch in file_fixture.write_batches()]\n\n    file_contents = pa.BufferReader(file_fixture.get_source())\n    reader = pa.ipc.open_file(file_contents)\n    result = reader.read_pandas()\n\n    expected = pd.concat(frames).reset_index(drop=True)\n    assert_frame_equal(result, expected)\n\n\ndef test_file_pathlib(file_fixture, tmpdir):\n    file_fixture.write_batches()\n    source = file_fixture.get_source()\n\n    path = tmpdir.join('file.arrow').strpath\n    with open(path, 'wb') as f:\n        f.write(source)\n\n    t1 = pa.ipc.open_file(pathlib.Path(path)).read_all()\n    t2 = pa.ipc.open_file(pa.OSFile(path)).read_all()\n\n    assert t1.equals(t2)\n\n\ndef test_empty_stream():\n    buf = io.BytesIO(b'')\n    with pytest.raises(pa.ArrowInvalid):\n        pa.ipc.open_stream(buf)\n\n\n@pytest.mark.pandas\ndef test_read_year_month_nano_interval(tmpdir):\n    \"\"\"ARROW-15783: Verify to_pandas works for interval types.\n\n    Interval types require static structures to be enabled. This test verifies\n    that they are when no other library functions are invoked.\n    \"\"\"\n    mdn_interval_type = pa.month_day_nano_interval()\n    schema = pa.schema([pa.field('nums', mdn_interval_type)])\n\n    path = tmpdir.join('file.arrow').strpath\n    with pa.OSFile(path, 'wb') as sink:\n        with pa.ipc.new_file(sink, schema) as writer:\n            interval_array = pa.array([(1, 2, 3)], type=mdn_interval_type)\n            batch = pa.record_batch([interval_array], schema)\n            writer.write(batch)\n    invoke_script('read_record_batch.py', path)\n\n\n@pytest.mark.pandas\ndef test_stream_categorical_roundtrip(stream_fixture):\n    df = pd.DataFrame({\n        'one': np.random.randn(5),\n        'two': pd.Categorical(['foo', np.nan, 'bar', 'foo', 'foo'],\n                              categories=['foo', 'bar'],\n                              ordered=True)\n    })\n    batch = pa.RecordBatch.from_pandas(df)\n    with stream_fixture._get_writer(stream_fixture.sink, batch.schema) as wr:\n        wr.write_batch(batch)\n\n    table = (pa.ipc.open_stream(pa.BufferReader(stream_fixture.get_source()))\n             .read_all())\n    assert_frame_equal(table.to_pandas(), df)\n\n\ndef test_open_stream_from_buffer(stream_fixture):\n    # ARROW-2859\n    stream_fixture.write_batches()\n    source = stream_fixture.get_source()\n\n    reader1 = pa.ipc.open_stream(source)\n    reader2 = pa.ipc.open_stream(pa.BufferReader(source))\n    reader3 = pa.RecordBatchStreamReader(source)\n\n    result1 = reader1.read_all()\n    result2 = reader2.read_all()\n    result3 = reader3.read_all()\n\n    assert result1.equals(result2)\n    assert result1.equals(result3)\n\n    st1 = reader1.stats\n    assert st1.num_messages == 6\n    assert st1.num_record_batches == 5\n    assert reader2.stats == st1\n    assert reader3.stats == st1\n\n    assert tuple(st1) == tuple(stream_fixture.write_stats)\n\n\n@pytest.mark.parametrize('options', [\n    pa.ipc.IpcReadOptions(),\n    pa.ipc.IpcReadOptions(use_threads=False),\n])\ndef test_open_stream_options(stream_fixture, options):\n    stream_fixture.write_batches()\n    source = stream_fixture.get_source()\n\n    reader = pa.ipc.open_stream(source, options=options)\n\n    reader.read_all()\n    st = reader.stats\n    assert st.num_messages == 6\n    assert st.num_record_batches == 5\n\n    assert tuple(st) == tuple(stream_fixture.write_stats)\n\n\ndef test_open_stream_with_wrong_options(stream_fixture):\n    stream_fixture.write_batches()\n    source = stream_fixture.get_source()\n\n    with pytest.raises(TypeError):\n        pa.ipc.open_stream(source, options=True)\n\n\n@pytest.mark.parametrize('options', [\n    pa.ipc.IpcReadOptions(),\n    pa.ipc.IpcReadOptions(use_threads=False),\n])\ndef test_open_file_options(file_fixture, options):\n    file_fixture.write_batches()\n    source = file_fixture.get_source()\n\n    reader = pa.ipc.open_file(source, options=options)\n\n    reader.read_all()\n\n    st = reader.stats\n    assert st.num_messages == 6\n    assert st.num_record_batches == 5\n\n\ndef test_open_file_with_wrong_options(file_fixture):\n    file_fixture.write_batches()\n    source = file_fixture.get_source()\n\n    with pytest.raises(TypeError):\n        pa.ipc.open_file(source, options=True)\n\n\n@pytest.mark.pandas\ndef test_stream_write_dispatch(stream_fixture):\n    # ARROW-1616\n    df = pd.DataFrame({\n        'one': np.random.randn(5),\n        'two': pd.Categorical(['foo', np.nan, 'bar', 'foo', 'foo'],\n                              categories=['foo', 'bar'],\n                              ordered=True)\n    })\n    table = pa.Table.from_pandas(df, preserve_index=False)\n    batch = pa.RecordBatch.from_pandas(df, preserve_index=False)\n    with stream_fixture._get_writer(stream_fixture.sink, table.schema) as wr:\n        wr.write(table)\n        wr.write(batch)\n\n    table = (pa.ipc.open_stream(pa.BufferReader(stream_fixture.get_source()))\n             .read_all())\n    assert_frame_equal(table.to_pandas(),\n                       pd.concat([df, df], ignore_index=True))\n\n\n@pytest.mark.pandas\ndef test_stream_write_table_batches(stream_fixture):\n    # ARROW-504\n    df = pd.DataFrame({\n        'one': np.random.randn(20),\n    })\n\n    b1 = pa.RecordBatch.from_pandas(df[:10], preserve_index=False)\n    b2 = pa.RecordBatch.from_pandas(df, preserve_index=False)\n\n    table = pa.Table.from_batches([b1, b2, b1])\n\n    with stream_fixture._get_writer(stream_fixture.sink, table.schema) as wr:\n        wr.write_table(table, max_chunksize=15)\n\n    batches = list(pa.ipc.open_stream(stream_fixture.get_source()))\n\n    assert list(map(len, batches)) == [10, 15, 5, 10]\n    result_table = pa.Table.from_batches(batches)\n    assert_frame_equal(result_table.to_pandas(),\n                       pd.concat([df[:10], df, df[:10]],\n                                 ignore_index=True))\n\n\n@pytest.mark.parametrize('use_legacy_ipc_format', [False, True])\ndef test_stream_simple_roundtrip(stream_fixture, use_legacy_ipc_format):\n    stream_fixture.use_legacy_ipc_format = use_legacy_ipc_format\n    batches = stream_fixture.write_batches()\n    file_contents = pa.BufferReader(stream_fixture.get_source())\n    reader = pa.ipc.open_stream(file_contents)\n\n    assert reader.schema.equals(batches[0].schema)\n\n    total = 0\n    for i, next_batch in enumerate(reader):\n        assert next_batch.equals(batches[i])\n        total += 1\n\n    assert total == len(batches)\n\n    with pytest.raises(StopIteration):\n        reader.read_next_batch()\n\n\n@pytest.mark.zstd\ndef test_compression_roundtrip():\n    sink = io.BytesIO()\n    values = np.random.randint(0, 3, 10000)\n    table = pa.Table.from_arrays([values], names=[\"values\"])\n\n    options = pa.ipc.IpcWriteOptions(compression='zstd')\n    with pa.ipc.RecordBatchFileWriter(\n            sink, table.schema, options=options) as writer:\n        writer.write_table(table)\n    len1 = len(sink.getvalue())\n\n    sink2 = io.BytesIO()\n    codec = pa.Codec('zstd', compression_level=5)\n    options = pa.ipc.IpcWriteOptions(compression=codec)\n    with pa.ipc.RecordBatchFileWriter(\n            sink2, table.schema, options=options) as writer:\n        writer.write_table(table)\n    len2 = len(sink2.getvalue())\n\n    # In theory len2 should be less than len1 but for this test we just want\n    # to ensure compression_level is being correctly passed down to the C++\n    # layer so we don't really care if it makes it worse or better\n    assert len2 != len1\n\n    t1 = pa.ipc.open_file(sink).read_all()\n    t2 = pa.ipc.open_file(sink2).read_all()\n\n    assert t1 == t2\n\n\ndef test_write_options():\n    options = pa.ipc.IpcWriteOptions()\n    assert options.allow_64bit is False\n    assert options.use_legacy_format is False\n    assert options.metadata_version == pa.ipc.MetadataVersion.V5\n\n    options.allow_64bit = True\n    assert options.allow_64bit is True\n\n    options.use_legacy_format = True\n    assert options.use_legacy_format is True\n\n    options.metadata_version = pa.ipc.MetadataVersion.V4\n    assert options.metadata_version == pa.ipc.MetadataVersion.V4\n    for value in ('V5', 42):\n        with pytest.raises((TypeError, ValueError)):\n            options.metadata_version = value\n\n    assert options.compression is None\n    for value in ['lz4', 'zstd']:\n        if pa.Codec.is_available(value):\n            options.compression = value\n            assert options.compression == value\n            options.compression = value.upper()\n            assert options.compression == value\n    options.compression = None\n    assert options.compression is None\n\n    with pytest.raises(TypeError):\n        options.compression = 0\n\n    assert options.use_threads is True\n    options.use_threads = False\n    assert options.use_threads is False\n\n    if pa.Codec.is_available('lz4'):\n        options = pa.ipc.IpcWriteOptions(\n            metadata_version=pa.ipc.MetadataVersion.V4,\n            allow_64bit=True,\n            use_legacy_format=True,\n            compression='lz4',\n            use_threads=False)\n        assert options.metadata_version == pa.ipc.MetadataVersion.V4\n        assert options.allow_64bit is True\n        assert options.use_legacy_format is True\n        assert options.compression == 'lz4'\n        assert options.use_threads is False\n\n\ndef test_write_options_legacy_exclusive(stream_fixture):\n    with pytest.raises(\n            ValueError,\n            match=\"provide at most one of options and use_legacy_format\"):\n        stream_fixture.use_legacy_ipc_format = True\n        stream_fixture.options = pa.ipc.IpcWriteOptions()\n        stream_fixture.write_batches()\n\n\n@pytest.mark.parametrize('options', [\n    pa.ipc.IpcWriteOptions(),\n    pa.ipc.IpcWriteOptions(allow_64bit=True),\n    pa.ipc.IpcWriteOptions(use_legacy_format=True),\n    pa.ipc.IpcWriteOptions(metadata_version=pa.ipc.MetadataVersion.V4),\n    pa.ipc.IpcWriteOptions(use_legacy_format=True,\n                           metadata_version=pa.ipc.MetadataVersion.V4),\n])\ndef test_stream_options_roundtrip(stream_fixture, options):\n    stream_fixture.use_legacy_ipc_format = None\n    stream_fixture.options = options\n    batches = stream_fixture.write_batches()\n    file_contents = pa.BufferReader(stream_fixture.get_source())\n\n    message = pa.ipc.read_message(stream_fixture.get_source())\n    assert message.metadata_version == options.metadata_version\n\n    reader = pa.ipc.open_stream(file_contents)\n\n    assert reader.schema.equals(batches[0].schema)\n\n    total = 0\n    for i, next_batch in enumerate(reader):\n        assert next_batch.equals(batches[i])\n        total += 1\n\n    assert total == len(batches)\n\n    with pytest.raises(StopIteration):\n        reader.read_next_batch()\n\n\ndef test_read_options():\n    options = pa.ipc.IpcReadOptions()\n    assert options.use_threads is True\n    assert options.ensure_native_endian is True\n    assert options.included_fields == []\n\n    options.ensure_native_endian = False\n    assert options.ensure_native_endian is False\n\n    options.use_threads = False\n    assert options.use_threads is False\n\n    options.included_fields = [0, 1]\n    assert options.included_fields == [0, 1]\n\n    with pytest.raises(TypeError):\n        options.included_fields = None\n\n    options = pa.ipc.IpcReadOptions(\n        use_threads=False, ensure_native_endian=False,\n        included_fields=[1]\n    )\n    assert options.use_threads is False\n    assert options.ensure_native_endian is False\n    assert options.included_fields == [1]\n\n\ndef test_read_options_included_fields(stream_fixture):\n    options1 = pa.ipc.IpcReadOptions()\n    options2 = pa.ipc.IpcReadOptions(included_fields=[1])\n    table = pa.Table.from_arrays([pa.array(['foo', 'bar', 'baz', 'qux']),\n                                 pa.array([1, 2, 3, 4])],\n                                 names=['a', 'b'])\n    with stream_fixture._get_writer(stream_fixture.sink, table.schema) as wr:\n        wr.write_table(table)\n    source = stream_fixture.get_source()\n\n    reader1 = pa.ipc.open_stream(source, options=options1)\n    reader2 = pa.ipc.open_stream(\n        source, options=options2, memory_pool=pa.system_memory_pool())\n\n    result1 = reader1.read_all()\n    result2 = reader2.read_all()\n\n    assert result1.num_columns == 2\n    assert result2.num_columns == 1\n\n    expected = pa.Table.from_arrays([pa.array([1, 2, 3, 4])], names=[\"b\"])\n    assert result2 == expected\n    assert result1 == table\n\n\ndef test_dictionary_delta(format_fixture):\n    ty = pa.dictionary(pa.int8(), pa.utf8())\n    data = [[\"foo\", \"foo\", None],\n            [\"foo\", \"bar\", \"foo\"],  # potential delta\n            [\"foo\", \"bar\"],  # nothing new\n            [\"foo\", None, \"bar\", \"quux\"],  # potential delta\n            [\"bar\", \"quux\"],  # replacement\n            ]\n    batches = [\n        pa.RecordBatch.from_arrays([pa.array(v, type=ty)], names=['dicts'])\n        for v in data]\n    batches_delta_only = batches[:4]\n    schema = batches[0].schema\n\n    def write_batches(batches, as_table=False):\n        with format_fixture._get_writer(pa.MockOutputStream(),\n                                        schema) as writer:\n            if as_table:\n                table = pa.Table.from_batches(batches)\n                writer.write_table(table)\n            else:\n                for batch in batches:\n                    writer.write_batch(batch)\n            return writer.stats\n\n    if format_fixture.is_file:\n        # File format cannot handle replacement\n        with pytest.raises(pa.ArrowInvalid):\n            write_batches(batches)\n        # File format cannot handle delta if emit_deltas\n        # is not provided\n        with pytest.raises(pa.ArrowInvalid):\n            write_batches(batches_delta_only)\n    else:\n        st = write_batches(batches)\n        assert st.num_record_batches == 5\n        assert st.num_dictionary_batches == 4\n        assert st.num_replaced_dictionaries == 3\n        assert st.num_dictionary_deltas == 0\n\n    format_fixture.use_legacy_ipc_format = None\n    format_fixture.options = pa.ipc.IpcWriteOptions(\n        emit_dictionary_deltas=True)\n    if format_fixture.is_file:\n        # File format cannot handle replacement\n        with pytest.raises(pa.ArrowInvalid):\n            write_batches(batches)\n    else:\n        st = write_batches(batches)\n        assert st.num_record_batches == 5\n        assert st.num_dictionary_batches == 4\n        assert st.num_replaced_dictionaries == 1\n        assert st.num_dictionary_deltas == 2\n\n    st = write_batches(batches_delta_only)\n    assert st.num_record_batches == 4\n    assert st.num_dictionary_batches == 3\n    assert st.num_replaced_dictionaries == 0\n    assert st.num_dictionary_deltas == 2\n\n    format_fixture.options = pa.ipc.IpcWriteOptions(\n        unify_dictionaries=True\n    )\n    st = write_batches(batches, as_table=True)\n    assert st.num_record_batches == 5\n    if format_fixture.is_file:\n        assert st.num_dictionary_batches == 1\n        assert st.num_replaced_dictionaries == 0\n        assert st.num_dictionary_deltas == 0\n    else:\n        assert st.num_dictionary_batches == 4\n        assert st.num_replaced_dictionaries == 3\n        assert st.num_dictionary_deltas == 0\n\n\ndef test_envvar_set_legacy_ipc_format():\n    schema = pa.schema([pa.field('foo', pa.int32())])\n\n    writer = pa.ipc.new_stream(pa.BufferOutputStream(), schema)\n    assert not writer._use_legacy_format\n    assert writer._metadata_version == pa.ipc.MetadataVersion.V5\n    writer = pa.ipc.new_file(pa.BufferOutputStream(), schema)\n    assert not writer._use_legacy_format\n    assert writer._metadata_version == pa.ipc.MetadataVersion.V5\n\n    with changed_environ('ARROW_PRE_0_15_IPC_FORMAT', '1'):\n        writer = pa.ipc.new_stream(pa.BufferOutputStream(), schema)\n        assert writer._use_legacy_format\n        assert writer._metadata_version == pa.ipc.MetadataVersion.V5\n        writer = pa.ipc.new_file(pa.BufferOutputStream(), schema)\n        assert writer._use_legacy_format\n        assert writer._metadata_version == pa.ipc.MetadataVersion.V5\n\n    with changed_environ('ARROW_PRE_1_0_METADATA_VERSION', '1'):\n        writer = pa.ipc.new_stream(pa.BufferOutputStream(), schema)\n        assert not writer._use_legacy_format\n        assert writer._metadata_version == pa.ipc.MetadataVersion.V4\n        writer = pa.ipc.new_file(pa.BufferOutputStream(), schema)\n        assert not writer._use_legacy_format\n        assert writer._metadata_version == pa.ipc.MetadataVersion.V4\n\n    with changed_environ('ARROW_PRE_1_0_METADATA_VERSION', '1'):\n        with changed_environ('ARROW_PRE_0_15_IPC_FORMAT', '1'):\n            writer = pa.ipc.new_stream(pa.BufferOutputStream(), schema)\n            assert writer._use_legacy_format\n            assert writer._metadata_version == pa.ipc.MetadataVersion.V4\n            writer = pa.ipc.new_file(pa.BufferOutputStream(), schema)\n            assert writer._use_legacy_format\n            assert writer._metadata_version == pa.ipc.MetadataVersion.V4\n\n\ndef test_stream_read_all(stream_fixture):\n    batches = stream_fixture.write_batches()\n    file_contents = pa.BufferReader(stream_fixture.get_source())\n    reader = pa.ipc.open_stream(file_contents)\n\n    result = reader.read_all()\n    expected = pa.Table.from_batches(batches)\n    assert result.equals(expected)\n\n\n@pytest.mark.pandas\ndef test_stream_read_pandas(stream_fixture):\n    frames = [batch.to_pandas() for batch in stream_fixture.write_batches()]\n    file_contents = stream_fixture.get_source()\n    reader = pa.ipc.open_stream(file_contents)\n    result = reader.read_pandas()\n\n    expected = pd.concat(frames).reset_index(drop=True)\n    assert_frame_equal(result, expected)\n\n\n@pytest.fixture\ndef example_messages(stream_fixture):\n    batches = stream_fixture.write_batches()\n    file_contents = stream_fixture.get_source()\n    buf_reader = pa.BufferReader(file_contents)\n    reader = pa.MessageReader.open_stream(buf_reader)\n    return batches, list(reader)\n\n\ndef test_message_ctors_no_segfault():\n    with pytest.raises(TypeError):\n        repr(pa.Message())\n\n    with pytest.raises(TypeError):\n        repr(pa.MessageReader())\n\n\ndef test_message_reader(example_messages):\n    _, messages = example_messages\n\n    assert len(messages) == 6\n    assert messages[0].type == 'schema'\n    assert isinstance(messages[0].metadata, pa.Buffer)\n    assert isinstance(messages[0].body, pa.Buffer)\n    assert messages[0].metadata_version == pa.MetadataVersion.V5\n\n    for msg in messages[1:]:\n        assert msg.type == 'record batch'\n        assert isinstance(msg.metadata, pa.Buffer)\n        assert isinstance(msg.body, pa.Buffer)\n        assert msg.metadata_version == pa.MetadataVersion.V5\n\n\ndef test_message_serialize_read_message(example_messages):\n    _, messages = example_messages\n\n    msg = messages[0]\n    buf = msg.serialize()\n    reader = pa.BufferReader(buf.to_pybytes() * 2)\n\n    restored = pa.ipc.read_message(buf)\n    restored2 = pa.ipc.read_message(reader)\n    restored3 = pa.ipc.read_message(buf.to_pybytes())\n    restored4 = pa.ipc.read_message(reader)\n\n    assert msg.equals(restored)\n    assert msg.equals(restored2)\n    assert msg.equals(restored3)\n    assert msg.equals(restored4)\n\n    with pytest.raises(pa.ArrowInvalid, match=\"Corrupted message\"):\n        pa.ipc.read_message(pa.BufferReader(b'ab'))\n\n    with pytest.raises(EOFError):\n        pa.ipc.read_message(reader)\n\n\n@pytest.mark.gzip\ndef test_message_read_from_compressed(example_messages):\n    # Part of ARROW-5910\n    _, messages = example_messages\n    for message in messages:\n        raw_out = pa.BufferOutputStream()\n        with pa.output_stream(raw_out, compression='gzip') as compressed_out:\n            message.serialize_to(compressed_out)\n\n        compressed_buf = raw_out.getvalue()\n\n        result = pa.ipc.read_message(pa.input_stream(compressed_buf,\n                                                     compression='gzip'))\n        assert result.equals(message)\n\n\ndef test_message_read_schema(example_messages):\n    batches, messages = example_messages\n    schema = pa.ipc.read_schema(messages[0])\n    assert schema.equals(batches[1].schema)\n\n\ndef test_message_read_record_batch(example_messages):\n    batches, messages = example_messages\n\n    for batch, message in zip(batches, messages[1:]):\n        read_batch = pa.ipc.read_record_batch(message, batch.schema)\n        assert read_batch.equals(batch)\n\n\ndef test_read_record_batch_on_stream_error_message():\n    # ARROW-5374\n    batch = pa.record_batch([pa.array([b\"foo\"], type=pa.utf8())],\n                            names=['strs'])\n    stream = pa.BufferOutputStream()\n    with pa.ipc.new_stream(stream, batch.schema) as writer:\n        writer.write_batch(batch)\n    buf = stream.getvalue()\n    with pytest.raises(IOError,\n                       match=\"type record batch but got schema\"):\n        pa.ipc.read_record_batch(buf, batch.schema)\n\n\n# ----------------------------------------------------------------------\n# Socket streaming testa\n\n\nclass StreamReaderServer(threading.Thread):\n\n    def init(self, do_read_all):\n        self._sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self._sock.bind(('127.0.0.1', 0))\n        self._sock.listen(1)\n        host, port = self._sock.getsockname()\n        self._do_read_all = do_read_all\n        self._schema = None\n        self._batches = []\n        self._table = None\n        return port\n\n    def run(self):\n        connection, client_address = self._sock.accept()\n        try:\n            source = connection.makefile(mode='rb')\n            reader = pa.ipc.open_stream(source)\n            self._schema = reader.schema\n            if self._do_read_all:\n                self._table = reader.read_all()\n            else:\n                for i, batch in enumerate(reader):\n                    self._batches.append(batch)\n        finally:\n            connection.close()\n            self._sock.close()\n\n    def get_result(self):\n        return (self._schema, self._table if self._do_read_all\n                else self._batches)\n\n\nclass SocketStreamFixture(IpcFixture):\n\n    def __init__(self):\n        # XXX(wesm): test will decide when to start socket server. This should\n        # probably be refactored\n        pass\n\n    def start_server(self, do_read_all):\n        self._server = StreamReaderServer()\n        port = self._server.init(do_read_all)\n        self._server.start()\n        self._sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self._sock.connect(('127.0.0.1', port))\n        self.sink = self.get_sink()\n\n    def stop_and_get_result(self):\n        import struct\n        self.sink.write(struct.pack('Q', 0))\n        self.sink.flush()\n        self._sock.close()\n        self._server.join()\n        return self._server.get_result()\n\n    def get_sink(self):\n        return self._sock.makefile(mode='wb')\n\n    def _get_writer(self, sink, schema):\n        return pa.RecordBatchStreamWriter(sink, schema)\n\n\n@pytest.fixture\ndef socket_fixture():\n    return SocketStreamFixture()\n\n\ndef test_socket_simple_roundtrip(socket_fixture):\n    socket_fixture.start_server(do_read_all=False)\n    writer_batches = socket_fixture.write_batches()\n    reader_schema, reader_batches = socket_fixture.stop_and_get_result()\n\n    assert reader_schema.equals(writer_batches[0].schema)\n    assert len(reader_batches) == len(writer_batches)\n    for i, batch in enumerate(writer_batches):\n        assert reader_batches[i].equals(batch)\n\n\ndef test_socket_read_all(socket_fixture):\n    socket_fixture.start_server(do_read_all=True)\n    writer_batches = socket_fixture.write_batches()\n    _, result = socket_fixture.stop_and_get_result()\n\n    expected = pa.Table.from_batches(writer_batches)\n    assert result.equals(expected)\n\n\n# ----------------------------------------------------------------------\n# Miscellaneous IPC tests\n\n@pytest.mark.pandas\ndef test_ipc_file_stream_has_eos():\n    # ARROW-5395\n    df = pd.DataFrame({'foo': [1.5]})\n    batch = pa.RecordBatch.from_pandas(df)\n    sink = pa.BufferOutputStream()\n    write_file(batch, sink)\n    buffer = sink.getvalue()\n\n    # skip the file magic\n    reader = pa.ipc.open_stream(buffer[8:])\n\n    # will fail if encounters footer data instead of eos\n    rdf = reader.read_pandas()\n\n    assert_frame_equal(df, rdf)\n\n\n@pytest.mark.pandas\ndef test_ipc_zero_copy_numpy():\n    df = pd.DataFrame({'foo': [1.5]})\n\n    batch = pa.RecordBatch.from_pandas(df)\n    sink = pa.BufferOutputStream()\n    write_file(batch, sink)\n    buffer = sink.getvalue()\n    reader = pa.BufferReader(buffer)\n\n    batches = read_file(reader)\n\n    data = batches[0].to_pandas()\n    rdf = pd.DataFrame(data)\n    assert_frame_equal(df, rdf)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\"ipc_type\", [\"stream\", \"file\"])\ndef test_batches_with_custom_metadata_roundtrip(ipc_type):\n    df = pd.DataFrame({'foo': [1.5]})\n\n    batch = pa.RecordBatch.from_pandas(df)\n    sink = pa.BufferOutputStream()\n\n    batch_count = 2\n    file_factory = {\"stream\": pa.ipc.new_stream,\n                    \"file\": pa.ipc.new_file}[ipc_type]\n\n    with file_factory(sink, batch.schema) as writer:\n        for i in range(batch_count):\n            writer.write_batch(batch, custom_metadata={\"batch_id\": str(i)})\n        # write a batch without custom metadata\n        writer.write_batch(batch)\n\n    buffer = sink.getvalue()\n\n    if ipc_type == \"stream\":\n        with pa.ipc.open_stream(buffer) as reader:\n            batch_with_metas = list(reader.iter_batches_with_custom_metadata())\n    else:\n        with pa.ipc.open_file(buffer) as reader:\n            batch_with_metas = [reader.get_batch_with_custom_metadata(i)\n                                for i in range(reader.num_record_batches)]\n\n    for i in range(batch_count):\n        assert batch_with_metas[i].batch.num_rows == 1\n        assert isinstance(\n            batch_with_metas[i].custom_metadata, pa.KeyValueMetadata)\n        assert batch_with_metas[i].custom_metadata == {\"batch_id\": str(i)}\n\n    # the last batch has no custom metadata\n    assert batch_with_metas[batch_count].batch.num_rows == 1\n    assert batch_with_metas[batch_count].custom_metadata is None\n\n\ndef test_ipc_stream_no_batches():\n    # ARROW-2307\n    table = pa.Table.from_arrays([pa.array([1, 2, 3, 4]),\n                                  pa.array(['foo', 'bar', 'baz', 'qux'])],\n                                 names=['a', 'b'])\n\n    sink = pa.BufferOutputStream()\n    with pa.ipc.new_stream(sink, table.schema):\n        pass\n\n    source = sink.getvalue()\n    with pa.ipc.open_stream(source) as reader:\n        result = reader.read_all()\n\n    assert result.schema.equals(table.schema)\n    assert len(result) == 0\n\n\n@pytest.mark.pandas\ndef test_get_record_batch_size():\n    N = 10\n    itemsize = 8\n    df = pd.DataFrame({'foo': np.random.randn(N)})\n\n    batch = pa.RecordBatch.from_pandas(df)\n    assert pa.ipc.get_record_batch_size(batch) > (N * itemsize)\n\n\n@pytest.mark.pandas\ndef _check_serialize_pandas_round_trip(df, use_threads=False):\n    buf = pa.serialize_pandas(df, nthreads=2 if use_threads else 1)\n    result = pa.deserialize_pandas(buf, use_threads=use_threads)\n    assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\ndef test_pandas_serialize_round_trip():\n    index = pd.Index([1, 2, 3], name='my_index')\n    columns = ['foo', 'bar']\n    df = pd.DataFrame(\n        {'foo': [1.5, 1.6, 1.7], 'bar': list('abc')},\n        index=index, columns=columns\n    )\n    _check_serialize_pandas_round_trip(df)\n\n\n@pytest.mark.pandas\ndef test_pandas_serialize_round_trip_nthreads():\n    index = pd.Index([1, 2, 3], name='my_index')\n    columns = ['foo', 'bar']\n    df = pd.DataFrame(\n        {'foo': [1.5, 1.6, 1.7], 'bar': list('abc')},\n        index=index, columns=columns\n    )\n    _check_serialize_pandas_round_trip(df, use_threads=True)\n\n\n@pytest.mark.pandas\ndef test_pandas_serialize_round_trip_multi_index():\n    index1 = pd.Index([1, 2, 3], name='level_1')\n    index2 = pd.Index(list('def'), name=None)\n    index = pd.MultiIndex.from_arrays([index1, index2])\n\n    columns = ['foo', 'bar']\n    df = pd.DataFrame(\n        {'foo': [1.5, 1.6, 1.7], 'bar': list('abc')},\n        index=index,\n        columns=columns,\n    )\n    _check_serialize_pandas_round_trip(df)\n\n\n@pytest.mark.pandas\ndef test_serialize_pandas_empty_dataframe():\n    df = pd.DataFrame()\n    _check_serialize_pandas_round_trip(df)\n\n\n@pytest.mark.pandas\ndef test_pandas_serialize_round_trip_not_string_columns():\n    df = pd.DataFrame(list(zip([1.5, 1.6, 1.7], 'abc')))\n    buf = pa.serialize_pandas(df)\n    result = pa.deserialize_pandas(buf)\n    assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\ndef test_serialize_pandas_no_preserve_index():\n    df = pd.DataFrame({'a': [1, 2, 3]}, index=[1, 2, 3])\n    expected = pd.DataFrame({'a': [1, 2, 3]})\n\n    buf = pa.serialize_pandas(df, preserve_index=False)\n    result = pa.deserialize_pandas(buf)\n    assert_frame_equal(result, expected)\n\n    buf = pa.serialize_pandas(df, preserve_index=True)\n    result = pa.deserialize_pandas(buf)\n    assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\ndef test_schema_batch_serialize_methods():\n    nrows = 5\n    df = pd.DataFrame({\n        'one': np.random.randn(nrows),\n        'two': ['foo', np.nan, 'bar', 'bazbaz', 'qux']})\n    batch = pa.RecordBatch.from_pandas(df)\n\n    s_schema = batch.schema.serialize()\n    s_batch = batch.serialize()\n\n    recons_schema = pa.ipc.read_schema(s_schema)\n    recons_batch = pa.ipc.read_record_batch(s_batch, recons_schema)\n    assert recons_batch.equals(batch)\n\n\ndef test_schema_serialization_with_metadata():\n    field_metadata = {b'foo': b'bar', b'kind': b'field'}\n    schema_metadata = {b'foo': b'bar', b'kind': b'schema'}\n\n    f0 = pa.field('a', pa.int8())\n    f1 = pa.field('b', pa.string(), metadata=field_metadata)\n\n    schema = pa.schema([f0, f1], metadata=schema_metadata)\n\n    s_schema = schema.serialize()\n    recons_schema = pa.ipc.read_schema(s_schema)\n\n    assert recons_schema.equals(schema)\n    assert recons_schema.metadata == schema_metadata\n    assert recons_schema[0].metadata is None\n    assert recons_schema[1].metadata == field_metadata\n\n\ndef write_file(batch, sink):\n    with pa.ipc.new_file(sink, batch.schema) as writer:\n        writer.write_batch(batch)\n\n\ndef read_file(source):\n    with pa.ipc.open_file(source) as reader:\n        return [reader.get_batch(i) for i in range(reader.num_record_batches)]\n\n\ndef test_write_empty_ipc_file():\n    # ARROW-3894: IPC file was not being properly initialized when no record\n    # batches are being written\n    schema = pa.schema([('field', pa.int64())])\n\n    sink = pa.BufferOutputStream()\n    with pa.ipc.new_file(sink, schema):\n        pass\n\n    buf = sink.getvalue()\n    with pa.RecordBatchFileReader(pa.BufferReader(buf)) as reader:\n        table = reader.read_all()\n    assert len(table) == 0\n    assert table.schema.equals(schema)\n\n\ndef test_py_record_batch_reader():\n    def make_schema():\n        return pa.schema([('field', pa.int64())])\n\n    def make_batches():\n        schema = make_schema()\n        batch1 = pa.record_batch([[1, 2, 3]], schema=schema)\n        batch2 = pa.record_batch([[4, 5]], schema=schema)\n        return [batch1, batch2]\n\n    # With iterable\n    batches = UserList(make_batches())  # weakrefable\n    wr = weakref.ref(batches)\n\n    with pa.RecordBatchReader.from_batches(make_schema(),\n                                           batches) as reader:\n        batches = None\n        assert wr() is not None\n        assert list(reader) == make_batches()\n        assert wr() is None\n\n    # With iterator\n    batches = iter(UserList(make_batches()))  # weakrefable\n    wr = weakref.ref(batches)\n\n    with pa.RecordBatchReader.from_batches(make_schema(),\n                                           batches) as reader:\n        batches = None\n        assert wr() is not None\n        assert list(reader) == make_batches()\n        assert wr() is None\n\n    # ensure we get proper error when not passing a schema\n    # (https://issues.apache.org/jira/browse/ARROW-18229)\n    batches = make_batches()\n    with pytest.raises(TypeError):\n        reader = pa.RecordBatchReader.from_batches(\n            [('field', pa.int64())], batches)\n        pass\n\n    with pytest.raises(TypeError):\n        reader = pa.RecordBatchReader.from_batches(None, batches)\n        pass\n\n\ndef test_record_batch_reader_from_arrow_stream():\n\n    class StreamWrapper:\n        def __init__(self, batches):\n            self.batches = batches\n\n        def __arrow_c_stream__(self, requested_schema=None):\n            reader = pa.RecordBatchReader.from_batches(\n                self.batches[0].schema, self.batches)\n            return reader.__arrow_c_stream__(requested_schema)\n\n    data = [\n        pa.record_batch([pa.array([1, 2, 3], type=pa.int64())], names=['a']),\n        pa.record_batch([pa.array([4, 5, 6], type=pa.int64())], names=['a'])\n    ]\n    wrapper = StreamWrapper(data)\n\n    # Can roundtrip a pyarrow stream-like object\n    expected = pa.Table.from_batches(data)\n    reader = pa.RecordBatchReader.from_stream(expected)\n    assert reader.read_all() == expected\n\n    # Can roundtrip through the wrapper.\n    reader = pa.RecordBatchReader.from_stream(wrapper)\n    assert reader.read_all() == expected\n\n    # Passing schema works if already that schema\n    reader = pa.RecordBatchReader.from_stream(wrapper, schema=data[0].schema)\n    assert reader.read_all() == expected\n\n    # Passing a different but castable schema works\n    good_schema = pa.schema([pa.field(\"a\", pa.int32())])\n    reader = pa.RecordBatchReader.from_stream(wrapper, schema=good_schema)\n    assert reader.read_all() == expected.cast(good_schema)\n\n    # If schema doesn't match, raises TypeError\n    with pytest.raises(pa.lib.ArrowTypeError, match='Field 0 cannot be cast'):\n        pa.RecordBatchReader.from_stream(\n            wrapper, schema=pa.schema([pa.field('a', pa.list_(pa.int32()))])\n        )\n\n    # Proper type errors for wrong input\n    with pytest.raises(TypeError):\n        pa.RecordBatchReader.from_stream(data[0]['a'])\n\n    with pytest.raises(TypeError):\n        pa.RecordBatchReader.from_stream(expected, schema=data[0])\n\n\ndef test_record_batch_reader_cast():\n    schema_src = pa.schema([pa.field('a', pa.int64())])\n    data = [\n        pa.record_batch([pa.array([1, 2, 3], type=pa.int64())], names=['a']),\n        pa.record_batch([pa.array([4, 5, 6], type=pa.int64())], names=['a']),\n    ]\n    table_src = pa.Table.from_batches(data)\n\n    # Cast to same type should always work\n    reader = pa.RecordBatchReader.from_batches(schema_src, data)\n    assert reader.cast(schema_src).read_all() == table_src\n\n    # Check non-trivial cast\n    schema_dst = pa.schema([pa.field('a', pa.int32())])\n    reader = pa.RecordBatchReader.from_batches(schema_src, data)\n    assert reader.cast(schema_dst).read_all() == table_src.cast(schema_dst)\n\n    # Check error for field name/length mismatch\n    reader = pa.RecordBatchReader.from_batches(schema_src, data)\n    with pytest.raises(ValueError, match=\"Target schema's field names\"):\n        reader.cast(pa.schema([]))\n\n    # Check error for impossible cast in call to .cast()\n    reader = pa.RecordBatchReader.from_batches(schema_src, data)\n    with pytest.raises(pa.lib.ArrowTypeError, match='Field 0 cannot be cast'):\n        reader.cast(pa.schema([pa.field('a', pa.list_(pa.int32()))]))\n\n    # Cast to same type should always work (also for types without a T->T cast function)\n    # (https://github.com/apache/arrow/issues/41884)\n    schema_src = pa.schema([pa.field('a', pa.date32())])\n    arr = pa.array([datetime.date(2024, 6, 11)], type=pa.date32())\n    data = [pa.record_batch([arr], names=['a']), pa.record_batch([arr], names=['a'])]\n    table_src = pa.Table.from_batches(data)\n    reader = pa.RecordBatchReader.from_batches(schema_src, data)\n    assert reader.cast(schema_src).read_all() == table_src\n\n\ndef test_record_batch_reader_cast_nulls():\n    schema_src = pa.schema([pa.field('a', pa.int64())])\n    data_with_nulls = [\n        pa.record_batch([pa.array([1, 2, None], type=pa.int64())], names=['a']),\n    ]\n    data_without_nulls = [\n        pa.record_batch([pa.array([1, 2, 3], type=pa.int64())], names=['a']),\n    ]\n    table_with_nulls = pa.Table.from_batches(data_with_nulls)\n    table_without_nulls = pa.Table.from_batches(data_without_nulls)\n\n    # Cast to nullable destination should work\n    reader = pa.RecordBatchReader.from_batches(schema_src, data_with_nulls)\n    schema_dst = pa.schema([pa.field('a', pa.int32())])\n    assert reader.cast(schema_dst).read_all() == table_with_nulls.cast(schema_dst)\n\n    # Cast to non-nullable destination should work if there are no nulls\n    reader = pa.RecordBatchReader.from_batches(schema_src, data_without_nulls)\n    schema_dst = pa.schema([pa.field('a', pa.int32(), nullable=False)])\n    assert reader.cast(schema_dst).read_all() == table_without_nulls.cast(schema_dst)\n\n    # Cast to non-nullable destination should error if there are nulls\n    # when the batch is pulled\n    reader = pa.RecordBatchReader.from_batches(schema_src, data_with_nulls)\n    casted_reader = reader.cast(schema_dst)\n    with pytest.raises(pa.lib.ArrowInvalid, match=\"Can't cast array\"):\n        casted_reader.read_all()\n", "python/pyarrow/tests/test_cuda.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"\nUNTESTED:\nread_message\n\"\"\"\n\nimport sys\nimport sysconfig\n\nimport pytest\n\nimport pyarrow as pa\nimport numpy as np\n\n\ncuda = pytest.importorskip(\"pyarrow.cuda\")\n\nplatform = sysconfig.get_platform()\n# TODO: enable ppc64 when Arrow C++ supports IPC in ppc64 systems:\nhas_ipc_support = platform == 'linux-x86_64'  # or 'ppc64' in platform\n\ncuda_ipc = pytest.mark.skipif(\n    not has_ipc_support,\n    reason='CUDA IPC not supported in platform `%s`' % (platform))\n\nglobal_context = None  # for flake8\nglobal_context1 = None  # for flake8\n\n\ndef setup_module(module):\n    module.global_context = cuda.Context(0)\n    module.global_context1 = cuda.Context(cuda.Context.get_num_devices() - 1)\n\n\ndef teardown_module(module):\n    del module.global_context\n\n\ndef test_Context():\n    assert cuda.Context.get_num_devices() > 0\n    assert global_context.device_number == 0\n    assert global_context1.device_number == cuda.Context.get_num_devices() - 1\n\n    with pytest.raises(ValueError,\n                       match=(\"device_number argument must \"\n                              \"be non-negative less than\")):\n        cuda.Context(cuda.Context.get_num_devices())\n\n\n@pytest.mark.parametrize(\"size\", [0, 1, 1000])\ndef test_manage_allocate_free_host(size):\n    buf = cuda.new_host_buffer(size)\n    arr = np.frombuffer(buf, dtype=np.uint8)\n    arr[size//4:3*size//4] = 1\n    arr_cp = arr.copy()\n    arr2 = np.frombuffer(buf, dtype=np.uint8)\n    np.testing.assert_equal(arr2, arr_cp)\n    assert buf.size == size\n\n\ndef test_context_allocate_del():\n    bytes_allocated = global_context.bytes_allocated\n    cudabuf = global_context.new_buffer(128)\n    assert global_context.bytes_allocated == bytes_allocated + 128\n    del cudabuf\n    assert global_context.bytes_allocated == bytes_allocated\n\n\ndef make_random_buffer(size, target='host'):\n    \"\"\"Return a host or device buffer with random data.\n    \"\"\"\n    if target == 'host':\n        assert size >= 0\n        buf = pa.allocate_buffer(size)\n        assert buf.size == size\n        arr = np.frombuffer(buf, dtype=np.uint8)\n        assert arr.size == size\n        arr[:] = np.random.randint(low=1, high=255, size=size, dtype=np.uint8)\n        assert arr.sum() > 0 or size == 0\n        arr_ = np.frombuffer(buf, dtype=np.uint8)\n        np.testing.assert_equal(arr, arr_)\n        return arr, buf\n    elif target == 'device':\n        arr, buf = make_random_buffer(size, target='host')\n        dbuf = global_context.new_buffer(size)\n        assert dbuf.size == size\n        dbuf.copy_from_host(buf, position=0, nbytes=size)\n        return arr, dbuf\n    raise ValueError('invalid target value')\n\n\n@pytest.mark.parametrize(\"size\", [0, 1, 1000])\ndef test_context_device_buffer(size):\n    # Creating device buffer from host buffer;\n    arr, buf = make_random_buffer(size)\n    cudabuf = global_context.buffer_from_data(buf)\n    assert cudabuf.size == size\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr, arr2)\n\n    # CudaBuffer does not support buffer protocol\n    with pytest.raises(BufferError):\n        memoryview(cudabuf)\n\n    # Creating device buffer from array:\n    cudabuf = global_context.buffer_from_data(arr)\n    assert cudabuf.size == size\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr, arr2)\n\n    # Creating device buffer from bytes:\n    cudabuf = global_context.buffer_from_data(arr.tobytes())\n    assert cudabuf.size == size\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr, arr2)\n\n    # Creating a device buffer from another device buffer, view:\n    cudabuf2 = cudabuf.slice(0, cudabuf.size)\n    assert cudabuf2.size == size\n    arr2 = np.frombuffer(cudabuf2.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr, arr2)\n\n    if size > 1:\n        cudabuf2.copy_from_host(arr[size//2:])\n        arr3 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n        np.testing.assert_equal(np.concatenate((arr[size//2:], arr[size//2:])),\n                                arr3)\n        cudabuf2.copy_from_host(arr[:size//2])  # restoring arr\n\n    # Creating a device buffer from another device buffer, copy:\n    cudabuf2 = global_context.buffer_from_data(cudabuf)\n    assert cudabuf2.size == size\n    arr2 = np.frombuffer(cudabuf2.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr, arr2)\n\n    cudabuf2.copy_from_host(arr[size//2:])\n    arr3 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr, arr3)\n\n    # Slice of a device buffer\n    cudabuf2 = cudabuf.slice(0, cudabuf.size+10)\n    assert cudabuf2.size == size\n    arr2 = np.frombuffer(cudabuf2.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr, arr2)\n\n    cudabuf2 = cudabuf.slice(size//4, size+10)\n    assert cudabuf2.size == size - size//4\n    arr2 = np.frombuffer(cudabuf2.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr[size//4:], arr2)\n\n    # Creating a device buffer from a slice of host buffer\n    soffset = size//4\n    ssize = 2*size//4\n    cudabuf = global_context.buffer_from_data(buf, offset=soffset,\n                                              size=ssize)\n    assert cudabuf.size == ssize\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr[soffset:soffset + ssize], arr2)\n\n    cudabuf = global_context.buffer_from_data(buf.slice(offset=soffset,\n                                                        length=ssize))\n    assert cudabuf.size == ssize\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr[soffset:soffset + ssize], arr2)\n\n    # Creating a device buffer from a slice of an array\n    cudabuf = global_context.buffer_from_data(arr, offset=soffset, size=ssize)\n    assert cudabuf.size == ssize\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr[soffset:soffset + ssize], arr2)\n\n    cudabuf = global_context.buffer_from_data(arr[soffset:soffset+ssize])\n    assert cudabuf.size == ssize\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr[soffset:soffset + ssize], arr2)\n\n    # Creating a device buffer from a slice of bytes\n    cudabuf = global_context.buffer_from_data(arr.tobytes(),\n                                              offset=soffset,\n                                              size=ssize)\n    assert cudabuf.size == ssize\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr[soffset:soffset + ssize], arr2)\n\n    # Creating a device buffer from size\n    cudabuf = global_context.new_buffer(size)\n    assert cudabuf.size == size\n\n    # Creating device buffer from a slice of another device buffer:\n    cudabuf = global_context.buffer_from_data(arr)\n    cudabuf2 = cudabuf.slice(soffset, ssize)\n    assert cudabuf2.size == ssize\n    arr2 = np.frombuffer(cudabuf2.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr[soffset:soffset+ssize], arr2)\n\n    # Creating device buffer from HostBuffer\n\n    buf = cuda.new_host_buffer(size)\n    arr_ = np.frombuffer(buf, dtype=np.uint8)\n    arr_[:] = arr\n    cudabuf = global_context.buffer_from_data(buf)\n    assert cudabuf.size == size\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr, arr2)\n\n    # Creating device buffer from HostBuffer slice\n\n    cudabuf = global_context.buffer_from_data(buf, offset=soffset, size=ssize)\n    assert cudabuf.size == ssize\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr[soffset:soffset+ssize], arr2)\n\n    cudabuf = global_context.buffer_from_data(\n        buf.slice(offset=soffset, length=ssize))\n    assert cudabuf.size == ssize\n    arr2 = np.frombuffer(cudabuf.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr[soffset:soffset+ssize], arr2)\n\n\n@pytest.mark.parametrize(\"size\", [0, 1, 1000])\ndef test_context_from_object(size):\n    ctx = global_context\n    arr, cbuf = make_random_buffer(size, target='device')\n    dtype = arr.dtype\n\n    # Creating device buffer from a CUDA host buffer\n    hbuf = cuda.new_host_buffer(size * arr.dtype.itemsize)\n    np.frombuffer(hbuf, dtype=dtype)[:] = arr\n    cbuf2 = ctx.buffer_from_object(hbuf)\n    assert cbuf2.size == cbuf.size\n    arr2 = np.frombuffer(cbuf2.copy_to_host(), dtype=dtype)\n    np.testing.assert_equal(arr, arr2)\n\n    # Creating device buffer from a device buffer\n    cbuf2 = ctx.buffer_from_object(cbuf2)\n    assert cbuf2.size == cbuf.size\n    arr2 = np.frombuffer(cbuf2.copy_to_host(), dtype=dtype)\n    np.testing.assert_equal(arr, arr2)\n\n    # Trying to create a device buffer from a Buffer\n    with pytest.raises(pa.ArrowTypeError,\n                       match=('buffer is not backed by a CudaBuffer')):\n        ctx.buffer_from_object(pa.py_buffer(b\"123\"))\n\n    # Trying to create a device buffer from numpy.array\n    with pytest.raises(pa.ArrowTypeError,\n                       match=(\"cannot create device buffer view from \"\n                              \".* \\'numpy.ndarray\\'\")):\n        ctx.buffer_from_object(np.array([1, 2, 3]))\n\n\ndef test_foreign_buffer():\n    ctx = global_context\n    dtype = np.dtype(np.uint8)\n    size = 10\n    hbuf = cuda.new_host_buffer(size * dtype.itemsize)\n\n    # test host buffer memory reference counting\n    rc = sys.getrefcount(hbuf)\n    fbuf = ctx.foreign_buffer(hbuf.address, hbuf.size, hbuf)\n    assert sys.getrefcount(hbuf) == rc + 1\n    del fbuf\n    assert sys.getrefcount(hbuf) == rc\n\n    # test postponed deallocation of host buffer memory\n    fbuf = ctx.foreign_buffer(hbuf.address, hbuf.size, hbuf)\n    del hbuf\n    fbuf.copy_to_host()\n\n    # test deallocating the host buffer memory making it inaccessible\n    hbuf = cuda.new_host_buffer(size * dtype.itemsize)\n    fbuf = ctx.foreign_buffer(hbuf.address, hbuf.size)\n    del hbuf\n    with pytest.raises(pa.ArrowIOError,\n                       match=('Cuda error ')):\n        fbuf.copy_to_host()\n\n\n@pytest.mark.parametrize(\"size\", [0, 1, 1000])\ndef test_CudaBuffer(size):\n    arr, buf = make_random_buffer(size)\n    assert arr.tobytes() == buf.to_pybytes()\n    cbuf = global_context.buffer_from_data(buf)\n    assert cbuf.size == size\n    assert not cbuf.is_cpu\n    assert arr.tobytes() == cbuf.to_pybytes()\n    if size > 0:\n        assert cbuf.address > 0\n\n    for i in range(size):\n        assert cbuf[i] == arr[i]\n\n    for s in [\n            slice(None),\n            slice(size//4, size//2),\n    ]:\n        assert cbuf[s].to_pybytes() == arr[s].tobytes()\n\n    sbuf = cbuf.slice(size//4, size//2)\n    assert sbuf.parent == cbuf\n\n    with pytest.raises(TypeError,\n                       match=\"Do not call CudaBuffer's constructor directly\"):\n        cuda.CudaBuffer()\n\n\n@pytest.mark.parametrize(\"size\", [0, 1, 1000])\ndef test_HostBuffer(size):\n    arr, buf = make_random_buffer(size)\n    assert arr.tobytes() == buf.to_pybytes()\n    hbuf = cuda.new_host_buffer(size)\n    np.frombuffer(hbuf, dtype=np.uint8)[:] = arr\n    assert hbuf.size == size\n    assert hbuf.is_cpu\n    assert arr.tobytes() == hbuf.to_pybytes()\n    for i in range(size):\n        assert hbuf[i] == arr[i]\n    for s in [\n            slice(None),\n            slice(size//4, size//2),\n    ]:\n        assert hbuf[s].to_pybytes() == arr[s].tobytes()\n\n    sbuf = hbuf.slice(size//4, size//2)\n    assert sbuf.parent == hbuf\n\n    del hbuf\n\n    with pytest.raises(TypeError,\n                       match=\"Do not call HostBuffer's constructor directly\"):\n        cuda.HostBuffer()\n\n\n@pytest.mark.parametrize(\"size\", [0, 1, 1000])\ndef test_copy_from_to_host(size):\n    # Create a buffer in host containing range(size)\n    dt = np.dtype('uint16')\n    nbytes = size * dt.itemsize\n    buf = pa.allocate_buffer(nbytes, resizable=True)  # in host\n    assert isinstance(buf, pa.Buffer)\n    assert not isinstance(buf, cuda.CudaBuffer)\n    arr = np.frombuffer(buf, dtype=dt)\n    assert arr.size == size\n    arr[:] = range(size)\n    arr_ = np.frombuffer(buf, dtype=dt)\n    np.testing.assert_equal(arr, arr_)\n\n    # Create a device buffer of the same size and copy from host\n    device_buffer = global_context.new_buffer(nbytes)\n    assert isinstance(device_buffer, cuda.CudaBuffer)\n    assert isinstance(device_buffer, pa.Buffer)\n    assert device_buffer.size == nbytes\n    assert not device_buffer.is_cpu\n    device_buffer.copy_from_host(buf, position=0, nbytes=nbytes)\n\n    # Copy back to host and compare contents\n    buf2 = device_buffer.copy_to_host(position=0, nbytes=nbytes)\n    arr2 = np.frombuffer(buf2, dtype=dt)\n    np.testing.assert_equal(arr, arr2)\n\n\n@pytest.mark.parametrize(\"size\", [0, 1, 1000])\ndef test_copy_to_host(size):\n    arr, dbuf = make_random_buffer(size, target='device')\n\n    buf = dbuf.copy_to_host()\n    assert buf.is_cpu\n    np.testing.assert_equal(arr, np.frombuffer(buf, dtype=np.uint8))\n\n    buf = dbuf.copy_to_host(position=size//4)\n    assert buf.is_cpu\n    np.testing.assert_equal(arr[size//4:], np.frombuffer(buf, dtype=np.uint8))\n\n    buf = dbuf.copy_to_host(position=size//4, nbytes=size//8)\n    assert buf.is_cpu\n    np.testing.assert_equal(arr[size//4:size//4+size//8],\n                            np.frombuffer(buf, dtype=np.uint8))\n\n    buf = dbuf.copy_to_host(position=size//4, nbytes=0)\n    assert buf.is_cpu\n    assert buf.size == 0\n\n    for (position, nbytes) in [\n        (size+2, -1), (-2, -1), (size+1, 0), (-3, 0),\n    ]:\n        with pytest.raises(ValueError,\n                           match='position argument is out-of-range'):\n            dbuf.copy_to_host(position=position, nbytes=nbytes)\n\n    for (position, nbytes) in [\n        (0, size+1), (size//2, (size+1)//2+1), (size, 1)\n    ]:\n        with pytest.raises(ValueError,\n                           match=('requested more to copy than'\n                                  ' available from device buffer')):\n            dbuf.copy_to_host(position=position, nbytes=nbytes)\n\n    buf = pa.allocate_buffer(size//4)\n    dbuf.copy_to_host(buf=buf)\n    np.testing.assert_equal(arr[:size//4], np.frombuffer(buf, dtype=np.uint8))\n\n    if size < 12:\n        return\n\n    dbuf.copy_to_host(buf=buf, position=12)\n    np.testing.assert_equal(arr[12:12+size//4],\n                            np.frombuffer(buf, dtype=np.uint8))\n\n    dbuf.copy_to_host(buf=buf, nbytes=12)\n    np.testing.assert_equal(arr[:12], np.frombuffer(buf, dtype=np.uint8)[:12])\n\n    dbuf.copy_to_host(buf=buf, nbytes=12, position=6)\n    np.testing.assert_equal(arr[6:6+12],\n                            np.frombuffer(buf, dtype=np.uint8)[:12])\n\n    for (position, nbytes) in [\n            (0, size+10), (10, size-5),\n            (0, size//2), (size//4, size//4+1)\n    ]:\n        with pytest.raises(ValueError,\n                           match=('requested copy does not '\n                                  'fit into host buffer')):\n            dbuf.copy_to_host(buf=buf, position=position, nbytes=nbytes)\n\n\n@pytest.mark.parametrize(\"dest_ctx\", ['same', 'another'])\n@pytest.mark.parametrize(\"size\", [0, 1, 1000])\ndef test_copy_from_device(dest_ctx, size):\n    arr, buf = make_random_buffer(size=size, target='device')\n    lst = arr.tolist()\n    if dest_ctx == 'another':\n        dest_ctx = global_context1\n        if buf.context.device_number == dest_ctx.device_number:\n            pytest.skip(\"not a multi-GPU system\")\n    else:\n        dest_ctx = buf.context\n    dbuf = dest_ctx.new_buffer(size)\n\n    def put(*args, **kwargs):\n        dbuf.copy_from_device(buf, *args, **kwargs)\n        rbuf = dbuf.copy_to_host()\n        return np.frombuffer(rbuf, dtype=np.uint8).tolist()\n    assert put() == lst\n    if size > 4:\n        assert put(position=size//4) == lst[:size//4]+lst[:-size//4]\n        assert put() == lst\n        assert put(position=1, nbytes=size//2) == \\\n            lst[:1] + lst[:size//2] + lst[-(size-size//2-1):]\n\n    for (position, nbytes) in [\n            (size+2, -1), (-2, -1), (size+1, 0), (-3, 0),\n    ]:\n        with pytest.raises(ValueError,\n                           match='position argument is out-of-range'):\n            put(position=position, nbytes=nbytes)\n\n    for (position, nbytes) in [\n        (0, size+1),\n    ]:\n        with pytest.raises(ValueError,\n                           match=('requested more to copy than'\n                                  ' available from device buffer')):\n            put(position=position, nbytes=nbytes)\n\n    if size < 4:\n        return\n\n    for (position, nbytes) in [\n        (size//2, (size+1)//2+1)\n    ]:\n        with pytest.raises(ValueError,\n                           match=('requested more to copy than'\n                                  ' available in device buffer')):\n            put(position=position, nbytes=nbytes)\n\n\n@pytest.mark.parametrize(\"size\", [0, 1, 1000])\ndef test_copy_from_host(size):\n    arr, buf = make_random_buffer(size=size, target='host')\n    lst = arr.tolist()\n    dbuf = global_context.new_buffer(size)\n\n    def put(*args, **kwargs):\n        dbuf.copy_from_host(buf, *args, **kwargs)\n        rbuf = dbuf.copy_to_host()\n        return np.frombuffer(rbuf, dtype=np.uint8).tolist()\n    assert put() == lst\n    if size > 4:\n        assert put(position=size//4) == lst[:size//4]+lst[:-size//4]\n        assert put() == lst\n        assert put(position=1, nbytes=size//2) == \\\n            lst[:1] + lst[:size//2] + lst[-(size-size//2-1):]\n\n    for (position, nbytes) in [\n            (size+2, -1), (-2, -1), (size+1, 0), (-3, 0),\n    ]:\n        with pytest.raises(ValueError,\n                           match='position argument is out-of-range'):\n            put(position=position, nbytes=nbytes)\n\n    for (position, nbytes) in [\n        (0, size+1),\n    ]:\n        with pytest.raises(ValueError,\n                           match=('requested more to copy than'\n                                  ' available from host buffer')):\n            put(position=position, nbytes=nbytes)\n\n    if size < 4:\n        return\n\n    for (position, nbytes) in [\n        (size//2, (size+1)//2+1)\n    ]:\n        with pytest.raises(ValueError,\n                           match=('requested more to copy than'\n                                  ' available in device buffer')):\n            put(position=position, nbytes=nbytes)\n\n\ndef test_BufferWriter():\n    def allocate(size):\n        cbuf = global_context.new_buffer(size)\n        writer = cuda.BufferWriter(cbuf)\n        return cbuf, writer\n\n    def test_writes(total_size, chunksize, buffer_size=0):\n        cbuf, writer = allocate(total_size)\n        arr, buf = make_random_buffer(size=total_size, target='host')\n\n        if buffer_size > 0:\n            writer.buffer_size = buffer_size\n\n        position = writer.tell()\n        assert position == 0\n        writer.write(buf.slice(length=chunksize))\n        assert writer.tell() == chunksize\n        writer.seek(0)\n        position = writer.tell()\n        assert position == 0\n\n        while position < total_size:\n            bytes_to_write = min(chunksize, total_size - position)\n            writer.write(buf.slice(offset=position, length=bytes_to_write))\n            position += bytes_to_write\n\n        writer.flush()\n        assert cbuf.size == total_size\n        cbuf.context.synchronize()\n        buf2 = cbuf.copy_to_host()\n        cbuf.context.synchronize()\n        assert buf2.size == total_size\n        arr2 = np.frombuffer(buf2, dtype=np.uint8)\n        np.testing.assert_equal(arr, arr2)\n\n    total_size, chunk_size = 1 << 16, 1000\n    test_writes(total_size, chunk_size)\n    test_writes(total_size, chunk_size, total_size // 16)\n\n    cbuf, writer = allocate(100)\n    writer.write(np.arange(100, dtype=np.uint8))\n    writer.writeat(50, np.arange(25, dtype=np.uint8))\n    writer.write(np.arange(25, dtype=np.uint8))\n    writer.flush()\n\n    arr = np.frombuffer(cbuf.copy_to_host(), np.uint8)\n    np.testing.assert_equal(arr[:50], np.arange(50, dtype=np.uint8))\n    np.testing.assert_equal(arr[50:75], np.arange(25, dtype=np.uint8))\n    np.testing.assert_equal(arr[75:], np.arange(25, dtype=np.uint8))\n\n\ndef test_BufferWriter_edge_cases():\n    # edge cases, see cuda-test.cc for more information:\n    size = 1000\n    cbuf = global_context.new_buffer(size)\n    writer = cuda.BufferWriter(cbuf)\n    arr, buf = make_random_buffer(size=size, target='host')\n\n    assert writer.buffer_size == 0\n    writer.buffer_size = 100\n    assert writer.buffer_size == 100\n\n    writer.write(buf.slice(length=0))\n    assert writer.tell() == 0\n\n    writer.write(buf.slice(length=10))\n    writer.buffer_size = 200\n    assert writer.buffer_size == 200\n    assert writer.num_bytes_buffered == 0\n\n    writer.write(buf.slice(offset=10, length=300))\n    assert writer.num_bytes_buffered == 0\n\n    writer.write(buf.slice(offset=310, length=200))\n    assert writer.num_bytes_buffered == 0\n\n    writer.write(buf.slice(offset=510, length=390))\n    writer.write(buf.slice(offset=900, length=100))\n\n    writer.flush()\n\n    buf2 = cbuf.copy_to_host()\n    assert buf2.size == size\n    arr2 = np.frombuffer(buf2, dtype=np.uint8)\n    np.testing.assert_equal(arr, arr2)\n\n\ndef test_BufferReader():\n    size = 1000\n    arr, cbuf = make_random_buffer(size=size, target='device')\n\n    reader = cuda.BufferReader(cbuf)\n    reader.seek(950)\n    assert reader.tell() == 950\n\n    data = reader.read(100)\n    assert len(data) == 50\n    assert reader.tell() == 1000\n\n    reader.seek(925)\n    arr2 = np.zeros(100, dtype=np.uint8)\n    n = reader.readinto(arr2)\n    assert n == 75\n    assert reader.tell() == 1000\n    np.testing.assert_equal(arr[925:], arr2[:75])\n\n    reader.seek(0)\n    assert reader.tell() == 0\n    buf2 = reader.read_buffer()\n    arr2 = np.frombuffer(buf2.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr, arr2)\n\n\ndef test_BufferReader_zero_size():\n    arr, cbuf = make_random_buffer(size=0, target='device')\n    reader = cuda.BufferReader(cbuf)\n    reader.seek(0)\n    data = reader.read()\n    assert len(data) == 0\n    assert reader.tell() == 0\n    buf2 = reader.read_buffer()\n    arr2 = np.frombuffer(buf2.copy_to_host(), dtype=np.uint8)\n    np.testing.assert_equal(arr, arr2)\n\n\ndef make_recordbatch(length):\n    schema = pa.schema([pa.field('f0', pa.int16()),\n                        pa.field('f1', pa.int16())])\n    a0 = pa.array(np.random.randint(0, 255, size=length, dtype=np.int16))\n    a1 = pa.array(np.random.randint(0, 255, size=length, dtype=np.int16))\n    batch = pa.record_batch([a0, a1], schema=schema)\n    return batch\n\n\ndef test_batch_serialize():\n    batch = make_recordbatch(10)\n    hbuf = batch.serialize()\n    cbuf = cuda.serialize_record_batch(batch, global_context)\n\n    # Test that read_record_batch works properly\n    cbatch = cuda.read_record_batch(cbuf, batch.schema)\n    assert isinstance(cbatch, pa.RecordBatch)\n    assert batch.schema == cbatch.schema\n    assert batch.num_columns == cbatch.num_columns\n    assert batch.num_rows == cbatch.num_rows\n\n    # Deserialize CUDA-serialized batch on host\n    buf = cbuf.copy_to_host()\n    assert hbuf.equals(buf)\n    batch2 = pa.ipc.read_record_batch(buf, batch.schema)\n    assert hbuf.equals(batch2.serialize())\n\n    assert batch.num_columns == batch2.num_columns\n    assert batch.num_rows == batch2.num_rows\n    assert batch.column(0).equals(batch2.column(0))\n    assert batch.equals(batch2)\n\n\ndef make_table():\n    a0 = pa.array([0, 1, 42, None], type=pa.int16())\n    a1 = pa.array([[0, 1], [2], [], None], type=pa.list_(pa.int32()))\n    a2 = pa.array([(\"ab\", True), (\"cde\", False), (None, None), None],\n                  type=pa.struct([(\"strs\", pa.utf8()),\n                                  (\"bools\", pa.bool_())]))\n    # Dictionaries are validated on the IPC read path, but that can produce\n    # issues for GPU-located dictionaries.  Check that they work fine.\n    a3 = pa.DictionaryArray.from_arrays(\n        indices=[0, 1, 1, None],\n        dictionary=pa.array(['foo', 'bar']))\n    a4 = pa.DictionaryArray.from_arrays(\n        indices=[2, 1, 2, None],\n        dictionary=a1)\n    a5 = pa.DictionaryArray.from_arrays(\n        indices=[2, 1, 0, None],\n        dictionary=a2)\n\n    arrays = [a0, a1, a2, a3, a4, a5]\n    schema = pa.schema([('f{}'.format(i), arr.type)\n                        for i, arr in enumerate(arrays)])\n    batch = pa.record_batch(arrays, schema=schema)\n    table = pa.Table.from_batches([batch])\n    return table\n\n\ndef make_table_cuda():\n    htable = make_table()\n    # Serialize the host table to bytes\n    sink = pa.BufferOutputStream()\n    with pa.ipc.new_stream(sink, htable.schema) as out:\n        out.write_table(htable)\n    hbuf = pa.py_buffer(sink.getvalue().to_pybytes())\n\n    # Copy the host bytes to a device buffer\n    dbuf = global_context.new_buffer(len(hbuf))\n    dbuf.copy_from_host(hbuf, nbytes=len(hbuf))\n    # Deserialize the device buffer into a Table\n    dtable = pa.ipc.open_stream(cuda.BufferReader(dbuf)).read_all()\n    return hbuf, htable, dbuf, dtable\n\n\ndef test_table_deserialize():\n    # ARROW-9659: make sure that we can deserialize a GPU-located table\n    # without crashing when initializing or validating the underlying arrays.\n    hbuf, htable, dbuf, dtable = make_table_cuda()\n    # Assert basic fields the same between host and device tables\n    assert htable.schema == dtable.schema\n    assert htable.num_rows == dtable.num_rows\n    assert htable.num_columns == dtable.num_columns\n    # Assert byte-level equality\n    assert hbuf.equals(dbuf.copy_to_host())\n    # Copy DtoH and assert the tables are still equivalent\n    assert htable.equals(pa.ipc.open_stream(\n        dbuf.copy_to_host()\n    ).read_all())\n\n\ndef test_create_table_with_device_buffers():\n    # ARROW-11872: make sure that we can create an Arrow Table from\n    # GPU-located Arrays without crashing.\n    hbuf, htable, dbuf, dtable = make_table_cuda()\n    # Construct a new Table from the device Table\n    dtable2 = pa.Table.from_arrays(dtable.columns, dtable.column_names)\n    # Assert basic fields the same between host and device tables\n    assert htable.schema == dtable2.schema\n    assert htable.num_rows == dtable2.num_rows\n    assert htable.num_columns == dtable2.num_columns\n    # Assert byte-level equality\n    assert hbuf.equals(dbuf.copy_to_host())\n    # Copy DtoH and assert the tables are still equivalent\n    assert htable.equals(pa.ipc.open_stream(\n        dbuf.copy_to_host()\n    ).read_all())\n\n\ndef other_process_for_test_IPC(handle_buffer, expected_arr):\n    other_context = pa.cuda.Context(0)\n    ipc_handle = pa.cuda.IpcMemHandle.from_buffer(handle_buffer)\n    ipc_buf = other_context.open_ipc_buffer(ipc_handle)\n    ipc_buf.context.synchronize()\n    buf = ipc_buf.copy_to_host()\n    assert buf.size == expected_arr.size, repr((buf.size, expected_arr.size))\n    arr = np.frombuffer(buf, dtype=expected_arr.dtype)\n    np.testing.assert_equal(arr, expected_arr)\n\n\n@cuda_ipc\n@pytest.mark.parametrize(\"size\", [0, 1, 1000])\ndef test_IPC(size):\n    import multiprocessing\n    ctx = multiprocessing.get_context('spawn')\n    arr, cbuf = make_random_buffer(size=size, target='device')\n    ipc_handle = cbuf.export_for_ipc()\n    handle_buffer = ipc_handle.serialize()\n    p = ctx.Process(target=other_process_for_test_IPC,\n                    args=(handle_buffer, arr))\n    p.start()\n    p.join()\n    assert p.exitcode == 0\n\n\ndef _arr_copy_to_host(carr):\n    # TODO replace below with copy to device when exposed in python\n    buffers = []\n    for cbuf in carr.buffers():\n        if cbuf is None:\n            buffers.append(None)\n        else:\n            buf = global_context.foreign_buffer(\n                cbuf.address, cbuf.size, cbuf\n            ).copy_to_host()\n            buffers.append(buf)\n\n    child = pa.Array.from_buffers(carr.type.value_type, 3, buffers[2:])\n    new = pa.Array.from_buffers(carr.type, 2, buffers[:2], children=[child])\n    return new\n\n\ndef test_device_interface_array():\n    cffi = pytest.importorskip(\"pyarrow.cffi\")\n    ffi = cffi.ffi\n\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n    c_array = ffi.new(\"struct ArrowDeviceArray*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n\n    typ = pa.list_(pa.int32())\n    arr = pa.array([[1], [2, 42]], type=typ)\n\n    # TODO replace below with copy to device when exposed in python\n    cbuffers = []\n    for buf in arr.buffers():\n        if buf is None:\n            cbuffers.append(None)\n        else:\n            cbuf = global_context.new_buffer(buf.size)\n            cbuf.copy_from_host(buf, position=0, nbytes=buf.size)\n            cbuffers.append(cbuf)\n\n    carr = pa.Array.from_buffers(typ, 2, cbuffers[:2], children=[\n        pa.Array.from_buffers(typ.value_type, 3, cbuffers[2:])\n    ])\n\n    # Type is known up front\n    carr._export_to_c_device(ptr_array)\n\n    # verify exported struct\n    assert c_array.device_type == 2  # ARROW_DEVICE_CUDA 2\n    assert c_array.device_id == global_context.device_number\n    assert c_array.array.length == 2\n\n    # Delete recreate C++ object from exported pointer\n    del carr\n    carr_new = pa.Array._import_from_c_device(ptr_array, typ)\n    assert carr_new.type == pa.list_(pa.int32())\n    arr_new = _arr_copy_to_host(carr_new)\n    assert arr_new.equals(arr)\n\n    del carr_new\n    # Now released\n    with pytest.raises(ValueError, match=\"Cannot import released ArrowArray\"):\n        pa.Array._import_from_c_device(ptr_array, typ)\n\n    # Schema is exported and imported at the same time\n    carr = pa.Array.from_buffers(typ, 2, cbuffers[:2], children=[\n        pa.Array.from_buffers(typ.value_type, 3, cbuffers[2:])\n    ])\n    carr._export_to_c_device(ptr_array, ptr_schema)\n    # Delete and recreate C++ objects from exported pointers\n    del carr\n    carr_new = pa.Array._import_from_c_device(ptr_array, ptr_schema)\n    assert carr_new.type == pa.list_(pa.int32())\n    arr_new = _arr_copy_to_host(carr_new)\n    assert arr_new.equals(arr)\n\n    del carr_new\n    # Now released\n    with pytest.raises(ValueError, match=\"Cannot import released ArrowSchema\"):\n        pa.Array._import_from_c_device(ptr_array, ptr_schema)\n\n\ndef _batch_copy_to_host(cbatch):\n    # TODO replace below with copy to device when exposed in python\n    arrs = []\n    for col in cbatch.columns:\n        buffers = [\n            global_context.foreign_buffer(buf.address, buf.size, buf).copy_to_host()\n            if buf is not None else None\n            for buf in col.buffers()\n        ]\n        new = pa.Array.from_buffers(col.type, len(col), buffers)\n        arrs.append(new)\n\n    return pa.RecordBatch.from_arrays(arrs, schema=cbatch.schema)\n\n\ndef test_device_interface_batch_array():\n    cffi = pytest.importorskip(\"pyarrow.cffi\")\n    ffi = cffi.ffi\n\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n    c_array = ffi.new(\"struct ArrowDeviceArray*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n\n    batch = make_recordbatch(10)\n    schema = batch.schema\n    cbuf = cuda.serialize_record_batch(batch, global_context)\n    cbatch = cuda.read_record_batch(cbuf, schema)\n\n    # Schema is known up front\n    cbatch._export_to_c_device(ptr_array)\n\n    # verify exported struct\n    assert c_array.device_type == 2  # ARROW_DEVICE_CUDA 2\n    assert c_array.device_id == global_context.device_number\n    assert c_array.array.length == 10\n\n    # Delete recreate C++ object from exported pointer\n    del cbatch\n    cbatch_new = pa.RecordBatch._import_from_c_device(ptr_array, schema)\n    assert cbatch_new.schema == schema\n    batch_new = _batch_copy_to_host(cbatch_new)\n    assert batch_new.equals(batch)\n\n    del cbatch_new\n    # Now released\n    with pytest.raises(ValueError, match=\"Cannot import released ArrowArray\"):\n        pa.RecordBatch._import_from_c_device(ptr_array, schema)\n\n    # Schema is exported and imported at the same time\n    cbatch = cuda.read_record_batch(cbuf, schema)\n    cbatch._export_to_c_device(ptr_array, ptr_schema)\n    # Delete and recreate C++ objects from exported pointers\n    del cbatch\n    cbatch_new = pa.RecordBatch._import_from_c_device(ptr_array, ptr_schema)\n    assert cbatch_new.schema == schema\n    batch_new = _batch_copy_to_host(cbatch_new)\n    assert batch_new.equals(batch)\n\n    del cbatch_new\n    # Now released\n    with pytest.raises(ValueError, match=\"Cannot import released ArrowSchema\"):\n        pa.RecordBatch._import_from_c_device(ptr_array, ptr_schema)\n\n    # Not a struct type\n    pa.int32()._export_to_c(ptr_schema)\n    with pytest.raises(ValueError,\n                       match=\"ArrowSchema describes non-struct type\"):\n        pa.RecordBatch._import_from_c_device(ptr_array, ptr_schema)\n", "python/pyarrow/tests/test_device.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pyarrow as pa\n\n\ndef test_device_memory_manager():\n    mm = pa.default_cpu_memory_manager()\n    assert mm.is_cpu\n    device = mm.device\n    assert device.is_cpu\n    assert device.device_id == -1\n    assert device.device_type == pa.DeviceAllocationType.CPU\n    assert device.type_name == \"arrow::CPUDevice\"\n    assert device == device\n    assert repr(device) == \"<pyarrow.Device: CPUDevice()>\"\n    assert repr(mm) == \"<pyarrow.MemoryManager device: CPUDevice()>\"\n\n\ndef test_buffer_device():\n    arr = pa.array([0, 1, 2])\n    buf = arr.buffers()[1]\n    assert buf.device_type == pa.DeviceAllocationType.CPU\n    assert isinstance(buf.device, pa.Device)\n    assert isinstance(buf.memory_manager, pa.MemoryManager)\n    assert buf.is_cpu\n    assert buf.device.is_cpu\n    assert buf.device == pa.default_cpu_memory_manager().device\n    assert buf.memory_manager.is_cpu\n", "python/pyarrow/tests/test_strategies.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport hypothesis as h\n\nimport pyarrow as pa\nimport pyarrow.tests.strategies as past\n\n\n@h.given(past.all_types)\ndef test_types(ty):\n    assert isinstance(ty, pa.lib.DataType)\n\n\n@h.given(past.all_fields)\ndef test_fields(field):\n    assert isinstance(field, pa.lib.Field)\n\n\n@h.given(past.all_schemas)\ndef test_schemas(schema):\n    assert isinstance(schema, pa.lib.Schema)\n\n\n@h.given(past.all_arrays)\ndef test_arrays(array):\n    assert isinstance(array, pa.lib.Array)\n\n\n@h.given(past.arrays(past.primitive_types, nullable=False))\ndef test_array_nullability(array):\n    assert array.null_count == 0\n\n\n@h.given(past.all_chunked_arrays)\ndef test_chunked_arrays(chunked_array):\n    assert isinstance(chunked_array, pa.lib.ChunkedArray)\n\n\n@h.given(past.all_record_batches)\ndef test_record_batches(record_bath):\n    assert isinstance(record_bath, pa.lib.RecordBatch)\n\n\n@h.given(past.all_tables)\ndef test_tables(table):\n    assert isinstance(table, pa.lib.Table)\n", "python/pyarrow/tests/test_jvm.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport json\nimport os\nimport pyarrow as pa\nimport pyarrow.jvm as pa_jvm\nimport pytest\nimport sys\nimport xml.etree.ElementTree as ET\n\n\njpype = pytest.importorskip(\"jpype\")\n\n\n@pytest.fixture(scope=\"session\")\ndef root_allocator():\n    # This test requires Arrow Java to be built in the same source tree\n    try:\n        arrow_dir = os.environ[\"ARROW_SOURCE_DIR\"]\n    except KeyError:\n        arrow_dir = os.path.join(os.path.dirname(__file__), '..', '..', '..')\n    pom_path = os.path.join(arrow_dir, 'java', 'pom.xml')\n    tree = ET.parse(pom_path)\n    version = tree.getroot().find(\n        'POM:version',\n        namespaces={\n            'POM': 'http://maven.apache.org/POM/4.0.0'\n        }).text\n    jar_path = os.path.join(\n        arrow_dir, 'java', 'tools', 'target',\n        'arrow-tools-{}-jar-with-dependencies.jar'.format(version))\n    jar_path = os.getenv(\"ARROW_TOOLS_JAR\", jar_path)\n    kwargs = {}\n    # This will be the default behaviour in jpype 0.8+\n    kwargs['convertStrings'] = False\n    jpype.startJVM(jpype.getDefaultJVMPath(), \"-Djava.class.path=\" + jar_path,\n                   **kwargs)\n    return jpype.JPackage(\"org\").apache.arrow.memory.RootAllocator(sys.maxsize)\n\n\ndef test_jvm_buffer(root_allocator):\n    # Create a Java buffer\n    jvm_buffer = root_allocator.buffer(8)\n    for i in range(8):\n        jvm_buffer.setByte(i, 8 - i)\n\n    orig_refcnt = jvm_buffer.refCnt()\n\n    # Convert to Python\n    buf = pa_jvm.jvm_buffer(jvm_buffer)\n\n    # Check its content\n    assert buf.to_pybytes() == b'\\x08\\x07\\x06\\x05\\x04\\x03\\x02\\x01'\n\n    # Check Java buffer lifetime is tied to PyArrow buffer lifetime\n    assert jvm_buffer.refCnt() == orig_refcnt + 1\n    del buf\n    assert jvm_buffer.refCnt() == orig_refcnt\n\n\ndef test_jvm_buffer_released(root_allocator):\n    import jpype.imports  # noqa\n    from java.lang import IllegalArgumentException\n\n    jvm_buffer = root_allocator.buffer(8)\n    jvm_buffer.release()\n\n    with pytest.raises(IllegalArgumentException):\n        pa_jvm.jvm_buffer(jvm_buffer)\n\n\ndef _jvm_field(jvm_spec):\n    om = jpype.JClass('com.fasterxml.jackson.databind.ObjectMapper')()\n    pojo_Field = jpype.JClass('org.apache.arrow.vector.types.pojo.Field')\n    return om.readValue(jvm_spec, pojo_Field)\n\n\ndef _jvm_schema(jvm_spec, metadata=None):\n    field = _jvm_field(jvm_spec)\n    schema_cls = jpype.JClass('org.apache.arrow.vector.types.pojo.Schema')\n    fields = jpype.JClass('java.util.ArrayList')()\n    fields.add(field)\n    if metadata:\n        dct = jpype.JClass('java.util.HashMap')()\n        for k, v in metadata.items():\n            dct.put(k, v)\n        return schema_cls(fields, dct)\n    else:\n        return schema_cls(fields)\n\n\n# In the following, we use the JSON serialization of the Field objects in Java.\n# This ensures that we neither rely on the exact mechanics on how to construct\n# them using Java code as well as enables us to define them as parameters\n# without to invoke the JVM.\n#\n# The specifications were created using:\n#\n#   om = jpype.JClass('com.fasterxml.jackson.databind.ObjectMapper')()\n#   field = \u2026  # Code to instantiate the field\n#   jvm_spec = om.writeValueAsString(field)\n@pytest.mark.parametrize('pa_type,jvm_spec', [\n    (pa.null(), '{\"name\":\"null\"}'),\n    (pa.bool_(), '{\"name\":\"bool\"}'),\n    (pa.int8(), '{\"name\":\"int\",\"bitWidth\":8,\"isSigned\":true}'),\n    (pa.int16(), '{\"name\":\"int\",\"bitWidth\":16,\"isSigned\":true}'),\n    (pa.int32(), '{\"name\":\"int\",\"bitWidth\":32,\"isSigned\":true}'),\n    (pa.int64(), '{\"name\":\"int\",\"bitWidth\":64,\"isSigned\":true}'),\n    (pa.uint8(), '{\"name\":\"int\",\"bitWidth\":8,\"isSigned\":false}'),\n    (pa.uint16(), '{\"name\":\"int\",\"bitWidth\":16,\"isSigned\":false}'),\n    (pa.uint32(), '{\"name\":\"int\",\"bitWidth\":32,\"isSigned\":false}'),\n    (pa.uint64(), '{\"name\":\"int\",\"bitWidth\":64,\"isSigned\":false}'),\n    (pa.float16(), '{\"name\":\"floatingpoint\",\"precision\":\"HALF\"}'),\n    (pa.float32(), '{\"name\":\"floatingpoint\",\"precision\":\"SINGLE\"}'),\n    (pa.float64(), '{\"name\":\"floatingpoint\",\"precision\":\"DOUBLE\"}'),\n    (pa.time32('s'), '{\"name\":\"time\",\"unit\":\"SECOND\",\"bitWidth\":32}'),\n    (pa.time32('ms'), '{\"name\":\"time\",\"unit\":\"MILLISECOND\",\"bitWidth\":32}'),\n    (pa.time64('us'), '{\"name\":\"time\",\"unit\":\"MICROSECOND\",\"bitWidth\":64}'),\n    (pa.time64('ns'), '{\"name\":\"time\",\"unit\":\"NANOSECOND\",\"bitWidth\":64}'),\n    (pa.timestamp('s'), '{\"name\":\"timestamp\",\"unit\":\"SECOND\",'\n        '\"timezone\":null}'),\n    (pa.timestamp('ms'), '{\"name\":\"timestamp\",\"unit\":\"MILLISECOND\",'\n        '\"timezone\":null}'),\n    (pa.timestamp('us'), '{\"name\":\"timestamp\",\"unit\":\"MICROSECOND\",'\n        '\"timezone\":null}'),\n    (pa.timestamp('ns'), '{\"name\":\"timestamp\",\"unit\":\"NANOSECOND\",'\n        '\"timezone\":null}'),\n    (pa.timestamp('ns', tz='UTC'), '{\"name\":\"timestamp\",\"unit\":\"NANOSECOND\"'\n        ',\"timezone\":\"UTC\"}'),\n    (pa.timestamp('ns', tz='Europe/Paris'), '{\"name\":\"timestamp\",'\n        '\"unit\":\"NANOSECOND\",\"timezone\":\"Europe/Paris\"}'),\n    (pa.date32(), '{\"name\":\"date\",\"unit\":\"DAY\"}'),\n    (pa.date64(), '{\"name\":\"date\",\"unit\":\"MILLISECOND\"}'),\n    (pa.decimal128(19, 4), '{\"name\":\"decimal\",\"precision\":19,\"scale\":4}'),\n    (pa.string(), '{\"name\":\"utf8\"}'),\n    (pa.binary(), '{\"name\":\"binary\"}'),\n    (pa.binary(10), '{\"name\":\"fixedsizebinary\",\"byteWidth\":10}'),\n    # TODO(ARROW-2609): complex types that have children\n    # pa.list_(pa.int32()),\n    # pa.struct([pa.field('a', pa.int32()),\n    #            pa.field('b', pa.int8()),\n    #            pa.field('c', pa.string())]),\n    # pa.union([pa.field('a', pa.binary(10)),\n    #           pa.field('b', pa.string())], mode=pa.lib.UnionMode_DENSE),\n    # pa.union([pa.field('a', pa.binary(10)),\n    #           pa.field('b', pa.string())], mode=pa.lib.UnionMode_SPARSE),\n    # TODO: DictionaryType requires a vector in the type\n    # pa.dictionary(pa.int32(), pa.array(['a', 'b', 'c'])),\n])\n@pytest.mark.parametrize('nullable', [True, False])\ndef test_jvm_types(root_allocator, pa_type, jvm_spec, nullable):\n    if pa_type == pa.null() and not nullable:\n        return\n    spec = {\n        'name': 'field_name',\n        'nullable': nullable,\n        'type': json.loads(jvm_spec),\n        # TODO: This needs to be set for complex types\n        'children': []\n    }\n    jvm_field = _jvm_field(json.dumps(spec))\n    result = pa_jvm.field(jvm_field)\n    expected_field = pa.field('field_name', pa_type, nullable=nullable)\n    assert result == expected_field\n\n    jvm_schema = _jvm_schema(json.dumps(spec))\n    result = pa_jvm.schema(jvm_schema)\n    assert result == pa.schema([expected_field])\n\n    # Schema with custom metadata\n    jvm_schema = _jvm_schema(json.dumps(spec), {'meta': 'data'})\n    result = pa_jvm.schema(jvm_schema)\n    assert result == pa.schema([expected_field], {'meta': 'data'})\n\n    # Schema with custom field metadata\n    spec['metadata'] = [{'key': 'field meta', 'value': 'field data'}]\n    jvm_schema = _jvm_schema(json.dumps(spec))\n    result = pa_jvm.schema(jvm_schema)\n    expected_field = expected_field.with_metadata(\n        {'field meta': 'field data'})\n    assert result == pa.schema([expected_field])\n\n\n# These test parameters mostly use an integer range as an input as this is\n# often the only type that is understood by both Python and Java\n# implementations of Arrow.\n@pytest.mark.parametrize('pa_type,py_data,jvm_type', [\n    (pa.bool_(), [True, False, True, True], 'BitVector'),\n    (pa.uint8(), list(range(128)), 'UInt1Vector'),\n    (pa.uint16(), list(range(128)), 'UInt2Vector'),\n    (pa.int32(), list(range(128)), 'IntVector'),\n    (pa.int64(), list(range(128)), 'BigIntVector'),\n    (pa.float32(), list(range(128)), 'Float4Vector'),\n    (pa.float64(), list(range(128)), 'Float8Vector'),\n    (pa.timestamp('s'), list(range(128)), 'TimeStampSecVector'),\n    (pa.timestamp('ms'), list(range(128)), 'TimeStampMilliVector'),\n    (pa.timestamp('us'), list(range(128)), 'TimeStampMicroVector'),\n    (pa.timestamp('ns'), list(range(128)), 'TimeStampNanoVector'),\n    # TODO(ARROW-2605): These types miss a conversion from pure Python objects\n    #  * pa.time32('s')\n    #  * pa.time32('ms')\n    #  * pa.time64('us')\n    #  * pa.time64('ns')\n    (pa.date32(), list(range(128)), 'DateDayVector'),\n    (pa.date64(), list(range(128)), 'DateMilliVector'),\n    # TODO(ARROW-2606): pa.decimal128(19, 4)\n])\ndef test_jvm_array(root_allocator, pa_type, py_data, jvm_type):\n    # Create vector\n    cls = \"org.apache.arrow.vector.{}\".format(jvm_type)\n    jvm_vector = jpype.JClass(cls)(\"vector\", root_allocator)\n    jvm_vector.allocateNew(len(py_data))\n    for i, val in enumerate(py_data):\n        # char and int are ambiguous overloads for these two setSafe calls\n        if jvm_type in {'UInt1Vector', 'UInt2Vector'}:\n            val = jpype.JInt(val)\n        jvm_vector.setSafe(i, val)\n    jvm_vector.setValueCount(len(py_data))\n\n    py_array = pa.array(py_data, type=pa_type)\n    jvm_array = pa_jvm.array(jvm_vector)\n\n    assert py_array.equals(jvm_array)\n\n\ndef test_jvm_array_empty(root_allocator):\n    cls = \"org.apache.arrow.vector.{}\".format('IntVector')\n    jvm_vector = jpype.JClass(cls)(\"vector\", root_allocator)\n    jvm_vector.allocateNew()\n    jvm_array = pa_jvm.array(jvm_vector)\n    assert len(jvm_array) == 0\n    assert jvm_array.type == pa.int32()\n\n\n# These test parameters mostly use an integer range as an input as this is\n# often the only type that is understood by both Python and Java\n# implementations of Arrow.\n@pytest.mark.parametrize('pa_type,py_data,jvm_type,jvm_spec', [\n    # TODO: null\n    (pa.bool_(), [True, False, True, True], 'BitVector', '{\"name\":\"bool\"}'),\n    (\n        pa.uint8(),\n        list(range(128)),\n        'UInt1Vector',\n        '{\"name\":\"int\",\"bitWidth\":8,\"isSigned\":false}'\n    ),\n    (\n        pa.uint16(),\n        list(range(128)),\n        'UInt2Vector',\n        '{\"name\":\"int\",\"bitWidth\":16,\"isSigned\":false}'\n    ),\n    (\n        pa.uint32(),\n        list(range(128)),\n        'UInt4Vector',\n        '{\"name\":\"int\",\"bitWidth\":32,\"isSigned\":false}'\n    ),\n    (\n        pa.uint64(),\n        list(range(128)),\n        'UInt8Vector',\n        '{\"name\":\"int\",\"bitWidth\":64,\"isSigned\":false}'\n    ),\n    (\n        pa.int8(),\n        list(range(128)),\n        'TinyIntVector',\n        '{\"name\":\"int\",\"bitWidth\":8,\"isSigned\":true}'\n    ),\n    (\n        pa.int16(),\n        list(range(128)),\n        'SmallIntVector',\n        '{\"name\":\"int\",\"bitWidth\":16,\"isSigned\":true}'\n    ),\n    (\n        pa.int32(),\n        list(range(128)),\n        'IntVector',\n        '{\"name\":\"int\",\"bitWidth\":32,\"isSigned\":true}'\n    ),\n    (\n        pa.int64(),\n        list(range(128)),\n        'BigIntVector',\n        '{\"name\":\"int\",\"bitWidth\":64,\"isSigned\":true}'\n    ),\n    # TODO: float16\n    (\n        pa.float32(),\n        list(range(128)),\n        'Float4Vector',\n        '{\"name\":\"floatingpoint\",\"precision\":\"SINGLE\"}'\n    ),\n    (\n        pa.float64(),\n        list(range(128)),\n        'Float8Vector',\n        '{\"name\":\"floatingpoint\",\"precision\":\"DOUBLE\"}'\n    ),\n    (\n        pa.timestamp('s'),\n        list(range(128)),\n        'TimeStampSecVector',\n        '{\"name\":\"timestamp\",\"unit\":\"SECOND\",\"timezone\":null}'\n    ),\n    (\n        pa.timestamp('ms'),\n        list(range(128)),\n        'TimeStampMilliVector',\n        '{\"name\":\"timestamp\",\"unit\":\"MILLISECOND\",\"timezone\":null}'\n    ),\n    (\n        pa.timestamp('us'),\n        list(range(128)),\n        'TimeStampMicroVector',\n        '{\"name\":\"timestamp\",\"unit\":\"MICROSECOND\",\"timezone\":null}'\n    ),\n    (\n        pa.timestamp('ns'),\n        list(range(128)),\n        'TimeStampNanoVector',\n        '{\"name\":\"timestamp\",\"unit\":\"NANOSECOND\",\"timezone\":null}'\n    ),\n    # TODO(ARROW-2605): These types miss a conversion from pure Python objects\n    #  * pa.time32('s')\n    #  * pa.time32('ms')\n    #  * pa.time64('us')\n    #  * pa.time64('ns')\n    (\n        pa.date32(),\n        list(range(128)),\n        'DateDayVector',\n        '{\"name\":\"date\",\"unit\":\"DAY\"}'\n    ),\n    (\n        pa.date64(),\n        list(range(128)),\n        'DateMilliVector',\n        '{\"name\":\"date\",\"unit\":\"MILLISECOND\"}'\n    ),\n    # TODO(ARROW-2606): pa.decimal128(19, 4)\n])\ndef test_jvm_record_batch(root_allocator, pa_type, py_data, jvm_type,\n                          jvm_spec):\n    # Create vector\n    cls = \"org.apache.arrow.vector.{}\".format(jvm_type)\n    jvm_vector = jpype.JClass(cls)(\"vector\", root_allocator)\n    jvm_vector.allocateNew(len(py_data))\n    for i, val in enumerate(py_data):\n        if jvm_type in {'UInt1Vector', 'UInt2Vector'}:\n            val = jpype.JInt(val)\n        jvm_vector.setSafe(i, val)\n    jvm_vector.setValueCount(len(py_data))\n\n    # Create field\n    spec = {\n        'name': 'field_name',\n        'nullable': False,\n        'type': json.loads(jvm_spec),\n        # TODO: This needs to be set for complex types\n        'children': []\n    }\n    jvm_field = _jvm_field(json.dumps(spec))\n\n    # Create VectorSchemaRoot\n    jvm_fields = jpype.JClass('java.util.ArrayList')()\n    jvm_fields.add(jvm_field)\n    jvm_vectors = jpype.JClass('java.util.ArrayList')()\n    jvm_vectors.add(jvm_vector)\n    jvm_vsr = jpype.JClass('org.apache.arrow.vector.VectorSchemaRoot')\n    jvm_vsr = jvm_vsr(jvm_fields, jvm_vectors, len(py_data))\n\n    py_record_batch = pa.RecordBatch.from_arrays(\n        [pa.array(py_data, type=pa_type)],\n        ['col']\n    )\n    jvm_record_batch = pa_jvm.record_batch(jvm_vsr)\n\n    assert py_record_batch.equals(jvm_record_batch)\n\n\ndef _string_to_varchar_holder(ra, string):\n    nvch_cls = \"org.apache.arrow.vector.holders.NullableVarCharHolder\"\n    holder = jpype.JClass(nvch_cls)()\n    if string is None:\n        holder.isSet = 0\n    else:\n        holder.isSet = 1\n        value = jpype.JClass(\"java.lang.String\")(\"string\")\n        std_charsets = jpype.JClass(\"java.nio.charset.StandardCharsets\")\n        bytes_ = value.getBytes(std_charsets.UTF_8)\n        holder.buffer = ra.buffer(len(bytes_))\n        holder.buffer.setBytes(0, bytes_, 0, len(bytes_))\n        holder.start = 0\n        holder.end = len(bytes_)\n    return holder\n\n\n# TODO(ARROW-2607)\n@pytest.mark.xfail(reason=\"from_buffers is only supported for \"\n                          \"primitive arrays yet\")\ndef test_jvm_string_array(root_allocator):\n    data = [\"string\", None, \"t\u00f6st\"]\n    cls = \"org.apache.arrow.vector.VarCharVector\"\n    jvm_vector = jpype.JClass(cls)(\"vector\", root_allocator)\n    jvm_vector.allocateNew()\n\n    for i, string in enumerate(data):\n        holder = _string_to_varchar_holder(root_allocator, \"string\")\n        jvm_vector.setSafe(i, holder)\n        jvm_vector.setValueCount(i + 1)\n\n    py_array = pa.array(data, type=pa.string())\n    jvm_array = pa_jvm.array(jvm_vector)\n\n    assert py_array.equals(jvm_array)\n", "python/pyarrow/tests/test_exec_plan.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\nimport pyarrow as pa\nimport pyarrow.compute as pc\nfrom .test_extension_type import IntegerType\n\ntry:\n    import pyarrow.dataset as ds\nexcept ImportError:\n    pass\n\ntry:\n    from pyarrow.acero import _perform_join, _filter_table\nexcept ImportError:\n    pass\n\npytestmark = pytest.mark.acero\n\n\ndef test_joins_corner_cases():\n    t1 = pa.Table.from_pydict({\n        \"colA\": [1, 2, 3, 4, 5, 6],\n        \"col2\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n    })\n\n    t2 = pa.Table.from_pydict({\n        \"colB\": [1, 2, 3, 4, 5],\n        \"col3\": [\"A\", \"B\", \"C\", \"D\", \"E\"]\n    })\n\n    with pytest.raises(pa.ArrowInvalid):\n        _perform_join(\"left outer\", t1, \"\", t2, \"\")\n\n    with pytest.raises(TypeError):\n        _perform_join(\"left outer\", None, \"colA\", t2, \"colB\")\n\n    with pytest.raises(ValueError):\n        _perform_join(\"super mario join\", t1, \"colA\", t2, \"colB\")\n\n\n@pytest.mark.parametrize(\"jointype,expected\", [\n    (\"left semi\", {\n        \"colA\": [1, 2],\n        \"col2\": [\"a\", \"b\"]\n    }),\n    (\"right semi\", {\n        \"colB\": [1, 2],\n        \"col3\": [\"A\", \"B\"]\n    }),\n    (\"left anti\", {\n        \"colA\": [6],\n        \"col2\": [\"f\"]\n    }),\n    (\"right anti\", {\n        \"colB\": [99],\n        \"col3\": [\"Z\"]\n    }),\n    (\"inner\", {\n        \"colA\": [1, 2],\n        \"col2\": [\"a\", \"b\"],\n        \"colB\": [1, 2],\n        \"col3\": [\"A\", \"B\"]\n    }),\n    (\"left outer\", {\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"],\n        \"colB\": [1, 2, None],\n        \"col3\": [\"A\", \"B\", None]\n    }),\n    (\"right outer\", {\n        \"colA\": [1, 2, None],\n        \"col2\": [\"a\", \"b\", None],\n        \"colB\": [1, 2, 99],\n        \"col3\": [\"A\", \"B\", \"Z\"]\n    }),\n    (\"full outer\", {\n        \"colA\": [1, 2, 6, None],\n        \"col2\": [\"a\", \"b\", \"f\", None],\n        \"colB\": [1, 2, None, 99],\n        \"col3\": [\"A\", \"B\", None, \"Z\"]\n    })\n])\n@pytest.mark.parametrize(\"use_threads\", [True, False])\n@pytest.mark.parametrize(\"coalesce_keys\", [True, False])\n@pytest.mark.parametrize(\"use_datasets\",\n                         [False, pytest.param(True, marks=pytest.mark.dataset)])\ndef test_joins(jointype, expected, use_threads, coalesce_keys, use_datasets):\n    # Allocate table here instead of using parametrize\n    # this prevents having arrow allocated memory forever around.\n    expected = pa.table(expected)\n\n    t1 = pa.Table.from_pydict({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"]\n    })\n\n    t2 = pa.Table.from_pydict({\n        \"colB\": [99, 2, 1],\n        \"col3\": [\"Z\", \"B\", \"A\"]\n    })\n\n    if use_datasets:\n        t1 = ds.dataset([t1])\n        t2 = ds.dataset([t2])\n\n    r = _perform_join(jointype, t1, \"colA\", t2, \"colB\",\n                      use_threads=use_threads, coalesce_keys=coalesce_keys)\n    r = r.combine_chunks()\n    if \"right\" in jointype:\n        r = r.sort_by(\"colB\")\n    else:\n        r = r.sort_by(\"colA\")\n    if coalesce_keys:\n        if jointype in (\"inner\", \"left outer\"):\n            expected = expected.drop([\"colB\"])\n        elif jointype == \"right outer\":\n            expected = expected.drop([\"colA\"])\n        elif jointype == \"full outer\":\n            expected = expected.drop([\"colB\"]).set_column(0, \"colA\", [[1, 2, 6, 99]])\n    assert r == expected\n\n\ndef test_table_join_collisions():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"colVals\": [\"a\", \"b\", \"f\"]\n    })\n\n    t2 = pa.table({\n        \"colB\": [99, 20, 10],\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"colUniq\": [100, 200, 300],\n        \"colA\": [99, 2, 1],\n    })\n\n    result = _perform_join(\n        \"full outer\", t1, [\"colA\", \"colB\"], t2, [\"colA\", \"colB\"])\n    result = result.combine_chunks()\n    result = result.sort_by(\"colUniq\")\n    assert result == pa.table([\n        [None, 2, 1, 6],\n        [None, 20, 10, 60],\n        [None, \"b\", \"a\", \"f\"],\n        [99, 20, 10, None],\n        [\"Z\", \"B\", \"A\", None],\n        [100, 200, 300, None],\n        [99, 2, 1, None],\n    ], names=[\"colA\", \"colB\", \"colVals\", \"colB\", \"colVals\", \"colUniq\", \"colA\"])\n\n    result = _perform_join(\"full outer\", t1, \"colA\",\n                           t2, \"colA\", right_suffix=\"_r\",\n                           coalesce_keys=False)\n    result = result.combine_chunks()\n    result = result.sort_by(\"colA\")\n    assert result == pa.table({\n        \"colA\": [1, 2, 6, None],\n        \"colB\": [10, 20, 60, None],\n        \"colVals\": [\"a\", \"b\", \"f\", None],\n        \"colB_r\": [10, 20, None, 99],\n        \"colVals_r\": [\"A\", \"B\", None, \"Z\"],\n        \"colUniq\": [300, 200, None, 100],\n        \"colA_r\": [1, 2, None, 99],\n    })\n\n    result = _perform_join(\"full outer\", t1, \"colA\",\n                           t2, \"colA\", right_suffix=\"_r\",\n                           coalesce_keys=True)\n    result = result.combine_chunks()\n    result = result.sort_by(\"colA\")\n    assert result == pa.table({\n        \"colA\": [1, 2, 6, 99],\n        \"colB\": [10, 20, 60, None],\n        \"colVals\": [\"a\", \"b\", \"f\", None],\n        \"colB_r\": [10, 20, None, 99],\n        \"colVals_r\": [\"A\", \"B\", None, \"Z\"],\n        \"colUniq\": [300, 200, None, 100]\n    })\n\n\ndef test_table_join_keys_order():\n    t1 = pa.table({\n        \"colB\": [10, 20, 60],\n        \"colA\": [1, 2, 6],\n        \"colVals\": [\"a\", \"b\", \"f\"]\n    })\n\n    t2 = pa.table({\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"colX\": [99, 2, 1],\n    })\n\n    result = _perform_join(\"full outer\", t1, \"colA\", t2, \"colX\",\n                           left_suffix=\"_l\", right_suffix=\"_r\",\n                           coalesce_keys=True)\n    result = result.combine_chunks()\n    result = result.sort_by(\"colA\")\n    assert result == pa.table({\n        \"colB\": [10, 20, 60, None],\n        \"colA\": [1, 2, 6, 99],\n        \"colVals_l\": [\"a\", \"b\", \"f\", None],\n        \"colVals_r\": [\"A\", \"B\", None, \"Z\"],\n    })\n\n\ndef test_filter_table_errors():\n    t = pa.table({\n        \"a\": [1, 2, 3, 4, 5],\n        \"b\": [10, 20, 30, 40, 50]\n    })\n\n    with pytest.raises(pa.ArrowTypeError):\n        _filter_table(t, pc.divide(pc.field(\"a\"), pc.scalar(2)))\n\n    with pytest.raises(pa.ArrowInvalid):\n        _filter_table(t, (pc.field(\"Z\") <= pc.scalar(2)))\n\n\ndef test_filter_table():\n    t = pa.table({\n        \"a\": [1, 2, 3, 4, 5],\n        \"b\": [10, 20, 30, 40, 50]\n    })\n\n    result = _filter_table(\n        t, (pc.field(\"a\") <= pc.scalar(3)) & (pc.field(\"b\") == pc.scalar(20)),\n    )\n    assert result == pa.table({\n        \"a\": [2],\n        \"b\": [20]\n    })\n\n    result = _filter_table(t, pc.field(\"b\") > pc.scalar(30))\n    assert result == pa.table({\n        \"a\": [4, 5],\n        \"b\": [40, 50]\n    })\n\n\ndef test_filter_table_ordering():\n    table1 = pa.table({'a': [1, 2, 3, 4], 'b': ['a'] * 4})\n    table2 = pa.table({'a': [1, 2, 3, 4], 'b': ['b'] * 4})\n    table = pa.concat_tables([table1, table2])\n\n    for _ in range(20):\n        # 20 seems to consistently cause errors when order is not preserved.\n        # If the order problem is reintroduced this test will become flaky\n        # which is still a signal that the order is not preserved.\n        r = _filter_table(table, pc.field('a') == 1)\n        assert r[\"b\"] == pa.chunked_array([[\"a\"], [\"b\"]])\n\n\ndef test_complex_filter_table():\n    t = pa.table({\n        \"a\": [1, 2, 3, 4, 5, 6, 6],\n        \"b\": [10, 20, 30, 40, 50, 60, 61]\n    })\n\n    result = _filter_table(\n        t, ((pc.bit_wise_and(pc.field(\"a\"), pc.scalar(1)) == pc.scalar(0)) &\n            (pc.multiply(pc.field(\"a\"), pc.scalar(10)) == pc.field(\"b\")))\n    )\n\n    assert result == pa.table({\n        \"a\": [2, 4, 6],  # second six must be omitted because 6*10 != 61\n        \"b\": [20, 40, 60]\n    })\n\n\ndef test_join_extension_array_column():\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    ty = IntegerType()\n    ext_array = pa.ExtensionArray.from_storage(ty, storage)\n    dict_array = pa.DictionaryArray.from_arrays(\n        pa.array([0, 2, 1]), pa.array(['a', 'b', 'c']))\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": ext_array,\n        \"colVals\": ext_array,\n    })\n\n    t2 = pa.table({\n        \"colA\": [99, 2, 1],\n        \"colC\": ext_array,\n    })\n\n    t3 = pa.table({\n        \"colA\": [99, 2, 1],\n        \"colC\": ext_array,\n        \"colD\": dict_array,\n    })\n\n    result = _perform_join(\n        \"left outer\", t1, [\"colA\"], t2, [\"colA\"])\n    assert result[\"colVals\"] == pa.chunked_array(ext_array)\n\n    result = _perform_join(\n        \"left outer\", t1, [\"colB\"], t2, [\"colC\"])\n    assert result[\"colB\"] == pa.chunked_array(ext_array)\n\n    result = _perform_join(\n        \"left outer\", t1, [\"colA\"], t3, [\"colA\"])\n    assert result[\"colVals\"] == pa.chunked_array(ext_array)\n\n    result = _perform_join(\n        \"left outer\", t1, [\"colB\"], t3, [\"colC\"])\n    assert result[\"colB\"] == pa.chunked_array(ext_array)\n\n\ndef test_group_by_ordering():\n    # GH-36709 - preserve ordering in groupby by setting use_threads=False\n    table1 = pa.table({'a': [1, 2, 3, 4], 'b': ['a'] * 4})\n    table2 = pa.table({'a': [1, 2, 3, 4], 'b': ['b'] * 4})\n    table = pa.concat_tables([table1, table2])\n\n    for _ in range(50):\n        # 50 seems to consistently cause errors when order is not preserved.\n        # If the order problem is reintroduced this test will become flaky\n        # which is still a signal that the order is not preserved.\n        result = table.group_by(\"b\", use_threads=False).aggregate([])\n        assert result[\"b\"] == pa.chunked_array([[\"a\"], [\"b\"]])\n", "python/pyarrow/tests/conftest.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport functools\nimport os\nimport pathlib\nimport subprocess\nimport sys\nimport time\nimport urllib.request\n\nimport pytest\nimport hypothesis as h\nfrom ..conftest import groups, defaults\n\nfrom pyarrow import set_timezone_db_path\nfrom pyarrow.util import find_free_port\n\n\n# setup hypothesis profiles\nh.settings.register_profile('ci', max_examples=1000)\nh.settings.register_profile('dev', max_examples=50)\nh.settings.register_profile('debug', max_examples=10,\n                            verbosity=h.Verbosity.verbose)\n\n# load default hypothesis profile, either set HYPOTHESIS_PROFILE environment\n# variable or pass --hypothesis-profile option to pytest, to see the generated\n# examples try:\n# pytest pyarrow -sv --enable-hypothesis --hypothesis-profile=debug\nh.settings.load_profile(os.environ.get('HYPOTHESIS_PROFILE', 'dev'))\n\n# Set this at the beginning before the AWS SDK was loaded to avoid reading in\n# user configuration values.\nos.environ['AWS_CONFIG_FILE'] = \"/dev/null\"\n\n\nif sys.platform == 'win32':\n    tzdata_set_path = os.environ.get('PYARROW_TZDATA_PATH', None)\n    if tzdata_set_path:\n        set_timezone_db_path(tzdata_set_path)\n\n\ndef pytest_addoption(parser):\n    # Create options to selectively enable test groups\n    def bool_env(name, default=None):\n        value = os.environ.get(name.upper())\n        if not value:  # missing or empty\n            return default\n        value = value.lower()\n        if value in {'1', 'true', 'on', 'yes', 'y'}:\n            return True\n        elif value in {'0', 'false', 'off', 'no', 'n'}:\n            return False\n        else:\n            raise ValueError('{}={} is not parsable as boolean'\n                             .format(name.upper(), value))\n\n    for group in groups:\n        default = bool_env('PYARROW_TEST_{}'.format(group), defaults[group])\n        parser.addoption('--enable-{}'.format(group),\n                         action='store_true', default=default,\n                         help=('Enable the {} test group'.format(group)))\n        parser.addoption('--disable-{}'.format(group),\n                         action='store_true', default=False,\n                         help=('Disable the {} test group'.format(group)))\n\n\nclass PyArrowConfig:\n    def __init__(self):\n        self.is_enabled = {}\n\n    def apply_mark(self, mark):\n        group = mark.name\n        if group in groups:\n            self.requires(group)\n\n    def requires(self, group):\n        if not self.is_enabled[group]:\n            pytest.skip('{} NOT enabled'.format(group))\n\n\ndef pytest_configure(config):\n    # Apply command-line options to initialize PyArrow-specific config object\n    config.pyarrow = PyArrowConfig()\n\n    for mark in groups:\n        config.addinivalue_line(\n            \"markers\", mark,\n        )\n\n        enable_flag = '--enable-{}'.format(mark)\n        disable_flag = '--disable-{}'.format(mark)\n\n        is_enabled = (config.getoption(enable_flag) and not\n                      config.getoption(disable_flag))\n        config.pyarrow.is_enabled[mark] = is_enabled\n\n\ndef pytest_runtest_setup(item):\n    # Apply test markers to skip tests selectively\n    for mark in item.iter_markers():\n        item.config.pyarrow.apply_mark(mark)\n\n\n@pytest.fixture\ndef tempdir(tmpdir):\n    # convert pytest's LocalPath to pathlib.Path\n    return pathlib.Path(tmpdir.strpath)\n\n\n@pytest.fixture(scope='session')\ndef base_datadir():\n    return pathlib.Path(__file__).parent / 'data'\n\n\n@pytest.fixture(autouse=True)\ndef disable_aws_metadata(monkeypatch):\n    \"\"\"Stop the AWS SDK from trying to contact the EC2 metadata server.\n\n    Otherwise, this causes a 5 second delay in tests that exercise the\n    S3 filesystem.\n    \"\"\"\n    monkeypatch.setenv(\"AWS_EC2_METADATA_DISABLED\", \"true\")\n\n\n# TODO(kszucs): move the following fixtures to test_fs.py once the previous\n# parquet dataset implementation and hdfs implementation are removed.\n\n@pytest.fixture(scope='session')\ndef hdfs_connection():\n    host = os.environ.get('ARROW_HDFS_TEST_HOST', 'default')\n    port = int(os.environ.get('ARROW_HDFS_TEST_PORT', 0))\n    user = os.environ.get('ARROW_HDFS_TEST_USER', 'hdfs')\n    return host, port, user\n\n\n@pytest.fixture(scope='session')\ndef s3_connection():\n    host, port = 'localhost', find_free_port()\n    access_key, secret_key = 'arrow', 'apachearrow'\n    return host, port, access_key, secret_key\n\n\ndef retry(attempts=3, delay=1.0, max_delay=None, backoff=1):\n    \"\"\"\n    Retry decorator\n\n    Parameters\n    ----------\n    attempts : int, default 3\n        The number of attempts.\n    delay : float, default 1\n        Initial delay in seconds.\n    max_delay : float, optional\n        The max delay between attempts.\n    backoff : float, default 1\n        The multiplier to delay after each attempt.\n    \"\"\"\n    def decorate(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            remaining_attempts = attempts\n            curr_delay = delay\n            while remaining_attempts > 0:\n                try:\n                    return func(*args, **kwargs)\n                except Exception as err:\n                    remaining_attempts -= 1\n                    last_exception = err\n                    curr_delay *= backoff\n                    if max_delay:\n                        curr_delay = min(curr_delay, max_delay)\n                    time.sleep(curr_delay)\n            raise last_exception\n        return wrapper\n    return decorate\n\n\n@pytest.fixture(scope='session')\ndef s3_server(s3_connection, tmpdir_factory):\n    @retry(attempts=5, delay=1, backoff=2)\n    def minio_server_health_check(address):\n        resp = urllib.request.urlopen(f\"http://{address}/minio/health/cluster\")\n        assert resp.getcode() == 200\n\n    tmpdir = tmpdir_factory.getbasetemp()\n    host, port, access_key, secret_key = s3_connection\n\n    address = '{}:{}'.format(host, port)\n    env = os.environ.copy()\n    env.update({\n        'MINIO_ACCESS_KEY': access_key,\n        'MINIO_SECRET_KEY': secret_key\n    })\n\n    args = ['minio', '--compat', 'server', '--quiet', '--address',\n            address, tmpdir]\n    proc = None\n    try:\n        proc = subprocess.Popen(args, env=env)\n    except OSError:\n        pytest.skip('`minio` command cannot be located')\n    else:\n        # Wait for the server to startup before yielding\n        minio_server_health_check(address)\n\n        yield {\n            'connection': s3_connection,\n            'process': proc,\n            'tempdir': tmpdir\n        }\n    finally:\n        if proc is not None:\n            proc.kill()\n            proc.wait()\n\n\n@pytest.fixture(scope='session')\ndef gcs_server():\n    port = find_free_port()\n    env = os.environ.copy()\n    args = [sys.executable, '-m', 'testbench', '--port', str(port)]\n    proc = None\n    try:\n        # check first if testbench module is available\n        import testbench  # noqa:F401\n        # start server\n        proc = subprocess.Popen(args, env=env)\n        # Make sure the server is alive.\n        if proc.poll() is not None:\n            pytest.skip(f\"Command {args} did not start server successfully!\")\n    except (ModuleNotFoundError, OSError) as e:\n        pytest.skip(f\"Command {args} failed to execute: {e}\")\n    else:\n        yield {\n            'connection': ('localhost', port),\n            'process': proc,\n        }\n    finally:\n        if proc is not None:\n            proc.kill()\n            proc.wait()\n\n\n@pytest.fixture(scope='session')\ndef azure_server(tmpdir_factory):\n    port = find_free_port()\n    env = os.environ.copy()\n    tmpdir = tmpdir_factory.getbasetemp()\n    # We only need blob service emulator, not queue or table.\n    args = ['azurite-blob', \"--location\", tmpdir, \"--blobPort\", str(port)]\n    proc = None\n    try:\n        proc = subprocess.Popen(args, env=env)\n        # Make sure the server is alive.\n        if proc.poll() is not None:\n            pytest.skip(f\"Command {args} did not start server successfully!\")\n    except (ModuleNotFoundError, OSError) as e:\n        pytest.skip(f\"Command {args} failed to execute: {e}\")\n    else:\n        yield {\n            # Use the standard azurite account_name and account_key.\n            # https://learn.microsoft.com/en-us/azure/storage/common/storage-use-emulator#authorize-with-shared-key-credentials\n            'connection': ('127.0.0.1', port, 'devstoreaccount1',\n                           'Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2'\n                           'UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw=='),\n            'process': proc,\n            'tempdir': tmpdir,\n        }\n    finally:\n        if proc is not None:\n            proc.kill()\n            proc.wait()\n\n\n@pytest.fixture(\n    params=[\n        'builtin_pickle',\n        'cloudpickle'\n    ],\n    scope='session'\n)\ndef pickle_module(request):\n    return request.getfixturevalue(request.param)\n\n\n@pytest.fixture(scope='session')\ndef builtin_pickle():\n    import pickle\n    return pickle\n\n\n@pytest.fixture(scope='session')\ndef cloudpickle():\n    cp = pytest.importorskip('cloudpickle')\n    if 'HIGHEST_PROTOCOL' not in cp.__dict__:\n        cp.HIGHEST_PROTOCOL = cp.DEFAULT_PROTOCOL\n    return cp\n", "python/pyarrow/tests/test_gandiva.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport datetime\nimport pytest\n\nimport pyarrow as pa\n\n\n@pytest.mark.gandiva\ndef test_tree_exp_builder():\n    import pyarrow.gandiva as gandiva\n\n    builder = gandiva.TreeExprBuilder()\n\n    field_a = pa.field('a', pa.int32())\n    field_b = pa.field('b', pa.int32())\n\n    schema = pa.schema([field_a, field_b])\n\n    field_result = pa.field('res', pa.int32())\n\n    node_a = builder.make_field(field_a)\n    node_b = builder.make_field(field_b)\n\n    assert node_a.return_type() == field_a.type\n\n    condition = builder.make_function(\"greater_than\", [node_a, node_b],\n                                      pa.bool_())\n    if_node = builder.make_if(condition, node_a, node_b, pa.int32())\n\n    expr = builder.make_expression(if_node, field_result)\n\n    assert expr.result().type == pa.int32()\n\n    config = gandiva.Configuration(dump_ir=True)\n    projector = gandiva.make_projector(\n        schema, [expr], pa.default_memory_pool(), \"NONE\", config)\n\n    # Gandiva generates compute kernel function named `@expr_X`\n    assert projector.llvm_ir.find(\"@expr_\") != -1\n\n    a = pa.array([10, 12, -20, 5], type=pa.int32())\n    b = pa.array([5, 15, 15, 17], type=pa.int32())\n    e = pa.array([10, 15, 15, 17], type=pa.int32())\n    input_batch = pa.RecordBatch.from_arrays([a, b], names=['a', 'b'])\n\n    r, = projector.evaluate(input_batch)\n    assert r.equals(e)\n\n\n@pytest.mark.gandiva\ndef test_table():\n    import pyarrow.gandiva as gandiva\n\n    table = pa.Table.from_arrays([pa.array([1.0, 2.0]), pa.array([3.0, 4.0])],\n                                 ['a', 'b'])\n\n    builder = gandiva.TreeExprBuilder()\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    node_b = builder.make_field(table.schema.field(\"b\"))\n\n    sum = builder.make_function(\"add\", [node_a, node_b], pa.float64())\n\n    field_result = pa.field(\"c\", pa.float64())\n    expr = builder.make_expression(sum, field_result)\n\n    projector = gandiva.make_projector(\n        table.schema, [expr], pa.default_memory_pool())\n\n    # TODO: Add .evaluate function which can take Tables instead of\n    # RecordBatches\n    r, = projector.evaluate(table.to_batches()[0])\n\n    e = pa.array([4.0, 6.0])\n    assert r.equals(e)\n\n\n@pytest.mark.gandiva\ndef test_filter():\n    import pyarrow.gandiva as gandiva\n\n    table = pa.Table.from_arrays([pa.array([1.0 * i for i in range(10000)])],\n                                 ['a'])\n\n    builder = gandiva.TreeExprBuilder()\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    thousand = builder.make_literal(1000.0, pa.float64())\n    cond = builder.make_function(\"less_than\", [node_a, thousand], pa.bool_())\n    condition = builder.make_condition(cond)\n\n    assert condition.result().type == pa.bool_()\n\n    config = gandiva.Configuration(dump_ir=True)\n    filter = gandiva.make_filter(table.schema, condition, config)\n    # Gandiva generates compute kernel function named `@expr_X`\n    assert filter.llvm_ir.find(\"@expr_\") != -1\n\n    result = filter.evaluate(table.to_batches()[0], pa.default_memory_pool())\n    assert result.to_array().equals(pa.array(range(1000), type=pa.uint32()))\n\n\n@pytest.mark.gandiva\ndef test_in_expr():\n    import pyarrow.gandiva as gandiva\n\n    arr = pa.array([\"ga\", \"an\", \"nd\", \"di\", \"iv\", \"va\"])\n    table = pa.Table.from_arrays([arr], [\"a\"])\n\n    # string\n    builder = gandiva.TreeExprBuilder()\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    cond = builder.make_in_expression(node_a, [\"an\", \"nd\"], pa.string())\n    condition = builder.make_condition(cond)\n    filter = gandiva.make_filter(table.schema, condition)\n    result = filter.evaluate(table.to_batches()[0], pa.default_memory_pool())\n    assert result.to_array().equals(pa.array([1, 2], type=pa.uint32()))\n\n    # int32\n    arr = pa.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 4])\n    table = pa.Table.from_arrays([arr.cast(pa.int32())], [\"a\"])\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    cond = builder.make_in_expression(node_a, [1, 5], pa.int32())\n    condition = builder.make_condition(cond)\n    filter = gandiva.make_filter(table.schema, condition)\n    result = filter.evaluate(table.to_batches()[0], pa.default_memory_pool())\n    assert result.to_array().equals(pa.array([1, 3, 4, 8], type=pa.uint32()))\n\n    # int64\n    arr = pa.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 4])\n    table = pa.Table.from_arrays([arr], [\"a\"])\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    cond = builder.make_in_expression(node_a, [1, 5], pa.int64())\n    condition = builder.make_condition(cond)\n    filter = gandiva.make_filter(table.schema, condition)\n    result = filter.evaluate(table.to_batches()[0], pa.default_memory_pool())\n    assert result.to_array().equals(pa.array([1, 3, 4, 8], type=pa.uint32()))\n\n\n@pytest.mark.skip(reason=\"Gandiva C++ did not have *real* binary, \"\n                         \"time and date support.\")\ndef test_in_expr_todo():\n    import pyarrow.gandiva as gandiva\n    # TODO: Implement reasonable support for timestamp, time & date.\n    # Current exceptions:\n    # pyarrow.lib.ArrowException: ExpressionValidationError:\n    # Evaluation expression for IN clause returns XXXX values are of typeXXXX\n\n    # binary\n    arr = pa.array([b\"ga\", b\"an\", b\"nd\", b\"di\", b\"iv\", b\"va\"])\n    table = pa.Table.from_arrays([arr], [\"a\"])\n\n    builder = gandiva.TreeExprBuilder()\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    cond = builder.make_in_expression(node_a, [b'an', b'nd'], pa.binary())\n    condition = builder.make_condition(cond)\n\n    filter = gandiva.make_filter(table.schema, condition)\n    result = filter.evaluate(table.to_batches()[0], pa.default_memory_pool())\n    assert result.to_array().equals(pa.array([1, 2], type=pa.uint32()))\n\n    # timestamp\n    datetime_1 = datetime.datetime.utcfromtimestamp(1542238951.621877)\n    datetime_2 = datetime.datetime.utcfromtimestamp(1542238911.621877)\n    datetime_3 = datetime.datetime.utcfromtimestamp(1542238051.621877)\n\n    arr = pa.array([datetime_1, datetime_2, datetime_3])\n    table = pa.Table.from_arrays([arr], [\"a\"])\n\n    builder = gandiva.TreeExprBuilder()\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    cond = builder.make_in_expression(node_a, [datetime_2], pa.timestamp('ms'))\n    condition = builder.make_condition(cond)\n\n    filter = gandiva.make_filter(table.schema, condition)\n    result = filter.evaluate(table.to_batches()[0], pa.default_memory_pool())\n    assert list(result.to_array()) == [1]\n\n    # time\n    time_1 = datetime_1.time()\n    time_2 = datetime_2.time()\n    time_3 = datetime_3.time()\n\n    arr = pa.array([time_1, time_2, time_3])\n    table = pa.Table.from_arrays([arr], [\"a\"])\n\n    builder = gandiva.TreeExprBuilder()\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    cond = builder.make_in_expression(node_a, [time_2], pa.time64('ms'))\n    condition = builder.make_condition(cond)\n\n    filter = gandiva.make_filter(table.schema, condition)\n    result = filter.evaluate(table.to_batches()[0], pa.default_memory_pool())\n    assert list(result.to_array()) == [1]\n\n    # date\n    date_1 = datetime_1.date()\n    date_2 = datetime_2.date()\n    date_3 = datetime_3.date()\n\n    arr = pa.array([date_1, date_2, date_3])\n    table = pa.Table.from_arrays([arr], [\"a\"])\n\n    builder = gandiva.TreeExprBuilder()\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    cond = builder.make_in_expression(node_a, [date_2], pa.date32())\n    condition = builder.make_condition(cond)\n\n    filter = gandiva.make_filter(table.schema, condition)\n    result = filter.evaluate(table.to_batches()[0], pa.default_memory_pool())\n    assert list(result.to_array()) == [1]\n\n\n@pytest.mark.gandiva\ndef test_boolean():\n    import pyarrow.gandiva as gandiva\n\n    table = pa.Table.from_arrays([\n        pa.array([1., 31., 46., 3., 57., 44., 22.]),\n        pa.array([5., 45., 36., 73., 83., 23., 76.])],\n        ['a', 'b'])\n\n    builder = gandiva.TreeExprBuilder()\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    node_b = builder.make_field(table.schema.field(\"b\"))\n    fifty = builder.make_literal(50.0, pa.float64())\n    eleven = builder.make_literal(11.0, pa.float64())\n\n    cond_1 = builder.make_function(\"less_than\", [node_a, fifty], pa.bool_())\n    cond_2 = builder.make_function(\"greater_than\", [node_a, node_b],\n                                   pa.bool_())\n    cond_3 = builder.make_function(\"less_than\", [node_b, eleven], pa.bool_())\n    cond = builder.make_or([builder.make_and([cond_1, cond_2]), cond_3])\n    condition = builder.make_condition(cond)\n\n    filter = gandiva.make_filter(table.schema, condition)\n    result = filter.evaluate(table.to_batches()[0], pa.default_memory_pool())\n    assert result.to_array().equals(pa.array([0, 2, 5], type=pa.uint32()))\n\n\n@pytest.mark.gandiva\ndef test_literals():\n    import pyarrow.gandiva as gandiva\n\n    builder = gandiva.TreeExprBuilder()\n\n    builder.make_literal(True, pa.bool_())\n    builder.make_literal(0, pa.uint8())\n    builder.make_literal(1, pa.uint16())\n    builder.make_literal(2, pa.uint32())\n    builder.make_literal(3, pa.uint64())\n    builder.make_literal(4, pa.int8())\n    builder.make_literal(5, pa.int16())\n    builder.make_literal(6, pa.int32())\n    builder.make_literal(7, pa.int64())\n    builder.make_literal(8.0, pa.float32())\n    builder.make_literal(9.0, pa.float64())\n    builder.make_literal(\"hello\", pa.string())\n    builder.make_literal(b\"world\", pa.binary())\n\n    builder.make_literal(True, \"bool\")\n    builder.make_literal(0, \"uint8\")\n    builder.make_literal(1, \"uint16\")\n    builder.make_literal(2, \"uint32\")\n    builder.make_literal(3, \"uint64\")\n    builder.make_literal(4, \"int8\")\n    builder.make_literal(5, \"int16\")\n    builder.make_literal(6, \"int32\")\n    builder.make_literal(7, \"int64\")\n    builder.make_literal(8.0, \"float32\")\n    builder.make_literal(9.0, \"float64\")\n    builder.make_literal(\"hello\", \"string\")\n    builder.make_literal(b\"world\", \"binary\")\n\n    with pytest.raises(TypeError):\n        builder.make_literal(\"hello\", pa.int64())\n    with pytest.raises(TypeError):\n        builder.make_literal(True, None)\n\n\n@pytest.mark.gandiva\ndef test_regex():\n    import pyarrow.gandiva as gandiva\n\n    elements = [\"park\", \"sparkle\", \"bright spark and fire\", \"spark\"]\n    data = pa.array(elements, type=pa.string())\n    table = pa.Table.from_arrays([data], names=['a'])\n\n    builder = gandiva.TreeExprBuilder()\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    regex = builder.make_literal(\"%spark%\", pa.string())\n    like = builder.make_function(\"like\", [node_a, regex], pa.bool_())\n\n    field_result = pa.field(\"b\", pa.bool_())\n    expr = builder.make_expression(like, field_result)\n\n    projector = gandiva.make_projector(\n        table.schema, [expr], pa.default_memory_pool())\n\n    r, = projector.evaluate(table.to_batches()[0])\n    b = pa.array([False, True, True, True], type=pa.bool_())\n    assert r.equals(b)\n\n\n@pytest.mark.gandiva\ndef test_get_registered_function_signatures():\n    import pyarrow.gandiva as gandiva\n    signatures = gandiva.get_registered_function_signatures()\n\n    assert type(signatures[0].return_type()) is pa.DataType\n    assert type(signatures[0].param_types()) is list\n    assert hasattr(signatures[0], \"name\")\n\n\n@pytest.mark.gandiva\ndef test_filter_project():\n    import pyarrow.gandiva as gandiva\n    mpool = pa.default_memory_pool()\n    # Create a table with some sample data\n    array0 = pa.array([10, 12, -20, 5, 21, 29], pa.int32())\n    array1 = pa.array([5, 15, 15, 17, 12, 3], pa.int32())\n    array2 = pa.array([1, 25, 11, 30, -21, None], pa.int32())\n\n    table = pa.Table.from_arrays([array0, array1, array2], ['a', 'b', 'c'])\n\n    field_result = pa.field(\"res\", pa.int32())\n\n    builder = gandiva.TreeExprBuilder()\n    node_a = builder.make_field(table.schema.field(\"a\"))\n    node_b = builder.make_field(table.schema.field(\"b\"))\n    node_c = builder.make_field(table.schema.field(\"c\"))\n\n    greater_than_function = builder.make_function(\"greater_than\",\n                                                  [node_a, node_b], pa.bool_())\n    filter_condition = builder.make_condition(\n        greater_than_function)\n\n    project_condition = builder.make_function(\"less_than\",\n                                              [node_b, node_c], pa.bool_())\n    if_node = builder.make_if(project_condition,\n                              node_b, node_c, pa.int32())\n    expr = builder.make_expression(if_node, field_result)\n\n    # Build a filter for the expressions.\n    filter = gandiva.make_filter(table.schema, filter_condition)\n\n    # Build a projector for the expressions.\n    projector = gandiva.make_projector(\n        table.schema, [expr], mpool, \"UINT32\")\n\n    # Evaluate filter\n    selection_vector = filter.evaluate(table.to_batches()[0], mpool)\n\n    # Evaluate project\n    r, = projector.evaluate(\n        table.to_batches()[0], selection_vector)\n\n    exp = pa.array([1, -21, None], pa.int32())\n    assert r.equals(exp)\n\n\n@pytest.mark.gandiva\ndef test_to_string():\n    import pyarrow.gandiva as gandiva\n    builder = gandiva.TreeExprBuilder()\n\n    assert str(builder.make_literal(2.0, pa.float64())\n               ).startswith('(const double) 2 raw(')\n    assert str(builder.make_literal(2, pa.int64())) == '(const int64) 2'\n    assert str(builder.make_field(pa.field('x', pa.float64()))) == '(double) x'\n    assert str(builder.make_field(pa.field('y', pa.string()))) == '(string) y'\n\n    field_z = builder.make_field(pa.field('z', pa.bool_()))\n    func_node = builder.make_function('not', [field_z], pa.bool_())\n    assert str(func_node) == 'bool not((bool) z)'\n\n    field_y = builder.make_field(pa.field('y', pa.bool_()))\n    and_node = builder.make_and([func_node, field_y])\n    assert str(and_node) == 'bool not((bool) z) && (bool) y'\n\n\n@pytest.mark.gandiva\ndef test_rejects_none():\n    import pyarrow.gandiva as gandiva\n\n    builder = gandiva.TreeExprBuilder()\n\n    field_x = pa.field('x', pa.int32())\n    schema = pa.schema([field_x])\n    literal_true = builder.make_literal(True, pa.bool_())\n\n    with pytest.raises(TypeError):\n        builder.make_field(None)\n\n    with pytest.raises(TypeError):\n        builder.make_if(literal_true, None, None, None)\n\n    with pytest.raises(TypeError):\n        builder.make_and([literal_true, None])\n\n    with pytest.raises(TypeError):\n        builder.make_or([None, literal_true])\n\n    with pytest.raises(TypeError):\n        builder.make_in_expression(None, [1, 2, 3], pa.int32())\n\n    with pytest.raises(TypeError):\n        builder.make_expression(None, field_x)\n\n    with pytest.raises(TypeError):\n        builder.make_condition(None)\n\n    with pytest.raises(TypeError):\n        builder.make_function('less_than', [literal_true, None], pa.bool_())\n\n    with pytest.raises(TypeError):\n        gandiva.make_projector(schema, [None])\n\n    with pytest.raises(TypeError):\n        gandiva.make_filter(schema, None)\n", "python/pyarrow/tests/test_feather.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport io\nimport os\nimport sys\nimport tempfile\nimport pytest\nimport hypothesis as h\nimport hypothesis.strategies as st\n\nimport numpy as np\n\nimport pyarrow as pa\nimport pyarrow.tests.strategies as past\nfrom pyarrow.feather import (read_feather, write_feather, read_table,\n                             FeatherDataset)\n\ntry:\n    from pandas.testing import assert_frame_equal\n    import pandas as pd\n    import pyarrow.pandas_compat\nexcept ImportError:\n    pass\n\n\n@pytest.fixture(scope='module')\ndef datadir(base_datadir):\n    return base_datadir / 'feather'\n\n\ndef random_path(prefix='feather_'):\n    return tempfile.mktemp(prefix=prefix)\n\n\n@pytest.fixture(scope=\"module\", params=[1, 2])\ndef version(request):\n    yield request.param\n\n\n@pytest.fixture(scope=\"module\", params=[None, \"uncompressed\", \"lz4\", \"zstd\"])\ndef compression(request):\n    if request.param in ['lz4', 'zstd'] and not pa.Codec.is_available(\n            request.param):\n        pytest.skip(f'{request.param} is not available')\n    yield request.param\n\n\nTEST_FILES = None\n\n\ndef setup_module(module):\n    global TEST_FILES\n    TEST_FILES = []\n\n\ndef teardown_module(module):\n    for path in TEST_FILES:\n        try:\n            os.remove(path)\n        except os.error:\n            pass\n\n\n@pytest.mark.pandas\ndef test_file_not_exist():\n    with pytest.raises(pa.ArrowIOError):\n        read_feather('test_invalid_file')\n\n\ndef _check_pandas_roundtrip(df, expected=None, path=None,\n                            columns=None, use_threads=False,\n                            version=None, compression=None,\n                            compression_level=None):\n    if path is None:\n        path = random_path()\n\n    if version is None:\n        version = 2\n\n    TEST_FILES.append(path)\n    write_feather(df, path, compression=compression,\n                  compression_level=compression_level, version=version)\n\n    if not os.path.exists(path):\n        raise Exception('file not written')\n\n    result = read_feather(path, columns, use_threads=use_threads)\n\n    if expected is None:\n        expected = df\n\n    assert_frame_equal(result, expected)\n\n\ndef _check_arrow_roundtrip(table, path=None, compression=None):\n    if path is None:\n        path = random_path()\n\n    TEST_FILES.append(path)\n    write_feather(table, path, compression=compression)\n    if not os.path.exists(path):\n        raise Exception('file not written')\n\n    result = read_table(path)\n    assert result.equals(table)\n\n\ndef _assert_error_on_write(df, exc, path=None, version=2):\n    # check that we are raising the exception\n    # on writing\n\n    if path is None:\n        path = random_path()\n\n    TEST_FILES.append(path)\n\n    def f():\n        write_feather(df, path, version=version)\n\n    pytest.raises(exc, f)\n\n\ndef test_dataset(version):\n    num_values = (100, 100)\n    num_files = 5\n    paths = [random_path() for i in range(num_files)]\n    data = {\n        \"col_\" + str(i): np.random.randn(num_values[0])\n        for i in range(num_values[1])\n    }\n    table = pa.table(data)\n\n    TEST_FILES.extend(paths)\n    for index, path in enumerate(paths):\n        rows = (\n            index * (num_values[0] // num_files),\n            (index + 1) * (num_values[0] // num_files),\n        )\n\n        write_feather(table[rows[0]: rows[1]], path, version=version)\n\n    data = FeatherDataset(paths).read_table()\n    assert data.equals(table)\n\n\n@pytest.mark.pandas\ndef test_float_no_nulls(version):\n    data = {}\n    numpy_dtypes = ['f4', 'f8']\n    num_values = 100\n\n    for dtype in numpy_dtypes:\n        values = np.random.randn(num_values)\n        data[dtype] = values.astype(dtype)\n\n    df = pd.DataFrame(data)\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_read_table(version):\n    num_values = (100, 100)\n    path = random_path()\n\n    TEST_FILES.append(path)\n\n    values = np.random.randint(0, 100, size=num_values)\n    columns = ['col_' + str(i) for i in range(100)]\n    table = pa.Table.from_arrays(values, columns)\n\n    write_feather(table, path, version=version)\n\n    result = read_table(path)\n    assert result.equals(table)\n\n    # Test without memory mapping\n    result = read_table(path, memory_map=False)\n    assert result.equals(table)\n\n    result = read_feather(path, memory_map=False)\n    assert_frame_equal(table.to_pandas(), result)\n\n\n@pytest.mark.pandas\ndef test_use_threads(version):\n    # ARROW-14470\n    num_values = (10, 10)\n    path = random_path()\n\n    TEST_FILES.append(path)\n\n    values = np.random.randint(0, 10, size=num_values)\n    columns = ['col_' + str(i) for i in range(10)]\n    table = pa.Table.from_arrays(values, columns)\n\n    write_feather(table, path, version=version)\n\n    result = read_feather(path)\n    assert_frame_equal(table.to_pandas(), result)\n\n    # Test read_feather with use_threads=False\n    result = read_feather(path, use_threads=False)\n    assert_frame_equal(table.to_pandas(), result)\n\n    # Test read_table with use_threads=False\n    result = read_table(path, use_threads=False)\n    assert result.equals(table)\n\n\n@pytest.mark.pandas\ndef test_float_nulls(version):\n    num_values = 100\n\n    path = random_path()\n    TEST_FILES.append(path)\n\n    null_mask = np.random.randint(0, 10, size=num_values) < 3\n    dtypes = ['f4', 'f8']\n    expected_cols = []\n\n    arrays = []\n    for name in dtypes:\n        values = np.random.randn(num_values).astype(name)\n        arrays.append(pa.array(values, mask=null_mask))\n\n        values[null_mask] = np.nan\n\n        expected_cols.append(values)\n\n    table = pa.table(arrays, names=dtypes)\n    _check_arrow_roundtrip(table)\n\n    df = table.to_pandas()\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_integer_no_nulls(version):\n    data, arr = {}, []\n\n    numpy_dtypes = ['i1', 'i2', 'i4', 'i8',\n                    'u1', 'u2', 'u4', 'u8']\n    num_values = 100\n\n    for dtype in numpy_dtypes:\n        values = np.random.randint(0, 100, size=num_values)\n        data[dtype] = values.astype(dtype)\n        arr.append(values.astype(dtype))\n\n    df = pd.DataFrame(data)\n    _check_pandas_roundtrip(df, version=version)\n\n    table = pa.table(arr, names=numpy_dtypes)\n    _check_arrow_roundtrip(table)\n\n\n@pytest.mark.pandas\ndef test_platform_numpy_integers(version):\n    data = {}\n\n    numpy_dtypes = ['longlong']\n    num_values = 100\n\n    for dtype in numpy_dtypes:\n        values = np.random.randint(0, 100, size=num_values)\n        data[dtype] = values.astype(dtype)\n\n    df = pd.DataFrame(data)\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_integer_with_nulls(version):\n    # pandas requires upcast to float dtype\n    path = random_path()\n    TEST_FILES.append(path)\n\n    int_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8']\n    num_values = 100\n\n    arrays = []\n    null_mask = np.random.randint(0, 10, size=num_values) < 3\n    expected_cols = []\n    for name in int_dtypes:\n        values = np.random.randint(0, 100, size=num_values)\n        arrays.append(pa.array(values, mask=null_mask))\n\n        expected = values.astype('f8')\n        expected[null_mask] = np.nan\n\n        expected_cols.append(expected)\n\n    table = pa.table(arrays, names=int_dtypes)\n    _check_arrow_roundtrip(table)\n\n    df = table.to_pandas()\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_boolean_no_nulls(version):\n    num_values = 100\n\n    np.random.seed(0)\n\n    df = pd.DataFrame({'bools': np.random.randn(num_values) > 0})\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_boolean_nulls(version):\n    # pandas requires upcast to object dtype\n    path = random_path()\n    TEST_FILES.append(path)\n\n    num_values = 100\n    np.random.seed(0)\n\n    mask = np.random.randint(0, 10, size=num_values) < 3\n    values = np.random.randint(0, 10, size=num_values) < 5\n\n    table = pa.table([pa.array(values, mask=mask)], names=['bools'])\n    _check_arrow_roundtrip(table)\n\n    df = table.to_pandas()\n    _check_pandas_roundtrip(df, version=version)\n\n\ndef test_buffer_bounds_error(version):\n    # ARROW-1676\n    path = random_path()\n    TEST_FILES.append(path)\n\n    for i in range(16, 256):\n        table = pa.Table.from_arrays(\n            [pa.array([None] + list(range(i)), type=pa.float64())],\n            names=[\"arr\"]\n        )\n        _check_arrow_roundtrip(table)\n\n\ndef test_boolean_object_nulls(version):\n    repeats = 100\n    table = pa.Table.from_arrays(\n        [np.array([False, None, True] * repeats, dtype=object)],\n        names=[\"arr\"]\n    )\n    _check_arrow_roundtrip(table)\n\n\n@pytest.mark.pandas\ndef test_delete_partial_file_on_error(version):\n    if sys.platform == 'win32':\n        pytest.skip('Windows hangs on to file handle for some reason')\n\n    class CustomClass:\n        pass\n\n    # strings will fail\n    df = pd.DataFrame(\n        {\n            'numbers': range(5),\n            'strings': [b'foo', None, 'bar', CustomClass(), np.nan]},\n        columns=['numbers', 'strings'])\n\n    path = random_path()\n    try:\n        write_feather(df, path, version=version)\n    except Exception:\n        pass\n\n    assert not os.path.exists(path)\n\n\n@pytest.mark.pandas\ndef test_strings(version):\n    repeats = 1000\n\n    # Mixed bytes, unicode, strings coerced to binary\n    values = [b'foo', None, 'bar', 'qux', np.nan]\n    df = pd.DataFrame({'strings': values * repeats})\n\n    ex_values = [b'foo', None, b'bar', b'qux', None]\n    expected = pd.DataFrame({'strings': ex_values * repeats})\n    _check_pandas_roundtrip(df, expected, version=version)\n\n    # embedded nulls are ok\n    values = ['foo', None, 'bar', 'qux', None]\n    df = pd.DataFrame({'strings': values * repeats})\n    expected = pd.DataFrame({'strings': values * repeats})\n    _check_pandas_roundtrip(df, expected, version=version)\n\n    values = ['foo', None, 'bar', 'qux', np.nan]\n    df = pd.DataFrame({'strings': values * repeats})\n    ex_values = ['foo', None, 'bar', 'qux', None]\n    expected = pd.DataFrame({'strings': ex_values * repeats})\n    _check_pandas_roundtrip(df, expected, version=version)\n\n\n@pytest.mark.pandas\ndef test_empty_strings(version):\n    df = pd.DataFrame({'strings': [''] * 10})\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_all_none(version):\n    df = pd.DataFrame({'all_none': [None] * 10})\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_all_null_category(version):\n    # ARROW-1188\n    df = pd.DataFrame({\"A\": (1, 2, 3), \"B\": (None, None, None)})\n    df = df.assign(B=df.B.astype(\"category\"))\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_multithreaded_read(version):\n    data = {'c{}'.format(i): [''] * 10\n            for i in range(100)}\n    df = pd.DataFrame(data)\n    _check_pandas_roundtrip(df, use_threads=True, version=version)\n\n\n@pytest.mark.pandas\ndef test_nan_as_null(version):\n    # Create a nan that is not numpy.nan\n    values = np.array(['foo', np.nan, np.nan * 2, 'bar'] * 10)\n    df = pd.DataFrame({'strings': values})\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_category(version):\n    repeats = 1000\n    values = ['foo', None, 'bar', 'qux', np.nan]\n    df = pd.DataFrame({'strings': values * repeats})\n    df['strings'] = df['strings'].astype('category')\n\n    values = ['foo', None, 'bar', 'qux', None]\n    expected = pd.DataFrame({'strings': pd.Categorical(values * repeats)})\n    _check_pandas_roundtrip(df, expected, version=version)\n\n\n@pytest.mark.pandas\ndef test_timestamp(version):\n    df = pd.DataFrame({'naive': pd.date_range('2016-03-28', periods=10)})\n    df['with_tz'] = (df.naive.dt.tz_localize('utc')\n                     .dt.tz_convert('America/Los_Angeles'))\n\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_timestamp_with_nulls(version):\n    df = pd.DataFrame({'test': [pd.Timestamp(2016, 1, 1),\n                                None,\n                                pd.Timestamp(2016, 1, 3)]})\n    df['with_tz'] = df.test.dt.tz_localize('utc')\n\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\n@pytest.mark.xfail(reason=\"not supported\", raises=TypeError)\ndef test_timedelta_with_nulls_v1():\n    df = pd.DataFrame({'test': [pd.Timedelta('1 day'),\n                                None,\n                                pd.Timedelta('3 day')]})\n    _check_pandas_roundtrip(df, version=1)\n\n\n@pytest.mark.pandas\ndef test_timedelta_with_nulls():\n    df = pd.DataFrame({'test': [pd.Timedelta('1 day'),\n                                None,\n                                pd.Timedelta('3 day')]})\n    _check_pandas_roundtrip(df, version=2)\n\n\n@pytest.mark.pandas\ndef test_out_of_float64_timestamp_with_nulls(version):\n    df = pd.DataFrame(\n        {'test': pd.DatetimeIndex([1451606400000000001,\n                                   None, 14516064000030405])})\n    df['with_tz'] = df.test.dt.tz_localize('utc')\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.pandas\ndef test_non_string_columns(version):\n    df = pd.DataFrame({0: [1, 2, 3, 4],\n                       1: [True, False, True, False]})\n    expected = df\n\n    if version == 1:\n        expected = df.rename(columns=str)\n    _check_pandas_roundtrip(df, expected, version=version)\n\n\n@pytest.mark.pandas\n@pytest.mark.skipif(not os.path.supports_unicode_filenames,\n                    reason='unicode filenames not supported')\ndef test_unicode_filename(version):\n    # GH #209\n    name = (b'Besa_Kavaj\\xc3\\xab.feather').decode('utf-8')\n    df = pd.DataFrame({'foo': [1, 2, 3, 4]})\n    _check_pandas_roundtrip(df, path=random_path(prefix=name),\n                            version=version)\n\n\n@pytest.mark.pandas\ndef test_read_columns(version):\n    df = pd.DataFrame({\n        'foo': [1, 2, 3, 4],\n        'boo': [5, 6, 7, 8],\n        'woo': [1, 3, 5, 7]\n    })\n    expected = df[['boo', 'woo']]\n\n    _check_pandas_roundtrip(df, expected, version=version,\n                            columns=['boo', 'woo'])\n\n\ndef test_overwritten_file(version):\n    path = random_path()\n    TEST_FILES.append(path)\n\n    num_values = 100\n    np.random.seed(0)\n\n    values = np.random.randint(0, 10, size=num_values)\n\n    table = pa.table({'ints': values})\n    write_feather(table, path)\n\n    table = pa.table({'more_ints': values[0:num_values//2]})\n    _check_arrow_roundtrip(table, path=path)\n\n\n@pytest.mark.pandas\ndef test_filelike_objects(version):\n    buf = io.BytesIO()\n\n    # the copy makes it non-strided\n    df = pd.DataFrame(np.arange(12).reshape(4, 3),\n                      columns=['a', 'b', 'c']).copy()\n    write_feather(df, buf, version=version)\n\n    buf.seek(0)\n\n    result = read_feather(buf)\n    assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\n@pytest.mark.filterwarnings(\"ignore:Sparse:FutureWarning\")\n@pytest.mark.filterwarnings(\"ignore:DataFrame.to_sparse:FutureWarning\")\ndef test_sparse_dataframe(version):\n    if not pa.pandas_compat._pandas_api.has_sparse:\n        pytest.skip(\"version of pandas does not support SparseDataFrame\")\n    # GH #221\n    data = {'A': [0, 1, 2],\n            'B': [1, 0, 1]}\n    df = pd.DataFrame(data).to_sparse(fill_value=1)\n    expected = df.to_dense()\n    _check_pandas_roundtrip(df, expected, version=version)\n\n\n@pytest.mark.pandas\ndef test_duplicate_columns_pandas():\n\n    # https://github.com/wesm/feather/issues/53\n    # not currently able to handle duplicate columns\n    df = pd.DataFrame(np.arange(12).reshape(4, 3),\n                      columns=list('aaa')).copy()\n    _assert_error_on_write(df, ValueError)\n\n\ndef test_duplicate_columns():\n    # only works for version 2\n    table = pa.table([[1, 2, 3], [4, 5, 6], [7, 8, 9]], names=['a', 'a', 'b'])\n    _check_arrow_roundtrip(table)\n    _assert_error_on_write(table, ValueError, version=1)\n\n\n@pytest.mark.pandas\ndef test_unsupported():\n    # https://github.com/wesm/feather/issues/240\n    # serializing actual python objects\n\n    # custom python objects\n    class A:\n        pass\n\n    df = pd.DataFrame({'a': [A(), A()]})\n    _assert_error_on_write(df, ValueError)\n\n    # non-strings\n    df = pd.DataFrame({'a': ['a', 1, 2.0]})\n    _assert_error_on_write(df, TypeError)\n\n\n@pytest.mark.pandas\ndef test_v2_set_chunksize():\n    df = pd.DataFrame({'A': np.arange(1000)})\n    table = pa.table(df)\n\n    buf = io.BytesIO()\n    write_feather(table, buf, chunksize=250, version=2)\n\n    result = buf.getvalue()\n\n    ipc_file = pa.ipc.open_file(pa.BufferReader(result))\n    assert ipc_file.num_record_batches == 4\n    assert len(ipc_file.get_batch(0)) == 250\n\n\n@pytest.mark.pandas\n@pytest.mark.lz4\n@pytest.mark.snappy\n@pytest.mark.zstd\ndef test_v2_compression_options():\n    df = pd.DataFrame({'A': np.arange(1000)})\n\n    cases = [\n        # compression, compression_level\n        ('uncompressed', None),\n        ('lz4', None),\n        ('lz4', 1),\n        ('lz4', 12),\n        ('zstd', 1),\n        ('zstd', 10)\n    ]\n\n    for compression, compression_level in cases:\n        _check_pandas_roundtrip(df, compression=compression,\n                                compression_level=compression_level)\n\n    buf = io.BytesIO()\n\n    # Trying to compress with V1\n    with pytest.raises(\n            ValueError,\n            match=\"Feather V1 files do not support compression option\"):\n        write_feather(df, buf, compression='lz4', version=1)\n\n    # Trying to set chunksize with V1\n    with pytest.raises(\n            ValueError,\n            match=\"Feather V1 files do not support chunksize option\"):\n        write_feather(df, buf, chunksize=4096, version=1)\n\n    # Unsupported compressor\n    with pytest.raises(ValueError,\n                       match='compression=\"snappy\" not supported'):\n        write_feather(df, buf, compression='snappy')\n\n\ndef test_v2_lz4_default_compression():\n    # ARROW-8750: Make sure that the compression=None option selects lz4 if\n    # it's available\n    if not pa.Codec.is_available('lz4_frame'):\n        pytest.skip(\"LZ4 compression support is not built in C++\")\n\n    # some highly compressible data\n    t = pa.table([np.repeat(0, 100000)], names=['f0'])\n\n    buf = io.BytesIO()\n    write_feather(t, buf)\n    default_result = buf.getvalue()\n\n    buf = io.BytesIO()\n    write_feather(t, buf, compression='uncompressed')\n    uncompressed_result = buf.getvalue()\n\n    assert len(default_result) < len(uncompressed_result)\n\n\ndef test_v1_unsupported_types():\n    table = pa.table([pa.array([[1, 2, 3], [], None])], names=['f0'])\n\n    buf = io.BytesIO()\n    with pytest.raises(TypeError,\n                       match=(\"Unsupported Feather V1 type: \"\n                              \"list<item: int64>. \"\n                              \"Use V2 format to serialize all Arrow types.\")):\n        write_feather(table, buf, version=1)\n\n\n@pytest.mark.slow\n@pytest.mark.pandas\ndef test_large_dataframe(version):\n    df = pd.DataFrame({'A': np.arange(400000000)})\n    _check_pandas_roundtrip(df, version=version)\n\n\n@pytest.mark.large_memory\n@pytest.mark.pandas\ndef test_chunked_binary_error_message():\n    # ARROW-3058: As Feather does not yet support chunked columns, we at least\n    # make sure it's clear to the user what is going on\n\n    # 2^31 + 1 bytes\n    values = [b'x'] + [\n        b'x' * (1 << 20)\n    ] * 2 * (1 << 10)\n    df = pd.DataFrame({'byte_col': values})\n\n    # Works fine with version 2\n    buf = io.BytesIO()\n    write_feather(df, buf, version=2)\n    result = read_feather(pa.BufferReader(buf.getvalue()))\n    assert_frame_equal(result, df)\n\n    with pytest.raises(ValueError, match=\"'byte_col' exceeds 2GB maximum \"\n                       \"capacity of a Feather binary column. This restriction \"\n                       \"may be lifted in the future\"):\n        write_feather(df, io.BytesIO(), version=1)\n\n\ndef test_feather_without_pandas(tempdir, version):\n    # ARROW-8345\n    table = pa.table([pa.array([1, 2, 3])], names=['f0'])\n    path = str(tempdir / \"data.feather\")\n    _check_arrow_roundtrip(table, path)\n\n\n@pytest.mark.pandas\ndef test_read_column_selection(version):\n    # ARROW-8641\n    df = pd.DataFrame(np.arange(12).reshape(4, 3), columns=['a', 'b', 'c'])\n\n    # select columns as string names or integer indices\n    _check_pandas_roundtrip(\n        df, columns=['a', 'c'], expected=df[['a', 'c']], version=version)\n    _check_pandas_roundtrip(\n        df, columns=[0, 2], expected=df[['a', 'c']], version=version)\n\n    # different order is followed\n    _check_pandas_roundtrip(\n        df, columns=['b', 'a'], expected=df[['b', 'a']], version=version)\n    _check_pandas_roundtrip(\n        df, columns=[1, 0], expected=df[['b', 'a']], version=version)\n\n\ndef test_read_column_duplicated_selection(tempdir, version):\n    # duplicated columns in the column selection\n    table = pa.table([[1, 2, 3], [4, 5, 6], [7, 8, 9]], names=['a', 'b', 'c'])\n    path = str(tempdir / \"data.feather\")\n    write_feather(table, path, version=version)\n\n    expected = pa.table([[1, 2, 3], [4, 5, 6], [1, 2, 3]],\n                        names=['a', 'b', 'a'])\n    for col_selection in [['a', 'b', 'a'], [0, 1, 0]]:\n        result = read_table(path, columns=col_selection)\n        assert result.equals(expected)\n\n\ndef test_read_column_duplicated_in_file(tempdir):\n    # duplicated columns in feather file (only works for feather v2)\n    table = pa.table([[1, 2, 3], [4, 5, 6], [7, 8, 9]], names=['a', 'b', 'a'])\n    path = str(tempdir / \"data.feather\")\n    write_feather(table, path, version=2)\n\n    # no selection works fine\n    result = read_table(path)\n    assert result.equals(table)\n\n    # selection with indices works\n    result = read_table(path, columns=[0, 2])\n    assert result.column_names == ['a', 'a']\n\n    # selection with column names errors\n    with pytest.raises(ValueError):\n        read_table(path, columns=['a', 'b'])\n\n\ndef test_nested_types(compression):\n    # https://issues.apache.org/jira/browse/ARROW-8860\n    table = pa.table({'col': pa.StructArray.from_arrays(\n        [[0, 1, 2], [1, 2, 3]], names=[\"f1\", \"f2\"])})\n    _check_arrow_roundtrip(table, compression=compression)\n\n    table = pa.table({'col': pa.array([[1, 2], [3, 4]])})\n    _check_arrow_roundtrip(table, compression=compression)\n\n    table = pa.table({'col': pa.array([[[1, 2], [3, 4]], [[5, 6], None]])})\n    _check_arrow_roundtrip(table, compression=compression)\n\n\n@h.given(past.all_tables, st.sampled_from([\"uncompressed\", \"lz4\", \"zstd\"]))\ndef test_roundtrip(table, compression):\n    _check_arrow_roundtrip(table, compression=compression)\n\n\n@pytest.mark.lz4\ndef test_feather_v017_experimental_compression_backward_compatibility(datadir):\n    # ARROW-11163 - ensure newer pyarrow versions can read the old feather\n    # files from version 0.17.0 with experimental compression support (before\n    # it was officially added to IPC format in 1.0.0)\n\n    # file generated with:\n    #     table = pa.table({'a': range(5)})\n    #     from pyarrow import feather\n    #     feather.write_feather(\n    #         table, \"v0.17.0.version.2-compression.lz4.feather\",\n    #         compression=\"lz4\", version=2)\n    expected = pa.table({'a': range(5)})\n    result = read_table(datadir / \"v0.17.0.version.2-compression.lz4.feather\")\n    assert result.equals(expected)\n\n\n@pytest.mark.pandas\ndef test_preserve_index_pandas(version):\n    df = pd.DataFrame({'a': [1, 2, 3]}, index=['a', 'b', 'c'])\n\n    if version == 1:\n        expected = df.reset_index(drop=True).rename(columns=str)\n    else:\n        expected = df\n\n    _check_pandas_roundtrip(df, expected, version=version)\n\n\n@pytest.mark.pandas\ndef test_feather_datetime_resolution_arrow_to_pandas(tempdir):\n    # ARROW-17192 - ensure timestamp_as_object=True (together with other\n    # **kwargs) can be passed in read_feather to to_pandas.\n\n    from datetime import datetime\n    df = pd.DataFrame({\"date\": [\n        datetime.fromisoformat(\"1654-01-01\"),\n        datetime.fromisoformat(\"1920-01-01\"), ],\n    })\n    write_feather(df, tempdir / \"test_resolution.feather\")\n\n    expected_0 = datetime.fromisoformat(\"1654-01-01\")\n    expected_1 = datetime.fromisoformat(\"1920-01-01\")\n\n    result = read_feather(tempdir / \"test_resolution.feather\",\n                          timestamp_as_object=True)\n\n    assert expected_0 == result['date'][0]\n    assert expected_1 == result['date'][1]\n", "python/pyarrow/tests/test_compute.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import namedtuple\nimport datetime\nimport decimal\nfrom functools import lru_cache, partial\nimport inspect\nimport itertools\nimport math\nimport os\nimport pytest\nimport random\nimport sys\nimport textwrap\n\nimport numpy as np\n\ntry:\n    import pandas as pd\nexcept ImportError:\n    pd = None\n\nimport pyarrow as pa\nimport pyarrow.compute as pc\nfrom pyarrow.lib import ArrowNotImplementedError\nfrom pyarrow.tests import util\n\ntry:\n    import pyarrow.substrait as pas\nexcept ImportError:\n    pas = None\n\nall_array_types = [\n    ('bool', [True, False, False, True, True]),\n    ('uint8', np.arange(5)),\n    ('int8', np.arange(5)),\n    ('uint16', np.arange(5)),\n    ('int16', np.arange(5)),\n    ('uint32', np.arange(5)),\n    ('int32', np.arange(5)),\n    ('uint64', np.arange(5, 10)),\n    ('int64', np.arange(5, 10)),\n    ('float', np.arange(0, 0.5, 0.1)),\n    ('double', np.arange(0, 0.5, 0.1)),\n    ('string', ['a', 'b', None, 'ddd', 'ee']),\n    ('binary', [b'a', b'b', b'c', b'ddd', b'ee']),\n    (pa.binary(3), [b'abc', b'bcd', b'cde', b'def', b'efg']),\n    (pa.list_(pa.int8()), [[1, 2], [3, 4], [5, 6], None, [9, 16]]),\n    (pa.large_list(pa.int16()), [[1], [2, 3, 4], [5, 6], None, [9, 16]]),\n    (pa.struct([('a', pa.int8()), ('b', pa.int8())]), [\n        {'a': 1, 'b': 2}, None, {'a': 3, 'b': 4}, None, {'a': 5, 'b': 6}]),\n]\n\nexported_functions = [\n    func for (name, func) in sorted(pc.__dict__.items())\n    if hasattr(func, '__arrow_compute_function__')]\n\nexported_option_classes = [\n    cls for (name, cls) in sorted(pc.__dict__.items())\n    if (isinstance(cls, type) and\n        cls is not pc.FunctionOptions and\n        issubclass(cls, pc.FunctionOptions))]\n\nnumerical_arrow_types = [\n    pa.int8(),\n    pa.int16(),\n    pa.int64(),\n    pa.uint8(),\n    pa.uint16(),\n    pa.uint64(),\n    pa.float32(),\n    pa.float64()\n]\n\n\ndef test_exported_functions():\n    # Check that all exported concrete functions can be called with\n    # the right number of arguments.\n    # Note that unregistered functions (e.g. with a mismatching name)\n    # will raise KeyError.\n    functions = exported_functions\n    assert len(functions) >= 10\n    for func in functions:\n        desc = func.__arrow_compute_function__\n        if desc['options_required']:\n            # Skip this function as it will fail with a different error\n            # message if we don't pass an options instance.\n            continue\n        arity = desc['arity']\n        if arity == 0:\n            continue\n        if arity is Ellipsis:\n            args = [object()] * 3\n        else:\n            args = [object()] * arity\n        with pytest.raises(TypeError,\n                           match=\"Got unexpected argument type \"\n                                 \"<class 'object'> for compute function\"):\n            func(*args)\n\n\ndef test_hash_aggregate_not_exported():\n    # Ensure we are not leaking hash aggregate functions\n    # which are not callable by themselves.\n    for func in exported_functions:\n        arrow_f = pc.get_function(func.__arrow_compute_function__[\"name\"])\n        assert arrow_f.kind != \"hash_aggregate\"\n\n\ndef test_exported_option_classes():\n    classes = exported_option_classes\n    assert len(classes) >= 10\n    for cls in classes:\n        # Option classes must have an introspectable constructor signature,\n        # and that signature should not have any *args or **kwargs.\n        sig = inspect.signature(cls)\n        for param in sig.parameters.values():\n            assert param.kind not in (param.VAR_POSITIONAL,\n                                      param.VAR_KEYWORD)\n\n\n@pytest.mark.filterwarnings(\n    \"ignore:pyarrow.CumulativeSumOptions is deprecated as of 14.0\"\n)\ndef test_option_class_equality():\n    options = [\n        pc.ArraySortOptions(),\n        pc.AssumeTimezoneOptions(\"UTC\"),\n        pc.CastOptions.safe(pa.int8()),\n        pc.CountOptions(),\n        pc.DayOfWeekOptions(count_from_zero=False, week_start=0),\n        pc.DictionaryEncodeOptions(),\n        pc.RunEndEncodeOptions(),\n        pc.ElementWiseAggregateOptions(skip_nulls=True),\n        pc.ExtractRegexOptions(\"pattern\"),\n        pc.FilterOptions(),\n        pc.IndexOptions(pa.scalar(1)),\n        pc.JoinOptions(),\n        pc.ListSliceOptions(0, -1, 1, True),\n        pc.ListFlattenOptions(recursive=False),\n        pc.MakeStructOptions([\"field\", \"names\"],\n                             field_nullability=[True, True],\n                             field_metadata=[pa.KeyValueMetadata({\"a\": \"1\"}),\n                                             pa.KeyValueMetadata({\"b\": \"2\"})]),\n        pc.MapLookupOptions(pa.scalar(1), \"first\"),\n        pc.MatchSubstringOptions(\"pattern\"),\n        pc.ModeOptions(),\n        pc.NullOptions(),\n        pc.PadOptions(5),\n        pc.PairwiseOptions(period=1),\n        pc.PartitionNthOptions(1, null_placement=\"at_start\"),\n        pc.CumulativeOptions(start=None, skip_nulls=False),\n        pc.QuantileOptions(),\n        pc.RandomOptions(),\n        pc.RankOptions(sort_keys=\"ascending\",\n                       null_placement=\"at_start\", tiebreaker=\"max\"),\n        pc.ReplaceSliceOptions(0, 1, \"a\"),\n        pc.ReplaceSubstringOptions(\"a\", \"b\"),\n        pc.RoundOptions(2, \"towards_infinity\"),\n        pc.RoundBinaryOptions(\"towards_infinity\"),\n        pc.RoundTemporalOptions(1, \"second\", week_starts_monday=True),\n        pc.RoundToMultipleOptions(100, \"towards_infinity\"),\n        pc.ScalarAggregateOptions(),\n        pc.SelectKOptions(0, sort_keys=[(\"b\", \"ascending\")]),\n        pc.SetLookupOptions(pa.array([1])),\n        pc.SliceOptions(0, 1, 1),\n        pc.SortOptions([(\"dummy\", \"descending\")], null_placement=\"at_start\"),\n        pc.SplitOptions(),\n        pc.SplitPatternOptions(\"pattern\"),\n        pc.StrftimeOptions(),\n        pc.StrptimeOptions(\"%Y\", \"s\", True),\n        pc.StructFieldOptions(indices=[]),\n        pc.TakeOptions(),\n        pc.TDigestOptions(),\n        pc.TrimOptions(\" \"),\n        pc.Utf8NormalizeOptions(\"NFKC\"),\n        pc.VarianceOptions(),\n        pc.WeekOptions(week_starts_monday=True, count_from_zero=False,\n                       first_week_is_fully_in_year=False),\n    ]\n    # Timezone database might not be installed on Windows\n    if sys.platform != \"win32\" or util.windows_has_tzdata():\n        options.append(pc.AssumeTimezoneOptions(\"Europe/Ljubljana\"))\n\n    classes = {type(option) for option in options}\n\n    for cls in exported_option_classes:\n        # Timezone database might not be installed on Windows\n        if (\n            cls not in classes\n            and (sys.platform != \"win32\" or util.windows_has_tzdata())\n            and cls != pc.AssumeTimezoneOptions\n        ):\n            try:\n                options.append(cls())\n            except TypeError:\n                pytest.fail(f\"Options class is not tested: {cls}\")\n\n    for option in options:\n        assert option == option\n        assert repr(option).startswith(option.__class__.__name__)\n        buf = option.serialize()\n        deserialized = pc.FunctionOptions.deserialize(buf)\n        assert option == deserialized\n        # TODO remove the check under the if statement and the filterwarnings\n        # mark when the deprecated class CumulativeSumOptions is removed.\n        if repr(option).startswith(\"CumulativeSumOptions\"):\n            assert repr(deserialized).startswith(\"CumulativeOptions\")\n        else:\n            assert repr(option) == repr(deserialized)\n    for option1, option2 in zip(options, options[1:]):\n        assert option1 != option2\n\n    assert repr(pc.IndexOptions(pa.scalar(1))) == \"IndexOptions(value=int64:1)\"\n    assert repr(pc.ArraySortOptions()) == \\\n        \"ArraySortOptions(order=Ascending, null_placement=AtEnd)\"\n\n\ndef test_list_functions():\n    assert len(pc.list_functions()) > 10\n    assert \"add\" in pc.list_functions()\n\n\ndef _check_get_function(name, expected_func_cls, expected_ker_cls,\n                        min_num_kernels=1):\n    func = pc.get_function(name)\n    assert isinstance(func, expected_func_cls)\n    n = func.num_kernels\n    assert n >= min_num_kernels\n    assert n == len(func.kernels)\n    assert all(isinstance(ker, expected_ker_cls) for ker in func.kernels)\n\n\ndef test_get_function_scalar():\n    _check_get_function(\"add\", pc.ScalarFunction, pc.ScalarKernel, 8)\n\n\ndef test_get_function_vector():\n    _check_get_function(\"unique\", pc.VectorFunction, pc.VectorKernel, 8)\n\n\ndef test_get_function_scalar_aggregate():\n    _check_get_function(\"mean\", pc.ScalarAggregateFunction,\n                        pc.ScalarAggregateKernel, 8)\n\n\ndef test_get_function_hash_aggregate():\n    _check_get_function(\"hash_sum\", pc.HashAggregateFunction,\n                        pc.HashAggregateKernel, 1)\n\n\ndef test_call_function_with_memory_pool():\n    arr = pa.array([\"foo\", \"bar\", \"baz\"])\n    indices = np.array([2, 2, 1])\n    result1 = arr.take(indices)\n    result2 = pc.call_function('take', [arr, indices],\n                               memory_pool=pa.default_memory_pool())\n    expected = pa.array([\"baz\", \"baz\", \"bar\"])\n    assert result1.equals(expected)\n    assert result2.equals(expected)\n\n    result3 = pc.take(arr, indices, memory_pool=pa.default_memory_pool())\n    assert result3.equals(expected)\n\n\ndef test_pickle_functions(pickle_module):\n    # Pickle registered functions\n    for name in pc.list_functions():\n        func = pc.get_function(name)\n        reconstructed = pickle_module.loads(pickle_module.dumps(func))\n        assert type(reconstructed) is type(func)\n        assert reconstructed.name == func.name\n        assert reconstructed.arity == func.arity\n        assert reconstructed.num_kernels == func.num_kernels\n\n\ndef test_pickle_global_functions(pickle_module):\n    # Pickle global wrappers (manual or automatic) of registered functions\n    for name in pc.list_functions():\n        try:\n            func = getattr(pc, name)\n        except AttributeError:\n            # hash_aggregate functions are not exported as callables.\n            continue\n        reconstructed = pickle_module.loads(pickle_module.dumps(func))\n        assert reconstructed is func\n\n\ndef test_function_attributes():\n    # Sanity check attributes of registered functions\n    for name in pc.list_functions():\n        func = pc.get_function(name)\n        assert isinstance(func, pc.Function)\n        assert func.name == name\n        kernels = func.kernels\n        assert func.num_kernels == len(kernels)\n        assert all(isinstance(ker, pc.Kernel) for ker in kernels)\n        repr(func)\n        for ker in kernels:\n            repr(ker)\n\n\ndef test_input_type_conversion():\n    # Automatic array conversion from Python\n    arr = pc.add([1, 2], [4, None])\n    assert arr.to_pylist() == [5, None]\n    # Automatic scalar conversion from Python\n    arr = pc.add([1, 2], 4)\n    assert arr.to_pylist() == [5, 6]\n    # Other scalar type\n    assert pc.equal([\"foo\", \"bar\", None],\n                    \"foo\").to_pylist() == [True, False, None]\n\n\n@pytest.mark.parametrize('arrow_type', numerical_arrow_types)\ndef test_sum_array(arrow_type):\n    arr = pa.array([1, 2, 3, 4], type=arrow_type)\n    assert arr.sum().as_py() == 10\n    assert pc.sum(arr).as_py() == 10\n\n    arr = pa.array([1, 2, 3, 4, None], type=arrow_type)\n    assert arr.sum().as_py() == 10\n    assert pc.sum(arr).as_py() == 10\n\n    arr = pa.array([None], type=arrow_type)\n    assert arr.sum().as_py() is None  # noqa: E711\n    assert pc.sum(arr).as_py() is None  # noqa: E711\n    assert arr.sum(min_count=0).as_py() == 0\n    assert pc.sum(arr, min_count=0).as_py() == 0\n\n    arr = pa.array([], type=arrow_type)\n    assert arr.sum().as_py() is None  # noqa: E711\n    assert arr.sum(min_count=0).as_py() == 0\n    assert pc.sum(arr, min_count=0).as_py() == 0\n\n\n@pytest.mark.parametrize('arrow_type', numerical_arrow_types)\ndef test_sum_chunked_array(arrow_type):\n    arr = pa.chunked_array([pa.array([1, 2, 3, 4], type=arrow_type)])\n    assert pc.sum(arr).as_py() == 10\n\n    arr = pa.chunked_array([\n        pa.array([1, 2], type=arrow_type), pa.array([3, 4], type=arrow_type)\n    ])\n    assert pc.sum(arr).as_py() == 10\n\n    arr = pa.chunked_array([\n        pa.array([1, 2], type=arrow_type),\n        pa.array([], type=arrow_type),\n        pa.array([3, 4], type=arrow_type)\n    ])\n    assert pc.sum(arr).as_py() == 10\n\n    arr = pa.chunked_array((), type=arrow_type)\n    assert arr.num_chunks == 0\n    assert pc.sum(arr).as_py() is None  # noqa: E711\n    assert pc.sum(arr, min_count=0).as_py() == 0\n\n\ndef test_mode_array():\n    # ARROW-9917\n    arr = pa.array([1, 1, 3, 4, 3, 5], type='int64')\n    mode = pc.mode(arr)\n    assert len(mode) == 1\n    assert mode[0].as_py() == {\"mode\": 1, \"count\": 2}\n\n    mode = pc.mode(arr, n=2)\n    assert len(mode) == 2\n    assert mode[0].as_py() == {\"mode\": 1, \"count\": 2}\n    assert mode[1].as_py() == {\"mode\": 3, \"count\": 2}\n\n    arr = pa.array([], type='int64')\n    assert len(pc.mode(arr)) == 0\n\n    arr = pa.array([1, 1, 3, 4, 3, None], type='int64')\n    mode = pc.mode(arr, skip_nulls=False)\n    assert len(mode) == 0\n    mode = pc.mode(arr, min_count=6)\n    assert len(mode) == 0\n    mode = pc.mode(arr, skip_nulls=False, min_count=5)\n    assert len(mode) == 0\n\n    arr = pa.array([True, False])\n    mode = pc.mode(arr, n=2)\n    assert len(mode) == 2\n    assert mode[0].as_py() == {\"mode\": False, \"count\": 1}\n    assert mode[1].as_py() == {\"mode\": True, \"count\": 1}\n\n\ndef test_mode_chunked_array():\n    # ARROW-9917\n    arr = pa.chunked_array([pa.array([1, 1, 3, 4, 3, 5], type='int64')])\n    mode = pc.mode(arr)\n    assert len(mode) == 1\n    assert mode[0].as_py() == {\"mode\": 1, \"count\": 2}\n\n    mode = pc.mode(arr, n=2)\n    assert len(mode) == 2\n    assert mode[0].as_py() == {\"mode\": 1, \"count\": 2}\n    assert mode[1].as_py() == {\"mode\": 3, \"count\": 2}\n\n    arr = pa.chunked_array((), type='int64')\n    assert arr.num_chunks == 0\n    assert len(pc.mode(arr)) == 0\n\n\ndef test_empty_chunked_array():\n    msg = \"cannot construct ChunkedArray from empty vector and omitted type\"\n    with pytest.raises(pa.ArrowInvalid, match=msg):\n        pa.chunked_array([])\n\n    pa.chunked_array([], type=pa.int8())\n\n\ndef test_variance():\n    data = [1, 2, 3, 4, 5, 6, 7, 8]\n    assert pc.variance(data).as_py() == 5.25\n    assert pc.variance(data, ddof=0).as_py() == 5.25\n    assert pc.variance(data, ddof=1).as_py() == 6.0\n\n\ndef test_count_substring():\n    for (ty, offset) in [(pa.string(), pa.int32()),\n                         (pa.large_string(), pa.int64())]:\n        arr = pa.array([\"ab\", \"cab\", \"abcab\", \"ba\", \"AB\", None], type=ty)\n\n        result = pc.count_substring(arr, \"ab\")\n        expected = pa.array([1, 1, 2, 0, 0, None], type=offset)\n        assert expected == result\n\n        result = pc.count_substring(arr, \"ab\", ignore_case=True)\n        expected = pa.array([1, 1, 2, 0, 1, None], type=offset)\n        assert expected == result\n\n\ndef test_count_substring_regex():\n    for (ty, offset) in [(pa.string(), pa.int32()),\n                         (pa.large_string(), pa.int64())]:\n        arr = pa.array([\"ab\", \"cab\", \"baAacaa\", \"ba\", \"AB\", None], type=ty)\n\n        result = pc.count_substring_regex(arr, \"a+\")\n        expected = pa.array([1, 1, 3, 1, 0, None], type=offset)\n        assert expected.equals(result)\n\n        result = pc.count_substring_regex(arr, \"a+\", ignore_case=True)\n        expected = pa.array([1, 1, 2, 1, 1, None], type=offset)\n        assert expected.equals(result)\n\n\ndef test_find_substring():\n    for ty in [pa.string(), pa.binary(), pa.large_string(), pa.large_binary()]:\n        arr = pa.array([\"ab\", \"cab\", \"ba\", None], type=ty)\n        result = pc.find_substring(arr, \"ab\")\n        assert result.to_pylist() == [0, 1, -1, None]\n\n        result = pc.find_substring_regex(arr, \"a?b\")\n        assert result.to_pylist() == [0, 1, 0, None]\n\n        arr = pa.array([\"ab*\", \"cAB*\", \"ba\", \"aB?\"], type=ty)\n        result = pc.find_substring(arr, \"aB*\", ignore_case=True)\n        assert result.to_pylist() == [0, 1, -1, -1]\n\n        result = pc.find_substring_regex(arr, \"a?b\", ignore_case=True)\n        assert result.to_pylist() == [0, 1, 0, 0]\n\n\ndef test_match_like():\n    arr = pa.array([\"ab\", \"ba%\", \"ba\", \"ca%d\", None])\n    result = pc.match_like(arr, r\"_a\\%%\")\n    expected = pa.array([False, True, False, True, None])\n    assert expected.equals(result)\n\n    arr = pa.array([\"aB\", \"bA%\", \"ba\", \"ca%d\", None])\n    result = pc.match_like(arr, r\"_a\\%%\", ignore_case=True)\n    expected = pa.array([False, True, False, True, None])\n    assert expected.equals(result)\n    result = pc.match_like(arr, r\"_a\\%%\", ignore_case=False)\n    expected = pa.array([False, False, False, True, None])\n    assert expected.equals(result)\n\n\ndef test_match_substring():\n    arr = pa.array([\"ab\", \"abc\", \"ba\", None])\n    result = pc.match_substring(arr, \"ab\")\n    expected = pa.array([True, True, False, None])\n    assert expected.equals(result)\n\n    arr = pa.array([\"\u00e1B\", \"\u00c1bc\", \"ba\", None])\n    result = pc.match_substring(arr, \"\u00e1b\", ignore_case=True)\n    expected = pa.array([True, True, False, None])\n    assert expected.equals(result)\n    result = pc.match_substring(arr, \"\u00e1b\", ignore_case=False)\n    expected = pa.array([False, False, False, None])\n    assert expected.equals(result)\n\n\ndef test_match_substring_regex():\n    arr = pa.array([\"ab\", \"abc\", \"ba\", \"c\", None])\n    result = pc.match_substring_regex(arr, \"^a?b\")\n    expected = pa.array([True, True, True, False, None])\n    assert expected.equals(result)\n\n    arr = pa.array([\"aB\", \"Abc\", \"BA\", \"c\", None])\n    result = pc.match_substring_regex(arr, \"^a?b\", ignore_case=True)\n    expected = pa.array([True, True, True, False, None])\n    assert expected.equals(result)\n    result = pc.match_substring_regex(arr, \"^a?b\", ignore_case=False)\n    expected = pa.array([False, False, False, False, None])\n    assert expected.equals(result)\n\n\ndef test_trim():\n    # \\u3000 is unicode whitespace\n    arr = pa.array([\" foo\", None, \" \\u3000foo bar \\t\"])\n    result = pc.utf8_trim_whitespace(arr)\n    expected = pa.array([\"foo\", None, \"foo bar\"])\n    assert expected.equals(result)\n\n    arr = pa.array([\" foo\", None, \" \\u3000foo bar \\t\"])\n    result = pc.ascii_trim_whitespace(arr)\n    expected = pa.array([\"foo\", None, \"\\u3000foo bar\"])\n    assert expected.equals(result)\n\n    arr = pa.array([\" foo\", None, \" \\u3000foo bar \\t\"])\n    result = pc.utf8_trim(arr, characters=' f\\u3000')\n    expected = pa.array([\"oo\", None, \"oo bar \\t\"])\n    assert expected.equals(result)\n    # Positional option\n    result = pc.utf8_trim(arr, ' f\\u3000')\n    expected = pa.array([\"oo\", None, \"oo bar \\t\"])\n    assert expected.equals(result)\n\n\ndef test_slice_compatibility():\n    arr = pa.array([\"\", \"\ud835\udc53\", \"\ud835\udc53\u00f6\", \"\ud835\udc53\u00f6\u00f5\", \"\ud835\udc53\u00f6\u00f5\u1e0d\", \"\ud835\udc53\u00f6\u00f5\u1e0d\u0161\"])\n    for start in range(-6, 6):\n        for stop in itertools.chain(range(-6, 6), [None]):\n            for step in [-3, -2, -1, 1, 2, 3]:\n                expected = pa.array([k.as_py()[start:stop:step]\n                                     for k in arr])\n                result = pc.utf8_slice_codeunits(\n                    arr, start=start, stop=stop, step=step)\n                assert expected.equals(result)\n                # Positional options\n                assert pc.utf8_slice_codeunits(arr,\n                                               start, stop, step) == result\n\n\ndef test_binary_slice_compatibility():\n    data = [b\"\", b\"a\", b\"a\\xff\", b\"ab\\x00\", b\"abc\\xfb\", b\"ab\\xf2de\"]\n    arr = pa.array(data)\n    for start, stop, step in itertools.product(range(-6, 6),\n                                               range(-6, 6),\n                                               range(-3, 4)):\n        if step == 0:\n            continue\n        expected = pa.array([k.as_py()[start:stop:step]\n                             for k in arr])\n        result = pc.binary_slice(\n            arr, start=start, stop=stop, step=step)\n        assert expected.equals(result)\n        # Positional options\n        assert pc.binary_slice(arr, start, stop, step) == result\n        # Fixed size binary input / output\n        for item in data:\n            fsb_scalar = pa.scalar(item, type=pa.binary(len(item)))\n            expected = item[start:stop:step]\n            actual = pc.binary_slice(fsb_scalar, start, stop, step)\n            assert actual.type == pa.binary(len(expected))\n            assert actual.as_py() == expected\n\n\ndef test_split_pattern():\n    arr = pa.array([\"-foo---bar--\", \"---foo---b\"])\n    result = pc.split_pattern(arr, pattern=\"---\")\n    expected = pa.array([[\"-foo\", \"bar--\"], [\"\", \"foo\", \"b\"]])\n    assert expected.equals(result)\n\n    result = pc.split_pattern(arr, \"---\", max_splits=1)\n    expected = pa.array([[\"-foo\", \"bar--\"], [\"\", \"foo---b\"]])\n    assert expected.equals(result)\n\n    result = pc.split_pattern(arr, \"---\", max_splits=1, reverse=True)\n    expected = pa.array([[\"-foo\", \"bar--\"], [\"---foo\", \"b\"]])\n    assert expected.equals(result)\n\n\ndef test_split_whitespace_utf8():\n    arr = pa.array([\"foo bar\", \" foo  \\u3000\\tb\"])\n    result = pc.utf8_split_whitespace(arr)\n    expected = pa.array([[\"foo\", \"bar\"], [\"\", \"foo\", \"b\"]])\n    assert expected.equals(result)\n\n    result = pc.utf8_split_whitespace(arr, max_splits=1)\n    expected = pa.array([[\"foo\", \"bar\"], [\"\", \"foo  \\u3000\\tb\"]])\n    assert expected.equals(result)\n\n    result = pc.utf8_split_whitespace(arr, max_splits=1, reverse=True)\n    expected = pa.array([[\"foo\", \"bar\"], [\" foo\", \"b\"]])\n    assert expected.equals(result)\n\n\ndef test_split_whitespace_ascii():\n    arr = pa.array([\"foo bar\", \" foo  \\u3000\\tb\"])\n    result = pc.ascii_split_whitespace(arr)\n    expected = pa.array([[\"foo\", \"bar\"], [\"\", \"foo\", \"\\u3000\", \"b\"]])\n    assert expected.equals(result)\n\n    result = pc.ascii_split_whitespace(arr, max_splits=1)\n    expected = pa.array([[\"foo\", \"bar\"], [\"\", \"foo  \\u3000\\tb\"]])\n    assert expected.equals(result)\n\n    result = pc.ascii_split_whitespace(arr, max_splits=1, reverse=True)\n    expected = pa.array([[\"foo\", \"bar\"], [\" foo  \\u3000\", \"b\"]])\n    assert expected.equals(result)\n\n\ndef test_split_pattern_regex():\n    arr = pa.array([\"-foo---bar--\", \"---foo---b\"])\n    result = pc.split_pattern_regex(arr, pattern=\"-+\")\n    expected = pa.array([[\"\", \"foo\", \"bar\", \"\"], [\"\", \"foo\", \"b\"]])\n    assert expected.equals(result)\n\n    result = pc.split_pattern_regex(arr, \"-+\", max_splits=1)\n    expected = pa.array([[\"\", \"foo---bar--\"], [\"\", \"foo---b\"]])\n    assert expected.equals(result)\n\n    with pytest.raises(NotImplementedError,\n                       match=\"Cannot split in reverse with regex\"):\n        result = pc.split_pattern_regex(\n            arr, pattern=\"---\", max_splits=1, reverse=True)\n\n\ndef test_min_max():\n    # An example generated function wrapper with possible options\n    data = [4, 5, 6, None, 1]\n    s = pc.min_max(data)\n    assert s.as_py() == {'min': 1, 'max': 6}\n    s = pc.min_max(data, options=pc.ScalarAggregateOptions())\n    assert s.as_py() == {'min': 1, 'max': 6}\n    s = pc.min_max(data, options=pc.ScalarAggregateOptions(skip_nulls=True))\n    assert s.as_py() == {'min': 1, 'max': 6}\n    s = pc.min_max(data, options=pc.ScalarAggregateOptions(skip_nulls=False))\n    assert s.as_py() == {'min': None, 'max': None}\n\n    # Options as dict of kwargs\n    s = pc.min_max(data, options={'skip_nulls': False})\n    assert s.as_py() == {'min': None, 'max': None}\n    # Options as named functions arguments\n    s = pc.min_max(data, skip_nulls=False)\n    assert s.as_py() == {'min': None, 'max': None}\n\n    # Both options and named arguments\n    with pytest.raises(TypeError):\n        s = pc.min_max(\n            data, options=pc.ScalarAggregateOptions(), skip_nulls=False)\n\n    # Wrong options type\n    options = pc.TakeOptions()\n    with pytest.raises(TypeError):\n        s = pc.min_max(data, options=options)\n\n    # Missing argument\n    with pytest.raises(TypeError, match=\"min_max takes 1 positional\"):\n        s = pc.min_max()\n\n\ndef test_any():\n    # ARROW-1846\n\n    options = pc.ScalarAggregateOptions(skip_nulls=False, min_count=0)\n\n    a = pa.array([], type='bool')\n    assert pc.any(a).as_py() is None\n    assert pc.any(a, min_count=0).as_py() is False\n    assert pc.any(a, options=options).as_py() is False\n\n    a = pa.array([False, None, True])\n    assert pc.any(a).as_py() is True\n    assert pc.any(a, options=options).as_py() is True\n\n    a = pa.array([False, None, False])\n    assert pc.any(a).as_py() is False\n    assert pc.any(a, options=options).as_py() is None\n\n\ndef test_all():\n    # ARROW-10301\n\n    options = pc.ScalarAggregateOptions(skip_nulls=False, min_count=0)\n\n    a = pa.array([], type='bool')\n    assert pc.all(a).as_py() is None\n    assert pc.all(a, min_count=0).as_py() is True\n    assert pc.all(a, options=options).as_py() is True\n\n    a = pa.array([False, True])\n    assert pc.all(a).as_py() is False\n    assert pc.all(a, options=options).as_py() is False\n\n    a = pa.array([True, None])\n    assert pc.all(a).as_py() is True\n    assert pc.all(a, options=options).as_py() is None\n\n    a = pa.chunked_array([[True], [True, None]])\n    assert pc.all(a).as_py() is True\n    assert pc.all(a, options=options).as_py() is None\n\n    a = pa.chunked_array([[True], [False]])\n    assert pc.all(a).as_py() is False\n    assert pc.all(a, options=options).as_py() is False\n\n\ndef test_is_valid():\n    # An example generated function wrapper without options\n    data = [4, 5, None]\n    assert pc.is_valid(data).to_pylist() == [True, True, False]\n\n    with pytest.raises(TypeError):\n        pc.is_valid(data, options=None)\n\n\ndef test_generated_docstrings():\n    # With options\n    assert pc.min_max.__doc__ == textwrap.dedent(\"\"\"\\\n        Compute the minimum and maximum values of a numeric array.\n\n        Null values are ignored by default.\n        This can be changed through ScalarAggregateOptions.\n\n        Parameters\n        ----------\n        array : Array-like\n            Argument to compute function.\n        skip_nulls : bool, default True\n            Whether to skip (ignore) nulls in the input.\n            If False, any null in the input forces the output to null.\n        min_count : int, default 1\n            Minimum number of non-null values in the input.  If the number\n            of non-null values is below `min_count`, the output is null.\n        options : pyarrow.compute.ScalarAggregateOptions, optional\n            Alternative way of passing options.\n        memory_pool : pyarrow.MemoryPool, optional\n            If not passed, will allocate memory from the default memory pool.\n        \"\"\")\n    # Without options\n    assert pc.add.__doc__ == textwrap.dedent(\"\"\"\\\n        Add the arguments element-wise.\n\n        Results will wrap around on integer overflow.\n        Use function \"add_checked\" if you want overflow\n        to return an error.\n\n        Parameters\n        ----------\n        x : Array-like or scalar-like\n            Argument to compute function.\n        y : Array-like or scalar-like\n            Argument to compute function.\n        memory_pool : pyarrow.MemoryPool, optional\n            If not passed, will allocate memory from the default memory pool.\n        \"\"\")\n    # Varargs with options\n    assert pc.min_element_wise.__doc__ == textwrap.dedent(\"\"\"\\\n        Find the element-wise minimum value.\n\n        Nulls are ignored (by default) or propagated.\n        NaN is preferred over null, but not over any valid value.\n\n        Parameters\n        ----------\n        *args : Array-like or scalar-like\n            Argument to compute function.\n        skip_nulls : bool, default True\n            Whether to skip (ignore) nulls in the input.\n            If False, any null in the input forces the output to null.\n        options : pyarrow.compute.ElementWiseAggregateOptions, optional\n            Alternative way of passing options.\n        memory_pool : pyarrow.MemoryPool, optional\n            If not passed, will allocate memory from the default memory pool.\n        \"\"\")\n    assert pc.filter.__doc__ == textwrap.dedent(\"\"\"\\\n        Filter with a boolean selection filter.\n\n        The output is populated with values from the input at positions\n        where the selection filter is non-zero.  Nulls in the selection filter\n        are handled based on FilterOptions.\n\n        Parameters\n        ----------\n        input : Array-like or scalar-like\n            Argument to compute function.\n        selection_filter : Array-like or scalar-like\n            Argument to compute function.\n        null_selection_behavior : str, default \"drop\"\n            How to handle nulls in the selection filter.\n            Accepted values are \"drop\", \"emit_null\".\n        options : pyarrow.compute.FilterOptions, optional\n            Alternative way of passing options.\n        memory_pool : pyarrow.MemoryPool, optional\n            If not passed, will allocate memory from the default memory pool.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> arr = pa.array([\"a\", \"b\", \"c\", None, \"e\"])\n        >>> mask = pa.array([True, False, None, False, True])\n        >>> arr.filter(mask)\n        <pyarrow.lib.StringArray object at ...>\n        [\n          \"a\",\n          \"e\"\n        ]\n        >>> arr.filter(mask, null_selection_behavior='emit_null')\n        <pyarrow.lib.StringArray object at ...>\n        [\n          \"a\",\n          null,\n          \"e\"\n        ]\n        \"\"\")\n\n\ndef test_generated_signatures():\n    # The self-documentation provided by signatures should show acceptable\n    # options and their default values.\n\n    # Without options\n    sig = inspect.signature(pc.add)\n    assert str(sig) == \"(x, y, /, *, memory_pool=None)\"\n    # With options\n    sig = inspect.signature(pc.min_max)\n    assert str(sig) == (\"(array, /, *, skip_nulls=True, min_count=1, \"\n                        \"options=None, memory_pool=None)\")\n    # With positional options\n    sig = inspect.signature(pc.quantile)\n    assert str(sig) == (\"(array, /, q=0.5, *, interpolation='linear', \"\n                        \"skip_nulls=True, min_count=0, \"\n                        \"options=None, memory_pool=None)\")\n    # Varargs with options\n    sig = inspect.signature(pc.binary_join_element_wise)\n    assert str(sig) == (\"(*strings, null_handling='emit_null', \"\n                        \"null_replacement='', options=None, \"\n                        \"memory_pool=None)\")\n    # Varargs without options\n    sig = inspect.signature(pc.choose)\n    assert str(sig) == \"(indices, /, *values, memory_pool=None)\"\n    # Nullary with options\n    sig = inspect.signature(pc.random)\n    assert str(sig) == (\"(n, *, initializer='system', \"\n                        \"options=None, memory_pool=None)\")\n\n\n# We use isprintable to find about codepoints that Python doesn't know, but\n# utf8proc does (or in a future version of Python the other way around).\n# These codepoints cannot be compared between Arrow and the Python\n# implementation.\n@lru_cache()\ndef find_new_unicode_codepoints():\n    new = set()\n    characters = [chr(c) for c in range(0x80, 0x11000)\n                  if not (0xD800 <= c < 0xE000)]\n    is_printable = pc.utf8_is_printable(pa.array(characters)).to_pylist()\n    for i, c in enumerate(characters):\n        if is_printable[i] != c.isprintable():\n            new.add(ord(c))\n    return new\n\n\n# Python claims there are not alpha, not sure why, they are in\n#  gc='Other Letter': https://graphemica.com/%E1%B3%B2\nunknown_issue_is_alpha = {0x1cf2, 0x1cf3}\n# utf8proc does not know if codepoints are lower case\nutf8proc_issue_is_lower = {\n    0xaa, 0xba, 0x2b0, 0x2b1, 0x2b2, 0x2b3, 0x2b4,\n    0x2b5, 0x2b6, 0x2b7, 0x2b8, 0x2c0, 0x2c1, 0x2e0,\n    0x2e1, 0x2e2, 0x2e3, 0x2e4, 0x37a, 0x1d2c, 0x1d2d,\n    0x1d2e, 0x1d2f, 0x1d30, 0x1d31, 0x1d32, 0x1d33,\n    0x1d34, 0x1d35, 0x1d36, 0x1d37, 0x1d38, 0x1d39,\n    0x1d3a, 0x1d3b, 0x1d3c, 0x1d3d, 0x1d3e, 0x1d3f,\n    0x1d40, 0x1d41, 0x1d42, 0x1d43, 0x1d44, 0x1d45,\n    0x1d46, 0x1d47, 0x1d48, 0x1d49, 0x1d4a, 0x1d4b,\n    0x1d4c, 0x1d4d, 0x1d4e, 0x1d4f, 0x1d50, 0x1d51,\n    0x1d52, 0x1d53, 0x1d54, 0x1d55, 0x1d56, 0x1d57,\n    0x1d58, 0x1d59, 0x1d5a, 0x1d5b, 0x1d5c, 0x1d5d,\n    0x1d5e, 0x1d5f, 0x1d60, 0x1d61, 0x1d62, 0x1d63,\n    0x1d64, 0x1d65, 0x1d66, 0x1d67, 0x1d68, 0x1d69,\n    0x1d6a, 0x1d78, 0x1d9b, 0x1d9c, 0x1d9d, 0x1d9e,\n    0x1d9f, 0x1da0, 0x1da1, 0x1da2, 0x1da3, 0x1da4,\n    0x1da5, 0x1da6, 0x1da7, 0x1da8, 0x1da9, 0x1daa,\n    0x1dab, 0x1dac, 0x1dad, 0x1dae, 0x1daf, 0x1db0,\n    0x1db1, 0x1db2, 0x1db3, 0x1db4, 0x1db5, 0x1db6,\n    0x1db7, 0x1db8, 0x1db9, 0x1dba, 0x1dbb, 0x1dbc,\n    0x1dbd, 0x1dbe, 0x1dbf, 0x2071, 0x207f, 0x2090,\n    0x2091, 0x2092, 0x2093, 0x2094, 0x2095, 0x2096,\n    0x2097, 0x2098, 0x2099, 0x209a, 0x209b, 0x209c,\n    0x2c7c, 0x2c7d, 0xa69c, 0xa69d, 0xa770, 0xa7f8,\n    0xa7f9, 0xab5c, 0xab5d, 0xab5e, 0xab5f, }\n# utf8proc does not store if a codepoint is numeric\nnumeric_info_missing = {\n    0x3405, 0x3483, 0x382a, 0x3b4d, 0x4e00, 0x4e03,\n    0x4e07, 0x4e09, 0x4e5d, 0x4e8c, 0x4e94, 0x4e96,\n    0x4ebf, 0x4ec0, 0x4edf, 0x4ee8, 0x4f0d, 0x4f70,\n    0x5104, 0x5146, 0x5169, 0x516b, 0x516d, 0x5341,\n    0x5343, 0x5344, 0x5345, 0x534c, 0x53c1, 0x53c2,\n    0x53c3, 0x53c4, 0x56db, 0x58f1, 0x58f9, 0x5e7a,\n    0x5efe, 0x5eff, 0x5f0c, 0x5f0d, 0x5f0e, 0x5f10,\n    0x62fe, 0x634c, 0x67d2, 0x6f06, 0x7396, 0x767e,\n    0x8086, 0x842c, 0x8cae, 0x8cb3, 0x8d30, 0x9621,\n    0x9646, 0x964c, 0x9678, 0x96f6, 0xf96b, 0xf973,\n    0xf978, 0xf9b2, 0xf9d1, 0xf9d3, 0xf9fd, 0x10fc5,\n    0x10fc6, 0x10fc7, 0x10fc8, 0x10fc9, 0x10fca,\n    0x10fcb, }\n# utf8proc has no no digit/numeric information\ndigit_info_missing = {\n    0xb2, 0xb3, 0xb9, 0x1369, 0x136a, 0x136b, 0x136c,\n    0x136d, 0x136e, 0x136f, 0x1370, 0x1371, 0x19da, 0x2070,\n    0x2074, 0x2075, 0x2076, 0x2077, 0x2078, 0x2079, 0x2080,\n    0x2081, 0x2082, 0x2083, 0x2084, 0x2085, 0x2086, 0x2087,\n    0x2088, 0x2089, 0x2460, 0x2461, 0x2462, 0x2463, 0x2464,\n    0x2465, 0x2466, 0x2467, 0x2468, 0x2474, 0x2475, 0x2476,\n    0x2477, 0x2478, 0x2479, 0x247a, 0x247b, 0x247c, 0x2488,\n    0x2489, 0x248a, 0x248b, 0x248c, 0x248d, 0x248e, 0x248f,\n    0x2490, 0x24ea, 0x24f5, 0x24f6, 0x24f7, 0x24f8, 0x24f9,\n    0x24fa, 0x24fb, 0x24fc, 0x24fd, 0x24ff, 0x2776, 0x2777,\n    0x2778, 0x2779, 0x277a, 0x277b, 0x277c, 0x277d, 0x277e,\n    0x2780, 0x2781, 0x2782, 0x2783, 0x2784, 0x2785, 0x2786,\n    0x2787, 0x2788, 0x278a, 0x278b, 0x278c, 0x278d, 0x278e,\n    0x278f, 0x2790, 0x2791, 0x2792, 0x10a40, 0x10a41,\n    0x10a42, 0x10a43, 0x10e60, 0x10e61, 0x10e62, 0x10e63,\n    0x10e64, 0x10e65, 0x10e66, 0x10e67, 0x10e68, }\nnumeric_info_missing = {\n    0x3405, 0x3483, 0x382a, 0x3b4d, 0x4e00, 0x4e03,\n    0x4e07, 0x4e09, 0x4e5d, 0x4e8c, 0x4e94, 0x4e96,\n    0x4ebf, 0x4ec0, 0x4edf, 0x4ee8, 0x4f0d, 0x4f70,\n    0x5104, 0x5146, 0x5169, 0x516b, 0x516d, 0x5341,\n    0x5343, 0x5344, 0x5345, 0x534c, 0x53c1, 0x53c2,\n    0x53c3, 0x53c4, 0x56db, 0x58f1, 0x58f9, 0x5e7a,\n    0x5efe, 0x5eff, 0x5f0c, 0x5f0d, 0x5f0e, 0x5f10,\n    0x62fe, 0x634c, 0x67d2, 0x6f06, 0x7396, 0x767e,\n    0x8086, 0x842c, 0x8cae, 0x8cb3, 0x8d30, 0x9621,\n    0x9646, 0x964c, 0x9678, 0x96f6, 0xf96b, 0xf973,\n    0xf978, 0xf9b2, 0xf9d1, 0xf9d3, 0xf9fd, }\n\ncodepoints_ignore = {\n    'is_alnum': numeric_info_missing | digit_info_missing |\n    unknown_issue_is_alpha,\n    'is_alpha': unknown_issue_is_alpha,\n    'is_digit': digit_info_missing,\n    'is_numeric': numeric_info_missing,\n    'is_lower': utf8proc_issue_is_lower\n}\n\n\n@pytest.mark.parametrize('function_name', ['is_alnum', 'is_alpha',\n                                           'is_ascii', 'is_decimal',\n                                           'is_digit', 'is_lower',\n                                           'is_numeric', 'is_printable',\n                                           'is_space', 'is_upper', ])\n@pytest.mark.parametrize('variant', ['ascii', 'utf8'])\ndef test_string_py_compat_boolean(function_name, variant):\n    arrow_name = variant + \"_\" + function_name\n    py_name = function_name.replace('_', '')\n    ignore = codepoints_ignore.get(function_name, set()) | \\\n        find_new_unicode_codepoints()\n    for i in range(128 if ascii else 0x11000):\n        if i in range(0xD800, 0xE000):\n            continue  # bug? pyarrow doesn't allow utf16 surrogates\n        # the issues we know of, we skip\n        if i in ignore:\n            continue\n        # Compare results with the equivalent Python predicate\n        # (except \"is_space\" where functions are known to be incompatible)\n        c = chr(i)\n        if hasattr(pc, arrow_name) and function_name != 'is_space':\n            ar = pa.array([c])\n            arrow_func = getattr(pc, arrow_name)\n            assert arrow_func(ar)[0].as_py() == getattr(c, py_name)()\n\n\ndef test_pad():\n    arr = pa.array([None, 'a', 'abcd'])\n    assert pc.ascii_center(arr, width=3).tolist() == [None, ' a ', 'abcd']\n    assert pc.ascii_lpad(arr, width=3).tolist() == [None, '  a', 'abcd']\n    assert pc.ascii_rpad(arr, width=3).tolist() == [None, 'a  ', 'abcd']\n    assert pc.ascii_center(arr, 3).tolist() == [None, ' a ', 'abcd']\n    assert pc.ascii_lpad(arr, 3).tolist() == [None, '  a', 'abcd']\n    assert pc.ascii_rpad(arr, 3).tolist() == [None, 'a  ', 'abcd']\n\n    arr = pa.array([None, '\u00e1', 'abcd'])\n    assert pc.utf8_center(arr, width=3).tolist() == [None, ' \u00e1 ', 'abcd']\n    assert pc.utf8_lpad(arr, width=3).tolist() == [None, '  \u00e1', 'abcd']\n    assert pc.utf8_rpad(arr, width=3).tolist() == [None, '\u00e1  ', 'abcd']\n    assert pc.utf8_center(arr, 3).tolist() == [None, ' \u00e1 ', 'abcd']\n    assert pc.utf8_lpad(arr, 3).tolist() == [None, '  \u00e1', 'abcd']\n    assert pc.utf8_rpad(arr, 3).tolist() == [None, '\u00e1  ', 'abcd']\n\n\n@pytest.mark.pandas\ndef test_replace_slice():\n    offsets = range(-3, 4)\n\n    arr = pa.array([None, '', 'a', 'ab', 'abc', 'abcd', 'abcde'])\n    series = arr.to_pandas()\n    for start in offsets:\n        for stop in offsets:\n            expected = series.str.slice_replace(start, stop, 'XX')\n            actual = pc.binary_replace_slice(\n                arr, start=start, stop=stop, replacement='XX')\n            assert actual.tolist() == expected.tolist()\n            # Positional options\n            assert pc.binary_replace_slice(arr, start, stop, 'XX') == actual\n\n    arr = pa.array([None, '', '\u03c0', '\u03c0b', '\u03c0b\u03b8', '\u03c0b\u03b8d', '\u03c0b\u03b8de'])\n    series = arr.to_pandas()\n    for start in offsets:\n        for stop in offsets:\n            expected = series.str.slice_replace(start, stop, 'XX')\n            actual = pc.utf8_replace_slice(\n                arr, start=start, stop=stop, replacement='XX')\n            assert actual.tolist() == expected.tolist()\n\n\ndef test_replace_plain():\n    data = pa.array(['foozfoo', 'food', None])\n    ar = pc.replace_substring(data, pattern='foo', replacement='bar')\n    assert ar.tolist() == ['barzbar', 'bard', None]\n    ar = pc.replace_substring(data, 'foo', 'bar')\n    assert ar.tolist() == ['barzbar', 'bard', None]\n\n    ar = pc.replace_substring(data, pattern='foo', replacement='bar',\n                              max_replacements=1)\n    assert ar.tolist() == ['barzfoo', 'bard', None]\n    ar = pc.replace_substring(data, 'foo', 'bar', max_replacements=1)\n    assert ar.tolist() == ['barzfoo', 'bard', None]\n\n\ndef test_replace_regex():\n    data = pa.array(['foo', 'mood', None])\n    expected = ['f00', 'm00d', None]\n    ar = pc.replace_substring_regex(data, pattern='(.)oo', replacement=r'\\100')\n    assert ar.tolist() == expected\n    ar = pc.replace_substring_regex(data, '(.)oo', replacement=r'\\100')\n    assert ar.tolist() == expected\n    ar = pc.replace_substring_regex(data, '(.)oo', r'\\100')\n    assert ar.tolist() == expected\n\n\ndef test_extract_regex():\n    ar = pa.array(['a1', 'zb2z'])\n    expected = [{'letter': 'a', 'digit': '1'}, {'letter': 'b', 'digit': '2'}]\n    struct = pc.extract_regex(ar, pattern=r'(?P<letter>[ab])(?P<digit>\\d)')\n    assert struct.tolist() == expected\n    struct = pc.extract_regex(ar, r'(?P<letter>[ab])(?P<digit>\\d)')\n    assert struct.tolist() == expected\n\n\ndef test_binary_join():\n    ar_list = pa.array([['foo', 'bar'], None, []])\n    expected = pa.array(['foo-bar', None, ''])\n    assert pc.binary_join(ar_list, '-').equals(expected)\n\n    separator_array = pa.array(['1', '2'], type=pa.binary())\n    expected = pa.array(['a1b', 'c2d'], type=pa.binary())\n    ar_list = pa.array([['a', 'b'], ['c', 'd']], type=pa.list_(pa.binary()))\n    assert pc.binary_join(ar_list, separator_array).equals(expected)\n\n\ndef test_binary_join_element_wise():\n    null = pa.scalar(None, type=pa.string())\n    arrs = [[None, 'a', 'b'], ['c', None, 'd'], [None, '-', '--']]\n    assert pc.binary_join_element_wise(*arrs).to_pylist() == \\\n        [None, None, 'b--d']\n    assert pc.binary_join_element_wise('a', 'b', '-').as_py() == 'a-b'\n    assert pc.binary_join_element_wise('a', null, '-').as_py() is None\n    assert pc.binary_join_element_wise('a', 'b', null).as_py() is None\n\n    skip = pc.JoinOptions(null_handling='skip')\n    assert pc.binary_join_element_wise(*arrs, options=skip).to_pylist() == \\\n        [None, 'a', 'b--d']\n    assert pc.binary_join_element_wise(\n        'a', 'b', '-', options=skip).as_py() == 'a-b'\n    assert pc.binary_join_element_wise(\n        'a', null, '-', options=skip).as_py() == 'a'\n    assert pc.binary_join_element_wise(\n        'a', 'b', null, options=skip).as_py() is None\n\n    replace = pc.JoinOptions(null_handling='replace', null_replacement='spam')\n    assert pc.binary_join_element_wise(*arrs, options=replace).to_pylist() == \\\n        [None, 'a-spam', 'b--d']\n    assert pc.binary_join_element_wise(\n        'a', 'b', '-', options=replace).as_py() == 'a-b'\n    assert pc.binary_join_element_wise(\n        'a', null, '-', options=replace).as_py() == 'a-spam'\n    assert pc.binary_join_element_wise(\n        'a', 'b', null, options=replace).as_py() is None\n\n\n@pytest.mark.parametrize(('ty', 'values'), all_array_types)\ndef test_take(ty, values):\n    arr = pa.array(values, type=ty)\n    for indices_type in [pa.int8(), pa.int64()]:\n        indices = pa.array([0, 4, 2, None], type=indices_type)\n        result = arr.take(indices)\n        result.validate()\n        expected = pa.array([values[0], values[4], values[2], None], type=ty)\n        assert result.equals(expected)\n\n        # empty indices\n        indices = pa.array([], type=indices_type)\n        result = arr.take(indices)\n        result.validate()\n        expected = pa.array([], type=ty)\n        assert result.equals(expected)\n\n    indices = pa.array([2, 5])\n    with pytest.raises(IndexError):\n        arr.take(indices)\n\n    indices = pa.array([2, -1])\n    with pytest.raises(IndexError):\n        arr.take(indices)\n\n\ndef test_take_indices_types():\n    arr = pa.array(range(5))\n\n    for indices_type in ['uint8', 'int8', 'uint16', 'int16',\n                         'uint32', 'int32', 'uint64', 'int64']:\n        indices = pa.array([0, 4, 2, None], type=indices_type)\n        result = arr.take(indices)\n        result.validate()\n        expected = pa.array([0, 4, 2, None])\n        assert result.equals(expected)\n\n    for indices_type in [pa.float32(), pa.float64()]:\n        indices = pa.array([0, 4, 2], type=indices_type)\n        with pytest.raises(NotImplementedError):\n            arr.take(indices)\n\n\ndef test_take_on_chunked_array():\n    # ARROW-9504\n    arr = pa.chunked_array([\n        [\n            \"a\",\n            \"b\",\n            \"c\",\n            \"d\",\n            \"e\"\n        ],\n        [\n            \"f\",\n            \"g\",\n            \"h\",\n            \"i\",\n            \"j\"\n        ]\n    ])\n\n    indices = np.array([0, 5, 1, 6, 9, 2])\n    result = arr.take(indices)\n    expected = pa.chunked_array([[\"a\", \"f\", \"b\", \"g\", \"j\", \"c\"]])\n    assert result.equals(expected)\n\n    indices = pa.chunked_array([[1], [9, 2]])\n    result = arr.take(indices)\n    expected = pa.chunked_array([\n        [\n            \"b\"\n        ],\n        [\n            \"j\",\n            \"c\"\n        ]\n    ])\n    assert result.equals(expected)\n\n\n@pytest.mark.parametrize('ordered', [False, True])\ndef test_take_dictionary(ordered):\n    arr = pa.DictionaryArray.from_arrays([0, 1, 2, 0, 1, 2], ['a', 'b', 'c'],\n                                         ordered=ordered)\n    result = arr.take(pa.array([0, 1, 3]))\n    result.validate()\n    assert result.to_pylist() == ['a', 'b', 'a']\n    assert result.dictionary.to_pylist() == ['a', 'b', 'c']\n    assert result.type.ordered is ordered\n\n\ndef test_take_null_type():\n    # ARROW-10027\n    arr = pa.array([None] * 10)\n    chunked_arr = pa.chunked_array([[None] * 5] * 2)\n    batch = pa.record_batch([arr], names=['a'])\n    table = pa.table({'a': arr})\n\n    indices = pa.array([1, 3, 7, None])\n    assert len(arr.take(indices)) == 4\n    assert len(chunked_arr.take(indices)) == 4\n    assert len(batch.take(indices).column(0)) == 4\n    assert len(table.take(indices).column(0)) == 4\n\n\n@pytest.mark.parametrize(('ty', 'values'), all_array_types)\ndef test_drop_null(ty, values):\n    arr = pa.array(values, type=ty)\n    result = arr.drop_null()\n    result.validate(full=True)\n    indices = [i for i in range(len(arr)) if arr[i].is_valid]\n    expected = arr.take(pa.array(indices))\n    assert result.equals(expected)\n\n\ndef test_drop_null_chunked_array():\n    arr = pa.chunked_array([[\"a\", None], [\"c\", \"d\", None], [None], []])\n    expected_drop = pa.chunked_array([[\"a\"], [\"c\", \"d\"], [], []])\n\n    result = arr.drop_null()\n    assert result.equals(expected_drop)\n\n\ndef test_drop_null_record_batch():\n    batch = pa.record_batch(\n        [pa.array([\"a\", None, \"c\", \"d\", None])], names=[\"a'\"])\n    result = batch.drop_null()\n    expected = pa.record_batch([pa.array([\"a\", \"c\", \"d\"])], names=[\"a'\"])\n    assert result.equals(expected)\n\n    batch = pa.record_batch(\n        [pa.array([\"a\", None, \"c\", \"d\", None]),\n         pa.array([None, None, \"c\", None, \"e\"])], names=[\"a'\", \"b'\"])\n\n    result = batch.drop_null()\n    expected = pa.record_batch(\n        [pa.array([\"c\"]), pa.array([\"c\"])], names=[\"a'\", \"b'\"])\n    assert result.equals(expected)\n\n\ndef test_drop_null_table():\n    table = pa.table([pa.array([\"a\", None, \"c\", \"d\", None])], names=[\"a\"])\n    expected = pa.table([pa.array([\"a\", \"c\", \"d\"])], names=[\"a\"])\n    result = table.drop_null()\n    assert result.equals(expected)\n\n    table = pa.table([pa.chunked_array([[\"a\", None], [\"c\", \"d\", None]]),\n                      pa.chunked_array([[\"a\", None], [None, \"d\", None]]),\n                      pa.chunked_array([[\"a\"], [\"b\"], [None], [\"d\", None]])],\n                     names=[\"a\", \"b\", \"c\"])\n    expected = pa.table([pa.array([\"a\", \"d\"]),\n                         pa.array([\"a\", \"d\"]),\n                         pa.array([\"a\", \"d\"])],\n                        names=[\"a\", \"b\", \"c\"])\n    result = table.drop_null()\n    assert result.equals(expected)\n\n    table = pa.table([pa.chunked_array([[\"a\", \"b\"], [\"c\", \"d\", \"e\"]]),\n                      pa.chunked_array([[\"A\"], [\"B\"], [None], [\"D\", None]]),\n                      pa.chunked_array([[\"a`\", None], [\"c`\", \"d`\", None]])],\n                     names=[\"a\", \"b\", \"c\"])\n    expected = pa.table([pa.array([\"a\", \"d\"]),\n                         pa.array([\"A\", \"D\"]),\n                         pa.array([\"a`\", \"d`\"])],\n                        names=[\"a\", \"b\", \"c\"])\n    result = table.drop_null()\n    assert result.equals(expected)\n\n\ndef test_drop_null_null_type():\n    arr = pa.array([None] * 10)\n    chunked_arr = pa.chunked_array([[None] * 5] * 2)\n    batch = pa.record_batch([arr], names=['a'])\n    table = pa.table({'a': arr})\n\n    assert len(arr.drop_null()) == 0\n    assert len(chunked_arr.drop_null()) == 0\n    assert len(batch.drop_null().column(0)) == 0\n    assert len(table.drop_null().column(0)) == 0\n\n\n@pytest.mark.parametrize(('ty', 'values'), all_array_types)\ndef test_filter(ty, values):\n    arr = pa.array(values, type=ty)\n\n    mask = pa.array([True, False, False, True, None])\n    result = arr.filter(mask, null_selection_behavior='drop')\n    result.validate()\n    assert result.equals(pa.array([values[0], values[3]], type=ty))\n    result = arr.filter(mask, null_selection_behavior='emit_null')\n    result.validate()\n    assert result.equals(pa.array([values[0], values[3], None], type=ty))\n\n    # same test with different array type\n    mask = np.array([True, False, False, True, None])\n    result = arr.filter(mask, null_selection_behavior='drop')\n    result.validate()\n    assert result.equals(pa.array([values[0], values[3]], type=ty))\n\n    # non-boolean dtype\n    mask = pa.array([0, 1, 0, 1, 0])\n    with pytest.raises(NotImplementedError):\n        arr.filter(mask)\n\n    # wrong length\n    mask = pa.array([True, False, True])\n    with pytest.raises(ValueError, match=\"must all be the same length\"):\n        arr.filter(mask)\n\n\ndef test_filter_chunked_array():\n    arr = pa.chunked_array([[\"a\", None], [\"c\", \"d\", \"e\"]])\n    expected_drop = pa.chunked_array([[\"a\"], [\"e\"]])\n    expected_null = pa.chunked_array([[\"a\"], [None, \"e\"]])\n\n    for mask in [\n        # mask is array\n        pa.array([True, False, None, False, True]),\n        # mask is chunked array\n        pa.chunked_array([[True, False, None], [False, True]]),\n        # mask is python object\n        [True, False, None, False, True]\n    ]:\n        result = arr.filter(mask)\n        assert result.equals(expected_drop)\n        result = arr.filter(mask, null_selection_behavior=\"emit_null\")\n        assert result.equals(expected_null)\n\n\ndef test_filter_record_batch():\n    batch = pa.record_batch(\n        [pa.array([\"a\", None, \"c\", \"d\", \"e\"])], names=[\"a'\"])\n\n    # mask is array\n    mask = pa.array([True, False, None, False, True])\n    result = batch.filter(mask)\n    expected = pa.record_batch([pa.array([\"a\", \"e\"])], names=[\"a'\"])\n    assert result.equals(expected)\n\n    # GH-38770: mask is chunked array\n    chunked_mask = pa.chunked_array([[True, False], [None], [False, True]])\n    result = batch.filter(chunked_mask)\n    assert result.equals(expected)\n\n    result = batch.filter(mask, null_selection_behavior=\"emit_null\")\n    expected = pa.record_batch([pa.array([\"a\", None, \"e\"])], names=[\"a'\"])\n    assert result.equals(expected)\n\n\ndef test_filter_table():\n    table = pa.table([pa.array([\"a\", None, \"c\", \"d\", \"e\"])], names=[\"a\"])\n    expected_drop = pa.table([pa.array([\"a\", \"e\"])], names=[\"a\"])\n    expected_null = pa.table([pa.array([\"a\", None, \"e\"])], names=[\"a\"])\n\n    for mask in [\n        # mask is array\n        pa.array([True, False, None, False, True]),\n        # mask is chunked array\n        pa.chunked_array([[True, False], [None, False, True]]),\n        # mask is python object\n        [True, False, None, False, True]\n    ]:\n        result = table.filter(mask)\n        assert result.equals(expected_drop)\n        result = table.filter(mask, null_selection_behavior=\"emit_null\")\n        assert result.equals(expected_null)\n\n\ndef test_filter_errors():\n    arr = pa.chunked_array([[\"a\", None], [\"c\", \"d\", \"e\"]])\n    batch = pa.record_batch(\n        [pa.array([\"a\", None, \"c\", \"d\", \"e\"])], names=[\"a'\"])\n    table = pa.table([pa.array([\"a\", None, \"c\", \"d\", \"e\"])], names=[\"a\"])\n\n    for obj in [arr, batch, table]:\n        # non-boolean dtype\n        mask = pa.array([0, 1, 0, 1, 0])\n        with pytest.raises(NotImplementedError):\n            obj.filter(mask)\n\n        # wrong length\n        mask = pa.array([True, False, True])\n        with pytest.raises(pa.ArrowInvalid,\n                           match=\"must all be the same length\"):\n            obj.filter(mask)\n\n    scalar = pa.scalar(True)\n    for filt in [batch, table, scalar]:\n        with pytest.raises(TypeError):\n            table.filter(filt)\n\n\ndef test_filter_null_type():\n    # ARROW-10027\n    arr = pa.array([None] * 10)\n    chunked_arr = pa.chunked_array([[None] * 5] * 2)\n    batch = pa.record_batch([arr], names=['a'])\n    table = pa.table({'a': arr})\n\n    mask = pa.array([True, False] * 5)\n    assert len(arr.filter(mask)) == 5\n    assert len(chunked_arr.filter(mask)) == 5\n    assert len(batch.filter(mask).column(0)) == 5\n    assert len(table.filter(mask).column(0)) == 5\n\n\n@pytest.mark.parametrize(\"typ\", [\"array\", \"chunked_array\"])\ndef test_compare_array(typ):\n    if typ == \"array\":\n        def con(values):\n            return pa.array(values)\n    else:\n        def con(values):\n            return pa.chunked_array([values])\n\n    arr1 = con([1, 2, 3, 4, None])\n    arr2 = con([1, 1, 4, None, 4])\n\n    result = pc.equal(arr1, arr2)\n    assert result.equals(con([True, False, False, None, None]))\n\n    result = pc.not_equal(arr1, arr2)\n    assert result.equals(con([False, True, True, None, None]))\n\n    result = pc.less(arr1, arr2)\n    assert result.equals(con([False, False, True, None, None]))\n\n    result = pc.less_equal(arr1, arr2)\n    assert result.equals(con([True, False, True, None, None]))\n\n    result = pc.greater(arr1, arr2)\n    assert result.equals(con([False, True, False, None, None]))\n\n    result = pc.greater_equal(arr1, arr2)\n    assert result.equals(con([True, True, False, None, None]))\n\n\n@pytest.mark.parametrize(\"typ\", [\"array\", \"chunked_array\"])\ndef test_compare_string_scalar(typ):\n    if typ == \"array\":\n        def con(values):\n            return pa.array(values)\n    else:\n        def con(values):\n            return pa.chunked_array([values])\n\n    arr = con(['a', 'b', 'c', None])\n    scalar = pa.scalar('b')\n\n    result = pc.equal(arr, scalar)\n    assert result.equals(con([False, True, False, None]))\n\n    if typ == \"array\":\n        nascalar = pa.scalar(None, type=\"string\")\n        result = pc.equal(arr, nascalar)\n        isnull = pc.is_null(result)\n        assert isnull.equals(con([True, True, True, True]))\n\n    result = pc.not_equal(arr, scalar)\n    assert result.equals(con([True, False, True, None]))\n\n    result = pc.less(arr, scalar)\n    assert result.equals(con([True, False, False, None]))\n\n    result = pc.less_equal(arr, scalar)\n    assert result.equals(con([True, True, False, None]))\n\n    result = pc.greater(arr, scalar)\n    assert result.equals(con([False, False, True, None]))\n\n    result = pc.greater_equal(arr, scalar)\n    assert result.equals(con([False, True, True, None]))\n\n\n@pytest.mark.parametrize(\"typ\", [\"array\", \"chunked_array\"])\ndef test_compare_scalar(typ):\n    if typ == \"array\":\n        def con(values):\n            return pa.array(values)\n    else:\n        def con(values):\n            return pa.chunked_array([values])\n\n    arr = con([1, 2, 3, None])\n    scalar = pa.scalar(2)\n\n    result = pc.equal(arr, scalar)\n    assert result.equals(con([False, True, False, None]))\n\n    if typ == \"array\":\n        nascalar = pa.scalar(None, type=\"int64\")\n        result = pc.equal(arr, nascalar)\n        assert result.to_pylist() == [None, None, None, None]\n\n    result = pc.not_equal(arr, scalar)\n    assert result.equals(con([True, False, True, None]))\n\n    result = pc.less(arr, scalar)\n    assert result.equals(con([True, False, False, None]))\n\n    result = pc.less_equal(arr, scalar)\n    assert result.equals(con([True, True, False, None]))\n\n    result = pc.greater(arr, scalar)\n    assert result.equals(con([False, False, True, None]))\n\n    result = pc.greater_equal(arr, scalar)\n    assert result.equals(con([False, True, True, None]))\n\n\ndef test_compare_chunked_array_mixed():\n    arr = pa.array([1, 2, 3, 4, None])\n    arr_chunked = pa.chunked_array([[1, 2, 3], [4, None]])\n    arr_chunked2 = pa.chunked_array([[1, 2], [3, 4, None]])\n\n    expected = pa.chunked_array([[True, True, True, True, None]])\n\n    for left, right in [\n        (arr, arr_chunked),\n        (arr_chunked, arr),\n        (arr_chunked, arr_chunked2),\n    ]:\n        result = pc.equal(left, right)\n        assert result.equals(expected)\n\n\ndef test_arithmetic_add():\n    left = pa.array([1, 2, 3, 4, 5])\n    right = pa.array([0, -1, 1, 2, 3])\n    result = pc.add(left, right)\n    expected = pa.array([1, 1, 4, 6, 8])\n    assert result.equals(expected)\n\n\ndef test_arithmetic_subtract():\n    left = pa.array([1, 2, 3, 4, 5])\n    right = pa.array([0, -1, 1, 2, 3])\n    result = pc.subtract(left, right)\n    expected = pa.array([1, 3, 2, 2, 2])\n    assert result.equals(expected)\n\n\ndef test_arithmetic_multiply():\n    left = pa.array([1, 2, 3, 4, 5])\n    right = pa.array([0, -1, 1, 2, 3])\n    result = pc.multiply(left, right)\n    expected = pa.array([0, -2, 3, 8, 15])\n    assert result.equals(expected)\n\n\n@pytest.mark.parametrize(\"ty\", [\"round\", \"round_to_multiple\"])\ndef test_round_to_integer(ty):\n    if ty == \"round\":\n        round = pc.round\n        RoundOptions = partial(pc.RoundOptions, ndigits=0)\n    elif ty == \"round_to_multiple\":\n        round = pc.round_to_multiple\n        RoundOptions = partial(pc.RoundToMultipleOptions, multiple=1)\n\n    values = [3.2, 3.5, 3.7, 4.5, -3.2, -3.5, -3.7, None]\n    rmode_and_expected = {\n        \"down\": [3, 3, 3, 4, -4, -4, -4, None],\n        \"up\": [4, 4, 4, 5, -3, -3, -3, None],\n        \"towards_zero\": [3, 3, 3, 4, -3, -3, -3, None],\n        \"towards_infinity\": [4, 4, 4, 5, -4, -4, -4, None],\n        \"half_down\": [3, 3, 4, 4, -3, -4, -4, None],\n        \"half_up\": [3, 4, 4, 5, -3, -3, -4, None],\n        \"half_towards_zero\": [3, 3, 4, 4, -3, -3, -4, None],\n        \"half_towards_infinity\": [3, 4, 4, 5, -3, -4, -4, None],\n        \"half_to_even\": [3, 4, 4, 4, -3, -4, -4, None],\n        \"half_to_odd\": [3, 3, 4, 5, -3, -3, -4, None],\n    }\n    for round_mode, expected in rmode_and_expected.items():\n        options = RoundOptions(round_mode=round_mode)\n        result = round(values, options=options)\n        np.testing.assert_array_equal(result, pa.array(expected))\n\n\ndef test_round():\n    values = [320, 3.5, 3.075, 4.5, -3.212, -35.1234, -3.045, None]\n    ndigits_and_expected = {\n        -2: [300, 0, 0, 0, -0, -0, -0, None],\n        -1: [320, 0, 0, 0, -0, -40, -0, None],\n        0: [320, 4, 3, 5, -3, -35, -3, None],\n        1: [320, 3.5, 3.1, 4.5, -3.2, -35.1, -3, None],\n        2: [320, 3.5, 3.08, 4.5, -3.21, -35.12, -3.05, None],\n    }\n    for ndigits, expected in ndigits_and_expected.items():\n        options = pc.RoundOptions(ndigits, \"half_towards_infinity\")\n        result = pc.round(values, options=options)\n        np.testing.assert_allclose(result, pa.array(expected), equal_nan=True)\n        assert pc.round(values, ndigits,\n                        round_mode=\"half_towards_infinity\") == result\n        assert pc.round(values, ndigits, \"half_towards_infinity\") == result\n\n\ndef test_round_to_multiple():\n    values = [320, 3.5, 3.075, 4.5, -3.212, -35.1234, -3.045, None]\n    multiple_and_expected = {\n        0.05: [320, 3.5, 3.1, 4.5, -3.2, -35.1, -3.05, None],\n        pa.scalar(0.1): [320, 3.5, 3.1, 4.5, -3.2, -35.1, -3, None],\n        2: [320, 4, 4, 4, -4, -36, -4, None],\n        10: [320, 0, 0, 0, -0, -40, -0, None],\n        pa.scalar(100, type=pa.decimal256(10, 4)):\n            [300, 0, 0, 0, -0, -0, -0, None],\n    }\n    for multiple, expected in multiple_and_expected.items():\n        options = pc.RoundToMultipleOptions(multiple, \"half_towards_infinity\")\n        result = pc.round_to_multiple(values, options=options)\n        np.testing.assert_allclose(result, pa.array(expected), equal_nan=True)\n        assert pc.round_to_multiple(values, multiple,\n                                    \"half_towards_infinity\") == result\n\n    for multiple in [0, -2, pa.scalar(-10.4)]:\n        with pytest.raises(pa.ArrowInvalid,\n                           match=\"Rounding multiple must be positive\"):\n            pc.round_to_multiple(values, multiple=multiple)\n\n    for multiple in [object, 99999999999999999999999]:\n        with pytest.raises(TypeError, match=\"is not a valid multiple type\"):\n            pc.round_to_multiple(values, multiple=multiple)\n\n\ndef test_round_binary():\n    values = [123.456, 234.567, 345.678, 456.789, 123.456, 234.567, 345.678]\n    scales = pa.array([-3, -2, -1, 0, 1, 2, 3], pa.int32())\n    expected = pa.array(\n        [0, 200, 350, 457, 123.5, 234.57, 345.678], pa.float64())\n    assert pc.round_binary(values, scales) == expected\n\n    expect_zero = pa.scalar(0, pa.float64())\n    expect_inf = pa.scalar(10, pa.float64())\n    scale = pa.scalar(-1, pa.int32())\n\n    assert pc.round_binary(\n        5.0, scale, round_mode=\"half_towards_zero\") == expect_zero\n    assert pc.round_binary(\n        5.0, scale, round_mode=\"half_towards_infinity\") == expect_inf\n\n\ndef test_is_null():\n    arr = pa.array([1, 2, 3, None])\n    result = arr.is_null()\n    expected = pa.array([False, False, False, True])\n    assert result.equals(expected)\n    assert result.equals(pc.is_null(arr))\n    result = arr.is_valid()\n    expected = pa.array([True, True, True, False])\n    assert result.equals(expected)\n    assert result.equals(pc.is_valid(arr))\n\n    arr = pa.chunked_array([[1, 2], [3, None]])\n    result = arr.is_null()\n    expected = pa.chunked_array([[False, False], [False, True]])\n    assert result.equals(expected)\n    result = arr.is_valid()\n    expected = pa.chunked_array([[True, True], [True, False]])\n    assert result.equals(expected)\n\n    arr = pa.array([1, 2, 3, None, np.nan])\n    result = arr.is_null()\n    expected = pa.array([False, False, False, True, False])\n    assert result.equals(expected)\n\n    result = arr.is_null(nan_is_null=True)\n    expected = pa.array([False, False, False, True, True])\n    assert result.equals(expected)\n\n\ndef test_is_nan():\n    arr = pa.array([1, 2, 3, None, np.nan])\n    result = arr.is_nan()\n    expected = pa.array([False, False, False, None, True])\n    assert result.equals(expected)\n\n    arr = pa.array([\"1\", \"2\", None], type=pa.string())\n    with pytest.raises(\n            ArrowNotImplementedError, match=\"has no kernel matching input types\"):\n        _ = arr.is_nan()\n\n    with pytest.raises(\n            ArrowNotImplementedError, match=\"has no kernel matching input types\"):\n        arr = pa.array([b'a', b'bb', None], type=pa.large_binary())\n        _ = arr.is_nan()\n\n\ndef test_fill_null():\n    arr = pa.array([1, 2, None, 4], type=pa.int8())\n    fill_value = pa.array([5], type=pa.int8())\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"Array arguments must all be the same length\"):\n        arr.fill_null(fill_value)\n\n    arr = pa.array([None, None, None, None], type=pa.null())\n    fill_value = pa.scalar(None, type=pa.null())\n    result = arr.fill_null(fill_value)\n    expected = pa.array([None, None, None, None])\n    assert result.equals(expected)\n\n    arr = pa.array(['a', 'bb', None])\n    result = arr.fill_null('ccc')\n    expected = pa.array(['a', 'bb', 'ccc'])\n    assert result.equals(expected)\n\n    arr = pa.array([b'a', b'bb', None], type=pa.large_binary())\n    result = arr.fill_null('ccc')\n    expected = pa.array([b'a', b'bb', b'ccc'], type=pa.large_binary())\n    assert result.equals(expected)\n\n    arr = pa.array(['a', 'bb', None])\n    result = arr.fill_null(None)\n    expected = pa.array(['a', 'bb', None])\n    assert result.equals(expected)\n\n\n@pytest.mark.parametrize('arrow_type', numerical_arrow_types)\ndef test_fill_null_array(arrow_type):\n    arr = pa.array([1, 2, None, 4], type=arrow_type)\n    fill_value = pa.scalar(5, type=arrow_type)\n    result = arr.fill_null(fill_value)\n    expected = pa.array([1, 2, 5, 4], type=arrow_type)\n    assert result.equals(expected)\n\n    # Implicit conversions\n    result = arr.fill_null(5)\n    assert result.equals(expected)\n\n    # ARROW-9451: Unsigned integers allow this for some reason\n    if not pa.types.is_unsigned_integer(arr.type):\n        with pytest.raises((ValueError, TypeError)):\n            arr.fill_null('5')\n\n    result = arr.fill_null(pa.scalar(5, type='int8'))\n    assert result.equals(expected)\n\n\n@pytest.mark.parametrize('arrow_type', numerical_arrow_types)\ndef test_fill_null_chunked_array(arrow_type):\n    fill_value = pa.scalar(5, type=arrow_type)\n    arr = pa.chunked_array([pa.array([None, 2, 3, 4], type=arrow_type)])\n    result = arr.fill_null(fill_value)\n    expected = pa.chunked_array([pa.array([5, 2, 3, 4], type=arrow_type)])\n    assert result.equals(expected)\n\n    arr = pa.chunked_array([\n        pa.array([1, 2], type=arrow_type),\n        pa.array([], type=arrow_type),\n        pa.array([None, 4], type=arrow_type)\n    ])\n    expected = pa.chunked_array([\n        pa.array([1, 2], type=arrow_type),\n        pa.array([], type=arrow_type),\n        pa.array([5, 4], type=arrow_type)\n    ])\n    result = arr.fill_null(fill_value)\n    assert result.equals(expected)\n\n    # Implicit conversions\n    result = arr.fill_null(5)\n    assert result.equals(expected)\n\n    result = arr.fill_null(pa.scalar(5, type='int8'))\n    assert result.equals(expected)\n\n\ndef test_logical():\n    a = pa.array([True, False, False, None])\n    b = pa.array([True, True, False, True])\n\n    assert pc.and_(a, b) == pa.array([True, False, False, None])\n    assert pc.and_kleene(a, b) == pa.array([True, False, False, None])\n\n    assert pc.or_(a, b) == pa.array([True, True, False, None])\n    assert pc.or_kleene(a, b) == pa.array([True, True, False, True])\n\n    assert pc.xor(a, b) == pa.array([False, True, False, None])\n\n    assert pc.invert(a) == pa.array([False, True, True, None])\n\n\ndef test_dictionary_decode():\n    array = pa.array([\"a\", \"a\", \"b\", \"c\", \"b\"])\n    dictionary_array = array.dictionary_encode()\n    dictionary_array_decode = pc.dictionary_decode(dictionary_array)\n\n    assert array != dictionary_array\n\n    assert array == dictionary_array_decode\n    assert array == pc.dictionary_decode(array)\n    assert pc.dictionary_encode(dictionary_array) == dictionary_array\n\n\ndef test_cast():\n    arr = pa.array([1, 2, 3, 4], type='int64')\n    options = pc.CastOptions(pa.int8())\n\n    with pytest.raises(TypeError):\n        pc.cast(arr, target_type=None)\n\n    with pytest.raises(ValueError):\n        pc.cast(arr, 'int32', options=options)\n\n    with pytest.raises(ValueError):\n        pc.cast(arr, safe=True, options=options)\n\n    assert pc.cast(arr, options=options) == pa.array(\n        [1, 2, 3, 4], type='int8')\n\n    arr = pa.array([2 ** 63 - 1], type='int64')\n    allow_overflow_options = pc.CastOptions(\n        pa.int32(), allow_int_overflow=True)\n\n    with pytest.raises(pa.ArrowInvalid):\n        pc.cast(arr, 'int32')\n\n    assert pc.cast(arr, 'int32', safe=False) == pa.array([-1], type='int32')\n\n    assert pc.cast(arr, options=allow_overflow_options) == pa.array(\n        [-1], type='int32')\n\n    arr = pa.array(\n        [datetime.datetime(2010, 1, 1), datetime.datetime(2015, 1, 1)])\n    expected = pa.array([1262304000000, 1420070400000], type='timestamp[ms]')\n    assert pc.cast(arr, 'timestamp[ms]') == expected\n\n    arr = pa.array([[1, 2], [3, 4, 5]], type=pa.large_list(pa.int8()))\n    expected = pa.array([[\"1\", \"2\"], [\"3\", \"4\", \"5\"]],\n                        type=pa.list_(pa.utf8()))\n    assert pc.cast(arr, expected.type) == expected\n\n\n@pytest.mark.parametrize('value_type', numerical_arrow_types)\ndef test_fsl_to_fsl_cast(value_type):\n    # Different field name and different type.\n    cast_type = pa.list_(pa.field(\"element\", value_type), 2)\n\n    dtype = pa.int32()\n    type = pa.list_(pa.field(\"values\", dtype), 2)\n\n    fsl = pa.FixedSizeListArray.from_arrays(\n        pa.array([1, 2, 3, 4, 5, 6], type=dtype), type=type)\n    assert cast_type == fsl.cast(cast_type).type\n\n    # Different field name and different type (with null values).\n    fsl = pa.FixedSizeListArray.from_arrays(\n        pa.array([1, None, None, 4, 5, 6], type=dtype), type=type)\n    assert cast_type == fsl.cast(cast_type).type\n\n    # Null FSL type.\n    dtype = pa.null()\n    type = pa.list_(pa.field(\"values\", dtype), 2)\n    fsl = pa.FixedSizeListArray.from_arrays(\n        pa.array([None, None, None, None, None, None], type=dtype), type=type)\n    assert cast_type == fsl.cast(cast_type).type\n\n    # Different sized FSL\n    cast_type = pa.list_(pa.field(\"element\", value_type), 3)\n    err_msg = 'Size of FixedSizeList is not the same.'\n    with pytest.raises(pa.lib.ArrowTypeError, match=err_msg):\n        fsl.cast(cast_type)\n\n\nDecimalTypeTraits = namedtuple('DecimalTypeTraits',\n                               ('name', 'factory', 'max_precision'))\n\nFloatToDecimalCase = namedtuple('FloatToDecimalCase',\n                                ('precision', 'scale', 'float_val'))\n\ndecimal_type_traits = [DecimalTypeTraits('decimal128', pa.decimal128, 38),\n                       DecimalTypeTraits('decimal256', pa.decimal256, 76)]\n\n\ndef largest_scaled_float_not_above(val, scale):\n    \"\"\"\n    Find the largest float f such as `f * 10**scale <= val`\n    \"\"\"\n    assert val >= 0\n    assert scale >= 0\n    float_val = float(val) / 10**scale\n    if float_val * 10**scale > val:\n        # Take the float just below... it *should* satisfy\n        float_val = np.nextafter(float_val, 0.0)\n        if float_val * 10**scale > val:\n            float_val = np.nextafter(float_val, 0.0)\n    assert float_val * 10**scale <= val\n    return float_val\n\n\ndef scaled_float(int_val, scale):\n    \"\"\"\n    Return a float representation (possibly approximate) of `int_val**-scale`\n    \"\"\"\n    assert isinstance(int_val, int)\n    unscaled = decimal.Decimal(int_val)\n    scaled = unscaled.scaleb(-scale)\n    float_val = float(scaled)\n    return float_val\n\n\ndef integral_float_to_decimal_cast_cases(float_ty, max_precision):\n    \"\"\"\n    Return FloatToDecimalCase instances with integral values.\n    \"\"\"\n    mantissa_digits = 16\n    for precision in range(1, max_precision, 3):\n        for scale in range(0, precision, 2):\n            yield FloatToDecimalCase(precision, scale, 0.0)\n            yield FloatToDecimalCase(precision, scale, 1.0)\n            epsilon = 10**max(precision - mantissa_digits, scale)\n            abs_maxval = largest_scaled_float_not_above(\n                10**precision - epsilon, scale)\n            yield FloatToDecimalCase(precision, scale, abs_maxval)\n\n\ndef real_float_to_decimal_cast_cases(float_ty, max_precision):\n    \"\"\"\n    Return FloatToDecimalCase instances with real values.\n    \"\"\"\n    mantissa_digits = 16\n    for precision in range(1, max_precision, 3):\n        for scale in range(0, precision, 2):\n            epsilon = 2 * 10**max(precision - mantissa_digits, 0)\n            abs_minval = largest_scaled_float_not_above(epsilon, scale)\n            abs_maxval = largest_scaled_float_not_above(\n                10**precision - epsilon, scale)\n            yield FloatToDecimalCase(precision, scale, abs_minval)\n            yield FloatToDecimalCase(precision, scale, abs_maxval)\n\n\ndef random_float_to_decimal_cast_cases(float_ty, max_precision):\n    \"\"\"\n    Return random-generated FloatToDecimalCase instances.\n    \"\"\"\n    r = random.Random(42)\n    for precision in range(1, max_precision, 6):\n        for scale in range(0, precision, 4):\n            for i in range(20):\n                unscaled = r.randrange(0, 10**precision)\n                float_val = scaled_float(unscaled, scale)\n                assert float_val * 10**scale < 10**precision\n                yield FloatToDecimalCase(precision, scale, float_val)\n\n\ndef check_cast_float_to_decimal(float_ty, float_val, decimal_ty, decimal_ctx,\n                                max_precision):\n    # Use the Python decimal module to build the expected result\n    # using the right precision\n    decimal_ctx.prec = decimal_ty.precision\n    decimal_ctx.rounding = decimal.ROUND_HALF_EVEN\n    expected = decimal_ctx.create_decimal_from_float(float_val)\n    # Round `expected` to `scale` digits after the decimal point\n    expected = expected.quantize(decimal.Decimal(1).scaleb(-decimal_ty.scale))\n    s = pa.scalar(float_val, type=float_ty)\n    actual = pc.cast(s, decimal_ty).as_py()\n    if actual != expected:\n        # Allow the last digit to vary. The tolerance is higher for\n        # very high precisions as rounding errors can accumulate in\n        # the iterative algorithm (GH-35576).\n        diff_digits = abs(actual - expected) * 10**decimal_ty.scale\n        limit = 2 if decimal_ty.precision < max_precision - 1 else 4\n        assert diff_digits <= limit, (\n            f\"float_val = {float_val!r}, precision={decimal_ty.precision}, \"\n            f\"expected = {expected!r}, actual = {actual!r}, \"\n            f\"diff_digits = {diff_digits!r}\")\n\n\n# Cannot test float32 as case generators above assume float64\n@pytest.mark.parametrize('float_ty', [pa.float64()], ids=str)\n@pytest.mark.parametrize('decimal_ty', decimal_type_traits,\n                         ids=lambda v: v.name)\n@pytest.mark.parametrize('case_generator',\n                         [integral_float_to_decimal_cast_cases,\n                          real_float_to_decimal_cast_cases,\n                          random_float_to_decimal_cast_cases],\n                         ids=['integrals', 'reals', 'random'])\ndef test_cast_float_to_decimal(float_ty, decimal_ty, case_generator):\n    with decimal.localcontext() as ctx:\n        for case in case_generator(float_ty, decimal_ty.max_precision):\n            check_cast_float_to_decimal(\n                float_ty, case.float_val,\n                decimal_ty.factory(case.precision, case.scale),\n                ctx, decimal_ty.max_precision)\n\n\n@pytest.mark.parametrize('float_ty', [pa.float32(), pa.float64()], ids=str)\n@pytest.mark.parametrize('decimal_traits', decimal_type_traits,\n                         ids=lambda v: v.name)\ndef test_cast_float_to_decimal_random(float_ty, decimal_traits):\n    \"\"\"\n    Test float-to-decimal conversion against exactly generated values.\n    \"\"\"\n    r = random.Random(43)\n    np_float_ty = {\n        pa.float32(): np.float32,\n        pa.float64(): np.float64,\n    }[float_ty]\n    mantissa_bits = {\n        pa.float32(): 24,\n        pa.float64(): 53,\n    }[float_ty]\n    float_exp_min, float_exp_max = {\n        pa.float32(): (-126, 127),\n        pa.float64(): (-1022, 1023),\n    }[float_ty]\n    mantissa_digits = math.floor(math.log10(2**mantissa_bits))\n    max_precision = decimal_traits.max_precision\n\n    with decimal.localcontext() as ctx:\n        precision = mantissa_digits\n        ctx.prec = precision\n        # The scale must be chosen so as\n        # 1) it's within bounds for the decimal type\n        # 2) the floating point exponent is within bounds\n        min_scale = max(-max_precision,\n                        precision + math.ceil(math.log10(2**float_exp_min)))\n        max_scale = min(max_precision,\n                        math.floor(math.log10(2**float_exp_max)))\n        for scale in range(min_scale, max_scale):\n            decimal_ty = decimal_traits.factory(precision, scale)\n            # We want to random-generate a float from its mantissa bits\n            # and exponent, and compute the expected value in the\n            # decimal domain. The float exponent has to ensure the\n            # expected value doesn't overflow and doesn't lose precision.\n            float_exp = (-mantissa_bits +\n                         math.floor(math.log2(10**(precision - scale))))\n            assert float_exp_min <= float_exp <= float_exp_max\n            for i in range(5):\n                mantissa = r.randrange(0, 2**mantissa_bits)\n                float_val = np.ldexp(np_float_ty(mantissa), float_exp)\n                assert isinstance(float_val, np_float_ty)\n                # Make sure we compute the exact expected value and\n                # round by half-to-even when converting to the expected precision.\n                if float_exp >= 0:\n                    expected = decimal.Decimal(mantissa) * 2**float_exp\n                else:\n                    expected = decimal.Decimal(mantissa) / 2**-float_exp\n                expected_as_int = round(expected.scaleb(scale))\n                actual = pc.cast(\n                    pa.scalar(float_val, type=float_ty), decimal_ty).as_py()\n                actual_as_int = round(actual.scaleb(scale))\n                # We allow for a minor rounding error between expected and actual\n                assert abs(actual_as_int - expected_as_int) <= 1\n\n\ndef test_strptime():\n    arr = pa.array([\"5/1/2020\", None, \"12/13/1900\"])\n\n    got = pc.strptime(arr, format='%m/%d/%Y', unit='s')\n    expected = pa.array(\n        [datetime.datetime(2020, 5, 1), None, datetime.datetime(1900, 12, 13)],\n        type=pa.timestamp('s'))\n    assert got == expected\n    # Positional format\n    assert pc.strptime(arr, '%m/%d/%Y', unit='s') == got\n\n    expected = pa.array([datetime.datetime(2020, 1, 5), None, None],\n                        type=pa.timestamp('s'))\n    got = pc.strptime(arr, format='%d/%m/%Y', unit='s', error_is_null=True)\n    assert got == expected\n\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"Failed to parse string: '5/1/2020'\"):\n        pc.strptime(arr, format='%Y-%m-%d', unit='s', error_is_null=False)\n\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"Failed to parse string: '5/1/2020'\"):\n        pc.strptime(arr, format='%Y-%m-%d', unit='s')\n\n    got = pc.strptime(arr, format='%Y-%m-%d', unit='s', error_is_null=True)\n    assert got == pa.array([None, None, None], type=pa.timestamp('s'))\n\n\n@pytest.mark.pandas\n@pytest.mark.skipif(sys.platform == \"win32\" and not util.windows_has_tzdata(),\n                    reason=\"Timezone database is not installed on Windows\")\ndef test_strftime():\n    times = [\"2018-03-10 09:00\", \"2038-01-31 12:23\", None]\n    timezones = [\"CET\", \"UTC\", \"Europe/Ljubljana\"]\n\n    formats = [\"%a\", \"%A\", \"%w\", \"%d\", \"%b\", \"%B\", \"%m\", \"%y\", \"%Y\", \"%H\", \"%I\",\n               \"%p\", \"%M\", \"%z\", \"%Z\", \"%j\", \"%U\", \"%W\", \"%%\", \"%G\", \"%V\", \"%u\"]\n    if sys.platform != \"win32\":\n        # Locale-dependent formats don't match on Windows\n        formats.extend([\"%c\", \"%x\", \"%X\"])\n\n    for timezone in timezones:\n        ts = pd.to_datetime(times).tz_localize(timezone)\n        for unit in [\"s\", \"ms\", \"us\", \"ns\"]:\n            tsa = pa.array(ts, type=pa.timestamp(unit, timezone))\n            for fmt in formats:\n                options = pc.StrftimeOptions(fmt)\n                result = pc.strftime(tsa, options=options)\n                expected = pa.array(ts.strftime(fmt))\n                assert result.equals(expected)\n\n        fmt = \"%Y-%m-%dT%H:%M:%S\"\n\n        # Default format\n        tsa = pa.array(ts, type=pa.timestamp(\"s\", timezone))\n        result = pc.strftime(tsa, options=pc.StrftimeOptions())\n        expected = pa.array(ts.strftime(fmt))\n        assert result.equals(expected)\n\n        # Default format plus timezone\n        tsa = pa.array(ts, type=pa.timestamp(\"s\", timezone))\n        result = pc.strftime(tsa, options=pc.StrftimeOptions(fmt + \"%Z\"))\n        expected = pa.array(ts.strftime(fmt + \"%Z\"))\n        assert result.equals(expected)\n\n        # Pandas %S is equivalent to %S in arrow for unit=\"s\"\n        tsa = pa.array(ts, type=pa.timestamp(\"s\", timezone))\n        options = pc.StrftimeOptions(\"%S\")\n        result = pc.strftime(tsa, options=options)\n        expected = pa.array(ts.strftime(\"%S\"))\n        assert result.equals(expected)\n\n        # Pandas %S.%f is equivalent to %S in arrow for unit=\"us\"\n        tsa = pa.array(ts, type=pa.timestamp(\"us\", timezone))\n        options = pc.StrftimeOptions(\"%S\")\n        result = pc.strftime(tsa, options=options)\n        expected = pa.array(ts.strftime(\"%S.%f\"))\n        assert result.equals(expected)\n\n        # Test setting locale\n        tsa = pa.array(ts, type=pa.timestamp(\"s\", timezone))\n        options = pc.StrftimeOptions(fmt, locale=\"C\")\n        result = pc.strftime(tsa, options=options)\n        expected = pa.array(ts.strftime(fmt))\n        assert result.equals(expected)\n\n    # Test timestamps without timezone\n    fmt = \"%Y-%m-%dT%H:%M:%S\"\n    ts = pd.to_datetime(times)\n    tsa = pa.array(ts, type=pa.timestamp(\"s\"))\n    result = pc.strftime(tsa, options=pc.StrftimeOptions(fmt))\n    expected = pa.array(ts.strftime(fmt))\n\n    # Positional format\n    assert pc.strftime(tsa, fmt) == result\n\n    assert result.equals(expected)\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"Timezone not present, cannot convert to string\"):\n        pc.strftime(tsa, options=pc.StrftimeOptions(fmt + \"%Z\"))\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"Timezone not present, cannot convert to string\"):\n        pc.strftime(tsa, options=pc.StrftimeOptions(fmt + \"%z\"))\n\n\ndef _check_datetime_components(timestamps, timezone=None):\n    from pyarrow.vendored.version import Version\n\n    ts = pd.to_datetime(timestamps).tz_localize(\n        \"UTC\").tz_convert(timezone).to_series()\n    tsa = pa.array(ts, pa.timestamp(\"ns\", tz=timezone))\n\n    subseconds = ((ts.dt.microsecond * 10 ** 3 +\n                   ts.dt.nanosecond) * 10 ** -9).round(9)\n    iso_calendar_fields = [\n        pa.field('iso_year', pa.int64()),\n        pa.field('iso_week', pa.int64()),\n        pa.field('iso_day_of_week', pa.int64())\n    ]\n\n    if Version(pd.__version__) < Version(\"1.1.0\"):\n        # https://github.com/pandas-dev/pandas/issues/33206\n        iso_year = ts.map(lambda x: x.isocalendar()[0]).astype(\"int64\")\n        iso_week = ts.map(lambda x: x.isocalendar()[1]).astype(\"int64\")\n        iso_day = ts.map(lambda x: x.isocalendar()[2]).astype(\"int64\")\n    else:\n        # Casting is required because pandas isocalendar returns int32\n        # while arrow isocalendar returns int64.\n        iso_year = ts.dt.isocalendar()[\"year\"].astype(\"int64\")\n        iso_week = ts.dt.isocalendar()[\"week\"].astype(\"int64\")\n        iso_day = ts.dt.isocalendar()[\"day\"].astype(\"int64\")\n\n    iso_calendar = pa.StructArray.from_arrays(\n        [iso_year, iso_week, iso_day],\n        fields=iso_calendar_fields)\n\n    # Casting is required because pandas with 2.0.0 various numeric\n    # date/time attributes have dtype int32 (previously int64)\n    year = ts.dt.year.astype(\"int64\")\n    month = ts.dt.month.astype(\"int64\")\n    day = ts.dt.day.astype(\"int64\")\n    dayofweek = ts.dt.dayofweek.astype(\"int64\")\n    dayofyear = ts.dt.dayofyear.astype(\"int64\")\n    quarter = ts.dt.quarter.astype(\"int64\")\n    hour = ts.dt.hour.astype(\"int64\")\n    minute = ts.dt.minute.astype(\"int64\")\n    second = ts.dt.second.values.astype(\"int64\")\n    microsecond = ts.dt.microsecond.astype(\"int64\")\n    nanosecond = ts.dt.nanosecond.astype(\"int64\")\n\n    assert pc.year(tsa).equals(pa.array(year))\n    assert pc.is_leap_year(tsa).equals(pa.array(ts.dt.is_leap_year))\n    assert pc.month(tsa).equals(pa.array(month))\n    assert pc.day(tsa).equals(pa.array(day))\n    assert pc.day_of_week(tsa).equals(pa.array(dayofweek))\n    assert pc.day_of_year(tsa).equals(pa.array(dayofyear))\n    assert pc.iso_year(tsa).equals(pa.array(iso_year))\n    assert pc.iso_week(tsa).equals(pa.array(iso_week))\n    assert pc.iso_calendar(tsa).equals(iso_calendar)\n    assert pc.quarter(tsa).equals(pa.array(quarter))\n    assert pc.hour(tsa).equals(pa.array(hour))\n    assert pc.minute(tsa).equals(pa.array(minute))\n    assert pc.second(tsa).equals(pa.array(second))\n    assert pc.millisecond(tsa).equals(pa.array(microsecond // 10 ** 3))\n    assert pc.microsecond(tsa).equals(pa.array(microsecond % 10 ** 3))\n    assert pc.nanosecond(tsa).equals(pa.array(nanosecond))\n    assert pc.subsecond(tsa).equals(pa.array(subseconds))\n    assert pc.local_timestamp(tsa).equals(pa.array(ts.dt.tz_localize(None)))\n\n    if ts.dt.tz:\n        if ts.dt.tz is datetime.timezone.utc:\n            # datetime with utc returns None for dst()\n            is_dst = [False] * len(ts)\n        else:\n            is_dst = ts.apply(lambda x: x.dst().seconds > 0)\n        assert pc.is_dst(tsa).equals(pa.array(is_dst))\n\n    day_of_week_options = pc.DayOfWeekOptions(\n        count_from_zero=False, week_start=1)\n    assert pc.day_of_week(tsa, options=day_of_week_options).equals(\n        pa.array(dayofweek + 1))\n\n    week_options = pc.WeekOptions(\n        week_starts_monday=True, count_from_zero=False,\n        first_week_is_fully_in_year=False)\n    assert pc.week(tsa, options=week_options).equals(pa.array(iso_week))\n\n\n@pytest.mark.pandas\ndef test_extract_datetime_components():\n    timestamps = [\"1970-01-01T00:00:59.123456789\",\n                  \"2000-02-29T23:23:23.999999999\",\n                  \"2033-05-18T03:33:20.000000000\",\n                  \"2020-01-01T01:05:05.001\",\n                  \"2019-12-31T02:10:10.002\",\n                  \"2019-12-30T03:15:15.003\",\n                  \"2009-12-31T04:20:20.004132\",\n                  \"2010-01-01T05:25:25.005321\",\n                  \"2010-01-03T06:30:30.006163\",\n                  \"2010-01-04T07:35:35.0\",\n                  \"2006-01-01T08:40:40.0\",\n                  \"2005-12-31T09:45:45.0\",\n                  \"2008-12-28T00:00:00.0\",\n                  \"2008-12-29T00:00:00.0\",\n                  \"2012-01-01T01:02:03.0\"]\n    timezones = [\"UTC\", \"US/Central\", \"Asia/Kolkata\",\n                 \"Etc/GMT-4\", \"Etc/GMT+4\", \"Australia/Broken_Hill\"]\n\n    # Test timezone naive timestamp array\n    _check_datetime_components(timestamps)\n\n    # Test timezone aware timestamp array\n    if sys.platform == \"win32\" and not util.windows_has_tzdata():\n        pytest.skip('Timezone database is not installed on Windows')\n    else:\n        for timezone in timezones:\n            _check_datetime_components(timestamps, timezone)\n\n\n@pytest.mark.parametrize(\"unit\", [\"s\", \"ms\", \"us\", \"ns\"])\ndef test_iso_calendar_longer_array(unit):\n    # https://github.com/apache/arrow/issues/38655\n    # ensure correct result for array length > 32\n    arr = pa.array([datetime.datetime(2022, 1, 2, 9)]*50, pa.timestamp(unit))\n    result = pc.iso_calendar(arr)\n    expected = pa.StructArray.from_arrays(\n        [[2021]*50, [52]*50, [7]*50],\n        names=['iso_year', 'iso_week', 'iso_day_of_week']\n    )\n    assert result.equals(expected)\n\n\n@pytest.mark.pandas\n@pytest.mark.skipif(sys.platform == \"win32\" and not util.windows_has_tzdata(),\n                    reason=\"Timezone database is not installed on Windows\")\ndef test_assume_timezone():\n    ts_type = pa.timestamp(\"ns\")\n    timestamps = pd.to_datetime([\"1970-01-01T00:00:59.123456789\",\n                                 \"2000-02-29T23:23:23.999999999\",\n                                 \"2033-05-18T03:33:20.000000000\",\n                                 \"2020-01-01T01:05:05.001\",\n                                 \"2019-12-31T02:10:10.002\",\n                                 \"2019-12-30T03:15:15.003\",\n                                 \"2009-12-31T04:20:20.004132\",\n                                 \"2010-01-01T05:25:25.005321\",\n                                 \"2010-01-03T06:30:30.006163\",\n                                 \"2010-01-04T07:35:35.0\",\n                                 \"2006-01-01T08:40:40.0\",\n                                 \"2005-12-31T09:45:45.0\",\n                                 \"2008-12-28T00:00:00.0\",\n                                 \"2008-12-29T00:00:00.0\",\n                                 \"2012-01-01T01:02:03.0\"])\n    nonexistent = pd.to_datetime([\"2015-03-29 02:30:00\",\n                                  \"2015-03-29 03:30:00\"])\n    ambiguous = pd.to_datetime([\"2018-10-28 01:20:00\",\n                                \"2018-10-28 02:36:00\",\n                                \"2018-10-28 03:46:00\"])\n    ambiguous_array = pa.array(ambiguous, type=ts_type)\n    nonexistent_array = pa.array(nonexistent, type=ts_type)\n\n    for timezone in [\"UTC\", \"US/Central\", \"Asia/Kolkata\"]:\n        options = pc.AssumeTimezoneOptions(timezone)\n        ta = pa.array(timestamps, type=ts_type)\n        expected = timestamps.tz_localize(timezone)\n        result = pc.assume_timezone(ta, options=options)\n        assert result.equals(pa.array(expected))\n        result = pc.assume_timezone(ta, timezone)  # Positional option\n        assert result.equals(pa.array(expected))\n\n        ta_zoned = pa.array(timestamps, type=pa.timestamp(\"ns\", timezone))\n        with pytest.raises(pa.ArrowInvalid, match=\"already have a timezone:\"):\n            pc.assume_timezone(ta_zoned, options=options)\n\n    invalid_options = pc.AssumeTimezoneOptions(\"Europe/Brusselsss\")\n    with pytest.raises(ValueError, match=\"not found in timezone database\"):\n        pc.assume_timezone(ta, options=invalid_options)\n\n    timezone = \"Europe/Brussels\"\n\n    options_nonexistent_raise = pc.AssumeTimezoneOptions(timezone)\n    options_nonexistent_earliest = pc.AssumeTimezoneOptions(\n        timezone, ambiguous=\"raise\", nonexistent=\"earliest\")\n    options_nonexistent_latest = pc.AssumeTimezoneOptions(\n        timezone, ambiguous=\"raise\", nonexistent=\"latest\")\n\n    with pytest.raises(ValueError,\n                       match=\"Timestamp doesn't exist in \"\n                       f\"timezone '{timezone}'\"):\n        pc.assume_timezone(nonexistent_array,\n                           options=options_nonexistent_raise)\n\n    expected = pa.array(nonexistent.tz_localize(\n        timezone, nonexistent=\"shift_forward\"))\n    result = pc.assume_timezone(\n        nonexistent_array, options=options_nonexistent_latest)\n    expected.equals(result)\n\n    expected = pa.array(nonexistent.tz_localize(\n        timezone, nonexistent=\"shift_backward\"))\n    result = pc.assume_timezone(\n        nonexistent_array, options=options_nonexistent_earliest)\n    expected.equals(result)\n\n    options_ambiguous_raise = pc.AssumeTimezoneOptions(timezone)\n    options_ambiguous_latest = pc.AssumeTimezoneOptions(\n        timezone, ambiguous=\"latest\", nonexistent=\"raise\")\n    options_ambiguous_earliest = pc.AssumeTimezoneOptions(\n        timezone, ambiguous=\"earliest\", nonexistent=\"raise\")\n\n    with pytest.raises(ValueError,\n                       match=\"Timestamp is ambiguous in \"\n                             f\"timezone '{timezone}'\"):\n        pc.assume_timezone(ambiguous_array, options=options_ambiguous_raise)\n\n    expected = ambiguous.tz_localize(timezone, ambiguous=[True, True, True])\n    result = pc.assume_timezone(\n        ambiguous_array, options=options_ambiguous_earliest)\n    result.equals(pa.array(expected))\n\n    expected = ambiguous.tz_localize(timezone, ambiguous=[False, False, False])\n    result = pc.assume_timezone(\n        ambiguous_array, options=options_ambiguous_latest)\n    result.equals(pa.array(expected))\n\n\ndef _check_temporal_rounding(ts, values, unit):\n    unit_shorthand = {\n        \"nanosecond\": \"ns\",\n        \"microsecond\": \"us\",\n        \"millisecond\": \"ms\",\n        \"second\": \"s\",\n        \"minute\": \"min\",\n        \"hour\": \"h\",\n        \"day\": \"D\"\n    }\n    greater_unit = {\n        \"nanosecond\": \"us\",\n        \"microsecond\": \"ms\",\n        \"millisecond\": \"s\",\n        \"second\": \"min\",\n        \"minute\": \"h\",\n        \"hour\": \"d\",\n    }\n    ta = pa.array(ts)\n\n    for value in values:\n        frequency = str(value) + unit_shorthand[unit]\n        options = pc.RoundTemporalOptions(value, unit)\n\n        result = pc.ceil_temporal(ta, options=options).to_pandas()\n        expected = ts.dt.ceil(frequency)\n        np.testing.assert_array_equal(result, expected)\n\n        result = pc.floor_temporal(ta, options=options).to_pandas()\n        expected = ts.dt.floor(frequency)\n        np.testing.assert_array_equal(result, expected)\n\n        result = pc.round_temporal(ta, options=options).to_pandas()\n        expected = ts.dt.round(frequency)\n        np.testing.assert_array_equal(result, expected)\n\n        # Check rounding with calendar_based_origin=True.\n        # Note: rounding to month is not supported in Pandas so we can't\n        # approximate this functionality and exclude unit == \"day\".\n        if unit != \"day\":\n            options = pc.RoundTemporalOptions(\n                value, unit, calendar_based_origin=True)\n            origin = ts.dt.floor(greater_unit[unit])\n\n            if ta.type.tz is None:\n                result = pc.ceil_temporal(ta, options=options).to_pandas()\n                expected = (ts - origin).dt.ceil(frequency) + origin\n                np.testing.assert_array_equal(result, expected)\n\n            result = pc.floor_temporal(ta, options=options).to_pandas()\n            expected = (ts - origin).dt.floor(frequency) + origin\n            np.testing.assert_array_equal(result, expected)\n\n            result = pc.round_temporal(ta, options=options).to_pandas()\n            expected = (ts - origin).dt.round(frequency) + origin\n            np.testing.assert_array_equal(result, expected)\n\n        # Check RoundTemporalOptions partial defaults\n        if unit == \"day\":\n            result = pc.ceil_temporal(ta, multiple=value).to_pandas()\n            expected = ts.dt.ceil(frequency)\n            np.testing.assert_array_equal(result, expected)\n\n            result = pc.floor_temporal(ta, multiple=value).to_pandas()\n            expected = ts.dt.floor(frequency)\n            np.testing.assert_array_equal(result, expected)\n\n            result = pc.round_temporal(ta, multiple=value).to_pandas()\n            expected = ts.dt.round(frequency)\n            np.testing.assert_array_equal(result, expected)\n\n    # We naively test ceil_is_strictly_greater by adding time unit multiple\n    # to regular ceiled timestamp if it is equal to the original timestamp.\n    # This does not work if timestamp is zoned since our logic will not\n    # account for DST jumps.\n    if ta.type.tz is None:\n        options = pc.RoundTemporalOptions(\n            value, unit, ceil_is_strictly_greater=True)\n        result = pc.ceil_temporal(ta, options=options)\n        expected = ts.dt.ceil(frequency)\n\n        expected = np.where(\n            expected == ts,\n            expected + pd.Timedelta(value, unit_shorthand[unit]),\n            expected)\n        np.testing.assert_array_equal(result, expected)\n\n    # Check RoundTemporalOptions defaults\n    if unit == \"day\":\n        frequency = \"1D\"\n\n        result = pc.ceil_temporal(ta).to_pandas()\n        expected = ts.dt.ceil(frequency)\n        np.testing.assert_array_equal(result, expected)\n\n        result = pc.floor_temporal(ta).to_pandas()\n        expected = ts.dt.floor(frequency)\n        np.testing.assert_array_equal(result, expected)\n\n        result = pc.round_temporal(ta).to_pandas()\n        expected = ts.dt.round(frequency)\n        np.testing.assert_array_equal(result, expected)\n\n\n@pytest.mark.skipif(sys.platform == \"win32\" and not util.windows_has_tzdata(),\n                    reason=\"Timezone database is not installed on Windows\")\n@pytest.mark.parametrize('unit', (\"nanosecond\", \"microsecond\", \"millisecond\",\n                                  \"second\", \"minute\", \"hour\", \"day\"))\n@pytest.mark.pandas\ndef test_round_temporal(unit):\n    values = (1, 2, 3, 4, 5, 6, 7, 10, 15, 24, 60, 250, 500, 750)\n    timestamps = [\n        \"1923-07-07 08:52:35.203790336\",\n        \"1931-03-17 10:45:00.641559040\",\n        \"1932-06-16 01:16:42.911994368\",\n        \"1941-05-27 11:46:43.822831872\",\n        \"1943-12-14 07:32:05.424766464\",\n        \"1954-04-12 04:31:50.699881472\",\n        \"1966-02-12 17:41:28.693282560\",\n        \"1967-02-26 05:56:46.922376960\",\n        \"1975-11-01 10:55:37.016146432\",\n        \"1982-01-21 18:43:44.517366784\",\n        \"1992-01-01 00:00:00.100000000\",\n        \"1999-12-04 05:55:34.794991104\",\n        \"2026-10-26 08:39:00.316686848\"]\n    ts = pd.Series([pd.Timestamp(x, unit=\"ns\") for x in timestamps])\n    _check_temporal_rounding(ts, values, unit)\n\n    timezones = [\"Asia/Kolkata\", \"America/New_York\", \"Etc/GMT-4\", \"Etc/GMT+4\",\n                 \"Europe/Brussels\", \"Pacific/Marquesas\", \"US/Central\", \"UTC\"]\n\n    for timezone in timezones:\n        ts_zoned = ts.dt.tz_localize(\"UTC\").dt.tz_convert(timezone)\n        _check_temporal_rounding(ts_zoned, values, unit)\n\n\ndef test_count():\n    arr = pa.array([1, 2, 3, None, None])\n    assert pc.count(arr).as_py() == 3\n    assert pc.count(arr, mode='only_valid').as_py() == 3\n    assert pc.count(arr, mode='only_null').as_py() == 2\n    assert pc.count(arr, mode='all').as_py() == 5\n    assert pc.count(arr, 'all').as_py() == 5\n\n    with pytest.raises(ValueError,\n                       match='\"something else\" is not a valid count mode'):\n        pc.count(arr, 'something else')\n\n\ndef test_index():\n    arr = pa.array([0, 1, None, 3, 4], type=pa.int64())\n    assert pc.index(arr, pa.scalar(0)).as_py() == 0\n    assert pc.index(arr, pa.scalar(2, type=pa.int8())).as_py() == -1\n    assert pc.index(arr, 4).as_py() == 4\n    assert arr.index(3, start=2).as_py() == 3\n    assert arr.index(None).as_py() == -1\n\n    arr = pa.chunked_array([[1, 2], [1, 3]], type=pa.int64())\n    assert arr.index(1).as_py() == 0\n    assert arr.index(1, start=2).as_py() == 2\n    assert arr.index(1, start=1, end=2).as_py() == -1\n\n\ndef check_partition_nth(data, indices, pivot, null_placement):\n    indices = indices.to_pylist()\n    assert len(indices) == len(data)\n    assert sorted(indices) == list(range(len(data)))\n    until_pivot = [data[indices[i]] for i in range(pivot)]\n    after_pivot = [data[indices[i]] for i in range(pivot, len(data))]\n    p = data[indices[pivot]]\n    if p is None:\n        if null_placement == \"at_start\":\n            assert all(v is None for v in until_pivot)\n        else:\n            assert all(v is None for v in after_pivot)\n    else:\n        if null_placement == \"at_start\":\n            assert all(v is None or v <= p for v in until_pivot)\n            assert all(v >= p for v in after_pivot)\n        else:\n            assert all(v <= p for v in until_pivot)\n            assert all(v is None or v >= p for v in after_pivot)\n\n\ndef test_partition_nth():\n    data = list(range(100, 140))\n    random.shuffle(data)\n    pivot = 10\n    indices = pc.partition_nth_indices(data, pivot=pivot)\n    check_partition_nth(data, indices, pivot, \"at_end\")\n    # Positional pivot argument\n    assert pc.partition_nth_indices(data, pivot) == indices\n\n    with pytest.raises(\n            ValueError,\n            match=\"'partition_nth_indices' cannot be called without options\"):\n        pc.partition_nth_indices(data)\n\n\ndef test_partition_nth_null_placement():\n    data = list(range(10)) + [None] * 10\n    random.shuffle(data)\n\n    for pivot in (0, 7, 13, 19):\n        for null_placement in (\"at_start\", \"at_end\"):\n            indices = pc.partition_nth_indices(data, pivot=pivot,\n                                               null_placement=null_placement)\n            check_partition_nth(data, indices, pivot, null_placement)\n\n\ndef test_select_k_array():\n    def validate_select_k(select_k_indices, arr, order, stable_sort=False):\n        sorted_indices = pc.sort_indices(arr, sort_keys=[(\"dummy\", order)])\n        head_k_indices = sorted_indices.slice(0, len(select_k_indices))\n        if stable_sort:\n            assert select_k_indices == head_k_indices\n        else:\n            expected = pc.take(arr, head_k_indices)\n            actual = pc.take(arr, select_k_indices)\n            assert actual == expected\n\n    arr = pa.array([1, 2, None, 0])\n    for k in [0, 2, 4]:\n        for order in [\"descending\", \"ascending\"]:\n            result = pc.select_k_unstable(\n                arr, k=k, sort_keys=[(\"dummy\", order)])\n            validate_select_k(result, arr, order)\n\n        result = pc.top_k_unstable(arr, k=k)\n        validate_select_k(result, arr, \"descending\")\n\n        result = pc.bottom_k_unstable(arr, k=k)\n        validate_select_k(result, arr, \"ascending\")\n\n    result = pc.select_k_unstable(\n        arr, options=pc.SelectKOptions(\n            k=2, sort_keys=[(\"dummy\", \"descending\")])\n    )\n    validate_select_k(result, arr, \"descending\")\n\n    result = pc.select_k_unstable(\n        arr, options=pc.SelectKOptions(k=2, sort_keys=[(\"dummy\", \"ascending\")])\n    )\n    validate_select_k(result, arr, \"ascending\")\n\n    # Position options\n    assert pc.select_k_unstable(arr, 2,\n                                sort_keys=[(\"dummy\", \"ascending\")]) == result\n    assert pc.select_k_unstable(arr, 2, [(\"dummy\", \"ascending\")]) == result\n\n\ndef test_select_k_table():\n    def validate_select_k(select_k_indices, tbl, sort_keys, stable_sort=False):\n        sorted_indices = pc.sort_indices(tbl, sort_keys=sort_keys)\n        head_k_indices = sorted_indices.slice(0, len(select_k_indices))\n        if stable_sort:\n            assert select_k_indices == head_k_indices\n        else:\n            expected = pc.take(tbl, head_k_indices)\n            actual = pc.take(tbl, select_k_indices)\n            assert actual == expected\n\n    table = pa.table({\"a\": [1, 2, 0], \"b\": [1, 0, 1]})\n    for k in [0, 2, 4]:\n        result = pc.select_k_unstable(\n            table, k=k, sort_keys=[(\"a\", \"ascending\")])\n        validate_select_k(result, table, sort_keys=[(\"a\", \"ascending\")])\n\n        result = pc.select_k_unstable(\n            table, k=k, sort_keys=[(pc.field(\"a\"), \"ascending\"), (\"b\", \"ascending\")])\n        validate_select_k(\n            result, table, sort_keys=[(\"a\", \"ascending\"), (\"b\", \"ascending\")])\n\n        result = pc.top_k_unstable(table, k=k, sort_keys=[\"a\"])\n        validate_select_k(result, table, sort_keys=[(\"a\", \"descending\")])\n\n        result = pc.bottom_k_unstable(table, k=k, sort_keys=[\"a\", \"b\"])\n        validate_select_k(\n            result, table, sort_keys=[(\"a\", \"ascending\"), (\"b\", \"ascending\")])\n\n    with pytest.raises(\n            ValueError,\n            match=\"'select_k_unstable' cannot be called without options\"):\n        pc.select_k_unstable(table)\n\n    with pytest.raises(ValueError,\n                       match=\"select_k_unstable requires a nonnegative `k`\"):\n        pc.select_k_unstable(table, k=-1, sort_keys=[(\"a\", \"ascending\")])\n\n    with pytest.raises(ValueError,\n                       match=\"select_k_unstable requires a \"\n                             \"non-empty `sort_keys`\"):\n        pc.select_k_unstable(table, k=2, sort_keys=[])\n\n    with pytest.raises(ValueError, match=\"not a valid sort order\"):\n        pc.select_k_unstable(table, k=k, sort_keys=[(\"a\", \"nonscending\")])\n\n    with pytest.raises(ValueError,\n                       match=\"Invalid sort key column: No match for.*unknown\"):\n        pc.select_k_unstable(table, k=k, sort_keys=[(\"unknown\", \"ascending\")])\n\n\ndef test_array_sort_indices():\n    arr = pa.array([1, 2, None, 0])\n    result = pc.array_sort_indices(arr)\n    assert result.to_pylist() == [3, 0, 1, 2]\n    result = pc.array_sort_indices(arr, order=\"ascending\")\n    assert result.to_pylist() == [3, 0, 1, 2]\n    result = pc.array_sort_indices(arr, order=\"descending\")\n    assert result.to_pylist() == [1, 0, 3, 2]\n    result = pc.array_sort_indices(arr, order=\"descending\",\n                                   null_placement=\"at_start\")\n    assert result.to_pylist() == [2, 1, 0, 3]\n    result = pc.array_sort_indices(arr, \"descending\",\n                                   null_placement=\"at_start\")\n    assert result.to_pylist() == [2, 1, 0, 3]\n\n    with pytest.raises(ValueError, match=\"not a valid sort order\"):\n        pc.array_sort_indices(arr, order=\"nonscending\")\n\n\ndef test_sort_indices_array():\n    arr = pa.array([1, 2, None, 0])\n    result = pc.sort_indices(arr)\n    assert result.to_pylist() == [3, 0, 1, 2]\n    result = pc.sort_indices(arr, sort_keys=[(\"dummy\", \"ascending\")])\n    assert result.to_pylist() == [3, 0, 1, 2]\n    result = pc.sort_indices(arr, sort_keys=[(\"dummy\", \"descending\")])\n    assert result.to_pylist() == [1, 0, 3, 2]\n    result = pc.sort_indices(arr, sort_keys=[(\"dummy\", \"descending\")],\n                             null_placement=\"at_start\")\n    assert result.to_pylist() == [2, 1, 0, 3]\n    # Positional `sort_keys`\n    result = pc.sort_indices(arr, [(\"dummy\", \"descending\")],\n                             null_placement=\"at_start\")\n    assert result.to_pylist() == [2, 1, 0, 3]\n    # Using SortOptions\n    result = pc.sort_indices(\n        arr, options=pc.SortOptions(sort_keys=[(\"dummy\", \"descending\")])\n    )\n    assert result.to_pylist() == [1, 0, 3, 2]\n    result = pc.sort_indices(\n        arr, options=pc.SortOptions(sort_keys=[(\"dummy\", \"descending\")],\n                                    null_placement=\"at_start\")\n    )\n    assert result.to_pylist() == [2, 1, 0, 3]\n\n\ndef test_sort_indices_table():\n    table = pa.table({\"a\": [1, 1, None, 0], \"b\": [1, 0, 0, 1]})\n\n    result = pc.sort_indices(table, sort_keys=[(\"a\", \"ascending\")])\n    assert result.to_pylist() == [3, 0, 1, 2]\n    result = pc.sort_indices(table, sort_keys=[(pc.field(\"a\"), \"ascending\")],\n                             null_placement=\"at_start\")\n    assert result.to_pylist() == [2, 3, 0, 1]\n\n    result = pc.sort_indices(\n        table, sort_keys=[(\"a\", \"descending\"), (\"b\", \"ascending\")]\n    )\n    assert result.to_pylist() == [1, 0, 3, 2]\n    result = pc.sort_indices(\n        table, sort_keys=[(\"a\", \"descending\"), (\"b\", \"ascending\")],\n        null_placement=\"at_start\"\n    )\n    assert result.to_pylist() == [2, 1, 0, 3]\n    # Positional `sort_keys`\n    result = pc.sort_indices(\n        table, [(\"a\", \"descending\"), (\"b\", \"ascending\")],\n        null_placement=\"at_start\"\n    )\n    assert result.to_pylist() == [2, 1, 0, 3]\n\n    with pytest.raises(ValueError, match=\"Must specify one or more sort keys\"):\n        pc.sort_indices(table)\n\n    with pytest.raises(ValueError,\n                       match=\"Invalid sort key column: No match for.*unknown\"):\n        pc.sort_indices(table, sort_keys=[(\"unknown\", \"ascending\")])\n\n    with pytest.raises(ValueError, match=\"not a valid sort order\"):\n        pc.sort_indices(table, sort_keys=[(\"a\", \"nonscending\")])\n\n\ndef test_is_in():\n    arr = pa.array([1, 2, None, 1, 2, 3])\n\n    result = pc.is_in(arr, value_set=pa.array([1, 3, None]))\n    assert result.to_pylist() == [True, False, True, True, False, True]\n\n    result = pc.is_in(arr, value_set=pa.array([1, 3, None]), skip_nulls=True)\n    assert result.to_pylist() == [True, False, False, True, False, True]\n\n    result = pc.is_in(arr, value_set=pa.array([1, 3]))\n    assert result.to_pylist() == [True, False, False, True, False, True]\n\n    result = pc.is_in(arr, value_set=pa.array([1, 3]), skip_nulls=True)\n    assert result.to_pylist() == [True, False, False, True, False, True]\n\n\ndef test_index_in():\n    arr = pa.array([1, 2, None, 1, 2, 3])\n\n    result = pc.index_in(arr, value_set=pa.array([1, 3, None]))\n    assert result.to_pylist() == [0, None, 2, 0, None, 1]\n\n    result = pc.index_in(arr, value_set=pa.array([1, 3, None]),\n                         skip_nulls=True)\n    assert result.to_pylist() == [0, None, None, 0, None, 1]\n\n    result = pc.index_in(arr, value_set=pa.array([1, 3]))\n    assert result.to_pylist() == [0, None, None, 0, None, 1]\n\n    result = pc.index_in(arr, value_set=pa.array([1, 3]), skip_nulls=True)\n    assert result.to_pylist() == [0, None, None, 0, None, 1]\n\n    # Positional value_set\n    result = pc.index_in(arr, pa.array([1, 3]), skip_nulls=True)\n    assert result.to_pylist() == [0, None, None, 0, None, 1]\n\n\ndef test_quantile():\n    arr = pa.array([1, 2, 3, 4])\n\n    result = pc.quantile(arr)\n    assert result.to_pylist() == [2.5]\n\n    result = pc.quantile(arr, interpolation='lower')\n    assert result.to_pylist() == [2]\n    result = pc.quantile(arr, interpolation='higher')\n    assert result.to_pylist() == [3]\n    result = pc.quantile(arr, interpolation='nearest')\n    assert result.to_pylist() == [3]\n    result = pc.quantile(arr, interpolation='midpoint')\n    assert result.to_pylist() == [2.5]\n    result = pc.quantile(arr, interpolation='linear')\n    assert result.to_pylist() == [2.5]\n\n    arr = pa.array([1, 2])\n\n    result = pc.quantile(arr, q=[0.25, 0.5, 0.75])\n    assert result.to_pylist() == [1.25, 1.5, 1.75]\n\n    result = pc.quantile(arr, q=[0.25, 0.5, 0.75], interpolation='lower')\n    assert result.to_pylist() == [1, 1, 1]\n    result = pc.quantile(arr, q=[0.25, 0.5, 0.75], interpolation='higher')\n    assert result.to_pylist() == [2, 2, 2]\n    result = pc.quantile(arr, q=[0.25, 0.5, 0.75], interpolation='midpoint')\n    assert result.to_pylist() == [1.5, 1.5, 1.5]\n    result = pc.quantile(arr, q=[0.25, 0.5, 0.75], interpolation='nearest')\n    assert result.to_pylist() == [1, 1, 2]\n    result = pc.quantile(arr, q=[0.25, 0.5, 0.75], interpolation='linear')\n    assert result.to_pylist() == [1.25, 1.5, 1.75]\n\n    # Positional `q`\n    result = pc.quantile(arr, [0.25, 0.5, 0.75], interpolation='linear')\n    assert result.to_pylist() == [1.25, 1.5, 1.75]\n\n    with pytest.raises(ValueError, match=\"Quantile must be between 0 and 1\"):\n        pc.quantile(arr, q=1.1)\n    with pytest.raises(ValueError, match=\"not a valid quantile interpolation\"):\n        pc.quantile(arr, interpolation='zzz')\n\n\ndef test_tdigest():\n    arr = pa.array([1, 2, 3, 4])\n    result = pc.tdigest(arr)\n    assert result.to_pylist() == [2.5]\n\n    arr = pa.chunked_array([pa.array([1, 2]), pa.array([3, 4])])\n    result = pc.tdigest(arr)\n    assert result.to_pylist() == [2.5]\n\n    arr = pa.array([1, 2, 3, 4])\n    result = pc.tdigest(arr, q=[0, 0.5, 1])\n    assert result.to_pylist() == [1, 2.5, 4]\n\n    arr = pa.chunked_array([pa.array([1, 2]), pa.array([3, 4])])\n    result = pc.tdigest(arr, [0, 0.5, 1])  # positional `q`\n    assert result.to_pylist() == [1, 2.5, 4]\n\n\ndef test_fill_null_segfault():\n    # ARROW-12672\n    arr = pa.array([None], pa.bool_()).fill_null(False)\n    result = arr.cast(pa.int8())\n    assert result == pa.array([0], pa.int8())\n\n\ndef test_min_max_element_wise():\n    arr1 = pa.array([1, 2, 3])\n    arr2 = pa.array([3, 1, 2])\n    arr3 = pa.array([2, 3, None])\n\n    result = pc.max_element_wise(arr1, arr2)\n    assert result == pa.array([3, 2, 3])\n    result = pc.min_element_wise(arr1, arr2)\n    assert result == pa.array([1, 1, 2])\n\n    result = pc.max_element_wise(arr1, arr2, arr3)\n    assert result == pa.array([3, 3, 3])\n    result = pc.min_element_wise(arr1, arr2, arr3)\n    assert result == pa.array([1, 1, 2])\n\n    # with specifying the option\n    result = pc.max_element_wise(arr1, arr3, skip_nulls=True)\n    assert result == pa.array([2, 3, 3])\n    result = pc.min_element_wise(arr1, arr3, skip_nulls=True)\n    assert result == pa.array([1, 2, 3])\n    result = pc.max_element_wise(\n        arr1, arr3, options=pc.ElementWiseAggregateOptions())\n    assert result == pa.array([2, 3, 3])\n    result = pc.min_element_wise(\n        arr1, arr3, options=pc.ElementWiseAggregateOptions())\n    assert result == pa.array([1, 2, 3])\n\n    # not skipping nulls\n    result = pc.max_element_wise(arr1, arr3, skip_nulls=False)\n    assert result == pa.array([2, 3, None])\n    result = pc.min_element_wise(arr1, arr3, skip_nulls=False)\n    assert result == pa.array([1, 2, None])\n\n\n@pytest.mark.parametrize('start', (1.25, 10.5, -10.5))\n@pytest.mark.parametrize('skip_nulls', (True, False))\ndef test_cumulative_sum(start, skip_nulls):\n    # Exact tests (e.g., integral types)\n    start_int = int(start)\n    starts = [None, start_int, pa.scalar(start_int, type=pa.int8()),\n              pa.scalar(start_int, type=pa.int64())]\n    for strt in starts:\n        arrays = [\n            pa.array([1, 2, 3]),\n            pa.array([0, None, 20, 30]),\n            pa.chunked_array([[0, None], [20, 30]])\n        ]\n        expected_arrays = [\n            pa.array([1, 3, 6]),\n            pa.array([0, None, 20, 50])\n            if skip_nulls else pa.array([0, None, None, None]),\n            pa.chunked_array([[0, None, 20, 50]])\n            if skip_nulls else pa.chunked_array([[0, None, None, None]])\n        ]\n        for i, arr in enumerate(arrays):\n            result = pc.cumulative_sum(arr, start=strt, skip_nulls=skip_nulls)\n            # Add `start` offset to expected array before comparing\n            expected = pc.add(expected_arrays[i], strt if strt is not None\n                              else 0)\n            assert result.equals(expected)\n\n    starts = [None, start, pa.scalar(start, type=pa.float32()),\n              pa.scalar(start, type=pa.float64())]\n    for strt in starts:\n        arrays = [\n            pa.array([1.125, 2.25, 3.03125]),\n            pa.array([1, np.nan, 2, -3, 4, 5]),\n            pa.array([1, np.nan, None, 3, None, 5])\n        ]\n        expected_arrays = [\n            np.array([1.125, 3.375, 6.40625]),\n            np.array([1, np.nan, np.nan, np.nan, np.nan, np.nan]),\n            np.array([1, np.nan, None, np.nan, None, np.nan])\n            if skip_nulls else np.array([1, np.nan, None, None, None, None])\n        ]\n        for i, arr in enumerate(arrays):\n            result = pc.cumulative_sum(arr, start=strt, skip_nulls=skip_nulls)\n            # Add `start` offset to expected array before comparing\n            expected = pc.add(expected_arrays[i], strt if strt is not None\n                              else 0)\n            np.testing.assert_array_almost_equal(result.to_numpy(\n                zero_copy_only=False), expected.to_numpy(zero_copy_only=False))\n\n    for strt in ['a', pa.scalar('arrow'), 1.1]:\n        with pytest.raises(pa.ArrowInvalid):\n            pc.cumulative_sum([1, 2, 3], start=strt)\n\n\n@pytest.mark.parametrize('start', (1.25, 10.5, -10.5))\n@pytest.mark.parametrize('skip_nulls', (True, False))\ndef test_cumulative_prod(start, skip_nulls):\n    # Exact tests (e.g., integral types)\n    start_int = int(start)\n    starts = [None, start_int, pa.scalar(start_int, type=pa.int8()),\n              pa.scalar(start_int, type=pa.int64())]\n    for strt in starts:\n        arrays = [\n            pa.array([1, 2, 3]),\n            pa.array([1, None, 20, 5]),\n            pa.chunked_array([[1, None], [20, 5]])\n        ]\n        expected_arrays = [\n            pa.array([1, 2, 6]),\n            pa.array([1, None, 20, 100])\n            if skip_nulls else pa.array([1, None, None, None]),\n            pa.chunked_array([[1, None, 20, 100]])\n            if skip_nulls else pa.chunked_array([[1, None, None, None]])\n        ]\n        for i, arr in enumerate(arrays):\n            result = pc.cumulative_prod(arr, start=strt, skip_nulls=skip_nulls)\n            # Multiply `start` offset to expected array before comparing\n            expected = pc.multiply(expected_arrays[i], strt if strt is not None\n                                   else 1)\n            assert result.equals(expected)\n\n    starts = [None, start, pa.scalar(start, type=pa.float32()),\n              pa.scalar(start, type=pa.float64())]\n    for strt in starts:\n        arrays = [\n            pa.array([1.5, 2.5, 3.5]),\n            pa.array([1, np.nan, 2, -3, 4, 5]),\n            pa.array([1, np.nan, None, 3, None, 5])\n        ]\n        expected_arrays = [\n            np.array([1.5, 3.75, 13.125]),\n            np.array([1, np.nan, np.nan, np.nan, np.nan, np.nan]),\n            np.array([1, np.nan, None, np.nan, None, np.nan])\n            if skip_nulls else np.array([1, np.nan, None, None, None, None])\n        ]\n        for i, arr in enumerate(arrays):\n            result = pc.cumulative_prod(arr, start=strt, skip_nulls=skip_nulls)\n            # Multiply `start` offset to expected array before comparing\n            expected = pc.multiply(expected_arrays[i], strt if strt is not None\n                                   else 1)\n            np.testing.assert_array_almost_equal(result.to_numpy(\n                zero_copy_only=False), expected.to_numpy(zero_copy_only=False))\n\n    for strt in ['a', pa.scalar('arrow'), 1.1]:\n        with pytest.raises(pa.ArrowInvalid):\n            pc.cumulative_prod([1, 2, 3], start=strt)\n\n\n@pytest.mark.parametrize('start', (0.5, 3.5, 6.5))\n@pytest.mark.parametrize('skip_nulls', (True, False))\ndef test_cumulative_max(start, skip_nulls):\n    # Exact tests (e.g., integral types)\n    start_int = int(start)\n    starts = [None, start_int, pa.scalar(start_int, type=pa.int8()),\n              pa.scalar(start_int, type=pa.int64())]\n    for strt in starts:\n        arrays = [\n            pa.array([2, 1, 3, 5, 4, 6]),\n            pa.array([2, 1, None, 5, 4, None]),\n            pa.chunked_array([[2, 1, None], [5, 4, None]])\n        ]\n        expected_arrays = [\n            pa.array([2, 2, 3, 5, 5, 6]),\n            pa.array([2, 2, None, 5, 5, None])\n            if skip_nulls else pa.array([2, 2, None, None, None, None]),\n            pa.chunked_array([[2, 2, None, 5, 5, None]])\n            if skip_nulls else\n            pa.chunked_array([[2, 2, None, None, None, None]])\n        ]\n        for i, arr in enumerate(arrays):\n            result = pc.cumulative_max(arr, start=strt, skip_nulls=skip_nulls)\n            # Max `start` offset with expected array before comparing\n            expected = pc.max_element_wise(\n                expected_arrays[i], strt if strt is not None else int(-1e9),\n                skip_nulls=False)\n            assert result.equals(expected)\n\n    starts = [None, start, pa.scalar(start, type=pa.float32()),\n              pa.scalar(start, type=pa.float64())]\n    for strt in starts:\n        arrays = [\n            pa.array([2.5, 1.3, 3.7, 5.1, 4.9, 6.2]),\n            pa.array([2.5, 1.3, 3.7, np.nan, 4.9, 6.2]),\n            pa.array([2.5, 1.3, None, np.nan, 4.9, None])\n        ]\n        expected_arrays = [\n            np.array([2.5, 2.5, 3.7, 5.1, 5.1, 6.2]),\n            np.array([2.5, 2.5, 3.7, 3.7, 4.9, 6.2]),\n            np.array([2.5, 2.5, None, 2.5, 4.9, None])\n            if skip_nulls else np.array([2.5, 2.5, None, None, None, None])\n        ]\n        for i, arr in enumerate(arrays):\n            result = pc.cumulative_max(arr, start=strt, skip_nulls=skip_nulls)\n            # Max `start` offset with expected array before comparing\n            expected = pc.max_element_wise(\n                expected_arrays[i], strt if strt is not None else -1e9,\n                skip_nulls=False)\n            np.testing.assert_array_almost_equal(result.to_numpy(\n                zero_copy_only=False), expected.to_numpy(zero_copy_only=False))\n\n    for strt in ['a', pa.scalar('arrow'), 1.1]:\n        with pytest.raises(pa.ArrowInvalid):\n            pc.cumulative_max([1, 2, 3], start=strt)\n\n\n@pytest.mark.parametrize('start', (0.5, 3.5, 6.5))\n@pytest.mark.parametrize('skip_nulls', (True, False))\ndef test_cumulative_min(start, skip_nulls):\n    # Exact tests (e.g., integral types)\n    start_int = int(start)\n    starts = [None, start_int, pa.scalar(start_int, type=pa.int8()),\n              pa.scalar(start_int, type=pa.int64())]\n    for strt in starts:\n        arrays = [\n            pa.array([5, 6, 4, 2, 3, 1]),\n            pa.array([5, 6, None, 2, 3, None]),\n            pa.chunked_array([[5, 6, None], [2, 3, None]])\n        ]\n        expected_arrays = [\n            pa.array([5, 5, 4, 2, 2, 1]),\n            pa.array([5, 5, None, 2, 2, None])\n            if skip_nulls else pa.array([5, 5, None, None, None, None]),\n            pa.chunked_array([[5, 5, None, 2, 2, None]])\n            if skip_nulls else\n            pa.chunked_array([[5, 5, None, None, None, None]])\n        ]\n        for i, arr in enumerate(arrays):\n            result = pc.cumulative_min(arr, start=strt, skip_nulls=skip_nulls)\n            # Min `start` offset with expected array before comparing\n            expected = pc.min_element_wise(\n                expected_arrays[i], strt if strt is not None else int(1e9),\n                skip_nulls=False)\n            assert result.equals(expected)\n\n    starts = [None, start, pa.scalar(start, type=pa.float32()),\n              pa.scalar(start, type=pa.float64())]\n    for strt in starts:\n        arrays = [\n            pa.array([5.5, 6.3, 4.7, 2.1, 3.9, 1.2]),\n            pa.array([5.5, 6.3, 4.7, np.nan, 3.9, 1.2]),\n            pa.array([5.5, 6.3, None, np.nan, 3.9, None])\n        ]\n        expected_arrays = [\n            np.array([5.5, 5.5, 4.7, 2.1, 2.1, 1.2]),\n            np.array([5.5, 5.5, 4.7, 4.7, 3.9, 1.2]),\n            np.array([5.5, 5.5, None, 5.5, 3.9, None])\n            if skip_nulls else np.array([5.5, 5.5, None, None, None, None])\n        ]\n        for i, arr in enumerate(arrays):\n            result = pc.cumulative_min(arr, start=strt, skip_nulls=skip_nulls)\n            # Min `start` offset with expected array before comparing\n            expected = pc.min_element_wise(\n                expected_arrays[i], strt if strt is not None else 1e9,\n                skip_nulls=False)\n            np.testing.assert_array_almost_equal(result.to_numpy(\n                zero_copy_only=False), expected.to_numpy(zero_copy_only=False))\n\n    for strt in ['a', pa.scalar('arrow'), 1.1]:\n        with pytest.raises(pa.ArrowInvalid):\n            pc.cumulative_max([1, 2, 3], start=strt)\n\n\ndef test_make_struct():\n    assert pc.make_struct(1, 'a').as_py() == {'0': 1, '1': 'a'}\n\n    assert pc.make_struct(1, 'a', field_names=['i', 's']).as_py() == {\n        'i': 1, 's': 'a'}\n\n    assert pc.make_struct([1, 2, 3],\n                          \"a b c\".split()) == pa.StructArray.from_arrays([\n                              [1, 2, 3],\n                              \"a b c\".split()], names='0 1'.split())\n\n    with pytest.raises(ValueError,\n                       match=\"Array arguments must all be the same length\"):\n        pc.make_struct([1, 2, 3, 4], \"a b c\".split())\n\n    with pytest.raises(ValueError, match=\"0 arguments but 2 field names\"):\n        pc.make_struct(field_names=['one', 'two'])\n\n\ndef test_map_lookup():\n    ty = pa.map_(pa.utf8(), pa.int32())\n    arr = pa.array([[('one', 1), ('two', 2)], [('none', 3)],\n                    [], [('one', 5), ('one', 7)], None], type=ty)\n    result_first = pa.array([1, None, None, 5, None], type=pa.int32())\n    result_last = pa.array([1, None, None, 7, None], type=pa.int32())\n    result_all = pa.array([[1], None, None, [5, 7], None],\n                          type=pa.list_(pa.int32()))\n\n    assert pc.map_lookup(arr, 'one', 'first') == result_first\n    assert pc.map_lookup(arr, pa.scalar(\n        'one', type=pa.utf8()), 'first') == result_first\n    assert pc.map_lookup(arr, pa.scalar(\n        'one', type=pa.utf8()), 'last') == result_last\n    assert pc.map_lookup(arr, pa.scalar(\n        'one', type=pa.utf8()), 'all') == result_all\n\n\ndef test_struct_fields_options():\n    a = pa.array([4, 5, 6], type=pa.int64())\n    b = pa.array([\"bar\", None, \"\"])\n    c = pa.StructArray.from_arrays([a, b], [\"a\", \"b\"])\n    arr = pa.StructArray.from_arrays([a, c], [\"a\", \"c\"])\n\n    assert pc.struct_field(arr, '.c.b') == b\n    assert pc.struct_field(arr, b'.c.b') == b\n    assert pc.struct_field(arr, ['c', 'b']) == b\n    assert pc.struct_field(arr, [1, 'b']) == b\n    assert pc.struct_field(arr, (b'c', 'b')) == b\n    assert pc.struct_field(arr, pc.field(('c', 'b'))) == b\n\n    assert pc.struct_field(arr, '.a') == a\n    assert pc.struct_field(arr, ['a']) == a\n    assert pc.struct_field(arr, 'a') == a\n    assert pc.struct_field(arr, pc.field(('a',))) == a\n\n    assert pc.struct_field(arr, indices=[1, 1]) == b\n    assert pc.struct_field(arr, (1, 1)) == b\n    assert pc.struct_field(arr, [0]) == a\n    assert pc.struct_field(arr, []) == arr\n\n    with pytest.raises(pa.ArrowInvalid, match=\"No match for FieldRef\"):\n        pc.struct_field(arr, 'foo')\n\n    with pytest.raises(pa.ArrowInvalid, match=\"No match for FieldRef\"):\n        pc.struct_field(arr, '.c.foo')\n\n    # drill into a non-struct array and continue to ask for a field\n    with pytest.raises(pa.ArrowInvalid, match=\"No match for FieldRef\"):\n        pc.struct_field(arr, '.a.foo')\n\n    # TODO: https://issues.apache.org/jira/browse/ARROW-14853\n    # assert pc.struct_field(arr) == arr\n\n\ndef test_case_when():\n    assert pc.case_when(pc.make_struct([True, False, None],\n                                       [False, True, None]),\n                        [1, 2, 3],\n                        [11, 12, 13]) == pa.array([1, 12, None])\n\n\ndef test_list_element():\n    element_type = pa.struct([('a', pa.float64()), ('b', pa.int8())])\n    list_type = pa.list_(element_type)\n    l1 = [{'a': .4, 'b': 2}, None, {'a': .2, 'b': 4}, None, {'a': 5.6, 'b': 6}]\n    l2 = [None, {'a': .52, 'b': 3}, {'a': .7, 'b': 4}, None, {'a': .6, 'b': 8}]\n    lists = pa.array([l1, l2], list_type)\n\n    index = 1\n    result = pa.compute.list_element(lists, index)\n    expected = pa.array([None, {'a': 0.52, 'b': 3}], element_type)\n    assert result.equals(expected)\n\n    index = 4\n    result = pa.compute.list_element(lists, index)\n    expected = pa.array([{'a': 5.6, 'b': 6}, {'a': .6, 'b': 8}], element_type)\n    assert result.equals(expected)\n\n\ndef test_count_distinct():\n    samples = [datetime.datetime(year=y, month=1, day=1) for y in range(1992, 2092)]\n    arr = pa.array(samples, pa.timestamp(\"ns\"))\n    assert pc.count_distinct(arr) == pa.scalar(len(samples), type=pa.int64())\n\n\ndef test_count_distinct_options():\n    arr = pa.array([1, 2, 3, None, None])\n    assert pc.count_distinct(arr).as_py() == 3\n    assert pc.count_distinct(arr, mode='only_valid').as_py() == 3\n    assert pc.count_distinct(arr, mode='only_null').as_py() == 1\n    assert pc.count_distinct(arr, mode='all').as_py() == 4\n    assert pc.count_distinct(arr, 'all').as_py() == 4\n\n\ndef test_utf8_normalize():\n    arr = pa.array([\"01\u00b23\"])\n    assert pc.utf8_normalize(arr, form=\"NFC\") == arr\n    assert pc.utf8_normalize(arr, form=\"NFKC\") == pa.array([\"0123\"])\n    assert pc.utf8_normalize(arr, \"NFD\") == arr\n    assert pc.utf8_normalize(arr, \"NFKD\") == pa.array([\"0123\"])\n    with pytest.raises(\n            ValueError,\n            match='\"NFZ\" is not a valid Unicode normalization form'):\n        pc.utf8_normalize(arr, form=\"NFZ\")\n\n\ndef test_random():\n    # (note negative integer initializers are accepted)\n    for initializer in ['system', 42, -42, b\"abcdef\"]:\n        assert pc.random(0, initializer=initializer) == \\\n            pa.array([], type=pa.float64())\n\n    # System random initialization => outputs all distinct\n    arrays = [tuple(pc.random(100).to_pylist()) for i in range(10)]\n    assert len(set(arrays)) == len(arrays)\n\n    arrays = [tuple(pc.random(100, initializer=i % 7).to_pylist())\n              for i in range(0, 100)]\n    assert len(set(arrays)) == 7\n\n    # Arbitrary hashable objects can be given as initializer\n    initializers = [object(), (4, 5, 6), \"foo\"]\n    initializers.extend(os.urandom(10) for i in range(10))\n    arrays = [tuple(pc.random(100, initializer=i).to_pylist())\n              for i in initializers]\n    assert len(set(arrays)) == len(arrays)\n\n    with pytest.raises(TypeError,\n                       match=r\"initializer should be 'system', an integer, \"\n                             r\"or a hashable object; got \\[\\]\"):\n        pc.random(100, initializer=[])\n\n\n@pytest.mark.parametrize(\n    \"tiebreaker,expected_values\",\n    [(\"min\", [3, 1, 4, 6, 4, 6, 1]),\n     (\"max\", [3, 2, 5, 7, 5, 7, 2]),\n     (\"first\", [3, 1, 4, 6, 5, 7, 2]),\n     (\"dense\", [2, 1, 3, 4, 3, 4, 1])]\n)\ndef test_rank_options_tiebreaker(tiebreaker, expected_values):\n    arr = pa.array([1.2, 0.0, 5.3, None, 5.3, None, 0.0])\n    rank_options = pc.RankOptions(sort_keys=\"ascending\",\n                                  null_placement=\"at_end\",\n                                  tiebreaker=tiebreaker)\n    result = pc.rank(arr, options=rank_options)\n    expected = pa.array(expected_values, type=pa.uint64())\n    assert result.equals(expected)\n\n\ndef test_rank_options():\n    arr = pa.array([1.2, 0.0, 5.3, None, 5.3, None, 0.0])\n    expected = pa.array([3, 1, 4, 6, 5, 7, 2], type=pa.uint64())\n\n    # Ensure rank can be called without specifying options\n    result = pc.rank(arr)\n    assert result.equals(expected)\n\n    # Ensure default RankOptions\n    result = pc.rank(arr, options=pc.RankOptions())\n    assert result.equals(expected)\n\n    # Ensure sort_keys tuple usage\n    result = pc.rank(arr, options=pc.RankOptions(\n        sort_keys=[(\"b\", \"ascending\")])\n    )\n    assert result.equals(expected)\n\n    result = pc.rank(arr, null_placement=\"at_start\")\n    expected_at_start = pa.array([5, 3, 6, 1, 7, 2, 4], type=pa.uint64())\n    assert result.equals(expected_at_start)\n\n    result = pc.rank(arr, sort_keys=\"descending\")\n    expected_descending = pa.array([3, 4, 1, 6, 2, 7, 5], type=pa.uint64())\n    assert result.equals(expected_descending)\n\n    with pytest.raises(ValueError,\n                       match=r'\"NonExisting\" is not a valid tiebreaker'):\n        pc.RankOptions(sort_keys=\"descending\",\n                       null_placement=\"at_end\",\n                       tiebreaker=\"NonExisting\")\n\n\ndef create_sample_expressions():\n    # We need a schema for substrait conversion\n    schema = pa.schema([pa.field(\"i64\", pa.int64()), pa.field(\n        \"foo\", pa.struct([pa.field(\"bar\", pa.string())]))])\n\n    # Creates a bunch of sample expressions for testing\n    # serialization and deserialization. The expressions are categorized\n    # to reflect certain nuances in Substrait conversion.\n    a = pc.scalar(1)\n    b = pc.scalar(1.1)\n    c = pc.scalar(True)\n    d = pc.scalar(\"string\")\n    e = pc.scalar(None)\n    f = pc.scalar({'a': 1})\n    g = pc.scalar(pa.scalar(1))\n    h = pc.scalar(np.int64(2))\n    j = pc.scalar(False)\n\n    # These expression consist entirely of literals\n    literal_exprs = [a, b, c, d, e, g, h, j]\n\n    # These expressions include at least one function call\n    exprs_with_call = [a == b, a != b, a > b, c & j, c | j, ~c, d.is_valid(),\n                       a + b, a - b, a * b, a / b, pc.negate(a),\n                       pc.add(a, b), pc.subtract(a, b), pc.divide(a, b),\n                       pc.multiply(a, b), pc.power(a, a), pc.sqrt(a),\n                       pc.exp(b), pc.cos(b), pc.sin(b), pc.tan(b),\n                       pc.acos(b), pc.atan(b), pc.asin(b), pc.atan2(b, b),\n                       pc.abs(b), pc.sign(a), pc.bit_wise_not(a),\n                       pc.bit_wise_and(a, a), pc.bit_wise_or(a, a),\n                       pc.bit_wise_xor(a, a), pc.is_nan(b), pc.is_finite(b),\n                       pc.coalesce(a, b),\n                       a.cast(pa.int32(), safe=False)]\n\n    # These expressions test out various reference styles and may include function\n    # calls.  Named references are used here.\n    exprs_with_ref = [pc.field('i64') > 5, pc.field('i64') == 5,\n                      pc.field('i64') == 7,\n                      pc.field(('foo', 'bar')) == 'value',\n                      pc.field('foo', 'bar') == 'value']\n\n    # Similar to above but these use numeric references instead of string refs\n    exprs_with_numeric_refs = [pc.field(0) > 5, pc.field(0) == 5,\n                               pc.field(0) == 7,\n                               pc.field((1, 0)) == 'value',\n                               pc.field(1, 0) == 'value']\n\n    # Expressions that behave uniquely when converting to/from substrait\n    special_cases = [\n        f,  # Struct literals lose their field names\n        a.isin([1, 2, 3]),  # isin converts to an or list\n        pc.field('i64').is_null()  # pyarrow always specifies a FunctionOptions\n                                   # for is_null which, being the default, is\n                                   # dropped on serialization\n    ]\n\n    all_exprs = literal_exprs.copy()\n    all_exprs += exprs_with_call\n    all_exprs += exprs_with_ref\n    all_exprs += special_cases\n\n    return {\n        \"all\": all_exprs,\n        \"literals\": literal_exprs,\n        \"calls\": exprs_with_call,\n        \"refs\": exprs_with_ref,\n        \"numeric_refs\": exprs_with_numeric_refs,\n        \"special\": special_cases,\n        \"schema\": schema\n    }\n\n# Tests the Arrow-specific serialization mechanism\n\n\ndef test_expression_serialization_arrow(pickle_module):\n    for expr in create_sample_expressions()[\"all\"]:\n        assert isinstance(expr, pc.Expression)\n        restored = pickle_module.loads(pickle_module.dumps(expr))\n        assert expr.equals(restored)\n\n\n@pytest.mark.substrait\ndef test_expression_serialization_substrait():\n\n    exprs = create_sample_expressions()\n    schema = exprs[\"schema\"]\n\n    # Basic literals don't change on binding and so they will round\n    # trip without any change\n    for expr in exprs[\"literals\"]:\n        serialized = expr.to_substrait(schema)\n        deserialized = pc.Expression.from_substrait(serialized)\n        assert expr.equals(deserialized)\n\n    # Expressions are bound when they get serialized.  Since bound\n    # expressions are not equal to their unbound variants we cannot\n    # compare the round tripped with the original\n    for expr in exprs[\"calls\"]:\n        serialized = expr.to_substrait(schema)\n        deserialized = pc.Expression.from_substrait(serialized)\n        # We can't compare the expressions themselves because of the bound\n        # unbound difference. But we can compare the string representation\n        assert str(deserialized) == str(expr)\n        serialized_again = deserialized.to_substrait(schema)\n        deserialized_again = pc.Expression.from_substrait(serialized_again)\n        assert deserialized.equals(deserialized_again)\n\n    for expr, expr_norm in zip(exprs[\"refs\"], exprs[\"numeric_refs\"]):\n        serialized = expr.to_substrait(schema)\n        deserialized = pc.Expression.from_substrait(serialized)\n        assert str(deserialized) == str(expr_norm)\n        serialized_again = deserialized.to_substrait(schema)\n        deserialized_again = pc.Expression.from_substrait(serialized_again)\n        assert deserialized.equals(deserialized_again)\n\n    # For the special cases we get various wrinkles in serialization but we\n    # should always get the same thing from round tripping twice\n    for expr in exprs[\"special\"]:\n        serialized = expr.to_substrait(schema)\n        deserialized = pc.Expression.from_substrait(serialized)\n        serialized_again = deserialized.to_substrait(schema)\n        deserialized_again = pc.Expression.from_substrait(serialized_again)\n        assert deserialized.equals(deserialized_again)\n\n    # Special case, we lose the field names of struct literals\n    f = exprs[\"special\"][0]\n    serialized = f.to_substrait(schema)\n    deserialized = pc.Expression.from_substrait(serialized)\n    assert deserialized.equals(pc.scalar({'': 1}))\n\n    # Special case, is_in converts to a == opt[0] || a == opt[1] ...\n    a = pc.scalar(1)\n    expr = a.isin([1, 2, 3])\n    target = (a == 1) | (a == 2) | (a == 3)\n    serialized = expr.to_substrait(schema)\n    deserialized = pc.Expression.from_substrait(serialized)\n    # Compare str's here to bypass the bound/unbound difference\n    assert str(target) == str(deserialized)\n    serialized_again = deserialized.to_substrait(schema)\n    deserialized_again = pc.Expression.from_substrait(serialized_again)\n    assert deserialized.equals(deserialized_again)\n\n\ndef test_expression_construction():\n    zero = pc.scalar(0)\n    one = pc.scalar(1)\n    true = pc.scalar(True)\n    false = pc.scalar(False)\n    string = pc.scalar(\"string\")\n    field = pc.field(\"field\")\n    nested_mixed_types = pc.field(b\"a\", 1, \"b\")\n    nested_field = pc.field((\"nested\", \"field\"))\n    nested_field2 = pc.field(\"nested\", \"field\")\n\n    zero | one == string\n    ~true == false\n    for typ in (\"bool\", pa.bool_()):\n        field.cast(typ) == true\n\n    field.isin([1, 2])\n    nested_mixed_types.isin([\"foo\", \"bar\"])\n    nested_field.isin([\"foo\", \"bar\"])\n    nested_field2.isin([\"foo\", \"bar\"])\n\n    with pytest.raises(TypeError):\n        field.isin(1)\n\n    with pytest.raises(pa.ArrowInvalid):\n        field != object()\n\n\ndef test_expression_boolean_operators():\n    # https://issues.apache.org/jira/browse/ARROW-11412\n    true = pc.scalar(True)\n    false = pc.scalar(False)\n\n    with pytest.raises(ValueError, match=\"cannot be evaluated to python True\"):\n        true and false\n\n    with pytest.raises(ValueError, match=\"cannot be evaluated to python True\"):\n        true or false\n\n    with pytest.raises(ValueError, match=\"cannot be evaluated to python True\"):\n        bool(true)\n\n    with pytest.raises(ValueError, match=\"cannot be evaluated to python True\"):\n        not true\n\n\ndef test_expression_call_function():\n    field = pc.field(\"field\")\n\n    # no options\n    assert str(pc.hour(field)) == \"hour(field)\"\n\n    # default options\n    assert str(pc.round(field)) == \"round(field)\"\n    # specified options\n    assert str(pc.round(field, ndigits=1)) == \\\n        \"round(field, {ndigits=1, round_mode=HALF_TO_EVEN})\"\n\n    # Will convert non-expression arguments if possible\n    assert str(pc.add(field, 1)) == \"add(field, 1)\"\n    assert str(pc.add(field, pa.scalar(1))) == \"add(field, 1)\"\n\n    # Invalid pc.scalar input gives original error message\n    msg = \"only other expressions allowed as arguments\"\n    with pytest.raises(TypeError, match=msg):\n        pc.add(field, object)\n\n\ndef test_cast_table_raises():\n    table = pa.table({'a': [1, 2]})\n\n    with pytest.raises(pa.lib.ArrowTypeError):\n        pc.cast(table, pa.int64())\n\n\n@pytest.mark.parametrize(\"start,stop,expected\", (\n    (0, None, [[1, 2, 3], [4, 5, None], [6, None, None], None]),\n    (0, 1, [[1], [4], [6], None]),\n    (0, 2, [[1, 2], [4, 5], [6, None], None]),\n    (1, 2, [[2], [5], [None], None]),\n    (2, 4, [[3, None], [None, None], [None, None], None])\n))\n@pytest.mark.parametrize(\"step\", (1, 2))\n@pytest.mark.parametrize(\"value_type\", (pa.string, pa.int16, pa.float64))\n@pytest.mark.parametrize(\"list_type\", (pa.list_, pa.large_list, \"fixed\"))\ndef test_list_slice_output_fixed(start, stop, step, expected, value_type,\n                                 list_type):\n    if list_type == \"fixed\":\n        arr = pa.array([[1, 2, 3], [4, 5, None], [6, None, None], None],\n                       pa.list_(pa.int8(), 3)).cast(pa.list_(value_type(), 3))\n    else:\n        arr = pa.array([[1, 2, 3], [4, 5], [6], None],\n                       pa.list_(pa.int8())).cast(list_type(value_type()))\n\n    args = arr, start, stop, step, True\n    if stop is None and list_type != \"fixed\":\n        msg = (\"Unable to produce FixedSizeListArray from \"\n               \"non-FixedSizeListArray without `stop` being set.\")\n        with pytest.raises(pa.ArrowInvalid, match=msg):\n            pc.list_slice(*args)\n    else:\n        result = pc.list_slice(*args)\n        pylist = result.cast(pa.list_(pa.int8(),\n                             result.type.list_size)).to_pylist()\n        assert pylist == [e[::step] if e else e for e in expected]\n\n\n@pytest.mark.parametrize(\"start,stop\", (\n    (0, None,),\n    (0, 1,),\n    (0, 2,),\n    (1, 2,),\n    (2, 4,)\n))\n@pytest.mark.parametrize(\"step\", (1, 2))\n@pytest.mark.parametrize(\"value_type\", (pa.string, pa.int16, pa.float64))\n@pytest.mark.parametrize(\"list_type\", (pa.list_, pa.large_list, \"fixed\"))\ndef test_list_slice_output_variable(start, stop, step, value_type, list_type):\n    if list_type == \"fixed\":\n        data = [[1, 2, 3], [4, 5, None], [6, None, None], None]\n        arr = pa.array(\n            data,\n            pa.list_(pa.int8(), 3)).cast(pa.list_(value_type(), 3))\n    else:\n        data = [[1, 2, 3], [4, 5], [6], None]\n        arr = pa.array(data,\n                       pa.list_(pa.int8())).cast(list_type(value_type()))\n\n    # Gets same list type (ListArray vs LargeList)\n    if list_type == \"fixed\":\n        list_type = pa.list_  # non fixed output type\n\n    result = pc.list_slice(arr, start, stop, step,\n                           return_fixed_size_list=False)\n    assert result.type == list_type(value_type())\n\n    pylist = result.cast(pa.list_(pa.int8())).to_pylist()\n\n    # Variable output slicing follows Python's slice semantics\n    expected = [d[start:stop:step] if d is not None else None for d in data]\n    assert pylist == expected\n\n\n@pytest.mark.parametrize(\"return_fixed_size\", (True, False, None))\n@pytest.mark.parametrize(\"type\", (\n    lambda: pa.list_(pa.field('col', pa.int8())),\n    lambda: pa.list_(pa.field('col', pa.int8()), 1),\n    lambda: pa.large_list(pa.field('col', pa.int8()))))\ndef test_list_slice_field_names_retained(return_fixed_size, type):\n    arr = pa.array([[1]], type())\n    out = pc.list_slice(arr, 0, 1, return_fixed_size_list=return_fixed_size)\n    assert arr.type.field(0).name == out.type.field(0).name\n\n    # Verify out type matches in type if return_fixed_size_list==None\n    if return_fixed_size is None:\n        assert arr.type == out.type\n\n\ndef test_list_slice_bad_parameters():\n    arr = pa.array([[1]], pa.list_(pa.int8(), 1))\n    msg = r\"`start`(.*) should be greater than 0 and smaller than `stop`(.*)\"\n    with pytest.raises(pa.ArrowInvalid, match=msg):\n        pc.list_slice(arr, -1, 1)  # negative start?\n    with pytest.raises(pa.ArrowInvalid, match=msg):\n        pc.list_slice(arr, 2, 1)  # start > stop?\n\n    # TODO(ARROW-18281): start==stop -> empty lists\n    with pytest.raises(pa.ArrowInvalid, match=msg):\n        pc.list_slice(arr, 0, 0)  # start == stop?\n\n    # Step not >= 1\n    msg = \"`step` must be >= 1, got: \"\n    with pytest.raises(pa.ArrowInvalid, match=msg + \"0\"):\n        pc.list_slice(arr, 0, 1, step=0)\n    with pytest.raises(pa.ArrowInvalid, match=msg + \"-1\"):\n        pc.list_slice(arr, 0, 1, step=-1)\n\n\ndef check_run_end_encode_decode(run_end_encode_opts=None):\n    arr = pa.array([1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n    encoded = pc.run_end_encode(arr, options=run_end_encode_opts)\n    decoded = pc.run_end_decode(encoded)\n    assert decoded.type == arr.type\n    assert decoded.equals(arr)\n\n\ndef test_run_end_encode():\n    check_run_end_encode_decode()\n    check_run_end_encode_decode(pc.RunEndEncodeOptions(pa.int16()))\n    check_run_end_encode_decode(pc.RunEndEncodeOptions('int32'))\n    check_run_end_encode_decode(pc.RunEndEncodeOptions(pa.int64()))\n\n\ndef test_pairwise_diff():\n    arr = pa.array([1, 2, 3, None, 4, 5])\n    expected = pa.array([None, 1, 1, None, None, 1])\n    result = pa.compute.pairwise_diff(arr, period=1)\n    assert result.equals(expected)\n\n    arr = pa.array([1, 2, 3, None, 4, 5])\n    expected = pa.array([None, None, 2, None, 1, None])\n    result = pa.compute.pairwise_diff(arr, period=2)\n    assert result.equals(expected)\n\n    # negative period\n    arr = pa.array([1, 2, 3, None, 4, 5], type=pa.int8())\n    expected = pa.array([-1, -1, None, None, -1, None], type=pa.int8())\n    result = pa.compute.pairwise_diff(arr, period=-1)\n    assert result.equals(expected)\n\n    # wrap around overflow\n    arr = pa.array([1, 2, 3, None, 4, 5], type=pa.uint8())\n    expected = pa.array([255, 255, None, None, 255, None], type=pa.uint8())\n    result = pa.compute.pairwise_diff(arr, period=-1)\n    assert result.equals(expected)\n\n    # fail on overflow\n    arr = pa.array([1, 2, 3, None, 4, 5], type=pa.uint8())\n    with pytest.raises(pa.ArrowInvalid,\n                       match=\"overflow\"):\n        pa.compute.pairwise_diff_checked(arr, period=-1)\n", "python/pyarrow/tests/__init__.py": "", "python/pyarrow/tests/test_types.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import OrderedDict\nfrom collections.abc import Iterator\nfrom functools import partial\nimport datetime\nimport sys\n\nimport pytest\nimport hypothesis as h\nimport hypothesis.strategies as st\ntry:\n    import hypothesis.extra.pytz as tzst\nexcept ImportError:\n    tzst = None\nimport weakref\n\nimport numpy as np\nimport pyarrow as pa\nimport pyarrow.types as types\nimport pyarrow.tests.strategies as past\n\n\ndef get_many_types():\n    # returning them from a function is required because of pa.dictionary\n    # type holds a pyarrow array and test_array.py::test_toal_bytes_allocated\n    # checks that the default memory pool has zero allocated bytes\n    return (\n        pa.null(),\n        pa.bool_(),\n        pa.int32(),\n        pa.time32('s'),\n        pa.time64('us'),\n        pa.date32(),\n        pa.timestamp('us'),\n        pa.timestamp('us', tz='UTC'),\n        pa.timestamp('us', tz='Europe/Paris'),\n        pa.duration('s'),\n        pa.float16(),\n        pa.float32(),\n        pa.float64(),\n        pa.decimal128(19, 4),\n        pa.decimal256(76, 38),\n        pa.string(),\n        pa.binary(),\n        pa.binary(10),\n        pa.large_string(),\n        pa.large_binary(),\n        pa.string_view(),\n        pa.binary_view(),\n        pa.list_(pa.int32()),\n        pa.list_(pa.int32(), 2),\n        pa.large_list(pa.uint16()),\n        pa.list_view(pa.int32()),\n        pa.large_list_view(pa.uint16()),\n        pa.map_(pa.string(), pa.int32()),\n        pa.map_(pa.field('key', pa.int32(), nullable=False),\n                pa.field('value', pa.int32())),\n        pa.struct([pa.field('a', pa.int32()),\n                   pa.field('b', pa.int8()),\n                   pa.field('c', pa.string())]),\n        pa.struct([pa.field('a', pa.int32(), nullable=False),\n                   pa.field('b', pa.int8(), nullable=False),\n                   pa.field('c', pa.string())]),\n        pa.union([pa.field('a', pa.binary(10)),\n                  pa.field('b', pa.string())], mode=pa.lib.UnionMode_DENSE),\n        pa.union([pa.field('a', pa.binary(10)),\n                  pa.field('b', pa.string())], mode=pa.lib.UnionMode_DENSE,\n                 type_codes=[4, 8]),\n        pa.union([pa.field('a', pa.binary(10)),\n                  pa.field('b', pa.string())], mode=pa.lib.UnionMode_SPARSE),\n        pa.union([pa.field('a', pa.binary(10), nullable=False),\n                  pa.field('b', pa.string())], mode=pa.lib.UnionMode_SPARSE),\n        pa.dictionary(pa.int32(), pa.string()),\n        pa.run_end_encoded(pa.int16(), pa.int32()),\n        pa.run_end_encoded(pa.int32(), pa.string()),\n        pa.run_end_encoded(pa.int64(), pa.uint8())\n    )\n\n\ndef test_is_boolean():\n    assert types.is_boolean(pa.bool_())\n    assert not types.is_boolean(pa.int8())\n\n\ndef test_is_integer():\n    signed_ints = [pa.int8(), pa.int16(), pa.int32(), pa.int64()]\n    unsigned_ints = [pa.uint8(), pa.uint16(), pa.uint32(), pa.uint64()]\n\n    for t in signed_ints + unsigned_ints:\n        assert types.is_integer(t)\n\n    for t in signed_ints:\n        assert types.is_signed_integer(t)\n        assert not types.is_unsigned_integer(t)\n\n    for t in unsigned_ints:\n        assert types.is_unsigned_integer(t)\n        assert not types.is_signed_integer(t)\n\n    assert not types.is_integer(pa.float32())\n    assert not types.is_signed_integer(pa.float32())\n\n\ndef test_is_floating():\n    for t in [pa.float16(), pa.float32(), pa.float64()]:\n        assert types.is_floating(t)\n\n    assert not types.is_floating(pa.int32())\n\n\ndef test_is_null():\n    assert types.is_null(pa.null())\n    assert not types.is_null(pa.list_(pa.int32()))\n\n\ndef test_null_field_may_not_be_non_nullable():\n    # ARROW-7273\n    with pytest.raises(ValueError):\n        pa.field('f0', pa.null(), nullable=False)\n\n\ndef test_is_decimal():\n    decimal128 = pa.decimal128(19, 4)\n    decimal256 = pa.decimal256(76, 38)\n    int32 = pa.int32()\n\n    assert types.is_decimal(decimal128)\n    assert types.is_decimal(decimal256)\n    assert not types.is_decimal(int32)\n\n    assert types.is_decimal128(decimal128)\n    assert not types.is_decimal128(decimal256)\n    assert not types.is_decimal128(int32)\n\n    assert not types.is_decimal256(decimal128)\n    assert types.is_decimal256(decimal256)\n    assert not types.is_decimal256(int32)\n\n\ndef test_is_list():\n    a = pa.list_(pa.int32())\n    b = pa.large_list(pa.int32())\n    c = pa.list_(pa.int32(), 3)\n\n    assert types.is_list(a)\n    assert not types.is_large_list(a)\n    assert not types.is_fixed_size_list(a)\n    assert types.is_large_list(b)\n    assert not types.is_list(b)\n    assert not types.is_fixed_size_list(b)\n    assert types.is_fixed_size_list(c)\n    assert not types.is_list(c)\n    assert not types.is_large_list(c)\n\n    assert not types.is_list(pa.int32())\n\n\ndef test_is_list_view():\n    a = pa.list_view(pa.int32())\n    b = pa.large_list_view(pa.int32())\n\n    assert types.is_list_view(a)\n    assert not types.is_large_list_view(a)\n    assert not types.is_list(a)\n    assert types.is_large_list_view(b)\n    assert not types.is_list_view(b)\n    assert not types.is_large_list(b)\n\n\ndef test_is_map():\n    m = pa.map_(pa.utf8(), pa.int32())\n\n    assert types.is_map(m)\n    assert not types.is_map(pa.int32())\n\n    fields = pa.map_(pa.field('key_name', pa.utf8(), nullable=False),\n                     pa.field('value_name', pa.int32()))\n    assert types.is_map(fields)\n\n    entries_type = pa.struct([pa.field('key', pa.int8()),\n                              pa.field('value', pa.int8())])\n    list_type = pa.list_(entries_type)\n    assert not types.is_map(list_type)\n\n\ndef test_is_dictionary():\n    assert types.is_dictionary(pa.dictionary(pa.int32(), pa.string()))\n    assert not types.is_dictionary(pa.int32())\n\n\ndef test_is_nested_or_struct():\n    struct_ex = pa.struct([pa.field('a', pa.int32()),\n                           pa.field('b', pa.int8()),\n                           pa.field('c', pa.string())])\n\n    assert types.is_struct(struct_ex)\n    assert not types.is_struct(pa.list_(pa.int32()))\n\n    assert types.is_nested(struct_ex)\n    assert types.is_nested(pa.list_(pa.int32()))\n    assert types.is_nested(pa.list_(pa.int32(), 3))\n    assert types.is_nested(pa.large_list(pa.int32()))\n    assert types.is_nested(pa.list_view(pa.int32()))\n    assert types.is_nested(pa.large_list_view(pa.int32()))\n    assert not types.is_nested(pa.int32())\n\n\ndef test_is_union():\n    for mode in [pa.lib.UnionMode_SPARSE, pa.lib.UnionMode_DENSE]:\n        assert types.is_union(pa.union([pa.field('a', pa.int32()),\n                                        pa.field('b', pa.int8()),\n                                        pa.field('c', pa.string())],\n                                       mode=mode))\n    assert not types.is_union(pa.list_(pa.int32()))\n\n\ndef test_is_run_end_encoded():\n    assert types.is_run_end_encoded(pa.run_end_encoded(pa.int32(), pa.int64()))\n    assert not types.is_run_end_encoded(pa.utf8())\n\n\n# TODO(wesm): is_map, once implemented\n\n\ndef test_is_binary_string():\n    assert types.is_binary(pa.binary())\n    assert not types.is_binary(pa.string())\n    assert not types.is_binary(pa.large_binary())\n    assert not types.is_binary(pa.large_string())\n\n    assert types.is_string(pa.string())\n    assert types.is_unicode(pa.string())\n    assert not types.is_string(pa.binary())\n    assert not types.is_string(pa.large_string())\n    assert not types.is_string(pa.large_binary())\n\n    assert types.is_large_binary(pa.large_binary())\n    assert not types.is_large_binary(pa.large_string())\n    assert not types.is_large_binary(pa.binary())\n    assert not types.is_large_binary(pa.string())\n\n    assert types.is_large_string(pa.large_string())\n    assert not types.is_large_string(pa.large_binary())\n    assert not types.is_large_string(pa.string())\n    assert not types.is_large_string(pa.binary())\n\n    assert types.is_fixed_size_binary(pa.binary(5))\n    assert not types.is_fixed_size_binary(pa.binary())\n\n    assert types.is_string_view(pa.string_view())\n    assert not types.is_string_view(pa.string())\n    assert types.is_binary_view(pa.binary_view())\n    assert not types.is_binary_view(pa.binary())\n    assert not types.is_binary_view(pa.string_view())\n\n\ndef test_is_temporal_date_time_timestamp():\n    date_types = [pa.date32(), pa.date64()]\n    time_types = [pa.time32('s'), pa.time64('ns')]\n    timestamp_types = [pa.timestamp('ms')]\n    duration_types = [pa.duration('ms')]\n    interval_types = [pa.month_day_nano_interval()]\n\n    for case in (date_types + time_types + timestamp_types + duration_types +\n                 interval_types):\n        assert types.is_temporal(case)\n\n    for case in date_types:\n        assert types.is_date(case)\n        assert not types.is_time(case)\n        assert not types.is_timestamp(case)\n        assert not types.is_duration(case)\n        assert not types.is_interval(case)\n\n    for case in time_types:\n        assert types.is_time(case)\n        assert not types.is_date(case)\n        assert not types.is_timestamp(case)\n        assert not types.is_duration(case)\n        assert not types.is_interval(case)\n\n    for case in timestamp_types:\n        assert types.is_timestamp(case)\n        assert not types.is_date(case)\n        assert not types.is_time(case)\n        assert not types.is_duration(case)\n        assert not types.is_interval(case)\n\n    for case in duration_types:\n        assert types.is_duration(case)\n        assert not types.is_date(case)\n        assert not types.is_time(case)\n        assert not types.is_timestamp(case)\n        assert not types.is_interval(case)\n\n    for case in interval_types:\n        assert types.is_interval(case)\n        assert not types.is_date(case)\n        assert not types.is_time(case)\n        assert not types.is_timestamp(case)\n\n    assert not types.is_temporal(pa.int32())\n\n\ndef test_is_primitive():\n    assert types.is_primitive(pa.int32())\n    assert not types.is_primitive(pa.list_(pa.int32()))\n\n\n@pytest.mark.parametrize(('tz', 'expected'), [\n    (datetime.timezone.utc, 'UTC'),\n    (datetime.timezone(datetime.timedelta(hours=1, minutes=30)), '+01:30')\n])\ndef test_tzinfo_to_string(tz, expected):\n    assert pa.lib.tzinfo_to_string(tz) == expected\n\n\ndef test_pytz_tzinfo_to_string():\n    pytz = pytest.importorskip(\"pytz\")\n\n    tz = [pytz.utc, pytz.timezone('Europe/Paris')]\n    expected = ['UTC', 'Europe/Paris']\n    assert [pa.lib.tzinfo_to_string(i) for i in tz] == expected\n\n    # StaticTzInfo.tzname returns with '-09' so we need to infer the timezone's\n    # name from the tzinfo.zone attribute\n    tz = [pytz.timezone('Etc/GMT-9'), pytz.FixedOffset(180)]\n    expected = ['Etc/GMT-9', '+03:00']\n    assert [pa.lib.tzinfo_to_string(i) for i in tz] == expected\n\n\ndef test_dateutil_tzinfo_to_string():\n    if sys.platform == 'win32':\n        # Skip due to new release of python-dateutil\n        # https://github.com/apache/arrow/issues/40485\n        pytest.skip('Skip on Win due to new release of python-dateutil')\n\n    pytest.importorskip(\"dateutil\")\n    import dateutil.tz\n\n    tz = dateutil.tz.UTC\n    assert pa.lib.tzinfo_to_string(tz) == 'UTC'\n    tz = dateutil.tz.gettz('Europe/Paris')\n    assert pa.lib.tzinfo_to_string(tz) == 'Europe/Paris'\n\n\ndef test_zoneinfo_tzinfo_to_string():\n    zoneinfo = pytest.importorskip('zoneinfo')\n    if sys.platform == 'win32':\n        # zoneinfo requires an additional dependency On Windows\n        # tzdata provides IANA time zone data\n        pytest.importorskip('tzdata')\n\n    tz = zoneinfo.ZoneInfo('UTC')\n    assert pa.lib.tzinfo_to_string(tz) == 'UTC'\n    tz = zoneinfo.ZoneInfo('Europe/Paris')\n    assert pa.lib.tzinfo_to_string(tz) == 'Europe/Paris'\n\n\ndef test_tzinfo_to_string_errors():\n    msg = \"Not an instance of datetime.tzinfo\"\n    with pytest.raises(TypeError):\n        pa.lib.tzinfo_to_string(\"Europe/Budapest\")\n\n    if sys.version_info >= (3, 8):\n        # before 3.8 it was only possible to create timezone objects with whole\n        # number of minutes\n        tz = datetime.timezone(datetime.timedelta(hours=1, seconds=30))\n        msg = \"Offset must represent whole number of minutes\"\n        with pytest.raises(ValueError, match=msg):\n            pa.lib.tzinfo_to_string(tz)\n\n\nif tzst:\n    timezones = tzst.timezones()\nelse:\n    timezones = st.none()\n\n\n@h.given(timezones)\ndef test_pytz_timezone_roundtrip(tz):\n    if tz is None:\n        pytest.skip('requires timezone not None')\n    timezone_string = pa.lib.tzinfo_to_string(tz)\n    timezone_tzinfo = pa.lib.string_to_tzinfo(timezone_string)\n    assert timezone_tzinfo == tz\n\n\ndef test_convert_custom_tzinfo_objects_to_string():\n    class CorrectTimezone1(datetime.tzinfo):\n        \"\"\"\n        Conversion is using utcoffset()\n        \"\"\"\n\n        def tzname(self, dt):\n            return None\n\n        def utcoffset(self, dt):\n            return datetime.timedelta(hours=-3, minutes=30)\n\n    class CorrectTimezone2(datetime.tzinfo):\n        \"\"\"\n        Conversion is using tzname()\n        \"\"\"\n\n        def tzname(self, dt):\n            return \"+03:00\"\n\n        def utcoffset(self, dt):\n            return datetime.timedelta(hours=3)\n\n    class BuggyTimezone1(datetime.tzinfo):\n        \"\"\"\n        Unable to infer name or offset\n        \"\"\"\n\n        def tzname(self, dt):\n            return None\n\n        def utcoffset(self, dt):\n            return None\n\n    class BuggyTimezone2(datetime.tzinfo):\n        \"\"\"\n        Wrong offset type\n        \"\"\"\n\n        def tzname(self, dt):\n            return None\n\n        def utcoffset(self, dt):\n            return \"one hour\"\n\n    class BuggyTimezone3(datetime.tzinfo):\n        \"\"\"\n        Wrong timezone name type\n        \"\"\"\n\n        def tzname(self, dt):\n            return 240\n\n        def utcoffset(self, dt):\n            return None\n\n    assert pa.lib.tzinfo_to_string(CorrectTimezone1()) == \"-02:30\"\n    assert pa.lib.tzinfo_to_string(CorrectTimezone2()) == \"+03:00\"\n\n    msg = (r\"Object returned by tzinfo.utcoffset\\(None\\) is not an instance \"\n           r\"of datetime.timedelta\")\n    for wrong in [BuggyTimezone1(), BuggyTimezone2(), BuggyTimezone3()]:\n        with pytest.raises(ValueError, match=msg):\n            pa.lib.tzinfo_to_string(wrong)\n\n\ndef test_string_to_tzinfo():\n    string = ['UTC', 'Europe/Paris', '+03:00', '+01:30', '-02:00']\n    try:\n        import pytz\n        expected = [pytz.utc, pytz.timezone('Europe/Paris'),\n                    pytz.FixedOffset(180), pytz.FixedOffset(90),\n                    pytz.FixedOffset(-120)]\n        result = [pa.lib.string_to_tzinfo(i) for i in string]\n        assert result == expected\n\n    except ImportError:\n        try:\n            import zoneinfo\n            expected = [zoneinfo.ZoneInfo(key='UTC'),\n                        zoneinfo.ZoneInfo(key='Europe/Paris'),\n                        datetime.timezone(datetime.timedelta(hours=3)),\n                        datetime.timezone(\n                            datetime.timedelta(hours=1, minutes=30)),\n                        datetime.timezone(-datetime.timedelta(hours=2))]\n            result = [pa.lib.string_to_tzinfo(i) for i in string]\n            assert result == expected\n\n        except ImportError:\n            pytest.skip('requires pytz or zoneinfo to be installed')\n\n\ndef test_timezone_string_roundtrip_pytz():\n    pytz = pytest.importorskip(\"pytz\")\n\n    tz = [pytz.FixedOffset(90), pytz.FixedOffset(-90),\n          pytz.utc, pytz.timezone('America/New_York')]\n    name = ['+01:30', '-01:30', 'UTC', 'America/New_York']\n\n    assert [pa.lib.tzinfo_to_string(i) for i in tz] == name\n    assert [pa.lib.string_to_tzinfo(i)for i in name] == tz\n\n\ndef test_timestamp():\n    for unit in ('s', 'ms', 'us', 'ns'):\n        for tz in (None, 'UTC', 'Europe/Paris'):\n            ty = pa.timestamp(unit, tz=tz)\n            assert ty.unit == unit\n            assert ty.tz == tz\n\n    for invalid_unit in ('m', 'arbit', 'rary'):\n        with pytest.raises(ValueError, match='Invalid time unit'):\n            pa.timestamp(invalid_unit)\n\n\ndef test_timestamp_print():\n    for unit in ('s', 'ms', 'us', 'ns'):\n        for tz in ('UTC', 'Europe/Paris', 'Pacific/Marquesas',\n                   'Mars/Mariner_Valley', '-00:42', '+42:00'):\n            ty = pa.timestamp(unit, tz=tz)\n            arr = pa.array([0], ty)\n            assert \"Z\" in str(arr)\n        arr = pa.array([0], pa.timestamp(unit))\n        assert \"Z\" not in str(arr)\n\n\ndef test_time32_units():\n    for valid_unit in ('s', 'ms'):\n        ty = pa.time32(valid_unit)\n        assert ty.unit == valid_unit\n\n    for invalid_unit in ('m', 'us', 'ns'):\n        error_msg = 'Invalid time unit for time32: {!r}'.format(invalid_unit)\n        with pytest.raises(ValueError, match=error_msg):\n            pa.time32(invalid_unit)\n\n\ndef test_time64_units():\n    for valid_unit in ('us', 'ns'):\n        ty = pa.time64(valid_unit)\n        assert ty.unit == valid_unit\n\n    for invalid_unit in ('m', 's', 'ms'):\n        error_msg = 'Invalid time unit for time64: {!r}'.format(invalid_unit)\n        with pytest.raises(ValueError, match=error_msg):\n            pa.time64(invalid_unit)\n\n\ndef test_duration():\n    for unit in ('s', 'ms', 'us', 'ns'):\n        ty = pa.duration(unit)\n        assert ty.unit == unit\n\n    for invalid_unit in ('m', 'arbit', 'rary'):\n        with pytest.raises(ValueError, match='Invalid time unit'):\n            pa.duration(invalid_unit)\n\n\ndef test_list_type():\n    ty = pa.list_(pa.int64())\n    assert isinstance(ty, pa.ListType)\n    assert ty.value_type == pa.int64()\n    assert ty.value_field == pa.field(\"item\", pa.int64(), nullable=True)\n\n    # nullability matters in comparison\n    ty_non_nullable = pa.list_(pa.field(\"item\", pa.int64(), nullable=False))\n    assert ty != ty_non_nullable\n\n    # field names don't matter by default\n    ty_named = pa.list_(pa.field(\"element\", pa.int64()))\n    assert ty == ty_named\n    assert not ty.equals(ty_named, check_metadata=True)\n\n    # metadata doesn't matter by default\n    ty_metadata = pa.list_(\n        pa.field(\"item\", pa.int64(), metadata={\"hello\": \"world\"}))\n    assert ty == ty_metadata\n    assert not ty.equals(ty_metadata, check_metadata=True)\n\n    with pytest.raises(TypeError):\n        pa.list_(None)\n\n\ndef test_large_list_type():\n    ty = pa.large_list(pa.utf8())\n    assert isinstance(ty, pa.LargeListType)\n    assert ty.value_type == pa.utf8()\n    assert ty.value_field == pa.field(\"item\", pa.utf8(), nullable=True)\n\n    with pytest.raises(TypeError):\n        pa.large_list(None)\n\n\ndef test_list_view_type():\n    ty = pa.list_view(pa.int64())\n    assert isinstance(ty, pa.ListViewType)\n    assert ty.value_type == pa.int64()\n    assert ty.value_field == pa.field(\"item\", pa.int64(), nullable=True)\n\n    # nullability matters in comparison\n    ty_non_nullable = pa.list_view(pa.field(\"item\", pa.int64(), nullable=False))\n    assert ty != ty_non_nullable\n\n    # field names don't matter by default\n    ty_named = pa.list_view(pa.field(\"element\", pa.int64()))\n    assert ty == ty_named\n    assert not ty.equals(ty_named, check_metadata=True)\n\n    # metadata doesn't matter by default\n    ty_metadata = pa.list_view(\n        pa.field(\"item\", pa.int64(), metadata={\"hello\": \"world\"}))\n    assert ty == ty_metadata\n    assert not ty.equals(ty_metadata, check_metadata=True)\n\n    with pytest.raises(TypeError):\n        pa.list_view(None)\n\n\ndef test_large_list_view_type():\n    ty = pa.large_list_view(pa.utf8())\n    assert isinstance(ty, pa.LargeListViewType)\n    assert ty.value_type == pa.utf8()\n    assert ty.value_field == pa.field(\"item\", pa.utf8(), nullable=True)\n\n    with pytest.raises(TypeError):\n        pa.large_list_view(None)\n\n\ndef test_map_type():\n    ty = pa.map_(pa.utf8(), pa.int32())\n    assert isinstance(ty, pa.MapType)\n    assert ty.key_type == pa.utf8()\n    assert ty.key_field == pa.field(\"key\", pa.utf8(), nullable=False)\n    assert ty.item_type == pa.int32()\n    assert ty.item_field == pa.field(\"value\", pa.int32(), nullable=True)\n\n    # nullability matters in comparison\n    ty_non_nullable = pa.map_(pa.utf8(), pa.field(\n        \"value\", pa.int32(), nullable=False))\n    assert ty != ty_non_nullable\n\n    # field names don't matter by default\n    ty_named = pa.map_(pa.field(\"x\", pa.utf8(), nullable=False),\n                       pa.field(\"y\", pa.int32()))\n    assert ty == ty_named\n    assert not ty.equals(ty_named, check_metadata=True)\n\n    # metadata doesn't matter by default\n    ty_metadata = pa.map_(pa.utf8(), pa.field(\n        \"value\", pa.int32(), metadata={\"hello\": \"world\"}))\n    assert ty == ty_metadata\n    assert not ty.equals(ty_metadata, check_metadata=True)\n\n    for keys_sorted in [True, False]:\n        assert pa.map_(pa.utf8(), pa.int32(),\n                       keys_sorted=keys_sorted).keys_sorted == keys_sorted\n\n    with pytest.raises(TypeError):\n        pa.map_(None)\n    with pytest.raises(TypeError):\n        pa.map_(pa.int32(), None)\n    with pytest.raises(TypeError):\n        pa.map_(pa.field(\"name\", pa.string(), nullable=True), pa.int64())\n\n\ndef test_fixed_size_list_type():\n    ty = pa.list_(pa.float64(), 2)\n    assert isinstance(ty, pa.FixedSizeListType)\n    assert ty.value_type == pa.float64()\n    assert ty.value_field == pa.field(\"item\", pa.float64(), nullable=True)\n    assert ty.list_size == 2\n\n    with pytest.raises(ValueError):\n        pa.list_(pa.float64(), -2)\n\n\ndef test_struct_type():\n    fields = [\n        # Duplicate field name on purpose\n        pa.field('a', pa.int64()),\n        pa.field('a', pa.int32()),\n        pa.field('b', pa.int32())\n    ]\n    ty = pa.struct(fields)\n\n    assert len(ty) == ty.num_fields == 3\n    assert list(ty) == fields\n    assert ty[0].name == 'a'\n    assert ty[2].type == pa.int32()\n    with pytest.raises(IndexError):\n        assert ty[3]\n\n    assert ty['b'] == ty[2]\n\n    assert ty['b'] == ty.field('b')\n\n    assert ty[2] == ty.field(2)\n\n    # Not found\n    with pytest.raises(KeyError):\n        ty['c']\n\n    with pytest.raises(KeyError):\n        ty.field('c')\n\n    # Neither integer nor string\n    with pytest.raises(TypeError):\n        ty[None]\n\n    with pytest.raises(TypeError):\n        ty.field(None)\n\n    for a, b in zip(ty, fields):\n        a == b\n\n    # Construct from list of tuples\n    ty = pa.struct([('a', pa.int64()),\n                    ('a', pa.int32()),\n                    ('b', pa.int32())])\n    assert list(ty) == fields\n    for a, b in zip(ty, fields):\n        a == b\n\n    # Construct from mapping\n    fields = [pa.field('a', pa.int64()),\n              pa.field('b', pa.int32())]\n    ty = pa.struct(OrderedDict([('a', pa.int64()),\n                                ('b', pa.int32())]))\n    assert list(ty) == fields\n    for a, b in zip(ty, fields):\n        a == b\n\n    # Invalid args\n    with pytest.raises(TypeError):\n        pa.struct([('a', None)])\n\n\ndef test_struct_duplicate_field_names():\n    fields = [\n        pa.field('a', pa.int64()),\n        pa.field('b', pa.int32()),\n        pa.field('a', pa.int32())\n    ]\n    ty = pa.struct(fields)\n\n    # Duplicate\n    with pytest.warns(UserWarning):\n        with pytest.raises(KeyError):\n            ty['a']\n\n    # StructType::GetFieldIndex\n    assert ty.get_field_index('a') == -1\n\n    # StructType::GetAllFieldIndices\n    assert ty.get_all_field_indices('a') == [0, 2]\n\n\ndef test_union_type():\n    def check_fields(ty, fields):\n        assert ty.num_fields == len(fields)\n        assert [ty[i] for i in range(ty.num_fields)] == fields\n        assert [ty.field(i) for i in range(ty.num_fields)] == fields\n\n    fields = [pa.field('x', pa.list_(pa.int32())),\n              pa.field('y', pa.binary())]\n    type_codes = [5, 9]\n\n    sparse_factories = [\n        partial(pa.union, mode='sparse'),\n        partial(pa.union, mode=pa.lib.UnionMode_SPARSE),\n        pa.sparse_union,\n    ]\n\n    dense_factories = [\n        partial(pa.union, mode='dense'),\n        partial(pa.union, mode=pa.lib.UnionMode_DENSE),\n        pa.dense_union,\n    ]\n\n    for factory in sparse_factories:\n        ty = factory(fields)\n        assert isinstance(ty, pa.SparseUnionType)\n        assert ty.mode == 'sparse'\n        check_fields(ty, fields)\n        assert ty.type_codes == [0, 1]\n        ty = factory(fields, type_codes=type_codes)\n        assert ty.mode == 'sparse'\n        check_fields(ty, fields)\n        assert ty.type_codes == type_codes\n        # Invalid number of type codes\n        with pytest.raises(ValueError):\n            factory(fields, type_codes=type_codes[1:])\n\n    for factory in dense_factories:\n        ty = factory(fields)\n        assert isinstance(ty, pa.DenseUnionType)\n        assert ty.mode == 'dense'\n        check_fields(ty, fields)\n        assert ty.type_codes == [0, 1]\n        ty = factory(fields, type_codes=type_codes)\n        assert ty.mode == 'dense'\n        check_fields(ty, fields)\n        assert ty.type_codes == type_codes\n        # Invalid number of type codes\n        with pytest.raises(ValueError):\n            factory(fields, type_codes=type_codes[1:])\n\n    for mode in ('unknown', 2):\n        with pytest.raises(ValueError, match='Invalid union mode'):\n            pa.union(fields, mode=mode)\n\n\ndef test_dictionary_type():\n    ty0 = pa.dictionary(pa.int32(), pa.string())\n    assert ty0.index_type == pa.int32()\n    assert ty0.value_type == pa.string()\n    assert ty0.ordered is False\n\n    ty1 = pa.dictionary(pa.int8(), pa.float64(), ordered=True)\n    assert ty1.index_type == pa.int8()\n    assert ty1.value_type == pa.float64()\n    assert ty1.ordered is True\n\n    # construct from non-arrow objects\n    ty2 = pa.dictionary('int8', 'string')\n    assert ty2.index_type == pa.int8()\n    assert ty2.value_type == pa.string()\n    assert ty2.ordered is False\n\n    # allow unsigned integers for index type\n    ty3 = pa.dictionary(pa.uint32(), pa.string())\n    assert ty3.index_type == pa.uint32()\n    assert ty3.value_type == pa.string()\n    assert ty3.ordered is False\n\n    # invalid index type raises\n    with pytest.raises(TypeError):\n        pa.dictionary(pa.string(), pa.int64())\n\n\ndef test_dictionary_ordered_equals():\n    # Python side checking of ARROW-6345\n    d1 = pa.dictionary('int32', 'binary', ordered=True)\n    d2 = pa.dictionary('int32', 'binary', ordered=False)\n    d3 = pa.dictionary('int8', 'binary', ordered=True)\n    d4 = pa.dictionary('int32', 'binary', ordered=True)\n\n    assert not d1.equals(d2)\n    assert not d1.equals(d3)\n    assert d1.equals(d4)\n\n\ndef test_types_hashable():\n    many_types = get_many_types()\n    in_dict = {}\n    for i, type_ in enumerate(many_types):\n        assert hash(type_) == hash(type_)\n        in_dict[type_] = i\n    assert len(in_dict) == len(many_types)\n    for i, type_ in enumerate(many_types):\n        assert in_dict[type_] == i\n\n\ndef test_types_picklable(pickle_module):\n    for ty in get_many_types():\n        data = pickle_module.dumps(ty)\n        assert pickle_module.loads(data) == ty\n\n\ndef test_types_weakref():\n    for ty in get_many_types():\n        wr = weakref.ref(ty)\n        assert wr() is not None\n        # Note that ty may be a singleton and therefore outlive this loop\n\n    wr = weakref.ref(pa.int32())\n    assert wr() is not None  # singleton\n    wr = weakref.ref(pa.list_(pa.int32()))\n    assert wr() is None  # not a singleton\n\n\ndef test_fields_hashable():\n    in_dict = {}\n    fields = [pa.field('a', pa.int32()),\n              pa.field('a', pa.int64()),\n              pa.field('a', pa.int64(), nullable=False),\n              pa.field('b', pa.int32()),\n              pa.field('b', pa.int32(), nullable=False)]\n    for i, field in enumerate(fields):\n        in_dict[field] = i\n    assert len(in_dict) == len(fields)\n    for i, field in enumerate(fields):\n        assert in_dict[field] == i\n\n\ndef test_fields_weakrefable():\n    field = pa.field('a', pa.int32())\n    wr = weakref.ref(field)\n    assert wr() is not None\n    del field\n    assert wr() is None\n\n\ndef test_run_end_encoded_type():\n    ty = pa.run_end_encoded(pa.int64(), pa.utf8())\n    assert isinstance(ty, pa.RunEndEncodedType)\n    assert ty.run_end_type == pa.int64()\n    assert ty.value_type == pa.utf8()\n    assert ty.num_buffers == 1  # buffers expected to be {NULLPTR}\n    assert ty.num_fields == 2\n\n    with pytest.raises(TypeError):\n        pa.run_end_encoded(pa.int64(), None)\n\n    with pytest.raises(TypeError):\n        pa.run_end_encoded(None, pa.utf8())\n\n    with pytest.raises(ValueError):\n        pa.run_end_encoded(pa.int8(), pa.utf8())\n\n\n@pytest.mark.parametrize('t,check_func', [\n    (pa.date32(), types.is_date32),\n    (pa.date64(), types.is_date64),\n    (pa.time32('s'), types.is_time32),\n    (pa.time64('ns'), types.is_time64),\n    (pa.int8(), types.is_int8),\n    (pa.int16(), types.is_int16),\n    (pa.int32(), types.is_int32),\n    (pa.int64(), types.is_int64),\n    (pa.uint8(), types.is_uint8),\n    (pa.uint16(), types.is_uint16),\n    (pa.uint32(), types.is_uint32),\n    (pa.uint64(), types.is_uint64),\n    (pa.float16(), types.is_float16),\n    (pa.float32(), types.is_float32),\n    (pa.float64(), types.is_float64)\n])\ndef test_exact_primitive_types(t, check_func):\n    assert check_func(t)\n\n\ndef test_type_id():\n    # enum values are not exposed publicly\n    for ty in get_many_types():\n        assert isinstance(ty.id, int)\n\n\ndef test_bit_and_byte_width():\n    for ty, expected_bit_width, expected_byte_width in [\n        (pa.bool_(), 1, 0),\n        (pa.int8(), 8, 1),\n        (pa.uint32(), 32, 4),\n        (pa.float16(), 16, 2),\n        (pa.timestamp('s'), 64, 8),\n        (pa.date32(), 32, 4),\n        (pa.decimal128(19, 4), 128, 16),\n        (pa.decimal256(76, 38), 256, 32),\n        (pa.binary(42), 42 * 8, 42),\n        (pa.binary(0), 0, 0),\n    ]:\n        assert ty.bit_width == expected_bit_width\n\n        if 0 < expected_bit_width < 8:\n            with pytest.raises(ValueError, match=\"Less than one byte\"):\n                ty.byte_width\n        else:\n            assert ty.byte_width == expected_byte_width\n\n    for ty in [\n        pa.binary(),\n        pa.string(),\n        pa.list_(pa.int16()),\n        pa.map_(pa.string(), pa.int32()),\n        pa.struct([('f1', pa.int32())])\n    ]:\n        with pytest.raises(ValueError, match=\"fixed width\"):\n            ty.bit_width\n        with pytest.raises(ValueError, match=\"fixed width\"):\n            ty.byte_width\n\n\ndef test_fixed_size_binary_byte_width():\n    ty = pa.binary(5)\n    assert ty.byte_width == 5\n\n\ndef test_decimal_properties():\n    ty = pa.decimal128(19, 4)\n    assert ty.byte_width == 16\n    assert ty.precision == 19\n    assert ty.scale == 4\n    ty = pa.decimal256(76, 38)\n    assert ty.byte_width == 32\n    assert ty.precision == 76\n    assert ty.scale == 38\n\n\ndef test_decimal_overflow():\n    pa.decimal128(1, 0)\n    pa.decimal128(38, 0)\n    for i in (0, -1, 39):\n        with pytest.raises(ValueError):\n            pa.decimal128(i, 0)\n\n    pa.decimal256(1, 0)\n    pa.decimal256(76, 0)\n    for i in (0, -1, 77):\n        with pytest.raises(ValueError):\n            pa.decimal256(i, 0)\n\n\ndef test_timedelta_overflow():\n    # microsecond resolution, overflow\n    d = datetime.timedelta(days=-106751992, seconds=71945, microseconds=224192)\n    with pytest.raises(pa.ArrowInvalid):\n        pa.scalar(d)\n\n    # microsecond resolution, overflow\n    d = datetime.timedelta(days=106751991, seconds=14454, microseconds=775808)\n    with pytest.raises(pa.ArrowInvalid):\n        pa.scalar(d)\n\n    # nanosecond resolution, overflow\n    d = datetime.timedelta(days=-106752, seconds=763, microseconds=145224)\n    with pytest.raises(pa.ArrowInvalid):\n        pa.scalar(d, type=pa.duration('ns'))\n\n    # microsecond resolution, not overflow\n    pa.scalar(d, type=pa.duration('us')).as_py() == d\n\n    # second/millisecond resolution, not overflow\n    for d in [datetime.timedelta.min, datetime.timedelta.max]:\n        pa.scalar(d, type=pa.duration('ms')).as_py() == d\n        pa.scalar(d, type=pa.duration('s')).as_py() == d\n\n\ndef test_type_equality_operators():\n    many_types = get_many_types()\n    non_pyarrow = ('foo', 16, {'s', 'e', 't'})\n\n    for index, ty in enumerate(many_types):\n        # could use two parametrization levels,\n        # but that'd bloat pytest's output\n        for i, other in enumerate(many_types + non_pyarrow):\n            if i == index:\n                assert ty == other\n            else:\n                assert ty != other\n\n\ndef test_key_value_metadata():\n    m = pa.KeyValueMetadata({'a': 'A', 'b': 'B'})\n    assert len(m) == 2\n    assert m['a'] == b'A'\n    assert m[b'a'] == b'A'\n    assert m['b'] == b'B'\n    assert 'a' in m\n    assert b'a' in m\n    assert 'c' not in m\n\n    m1 = pa.KeyValueMetadata({'a': 'A', 'b': 'B'})\n    m2 = pa.KeyValueMetadata(a='A', b='B')\n    m3 = pa.KeyValueMetadata([('a', 'A'), ('b', 'B')])\n\n    assert m1 != 2\n    assert m1 == m2\n    assert m2 == m3\n    assert m1 == {'a': 'A', 'b': 'B'}\n    assert m1 != {'a': 'A', 'b': 'C'}\n\n    with pytest.raises(TypeError):\n        pa.KeyValueMetadata({'a': 1})\n    with pytest.raises(TypeError):\n        pa.KeyValueMetadata({1: 'a'})\n    with pytest.raises(TypeError):\n        pa.KeyValueMetadata(a=1)\n\n    expected = [(b'a', b'A'), (b'b', b'B')]\n    result = [(k, v) for k, v in m3.items()]\n    assert result == expected\n    assert list(m3.items()) == expected\n    assert list(m3.keys()) == [b'a', b'b']\n    assert list(m3.values()) == [b'A', b'B']\n    assert len(m3) == 2\n\n    # test duplicate key support\n    md = pa.KeyValueMetadata([\n        ('a', 'alpha'),\n        ('b', 'beta'),\n        ('a', 'Alpha'),\n        ('a', 'ALPHA'),\n    ])\n\n    expected = [\n        (b'a', b'alpha'),\n        (b'b', b'beta'),\n        (b'a', b'Alpha'),\n        (b'a', b'ALPHA')\n    ]\n    assert len(md) == 4\n    assert isinstance(md.keys(), Iterator)\n    assert isinstance(md.values(), Iterator)\n    assert isinstance(md.items(), Iterator)\n    assert list(md.items()) == expected\n    assert list(md.keys()) == [k for k, _ in expected]\n    assert list(md.values()) == [v for _, v in expected]\n\n    # first occurrence\n    assert md['a'] == b'alpha'\n    assert md['b'] == b'beta'\n    assert md.get_all('a') == [b'alpha', b'Alpha', b'ALPHA']\n    assert md.get_all('b') == [b'beta']\n    assert md.get_all('unknown') == []\n\n    with pytest.raises(KeyError):\n        md = pa.KeyValueMetadata([\n            ('a', 'alpha'),\n            ('b', 'beta'),\n            ('a', 'Alpha'),\n            ('a', 'ALPHA'),\n        ], b='BETA')\n\n\ndef test_key_value_metadata_duplicates():\n    meta = pa.KeyValueMetadata({'a': '1', 'b': '2'})\n\n    with pytest.raises(KeyError):\n        pa.KeyValueMetadata(meta, a='3')\n\n\ndef test_field_basic():\n    t = pa.string()\n    f = pa.field('foo', t)\n\n    assert f.name == 'foo'\n    assert f.nullable\n    assert f.type is t\n    assert repr(f) == \"pyarrow.Field<foo: string>\"\n\n    f = pa.field('foo', t, False)\n    assert not f.nullable\n\n    with pytest.raises(TypeError):\n        pa.field('foo', None)\n\n\ndef test_field_equals():\n    meta1 = {b'foo': b'bar'}\n    meta2 = {b'bizz': b'bazz'}\n\n    f1 = pa.field('a', pa.int8(), nullable=True)\n    f2 = pa.field('a', pa.int8(), nullable=True)\n    f3 = pa.field('a', pa.int8(), nullable=False)\n    f4 = pa.field('a', pa.int16(), nullable=False)\n    f5 = pa.field('b', pa.int16(), nullable=False)\n    f6 = pa.field('a', pa.int8(), nullable=True, metadata=meta1)\n    f7 = pa.field('a', pa.int8(), nullable=True, metadata=meta1)\n    f8 = pa.field('a', pa.int8(), nullable=True, metadata=meta2)\n\n    assert f1.equals(f2)\n    assert f6.equals(f7)\n    assert not f1.equals(f3)\n    assert not f1.equals(f4)\n    assert not f3.equals(f4)\n    assert not f4.equals(f5)\n\n    # No metadata in f1, but metadata in f6\n    assert f1.equals(f6)\n    assert not f1.equals(f6, check_metadata=True)\n\n    # Different metadata\n    assert f6.equals(f7)\n    assert f7.equals(f8)\n    assert not f7.equals(f8, check_metadata=True)\n\n\ndef test_field_equality_operators():\n    f1 = pa.field('a', pa.int8(), nullable=True)\n    f2 = pa.field('a', pa.int8(), nullable=True)\n    f3 = pa.field('b', pa.int8(), nullable=True)\n    f4 = pa.field('b', pa.int8(), nullable=False)\n\n    assert f1 == f2\n    assert f1 != f3\n    assert f3 != f4\n    assert f1 != 'foo'\n\n\ndef test_field_metadata():\n    f1 = pa.field('a', pa.int8())\n    f2 = pa.field('a', pa.int8(), metadata={})\n    f3 = pa.field('a', pa.int8(), metadata={b'bizz': b'bazz'})\n\n    assert f1.metadata is None\n    assert f2.metadata == {}\n    assert f3.metadata[b'bizz'] == b'bazz'\n\n\ndef test_field_add_remove_metadata():\n    import collections\n\n    f0 = pa.field('foo', pa.int32())\n\n    assert f0.metadata is None\n\n    metadata = {b'foo': b'bar', b'pandas': b'badger'}\n    metadata2 = collections.OrderedDict([\n        (b'a', b'alpha'),\n        (b'b', b'beta')\n    ])\n\n    f1 = f0.with_metadata(metadata)\n    assert f1.metadata == metadata\n\n    f2 = f0.with_metadata(metadata2)\n    assert f2.metadata == metadata2\n\n    with pytest.raises(TypeError):\n        f0.with_metadata([1, 2, 3])\n\n    f3 = f1.remove_metadata()\n    assert f3.metadata is None\n\n    # idempotent\n    f4 = f3.remove_metadata()\n    assert f4.metadata is None\n\n    f5 = pa.field('foo', pa.int32(), True, metadata)\n    f6 = f0.with_metadata(metadata)\n    assert f5.equals(f6)\n\n\ndef test_field_modified_copies():\n    f0 = pa.field('foo', pa.int32(), True)\n    f0_ = pa.field('foo', pa.int32(), True)\n    assert f0.equals(f0_)\n\n    f1 = pa.field('foo', pa.int64(), True)\n    f1_ = f0.with_type(pa.int64())\n    assert f1.equals(f1_)\n    # Original instance is unmodified\n    assert f0.equals(f0_)\n\n    f2 = pa.field('foo', pa.int32(), False)\n    f2_ = f0.with_nullable(False)\n    assert f2.equals(f2_)\n    # Original instance is unmodified\n    assert f0.equals(f0_)\n\n    f3 = pa.field('bar', pa.int32(), True)\n    f3_ = f0.with_name('bar')\n    assert f3.equals(f3_)\n    # Original instance is unmodified\n    assert f0.equals(f0_)\n\n\ndef test_is_integer_value():\n    assert pa.types.is_integer_value(1)\n    assert pa.types.is_integer_value(np.int64(1))\n    assert not pa.types.is_integer_value('1')\n\n\ndef test_is_float_value():\n    assert not pa.types.is_float_value(1)\n    assert pa.types.is_float_value(1.)\n    assert pa.types.is_float_value(np.float64(1))\n    assert not pa.types.is_float_value('1.0')\n\n\ndef test_is_boolean_value():\n    assert not pa.types.is_boolean_value(1)\n    assert pa.types.is_boolean_value(True)\n    assert pa.types.is_boolean_value(False)\n    assert pa.types.is_boolean_value(np.bool_(True))\n    assert pa.types.is_boolean_value(np.bool_(False))\n\n\n@h.settings(suppress_health_check=(h.HealthCheck.too_slow,))\n@h.given(\n    past.all_types |\n    past.all_fields |\n    past.all_schemas\n)\n@h.example(\n    pa.field(name='', type=pa.null(), metadata={'0': '', '': ''})\n)\ndef test_pickling(pickle_module, field):\n    data = pickle_module.dumps(field)\n    assert pickle_module.loads(data) == field\n\n\n@h.given(\n    st.lists(past.all_types) |\n    st.lists(past.all_fields) |\n    st.lists(past.all_schemas)\n)\ndef test_hashing(items):\n    h.assume(\n        # well, this is still O(n^2), but makes the input unique\n        all(not a.equals(b) for i, a in enumerate(items) for b in items[:i])\n    )\n\n    container = {}\n    for i, item in enumerate(items):\n        assert hash(item) == hash(item)\n        container[item] = i\n\n    assert len(container) == len(items)\n\n    for i, item in enumerate(items):\n        assert container[item] == i\n\n\ndef test_types_come_back_with_specific_type():\n    for arrow_type in get_many_types():\n        schema = pa.schema([pa.field(\"field_name\", arrow_type)])\n        type_back = schema.field(\"field_name\").type\n        assert type(type_back) is type(arrow_type)\n\n\ndef test_schema_import_c_schema_interface():\n    class Wrapper:\n        def __init__(self, schema):\n            self.schema = schema\n\n        def __arrow_c_schema__(self):\n            return self.schema.__arrow_c_schema__()\n\n    schema = pa.schema([pa.field(\"field_name\", pa.int32())], metadata={\"a\": \"b\"})\n    assert schema.metadata == {b\"a\": b\"b\"}\n    wrapped_schema = Wrapper(schema)\n\n    assert pa.schema(wrapped_schema) == schema\n    assert pa.schema(wrapped_schema).metadata == {b\"a\": b\"b\"}\n    assert pa.schema(wrapped_schema, metadata={\"a\": \"c\"}).metadata == {b\"a\": b\"c\"}\n\n\ndef test_field_import_c_schema_interface():\n    class Wrapper:\n        def __init__(self, field):\n            self.field = field\n\n        def __arrow_c_schema__(self):\n            return self.field.__arrow_c_schema__()\n\n    field = pa.field(\"field_name\", pa.int32(), metadata={\"key\": \"value\"})\n    wrapped_field = Wrapper(field)\n\n    assert pa.field(wrapped_field) == field\n\n    with pytest.raises(ValueError, match=\"cannot specify 'type'\"):\n        pa.field(wrapped_field, type=pa.int64())\n\n    # override nullable or metadata\n    assert pa.field(wrapped_field, nullable=False).nullable is False\n    result = pa.field(wrapped_field, metadata={\"other\": \"meta\"})\n    assert result.metadata == {b\"other\": b\"meta\"}\n", "python/pyarrow/tests/arrow_39313.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# This file is called from a test in test_pandas.py.\n\nfrom threading import Thread\n\nimport pandas as pd\nfrom pyarrow.pandas_compat import _pandas_api\n\nif __name__ == \"__main__\":\n    wait = True\n    num_threads = 10\n    df = pd.DataFrame()\n    results = []\n\n    def rc():\n        while wait:\n            pass\n        results.append(_pandas_api.is_data_frame(df))\n\n    threads = [Thread(target=rc) for _ in range(num_threads)]\n\n    for t in threads:\n        t.start()\n\n    wait = False\n\n    for t in threads:\n        t.join()\n\n    assert len(results) == num_threads\n    assert all(results), \"`is_data_frame` returned False when given a DataFrame\"\n", "python/pyarrow/tests/test_schema.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import OrderedDict\nimport sys\nimport weakref\n\nimport pytest\nimport numpy as np\nimport pyarrow as pa\n\nimport pyarrow.tests.util as test_util\nfrom pyarrow.vendored.version import Version\n\ntry:\n    import pandas as pd\nexcept ImportError:\n    pass\n\n\ndef test_schema_constructor_errors():\n    msg = (\"Do not call Schema's constructor directly, use `pyarrow.schema` \"\n           \"instead\")\n    with pytest.raises(TypeError, match=msg):\n        pa.Schema()\n\n\ndef test_type_integers():\n    dtypes = ['int8', 'int16', 'int32', 'int64',\n              'uint8', 'uint16', 'uint32', 'uint64']\n\n    for name in dtypes:\n        factory = getattr(pa, name)\n        t = factory()\n        assert str(t) == name\n\n\n@pytest.mark.pandas\ndef test_type_to_pandas_dtype():\n    M8 = np.dtype('datetime64[ms]')\n    if Version(pd.__version__) < Version(\"2.0.0\"):\n        M8 = np.dtype('datetime64[ns]')\n    cases = [\n        (pa.null(), np.object_),\n        (pa.bool_(), np.bool_),\n        (pa.int8(), np.int8),\n        (pa.int16(), np.int16),\n        (pa.int32(), np.int32),\n        (pa.int64(), np.int64),\n        (pa.uint8(), np.uint8),\n        (pa.uint16(), np.uint16),\n        (pa.uint32(), np.uint32),\n        (pa.uint64(), np.uint64),\n        (pa.float16(), np.float16),\n        (pa.float32(), np.float32),\n        (pa.float64(), np.float64),\n        (pa.date32(), M8),\n        (pa.date64(), M8),\n        (pa.timestamp('ms'), M8),\n        (pa.binary(), np.object_),\n        (pa.binary(12), np.object_),\n        (pa.string(), np.object_),\n        (pa.list_(pa.int8()), np.object_),\n        # (pa.list_(pa.int8(), 2), np.object_),  # TODO needs pandas conversion\n        (pa.map_(pa.int64(), pa.float64()), np.object_),\n    ]\n    for arrow_type, numpy_type in cases:\n        assert arrow_type.to_pandas_dtype() == numpy_type\n\n\n@pytest.mark.pandas\ndef test_type_to_pandas_dtype_check_import():\n    # ARROW-7980\n    test_util.invoke_script('arrow_7980.py')\n\n\ndef test_type_list():\n    value_type = pa.int32()\n    list_type = pa.list_(value_type)\n    assert str(list_type) == 'list<item: int32>'\n\n    field = pa.field('my_item', pa.string())\n    l2 = pa.list_(field)\n    assert str(l2) == 'list<my_item: string>'\n\n\ndef test_type_comparisons():\n    val = pa.int32()\n    assert val == pa.int32()\n    assert val == 'int32'\n    assert val != 5\n\n\ndef test_type_for_alias():\n    cases = [\n        ('i1', pa.int8()),\n        ('int8', pa.int8()),\n        ('i2', pa.int16()),\n        ('int16', pa.int16()),\n        ('i4', pa.int32()),\n        ('int32', pa.int32()),\n        ('i8', pa.int64()),\n        ('int64', pa.int64()),\n        ('u1', pa.uint8()),\n        ('uint8', pa.uint8()),\n        ('u2', pa.uint16()),\n        ('uint16', pa.uint16()),\n        ('u4', pa.uint32()),\n        ('uint32', pa.uint32()),\n        ('u8', pa.uint64()),\n        ('uint64', pa.uint64()),\n        ('f4', pa.float32()),\n        ('float32', pa.float32()),\n        ('f8', pa.float64()),\n        ('float64', pa.float64()),\n        ('date32', pa.date32()),\n        ('date64', pa.date64()),\n        ('string', pa.string()),\n        ('str', pa.string()),\n        ('binary', pa.binary()),\n        ('time32[s]', pa.time32('s')),\n        ('time32[ms]', pa.time32('ms')),\n        ('time64[us]', pa.time64('us')),\n        ('time64[ns]', pa.time64('ns')),\n        ('timestamp[s]', pa.timestamp('s')),\n        ('timestamp[ms]', pa.timestamp('ms')),\n        ('timestamp[us]', pa.timestamp('us')),\n        ('timestamp[ns]', pa.timestamp('ns')),\n        ('duration[s]', pa.duration('s')),\n        ('duration[ms]', pa.duration('ms')),\n        ('duration[us]', pa.duration('us')),\n        ('duration[ns]', pa.duration('ns')),\n        ('month_day_nano_interval', pa.month_day_nano_interval()),\n    ]\n\n    for val, expected in cases:\n        assert pa.type_for_alias(val) == expected\n\n\ndef test_type_string():\n    t = pa.string()\n    assert str(t) == 'string'\n\n\ndef test_type_timestamp_with_tz():\n    tz = 'America/Los_Angeles'\n    t = pa.timestamp('ns', tz=tz)\n    assert t.unit == 'ns'\n    assert t.tz == tz\n\n\ndef test_time_types():\n    t1 = pa.time32('s')\n    t2 = pa.time32('ms')\n    t3 = pa.time64('us')\n    t4 = pa.time64('ns')\n\n    assert t1.unit == 's'\n    assert t2.unit == 'ms'\n    assert t3.unit == 'us'\n    assert t4.unit == 'ns'\n\n    assert str(t1) == 'time32[s]'\n    assert str(t4) == 'time64[ns]'\n\n    with pytest.raises(ValueError):\n        pa.time32('us')\n\n    with pytest.raises(ValueError):\n        pa.time64('s')\n\n\ndef test_from_numpy_dtype():\n    cases = [\n        (np.dtype('bool'), pa.bool_()),\n        (np.dtype('int8'), pa.int8()),\n        (np.dtype('int16'), pa.int16()),\n        (np.dtype('int32'), pa.int32()),\n        (np.dtype('int64'), pa.int64()),\n        (np.dtype('uint8'), pa.uint8()),\n        (np.dtype('uint16'), pa.uint16()),\n        (np.dtype('uint32'), pa.uint32()),\n        (np.dtype('float16'), pa.float16()),\n        (np.dtype('float32'), pa.float32()),\n        (np.dtype('float64'), pa.float64()),\n        (np.dtype('U'), pa.string()),\n        (np.dtype('S'), pa.binary()),\n        (np.dtype('datetime64[s]'), pa.timestamp('s')),\n        (np.dtype('datetime64[ms]'), pa.timestamp('ms')),\n        (np.dtype('datetime64[us]'), pa.timestamp('us')),\n        (np.dtype('datetime64[ns]'), pa.timestamp('ns')),\n        (np.dtype('timedelta64[s]'), pa.duration('s')),\n        (np.dtype('timedelta64[ms]'), pa.duration('ms')),\n        (np.dtype('timedelta64[us]'), pa.duration('us')),\n        (np.dtype('timedelta64[ns]'), pa.duration('ns')),\n    ]\n\n    for dt, pt in cases:\n        result = pa.from_numpy_dtype(dt)\n        assert result == pt\n\n    # Things convertible to numpy dtypes work\n    assert pa.from_numpy_dtype('U') == pa.string()\n    assert pa.from_numpy_dtype(np.str_) == pa.string()\n    assert pa.from_numpy_dtype('int32') == pa.int32()\n    assert pa.from_numpy_dtype(bool) == pa.bool_()\n\n    with pytest.raises(NotImplementedError):\n        pa.from_numpy_dtype(np.dtype('O'))\n\n    with pytest.raises(TypeError):\n        pa.from_numpy_dtype('not_convertible_to_dtype')\n\n\ndef test_schema():\n    fields = [\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n        pa.field('baz', pa.list_(pa.int8()))\n    ]\n    sch = pa.schema(fields)\n\n    assert sch.names == ['foo', 'bar', 'baz']\n    assert sch.types == [pa.int32(), pa.string(), pa.list_(pa.int8())]\n\n    assert len(sch) == 3\n    assert sch[0].name == 'foo'\n    assert sch[0].type == fields[0].type\n    assert sch.field('foo').name == 'foo'\n    assert sch.field('foo').type == fields[0].type\n\n    assert repr(sch) == \"\"\"\\\nfoo: int32\nbar: string\nbaz: list<item: int8>\n  child 0, item: int8\"\"\"\n\n    with pytest.raises(TypeError):\n        pa.schema([None])\n\n\ndef test_schema_weakref():\n    fields = [\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n        pa.field('baz', pa.list_(pa.int8()))\n    ]\n    schema = pa.schema(fields)\n    wr = weakref.ref(schema)\n    assert wr() is not None\n    del schema\n    assert wr() is None\n\n\ndef test_schema_to_string_with_metadata():\n    lorem = \"\"\"\\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla accumsan vel\nturpis et mollis. Aliquam tincidunt arcu id tortor blandit blandit. Donec\neget leo quis lectus scelerisque varius. Class aptent taciti sociosqu ad\nlitora torquent per conubia nostra, per inceptos himenaeos. Praesent\nfaucibus, diam eu volutpat iaculis, tellus est porta ligula, a efficitur\nturpis nulla facilisis quam. Aliquam vitae lorem erat. Proin a dolor ac libero\ndignissim mollis vitae eu mauris. Quisque posuere tellus vitae massa\npellentesque sagittis. Aenean feugiat, diam ac dignissim fermentum, lorem\nsapien commodo massa, vel volutpat orci nisi eu justo. Nulla non blandit\nsapien. Quisque pretium vestibulum urna eu vehicula.\"\"\"\n    # ARROW-7063\n    my_schema = pa.schema([pa.field(\"foo\", \"int32\", False,\n                                    metadata={\"key1\": \"value1\"}),\n                           pa.field(\"bar\", \"string\", True,\n                                    metadata={\"key3\": \"value3\"})],\n                          metadata={\"lorem\": lorem})\n\n    assert my_schema.to_string() == \"\"\"\\\nfoo: int32 not null\n  -- field metadata --\n  key1: 'value1'\nbar: string\n  -- field metadata --\n  key3: 'value3'\n-- schema metadata --\nlorem: '\"\"\" + lorem[:65] + \"' + \" + str(len(lorem) - 65)\n\n    # Metadata that exactly fits\n    result = pa.schema([('f0', 'int32')],\n                       metadata={'key': 'value' + 'x' * 62}).to_string()\n    assert result == \"\"\"\\\nf0: int32\n-- schema metadata --\nkey: 'valuexxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\"\"\"\n\n    assert my_schema.to_string(truncate_metadata=False) == \"\"\"\\\nfoo: int32 not null\n  -- field metadata --\n  key1: 'value1'\nbar: string\n  -- field metadata --\n  key3: 'value3'\n-- schema metadata --\nlorem: '{}'\"\"\".format(lorem)\n\n    assert my_schema.to_string(truncate_metadata=False,\n                               show_field_metadata=False) == \"\"\"\\\nfoo: int32 not null\nbar: string\n-- schema metadata --\nlorem: '{}'\"\"\".format(lorem)\n\n    assert my_schema.to_string(truncate_metadata=False,\n                               show_schema_metadata=False) == \"\"\"\\\nfoo: int32 not null\n  -- field metadata --\n  key1: 'value1'\nbar: string\n  -- field metadata --\n  key3: 'value3'\"\"\"\n\n    assert my_schema.to_string(truncate_metadata=False,\n                               show_field_metadata=False,\n                               show_schema_metadata=False) == \"\"\"\\\nfoo: int32 not null\nbar: string\"\"\"\n\n\ndef test_schema_from_tuples():\n    fields = [\n        ('foo', pa.int32()),\n        ('bar', pa.string()),\n        ('baz', pa.list_(pa.int8())),\n    ]\n    sch = pa.schema(fields)\n    assert sch.names == ['foo', 'bar', 'baz']\n    assert sch.types == [pa.int32(), pa.string(), pa.list_(pa.int8())]\n    assert len(sch) == 3\n    assert repr(sch) == \"\"\"\\\nfoo: int32\nbar: string\nbaz: list<item: int8>\n  child 0, item: int8\"\"\"\n\n    with pytest.raises(TypeError):\n        pa.schema([('foo', None)])\n\n\ndef test_schema_from_mapping():\n    fields = OrderedDict([\n        ('foo', pa.int32()),\n        ('bar', pa.string()),\n        ('baz', pa.list_(pa.int8())),\n    ])\n    sch = pa.schema(fields)\n    assert sch.names == ['foo', 'bar', 'baz']\n    assert sch.types == [pa.int32(), pa.string(), pa.list_(pa.int8())]\n    assert len(sch) == 3\n    assert repr(sch) == \"\"\"\\\nfoo: int32\nbar: string\nbaz: list<item: int8>\n  child 0, item: int8\"\"\"\n\n    fields = OrderedDict([('foo', None)])\n    with pytest.raises(TypeError):\n        pa.schema(fields)\n\n\ndef test_schema_duplicate_fields():\n    fields = [\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n        pa.field('foo', pa.list_(pa.int8())),\n    ]\n    sch = pa.schema(fields)\n    assert sch.names == ['foo', 'bar', 'foo']\n    assert sch.types == [pa.int32(), pa.string(), pa.list_(pa.int8())]\n    assert len(sch) == 3\n    assert repr(sch) == \"\"\"\\\nfoo: int32\nbar: string\nfoo: list<item: int8>\n  child 0, item: int8\"\"\"\n\n    assert sch[0].name == 'foo'\n    assert sch[0].type == fields[0].type\n    with pytest.warns(FutureWarning):\n        assert sch.field_by_name('bar') == fields[1]\n    with pytest.warns(FutureWarning):\n        assert sch.field_by_name('xxx') is None\n    with pytest.warns((UserWarning, FutureWarning)):\n        assert sch.field_by_name('foo') is None\n\n    # Schema::GetFieldIndex\n    assert sch.get_field_index('foo') == -1\n\n    # Schema::GetAllFieldIndices\n    assert sch.get_all_field_indices('foo') == [0, 2]\n\n\ndef test_field_flatten():\n    f0 = pa.field('foo', pa.int32()).with_metadata({b'foo': b'bar'})\n    assert f0.flatten() == [f0]\n\n    f1 = pa.field('bar', pa.float64(), nullable=False)\n    ff = pa.field('ff', pa.struct([f0, f1]), nullable=False)\n    assert ff.flatten() == [\n        pa.field('ff.foo', pa.int32()).with_metadata({b'foo': b'bar'}),\n        pa.field('ff.bar', pa.float64(), nullable=False)]  # XXX\n\n    # Nullable parent makes flattened child nullable\n    ff = pa.field('ff', pa.struct([f0, f1]))\n    assert ff.flatten() == [\n        pa.field('ff.foo', pa.int32()).with_metadata({b'foo': b'bar'}),\n        pa.field('ff.bar', pa.float64())]\n\n    fff = pa.field('fff', pa.struct([ff]))\n    assert fff.flatten() == [pa.field('fff.ff', pa.struct([f0, f1]))]\n\n\ndef test_schema_add_remove_metadata():\n    fields = [\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n        pa.field('baz', pa.list_(pa.int8()))\n    ]\n\n    s1 = pa.schema(fields)\n\n    assert s1.metadata is None\n\n    metadata = {b'foo': b'bar', b'pandas': b'badger'}\n\n    s2 = s1.with_metadata(metadata)\n    assert s2.metadata == metadata\n\n    s3 = s2.remove_metadata()\n    assert s3.metadata is None\n\n    # idempotent\n    s4 = s3.remove_metadata()\n    assert s4.metadata is None\n\n\ndef test_schema_equals():\n    fields = [\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n        pa.field('baz', pa.list_(pa.int8()))\n    ]\n    metadata = {b'foo': b'bar', b'pandas': b'badger'}\n\n    sch1 = pa.schema(fields)\n    sch2 = pa.schema(fields)\n    sch3 = pa.schema(fields, metadata=metadata)\n    sch4 = pa.schema(fields, metadata=metadata)\n\n    assert sch1.equals(sch2, check_metadata=True)\n    assert sch3.equals(sch4, check_metadata=True)\n    assert sch1.equals(sch3)\n    assert not sch1.equals(sch3, check_metadata=True)\n    assert not sch1.equals(sch3, check_metadata=True)\n\n    del fields[-1]\n    sch3 = pa.schema(fields)\n    assert not sch1.equals(sch3)\n\n\ndef test_schema_equals_propagates_check_metadata():\n    # ARROW-4088\n    schema1 = pa.schema([\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string())\n    ])\n    schema2 = pa.schema([\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string(), metadata={'a': 'alpha'}),\n    ])\n    assert not schema1.equals(schema2, check_metadata=True)\n    assert schema1.equals(schema2)\n\n\ndef test_schema_equals_invalid_type():\n    # ARROW-5873\n    schema = pa.schema([pa.field(\"a\", pa.int64())])\n\n    for val in [None, 'string', pa.array([1, 2])]:\n        with pytest.raises(TypeError):\n            schema.equals(val)\n\n\ndef test_schema_equality_operators():\n    fields = [\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n        pa.field('baz', pa.list_(pa.int8()))\n    ]\n    metadata = {b'foo': b'bar', b'pandas': b'badger'}\n\n    sch1 = pa.schema(fields)\n    sch2 = pa.schema(fields)\n    sch3 = pa.schema(fields, metadata=metadata)\n    sch4 = pa.schema(fields, metadata=metadata)\n\n    assert sch1 == sch2\n    assert sch3 == sch4\n\n    # __eq__ and __ne__ do not check metadata\n    assert sch1 == sch3\n    assert not sch1 != sch3\n\n    assert sch2 == sch4\n\n    # comparison with other types doesn't raise\n    assert sch1 != []\n    assert sch3 != 'foo'\n\n\ndef test_schema_get_fields():\n    fields = [\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n        pa.field('baz', pa.list_(pa.int8()))\n    ]\n\n    schema = pa.schema(fields)\n\n    assert schema.field('foo').name == 'foo'\n    assert schema.field(0).name == 'foo'\n    assert schema.field(-1).name == 'baz'\n\n    with pytest.raises(KeyError):\n        schema.field('other')\n    with pytest.raises(TypeError):\n        schema.field(0.0)\n    with pytest.raises(IndexError):\n        schema.field(4)\n\n\ndef test_schema_negative_indexing():\n    fields = [\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n        pa.field('baz', pa.list_(pa.int8()))\n    ]\n\n    schema = pa.schema(fields)\n\n    assert schema[-1].equals(schema[2])\n    assert schema[-2].equals(schema[1])\n    assert schema[-3].equals(schema[0])\n\n    with pytest.raises(IndexError):\n        schema[-4]\n\n    with pytest.raises(IndexError):\n        schema[3]\n\n\ndef test_schema_repr_with_dictionaries():\n    fields = [\n        pa.field('one', pa.dictionary(pa.int16(), pa.string())),\n        pa.field('two', pa.int32())\n    ]\n    sch = pa.schema(fields)\n\n    expected = (\n        \"\"\"\\\none: dictionary<values=string, indices=int16, ordered=0>\ntwo: int32\"\"\")\n\n    assert repr(sch) == expected\n\n\ndef test_type_schema_pickling(pickle_module):\n    cases = [\n        pa.int8(),\n        pa.string(),\n        pa.binary(),\n        pa.binary(10),\n        pa.list_(pa.string()),\n        pa.map_(pa.string(), pa.int8()),\n        pa.struct([\n            pa.field('a', 'int8'),\n            pa.field('b', 'string')\n        ]),\n        pa.union([\n            pa.field('a', pa.int8()),\n            pa.field('b', pa.int16())\n        ], pa.lib.UnionMode_SPARSE),\n        pa.union([\n            pa.field('a', pa.int8()),\n            pa.field('b', pa.int16())\n        ], pa.lib.UnionMode_DENSE),\n        pa.time32('s'),\n        pa.time64('us'),\n        pa.date32(),\n        pa.date64(),\n        pa.timestamp('ms'),\n        pa.timestamp('ns'),\n        pa.decimal128(12, 2),\n        pa.decimal256(76, 38),\n        pa.field('a', 'string', metadata={b'foo': b'bar'}),\n        pa.list_(pa.field(\"element\", pa.int64())),\n        pa.large_list(pa.field(\"element\", pa.int64())),\n        pa.map_(pa.field(\"key\", pa.string(), nullable=False),\n                pa.field(\"value\", pa.int8()))\n    ]\n\n    for val in cases:\n        roundtripped = pickle_module.loads(pickle_module.dumps(val))\n        assert val == roundtripped\n\n    fields = []\n    for i, f in enumerate(cases):\n        if isinstance(f, pa.Field):\n            fields.append(f)\n        else:\n            fields.append(pa.field('_f{}'.format(i), f))\n\n    schema = pa.schema(fields, metadata={b'foo': b'bar'})\n    roundtripped = pickle_module.loads(pickle_module.dumps(schema))\n    assert schema == roundtripped\n\n\ndef test_empty_table():\n    schema1 = pa.schema([\n        pa.field('f0', pa.int64()),\n        pa.field('f1', pa.dictionary(pa.int32(), pa.string())),\n        pa.field('f2', pa.list_(pa.list_(pa.int64()))),\n    ])\n    # test it preserves field nullability\n    schema2 = pa.schema([\n        pa.field('a', pa.int64(), nullable=False),\n        pa.field('b', pa.int64())\n    ])\n\n    for schema in [schema1, schema2]:\n        table = schema.empty_table()\n        assert isinstance(table, pa.Table)\n        assert table.num_rows == 0\n        assert table.schema == schema\n\n\n@pytest.mark.pandas\ndef test_schema_from_pandas():\n    import pandas as pd\n    inputs = [\n        list(range(10)),\n        pd.Categorical(list(range(10))),\n        ['foo', 'bar', None, 'baz', 'qux'],\n        np.array([\n            '2007-07-13T01:23:34.123456789',\n            '2006-01-13T12:34:56.432539784',\n            '2010-08-13T05:46:57.437699912'\n        ], dtype='datetime64[ns]'),\n        pd.array([1, 2, None], dtype=pd.Int32Dtype()),\n    ]\n    for data in inputs:\n        df = pd.DataFrame({'a': data}, index=data)\n        schema = pa.Schema.from_pandas(df)\n        expected = pa.Table.from_pandas(df).schema\n        assert schema == expected\n\n\ndef test_schema_sizeof():\n    schema = pa.schema([\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n    ])\n\n    # Note: pa.schema is twice as large on 64-bit systems\n    assert sys.getsizeof(schema) > (30 if sys.maxsize > 2**32 else 15)\n\n    schema2 = schema.with_metadata({\"key\": \"some metadata\"})\n    assert sys.getsizeof(schema2) > sys.getsizeof(schema)\n    schema3 = schema.with_metadata({\"key\": \"some more metadata\"})\n    assert sys.getsizeof(schema3) > sys.getsizeof(schema2)\n\n\ndef test_schema_merge():\n    a = pa.schema([\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n        pa.field('baz', pa.list_(pa.int8()))\n    ])\n    b = pa.schema([\n        pa.field('foo', pa.int32()),\n        pa.field('qux', pa.bool_())\n    ])\n    c = pa.schema([\n        pa.field('quux', pa.dictionary(pa.int32(), pa.string()))\n    ])\n    d = pa.schema([\n        pa.field('foo', pa.int64()),\n        pa.field('qux', pa.bool_())\n    ])\n\n    result = pa.unify_schemas([a, b, c])\n    expected = pa.schema([\n        pa.field('foo', pa.int32()),\n        pa.field('bar', pa.string()),\n        pa.field('baz', pa.list_(pa.int8())),\n        pa.field('qux', pa.bool_()),\n        pa.field('quux', pa.dictionary(pa.int32(), pa.string()))\n    ])\n    assert result.equals(expected)\n\n    with pytest.raises(pa.ArrowTypeError):\n        pa.unify_schemas([b, d])\n\n    # ARROW-14002: Try with tuple instead of list\n    result = pa.unify_schemas((a, b, c))\n    assert result.equals(expected)\n\n    result = pa.unify_schemas([b, d], promote_options=\"permissive\")\n    assert result.equals(d)\n\n    # raise proper error when passing a non-Schema value\n    with pytest.raises(TypeError):\n        pa.unify_schemas([a, 1])\n\n\ndef test_undecodable_metadata():\n    # ARROW-10214: undecodable metadata shouldn't fail repr()\n    data1 = b'abcdef\\xff\\x00'\n    data2 = b'ghijkl\\xff\\x00'\n    schema = pa.schema(\n        [pa.field('ints', pa.int16(), metadata={'key': data1})],\n        metadata={'key': data2})\n    assert 'abcdef' in str(schema)\n    assert 'ghijkl' in str(schema)\n", "python/pyarrow/tests/test_dlpack.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport ctypes\nfrom functools import wraps\nimport pytest\n\nimport numpy as np\n\nimport pyarrow as pa\nfrom pyarrow.vendored.version import Version\n\n\ndef PyCapsule_IsValid(capsule, name):\n    return ctypes.pythonapi.PyCapsule_IsValid(ctypes.py_object(capsule), name) == 1\n\n\ndef check_dlpack_export(arr, expected_arr):\n    DLTensor = arr.__dlpack__()\n    assert PyCapsule_IsValid(DLTensor, b\"dltensor\") is True\n\n    result = np.from_dlpack(arr)\n    np.testing.assert_array_equal(result, expected_arr, strict=True)\n\n    assert arr.__dlpack_device__() == (1, 0)\n\n\ndef check_bytes_allocated(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        allocated_bytes = pa.total_allocated_bytes()\n        try:\n            return f(*args, **kwargs)\n        finally:\n            assert pa.total_allocated_bytes() == allocated_bytes\n    return wrapper\n\n\n@check_bytes_allocated\n@pytest.mark.parametrize(\n    ('value_type', 'np_type'),\n    [\n        (pa.uint8(), np.uint8),\n        (pa.uint16(), np.uint16),\n        (pa.uint32(), np.uint32),\n        (pa.uint64(), np.uint64),\n        (pa.int8(), np.int8),\n        (pa.int16(), np.int16),\n        (pa.int32(), np.int32),\n        (pa.int64(), np.int64),\n        (pa.float16(), np.float16),\n        (pa.float32(), np.float32),\n        (pa.float64(), np.float64),\n    ]\n)\ndef test_dlpack(value_type, np_type):\n    if Version(np.__version__) < Version(\"1.24.0\"):\n        pytest.skip(\"No dlpack support in numpy versions older than 1.22.0, \"\n                    \"strict keyword in assert_array_equal added in numpy version \"\n                    \"1.24.0\")\n\n    expected = np.array([1, 2, 3], dtype=np_type)\n    arr = pa.array(expected, type=value_type)\n    check_dlpack_export(arr, expected)\n\n    arr_sliced = arr.slice(1, 1)\n    expected = np.array([2], dtype=np_type)\n    check_dlpack_export(arr_sliced, expected)\n\n    arr_sliced = arr.slice(0, 1)\n    expected = np.array([1], dtype=np_type)\n    check_dlpack_export(arr_sliced, expected)\n\n    arr_sliced = arr.slice(1)\n    expected = np.array([2, 3], dtype=np_type)\n    check_dlpack_export(arr_sliced, expected)\n\n    arr_zero = pa.array([], type=value_type)\n    expected = np.array([], dtype=np_type)\n    check_dlpack_export(arr_zero, expected)\n\n\ndef test_dlpack_not_supported():\n    if Version(np.__version__) < Version(\"1.22.0\"):\n        pytest.skip(\"No dlpack support in numpy versions older than 1.22.0.\")\n\n    arr = pa.array([1, None, 3])\n    with pytest.raises(TypeError, match=\"Can only use DLPack \"\n                       \"on arrays with no nulls.\"):\n        np.from_dlpack(arr)\n\n    arr = pa.array(\n        [[0, 1], [3, 4]],\n        type=pa.list_(pa.int32())\n    )\n    with pytest.raises(TypeError, match=\"DataType is not compatible with DLPack spec\"):\n        np.from_dlpack(arr)\n\n    arr = pa.array([])\n    with pytest.raises(TypeError, match=\"DataType is not compatible with DLPack spec\"):\n        np.from_dlpack(arr)\n\n    # DLPack doesn't support bit-packed boolean values\n    arr = pa.array([True, False, True])\n    with pytest.raises(TypeError, match=\"Bit-packed boolean data type \"\n                       \"not supported by DLPack.\"):\n        np.from_dlpack(arr)\n\n\ndef test_dlpack_cuda_not_supported():\n    cuda = pytest.importorskip(\"pyarrow.cuda\")\n\n    schema = pa.schema([pa.field('f0', pa.int16())])\n    a0 = pa.array([1, 2, 3], type=pa.int16())\n    batch = pa.record_batch([a0], schema=schema)\n\n    cbuf = cuda.serialize_record_batch(batch, cuda.Context(0))\n    cbatch = cuda.read_record_batch(cbuf, batch.schema)\n    carr = cbatch[\"f0\"]\n\n    # CudaBuffers not yet supported\n    with pytest.raises(NotImplementedError, match=\"DLPack support is implemented \"\n                       \"only for buffers on CPU device.\"):\n        np.from_dlpack(carr)\n\n    with pytest.raises(NotImplementedError, match=\"DLPack support is implemented \"\n                       \"only for buffers on CPU device.\"):\n        carr.__dlpack_device__()\n", "python/pyarrow/tests/arrow_7980.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# This file is called from a test in test_schema.py.\n\nimport pyarrow as pa\n\n\n# the types where to_pandas_dtype returns a non-numpy dtype\ncases = [\n    (pa.timestamp('ns', tz='UTC'), \"datetime64[ns, UTC]\"),\n]\n\n\nfor arrow_type, pandas_type in cases:\n    assert str(arrow_type.to_pandas_dtype()) == pandas_type\n", "python/pyarrow/tests/test_orc.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\nimport decimal\nimport datetime\n\nimport pyarrow as pa\nfrom pyarrow import fs\nfrom pyarrow.tests import util\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not orc'\npytestmark = pytest.mark.orc\n\n\ntry:\n    from pandas.testing import assert_frame_equal\n    import pandas as pd\nexcept ImportError:\n    pass\n\n\n@pytest.fixture(scope=\"module\")\ndef datadir(base_datadir):\n    return base_datadir / \"orc\"\n\n\ndef fix_example_values(actual_cols, expected_cols):\n    \"\"\"\n    Fix type of expected values (as read from JSON) according to\n    actual ORC datatype.\n    \"\"\"\n    for name in expected_cols:\n        expected = expected_cols[name]\n        actual = actual_cols[name]\n        if (name == \"map\" and\n                [d.keys() == {'key', 'value'} for m in expected for d in m]):\n            # convert [{'key': k, 'value': v}, ...] to [(k, v), ...]\n            col = expected_cols[name].copy()\n            for i, m in enumerate(expected):\n                col[i] = [(d['key'], d['value']) for d in m]\n            expected_cols[name] = col\n            continue\n\n        typ = actual[0].__class__\n        if issubclass(typ, datetime.datetime):\n            # timestamp fields are represented as strings in JSON files\n            expected = pd.to_datetime(expected)\n        elif issubclass(typ, datetime.date):\n            # date fields are represented as strings in JSON files\n            expected = expected.dt.date\n        elif typ is decimal.Decimal:\n            converted_decimals = [None] * len(expected)\n            # decimal fields are represented as reals in JSON files\n            for i, (d, v) in enumerate(zip(actual, expected)):\n                if not pd.isnull(v):\n                    exp = d.as_tuple().exponent\n                    factor = 10 ** -exp\n                    converted_decimals[i] = (\n                        decimal.Decimal(round(v * factor)).scaleb(exp))\n            expected = pd.Series(converted_decimals)\n\n        expected_cols[name] = expected\n\n\ndef check_example_values(orc_df, expected_df, start=None, stop=None):\n    if start is not None or stop is not None:\n        expected_df = expected_df[start:stop].reset_index(drop=True)\n    assert_frame_equal(orc_df, expected_df, check_dtype=False)\n\n\ndef check_example_file(orc_path, expected_df, need_fix=False):\n    \"\"\"\n    Check a ORC file against the expected columns dictionary.\n    \"\"\"\n    from pyarrow import orc\n\n    orc_file = orc.ORCFile(orc_path)\n    # Exercise ORCFile.read()\n    table = orc_file.read()\n    assert isinstance(table, pa.Table)\n    table.validate()\n\n    # This workaround needed because of ARROW-3080\n    orc_df = pd.DataFrame(table.to_pydict())\n\n    assert set(expected_df.columns) == set(orc_df.columns)\n\n    # reorder columns if necessary\n    if not orc_df.columns.equals(expected_df.columns):\n        expected_df = expected_df.reindex(columns=orc_df.columns)\n\n    if need_fix:\n        fix_example_values(orc_df, expected_df)\n\n    check_example_values(orc_df, expected_df)\n    # Exercise ORCFile.read_stripe()\n    json_pos = 0\n    for i in range(orc_file.nstripes):\n        batch = orc_file.read_stripe(i)\n        check_example_values(pd.DataFrame(batch.to_pydict()),\n                             expected_df,\n                             start=json_pos,\n                             stop=json_pos + len(batch))\n        json_pos += len(batch)\n    assert json_pos == orc_file.nrows\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('filename', [\n    'TestOrcFile.test1.orc',\n    'TestOrcFile.testDate1900.orc',\n    'decimal.orc'\n])\ndef test_example_using_json(filename, datadir):\n    \"\"\"\n    Check a ORC file example against the equivalent JSON file, as given\n    in the Apache ORC repository (the JSON file has one JSON object per\n    line, corresponding to one row in the ORC file).\n    \"\"\"\n    # Read JSON file\n    path = datadir / filename\n    table = pd.read_json(str(path.with_suffix('.jsn.gz')), lines=True)\n    check_example_file(path, table, need_fix=True)\n\n\ndef test_orcfile_empty(datadir):\n    from pyarrow import orc\n\n    table = orc.ORCFile(datadir / \"TestOrcFile.emptyFile.orc\").read()\n    assert table.num_rows == 0\n\n    expected_schema = pa.schema([\n        (\"boolean1\", pa.bool_()),\n        (\"byte1\", pa.int8()),\n        (\"short1\", pa.int16()),\n        (\"int1\", pa.int32()),\n        (\"long1\", pa.int64()),\n        (\"float1\", pa.float32()),\n        (\"double1\", pa.float64()),\n        (\"bytes1\", pa.binary()),\n        (\"string1\", pa.string()),\n        (\"middle\", pa.struct(\n            [(\"list\", pa.list_(\n                pa.struct([(\"int1\", pa.int32()),\n                           (\"string1\", pa.string())])))\n             ])),\n        (\"list\", pa.list_(\n            pa.struct([(\"int1\", pa.int32()),\n                       (\"string1\", pa.string())])\n        )),\n        (\"map\", pa.map_(pa.string(),\n                        pa.struct([(\"int1\", pa.int32()),\n                                   (\"string1\", pa.string())])\n                        )),\n    ])\n    assert table.schema == expected_schema\n\n\ndef test_filesystem_uri(tmpdir):\n    from pyarrow import orc\n    table = pa.table({\"a\": [1, 2, 3]})\n\n    directory = tmpdir / \"data_dir\"\n    directory.mkdir()\n    path = directory / \"data.orc\"\n    orc.write_table(table, str(path))\n\n    # filesystem object\n    result = orc.read_table(path, filesystem=fs.LocalFileSystem())\n    assert result.equals(table)\n\n    # filesystem URI\n    result = orc.read_table(\n        \"data_dir/data.orc\", filesystem=util._filesystem_uri(tmpdir))\n    assert result.equals(table)\n\n    # use the path only\n    result = orc.read_table(\n        util._filesystem_uri(path))\n    assert result.equals(table)\n\n\ndef test_orcfile_readwrite(tmpdir):\n    from pyarrow import orc\n    a = pa.array([1, None, 3, None])\n    b = pa.array([None, \"Arrow\", None, \"ORC\"])\n    table = pa.table({\"int64\": a, \"utf8\": b})\n    file = tmpdir.join(\"test.orc\")\n    orc.write_table(table, file)\n    output_table = orc.read_table(file)\n    assert table.equals(output_table)\n\n    output_table = orc.read_table(file, [])\n    assert 4 == output_table.num_rows\n    assert 0 == output_table.num_columns\n\n    output_table = orc.read_table(file, columns=[\"int64\"])\n    assert 4 == output_table.num_rows\n    assert 1 == output_table.num_columns\n\n\ndef test_bytesio_readwrite():\n    from pyarrow import orc\n    from io import BytesIO\n\n    buf = BytesIO()\n    a = pa.array([1, None, 3, None])\n    b = pa.array([None, \"Arrow\", None, \"ORC\"])\n    table = pa.table({\"int64\": a, \"utf8\": b})\n    orc.write_table(table, buf)\n    buf.seek(0)\n    orc_file = orc.ORCFile(buf)\n    output_table = orc_file.read()\n    assert table.equals(output_table)\n\n\ndef test_buffer_readwrite():\n    from pyarrow import orc\n\n    buffer_output_stream = pa.BufferOutputStream()\n    a = pa.array([1, None, 3, None])\n    b = pa.array([None, \"Arrow\", None, \"ORC\"])\n    table = pa.table({\"int64\": a, \"utf8\": b})\n    orc.write_table(table, buffer_output_stream)\n    buffer_reader = pa.BufferReader(buffer_output_stream.getvalue())\n    orc_file = orc.ORCFile(buffer_reader)\n    output_table = orc_file.read()\n    assert table.equals(output_table)\n    # Check for default WriteOptions\n    assert orc_file.compression == 'UNCOMPRESSED'\n    assert orc_file.file_version == '0.12'\n    assert orc_file.row_index_stride == 10000\n    assert orc_file.compression_size == 65536\n\n    # deprecated keyword order\n    buffer_output_stream = pa.BufferOutputStream()\n    with pytest.warns(FutureWarning):\n        orc.write_table(buffer_output_stream, table)\n    buffer_reader = pa.BufferReader(buffer_output_stream.getvalue())\n    orc_file = orc.ORCFile(buffer_reader)\n    output_table = orc_file.read()\n    assert table.equals(output_table)\n    # Check for default WriteOptions\n    assert orc_file.compression == 'UNCOMPRESSED'\n    assert orc_file.file_version == '0.12'\n    assert orc_file.row_index_stride == 10000\n    assert orc_file.compression_size == 65536\n\n\n@pytest.mark.snappy\ndef test_buffer_readwrite_with_writeoptions():\n    from pyarrow import orc\n\n    buffer_output_stream = pa.BufferOutputStream()\n    a = pa.array([1, None, 3, None])\n    b = pa.array([None, \"Arrow\", None, \"ORC\"])\n    table = pa.table({\"int64\": a, \"utf8\": b})\n    orc.write_table(\n        table,\n        buffer_output_stream,\n        compression='snappy',\n        file_version='0.11',\n        row_index_stride=5000,\n        compression_block_size=32768,\n    )\n    buffer_reader = pa.BufferReader(buffer_output_stream.getvalue())\n    orc_file = orc.ORCFile(buffer_reader)\n    output_table = orc_file.read()\n    assert table.equals(output_table)\n    # Check for modified WriteOptions\n    assert orc_file.compression == 'SNAPPY'\n    assert orc_file.file_version == '0.11'\n    assert orc_file.row_index_stride == 5000\n    assert orc_file.compression_size == 32768\n\n    # deprecated keyword order\n    buffer_output_stream = pa.BufferOutputStream()\n    with pytest.warns(FutureWarning):\n        orc.write_table(\n            buffer_output_stream,\n            table,\n            compression='uncompressed',\n            file_version='0.11',\n            row_index_stride=20000,\n            compression_block_size=16384,\n        )\n    buffer_reader = pa.BufferReader(buffer_output_stream.getvalue())\n    orc_file = orc.ORCFile(buffer_reader)\n    output_table = orc_file.read()\n    assert table.equals(output_table)\n    # Check for default WriteOptions\n    assert orc_file.compression == 'UNCOMPRESSED'\n    assert orc_file.file_version == '0.11'\n    assert orc_file.row_index_stride == 20000\n    assert orc_file.compression_size == 16384\n\n\ndef test_buffer_readwrite_with_bad_writeoptions():\n    from pyarrow import orc\n    buffer_output_stream = pa.BufferOutputStream()\n    a = pa.array([1, None, 3, None])\n    table = pa.table({\"int64\": a})\n\n    # batch_size must be a positive integer\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            batch_size=0,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            batch_size=-100,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            batch_size=1024.23,\n        )\n\n    # file_version must be 0.11 or 0.12\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            file_version=0.13,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            file_version='1.1',\n        )\n\n    # stripe_size must be a positive integer\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            stripe_size=0,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            stripe_size=-400,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            stripe_size=4096.73,\n        )\n\n    # compression must be among the given options\n    with pytest.raises(TypeError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            compression=0,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            compression='none',\n        )\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            compression='zlid',\n        )\n\n    # compression_block_size must be a positive integer\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            compression_block_size=0,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            compression_block_size=-200,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            compression_block_size=1096.73,\n        )\n\n    # compression_strategy must be among the given options\n    with pytest.raises(TypeError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            compression_strategy=0,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            compression_strategy='no',\n        )\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            compression_strategy='large',\n        )\n\n    # row_index_stride must be a positive integer\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            row_index_stride=0,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            row_index_stride=-800,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            row_index_stride=3096.29,\n        )\n\n    # padding_tolerance must be possible to cast to float\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            padding_tolerance='cat',\n        )\n\n    # dictionary_key_size_threshold must be possible to cast to\n    # float between 0.0 and 1.0\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            dictionary_key_size_threshold='arrow',\n        )\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            dictionary_key_size_threshold=1.2,\n        )\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            dictionary_key_size_threshold=-3.2,\n        )\n\n    # bloom_filter_columns must be convertible to a list containing\n    # nonnegative integers\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            bloom_filter_columns=\"string\",\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            bloom_filter_columns=[0, 1.4],\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            bloom_filter_columns={0, 2, -1},\n        )\n\n    # bloom_filter_fpp must be convertible to a float between 0.0 and 1.0\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            bloom_filter_fpp='arrow',\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            bloom_filter_fpp=1.1,\n        )\n\n    with pytest.raises(ValueError):\n        orc.write_table(\n            table,\n            buffer_output_stream,\n            bloom_filter_fpp=-0.1,\n        )\n\n\ndef test_column_selection(tempdir):\n    from pyarrow import orc\n\n    # create a table with nested types\n    inner = pa.field('inner', pa.int64())\n    middle = pa.field('middle', pa.struct([inner]))\n    fields = [\n        pa.field('basic', pa.int32()),\n        pa.field(\n            'list', pa.list_(pa.field('item', pa.int32()))\n        ),\n        pa.field(\n            'struct', pa.struct([middle, pa.field('inner2', pa.int64())])\n        ),\n        pa.field(\n            'list-struct', pa.list_(pa.field(\n                'item', pa.struct([\n                    pa.field('inner1', pa.int64()),\n                    pa.field('inner2', pa.int64())\n                ])\n            ))\n        ),\n        pa.field('basic2', pa.int64()),\n    ]\n    arrs = [\n        [0], [[1, 2]], [{\"middle\": {\"inner\": 3}, \"inner2\": 4}],\n        [[{\"inner1\": 5, \"inner2\": 6}, {\"inner1\": 7, \"inner2\": 8}]], [9]]\n    table = pa.table(arrs, schema=pa.schema(fields))\n\n    path = str(tempdir / 'test.orc')\n    orc.write_table(table, path)\n    orc_file = orc.ORCFile(path)\n\n    # default selecting all columns\n    result1 = orc_file.read()\n    assert result1.equals(table)\n\n    # selecting with columns names\n    result2 = orc_file.read(columns=[\"basic\", \"basic2\"])\n    assert result2.equals(table.select([\"basic\", \"basic2\"]))\n\n    result3 = orc_file.read(columns=[\"list\", \"struct\", \"basic2\"])\n    assert result3.equals(table.select([\"list\", \"struct\", \"basic2\"]))\n\n    # using dotted paths\n    result4 = orc_file.read(columns=[\"struct.middle.inner\"])\n    expected4 = pa.table({\"struct\": [{\"middle\": {\"inner\": 3}}]})\n    assert result4.equals(expected4)\n\n    result5 = orc_file.read(columns=[\"struct.inner2\"])\n    expected5 = pa.table({\"struct\": [{\"inner2\": 4}]})\n    assert result5.equals(expected5)\n\n    result6 = orc_file.read(\n        columns=[\"list\", \"struct.middle.inner\", \"struct.inner2\"]\n    )\n    assert result6.equals(table.select([\"list\", \"struct\"]))\n\n    result7 = orc_file.read(columns=[\"list-struct.inner1\"])\n    expected7 = pa.table({\"list-struct\": [[{\"inner1\": 5}, {\"inner1\": 7}]]})\n    assert result7.equals(expected7)\n\n    # selecting with (Arrow-based) field indices\n    result2 = orc_file.read(columns=[0, 4])\n    assert result2.equals(table.select([\"basic\", \"basic2\"]))\n\n    result3 = orc_file.read(columns=[1, 2, 3])\n    assert result3.equals(table.select([\"list\", \"struct\", \"list-struct\"]))\n\n    # error on non-existing name or index\n    with pytest.raises(IOError):\n        # liborc returns ParseError, which gets translated into IOError\n        # instead of ValueError\n        orc_file.read(columns=[\"wrong\"])\n\n    with pytest.raises(ValueError):\n        orc_file.read(columns=[5])\n\n\ndef test_wrong_usage_orc_writer(tempdir):\n    from pyarrow import orc\n\n    path = str(tempdir / 'test.orc')\n    with orc.ORCWriter(path) as writer:\n        with pytest.raises(AttributeError):\n            writer.test()\n\n\ndef test_orc_writer_with_null_arrays(tempdir):\n    from pyarrow import orc\n\n    path = str(tempdir / 'test.orc')\n    a = pa.array([1, None, 3, None])\n    b = pa.array([None, None, None, None])\n    table = pa.table({\"int64\": a, \"utf8\": b})\n    with pytest.raises(pa.ArrowNotImplementedError):\n        orc.write_table(table, path)\n", "python/pyarrow/tests/pandas_examples.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import OrderedDict\nfrom datetime import date, time\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n\n\ndef dataframe_with_arrays(include_index=False):\n    \"\"\"\n    Dataframe with numpy arrays columns of every possible primitive type.\n\n    Returns\n    -------\n    df: pandas.DataFrame\n    schema: pyarrow.Schema\n        Arrow schema definition that is in line with the constructed df.\n    \"\"\"\n    dtypes = [('i1', pa.int8()), ('i2', pa.int16()),\n              ('i4', pa.int32()), ('i8', pa.int64()),\n              ('u1', pa.uint8()), ('u2', pa.uint16()),\n              ('u4', pa.uint32()), ('u8', pa.uint64()),\n              ('f4', pa.float32()), ('f8', pa.float64())]\n\n    arrays = OrderedDict()\n    fields = []\n    for dtype, arrow_dtype in dtypes:\n        fields.append(pa.field(dtype, pa.list_(arrow_dtype)))\n        arrays[dtype] = [\n            np.arange(10, dtype=dtype),\n            np.arange(5, dtype=dtype),\n            None,\n            np.arange(1, dtype=dtype)\n        ]\n\n    fields.append(pa.field('str', pa.list_(pa.string())))\n    arrays['str'] = [\n        np.array([\"1\", \"\u00e4\"], dtype=\"object\"),\n        None,\n        np.array([\"1\"], dtype=\"object\"),\n        np.array([\"1\", \"2\", \"3\"], dtype=\"object\")\n    ]\n\n    fields.append(pa.field('datetime64', pa.list_(pa.timestamp('ms'))))\n    arrays['datetime64'] = [\n        np.array(['2007-07-13T01:23:34.123456789',\n                  None,\n                  '2010-08-13T05:46:57.437699912'],\n                 dtype='datetime64[ms]'),\n        None,\n        None,\n        np.array(['2007-07-13T02',\n                  None,\n                  '2010-08-13T05:46:57.437699912'],\n                 dtype='datetime64[ms]'),\n    ]\n\n    if include_index:\n        fields.append(pa.field('__index_level_0__', pa.int64()))\n    df = pd.DataFrame(arrays)\n    schema = pa.schema(fields)\n\n    return df, schema\n\n\ndef dataframe_with_lists(include_index=False, parquet_compatible=False):\n    \"\"\"\n    Dataframe with list columns of every possible primitive type.\n\n    Returns\n    -------\n    df: pandas.DataFrame\n    schema: pyarrow.Schema\n        Arrow schema definition that is in line with the constructed df.\n    parquet_compatible: bool\n        Exclude types not supported by parquet\n    \"\"\"\n    arrays = OrderedDict()\n    fields = []\n\n    fields.append(pa.field('int64', pa.list_(pa.int64())))\n    arrays['int64'] = [\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n        [0, 1, 2, 3, 4],\n        None,\n        [],\n        np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 2,\n                 dtype=np.int64)[::2]\n    ]\n    fields.append(pa.field('double', pa.list_(pa.float64())))\n    arrays['double'] = [\n        [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n        [0., 1., 2., 3., 4.],\n        None,\n        [],\n        np.array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.] * 2)[::2],\n    ]\n    fields.append(pa.field('bytes_list', pa.list_(pa.binary())))\n    arrays['bytes_list'] = [\n        [b\"1\", b\"f\"],\n        None,\n        [b\"1\"],\n        [b\"1\", b\"2\", b\"3\"],\n        [],\n    ]\n    fields.append(pa.field('str_list', pa.list_(pa.string())))\n    arrays['str_list'] = [\n        [\"1\", \"\u00e4\"],\n        None,\n        [\"1\"],\n        [\"1\", \"2\", \"3\"],\n        [],\n    ]\n\n    date_data = [\n        [],\n        [date(2018, 1, 1), date(2032, 12, 30)],\n        [date(2000, 6, 7)],\n        None,\n        [date(1969, 6, 9), date(1972, 7, 3)]\n    ]\n    time_data = [\n        [time(23, 11, 11), time(1, 2, 3), time(23, 59, 59)],\n        [],\n        [time(22, 5, 59)],\n        None,\n        [time(0, 0, 0), time(18, 0, 2), time(12, 7, 3)]\n    ]\n\n    temporal_pairs = [\n        (pa.date32(), date_data),\n        (pa.date64(), date_data),\n        (pa.time32('s'), time_data),\n        (pa.time32('ms'), time_data),\n        (pa.time64('us'), time_data)\n    ]\n    if not parquet_compatible:\n        temporal_pairs += [\n            (pa.time64('ns'), time_data),\n        ]\n\n    for value_type, data in temporal_pairs:\n        field_name = '{}_list'.format(value_type)\n        field_type = pa.list_(value_type)\n        field = pa.field(field_name, field_type)\n        fields.append(field)\n        arrays[field_name] = data\n\n    if include_index:\n        fields.append(pa.field('__index_level_0__', pa.int64()))\n\n    df = pd.DataFrame(arrays)\n    schema = pa.schema(fields)\n\n    return df, schema\n", "python/pyarrow/tests/test_acero.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\n\nimport pyarrow as pa\nimport pyarrow.compute as pc\nfrom pyarrow.compute import field\n\ntry:\n    from pyarrow.acero import (\n        Declaration,\n        TableSourceNodeOptions,\n        FilterNodeOptions,\n        ProjectNodeOptions,\n        AggregateNodeOptions,\n        OrderByNodeOptions,\n        HashJoinNodeOptions,\n        AsofJoinNodeOptions,\n    )\nexcept ImportError:\n    pass\n\ntry:\n    import pyarrow.dataset as ds\n    from pyarrow.acero import ScanNodeOptions\nexcept ImportError:\n    ds = None\n\npytestmark = pytest.mark.acero\n\n\n@pytest.fixture\ndef table_source():\n    table = pa.table({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    table_opts = TableSourceNodeOptions(table)\n    table_source = Declaration(\"table_source\", options=table_opts)\n    return table_source\n\n\ndef test_declaration():\n\n    table = pa.table({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    table_opts = TableSourceNodeOptions(table)\n    filter_opts = FilterNodeOptions(field('a') > 1)\n\n    # using sequence\n    decl = Declaration.from_sequence([\n        Declaration(\"table_source\", options=table_opts),\n        Declaration(\"filter\", options=filter_opts)\n    ])\n    result = decl.to_table()\n    assert result.equals(table.slice(1, 2))\n\n    # using explicit inputs\n    table_source = Declaration(\"table_source\", options=table_opts)\n    filtered = Declaration(\"filter\", options=filter_opts, inputs=[table_source])\n    result = filtered.to_table()\n    assert result.equals(table.slice(1, 2))\n\n\ndef test_declaration_repr(table_source):\n\n    assert \"TableSourceNode\" in str(table_source)\n    assert \"TableSourceNode\" in repr(table_source)\n\n\ndef test_declaration_to_reader(table_source):\n    with table_source.to_reader() as reader:\n        assert reader.schema == pa.schema([(\"a\", pa.int64()), (\"b\", pa.int64())])\n        result = reader.read_all()\n    expected = pa.table({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    assert result.equals(expected)\n\n\ndef test_table_source():\n    with pytest.raises(TypeError):\n        TableSourceNodeOptions(pa.record_batch([pa.array([1, 2, 3])], [\"a\"]))\n\n    table_source = TableSourceNodeOptions(None)\n    decl = Declaration(\"table_source\", table_source)\n    with pytest.raises(\n        ValueError, match=\"TableSourceNode requires table which is not null\"\n    ):\n        _ = decl.to_table()\n\n\ndef test_filter(table_source):\n    # referencing unknown field\n    decl = Declaration.from_sequence([\n        table_source,\n        Declaration(\"filter\", options=FilterNodeOptions(field(\"c\") > 1))\n    ])\n    with pytest.raises(ValueError, match=r\"No match for FieldRef.Name\\(c\\)\"):\n        _ = decl.to_table()\n\n    # requires a pyarrow Expression\n    with pytest.raises(TypeError):\n        FilterNodeOptions(pa.array([True, False, True]))\n    with pytest.raises(TypeError):\n        FilterNodeOptions(None)\n\n\ndef test_project(table_source):\n    # default name from expression\n    decl = Declaration.from_sequence([\n        table_source,\n        Declaration(\"project\", ProjectNodeOptions([pc.multiply(field(\"a\"), 2)]))\n    ])\n    result = decl.to_table()\n    assert result.schema.names == [\"multiply(a, 2)\"]\n    assert result[0].to_pylist() == [2, 4, 6]\n\n    # provide name\n    decl = Declaration.from_sequence([\n        table_source,\n        Declaration(\"project\", ProjectNodeOptions([pc.multiply(field(\"a\"), 2)], [\"a2\"]))\n    ])\n    result = decl.to_table()\n    assert result.schema.names == [\"a2\"]\n    assert result[\"a2\"].to_pylist() == [2, 4, 6]\n\n    # input validation\n    with pytest.raises(ValueError):\n        ProjectNodeOptions([pc.multiply(field(\"a\"), 2)], [\"a2\", \"b2\"])\n\n    # no scalar expression\n    decl = Declaration.from_sequence([\n        table_source,\n        Declaration(\"project\", ProjectNodeOptions([pc.sum(field(\"a\"))]))\n    ])\n    with pytest.raises(ValueError, match=\"cannot Execute non-scalar expression\"):\n        _ = decl.to_table()\n\n\ndef test_aggregate_scalar(table_source):\n    decl = Declaration.from_sequence([\n        table_source,\n        Declaration(\"aggregate\", AggregateNodeOptions([(\"a\", \"sum\", None, \"a_sum\")]))\n    ])\n    result = decl.to_table()\n    assert result.schema.names == [\"a_sum\"]\n    assert result[\"a_sum\"].to_pylist() == [6]\n\n    # with options class\n    table = pa.table({'a': [1, 2, None]})\n    aggr_opts = AggregateNodeOptions(\n        [(\"a\", \"sum\", pc.ScalarAggregateOptions(skip_nulls=False), \"a_sum\")]\n    )\n    decl = Declaration.from_sequence([\n        Declaration(\"table_source\", TableSourceNodeOptions(table)),\n        Declaration(\"aggregate\", aggr_opts),\n    ])\n    result = decl.to_table()\n    assert result.schema.names == [\"a_sum\"]\n    assert result[\"a_sum\"].to_pylist() == [None]\n\n    # test various ways of specifying the target column\n    for target in [\"a\", field(\"a\"), 0, field(0), [\"a\"], [field(\"a\")], [0]]:\n        aggr_opts = AggregateNodeOptions([(target, \"sum\", None, \"a_sum\")])\n        decl = Declaration.from_sequence(\n            [table_source, Declaration(\"aggregate\", aggr_opts)]\n        )\n        result = decl.to_table()\n        assert result.schema.names == [\"a_sum\"]\n        assert result[\"a_sum\"].to_pylist() == [6]\n\n    # proper error when specifying the wrong number of target columns\n    aggr_opts = AggregateNodeOptions([([\"a\", \"b\"], \"sum\", None, \"a_sum\")])\n    decl = Declaration.from_sequence(\n        [table_source, Declaration(\"aggregate\", aggr_opts)]\n    )\n    with pytest.raises(\n        ValueError, match=\"Function 'sum' accepts 1 arguments but 2 passed\"\n    ):\n        _ = decl.to_table()\n\n    # proper error when using hash aggregation without keys\n    aggr_opts = AggregateNodeOptions([(\"a\", \"hash_sum\", None, \"a_sum\")])\n    decl = Declaration.from_sequence(\n        [table_source, Declaration(\"aggregate\", aggr_opts)]\n    )\n    with pytest.raises(ValueError, match=\"is a hash aggregate function\"):\n        _ = decl.to_table()\n\n\ndef test_aggregate_hash():\n    table = pa.table({'a': [1, 2, None], 'b': [\"foo\", \"bar\", \"foo\"]})\n    table_opts = TableSourceNodeOptions(table)\n    table_source = Declaration(\"table_source\", options=table_opts)\n\n    # default options\n    aggr_opts = AggregateNodeOptions(\n        [(\"a\", \"hash_count\", None, \"count(a)\")], keys=[\"b\"])\n    decl = Declaration.from_sequence([\n        table_source, Declaration(\"aggregate\", aggr_opts)\n    ])\n    result = decl.to_table()\n    expected = pa.table({\"b\": [\"foo\", \"bar\"], \"count(a)\": [1, 1]})\n    assert result.equals(expected)\n\n    # specify function options\n    aggr_opts = AggregateNodeOptions(\n        [(\"a\", \"hash_count\", pc.CountOptions(\"all\"), \"count(a)\")], keys=[\"b\"]\n    )\n    decl = Declaration.from_sequence([\n        table_source, Declaration(\"aggregate\", aggr_opts)\n    ])\n    result = decl.to_table()\n    expected_all = pa.table({\"b\": [\"foo\", \"bar\"], \"count(a)\": [2, 1]})\n    assert result.equals(expected_all)\n\n    # specify keys as field references\n    aggr_opts = AggregateNodeOptions(\n        [(\"a\", \"hash_count\", None, \"count(a)\")], keys=[field(\"b\")]\n    )\n    decl = Declaration.from_sequence([\n        table_source, Declaration(\"aggregate\", aggr_opts)\n    ])\n    result = decl.to_table()\n    assert result.equals(expected)\n\n    # wrong type of (aggregation) function\n    # TODO test with kernel that matches number of arguments (arity) -> avoid segfault\n    aggr_opts = AggregateNodeOptions([(\"a\", \"sum\", None, \"a_sum\")], keys=[\"b\"])\n    decl = Declaration.from_sequence([\n        table_source, Declaration(\"aggregate\", aggr_opts)\n    ])\n    with pytest.raises(ValueError):\n        _ = decl.to_table()\n\n\ndef test_order_by():\n    table = pa.table({'a': [1, 2, 3, 4], 'b': [1, 3, None, 2]})\n    table_source = Declaration(\"table_source\", TableSourceNodeOptions(table))\n\n    ord_opts = OrderByNodeOptions([(\"b\", \"ascending\")])\n    decl = Declaration.from_sequence([table_source, Declaration(\"order_by\", ord_opts)])\n    result = decl.to_table()\n    expected = pa.table({\"a\": [1, 4, 2, 3], \"b\": [1, 2, 3, None]})\n    assert result.equals(expected)\n\n    ord_opts = OrderByNodeOptions([(field(\"b\"), \"descending\")])\n    decl = Declaration.from_sequence([table_source, Declaration(\"order_by\", ord_opts)])\n    result = decl.to_table()\n    expected = pa.table({\"a\": [2, 4, 1, 3], \"b\": [3, 2, 1, None]})\n    assert result.equals(expected)\n\n    ord_opts = OrderByNodeOptions([(1, \"descending\")], null_placement=\"at_start\")\n    decl = Declaration.from_sequence([table_source, Declaration(\"order_by\", ord_opts)])\n    result = decl.to_table()\n    expected = pa.table({\"a\": [3, 2, 4, 1], \"b\": [None, 3, 2, 1]})\n    assert result.equals(expected)\n\n    # empty ordering\n    ord_opts = OrderByNodeOptions([])\n    decl = Declaration.from_sequence([table_source, Declaration(\"order_by\", ord_opts)])\n    with pytest.raises(\n        ValueError, match=\"`ordering` must be an explicit non-empty ordering\"\n    ):\n        _ = decl.to_table()\n\n    with pytest.raises(ValueError, match=\"\\\"decreasing\\\" is not a valid sort order\"):\n        _ = OrderByNodeOptions([(\"b\", \"decreasing\")])\n\n    with pytest.raises(ValueError, match=\"\\\"start\\\" is not a valid null placement\"):\n        _ = OrderByNodeOptions([(\"b\", \"ascending\")], null_placement=\"start\")\n\n\ndef test_hash_join():\n    left = pa.table({'key': [1, 2, 3], 'a': [4, 5, 6]})\n    left_source = Declaration(\"table_source\", options=TableSourceNodeOptions(left))\n    right = pa.table({'key': [2, 3, 4], 'b': [4, 5, 6]})\n    right_source = Declaration(\"table_source\", options=TableSourceNodeOptions(right))\n\n    # inner join\n    join_opts = HashJoinNodeOptions(\"inner\", left_keys=\"key\", right_keys=\"key\")\n    joined = Declaration(\n        \"hashjoin\", options=join_opts, inputs=[left_source, right_source])\n    result = joined.to_table()\n    expected = pa.table(\n        [[2, 3], [5, 6], [2, 3], [4, 5]],\n        names=[\"key\", \"a\", \"key\", \"b\"])\n    assert result.equals(expected)\n\n    for keys in [field(\"key\"), [\"key\"], [field(\"key\")]]:\n        join_opts = HashJoinNodeOptions(\"inner\", left_keys=keys, right_keys=keys)\n        joined = Declaration(\n            \"hashjoin\", options=join_opts, inputs=[left_source, right_source])\n        result = joined.to_table()\n        assert result.equals(expected)\n\n    # left join\n    join_opts = HashJoinNodeOptions(\n        \"left outer\", left_keys=\"key\", right_keys=\"key\")\n    joined = Declaration(\n        \"hashjoin\", options=join_opts, inputs=[left_source, right_source])\n    result = joined.to_table()\n    expected = pa.table(\n        [[1, 2, 3], [4, 5, 6], [None, 2, 3], [None, 4, 5]],\n        names=[\"key\", \"a\", \"key\", \"b\"]\n    )\n    assert result.sort_by(\"a\").equals(expected)\n\n    # suffixes\n    join_opts = HashJoinNodeOptions(\n        \"left outer\", left_keys=\"key\", right_keys=\"key\",\n        output_suffix_for_left=\"_left\", output_suffix_for_right=\"_right\")\n    joined = Declaration(\n        \"hashjoin\", options=join_opts, inputs=[left_source, right_source])\n    result = joined.to_table()\n    expected = pa.table(\n        [[1, 2, 3], [4, 5, 6], [None, 2, 3], [None, 4, 5]],\n        names=[\"key_left\", \"a\", \"key_right\", \"b\"]\n    )\n    assert result.sort_by(\"a\").equals(expected)\n\n    # manually specifying output columns\n    join_opts = HashJoinNodeOptions(\n        \"left outer\", left_keys=\"key\", right_keys=\"key\",\n        left_output=[\"key\", \"a\"], right_output=[field(\"b\")])\n    joined = Declaration(\n        \"hashjoin\", options=join_opts, inputs=[left_source, right_source])\n    result = joined.to_table()\n    expected = pa.table(\n        [[1, 2, 3], [4, 5, 6], [None, 4, 5]],\n        names=[\"key\", \"a\", \"b\"]\n    )\n    assert result.sort_by(\"a\").equals(expected)\n\n\ndef test_asof_join():\n    left = pa.table({'key': [1, 2, 3], 'ts': [1, 1, 1], 'a': [4, 5, 6]})\n    left_source = Declaration(\"table_source\", options=TableSourceNodeOptions(left))\n    right = pa.table({'key': [2, 3, 4], 'ts': [2, 5, 2], 'b': [4, 5, 6]})\n    right_source = Declaration(\"table_source\", options=TableSourceNodeOptions(right))\n\n    # asof join\n    join_opts = AsofJoinNodeOptions(\n        left_on=\"ts\", left_by=[\"key\"],\n        right_on=\"ts\", right_by=[\"key\"],\n        tolerance=1,\n    )\n    joined = Declaration(\n        \"asofjoin\", options=join_opts, inputs=[left_source, right_source]\n    )\n    result = joined.to_table()\n    expected = pa.table(\n        [[1, 2, 3], [1, 1, 1], [4, 5, 6], [None, 4, None]],\n        names=[\"key\", \"ts\", \"a\", \"b\"])\n    assert result == expected\n\n    for by in [field(\"key\"), [\"key\"], [field(\"key\")]]:\n        for on in [field(\"ts\"), \"ts\"]:\n            join_opts = AsofJoinNodeOptions(\n                left_on=on, left_by=by,\n                right_on=on, right_by=by,\n                tolerance=1,\n            )\n            joined = Declaration(\n                \"asofjoin\", options=join_opts, inputs=[left_source, right_source])\n            result = joined.to_table()\n            assert result == expected\n\n\n@pytest.mark.dataset\ndef test_scan(tempdir):\n    table = pa.table({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    ds.write_dataset(table, tempdir / \"dataset\", format=\"parquet\")\n    dataset = ds.dataset(tempdir / \"dataset\", format=\"parquet\")\n    decl = Declaration(\"scan\", ScanNodeOptions(dataset))\n    result = decl.to_table()\n    assert result.schema.names == [\n        \"a\", \"b\", \"__fragment_index\", \"__batch_index\",\n        \"__last_in_fragment\", \"__filename\"\n    ]\n    assert result.select([\"a\", \"b\"]).equals(table)\n\n    # using a filter only does pushdown (depending on file format), not actual filter\n\n    scan_opts = ScanNodeOptions(dataset, filter=field('a') > 1)\n    decl = Declaration(\"scan\", scan_opts)\n    # fragment not filtered based on min/max statistics\n    assert decl.to_table().num_rows == 3\n\n    scan_opts = ScanNodeOptions(dataset, filter=field('a') > 4)\n    decl = Declaration(\"scan\", scan_opts)\n    # full fragment filtered based on min/max statistics\n    assert decl.to_table().num_rows == 0\n\n    # projection scan option\n\n    scan_opts = ScanNodeOptions(dataset, columns={\"a2\": pc.multiply(field(\"a\"), 2)})\n    decl = Declaration(\"scan\", scan_opts)\n    result = decl.to_table()\n    # \"a\" is included in the result (needed later on for the actual projection)\n    assert result[\"a\"].to_pylist() == [1, 2, 3]\n    # \"b\" is still included, but without data as it will be removed by the projection\n    assert pc.all(result[\"b\"].is_null()).as_py()\n", "python/pyarrow/tests/test_table.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nimport sys\nimport weakref\n\nimport numpy as np\nimport pytest\nimport pyarrow as pa\nimport pyarrow.compute as pc\nfrom pyarrow.vendored.version import Version\n\n\ndef test_chunked_array_basics():\n    data = pa.chunked_array([], type=pa.string())\n    assert data.type == pa.string()\n    assert data.to_pylist() == []\n    data.validate()\n\n    data2 = pa.chunked_array([], type='binary')\n    assert data2.type == pa.binary()\n\n    with pytest.raises(ValueError):\n        pa.chunked_array([])\n\n    data = pa.chunked_array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ])\n    assert isinstance(data.chunks, list)\n    assert all(isinstance(c, pa.lib.Int64Array) for c in data.chunks)\n    assert all(isinstance(c, pa.lib.Int64Array) for c in data.iterchunks())\n    assert len(data.chunks) == 3\n    assert data.get_total_buffer_size() == sum(c.get_total_buffer_size()\n                                               for c in data.iterchunks())\n    assert sys.getsizeof(data) >= object.__sizeof__(\n        data) + data.get_total_buffer_size()\n    assert data.nbytes == 3 * 3 * 8  # 3 items per 3 lists with int64 size(8)\n    data.validate()\n\n    wr = weakref.ref(data)\n    assert wr() is not None\n    del data\n    assert wr() is None\n\n\ndef test_chunked_array_construction():\n    arr = pa.chunked_array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9],\n    ])\n    assert arr.type == pa.int64()\n    assert len(arr) == 9\n    assert len(arr.chunks) == 3\n\n    arr = pa.chunked_array([\n        [1, 2, 3],\n        [4., 5., 6.],\n        [7, 8, 9],\n    ])\n    assert arr.type == pa.int64()\n    assert len(arr) == 9\n    assert len(arr.chunks) == 3\n\n    arr = pa.chunked_array([\n        [1, 2, 3],\n        [4., 5., 6.],\n        [7, 8, 9],\n    ], type=pa.int8())\n    assert arr.type == pa.int8()\n    assert len(arr) == 9\n    assert len(arr.chunks) == 3\n\n    arr = pa.chunked_array([\n        [1, 2, 3],\n        []\n    ])\n    assert arr.type == pa.int64()\n    assert len(arr) == 3\n    assert len(arr.chunks) == 2\n\n    msg = \"cannot construct ChunkedArray from empty vector and omitted type\"\n    with pytest.raises(ValueError, match=msg):\n        assert pa.chunked_array([])\n\n    assert pa.chunked_array([], type=pa.string()).type == pa.string()\n    assert pa.chunked_array([[]]).type == pa.null()\n    assert pa.chunked_array([[]], type=pa.string()).type == pa.string()\n\n\ndef test_combine_chunks():\n    # ARROW-77363\n    arr = pa.array([1, 2])\n    chunked_arr = pa.chunked_array([arr, arr])\n    res = chunked_arr.combine_chunks()\n    expected = pa.array([1, 2, 1, 2])\n    assert res.equals(expected)\n\n\ndef test_chunked_array_can_combine_chunks_with_no_chunks():\n    # https://issues.apache.org/jira/browse/ARROW-17256\n    assert pa.chunked_array([], type=pa.bool_()).combine_chunks() == pa.array(\n        [], type=pa.bool_()\n    )\n    assert pa.chunked_array(\n        [pa.array([], type=pa.bool_())], type=pa.bool_()\n    ).combine_chunks() == pa.array([], type=pa.bool_())\n\n\ndef test_chunked_array_to_numpy():\n    data = pa.chunked_array([\n        [1, 2, 3],\n        [4, 5, 6],\n        []\n    ])\n    arr1 = np.asarray(data)\n    arr2 = data.to_numpy()\n\n    assert isinstance(arr2, np.ndarray)\n    assert arr2.shape == (6,)\n    assert np.array_equal(arr1, arr2)\n\n\ndef test_chunked_array_mismatch_types():\n    msg = \"chunks must all be same type\"\n    with pytest.raises(TypeError, match=msg):\n        # Given array types are different\n        pa.chunked_array([\n            pa.array([1, 2, 3]),\n            pa.array([1., 2., 3.])\n        ])\n\n    with pytest.raises(TypeError, match=msg):\n        # Given array type is different from explicit type argument\n        pa.chunked_array([pa.array([1, 2, 3])], type=pa.float64())\n\n\ndef test_chunked_array_str():\n    data = [\n        pa.array([1, 2, 3]),\n        pa.array([4, 5, 6])\n    ]\n    data = pa.chunked_array(data)\n    assert str(data) == \"\"\"[\n  [\n    1,\n    2,\n    3\n  ],\n  [\n    4,\n    5,\n    6\n  ]\n]\"\"\"\n\n\ndef test_chunked_array_getitem():\n    data = [\n        pa.array([1, 2, 3]),\n        pa.array([4, 5, 6])\n    ]\n    data = pa.chunked_array(data)\n    assert data[1].as_py() == 2\n    assert data[-1].as_py() == 6\n    assert data[-6].as_py() == 1\n    with pytest.raises(IndexError):\n        data[6]\n    with pytest.raises(IndexError):\n        data[-7]\n    # Ensure this works with numpy scalars\n    assert data[np.int32(1)].as_py() == 2\n\n    data_slice = data[2:4]\n    assert data_slice.to_pylist() == [3, 4]\n\n    data_slice = data[4:-1]\n    assert data_slice.to_pylist() == [5]\n\n    data_slice = data[99:99]\n    assert data_slice.type == data.type\n    assert data_slice.to_pylist() == []\n\n\ndef test_chunked_array_slice():\n    data = [\n        pa.array([1, 2, 3]),\n        pa.array([4, 5, 6])\n    ]\n    data = pa.chunked_array(data)\n\n    data_slice = data.slice(len(data))\n    assert data_slice.type == data.type\n    assert data_slice.to_pylist() == []\n\n    data_slice = data.slice(len(data) + 10)\n    assert data_slice.type == data.type\n    assert data_slice.to_pylist() == []\n\n    table = pa.Table.from_arrays([data], names=[\"a\"])\n    table_slice = table.slice(len(table))\n    assert len(table_slice) == 0\n\n    table = pa.Table.from_arrays([data], names=[\"a\"])\n    table_slice = table.slice(len(table) + 10)\n    assert len(table_slice) == 0\n\n\ndef test_chunked_array_iter():\n    data = [\n        pa.array([0]),\n        pa.array([1, 2, 3]),\n        pa.array([4, 5, 6]),\n        pa.array([7, 8, 9])\n    ]\n    arr = pa.chunked_array(data)\n\n    for i, j in zip(range(10), arr):\n        assert i == j.as_py()\n\n    assert isinstance(arr, Iterable)\n\n\ndef test_chunked_array_equals():\n    def eq(xarrs, yarrs):\n        if isinstance(xarrs, pa.ChunkedArray):\n            x = xarrs\n        else:\n            x = pa.chunked_array(xarrs)\n        if isinstance(yarrs, pa.ChunkedArray):\n            y = yarrs\n        else:\n            y = pa.chunked_array(yarrs)\n        assert x.equals(y)\n        assert y.equals(x)\n        assert x == y\n        assert x != str(y)\n\n    def ne(xarrs, yarrs):\n        if isinstance(xarrs, pa.ChunkedArray):\n            x = xarrs\n        else:\n            x = pa.chunked_array(xarrs)\n        if isinstance(yarrs, pa.ChunkedArray):\n            y = yarrs\n        else:\n            y = pa.chunked_array(yarrs)\n        assert not x.equals(y)\n        assert not y.equals(x)\n        assert x != y\n\n    eq(pa.chunked_array([], type=pa.int32()),\n       pa.chunked_array([], type=pa.int32()))\n    ne(pa.chunked_array([], type=pa.int32()),\n       pa.chunked_array([], type=pa.int64()))\n\n    a = pa.array([0, 2], type=pa.int32())\n    b = pa.array([0, 2], type=pa.int64())\n    c = pa.array([0, 3], type=pa.int32())\n    d = pa.array([0, 2, 0, 3], type=pa.int32())\n\n    eq([a], [a])\n    ne([a], [b])\n    eq([a, c], [a, c])\n    eq([a, c], [d])\n    ne([c, a], [a, c])\n\n    # ARROW-4822\n    assert not pa.chunked_array([], type=pa.int32()).equals(None)\n\n\n@pytest.mark.parametrize(\n    ('data', 'typ'),\n    [\n        ([True, False, True, True], pa.bool_()),\n        ([1, 2, 4, 6], pa.int64()),\n        ([1.0, 2.5, None], pa.float64()),\n        (['a', None, 'b'], pa.string()),\n        ([], pa.list_(pa.uint8())),\n        ([[1, 2], [3]], pa.list_(pa.int64())),\n        ([['a'], None, ['b', 'c']], pa.list_(pa.string())),\n        ([(1, 'a'), (2, 'c'), None],\n            pa.struct([pa.field('a', pa.int64()), pa.field('b', pa.string())]))\n    ]\n)\ndef test_chunked_array_pickle(data, typ, pickle_module):\n    arrays = []\n    while data:\n        arrays.append(pa.array(data[:2], type=typ))\n        data = data[2:]\n    array = pa.chunked_array(arrays, type=typ)\n    array.validate()\n    result = pickle_module.loads(pickle_module.dumps(array))\n    result.validate()\n    assert result.equals(array)\n\n\n@pytest.mark.pandas\ndef test_chunked_array_to_pandas():\n    import pandas as pd\n\n    data = [\n        pa.array([-10, -5, 0, 5, 10])\n    ]\n    table = pa.table(data, names=['a'])\n    col = table.column(0)\n    assert isinstance(col, pa.ChunkedArray)\n    series = col.to_pandas()\n    assert isinstance(series, pd.Series)\n    assert series.shape == (5,)\n    assert series[0] == -10\n    assert series.name == 'a'\n\n\n@pytest.mark.pandas\ndef test_chunked_array_to_pandas_preserve_name():\n    # https://issues.apache.org/jira/browse/ARROW-7709\n    import pandas as pd\n    import pandas.testing as tm\n\n    for data in [\n            pa.array([1, 2, 3]),\n            pa.array(pd.Categorical([\"a\", \"b\", \"a\"])),\n            pa.array(pd.date_range(\"2012\", periods=3)),\n            pa.array(pd.date_range(\"2012\", periods=3, tz=\"Europe/Brussels\")),\n            pa.array([1, 2, 3], pa.timestamp(\"ms\")),\n            pa.array([1, 2, 3], pa.timestamp(\"ms\", \"Europe/Brussels\"))]:\n        table = pa.table({\"name\": data})\n        result = table.column(\"name\").to_pandas()\n        assert result.name == \"name\"\n        expected = pd.Series(data.to_pandas(), name=\"name\")\n        tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_table_roundtrip_to_pandas_empty_dataframe():\n    # https://issues.apache.org/jira/browse/ARROW-10643\n    # The conversion should not results in a table with 0 rows if the original\n    # DataFrame has a RangeIndex but is empty.\n    import pandas as pd\n\n    data = pd.DataFrame(index=pd.RangeIndex(0, 10, 1))\n    table = pa.table(data)\n    result = table.to_pandas()\n\n    assert table.num_rows == 10\n    assert data.shape == (10, 0)\n    assert result.shape == (10, 0)\n    assert result.index.equals(data.index)\n\n    data = pd.DataFrame(index=pd.RangeIndex(0, 10, 3))\n    table = pa.table(data)\n    result = table.to_pandas()\n\n    assert table.num_rows == 4\n    assert data.shape == (4, 0)\n    assert result.shape == (4, 0)\n    assert result.index.equals(data.index)\n\n\n@pytest.mark.pandas\ndef test_recordbatch_roundtrip_to_pandas_empty_dataframe():\n    # https://issues.apache.org/jira/browse/ARROW-10643\n    # The conversion should not results in a RecordBatch with 0 rows if\n    #  the original DataFrame has a RangeIndex but is empty.\n    import pandas as pd\n\n    data = pd.DataFrame(index=pd.RangeIndex(0, 10, 1))\n    batch = pa.RecordBatch.from_pandas(data)\n    result = batch.to_pandas()\n\n    assert batch.num_rows == 10\n    assert data.shape == (10, 0)\n    assert result.shape == (10, 0)\n    assert result.index.equals(data.index)\n\n    data = pd.DataFrame(index=pd.RangeIndex(0, 10, 3))\n    batch = pa.RecordBatch.from_pandas(data)\n    result = batch.to_pandas()\n\n    assert batch.num_rows == 4\n    assert data.shape == (4, 0)\n    assert result.shape == (4, 0)\n    assert result.index.equals(data.index)\n\n\n@pytest.mark.pandas\ndef test_to_pandas_empty_table():\n    # https://issues.apache.org/jira/browse/ARROW-15370\n    import pandas as pd\n    import pandas.testing as tm\n\n    df = pd.DataFrame({'a': [1, 2], 'b': [0.1, 0.2]})\n    table = pa.table(df)\n    result = table.schema.empty_table().to_pandas()\n    assert result.shape == (0, 2)\n    tm.assert_frame_equal(result, df.iloc[:0])\n\n\n@pytest.mark.pandas\n@pytest.mark.nopandas\ndef test_chunked_array_asarray():\n    # ensure this is tested both when pandas is present or not (ARROW-6564)\n\n    data = [\n        pa.array([0]),\n        pa.array([1, 2, 3])\n    ]\n    chunked_arr = pa.chunked_array(data)\n\n    np_arr = np.asarray(chunked_arr)\n    assert np_arr.tolist() == [0, 1, 2, 3]\n    assert np_arr.dtype == np.dtype('int64')\n\n    # An optional type can be specified when calling np.asarray\n    np_arr = np.asarray(chunked_arr, dtype='str')\n    assert np_arr.tolist() == ['0', '1', '2', '3']\n\n    # Types are modified when there are nulls\n    data = [\n        pa.array([1, None]),\n        pa.array([1, 2, 3])\n    ]\n    chunked_arr = pa.chunked_array(data)\n\n    np_arr = np.asarray(chunked_arr)\n    elements = np_arr.tolist()\n    assert elements[0] == 1.\n    assert np.isnan(elements[1])\n    assert elements[2:] == [1., 2., 3.]\n    assert np_arr.dtype == np.dtype('float64')\n\n    # DictionaryType data will be converted to dense numpy array\n    arr = pa.DictionaryArray.from_arrays(\n        pa.array([0, 1, 2, 0, 1]), pa.array(['a', 'b', 'c']))\n    chunked_arr = pa.chunked_array([arr, arr])\n    np_arr = np.asarray(chunked_arr)\n    assert np_arr.dtype == np.dtype('object')\n    assert np_arr.tolist() == ['a', 'b', 'c', 'a', 'b'] * 2\n\n\ndef test_chunked_array_flatten():\n    ty = pa.struct([pa.field('x', pa.int16()),\n                    pa.field('y', pa.float32())])\n    a = pa.array([(1, 2.5), (3, 4.5), (5, 6.5)], type=ty)\n    carr = pa.chunked_array(a)\n    x, y = carr.flatten()\n    assert x.equals(pa.chunked_array(pa.array([1, 3, 5], type=pa.int16())))\n    assert y.equals(pa.chunked_array(pa.array([2.5, 4.5, 6.5],\n                                              type=pa.float32())))\n\n    # Empty column\n    a = pa.array([], type=ty)\n    carr = pa.chunked_array(a)\n    x, y = carr.flatten()\n    assert x.equals(pa.chunked_array(pa.array([], type=pa.int16())))\n    assert y.equals(pa.chunked_array(pa.array([], type=pa.float32())))\n\n\ndef test_chunked_array_unify_dictionaries():\n    arr = pa.chunked_array([\n        pa.array([\"foo\", \"bar\", None, \"foo\"]).dictionary_encode(),\n        pa.array([\"quux\", None, \"foo\"]).dictionary_encode(),\n    ])\n    assert arr.chunk(0).dictionary.equals(pa.array([\"foo\", \"bar\"]))\n    assert arr.chunk(1).dictionary.equals(pa.array([\"quux\", \"foo\"]))\n    arr = arr.unify_dictionaries()\n    expected_dict = pa.array([\"foo\", \"bar\", \"quux\"])\n    assert arr.chunk(0).dictionary.equals(expected_dict)\n    assert arr.chunk(1).dictionary.equals(expected_dict)\n    assert arr.to_pylist() == [\"foo\", \"bar\", None, \"foo\", \"quux\", None, \"foo\"]\n\n\ndef test_recordbatch_dunder_init():\n    with pytest.raises(TypeError, match='RecordBatch'):\n        pa.RecordBatch()\n\n\ndef test_chunked_array_c_array_interface():\n    class ArrayWrapper:\n        def __init__(self, array):\n            self.array = array\n\n        def __arrow_c_array__(self, requested_schema=None):\n            return self.array.__arrow_c_array__(requested_schema)\n\n    data = pa.array([1, 2, 3], pa.int64())\n    chunked = pa.chunked_array([data])\n    wrapper = ArrayWrapper(data)\n\n    # Can roundtrip through the wrapper.\n    result = pa.chunked_array(wrapper)\n    assert result == chunked\n\n    # Can also import with a type that implementer can cast to.\n    result = pa.chunked_array(wrapper, type=pa.int16())\n    assert result == chunked.cast(pa.int16())\n\n\ndef test_chunked_array_c_stream_interface():\n    class ChunkedArrayWrapper:\n        def __init__(self, chunked):\n            self.chunked = chunked\n\n        def __arrow_c_stream__(self, requested_schema=None):\n            return self.chunked.__arrow_c_stream__(requested_schema)\n\n    data = pa.chunked_array([[1, 2, 3], [4, None, 6]])\n    wrapper = ChunkedArrayWrapper(data)\n\n    # Can roundtrip through the wrapper.\n    result = pa.chunked_array(wrapper)\n    assert result == data\n\n    # Can also import with a type that implementer can cast to.\n    result = pa.chunked_array(wrapper, type=pa.int16())\n    assert result == data.cast(pa.int16())\n\n\ndef test_recordbatch_c_array_interface():\n    class BatchWrapper:\n        def __init__(self, batch):\n            self.batch = batch\n\n        def __arrow_c_array__(self, requested_schema=None):\n            return self.batch.__arrow_c_array__(requested_schema)\n\n    data = pa.record_batch([\n        pa.array([1, 2, 3], type=pa.int64())\n    ], names=['a'])\n    wrapper = BatchWrapper(data)\n\n    # Can roundtrip through the wrapper.\n    result = pa.record_batch(wrapper)\n    assert result == data\n\n    # Can also import with a schema that implementer can cast to.\n    castable_schema = pa.schema([\n        pa.field('a', pa.int32())\n    ])\n    result = pa.record_batch(wrapper, schema=castable_schema)\n    expected = pa.record_batch([\n        pa.array([1, 2, 3], type=pa.int32())\n    ], names=['a'])\n    assert result == expected\n\n\ndef test_table_c_array_interface():\n    class BatchWrapper:\n        def __init__(self, batch):\n            self.batch = batch\n\n        def __arrow_c_array__(self, requested_schema=None):\n            return self.batch.__arrow_c_array__(requested_schema)\n\n    data = pa.record_batch([\n        pa.array([1, 2, 3], type=pa.int64())\n    ], names=['a'])\n    wrapper = BatchWrapper(data)\n\n    # Can roundtrip through the wrapper.\n    result = pa.table(wrapper)\n    expected = pa.Table.from_batches([data])\n    assert result == expected\n\n    # Can also import with a schema that implementer can cast to.\n    castable_schema = pa.schema([\n        pa.field('a', pa.int32())\n    ])\n    result = pa.table(wrapper, schema=castable_schema)\n    expected = pa.table({\n        'a': pa.array([1, 2, 3], type=pa.int32())\n    })\n    assert result == expected\n\n\ndef test_table_c_stream_interface():\n    class StreamWrapper:\n        def __init__(self, batches):\n            self.batches = batches\n\n        def __arrow_c_stream__(self, requested_schema=None):\n            reader = pa.RecordBatchReader.from_batches(\n                self.batches[0].schema, self.batches)\n            return reader.__arrow_c_stream__(requested_schema)\n\n    data = [\n        pa.record_batch([pa.array([1, 2, 3], type=pa.int64())], names=['a']),\n        pa.record_batch([pa.array([4, 5, 6], type=pa.int64())], names=['a'])\n    ]\n    wrapper = StreamWrapper(data)\n\n    # Can roundtrip through the wrapper.\n    result = pa.table(wrapper)\n    expected = pa.Table.from_batches(data)\n    assert result == expected\n\n    # Passing schema works if already that schema\n    result = pa.table(wrapper, schema=data[0].schema)\n    assert result == expected\n\n    # Passing a different schema will cast\n    good_schema = pa.schema([pa.field('a', pa.int32())])\n    result = pa.table(wrapper, schema=good_schema)\n    assert result == expected.cast(good_schema)\n\n    # If schema doesn't match, raises NotImplementedError\n    with pytest.raises(\n        pa.lib.ArrowTypeError, match=\"Field 0 cannot be cast\"\n    ):\n        pa.table(\n            wrapper, schema=pa.schema([pa.field('a', pa.list_(pa.int32()))])\n        )\n\n\ndef test_recordbatch_itercolumns():\n    data = [\n        pa.array(range(5), type='int16'),\n        pa.array([-10, -5, 0, None, 10], type='int32')\n    ]\n    batch = pa.record_batch(data, ['c0', 'c1'])\n\n    columns = []\n    for col in batch.itercolumns():\n        columns.append(col)\n\n    assert batch.columns == columns\n    assert batch == pa.record_batch(columns, names=batch.column_names)\n    assert batch != pa.record_batch(columns[1:], names=batch.column_names[1:])\n    assert batch != columns\n\n\ndef test_recordbatch_equals():\n    data1 = [\n        pa.array(range(5), type='int16'),\n        pa.array([-10, -5, 0, None, 10], type='int32')\n    ]\n    data2 = [\n        pa.array(['a', 'b', 'c']),\n        pa.array([['d'], ['e'], ['f']]),\n    ]\n    column_names = ['c0', 'c1']\n\n    batch = pa.record_batch(data1, column_names)\n    assert batch == pa.record_batch(data1, column_names)\n    assert batch.equals(pa.record_batch(data1, column_names))\n\n    assert batch != pa.record_batch(data2, column_names)\n    assert not batch.equals(pa.record_batch(data2, column_names))\n\n    batch_meta = pa.record_batch(data1, names=column_names,\n                                 metadata={'key': 'value'})\n    assert batch_meta.equals(batch)\n    assert not batch_meta.equals(batch, check_metadata=True)\n\n    # ARROW-8889\n    assert not batch.equals(None)\n    assert batch != \"foo\"\n\n\ndef test_recordbatch_take():\n    batch = pa.record_batch(\n        [pa.array([1, 2, 3, None, 5]),\n         pa.array(['a', 'b', 'c', 'd', 'e'])],\n        ['f1', 'f2'])\n    assert batch.take(pa.array([2, 3])).equals(batch.slice(2, 2))\n    assert batch.take(pa.array([2, None])).equals(\n        pa.record_batch([pa.array([3, None]), pa.array(['c', None])],\n                        ['f1', 'f2']))\n\n\ndef test_recordbatch_column_sets_private_name():\n    # ARROW-6429\n    rb = pa.record_batch([pa.array([1, 2, 3, 4])], names=['a0'])\n    assert rb[0]._name == 'a0'\n\n\ndef test_recordbatch_from_arrays_validate_schema():\n    # ARROW-6263\n    arr = pa.array([1, 2])\n    schema = pa.schema([pa.field('f0', pa.list_(pa.utf8()))])\n    with pytest.raises(NotImplementedError):\n        pa.record_batch([arr], schema=schema)\n\n\ndef test_recordbatch_from_arrays_validate_lengths():\n    # ARROW-2820\n    data = [pa.array([1]), pa.array([\"tokyo\", \"like\", \"happy\"]),\n            pa.array([\"derek\"])]\n\n    with pytest.raises(ValueError):\n        pa.record_batch(data, ['id', 'tags', 'name'])\n\n\ndef test_recordbatch_no_fields():\n    batch = pa.record_batch([], [])\n\n    assert len(batch) == 0\n    assert batch.num_rows == 0\n    assert batch.num_columns == 0\n\n\ndef test_recordbatch_from_arrays_invalid_names():\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10])\n    ]\n    with pytest.raises(ValueError):\n        pa.record_batch(data, names=['a', 'b', 'c'])\n\n    with pytest.raises(ValueError):\n        pa.record_batch(data, names=['a'])\n\n\ndef test_recordbatch_empty_metadata():\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10])\n    ]\n\n    batch = pa.record_batch(data, ['c0', 'c1'])\n    assert batch.schema.metadata is None\n\n\ndef test_recordbatch_pickle(pickle_module):\n    data = [\n        pa.array(range(5), type='int8'),\n        pa.array([-10, -5, 0, 5, 10], type='float32')\n    ]\n    fields = [\n        pa.field('ints', pa.int8()),\n        pa.field('floats', pa.float32()),\n    ]\n    schema = pa.schema(fields, metadata={b'foo': b'bar'})\n    batch = pa.record_batch(data, schema=schema)\n\n    result = pickle_module.loads(pickle_module.dumps(batch))\n    assert result.equals(batch)\n    assert result.schema == schema\n\n\ndef test_recordbatch_get_field():\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n        pa.array(range(5, 10))\n    ]\n    batch = pa.RecordBatch.from_arrays(data, names=('a', 'b', 'c'))\n\n    assert batch.field('a').equals(batch.schema.field('a'))\n    assert batch.field(0).equals(batch.schema.field('a'))\n\n    with pytest.raises(KeyError):\n        batch.field('d')\n\n    with pytest.raises(TypeError):\n        batch.field(None)\n\n    with pytest.raises(IndexError):\n        batch.field(4)\n\n\ndef test_recordbatch_select_column():\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n        pa.array(range(5, 10))\n    ]\n    batch = pa.RecordBatch.from_arrays(data, names=('a', 'b', 'c'))\n\n    assert batch.column('a').equals(batch.column(0))\n\n    with pytest.raises(\n            KeyError, match='Field \"d\" does not exist in schema'):\n        batch.column('d')\n\n    with pytest.raises(TypeError):\n        batch.column(None)\n\n    with pytest.raises(IndexError):\n        batch.column(4)\n\n\ndef test_recordbatch_select():\n    a1 = pa.array([1, 2, 3, None, 5])\n    a2 = pa.array(['a', 'b', 'c', 'd', 'e'])\n    a3 = pa.array([[1, 2], [3, 4], [5, 6], None, [9, 10]])\n    batch = pa.record_batch([a1, a2, a3], ['f1', 'f2', 'f3'])\n\n    # selecting with string names\n    result = batch.select(['f1'])\n    expected = pa.record_batch([a1], ['f1'])\n    assert result.equals(expected)\n\n    result = batch.select(['f3', 'f2'])\n    expected = pa.record_batch([a3, a2], ['f3', 'f2'])\n    assert result.equals(expected)\n\n    # selecting with integer indices\n    result = batch.select([0])\n    expected = pa.record_batch([a1], ['f1'])\n    assert result.equals(expected)\n\n    result = batch.select([2, 1])\n    expected = pa.record_batch([a3, a2], ['f3', 'f2'])\n    assert result.equals(expected)\n\n    # preserve metadata\n    batch2 = batch.replace_schema_metadata({\"a\": \"test\"})\n    result = batch2.select([\"f1\", \"f2\"])\n    assert b\"a\" in result.schema.metadata\n\n    # selecting non-existing column raises\n    with pytest.raises(KeyError, match='Field \"f5\" does not exist'):\n        batch.select(['f5'])\n\n    with pytest.raises(IndexError, match=\"index out of bounds\"):\n        batch.select([5])\n\n    # duplicate selection gives duplicated names in resulting recordbatch\n    result = batch.select(['f2', 'f2'])\n    expected = pa.record_batch([a2, a2], ['f2', 'f2'])\n    assert result.equals(expected)\n\n    # selection duplicated column raises\n    batch = pa.record_batch([a1, a2, a3], ['f1', 'f2', 'f1'])\n    with pytest.raises(KeyError, match='Field \"f1\" exists 2 times'):\n        batch.select(['f1'])\n\n    result = batch.select(['f2'])\n    expected = pa.record_batch([a2], ['f2'])\n    assert result.equals(expected)\n\n\ndef test_recordbatch_from_struct_array_invalid():\n    with pytest.raises(TypeError):\n        pa.RecordBatch.from_struct_array(pa.array(range(5)))\n\n\ndef test_recordbatch_from_struct_array():\n    struct_array = pa.array(\n        [{\"ints\": 1}, {\"floats\": 1.0}],\n        type=pa.struct([(\"ints\", pa.int32()), (\"floats\", pa.float32())]),\n    )\n    result = pa.RecordBatch.from_struct_array(struct_array)\n    assert result.equals(pa.RecordBatch.from_arrays(\n        [\n            pa.array([1, None], type=pa.int32()),\n            pa.array([None, 1.0], type=pa.float32()),\n        ], [\"ints\", \"floats\"]\n    ))\n\n\ndef test_recordbatch_to_struct_array():\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array([1, None], type=pa.int32()),\n            pa.array([None, 1.0], type=pa.float32()),\n        ], [\"ints\", \"floats\"]\n    )\n    result = batch.to_struct_array()\n    assert result.equals(pa.array(\n        [{\"ints\": 1}, {\"floats\": 1.0}],\n        type=pa.struct([(\"ints\", pa.int32()), (\"floats\", pa.float32())]),\n    ))\n\n\ndef test_table_from_struct_array_invalid():\n    with pytest.raises(TypeError, match=\"Argument 'struct_array' has incorrect type\"):\n        pa.Table.from_struct_array(pa.array(range(5)))\n\n\ndef test_table_from_struct_array():\n    struct_array = pa.array(\n        [{\"ints\": 1}, {\"floats\": 1.0}],\n        type=pa.struct([(\"ints\", pa.int32()), (\"floats\", pa.float32())]),\n    )\n    result = pa.Table.from_struct_array(struct_array)\n    assert result.equals(pa.Table.from_arrays(\n        [\n            pa.array([1, None], type=pa.int32()),\n            pa.array([None, 1.0], type=pa.float32()),\n        ], [\"ints\", \"floats\"]\n    ))\n\n\ndef test_table_from_struct_array_chunked_array():\n    chunked_struct_array = pa.chunked_array(\n        [[{\"ints\": 1}, {\"floats\": 1.0}]],\n        type=pa.struct([(\"ints\", pa.int32()), (\"floats\", pa.float32())]),\n    )\n    result = pa.Table.from_struct_array(chunked_struct_array)\n    assert result.equals(pa.Table.from_arrays(\n        [\n            pa.array([1, None], type=pa.int32()),\n            pa.array([None, 1.0], type=pa.float32()),\n        ], [\"ints\", \"floats\"]\n    ))\n\n\ndef test_table_to_struct_array():\n    table = pa.Table.from_arrays(\n        [\n            pa.array([1, None], type=pa.int32()),\n            pa.array([None, 1.0], type=pa.float32()),\n        ], [\"ints\", \"floats\"]\n    )\n    result = table.to_struct_array()\n    assert result.equals(pa.chunked_array(\n        [[{\"ints\": 1}, {\"floats\": 1.0}]],\n        type=pa.struct([(\"ints\", pa.int32()), (\"floats\", pa.float32())]),\n    ))\n\n\ndef test_table_to_struct_array_with_max_chunksize():\n    table = pa.Table.from_arrays(\n        [\n            pa.array([1, None], type=pa.int32()),\n            pa.array([None, 1.0], type=pa.float32()),\n        ], [\"ints\", \"floats\"]\n    )\n    result = table.to_struct_array(max_chunksize=1)\n    assert result.equals(pa.chunked_array(\n        [[{\"ints\": 1}], [{\"floats\": 1.0}]],\n        type=pa.struct([(\"ints\", pa.int32()), (\"floats\", pa.float32())]),\n    ))\n\n\ndef check_tensors(tensor, expected_tensor, type, size):\n    assert tensor.equals(expected_tensor)\n    assert tensor.size == size\n    assert tensor.type == type\n    assert tensor.shape == expected_tensor.shape\n    assert tensor.strides == expected_tensor.strides\n\n\n@pytest.mark.parametrize('typ', [\n    np.uint8, np.uint16, np.uint32, np.uint64,\n    np.int8, np.int16, np.int32, np.int64,\n    np.float32, np.float64,\n])\ndef test_recordbatch_to_tensor_uniform_type(typ):\n    arr1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    arr2 = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n    arr3 = [100, 100, 100, 100, 100, 100, 100, 100, 100]\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array(arr1, type=pa.from_numpy_dtype(typ)),\n            pa.array(arr2, type=pa.from_numpy_dtype(typ)),\n            pa.array(arr3, type=pa.from_numpy_dtype(typ)),\n        ], [\"a\", \"b\", \"c\"]\n    )\n\n    result = batch.to_tensor(row_major=False)\n    x = np.column_stack([arr1, arr2, arr3]).astype(typ, order=\"F\")\n    expected = pa.Tensor.from_numpy(x)\n    check_tensors(result, expected, pa.from_numpy_dtype(typ), 27)\n\n    result = batch.to_tensor()\n    x = np.column_stack([arr1, arr2, arr3]).astype(typ, order=\"C\")\n    expected = pa.Tensor.from_numpy(x)\n    check_tensors(result, expected, pa.from_numpy_dtype(typ), 27)\n\n    # Test offset\n    batch1 = batch.slice(1)\n    arr1 = [2, 3, 4, 5, 6, 7, 8, 9]\n    arr2 = [20, 30, 40, 50, 60, 70, 80, 90]\n    arr3 = [100, 100, 100, 100, 100, 100, 100, 100]\n\n    result = batch1.to_tensor(row_major=False)\n    x = np.column_stack([arr1, arr2, arr3]).astype(typ, order=\"F\")\n    expected = pa.Tensor.from_numpy(x)\n    check_tensors(result, expected, pa.from_numpy_dtype(typ), 24)\n\n    result = batch1.to_tensor()\n    x = np.column_stack([arr1, arr2, arr3]).astype(typ, order=\"C\")\n    expected = pa.Tensor.from_numpy(x)\n    check_tensors(result, expected, pa.from_numpy_dtype(typ), 24)\n\n    batch2 = batch.slice(1, 5)\n    arr1 = [2, 3, 4, 5, 6]\n    arr2 = [20, 30, 40, 50, 60]\n    arr3 = [100, 100, 100, 100, 100]\n\n    result = batch2.to_tensor(row_major=False)\n    x = np.column_stack([arr1, arr2, arr3]).astype(typ, order=\"F\")\n    expected = pa.Tensor.from_numpy(x)\n    check_tensors(result, expected, pa.from_numpy_dtype(typ), 15)\n\n    result = batch2.to_tensor()\n    x = np.column_stack([arr1, arr2, arr3]).astype(typ, order=\"C\")\n    expected = pa.Tensor.from_numpy(x)\n    check_tensors(result, expected, pa.from_numpy_dtype(typ), 15)\n\n\ndef test_recordbatch_to_tensor_uniform_float_16():\n    arr1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    arr2 = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n    arr3 = [100, 100, 100, 100, 100, 100, 100, 100, 100]\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array(np.array(arr1, dtype=np.float16), type=pa.float16()),\n            pa.array(np.array(arr2, dtype=np.float16), type=pa.float16()),\n            pa.array(np.array(arr3, dtype=np.float16), type=pa.float16()),\n        ], [\"a\", \"b\", \"c\"]\n    )\n\n    result = batch.to_tensor(row_major=False)\n    x = np.column_stack([arr1, arr2, arr3]).astype(np.float16, order=\"F\")\n    expected = pa.Tensor.from_numpy(x)\n    check_tensors(result, expected, pa.float16(), 27)\n\n    result = batch.to_tensor()\n    x = np.column_stack([arr1, arr2, arr3]).astype(np.float16, order=\"C\")\n    expected = pa.Tensor.from_numpy(x)\n    check_tensors(result, expected, pa.float16(), 27)\n\n\ndef test_recordbatch_to_tensor_mixed_type():\n    # uint16 + int16 = int32\n    arr1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    arr2 = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n    arr3 = [100, 200, 300, np.nan, 500, 600, 700, 800, 900]\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array(arr1, type=pa.uint16()),\n            pa.array(arr2, type=pa.int16()),\n        ], [\"a\", \"b\"]\n    )\n\n    result = batch.to_tensor(row_major=False)\n    x = np.column_stack([arr1, arr2]).astype(np.int32, order=\"F\")\n    expected = pa.Tensor.from_numpy(x)\n    check_tensors(result, expected, pa.int32(), 18)\n\n    result = batch.to_tensor()\n    x = np.column_stack([arr1, arr2]).astype(np.int32, order=\"C\")\n    expected = pa.Tensor.from_numpy(x)\n    check_tensors(result, expected, pa.int32(), 18)\n\n    # uint16 + int16 + float32 = float64\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array(arr1, type=pa.uint16()),\n            pa.array(arr2, type=pa.int16()),\n            pa.array(arr3, type=pa.float32()),\n        ], [\"a\", \"b\", \"c\"]\n    )\n    result = batch.to_tensor(row_major=False)\n    x = np.column_stack([arr1, arr2, arr3]).astype(np.float64, order=\"F\")\n    expected = pa.Tensor.from_numpy(x)\n\n    np.testing.assert_equal(result.to_numpy(), x)\n    assert result.size == 27\n    assert result.type == pa.float64()\n    assert result.shape == expected.shape\n    assert result.strides == expected.strides\n\n    result = batch.to_tensor()\n    x = np.column_stack([arr1, arr2, arr3]).astype(np.float64, order=\"C\")\n    expected = pa.Tensor.from_numpy(x)\n\n    np.testing.assert_equal(result.to_numpy(), x)\n    assert result.size == 27\n    assert result.type == pa.float64()\n    assert result.shape == expected.shape\n    assert result.strides == expected.strides\n\n\ndef test_recordbatch_to_tensor_unsupported_mixed_type_with_float16():\n    arr1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    arr2 = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n    arr3 = [100, 200, 300, 400, 500, 600, 700, 800, 900]\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array(arr1, type=pa.uint16()),\n            pa.array(np.array(arr2, dtype=np.float16), type=pa.float16()),\n            pa.array(arr3, type=pa.float32()),\n        ], [\"a\", \"b\", \"c\"]\n    )\n\n    with pytest.raises(\n        NotImplementedError,\n        match=\"Casting from or to halffloat is not supported.\"\n    ):\n        batch.to_tensor()\n\n\ndef test_recordbatch_to_tensor_nan():\n    arr1 = [1, 2, 3, 4, np.nan, 6, 7, 8, 9]\n    arr2 = [10, 20, 30, 40, 50, 60, 70, np.nan, 90]\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array(arr1, type=pa.float32()),\n            pa.array(arr2, type=pa.float32()),\n        ], [\"a\", \"b\"]\n    )\n    result = batch.to_tensor(row_major=False)\n    x = np.column_stack([arr1, arr2]).astype(np.float32, order=\"F\")\n    expected = pa.Tensor.from_numpy(x)\n\n    np.testing.assert_equal(result.to_numpy(), x)\n    assert result.size == 18\n    assert result.type == pa.float32()\n    assert result.shape == expected.shape\n    assert result.strides == expected.strides\n\n\ndef test_recordbatch_to_tensor_null():\n    arr1 = [1, 2, 3, 4, None, 6, 7, 8, 9]\n    arr2 = [10, 20, 30, 40, 50, 60, 70, None, 90]\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array(arr1, type=pa.int32()),\n            pa.array(arr2, type=pa.float32()),\n        ], [\"a\", \"b\"]\n    )\n    with pytest.raises(\n        pa.ArrowTypeError,\n        match=\"Can only convert a RecordBatch with no nulls.\"\n    ):\n        batch.to_tensor()\n\n    result = batch.to_tensor(null_to_nan=True, row_major=False)\n    x = np.column_stack([arr1, arr2]).astype(np.float64, order=\"F\")\n    expected = pa.Tensor.from_numpy(x)\n\n    np.testing.assert_equal(result.to_numpy(), x)\n    assert result.size == 18\n    assert result.type == pa.float64()\n    assert result.shape == expected.shape\n    assert result.strides == expected.strides\n\n    # int32 -> float64\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array(arr1, type=pa.int32()),\n            pa.array(arr2, type=pa.int32()),\n        ], [\"a\", \"b\"]\n    )\n\n    result = batch.to_tensor(null_to_nan=True, row_major=False)\n\n    np.testing.assert_equal(result.to_numpy(), x)\n    assert result.size == 18\n    assert result.type == pa.float64()\n    assert result.shape == expected.shape\n    assert result.strides == expected.strides\n\n    # int8 -> float32\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array(arr1, type=pa.int8()),\n            pa.array(arr2, type=pa.int8()),\n        ], [\"a\", \"b\"]\n    )\n\n    result = batch.to_tensor(null_to_nan=True, row_major=False)\n    x = np.column_stack([arr1, arr2]).astype(np.float32, order=\"F\")\n    expected = pa.Tensor.from_numpy(x)\n\n    np.testing.assert_equal(result.to_numpy(), x)\n    assert result.size == 18\n    assert result.type == pa.float32()\n    assert result.shape == expected.shape\n    assert result.strides == expected.strides\n\n\ndef test_recordbatch_to_tensor_empty():\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array([], type=pa.float32()),\n            pa.array([], type=pa.float32()),\n        ], [\"a\", \"b\"]\n    )\n    result = batch.to_tensor()\n\n    x = np.column_stack([[], []]).astype(np.float32, order=\"F\")\n    expected = pa.Tensor.from_numpy(x)\n\n    assert result.size == expected.size\n    assert result.type == pa.float32()\n    assert result.shape == expected.shape\n    assert result.strides == (4, 4)\n\n\ndef test_recordbatch_to_tensor_unsupported():\n    arr1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    # Unsupported data type\n    arr2 = [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\", \"a\", \"b\", \"c\"]\n    batch = pa.RecordBatch.from_arrays(\n        [\n            pa.array(arr1, type=pa.int32()),\n            pa.array(arr2, type=pa.utf8()),\n        ], [\"a\", \"b\"]\n    )\n    with pytest.raises(\n        pa.ArrowTypeError,\n        match=\"DataType is not supported\"\n    ):\n        batch.to_tensor()\n\n\ndef _table_like_slice_tests(factory):\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10])\n    ]\n    names = ['c0', 'c1']\n\n    obj = factory(data, names=names)\n\n    sliced = obj.slice(2)\n    assert sliced.num_rows == 3\n\n    expected = factory([x.slice(2) for x in data], names=names)\n    assert sliced.equals(expected)\n\n    sliced2 = obj.slice(2, 2)\n    expected2 = factory([x.slice(2, 2) for x in data], names=names)\n    assert sliced2.equals(expected2)\n\n    # 0 offset\n    assert obj.slice(0).equals(obj)\n\n    # Slice past end of array\n    assert len(obj.slice(len(obj))) == 0\n\n    with pytest.raises(IndexError):\n        obj.slice(-1)\n\n    # Check __getitem__-based slicing\n    assert obj.slice(0, 0).equals(obj[:0])\n    assert obj.slice(0, 2).equals(obj[:2])\n    assert obj.slice(2, 2).equals(obj[2:4])\n    assert obj.slice(2, len(obj) - 2).equals(obj[2:])\n    assert obj.slice(len(obj) - 2, 2).equals(obj[-2:])\n    assert obj.slice(len(obj) - 4, 2).equals(obj[-4:-2])\n\n\ndef test_recordbatch_slice_getitem():\n    return _table_like_slice_tests(pa.RecordBatch.from_arrays)\n\n\ndef test_table_slice_getitem():\n    return _table_like_slice_tests(pa.table)\n\n\n@pytest.mark.pandas\ndef test_slice_zero_length_table():\n    # ARROW-7907: a segfault on this code was fixed after 0.16.0\n    table = pa.table({'a': pa.array([], type=pa.timestamp('us'))})\n    table_slice = table.slice(0, 0)\n    table_slice.to_pandas()\n\n    table = pa.table({'a': pa.chunked_array([], type=pa.string())})\n    table.to_pandas()\n\n\ndef test_recordbatchlist_schema_equals():\n    a1 = np.array([1], dtype='uint32')\n    a2 = np.array([4.0, 5.0], dtype='float64')\n    batch1 = pa.record_batch([pa.array(a1)], ['c1'])\n    batch2 = pa.record_batch([pa.array(a2)], ['c1'])\n\n    with pytest.raises(pa.ArrowInvalid):\n        pa.Table.from_batches([batch1, batch2])\n\n\ndef test_table_column_sets_private_name():\n    # ARROW-6429\n    t = pa.table([pa.array([1, 2, 3, 4])], names=['a0'])\n    assert t[0]._name == 'a0'\n\n\ndef test_table_equals():\n    table = pa.Table.from_arrays([], names=[])\n    assert table.equals(table)\n\n    # ARROW-4822\n    assert not table.equals(None)\n\n    other = pa.Table.from_arrays([], names=[], metadata={'key': 'value'})\n    assert not table.equals(other, check_metadata=True)\n    assert table.equals(other)\n\n\ndef test_table_from_batches_and_schema():\n    schema = pa.schema([\n        pa.field('a', pa.int64()),\n        pa.field('b', pa.float64()),\n    ])\n    batch = pa.record_batch([pa.array([1]), pa.array([3.14])],\n                            names=['a', 'b'])\n    table = pa.Table.from_batches([batch], schema)\n    assert table.schema.equals(schema)\n    assert table.column(0) == pa.chunked_array([[1]])\n    assert table.column(1) == pa.chunked_array([[3.14]])\n\n    incompatible_schema = pa.schema([pa.field('a', pa.int64())])\n    with pytest.raises(pa.ArrowInvalid):\n        pa.Table.from_batches([batch], incompatible_schema)\n\n    incompatible_batch = pa.record_batch([pa.array([1])], ['a'])\n    with pytest.raises(pa.ArrowInvalid):\n        pa.Table.from_batches([incompatible_batch], schema)\n\n\n@pytest.mark.pandas\ndef test_table_to_batches():\n    from pandas.testing import assert_frame_equal\n    import pandas as pd\n\n    df1 = pd.DataFrame({'a': list(range(10))})\n    df2 = pd.DataFrame({'a': list(range(10, 30))})\n\n    batch1 = pa.RecordBatch.from_pandas(df1, preserve_index=False)\n    batch2 = pa.RecordBatch.from_pandas(df2, preserve_index=False)\n\n    table = pa.Table.from_batches([batch1, batch2, batch1])\n\n    expected_df = pd.concat([df1, df2, df1], ignore_index=True)\n\n    batches = table.to_batches()\n    assert len(batches) == 3\n\n    assert_frame_equal(pa.Table.from_batches(batches).to_pandas(),\n                       expected_df)\n\n    batches = table.to_batches(max_chunksize=15)\n    assert list(map(len, batches)) == [10, 15, 5, 10]\n\n    assert_frame_equal(table.to_pandas(), expected_df)\n    assert_frame_equal(pa.Table.from_batches(batches).to_pandas(),\n                       expected_df)\n\n    table_from_iter = pa.Table.from_batches(iter([batch1, batch2, batch1]))\n    assert table.equals(table_from_iter)\n\n    with pytest.raises(ValueError):\n        table.to_batches(max_chunksize=0)\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_basics(cls):\n    data = [\n        pa.array(range(5), type='int16'),\n        pa.array([-10, -5, 0, None, 10], type='int32')\n    ]\n    table = cls.from_arrays(data, names=('a', 'b'))\n    table.validate()\n\n    assert not table.schema.metadata\n    assert len(table) == 5\n    assert table.num_rows == 5\n    assert table.num_columns == len(data)\n    assert table.shape == (5, 2)\n    # (only the second array has a null bitmap)\n    assert table.get_total_buffer_size() == (5 * 2) + (5 * 4 + 1)\n    assert table.nbytes == (5 * 2) + (5 * 4 + 1)\n    assert sys.getsizeof(table) >= object.__sizeof__(\n        table) + table.get_total_buffer_size()\n\n    pydict = table.to_pydict()\n    assert pydict == OrderedDict([\n        ('a', [0, 1, 2, 3, 4]),\n        ('b', [-10, -5, 0, None, 10])\n    ])\n    assert isinstance(pydict, dict)\n    assert table == cls.from_pydict(pydict, schema=table.schema)\n\n    with pytest.raises(IndexError):\n        # bounds checking\n        table[2]\n\n    columns = []\n    for col in table.itercolumns():\n\n        if cls is pa.Table:\n            assert type(col) is pa.ChunkedArray\n\n            for chunk in col.iterchunks():\n                assert chunk is not None\n\n            with pytest.raises(IndexError):\n                col.chunk(-1)\n\n            with pytest.raises(IndexError):\n                col.chunk(col.num_chunks)\n\n        else:\n            assert issubclass(type(col), pa.Array)\n\n        columns.append(col)\n\n    assert table.columns == columns\n    assert table == cls.from_arrays(columns, names=table.column_names)\n    assert table != cls.from_arrays(columns[1:], names=table.column_names[1:])\n    assert table != columns\n\n    # Schema passed explicitly\n    schema = pa.schema([pa.field('c0', pa.int16(),\n                                 metadata={'key': 'value'}),\n                        pa.field('c1', pa.int32())],\n                       metadata={b'foo': b'bar'})\n    table = cls.from_arrays(data, schema=schema)\n    assert table.schema == schema\n\n    wr = weakref.ref(table)\n    assert wr() is not None\n    del table\n    assert wr() is None\n\n\ndef test_table_dunder_init():\n    with pytest.raises(TypeError, match='Table'):\n        pa.Table()\n\n\ndef test_table_from_arrays_preserves_column_metadata():\n    # Added to test https://issues.apache.org/jira/browse/ARROW-3866\n    arr0 = pa.array([1, 2])\n    arr1 = pa.array([3, 4])\n    field0 = pa.field('field1', pa.int64(), metadata=dict(a=\"A\", b=\"B\"))\n    field1 = pa.field('field2', pa.int64(), nullable=False)\n    table = pa.Table.from_arrays([arr0, arr1],\n                                 schema=pa.schema([field0, field1]))\n    assert b\"a\" in table.field(0).metadata\n    assert table.field(1).nullable is False\n\n\ndef test_table_from_arrays_invalid_names():\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10])\n    ]\n    with pytest.raises(ValueError):\n        pa.Table.from_arrays(data, names=['a', 'b', 'c'])\n\n    with pytest.raises(ValueError):\n        pa.Table.from_arrays(data, names=['a'])\n\n\ndef test_table_from_lists():\n    data = [\n        list(range(5)),\n        [-10, -5, 0, 5, 10]\n    ]\n\n    result = pa.table(data, names=['a', 'b'])\n    expected = pa.Table.from_arrays(data, names=['a', 'b'])\n    assert result.equals(expected)\n\n    schema = pa.schema([\n        pa.field('a', pa.uint16()),\n        pa.field('b', pa.int64())\n    ])\n    result = pa.table(data, schema=schema)\n    expected = pa.Table.from_arrays(data, schema=schema)\n    assert result.equals(expected)\n\n\ndef test_table_pickle(pickle_module):\n    data = [\n        pa.chunked_array([[1, 2], [3, 4]], type=pa.uint32()),\n        pa.chunked_array([[\"some\", \"strings\", None, \"\"]], type=pa.string()),\n    ]\n    schema = pa.schema([pa.field('ints', pa.uint32()),\n                        pa.field('strs', pa.string())],\n                       metadata={b'foo': b'bar'})\n    table = pa.Table.from_arrays(data, schema=schema)\n\n    result = pickle_module.loads(pickle_module.dumps(table))\n    result.validate()\n    assert result.equals(table)\n\n\ndef test_table_get_field():\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n        pa.array(range(5, 10))\n    ]\n    table = pa.Table.from_arrays(data, names=('a', 'b', 'c'))\n\n    assert table.field('a').equals(table.schema.field('a'))\n    assert table.field(0).equals(table.schema.field('a'))\n\n    with pytest.raises(KeyError):\n        table.field('d')\n\n    with pytest.raises(TypeError):\n        table.field(None)\n\n    with pytest.raises(IndexError):\n        table.field(4)\n\n\ndef test_table_select_column():\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n        pa.array(range(5, 10))\n    ]\n    table = pa.Table.from_arrays(data, names=('a', 'b', 'c'))\n\n    assert table.column('a').equals(table.column(0))\n\n    with pytest.raises(KeyError,\n                       match='Field \"d\" does not exist in schema'):\n        table.column('d')\n\n    with pytest.raises(TypeError):\n        table.column(None)\n\n    with pytest.raises(IndexError):\n        table.column(4)\n\n\ndef test_table_column_with_duplicates():\n    # ARROW-8209\n    table = pa.table([pa.array([1, 2, 3]),\n                      pa.array([4, 5, 6]),\n                      pa.array([7, 8, 9])], names=['a', 'b', 'a'])\n\n    with pytest.raises(KeyError,\n                       match='Field \"a\" exists 2 times in schema'):\n        table.column('a')\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_add_column(cls):\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n        pa.array(range(5, 10))\n    ]\n    table = cls.from_arrays(data, names=('a', 'b', 'c'))\n\n    new_field = pa.field('d', data[1].type)\n    t2 = table.add_column(3, new_field, data[1])\n    t3 = table.append_column(new_field, data[1])\n\n    expected = cls.from_arrays(data + [data[1]],\n                               names=('a', 'b', 'c', 'd'))\n    assert t2.equals(expected)\n    assert t3.equals(expected)\n\n    t4 = table.add_column(0, new_field, data[1])\n    expected = cls.from_arrays([data[1]] + data,\n                               names=('d', 'a', 'b', 'c'))\n    assert t4.equals(expected)\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_set_column(cls):\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n        pa.array(range(5, 10))\n    ]\n    table = cls.from_arrays(data, names=('a', 'b', 'c'))\n\n    new_field = pa.field('d', data[1].type)\n    t2 = table.set_column(0, new_field, data[1])\n\n    expected_data = list(data)\n    expected_data[0] = data[1]\n    expected = cls.from_arrays(expected_data,\n                               names=('d', 'b', 'c'))\n    assert t2.equals(expected)\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_drop_columns(cls):\n    \"\"\" drop one or more columns given labels\"\"\"\n    a = pa.array(range(5))\n    b = pa.array([-10, -5, 0, 5, 10])\n    c = pa.array(range(5, 10))\n\n    table = cls.from_arrays([a, b, c], names=('a', 'b', 'c'))\n    t2 = table.drop_columns(['a', 'b'])\n    t3 = table.drop_columns('a')\n\n    exp_t2 = cls.from_arrays([c], names=('c',))\n    assert exp_t2.equals(t2)\n    exp_t3 = cls.from_arrays([b, c], names=('b', 'c',))\n    assert exp_t3.equals(t3)\n\n    # -- raise KeyError if column not in Table\n    with pytest.raises(KeyError, match=\"Column 'd' not found\"):\n        table.drop_columns(['d'])\n\n\ndef test_table_drop():\n    \"\"\" verify the alias of drop_columns is working\"\"\"\n    a = pa.array(range(5))\n    b = pa.array([-10, -5, 0, 5, 10])\n    c = pa.array(range(5, 10))\n\n    table = pa.Table.from_arrays([a, b, c], names=('a', 'b', 'c'))\n    t2 = table.drop(['a', 'b'])\n    t3 = table.drop('a')\n\n    exp_t2 = pa.Table.from_arrays([c], names=('c',))\n    assert exp_t2.equals(t2)\n    exp_t3 = pa.Table.from_arrays([b, c], names=('b', 'c',))\n    assert exp_t3.equals(t3)\n\n    # -- raise KeyError if column not in Table\n    with pytest.raises(KeyError, match=\"Column 'd' not found\"):\n        table.drop(['d'])\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_remove_column(cls):\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n        pa.array(range(5, 10))\n    ]\n    table = cls.from_arrays(data, names=('a', 'b', 'c'))\n\n    t2 = table.remove_column(0)\n    t2.validate()\n    expected = cls.from_arrays(data[1:], names=('b', 'c'))\n    assert t2.equals(expected)\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_remove_column_empty(cls):\n    # ARROW-1865\n    data = [\n        pa.array(range(5)),\n    ]\n    table = cls.from_arrays(data, names=['a'])\n\n    t2 = table.remove_column(0)\n    t2.validate()\n    assert len(t2) == len(table)\n\n    t3 = t2.add_column(0, table.field(0), table[0])\n    t3.validate()\n    assert t3.equals(table)\n\n\ndef test_empty_table_with_names():\n    # ARROW-13784\n    data = []\n    names = [\"a\", \"b\"]\n    message = (\n        'Length of names [(]2[)] does not match length of arrays [(]0[)]')\n    with pytest.raises(ValueError, match=message):\n        pa.Table.from_arrays(data, names=names)\n\n\ndef test_empty_table():\n    table = pa.table([])\n\n    assert table.column_names == []\n    assert table.equals(pa.Table.from_arrays([], []))\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_rename_columns(cls):\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n        pa.array(range(5, 10))\n    ]\n    table = cls.from_arrays(data, names=['a', 'b', 'c'])\n    assert table.column_names == ['a', 'b', 'c']\n\n    t2 = table.rename_columns(['eh', 'bee', 'sea'])\n    t2.validate()\n    assert t2.column_names == ['eh', 'bee', 'sea']\n\n    expected = cls.from_arrays(data, names=['eh', 'bee', 'sea'])\n    assert t2.equals(expected)\n\n    message = \"names must be a list or dict not <class 'str'>\"\n    with pytest.raises(TypeError, match=message):\n        table.rename_columns('not a list')\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_rename_columns_mapping(cls):\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n        pa.array(range(5, 10))\n    ]\n    table = cls.from_arrays(data, names=['a', 'b', 'c'])\n    assert table.column_names == ['a', 'b', 'c']\n\n    expected = cls.from_arrays(data, names=['eh', 'b', 'sea'])\n    t1 = table.rename_columns({'a': 'eh', 'c': 'sea'})\n    t1.validate()\n    assert t1 == expected\n\n    # Test renaming duplicate column names\n    table = cls.from_arrays(data, names=['a', 'a', 'c'])\n    expected = cls.from_arrays(data, names=['eh', 'eh', 'sea'])\n    t2 = table.rename_columns({'a': 'eh', 'c': 'sea'})\n    t2.validate()\n    assert t2 == expected\n\n    # Test column not found\n    with pytest.raises(KeyError, match=r\"Column 'd' not found\"):\n        table.rename_columns({'a': 'eh', 'd': 'sea'})\n\n\ndef test_table_flatten():\n    ty1 = pa.struct([pa.field('x', pa.int16()),\n                     pa.field('y', pa.float32())])\n    ty2 = pa.struct([pa.field('nest', ty1)])\n    a = pa.array([(1, 2.5), (3, 4.5)], type=ty1)\n    b = pa.array([((11, 12.5),), ((13, 14.5),)], type=ty2)\n    c = pa.array([False, True], type=pa.bool_())\n\n    table = pa.Table.from_arrays([a, b, c], names=['a', 'b', 'c'])\n    t2 = table.flatten()\n    t2.validate()\n    expected = pa.Table.from_arrays([\n        pa.array([1, 3], type=pa.int16()),\n        pa.array([2.5, 4.5], type=pa.float32()),\n        pa.array([(11, 12.5), (13, 14.5)], type=ty1),\n        c],\n        names=['a.x', 'a.y', 'b.nest', 'c'])\n    assert t2.equals(expected)\n\n\ndef test_table_combine_chunks():\n    batch1 = pa.record_batch([pa.array([1]), pa.array([\"a\"])],\n                             names=['f1', 'f2'])\n    batch2 = pa.record_batch([pa.array([2]), pa.array([\"b\"])],\n                             names=['f1', 'f2'])\n    table = pa.Table.from_batches([batch1, batch2])\n    combined = table.combine_chunks()\n    combined.validate()\n    assert combined.equals(table)\n    for c in combined.columns:\n        assert c.num_chunks == 1\n\n\ndef test_table_unify_dictionaries():\n    batch1 = pa.record_batch([\n        pa.array([\"foo\", \"bar\", None, \"foo\"]).dictionary_encode(),\n        pa.array([123, 456, 456, 789]).dictionary_encode(),\n        pa.array([True, False, None, None])], names=['a', 'b', 'c'])\n    batch2 = pa.record_batch([\n        pa.array([\"quux\", \"foo\", None, \"quux\"]).dictionary_encode(),\n        pa.array([456, 789, 789, None]).dictionary_encode(),\n        pa.array([False, None, None, True])], names=['a', 'b', 'c'])\n\n    table = pa.Table.from_batches([batch1, batch2])\n    table = table.replace_schema_metadata({b\"key1\": b\"value1\"})\n    assert table.column(0).chunk(0).dictionary.equals(\n        pa.array([\"foo\", \"bar\"]))\n    assert table.column(0).chunk(1).dictionary.equals(\n        pa.array([\"quux\", \"foo\"]))\n    assert table.column(1).chunk(0).dictionary.equals(\n        pa.array([123, 456, 789]))\n    assert table.column(1).chunk(1).dictionary.equals(\n        pa.array([456, 789]))\n\n    table = table.unify_dictionaries(pa.default_memory_pool())\n    expected_dict_0 = pa.array([\"foo\", \"bar\", \"quux\"])\n    expected_dict_1 = pa.array([123, 456, 789])\n    assert table.column(0).chunk(0).dictionary.equals(expected_dict_0)\n    assert table.column(0).chunk(1).dictionary.equals(expected_dict_0)\n    assert table.column(1).chunk(0).dictionary.equals(expected_dict_1)\n    assert table.column(1).chunk(1).dictionary.equals(expected_dict_1)\n\n    assert table.to_pydict() == {\n        'a': [\"foo\", \"bar\", None, \"foo\", \"quux\", \"foo\", None, \"quux\"],\n        'b': [123, 456, 456, 789, 456, 789, 789, None],\n        'c': [True, False, None, None, False, None, None, True],\n    }\n    assert table.schema.metadata == {b\"key1\": b\"value1\"}\n\n\ndef test_concat_tables():\n    data = [\n        list(range(5)),\n        [-10., -5., 0., 5., 10.]\n    ]\n    data2 = [\n        list(range(5, 10)),\n        [1., 2., 3., 4., 5.]\n    ]\n\n    t1 = pa.Table.from_arrays([pa.array(x) for x in data],\n                              names=('a', 'b'))\n    t2 = pa.Table.from_arrays([pa.array(x) for x in data2],\n                              names=('a', 'b'))\n\n    result = pa.concat_tables([t1, t2])\n    result.validate()\n    assert len(result) == 10\n\n    expected = pa.Table.from_arrays([pa.array(x + y)\n                                     for x, y in zip(data, data2)],\n                                    names=('a', 'b'))\n\n    assert result.equals(expected)\n\n\ndef test_concat_tables_permissive():\n    t1 = pa.Table.from_arrays([list(range(10))], names=('a',))\n    t2 = pa.Table.from_arrays([list(('a', 'b', 'c'))], names=('a',))\n\n    with pytest.raises(\n            pa.ArrowTypeError,\n            match=\"Unable to merge: Field a has incompatible types: int64 vs string\"):\n        _ = pa.concat_tables([t1, t2], promote_options=\"permissive\")\n\n\ndef test_concat_tables_invalid_option():\n    t = pa.Table.from_arrays([list(range(10))], names=('a',))\n\n    with pytest.raises(ValueError, match=\"Invalid promote options: invalid\"):\n        pa.concat_tables([t, t], promote_options=\"invalid\")\n\n\ndef test_concat_tables_none_table():\n    # ARROW-11997\n    with pytest.raises(AttributeError):\n        pa.concat_tables([None])\n\n\n@pytest.mark.pandas\ndef test_concat_tables_with_different_schema_metadata():\n    import pandas as pd\n\n    schema = pa.schema([\n        pa.field('a', pa.string()),\n        pa.field('b', pa.string()),\n    ])\n\n    values = list('abcdefgh')\n    df1 = pd.DataFrame({'a': values, 'b': values})\n    df2 = pd.DataFrame({'a': [np.nan] * 8, 'b': values})\n\n    table1 = pa.Table.from_pandas(df1, schema=schema, preserve_index=False)\n    table2 = pa.Table.from_pandas(df2, schema=schema, preserve_index=False)\n    assert table1.schema.equals(table2.schema)\n    assert not table1.schema.equals(table2.schema, check_metadata=True)\n\n    table3 = pa.concat_tables([table1, table2])\n    assert table1.schema.equals(table3.schema, check_metadata=True)\n    assert table2.schema.equals(table3.schema)\n\n\ndef test_concat_tables_with_promote_option():\n    t1 = pa.Table.from_arrays(\n        [pa.array([1, 2], type=pa.int64())], [\"int64_field\"])\n    t2 = pa.Table.from_arrays(\n        [pa.array([1.0, 2.0], type=pa.float32())], [\"float_field\"])\n\n    with pytest.warns(FutureWarning):\n        result = pa.concat_tables([t1, t2], promote=True)\n\n    assert result.equals(pa.Table.from_arrays([\n        pa.array([1, 2, None, None], type=pa.int64()),\n        pa.array([None, None, 1.0, 2.0], type=pa.float32()),\n    ], [\"int64_field\", \"float_field\"]))\n\n    t1 = pa.Table.from_arrays(\n        [pa.array([1, 2], type=pa.int64())], [\"f\"])\n    t2 = pa.Table.from_arrays(\n        [pa.array([1, 2], type=pa.float32())], [\"f\"])\n\n    with pytest.raises(pa.ArrowInvalid, match=\"Schema at index 1 was different:\"):\n        with pytest.warns(FutureWarning):\n            pa.concat_tables([t1, t2], promote=False)\n\n\ndef test_concat_tables_with_promotion():\n    t1 = pa.Table.from_arrays(\n        [pa.array([1, 2], type=pa.int64())], [\"int64_field\"])\n    t2 = pa.Table.from_arrays(\n        [pa.array([1.0, 2.0], type=pa.float32())], [\"float_field\"])\n\n    result = pa.concat_tables([t1, t2], promote_options=\"default\")\n\n    assert result.equals(pa.Table.from_arrays([\n        pa.array([1, 2, None, None], type=pa.int64()),\n        pa.array([None, None, 1.0, 2.0], type=pa.float32()),\n    ], [\"int64_field\", \"float_field\"]))\n\n    t3 = pa.Table.from_arrays(\n        [pa.array([1, 2], type=pa.int32())], [\"int64_field\"])\n    result = pa.concat_tables(\n        [t1, t3], promote_options=\"permissive\")\n    assert result.equals(pa.Table.from_arrays([\n        pa.array([1, 2, 1, 2], type=pa.int64()),\n    ], [\"int64_field\"]))\n\n\ndef test_concat_tables_with_promotion_error():\n    t1 = pa.Table.from_arrays(\n        [pa.array([1, 2], type=pa.int64())], [\"f\"])\n    t2 = pa.Table.from_arrays(\n        [pa.array([1, 2], type=pa.float32())], [\"f\"])\n\n    with pytest.raises(pa.ArrowTypeError, match=\"Unable to merge:\"):\n        pa.concat_tables([t1, t2], promote_options=\"default\")\n\n\ndef test_table_negative_indexing():\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n        pa.array([1.0, 2.0, 3.0, 4.0, 5.0]),\n        pa.array(['ab', 'bc', 'cd', 'de', 'ef']),\n    ]\n    table = pa.Table.from_arrays(data, names=tuple('abcd'))\n\n    assert table[-1].equals(table[3])\n    assert table[-2].equals(table[2])\n    assert table[-3].equals(table[1])\n    assert table[-4].equals(table[0])\n\n    with pytest.raises(IndexError):\n        table[-5]\n\n    with pytest.raises(IndexError):\n        table[4]\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_cast_to_incompatible_schema(cls):\n    data = [\n        pa.array(range(5)),\n        pa.array([-10, -5, 0, 5, 10]),\n    ]\n    table = cls.from_arrays(data, names=tuple('ab'))\n\n    target_schema1 = pa.schema([\n        pa.field('A', pa.int32()),\n        pa.field('b', pa.int16()),\n    ])\n    target_schema2 = pa.schema([\n        pa.field('a', pa.int32()),\n    ])\n\n    if cls is pa.Table:\n        cls_name = 'table'\n    else:\n        cls_name = 'record batch'\n    message = (\"Target schema's field names are not matching the \"\n               f\"{cls_name}'s field names:.*\")\n\n    with pytest.raises(ValueError, match=message):\n        table.cast(target_schema1)\n    with pytest.raises(ValueError, match=message):\n        table.cast(target_schema2)\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_safe_casting(cls):\n    data = [\n        pa.array(range(5), type=pa.int64()),\n        pa.array([-10, -5, 0, 5, 10], type=pa.int32()),\n        pa.array([1.0, 2.0, 3.0, 4.0, 5.0], type=pa.float64()),\n        pa.array(['ab', 'bc', 'cd', 'de', 'ef'], type=pa.string())\n    ]\n    table = cls.from_arrays(data, names=tuple('abcd'))\n\n    expected_data = [\n        pa.array(range(5), type=pa.int32()),\n        pa.array([-10, -5, 0, 5, 10], type=pa.int16()),\n        pa.array([1, 2, 3, 4, 5], type=pa.int64()),\n        pa.array(['ab', 'bc', 'cd', 'de', 'ef'], type=pa.string())\n    ]\n    expected_table = cls.from_arrays(expected_data, names=tuple('abcd'))\n\n    target_schema = pa.schema([\n        pa.field('a', pa.int32()),\n        pa.field('b', pa.int16()),\n        pa.field('c', pa.int64()),\n        pa.field('d', pa.string())\n    ])\n    casted_table = table.cast(target_schema)\n\n    assert casted_table.equals(expected_table)\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_unsafe_casting(cls):\n    data = [\n        pa.array(range(5), type=pa.int64()),\n        pa.array([-10, -5, 0, 5, 10], type=pa.int32()),\n        pa.array([1.1, 2.2, 3.3, 4.4, 5.5], type=pa.float64()),\n        pa.array(['ab', 'bc', 'cd', 'de', 'ef'], type=pa.string())\n    ]\n    table = cls.from_arrays(data, names=tuple('abcd'))\n\n    expected_data = [\n        pa.array(range(5), type=pa.int32()),\n        pa.array([-10, -5, 0, 5, 10], type=pa.int16()),\n        pa.array([1, 2, 3, 4, 5], type=pa.int64()),\n        pa.array(['ab', 'bc', 'cd', 'de', 'ef'], type=pa.string())\n    ]\n    expected_table = cls.from_arrays(expected_data, names=tuple('abcd'))\n\n    target_schema = pa.schema([\n        pa.field('a', pa.int32()),\n        pa.field('b', pa.int16()),\n        pa.field('c', pa.int64()),\n        pa.field('d', pa.string())\n    ])\n\n    with pytest.raises(pa.ArrowInvalid, match='truncated'):\n        table.cast(target_schema)\n\n    casted_table = table.cast(target_schema, safe=False)\n    assert casted_table.equals(expected_table)\n\n\ndef test_invalid_table_construct():\n    array = np.array([0, 1], dtype=np.uint8)\n    u8 = pa.uint8()\n    arrays = [pa.array(array, type=u8), pa.array(array[1:], type=u8)]\n\n    with pytest.raises(pa.lib.ArrowInvalid):\n        pa.Table.from_arrays(arrays, names=[\"a1\", \"a2\"])\n\n\n@pytest.mark.parametrize('data, klass', [\n    ((['', 'foo', 'bar'], [4.5, 5, None]), list),\n    ((['', 'foo', 'bar'], [4.5, 5, None]), pa.array),\n    (([[''], ['foo', 'bar']], [[4.5], [5., None]]), pa.chunked_array),\n])\ndef test_from_arrays_schema(data, klass):\n    data = [klass(data[0]), klass(data[1])]\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.float32())])\n\n    table = pa.Table.from_arrays(data, schema=schema)\n    assert table.num_columns == 2\n    assert table.num_rows == 3\n    assert table.schema == schema\n\n    # length of data and schema not matching\n    schema = pa.schema([('strs', pa.utf8())])\n    with pytest.raises(ValueError):\n        pa.Table.from_arrays(data, schema=schema)\n\n    # with different but compatible schema\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.float32())])\n    table = pa.Table.from_arrays(data, schema=schema)\n    assert pa.types.is_float32(table.column('floats').type)\n    assert table.num_columns == 2\n    assert table.num_rows == 3\n    assert table.schema == schema\n\n    # with different and incompatible schema\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.timestamp('s'))])\n    with pytest.raises((NotImplementedError, TypeError)):\n        pa.Table.from_pydict(data, schema=schema)\n\n    # Cannot pass both schema and metadata / names\n    with pytest.raises(ValueError):\n        pa.Table.from_arrays(data, schema=schema, names=['strs', 'floats'])\n\n    with pytest.raises(ValueError):\n        pa.Table.from_arrays(data, schema=schema, metadata={b'foo': b'bar'})\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_from_pydict(cls):\n    table = cls.from_pydict({})\n    assert table.num_columns == 0\n    assert table.num_rows == 0\n    assert table.schema == pa.schema([])\n    assert table.to_pydict() == {}\n\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.float64())])\n\n    # With lists as values\n    data = OrderedDict([('strs', ['', 'foo', 'bar']),\n                        ('floats', [4.5, 5, None])])\n    table = cls.from_pydict(data)\n    assert table.num_columns == 2\n    assert table.num_rows == 3\n    assert table.schema == schema\n    assert table.to_pydict() == data\n\n    # With metadata and inferred schema\n    metadata = {b'foo': b'bar'}\n    schema = schema.with_metadata(metadata)\n    table = cls.from_pydict(data, metadata=metadata)\n    assert table.schema == schema\n    assert table.schema.metadata == metadata\n    assert table.to_pydict() == data\n\n    # With explicit schema\n    table = cls.from_pydict(data, schema=schema)\n    assert table.schema == schema\n    assert table.schema.metadata == metadata\n    assert table.to_pydict() == data\n\n    # Cannot pass both schema and metadata\n    with pytest.raises(ValueError):\n        cls.from_pydict(data, schema=schema, metadata=metadata)\n\n    # Non-convertible values given schema\n    with pytest.raises(TypeError):\n        cls.from_pydict({'c0': [0, 1, 2]},\n                        schema=pa.schema([(\"c0\", pa.string())]))\n\n    # Missing schema fields from the passed mapping\n    with pytest.raises(KeyError, match=\"doesn\\'t contain.* c, d\"):\n        cls.from_pydict(\n            {'a': [1, 2, 3], 'b': [3, 4, 5]},\n            schema=pa.schema([\n                ('a', pa.int64()),\n                ('c', pa.int32()),\n                ('d', pa.int16())\n            ])\n        )\n\n    # Passed wrong schema type\n    with pytest.raises(TypeError):\n        cls.from_pydict({'a': [1, 2, 3]}, schema={})\n\n\n@pytest.mark.parametrize('data, klass', [\n    ((['', 'foo', 'bar'], [4.5, 5, None]), pa.array),\n    (([[''], ['foo', 'bar']], [[4.5], [5., None]]), pa.chunked_array),\n])\ndef test_table_from_pydict_arrow_arrays(data, klass):\n    data = OrderedDict([('strs', klass(data[0])), ('floats', klass(data[1]))])\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.float64())])\n\n    # With arrays as values\n    table = pa.Table.from_pydict(data)\n    assert table.num_columns == 2\n    assert table.num_rows == 3\n    assert table.schema == schema\n\n    # With explicit (matching) schema\n    table = pa.Table.from_pydict(data, schema=schema)\n    assert table.num_columns == 2\n    assert table.num_rows == 3\n    assert table.schema == schema\n\n    # with different but compatible schema\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.float32())])\n    table = pa.Table.from_pydict(data, schema=schema)\n    assert pa.types.is_float32(table.column('floats').type)\n    assert table.num_columns == 2\n    assert table.num_rows == 3\n    assert table.schema == schema\n\n    # with different and incompatible schema\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.timestamp('s'))])\n    with pytest.raises((NotImplementedError, TypeError)):\n        pa.Table.from_pydict(data, schema=schema)\n\n\n@pytest.mark.parametrize('data, klass', [\n    ((['', 'foo', 'bar'], [4.5, 5, None]), list),\n    ((['', 'foo', 'bar'], [4.5, 5, None]), pa.array),\n    (([[''], ['foo', 'bar']], [[4.5], [5., None]]), pa.chunked_array),\n])\ndef test_table_from_pydict_schema(data, klass):\n    # passed schema is source of truth for the columns\n\n    data = OrderedDict([('strs', klass(data[0])), ('floats', klass(data[1]))])\n\n    # schema has columns not present in data -> error\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.float64()),\n                        ('ints', pa.int64())])\n    with pytest.raises(KeyError, match='ints'):\n        pa.Table.from_pydict(data, schema=schema)\n\n    # data has columns not present in schema -> ignored\n    schema = pa.schema([('strs', pa.utf8())])\n    table = pa.Table.from_pydict(data, schema=schema)\n    assert table.num_columns == 1\n    assert table.schema == schema\n    assert table.column_names == ['strs']\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_from_pylist(cls):\n    table = cls.from_pylist([])\n    assert table.num_columns == 0\n    assert table.num_rows == 0\n    assert table.schema == pa.schema([])\n    assert table.to_pylist() == []\n\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.float64())])\n\n    # With lists as values\n    data = [{'strs': '', 'floats': 4.5},\n            {'strs': 'foo', 'floats': 5},\n            {'strs': 'bar', 'floats': None}]\n    table = cls.from_pylist(data)\n    assert table.num_columns == 2\n    assert table.num_rows == 3\n    assert table.schema == schema\n    assert table.to_pylist() == data\n\n    # With metadata and inferred schema\n    metadata = {b'foo': b'bar'}\n    schema = schema.with_metadata(metadata)\n    table = cls.from_pylist(data, metadata=metadata)\n    assert table.schema == schema\n    assert table.schema.metadata == metadata\n    assert table.to_pylist() == data\n\n    # With explicit schema\n    table = cls.from_pylist(data, schema=schema)\n    assert table.schema == schema\n    assert table.schema.metadata == metadata\n    assert table.to_pylist() == data\n\n    # Cannot pass both schema and metadata\n    with pytest.raises(ValueError):\n        cls.from_pylist(data, schema=schema, metadata=metadata)\n\n    # Non-convertible values given schema\n    with pytest.raises(TypeError):\n        cls.from_pylist([{'c0': 0}, {'c0': 1}, {'c0': 2}],\n                        schema=pa.schema([(\"c0\", pa.string())]))\n\n    # Missing schema fields in the passed mapping translate to None\n    schema = pa.schema([('a', pa.int64()),\n                        ('c', pa.int32()),\n                        ('d', pa.int16())\n                        ])\n    table = cls.from_pylist(\n        [{'a': 1, 'b': 3}, {'a': 2, 'b': 4}, {'a': 3, 'b': 5}],\n        schema=schema\n    )\n    data = [{'a': 1, 'c': None, 'd': None},\n            {'a': 2, 'c': None, 'd': None},\n            {'a': 3, 'c': None, 'd': None}]\n    assert table.schema == schema\n    assert table.to_pylist() == data\n\n    # Passed wrong schema type\n    with pytest.raises(TypeError):\n        cls.from_pylist([{'a': 1}, {'a': 2}, {'a': 3}], schema={})\n\n    # If the dictionaries of rows are not same length\n    data = [{'strs': '', 'floats': 4.5},\n            {'floats': 5},\n            {'strs': 'bar'}]\n    data2 = [{'strs': '', 'floats': 4.5},\n             {'strs': None, 'floats': 5},\n             {'strs': 'bar', 'floats': None}]\n    table = cls.from_pylist(data)\n    assert table.num_columns == 2\n    assert table.num_rows == 3\n    assert table.to_pylist() == data2\n\n    data = [{'strs': ''},\n            {'strs': 'foo', 'floats': 5},\n            {'floats': None}]\n    data2 = [{'strs': ''},\n             {'strs': 'foo'},\n             {'strs': None}]\n    table = cls.from_pylist(data)\n    assert table.num_columns == 1\n    assert table.num_rows == 3\n    assert table.to_pylist() == data2\n\n\n@pytest.mark.pandas\ndef test_table_from_pandas_schema():\n    # passed schema is source of truth for the columns\n    import pandas as pd\n\n    df = pd.DataFrame(OrderedDict([('strs', ['', 'foo', 'bar']),\n                                   ('floats', [4.5, 5, None])]))\n\n    # with different but compatible schema\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.float32())])\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert pa.types.is_float32(table.column('floats').type)\n    assert table.schema.remove_metadata() == schema\n\n    # with different and incompatible schema\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.timestamp('s'))])\n    with pytest.raises((NotImplementedError, TypeError)):\n        pa.Table.from_pandas(df, schema=schema)\n\n    # schema has columns not present in data -> error\n    schema = pa.schema([('strs', pa.utf8()), ('floats', pa.float64()),\n                        ('ints', pa.int64())])\n    with pytest.raises(KeyError, match='ints'):\n        pa.Table.from_pandas(df, schema=schema)\n\n    # data has columns not present in schema -> ignored\n    schema = pa.schema([('strs', pa.utf8())])\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.num_columns == 1\n    assert table.schema.remove_metadata() == schema\n    assert table.column_names == ['strs']\n\n\n@pytest.mark.pandas\ndef test_table_factory_function():\n    import pandas as pd\n\n    # Put in wrong order to make sure that lines up with schema\n    d = OrderedDict([('b', ['a', 'b', 'c']), ('a', [1, 2, 3])])\n\n    d_explicit = {'b': pa.array(['a', 'b', 'c'], type='string'),\n                  'a': pa.array([1, 2, 3], type='int32')}\n\n    schema = pa.schema([('a', pa.int32()), ('b', pa.string())])\n\n    df = pd.DataFrame(d)\n    table1 = pa.table(df)\n    table2 = pa.Table.from_pandas(df)\n    assert table1.equals(table2)\n    table1 = pa.table(df, schema=schema)\n    table2 = pa.Table.from_pandas(df, schema=schema)\n    assert table1.equals(table2)\n\n    table1 = pa.table(d_explicit)\n    table2 = pa.Table.from_pydict(d_explicit)\n    assert table1.equals(table2)\n\n    # schema coerces type\n    table1 = pa.table(d, schema=schema)\n    table2 = pa.Table.from_pydict(d, schema=schema)\n    assert table1.equals(table2)\n\n\ndef test_table_factory_function_args():\n    # from_pydict not accepting names:\n    with pytest.raises(ValueError):\n        pa.table({'a': [1, 2, 3]}, names=['a'])\n\n    # backwards compatibility for schema as first positional argument\n    schema = pa.schema([('a', pa.int32())])\n    table = pa.table({'a': pa.array([1, 2, 3], type=pa.int64())}, schema)\n    assert table.column('a').type == pa.int32()\n\n    # from_arrays: accept both names and schema as positional first argument\n    data = [pa.array([1, 2, 3], type='int64')]\n    names = ['a']\n    table = pa.table(data, names)\n    assert table.column_names == names\n    schema = pa.schema([('a', pa.int64())])\n    table = pa.table(data, schema)\n    assert table.column_names == names\n\n\n@pytest.mark.pandas\ndef test_table_factory_function_args_pandas():\n    import pandas as pd\n\n    # from_pandas not accepting names or metadata:\n    with pytest.raises(ValueError):\n        pa.table(pd.DataFrame({'a': [1, 2, 3]}), names=['a'])\n\n    with pytest.raises(ValueError):\n        pa.table(pd.DataFrame({'a': [1, 2, 3]}), metadata={b'foo': b'bar'})\n\n    # backwards compatibility for schema as first positional argument\n    schema = pa.schema([('a', pa.int32())])\n    table = pa.table(pd.DataFrame({'a': [1, 2, 3]}), schema)\n    assert table.column('a').type == pa.int32()\n\n\ndef test_factory_functions_invalid_input():\n    with pytest.raises(TypeError, match=\"Expected pandas DataFrame, python\"):\n        pa.table(\"invalid input\")\n\n    with pytest.raises(TypeError, match=\"Expected pandas DataFrame\"):\n        pa.record_batch(\"invalid input\")\n\n\ndef test_table_repr_to_string():\n    # Schema passed explicitly\n    schema = pa.schema([pa.field('c0', pa.int16(),\n                                 metadata={'key': 'value'}),\n                        pa.field('c1', pa.int32())],\n                       metadata={b'foo': b'bar'})\n\n    tab = pa.table([pa.array([1, 2, 3, 4], type='int16'),\n                    pa.array([10, 20, 30, 40], type='int32')], schema=schema)\n    assert str(tab) == \"\"\"pyarrow.Table\nc0: int16\nc1: int32\n----\nc0: [[1,2,3,4]]\nc1: [[10,20,30,40]]\"\"\"\n\n    assert tab.to_string(show_metadata=True) == \"\"\"\\\npyarrow.Table\nc0: int16\n  -- field metadata --\n  key: 'value'\nc1: int32\n-- schema metadata --\nfoo: 'bar'\"\"\"\n\n    assert tab.to_string(preview_cols=5) == \"\"\"\\\npyarrow.Table\nc0: int16\nc1: int32\n----\nc0: [[1,2,3,4]]\nc1: [[10,20,30,40]]\"\"\"\n\n    assert tab.to_string(preview_cols=1) == \"\"\"\\\npyarrow.Table\nc0: int16\nc1: int32\n----\nc0: [[1,2,3,4]]\n...\"\"\"\n\n\ndef test_table_repr_to_string_ellipsis():\n    # Schema passed explicitly\n    schema = pa.schema([pa.field('c0', pa.int16(),\n                                 metadata={'key': 'value'}),\n                        pa.field('c1', pa.int32())],\n                       metadata={b'foo': b'bar'})\n\n    tab = pa.table([pa.array([1, 2, 3, 4]*10, type='int16'),\n                    pa.array([10, 20, 30, 40]*10, type='int32')],\n                   schema=schema)\n    assert str(tab) == \"\"\"pyarrow.Table\nc0: int16\nc1: int32\n----\nc0: [[1,2,3,4,1,...,4,1,2,3,4]]\nc1: [[10,20,30,40,10,...,40,10,20,30,40]]\"\"\"\n\n\ndef test_record_batch_repr_to_string():\n    # Schema passed explicitly\n    schema = pa.schema([pa.field('c0', pa.int16(),\n                                 metadata={'key': 'value'}),\n                        pa.field('c1', pa.int32())],\n                       metadata={b'foo': b'bar'})\n\n    batch = pa.record_batch([pa.array([1, 2, 3, 4], type='int16'),\n                             pa.array([10, 20, 30, 40], type='int32')],\n                            schema=schema)\n    assert str(batch) == \"\"\"pyarrow.RecordBatch\nc0: int16\nc1: int32\n----\nc0: [1,2,3,4]\nc1: [10,20,30,40]\"\"\"\n\n    assert batch.to_string(show_metadata=True) == \"\"\"\\\npyarrow.RecordBatch\nc0: int16\n  -- field metadata --\n  key: 'value'\nc1: int32\n-- schema metadata --\nfoo: 'bar'\"\"\"\n\n    assert batch.to_string(preview_cols=5) == \"\"\"\\\npyarrow.RecordBatch\nc0: int16\nc1: int32\n----\nc0: [1,2,3,4]\nc1: [10,20,30,40]\"\"\"\n\n    assert batch.to_string(preview_cols=1) == \"\"\"\\\npyarrow.RecordBatch\nc0: int16\nc1: int32\n----\nc0: [1,2,3,4]\n...\"\"\"\n\n\ndef test_record_batch_repr_to_string_ellipsis():\n    # Schema passed explicitly\n    schema = pa.schema([pa.field('c0', pa.int16(),\n                                 metadata={'key': 'value'}),\n                        pa.field('c1', pa.int32())],\n                       metadata={b'foo': b'bar'})\n\n    batch = pa.record_batch([pa.array([1, 2, 3, 4]*10, type='int16'),\n                             pa.array([10, 20, 30, 40]*10, type='int32')],\n                            schema=schema)\n    assert str(batch) == \"\"\"pyarrow.RecordBatch\nc0: int16\nc1: int32\n----\nc0: [1,2,3,4,1,2,3,4,1,2,...,3,4,1,2,3,4,1,2,3,4]\nc1: [10,20,30,40,10,20,30,40,10,20,...,30,40,10,20,30,40,10,20,30,40]\"\"\"\n\n\ndef test_table_function_unicode_schema():\n    col_a = \"\u00e4\u00e4\u00e4h\"\n    col_b = \"\u00f6\u00f6\u00f6f\"\n\n    # Put in wrong order to make sure that lines up with schema\n    d = OrderedDict([(col_b, ['a', 'b', 'c']), (col_a, [1, 2, 3])])\n\n    schema = pa.schema([(col_a, pa.int32()), (col_b, pa.string())])\n\n    result = pa.table(d, schema=schema)\n    assert result[0].chunk(0).equals(pa.array([1, 2, 3], type='int32'))\n    assert result[1].chunk(0).equals(pa.array(['a', 'b', 'c'], type='string'))\n\n\ndef test_table_take_vanilla_functionality():\n    table = pa.table(\n        [pa.array([1, 2, 3, None, 5]),\n         pa.array(['a', 'b', 'c', 'd', 'e'])],\n        ['f1', 'f2'])\n\n    assert table.take(pa.array([2, 3])).equals(table.slice(2, 2))\n\n\ndef test_table_take_null_index():\n    table = pa.table(\n        [pa.array([1, 2, 3, None, 5]),\n         pa.array(['a', 'b', 'c', 'd', 'e'])],\n        ['f1', 'f2'])\n\n    result_with_null_index = pa.table(\n        [pa.array([1, None]),\n         pa.array(['a', None])],\n        ['f1', 'f2'])\n\n    assert table.take(pa.array([0, None])).equals(result_with_null_index)\n\n\ndef test_table_take_non_consecutive():\n    table = pa.table(\n        [pa.array([1, 2, 3, None, 5]),\n         pa.array(['a', 'b', 'c', 'd', 'e'])],\n        ['f1', 'f2'])\n\n    result_non_consecutive = pa.table(\n        [pa.array([2, None]),\n         pa.array(['b', 'd'])],\n        ['f1', 'f2'])\n\n    assert table.take(pa.array([1, 3])).equals(result_non_consecutive)\n\n\ndef test_table_select():\n    a1 = pa.array([1, 2, 3, None, 5])\n    a2 = pa.array(['a', 'b', 'c', 'd', 'e'])\n    a3 = pa.array([[1, 2], [3, 4], [5, 6], None, [9, 10]])\n    table = pa.table([a1, a2, a3], ['f1', 'f2', 'f3'])\n\n    # selecting with string names\n    result = table.select(['f1'])\n    expected = pa.table([a1], ['f1'])\n    assert result.equals(expected)\n\n    result = table.select(['f3', 'f2'])\n    expected = pa.table([a3, a2], ['f3', 'f2'])\n    assert result.equals(expected)\n\n    # selecting with integer indices\n    result = table.select([0])\n    expected = pa.table([a1], ['f1'])\n    assert result.equals(expected)\n\n    result = table.select([2, 1])\n    expected = pa.table([a3, a2], ['f3', 'f2'])\n    assert result.equals(expected)\n\n    # preserve metadata\n    table2 = table.replace_schema_metadata({\"a\": \"test\"})\n    result = table2.select([\"f1\", \"f2\"])\n    assert b\"a\" in result.schema.metadata\n\n    # selecting non-existing column raises\n    with pytest.raises(KeyError, match='Field \"f5\" does not exist'):\n        table.select(['f5'])\n\n    with pytest.raises(IndexError, match=\"index out of bounds\"):\n        table.select([5])\n\n    # duplicate selection gives duplicated names in resulting table\n    result = table.select(['f2', 'f2'])\n    expected = pa.table([a2, a2], ['f2', 'f2'])\n    assert result.equals(expected)\n\n    # selection duplicated column raises\n    table = pa.table([a1, a2, a3], ['f1', 'f2', 'f1'])\n    with pytest.raises(KeyError, match='Field \"f1\" exists 2 times'):\n        table.select(['f1'])\n\n    result = table.select(['f2'])\n    expected = pa.table([a2], ['f2'])\n    assert result.equals(expected)\n\n\n@pytest.mark.acero\ndef test_table_group_by():\n    def sorted_by_keys(d):\n        # Ensure a guaranteed order of keys for aggregation results.\n        if \"keys2\" in d:\n            keys = tuple(zip(d[\"keys\"], d[\"keys2\"]))\n        else:\n            keys = d[\"keys\"]\n        sorted_keys = sorted(keys)\n        sorted_d = {\"keys\": sorted(d[\"keys\"])}\n        for entry in d:\n            if entry == \"keys\":\n                continue\n            values = dict(zip(keys, d[entry]))\n            for k in sorted_keys:\n                sorted_d.setdefault(entry, []).append(values[k])\n        return sorted_d\n\n    table = pa.table([\n        pa.array([\"a\", \"a\", \"b\", \"b\", \"c\"]),\n        pa.array([\"X\", \"X\", \"Y\", \"Z\", \"Z\"]),\n        pa.array([1, 2, 3, 4, 5]),\n        pa.array([10, 20, 30, 40, 50])\n    ], names=[\"keys\", \"keys2\", \"values\", \"bigvalues\"])\n\n    r = table.group_by(\"keys\").aggregate([\n        (\"values\", \"hash_sum\")\n    ])\n    assert sorted_by_keys(r.to_pydict()) == {\n        \"keys\": [\"a\", \"b\", \"c\"],\n        \"values_sum\": [3, 7, 5]\n    }\n\n    r = table.group_by(\"keys\").aggregate([\n        (\"values\", \"hash_sum\"),\n        (\"values\", \"hash_count\")\n    ])\n    assert sorted_by_keys(r.to_pydict()) == {\n        \"keys\": [\"a\", \"b\", \"c\"],\n        \"values_sum\": [3, 7, 5],\n        \"values_count\": [2, 2, 1]\n    }\n\n    # Test without hash_ prefix\n    r = table.group_by(\"keys\").aggregate([\n        (\"values\", \"sum\")\n    ])\n    assert sorted_by_keys(r.to_pydict()) == {\n        \"keys\": [\"a\", \"b\", \"c\"],\n        \"values_sum\": [3, 7, 5]\n    }\n\n    r = table.group_by(\"keys\").aggregate([\n        (\"values\", \"max\"),\n        (\"bigvalues\", \"sum\")\n    ])\n    assert sorted_by_keys(r.to_pydict()) == {\n        \"keys\": [\"a\", \"b\", \"c\"],\n        \"values_max\": [2, 4, 5],\n        \"bigvalues_sum\": [30, 70, 50]\n    }\n\n    r = table.group_by(\"keys\").aggregate([\n        (\"bigvalues\", \"max\"),\n        (\"values\", \"sum\")\n    ])\n    assert sorted_by_keys(r.to_pydict()) == {\n        \"keys\": [\"a\", \"b\", \"c\"],\n        \"values_sum\": [3, 7, 5],\n        \"bigvalues_max\": [20, 40, 50]\n    }\n\n    r = table.group_by([\"keys\", \"keys2\"]).aggregate([\n        (\"values\", \"sum\")\n    ])\n    assert sorted_by_keys(r.to_pydict()) == {\n        \"keys\": [\"a\", \"b\", \"b\", \"c\"],\n        \"keys2\": [\"X\", \"Y\", \"Z\", \"Z\"],\n        \"values_sum\": [3, 3, 4, 5]\n    }\n\n    # Test many arguments\n    r = table.group_by(\"keys\").aggregate([\n        (\"values\", \"max\"),\n        (\"bigvalues\", \"sum\"),\n        (\"bigvalues\", \"max\"),\n        ([], \"count_all\"),\n        (\"values\", \"sum\")\n    ])\n    assert sorted_by_keys(r.to_pydict()) == {\n        \"keys\": [\"a\", \"b\", \"c\"],\n        \"values_max\": [2, 4, 5],\n        \"bigvalues_sum\": [30, 70, 50],\n        \"bigvalues_max\": [20, 40, 50],\n        \"count_all\": [2, 2, 1],\n        \"values_sum\": [3, 7, 5]\n    }\n\n    table_with_nulls = pa.table([\n        pa.array([\"a\", \"a\", \"a\"]),\n        pa.array([1, None, None])\n    ], names=[\"keys\", \"values\"])\n\n    r = table_with_nulls.group_by([\"keys\"]).aggregate([\n        (\"values\", \"count\", pc.CountOptions(mode=\"all\"))\n    ])\n    assert r.to_pydict() == {\n        \"keys\": [\"a\"],\n        \"values_count\": [3]\n    }\n\n    r = table_with_nulls.group_by([\"keys\"]).aggregate([\n        (\"values\", \"count\", pc.CountOptions(mode=\"only_null\"))\n    ])\n    assert r.to_pydict() == {\n        \"keys\": [\"a\"],\n        \"values_count\": [2]\n    }\n\n    r = table_with_nulls.group_by([\"keys\"]).aggregate([\n        (\"values\", \"count\", pc.CountOptions(mode=\"only_valid\"))\n    ])\n    assert r.to_pydict() == {\n        \"keys\": [\"a\"],\n        \"values_count\": [1]\n    }\n\n    r = table_with_nulls.group_by([\"keys\"]).aggregate([\n        ([], \"count_all\"),  # nullary count that takes no parameters\n        (\"values\", \"count\", pc.CountOptions(mode=\"only_valid\"))\n    ])\n    assert r.to_pydict() == {\n        \"keys\": [\"a\"],\n        \"count_all\": [3],\n        \"values_count\": [1]\n    }\n\n    r = table_with_nulls.group_by([\"keys\"]).aggregate([\n        ([], \"count_all\")\n    ])\n    assert r.to_pydict() == {\n        \"keys\": [\"a\"],\n        \"count_all\": [3]\n    }\n\n    table = pa.table({\n        'keys': ['a', 'b', 'a', 'b', 'a', 'b'],\n        'values': range(6)})\n    table_with_chunks = pa.Table.from_batches(\n        table.to_batches(max_chunksize=3))\n    r = table_with_chunks.group_by('keys').aggregate([('values', 'sum')])\n    assert sorted_by_keys(r.to_pydict()) == {\n        \"keys\": [\"a\", \"b\"],\n        \"values_sum\": [6, 9]\n    }\n\n\n@pytest.mark.acero\ndef test_table_group_by_first():\n    # \"first\" is an ordered aggregation -> requires to specify use_threads=False\n    table1 = pa.table({'a': [1, 2, 3, 4], 'b': ['a', 'b'] * 2})\n    table2 = pa.table({'a': [1, 2, 3, 4], 'b': ['b', 'a'] * 2})\n    table = pa.concat_tables([table1, table2])\n\n    with pytest.raises(NotImplementedError):\n        table.group_by(\"b\").aggregate([(\"a\", \"first\")])\n\n    result = table.group_by(\"b\", use_threads=False).aggregate([(\"a\", \"first\")])\n    expected = pa.table({\"b\": [\"a\", \"b\"], \"a_first\": [1, 2]})\n    assert result.equals(expected)\n\n\ndef test_table_to_recordbatchreader():\n    table = pa.Table.from_pydict({'x': [1, 2, 3]})\n    reader = table.to_reader()\n    assert table.schema == reader.schema\n    assert table == reader.read_all()\n\n    reader = table.to_reader(max_chunksize=2)\n    assert reader.read_next_batch().num_rows == 2\n    assert reader.read_next_batch().num_rows == 1\n\n\n@pytest.mark.acero\ndef test_table_join():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"]\n    })\n\n    t2 = pa.table({\n        \"colB\": [99, 2, 1],\n        \"col3\": [\"Z\", \"B\", \"A\"]\n    })\n\n    result = t1.join(t2, \"colA\", \"colB\")\n    assert result.combine_chunks() == pa.table({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"],\n        \"col3\": [\"A\", \"B\", None]\n    })\n\n    result = t1.join(t2, \"colA\", \"colB\", join_type=\"full outer\")\n    assert result.combine_chunks().sort_by(\"colA\") == pa.table({\n        \"colA\": [1, 2, 6, 99],\n        \"col2\": [\"a\", \"b\", \"f\", None],\n        \"col3\": [\"A\", \"B\", None, \"Z\"]\n    })\n\n\n@pytest.mark.acero\ndef test_table_join_unique_key():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"]\n    })\n\n    t2 = pa.table({\n        \"colA\": [99, 2, 1],\n        \"col3\": [\"Z\", \"B\", \"A\"]\n    })\n\n    result = t1.join(t2, \"colA\")\n    assert result.combine_chunks() == pa.table({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"],\n        \"col3\": [\"A\", \"B\", None]\n    })\n\n    result = t1.join(t2, \"colA\", join_type=\"full outer\", right_suffix=\"_r\")\n    assert result.combine_chunks().sort_by(\"colA\") == pa.table({\n        \"colA\": [1, 2, 6, 99],\n        \"col2\": [\"a\", \"b\", \"f\", None],\n        \"col3\": [\"A\", \"B\", None, \"Z\"]\n    })\n\n\n@pytest.mark.acero\ndef test_table_join_collisions():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"colVals\": [\"a\", \"b\", \"f\"]\n    })\n\n    t2 = pa.table({\n        \"colA\": [99, 2, 1],\n        \"colB\": [99, 20, 10],\n        \"colVals\": [\"Z\", \"B\", \"A\"]\n    })\n\n    result = t1.join(t2, \"colA\", join_type=\"full outer\")\n    assert result.combine_chunks().sort_by(\"colA\") == pa.table([\n        [1, 2, 6, 99],\n        [10, 20, 60, None],\n        [\"a\", \"b\", \"f\", None],\n        [10, 20, None, 99],\n        [\"A\", \"B\", None, \"Z\"],\n    ], names=[\"colA\", \"colB\", \"colVals\", \"colB\", \"colVals\"])\n\n\n@pytest.mark.acero\ndef test_table_filter_expression():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"colVals\": [\"a\", \"b\", \"f\"]\n    })\n\n    t2 = pa.table({\n        \"colA\": [99, 2, 1],\n        \"colB\": [99, 20, 10],\n        \"colVals\": [\"Z\", \"B\", \"A\"]\n    })\n\n    t3 = pa.concat_tables([t1, t2])\n\n    result = t3.filter(pc.field(\"colA\") < 10)\n    assert result.combine_chunks() == pa.table({\n        \"colA\": [1, 2, 6, 2, 1],\n        \"colB\": [10, 20, 60, 20, 10],\n        \"colVals\": [\"a\", \"b\", \"f\", \"B\", \"A\"]\n    })\n\n\n@pytest.mark.acero\ndef test_table_join_many_columns():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"]\n    })\n\n    t2 = pa.table({\n        \"colB\": [99, 2, 1],\n        \"col3\": [\"Z\", \"B\", \"A\"],\n        \"col4\": [\"Z\", \"B\", \"A\"],\n        \"col5\": [\"Z\", \"B\", \"A\"],\n        \"col6\": [\"Z\", \"B\", \"A\"],\n        \"col7\": [\"Z\", \"B\", \"A\"]\n    })\n\n    result = t1.join(t2, \"colA\", \"colB\")\n    assert result.combine_chunks() == pa.table({\n        \"colA\": [1, 2, 6],\n        \"col2\": [\"a\", \"b\", \"f\"],\n        \"col3\": [\"A\", \"B\", None],\n        \"col4\": [\"A\", \"B\", None],\n        \"col5\": [\"A\", \"B\", None],\n        \"col6\": [\"A\", \"B\", None],\n        \"col7\": [\"A\", \"B\", None]\n    })\n\n    result = t1.join(t2, \"colA\", \"colB\", join_type=\"full outer\")\n    assert result.combine_chunks().sort_by(\"colA\") == pa.table({\n        \"colA\": [1, 2, 6, 99],\n        \"col2\": [\"a\", \"b\", \"f\", None],\n        \"col3\": [\"A\", \"B\", None, \"Z\"],\n        \"col4\": [\"A\", \"B\", None, \"Z\"],\n        \"col5\": [\"A\", \"B\", None, \"Z\"],\n        \"col6\": [\"A\", \"B\", None, \"Z\"],\n        \"col7\": [\"A\", \"B\", None, \"Z\"],\n    })\n\n\n@pytest.mark.dataset\ndef test_table_join_asof():\n    t1 = pa.Table.from_pydict({\n        \"colA\": [1, 1, 5, 6, 7],\n        \"col2\": [\"a\", \"b\", \"a\", \"b\", \"f\"]\n    })\n\n    t2 = pa.Table.from_pydict({\n        \"colB\": [2, 9, 15],\n        \"col3\": [\"a\", \"b\", \"g\"],\n        \"colC\": [1., 3., 5.]\n    })\n\n    r = t1.join_asof(\n        t2, on=\"colA\", by=\"col2\", tolerance=1,\n        right_on=\"colB\", right_by=\"col3\",\n    )\n    assert r.combine_chunks() == pa.table({\n        \"colA\": [1, 1, 5, 6, 7],\n        \"col2\": [\"a\", \"b\", \"a\", \"b\", \"f\"],\n        \"colC\": [1., None, None, None, None],\n    })\n\n\n@pytest.mark.dataset\ndef test_table_join_asof_multiple_by():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"on\": [1, 2, 3],\n    })\n\n    t2 = pa.table({\n        \"colB\": [99, 20, 10],\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"colA\": [99, 2, 1],\n        \"on\": [2, 3, 4],\n    })\n\n    result = t1.join_asof(\n        t2, on=\"on\", by=[\"colA\", \"colB\"], tolerance=1\n    )\n    assert result.sort_by(\"colA\") == pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"on\": [1, 2, 3],\n        \"colVals\": [None, \"B\", None],\n    })\n\n\n@pytest.mark.dataset\ndef test_table_join_asof_empty_by():\n    t1 = pa.table({\n        \"on\": [1, 2, 3],\n    })\n\n    t2 = pa.table({\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"on\": [2, 3, 4],\n    })\n\n    result = t1.join_asof(\n        t2, on=\"on\", by=[], tolerance=1\n    )\n    assert result == pa.table({\n        \"on\": [1, 2, 3],\n        \"colVals\": [\"Z\", \"Z\", \"B\"],\n    })\n\n\n@pytest.mark.dataset\ndef test_table_join_asof_collisions():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"on\": [1, 2, 3],\n        \"colVals\": [\"a\", \"b\", \"f\"]\n    })\n\n    t2 = pa.table({\n        \"colB\": [99, 20, 10],\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"colUniq\": [100, 200, 300],\n        \"colA\": [99, 2, 1],\n        \"on\": [2, 3, 4],\n    })\n\n    msg = (\n        \"Columns {'colVals'} present in both tables. \"\n        \"AsofJoin does not support column collisions.\"\n    )\n    with pytest.raises(ValueError, match=msg):\n        t1.join_asof(\n            t2, on=\"on\", by=[\"colA\", \"colB\"], tolerance=1,\n            right_on=\"on\", right_by=[\"colA\", \"colB\"],\n        )\n\n\n@pytest.mark.dataset\ndef test_table_join_asof_by_length_mismatch():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"colB\": [10, 20, 60],\n        \"on\": [1, 2, 3],\n    })\n\n    t2 = pa.table({\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"colUniq\": [100, 200, 300],\n        \"colA\": [99, 2, 1],\n        \"on\": [2, 3, 4],\n    })\n\n    msg = \"inconsistent size of by-key across inputs\"\n    with pytest.raises(pa.lib.ArrowInvalid, match=msg):\n        t1.join_asof(\n            t2, on=\"on\", by=[\"colA\", \"colB\"], tolerance=1,\n            right_on=\"on\", right_by=[\"colA\"],\n        )\n\n\n@pytest.mark.dataset\ndef test_table_join_asof_by_type_mismatch():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"on\": [1, 2, 3],\n    })\n\n    t2 = pa.table({\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"colUniq\": [100, 200, 300],\n        \"colA\": [99., 2., 1.],\n        \"on\": [2, 3, 4],\n    })\n\n    msg = \"Expected by-key type int64 but got double for field colA in input 1\"\n    with pytest.raises(pa.lib.ArrowInvalid, match=msg):\n        t1.join_asof(\n            t2, on=\"on\", by=[\"colA\"], tolerance=1,\n            right_on=\"on\", right_by=[\"colA\"],\n        )\n\n\n@pytest.mark.dataset\ndef test_table_join_asof_on_type_mismatch():\n    t1 = pa.table({\n        \"colA\": [1, 2, 6],\n        \"on\": [1, 2, 3],\n    })\n\n    t2 = pa.table({\n        \"colVals\": [\"Z\", \"B\", \"A\"],\n        \"colUniq\": [100, 200, 300],\n        \"colA\": [99, 2, 1],\n        \"on\": [2., 3., 4.],\n    })\n\n    msg = \"Expected on-key type int64 but got double for field on in input 1\"\n    with pytest.raises(pa.lib.ArrowInvalid, match=msg):\n        t1.join_asof(\n            t2, on=\"on\", by=[\"colA\"], tolerance=1,\n            right_on=\"on\", right_by=[\"colA\"],\n        )\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_cast_invalid(cls):\n    # Casting a nullable field to non-nullable should be invalid!\n    table = cls.from_pydict({'a': [None, 1], 'b': [None, True]})\n    new_schema = pa.schema([pa.field(\"a\", \"int64\", nullable=True),\n                            pa.field(\"b\", \"bool\", nullable=False)])\n    with pytest.raises(ValueError):\n        table.cast(new_schema)\n\n    table = cls.from_pydict({'a': [None, 1], 'b': [False, True]})\n    assert table.cast(new_schema).schema == new_schema\n\n\n@pytest.mark.parametrize(\n    ('cls'),\n    [\n        (pa.Table),\n        (pa.RecordBatch)\n    ]\n)\ndef test_table_sort_by(cls):\n    table = cls.from_arrays([\n        pa.array([3, 1, 4, 2, 5]),\n        pa.array([\"b\", \"a\", \"b\", \"a\", \"c\"]),\n    ], names=[\"values\", \"keys\"])\n\n    assert table.sort_by(\"values\").to_pydict() == {\n        \"keys\": [\"a\", \"a\", \"b\", \"b\", \"c\"],\n        \"values\": [1, 2, 3, 4, 5]\n    }\n\n    assert table.sort_by([(\"values\", \"descending\")]).to_pydict() == {\n        \"keys\": [\"c\", \"b\", \"b\", \"a\", \"a\"],\n        \"values\": [5, 4, 3, 2, 1]\n    }\n\n    tab = cls.from_arrays([\n        pa.array([5, 7, 7, 35], type=pa.int64()),\n        pa.array([\"foo\", \"car\", \"bar\", \"foobar\"])\n    ], names=[\"a\", \"b\"])\n\n    sorted_tab = tab.sort_by([(\"a\", \"descending\")])\n    sorted_tab_dict = sorted_tab.to_pydict()\n    assert sorted_tab_dict[\"a\"] == [35, 7, 7, 5]\n    assert sorted_tab_dict[\"b\"] == [\"foobar\", \"car\", \"bar\", \"foo\"]\n\n    sorted_tab = tab.sort_by([(\"a\", \"ascending\")])\n    sorted_tab_dict = sorted_tab.to_pydict()\n    assert sorted_tab_dict[\"a\"] == [5, 7, 7, 35]\n    assert sorted_tab_dict[\"b\"] == [\"foo\", \"car\", \"bar\", \"foobar\"]\n\n\n@pytest.mark.parametrize(\"constructor\", [pa.table, pa.record_batch])\ndef test_numpy_asarray(constructor):\n    table = constructor([[1, 2, 3], [4.0, 5.0, 6.0]], names=[\"a\", \"b\"])\n    result = np.asarray(table)\n    expected = np.array([[1, 4], [2, 5], [3, 6]], dtype=\"float64\")\n    np.testing.assert_allclose(result, expected)\n\n    result = np.asarray(table, dtype=\"int32\")\n    np.testing.assert_allclose(result, expected)\n    assert result.dtype == \"int32\"\n\n    # no columns\n    table2 = table.select([])\n    result = np.asarray(table2)\n    expected = np.empty((3, 0))\n    np.testing.assert_allclose(result, expected)\n    assert result.dtype == \"float64\"\n    result = np.asarray(table2, dtype=\"int32\")\n    np.testing.assert_allclose(result, expected)\n    assert result.dtype == \"int32\"\n\n    # no rows\n    table3 = table.slice(0, 0)\n    result = np.asarray(table3)\n    expected = np.empty((0, 2))\n    np.testing.assert_allclose(result, expected)\n    assert result.dtype == \"float64\"\n    result = np.asarray(table3, dtype=\"int32\")\n    np.testing.assert_allclose(result, expected)\n    assert result.dtype == \"int32\"\n\n\n@pytest.mark.parametrize(\"constructor\", [pa.table, pa.record_batch])\ndef test_numpy_array_protocol(constructor):\n    table = constructor([[1, 2, 3], [4.0, 5.0, 6.0]], names=[\"a\", \"b\"])\n    expected = np.array([[1, 4], [2, 5], [3, 6]], dtype=\"float64\")\n\n    if Version(np.__version__) < Version(\"2.0.0.dev0\"):\n        # copy keyword is not strict and not passed down to __array__\n        result = np.array(table, copy=False)\n        np.testing.assert_array_equal(result, expected)\n    else:\n        # starting with numpy 2.0, the copy=False keyword is assumed to be strict\n        with pytest.raises(ValueError, match=\"Unable to avoid a copy\"):\n            np.array(table, copy=False)\n\n\n@pytest.mark.acero\ndef test_invalid_non_join_column():\n    NUM_ITEMS = 30\n    t1 = pa.Table.from_pydict({\n        'id': range(NUM_ITEMS),\n        'array_column': [[z for z in range(3)] for x in range(NUM_ITEMS)],\n    })\n    t2 = pa.Table.from_pydict({\n        'id': range(NUM_ITEMS),\n        'value': [x for x in range(NUM_ITEMS)]\n    })\n\n    # check as left table\n    with pytest.raises(pa.lib.ArrowInvalid) as excinfo:\n        t1.join(t2, 'id', join_type='inner')\n    exp_error_msg = \"Data type list<item: int64> is not supported \" \\\n        + \"in join non-key field array_column\"\n    assert exp_error_msg in str(excinfo.value)\n\n    # check as right table\n    with pytest.raises(pa.lib.ArrowInvalid) as excinfo:\n        t2.join(t1, 'id', join_type='inner')\n    assert exp_error_msg in str(excinfo.value)\n", "python/pyarrow/tests/test_json.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import OrderedDict\nfrom decimal import Decimal\nimport io\nimport itertools\nimport json\nimport string\nimport unittest\n\nimport numpy as np\nimport pytest\n\nimport pyarrow as pa\nfrom pyarrow.json import read_json, ReadOptions, ParseOptions\n\n\ndef generate_col_names():\n    # 'a', 'b'... 'z', then 'aa', 'ab'...\n    letters = string.ascii_lowercase\n    yield from letters\n    for first in letters:\n        for second in letters:\n            yield first + second\n\n\ndef make_random_json(num_cols=2, num_rows=10, linesep='\\r\\n'):\n    arr = np.random.RandomState(42).randint(0, 1000, size=(num_cols, num_rows))\n    col_names = list(itertools.islice(generate_col_names(), num_cols))\n    lines = []\n    for row in arr.T:\n        json_obj = OrderedDict([(k, int(v)) for (k, v) in zip(col_names, row)])\n        lines.append(json.dumps(json_obj))\n    data = linesep.join(lines).encode()\n    columns = [pa.array(col, type=pa.int64()) for col in arr]\n    expected = pa.Table.from_arrays(columns, col_names)\n    return data, expected\n\n\ndef check_options_class_pickling(cls, pickler, **attr_values):\n    opts = cls(**attr_values)\n    new_opts = pickler.loads(pickler.dumps(opts,\n                                           protocol=pickler.HIGHEST_PROTOCOL))\n    for name, value in attr_values.items():\n        assert getattr(new_opts, name) == value\n\n\ndef test_read_options(pickle_module):\n    cls = ReadOptions\n    opts = cls()\n\n    assert opts.block_size > 0\n    opts.block_size = 12345\n    assert opts.block_size == 12345\n\n    assert opts.use_threads is True\n    opts.use_threads = False\n    assert opts.use_threads is False\n\n    opts = cls(block_size=1234, use_threads=False)\n    assert opts.block_size == 1234\n    assert opts.use_threads is False\n\n    check_options_class_pickling(cls, pickler=pickle_module,\n                                 block_size=1234,\n                                 use_threads=False)\n\n\ndef test_parse_options(pickle_module):\n    cls = ParseOptions\n    opts = cls()\n    assert opts.newlines_in_values is False\n    assert opts.explicit_schema is None\n\n    opts.newlines_in_values = True\n    assert opts.newlines_in_values is True\n\n    schema = pa.schema([pa.field('foo', pa.int32())])\n    opts.explicit_schema = schema\n    assert opts.explicit_schema == schema\n\n    assert opts.unexpected_field_behavior == \"infer\"\n    for value in [\"ignore\", \"error\", \"infer\"]:\n        opts.unexpected_field_behavior = value\n        assert opts.unexpected_field_behavior == value\n\n    with pytest.raises(ValueError):\n        opts.unexpected_field_behavior = \"invalid-value\"\n\n    check_options_class_pickling(cls, pickler=pickle_module,\n                                 explicit_schema=schema,\n                                 newlines_in_values=False,\n                                 unexpected_field_behavior=\"ignore\")\n\n\nclass BaseTestJSONRead:\n\n    def read_bytes(self, b, **kwargs):\n        return self.read_json(pa.py_buffer(b), **kwargs)\n\n    def check_names(self, table, names):\n        assert table.num_columns == len(names)\n        assert [c.name for c in table.columns] == names\n\n    def test_file_object(self):\n        data = b'{\"a\": 1, \"b\": 2}\\n'\n        expected_data = {'a': [1], 'b': [2]}\n        bio = io.BytesIO(data)\n        table = self.read_json(bio)\n        assert table.to_pydict() == expected_data\n        # Text files not allowed\n        sio = io.StringIO(data.decode())\n        with pytest.raises(TypeError):\n            self.read_json(sio)\n\n    def test_block_sizes(self):\n        rows = b'{\"a\": 1}\\n{\"a\": 2}\\n{\"a\": 3}'\n        read_options = ReadOptions()\n        parse_options = ParseOptions()\n\n        for data in [rows, rows + b'\\n']:\n            for newlines_in_values in [False, True]:\n                parse_options.newlines_in_values = newlines_in_values\n                read_options.block_size = 4\n                with pytest.raises(ValueError,\n                                   match=\"try to increase block size\"):\n                    self.read_bytes(data, read_options=read_options,\n                                    parse_options=parse_options)\n\n                # Validate reader behavior with various block sizes.\n                # There used to be bugs in this area.\n                for block_size in range(9, 20):\n                    read_options.block_size = block_size\n                    table = self.read_bytes(data, read_options=read_options,\n                                            parse_options=parse_options)\n                    assert table.to_pydict() == {'a': [1, 2, 3]}\n\n    def test_no_newline_at_end(self):\n        rows = b'{\"a\": 1,\"b\": 2, \"c\": 3}\\n{\"a\": 4,\"b\": 5, \"c\": 6}'\n        table = self.read_bytes(rows)\n        assert table.to_pydict() == {\n            'a': [1, 4],\n            'b': [2, 5],\n            'c': [3, 6],\n        }\n\n    def test_simple_ints(self):\n        # Infer integer columns\n        rows = b'{\"a\": 1,\"b\": 2, \"c\": 3}\\n{\"a\": 4,\"b\": 5, \"c\": 6}\\n'\n        table = self.read_bytes(rows)\n        schema = pa.schema([('a', pa.int64()),\n                            ('b', pa.int64()),\n                            ('c', pa.int64())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [1, 4],\n            'b': [2, 5],\n            'c': [3, 6],\n        }\n\n    def test_simple_varied(self):\n        # Infer various kinds of data\n        rows = (b'{\"a\": 1,\"b\": 2, \"c\": \"3\", \"d\": false}\\n'\n                b'{\"a\": 4.0, \"b\": -5, \"c\": \"foo\", \"d\": true}\\n')\n        table = self.read_bytes(rows)\n        schema = pa.schema([('a', pa.float64()),\n                            ('b', pa.int64()),\n                            ('c', pa.string()),\n                            ('d', pa.bool_())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [1.0, 4.0],\n            'b': [2, -5],\n            'c': [\"3\", \"foo\"],\n            'd': [False, True],\n        }\n\n    def test_simple_nulls(self):\n        # Infer various kinds of data, with nulls\n        rows = (b'{\"a\": 1, \"b\": 2, \"c\": null, \"d\": null, \"e\": null}\\n'\n                b'{\"a\": null, \"b\": -5, \"c\": \"foo\", \"d\": null, \"e\": true}\\n'\n                b'{\"a\": 4.5, \"b\": null, \"c\": \"nan\", \"d\": null,\"e\": false}\\n')\n        table = self.read_bytes(rows)\n        schema = pa.schema([('a', pa.float64()),\n                            ('b', pa.int64()),\n                            ('c', pa.string()),\n                            ('d', pa.null()),\n                            ('e', pa.bool_())])\n        assert table.schema == schema\n        assert table.to_pydict() == {\n            'a': [1.0, None, 4.5],\n            'b': [2, -5, None],\n            'c': [None, \"foo\", \"nan\"],\n            'd': [None, None, None],\n            'e': [None, True, False],\n        }\n\n    def test_empty_lists(self):\n        # ARROW-10955: Infer list(null)\n        rows = b'{\"a\": []}'\n        table = self.read_bytes(rows)\n        schema = pa.schema([('a', pa.list_(pa.null()))])\n        assert table.schema == schema\n        assert table.to_pydict() == {'a': [[]]}\n\n    def test_empty_rows(self):\n        rows = b'{}\\n{}\\n'\n        table = self.read_bytes(rows)\n        schema = pa.schema([])\n        assert table.schema == schema\n        assert table.num_columns == 0\n        assert table.num_rows == 2\n\n    def test_reconcile_across_blocks(self):\n        # ARROW-12065: reconciling inferred types across blocks\n        first_row = b'{                               }\\n'\n        read_options = ReadOptions(block_size=len(first_row))\n        for next_rows, expected_pylist in [\n            (b'{\"a\": 0}', [None, 0]),\n            (b'{\"a\": []}', [None, []]),\n            (b'{\"a\": []}\\n{\"a\": [[1]]}', [None, [], [[1]]]),\n            (b'{\"a\": {}}', [None, {}]),\n            (b'{\"a\": {}}\\n{\"a\": {\"b\": {\"c\": 1}}}',\n             [None, {\"b\": None}, {\"b\": {\"c\": 1}}]),\n        ]:\n            table = self.read_bytes(first_row + next_rows,\n                                    read_options=read_options)\n            expected = {\"a\": expected_pylist}\n            assert table.to_pydict() == expected\n            # Check that the issue was exercised\n            assert table.column(\"a\").num_chunks > 1\n\n    def test_explicit_schema_decimal(self):\n        rows = (b'{\"a\": 1}\\n'\n                b'{\"a\": 1.45}\\n'\n                b'{\"a\": -23.456}\\n'\n                b'{}\\n')\n        expected = {\n            'a': [Decimal(\"1\"), Decimal(\"1.45\"), Decimal(\"-23.456\"), None],\n        }\n        for type_factory in (pa.decimal128, pa.decimal256):\n            schema = pa.schema([('a', type_factory(9, 4))])\n            opts = ParseOptions(explicit_schema=schema)\n            table = self.read_bytes(rows, parse_options=opts)\n            assert table.schema == schema\n            assert table.to_pydict() == expected\n\n    def test_explicit_schema_with_unexpected_behaviour(self):\n        # infer by default\n        rows = (b'{\"foo\": \"bar\", \"num\": 0}\\n'\n                b'{\"foo\": \"baz\", \"num\": 1}\\n')\n        schema = pa.schema([\n            ('foo', pa.binary())\n        ])\n\n        opts = ParseOptions(explicit_schema=schema)\n        table = self.read_bytes(rows, parse_options=opts)\n        assert table.schema == pa.schema([\n            ('foo', pa.binary()),\n            ('num', pa.int64())\n        ])\n        assert table.to_pydict() == {\n            'foo': [b'bar', b'baz'],\n            'num': [0, 1],\n        }\n\n        # ignore the unexpected fields\n        opts = ParseOptions(explicit_schema=schema,\n                            unexpected_field_behavior=\"ignore\")\n        table = self.read_bytes(rows, parse_options=opts)\n        assert table.schema == pa.schema([\n            ('foo', pa.binary()),\n        ])\n        assert table.to_pydict() == {\n            'foo': [b'bar', b'baz'],\n        }\n\n        # raise error\n        opts = ParseOptions(explicit_schema=schema,\n                            unexpected_field_behavior=\"error\")\n        with pytest.raises(pa.ArrowInvalid,\n                           match=\"JSON parse error: unexpected field\"):\n            self.read_bytes(rows, parse_options=opts)\n\n    def test_small_random_json(self):\n        data, expected = make_random_json(num_cols=2, num_rows=10)\n        table = self.read_bytes(data)\n        assert table.schema == expected.schema\n        assert table.equals(expected)\n        assert table.to_pydict() == expected.to_pydict()\n\n    def test_load_large_json(self):\n        data, expected = make_random_json(num_cols=2, num_rows=100100)\n        # set block size is 10MB\n        read_options = ReadOptions(block_size=1024*1024*10)\n        table = self.read_bytes(data, read_options=read_options)\n        assert table.num_rows == 100100\n        assert expected.num_rows == 100100\n\n    def test_stress_block_sizes(self):\n        # Test a number of small block sizes to stress block stitching\n        data_base, expected = make_random_json(num_cols=2, num_rows=100)\n        read_options = ReadOptions()\n        parse_options = ParseOptions()\n\n        for data in [data_base, data_base.rstrip(b'\\r\\n')]:\n            for newlines_in_values in [False, True]:\n                parse_options.newlines_in_values = newlines_in_values\n                for block_size in [22, 23, 37]:\n                    read_options.block_size = block_size\n                    table = self.read_bytes(data, read_options=read_options,\n                                            parse_options=parse_options)\n                    assert table.schema == expected.schema\n                    if not table.equals(expected):\n                        # Better error output\n                        assert table.to_pydict() == expected.to_pydict()\n\n\nclass TestSerialJSONRead(BaseTestJSONRead, unittest.TestCase):\n\n    def read_json(self, *args, **kwargs):\n        read_options = kwargs.setdefault('read_options', ReadOptions())\n        read_options.use_threads = False\n        table = read_json(*args, **kwargs)\n        table.validate(full=True)\n        return table\n\n\nclass TestParallelJSONRead(BaseTestJSONRead, unittest.TestCase):\n\n    def read_json(self, *args, **kwargs):\n        read_options = kwargs.setdefault('read_options', ReadOptions())\n        read_options.use_threads = True\n        table = read_json(*args, **kwargs)\n        table.validate(full=True)\n        return table\n", "python/pyarrow/tests/interchange/test_conversion.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom datetime import datetime as dt\nimport numpy as np\nimport pyarrow as pa\nfrom pyarrow.vendored.version import Version\nimport pytest\n\nimport pyarrow.interchange as pi\nfrom pyarrow.interchange.column import (\n    _PyArrowColumn,\n    ColumnNullType,\n    DtypeKind,\n)\nfrom pyarrow.interchange.from_dataframe import _from_dataframe\n\ntry:\n    import pandas as pd\n    # import pandas.testing as tm\nexcept ImportError:\n    pass\n\n\n@pytest.mark.parametrize(\"unit\", ['s', 'ms', 'us', 'ns'])\n@pytest.mark.parametrize(\"tz\", ['', 'America/New_York', '+07:30', '-04:30'])\ndef test_datetime(unit, tz):\n    dt_arr = [dt(2007, 7, 13), dt(2007, 7, 14), None]\n    table = pa.table({\"A\": pa.array(dt_arr, type=pa.timestamp(unit, tz=tz))})\n    col = table.__dataframe__().get_column_by_name(\"A\")\n\n    assert col.size() == 3\n    assert col.offset == 0\n    assert col.null_count == 1\n    assert col.dtype[0] == DtypeKind.DATETIME\n    assert col.describe_null == (ColumnNullType.USE_BITMASK, 0)\n\n\n@pytest.mark.parametrize(\n    [\"test_data\", \"kind\"],\n    [\n        ([\"foo\", \"bar\"], 21),\n        ([1.5, 2.5, 3.5], 2),\n        ([1, 2, 3, 4], 0),\n    ],\n)\ndef test_array_to_pyarrowcolumn(test_data, kind):\n    arr = pa.array(test_data)\n    arr_column = _PyArrowColumn(arr)\n\n    assert arr_column._col == arr\n    assert arr_column.size() == len(test_data)\n    assert arr_column.dtype[0] == kind\n    assert arr_column.num_chunks() == 1\n    assert arr_column.null_count == 0\n    assert arr_column.get_buffers()[\"validity\"] is None\n    assert len(list(arr_column.get_chunks())) == 1\n\n    for chunk in arr_column.get_chunks():\n        assert chunk == arr_column\n\n\ndef test_offset_of_sliced_array():\n    arr = pa.array([1, 2, 3, 4])\n    arr_sliced = arr.slice(2, 2)\n\n    table = pa.table([arr], names=[\"arr\"])\n    table_sliced = pa.table([arr_sliced], names=[\"arr_sliced\"])\n\n    col = table_sliced.__dataframe__().get_column(0)\n    assert col.offset == 2\n\n    result = _from_dataframe(table_sliced.__dataframe__())\n    assert table_sliced.equals(result)\n    assert not table.equals(result)\n\n    # pandas hardcodes offset to 0:\n    # https://github.com/pandas-dev/pandas/blob/5c66e65d7b9fef47ccb585ce2fd0b3ea18dc82ea/pandas/core/interchange/from_dataframe.py#L247\n    # so conversion to pandas can't be tested currently\n\n    # df = pandas_from_dataframe(table)\n    # df_sliced = pandas_from_dataframe(table_sliced)\n\n    # tm.assert_series_equal(df[\"arr\"][2:4], df_sliced[\"arr_sliced\"],\n    #                        check_index=False, check_names=False)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\n    \"uint\", [pa.uint8(), pa.uint16(), pa.uint32()]\n)\n@pytest.mark.parametrize(\n    \"int\", [pa.int8(), pa.int16(), pa.int32(), pa.int64()]\n)\n@pytest.mark.parametrize(\n    \"float, np_float\", [\n        # (pa.float16(), np.float16),   #not supported by pandas\n        (pa.float32(), np.float32),\n        (pa.float64(), np.float64)\n    ]\n)\ndef test_pandas_roundtrip(uint, int, float, np_float):\n    if Version(pd.__version__) < Version(\"1.5.0\"):\n        pytest.skip(\"__dataframe__ added to pandas in 1.5.0\")\n\n    arr = [1, 2, 3]\n    table = pa.table(\n        {\n            \"a\": pa.array(arr, type=uint),\n            \"b\": pa.array(arr, type=int),\n            \"c\": pa.array(np.array(arr, dtype=np_float), type=float),\n            \"d\": [True, False, True],\n        }\n    )\n    from pandas.api.interchange import (\n        from_dataframe as pandas_from_dataframe\n    )\n    pandas_df = pandas_from_dataframe(table)\n    result = pi.from_dataframe(pandas_df)\n    assert table.equals(result)\n\n    table_protocol = table.__dataframe__()\n    result_protocol = result.__dataframe__()\n\n    assert table_protocol.num_columns() == result_protocol.num_columns()\n    assert table_protocol.num_rows() == result_protocol.num_rows()\n    assert table_protocol.num_chunks() == result_protocol.num_chunks()\n    assert table_protocol.column_names() == result_protocol.column_names()\n\n\n@pytest.mark.pandas\ndef test_pandas_roundtrip_string():\n    # See https://github.com/pandas-dev/pandas/issues/50554\n    if Version(pd.__version__) < Version(\"1.6\"):\n        pytest.skip(\"Column.size() bug in pandas\")\n\n    arr = [\"a\", \"\", \"c\"]\n    table = pa.table({\"a\": pa.array(arr)})\n\n    from pandas.api.interchange import (\n        from_dataframe as pandas_from_dataframe\n    )\n\n    pandas_df = pandas_from_dataframe(table)\n    result = pi.from_dataframe(pandas_df)\n\n    assert result[\"a\"].to_pylist() == table[\"a\"].to_pylist()\n    assert pa.types.is_string(table[\"a\"].type)\n    assert pa.types.is_large_string(result[\"a\"].type)\n\n    table_protocol = table.__dataframe__()\n    result_protocol = result.__dataframe__()\n\n    assert table_protocol.num_columns() == result_protocol.num_columns()\n    assert table_protocol.num_rows() == result_protocol.num_rows()\n    assert table_protocol.num_chunks() == result_protocol.num_chunks()\n    assert table_protocol.column_names() == result_protocol.column_names()\n\n\n@pytest.mark.pandas\ndef test_pandas_roundtrip_large_string():\n    # See https://github.com/pandas-dev/pandas/issues/50554\n    if Version(pd.__version__) < Version(\"1.6\"):\n        pytest.skip(\"Column.size() bug in pandas\")\n\n    arr = [\"a\", \"\", \"c\"]\n    table = pa.table({\"a_large\": pa.array(arr, type=pa.large_string())})\n\n    from pandas.api.interchange import (\n        from_dataframe as pandas_from_dataframe\n    )\n\n    if Version(pd.__version__) >= Version(\"2.0.1\"):\n        pandas_df = pandas_from_dataframe(table)\n        result = pi.from_dataframe(pandas_df)\n\n        assert result[\"a_large\"].to_pylist() == table[\"a_large\"].to_pylist()\n        assert pa.types.is_large_string(table[\"a_large\"].type)\n        assert pa.types.is_large_string(result[\"a_large\"].type)\n\n        table_protocol = table.__dataframe__()\n        result_protocol = result.__dataframe__()\n\n        assert table_protocol.num_columns() == result_protocol.num_columns()\n        assert table_protocol.num_rows() == result_protocol.num_rows()\n        assert table_protocol.num_chunks() == result_protocol.num_chunks()\n        assert table_protocol.column_names() == result_protocol.column_names()\n\n    else:\n        # large string not supported by pandas implementation for\n        # older versions of pandas\n        # https://github.com/pandas-dev/pandas/issues/52795\n        with pytest.raises(AssertionError):\n            pandas_from_dataframe(table)\n\n\n@pytest.mark.pandas\ndef test_pandas_roundtrip_string_with_missing():\n    # See https://github.com/pandas-dev/pandas/issues/50554\n    if Version(pd.__version__) < Version(\"1.6\"):\n        pytest.skip(\"Column.size() bug in pandas\")\n\n    arr = [\"a\", \"\", \"c\", None]\n    table = pa.table({\"a\": pa.array(arr),\n                      \"a_large\": pa.array(arr, type=pa.large_string())})\n\n    from pandas.api.interchange import (\n        from_dataframe as pandas_from_dataframe\n    )\n\n    if Version(pd.__version__) >= Version(\"2.0.2\"):\n        pandas_df = pandas_from_dataframe(table)\n        result = pi.from_dataframe(pandas_df)\n\n        assert result[\"a\"].to_pylist() == table[\"a\"].to_pylist()\n        assert pa.types.is_string(table[\"a\"].type)\n        assert pa.types.is_large_string(result[\"a\"].type)\n\n        assert result[\"a_large\"].to_pylist() == table[\"a_large\"].to_pylist()\n        assert pa.types.is_large_string(table[\"a_large\"].type)\n        assert pa.types.is_large_string(result[\"a_large\"].type)\n    else:\n        # older versions of pandas do not have bitmask support\n        # https://github.com/pandas-dev/pandas/issues/49888\n        with pytest.raises(NotImplementedError):\n            pandas_from_dataframe(table)\n\n\n@pytest.mark.pandas\ndef test_pandas_roundtrip_categorical():\n    if Version(pd.__version__) < Version(\"2.0.2\"):\n        pytest.skip(\"Bitmasks not supported in pandas interchange implementation\")\n\n    arr = [\"Mon\", \"Tue\", \"Mon\", \"Wed\", \"Mon\", \"Thu\", \"Fri\", \"Sat\", None]\n    table = pa.table(\n        {\"weekday\": pa.array(arr).dictionary_encode()}\n    )\n\n    from pandas.api.interchange import (\n        from_dataframe as pandas_from_dataframe\n    )\n    pandas_df = pandas_from_dataframe(table)\n    result = pi.from_dataframe(pandas_df)\n\n    assert result[\"weekday\"].to_pylist() == table[\"weekday\"].to_pylist()\n    assert pa.types.is_dictionary(table[\"weekday\"].type)\n    assert pa.types.is_dictionary(result[\"weekday\"].type)\n    assert pa.types.is_string(table[\"weekday\"].chunk(0).dictionary.type)\n    assert pa.types.is_large_string(result[\"weekday\"].chunk(0).dictionary.type)\n    assert pa.types.is_int32(table[\"weekday\"].chunk(0).indices.type)\n    assert pa.types.is_int8(result[\"weekday\"].chunk(0).indices.type)\n\n    table_protocol = table.__dataframe__()\n    result_protocol = result.__dataframe__()\n\n    assert table_protocol.num_columns() == result_protocol.num_columns()\n    assert table_protocol.num_rows() == result_protocol.num_rows()\n    assert table_protocol.num_chunks() == result_protocol.num_chunks()\n    assert table_protocol.column_names() == result_protocol.column_names()\n\n    col_table = table_protocol.get_column(0)\n    col_result = result_protocol.get_column(0)\n\n    assert col_result.dtype[0] == DtypeKind.CATEGORICAL\n    assert col_result.dtype[0] == col_table.dtype[0]\n    assert col_result.size() == col_table.size()\n    assert col_result.offset == col_table.offset\n\n    desc_cat_table = col_result.describe_categorical\n    desc_cat_result = col_result.describe_categorical\n\n    assert desc_cat_table[\"is_ordered\"] == desc_cat_result[\"is_ordered\"]\n    assert desc_cat_table[\"is_dictionary\"] == desc_cat_result[\"is_dictionary\"]\n    assert isinstance(desc_cat_result[\"categories\"]._col, pa.Array)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\"unit\", ['s', 'ms', 'us', 'ns'])\ndef test_pandas_roundtrip_datetime(unit):\n    if Version(pd.__version__) < Version(\"1.5.0\"):\n        pytest.skip(\"__dataframe__ added to pandas in 1.5.0\")\n    from datetime import datetime as dt\n\n    # timezones not included as they are not yet supported in\n    # the pandas implementation\n    dt_arr = [dt(2007, 7, 13), dt(2007, 7, 14), dt(2007, 7, 15)]\n    table = pa.table({\"a\": pa.array(dt_arr, type=pa.timestamp(unit))})\n\n    if Version(pd.__version__) < Version(\"1.6\"):\n        # pandas < 2.0 always creates datetime64 in \"ns\"\n        # resolution\n        expected = pa.table({\"a\": pa.array(dt_arr, type=pa.timestamp('ns'))})\n    else:\n        expected = table\n\n    from pandas.api.interchange import (\n        from_dataframe as pandas_from_dataframe\n    )\n    pandas_df = pandas_from_dataframe(table)\n    result = pi.from_dataframe(pandas_df)\n\n    assert expected.equals(result)\n\n    expected_protocol = expected.__dataframe__()\n    result_protocol = result.__dataframe__()\n\n    assert expected_protocol.num_columns() == result_protocol.num_columns()\n    assert expected_protocol.num_rows() == result_protocol.num_rows()\n    assert expected_protocol.num_chunks() == result_protocol.num_chunks()\n    assert expected_protocol.column_names() == result_protocol.column_names()\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\n    \"np_float\", [np.float32, np.float64]\n)\ndef test_pandas_to_pyarrow_with_missing(np_float):\n    if Version(pd.__version__) < Version(\"1.5.0\"):\n        pytest.skip(\"__dataframe__ added to pandas in 1.5.0\")\n\n    np_array = np.array([0, np.nan, 2], dtype=np_float)\n    datetime_array = [None, dt(2007, 7, 14), dt(2007, 7, 15)]\n    df = pd.DataFrame({\n        # float, ColumnNullType.USE_NAN\n        \"a\": np_array,\n        # ColumnNullType.USE_SENTINEL\n        \"dt\": np.array(datetime_array, dtype=\"datetime64[ns]\")\n    })\n    expected = pa.table({\n        \"a\": pa.array(np_array, from_pandas=True),\n        \"dt\": pa.array(datetime_array, type=pa.timestamp(\"ns\"))\n    })\n    result = pi.from_dataframe(df)\n\n    assert result.equals(expected)\n\n\n@pytest.mark.pandas\ndef test_pandas_to_pyarrow_float16_with_missing():\n    if Version(pd.__version__) < Version(\"1.5.0\"):\n        pytest.skip(\"__dataframe__ added to pandas in 1.5.0\")\n\n    # np.float16 errors if ps.is_nan is used\n    # pyarrow.lib.ArrowNotImplementedError: Function 'is_nan' has no kernel\n    # matching input types (halffloat)\n    np_array = np.array([0, np.nan, 2], dtype=np.float16)\n    df = pd.DataFrame({\"a\": np_array})\n\n    with pytest.raises(NotImplementedError):\n        pi.from_dataframe(df)\n\n\n@pytest.mark.parametrize(\n    \"uint\", [pa.uint8(), pa.uint16(), pa.uint32()]\n)\n@pytest.mark.parametrize(\n    \"int\", [pa.int8(), pa.int16(), pa.int32(), pa.int64()]\n)\n@pytest.mark.parametrize(\n    \"float, np_float\", [\n        (pa.float16(), np.float16),\n        (pa.float32(), np.float32),\n        (pa.float64(), np.float64)\n    ]\n)\n@pytest.mark.parametrize(\"unit\", ['s', 'ms', 'us', 'ns'])\n@pytest.mark.parametrize(\"tz\", ['America/New_York', '+07:30', '-04:30'])\n@pytest.mark.parametrize(\"offset, length\", [(0, 3), (0, 2), (1, 2), (2, 1)])\ndef test_pyarrow_roundtrip(uint, int, float, np_float,\n                           unit, tz, offset, length):\n\n    from datetime import datetime as dt\n    arr = [1, 2, None]\n    dt_arr = [dt(2007, 7, 13), None, dt(2007, 7, 15)]\n\n    table = pa.table(\n        {\n            \"a\": pa.array(arr, type=uint),\n            \"b\": pa.array(arr, type=int),\n            \"c\": pa.array(np.array(arr, dtype=np_float),\n                          type=float, from_pandas=True),\n            \"d\": [True, False, True],\n            \"e\": [True, False, None],\n            \"f\": [\"a\", None, \"c\"],\n            \"g\": pa.array(dt_arr, type=pa.timestamp(unit, tz=tz))\n        }\n    )\n    table = table.slice(offset, length)\n    result = _from_dataframe(table.__dataframe__())\n\n    assert table.equals(result)\n\n    table_protocol = table.__dataframe__()\n    result_protocol = result.__dataframe__()\n\n    assert table_protocol.num_columns() == result_protocol.num_columns()\n    assert table_protocol.num_rows() == result_protocol.num_rows()\n    assert table_protocol.num_chunks() == result_protocol.num_chunks()\n    assert table_protocol.column_names() == result_protocol.column_names()\n\n\n@pytest.mark.parametrize(\"offset, length\", [(0, 10), (0, 2), (7, 3), (2, 1)])\ndef test_pyarrow_roundtrip_categorical(offset, length):\n    arr = [\"Mon\", \"Tue\", \"Mon\", \"Wed\", \"Mon\", \"Thu\", \"Fri\", None, \"Sun\"]\n    table = pa.table(\n        {\"weekday\": pa.array(arr).dictionary_encode()}\n    )\n    table = table.slice(offset, length)\n    result = _from_dataframe(table.__dataframe__())\n\n    assert table.equals(result)\n\n    table_protocol = table.__dataframe__()\n    result_protocol = result.__dataframe__()\n\n    assert table_protocol.num_columns() == result_protocol.num_columns()\n    assert table_protocol.num_rows() == result_protocol.num_rows()\n    assert table_protocol.num_chunks() == result_protocol.num_chunks()\n    assert table_protocol.column_names() == result_protocol.column_names()\n\n    col_table = table_protocol.get_column(0)\n    col_result = result_protocol.get_column(0)\n\n    assert col_result.dtype[0] == DtypeKind.CATEGORICAL\n    assert col_result.dtype[0] == col_table.dtype[0]\n    assert col_result.size() == col_table.size()\n    assert col_result.offset == col_table.offset\n\n    desc_cat_table = col_table.describe_categorical\n    desc_cat_result = col_result.describe_categorical\n\n    assert desc_cat_table[\"is_ordered\"] == desc_cat_result[\"is_ordered\"]\n    assert desc_cat_table[\"is_dictionary\"] == desc_cat_result[\"is_dictionary\"]\n    assert isinstance(desc_cat_result[\"categories\"]._col, pa.Array)\n\n\n@pytest.mark.large_memory\ndef test_pyarrow_roundtrip_large_string():\n\n    data = np.array([b'x'*1024]*(3*1024**2), dtype='object')  # 3GB bytes data\n    arr = pa.array(data, type=pa.large_string())\n    table = pa.table([arr], names=[\"large_string\"])\n\n    result = _from_dataframe(table.__dataframe__())\n    col = result.__dataframe__().get_column(0)\n\n    assert col.size() == 3*1024**2\n    assert pa.types.is_large_string(table[0].type)\n    assert pa.types.is_large_string(result[0].type)\n\n    assert table.equals(result)\n\n\ndef test_nan_as_null():\n    table = pa.table({\"a\": [1, 2, 3, 4]})\n    with pytest.raises(RuntimeError):\n        table.__dataframe__(nan_as_null=True)\n\n\n@pytest.mark.pandas\ndef test_allow_copy_false():\n    if Version(pd.__version__) < Version(\"1.5.0\"):\n        pytest.skip(\"__dataframe__ added to pandas in 1.5.0\")\n\n    # Test that an error is raised when a copy is needed\n    # to create a bitmask\n\n    df = pd.DataFrame({\"a\": [0, 1.0, 2.0]})\n    with pytest.raises(RuntimeError):\n        pi.from_dataframe(df, allow_copy=False)\n\n    df = pd.DataFrame({\n        \"dt\": [None, dt(2007, 7, 14), dt(2007, 7, 15)]\n    })\n    with pytest.raises(RuntimeError):\n        pi.from_dataframe(df, allow_copy=False)\n\n\n@pytest.mark.pandas\ndef test_allow_copy_false_bool_categorical():\n    if Version(pd.__version__) < Version(\"1.5.0\"):\n        pytest.skip(\"__dataframe__ added to pandas in 1.5.0\")\n\n    # Test that an error is raised for boolean\n    # and categorical dtype (copy is always made)\n\n    df = pd.DataFrame({\"a\": [None, False, True]})\n    with pytest.raises(RuntimeError):\n        pi.from_dataframe(df, allow_copy=False)\n\n    df = pd.DataFrame({\"a\": [True, False, True]})\n    with pytest.raises(RuntimeError):\n        pi.from_dataframe(df, allow_copy=False)\n\n    df = pd.DataFrame({\"weekday\": [\"a\", \"b\", None]})\n    df = df.astype(\"category\")\n    with pytest.raises(RuntimeError):\n        pi.from_dataframe(df, allow_copy=False)\n\n    df = pd.DataFrame({\"weekday\": [\"a\", \"b\", \"c\"]})\n    df = df.astype(\"category\")\n    with pytest.raises(RuntimeError):\n        pi.from_dataframe(df, allow_copy=False)\n\n\ndef test_empty_dataframe():\n    schema = pa.schema([('col1', pa.int8())])\n    df = pa.table([[]], schema=schema)\n    dfi = df.__dataframe__()\n    assert pi.from_dataframe(dfi) == df\n", "python/pyarrow/tests/interchange/test_interchange_spec.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport ctypes\nimport hypothesis as h\nimport hypothesis.strategies as st\n\nimport numpy as np\nimport pyarrow as pa\nimport pyarrow.tests.strategies as past\nimport pytest\n\n\nall_types = st.deferred(\n    lambda: (\n        past.signed_integer_types |\n        past.unsigned_integer_types |\n        past.floating_types |\n        past.bool_type |\n        past.string_type |\n        past.large_string_type\n    )\n)\n\n\n# datetime is tested in test_extra.py\n# dictionary is tested in test_categorical()\n@h.given(past.arrays(all_types, size=3))\ndef test_dtypes(arr):\n    table = pa.table([arr], names=[\"a\"])\n    df = table.__dataframe__()\n\n    null_count = df.get_column(0).null_count\n    assert null_count == arr.null_count\n    assert isinstance(null_count, int)\n    assert df.get_column(0).size() == 3\n    assert df.get_column(0).offset == 0\n\n\n@pytest.mark.parametrize(\n    \"uint, uint_bw\",\n    [\n        (pa.uint8(), 8),\n        (pa.uint16(), 16),\n        (pa.uint32(), 32)\n    ]\n)\n@pytest.mark.parametrize(\n    \"int, int_bw\", [\n        (pa.int8(), 8),\n        (pa.int16(), 16),\n        (pa.int32(), 32),\n        (pa.int64(), 64)\n    ]\n)\n@pytest.mark.parametrize(\n    \"float, float_bw, np_float\", [\n        (pa.float16(), 16, np.float16),\n        (pa.float32(), 32, np.float32),\n        (pa.float64(), 64, np.float64)\n    ]\n)\n@pytest.mark.parametrize(\"unit\", ['s', 'ms', 'us', 'ns'])\n@pytest.mark.parametrize(\"tz\", ['', 'America/New_York', '+07:30', '-04:30'])\n@pytest.mark.parametrize(\"use_batch\", [False, True])\ndef test_mixed_dtypes(uint, uint_bw, int, int_bw,\n                      float, float_bw, np_float, unit, tz,\n                      use_batch):\n    from datetime import datetime as dt\n    arr = [1, 2, 3]\n    dt_arr = [dt(2007, 7, 13), dt(2007, 7, 14), dt(2007, 7, 15)]\n    table = pa.table(\n        {\n            \"a\": pa.array(arr, type=uint),\n            \"b\": pa.array(arr, type=int),\n            \"c\": pa.array(np.array(arr, dtype=np_float), type=float),\n            \"d\": [True, False, True],\n            \"e\": [\"a\", \"\", \"c\"],\n            \"f\": pa.array(dt_arr, type=pa.timestamp(unit, tz=tz))\n        }\n    )\n    if use_batch:\n        table = table.to_batches()[0]\n    df = table.__dataframe__()\n    # 0 = DtypeKind.INT, 1 = DtypeKind.UINT, 2 = DtypeKind.FLOAT,\n    # 20 = DtypeKind.BOOL, 21 = DtypeKind.STRING, 22 = DtypeKind.DATETIME\n    # see DtypeKind class in column.py\n    columns = {\"a\": 1, \"b\": 0, \"c\": 2, \"d\": 20, \"e\": 21, \"f\": 22}\n\n    for column, kind in columns.items():\n        col = df.get_column_by_name(column)\n\n        assert col.null_count == 0\n        assert col.size() == 3\n        assert col.offset == 0\n        assert col.dtype[0] == kind\n\n    assert df.get_column_by_name(\"a\").dtype[1] == uint_bw\n    assert df.get_column_by_name(\"b\").dtype[1] == int_bw\n    assert df.get_column_by_name(\"c\").dtype[1] == float_bw\n\n\ndef test_na_float():\n    table = pa.table({\"a\": [1.0, None, 2.0]})\n    df = table.__dataframe__()\n    col = df.get_column_by_name(\"a\")\n    assert col.null_count == 1\n    assert isinstance(col.null_count, int)\n\n\ndef test_noncategorical():\n    table = pa.table({\"a\": [1, 2, 3]})\n    df = table.__dataframe__()\n    col = df.get_column_by_name(\"a\")\n    with pytest.raises(TypeError, match=\".*categorical.*\"):\n        col.describe_categorical\n\n\n@pytest.mark.parametrize(\"use_batch\", [False, True])\ndef test_categorical(use_batch):\n    import pyarrow as pa\n    arr = [\"Mon\", \"Tue\", \"Mon\", \"Wed\", \"Mon\", \"Thu\", \"Fri\", \"Sat\", None]\n    table = pa.table(\n        {\"weekday\": pa.array(arr).dictionary_encode()}\n    )\n    if use_batch:\n        table = table.to_batches()[0]\n\n    col = table.__dataframe__().get_column_by_name(\"weekday\")\n    categorical = col.describe_categorical\n    assert isinstance(categorical[\"is_ordered\"], bool)\n    assert isinstance(categorical[\"is_dictionary\"], bool)\n\n\n@pytest.mark.parametrize(\"use_batch\", [False, True])\ndef test_dataframe(use_batch):\n    n = pa.chunked_array([[2, 2, 4], [4, 5, 100]])\n    a = pa.chunked_array([[\"Flamingo\", \"Parrot\", \"Cow\"],\n                         [\"Horse\", \"Brittle stars\", \"Centipede\"]])\n    table = pa.table([n, a], names=['n_legs', 'animals'])\n    if use_batch:\n        table = table.combine_chunks().to_batches()[0]\n    df = table.__dataframe__()\n\n    assert df.num_columns() == 2\n    assert df.num_rows() == 6\n    if use_batch:\n        assert df.num_chunks() == 1\n    else:\n        assert df.num_chunks() == 2\n    assert list(df.column_names()) == ['n_legs', 'animals']\n    assert list(df.select_columns((1,)).column_names()) == list(\n        df.select_columns_by_name((\"animals\",)).column_names()\n    )\n\n\n@pytest.mark.parametrize(\"use_batch\", [False, True])\n@pytest.mark.parametrize([\"size\", \"n_chunks\"], [(10, 3), (12, 3), (12, 5)])\ndef test_df_get_chunks(use_batch, size, n_chunks):\n    table = pa.table({\"x\": list(range(size))})\n    if use_batch:\n        table = table.to_batches()[0]\n    df = table.__dataframe__()\n    chunks = list(df.get_chunks(n_chunks))\n    assert len(chunks) == n_chunks\n    assert sum(chunk.num_rows() for chunk in chunks) == size\n\n\n@pytest.mark.parametrize(\"use_batch\", [False, True])\n@pytest.mark.parametrize([\"size\", \"n_chunks\"], [(10, 3), (12, 3), (12, 5)])\ndef test_column_get_chunks(use_batch, size, n_chunks):\n    table = pa.table({\"x\": list(range(size))})\n    if use_batch:\n        table = table.to_batches()[0]\n    df = table.__dataframe__()\n    chunks = list(df.get_column(0).get_chunks(n_chunks))\n    assert len(chunks) == n_chunks\n    assert sum(chunk.size() for chunk in chunks) == size\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\n    \"uint\", [pa.uint8(), pa.uint16(), pa.uint32()]\n)\n@pytest.mark.parametrize(\n    \"int\", [pa.int8(), pa.int16(), pa.int32(), pa.int64()]\n)\n@pytest.mark.parametrize(\n    \"float, np_float\", [\n        (pa.float16(), np.float16),\n        (pa.float32(), np.float32),\n        (pa.float64(), np.float64)\n    ]\n)\n@pytest.mark.parametrize(\"use_batch\", [False, True])\ndef test_get_columns(uint, int, float, np_float, use_batch):\n    arr = [[1, 2, 3], [4, 5]]\n    arr_float = np.array([1, 2, 3, 4, 5], dtype=np_float)\n    table = pa.table(\n        {\n            \"a\": pa.chunked_array(arr, type=uint),\n            \"b\": pa.chunked_array(arr, type=int),\n            \"c\": pa.array(arr_float, type=float)\n        }\n    )\n    if use_batch:\n        table = table.combine_chunks().to_batches()[0]\n    df = table.__dataframe__()\n    for col in df.get_columns():\n        assert col.size() == 5\n        assert col.num_chunks() == 1\n\n    # 0 = DtypeKind.INT, 1 = DtypeKind.UINT, 2 = DtypeKind.FLOAT,\n    # see DtypeKind class in column.py\n    assert df.get_column(0).dtype[0] == 1  # UINT\n    assert df.get_column(1).dtype[0] == 0  # INT\n    assert df.get_column(2).dtype[0] == 2  # FLOAT\n\n\n@pytest.mark.parametrize(\n    \"int\", [pa.int8(), pa.int16(), pa.int32(), pa.int64()]\n)\n@pytest.mark.parametrize(\"use_batch\", [False, True])\ndef test_buffer(int, use_batch):\n    arr = [0, 1, -1]\n    table = pa.table({\"a\": pa.array(arr, type=int)})\n    if use_batch:\n        table = table.to_batches()[0]\n    df = table.__dataframe__()\n    col = df.get_column(0)\n    buf = col.get_buffers()\n\n    dataBuf, dataDtype = buf[\"data\"]\n\n    assert dataBuf.bufsize > 0\n    assert dataBuf.ptr != 0\n    device, _ = dataBuf.__dlpack_device__()\n\n    # 0 = DtypeKind.INT\n    # see DtypeKind class in column.py\n    assert dataDtype[0] == 0\n\n    if device == 1:  # CPU-only as we're going to directly read memory here\n        bitwidth = dataDtype[1]\n        ctype = {\n            8: ctypes.c_int8,\n            16: ctypes.c_int16,\n            32: ctypes.c_int32,\n            64: ctypes.c_int64,\n        }[bitwidth]\n\n        for idx, truth in enumerate(arr):\n            val = ctype.from_address(dataBuf.ptr + idx * (bitwidth // 8)).value\n            assert val == truth, f\"Buffer at index {idx} mismatch\"\n\n\n@pytest.mark.parametrize(\n    \"indices_type, bitwidth, f_string\", [\n        (pa.int8(), 8, \"c\"),\n        (pa.int16(), 16, \"s\"),\n        (pa.int32(), 32, \"i\"),\n        (pa.int64(), 64, \"l\")\n    ]\n)\ndef test_categorical_dtype(indices_type, bitwidth, f_string):\n    type = pa.dictionary(indices_type, pa.string())\n    arr = pa.array([\"a\", \"b\", None, \"d\"], type)\n    table = pa.table({'a': arr})\n\n    df = table.__dataframe__()\n    col = df.get_column(0)\n    assert col.dtype[0] == 23  # <DtypeKind.CATEGORICAL: 23>\n    assert col.dtype[1] == bitwidth\n    assert col.dtype[2] == f_string\n", "python/pyarrow/tests/interchange/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "python/pyarrow/tests/parquet/test_datetime.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport datetime\nimport io\nimport warnings\n\nimport numpy as np\nimport pytest\n\nimport pyarrow as pa\nfrom pyarrow.tests.parquet.common import _check_roundtrip\n\ntry:\n    import pyarrow.parquet as pq\n    from pyarrow.tests.parquet.common import _read_table, _write_table\nexcept ImportError:\n    pq = None\n\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n\n    from pyarrow.tests.parquet.common import _roundtrip_pandas_dataframe\nexcept ImportError:\n    pd = tm = None\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = pytest.mark.parquet\n\n\n@pytest.mark.pandas\ndef test_pandas_parquet_datetime_tz():\n    # Pandas v2 defaults to [ns], but Arrow defaults to [us] time units\n    # so we need to cast the pandas dtype. Pandas v1 will always silently\n    # coerce to [ns] due to lack of non-[ns] support.\n    s = pd.Series([datetime.datetime(2017, 9, 6)], dtype='datetime64[us]')\n    s = s.dt.tz_localize('utc')\n    s.index = s\n\n    # Both a column and an index to hit both use cases\n    df = pd.DataFrame({'tz_aware': s,\n                       'tz_eastern': s.dt.tz_convert('US/Eastern')},\n                      index=s)\n\n    f = io.BytesIO()\n\n    arrow_table = pa.Table.from_pandas(df)\n\n    _write_table(arrow_table, f)\n    f.seek(0)\n\n    table_read = pq.read_pandas(f)\n\n    df_read = table_read.to_pandas()\n    tm.assert_frame_equal(df, df_read)\n\n\n@pytest.mark.pandas\ndef test_datetime_timezone_tzinfo():\n    value = datetime.datetime(2018, 1, 1, 1, 23, 45,\n                              tzinfo=datetime.timezone.utc)\n    df = pd.DataFrame({'foo': [value]})\n\n    _roundtrip_pandas_dataframe(df, write_kwargs={})\n\n\n@pytest.mark.pandas\ndef test_coerce_timestamps(tempdir):\n    from collections import OrderedDict\n\n    # ARROW-622\n    arrays = OrderedDict()\n    fields = [pa.field('datetime64',\n                       pa.list_(pa.timestamp('ms')))]\n    arrays['datetime64'] = [\n        np.array(['2007-07-13T01:23:34.123456789',\n                  None,\n                  '2010-08-13T05:46:57.437699912'],\n                 dtype='datetime64[ms]'),\n        None,\n        None,\n        np.array(['2007-07-13T02',\n                  None,\n                  '2010-08-13T05:46:57.437699912'],\n                 dtype='datetime64[ms]'),\n    ]\n\n    df = pd.DataFrame(arrays)\n    schema = pa.schema(fields)\n\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df, schema=schema)\n\n    _write_table(arrow_table, filename, version='2.6', coerce_timestamps='us')\n    table_read = _read_table(filename)\n    df_read = table_read.to_pandas()\n\n    df_expected = df.copy()\n    for i, x in enumerate(df_expected['datetime64']):\n        if isinstance(x, np.ndarray):\n            df_expected.loc[i, 'datetime64'] = x.astype('M8[us]')\n\n    tm.assert_frame_equal(df_expected, df_read)\n\n    with pytest.raises(ValueError):\n        _write_table(arrow_table, filename, version='2.6',\n                     coerce_timestamps='unknown')\n\n\n@pytest.mark.pandas\ndef test_coerce_timestamps_truncated(tempdir):\n    \"\"\"\n    ARROW-2555: Test that we can truncate timestamps when coercing if\n    explicitly allowed.\n    \"\"\"\n    dt_us = datetime.datetime(year=2017, month=1, day=1, hour=1, minute=1,\n                              second=1, microsecond=1)\n    dt_ms = datetime.datetime(year=2017, month=1, day=1, hour=1, minute=1,\n                              second=1)\n\n    fields_us = [pa.field('datetime64', pa.timestamp('us'))]\n    arrays_us = {'datetime64': [dt_us, dt_ms]}\n\n    df_us = pd.DataFrame(arrays_us)\n    schema_us = pa.schema(fields_us)\n\n    filename = tempdir / 'pandas_truncated.parquet'\n    table_us = pa.Table.from_pandas(df_us, schema=schema_us)\n\n    _write_table(table_us, filename, version='2.6', coerce_timestamps='ms',\n                 allow_truncated_timestamps=True)\n    table_ms = _read_table(filename)\n    df_ms = table_ms.to_pandas()\n\n    arrays_expected = {'datetime64': [dt_ms, dt_ms]}\n    df_expected = pd.DataFrame(arrays_expected, dtype='datetime64[ms]')\n    tm.assert_frame_equal(df_expected, df_ms)\n\n\n@pytest.mark.pandas\ndef test_date_time_types(tempdir):\n    t1 = pa.date32()\n    data1 = np.array([17259, 17260, 17261], dtype='int32')\n    a1 = pa.array(data1, type=t1)\n\n    t2 = pa.date64()\n    data2 = data1.astype('int64') * 86400000\n    a2 = pa.array(data2, type=t2)\n\n    t3 = pa.timestamp('us')\n    start = pd.Timestamp('2001-01-01').value / 1000\n    data3 = np.array([start, start + 1, start + 2], dtype='int64')\n    a3 = pa.array(data3, type=t3)\n\n    t4 = pa.time32('ms')\n    data4 = np.arange(3, dtype='i4')\n    a4 = pa.array(data4, type=t4)\n\n    t5 = pa.time64('us')\n    a5 = pa.array(data4.astype('int64'), type=t5)\n\n    t6 = pa.time32('s')\n    a6 = pa.array(data4, type=t6)\n\n    ex_t6 = pa.time32('ms')\n    ex_a6 = pa.array(data4 * 1000, type=ex_t6)\n\n    t7 = pa.timestamp('ns')\n    start = pd.Timestamp('2001-01-01').value\n    data7 = np.array([start, start + 1000, start + 2000],\n                     dtype='int64')\n    a7 = pa.array(data7, type=t7)\n\n    table = pa.Table.from_arrays([a1, a2, a3, a4, a5, a6, a7],\n                                 ['date32', 'date64', 'timestamp[us]',\n                                  'time32[s]', 'time64[us]',\n                                  'time32_from64[s]',\n                                  'timestamp[ns]'])\n\n    # date64 as date32\n    # time32[s] to time32[ms]\n    expected = pa.Table.from_arrays([a1, a1, a3, a4, a5, ex_a6, a7],\n                                    ['date32', 'date64', 'timestamp[us]',\n                                     'time32[s]', 'time64[us]',\n                                     'time32_from64[s]',\n                                     'timestamp[ns]'])\n\n    _check_roundtrip(table, expected=expected, version='2.6')\n\n    t0 = pa.timestamp('ms')\n    data0 = np.arange(4, dtype='int64')\n    a0 = pa.array(data0, type=t0)\n\n    t1 = pa.timestamp('us')\n    data1 = np.arange(4, dtype='int64')\n    a1 = pa.array(data1, type=t1)\n\n    t2 = pa.timestamp('ns')\n    data2 = np.arange(4, dtype='int64')\n    a2 = pa.array(data2, type=t2)\n\n    table = pa.Table.from_arrays([a0, a1, a2],\n                                 ['ts[ms]', 'ts[us]', 'ts[ns]'])\n    expected = pa.Table.from_arrays([a0, a1, a2],\n                                    ['ts[ms]', 'ts[us]', 'ts[ns]'])\n\n    # int64 for all timestamps supported by default\n    filename = tempdir / 'int64_timestamps.parquet'\n    _write_table(table, filename, version='2.6')\n    parquet_schema = pq.ParquetFile(filename).schema\n    for i in range(3):\n        assert parquet_schema.column(i).physical_type == 'INT64'\n    read_table = _read_table(filename)\n    assert read_table.equals(expected)\n\n    t0_ns = pa.timestamp('ns')\n    data0_ns = np.array(data0 * 1000000, dtype='int64')\n    a0_ns = pa.array(data0_ns, type=t0_ns)\n\n    t1_ns = pa.timestamp('ns')\n    data1_ns = np.array(data1 * 1000, dtype='int64')\n    a1_ns = pa.array(data1_ns, type=t1_ns)\n\n    expected = pa.Table.from_arrays([a0_ns, a1_ns, a2],\n                                    ['ts[ms]', 'ts[us]', 'ts[ns]'])\n\n    # int96 nanosecond timestamps produced upon request\n    filename = tempdir / 'explicit_int96_timestamps.parquet'\n    _write_table(table, filename, version='2.6',\n                 use_deprecated_int96_timestamps=True)\n    parquet_schema = pq.ParquetFile(filename).schema\n    for i in range(3):\n        assert parquet_schema.column(i).physical_type == 'INT96'\n    read_table = _read_table(filename)\n    assert read_table.equals(expected)\n\n    # int96 nanosecond timestamps implied by flavor 'spark'\n    filename = tempdir / 'spark_int96_timestamps.parquet'\n    _write_table(table, filename, version='2.6',\n                 flavor='spark')\n    parquet_schema = pq.ParquetFile(filename).schema\n    for i in range(3):\n        assert parquet_schema.column(i).physical_type == 'INT96'\n    read_table = _read_table(filename)\n    assert read_table.equals(expected)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('unit', ['s', 'ms', 'us', 'ns'])\ndef test_coerce_int96_timestamp_unit(unit):\n    i_s = pd.Timestamp('2010-01-01').value / 1000000000  # := 1262304000\n\n    d_s = np.arange(i_s, i_s + 10, 1, dtype='int64')\n    d_ms = d_s * 1000\n    d_us = d_ms * 1000\n    d_ns = d_us * 1000\n\n    a_s = pa.array(d_s, type=pa.timestamp('s'))\n    a_ms = pa.array(d_ms, type=pa.timestamp('ms'))\n    a_us = pa.array(d_us, type=pa.timestamp('us'))\n    a_ns = pa.array(d_ns, type=pa.timestamp('ns'))\n\n    arrays = {\"s\": a_s, \"ms\": a_ms, \"us\": a_us, \"ns\": a_ns}\n    names = ['ts_s', 'ts_ms', 'ts_us', 'ts_ns']\n    table = pa.Table.from_arrays([a_s, a_ms, a_us, a_ns], names)\n\n    # For either Parquet version, coercing to nanoseconds is allowed\n    # if Int96 storage is used\n    expected = pa.Table.from_arrays([arrays.get(unit)]*4, names)\n    read_table_kwargs = {\"coerce_int96_timestamp_unit\": unit}\n    _check_roundtrip(table, expected,\n                     read_table_kwargs=read_table_kwargs,\n                     use_deprecated_int96_timestamps=True)\n    _check_roundtrip(table, expected, version='2.6',\n                     read_table_kwargs=read_table_kwargs,\n                     use_deprecated_int96_timestamps=True)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('pq_reader_method', ['ParquetFile', 'read_table'])\ndef test_coerce_int96_timestamp_overflow(pq_reader_method, tempdir):\n\n    def get_table(pq_reader_method, filename, **kwargs):\n        if pq_reader_method == \"ParquetFile\":\n            return pq.ParquetFile(filename, **kwargs).read()\n        elif pq_reader_method == \"read_table\":\n            return pq.read_table(filename, **kwargs)\n\n    # Recreating the initial JIRA issue referenced in ARROW-12096\n    oob_dts = [\n        datetime.datetime(1000, 1, 1),\n        datetime.datetime(2000, 1, 1),\n        datetime.datetime(3000, 1, 1)\n    ]\n    df = pd.DataFrame({\"a\": oob_dts})\n    table = pa.table(df)\n\n    filename = tempdir / \"test_round_trip_overflow.parquet\"\n    pq.write_table(table, filename, use_deprecated_int96_timestamps=True,\n                   version=\"1.0\")\n\n    # with the default resolution of ns, we get wrong values for INT96\n    # that are out of bounds for nanosecond range\n    tab_error = get_table(pq_reader_method, filename)\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\",\n                                \"Discarding nonzero nanoseconds in conversion\",\n                                UserWarning)\n        assert tab_error[\"a\"].to_pylist() != oob_dts\n\n    # avoid this overflow by specifying the resolution to use for INT96 values\n    tab_correct = get_table(\n        pq_reader_method, filename, coerce_int96_timestamp_unit=\"s\"\n    )\n    df_correct = tab_correct.to_pandas(timestamp_as_object=True)\n    df[\"a\"] = df[\"a\"].astype(object)\n    tm.assert_frame_equal(df, df_correct)\n\n\n@pytest.mark.parametrize('unit', ['ms', 'us', 'ns'])\ndef test_timestamp_restore_timezone(unit):\n    # ARROW-5888, restore timezone from serialized metadata\n    ty = pa.timestamp(unit, tz='America/New_York')\n    arr = pa.array([1, 2, 3], type=ty)\n    t = pa.table([arr], names=['f0'])\n    _check_roundtrip(t)\n\n\ndef test_timestamp_restore_timezone_nanosecond():\n    # ARROW-9634, also restore timezone for nanosecond data that get stored\n    # as microseconds in the parquet file for Parquet ver 2.4 and less\n    ty = pa.timestamp('ns', tz='America/New_York')\n    arr = pa.array([1000, 2000, 3000], type=ty)\n    table = pa.table([arr], names=['f0'])\n    ty_us = pa.timestamp('us', tz='America/New_York')\n    expected = pa.table([arr.cast(ty_us)], names=['f0'])\n    _check_roundtrip(table, expected=expected, version='2.4')\n\n\n@pytest.mark.pandas\ndef test_list_of_datetime_time_roundtrip():\n    # ARROW-4135\n    times = pd.to_datetime(['09:00', '09:30', '10:00', '10:30', '11:00',\n                            '11:30', '12:00'], format=\"%H:%M\")\n    df = pd.DataFrame({'time': [times.time]})\n    _roundtrip_pandas_dataframe(df, write_kwargs={})\n\n\n@pytest.mark.pandas\ndef test_parquet_version_timestamp_differences():\n    i_s = pd.Timestamp('2010-01-01').value / 1000000000  # := 1262304000\n\n    d_s = np.arange(i_s, i_s + 10, 1, dtype='int64')\n    d_ms = d_s * 1000\n    d_us = d_ms * 1000\n    d_ns = d_us * 1000\n\n    a_s = pa.array(d_s, type=pa.timestamp('s'))\n    a_ms = pa.array(d_ms, type=pa.timestamp('ms'))\n    a_us = pa.array(d_us, type=pa.timestamp('us'))\n    a_ns = pa.array(d_ns, type=pa.timestamp('ns'))\n\n    all_versions = ['1.0', '2.4', '2.6']\n\n    names = ['ts:s', 'ts:ms', 'ts:us', 'ts:ns']\n    table = pa.Table.from_arrays([a_s, a_ms, a_us, a_ns], names)\n\n    # Using Parquet version 1.0 and 2.4, seconds should be coerced to milliseconds\n    # and nanoseconds should be coerced to microseconds by default\n    expected = pa.Table.from_arrays([a_ms, a_ms, a_us, a_us], names)\n    _check_roundtrip(table, expected, version='1.0')\n    _check_roundtrip(table, expected, version='2.4')\n\n    # Using Parquet version 2.6, seconds should be coerced to milliseconds\n    # and nanoseconds should be retained by default\n    expected = pa.Table.from_arrays([a_ms, a_ms, a_us, a_ns], names)\n    _check_roundtrip(table, expected, version='2.6')\n\n    # For either Parquet version coercing to milliseconds or microseconds\n    # is allowed\n    expected = pa.Table.from_arrays([a_ms, a_ms, a_ms, a_ms], names)\n    for ver in all_versions:\n        _check_roundtrip(table, expected, coerce_timestamps='ms', version=ver)\n\n    expected = pa.Table.from_arrays([a_us, a_us, a_us, a_us], names)\n    for ver in all_versions:\n        _check_roundtrip(table, expected, version=ver, coerce_timestamps='us')\n\n    # TODO: after pyarrow allows coerce_timestamps='ns', tests like the\n    # following should pass ...\n\n    # Using Parquet version 1.0, coercing to nanoseconds is not allowed\n    # expected = None\n    # with pytest.raises(NotImplementedError):\n    #     _roundtrip_table(table, coerce_timestamps='ns')\n\n    # Using Parquet version 2.0, coercing to nanoseconds is allowed\n    # expected = pa.Table.from_arrays([a_ns, a_ns, a_ns, a_ns], names)\n    # _check_roundtrip(table, expected, version='2.6', coerce_timestamps='ns')\n\n    # For either Parquet version, coercing to nanoseconds is allowed\n    # if Int96 storage is used\n    expected = pa.Table.from_arrays([a_ns, a_ns, a_ns, a_ns], names)\n    for ver in all_versions:\n        _check_roundtrip(table, expected, version=ver,\n                         use_deprecated_int96_timestamps=True)\n\n\n@pytest.mark.pandas\ndef test_noncoerced_nanoseconds_written_without_exception(tempdir):\n    # ARROW-1957: the Parquet version 2.0 writer preserves Arrow\n    # nanosecond timestamps by default\n    n = 9\n    df = pd.DataFrame({'x': range(n)},\n                      index=pd.date_range('2017-01-01', freq='ns', periods=n))\n    tb = pa.Table.from_pandas(df)\n\n    filename = tempdir / 'written.parquet'\n    try:\n        pq.write_table(tb, filename, version='2.6')\n    except Exception:\n        pass\n    assert filename.exists()\n\n    recovered_table = pq.read_table(filename)\n    assert tb.equals(recovered_table)\n\n    # Loss of data through coercion (without explicit override) still an error\n    filename = tempdir / 'not_written.parquet'\n    with pytest.raises(ValueError):\n        pq.write_table(tb, filename, coerce_timestamps='ms', version='2.6')\n\n\ndef test_duration_type():\n    # ARROW-6780\n    arrays = [pa.array([0, 1, 2, 3], type=pa.duration(unit))\n              for unit in [\"s\", \"ms\", \"us\", \"ns\"]]\n    table = pa.Table.from_arrays(arrays, [\"d[s]\", \"d[ms]\", \"d[us]\", \"d[ns]\"])\n\n    _check_roundtrip(table)\n", "python/pyarrow/tests/parquet/test_parquet_file.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport io\nimport os\nimport sys\n\nimport pytest\n\nimport pyarrow as pa\n\ntry:\n    import pyarrow.parquet as pq\n    from pyarrow.tests.parquet.common import _write_table\nexcept ImportError:\n    pq = None\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n\n    from pyarrow.tests.parquet.common import alltypes_sample\nexcept ImportError:\n    pd = tm = None\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = pytest.mark.parquet\n\n\n@pytest.mark.pandas\ndef test_pass_separate_metadata():\n    # ARROW-471\n    df = alltypes_sample(size=10000)\n\n    a_table = pa.Table.from_pandas(df)\n\n    buf = io.BytesIO()\n    _write_table(a_table, buf, compression='snappy', version='2.6')\n\n    buf.seek(0)\n    metadata = pq.read_metadata(buf)\n\n    buf.seek(0)\n\n    fileh = pq.ParquetFile(buf, metadata=metadata)\n\n    tm.assert_frame_equal(df, fileh.read().to_pandas())\n\n\n@pytest.mark.pandas\ndef test_read_single_row_group():\n    # ARROW-471\n    N, K = 10000, 4\n    df = alltypes_sample(size=N)\n\n    a_table = pa.Table.from_pandas(df)\n\n    buf = io.BytesIO()\n    _write_table(a_table, buf, row_group_size=N / K,\n                 compression='snappy', version='2.6')\n\n    buf.seek(0)\n\n    pf = pq.ParquetFile(buf)\n\n    assert pf.num_row_groups == K\n\n    row_groups = [pf.read_row_group(i) for i in range(K)]\n    result = pa.concat_tables(row_groups)\n    tm.assert_frame_equal(df, result.to_pandas())\n\n\n@pytest.mark.pandas\ndef test_read_single_row_group_with_column_subset():\n    N, K = 10000, 4\n    df = alltypes_sample(size=N)\n    a_table = pa.Table.from_pandas(df)\n\n    buf = io.BytesIO()\n    _write_table(a_table, buf, row_group_size=N / K,\n                 compression='snappy', version='2.6')\n\n    buf.seek(0)\n    pf = pq.ParquetFile(buf)\n\n    cols = list(df.columns[:2])\n    row_groups = [pf.read_row_group(i, columns=cols) for i in range(K)]\n    result = pa.concat_tables(row_groups)\n    tm.assert_frame_equal(df[cols], result.to_pandas())\n\n    # ARROW-4267: Selection of duplicate columns still leads to these columns\n    # being read uniquely.\n    row_groups = [pf.read_row_group(i, columns=cols + cols) for i in range(K)]\n    result = pa.concat_tables(row_groups)\n    tm.assert_frame_equal(df[cols], result.to_pandas())\n\n\n@pytest.mark.pandas\ndef test_read_multiple_row_groups():\n    N, K = 10000, 4\n    df = alltypes_sample(size=N)\n\n    a_table = pa.Table.from_pandas(df)\n\n    buf = io.BytesIO()\n    _write_table(a_table, buf, row_group_size=N / K,\n                 compression='snappy', version='2.6')\n\n    buf.seek(0)\n\n    pf = pq.ParquetFile(buf)\n\n    assert pf.num_row_groups == K\n\n    result = pf.read_row_groups(range(K))\n    tm.assert_frame_equal(df, result.to_pandas())\n\n\n@pytest.mark.pandas\ndef test_read_multiple_row_groups_with_column_subset():\n    N, K = 10000, 4\n    df = alltypes_sample(size=N)\n    a_table = pa.Table.from_pandas(df)\n\n    buf = io.BytesIO()\n    _write_table(a_table, buf, row_group_size=N / K,\n                 compression='snappy', version='2.6')\n\n    buf.seek(0)\n    pf = pq.ParquetFile(buf)\n\n    cols = list(df.columns[:2])\n    result = pf.read_row_groups(range(K), columns=cols)\n    tm.assert_frame_equal(df[cols], result.to_pandas())\n\n    # ARROW-4267: Selection of duplicate columns still leads to these columns\n    # being read uniquely.\n    result = pf.read_row_groups(range(K), columns=cols + cols)\n    tm.assert_frame_equal(df[cols], result.to_pandas())\n\n\n@pytest.mark.pandas\ndef test_scan_contents():\n    N, K = 10000, 4\n    df = alltypes_sample(size=N)\n    a_table = pa.Table.from_pandas(df)\n\n    buf = io.BytesIO()\n    _write_table(a_table, buf, row_group_size=N / K,\n                 compression='snappy', version='2.6')\n\n    buf.seek(0)\n    pf = pq.ParquetFile(buf)\n\n    assert pf.scan_contents() == 10000\n    assert pf.scan_contents(df.columns[:4]) == 10000\n\n\ndef test_parquet_file_pass_directory_instead_of_file(tempdir):\n    # ARROW-7208\n    path = tempdir / 'directory'\n    os.mkdir(str(path))\n\n    msg = f\"Cannot open for reading: path '{str(path)}' is a directory\"\n    with pytest.raises(IOError) as exc:\n        pq.ParquetFile(path)\n    if exc.errisinstance(PermissionError) and sys.platform == 'win32':\n        return  # Windows CI can get a PermissionError here.\n    exc.match(msg)\n\n\ndef test_read_column_invalid_index():\n    table = pa.table([pa.array([4, 5]), pa.array([\"foo\", \"bar\"])],\n                     names=['ints', 'strs'])\n    bio = pa.BufferOutputStream()\n    pq.write_table(table, bio)\n    f = pq.ParquetFile(bio.getvalue())\n    assert f.reader.read_column(0).to_pylist() == [4, 5]\n    assert f.reader.read_column(1).to_pylist() == [\"foo\", \"bar\"]\n    for index in (-1, 2):\n        with pytest.raises((ValueError, IndexError)):\n            f.reader.read_column(index)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('batch_size', [300, 1000, 1300])\ndef test_iter_batches_columns_reader(tempdir, batch_size):\n    total_size = 3000\n    chunk_size = 1000\n    # TODO: Add categorical support\n    df = alltypes_sample(size=total_size)\n\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df)\n    _write_table(arrow_table, filename, version='2.6',\n                 chunk_size=chunk_size)\n\n    file_ = pq.ParquetFile(filename)\n    for columns in [df.columns[:10], df.columns[10:]]:\n        batches = file_.iter_batches(batch_size=batch_size, columns=columns)\n        batch_starts = range(0, total_size+batch_size, batch_size)\n        for batch, start in zip(batches, batch_starts):\n            end = min(total_size, start + batch_size)\n            tm.assert_frame_equal(\n                batch.to_pandas(),\n                df.iloc[start:end, :].loc[:, columns].reset_index(drop=True)\n            )\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('chunk_size', [1000])\ndef test_iter_batches_reader(tempdir, chunk_size):\n    df = alltypes_sample(size=10000, categorical=True)\n\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df)\n    assert arrow_table.schema.pandas_metadata is not None\n\n    _write_table(arrow_table, filename, version='2.6',\n                 chunk_size=chunk_size)\n\n    file_ = pq.ParquetFile(filename)\n\n    def get_all_batches(f):\n        for row_group in range(f.num_row_groups):\n            batches = f.iter_batches(\n                batch_size=900,\n                row_groups=[row_group],\n            )\n\n            for batch in batches:\n                yield batch\n\n    batches = list(get_all_batches(file_))\n    batch_no = 0\n\n    for i in range(file_.num_row_groups):\n        tm.assert_frame_equal(\n            batches[batch_no].to_pandas(),\n            file_.read_row_groups([i]).to_pandas().head(900)\n        )\n\n        batch_no += 1\n\n        tm.assert_frame_equal(\n            batches[batch_no].to_pandas().reset_index(drop=True),\n            file_.read_row_groups([i]).to_pandas().iloc[900:].reset_index(\n                drop=True\n            )\n        )\n\n        batch_no += 1\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('pre_buffer', [False, True])\ndef test_pre_buffer(pre_buffer):\n    N, K = 10000, 4\n    df = alltypes_sample(size=N)\n    a_table = pa.Table.from_pandas(df)\n\n    buf = io.BytesIO()\n    _write_table(a_table, buf, row_group_size=N / K,\n                 compression='snappy', version='2.6')\n\n    buf.seek(0)\n    pf = pq.ParquetFile(buf, pre_buffer=pre_buffer)\n    assert pf.read().num_rows == N\n\n\ndef test_parquet_file_explicitly_closed(tempdir):\n    \"\"\"\n    Unopened files should be closed explicitly after use,\n    and previously opened files should be left open.\n    Applies to read_table, ParquetDataset, and ParquetFile\n    \"\"\"\n    # create test parquet file\n    fn = tempdir.joinpath('file.parquet')\n    table = pa.table({'col1': [0, 1], 'col2': [0, 1]})\n    pq.write_table(table, fn)\n\n    # ParquetFile with opened file (will leave open)\n    with open(fn, 'rb') as f:\n        with pq.ParquetFile(f) as p:\n            p.read()\n            assert not f.closed\n            assert not p.closed\n        assert not f.closed  # opened input file was not closed\n        assert not p.closed  # parquet file obj reports as not closed\n    assert f.closed\n    assert p.closed  # parquet file being closed reflects underlying file\n\n    # ParquetFile with unopened file (will close)\n    with pq.ParquetFile(fn) as p:\n        p.read()\n        assert not p.closed\n    assert p.closed  # parquet file obj reports as closed\n\n\n@pytest.mark.s3\n@pytest.mark.parametrize(\"use_uri\", (True, False))\ndef test_parquet_file_with_filesystem(s3_example_fs, use_uri):\n    s3_fs, s3_uri, s3_path = s3_example_fs\n\n    args = (s3_uri if use_uri else s3_path,)\n    kwargs = {} if use_uri else dict(filesystem=s3_fs)\n\n    table = pa.table({\"a\": range(10)})\n    pq.write_table(table, s3_path, filesystem=s3_fs)\n\n    parquet_file = pq.ParquetFile(*args, **kwargs)\n    assert parquet_file.read() == table\n    assert not parquet_file.closed\n    parquet_file.close()\n    assert parquet_file.closed\n\n    with pq.ParquetFile(*args, **kwargs) as f:\n        assert f.read() == table\n        assert not f.closed\n    assert f.closed\n", "python/pyarrow/tests/parquet/common.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport io\n\nimport numpy as np\n\nimport pyarrow as pa\nfrom pyarrow.tests import util\n\n\ndef _write_table(table, path, **kwargs):\n    # So we see the ImportError somewhere\n    import pyarrow.parquet as pq\n    from pyarrow.pandas_compat import _pandas_api\n\n    if _pandas_api.is_data_frame(table):\n        table = pa.Table.from_pandas(table)\n\n    pq.write_table(table, path, **kwargs)\n    return table\n\n\ndef _read_table(*args, **kwargs):\n    import pyarrow.parquet as pq\n\n    table = pq.read_table(*args, **kwargs)\n    table.validate(full=True)\n    return table\n\n\ndef _roundtrip_table(table, read_table_kwargs=None,\n                     write_table_kwargs=None):\n    read_table_kwargs = read_table_kwargs or {}\n    write_table_kwargs = write_table_kwargs or {}\n\n    writer = pa.BufferOutputStream()\n    _write_table(table, writer, **write_table_kwargs)\n    reader = pa.BufferReader(writer.getvalue())\n    return _read_table(reader, **read_table_kwargs)\n\n\ndef _check_roundtrip(table, expected=None, read_table_kwargs=None,\n                     **write_table_kwargs):\n    if expected is None:\n        expected = table\n\n    read_table_kwargs = read_table_kwargs or {}\n\n    # intentionally check twice\n    result = _roundtrip_table(table, read_table_kwargs=read_table_kwargs,\n                              write_table_kwargs=write_table_kwargs)\n    assert result.equals(expected)\n    result = _roundtrip_table(result, read_table_kwargs=read_table_kwargs,\n                              write_table_kwargs=write_table_kwargs)\n    assert result.equals(expected)\n\n\ndef _roundtrip_pandas_dataframe(df, write_kwargs):\n    table = pa.Table.from_pandas(df)\n    result = _roundtrip_table(\n        table, write_table_kwargs=write_kwargs)\n    return result.to_pandas()\n\n\ndef _random_integers(size, dtype):\n    # We do not generate integers outside the int64 range\n    platform_int_info = np.iinfo('int_')\n    iinfo = np.iinfo(dtype)\n    return np.random.randint(max(iinfo.min, platform_int_info.min),\n                             min(iinfo.max, platform_int_info.max),\n                             size=size, dtype=dtype)\n\n\ndef _range_integers(size, dtype):\n    return pa.array(np.arange(size, dtype=dtype))\n\n\ndef _test_dataframe(size=10000, seed=0):\n    import pandas as pd\n\n    np.random.seed(seed)\n    df = pd.DataFrame({\n        'uint8': _random_integers(size, np.uint8),\n        'uint16': _random_integers(size, np.uint16),\n        'uint32': _random_integers(size, np.uint32),\n        'uint64': _random_integers(size, np.uint64),\n        'int8': _random_integers(size, np.int8),\n        'int16': _random_integers(size, np.int16),\n        'int32': _random_integers(size, np.int32),\n        'int64': _random_integers(size, np.int64),\n        'float32': np.random.randn(size).astype(np.float32),\n        'float64': np.arange(size, dtype=np.float64),\n        'bool': np.random.randn(size) > 0,\n        'strings': [util.rands(10) for i in range(size)],\n        'all_none': [None] * size,\n        'all_none_category': [None] * size\n    })\n\n    # TODO(PARQUET-1015)\n    # df['all_none_category'] = df['all_none_category'].astype('category')\n    return df\n\n\ndef make_sample_file(table_or_df):\n    import pyarrow.parquet as pq\n\n    if isinstance(table_or_df, pa.Table):\n        a_table = table_or_df\n    else:\n        a_table = pa.Table.from_pandas(table_or_df)\n\n    buf = io.BytesIO()\n    _write_table(a_table, buf, compression='SNAPPY', version='2.6')\n\n    buf.seek(0)\n    return pq.ParquetFile(buf)\n\n\ndef alltypes_sample(size=10000, seed=0, categorical=False):\n    import pandas as pd\n\n    np.random.seed(seed)\n    arrays = {\n        'uint8': np.arange(size, dtype=np.uint8),\n        'uint16': np.arange(size, dtype=np.uint16),\n        'uint32': np.arange(size, dtype=np.uint32),\n        'uint64': np.arange(size, dtype=np.uint64),\n        'int8': np.arange(size, dtype=np.int16),\n        'int16': np.arange(size, dtype=np.int16),\n        'int32': np.arange(size, dtype=np.int32),\n        'int64': np.arange(size, dtype=np.int64),\n        'float16': np.arange(size, dtype=np.float16),\n        'float32': np.arange(size, dtype=np.float32),\n        'float64': np.arange(size, dtype=np.float64),\n        'bool': np.random.randn(size) > 0,\n        'datetime_ms': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                 dtype='datetime64[ms]'),\n        'datetime_us': np.arange(\"2016-01-01T00:00:00.000001\", size,\n                                 dtype='datetime64[us]'),\n        'datetime_ns': np.arange(\"2016-01-01T00:00:00.000000001\", size,\n                                 dtype='datetime64[ns]'),\n        'timedelta': np.arange(0, size, dtype=\"timedelta64[s]\"),\n        'str': pd.Series([str(x) for x in range(size)]),\n        'empty_str': [''] * size,\n        'str_with_nulls': [None] + [str(x) for x in range(size - 2)] + [None],\n        'null': [None] * size,\n        'null_list': [None] * 2 + [[None] * (x % 4) for x in range(size - 2)],\n    }\n    if categorical:\n        arrays['str_category'] = arrays['str'].astype('category')\n    return pd.DataFrame(arrays)\n", "python/pyarrow/tests/parquet/test_pandas.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport io\nimport json\n\nimport numpy as np\nimport pytest\n\nimport pyarrow as pa\nfrom pyarrow.fs import LocalFileSystem, SubTreeFileSystem\nfrom pyarrow.util import guid\nfrom pyarrow.vendored.version import Version\n\ntry:\n    import pyarrow.parquet as pq\n    from pyarrow.tests.parquet.common import (_read_table, _test_dataframe,\n                                              _write_table)\nexcept ImportError:\n    pq = None\n\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n\n    from pyarrow.tests.parquet.common import (_roundtrip_pandas_dataframe,\n                                              alltypes_sample)\nexcept ImportError:\n    pd = tm = None\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = pytest.mark.parquet\n\n\n@pytest.mark.pandas\ndef test_pandas_parquet_custom_metadata(tempdir):\n    df = alltypes_sample(size=10000)\n\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df)\n    assert b'pandas' in arrow_table.schema.metadata\n\n    _write_table(arrow_table, filename)\n\n    metadata = pq.read_metadata(filename).metadata\n    assert b'pandas' in metadata\n\n    js = json.loads(metadata[b'pandas'].decode('utf8'))\n    assert js['index_columns'] == [{'kind': 'range',\n                                    'name': None,\n                                    'start': 0, 'stop': 10000,\n                                    'step': 1}]\n\n\n@pytest.mark.pandas\ndef test_merging_parquet_tables_with_different_pandas_metadata(tempdir):\n    # ARROW-3728: Merging Parquet Files - Pandas Meta in Schema Mismatch\n    schema = pa.schema([\n        pa.field('int', pa.int16()),\n        pa.field('float', pa.float32()),\n        pa.field('string', pa.string())\n    ])\n    df1 = pd.DataFrame({\n        'int': np.arange(3, dtype=np.uint8),\n        'float': np.arange(3, dtype=np.float32),\n        'string': ['ABBA', 'EDDA', 'ACDC']\n    })\n    df2 = pd.DataFrame({\n        'int': [4, 5],\n        'float': [1.1, None],\n        'string': [None, None]\n    })\n    table1 = pa.Table.from_pandas(df1, schema=schema, preserve_index=False)\n    table2 = pa.Table.from_pandas(df2, schema=schema, preserve_index=False)\n\n    assert not table1.schema.equals(table2.schema, check_metadata=True)\n    assert table1.schema.equals(table2.schema)\n\n    writer = pq.ParquetWriter(tempdir / 'merged.parquet', schema=schema)\n    writer.write_table(table1)\n    writer.write_table(table2)\n\n\n@pytest.mark.pandas\ndef test_pandas_parquet_column_multiindex(tempdir):\n    df = alltypes_sample(size=10)\n    df.columns = pd.MultiIndex.from_tuples(\n        list(zip(df.columns, df.columns[::-1])),\n        names=['level_1', 'level_2']\n    )\n\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df)\n    assert arrow_table.schema.pandas_metadata is not None\n\n    _write_table(arrow_table, filename)\n\n    table_read = pq.read_pandas(filename)\n    df_read = table_read.to_pandas()\n    tm.assert_frame_equal(df, df_read)\n\n\n@pytest.mark.pandas\ndef test_pandas_parquet_2_0_roundtrip_read_pandas_no_index_written(tempdir):\n    df = alltypes_sample(size=10000)\n\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n    js = arrow_table.schema.pandas_metadata\n    assert not js['index_columns']\n    # ARROW-2170\n    # While index_columns should be empty, columns needs to be filled still.\n    assert js['columns']\n\n    _write_table(arrow_table, filename)\n    table_read = pq.read_pandas(filename)\n\n    js = table_read.schema.pandas_metadata\n    assert not js['index_columns']\n\n    read_metadata = table_read.schema.metadata\n    assert arrow_table.schema.metadata == read_metadata\n\n    df_read = table_read.to_pandas()\n    tm.assert_frame_equal(df, df_read)\n\n\n@pytest.mark.pandas\ndef test_pandas_parquet_native_file_roundtrip():\n    df = _test_dataframe(10000)\n    arrow_table = pa.Table.from_pandas(df)\n    imos = pa.BufferOutputStream()\n    _write_table(arrow_table, imos, version='2.6')\n    buf = imos.getvalue()\n    reader = pa.BufferReader(buf)\n    df_read = _read_table(reader).to_pandas()\n    tm.assert_frame_equal(df, df_read)\n\n\n@pytest.mark.pandas\ndef test_read_pandas_column_subset():\n    df = _test_dataframe(10000)\n    arrow_table = pa.Table.from_pandas(df)\n    imos = pa.BufferOutputStream()\n    _write_table(arrow_table, imos, version='2.6')\n    buf = imos.getvalue()\n    reader = pa.BufferReader(buf)\n    df_read = pq.read_pandas(\n        reader, columns=['strings', 'uint8'],\n    ).to_pandas()\n    tm.assert_frame_equal(df[['strings', 'uint8']], df_read)\n\n\n@pytest.mark.pandas\ndef test_pandas_parquet_empty_roundtrip():\n    df = _test_dataframe(0)\n    arrow_table = pa.Table.from_pandas(df)\n    imos = pa.BufferOutputStream()\n    _write_table(arrow_table, imos, version='2.6')\n    buf = imos.getvalue()\n    reader = pa.BufferReader(buf)\n    df_read = _read_table(reader).to_pandas()\n    tm.assert_frame_equal(df, df_read)\n\n\n@pytest.mark.pandas\ndef test_pandas_can_write_nested_data():\n    data = {\n        \"agg_col\": [\n            {\"page_type\": 1},\n            {\"record_type\": 1},\n            {\"non_consecutive_home\": 0},\n        ],\n        \"uid_first\": \"1001\"\n    }\n    df = pd.DataFrame(data=data)\n    arrow_table = pa.Table.from_pandas(df)\n    imos = pa.BufferOutputStream()\n    # This succeeds under V2\n    _write_table(arrow_table, imos)\n\n\n@pytest.mark.pandas\ndef test_pandas_parquet_pyfile_roundtrip(tempdir):\n    filename = tempdir / 'pandas_pyfile_roundtrip.parquet'\n    size = 5\n    df = pd.DataFrame({\n        'int64': np.arange(size, dtype=np.int64),\n        'float32': np.arange(size, dtype=np.float32),\n        'float64': np.arange(size, dtype=np.float64),\n        'bool': np.random.randn(size) > 0,\n        'strings': ['foo', 'bar', None, 'baz', 'qux']\n    })\n\n    arrow_table = pa.Table.from_pandas(df)\n\n    with filename.open('wb') as f:\n        _write_table(arrow_table, f, version=\"2.6\")\n\n    data = io.BytesIO(filename.read_bytes())\n\n    table_read = _read_table(data)\n    df_read = table_read.to_pandas()\n    tm.assert_frame_equal(df, df_read)\n\n\n@pytest.mark.pandas\ndef test_pandas_parquet_configuration_options(tempdir):\n    size = 10000\n    np.random.seed(0)\n    df = pd.DataFrame({\n        'uint8': np.arange(size, dtype=np.uint8),\n        'uint16': np.arange(size, dtype=np.uint16),\n        'uint32': np.arange(size, dtype=np.uint32),\n        'uint64': np.arange(size, dtype=np.uint64),\n        'int8': np.arange(size, dtype=np.int16),\n        'int16': np.arange(size, dtype=np.int16),\n        'int32': np.arange(size, dtype=np.int32),\n        'int64': np.arange(size, dtype=np.int64),\n        'float32': np.arange(size, dtype=np.float32),\n        'float64': np.arange(size, dtype=np.float64),\n        'bool': np.random.randn(size) > 0\n    })\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df)\n\n    for use_dictionary in [True, False]:\n        _write_table(arrow_table, filename, version='2.6',\n                     use_dictionary=use_dictionary)\n        table_read = _read_table(filename)\n        df_read = table_read.to_pandas()\n        tm.assert_frame_equal(df, df_read)\n\n    for write_statistics in [True, False]:\n        _write_table(arrow_table, filename, version='2.6',\n                     write_statistics=write_statistics)\n        table_read = _read_table(filename)\n        df_read = table_read.to_pandas()\n        tm.assert_frame_equal(df, df_read)\n\n    for compression in ['NONE', 'SNAPPY', 'GZIP', 'LZ4', 'ZSTD']:\n        if (compression != 'NONE' and\n                not pa.lib.Codec.is_available(compression)):\n            continue\n        _write_table(arrow_table, filename, version='2.6',\n                     compression=compression)\n        table_read = _read_table(filename)\n        df_read = table_read.to_pandas()\n        tm.assert_frame_equal(df, df_read)\n\n\n@pytest.mark.pandas\n@pytest.mark.filterwarnings(\"ignore:Parquet format '2.0':FutureWarning\")\ndef test_spark_flavor_preserves_pandas_metadata():\n    df = _test_dataframe(size=100)\n    df.index = np.arange(0, 10 * len(df), 10)\n    df.index.name = 'foo'\n\n    result = _roundtrip_pandas_dataframe(df, {'version': '2.0',\n                                              'flavor': 'spark'})\n    tm.assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\ndef test_index_column_name_duplicate(tempdir):\n    data = {\n        'close': {\n            pd.Timestamp('2017-06-30 01:31:00'): 154.99958999999998,\n            pd.Timestamp('2017-06-30 01:32:00'): 154.99958999999998,\n        },\n        'time': {\n            pd.Timestamp('2017-06-30 01:31:00'): pd.Timestamp(\n                '2017-06-30 01:31:00'\n            ),\n            pd.Timestamp('2017-06-30 01:32:00'): pd.Timestamp(\n                '2017-06-30 01:32:00'\n            ),\n        }\n    }\n    path = str(tempdir / 'data.parquet')\n\n    # Pandas v2 defaults to [ns], but Arrow defaults to [us] time units\n    # so we need to cast the pandas dtype. Pandas v1 will always silently\n    # coerce to [ns] due to lack of non-[ns] support.\n    dfx = pd.DataFrame(data, dtype='datetime64[us]').set_index('time', drop=False)\n\n    tdfx = pa.Table.from_pandas(dfx)\n    _write_table(tdfx, path)\n    arrow_table = _read_table(path)\n    result_df = arrow_table.to_pandas()\n    tm.assert_frame_equal(result_df, dfx)\n\n\n@pytest.mark.pandas\ndef test_multiindex_duplicate_values(tempdir):\n    num_rows = 3\n    numbers = list(range(num_rows))\n    index = pd.MultiIndex.from_arrays(\n        [['foo', 'foo', 'bar'], numbers],\n        names=['foobar', 'some_numbers'],\n    )\n\n    df = pd.DataFrame({'numbers': numbers}, index=index)\n    table = pa.Table.from_pandas(df)\n\n    filename = tempdir / 'dup_multi_index_levels.parquet'\n\n    _write_table(table, filename)\n    result_table = _read_table(filename)\n    assert table.equals(result_table)\n\n    result_df = result_table.to_pandas()\n    tm.assert_frame_equal(result_df, df)\n\n\n@pytest.mark.pandas\ndef test_backwards_compatible_index_naming(datadir):\n    expected_string = b\"\"\"\\\ncarat        cut  color  clarity  depth  table  price     x     y     z\n 0.23      Ideal      E      SI2   61.5   55.0    326  3.95  3.98  2.43\n 0.21    Premium      E      SI1   59.8   61.0    326  3.89  3.84  2.31\n 0.23       Good      E      VS1   56.9   65.0    327  4.05  4.07  2.31\n 0.29    Premium      I      VS2   62.4   58.0    334  4.20  4.23  2.63\n 0.31       Good      J      SI2   63.3   58.0    335  4.34  4.35  2.75\n 0.24  Very Good      J     VVS2   62.8   57.0    336  3.94  3.96  2.48\n 0.24  Very Good      I     VVS1   62.3   57.0    336  3.95  3.98  2.47\n 0.26  Very Good      H      SI1   61.9   55.0    337  4.07  4.11  2.53\n 0.22       Fair      E      VS2   65.1   61.0    337  3.87  3.78  2.49\n 0.23  Very Good      H      VS1   59.4   61.0    338  4.00  4.05  2.39\"\"\"\n    expected = pd.read_csv(io.BytesIO(expected_string), sep=r'\\s{2,}',\n                           index_col=None, header=0, engine='python')\n    table = _read_table(datadir / 'v0.7.1.parquet')\n    result = table.to_pandas()\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_backwards_compatible_index_multi_level_named(datadir):\n    expected_string = b\"\"\"\\\ncarat        cut  color  clarity  depth  table  price     x     y     z\n 0.23      Ideal      E      SI2   61.5   55.0    326  3.95  3.98  2.43\n 0.21    Premium      E      SI1   59.8   61.0    326  3.89  3.84  2.31\n 0.23       Good      E      VS1   56.9   65.0    327  4.05  4.07  2.31\n 0.29    Premium      I      VS2   62.4   58.0    334  4.20  4.23  2.63\n 0.31       Good      J      SI2   63.3   58.0    335  4.34  4.35  2.75\n 0.24  Very Good      J     VVS2   62.8   57.0    336  3.94  3.96  2.48\n 0.24  Very Good      I     VVS1   62.3   57.0    336  3.95  3.98  2.47\n 0.26  Very Good      H      SI1   61.9   55.0    337  4.07  4.11  2.53\n 0.22       Fair      E      VS2   65.1   61.0    337  3.87  3.78  2.49\n 0.23  Very Good      H      VS1   59.4   61.0    338  4.00  4.05  2.39\"\"\"\n    expected = pd.read_csv(\n        io.BytesIO(expected_string), sep=r'\\s{2,}',\n        index_col=['cut', 'color', 'clarity'],\n        header=0, engine='python'\n    ).sort_index()\n\n    table = _read_table(datadir / 'v0.7.1.all-named-index.parquet')\n    result = table.to_pandas()\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_backwards_compatible_index_multi_level_some_named(datadir):\n    expected_string = b\"\"\"\\\ncarat        cut  color  clarity  depth  table  price     x     y     z\n 0.23      Ideal      E      SI2   61.5   55.0    326  3.95  3.98  2.43\n 0.21    Premium      E      SI1   59.8   61.0    326  3.89  3.84  2.31\n 0.23       Good      E      VS1   56.9   65.0    327  4.05  4.07  2.31\n 0.29    Premium      I      VS2   62.4   58.0    334  4.20  4.23  2.63\n 0.31       Good      J      SI2   63.3   58.0    335  4.34  4.35  2.75\n 0.24  Very Good      J     VVS2   62.8   57.0    336  3.94  3.96  2.48\n 0.24  Very Good      I     VVS1   62.3   57.0    336  3.95  3.98  2.47\n 0.26  Very Good      H      SI1   61.9   55.0    337  4.07  4.11  2.53\n 0.22       Fair      E      VS2   65.1   61.0    337  3.87  3.78  2.49\n 0.23  Very Good      H      VS1   59.4   61.0    338  4.00  4.05  2.39\"\"\"\n    expected = pd.read_csv(\n        io.BytesIO(expected_string),\n        sep=r'\\s{2,}', index_col=['cut', 'color', 'clarity'],\n        header=0, engine='python'\n    ).sort_index()\n    expected.index = expected.index.set_names(['cut', None, 'clarity'])\n\n    table = _read_table(datadir / 'v0.7.1.some-named-index.parquet')\n    result = table.to_pandas()\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_backwards_compatible_column_metadata_handling(datadir):\n    if Version(\"2.2.0\") <= Version(pd.__version__):\n        # TODO: regression in pandas\n        # https://github.com/pandas-dev/pandas/issues/56775\n        pytest.skip(\"Regression in pandas 2.2.0\")\n    expected = pd.DataFrame(\n        {'a': [1, 2, 3], 'b': [.1, .2, .3],\n         'c': pd.date_range(\"2017-01-01\", periods=3, tz='Europe/Brussels')})\n    expected.index = pd.MultiIndex.from_arrays(\n        [['a', 'b', 'c'],\n         pd.date_range(\"2017-01-01\", periods=3, tz='Europe/Brussels')],\n        names=['index', None])\n\n    path = datadir / 'v0.7.1.column-metadata-handling.parquet'\n    table = _read_table(path)\n    result = table.to_pandas()\n    tm.assert_frame_equal(result, expected)\n\n    table = _read_table(\n        path, columns=['a'])\n    result = table.to_pandas()\n    tm.assert_frame_equal(result, expected[['a']].reset_index(drop=True))\n\n\n@pytest.mark.pandas\ndef test_categorical_index_survives_roundtrip():\n    # ARROW-3652, addressed by ARROW-3246\n    df = pd.DataFrame([['a', 'b'], ['c', 'd']], columns=['c1', 'c2'])\n    df['c1'] = df['c1'].astype('category')\n    df = df.set_index(['c1'])\n\n    table = pa.Table.from_pandas(df)\n    bos = pa.BufferOutputStream()\n    pq.write_table(table, bos)\n    ref_df = pq.read_pandas(bos.getvalue()).to_pandas()\n    assert isinstance(ref_df.index, pd.CategoricalIndex)\n    assert ref_df.index.equals(df.index)\n\n\n@pytest.mark.pandas\ndef test_categorical_order_survives_roundtrip():\n    # ARROW-6302\n    df = pd.DataFrame({\"a\": pd.Categorical(\n        [\"a\", \"b\", \"c\", \"a\"], categories=[\"b\", \"c\", \"d\"], ordered=True)})\n\n    table = pa.Table.from_pandas(df)\n    bos = pa.BufferOutputStream()\n    pq.write_table(table, bos)\n\n    contents = bos.getvalue()\n    result = pq.read_pandas(contents).to_pandas()\n\n    tm.assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\ndef test_pandas_categorical_na_type_row_groups():\n    # ARROW-5085\n    df = pd.DataFrame({\"col\": [None] * 100, \"int\": [1.0] * 100})\n    df_category = df.astype({\"col\": \"category\", \"int\": \"category\"})\n    table = pa.Table.from_pandas(df)\n    table_cat = pa.Table.from_pandas(df_category)\n    buf = pa.BufferOutputStream()\n\n    # it works\n    pq.write_table(table_cat, buf, version='2.6', chunk_size=10)\n    result = pq.read_table(buf.getvalue())\n\n    # Result is non-categorical\n    assert result[0].equals(table[0])\n    assert result[1].equals(table[1])\n\n\n@pytest.mark.pandas\ndef test_pandas_categorical_roundtrip():\n    # ARROW-5480, this was enabled by ARROW-3246\n\n    # Have one of the categories unobserved and include a null (-1)\n    codes = np.array([2, 0, 0, 2, 0, -1, 2], dtype='int32')\n    categories = ['foo', 'bar', 'baz']\n    df = pd.DataFrame({'x': pd.Categorical.from_codes(\n        codes, categories=categories)})\n\n    buf = pa.BufferOutputStream()\n    pq.write_table(pa.table(df), buf)\n\n    result = pq.read_table(buf.getvalue()).to_pandas()\n    assert result.x.dtype == 'category'\n    assert (result.x.cat.categories == categories).all()\n    tm.assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\ndef test_categories_with_string_pyarrow_dtype(tempdir):\n    # gh-33727: writing to parquet should not fail\n    if Version(pd.__version__) < Version(\"1.3.0\"):\n        pytest.skip(\"PyArrow backed string data type introduced in pandas 1.3.0\")\n\n    df1 = pd.DataFrame({\"x\": [\"foo\", \"bar\", \"foo\"]}, dtype=\"string[pyarrow]\")\n    df1 = df1.astype(\"category\")\n\n    df2 = pd.DataFrame({\"x\": [\"foo\", \"bar\", \"foo\"]})\n    df2 = df2.astype(\"category\")\n\n    # categories should be converted to pa.Array\n    assert pa.array(df1[\"x\"]).to_pylist() == pa.array(df2[\"x\"]).to_pylist()\n    assert pa.array(df1[\"x\"].cat.categories.values).to_pylist() == pa.array(\n        df2[\"x\"].cat.categories.values).to_pylist()\n\n    path = str(tempdir / 'cat.parquet')\n    pq.write_table(pa.table(df1), path)\n    result = pq.read_table(path).to_pandas()\n\n    tm.assert_frame_equal(result, df2)\n\n\n@pytest.mark.pandas\ndef test_write_to_dataset_pandas_preserve_extensiondtypes(tempdir):\n    df = pd.DataFrame({'part': 'a', \"col\": [1, 2, 3]})\n    df['col'] = df['col'].astype(\"Int64\")\n    table = pa.table(df)\n\n    pq.write_to_dataset(\n        table, str(tempdir / \"case1\"), partition_cols=['part'],\n    )\n    result = pq.read_table(str(tempdir / \"case1\")).to_pandas()\n    tm.assert_frame_equal(result[[\"col\"]], df[[\"col\"]])\n\n    pq.write_to_dataset(table, str(tempdir / \"case2\"))\n    result = pq.read_table(str(tempdir / \"case2\")).to_pandas()\n    tm.assert_frame_equal(result[[\"col\"]], df[[\"col\"]])\n\n    pq.write_table(table, str(tempdir / \"data.parquet\"))\n    result = pq.read_table(str(tempdir / \"data.parquet\")).to_pandas()\n    tm.assert_frame_equal(result[[\"col\"]], df[[\"col\"]])\n\n\n@pytest.mark.pandas\ndef test_write_to_dataset_pandas_preserve_index(tempdir):\n    # ARROW-8251 - preserve pandas index in roundtrip\n\n    df = pd.DataFrame({'part': ['a', 'a', 'b'], \"col\": [1, 2, 3]})\n    df.index = pd.Index(['a', 'b', 'c'], name=\"idx\")\n    table = pa.table(df)\n    df_cat = df[[\"col\", \"part\"]].copy()\n    df_cat[\"part\"] = df_cat[\"part\"].astype(\"category\")\n\n    pq.write_to_dataset(\n        table, str(tempdir / \"case1\"), partition_cols=['part'],\n    )\n    result = pq.read_table(str(tempdir / \"case1\")).to_pandas()\n    tm.assert_frame_equal(result, df_cat)\n\n    pq.write_to_dataset(table, str(tempdir / \"case2\"))\n    result = pq.read_table(str(tempdir / \"case2\")).to_pandas()\n    tm.assert_frame_equal(result, df)\n\n    pq.write_table(table, str(tempdir / \"data.parquet\"))\n    result = pq.read_table(str(tempdir / \"data.parquet\")).to_pandas()\n    tm.assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('preserve_index', [True, False, None])\n@pytest.mark.parametrize('metadata_fname', [\"_metadata\", \"_common_metadata\"])\ndef test_dataset_read_pandas_common_metadata(\n    tempdir, preserve_index, metadata_fname\n):\n    # ARROW-1103\n    nfiles = 5\n    size = 5\n\n    dirpath = tempdir / guid()\n    dirpath.mkdir()\n\n    test_data = []\n    frames = []\n    paths = []\n    for i in range(nfiles):\n        df = _test_dataframe(size, seed=i)\n        df.index = pd.Index(\n            np.arange(i * size, (i + 1) * size, dtype=\"int64\"), name='index'\n        )\n\n        path = dirpath / '{}.parquet'.format(i)\n\n        table = pa.Table.from_pandas(df, preserve_index=preserve_index)\n\n        # Obliterate metadata\n        table = table.replace_schema_metadata(None)\n        assert table.schema.metadata is None\n\n        _write_table(table, path)\n        test_data.append(table)\n        frames.append(df)\n        paths.append(path)\n\n    # Write _metadata common file\n    table_for_metadata = pa.Table.from_pandas(\n        df, preserve_index=preserve_index\n    )\n    pq.write_metadata(table_for_metadata.schema, dirpath / metadata_fname)\n\n    dataset = pq.ParquetDataset(dirpath)\n    columns = ['uint8', 'strings']\n    result = dataset.read_pandas(columns=columns).to_pandas()\n    expected = pd.concat([x[columns] for x in frames])\n    expected.index.name = (\n        df.index.name if preserve_index is not False else None)\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_read_pandas_passthrough_keywords(tempdir):\n    # ARROW-11464 - previously not all keywords were passed through (such as\n    # the filesystem keyword)\n    df = pd.DataFrame({'a': [1, 2, 3]})\n\n    filename = tempdir / 'data.parquet'\n    _write_table(df, filename)\n\n    result = pq.read_pandas(\n        'data.parquet',\n        filesystem=SubTreeFileSystem(str(tempdir), LocalFileSystem())\n    )\n    assert result.equals(pa.table(df))\n\n\n@pytest.mark.pandas\ndef test_read_pandas_map_fields(tempdir):\n    # ARROW-10140 - table created from Pandas with mapping fields\n    df = pd.DataFrame({\n        'col1': pd.Series([\n            [('id', 'something'), ('value2', 'else')],\n            [('id', 'something2'), ('value', 'else2')],\n        ]),\n        'col2': pd.Series(['foo', 'bar'])\n    })\n\n    filename = tempdir / 'data.parquet'\n\n    udt = pa.map_(pa.string(), pa.string())\n    schema = pa.schema([pa.field('col1', udt), pa.field('col2', pa.string())])\n    arrow_table = pa.Table.from_pandas(df, schema)\n\n    _write_table(arrow_table, filename)\n\n    result = pq.read_pandas(filename).to_pandas()\n    tm.assert_frame_equal(result, df)\n", "python/pyarrow/tests/parquet/test_basic.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import OrderedDict\nimport io\nimport warnings\nfrom shutil import copytree\n\nimport numpy as np\nimport pytest\n\nimport pyarrow as pa\nfrom pyarrow import fs\nfrom pyarrow.tests import util\nfrom pyarrow.tests.parquet.common import (_check_roundtrip, _roundtrip_table,\n                                          _test_dataframe)\n\ntry:\n    import pyarrow.parquet as pq\n    from pyarrow.tests.parquet.common import _read_table, _write_table\nexcept ImportError:\n    pq = None\n\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n\n    from pyarrow.tests.pandas_examples import dataframe_with_lists\n    from pyarrow.tests.parquet.common import alltypes_sample\nexcept ImportError:\n    pd = tm = None\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = pytest.mark.parquet\n\n\ndef test_parquet_invalid_version(tempdir):\n    table = pa.table({'a': [1, 2, 3]})\n    with pytest.raises(ValueError, match=\"Unsupported Parquet format version\"):\n        _write_table(table, tempdir / 'test_version.parquet', version=\"2.2\")\n    with pytest.raises(ValueError, match=\"Unsupported Parquet data page \" +\n                       \"version\"):\n        _write_table(table, tempdir / 'test_version.parquet',\n                     data_page_version=\"2.2\")\n\n\ndef test_set_data_page_size():\n    arr = pa.array([1, 2, 3] * 100000)\n    t = pa.Table.from_arrays([arr], names=['f0'])\n\n    # 128K, 512K\n    page_sizes = [2 << 16, 2 << 18]\n    for target_page_size in page_sizes:\n        _check_roundtrip(t, data_page_size=target_page_size)\n\n\n@pytest.mark.pandas\ndef test_set_write_batch_size():\n    df = _test_dataframe(100)\n    table = pa.Table.from_pandas(df, preserve_index=False)\n\n    _check_roundtrip(\n        table, data_page_size=10, write_batch_size=1, version='2.4'\n    )\n\n\n@pytest.mark.pandas\ndef test_set_dictionary_pagesize_limit():\n    df = _test_dataframe(100)\n    table = pa.Table.from_pandas(df, preserve_index=False)\n\n    _check_roundtrip(table, dictionary_pagesize_limit=1,\n                     data_page_size=10, version='2.4')\n\n    with pytest.raises(TypeError):\n        _check_roundtrip(table, dictionary_pagesize_limit=\"a\",\n                         data_page_size=10, version='2.4')\n\n\n@pytest.mark.pandas\ndef test_chunked_table_write():\n    # ARROW-232\n    tables = []\n    batch = pa.RecordBatch.from_pandas(alltypes_sample(size=10))\n    tables.append(pa.Table.from_batches([batch] * 3))\n    df, _ = dataframe_with_lists()\n    batch = pa.RecordBatch.from_pandas(df)\n    tables.append(pa.Table.from_batches([batch] * 3))\n\n    for data_page_version in ['1.0', '2.0']:\n        for use_dictionary in [True, False]:\n            for table in tables:\n                _check_roundtrip(\n                    table, version='2.6',\n                    data_page_version=data_page_version,\n                    use_dictionary=use_dictionary)\n\n\n@pytest.mark.pandas\ndef test_memory_map(tempdir):\n    df = alltypes_sample(size=10)\n\n    table = pa.Table.from_pandas(df)\n    _check_roundtrip(table, read_table_kwargs={'memory_map': True},\n                     version='2.6')\n\n    filename = str(tempdir / 'tmp_file')\n    with open(filename, 'wb') as f:\n        _write_table(table, f, version='2.6')\n    table_read = pq.read_pandas(filename, memory_map=True)\n    assert table_read.equals(table)\n\n\n@pytest.mark.pandas\ndef test_enable_buffered_stream(tempdir):\n    df = alltypes_sample(size=10)\n\n    table = pa.Table.from_pandas(df)\n    _check_roundtrip(table, read_table_kwargs={'buffer_size': 1025},\n                     version='2.6')\n\n    filename = str(tempdir / 'tmp_file')\n    with open(filename, 'wb') as f:\n        _write_table(table, f, version='2.6')\n    table_read = pq.read_pandas(filename, buffer_size=4096)\n    assert table_read.equals(table)\n\n\ndef test_special_chars_filename(tempdir):\n    table = pa.Table.from_arrays([pa.array([42])], [\"ints\"])\n    filename = \"foo # bar\"\n    path = tempdir / filename\n    assert not path.exists()\n    _write_table(table, str(path))\n    assert path.exists()\n    table_read = _read_table(str(path))\n    assert table_read.equals(table)\n\n\ndef test_invalid_source():\n    # Test that we provide an helpful error message pointing out\n    # that None wasn't expected when trying to open a Parquet None file.\n    with pytest.raises(TypeError, match=\"None\"):\n        pq.read_table(None)\n\n    with pytest.raises(TypeError, match=\"None\"):\n        pq.ParquetFile(None)\n\n\n@pytest.mark.slow\ndef test_file_with_over_int16_max_row_groups():\n    # PARQUET-1857: Parquet encryption support introduced a INT16_MAX upper\n    # limit on the number of row groups, but this limit only impacts files with\n    # encrypted row group metadata because of the int16 row group ordinal used\n    # in the Parquet Thrift metadata. Unencrypted files are not impacted, so\n    # this test checks that it works (even if it isn't a good idea)\n    t = pa.table([list(range(40000))], names=['f0'])\n    _check_roundtrip(t, row_group_size=1)\n\n\n@pytest.mark.pandas\ndef test_empty_table_roundtrip():\n    df = alltypes_sample(size=10)\n\n    # Create a non-empty table to infer the types correctly, then slice to 0\n    table = pa.Table.from_pandas(df)\n    table = pa.Table.from_arrays(\n        [col.chunk(0)[:0] for col in table.itercolumns()],\n        names=table.schema.names)\n\n    assert table.schema.field('null').type == pa.null()\n    assert table.schema.field('null_list').type == pa.list_(pa.null())\n    _check_roundtrip(\n        table, version='2.6')\n\n\n@pytest.mark.pandas\ndef test_empty_table_no_columns():\n    df = pd.DataFrame()\n    empty = pa.Table.from_pandas(df, preserve_index=False)\n    _check_roundtrip(empty)\n\n\ndef test_write_nested_zero_length_array_chunk_failure():\n    # Bug report in ARROW-3792\n    cols = OrderedDict(\n        int32=pa.int32(),\n        list_string=pa.list_(pa.string())\n    )\n    data = [[], [OrderedDict(int32=1, list_string=('G',)), ]]\n\n    # This produces a table with a column like\n    # <Column name='list_string' type=ListType(list<item: string>)>\n    # [\n    #   [],\n    #   [\n    #     [\n    #       \"G\"\n    #     ]\n    #   ]\n    # ]\n    #\n    # Each column is a ChunkedArray with 2 elements\n    my_arrays = [pa.array(batch, type=pa.struct(cols)).flatten()\n                 for batch in data]\n    my_batches = [pa.RecordBatch.from_arrays(batch, schema=pa.schema(cols))\n                  for batch in my_arrays]\n    tbl = pa.Table.from_batches(my_batches, pa.schema(cols))\n    _check_roundtrip(tbl)\n\n\n@pytest.mark.pandas\ndef test_multiple_path_types(tempdir):\n    # Test compatibility with PEP 519 path-like objects\n    path = tempdir / 'zzz.parquet'\n    df = pd.DataFrame({'x': np.arange(10, dtype=np.int64)})\n    _write_table(df, path)\n    table_read = _read_table(path)\n    df_read = table_read.to_pandas()\n    tm.assert_frame_equal(df, df_read)\n\n    # Test compatibility with plain string paths\n    path = str(tempdir) + 'zzz.parquet'\n    df = pd.DataFrame({'x': np.arange(10, dtype=np.int64)})\n    _write_table(df, path)\n    table_read = _read_table(path)\n    df_read = table_read.to_pandas()\n    tm.assert_frame_equal(df, df_read)\n\n\ndef test_fspath(tempdir):\n    # ARROW-12472 support __fspath__ objects without using str()\n    path = tempdir / \"test.parquet\"\n    table = pa.table({\"a\": [1, 2, 3]})\n    _write_table(table, path)\n\n    fs_protocol_obj = util.FSProtocolClass(path)\n\n    result = _read_table(fs_protocol_obj)\n    assert result.equals(table)\n\n    # combined with non-local filesystem raises\n    with pytest.raises(TypeError):\n        _read_table(fs_protocol_obj, filesystem=fs.FileSystem())\n\n\n@pytest.mark.parametrize(\"filesystem\", [\n    None, fs.LocalFileSystem()\n])\n@pytest.mark.parametrize(\"name\", (\"data.parquet\", \"\u4f8b.parquet\"))\ndef test_relative_paths(tempdir, filesystem, name):\n    # reading and writing from relative paths\n    table = pa.table({\"a\": [1, 2, 3]})\n    path = tempdir / name\n\n    # reading\n    pq.write_table(table, str(path))\n    with util.change_cwd(tempdir):\n        result = pq.read_table(name, filesystem=filesystem)\n    assert result.equals(table)\n\n    path.unlink()\n    assert not path.exists()\n\n    # writing\n    with util.change_cwd(tempdir):\n        pq.write_table(table, name, filesystem=filesystem)\n    result = pq.read_table(path)\n    assert result.equals(table)\n\n\ndef test_read_non_existing_file():\n    # ensure we have a proper error message\n    with pytest.raises(FileNotFoundError):\n        pq.read_table('i-am-not-existing.parquet')\n\n\ndef test_file_error_python_exception():\n    class BogusFile(io.BytesIO):\n        def read(self, *args):\n            raise ZeroDivisionError(\"zorglub\")\n\n        def seek(self, *args):\n            raise ZeroDivisionError(\"zorglub\")\n\n    # ensure the Python exception is restored\n    with pytest.raises(ZeroDivisionError, match=\"zorglub\"):\n        pq.read_table(BogusFile(b\"\"))\n\n\ndef test_parquet_read_from_buffer(tempdir):\n    # reading from a buffer from python's open()\n    table = pa.table({\"a\": [1, 2, 3]})\n    pq.write_table(table, str(tempdir / \"data.parquet\"))\n\n    with open(str(tempdir / \"data.parquet\"), \"rb\") as f:\n        result = pq.read_table(f)\n    assert result.equals(table)\n\n    with open(str(tempdir / \"data.parquet\"), \"rb\") as f:\n        result = pq.read_table(pa.PythonFile(f))\n    assert result.equals(table)\n\n\ndef test_byte_stream_split():\n    # This is only a smoke test.\n    arr_float = pa.array(list(map(float, range(100))))\n    arr_int = pa.array(list(map(int, range(100))))\n    arr_bool = pa.array([True, False] * 50)\n    data_float = [arr_float, arr_float]\n    table = pa.Table.from_arrays(data_float, names=['a', 'b'])\n\n    # Check with byte_stream_split for both columns.\n    _check_roundtrip(table, expected=table, compression=\"gzip\",\n                     use_dictionary=False, use_byte_stream_split=True)\n\n    # Check with byte_stream_split for column 'b' and dictionary\n    # for column 'a'.\n    _check_roundtrip(table, expected=table, compression=\"gzip\",\n                     use_dictionary=['a'],\n                     use_byte_stream_split=['b'])\n\n    # Check with a collision for both columns.\n    _check_roundtrip(table, expected=table, compression=\"gzip\",\n                     use_dictionary=['a', 'b'],\n                     use_byte_stream_split=['a', 'b'])\n\n    # Check with mixed column types.\n    mixed_table = pa.Table.from_arrays([arr_float, arr_float, arr_int, arr_int],\n                                       names=['a', 'b', 'c', 'd'])\n    _check_roundtrip(mixed_table, expected=mixed_table,\n                     use_dictionary=['b', 'd'],\n                     use_byte_stream_split=['a', 'c'])\n\n    # Try to use the wrong data type with the byte_stream_split encoding.\n    # This should throw an exception.\n    table = pa.Table.from_arrays([arr_bool], names=['tmp'])\n    with pytest.raises(IOError, match='BYTE_STREAM_SPLIT only supports'):\n        _check_roundtrip(table, expected=table, use_byte_stream_split=True,\n                         use_dictionary=False)\n\n\ndef test_column_encoding():\n    arr_float = pa.array(list(map(float, range(100))))\n    arr_int = pa.array(list(map(int, range(100))))\n    arr_bin = pa.array([str(x) for x in range(100)], type=pa.binary())\n    arr_flba = pa.array(\n        [str(x).zfill(10) for x in range(100)], type=pa.binary(10))\n    arr_bool = pa.array([False, True, False, False] * 25)\n    mixed_table = pa.Table.from_arrays(\n        [arr_float, arr_int, arr_bin, arr_flba, arr_bool],\n        names=['a', 'b', 'c', 'd', 'e'])\n\n    # Check \"BYTE_STREAM_SPLIT\" for columns 'a', 'b', 'd'\n    # and \"PLAIN\" column_encoding for column 'c'.\n    _check_roundtrip(mixed_table, expected=mixed_table, use_dictionary=False,\n                     column_encoding={'a': \"BYTE_STREAM_SPLIT\",\n                                      'b': \"BYTE_STREAM_SPLIT\",\n                                      'c': \"PLAIN\",\n                                      'd': \"BYTE_STREAM_SPLIT\"})\n\n    # Check \"PLAIN\" for all columns.\n    _check_roundtrip(mixed_table, expected=mixed_table,\n                     use_dictionary=False,\n                     column_encoding=\"PLAIN\")\n\n    # Check \"DELTA_BINARY_PACKED\" for integer columns.\n    _check_roundtrip(mixed_table, expected=mixed_table,\n                     use_dictionary=False,\n                     column_encoding={'a': \"PLAIN\",\n                                      'b': \"DELTA_BINARY_PACKED\",\n                                      'c': \"PLAIN\"})\n\n    # Check \"DELTA_LENGTH_BYTE_ARRAY\" for byte columns.\n    _check_roundtrip(mixed_table, expected=mixed_table,\n                     use_dictionary=False,\n                     column_encoding={'a': \"PLAIN\",\n                                      'b': \"DELTA_BINARY_PACKED\",\n                                      'c': \"DELTA_LENGTH_BYTE_ARRAY\"})\n\n    # Check \"DELTA_BYTE_ARRAY\" for byte columns.\n    _check_roundtrip(mixed_table, expected=mixed_table,\n                     use_dictionary=False,\n                     column_encoding={'a': \"PLAIN\",\n                                      'b': \"DELTA_BINARY_PACKED\",\n                                      'c': \"DELTA_BYTE_ARRAY\",\n                                      'd': \"DELTA_BYTE_ARRAY\"})\n\n    # Check \"RLE\" for boolean columns.\n    _check_roundtrip(mixed_table, expected=mixed_table,\n                     use_dictionary=False,\n                     column_encoding={'e': \"RLE\"})\n\n    # Try to pass \"BYTE_STREAM_SPLIT\" column encoding for boolean column 'e'.\n    # This should throw an error as it is does not support BOOLEAN.\n    with pytest.raises(IOError,\n                       match=\"BYTE_STREAM_SPLIT only supports\"):\n        _check_roundtrip(mixed_table, expected=mixed_table,\n                         use_dictionary=False,\n                         column_encoding={'a': \"PLAIN\",\n                                          'c': \"PLAIN\",\n                                          'e': \"BYTE_STREAM_SPLIT\"})\n\n    # Try to pass use \"DELTA_BINARY_PACKED\" encoding on float column.\n    # This should throw an error as only integers are supported.\n    with pytest.raises(OSError,\n                       match=\"DELTA_BINARY_PACKED encoder only supports\"):\n        _check_roundtrip(mixed_table, expected=mixed_table,\n                         use_dictionary=False,\n                         column_encoding={'a': \"DELTA_BINARY_PACKED\",\n                                          'b': \"PLAIN\",\n                                          'c': \"PLAIN\"})\n\n    # Try to pass \"RLE_DICTIONARY\".\n    # This should throw an error as dictionary encoding is already used by\n    # default and not supported to be specified as \"fallback\" encoding\n    with pytest.raises(ValueError,\n                       match=\"'RLE_DICTIONARY' is already used by default\"):\n        _check_roundtrip(mixed_table, expected=mixed_table,\n                         use_dictionary=False,\n                         column_encoding=\"RLE_DICTIONARY\")\n\n    # Try to pass unsupported encoding.\n    with pytest.raises(ValueError,\n                       match=\"Unsupported column encoding: 'MADE_UP_ENCODING'\"):\n        _check_roundtrip(mixed_table, expected=mixed_table,\n                         use_dictionary=False,\n                         column_encoding={'a': \"MADE_UP_ENCODING\"})\n\n    # Try to pass column_encoding and use_dictionary.\n    # This should throw an error.\n    with pytest.raises(ValueError):\n        _check_roundtrip(mixed_table, expected=mixed_table,\n                         use_dictionary=['b'],\n                         column_encoding={'b': \"PLAIN\"})\n\n    # Try to pass column_encoding and use_dictionary=True (default value).\n    # This should throw an error.\n    with pytest.raises(ValueError):\n        _check_roundtrip(mixed_table, expected=mixed_table,\n                         column_encoding={'b': \"PLAIN\"})\n\n    # Try to pass column_encoding and use_byte_stream_split on same column.\n    # This should throw an error.\n    with pytest.raises(ValueError):\n        _check_roundtrip(mixed_table, expected=mixed_table,\n                         use_dictionary=False,\n                         use_byte_stream_split=['a'],\n                         column_encoding={'a': \"RLE\",\n                                          'b': \"BYTE_STREAM_SPLIT\",\n                                          'c': \"PLAIN\"})\n\n    # Try to pass column_encoding and use_byte_stream_split=True.\n    # This should throw an error.\n    with pytest.raises(ValueError):\n        _check_roundtrip(mixed_table, expected=mixed_table,\n                         use_dictionary=False,\n                         use_byte_stream_split=True,\n                         column_encoding={'a': \"RLE\",\n                                          'b': \"BYTE_STREAM_SPLIT\",\n                                          'c': \"PLAIN\"})\n\n    # Try to pass column_encoding=True.\n    # This should throw an error.\n    with pytest.raises(TypeError):\n        _check_roundtrip(mixed_table, expected=mixed_table,\n                         use_dictionary=False,\n                         column_encoding=True)\n\n\ndef test_compression_level():\n    arr = pa.array(list(map(int, range(1000))))\n    data = [arr, arr]\n    table = pa.Table.from_arrays(data, names=['a', 'b'])\n\n    # Check one compression level.\n    _check_roundtrip(table, expected=table, compression=\"gzip\",\n                     compression_level=1)\n\n    # Check another one to make sure that compression_level=1 does not\n    # coincide with the default one in Arrow.\n    _check_roundtrip(table, expected=table, compression=\"gzip\",\n                     compression_level=5)\n\n    # Check that the user can provide a compression per column\n    _check_roundtrip(table, expected=table,\n                     compression={'a': \"gzip\", 'b': \"snappy\"})\n\n    # Check that the user can provide a compression level per column\n    _check_roundtrip(table, expected=table, compression=\"gzip\",\n                     compression_level={'a': 2, 'b': 3})\n\n    # Check if both LZ4 compressors are working\n    # (level < 3 -> fast, level >= 3 -> HC)\n    _check_roundtrip(table, expected=table, compression=\"lz4\",\n                     compression_level=1)\n\n    _check_roundtrip(table, expected=table, compression=\"lz4\",\n                     compression_level=9)\n\n    # Check that specifying a compression level for a codec which does allow\n    # specifying one, results into an error.\n    # Uncompressed, snappy and lzo do not support specifying a compression\n    # level.\n    # GZIP (zlib) allows for specifying a compression level but as of up\n    # to version 1.2.11 the valid range is [-1, 9].\n    invalid_combinations = [(\"snappy\", 4), (\"gzip\", -1337),\n                            (\"None\", 444), (\"lzo\", 14)]\n    buf = io.BytesIO()\n    for (codec, level) in invalid_combinations:\n        with pytest.raises((ValueError, OSError)):\n            _write_table(table, buf, compression=codec,\n                         compression_level=level)\n\n\ndef test_sanitized_spark_field_names():\n    a0 = pa.array([0, 1, 2, 3, 4])\n    name = 'prohib; ,\\t{}'\n    table = pa.Table.from_arrays([a0], [name])\n\n    result = _roundtrip_table(table, write_table_kwargs={'flavor': 'spark'})\n\n    expected_name = 'prohib______'\n    assert result.schema[0].name == expected_name\n\n\n@pytest.mark.pandas\ndef test_multithreaded_read():\n    df = alltypes_sample(size=10000)\n\n    table = pa.Table.from_pandas(df)\n\n    buf = io.BytesIO()\n    _write_table(table, buf, compression='SNAPPY', version='2.6')\n\n    buf.seek(0)\n    table1 = _read_table(buf, use_threads=True)\n\n    buf.seek(0)\n    table2 = _read_table(buf, use_threads=False)\n\n    assert table1.equals(table2)\n\n\n@pytest.mark.pandas\ndef test_min_chunksize():\n    data = pd.DataFrame([np.arange(4)], columns=['A', 'B', 'C', 'D'])\n    table = pa.Table.from_pandas(data.reset_index())\n\n    buf = io.BytesIO()\n    _write_table(table, buf, chunk_size=-1)\n\n    buf.seek(0)\n    result = _read_table(buf)\n\n    assert result.equals(table)\n\n    with pytest.raises(ValueError):\n        _write_table(table, buf, chunk_size=0)\n\n\n@pytest.mark.pandas\ndef test_write_error_deletes_incomplete_file(tempdir):\n    # ARROW-1285\n    df = pd.DataFrame({'a': list('abc'),\n                       'b': list(range(1, 4)),\n                       'c': np.arange(3, 6).astype('u1'),\n                       'd': np.arange(4.0, 7.0, dtype='float64'),\n                       'e': [True, False, True],\n                       'f': pd.Categorical(list('abc')),\n                       'g': pd.date_range('20130101', periods=3),\n                       'h': pd.date_range('20130101', periods=3,\n                                          tz='US/Eastern'),\n                       'i': pd.date_range('20130101', periods=3, freq='ns')})\n\n    pdf = pa.Table.from_pandas(df)\n\n    filename = tempdir / 'tmp_file'\n    try:\n        # Test relies on writing nanoseconds to raise an error\n        # true for Parquet 2.4\n        _write_table(pdf, filename, version=\"2.4\")\n    except pa.ArrowException:\n        pass\n\n    assert not filename.exists()\n\n\ndef test_read_non_existent_file(tempdir):\n    path = 'nonexistent-file.parquet'\n    try:\n        pq.read_table(path)\n    except Exception as e:\n        assert path in e.args[0]\n\n\ndef test_read_table_doesnt_warn(datadir):\n    with warnings.catch_warnings():\n        warnings.simplefilter(action=\"error\")\n        pq.read_table(datadir / 'v0.7.1.parquet')\n\n\n@pytest.mark.pandas\ndef test_zlib_compression_bug():\n    # ARROW-3514: \"zlib deflate failed, output buffer too small\"\n    table = pa.Table.from_arrays([pa.array(['abc', 'def'])], ['some_col'])\n    f = io.BytesIO()\n    pq.write_table(table, f, compression='gzip')\n\n    f.seek(0)\n    roundtrip = pq.read_table(f)\n    tm.assert_frame_equal(roundtrip.to_pandas(), table.to_pandas())\n\n\ndef test_parquet_file_too_small(tempdir):\n    path = str(tempdir / \"test.parquet\")\n    # TODO(dataset) with datasets API it raises OSError instead\n    with pytest.raises((pa.ArrowInvalid, OSError),\n                       match='size is 0 bytes'):\n        with open(path, 'wb') as f:\n            pass\n        pq.read_table(path)\n\n    with pytest.raises((pa.ArrowInvalid, OSError),\n                       match='size is 4 bytes'):\n        with open(path, 'wb') as f:\n            f.write(b'ffff')\n        pq.read_table(path)\n\n\n@pytest.mark.pandas\n@pytest.mark.fastparquet\n@pytest.mark.filterwarnings(\"ignore:RangeIndex:FutureWarning\")\n@pytest.mark.filterwarnings(\"ignore:tostring:DeprecationWarning:fastparquet\")\ndef test_fastparquet_cross_compatibility(tempdir):\n    fp = pytest.importorskip('fastparquet')\n\n    df = pd.DataFrame(\n        {\n            \"a\": list(\"abc\"),\n            \"b\": list(range(1, 4)),\n            \"c\": np.arange(4.0, 7.0, dtype=\"float64\"),\n            \"d\": [True, False, True],\n            \"e\": pd.date_range(\"20130101\", periods=3),\n            \"f\": pd.Categorical([\"a\", \"b\", \"a\"]),\n            # fastparquet writes list as BYTE_ARRAY JSON, so no roundtrip\n            # \"g\": [[1, 2], None, [1, 2, 3]],\n        }\n    )\n    table = pa.table(df)\n\n    # Arrow -> fastparquet\n    file_arrow = str(tempdir / \"cross_compat_arrow.parquet\")\n    pq.write_table(table, file_arrow, compression=None)\n\n    fp_file = fp.ParquetFile(file_arrow)\n    df_fp = fp_file.to_pandas()\n    tm.assert_frame_equal(df, df_fp)\n\n    # Fastparquet -> arrow\n    file_fastparquet = str(tempdir / \"cross_compat_fastparquet.parquet\")\n    fp.write(file_fastparquet, df)\n\n    table_fp = pq.read_pandas(file_fastparquet)\n    # for fastparquet written file, categoricals comes back as strings\n    # (no arrow schema in parquet metadata)\n    df['f'] = df['f'].astype(object)\n    tm.assert_frame_equal(table_fp.to_pandas(), df)\n\n\n@pytest.mark.parametrize('array_factory', [\n    lambda: pa.array([0, None] * 10),\n    lambda: pa.array([0, None] * 10).dictionary_encode(),\n    lambda: pa.array([\"\", None] * 10),\n    lambda: pa.array([\"\", None] * 10).dictionary_encode(),\n])\n@pytest.mark.parametrize('read_dictionary', [False, True])\ndef test_buffer_contents(\n        array_factory, read_dictionary\n):\n    # Test that null values are deterministically initialized to zero\n    # after a roundtrip through Parquet.\n    # See ARROW-8006 and ARROW-8011.\n    orig_table = pa.Table.from_pydict({\"col\": array_factory()})\n    bio = io.BytesIO()\n    pq.write_table(orig_table, bio, use_dictionary=True)\n    bio.seek(0)\n    read_dictionary = ['col'] if read_dictionary else None\n    table = pq.read_table(bio, use_threads=False,\n                          read_dictionary=read_dictionary)\n\n    for col in table.columns:\n        [chunk] = col.chunks\n        buf = chunk.buffers()[1]\n        assert buf.to_pybytes() == buf.size * b\"\\0\"\n\n\ndef test_parquet_compression_roundtrip(tempdir):\n    # ARROW-10480: ensure even with nonstandard Parquet file naming\n    # conventions, writing and then reading a file works. In\n    # particular, ensure that we don't automatically double-compress\n    # the stream due to auto-detecting the extension in the filename\n    table = pa.table([pa.array(range(4))], names=[\"ints\"])\n    path = tempdir / \"arrow-10480.pyarrow.gz\"\n    pq.write_table(table, path, compression=\"GZIP\")\n    result = pq.read_table(path)\n    assert result.equals(table)\n\n\ndef test_empty_row_groups(tempdir):\n    # ARROW-3020\n    table = pa.Table.from_arrays([pa.array([], type='int32')], ['f0'])\n\n    path = tempdir / 'empty_row_groups.parquet'\n\n    num_groups = 3\n    with pq.ParquetWriter(path, table.schema) as writer:\n        for i in range(num_groups):\n            writer.write_table(table)\n\n    reader = pq.ParquetFile(path)\n    assert reader.metadata.num_row_groups == num_groups\n\n    for i in range(num_groups):\n        assert reader.read_row_group(i).equals(table)\n\n\ndef test_reads_over_batch(tempdir):\n    data = [None] * (1 << 20)\n    data.append([1])\n    # Large list<int64> with mostly nones and one final\n    # value.  This should force batched reads when\n    # reading back.\n    table = pa.Table.from_arrays([data], ['column'])\n\n    path = tempdir / 'arrow-11607.parquet'\n    pq.write_table(table, path)\n    table2 = pq.read_table(path)\n    assert table == table2\n\n\ndef test_permutation_of_column_order(tempdir):\n    # ARROW-2366\n    case = tempdir / \"dataset_column_order_permutation\"\n    case.mkdir(exist_ok=True)\n\n    data1 = pa.table([[1, 2, 3], [.1, .2, .3]], names=['a', 'b'])\n    pq.write_table(data1, case / \"data1.parquet\")\n\n    data2 = pa.table([[.4, .5, .6], [4, 5, 6]], names=['b', 'a'])\n    pq.write_table(data2, case / \"data2.parquet\")\n\n    table = pq.read_table(str(case))\n    table2 = pa.table([[1, 2, 3, 4, 5, 6],\n                       [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]],\n                      names=['a', 'b'])\n\n    assert table == table2\n\n\ndef test_thrift_size_limits(tempdir):\n    path = tempdir / 'largethrift.parquet'\n\n    array = pa.array(list(range(10)))\n    num_cols = 1000\n    table = pa.table(\n        [array] * num_cols,\n        names=[f'some_long_column_name_{i}' for i in range(num_cols)])\n    pq.write_table(table, path)\n\n    with pytest.raises(\n            OSError,\n            match=\"Couldn't deserialize thrift:.*Exceeded size limit\"):\n        pq.read_table(path, thrift_string_size_limit=50 * num_cols)\n    with pytest.raises(\n            OSError,\n            match=\"Couldn't deserialize thrift:.*Exceeded size limit\"):\n        pq.read_table(path, thrift_container_size_limit=num_cols)\n\n    got = pq.read_table(path, thrift_string_size_limit=100 * num_cols)\n    assert got == table\n    got = pq.read_table(path, thrift_container_size_limit=2 * num_cols)\n    assert got == table\n    got = pq.read_table(path)\n    assert got == table\n\n\ndef test_page_checksum_verification_write_table(tempdir):\n    \"\"\"Check that checksum verification works for datasets created with\n    pq.write_table()\"\"\"\n\n    # Write some sample data into a parquet file with page checksum enabled\n    original_path = tempdir / 'correct.parquet'\n    table_orig = pa.table({'a': [1, 2, 3, 4]})\n    pq.write_table(table_orig, original_path, write_page_checksum=True)\n\n    # Read file and verify that the data is correct\n    table_check = pq.read_table(original_path, page_checksum_verification=True)\n    assert table_orig == table_check\n\n    # Read the original file as binary and swap the 31-th and 36-th bytes. This\n    # should be equivalent to storing the following data:\n    #    pa.table({'a': [1, 3, 2, 4]})\n    bin_data = bytearray(original_path.read_bytes())\n\n    # Swap two bytes to emulate corruption. Also, check that the two bytes are\n    # different, otherwise no corruption occurs\n    assert bin_data[31] != bin_data[36]\n    bin_data[31], bin_data[36] = bin_data[36], bin_data[31]\n\n    # Write the corrupted data to another parquet file\n    corrupted_path = tempdir / 'corrupted.parquet'\n    corrupted_path.write_bytes(bin_data)\n\n    # Case 1: Reading the corrupted file with read_table() and without page\n    # checksum verification succeeds but yields corrupted data\n    table_corrupt = pq.read_table(corrupted_path,\n                                  page_checksum_verification=False)\n    # The read should complete without error, but the table has different\n    # content than the original file!\n    assert table_corrupt != table_orig\n    assert table_corrupt == pa.table({'a': [1, 3, 2, 4]})\n\n    # Case 2: Reading the corrupted file with read_table() and with page\n    # checksum verification enabled raises an exception\n    with pytest.raises(OSError, match=\"CRC checksum verification\"):\n        _ = pq.read_table(corrupted_path, page_checksum_verification=True)\n\n    # Case 3: Reading the corrupted file with ParquetFile.read() and without\n    # page checksum verification succeeds but yields corrupted data\n    corrupted_pq_file = pq.ParquetFile(corrupted_path,\n                                       page_checksum_verification=False)\n    table_corrupt2 = corrupted_pq_file.read()\n    assert table_corrupt2 != table_orig\n    assert table_corrupt2 == pa.table({'a': [1, 3, 2, 4]})\n\n    # Case 4: Reading the corrupted file with ParquetFile.read() and with page\n    # checksum verification enabled raises an exception\n    corrupted_pq_file = pq.ParquetFile(corrupted_path,\n                                       page_checksum_verification=True)\n    # Accessing the data should result in an error\n    with pytest.raises(OSError, match=\"CRC checksum verification\"):\n        _ = corrupted_pq_file.read()\n\n\n@pytest.mark.dataset\ndef test_checksum_write_to_dataset(tempdir):\n    \"\"\"Check that checksum verification works for datasets created with\n    pq.write_to_dataset\"\"\"\n\n    table_orig = pa.table({'a': [1, 2, 3, 4]})\n\n    # Write a sample dataset with page checksum enabled\n    original_dir_path = tempdir / 'correct_dir'\n    pq.write_to_dataset(table_orig,\n                        original_dir_path,\n                        write_page_checksum=True)\n\n    # Read file and verify that the data is correct\n    original_file_path_list = list(original_dir_path.iterdir())\n    assert len(original_file_path_list) == 1\n    original_path = original_file_path_list[0]\n    table_check = pq.read_table(original_path, page_checksum_verification=True)\n    assert table_orig == table_check\n\n    # Read the original file as binary and swap the 31-th and 36-th bytes. This\n    # should be equivalent to storing the following data:\n    #    pa.table({'a': [1, 3, 2, 4]})\n    bin_data = bytearray(original_path.read_bytes())\n\n    # Swap two bytes to emulate corruption. Also, check that the two bytes are\n    # different, otherwise no corruption occurs\n    assert bin_data[31] != bin_data[36]\n    bin_data[31], bin_data[36] = bin_data[36], bin_data[31]\n\n    # Write the corrupted data to another parquet dataset\n    # Copy dataset dir (which should be just one file)\n    corrupted_dir_path = tempdir / 'corrupted_dir'\n    copytree(original_dir_path, corrupted_dir_path)\n    # Corrupt just the one file with the dataset\n    corrupted_file_path = corrupted_dir_path / original_path.name\n    corrupted_file_path.write_bytes(bin_data)\n\n    # Case 1: Reading the corrupted file with read_table() and without page\n    # checksum verification succeeds but yields corrupted data\n    table_corrupt = pq.read_table(corrupted_file_path,\n                                  page_checksum_verification=False)\n    # The read should complete without error, but the table has different\n    # content than the original file!\n    assert table_corrupt != table_orig\n    assert table_corrupt == pa.table({'a': [1, 3, 2, 4]})\n\n    # Case 2: Reading the corrupted file with read_table() and with page\n    # checksum verification enabled raises an exception\n    with pytest.raises(OSError, match=\"CRC checksum verification\"):\n        _ = pq.read_table(corrupted_file_path, page_checksum_verification=True)\n\n\n@pytest.mark.dataset\ndef test_deprecated_use_legacy_dataset(tempdir):\n    # Test that specifying use_legacy_dataset in ParquetDataset, write_to_dataset\n    # and read_table doesn't raise an error but gives a warning.\n    table = pa.table({\"a\": [1, 2, 3]})\n    path = tempdir / \"deprecate_legacy\"\n\n    msg = \"Passing 'use_legacy_dataset'\"\n    with pytest.warns(FutureWarning, match=msg):\n        pq.write_to_dataset(table, path, use_legacy_dataset=False)\n\n    pq.write_to_dataset(table, path)\n\n    with pytest.warns(FutureWarning, match=msg):\n        pq.read_table(path, use_legacy_dataset=False)\n\n    with pytest.warns(FutureWarning, match=msg):\n        pq.ParquetDataset(path, use_legacy_dataset=False)\n", "python/pyarrow/tests/parquet/test_encryption.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport pytest\nfrom datetime import timedelta\n\nimport pyarrow as pa\ntry:\n    import pyarrow.parquet as pq\n    import pyarrow.parquet.encryption as pe\nexcept ImportError:\n    pq = None\n    pe = None\nelse:\n    from pyarrow.tests.parquet.encryption import (\n        InMemoryKmsClient, verify_file_encrypted)\n\n\nPARQUET_NAME = 'encrypted_table.in_mem.parquet'\nFOOTER_KEY = b\"0123456789112345\"\nFOOTER_KEY_NAME = \"footer_key\"\nCOL_KEY = b\"1234567890123450\"\nCOL_KEY_NAME = \"col_key\"\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet_encryption'\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = [\n    pytest.mark.parquet_encryption,\n    pytest.mark.parquet\n]\n\n\n@pytest.fixture(scope='module')\ndef data_table():\n    data_table = pa.Table.from_pydict({\n        'a': pa.array([1, 2, 3]),\n        'b': pa.array(['a', 'b', 'c']),\n        'c': pa.array(['x', 'y', 'z'])\n    })\n    return data_table\n\n\n@pytest.fixture(scope='module')\ndef basic_encryption_config():\n    basic_encryption_config = pe.EncryptionConfiguration(\n        footer_key=FOOTER_KEY_NAME,\n        column_keys={\n            COL_KEY_NAME: [\"a\", \"b\"],\n        })\n    return basic_encryption_config\n\n\ndef setup_encryption_environment(custom_kms_conf):\n    \"\"\"\n    Sets up and returns the KMS connection configuration and crypto factory\n    based on provided KMS configuration parameters.\n    \"\"\"\n    kms_connection_config = pe.KmsConnectionConfig(custom_kms_conf=custom_kms_conf)\n\n    def kms_factory(kms_connection_configuration):\n        return InMemoryKmsClient(kms_connection_configuration)\n\n    # Create our CryptoFactory\n    crypto_factory = pe.CryptoFactory(kms_factory)\n\n    return kms_connection_config, crypto_factory\n\n\ndef write_encrypted_file(path, data_table, footer_key_name, col_key_name,\n                         footer_key, col_key, encryption_config):\n    \"\"\"\n    Writes an encrypted parquet file based on the provided parameters.\n    \"\"\"\n    # Setup the custom KMS configuration with provided keys\n    custom_kms_conf = {\n        footer_key_name: footer_key.decode(\"UTF-8\"),\n        col_key_name: col_key.decode(\"UTF-8\"),\n    }\n\n    # Setup encryption environment\n    kms_connection_config, crypto_factory = setup_encryption_environment(\n        custom_kms_conf)\n\n    # Write the encrypted parquet file\n    write_encrypted_parquet(path, data_table, encryption_config,\n                            kms_connection_config, crypto_factory)\n\n    return kms_connection_config, crypto_factory\n\n\ndef test_encrypted_parquet_write_read(tempdir, data_table):\n    \"\"\"Write an encrypted parquet, verify it's encrypted, and then read it.\"\"\"\n    path = tempdir / PARQUET_NAME\n\n    # Encrypt the footer with the footer key,\n    # encrypt column `a` and column `b` with another key,\n    # keep `c` plaintext\n    encryption_config = pe.EncryptionConfiguration(\n        footer_key=FOOTER_KEY_NAME,\n        column_keys={\n            COL_KEY_NAME: [\"a\", \"b\"],\n        },\n        encryption_algorithm=\"AES_GCM_V1\",\n        cache_lifetime=timedelta(minutes=5.0),\n        data_key_length_bits=256)\n\n    kms_connection_config, crypto_factory = write_encrypted_file(\n        path, data_table, FOOTER_KEY_NAME, COL_KEY_NAME, FOOTER_KEY, COL_KEY,\n        encryption_config)\n\n    verify_file_encrypted(path)\n\n    # Read with decryption properties\n    decryption_config = pe.DecryptionConfiguration(\n        cache_lifetime=timedelta(minutes=5.0))\n    result_table = read_encrypted_parquet(\n        path, decryption_config, kms_connection_config, crypto_factory)\n    assert data_table.equals(result_table)\n\n\ndef write_encrypted_parquet(path, table, encryption_config,\n                            kms_connection_config, crypto_factory):\n    file_encryption_properties = crypto_factory.file_encryption_properties(\n        kms_connection_config, encryption_config)\n    assert file_encryption_properties is not None\n    with pq.ParquetWriter(\n            path, table.schema,\n            encryption_properties=file_encryption_properties) as writer:\n        writer.write_table(table)\n\n\ndef read_encrypted_parquet(path, decryption_config,\n                           kms_connection_config, crypto_factory):\n    file_decryption_properties = crypto_factory.file_decryption_properties(\n        kms_connection_config, decryption_config)\n    assert file_decryption_properties is not None\n    meta = pq.read_metadata(\n        path, decryption_properties=file_decryption_properties)\n    assert meta.num_columns == 3\n    schema = pq.read_schema(\n        path, decryption_properties=file_decryption_properties)\n    assert len(schema.names) == 3\n\n    result = pq.ParquetFile(\n        path, decryption_properties=file_decryption_properties)\n    return result.read(use_threads=True)\n\n\ndef test_encrypted_parquet_write_read_wrong_key(tempdir, data_table):\n    \"\"\"Write an encrypted parquet, verify it's encrypted,\n    and then read it using wrong keys.\"\"\"\n    path = tempdir / PARQUET_NAME\n\n    # Encrypt the footer with the footer key,\n    # encrypt column `a` and column `b` with another key,\n    # keep `c` plaintext\n    encryption_config = pe.EncryptionConfiguration(\n        footer_key=FOOTER_KEY_NAME,\n        column_keys={\n            COL_KEY_NAME: [\"a\", \"b\"],\n        },\n        encryption_algorithm=\"AES_GCM_V1\",\n        cache_lifetime=timedelta(minutes=5.0),\n        data_key_length_bits=256)\n\n    write_encrypted_file(path, data_table, FOOTER_KEY_NAME, COL_KEY_NAME,\n                         FOOTER_KEY, COL_KEY, encryption_config)\n\n    verify_file_encrypted(path)\n\n    wrong_kms_connection_config, wrong_crypto_factory = setup_encryption_environment({\n        FOOTER_KEY_NAME: COL_KEY.decode(\"UTF-8\"),  # Intentionally wrong\n        COL_KEY_NAME: FOOTER_KEY.decode(\"UTF-8\"),  # Intentionally wrong\n    })\n\n    decryption_config = pe.DecryptionConfiguration(\n        cache_lifetime=timedelta(minutes=5.0))\n    with pytest.raises(ValueError, match=r\"Incorrect master key used\"):\n        read_encrypted_parquet(\n            path, decryption_config, wrong_kms_connection_config,\n            wrong_crypto_factory)\n\n\ndef test_encrypted_parquet_read_no_decryption_config(tempdir, data_table):\n    \"\"\"Write an encrypted parquet, verify it's encrypted,\n    but then try to read it without decryption properties.\"\"\"\n    test_encrypted_parquet_write_read(tempdir, data_table)\n    # Read without decryption properties\n    with pytest.raises(IOError, match=r\"no decryption\"):\n        pq.ParquetFile(tempdir / PARQUET_NAME).read()\n\n\ndef test_encrypted_parquet_read_metadata_no_decryption_config(\n        tempdir, data_table):\n    \"\"\"Write an encrypted parquet, verify it's encrypted,\n    but then try to read its metadata without decryption properties.\"\"\"\n    test_encrypted_parquet_write_read(tempdir, data_table)\n    # Read metadata without decryption properties\n    with pytest.raises(IOError, match=r\"no decryption\"):\n        pq.read_metadata(tempdir / PARQUET_NAME)\n\n\ndef test_encrypted_parquet_read_schema_no_decryption_config(\n        tempdir, data_table):\n    \"\"\"Write an encrypted parquet, verify it's encrypted,\n    but then try to read its schema without decryption properties.\"\"\"\n    test_encrypted_parquet_write_read(tempdir, data_table)\n    with pytest.raises(IOError, match=r\"no decryption\"):\n        pq.read_schema(tempdir / PARQUET_NAME)\n\n\ndef test_encrypted_parquet_write_no_col_key(tempdir, data_table):\n    \"\"\"Write an encrypted parquet, but give only footer key,\n    without column key.\"\"\"\n    path = tempdir / 'encrypted_table_no_col_key.in_mem.parquet'\n\n    # Encrypt the footer with the footer key\n    encryption_config = pe.EncryptionConfiguration(\n        footer_key=FOOTER_KEY_NAME)\n\n    with pytest.raises(OSError,\n                       match=\"Either column_keys or uniform_encryption \"\n                       \"must be set\"):\n        # Write with encryption properties\n        write_encrypted_file(path, data_table, FOOTER_KEY_NAME, COL_KEY_NAME,\n                             FOOTER_KEY, b\"\", encryption_config)\n\n\ndef test_encrypted_parquet_write_kms_error(tempdir, data_table,\n                                           basic_encryption_config):\n    \"\"\"Write an encrypted parquet, but raise KeyError in KmsClient.\"\"\"\n    path = tempdir / 'encrypted_table_kms_error.in_mem.parquet'\n    encryption_config = basic_encryption_config\n\n    # Empty master_keys_map\n    kms_connection_config = pe.KmsConnectionConfig()\n\n    def kms_factory(kms_connection_configuration):\n        # Empty master keys map will cause KeyError to be raised\n        # on wrap/unwrap calls\n        return InMemoryKmsClient(kms_connection_configuration)\n\n    crypto_factory = pe.CryptoFactory(kms_factory)\n    with pytest.raises(KeyError, match=\"footer_key\"):\n        # Write with encryption properties\n        write_encrypted_parquet(path, data_table, encryption_config,\n                                kms_connection_config, crypto_factory)\n\n\ndef test_encrypted_parquet_write_kms_specific_error(tempdir, data_table,\n                                                    basic_encryption_config):\n    \"\"\"Write an encrypted parquet, but raise KeyError in KmsClient.\"\"\"\n    path = tempdir / 'encrypted_table_kms_error.in_mem.parquet'\n    encryption_config = basic_encryption_config\n\n    # Empty master_keys_map\n    kms_connection_config = pe.KmsConnectionConfig()\n\n    class ThrowingKmsClient(pe.KmsClient):\n        \"\"\"A KmsClient implementation that throws exception in\n        wrap/unwrap calls\n        \"\"\"\n\n        def __init__(self, config):\n            \"\"\"Create an InMemoryKmsClient instance.\"\"\"\n            pe.KmsClient.__init__(self)\n            self.config = config\n\n        def wrap_key(self, key_bytes, master_key_identifier):\n            raise ValueError(\"Cannot Wrap Key\")\n\n        def unwrap_key(self, wrapped_key, master_key_identifier):\n            raise ValueError(\"Cannot Unwrap Key\")\n\n    def kms_factory(kms_connection_configuration):\n        # Exception thrown in wrap/unwrap calls\n        return ThrowingKmsClient(kms_connection_configuration)\n\n    crypto_factory = pe.CryptoFactory(kms_factory)\n    with pytest.raises(ValueError, match=\"Cannot Wrap Key\"):\n        # Write with encryption properties\n        write_encrypted_parquet(path, data_table, encryption_config,\n                                kms_connection_config, crypto_factory)\n\n\ndef test_encrypted_parquet_write_kms_factory_error(tempdir, data_table,\n                                                   basic_encryption_config):\n    \"\"\"Write an encrypted parquet, but raise ValueError in kms_factory.\"\"\"\n    path = tempdir / 'encrypted_table_kms_factory_error.in_mem.parquet'\n    encryption_config = basic_encryption_config\n\n    # Empty master_keys_map\n    kms_connection_config = pe.KmsConnectionConfig()\n\n    def kms_factory(kms_connection_configuration):\n        raise ValueError('Cannot create KmsClient')\n\n    crypto_factory = pe.CryptoFactory(kms_factory)\n    with pytest.raises(ValueError,\n                       match=\"Cannot create KmsClient\"):\n        # Write with encryption properties\n        write_encrypted_parquet(path, data_table, encryption_config,\n                                kms_connection_config, crypto_factory)\n\n\ndef test_encrypted_parquet_write_kms_factory_type_error(\n        tempdir, data_table, basic_encryption_config):\n    \"\"\"Write an encrypted parquet, but use wrong KMS client type\n    that doesn't implement KmsClient.\"\"\"\n    path = tempdir / 'encrypted_table_kms_factory_error.in_mem.parquet'\n    encryption_config = basic_encryption_config\n\n    # Empty master_keys_map\n    kms_connection_config = pe.KmsConnectionConfig()\n\n    class WrongTypeKmsClient():\n        \"\"\"This is not an implementation of KmsClient.\n        \"\"\"\n\n        def __init__(self, config):\n            self.master_keys_map = config.custom_kms_conf\n\n        def wrap_key(self, key_bytes, master_key_identifier):\n            return None\n\n        def unwrap_key(self, wrapped_key, master_key_identifier):\n            return None\n\n    def kms_factory(kms_connection_configuration):\n        return WrongTypeKmsClient(kms_connection_configuration)\n\n    crypto_factory = pe.CryptoFactory(kms_factory)\n    with pytest.raises(TypeError):\n        # Write with encryption properties\n        write_encrypted_parquet(path, data_table, encryption_config,\n                                kms_connection_config, crypto_factory)\n\n\ndef test_encrypted_parquet_encryption_configuration():\n    def validate_encryption_configuration(encryption_config):\n        assert FOOTER_KEY_NAME == encryption_config.footer_key\n        assert [\"a\", \"b\"] == encryption_config.column_keys[COL_KEY_NAME]\n        assert \"AES_GCM_CTR_V1\" == encryption_config.encryption_algorithm\n        assert encryption_config.plaintext_footer\n        assert not encryption_config.double_wrapping\n        assert timedelta(minutes=10.0) == encryption_config.cache_lifetime\n        assert not encryption_config.internal_key_material\n        assert 192 == encryption_config.data_key_length_bits\n\n    encryption_config = pe.EncryptionConfiguration(\n        footer_key=FOOTER_KEY_NAME,\n        column_keys={COL_KEY_NAME: [\"a\", \"b\"], },\n        encryption_algorithm=\"AES_GCM_CTR_V1\",\n        plaintext_footer=True,\n        double_wrapping=False,\n        cache_lifetime=timedelta(minutes=10.0),\n        internal_key_material=False,\n        data_key_length_bits=192,\n    )\n    validate_encryption_configuration(encryption_config)\n\n    encryption_config_1 = pe.EncryptionConfiguration(\n        footer_key=FOOTER_KEY_NAME)\n    encryption_config_1.column_keys = {COL_KEY_NAME: [\"a\", \"b\"], }\n    encryption_config_1.encryption_algorithm = \"AES_GCM_CTR_V1\"\n    encryption_config_1.plaintext_footer = True\n    encryption_config_1.double_wrapping = False\n    encryption_config_1.cache_lifetime = timedelta(minutes=10.0)\n    encryption_config_1.internal_key_material = False\n    encryption_config_1.data_key_length_bits = 192\n    validate_encryption_configuration(encryption_config_1)\n\n\ndef test_encrypted_parquet_decryption_configuration():\n    decryption_config = pe.DecryptionConfiguration(\n        cache_lifetime=timedelta(minutes=10.0))\n    assert timedelta(minutes=10.0) == decryption_config.cache_lifetime\n\n    decryption_config_1 = pe.DecryptionConfiguration()\n    decryption_config_1.cache_lifetime = timedelta(minutes=10.0)\n    assert timedelta(minutes=10.0) == decryption_config_1.cache_lifetime\n\n\ndef test_encrypted_parquet_kms_configuration():\n    def validate_kms_connection_config(kms_connection_config):\n        assert \"Instance1\" == kms_connection_config.kms_instance_id\n        assert \"URL1\" == kms_connection_config.kms_instance_url\n        assert \"MyToken\" == kms_connection_config.key_access_token\n        assert ({\"key1\": \"key_material_1\", \"key2\": \"key_material_2\"} ==\n                kms_connection_config.custom_kms_conf)\n\n    kms_connection_config = pe.KmsConnectionConfig(\n        kms_instance_id=\"Instance1\",\n        kms_instance_url=\"URL1\",\n        key_access_token=\"MyToken\",\n        custom_kms_conf={\n            \"key1\": \"key_material_1\",\n            \"key2\": \"key_material_2\",\n        })\n    validate_kms_connection_config(kms_connection_config)\n\n    kms_connection_config_1 = pe.KmsConnectionConfig()\n    kms_connection_config_1.kms_instance_id = \"Instance1\"\n    kms_connection_config_1.kms_instance_url = \"URL1\"\n    kms_connection_config_1.key_access_token = \"MyToken\"\n    kms_connection_config_1.custom_kms_conf = {\n        \"key1\": \"key_material_1\",\n        \"key2\": \"key_material_2\",\n    }\n    validate_kms_connection_config(kms_connection_config_1)\n\n\n@pytest.mark.xfail(reason=\"Plaintext footer - reading plaintext column subset\"\n                   \" reads encrypted columns too\")\ndef test_encrypted_parquet_write_read_plain_footer_single_wrapping(\n        tempdir, data_table):\n    \"\"\"Write an encrypted parquet, with plaintext footer\n    and with single wrapping,\n    verify it's encrypted, and then read plaintext columns.\"\"\"\n    path = tempdir / PARQUET_NAME\n\n    # Encrypt the footer with the footer key,\n    # encrypt column `a` and column `b` with another key,\n    # keep `c` plaintext\n    encryption_config = pe.EncryptionConfiguration(\n        footer_key=FOOTER_KEY_NAME,\n        column_keys={\n            COL_KEY_NAME: [\"a\", \"b\"],\n        },\n        plaintext_footer=True,\n        double_wrapping=False)\n\n    kms_connection_config = pe.KmsConnectionConfig(\n        custom_kms_conf={\n            FOOTER_KEY_NAME: FOOTER_KEY.decode(\"UTF-8\"),\n            COL_KEY_NAME: COL_KEY.decode(\"UTF-8\"),\n        }\n    )\n\n    def kms_factory(kms_connection_configuration):\n        return InMemoryKmsClient(kms_connection_configuration)\n\n    crypto_factory = pe.CryptoFactory(kms_factory)\n    # Write with encryption properties\n    write_encrypted_parquet(path, data_table, encryption_config,\n                            kms_connection_config, crypto_factory)\n\n    # # Read without decryption properties only the plaintext column\n    # result = pq.ParquetFile(path)\n    # result_table = result.read(columns='c', use_threads=False)\n    # assert table.num_rows == result_table.num_rows\n\n\n@pytest.mark.xfail(reason=\"External key material not supported yet\")\ndef test_encrypted_parquet_write_external(tempdir, data_table):\n    \"\"\"Write an encrypted parquet, with external key\n    material.\n    Currently it's not implemented, so should throw\n    an exception\"\"\"\n    path = tempdir / PARQUET_NAME\n\n    # Encrypt the file with the footer key\n    encryption_config = pe.EncryptionConfiguration(\n        footer_key=FOOTER_KEY_NAME,\n        column_keys={},\n        internal_key_material=False)\n\n    kms_connection_config = pe.KmsConnectionConfig(\n        custom_kms_conf={FOOTER_KEY_NAME: FOOTER_KEY.decode(\"UTF-8\")}\n    )\n\n    def kms_factory(kms_connection_configuration):\n        return InMemoryKmsClient(kms_connection_configuration)\n\n    crypto_factory = pe.CryptoFactory(kms_factory)\n    # Write with encryption properties\n    write_encrypted_parquet(path, data_table, encryption_config,\n                            kms_connection_config, crypto_factory)\n\n\ndef test_encrypted_parquet_loop(tempdir, data_table, basic_encryption_config):\n    \"\"\"Write an encrypted parquet, verify it's encrypted,\n    and then read it multithreaded in a loop.\"\"\"\n    path = tempdir / PARQUET_NAME\n\n    # Encrypt the footer with the footer key,\n    # encrypt column `a` and column `b` with another key,\n    # keep `c` plaintext, defined in basic_encryption_config\n    kms_connection_config, crypto_factory = write_encrypted_file(\n        path, data_table, FOOTER_KEY_NAME, COL_KEY_NAME, FOOTER_KEY, COL_KEY,\n        basic_encryption_config)\n\n    verify_file_encrypted(path)\n\n    decryption_config = pe.DecryptionConfiguration(\n        cache_lifetime=timedelta(minutes=5.0))\n\n    for i in range(50):\n        # Read with decryption properties\n        file_decryption_properties = crypto_factory.file_decryption_properties(\n            kms_connection_config, decryption_config)\n        assert file_decryption_properties is not None\n\n        result = pq.ParquetFile(\n            path, decryption_properties=file_decryption_properties)\n        result_table = result.read(use_threads=True)\n        assert data_table.equals(result_table)\n\n\ndef test_read_with_deleted_crypto_factory(tempdir, data_table, basic_encryption_config):\n    \"\"\"\n    Test that decryption properties can be used if the crypto factory is no longer alive\n    \"\"\"\n    path = tempdir / PARQUET_NAME\n    kms_connection_config, crypto_factory = write_encrypted_file(\n        path, data_table, FOOTER_KEY_NAME, COL_KEY_NAME, FOOTER_KEY, COL_KEY,\n        basic_encryption_config)\n    verify_file_encrypted(path)\n\n    # Create decryption properties and delete the crypto factory that created\n    # the properties afterwards.\n    decryption_config = pe.DecryptionConfiguration(\n        cache_lifetime=timedelta(minutes=5.0))\n    file_decryption_properties = crypto_factory.file_decryption_properties(\n        kms_connection_config, decryption_config)\n    del crypto_factory\n\n    result = pq.ParquetFile(\n        path, decryption_properties=file_decryption_properties)\n    result_table = result.read(use_threads=True)\n    assert data_table.equals(result_table)\n\n\ndef test_encrypted_parquet_read_table(tempdir, data_table, basic_encryption_config):\n    \"\"\"Write an encrypted parquet then read it back using read_table.\"\"\"\n    path = tempdir / PARQUET_NAME\n\n    # Write the encrypted parquet file using the utility function\n    kms_connection_config, crypto_factory = write_encrypted_file(\n        path, data_table, FOOTER_KEY_NAME, COL_KEY_NAME, FOOTER_KEY, COL_KEY,\n        basic_encryption_config)\n\n    decryption_config = pe.DecryptionConfiguration(\n        cache_lifetime=timedelta(minutes=5.0))\n    file_decryption_properties = crypto_factory.file_decryption_properties(\n        kms_connection_config, decryption_config)\n\n    # Read the encrypted parquet file using read_table\n    result_table = pq.read_table(path, decryption_properties=file_decryption_properties)\n\n    # Assert that the read table matches the original data\n    assert data_table.equals(result_table)\n\n    # Read the encrypted parquet folder using read_table\n    result_table = pq.read_table(\n        tempdir, decryption_properties=file_decryption_properties)\n    assert data_table.equals(result_table)\n", "python/pyarrow/tests/parquet/encryption.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport base64\n\nimport pyarrow.parquet.encryption as pe\n\n\nclass InMemoryKmsClient(pe.KmsClient):\n    \"\"\"This is a mock class implementation of KmsClient, built for testing\n    only.\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"Create an InMemoryKmsClient instance.\"\"\"\n        pe.KmsClient.__init__(self)\n        self.master_keys_map = config.custom_kms_conf\n\n    def wrap_key(self, key_bytes, master_key_identifier):\n        \"\"\"Not a secure cipher - the wrapped key\n        is just the master key concatenated with key bytes\"\"\"\n        master_key_bytes = self.master_keys_map[master_key_identifier].encode(\n            'utf-8')\n        wrapped_key = b\"\".join([master_key_bytes, key_bytes])\n        result = base64.b64encode(wrapped_key)\n        return result\n\n    def unwrap_key(self, wrapped_key, master_key_identifier):\n        \"\"\"Not a secure cipher - just extract the key from\n        the wrapped key\"\"\"\n        expected_master_key = self.master_keys_map[master_key_identifier]\n        decoded_wrapped_key = base64.b64decode(wrapped_key)\n        master_key_bytes = decoded_wrapped_key[:16]\n        decrypted_key = decoded_wrapped_key[16:]\n        if (expected_master_key == master_key_bytes.decode('utf-8')):\n            return decrypted_key\n        raise ValueError(\"Incorrect master key used\",\n                         master_key_bytes, decrypted_key)\n\n\ndef verify_file_encrypted(path):\n    \"\"\"Verify that the file is encrypted by looking at its first 4 bytes.\n    If it's the magic string PARE\n    then this is a parquet with encrypted footer.\"\"\"\n    with open(path, \"rb\") as file:\n        magic_str = file.read(4)\n        # Verify magic string for parquet with encrypted footer is PARE\n        assert magic_str == b'PARE'\n", "python/pyarrow/tests/parquet/test_metadata.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport datetime\nimport decimal\nfrom collections import OrderedDict\nimport io\n\nimport numpy as np\nimport pytest\n\nimport pyarrow as pa\nfrom pyarrow.tests.parquet.common import _check_roundtrip, make_sample_file\nfrom pyarrow.fs import LocalFileSystem\nfrom pyarrow.tests import util\n\ntry:\n    import pyarrow.parquet as pq\n    from pyarrow.tests.parquet.common import _write_table\nexcept ImportError:\n    pq = None\n\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n\n    from pyarrow.tests.parquet.common import alltypes_sample\nexcept ImportError:\n    pd = tm = None\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = pytest.mark.parquet\n\n\n@pytest.mark.pandas\ndef test_parquet_metadata_api():\n    df = alltypes_sample(size=10000)\n    df = df.reindex(columns=sorted(df.columns))\n    df.index = np.random.randint(0, 1000000, size=len(df))\n\n    fileh = make_sample_file(df)\n    ncols = len(df.columns)\n\n    # Series of sniff tests\n    meta = fileh.metadata\n    repr(meta)\n    assert meta.num_rows == len(df)\n    assert meta.num_columns == ncols + 1  # +1 for index\n    assert meta.num_row_groups == 1\n    assert meta.format_version == '2.6'\n    assert 'parquet-cpp' in meta.created_by\n    assert isinstance(meta.serialized_size, int)\n    assert isinstance(meta.metadata, dict)\n\n    # Schema\n    schema = fileh.schema\n    assert meta.schema is schema\n    assert len(schema) == ncols + 1  # +1 for index\n    repr(schema)\n\n    col = schema[0]\n    repr(col)\n    assert col.name == df.columns[0]\n    assert col.max_definition_level == 1\n    assert col.max_repetition_level == 0\n    assert col.max_repetition_level == 0\n    assert col.physical_type == 'BOOLEAN'\n    assert col.converted_type == 'NONE'\n\n    col_float16 = schema[5]\n    assert col_float16.logical_type.type == 'FLOAT16'\n\n    with pytest.raises(IndexError):\n        schema[ncols + 1]  # +1 for index\n\n    with pytest.raises(IndexError):\n        schema[-1]\n\n    # Row group\n    for rg in range(meta.num_row_groups):\n        rg_meta = meta.row_group(rg)\n        assert isinstance(rg_meta, pq.RowGroupMetaData)\n        repr(rg_meta)\n\n        for col in range(rg_meta.num_columns):\n            col_meta = rg_meta.column(col)\n            assert isinstance(col_meta, pq.ColumnChunkMetaData)\n            repr(col_meta)\n\n    with pytest.raises(IndexError):\n        meta.row_group(-1)\n\n    with pytest.raises(IndexError):\n        meta.row_group(meta.num_row_groups + 1)\n\n    rg_meta = meta.row_group(0)\n    assert rg_meta.num_rows == len(df)\n    assert rg_meta.num_columns == ncols + 1  # +1 for index\n    assert rg_meta.total_byte_size > 0\n\n    with pytest.raises(IndexError):\n        col_meta = rg_meta.column(-1)\n\n    with pytest.raises(IndexError):\n        col_meta = rg_meta.column(ncols + 2)\n\n    col_meta = rg_meta.column(0)\n    assert col_meta.file_offset > 0\n    assert col_meta.file_path == ''  # created from BytesIO\n    assert col_meta.physical_type == 'BOOLEAN'\n    assert col_meta.num_values == 10000\n    assert col_meta.path_in_schema == 'bool'\n    assert col_meta.is_stats_set is True\n    assert isinstance(col_meta.statistics, pq.Statistics)\n    assert col_meta.compression == 'SNAPPY'\n    assert set(col_meta.encodings) == {'PLAIN', 'RLE'}\n    assert col_meta.has_dictionary_page is False\n    assert col_meta.dictionary_page_offset is None\n    assert col_meta.data_page_offset > 0\n    assert col_meta.total_compressed_size > 0\n    assert col_meta.total_uncompressed_size > 0\n    with pytest.raises(NotImplementedError):\n        col_meta.has_index_page\n    with pytest.raises(NotImplementedError):\n        col_meta.index_page_offset\n\n\ndef test_parquet_metadata_lifetime(tempdir):\n    # ARROW-6642 - ensure that chained access keeps parent objects alive\n    table = pa.table({'a': [1, 2, 3]})\n    pq.write_table(table, tempdir / 'test_metadata_segfault.parquet')\n    parquet_file = pq.ParquetFile(tempdir / 'test_metadata_segfault.parquet')\n    parquet_file.metadata.row_group(0).column(0).statistics\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\n    (\n        'data',\n        'type',\n        'physical_type',\n        'min_value',\n        'max_value',\n        'null_count',\n        'num_values',\n        'distinct_count'\n    ),\n    [\n        ([1, 2, 2, None, 4], pa.uint8(), 'INT32', 1, 4, 1, 4, None),\n        ([1, 2, 2, None, 4], pa.uint16(), 'INT32', 1, 4, 1, 4, None),\n        ([1, 2, 2, None, 4], pa.uint32(), 'INT32', 1, 4, 1, 4, None),\n        ([1, 2, 2, None, 4], pa.uint64(), 'INT64', 1, 4, 1, 4, None),\n        ([-1, 2, 2, None, 4], pa.int8(), 'INT32', -1, 4, 1, 4, None),\n        ([-1, 2, 2, None, 4], pa.int16(), 'INT32', -1, 4, 1, 4, None),\n        ([-1, 2, 2, None, 4], pa.int32(), 'INT32', -1, 4, 1, 4, None),\n        ([-1, 2, 2, None, 4], pa.int64(), 'INT64', -1, 4, 1, 4, None),\n        (\n            [-1.1, 2.2, 2.3, None, 4.4], pa.float32(),\n            'FLOAT', -1.1, 4.4, 1, 4, None\n        ),\n        (\n            [-1.1, 2.2, 2.3, None, 4.4], pa.float64(),\n            'DOUBLE', -1.1, 4.4, 1, 4, None\n        ),\n        (\n            ['', 'b', chr(1000), None, 'aaa'], pa.binary(),\n            'BYTE_ARRAY', b'', chr(1000).encode('utf-8'), 1, 4, None\n        ),\n        (\n            [True, False, False, True, True], pa.bool_(),\n            'BOOLEAN', False, True, 0, 5, None\n        ),\n        (\n            [b'\\x00', b'b', b'12', None, b'aaa'], pa.binary(),\n            'BYTE_ARRAY', b'\\x00', b'b', 1, 4, None\n        ),\n    ]\n)\ndef test_parquet_column_statistics_api(data, type, physical_type, min_value,\n                                       max_value, null_count, num_values,\n                                       distinct_count):\n    df = pd.DataFrame({'data': data})\n    schema = pa.schema([pa.field('data', type)])\n    table = pa.Table.from_pandas(df, schema=schema, safe=False)\n    fileh = make_sample_file(table)\n\n    meta = fileh.metadata\n\n    rg_meta = meta.row_group(0)\n    col_meta = rg_meta.column(0)\n\n    stat = col_meta.statistics\n    assert stat.has_min_max\n    assert _close(type, stat.min, min_value)\n    assert _close(type, stat.max, max_value)\n    assert stat.null_count == null_count\n    assert stat.num_values == num_values\n    # TODO(kszucs) until parquet-cpp API doesn't expose HasDistinctCount\n    # method, missing distinct_count is represented as zero instead of None\n    assert stat.distinct_count == distinct_count\n    assert stat.physical_type == physical_type\n\n\ndef _close(type, left, right):\n    if type == pa.float32():\n        return abs(left - right) < 1E-7\n    elif type == pa.float64():\n        return abs(left - right) < 1E-13\n    else:\n        return left == right\n\n\n# ARROW-6339\n@pytest.mark.pandas\ndef test_parquet_raise_on_unset_statistics():\n    df = pd.DataFrame({\"t\": pd.Series([pd.NaT], dtype=\"datetime64[ns]\")})\n    meta = make_sample_file(pa.Table.from_pandas(df)).metadata\n\n    assert not meta.row_group(0).column(0).statistics.has_min_max\n    assert meta.row_group(0).column(0).statistics.max is None\n\n\ndef test_statistics_convert_logical_types(tempdir):\n    # ARROW-5166, ARROW-4139\n\n    # (min, max, type)\n    cases = [(10, 11164359321221007157, pa.uint64()),\n             (10, 4294967295, pa.uint32()),\n             (\"\u00e4hnlich\", \"\u00f6ffentlich\", pa.utf8()),\n             (datetime.time(10, 30, 0, 1000), datetime.time(15, 30, 0, 1000),\n              pa.time32('ms')),\n             (datetime.time(10, 30, 0, 1000), datetime.time(15, 30, 0, 1000),\n              pa.time64('us')),\n             (datetime.datetime(2019, 6, 24, 0, 0, 0, 1000),\n              datetime.datetime(2019, 6, 25, 0, 0, 0, 1000),\n              pa.timestamp('ms')),\n             (datetime.datetime(2019, 6, 24, 0, 0, 0, 1000),\n              datetime.datetime(2019, 6, 25, 0, 0, 0, 1000),\n              pa.timestamp('us')),\n             (datetime.date(2019, 6, 24),\n              datetime.date(2019, 6, 25),\n              pa.date32()),\n             (decimal.Decimal(\"20.123\"),\n              decimal.Decimal(\"20.124\"),\n              pa.decimal128(12, 5))]\n\n    for i, (min_val, max_val, typ) in enumerate(cases):\n        t = pa.Table.from_arrays([pa.array([min_val, max_val], type=typ)],\n                                 ['col'])\n        path = str(tempdir / ('example{}.parquet'.format(i)))\n        pq.write_table(t, path, version='2.6')\n        pf = pq.ParquetFile(path)\n        stats = pf.metadata.row_group(0).column(0).statistics\n        assert stats.min == min_val\n        assert stats.max == max_val\n\n\ndef test_parquet_write_disable_statistics(tempdir):\n    table = pa.Table.from_pydict(\n        OrderedDict([\n            ('a', pa.array([1, 2, 3])),\n            ('b', pa.array(['a', 'b', 'c']))\n        ])\n    )\n    _write_table(table, tempdir / 'data.parquet')\n    meta = pq.read_metadata(tempdir / 'data.parquet')\n    for col in [0, 1]:\n        cc = meta.row_group(0).column(col)\n        assert cc.is_stats_set is True\n        assert cc.statistics is not None\n\n    _write_table(table, tempdir / 'data2.parquet', write_statistics=False)\n    meta = pq.read_metadata(tempdir / 'data2.parquet')\n    for col in [0, 1]:\n        cc = meta.row_group(0).column(col)\n        assert cc.is_stats_set is False\n        assert cc.statistics is None\n\n    _write_table(table, tempdir / 'data3.parquet', write_statistics=['a'])\n    meta = pq.read_metadata(tempdir / 'data3.parquet')\n    cc_a = meta.row_group(0).column(0)\n    cc_b = meta.row_group(0).column(1)\n    assert cc_a.is_stats_set is True\n    assert cc_b.is_stats_set is False\n    assert cc_a.statistics is not None\n    assert cc_b.statistics is None\n\n\ndef test_parquet_sorting_column():\n    sorting_col = pq.SortingColumn(10)\n    assert sorting_col.to_dict() == {\n        'column_index': 10,\n        'descending': False,\n        'nulls_first': False\n    }\n\n    sorting_col = pq.SortingColumn(0, descending=True, nulls_first=True)\n    assert sorting_col.to_dict() == {\n        'column_index': 0,\n        'descending': True,\n        'nulls_first': True\n    }\n\n    schema = pa.schema([('a', pa.int64()), ('b', pa.int64())])\n    sorting_cols = (\n        pq.SortingColumn(1, descending=True),\n        pq.SortingColumn(0, descending=False),\n    )\n    sort_order, null_placement = pq.SortingColumn.to_ordering(schema, sorting_cols)\n    assert sort_order == (('b', \"descending\"), ('a', \"ascending\"))\n    assert null_placement == \"at_end\"\n\n    sorting_cols_roundtripped = pq.SortingColumn.from_ordering(\n        schema, sort_order, null_placement)\n    assert sorting_cols_roundtripped == sorting_cols\n\n    sorting_cols = pq.SortingColumn.from_ordering(\n        schema, ('a', ('b', \"descending\")), null_placement=\"at_start\")\n    expected = (\n        pq.SortingColumn(0, descending=False, nulls_first=True),\n        pq.SortingColumn(1, descending=True, nulls_first=True),\n    )\n    assert sorting_cols == expected\n\n    # Conversions handle empty tuples\n    empty_sorting_cols = pq.SortingColumn.from_ordering(schema, ())\n    assert empty_sorting_cols == ()\n\n    assert pq.SortingColumn.to_ordering(schema, ()) == ((), \"at_end\")\n\n    with pytest.raises(ValueError):\n        pq.SortingColumn.from_ordering(schema, ((\"a\", \"not a valid sort order\")))\n\n    with pytest.raises(ValueError, match=\"inconsistent null placement\"):\n        sorting_cols = (\n            pq.SortingColumn(1, nulls_first=True),\n            pq.SortingColumn(0, nulls_first=False),\n        )\n        pq.SortingColumn.to_ordering(schema, sorting_cols)\n\n\ndef test_parquet_sorting_column_nested():\n    schema = pa.schema({\n        'a': pa.struct([('x', pa.int64()), ('y', pa.int64())]),\n        'b': pa.int64()\n    })\n\n    sorting_columns = [\n        pq.SortingColumn(0, descending=True),  # a.x\n        pq.SortingColumn(2, descending=False)  # b\n    ]\n\n    sort_order, null_placement = pq.SortingColumn.to_ordering(schema, sorting_columns)\n    assert null_placement == \"at_end\"\n    assert len(sort_order) == 2\n    assert sort_order[0] == (\"a.x\", \"descending\")\n    assert sort_order[1] == (\"b\", \"ascending\")\n\n\ndef test_parquet_file_sorting_columns():\n    table = pa.table({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})\n\n    sorting_columns = (\n        pq.SortingColumn(column_index=0, descending=True, nulls_first=True),\n        pq.SortingColumn(column_index=1, descending=False),\n    )\n    writer = pa.BufferOutputStream()\n    _write_table(table, writer, sorting_columns=sorting_columns)\n    reader = pa.BufferReader(writer.getvalue())\n\n    # Can retrieve sorting columns from metadata\n    metadata = pq.read_metadata(reader)\n    assert sorting_columns == metadata.row_group(0).sorting_columns\n\n    metadata_dict = metadata.to_dict()\n    assert metadata_dict.get('num_columns') == 2\n    assert metadata_dict.get('num_rows') == 3\n    assert metadata_dict.get('num_row_groups') == 1\n\n\ndef test_field_id_metadata():\n    # ARROW-7080\n    field_id = b'PARQUET:field_id'\n    inner = pa.field('inner', pa.int32(), metadata={field_id: b'100'})\n    middle = pa.field('middle', pa.struct(\n        [inner]), metadata={field_id: b'101'})\n    fields = [\n        pa.field('basic', pa.int32(), metadata={\n                 b'other': b'abc', field_id: b'1'}),\n        pa.field(\n            'list',\n            pa.list_(pa.field('list-inner', pa.int32(),\n                              metadata={field_id: b'10'})),\n            metadata={field_id: b'11'}),\n        pa.field('struct', pa.struct([middle]), metadata={field_id: b'102'}),\n        pa.field('no-metadata', pa.int32()),\n        pa.field('non-integral-field-id', pa.int32(),\n                 metadata={field_id: b'xyz'}),\n        pa.field('negative-field-id', pa.int32(),\n                 metadata={field_id: b'-1000'})\n    ]\n    arrs = [[] for _ in fields]\n    table = pa.table(arrs, schema=pa.schema(fields))\n\n    bio = pa.BufferOutputStream()\n    pq.write_table(table, bio)\n    contents = bio.getvalue()\n\n    pf = pq.ParquetFile(pa.BufferReader(contents))\n    schema = pf.schema_arrow\n\n    assert schema[0].metadata[field_id] == b'1'\n    assert schema[0].metadata[b'other'] == b'abc'\n\n    list_field = schema[1]\n    assert list_field.metadata[field_id] == b'11'\n\n    list_item_field = list_field.type.value_field\n    assert list_item_field.metadata[field_id] == b'10'\n\n    struct_field = schema[2]\n    assert struct_field.metadata[field_id] == b'102'\n\n    struct_middle_field = struct_field.type[0]\n    assert struct_middle_field.metadata[field_id] == b'101'\n\n    struct_inner_field = struct_middle_field.type[0]\n    assert struct_inner_field.metadata[field_id] == b'100'\n\n    assert schema[3].metadata is None\n    # Invalid input is passed through (ok) but does not\n    # have field_id in parquet (not tested)\n    assert schema[4].metadata[field_id] == b'xyz'\n    assert schema[5].metadata[field_id] == b'-1000'\n\n\ndef test_parquet_file_page_index():\n    for write_page_index in (False, True):\n        table = pa.table({'a': [1, 2, 3]})\n\n        writer = pa.BufferOutputStream()\n        _write_table(table, writer, write_page_index=write_page_index)\n        reader = pa.BufferReader(writer.getvalue())\n\n        # Can retrieve sorting columns from metadata\n        metadata = pq.read_metadata(reader)\n        cc = metadata.row_group(0).column(0)\n        assert cc.has_offset_index is write_page_index\n        assert cc.has_column_index is write_page_index\n\n\n@pytest.mark.pandas\ndef test_multi_dataset_metadata(tempdir):\n    filenames = [\"ARROW-1983-dataset.0\", \"ARROW-1983-dataset.1\"]\n    metapath = str(tempdir / \"_metadata\")\n\n    # create a test dataset\n    df = pd.DataFrame({\n        'one': [1, 2, 3],\n        'two': [-1, -2, -3],\n        'three': [[1, 2], [2, 3], [3, 4]],\n    })\n    table = pa.Table.from_pandas(df)\n\n    # write dataset twice and collect/merge metadata\n    _meta = None\n    for filename in filenames:\n        meta = []\n        pq.write_table(table, str(tempdir / filename),\n                       metadata_collector=meta)\n        meta[0].set_file_path(filename)\n        if _meta is None:\n            _meta = meta[0]\n        else:\n            _meta.append_row_groups(meta[0])\n\n    # Write merged metadata-only file\n    with open(metapath, \"wb\") as f:\n        _meta.write_metadata_file(f)\n\n    # Read back the metadata\n    meta = pq.read_metadata(metapath)\n    md = meta.to_dict()\n    _md = _meta.to_dict()\n    for key in _md:\n        if key != 'serialized_size':\n            assert _md[key] == md[key]\n    assert _md['num_columns'] == 3\n    assert _md['num_rows'] == 6\n    assert _md['num_row_groups'] == 2\n    assert _md['serialized_size'] == 0\n    assert md['serialized_size'] > 0\n\n\ndef test_metadata_hashing(tempdir):\n    path1 = str(tempdir / \"metadata1\")\n    schema1 = pa.schema([(\"a\", \"int64\"), (\"b\", \"float64\")])\n    pq.write_metadata(schema1, path1)\n    parquet_meta1 = pq.read_metadata(path1)\n\n    # Same as 1, just different path\n    path2 = str(tempdir / \"metadata2\")\n    schema2 = pa.schema([(\"a\", \"int64\"), (\"b\", \"float64\")])\n    pq.write_metadata(schema2, path2)\n    parquet_meta2 = pq.read_metadata(path2)\n\n    # different schema\n    path3 = str(tempdir / \"metadata3\")\n    schema3 = pa.schema([(\"a\", \"int64\"), (\"b\", \"float32\")])\n    pq.write_metadata(schema3, path3)\n    parquet_meta3 = pq.read_metadata(path3)\n\n    # Deterministic\n    assert hash(parquet_meta1) == hash(parquet_meta1)  # equal w/ same instance\n    assert hash(parquet_meta1) == hash(parquet_meta2)  # equal w/ different instance\n\n    # Not the same as other metadata with different schema\n    assert hash(parquet_meta1) != hash(parquet_meta3)\n\n\n@pytest.mark.filterwarnings(\"ignore:Parquet format:FutureWarning\")\ndef test_write_metadata(tempdir):\n    path = str(tempdir / \"metadata\")\n    schema = pa.schema([(\"a\", \"int64\"), (\"b\", \"float64\")])\n\n    # write a pyarrow schema\n    pq.write_metadata(schema, path)\n    parquet_meta = pq.read_metadata(path)\n    schema_as_arrow = parquet_meta.schema.to_arrow_schema()\n    assert schema_as_arrow.equals(schema)\n\n    # ARROW-8980: Check that the ARROW:schema metadata key was removed\n    if schema_as_arrow.metadata:\n        assert b'ARROW:schema' not in schema_as_arrow.metadata\n\n    # pass through writer keyword arguments\n    for version in [\"1.0\", \"2.0\", \"2.4\", \"2.6\"]:\n        pq.write_metadata(schema, path, version=version)\n        parquet_meta = pq.read_metadata(path)\n        # The version is stored as a single integer in the Parquet metadata,\n        # so it cannot correctly express dotted format versions\n        expected_version = \"1.0\" if version == \"1.0\" else \"2.6\"\n        assert parquet_meta.format_version == expected_version\n\n    # metadata_collector: list of FileMetaData objects\n    table = pa.table({'a': [1, 2], 'b': [.1, .2]}, schema=schema)\n    pq.write_table(table, tempdir / \"data.parquet\")\n    parquet_meta = pq.read_metadata(str(tempdir / \"data.parquet\"))\n    pq.write_metadata(\n        schema, path, metadata_collector=[parquet_meta, parquet_meta]\n    )\n    parquet_meta_mult = pq.read_metadata(path)\n    assert parquet_meta_mult.num_row_groups == 2\n\n    # append metadata with different schema raises an error\n    msg = (\"AppendRowGroups requires equal schemas.\\n\"\n           \"The two columns with index 0 differ.\")\n    with pytest.raises(RuntimeError, match=msg):\n        pq.write_metadata(\n            pa.schema([(\"a\", \"int32\"), (\"b\", \"null\")]),\n            path, metadata_collector=[parquet_meta, parquet_meta]\n        )\n\n\ndef test_table_large_metadata():\n    # ARROW-8694\n    my_schema = pa.schema([pa.field('f0', 'double')],\n                          metadata={'large': 'x' * 10000000})\n\n    table = pa.table([np.arange(10)], schema=my_schema)\n    _check_roundtrip(table)\n\n\n@pytest.mark.pandas\ndef test_compare_schemas():\n    df = alltypes_sample(size=10000)\n\n    fileh = make_sample_file(df)\n    fileh2 = make_sample_file(df)\n    fileh3 = make_sample_file(df[df.columns[::2]])\n\n    # ParquetSchema\n    assert isinstance(fileh.schema, pq.ParquetSchema)\n    assert fileh.schema.equals(fileh.schema)\n    assert fileh.schema == fileh.schema\n    assert fileh.schema.equals(fileh2.schema)\n    assert fileh.schema == fileh2.schema\n    assert fileh.schema != 'arbitrary object'\n    assert not fileh.schema.equals(fileh3.schema)\n    assert fileh.schema != fileh3.schema\n\n    # ColumnSchema\n    assert isinstance(fileh.schema[0], pq.ColumnSchema)\n    assert fileh.schema[0].equals(fileh.schema[0])\n    assert fileh.schema[0] == fileh.schema[0]\n    assert not fileh.schema[0].equals(fileh.schema[1])\n    assert fileh.schema[0] != fileh.schema[1]\n    assert fileh.schema[0] != 'arbitrary object'\n\n\n@pytest.mark.pandas\ndef test_read_schema(tempdir):\n    N = 100\n    df = pd.DataFrame({\n        'index': np.arange(N),\n        'values': np.random.randn(N)\n    }, columns=['index', 'values'])\n\n    data_path = tempdir / 'test.parquet'\n\n    table = pa.Table.from_pandas(df)\n    _write_table(table, data_path)\n\n    read1 = pq.read_schema(data_path)\n    read2 = pq.read_schema(data_path, memory_map=True)\n    assert table.schema.equals(read1)\n    assert table.schema.equals(read2)\n\n    assert table.schema.metadata[b'pandas'] == read1.metadata[b'pandas']\n\n\ndef test_parquet_metadata_empty_to_dict(tempdir):\n    # https://issues.apache.org/jira/browse/ARROW-10146\n    table = pa.table({\"a\": pa.array([], type=\"int64\")})\n    pq.write_table(table, tempdir / \"data.parquet\")\n    metadata = pq.read_metadata(tempdir / \"data.parquet\")\n    # ensure this doesn't error / statistics set to None\n    metadata_dict = metadata.to_dict()\n    assert len(metadata_dict[\"row_groups\"]) == 1\n    assert len(metadata_dict[\"row_groups\"][0][\"columns\"]) == 1\n    assert metadata_dict[\"row_groups\"][0][\"columns\"][0][\"statistics\"] is None\n\n\n@pytest.mark.slow\n@pytest.mark.large_memory\ndef test_metadata_exceeds_message_size():\n    # ARROW-13655: Thrift may enable a default message size that limits\n    # the size of Parquet metadata that can be written.\n    NCOLS = 1000\n    NREPEATS = 4000\n\n    table = pa.table({str(i): np.random.randn(10) for i in range(NCOLS)})\n\n    with pa.BufferOutputStream() as out:\n        pq.write_table(table, out)\n        buf = out.getvalue()\n\n    original_metadata = pq.read_metadata(pa.BufferReader(buf))\n    metadata = pq.read_metadata(pa.BufferReader(buf))\n    for i in range(NREPEATS):\n        metadata.append_row_groups(original_metadata)\n\n    with pa.BufferOutputStream() as out:\n        metadata.write_metadata_file(out)\n        buf = out.getvalue()\n\n    metadata = pq.read_metadata(pa.BufferReader(buf))\n\n\ndef test_metadata_schema_filesystem(tempdir):\n    table = pa.table({\"a\": [1, 2, 3]})\n\n    # URI writing to local file.\n    fname = \"data.parquet\"\n    file_path = str(tempdir / fname)\n    file_uri = 'file:///' + file_path\n\n    pq.write_table(table, file_path)\n\n    # Get expected `metadata` from path.\n    metadata = pq.read_metadata(tempdir / fname)\n    schema = table.schema\n\n    assert pq.read_metadata(file_uri).equals(metadata)\n    assert pq.read_metadata(\n        file_path, filesystem=LocalFileSystem()).equals(metadata)\n    assert pq.read_metadata(\n        fname, filesystem=f'file:///{tempdir}').equals(metadata)\n\n    assert pq.read_schema(file_uri).equals(schema)\n    assert pq.read_schema(\n        file_path, filesystem=LocalFileSystem()).equals(schema)\n    assert pq.read_schema(\n        fname, filesystem=f'file:///{tempdir}').equals(schema)\n\n    with util.change_cwd(tempdir):\n        # Pass `filesystem` arg\n        assert pq.read_metadata(\n            fname, filesystem=LocalFileSystem()).equals(metadata)\n\n        assert pq.read_schema(\n            fname, filesystem=LocalFileSystem()).equals(schema)\n\n\ndef test_metadata_equals():\n    table = pa.table({\"a\": [1, 2, 3]})\n    with pa.BufferOutputStream() as out:\n        pq.write_table(table, out)\n        buf = out.getvalue()\n\n    original_metadata = pq.read_metadata(pa.BufferReader(buf))\n    match = \"Argument 'other' has incorrect type\"\n    with pytest.raises(TypeError, match=match):\n        original_metadata.equals(None)\n\n\n@pytest.mark.parametrize(\"t1,t2,expected_error\", (\n    ({'col1': range(10)}, {'col1': range(10)}, None),\n    ({'col1': range(10)}, {'col2': range(10)},\n     \"The two columns with index 0 differ.\"),\n    ({'col1': range(10), 'col2': range(10)}, {'col3': range(10)},\n     \"This schema has 2 columns, other has 1\")\n))\ndef test_metadata_append_row_groups_diff(t1, t2, expected_error):\n    table1 = pa.table(t1)\n    table2 = pa.table(t2)\n\n    buf1 = io.BytesIO()\n    buf2 = io.BytesIO()\n    pq.write_table(table1, buf1)\n    pq.write_table(table2, buf2)\n    buf1.seek(0)\n    buf2.seek(0)\n\n    meta1 = pq.ParquetFile(buf1).metadata\n    meta2 = pq.ParquetFile(buf2).metadata\n\n    if expected_error:\n        # Error clearly defines it's happening at append row groups call\n        prefix = \"AppendRowGroups requires equal schemas.\\n\"\n        with pytest.raises(RuntimeError, match=prefix + expected_error):\n            meta1.append_row_groups(meta2)\n    else:\n        meta1.append_row_groups(meta2)\n\n\n@pytest.mark.s3\ndef test_write_metadata_fs_file_combinations(tempdir, s3_example_s3fs):\n    s3_fs, s3_path = s3_example_s3fs\n\n    meta1 = tempdir / \"meta1\"\n    meta2 = tempdir / \"meta2\"\n    meta3 = tempdir / \"meta3\"\n    meta4 = tempdir / \"meta4\"\n    meta5 = f\"{s3_path}/meta5\"\n\n    table = pa.table({\"col\": range(5)})\n\n    # plain local path\n    pq.write_metadata(table.schema, meta1, [])\n\n    # Used the localfilesystem to resolve opening an output stream\n    pq.write_metadata(table.schema, meta2, [], filesystem=LocalFileSystem())\n\n    # Can resolve local file URI\n    pq.write_metadata(table.schema, meta3.as_uri(), [])\n\n    # Take a file-like obj all the way thru?\n    with meta4.open('wb+') as meta4_stream:\n        pq.write_metadata(table.schema, meta4_stream, [])\n\n    # S3FileSystem\n    pq.write_metadata(table.schema, meta5, [], filesystem=s3_fs)\n\n    assert meta1.read_bytes() == meta2.read_bytes() \\\n        == meta3.read_bytes() == meta4.read_bytes() \\\n        == s3_fs.open(meta5).read()\n", "python/pyarrow/tests/parquet/test_dataset.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport datetime\nimport inspect\nimport os\nimport pathlib\n\nimport numpy as np\nimport pytest\nimport unittest.mock as mock\n\nimport pyarrow as pa\nimport pyarrow.compute as pc\nfrom pyarrow.fs import (FileSelector, FileSystem, LocalFileSystem,\n                        PyFileSystem, SubTreeFileSystem, FSSpecHandler)\nfrom pyarrow.tests import util\nfrom pyarrow.util import guid\n\ntry:\n    import pyarrow.parquet as pq\n    from pyarrow.tests.parquet.common import (\n        _read_table, _test_dataframe, _write_table)\nexcept ImportError:\n    pq = None\n\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n\nexcept ImportError:\n    pd = tm = None\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = [pytest.mark.parquet, pytest.mark.dataset]\n\n\ndef test_filesystem_uri(tempdir):\n    table = pa.table({\"a\": [1, 2, 3]})\n\n    directory = tempdir / \"data_dir\"\n    directory.mkdir()\n    path = directory / \"data.parquet\"\n    pq.write_table(table, str(path))\n\n    # filesystem object\n    result = pq.read_table(\n        path, filesystem=LocalFileSystem())\n    assert result.equals(table)\n\n    # filesystem URI\n    result = pq.read_table(\n        \"data_dir/data.parquet\", filesystem=util._filesystem_uri(tempdir))\n    assert result.equals(table)\n\n\n@pytest.mark.pandas\ndef test_read_partitioned_directory(tempdir):\n    local = LocalFileSystem()\n    _partition_test_for_filesystem(local, tempdir)\n\n\n@pytest.mark.pandas\ndef test_read_partitioned_columns_selection(tempdir):\n    # ARROW-3861 - do not include partition columns in resulting table when\n    # `columns` keyword was passed without those columns\n    local = LocalFileSystem()\n    base_path = tempdir\n    _partition_test_for_filesystem(local, base_path)\n\n    dataset = pq.ParquetDataset(base_path)\n    result = dataset.read(columns=[\"values\"])\n    assert result.column_names == [\"values\"]\n\n\n@pytest.mark.pandas\ndef test_filters_equivalency(tempdir):\n    local = LocalFileSystem()\n    base_path = tempdir\n\n    integer_keys = [0, 1]\n    string_keys = ['a', 'b', 'c']\n    boolean_keys = [True, False]\n    partition_spec = [\n        ['integer', integer_keys],\n        ['string', string_keys],\n        ['boolean', boolean_keys]\n    ]\n\n    df = pd.DataFrame({\n        'integer': np.array(integer_keys, dtype='i4').repeat(15),\n        'string': np.tile(np.tile(np.array(string_keys, dtype=object), 5), 2),\n        'boolean': np.tile(np.tile(np.array(boolean_keys, dtype='bool'), 5), 3),\n        'values': np.arange(30),\n    })\n\n    _generate_partition_directories(local, base_path, partition_spec, df)\n\n    # Old filters syntax:\n    #  integer == 1 AND string != b AND boolean == True\n    dataset = pq.ParquetDataset(\n        base_path, filesystem=local,\n        filters=[('integer', '=', 1), ('string', '!=', 'b'),\n                 ('boolean', '==', 'True')],\n    )\n    table = dataset.read()\n    result_df = (table.to_pandas().reset_index(drop=True))\n\n    assert 0 not in result_df['integer'].values\n    assert 'b' not in result_df['string'].values\n    assert False not in result_df['boolean'].values\n\n    # filters in disjunctive normal form:\n    #  (integer == 1 AND string != b AND boolean == True) OR\n    #  (integer == 2 AND boolean == False)\n    # TODO(ARROW-3388): boolean columns are reconstructed as string\n    filters = [\n        [\n            ('integer', '=', 1),\n            ('string', '!=', 'b'),\n            ('boolean', '==', 'True')\n        ],\n        [('integer', '=', 0), ('boolean', '==', 'False')]\n    ]\n    dataset = pq.ParquetDataset(\n        base_path, filesystem=local, filters=filters)\n    table = dataset.read()\n    result_df = table.to_pandas().reset_index(drop=True)\n\n    # Check that all rows in the DF fulfill the filter\n    df_filter_1 = (result_df['integer'] == 1) \\\n        & (result_df['string'] != 'b') \\\n        & (result_df['boolean'] == 'True')\n    df_filter_2 = (np.array(result_df['integer']) == 0) \\\n        & (result_df['boolean'] == 'False')\n    assert df_filter_1.sum() > 0\n    assert df_filter_2.sum() > 0\n    assert result_df.shape[0] == (df_filter_1.sum() + df_filter_2.sum())\n\n    for filters in [[[('string', '==', b'1\\0a')]],\n                    [[('string', '==', '1\\0a')]]]:\n        dataset = pq.ParquetDataset(\n            base_path, filesystem=local, filters=filters)\n        assert dataset.read().num_rows == 0\n\n\n@pytest.mark.pandas\ndef test_filters_cutoff_exclusive_integer(tempdir):\n    local = LocalFileSystem()\n    base_path = tempdir\n\n    integer_keys = [0, 1, 2, 3, 4]\n    partition_spec = [\n        ['integers', integer_keys],\n    ]\n    N = 5\n\n    df = pd.DataFrame({\n        'index': np.arange(N),\n        'integers': np.array(integer_keys, dtype='i4'),\n    }, columns=['index', 'integers'])\n\n    _generate_partition_directories(local, base_path, partition_spec, df)\n\n    dataset = pq.ParquetDataset(\n        base_path, filesystem=local,\n        filters=[\n            ('integers', '<', 4),\n            ('integers', '>', 1),\n        ],\n    )\n    table = dataset.read()\n    result_df = (table.to_pandas()\n                      .sort_values(by='index')\n                      .reset_index(drop=True))\n\n    result_list = [x for x in map(int, result_df['integers'].values)]\n    assert result_list == [2, 3]\n\n\n@pytest.mark.xfail(\n    # different error with use_legacy_datasets because result_df is no longer\n    # categorical\n    raises=(TypeError, AssertionError),\n    reason='Loss of type information in creation of categoricals.'\n)\n@pytest.mark.pandas\ndef test_filters_cutoff_exclusive_datetime(tempdir):\n    local = LocalFileSystem()\n    base_path = tempdir\n\n    date_keys = [\n        datetime.date(2018, 4, 9),\n        datetime.date(2018, 4, 10),\n        datetime.date(2018, 4, 11),\n        datetime.date(2018, 4, 12),\n        datetime.date(2018, 4, 13)\n    ]\n    partition_spec = [\n        ['dates', date_keys]\n    ]\n    N = 5\n\n    df = pd.DataFrame({\n        'index': np.arange(N),\n        'dates': np.array(date_keys, dtype='datetime64'),\n    }, columns=['index', 'dates'])\n\n    _generate_partition_directories(local, base_path, partition_spec, df)\n\n    dataset = pq.ParquetDataset(\n        base_path, filesystem=local,\n        filters=[\n            ('dates', '<', \"2018-04-12\"),\n            ('dates', '>', \"2018-04-10\")\n        ],\n    )\n    table = dataset.read()\n    result_df = (table.to_pandas()\n                      .sort_values(by='index')\n                      .reset_index(drop=True))\n\n    expected = pd.Categorical(\n        np.array([datetime.date(2018, 4, 11)], dtype='datetime64'),\n        categories=np.array(date_keys, dtype='datetime64'))\n\n    assert result_df['dates'].values == expected\n\n\n@pytest.mark.pandas\ndef test_filters_inclusive_datetime(tempdir):\n    # ARROW-11480\n    path = tempdir / 'timestamps.parquet'\n\n    pd.DataFrame({\n        \"dates\": pd.date_range(\"2020-01-01\", periods=10, freq=\"D\"),\n        \"id\": range(10)\n    }).to_parquet(path, use_deprecated_int96_timestamps=True)\n\n    table = pq.read_table(path, filters=[\n        (\"dates\", \"<=\", datetime.datetime(2020, 1, 5))\n    ])\n\n    assert table.column('id').to_pylist() == [0, 1, 2, 3, 4]\n\n\n@pytest.mark.pandas\ndef test_filters_inclusive_integer(tempdir):\n    local = LocalFileSystem()\n    base_path = tempdir\n\n    integer_keys = [0, 1, 2, 3, 4]\n    partition_spec = [\n        ['integers', integer_keys],\n    ]\n    N = 5\n\n    df = pd.DataFrame({\n        'index': np.arange(N),\n        'integers': np.array(integer_keys, dtype='i4'),\n    }, columns=['index', 'integers'])\n\n    _generate_partition_directories(local, base_path, partition_spec, df)\n\n    dataset = pq.ParquetDataset(\n        base_path, filesystem=local,\n        filters=[\n            ('integers', '<=', 3),\n            ('integers', '>=', 2),\n        ],\n    )\n    table = dataset.read()\n    result_df = (table.to_pandas()\n                 .sort_values(by='index')\n                 .reset_index(drop=True))\n\n    result_list = [int(x) for x in map(int, result_df['integers'].values)]\n    assert result_list == [2, 3]\n\n\n@pytest.mark.pandas\ndef test_filters_inclusive_set(tempdir):\n    local = LocalFileSystem()\n    base_path = tempdir\n\n    integer_keys = [0, 1]\n    string_keys = ['a', 'b', 'c']\n    boolean_keys = [True, False]\n    partition_spec = [\n        ['integer', integer_keys],\n        ['string', string_keys],\n        ['boolean', boolean_keys]\n    ]\n\n    df = pd.DataFrame({\n        'integer': np.array(integer_keys, dtype='i4').repeat(15),\n        'string': np.tile(np.tile(np.array(string_keys, dtype=object), 5), 2),\n        'boolean': np.tile(np.tile(np.array(boolean_keys, dtype='bool'), 5), 3),\n        'values': np.arange(30),\n    })\n\n    _generate_partition_directories(local, base_path, partition_spec, df)\n\n    dataset = pq.ParquetDataset(\n        base_path, filesystem=local,\n        filters=[('string', 'in', 'ab')],\n    )\n    table = dataset.read()\n    result_df = (table.to_pandas().reset_index(drop=True))\n\n    assert 'a' in result_df['string'].values\n    assert 'b' in result_df['string'].values\n    assert 'c' not in result_df['string'].values\n\n    dataset = pq.ParquetDataset(\n        base_path, filesystem=local,\n        filters=[('integer', 'in', [1]), ('string', 'in', ('a', 'b')),\n                 ('boolean', 'not in', {'False'})],\n    )\n    table = dataset.read()\n    result_df = (table.to_pandas().reset_index(drop=True))\n\n    assert 0 not in result_df['integer'].values\n    assert 'c' not in result_df['string'].values\n    assert False not in result_df['boolean'].values\n\n\n@pytest.mark.pandas\ndef test_filters_invalid_pred_op(tempdir):\n    local = LocalFileSystem()\n    base_path = tempdir\n\n    integer_keys = [0, 1, 2, 3, 4]\n    partition_spec = [\n        ['integers', integer_keys],\n    ]\n    N = 5\n\n    df = pd.DataFrame({\n        'index': np.arange(N),\n        'integers': np.array(integer_keys, dtype='i4'),\n    }, columns=['index', 'integers'])\n\n    _generate_partition_directories(local, base_path, partition_spec, df)\n\n    with pytest.raises(TypeError):\n        pq.ParquetDataset(base_path,\n                          filesystem=local,\n                          filters=[('integers', 'in', 3), ])\n\n    with pytest.raises(ValueError):\n        pq.ParquetDataset(base_path,\n                          filesystem=local,\n                          filters=[('integers', '=<', 3), ])\n\n    # Dataset API returns empty table\n    dataset = pq.ParquetDataset(base_path,\n                                filesystem=local,\n                                filters=[('integers', 'in', set()), ])\n    assert dataset.read().num_rows == 0\n\n    dataset = pq.ParquetDataset(base_path,\n                                filesystem=local,\n                                filters=[('integers', '!=', {3})])\n    with pytest.raises(NotImplementedError):\n        assert dataset.read().num_rows == 0\n\n\n@pytest.mark.pandas\ndef test_filters_invalid_column(tempdir):\n    # ARROW-5572 - raise error on invalid name in filter specification\n    # works with new dataset\n    local = LocalFileSystem()\n    base_path = tempdir\n\n    integer_keys = [0, 1, 2, 3, 4]\n    partition_spec = [['integers', integer_keys]]\n    N = 5\n\n    df = pd.DataFrame({\n        'index': np.arange(N),\n        'integers': np.array(integer_keys, dtype='i4'),\n    }, columns=['index', 'integers'])\n\n    _generate_partition_directories(local, base_path, partition_spec, df)\n\n    msg = r\"No match for FieldRef.Name\\(non_existent_column\\)\"\n    with pytest.raises(ValueError, match=msg):\n        pq.ParquetDataset(base_path, filesystem=local,\n                          filters=[('non_existent_column', '<', 3), ]).read()\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\"filters\",\n                         ([('integers', '<', 3)],\n                          [[('integers', '<', 3)]],\n                          pc.field('integers') < 3,\n                          pc.field('nested', 'a') < 3,\n                          pc.field('nested', 'b').cast(pa.int64()) < 3))\n@pytest.mark.parametrize(\"read_method\", (\"read_table\", \"read_pandas\"))\ndef test_filters_read_table(tempdir, filters, read_method):\n    read = getattr(pq, read_method)\n    # test that filters keyword is passed through in read_table\n    local = LocalFileSystem()\n    base_path = tempdir\n\n    integer_keys = [0, 1, 2, 3, 4]\n    partition_spec = [\n        ['integers', integer_keys],\n    ]\n    N = len(integer_keys)\n\n    df = pd.DataFrame({\n        'index': np.arange(N),\n        'integers': np.array(integer_keys, dtype='i4'),\n        'nested': np.array([{'a': i, 'b': str(i)} for i in range(N)])\n    })\n\n    _generate_partition_directories(local, base_path, partition_spec, df)\n\n    kwargs = dict(filesystem=local, filters=filters)\n\n    table = read(base_path, **kwargs)\n    assert table.num_rows == 3\n\n\n@pytest.mark.pandas\ndef test_partition_keys_with_underscores(tempdir):\n    # ARROW-5666 - partition field values with underscores preserve underscores\n    local = LocalFileSystem()\n    base_path = tempdir\n\n    string_keys = [\"2019_2\", \"2019_3\"]\n    partition_spec = [\n        ['year_week', string_keys],\n    ]\n    N = 2\n\n    df = pd.DataFrame({\n        'index': np.arange(N),\n        'year_week': np.array(string_keys, dtype='object'),\n    }, columns=['index', 'year_week'])\n\n    _generate_partition_directories(local, base_path, partition_spec, df)\n\n    dataset = pq.ParquetDataset(base_path)\n    result = dataset.read()\n    assert result.column(\"year_week\").to_pylist() == string_keys\n\n\n@pytest.mark.s3\ndef test_read_s3fs(s3_example_s3fs, ):\n    fs, path = s3_example_s3fs\n    path = path + \"/test.parquet\"\n    table = pa.table({\"a\": [1, 2, 3]})\n    _write_table(table, path, filesystem=fs)\n\n    result = _read_table(path, filesystem=fs)\n    assert result.equals(table)\n\n\n@pytest.mark.s3\ndef test_read_directory_s3fs(s3_example_s3fs):\n    fs, directory = s3_example_s3fs\n    path = directory + \"/test.parquet\"\n    table = pa.table({\"a\": [1, 2, 3]})\n    _write_table(table, path, filesystem=fs)\n\n    result = _read_table(directory, filesystem=fs)\n    assert result.equals(table)\n\n\n@pytest.mark.pandas\ndef test_read_single_file_list(tempdir):\n    data_path = str(tempdir / 'data.parquet')\n\n    table = pa.table({\"a\": [1, 2, 3]})\n    _write_table(table, data_path)\n\n    result = pq.ParquetDataset([data_path]).read()\n    assert result.equals(table)\n\n\n@pytest.mark.pandas\n@pytest.mark.s3\ndef test_read_partitioned_directory_s3fs(s3_example_s3fs):\n    fs, path = s3_example_s3fs\n    _partition_test_for_filesystem(fs, path)\n\n\ndef _partition_test_for_filesystem(fs, base_path):\n    foo_keys = [0, 1]\n    bar_keys = ['a', 'b', 'c']\n    partition_spec = [\n        ['foo', foo_keys],\n        ['bar', bar_keys]\n    ]\n    N = 30\n\n    df = pd.DataFrame({\n        'index': np.arange(N),\n        'foo': np.array(foo_keys, dtype='i4').repeat(15),\n        'bar': np.tile(np.tile(np.array(bar_keys, dtype=object), 5), 2),\n        'values': np.random.randn(N)\n    }, columns=['index', 'foo', 'bar', 'values'])\n\n    _generate_partition_directories(fs, base_path, partition_spec, df)\n\n    dataset = pq.ParquetDataset(base_path, filesystem=fs)\n    table = dataset.read()\n    result_df = (table.to_pandas()\n                 .sort_values(by='index')\n                 .reset_index(drop=True))\n\n    expected_df = (df.sort_values(by='index')\n                   .reset_index(drop=True)\n                   .reindex(columns=result_df.columns))\n\n    # With pandas 2.0.0 Index can store all numeric dtypes (not just\n    # int64/uint64/float64). Using astype() to create a categorical\n    # column preserves original dtype (int32)\n    expected_df['foo'] = expected_df['foo'].astype(\"category\")\n    expected_df['bar'] = expected_df['bar'].astype(\"category\")\n\n    assert (result_df.columns == ['index', 'values', 'foo', 'bar']).all()\n\n    tm.assert_frame_equal(result_df, expected_df)\n\n\ndef _generate_partition_directories(fs, base_dir, partition_spec, df):\n    # partition_spec : list of lists, e.g. [['foo', [0, 1, 2],\n    #                                       ['bar', ['a', 'b', 'c']]\n    # part_table : a pyarrow.Table to write to each partition\n    if not isinstance(fs, FileSystem):\n        fs = PyFileSystem(FSSpecHandler(fs))\n\n    DEPTH = len(partition_spec)\n\n    pathsep = getattr(fs, \"pathsep\", getattr(fs, \"sep\", \"/\"))\n\n    def _visit_level(base_dir, level, part_keys):\n        name, values = partition_spec[level]\n        for value in values:\n            this_part_keys = part_keys + [(name, value)]\n\n            level_dir = pathsep.join([\n                str(base_dir),\n                '{}={}'.format(name, value)\n            ])\n            fs.create_dir(level_dir)\n\n            if level == DEPTH - 1:\n                # Generate example data\n                from pyarrow.fs import FileType\n\n                file_path = pathsep.join([level_dir, guid()])\n                filtered_df = _filter_partition(df, this_part_keys)\n                part_table = pa.Table.from_pandas(filtered_df)\n                with fs.open_output_stream(file_path) as f:\n                    _write_table(part_table, f)\n                assert fs.get_file_info(file_path).type != FileType.NotFound\n                assert fs.get_file_info(file_path).type == FileType.File\n\n                file_success = pathsep.join([level_dir, '_SUCCESS'])\n                with fs.open_output_stream(file_success) as f:\n                    pass\n            else:\n                _visit_level(level_dir, level + 1, this_part_keys)\n                file_success = pathsep.join([level_dir, '_SUCCESS'])\n                with fs.open_output_stream(file_success) as f:\n                    pass\n\n    _visit_level(base_dir, 0, [])\n\n\ndef _filter_partition(df, part_keys):\n    predicate = np.ones(len(df), dtype=bool)\n\n    to_drop = []\n    for name, value in part_keys:\n        to_drop.append(name)\n\n        # to avoid pandas warning\n        if isinstance(value, (datetime.date, datetime.datetime)):\n            value = pd.Timestamp(value)\n\n        predicate &= df[name] == value\n\n    return df[predicate].drop(to_drop, axis=1)\n\n\n@pytest.mark.pandas\ndef test_filter_before_validate_schema(tempdir):\n    # ARROW-4076 apply filter before schema validation\n    # to avoid checking unneeded schemas\n\n    # create partitioned dataset with mismatching schemas which would\n    # otherwise raise if first validation all schemas\n    dir1 = tempdir / 'A=0'\n    dir1.mkdir()\n    table1 = pa.Table.from_pandas(pd.DataFrame({'B': [1, 2, 3]}))\n    pq.write_table(table1, dir1 / 'data.parquet')\n\n    dir2 = tempdir / 'A=1'\n    dir2.mkdir()\n    table2 = pa.Table.from_pandas(pd.DataFrame({'B': ['a', 'b', 'c']}))\n    pq.write_table(table2, dir2 / 'data.parquet')\n\n    # read single file using filter\n    table = pq.read_table(tempdir, filters=[[('A', '==', 0)]])\n    assert table.column('B').equals(pa.chunked_array([[1, 2, 3]]))\n\n\n@pytest.mark.pandas\ndef test_read_multiple_files(tempdir):\n    nfiles = 10\n    size = 5\n\n    dirpath = tempdir / guid()\n    dirpath.mkdir()\n\n    test_data = []\n    paths = []\n    for i in range(nfiles):\n        df = _test_dataframe(size, seed=i)\n\n        # Hack so that we don't have a dtype cast in v1 files\n        df['uint32'] = df['uint32'].astype(np.int64)\n\n        path = dirpath / '{}.parquet'.format(i)\n\n        table = pa.Table.from_pandas(df)\n        _write_table(table, path)\n\n        test_data.append(table)\n        paths.append(path)\n\n    # Write a _SUCCESS.crc file\n    (dirpath / '_SUCCESS.crc').touch()\n\n    def read_multiple_files(paths, columns=None, use_threads=True, **kwargs):\n        dataset = pq.ParquetDataset(paths, **kwargs)\n        return dataset.read(columns=columns, use_threads=use_threads)\n\n    result = read_multiple_files(paths)\n    expected = pa.concat_tables(test_data)\n\n    assert result.equals(expected)\n\n    # Read column subset\n    to_read = [0, 2, 6, result.num_columns - 1]\n\n    col_names = [result.field(i).name for i in to_read]\n    out = pq.read_table(dirpath, columns=col_names)\n    expected = pa.Table.from_arrays([result.column(i) for i in to_read],\n                                    names=col_names,\n                                    metadata=result.schema.metadata)\n    assert out.equals(expected)\n\n    # Read with multiple threads\n    pq.read_table(dirpath, use_threads=True)\n\n    # Test failure modes with non-uniform metadata\n    bad_apple = _test_dataframe(size, seed=i).iloc[:, :4]\n    bad_apple_path = tempdir / '{}.parquet'.format(guid())\n\n    t = pa.Table.from_pandas(bad_apple)\n    _write_table(t, bad_apple_path)\n\n    # TODO(dataset) Dataset API skips bad files\n\n    # bad_meta = pq.read_metadata(bad_apple_path)\n\n    # with pytest.raises(ValueError):\n    #     read_multiple_files(paths + [bad_apple_path])\n\n    # with pytest.raises(ValueError):\n    #     read_multiple_files(paths, metadata=bad_meta)\n\n    # mixed_paths = [bad_apple_path, paths[0]]\n\n    # with pytest.raises(ValueError):\n    #     read_multiple_files(mixed_paths)\n\n\n@pytest.mark.pandas\ndef test_dataset_read_pandas(tempdir):\n    nfiles = 5\n    size = 5\n\n    dirpath = tempdir / guid()\n    dirpath.mkdir()\n\n    test_data = []\n    frames = []\n    paths = []\n    for i in range(nfiles):\n        df = _test_dataframe(size, seed=i)\n        df.index = np.arange(i * size, (i + 1) * size)\n        df.index.name = 'index'\n\n        path = dirpath / '{}.parquet'.format(i)\n\n        table = pa.Table.from_pandas(df)\n        _write_table(table, path)\n        test_data.append(table)\n        frames.append(df)\n        paths.append(path)\n\n    dataset = pq.ParquetDataset(dirpath)\n    columns = ['uint8', 'strings']\n    result = dataset.read_pandas(columns=columns).to_pandas()\n    expected = pd.concat([x[columns] for x in frames])\n\n    tm.assert_frame_equal(result, expected)\n\n    # also be able to pass the columns as a set (ARROW-12314)\n    result = dataset.read_pandas(columns=set(columns)).to_pandas()\n    assert result.shape == expected.shape\n    # column order can be different because of using a set\n    tm.assert_frame_equal(result.reindex(columns=expected.columns), expected)\n\n\n@pytest.mark.pandas\ndef test_dataset_memory_map(tempdir):\n    # ARROW-2627: Check that we can use ParquetDataset with memory-mapping\n    dirpath = tempdir / guid()\n    dirpath.mkdir()\n\n    df = _test_dataframe(10, seed=0)\n    path = dirpath / '{}.parquet'.format(0)\n    table = pa.Table.from_pandas(df)\n    _write_table(table, path, version='2.6')\n\n    dataset = pq.ParquetDataset(\n        dirpath, memory_map=True)\n    assert dataset.read().equals(table)\n\n\n@pytest.mark.pandas\ndef test_dataset_enable_buffered_stream(tempdir):\n    dirpath = tempdir / guid()\n    dirpath.mkdir()\n\n    df = _test_dataframe(10, seed=0)\n    path = dirpath / '{}.parquet'.format(0)\n    table = pa.Table.from_pandas(df)\n    _write_table(table, path, version='2.6')\n\n    with pytest.raises(ValueError):\n        pq.ParquetDataset(\n            dirpath, buffer_size=-64)\n\n    for buffer_size in [128, 1024]:\n        dataset = pq.ParquetDataset(\n            dirpath, buffer_size=buffer_size)\n        assert dataset.read().equals(table)\n\n\n@pytest.mark.pandas\ndef test_dataset_enable_pre_buffer(tempdir):\n    dirpath = tempdir / guid()\n    dirpath.mkdir()\n\n    df = _test_dataframe(10, seed=0)\n    path = dirpath / '{}.parquet'.format(0)\n    table = pa.Table.from_pandas(df)\n    _write_table(table, path, version='2.6')\n\n    for pre_buffer in (True, False):\n        dataset = pq.ParquetDataset(\n            dirpath, pre_buffer=pre_buffer)\n        assert dataset.read().equals(table)\n        actual = pq.read_table(dirpath, pre_buffer=pre_buffer)\n        assert actual.equals(table)\n\n\ndef _make_example_multifile_dataset(base_path, nfiles=10, file_nrows=5):\n    test_data = []\n    paths = []\n    for i in range(nfiles):\n        df = _test_dataframe(file_nrows, seed=i)\n        path = base_path / '{}.parquet'.format(i)\n\n        test_data.append(_write_table(df, path))\n        paths.append(path)\n    return paths\n\n\ndef _assert_dataset_paths(dataset, paths):\n    paths = [str(path.as_posix()) for path in paths]\n    assert set(paths) == set(dataset.files)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('dir_prefix', ['_', '.'])\ndef test_ignore_private_directories(tempdir, dir_prefix):\n    dirpath = tempdir / guid()\n    dirpath.mkdir()\n\n    paths = _make_example_multifile_dataset(dirpath, nfiles=10,\n                                            file_nrows=5)\n\n    # private directory\n    (dirpath / '{}staging'.format(dir_prefix)).mkdir()\n\n    dataset = pq.ParquetDataset(dirpath)\n\n    _assert_dataset_paths(dataset, paths)\n\n\n@pytest.mark.pandas\ndef test_ignore_hidden_files_dot(tempdir):\n    dirpath = tempdir / guid()\n    dirpath.mkdir()\n\n    paths = _make_example_multifile_dataset(dirpath, nfiles=10,\n                                            file_nrows=5)\n\n    with (dirpath / '.DS_Store').open('wb') as f:\n        f.write(b'gibberish')\n\n    with (dirpath / '.private').open('wb') as f:\n        f.write(b'gibberish')\n\n    dataset = pq.ParquetDataset(dirpath)\n\n    _assert_dataset_paths(dataset, paths)\n\n\n@pytest.mark.pandas\ndef test_ignore_hidden_files_underscore(tempdir):\n    dirpath = tempdir / guid()\n    dirpath.mkdir()\n\n    paths = _make_example_multifile_dataset(dirpath, nfiles=10,\n                                            file_nrows=5)\n\n    with (dirpath / '_committed_123').open('wb') as f:\n        f.write(b'abcd')\n\n    with (dirpath / '_started_321').open('wb') as f:\n        f.write(b'abcd')\n\n    dataset = pq.ParquetDataset(dirpath)\n\n    _assert_dataset_paths(dataset, paths)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('dir_prefix', ['_', '.'])\ndef test_ignore_no_private_directories_in_base_path(tempdir, dir_prefix):\n    # ARROW-8427 - don't ignore explicitly listed files if parent directory\n    # is a private directory\n    dirpath = tempdir / \"{0}data\".format(dir_prefix) / guid()\n    dirpath.mkdir(parents=True)\n\n    paths = _make_example_multifile_dataset(dirpath, nfiles=10,\n                                            file_nrows=5)\n\n    dataset = pq.ParquetDataset(paths)\n    _assert_dataset_paths(dataset, paths)\n\n    # ARROW-9644 - don't ignore full directory with underscore in base path\n    dataset = pq.ParquetDataset(dirpath)\n    _assert_dataset_paths(dataset, paths)\n\n\ndef test_ignore_custom_prefixes(tempdir):\n    # ARROW-9573 - allow override of default ignore_prefixes\n    part = [\"xxx\"] * 3 + [\"yyy\"] * 3\n    table = pa.table([\n        pa.array(range(len(part))),\n        pa.array(part).dictionary_encode(),\n    ], names=['index', '_part'])\n\n    pq.write_to_dataset(table, str(tempdir), partition_cols=['_part'])\n\n    private_duplicate = tempdir / '_private_duplicate'\n    private_duplicate.mkdir()\n    pq.write_to_dataset(table, str(private_duplicate),\n                        partition_cols=['_part'])\n\n    read = pq.read_table(\n        tempdir, ignore_prefixes=['_private'])\n\n    assert read.equals(table)\n\n\ndef test_empty_directory(tempdir):\n    # ARROW-5310\n    empty_dir = tempdir / 'dataset'\n    empty_dir.mkdir()\n\n    dataset = pq.ParquetDataset(empty_dir)\n    result = dataset.read()\n    assert result.num_rows == 0\n    assert result.num_columns == 0\n\n\ndef _test_write_to_dataset_with_partitions(base_path,\n                                           filesystem=None,\n                                           schema=None,\n                                           index_name=None):\n    import pandas as pd\n    import pandas.testing as tm\n\n    import pyarrow.parquet as pq\n\n    # ARROW-1400\n    output_df = pd.DataFrame({\n        'group1': list('aaabbbbccc'),\n        'group2': list('eefeffgeee'),\n        'num': list(range(10)),\n        'nan': [np.nan] * 10,\n        'date': np.arange('2017-01-01', '2017-01-11', dtype='datetime64[D]').astype(\n            'datetime64[ns]')\n    })\n    cols = output_df.columns.tolist()\n    partition_by = ['group1', 'group2']\n    output_table = pa.Table.from_pandas(output_df, schema=schema, safe=False,\n                                        preserve_index=False)\n    pq.write_to_dataset(output_table, base_path, partition_by,\n                        filesystem=filesystem)\n\n    metadata_path = os.path.join(str(base_path), '_common_metadata')\n\n    if filesystem is not None:\n        with filesystem.open(metadata_path, 'wb') as f:\n            pq.write_metadata(output_table.schema, f)\n    else:\n        pq.write_metadata(output_table.schema, metadata_path)\n\n    dataset = pq.ParquetDataset(base_path,\n                                filesystem=filesystem)\n    # ARROW-2209: Ensure the dataset schema also includes the partition columns\n    # NB schema property is an arrow and not parquet schema\n    dataset_cols = set(dataset.schema.names)\n\n    assert dataset_cols == set(output_table.schema.names)\n\n    input_table = dataset.read()\n    input_df = input_table.to_pandas()\n\n    # Read data back in and compare with original DataFrame\n    # Partitioned columns added to the end of the DataFrame when read\n    input_df_cols = input_df.columns.tolist()\n    assert partition_by == input_df_cols[-1 * len(partition_by):]\n\n    input_df = input_df[cols]\n    # Partitioned columns become 'categorical' dtypes\n    for col in partition_by:\n        output_df[col] = output_df[col].astype('category')\n\n    if schema:\n        expected_date_type = schema.field('date').type.to_pandas_dtype()\n        output_df[\"date\"] = output_df[\"date\"].astype(expected_date_type)\n\n    tm.assert_frame_equal(output_df, input_df)\n\n\ndef _test_write_to_dataset_no_partitions(base_path,\n                                         filesystem=None):\n    import pandas as pd\n\n    import pyarrow.parquet as pq\n\n    # ARROW-1400\n    output_df = pd.DataFrame({\n        'group1': list('aaabbbbccc'),\n        'group2': list('eefeffgeee'),\n        'num': list(range(10)),\n        'date': np.arange('2017-01-01', '2017-01-11', dtype='datetime64[D]').astype(\n            'datetime64[ns]')\n    })\n    cols = output_df.columns.tolist()\n    output_table = pa.Table.from_pandas(output_df)\n\n    if filesystem is None:\n        filesystem = LocalFileSystem()\n    elif not isinstance(filesystem, FileSystem):\n        filesystem = PyFileSystem(FSSpecHandler(filesystem))\n\n    # Without partitions, append files to root_path\n    n = 5\n    for i in range(n):\n        pq.write_to_dataset(output_table, base_path,\n                            filesystem=filesystem)\n\n    selector = FileSelector(str(base_path), allow_not_found=False,\n                            recursive=True)\n\n    infos = filesystem.get_file_info(selector)\n    output_files = [info for info in infos if info.path.endswith(\".parquet\")]\n    assert len(output_files) == n\n\n    # Deduplicated incoming DataFrame should match\n    # original outgoing Dataframe\n    input_table = pq.ParquetDataset(\n        base_path, filesystem=filesystem\n    ).read()\n    input_df = input_table.to_pandas()\n    input_df = input_df.drop_duplicates()\n    input_df = input_df[cols]\n    tm.assert_frame_equal(output_df, input_df)\n\n\n@pytest.mark.pandas\ndef test_write_to_dataset_with_partitions(tempdir):\n    _test_write_to_dataset_with_partitions(str(tempdir))\n\n\n@pytest.mark.pandas\ndef test_write_to_dataset_with_partitions_and_schema(tempdir):\n    schema = pa.schema([pa.field('group1', type=pa.string()),\n                        pa.field('group2', type=pa.string()),\n                        pa.field('num', type=pa.int64()),\n                        pa.field('nan', type=pa.int32()),\n                        pa.field('date', type=pa.timestamp(unit='us'))])\n    _test_write_to_dataset_with_partitions(\n        str(tempdir), schema=schema)\n\n\n@pytest.mark.pandas\ndef test_write_to_dataset_with_partitions_and_index_name(tempdir):\n    _test_write_to_dataset_with_partitions(\n        str(tempdir), index_name='index_name')\n\n\n@pytest.mark.pandas\ndef test_write_to_dataset_no_partitions(tempdir):\n    _test_write_to_dataset_no_partitions(str(tempdir))\n\n\n@pytest.mark.pandas\ndef test_write_to_dataset_pathlib(tempdir):\n    _test_write_to_dataset_with_partitions(tempdir / \"test1\")\n    _test_write_to_dataset_no_partitions(tempdir / \"test2\")\n\n\n@pytest.mark.pandas\n@pytest.mark.s3\ndef test_write_to_dataset_pathlib_nonlocal(tempdir, s3_example_s3fs):\n    # pathlib paths are only accepted for local files\n    fs, _ = s3_example_s3fs\n\n    with pytest.raises(TypeError, match=\"path-like objects are only allowed\"):\n        _test_write_to_dataset_with_partitions(\n            tempdir / \"test1\", filesystem=fs)\n\n    with pytest.raises(TypeError, match=\"path-like objects are only allowed\"):\n        _test_write_to_dataset_no_partitions(\n            tempdir / \"test2\", filesystem=fs)\n\n\n@pytest.mark.pandas\n@pytest.mark.s3\ndef test_write_to_dataset_with_partitions_s3fs(s3_example_s3fs):\n    fs, path = s3_example_s3fs\n\n    _test_write_to_dataset_with_partitions(\n        path, filesystem=fs)\n\n\n@pytest.mark.pandas\n@pytest.mark.s3\ndef test_write_to_dataset_no_partitions_s3fs(s3_example_s3fs):\n    fs, path = s3_example_s3fs\n\n    _test_write_to_dataset_no_partitions(\n        path, filesystem=fs)\n\n\n@pytest.mark.pandas\ndef test_write_to_dataset_filesystem(tempdir):\n    df = pd.DataFrame({'A': [1, 2, 3]})\n    table = pa.Table.from_pandas(df)\n    path = str(tempdir)\n\n    pq.write_to_dataset(table, path, filesystem=LocalFileSystem())\n    result = pq.read_table(path)\n    assert result.equals(table)\n\n\ndef _make_dataset_for_pickling(tempdir, N=100):\n    path = tempdir / 'data.parquet'\n    local = LocalFileSystem()\n\n    df = pd.DataFrame({\n        'index': np.arange(N),\n        'values': np.random.randn(N)\n    }, columns=['index', 'values'])\n    table = pa.Table.from_pandas(df)\n\n    num_groups = 3\n    with pq.ParquetWriter(path, table.schema) as writer:\n        for i in range(num_groups):\n            writer.write_table(table)\n\n    reader = pq.ParquetFile(path)\n    assert reader.metadata.num_row_groups == num_groups\n\n    metadata_path = tempdir / '_metadata'\n    with local.open_output_stream(str(metadata_path)) as f:\n        pq.write_metadata(table.schema, f)\n\n    dataset = pq.ParquetDataset(\n        tempdir, filesystem=local)\n\n    return dataset\n\n\n@pytest.mark.pandas\ndef test_pickle_dataset(tempdir, pickle_module):\n    def is_pickleable(obj):\n        return obj == pickle_module.loads(pickle_module.dumps(obj))\n\n    dataset = _make_dataset_for_pickling(tempdir)\n    assert is_pickleable(dataset)\n\n\n@pytest.mark.pandas\ndef test_partitioned_dataset(tempdir):\n    # ARROW-3208: Segmentation fault when reading a Parquet partitioned dataset\n    # to a Parquet file\n    path = tempdir / \"ARROW-3208\"\n    df = pd.DataFrame({\n        'one': [-1, 10, 2.5, 100, 1000, 1, 29.2],\n        'two': [-1, 10, 2, 100, 1000, 1, 11],\n        'three': [0, 0, 0, 0, 0, 0, 0]\n    })\n    table = pa.Table.from_pandas(df)\n    pq.write_to_dataset(table, root_path=str(path),\n                        partition_cols=['one', 'two'])\n    table = pq.ParquetDataset(path).read()\n    pq.write_table(table, path / \"output.parquet\")\n\n\ndef test_dataset_read_dictionary(tempdir):\n    path = tempdir / \"ARROW-3325-dataset\"\n    t1 = pa.table([[util.rands(10) for i in range(5)] * 10], names=['f0'])\n    t2 = pa.table([[util.rands(10) for i in range(5)] * 10], names=['f0'])\n    pq.write_to_dataset(t1, root_path=str(path))\n    pq.write_to_dataset(t2, root_path=str(path))\n\n    result = pq.ParquetDataset(\n        path, read_dictionary=['f0']).read()\n\n    # The order of the chunks is non-deterministic\n    ex_chunks = [t1[0].chunk(0).dictionary_encode(),\n                 t2[0].chunk(0).dictionary_encode()]\n\n    assert result[0].num_chunks == 2\n    c0, c1 = result[0].chunk(0), result[0].chunk(1)\n    if c0.equals(ex_chunks[0]):\n        assert c1.equals(ex_chunks[1])\n    else:\n        assert c0.equals(ex_chunks[1])\n        assert c1.equals(ex_chunks[0])\n\n\ndef test_read_table_schema(tempdir):\n    # test that schema keyword is passed through in read_table\n    table = pa.table({'a': pa.array([1, 2, 3], pa.int32())})\n    pq.write_table(table, tempdir / \"data1.parquet\")\n    pq.write_table(table, tempdir / \"data2.parquet\")\n\n    schema = pa.schema([('a', 'int64')])\n\n    # reading single file (which is special cased in the code)\n    result = pq.read_table(tempdir / \"data1.parquet\", schema=schema)\n    expected = pa.table({'a': [1, 2, 3]}, schema=schema)\n    assert result.equals(expected)\n\n    # reading multiple fields\n    result = pq.read_table(tempdir, schema=schema)\n    expected = pa.table({'a': [1, 2, 3, 1, 2, 3]}, schema=schema)\n    assert result.equals(expected)\n\n    result = pq.ParquetDataset(tempdir, schema=schema)\n    expected = pa.table({'a': [1, 2, 3, 1, 2, 3]}, schema=schema)\n    assert result.read().equals(expected)\n\n\ndef test_read_table_duplicate_column_selection(tempdir):\n    # test that duplicate column selection gives duplicate columns\n    table = pa.table({'a': pa.array([1, 2, 3], pa.int32()),\n                      'b': pa.array([1, 2, 3], pa.uint8())})\n    pq.write_table(table, tempdir / \"data.parquet\")\n\n    result = pq.read_table(tempdir / \"data.parquet\", columns=['a', 'a'])\n    expected_schema = pa.schema([('a', 'int32'), ('a', 'int32')])\n\n    assert result.column_names == ['a', 'a']\n    assert result.schema == expected_schema\n\n\ndef test_dataset_partitioning(tempdir):\n    import pyarrow.dataset as ds\n\n    # create small dataset with directory partitioning\n    root_path = tempdir / \"test_partitioning\"\n    (root_path / \"2012\" / \"10\" / \"01\").mkdir(parents=True)\n\n    table = pa.table({'a': [1, 2, 3]})\n    pq.write_table(\n        table, str(root_path / \"2012\" / \"10\" / \"01\" / \"data.parquet\"))\n\n    # This works with new dataset API\n\n    # read_table\n    part = ds.partitioning(field_names=[\"year\", \"month\", \"day\"])\n    result = pq.read_table(\n        str(root_path), partitioning=part)\n    assert result.column_names == [\"a\", \"year\", \"month\", \"day\"]\n\n    result = pq.ParquetDataset(\n        str(root_path), partitioning=part).read()\n    assert result.column_names == [\"a\", \"year\", \"month\", \"day\"]\n\n\ndef test_parquet_dataset_new_filesystem(tempdir):\n    # Ensure we can pass new FileSystem object to ParquetDataset\n    table = pa.table({'a': [1, 2, 3]})\n    pq.write_table(table, tempdir / 'data.parquet')\n    filesystem = SubTreeFileSystem(str(tempdir), LocalFileSystem())\n    dataset = pq.ParquetDataset('.', filesystem=filesystem)\n    result = dataset.read()\n    assert result.equals(table)\n\n\ndef test_parquet_dataset_partitions_piece_path_with_fsspec(tempdir):\n    # ARROW-10462 ensure that on Windows we properly use posix-style paths\n    # as used by fsspec\n    fsspec = pytest.importorskip(\"fsspec\")\n    filesystem = fsspec.filesystem('file')\n    table = pa.table({'a': [1, 2, 3]})\n    pq.write_table(table, tempdir / 'data.parquet')\n\n    # pass a posix-style path (using \"/\" also on Windows)\n    path = str(tempdir).replace(\"\\\\\", \"/\")\n    dataset = pq.ParquetDataset(\n        path, filesystem=filesystem)\n    # ensure the piece path is also posix-style\n    expected = path + \"/data.parquet\"\n    assert dataset.fragments[0].path == expected\n\n\ndef test_parquet_write_to_dataset_exposed_keywords(tempdir):\n    table = pa.table({'a': [1, 2, 3]})\n    path = tempdir / 'partitioning'\n\n    paths_written = []\n\n    def file_visitor(written_file):\n        paths_written.append(written_file.path)\n\n    basename_template = 'part-{i}.parquet'\n\n    pq.write_to_dataset(table, path, partitioning=[\"a\"],\n                        file_visitor=file_visitor,\n                        basename_template=basename_template)\n\n    expected_paths = {\n        path / '1' / 'part-0.parquet',\n        path / '2' / 'part-0.parquet',\n        path / '3' / 'part-0.parquet'\n    }\n    paths_written_set = set(map(pathlib.Path, paths_written))\n    assert paths_written_set == expected_paths\n\n\n@pytest.mark.parametrize(\"write_dataset_kwarg\", (\n    (\"create_dir\", True),\n    (\"create_dir\", False),\n))\ndef test_write_to_dataset_kwargs_passed(tempdir, write_dataset_kwarg):\n    \"\"\"Verify kwargs in pq.write_to_dataset are passed onto ds.write_dataset\"\"\"\n    import pyarrow.dataset as ds\n\n    table = pa.table({\"a\": [1, 2, 3]})\n    path = tempdir / 'out.parquet'\n\n    signature = inspect.signature(ds.write_dataset)\n    key, arg = write_dataset_kwarg\n\n    # kwarg not in pq.write_to_dataset, but will be passed to ds.write_dataset\n    assert key not in inspect.signature(pq.write_to_dataset).parameters\n    assert key in signature.parameters\n\n    with mock.patch.object(ds, \"write_dataset\", autospec=True)\\\n            as mock_write_dataset:\n        pq.write_to_dataset(table, path, **{key: arg})\n        _name, _args, kwargs = mock_write_dataset.mock_calls[0]\n        assert kwargs[key] == arg\n\n\n@pytest.mark.pandas\ndef test_write_to_dataset_category_observed(tempdir):\n    # if we partition on a categorical variable with \"unobserved\" categories\n    # (values present in the dictionary, but not in the actual data)\n    # ensure those are not creating empty files/directories\n    df = pd.DataFrame({\n        \"cat\": pd.Categorical([\"a\", \"b\", \"a\"], categories=[\"a\", \"b\", \"c\"]),\n        \"col\": [1, 2, 3]\n    })\n    table = pa.table(df)\n    path = tempdir / \"dataset\"\n    pq.write_to_dataset(\n        table, tempdir / \"dataset\", partition_cols=[\"cat\"]\n    )\n    subdirs = [f.name for f in path.iterdir() if f.is_dir()]\n    assert len(subdirs) == 2\n    assert \"cat=c\" not in subdirs\n", "python/pyarrow/tests/parquet/conftest.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\n\nfrom pyarrow.util import guid\n\n\n@pytest.fixture(scope='module')\ndef datadir(base_datadir):\n    return base_datadir / 'parquet'\n\n\n@pytest.fixture\ndef s3_bucket(s3_server):\n    boto3 = pytest.importorskip('boto3')\n    botocore = pytest.importorskip('botocore')\n    s3_bucket_name = 'test-s3fs'\n\n    host, port, access_key, secret_key = s3_server['connection']\n    s3_client = boto3.client(\n        's3',\n        endpoint_url='http://{}:{}'.format(host, port),\n        aws_access_key_id=access_key,\n        aws_secret_access_key=secret_key,\n        config=botocore.client.Config(signature_version='s3v4'),\n        region_name='us-east-1'\n    )\n\n    try:\n        s3_client.create_bucket(Bucket=s3_bucket_name)\n    except Exception:\n        pass  # we get BucketAlreadyOwnedByYou error with fsspec handler\n    finally:\n        s3_client.close()\n\n    return s3_bucket_name\n\n\n@pytest.fixture\ndef s3_example_s3fs(s3_server, s3_bucket):\n    s3fs = pytest.importorskip('s3fs')\n\n    host, port, access_key, secret_key = s3_server['connection']\n    fs = s3fs.S3FileSystem(\n        key=access_key,\n        secret=secret_key,\n        client_kwargs={\n            'endpoint_url': 'http://{}:{}'.format(host, port)\n        }\n    )\n\n    test_path = '{}/{}'.format(s3_bucket, guid())\n\n    fs.mkdir(test_path)\n    yield fs, test_path\n    try:\n        fs.rm(test_path, recursive=True)\n    except FileNotFoundError:\n        pass\n\n\n@pytest.fixture\ndef s3_example_fs(s3_server):\n    from pyarrow.fs import FileSystem\n\n    host, port, access_key, secret_key = s3_server['connection']\n    uri = (\n        \"s3://{}:{}@mybucket/data.parquet?scheme=http&endpoint_override={}:{}\"\n        \"&allow_bucket_creation=True\"\n        .format(access_key, secret_key, host, port)\n    )\n    fs, path = FileSystem.from_uri(uri)\n\n    fs.create_dir(\"mybucket\")\n\n    yield fs, uri, path\n", "python/pyarrow/tests/parquet/test_parquet_writer.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\n\nimport pyarrow as pa\nfrom pyarrow import fs\n\ntry:\n    import pyarrow.parquet as pq\n    from pyarrow.tests.parquet.common import (_read_table, _test_dataframe,\n                                              _range_integers)\nexcept ImportError:\n    pq = None\n\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n\nexcept ImportError:\n    pd = tm = None\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = pytest.mark.parquet\n\n\n@pytest.mark.pandas\ndef test_parquet_incremental_file_build(tempdir):\n    df = _test_dataframe(100)\n    df['unique_id'] = 0\n\n    arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n    out = pa.BufferOutputStream()\n\n    writer = pq.ParquetWriter(out, arrow_table.schema, version='2.6')\n\n    frames = []\n    for i in range(10):\n        df['unique_id'] = i\n        arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n        writer.write_table(arrow_table)\n\n        frames.append(df.copy())\n\n    writer.close()\n\n    buf = out.getvalue()\n    result = _read_table(pa.BufferReader(buf))\n\n    expected = pd.concat(frames, ignore_index=True)\n    tm.assert_frame_equal(result.to_pandas(), expected)\n\n\ndef test_validate_schema_write_table(tempdir):\n    # ARROW-2926\n    simple_fields = [\n        pa.field('POS', pa.uint32()),\n        pa.field('desc', pa.string())\n    ]\n\n    simple_schema = pa.schema(simple_fields)\n\n    # simple_table schema does not match simple_schema\n    simple_from_array = [pa.array([1]), pa.array(['bla'])]\n    simple_table = pa.Table.from_arrays(simple_from_array, ['POS', 'desc'])\n\n    path = tempdir / 'simple_validate_schema.parquet'\n\n    with pq.ParquetWriter(path, simple_schema,\n                          version='2.6',\n                          compression='snappy', flavor='spark') as w:\n        with pytest.raises(ValueError):\n            w.write_table(simple_table)\n\n\ndef test_parquet_invalid_writer(tempdir):\n    # avoid segfaults with invalid construction\n    with pytest.raises(TypeError):\n        some_schema = pa.schema([pa.field(\"x\", pa.int32())])\n        pq.ParquetWriter(None, some_schema)\n\n    with pytest.raises(TypeError):\n        pq.ParquetWriter(tempdir / \"some_path\", None)\n\n\n@pytest.mark.pandas\ndef test_parquet_writer_context_obj(tempdir):\n    df = _test_dataframe(100)\n    df['unique_id'] = 0\n\n    arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n    out = pa.BufferOutputStream()\n\n    with pq.ParquetWriter(out, arrow_table.schema, version='2.6') as writer:\n\n        frames = []\n        for i in range(10):\n            df['unique_id'] = i\n            arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n            writer.write_table(arrow_table)\n\n            frames.append(df.copy())\n\n    buf = out.getvalue()\n    result = _read_table(pa.BufferReader(buf))\n\n    expected = pd.concat(frames, ignore_index=True)\n    tm.assert_frame_equal(result.to_pandas(), expected)\n\n\n@pytest.mark.pandas\ndef test_parquet_writer_context_obj_with_exception(tempdir):\n    df = _test_dataframe(100)\n    df['unique_id'] = 0\n\n    arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n    out = pa.BufferOutputStream()\n    error_text = 'Artificial Error'\n\n    try:\n        with pq.ParquetWriter(out,\n                              arrow_table.schema,\n                              version='2.6') as writer:\n\n            frames = []\n            for i in range(10):\n                df['unique_id'] = i\n                arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n                writer.write_table(arrow_table)\n                frames.append(df.copy())\n                if i == 5:\n                    raise ValueError(error_text)\n    except Exception as e:\n        assert str(e) == error_text\n\n    buf = out.getvalue()\n    result = _read_table(pa.BufferReader(buf))\n\n    expected = pd.concat(frames, ignore_index=True)\n    tm.assert_frame_equal(result.to_pandas(), expected)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\"filesystem\", [\n    None,\n    fs.LocalFileSystem(),\n])\ndef test_parquet_writer_write_wrappers(tempdir, filesystem):\n    df = _test_dataframe(100)\n    table = pa.Table.from_pandas(df, preserve_index=False)\n    batch = pa.RecordBatch.from_pandas(df, preserve_index=False)\n    path_table = str(tempdir / 'data_table.parquet')\n    path_batch = str(tempdir / 'data_batch.parquet')\n\n    with pq.ParquetWriter(\n        path_table, table.schema, filesystem=filesystem, version='2.6'\n    ) as writer:\n        writer.write_table(table)\n\n    result = _read_table(path_table).to_pandas()\n    tm.assert_frame_equal(result, df)\n\n    with pq.ParquetWriter(\n        path_batch, table.schema, filesystem=filesystem, version='2.6'\n    ) as writer:\n        writer.write_batch(batch)\n\n    result = _read_table(path_batch).to_pandas()\n    tm.assert_frame_equal(result, df)\n\n    with pq.ParquetWriter(\n        path_table, table.schema, filesystem=filesystem, version='2.6'\n    ) as writer:\n        writer.write(table)\n\n    result = _read_table(path_table).to_pandas()\n    tm.assert_frame_equal(result, df)\n\n    with pq.ParquetWriter(\n        path_batch, table.schema, filesystem=filesystem, version='2.6'\n    ) as writer:\n        writer.write(batch)\n\n    result = _read_table(path_batch).to_pandas()\n    tm.assert_frame_equal(result, df)\n\n\n@pytest.mark.large_memory\n@pytest.mark.pandas\ndef test_parquet_writer_chunk_size(tempdir):\n    default_chunk_size = 1024 * 1024\n    abs_max_chunk_size = 64 * 1024 * 1024\n\n    def check_chunk_size(data_size, chunk_size, expect_num_chunks):\n        table = pa.Table.from_arrays([\n            _range_integers(data_size, 'b')\n        ], names=['x'])\n        if chunk_size is None:\n            pq.write_table(table, tempdir / 'test.parquet')\n        else:\n            pq.write_table(table, tempdir / 'test.parquet', row_group_size=chunk_size)\n        metadata = pq.read_metadata(tempdir / 'test.parquet')\n        expected_chunk_size = default_chunk_size if chunk_size is None else chunk_size\n        assert metadata.num_row_groups == expect_num_chunks\n        latched_chunk_size = min(expected_chunk_size, abs_max_chunk_size)\n        # First chunks should be full size\n        for chunk_idx in range(expect_num_chunks - 1):\n            assert metadata.row_group(chunk_idx).num_rows == latched_chunk_size\n        # Last chunk may be smaller\n        remainder = data_size - (expected_chunk_size * (expect_num_chunks - 1))\n        if remainder == 0:\n            assert metadata.row_group(\n                expect_num_chunks - 1).num_rows == latched_chunk_size\n        else:\n            assert metadata.row_group(expect_num_chunks - 1).num_rows == remainder\n\n    check_chunk_size(default_chunk_size * 2, default_chunk_size - 100, 3)\n    check_chunk_size(default_chunk_size * 2, default_chunk_size, 2)\n    check_chunk_size(default_chunk_size * 2, default_chunk_size + 100, 2)\n    check_chunk_size(default_chunk_size + 100, default_chunk_size + 100, 1)\n    # Even though the chunk size requested is large enough it will be capped\n    # by the absolute max chunk size\n    check_chunk_size(abs_max_chunk_size * 2, abs_max_chunk_size * 2, 2)\n\n    # These tests don't pass a chunk_size to write_table and so the chunk size\n    # should be default_chunk_size\n    check_chunk_size(default_chunk_size, None, 1)\n    check_chunk_size(default_chunk_size + 1, None, 2)\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize(\"filesystem\", [\n    None,\n    fs.LocalFileSystem(),\n])\ndef test_parquet_writer_filesystem_local(tempdir, filesystem):\n    df = _test_dataframe(100)\n    table = pa.Table.from_pandas(df, preserve_index=False)\n    path = str(tempdir / 'data.parquet')\n\n    with pq.ParquetWriter(\n        path, table.schema, filesystem=filesystem, version='2.6'\n    ) as writer:\n        writer.write_table(table)\n\n    result = _read_table(path).to_pandas()\n    tm.assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\n@pytest.mark.s3\ndef test_parquet_writer_filesystem_s3(s3_example_fs):\n    df = _test_dataframe(100)\n    table = pa.Table.from_pandas(df, preserve_index=False)\n\n    fs, uri, path = s3_example_fs\n\n    with pq.ParquetWriter(\n        path, table.schema, filesystem=fs, version='2.6'\n    ) as writer:\n        writer.write_table(table)\n\n    result = _read_table(uri).to_pandas()\n    tm.assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\n@pytest.mark.s3\ndef test_parquet_writer_filesystem_s3_uri(s3_example_fs):\n    df = _test_dataframe(100)\n    table = pa.Table.from_pandas(df, preserve_index=False)\n\n    fs, uri, path = s3_example_fs\n\n    with pq.ParquetWriter(uri, table.schema, version='2.6') as writer:\n        writer.write_table(table)\n\n    result = _read_table(path, filesystem=fs).to_pandas()\n    tm.assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\n@pytest.mark.s3\ndef test_parquet_writer_filesystem_s3fs(s3_example_s3fs):\n    df = _test_dataframe(100)\n    table = pa.Table.from_pandas(df, preserve_index=False)\n\n    fs, directory = s3_example_s3fs\n    path = directory + \"/test.parquet\"\n\n    with pq.ParquetWriter(\n        path, table.schema, filesystem=fs, version='2.6'\n    ) as writer:\n        writer.write_table(table)\n\n    result = _read_table(path, filesystem=fs).to_pandas()\n    tm.assert_frame_equal(result, df)\n\n\n@pytest.mark.pandas\ndef test_parquet_writer_filesystem_buffer_raises():\n    df = _test_dataframe(100)\n    table = pa.Table.from_pandas(df, preserve_index=False)\n    filesystem = fs.LocalFileSystem()\n\n    # Should raise ValueError when filesystem is passed with file-like object\n    with pytest.raises(ValueError, match=\"specified path is file-like\"):\n        pq.ParquetWriter(\n            pa.BufferOutputStream(), table.schema, filesystem=filesystem\n        )\n\n\ndef test_parquet_writer_store_schema(tempdir):\n    table = pa.table({'a': [1, 2, 3]})\n\n    # default -> write schema information\n    path1 = tempdir / 'test_with_schema.parquet'\n    with pq.ParquetWriter(path1, table.schema) as writer:\n        writer.write_table(table)\n\n    meta = pq.read_metadata(path1)\n    assert b'ARROW:schema' in meta.metadata\n    assert meta.metadata[b'ARROW:schema']\n\n    # disable adding schema information\n    path2 = tempdir / 'test_without_schema.parquet'\n    with pq.ParquetWriter(path2, table.schema, store_schema=False) as writer:\n        writer.write_table(table)\n\n    meta = pq.read_metadata(path2)\n    assert meta.metadata is None\n\n\ndef test_parquet_writer_append_key_value_metadata(tempdir):\n    table = pa.Table.from_arrays([pa.array([], type='int32')], ['f0'])\n    path = tempdir / 'metadata.parquet'\n\n    with pq.ParquetWriter(path, table.schema) as writer:\n        writer.write_table(table)\n        writer.add_key_value_metadata({'key1': '1', 'key2': 'x'})\n        writer.add_key_value_metadata({'key2': '2', 'key3': '3'})\n    reader = pq.ParquetFile(path)\n    metadata = reader.metadata.metadata\n    assert metadata[b'key1'] == b'1'\n    assert metadata[b'key2'] == b'2'\n    assert metadata[b'key3'] == b'3'\n", "python/pyarrow/tests/parquet/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = [\n    pytest.mark.parquet,\n]\n", "python/pyarrow/tests/parquet/test_data_types.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport decimal\nimport io\n\nimport numpy as np\nimport pytest\n\nimport pyarrow as pa\nfrom pyarrow.tests import util\nfrom pyarrow.tests.parquet.common import _check_roundtrip\n\ntry:\n    import pyarrow.parquet as pq\n    from pyarrow.tests.parquet.common import _read_table, _write_table\nexcept ImportError:\n    pq = None\n\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n\n    from pyarrow.tests.pandas_examples import (dataframe_with_arrays,\n                                               dataframe_with_lists)\n    from pyarrow.tests.parquet.common import alltypes_sample\nexcept ImportError:\n    pd = tm = None\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = pytest.mark.parquet\n\n\n# General roundtrip of data types\n# -----------------------------------------------------------------------------\n\n\n@pytest.mark.pandas\n@pytest.mark.parametrize('chunk_size', [None, 1000])\ndef test_parquet_2_0_roundtrip(tempdir, chunk_size):\n    df = alltypes_sample(size=10000, categorical=True)\n\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df)\n    assert arrow_table.schema.pandas_metadata is not None\n\n    _write_table(arrow_table, filename, version='2.6',\n                 chunk_size=chunk_size)\n    table_read = pq.read_pandas(filename)\n    assert table_read.schema.pandas_metadata is not None\n\n    read_metadata = table_read.schema.metadata\n    assert arrow_table.schema.metadata == read_metadata\n\n    df_read = table_read.to_pandas()\n    tm.assert_frame_equal(df, df_read)\n\n\n@pytest.mark.pandas\ndef test_parquet_1_0_roundtrip(tempdir):\n    size = 10000\n    np.random.seed(0)\n    df = pd.DataFrame({\n        'uint8': np.arange(size, dtype=np.uint8),\n        'uint16': np.arange(size, dtype=np.uint16),\n        'uint32': np.arange(size, dtype=np.uint32),\n        'uint64': np.arange(size, dtype=np.uint64),\n        'int8': np.arange(size, dtype=np.int16),\n        'int16': np.arange(size, dtype=np.int16),\n        'int32': np.arange(size, dtype=np.int32),\n        'int64': np.arange(size, dtype=np.int64),\n        'float32': np.arange(size, dtype=np.float32),\n        'float64': np.arange(size, dtype=np.float64),\n        'bool': np.random.randn(size) > 0,\n        'str': [str(x) for x in range(size)],\n        'str_with_nulls': [None] + [str(x) for x in range(size - 2)] + [None],\n        'empty_str': [''] * size\n    })\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df)\n    _write_table(arrow_table, filename, version='1.0')\n    table_read = _read_table(filename)\n    df_read = table_read.to_pandas()\n\n    # We pass uint32_t as int64_t if we write Parquet version 1.0\n    df['uint32'] = df['uint32'].values.astype(np.int64)\n\n    tm.assert_frame_equal(df, df_read)\n\n\n# Dictionary\n# -----------------------------------------------------------------------------\n\n\ndef _simple_table_write_read(table):\n    bio = pa.BufferOutputStream()\n    pq.write_table(table, bio)\n    contents = bio.getvalue()\n    return pq.read_table(\n        pa.BufferReader(contents)\n    )\n\n\n@pytest.mark.pandas\ndef test_direct_read_dictionary():\n    # ARROW-3325\n    repeats = 10\n    nunique = 5\n\n    data = [\n        [util.rands(10) for i in range(nunique)] * repeats,\n\n    ]\n    table = pa.table(data, names=['f0'])\n\n    bio = pa.BufferOutputStream()\n    pq.write_table(table, bio)\n    contents = bio.getvalue()\n\n    result = pq.read_table(pa.BufferReader(contents),\n                           read_dictionary=['f0'])\n\n    # Compute dictionary-encoded subfield\n    expected = pa.table([table[0].dictionary_encode()], names=['f0'])\n    assert result.equals(expected)\n\n\n@pytest.mark.pandas\ndef test_direct_read_dictionary_subfield():\n    repeats = 10\n    nunique = 5\n\n    data = [\n        [[util.rands(10)] for i in range(nunique)] * repeats,\n    ]\n    table = pa.table(data, names=['f0'])\n\n    bio = pa.BufferOutputStream()\n    pq.write_table(table, bio)\n    contents = bio.getvalue()\n    result = pq.read_table(pa.BufferReader(contents),\n                           read_dictionary=['f0.list.element'])\n\n    arr = pa.array(data[0])\n    values_as_dict = arr.values.dictionary_encode()\n\n    inner_indices = values_as_dict.indices.cast('int32')\n    new_values = pa.DictionaryArray.from_arrays(inner_indices,\n                                                values_as_dict.dictionary)\n\n    offsets = pa.array(range(51), type='int32')\n    expected_arr = pa.ListArray.from_arrays(offsets, new_values)\n    expected = pa.table([expected_arr], names=['f0'])\n\n    assert result.equals(expected)\n    assert result[0].num_chunks == 1\n\n\ndef test_dictionary_array_automatically_read():\n    # ARROW-3246\n\n    # Make a large dictionary, a little over 4MB of data\n    dict_length = 4000\n    dict_values = pa.array([('x' * 1000 + '_{}'.format(i))\n                            for i in range(dict_length)])\n\n    num_chunks = 10\n    chunk_size = 100\n    chunks = []\n    for i in range(num_chunks):\n        indices = np.random.randint(0, dict_length,\n                                    size=chunk_size).astype(np.int32)\n        chunks.append(pa.DictionaryArray.from_arrays(pa.array(indices),\n                                                     dict_values))\n\n    table = pa.table([pa.chunked_array(chunks)], names=['f0'])\n    result = _simple_table_write_read(table)\n\n    assert result.equals(table)\n\n    # The only key in the metadata was the Arrow schema key\n    assert result.schema.metadata is None\n\n\n# Decimal\n# -----------------------------------------------------------------------------\n\n\n@pytest.mark.pandas\ndef test_decimal_roundtrip(tempdir):\n    num_values = 10\n\n    columns = {}\n    for precision in range(1, 39):\n        for scale in range(0, precision + 1):\n            with util.random_seed(0):\n                random_decimal_values = [\n                    util.randdecimal(precision, scale)\n                    for _ in range(num_values)\n                ]\n            column_name = ('dec_precision_{:d}_scale_{:d}'\n                           .format(precision, scale))\n            columns[column_name] = random_decimal_values\n\n    expected = pd.DataFrame(columns)\n    filename = tempdir / 'decimals.parquet'\n    string_filename = str(filename)\n    table = pa.Table.from_pandas(expected)\n    _write_table(table, string_filename)\n    result_table = _read_table(string_filename)\n    result = result_table.to_pandas()\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.pandas\n@pytest.mark.xfail(\n    raises=OSError, reason='Parquet does not support negative scale'\n)\ndef test_decimal_roundtrip_negative_scale(tempdir):\n    expected = pd.DataFrame({'decimal_num': [decimal.Decimal('1.23E4')]})\n    filename = tempdir / 'decimals.parquet'\n    string_filename = str(filename)\n    t = pa.Table.from_pandas(expected)\n    _write_table(t, string_filename)\n    result_table = _read_table(string_filename)\n    result = result_table.to_pandas()\n    tm.assert_frame_equal(result, expected)\n\n\n# List types\n# -----------------------------------------------------------------------------\n\n\n@pytest.mark.parametrize('dtype', [int, float])\ndef test_single_pylist_column_roundtrip(tempdir, dtype,):\n    filename = tempdir / 'single_{}_column.parquet'.format(dtype.__name__)\n    data = [pa.array(list(map(dtype, range(5))))]\n    table = pa.Table.from_arrays(data, names=['a'])\n    _write_table(table, filename)\n    table_read = _read_table(filename)\n    for i in range(table.num_columns):\n        col_written = table[i]\n        col_read = table_read[i]\n        assert table.field(i).name == table_read.field(i).name\n        assert col_read.num_chunks == 1\n        data_written = col_written.chunk(0)\n        data_read = col_read.chunk(0)\n        assert data_written.equals(data_read)\n\n\ndef test_empty_lists_table_roundtrip():\n    # ARROW-2744: Shouldn't crash when writing an array of empty lists\n    arr = pa.array([[], []], type=pa.list_(pa.int32()))\n    table = pa.Table.from_arrays([arr], [\"A\"])\n    _check_roundtrip(table)\n\n\ndef test_nested_list_nonnullable_roundtrip_bug():\n    # Reproduce failure in ARROW-5630\n    typ = pa.list_(pa.field(\"item\", pa.float32(), False))\n    num_rows = 10000\n    t = pa.table([\n        pa.array(([[0] * ((i + 5) % 10) for i in range(0, 10)] *\n                  (num_rows // 10)), type=typ)\n    ], ['a'])\n    _check_roundtrip(\n        t, data_page_size=4096)\n\n\ndef test_nested_list_struct_multiple_batches_roundtrip(tempdir):\n    # Reproduce failure in ARROW-11024\n    data = [[{'x': 'abc', 'y': 'abc'}]]*100 + [[{'x': 'abc', 'y': 'gcb'}]]*100\n    table = pa.table([pa.array(data)], names=['column'])\n    _check_roundtrip(\n        table, row_group_size=20)\n\n    # Reproduce failure in ARROW-11069 (plain non-nested structs with strings)\n    data = pa.array(\n        [{'a': '1', 'b': '2'}, {'a': '3', 'b': '4'}, {'a': '5', 'b': '6'}]*10\n    )\n    table = pa.table({'column': data})\n    _check_roundtrip(table, row_group_size=10)\n\n\ndef test_writing_empty_lists():\n    # ARROW-2591: [Python] Segmentation fault issue in pq.write_table\n    arr1 = pa.array([[], []], pa.list_(pa.int32()))\n    table = pa.Table.from_arrays([arr1], ['list(int32)'])\n    _check_roundtrip(table)\n\n\n@pytest.mark.pandas\ndef test_column_of_arrays(tempdir):\n    df, schema = dataframe_with_arrays()\n\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df, schema=schema)\n    _write_table(arrow_table, filename, version='2.6', coerce_timestamps='ms')\n    table_read = _read_table(filename)\n    df_read = table_read.to_pandas()\n    tm.assert_frame_equal(df, df_read)\n\n\n@pytest.mark.pandas\ndef test_column_of_lists(tempdir):\n    df, schema = dataframe_with_lists(parquet_compatible=True)\n\n    filename = tempdir / 'pandas_roundtrip.parquet'\n    arrow_table = pa.Table.from_pandas(df, schema=schema)\n    _write_table(arrow_table, filename, version='2.6')\n    table_read = _read_table(filename)\n    df_read = table_read.to_pandas()\n\n    tm.assert_frame_equal(df, df_read)\n\n\ndef test_large_list_records():\n    # This was fixed in PARQUET-1100\n\n    list_lengths = np.random.randint(0, 500, size=50)\n    list_lengths[::10] = 0\n\n    list_values = [list(map(int, np.random.randint(0, 100, size=x)))\n                   if i % 8 else None\n                   for i, x in enumerate(list_lengths)]\n\n    a1 = pa.array(list_values)\n\n    table = pa.Table.from_arrays([a1], ['int_lists'])\n    _check_roundtrip(table)\n\n\n@pytest.mark.pandas\ndef test_parquet_nested_convenience(tempdir):\n    # ARROW-1684\n    df = pd.DataFrame({\n        'a': [[1, 2, 3], None, [4, 5], []],\n        'b': [[1.], None, None, [6., 7.]],\n    })\n\n    path = str(tempdir / 'nested_convenience.parquet')\n\n    table = pa.Table.from_pandas(df, preserve_index=False)\n    _write_table(table, path)\n\n    read = pq.read_table(\n        path, columns=['a'])\n    tm.assert_frame_equal(read.to_pandas(), df[['a']])\n\n    read = pq.read_table(\n        path, columns=['a', 'b'])\n    tm.assert_frame_equal(read.to_pandas(), df)\n\n\n# Binary\n# -----------------------------------------------------------------------------\n\n\ndef test_fixed_size_binary():\n    t0 = pa.binary(10)\n    data = [b'fooooooooo', None, b'barooooooo', b'quxooooooo']\n    a0 = pa.array(data, type=t0)\n\n    table = pa.Table.from_arrays([a0],\n                                 ['binary[10]'])\n    _check_roundtrip(table)\n\n\n# Large types\n# -----------------------------------------------------------------------------\n\n\n@pytest.mark.slow\n@pytest.mark.large_memory\ndef test_large_table_int32_overflow():\n    size = np.iinfo('int32').max + 1\n\n    arr = np.ones(size, dtype='uint8')\n\n    parr = pa.array(arr, type=pa.uint8())\n\n    table = pa.Table.from_arrays([parr], names=['one'])\n    f = io.BytesIO()\n    _write_table(table, f)\n\n\ndef _simple_table_roundtrip(table, **write_kwargs):\n    stream = pa.BufferOutputStream()\n    _write_table(table, stream, **write_kwargs)\n    buf = stream.getvalue()\n    return _read_table(buf)\n\n\n@pytest.mark.slow\n@pytest.mark.large_memory\ndef test_byte_array_exactly_2gb():\n    # Test edge case reported in ARROW-3762\n    val = b'x' * (1 << 10)\n\n    base = pa.array([val] * ((1 << 21) - 1))\n    cases = [\n        [b'x' * 1023],  # 2^31 - 1\n        [b'x' * 1024],  # 2^31\n        [b'x' * 1025]   # 2^31 + 1\n    ]\n    for case in cases:\n        values = pa.chunked_array([base, pa.array(case)])\n        t = pa.table([values], names=['f0'])\n        result = _simple_table_roundtrip(\n            t, use_dictionary=False)\n        assert t.equals(result)\n\n\n@pytest.mark.slow\n@pytest.mark.pandas\n@pytest.mark.large_memory\ndef test_binary_array_overflow_to_chunked():\n    # ARROW-3762\n\n    # 2^31 + 1 bytes\n    values = [b'x'] + [\n        b'x' * (1 << 20)\n    ] * 2 * (1 << 10)\n    df = pd.DataFrame({'byte_col': values})\n\n    tbl = pa.Table.from_pandas(df, preserve_index=False)\n    read_tbl = _simple_table_roundtrip(tbl)\n\n    col0_data = read_tbl[0]\n    assert isinstance(col0_data, pa.ChunkedArray)\n\n    # Split up into 2GB chunks\n    assert col0_data.num_chunks == 2\n\n    assert tbl.equals(read_tbl)\n\n\n@pytest.mark.slow\n@pytest.mark.pandas\n@pytest.mark.large_memory\ndef test_list_of_binary_large_cell():\n    # ARROW-4688\n    data = []\n\n    # TODO(wesm): handle chunked children\n    # 2^31 - 1 bytes in a single cell\n    # data.append([b'x' * (1 << 20)] * 2047 + [b'x' * ((1 << 20) - 1)])\n\n    # A little under 2GB in cell each containing approximately 10MB each\n    data.extend([[b'x' * 1000000] * 10] * 214)\n\n    arr = pa.array(data)\n    table = pa.Table.from_arrays([arr], ['chunky_cells'])\n    read_table = _simple_table_roundtrip(table)\n    assert table.equals(read_table)\n\n\ndef test_large_binary():\n    data = [b'foo', b'bar'] * 50\n    for type in [pa.large_binary(), pa.large_string()]:\n        arr = pa.array(data, type=type)\n        table = pa.Table.from_arrays([arr], names=['strs'])\n        for use_dictionary in [False, True]:\n            _check_roundtrip(table, use_dictionary=use_dictionary)\n\n\n@pytest.mark.slow\n@pytest.mark.large_memory\ndef test_large_binary_huge():\n    s = b'xy' * 997\n    data = [s] * ((1 << 33) // len(s))\n    for type in [pa.large_binary(), pa.large_string()]:\n        arr = pa.array(data, type=type)\n        table = pa.Table.from_arrays([arr], names=['strs'])\n        for use_dictionary in [False, True]:\n            _check_roundtrip(table, use_dictionary=use_dictionary)\n        del arr, table\n\n\n@pytest.mark.large_memory\ndef test_large_binary_overflow():\n    s = b'x' * (1 << 31)\n    arr = pa.array([s], type=pa.large_binary())\n    table = pa.Table.from_arrays([arr], names=['strs'])\n    for use_dictionary in [False, True]:\n        writer = pa.BufferOutputStream()\n        with pytest.raises(\n                pa.ArrowInvalid,\n                match=\"Parquet cannot store strings with size 2GB or more\"):\n            _write_table(table, writer, use_dictionary=use_dictionary)\n", "python/pyarrow/tests/parquet/test_compliant_nested_type.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pytest\n\nimport pyarrow as pa\n\ntry:\n    import pyarrow.parquet as pq\n    from pyarrow.tests.parquet.common import (_read_table,\n                                              _check_roundtrip)\nexcept ImportError:\n    pq = None\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n\n    from pyarrow.tests.parquet.common import _roundtrip_pandas_dataframe\nexcept ImportError:\n    pd = tm = None\n\n\n# Marks all of the tests in this module\n# Ignore these with pytest ... -m 'not parquet'\npytestmark = pytest.mark.parquet\n\n\n# Tests for ARROW-11497\n_test_data_simple = [\n    {'items': [1, 2]},\n    {'items': [0]},\n]\n\n_test_data_complex = [\n    {'items': [{'name': 'elem1', 'value': '1'},\n               {'name': 'elem2', 'value': '2'}]},\n    {'items': [{'name': 'elem1', 'value': '0'}]},\n]\n\nparametrize_test_data = pytest.mark.parametrize(\n    \"test_data\", [_test_data_simple, _test_data_complex])\n\n\n@pytest.mark.pandas\n@parametrize_test_data\ndef test_write_compliant_nested_type_enable(tempdir, test_data):\n    # prepare dataframe for testing\n    df = pd.DataFrame(data=test_data)\n    # verify that we can read/write pandas df with new flag (default behaviour)\n    _roundtrip_pandas_dataframe(df,\n                                write_kwargs={})\n\n    # Write to a parquet file with compliant nested type\n    table = pa.Table.from_pandas(df, preserve_index=False)\n    path = str(tempdir / 'data.parquet')\n    with pq.ParquetWriter(path, table.schema,\n                          version='2.6') as writer:\n        writer.write_table(table)\n    # Read back as a table\n    new_table = _read_table(path)\n    # Validate that \"items\" columns compliant to Parquet nested format\n    # Should be like this: list<element: struct<name: string, value: string>>\n    assert isinstance(new_table.schema.types[0], pa.ListType)\n    assert new_table.schema.types[0].value_field.name == 'element'\n\n    # Verify that the new table can be read/written correctly\n    _check_roundtrip(new_table)\n\n\n@pytest.mark.pandas\n@parametrize_test_data\ndef test_write_compliant_nested_type_disable(tempdir, test_data):\n    # prepare dataframe for testing\n    df = pd.DataFrame(data=test_data)\n    # verify that we can read/write with new flag disabled\n    _roundtrip_pandas_dataframe(df, write_kwargs={\n        'use_compliant_nested_type': False})\n\n    # Write to a parquet file while disabling compliant nested type\n    table = pa.Table.from_pandas(df, preserve_index=False)\n    path = str(tempdir / 'data.parquet')\n    with pq.ParquetWriter(path, table.schema, version='2.6',\n                          use_compliant_nested_type=False) as writer:\n        writer.write_table(table)\n    new_table = _read_table(path)\n\n    # Validate that \"items\" columns is not compliant to Parquet nested format\n    # Should be like this: list<item: struct<name: string, value: string>>\n    assert isinstance(new_table.schema.types[0], pa.ListType)\n    assert new_table.schema.types[0].value_field.name == 'item'\n\n    # Verify that the new table can be read/written correctly\n    _check_roundtrip(new_table,\n                     use_compliant_nested_type=False)\n", "python/pyarrow/parquet/core.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nfrom collections import defaultdict\nfrom contextlib import nullcontext\nfrom functools import reduce\n\nimport inspect\nimport json\nimport os\nimport re\nimport operator\nimport warnings\n\nimport pyarrow as pa\n\ntry:\n    import pyarrow._parquet as _parquet\nexcept ImportError as exc:\n    raise ImportError(\n        \"The pyarrow installation is not built with support \"\n        f\"for the Parquet file format ({str(exc)})\"\n    ) from None\n\nfrom pyarrow._parquet import (ParquetReader, Statistics,  # noqa\n                              FileMetaData, RowGroupMetaData,\n                              ColumnChunkMetaData,\n                              ParquetSchema, ColumnSchema,\n                              ParquetLogicalType,\n                              FileEncryptionProperties,\n                              FileDecryptionProperties,\n                              SortingColumn)\nfrom pyarrow.fs import (LocalFileSystem, FileSystem, FileType,\n                        _resolve_filesystem_and_path, _ensure_filesystem)\nfrom pyarrow.util import guid, _is_path_like, _stringify_path, _deprecate_api\n\n\ndef _check_contains_null(val):\n    if isinstance(val, bytes):\n        for byte in val:\n            if isinstance(byte, bytes):\n                compare_to = chr(0)\n            else:\n                compare_to = 0\n            if byte == compare_to:\n                return True\n    elif isinstance(val, str):\n        return '\\x00' in val\n    return False\n\n\ndef _check_filters(filters, check_null_strings=True):\n    \"\"\"\n    Check if filters are well-formed.\n    \"\"\"\n    if filters is not None:\n        if len(filters) == 0 or any(len(f) == 0 for f in filters):\n            raise ValueError(\"Malformed filters\")\n        if isinstance(filters[0][0], str):\n            # We have encountered the situation where we have one nesting level\n            # too few:\n            #   We have [(,,), ..] instead of [[(,,), ..]]\n            filters = [filters]\n        if check_null_strings:\n            for conjunction in filters:\n                for col, op, val in conjunction:\n                    if (\n                        isinstance(val, list) and\n                        all(_check_contains_null(v) for v in val) or\n                        _check_contains_null(val)\n                    ):\n                        raise NotImplementedError(\n                            \"Null-terminated binary strings are not supported \"\n                            \"as filter values.\"\n                        )\n    return filters\n\n\n_DNF_filter_doc = \"\"\"Predicates are expressed using an ``Expression`` or using\n    the disjunctive normal form (DNF), like ``[[('x', '=', 0), ...], ...]``.\n    DNF allows arbitrary boolean logical combinations of single column predicates.\n    The innermost tuples each describe a single column predicate. The list of inner\n    predicates is interpreted as a conjunction (AND), forming a more selective and\n    multiple column predicate. Finally, the most outer list combines these filters\n    as a disjunction (OR).\n\n    Predicates may also be passed as List[Tuple]. This form is interpreted\n    as a single conjunction. To express OR in predicates, one must\n    use the (preferred) List[List[Tuple]] notation.\n\n    Each tuple has format: (``key``, ``op``, ``value``) and compares the\n    ``key`` with the ``value``.\n    The supported ``op`` are:  ``=`` or ``==``, ``!=``, ``<``, ``>``, ``<=``,\n    ``>=``, ``in`` and ``not in``. If the ``op`` is ``in`` or ``not in``, the\n    ``value`` must be a collection such as a ``list``, a ``set`` or a\n    ``tuple``.\n\n    Examples:\n\n    Using the ``Expression`` API:\n\n    .. code-block:: python\n\n        import pyarrow.compute as pc\n        pc.field('x') = 0\n        pc.field('y').isin(['a', 'b', 'c'])\n        ~pc.field('y').isin({'a', 'b'})\n\n    Using the DNF format:\n\n    .. code-block:: python\n\n        ('x', '=', 0)\n        ('y', 'in', ['a', 'b', 'c'])\n        ('z', 'not in', {'a','b'})\n\n    \"\"\"\n\n\ndef filters_to_expression(filters):\n    \"\"\"\n    Check if filters are well-formed and convert to an ``Expression``.\n\n    Parameters\n    ----------\n    filters : List[Tuple] or List[List[Tuple]]\n\n    Notes\n    -----\n    See internal ``pyarrow._DNF_filter_doc`` attribute for more details.\n\n    Examples\n    --------\n\n    >>> filters_to_expression([('foo', '==', 'bar')])\n    <pyarrow.compute.Expression (foo == \"bar\")>\n\n    Returns\n    -------\n    pyarrow.compute.Expression\n        An Expression representing the filters\n    \"\"\"\n    import pyarrow.dataset as ds\n\n    if isinstance(filters, ds.Expression):\n        return filters\n\n    filters = _check_filters(filters, check_null_strings=False)\n\n    def convert_single_predicate(col, op, val):\n        field = ds.field(col)\n\n        if op == \"=\" or op == \"==\":\n            return field == val\n        elif op == \"!=\":\n            return field != val\n        elif op == '<':\n            return field < val\n        elif op == '>':\n            return field > val\n        elif op == '<=':\n            return field <= val\n        elif op == '>=':\n            return field >= val\n        elif op == 'in':\n            return field.isin(val)\n        elif op == 'not in':\n            return ~field.isin(val)\n        else:\n            raise ValueError(\n                '\"{0}\" is not a valid operator in predicates.'.format(\n                    (col, op, val)))\n\n    disjunction_members = []\n\n    for conjunction in filters:\n        conjunction_members = [\n            convert_single_predicate(col, op, val)\n            for col, op, val in conjunction\n        ]\n\n        disjunction_members.append(reduce(operator.and_, conjunction_members))\n\n    return reduce(operator.or_, disjunction_members)\n\n\n_filters_to_expression = _deprecate_api(\n    \"_filters_to_expression\", \"filters_to_expression\",\n    filters_to_expression, \"10.0.0\", DeprecationWarning)\n\n\n# ----------------------------------------------------------------------\n# Reading a single Parquet file\n\n\nclass ParquetFile:\n    \"\"\"\n    Reader interface for a single Parquet file.\n\n    Parameters\n    ----------\n    source : str, pathlib.Path, pyarrow.NativeFile, or file-like object\n        Readable source. For passing bytes or buffer-like file containing a\n        Parquet file, use pyarrow.BufferReader.\n    metadata : FileMetaData, default None\n        Use existing metadata object, rather than reading from file.\n    common_metadata : FileMetaData, default None\n        Will be used in reads for pandas schema metadata if not found in the\n        main file's metadata, no other uses at the moment.\n    read_dictionary : list\n        List of column names to read directly as DictionaryArray.\n    memory_map : bool, default False\n        If the source is a file path, use a memory map to read file, which can\n        improve performance in some environments.\n    buffer_size : int, default 0\n        If positive, perform read buffering when deserializing individual\n        column chunks. Otherwise IO calls are unbuffered.\n    pre_buffer : bool, default False\n        Coalesce and issue file reads in parallel to improve performance on\n        high-latency filesystems (e.g. S3). If True, Arrow will use a\n        background I/O thread pool.\n    coerce_int96_timestamp_unit : str, default None\n        Cast timestamps that are stored in INT96 format to a particular\n        resolution (e.g. 'ms'). Setting to None is equivalent to 'ns'\n        and therefore INT96 timestamps will be inferred as timestamps\n        in nanoseconds.\n    decryption_properties : FileDecryptionProperties, default None\n        File decryption properties for Parquet Modular Encryption.\n    thrift_string_size_limit : int, default None\n        If not None, override the maximum total string size allocated\n        when decoding Thrift structures. The default limit should be\n        sufficient for most Parquet files.\n    thrift_container_size_limit : int, default None\n        If not None, override the maximum total size of containers allocated\n        when decoding Thrift structures. The default limit should be\n        sufficient for most Parquet files.\n    filesystem : FileSystem, default None\n        If nothing passed, will be inferred based on path.\n        Path will try to be found in the local on-disk filesystem otherwise\n        it will be parsed as an URI to determine the filesystem.\n    page_checksum_verification : bool, default False\n        If True, verify the checksum for each page read from the file.\n\n    Examples\n    --------\n\n    Generate an example PyArrow Table and write it to Parquet file:\n\n    >>> import pyarrow as pa\n    >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n    ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n    ...                              \"Brittle stars\", \"Centipede\"]})\n\n    >>> import pyarrow.parquet as pq\n    >>> pq.write_table(table, 'example.parquet')\n\n    Create a ``ParquetFile`` object from the Parquet file:\n\n    >>> parquet_file = pq.ParquetFile('example.parquet')\n\n    Read the data:\n\n    >>> parquet_file.read()\n    pyarrow.Table\n    n_legs: int64\n    animal: string\n    ----\n    n_legs: [[2,2,4,4,5,100]]\n    animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n\n    Create a ParquetFile object with \"animal\" column as DictionaryArray:\n\n    >>> parquet_file = pq.ParquetFile('example.parquet',\n    ...                               read_dictionary=[\"animal\"])\n    >>> parquet_file.read()\n    pyarrow.Table\n    n_legs: int64\n    animal: dictionary<values=string, indices=int32, ordered=0>\n    ----\n    n_legs: [[2,2,4,4,5,100]]\n    animal: [  -- dictionary:\n    [\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]  -- indices:\n    [0,1,2,3,4,5]]\n    \"\"\"\n\n    def __init__(self, source, *, metadata=None, common_metadata=None,\n                 read_dictionary=None, memory_map=False, buffer_size=0,\n                 pre_buffer=False, coerce_int96_timestamp_unit=None,\n                 decryption_properties=None, thrift_string_size_limit=None,\n                 thrift_container_size_limit=None, filesystem=None,\n                 page_checksum_verification=False):\n\n        self._close_source = getattr(source, 'closed', True)\n\n        filesystem, source = _resolve_filesystem_and_path(\n            source, filesystem, memory_map=memory_map)\n        if filesystem is not None:\n            source = filesystem.open_input_file(source)\n            self._close_source = True  # We opened it here, ensure we close it.\n\n        self.reader = ParquetReader()\n        self.reader.open(\n            source, use_memory_map=memory_map,\n            buffer_size=buffer_size, pre_buffer=pre_buffer,\n            read_dictionary=read_dictionary, metadata=metadata,\n            coerce_int96_timestamp_unit=coerce_int96_timestamp_unit,\n            decryption_properties=decryption_properties,\n            thrift_string_size_limit=thrift_string_size_limit,\n            thrift_container_size_limit=thrift_container_size_limit,\n            page_checksum_verification=page_checksum_verification,\n        )\n        self.common_metadata = common_metadata\n        self._nested_paths_by_prefix = self._build_nested_paths()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n    def _build_nested_paths(self):\n        paths = self.reader.column_paths\n\n        result = defaultdict(list)\n\n        for i, path in enumerate(paths):\n            key = path[0]\n            rest = path[1:]\n            while True:\n                result[key].append(i)\n\n                if not rest:\n                    break\n\n                key = '.'.join((key, rest[0]))\n                rest = rest[1:]\n\n        return result\n\n    @property\n    def metadata(self):\n        \"\"\"\n        Return the Parquet metadata.\n        \"\"\"\n        return self.reader.metadata\n\n    @property\n    def schema(self):\n        \"\"\"\n        Return the Parquet schema, unconverted to Arrow types\n        \"\"\"\n        return self.metadata.schema\n\n    @property\n    def schema_arrow(self):\n        \"\"\"\n        Return the inferred Arrow schema, converted from the whole Parquet\n        file's schema\n\n        Examples\n        --------\n        Generate an example Parquet file:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        Read the Arrow schema:\n\n        >>> parquet_file.schema_arrow\n        n_legs: int64\n        animal: string\n        \"\"\"\n        return self.reader.schema_arrow\n\n    @property\n    def num_row_groups(self):\n        \"\"\"\n        Return the number of row groups of the Parquet file.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        >>> parquet_file.num_row_groups\n        1\n        \"\"\"\n        return self.reader.num_row_groups\n\n    def close(self, force: bool = False):\n        if self._close_source or force:\n            self.reader.close()\n\n    @property\n    def closed(self) -> bool:\n        return self.reader.closed\n\n    def read_row_group(self, i, columns=None, use_threads=True,\n                       use_pandas_metadata=False):\n        \"\"\"\n        Read a single row group from a Parquet file.\n\n        Parameters\n        ----------\n        i : int\n            Index of the individual row group that we want to read.\n        columns : list\n            If not None, only these columns will be read from the row group. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'.\n        use_threads : bool, default True\n            Perform multi-threaded column reads.\n        use_pandas_metadata : bool, default False\n            If True and file has custom pandas schema metadata, ensure that\n            index columns are also loaded.\n\n        Returns\n        -------\n        pyarrow.table.Table\n            Content of the row group as a table (of columns)\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        >>> parquet_file.read_row_group(0)\n        pyarrow.Table\n        n_legs: int64\n        animal: string\n        ----\n        n_legs: [[2,2,4,4,5,100]]\n        animal: [[\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]]\n        \"\"\"\n        column_indices = self._get_column_indices(\n            columns, use_pandas_metadata=use_pandas_metadata)\n        return self.reader.read_row_group(i, column_indices=column_indices,\n                                          use_threads=use_threads)\n\n    def read_row_groups(self, row_groups, columns=None, use_threads=True,\n                        use_pandas_metadata=False):\n        \"\"\"\n        Read a multiple row groups from a Parquet file.\n\n        Parameters\n        ----------\n        row_groups : list\n            Only these row groups will be read from the file.\n        columns : list\n            If not None, only these columns will be read from the row group. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'.\n        use_threads : bool, default True\n            Perform multi-threaded column reads.\n        use_pandas_metadata : bool, default False\n            If True and file has custom pandas schema metadata, ensure that\n            index columns are also loaded.\n\n        Returns\n        -------\n        pyarrow.table.Table\n            Content of the row groups as a table (of columns).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        >>> parquet_file.read_row_groups([0,0])\n        pyarrow.Table\n        n_legs: int64\n        animal: string\n        ----\n        n_legs: [[2,2,4,4,5,...,2,4,4,5,100]]\n        animal: [[\"Flamingo\",\"Parrot\",\"Dog\",...,\"Brittle stars\",\"Centipede\"]]\n        \"\"\"\n        column_indices = self._get_column_indices(\n            columns, use_pandas_metadata=use_pandas_metadata)\n        return self.reader.read_row_groups(row_groups,\n                                           column_indices=column_indices,\n                                           use_threads=use_threads)\n\n    def iter_batches(self, batch_size=65536, row_groups=None, columns=None,\n                     use_threads=True, use_pandas_metadata=False):\n        \"\"\"\n        Read streaming batches from a Parquet file.\n\n        Parameters\n        ----------\n        batch_size : int, default 64K\n            Maximum number of records to yield per batch. Batches may be\n            smaller if there aren't enough rows in the file.\n        row_groups : list\n            Only these row groups will be read from the file.\n        columns : list\n            If not None, only these columns will be read from the file. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'.\n        use_threads : boolean, default True\n            Perform multi-threaded column reads.\n        use_pandas_metadata : boolean, default False\n            If True and file has custom pandas schema metadata, ensure that\n            index columns are also loaded.\n\n        Yields\n        ------\n        pyarrow.RecordBatch\n            Contents of each batch as a record batch\n\n        Examples\n        --------\n        Generate an example Parquet file:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n        >>> for i in parquet_file.iter_batches():\n        ...     print(\"RecordBatch\")\n        ...     print(i.to_pandas())\n        ...\n        RecordBatch\n           n_legs         animal\n        0       2       Flamingo\n        1       2         Parrot\n        2       4            Dog\n        3       4          Horse\n        4       5  Brittle stars\n        5     100      Centipede\n        \"\"\"\n        if row_groups is None:\n            row_groups = range(0, self.metadata.num_row_groups)\n        column_indices = self._get_column_indices(\n            columns, use_pandas_metadata=use_pandas_metadata)\n\n        batches = self.reader.iter_batches(batch_size,\n                                           row_groups=row_groups,\n                                           column_indices=column_indices,\n                                           use_threads=use_threads)\n        return batches\n\n    def read(self, columns=None, use_threads=True, use_pandas_metadata=False):\n        \"\"\"\n        Read a Table from Parquet format.\n\n        Parameters\n        ----------\n        columns : list\n            If not None, only these columns will be read from the file. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'.\n        use_threads : bool, default True\n            Perform multi-threaded column reads.\n        use_pandas_metadata : bool, default False\n            If True and file has custom pandas schema metadata, ensure that\n            index columns are also loaded.\n\n        Returns\n        -------\n        pyarrow.table.Table\n            Content of the file as a table (of columns).\n\n        Examples\n        --------\n        Generate an example Parquet file:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        Read a Table:\n\n        >>> parquet_file.read(columns=[\"animal\"])\n        pyarrow.Table\n        animal: string\n        ----\n        animal: [[\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]]\n        \"\"\"\n        column_indices = self._get_column_indices(\n            columns, use_pandas_metadata=use_pandas_metadata)\n        return self.reader.read_all(column_indices=column_indices,\n                                    use_threads=use_threads)\n\n    def scan_contents(self, columns=None, batch_size=65536):\n        \"\"\"\n        Read contents of file for the given columns and batch size.\n\n        Notes\n        -----\n        This function's primary purpose is benchmarking.\n        The scan is executed on a single thread.\n\n        Parameters\n        ----------\n        columns : list of integers, default None\n            Select columns to read, if None scan all columns.\n        batch_size : int, default 64K\n            Number of rows to read at a time internally.\n\n        Returns\n        -------\n        num_rows : int\n            Number of rows in file\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        >>> parquet_file.scan_contents()\n        6\n        \"\"\"\n        column_indices = self._get_column_indices(columns)\n        return self.reader.scan_contents(column_indices,\n                                         batch_size=batch_size)\n\n    def _get_column_indices(self, column_names, use_pandas_metadata=False):\n        if column_names is None:\n            return None\n\n        indices = []\n\n        for name in column_names:\n            if name in self._nested_paths_by_prefix:\n                indices.extend(self._nested_paths_by_prefix[name])\n\n        if use_pandas_metadata:\n            file_keyvalues = self.metadata.metadata\n            common_keyvalues = (self.common_metadata.metadata\n                                if self.common_metadata is not None\n                                else None)\n\n            if file_keyvalues and b'pandas' in file_keyvalues:\n                index_columns = _get_pandas_index_columns(file_keyvalues)\n            elif common_keyvalues and b'pandas' in common_keyvalues:\n                index_columns = _get_pandas_index_columns(common_keyvalues)\n            else:\n                index_columns = []\n\n            if indices is not None and index_columns:\n                indices += [self.reader.column_name_idx(descr)\n                            for descr in index_columns\n                            if not isinstance(descr, dict)]\n\n        return indices\n\n\n_SPARK_DISALLOWED_CHARS = re.compile('[ ,;{}()\\n\\t=]')\n\n\ndef _sanitized_spark_field_name(name):\n    return _SPARK_DISALLOWED_CHARS.sub('_', name)\n\n\ndef _sanitize_schema(schema, flavor):\n    if 'spark' in flavor:\n        sanitized_fields = []\n\n        schema_changed = False\n\n        for field in schema:\n            name = field.name\n            sanitized_name = _sanitized_spark_field_name(name)\n\n            if sanitized_name != name:\n                schema_changed = True\n                sanitized_field = pa.field(sanitized_name, field.type,\n                                           field.nullable, field.metadata)\n                sanitized_fields.append(sanitized_field)\n            else:\n                sanitized_fields.append(field)\n\n        new_schema = pa.schema(sanitized_fields, metadata=schema.metadata)\n        return new_schema, schema_changed\n    else:\n        return schema, False\n\n\ndef _sanitize_table(table, new_schema, flavor):\n    # TODO: This will not handle prohibited characters in nested field names\n    if 'spark' in flavor:\n        column_data = [table[i] for i in range(table.num_columns)]\n        return pa.Table.from_arrays(column_data, schema=new_schema)\n    else:\n        return table\n\n\n_parquet_writer_arg_docs = \"\"\"version : {\"1.0\", \"2.4\", \"2.6\"}, default \"2.6\"\n    Determine which Parquet logical types are available for use, whether the\n    reduced set from the Parquet 1.x.x format or the expanded logical types\n    added in later format versions.\n    Files written with version='2.4' or '2.6' may not be readable in all\n    Parquet implementations, so version='1.0' is likely the choice that\n    maximizes file compatibility.\n    UINT32 and some logical types are only available with version '2.4'.\n    Nanosecond timestamps are only available with version '2.6'.\n    Other features such as compression algorithms or the new serialized\n    data page format must be enabled separately (see 'compression' and\n    'data_page_version').\nuse_dictionary : bool or list, default True\n    Specify if we should use dictionary encoding in general or only for\n    some columns.\n    When encoding the column, if the dictionary size is too large, the\n    column will fallback to ``PLAIN`` encoding. Specially, ``BOOLEAN`` type\n    doesn't support dictionary encoding.\ncompression : str or dict, default 'snappy'\n    Specify the compression codec, either on a general basis or per-column.\n    Valid values: {'NONE', 'SNAPPY', 'GZIP', 'BROTLI', 'LZ4', 'ZSTD'}.\nwrite_statistics : bool or list, default True\n    Specify if we should write statistics in general (default is True) or only\n    for some columns.\nuse_deprecated_int96_timestamps : bool, default None\n    Write timestamps to INT96 Parquet format. Defaults to False unless enabled\n    by flavor argument. This take priority over the coerce_timestamps option.\ncoerce_timestamps : str, default None\n    Cast timestamps to a particular resolution. If omitted, defaults are chosen\n    depending on `version`. For ``version='1.0'`` and ``version='2.4'``,\n    nanoseconds are cast to microseconds ('us'), while for\n    ``version='2.6'`` (the default), they are written natively without loss\n    of resolution.  Seconds are always cast to milliseconds ('ms') by default,\n    as Parquet does not have any temporal type with seconds resolution.\n    If the casting results in loss of data, it will raise an exception\n    unless ``allow_truncated_timestamps=True`` is given.\n    Valid values: {None, 'ms', 'us'}\nallow_truncated_timestamps : bool, default False\n    Allow loss of data when coercing timestamps to a particular\n    resolution. E.g. if microsecond or nanosecond data is lost when coercing to\n    'ms', do not raise an exception. Passing ``allow_truncated_timestamp=True``\n    will NOT result in the truncation exception being ignored unless\n    ``coerce_timestamps`` is not None.\ndata_page_size : int, default None\n    Set a target threshold for the approximate encoded size of data\n    pages within a column chunk (in bytes). If None, use the default data page\n    size of 1MByte.\nflavor : {'spark'}, default None\n    Sanitize schema or set other compatibility options to work with\n    various target systems.\nfilesystem : FileSystem, default None\n    If nothing passed, will be inferred from `where` if path-like, else\n    `where` is already a file-like object so no filesystem is needed.\ncompression_level : int or dict, default None\n    Specify the compression level for a codec, either on a general basis or\n    per-column. If None is passed, arrow selects the compression level for\n    the compression codec in use. The compression level has a different\n    meaning for each codec, so you have to read the documentation of the\n    codec you are using.\n    An exception is thrown if the compression codec does not allow specifying\n    a compression level.\nuse_byte_stream_split : bool or list, default False\n    Specify if the byte_stream_split encoding should be used in general or\n    only for some columns. If both dictionary and byte_stream_stream are\n    enabled, then dictionary is preferred.\n    The byte_stream_split encoding is valid for integer, floating-point\n    and fixed-size binary data types (including decimals); it should be\n    combined with a compression codec so as to achieve size reduction.\ncolumn_encoding : string or dict, default None\n    Specify the encoding scheme on a per column basis.\n    Can only be used when ``use_dictionary`` is set to False, and\n    cannot be used in combination with ``use_byte_stream_split``.\n    Currently supported values: {'PLAIN', 'BYTE_STREAM_SPLIT',\n    'DELTA_BINARY_PACKED', 'DELTA_LENGTH_BYTE_ARRAY', 'DELTA_BYTE_ARRAY'}.\n    Certain encodings are only compatible with certain data types.\n    Please refer to the encodings section of `Reading and writing Parquet\n    files <https://arrow.apache.org/docs/cpp/parquet.html#encodings>`_.\ndata_page_version : {\"1.0\", \"2.0\"}, default \"1.0\"\n    The serialized Parquet data page format version to write, defaults to\n    1.0. This does not impact the file schema logical types and Arrow to\n    Parquet type casting behavior; for that use the \"version\" option.\nuse_compliant_nested_type : bool, default True\n    Whether to write compliant Parquet nested type (lists) as defined\n    `here <https://github.com/apache/parquet-format/blob/master/\n    LogicalTypes.md#nested-types>`_, defaults to ``True``.\n    For ``use_compliant_nested_type=True``, this will write into a list\n    with 3-level structure where the middle level, named ``list``,\n    is a repeated group with a single field named ``element``::\n\n        <list-repetition> group <name> (LIST) {\n            repeated group list {\n                  <element-repetition> <element-type> element;\n            }\n        }\n\n    For ``use_compliant_nested_type=False``, this will also write into a list\n    with 3-level structure, where the name of the single field of the middle\n    level ``list`` is taken from the element name for nested columns in Arrow,\n    which defaults to ``item``::\n\n        <list-repetition> group <name> (LIST) {\n            repeated group list {\n                <element-repetition> <element-type> item;\n            }\n        }\nencryption_properties : FileEncryptionProperties, default None\n    File encryption properties for Parquet Modular Encryption.\n    If None, no encryption will be done.\n    The encryption properties can be created using:\n    ``CryptoFactory.file_encryption_properties()``.\nwrite_batch_size : int, default None\n    Number of values to write to a page at a time. If None, use the default of\n    1024. ``write_batch_size`` is complementary to ``data_page_size``. If pages\n    are exceeding the ``data_page_size`` due to large column values, lowering\n    the batch size can help keep page sizes closer to the intended size.\ndictionary_pagesize_limit : int, default None\n    Specify the dictionary page size limit per row group. If None, use the\n    default 1MB.\nstore_schema : bool, default True\n    By default, the Arrow schema is serialized and stored in the Parquet\n    file metadata (in the \"ARROW:schema\" key). When reading the file,\n    if this key is available, it will be used to more faithfully recreate\n    the original Arrow data. For example, for tz-aware timestamp columns\n    it will restore the timezone (Parquet only stores the UTC values without\n    timezone), or columns with duration type will be restored from the int64\n    Parquet column.\nwrite_page_index : bool, default False\n    Whether to write a page index in general for all columns.\n    Writing statistics to the page index disables the old method of writing\n    statistics to each data page header. The page index makes statistics-based\n    filtering more efficient than the page header, as it gathers all the\n    statistics for a Parquet file in a single place, avoiding scattered I/O.\n    Note that the page index is not yet used on the read size by PyArrow.\nwrite_page_checksum : bool, default False\n    Whether to write page checksums in general for all columns.\n    Page checksums enable detection of data corruption, which might occur during\n    transmission or in the storage.\nsorting_columns : Sequence of SortingColumn, default None\n    Specify the sort order of the data being written. The writer does not sort\n    the data nor does it verify that the data is sorted. The sort order is\n    written to the row group metadata, which can then be used by readers.\n\"\"\"\n\n_parquet_writer_example_doc = \"\"\"\\\nGenerate an example PyArrow Table and RecordBatch:\n\n>>> import pyarrow as pa\n>>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n...                              \"Brittle stars\", \"Centipede\"]})\n>>> batch = pa.record_batch([[2, 2, 4, 4, 5, 100],\n...                         [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n...                          \"Brittle stars\", \"Centipede\"]],\n...                         names=['n_legs', 'animal'])\n\ncreate a ParquetWriter object:\n\n>>> import pyarrow.parquet as pq\n>>> writer = pq.ParquetWriter('example.parquet', table.schema)\n\nand write the Table into the Parquet file:\n\n>>> writer.write_table(table)\n>>> writer.close()\n\n>>> pq.read_table('example.parquet').to_pandas()\n   n_legs         animal\n0       2       Flamingo\n1       2         Parrot\n2       4            Dog\n3       4          Horse\n4       5  Brittle stars\n5     100      Centipede\n\ncreate a ParquetWriter object for the RecordBatch:\n\n>>> writer2 = pq.ParquetWriter('example2.parquet', batch.schema)\n\nand write the RecordBatch into the Parquet file:\n\n>>> writer2.write_batch(batch)\n>>> writer2.close()\n\n>>> pq.read_table('example2.parquet').to_pandas()\n   n_legs         animal\n0       2       Flamingo\n1       2         Parrot\n2       4            Dog\n3       4          Horse\n4       5  Brittle stars\n5     100      Centipede\n\"\"\"\n\n\nclass ParquetWriter:\n\n    __doc__ = \"\"\"\nClass for incrementally building a Parquet file for Arrow tables.\n\nParameters\n----------\nwhere : path or file-like object\nschema : pyarrow.Schema\n{}\nwriter_engine_version : unused\n**options : dict\n    If options contains a key `metadata_collector` then the\n    corresponding value is assumed to be a list (or any object with\n    `.append` method) that will be filled with the file metadata instance\n    of the written file.\n\nExamples\n--------\n{}\n\"\"\".format(_parquet_writer_arg_docs, _parquet_writer_example_doc)\n\n    def __init__(self, where, schema, filesystem=None,\n                 flavor=None,\n                 version='2.6',\n                 use_dictionary=True,\n                 compression='snappy',\n                 write_statistics=True,\n                 use_deprecated_int96_timestamps=None,\n                 compression_level=None,\n                 use_byte_stream_split=False,\n                 column_encoding=None,\n                 writer_engine_version=None,\n                 data_page_version='1.0',\n                 use_compliant_nested_type=True,\n                 encryption_properties=None,\n                 write_batch_size=None,\n                 dictionary_pagesize_limit=None,\n                 store_schema=True,\n                 write_page_index=False,\n                 write_page_checksum=False,\n                 sorting_columns=None,\n                 **options):\n        if use_deprecated_int96_timestamps is None:\n            # Use int96 timestamps for Spark\n            if flavor is not None and 'spark' in flavor:\n                use_deprecated_int96_timestamps = True\n            else:\n                use_deprecated_int96_timestamps = False\n\n        self.flavor = flavor\n        if flavor is not None:\n            schema, self.schema_changed = _sanitize_schema(schema, flavor)\n        else:\n            self.schema_changed = False\n\n        self.schema = schema\n        self.where = where\n\n        # If we open a file using a filesystem, store file handle so we can be\n        # sure to close it when `self.close` is called.\n        self.file_handle = None\n\n        filesystem, path = _resolve_filesystem_and_path(where, filesystem)\n        if filesystem is not None:\n            # ARROW-10480: do not auto-detect compression.  While\n            # a filename like foo.parquet.gz is nonconforming, it\n            # shouldn't implicitly apply compression.\n            sink = self.file_handle = filesystem.open_output_stream(\n                path, compression=None)\n        else:\n            sink = where\n        self._metadata_collector = options.pop('metadata_collector', None)\n        engine_version = 'V2'\n        self.writer = _parquet.ParquetWriter(\n            sink, schema,\n            version=version,\n            compression=compression,\n            use_dictionary=use_dictionary,\n            write_statistics=write_statistics,\n            use_deprecated_int96_timestamps=use_deprecated_int96_timestamps,\n            compression_level=compression_level,\n            use_byte_stream_split=use_byte_stream_split,\n            column_encoding=column_encoding,\n            writer_engine_version=engine_version,\n            data_page_version=data_page_version,\n            use_compliant_nested_type=use_compliant_nested_type,\n            encryption_properties=encryption_properties,\n            write_batch_size=write_batch_size,\n            dictionary_pagesize_limit=dictionary_pagesize_limit,\n            store_schema=store_schema,\n            write_page_index=write_page_index,\n            write_page_checksum=write_page_checksum,\n            sorting_columns=sorting_columns,\n            **options)\n        self.is_open = True\n\n    def __del__(self):\n        if getattr(self, 'is_open', False):\n            self.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n        # return false since we want to propagate exceptions\n        return False\n\n    def write(self, table_or_batch, row_group_size=None):\n        \"\"\"\n        Write RecordBatch or Table to the Parquet file.\n\n        Parameters\n        ----------\n        table_or_batch : {RecordBatch, Table}\n        row_group_size : int, default None\n            Maximum number of rows in each written row group. If None,\n            the row group size will be the minimum of the input\n            table or batch length and 1024 * 1024.\n        \"\"\"\n        if isinstance(table_or_batch, pa.RecordBatch):\n            self.write_batch(table_or_batch, row_group_size)\n        elif isinstance(table_or_batch, pa.Table):\n            self.write_table(table_or_batch, row_group_size)\n        else:\n            raise TypeError(type(table_or_batch))\n\n    def write_batch(self, batch, row_group_size=None):\n        \"\"\"\n        Write RecordBatch to the Parquet file.\n\n        Parameters\n        ----------\n        batch : RecordBatch\n        row_group_size : int, default None\n            Maximum number of rows in written row group. If None, the\n            row group size will be the minimum of the RecordBatch\n            size and 1024 * 1024.  If set larger than 64Mi then 64Mi\n            will be used instead.\n        \"\"\"\n        table = pa.Table.from_batches([batch], batch.schema)\n        self.write_table(table, row_group_size)\n\n    def write_table(self, table, row_group_size=None):\n        \"\"\"\n        Write Table to the Parquet file.\n\n        Parameters\n        ----------\n        table : Table\n        row_group_size : int, default None\n            Maximum number of rows in each written row group. If None,\n            the row group size will be the minimum of the Table size\n            and 1024 * 1024.  If set larger than 64Mi then 64Mi will\n            be used instead.\n\n        \"\"\"\n        if self.schema_changed:\n            table = _sanitize_table(table, self.schema, self.flavor)\n        assert self.is_open\n\n        if not table.schema.equals(self.schema, check_metadata=False):\n            msg = ('Table schema does not match schema used to create file: '\n                   '\\ntable:\\n{!s} vs. \\nfile:\\n{!s}'\n                   .format(table.schema, self.schema))\n            raise ValueError(msg)\n\n        self.writer.write_table(table, row_group_size=row_group_size)\n\n    def close(self):\n        \"\"\"\n        Close the connection to the Parquet file.\n        \"\"\"\n        if self.is_open:\n            self.writer.close()\n            self.is_open = False\n            if self._metadata_collector is not None:\n                self._metadata_collector.append(self.writer.metadata)\n        if self.file_handle is not None:\n            self.file_handle.close()\n\n    def add_key_value_metadata(self, key_value_metadata):\n        \"\"\"\n        Add key-value metadata to the file.\n        This will overwrite any existing metadata with the same key.\n\n        Parameters\n        ----------\n        key_value_metadata : dict\n            Keys and values must be string-like / coercible to bytes.\n        \"\"\"\n        assert self.is_open\n        self.writer.add_key_value_metadata(key_value_metadata)\n\n\ndef _get_pandas_index_columns(keyvalues):\n    return (json.loads(keyvalues[b'pandas'].decode('utf8'))\n            ['index_columns'])\n\n\nEXCLUDED_PARQUET_PATHS = {'_SUCCESS'}\n\n\n_read_docstring_common = \"\"\"\\\nread_dictionary : list, default None\n    List of names or column paths (for nested types) to read directly\n    as DictionaryArray. Only supported for BYTE_ARRAY storage. To read\n    a flat column as dictionary-encoded pass the column name. For\n    nested types, you must pass the full column \"path\", which could be\n    something like level1.level2.list.item. Refer to the Parquet\n    file's schema to obtain the paths.\nmemory_map : bool, default False\n    If the source is a file path, use a memory map to read file, which can\n    improve performance in some environments.\nbuffer_size : int, default 0\n    If positive, perform read buffering when deserializing individual\n    column chunks. Otherwise IO calls are unbuffered.\npartitioning : pyarrow.dataset.Partitioning or str or list of str, \\\ndefault \"hive\"\n    The partitioning scheme for a partitioned dataset. The default of \"hive\"\n    assumes directory names with key=value pairs like \"/year=2009/month=11\".\n    In addition, a scheme like \"/2009/11\" is also supported, in which case\n    you need to specify the field names or a full schema. See the\n    ``pyarrow.dataset.partitioning()`` function for more details.\"\"\"\n\n\n_parquet_dataset_example = \"\"\"\\\nGenerate an example PyArrow Table and write it to a partitioned dataset:\n\n>>> import pyarrow as pa\n>>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n...                   'n_legs': [2, 2, 4, 4, 5, 100],\n...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n...                              \"Brittle stars\", \"Centipede\"]})\n>>> import pyarrow.parquet as pq\n>>> pq.write_to_dataset(table, root_path='dataset_v2',\n...                     partition_cols=['year'])\n\ncreate a ParquetDataset object from the dataset source:\n\n>>> dataset = pq.ParquetDataset('dataset_v2/')\n\nand read the data:\n\n>>> dataset.read().to_pandas()\n   n_legs         animal  year\n0       5  Brittle stars  2019\n1       2       Flamingo  2020\n2       4            Dog  2021\n3     100      Centipede  2021\n4       2         Parrot  2022\n5       4          Horse  2022\n\ncreate a ParquetDataset object with filter:\n\n>>> dataset = pq.ParquetDataset('dataset_v2/',\n...                             filters=[('n_legs','=',4)])\n>>> dataset.read().to_pandas()\n   n_legs animal  year\n0       4    Dog  2021\n1       4  Horse  2022\n\"\"\"\n\n\nclass ParquetDataset:\n    __doc__ = \"\"\"\nEncapsulates details of reading a complete Parquet dataset possibly\nconsisting of multiple files and partitions in subdirectories.\n\nParameters\n----------\npath_or_paths : str or List[str]\n    A directory name, single file name, or list of file names.\nfilesystem : FileSystem, default None\n    If nothing passed, will be inferred based on path.\n    Path will try to be found in the local on-disk filesystem otherwise\n    it will be parsed as an URI to determine the filesystem.\nschema : pyarrow.parquet.Schema\n    Optionally provide the Schema for the Dataset, in which case it will\n    not be inferred from the source.\nfilters : pyarrow.compute.Expression or List[Tuple] or List[List[Tuple]], default None\n    Rows which do not match the filter predicate will be removed from scanned\n    data. Partition keys embedded in a nested directory structure will be\n    exploited to avoid loading files at all if they contain no matching rows.\n    Within-file level filtering and different partitioning schemes are supported.\n\n    {1}\n{0}\nignore_prefixes : list, optional\n    Files matching any of these prefixes will be ignored by the\n    discovery process.\n    This is matched to the basename of a path.\n    By default this is ['.', '_'].\n    Note that discovery happens only if a directory is passed as source.\npre_buffer : bool, default True\n    Coalesce and issue file reads in parallel to improve performance on\n    high-latency filesystems (e.g. S3, GCS). If True, Arrow will use a\n    background I/O thread pool. If using a filesystem layer that itself\n    performs readahead (e.g. fsspec's S3FS), disable readahead for best\n    results. Set to False if you want to prioritize minimal memory usage\n    over maximum speed.\ncoerce_int96_timestamp_unit : str, default None\n    Cast timestamps that are stored in INT96 format to a particular resolution\n    (e.g. 'ms'). Setting to None is equivalent to 'ns' and therefore INT96\n    timestamps will be inferred as timestamps in nanoseconds.\ndecryption_properties : FileDecryptionProperties or None\n    File-level decryption properties.\n    The decryption properties can be created using\n    ``CryptoFactory.file_decryption_properties()``.\nthrift_string_size_limit : int, default None\n    If not None, override the maximum total string size allocated\n    when decoding Thrift structures. The default limit should be\n    sufficient for most Parquet files.\nthrift_container_size_limit : int, default None\n    If not None, override the maximum total size of containers allocated\n    when decoding Thrift structures. The default limit should be\n    sufficient for most Parquet files.\npage_checksum_verification : bool, default False\n    If True, verify the page checksum for each page read from the file.\nuse_legacy_dataset : bool, optional\n    Deprecated and has no effect from PyArrow version 15.0.0.\n\nExamples\n--------\n{2}\n\"\"\".format(_read_docstring_common, _DNF_filter_doc, _parquet_dataset_example)\n\n    def __init__(self, path_or_paths, filesystem=None, schema=None, *, filters=None,\n                 read_dictionary=None, memory_map=False, buffer_size=None,\n                 partitioning=\"hive\", ignore_prefixes=None, pre_buffer=True,\n                 coerce_int96_timestamp_unit=None,\n                 decryption_properties=None, thrift_string_size_limit=None,\n                 thrift_container_size_limit=None,\n                 page_checksum_verification=False,\n                 use_legacy_dataset=None):\n\n        if use_legacy_dataset is not None:\n            warnings.warn(\n                \"Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 \"\n                \"and will be removed in a future version.\",\n                FutureWarning, stacklevel=2)\n\n        import pyarrow.dataset as ds\n\n        # map format arguments\n        read_options = {\n            \"pre_buffer\": pre_buffer,\n            \"coerce_int96_timestamp_unit\": coerce_int96_timestamp_unit,\n            \"thrift_string_size_limit\": thrift_string_size_limit,\n            \"thrift_container_size_limit\": thrift_container_size_limit,\n            \"page_checksum_verification\": page_checksum_verification,\n        }\n        if buffer_size:\n            read_options.update(use_buffered_stream=True,\n                                buffer_size=buffer_size)\n        if read_dictionary is not None:\n            read_options.update(dictionary_columns=read_dictionary)\n\n        if decryption_properties is not None:\n            read_options.update(decryption_properties=decryption_properties)\n\n        self._filter_expression = None\n        if filters is not None:\n            self._filter_expression = filters_to_expression(filters)\n\n        # map old filesystems to new one\n        if filesystem is not None:\n            filesystem = _ensure_filesystem(\n                filesystem, use_mmap=memory_map)\n        elif filesystem is None and memory_map:\n            # if memory_map is specified, assume local file system (string\n            # path can in principle be URI for any filesystem)\n            filesystem = LocalFileSystem(use_mmap=memory_map)\n\n        # This needs to be checked after _ensure_filesystem, because that\n        # handles the case of an fsspec LocalFileSystem\n        if (\n            hasattr(path_or_paths, \"__fspath__\") and\n            filesystem is not None and\n            not isinstance(filesystem, LocalFileSystem)\n        ):\n            raise TypeError(\n                \"Path-like objects with __fspath__ must only be used with \"\n                f\"local file systems, not {type(filesystem)}\"\n            )\n\n        # check for single fragment dataset or dataset directory\n        single_file = None\n        self._base_dir = None\n        if not isinstance(path_or_paths, list):\n            if _is_path_like(path_or_paths):\n                path_or_paths = _stringify_path(path_or_paths)\n                if filesystem is None:\n                    # path might be a URI describing the FileSystem as well\n                    try:\n                        filesystem, path_or_paths = FileSystem.from_uri(\n                            path_or_paths)\n                    except ValueError:\n                        filesystem = LocalFileSystem(use_mmap=memory_map)\n                finfo = filesystem.get_file_info(path_or_paths)\n                if finfo.type == FileType.Directory:\n                    self._base_dir = path_or_paths\n            else:\n                single_file = path_or_paths\n\n        parquet_format = ds.ParquetFileFormat(**read_options)\n\n        if single_file is not None:\n            fragment = parquet_format.make_fragment(single_file, filesystem)\n\n            self._dataset = ds.FileSystemDataset(\n                [fragment], schema=schema or fragment.physical_schema,\n                format=parquet_format,\n                filesystem=fragment.filesystem\n            )\n            return\n\n        # check partitioning to enable dictionary encoding\n        if partitioning == \"hive\":\n            partitioning = ds.HivePartitioning.discover(\n                infer_dictionary=True)\n\n        self._dataset = ds.dataset(path_or_paths, filesystem=filesystem,\n                                   schema=schema, format=parquet_format,\n                                   partitioning=partitioning,\n                                   ignore_prefixes=ignore_prefixes)\n\n    def equals(self, other):\n        if not isinstance(other, ParquetDataset):\n            raise TypeError('`other` must be an instance of ParquetDataset')\n\n        return (self.schema == other.schema and\n                self._dataset.format == other._dataset.format and\n                self.filesystem == other.filesystem and\n                # self.fragments == other.fragments and\n                self.files == other.files)\n\n    def __eq__(self, other):\n        try:\n            return self.equals(other)\n        except TypeError:\n            return NotImplemented\n\n    @property\n    def schema(self):\n        \"\"\"\n        Schema of the Dataset.\n\n        Examples\n        --------\n        Generate an example dataset:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n        ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_to_dataset(table, root_path='dataset_v2_schema',\n        ...                     partition_cols=['year'])\n        >>> dataset = pq.ParquetDataset('dataset_v2_schema/')\n\n        Read the schema:\n\n        >>> dataset.schema\n        n_legs: int64\n        animal: string\n        year: dictionary<values=int32, indices=int32, ordered=0>\n        \"\"\"\n        return self._dataset.schema\n\n    def read(self, columns=None, use_threads=True, use_pandas_metadata=False):\n        \"\"\"\n        Read (multiple) Parquet files as a single pyarrow.Table.\n\n        Parameters\n        ----------\n        columns : List[str]\n            Names of columns to read from the dataset. The partition fields\n            are not automatically included.\n        use_threads : bool, default True\n            Perform multi-threaded column reads.\n        use_pandas_metadata : bool, default False\n            If True and file has custom pandas schema metadata, ensure that\n            index columns are also loaded.\n\n        Returns\n        -------\n        pyarrow.Table\n            Content of the file as a table (of columns).\n\n        Examples\n        --------\n        Generate an example dataset:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n        ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_to_dataset(table, root_path='dataset_v2_read',\n        ...                     partition_cols=['year'])\n        >>> dataset = pq.ParquetDataset('dataset_v2_read/')\n\n        Read the dataset:\n\n        >>> dataset.read(columns=[\"n_legs\"])\n        pyarrow.Table\n        n_legs: int64\n        ----\n        n_legs: [[5],[2],[4,100],[2,4]]\n        \"\"\"\n        # if use_pandas_metadata, we need to include index columns in the\n        # column selection, to be able to restore those in the pandas DataFrame\n        metadata = self.schema.metadata or {}\n\n        if use_pandas_metadata:\n            # if the dataset schema metadata itself doesn't have pandas\n            # then try to get this from common file (for backwards compat)\n            if b\"pandas\" not in metadata:\n                common_metadata = self._get_common_pandas_metadata()\n                if common_metadata:\n                    metadata = common_metadata\n\n        if columns is not None and use_pandas_metadata:\n            if metadata and b'pandas' in metadata:\n                # RangeIndex can be represented as dict instead of column name\n                index_columns = [\n                    col for col in _get_pandas_index_columns(metadata)\n                    if not isinstance(col, dict)\n                ]\n                columns = (\n                    list(columns) + list(set(index_columns) - set(columns))\n                )\n\n        table = self._dataset.to_table(\n            columns=columns, filter=self._filter_expression,\n            use_threads=use_threads\n        )\n\n        # if use_pandas_metadata, restore the pandas metadata (which gets\n        # lost if doing a specific `columns` selection in to_table)\n        if use_pandas_metadata:\n            if metadata and b\"pandas\" in metadata:\n                new_metadata = table.schema.metadata or {}\n                new_metadata.update({b\"pandas\": metadata[b\"pandas\"]})\n                table = table.replace_schema_metadata(new_metadata)\n\n        return table\n\n    def _get_common_pandas_metadata(self):\n\n        if not self._base_dir:\n            return None\n\n        metadata = None\n        for name in [\"_common_metadata\", \"_metadata\"]:\n            metadata_path = os.path.join(str(self._base_dir), name)\n            finfo = self.filesystem.get_file_info(metadata_path)\n            if finfo.is_file:\n                pq_meta = read_metadata(\n                    metadata_path, filesystem=self.filesystem)\n                metadata = pq_meta.metadata\n                if metadata and b'pandas' in metadata:\n                    break\n\n        return metadata\n\n    def read_pandas(self, **kwargs):\n        \"\"\"\n        Read dataset including pandas metadata, if any. Other arguments passed\n        through to :func:`read`, see docstring for further details.\n\n        Parameters\n        ----------\n        **kwargs : optional\n            Additional options for :func:`read`\n\n        Examples\n        --------\n        Generate an example parquet file:\n\n        >>> import pyarrow as pa\n        >>> import pandas as pd\n        >>> df = pd.DataFrame({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n        ...                    'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                    'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                    \"Brittle stars\", \"Centipede\"]})\n        >>> table = pa.Table.from_pandas(df)\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'table_V2.parquet')\n        >>> dataset = pq.ParquetDataset('table_V2.parquet')\n\n        Read the dataset with pandas metadata:\n\n        >>> dataset.read_pandas(columns=[\"n_legs\"])\n        pyarrow.Table\n        n_legs: int64\n        ----\n        n_legs: [[2,2,4,4,5,100]]\n\n        >>> dataset.read_pandas(columns=[\"n_legs\"]).schema.pandas_metadata\n        {'index_columns': [{'kind': 'range', 'name': None, 'start': 0, ...}\n        \"\"\"\n        return self.read(use_pandas_metadata=True, **kwargs)\n\n    @property\n    def fragments(self):\n        \"\"\"\n        A list of the Dataset source fragments or pieces with absolute\n        file paths.\n\n        Examples\n        --------\n        Generate an example dataset:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n        ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_to_dataset(table, root_path='dataset_v2_fragments',\n        ...                     partition_cols=['year'])\n        >>> dataset = pq.ParquetDataset('dataset_v2_fragments/')\n\n        List the fragments:\n\n        >>> dataset.fragments\n        [<pyarrow.dataset.ParquetFileFragment path=dataset_v2_fragments/...\n        \"\"\"\n        return list(self._dataset.get_fragments())\n\n    @property\n    def files(self):\n        \"\"\"\n        A list of absolute Parquet file paths in the Dataset source.\n\n        Examples\n        --------\n        Generate an example dataset:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n        ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_to_dataset(table, root_path='dataset_v2_files',\n        ...                     partition_cols=['year'])\n        >>> dataset = pq.ParquetDataset('dataset_v2_files/')\n\n        List the files:\n\n        >>> dataset.files\n        ['dataset_v2_files/year=2019/...-0.parquet', ...\n        \"\"\"\n        return self._dataset.files\n\n    @property\n    def filesystem(self):\n        \"\"\"\n        The filesystem type of the Dataset source.\n        \"\"\"\n        return self._dataset.filesystem\n\n    @property\n    def partitioning(self):\n        \"\"\"\n        The partitioning of the Dataset source, if discovered.\n        \"\"\"\n        return self._dataset.partitioning\n\n\n_read_table_docstring = \"\"\"\n{0}\n\nParameters\n----------\nsource : str, pyarrow.NativeFile, or file-like object\n    If a string passed, can be a single file name or directory name. For\n    file-like objects, only read a single file. Use pyarrow.BufferReader to\n    read a file contained in a bytes or buffer-like object.\ncolumns : list\n    If not None, only these columns will be read from the file. A column\n    name may be a prefix of a nested field, e.g. 'a' will select 'a.b',\n    'a.c', and 'a.d.e'. If empty, no columns will be read. Note\n    that the table will still have the correct num_rows set despite having\n    no columns.\nuse_threads : bool, default True\n    Perform multi-threaded column reads.\nschema : Schema, optional\n    Optionally provide the Schema for the parquet dataset, in which case it\n    will not be inferred from the source.\n{1}\nfilesystem : FileSystem, default None\n    If nothing passed, will be inferred based on path.\n    Path will try to be found in the local on-disk filesystem otherwise\n    it will be parsed as an URI to determine the filesystem.\nfilters : pyarrow.compute.Expression or List[Tuple] or List[List[Tuple]], default None\n    Rows which do not match the filter predicate will be removed from scanned\n    data. Partition keys embedded in a nested directory structure will be\n    exploited to avoid loading files at all if they contain no matching rows.\n    Within-file level filtering and different partitioning schemes are supported.\n\n    {3}\nuse_legacy_dataset : bool, optional\n    Deprecated and has no effect from PyArrow version 15.0.0.\nignore_prefixes : list, optional\n    Files matching any of these prefixes will be ignored by the\n    discovery process.\n    This is matched to the basename of a path.\n    By default this is ['.', '_'].\n    Note that discovery happens only if a directory is passed as source.\npre_buffer : bool, default True\n    Coalesce and issue file reads in parallel to improve performance on\n    high-latency filesystems (e.g. S3). If True, Arrow will use a\n    background I/O thread pool. If using a filesystem layer that itself\n    performs readahead (e.g. fsspec's S3FS), disable readahead for best\n    results.\ncoerce_int96_timestamp_unit : str, default None\n    Cast timestamps that are stored in INT96 format to a particular\n    resolution (e.g. 'ms'). Setting to None is equivalent to 'ns'\n    and therefore INT96 timestamps will be inferred as timestamps\n    in nanoseconds.\ndecryption_properties : FileDecryptionProperties or None\n    File-level decryption properties.\n    The decryption properties can be created using\n    ``CryptoFactory.file_decryption_properties()``.\nthrift_string_size_limit : int, default None\n    If not None, override the maximum total string size allocated\n    when decoding Thrift structures. The default limit should be\n    sufficient for most Parquet files.\nthrift_container_size_limit : int, default None\n    If not None, override the maximum total size of containers allocated\n    when decoding Thrift structures. The default limit should be\n    sufficient for most Parquet files.\npage_checksum_verification : bool, default False\n    If True, verify the checksum for each page read from the file.\n\nReturns\n-------\n{2}\n\n{4}\n\"\"\"\n\n_read_table_example = \"\"\"\\\n\nExamples\n--------\n\nGenerate an example PyArrow Table and write it to a partitioned dataset:\n\n>>> import pyarrow as pa\n>>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n...                   'n_legs': [2, 2, 4, 4, 5, 100],\n...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n...                              \"Brittle stars\", \"Centipede\"]})\n>>> import pyarrow.parquet as pq\n>>> pq.write_to_dataset(table, root_path='dataset_name_2',\n...                     partition_cols=['year'])\n\nRead the data:\n\n>>> pq.read_table('dataset_name_2').to_pandas()\n   n_legs         animal  year\n0       5  Brittle stars  2019\n1       2       Flamingo  2020\n2       4            Dog  2021\n3     100      Centipede  2021\n4       2         Parrot  2022\n5       4          Horse  2022\n\n\nRead only a subset of columns:\n\n>>> pq.read_table('dataset_name_2', columns=[\"n_legs\", \"animal\"])\npyarrow.Table\nn_legs: int64\nanimal: string\n----\nn_legs: [[5],[2],[4,100],[2,4]]\nanimal: [[\"Brittle stars\"],[\"Flamingo\"],[\"Dog\",\"Centipede\"],[\"Parrot\",\"Horse\"]]\n\nRead a subset of columns and read one column as DictionaryArray:\n\n>>> pq.read_table('dataset_name_2', columns=[\"n_legs\", \"animal\"],\n...               read_dictionary=[\"animal\"])\npyarrow.Table\nn_legs: int64\nanimal: dictionary<values=string, indices=int32, ordered=0>\n----\nn_legs: [[5],[2],[4,100],[2,4]]\nanimal: [  -- dictionary:\n[\"Brittle stars\"]  -- indices:\n[0],  -- dictionary:\n[\"Flamingo\"]  -- indices:\n[0],  -- dictionary:\n[\"Dog\",\"Centipede\"]  -- indices:\n[0,1],  -- dictionary:\n[\"Parrot\",\"Horse\"]  -- indices:\n[0,1]]\n\nRead the table with filter:\n\n>>> pq.read_table('dataset_name_2', columns=[\"n_legs\", \"animal\"],\n...               filters=[('n_legs','<',4)]).to_pandas()\n   n_legs    animal\n0       2  Flamingo\n1       2    Parrot\n\nRead data from a single Parquet file:\n\n>>> pq.write_table(table, 'example.parquet')\n>>> pq.read_table('dataset_name_2').to_pandas()\n   n_legs         animal  year\n0       5  Brittle stars  2019\n1       2       Flamingo  2020\n2       4            Dog  2021\n3     100      Centipede  2021\n4       2         Parrot  2022\n5       4          Horse  2022\n\"\"\"\n\n\ndef read_table(source, *, columns=None, use_threads=True,\n               schema=None, use_pandas_metadata=False, read_dictionary=None,\n               memory_map=False, buffer_size=0, partitioning=\"hive\",\n               filesystem=None, filters=None, use_legacy_dataset=None,\n               ignore_prefixes=None, pre_buffer=True,\n               coerce_int96_timestamp_unit=None,\n               decryption_properties=None, thrift_string_size_limit=None,\n               thrift_container_size_limit=None,\n               page_checksum_verification=False):\n\n    if use_legacy_dataset is not None:\n        warnings.warn(\n            \"Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 \"\n            \"and will be removed in a future version.\",\n            FutureWarning, stacklevel=2)\n\n    try:\n        dataset = ParquetDataset(\n            source,\n            schema=schema,\n            filesystem=filesystem,\n            partitioning=partitioning,\n            memory_map=memory_map,\n            read_dictionary=read_dictionary,\n            buffer_size=buffer_size,\n            filters=filters,\n            ignore_prefixes=ignore_prefixes,\n            pre_buffer=pre_buffer,\n            coerce_int96_timestamp_unit=coerce_int96_timestamp_unit,\n            decryption_properties=decryption_properties,\n            thrift_string_size_limit=thrift_string_size_limit,\n            thrift_container_size_limit=thrift_container_size_limit,\n            page_checksum_verification=page_checksum_verification,\n        )\n    except ImportError:\n        # fall back on ParquetFile for simple cases when pyarrow.dataset\n        # module is not available\n        if filters is not None:\n            raise ValueError(\n                \"the 'filters' keyword is not supported when the \"\n                \"pyarrow.dataset module is not available\"\n            )\n        if partitioning != \"hive\":\n            raise ValueError(\n                \"the 'partitioning' keyword is not supported when the \"\n                \"pyarrow.dataset module is not available\"\n            )\n        if schema is not None:\n            raise ValueError(\n                \"the 'schema' argument is not supported when the \"\n                \"pyarrow.dataset module is not available\"\n            )\n        filesystem, path = _resolve_filesystem_and_path(source, filesystem)\n        if filesystem is not None:\n            source = filesystem.open_input_file(path)\n        # TODO test that source is not a directory or a list\n        dataset = ParquetFile(\n            source, read_dictionary=read_dictionary,\n            memory_map=memory_map, buffer_size=buffer_size,\n            pre_buffer=pre_buffer,\n            coerce_int96_timestamp_unit=coerce_int96_timestamp_unit,\n            decryption_properties=decryption_properties,\n            thrift_string_size_limit=thrift_string_size_limit,\n            thrift_container_size_limit=thrift_container_size_limit,\n            page_checksum_verification=page_checksum_verification,\n        )\n\n    return dataset.read(columns=columns, use_threads=use_threads,\n                        use_pandas_metadata=use_pandas_metadata)\n\n\nread_table.__doc__ = _read_table_docstring.format(\n    \"\"\"Read a Table from Parquet format\"\"\",\n    \"\\n\".join((\"\"\"use_pandas_metadata : bool, default False\n    If True and file has custom pandas schema metadata, ensure that\n    index columns are also loaded.\"\"\", _read_docstring_common)),\n    \"\"\"pyarrow.Table\n    Content of the file as a table (of columns)\"\"\",\n    _DNF_filter_doc, _read_table_example)\n\n\ndef read_pandas(source, columns=None, **kwargs):\n    return read_table(\n        source, columns=columns, use_pandas_metadata=True, **kwargs\n    )\n\n\nread_pandas.__doc__ = _read_table_docstring.format(\n    'Read a Table from Parquet format, also reading DataFrame\\n'\n    'index values if known in the file metadata',\n    \"\\n\".join((_read_docstring_common,\n               \"\"\"**kwargs\n    additional options for :func:`read_table`\"\"\")),\n    \"\"\"pyarrow.Table\n    Content of the file as a Table of Columns, including DataFrame\n    indexes as columns\"\"\",\n    _DNF_filter_doc, \"\")\n\n\ndef write_table(table, where, row_group_size=None, version='2.6',\n                use_dictionary=True, compression='snappy',\n                write_statistics=True,\n                use_deprecated_int96_timestamps=None,\n                coerce_timestamps=None,\n                allow_truncated_timestamps=False,\n                data_page_size=None, flavor=None,\n                filesystem=None,\n                compression_level=None,\n                use_byte_stream_split=False,\n                column_encoding=None,\n                data_page_version='1.0',\n                use_compliant_nested_type=True,\n                encryption_properties=None,\n                write_batch_size=None,\n                dictionary_pagesize_limit=None,\n                store_schema=True,\n                write_page_index=False,\n                write_page_checksum=False,\n                sorting_columns=None,\n                **kwargs):\n    # Implementor's note: when adding keywords here / updating defaults, also\n    # update it in write_to_dataset and _dataset_parquet.pyx ParquetFileWriteOptions\n    row_group_size = kwargs.pop('chunk_size', row_group_size)\n    use_int96 = use_deprecated_int96_timestamps\n    try:\n        with ParquetWriter(\n                where, table.schema,\n                filesystem=filesystem,\n                version=version,\n                flavor=flavor,\n                use_dictionary=use_dictionary,\n                write_statistics=write_statistics,\n                coerce_timestamps=coerce_timestamps,\n                data_page_size=data_page_size,\n                allow_truncated_timestamps=allow_truncated_timestamps,\n                compression=compression,\n                use_deprecated_int96_timestamps=use_int96,\n                compression_level=compression_level,\n                use_byte_stream_split=use_byte_stream_split,\n                column_encoding=column_encoding,\n                data_page_version=data_page_version,\n                use_compliant_nested_type=use_compliant_nested_type,\n                encryption_properties=encryption_properties,\n                write_batch_size=write_batch_size,\n                dictionary_pagesize_limit=dictionary_pagesize_limit,\n                store_schema=store_schema,\n                write_page_index=write_page_index,\n                write_page_checksum=write_page_checksum,\n                sorting_columns=sorting_columns,\n                **kwargs) as writer:\n            writer.write_table(table, row_group_size=row_group_size)\n    except Exception:\n        if _is_path_like(where):\n            try:\n                os.remove(_stringify_path(where))\n            except os.error:\n                pass\n        raise\n\n\n_write_table_example = \"\"\"\\\nGenerate an example PyArrow Table:\n\n>>> import pyarrow as pa\n>>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n...                              \"Brittle stars\", \"Centipede\"]})\n\nand write the Table into Parquet file:\n\n>>> import pyarrow.parquet as pq\n>>> pq.write_table(table, 'example.parquet')\n\nDefining row group size for the Parquet file:\n\n>>> pq.write_table(table, 'example.parquet', row_group_size=3)\n\nDefining row group compression (default is Snappy):\n\n>>> pq.write_table(table, 'example.parquet', compression='none')\n\nDefining row group compression and encoding per-column:\n\n>>> pq.write_table(table, 'example.parquet',\n...                compression={'n_legs': 'snappy', 'animal': 'gzip'},\n...                use_dictionary=['n_legs', 'animal'])\n\nDefining column encoding per-column:\n\n>>> pq.write_table(table, 'example.parquet',\n...                column_encoding={'animal':'PLAIN'},\n...                use_dictionary=False)\n\"\"\"\n\nwrite_table.__doc__ = \"\"\"\nWrite a Table to Parquet format.\n\nParameters\n----------\ntable : pyarrow.Table\nwhere : string or pyarrow.NativeFile\nrow_group_size : int\n    Maximum number of rows in each written row group. If None, the\n    row group size will be the minimum of the Table size and\n    1024 * 1024.\n{}\n**kwargs : optional\n    Additional options for ParquetWriter\n\nExamples\n--------\n{}\n\"\"\".format(_parquet_writer_arg_docs, _write_table_example)\n\n\ndef write_to_dataset(table, root_path, partition_cols=None,\n                     filesystem=None, use_legacy_dataset=None,\n                     schema=None, partitioning=None,\n                     basename_template=None, use_threads=None,\n                     file_visitor=None, existing_data_behavior=None,\n                     **kwargs):\n    \"\"\"Wrapper around dataset.write_dataset for writing a Table to\n    Parquet format by partitions.\n    For each combination of partition columns and values,\n    a subdirectories are created in the following\n    manner:\n\n    root_dir/\n      group1=value1\n        group2=value1\n          <uuid>.parquet\n        group2=value2\n          <uuid>.parquet\n      group1=valueN\n        group2=value1\n          <uuid>.parquet\n        group2=valueN\n          <uuid>.parquet\n\n    Parameters\n    ----------\n    table : pyarrow.Table\n    root_path : str, pathlib.Path\n        The root directory of the dataset.\n    partition_cols : list,\n        Column names by which to partition the dataset.\n        Columns are partitioned in the order they are given.\n    filesystem : FileSystem, default None\n        If nothing passed, will be inferred based on path.\n        Path will try to be found in the local on-disk filesystem otherwise\n        it will be parsed as an URI to determine the filesystem.\n    use_legacy_dataset : bool, optional\n        Deprecated and has no effect from PyArrow version 15.0.0.\n    schema : Schema, optional\n        This Schema of the dataset.\n    partitioning : Partitioning or list[str], optional\n        The partitioning scheme specified with the\n        ``pyarrow.dataset.partitioning()`` function or a list of field names.\n        When providing a list of field names, you can use\n        ``partitioning_flavor`` to drive which partitioning type should be\n        used.\n    basename_template : str, optional\n        A template string used to generate basenames of written data files.\n        The token '{i}' will be replaced with an automatically incremented\n        integer. If not specified, it defaults to \"guid-{i}.parquet\".\n    use_threads : bool, default True\n        Write files in parallel. If enabled, then maximum parallelism will be\n        used determined by the number of available CPU cores.\n    file_visitor : function\n        If set, this function will be called with a WrittenFile instance\n        for each file created during the call.  This object will have both\n        a path attribute and a metadata attribute.\n\n        The path attribute will be a string containing the path to\n        the created file.\n\n        The metadata attribute will be the parquet metadata of the file.\n        This metadata will have the file path attribute set and can be used\n        to build a _metadata file.  The metadata attribute will be None if\n        the format is not parquet.\n\n        Example visitor which simple collects the filenames created::\n\n            visited_paths = []\n\n            def file_visitor(written_file):\n                visited_paths.append(written_file.path)\n\n    existing_data_behavior : 'overwrite_or_ignore' | 'error' | \\\n'delete_matching'\n        Controls how the dataset will handle data that already exists in\n        the destination. The default behaviour is 'overwrite_or_ignore'.\n\n        'overwrite_or_ignore' will ignore any existing data and will\n        overwrite files with the same name as an output file.  Other\n        existing files will be ignored.  This behavior, in combination\n        with a unique basename_template for each write, will allow for\n        an append workflow.\n\n        'error' will raise an error if any data exists in the destination.\n\n        'delete_matching' is useful when you are writing a partitioned\n        dataset.  The first time each partition directory is encountered\n        the entire directory will be deleted.  This allows you to overwrite\n        old partitions completely.\n    **kwargs : dict,\n        Used as additional kwargs for :func:`pyarrow.dataset.write_dataset`\n        function for matching kwargs, and remainder to\n        :func:`pyarrow.dataset.ParquetFileFormat.make_write_options`.\n        See the docstring of :func:`write_table` and\n        :func:`pyarrow.dataset.write_dataset` for the available options.\n        Using `metadata_collector` in kwargs allows one to collect the\n        file metadata instances of dataset pieces. The file paths in the\n        ColumnChunkMetaData will be set relative to `root_path`.\n\n    Examples\n    --------\n    Generate an example PyArrow Table:\n\n    >>> import pyarrow as pa\n    >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n    ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n    ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n    ...                              \"Brittle stars\", \"Centipede\"]})\n\n    and write it to a partitioned dataset:\n\n    >>> import pyarrow.parquet as pq\n    >>> pq.write_to_dataset(table, root_path='dataset_name_3',\n    ...                     partition_cols=['year'])\n    >>> pq.ParquetDataset('dataset_name_3').files\n    ['dataset_name_3/year=2019/...-0.parquet', ...\n\n    Write a single Parquet file into the root folder:\n\n    >>> pq.write_to_dataset(table, root_path='dataset_name_4')\n    >>> pq.ParquetDataset('dataset_name_4/').files\n    ['dataset_name_4/...-0.parquet']\n    \"\"\"\n    if use_legacy_dataset is not None:\n        warnings.warn(\n            \"Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 \"\n            \"and will be removed in a future version.\",\n            FutureWarning, stacklevel=2)\n\n    metadata_collector = kwargs.pop('metadata_collector', None)\n\n    # Check for conflicting keywords\n    msg_confl = (\n        \"The '{1}' argument is not supported. \"\n        \"Use only '{0}' instead.\"\n    )\n    if partition_cols is not None and partitioning is not None:\n        raise ValueError(msg_confl.format(\"partitioning\",\n                                          \"partition_cols\"))\n\n    if metadata_collector is not None and file_visitor is not None:\n        raise ValueError(msg_confl.format(\"file_visitor\",\n                                          \"metadata_collector\"))\n\n    import pyarrow.dataset as ds\n\n    # extract write_dataset specific options\n    # reset assumed to go to make_write_options\n    write_dataset_kwargs = dict()\n    for key in inspect.signature(ds.write_dataset).parameters:\n        if key in kwargs:\n            write_dataset_kwargs[key] = kwargs.pop(key)\n    write_dataset_kwargs['max_rows_per_group'] = kwargs.pop(\n        'row_group_size', kwargs.pop(\"chunk_size\", None)\n    )\n\n    if metadata_collector is not None:\n        def file_visitor(written_file):\n            metadata_collector.append(written_file.metadata)\n\n    # map format arguments\n    parquet_format = ds.ParquetFileFormat()\n    write_options = parquet_format.make_write_options(**kwargs)\n\n    # map old filesystems to new one\n    if filesystem is not None:\n        filesystem = _ensure_filesystem(filesystem)\n\n    if partition_cols:\n        part_schema = table.select(partition_cols).schema\n        partitioning = ds.partitioning(part_schema, flavor=\"hive\")\n\n    if basename_template is None:\n        basename_template = guid() + '-{i}.parquet'\n\n    if existing_data_behavior is None:\n        existing_data_behavior = 'overwrite_or_ignore'\n\n    ds.write_dataset(\n        table, root_path, filesystem=filesystem,\n        format=parquet_format, file_options=write_options, schema=schema,\n        partitioning=partitioning, use_threads=use_threads,\n        file_visitor=file_visitor,\n        basename_template=basename_template,\n        existing_data_behavior=existing_data_behavior,\n        **write_dataset_kwargs)\n    return\n\n\ndef write_metadata(schema, where, metadata_collector=None, filesystem=None,\n                   **kwargs):\n    \"\"\"\n    Write metadata-only Parquet file from schema. This can be used with\n    `write_to_dataset` to generate `_common_metadata` and `_metadata` sidecar\n    files.\n\n    Parameters\n    ----------\n    schema : pyarrow.Schema\n    where : string or pyarrow.NativeFile\n    metadata_collector : list\n        where to collect metadata information.\n    filesystem : FileSystem, default None\n        If nothing passed, will be inferred from `where` if path-like, else\n        `where` is already a file-like object so no filesystem is needed.\n    **kwargs : dict,\n        Additional kwargs for ParquetWriter class. See docstring for\n        `ParquetWriter` for more information.\n\n    Examples\n    --------\n    Generate example data:\n\n    >>> import pyarrow as pa\n    >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n    ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n    ...                              \"Brittle stars\", \"Centipede\"]})\n\n    Write a dataset and collect metadata information.\n\n    >>> metadata_collector = []\n    >>> import pyarrow.parquet as pq\n    >>> pq.write_to_dataset(\n    ...     table, 'dataset_metadata',\n    ...      metadata_collector=metadata_collector)\n\n    Write the `_common_metadata` parquet file without row groups statistics.\n\n    >>> pq.write_metadata(\n    ...     table.schema, 'dataset_metadata/_common_metadata')\n\n    Write the `_metadata` parquet file with row groups statistics.\n\n    >>> pq.write_metadata(\n    ...     table.schema, 'dataset_metadata/_metadata',\n    ...     metadata_collector=metadata_collector)\n    \"\"\"\n    filesystem, where = _resolve_filesystem_and_path(where, filesystem)\n\n    if hasattr(where, \"seek\"):  # file-like\n        cursor_position = where.tell()\n\n    writer = ParquetWriter(where, schema, filesystem, **kwargs)\n    writer.close()\n\n    if metadata_collector is not None:\n        # ParquetWriter doesn't expose the metadata until it's written. Write\n        # it and read it again.\n        metadata = read_metadata(where, filesystem=filesystem)\n        if hasattr(where, \"seek\"):\n            where.seek(cursor_position)  # file-like, set cursor back.\n\n        for m in metadata_collector:\n            metadata.append_row_groups(m)\n        if filesystem is not None:\n            with filesystem.open_output_stream(where) as f:\n                metadata.write_metadata_file(f)\n        else:\n            metadata.write_metadata_file(where)\n\n\ndef read_metadata(where, memory_map=False, decryption_properties=None,\n                  filesystem=None):\n    \"\"\"\n    Read FileMetaData from footer of a single Parquet file.\n\n    Parameters\n    ----------\n    where : str (file path) or file-like object\n    memory_map : bool, default False\n        Create memory map when the source is a file path.\n    decryption_properties : FileDecryptionProperties, default None\n        Decryption properties for reading encrypted Parquet files.\n    filesystem : FileSystem, default None\n        If nothing passed, will be inferred based on path.\n        Path will try to be found in the local on-disk filesystem otherwise\n        it will be parsed as an URI to determine the filesystem.\n\n    Returns\n    -------\n    metadata : FileMetaData\n        The metadata of the Parquet file\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> import pyarrow.parquet as pq\n    >>> table = pa.table({'n_legs': [4, 5, 100],\n    ...                   'animal': [\"Dog\", \"Brittle stars\", \"Centipede\"]})\n    >>> pq.write_table(table, 'example.parquet')\n\n    >>> pq.read_metadata('example.parquet')\n    <pyarrow._parquet.FileMetaData object at ...>\n      created_by: parquet-cpp-arrow version ...\n      num_columns: 2\n      num_rows: 3\n      num_row_groups: 1\n      format_version: 2.6\n      serialized_size: ...\n    \"\"\"\n    filesystem, where = _resolve_filesystem_and_path(where, filesystem)\n    file_ctx = nullcontext()\n    if filesystem is not None:\n        file_ctx = where = filesystem.open_input_file(where)\n\n    with file_ctx:\n        file = ParquetFile(where, memory_map=memory_map,\n                           decryption_properties=decryption_properties)\n        return file.metadata\n\n\ndef read_schema(where, memory_map=False, decryption_properties=None,\n                filesystem=None):\n    \"\"\"\n    Read effective Arrow schema from Parquet file metadata.\n\n    Parameters\n    ----------\n    where : str (file path) or file-like object\n    memory_map : bool, default False\n        Create memory map when the source is a file path.\n    decryption_properties : FileDecryptionProperties, default None\n        Decryption properties for reading encrypted Parquet files.\n    filesystem : FileSystem, default None\n        If nothing passed, will be inferred based on path.\n        Path will try to be found in the local on-disk filesystem otherwise\n        it will be parsed as an URI to determine the filesystem.\n\n    Returns\n    -------\n    schema : pyarrow.Schema\n        The schema of the Parquet file\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> import pyarrow.parquet as pq\n    >>> table = pa.table({'n_legs': [4, 5, 100],\n    ...                   'animal': [\"Dog\", \"Brittle stars\", \"Centipede\"]})\n    >>> pq.write_table(table, 'example.parquet')\n\n    >>> pq.read_schema('example.parquet')\n    n_legs: int64\n    animal: string\n    \"\"\"\n    filesystem, where = _resolve_filesystem_and_path(where, filesystem)\n    file_ctx = nullcontext()\n    if filesystem is not None:\n        file_ctx = where = filesystem.open_input_file(where)\n\n    with file_ctx:\n        file = ParquetFile(\n            where, memory_map=memory_map,\n            decryption_properties=decryption_properties)\n        return file.schema.to_arrow_schema()\n\n\n__all__ = (\n    \"ColumnChunkMetaData\",\n    \"ColumnSchema\",\n    \"FileDecryptionProperties\",\n    \"FileEncryptionProperties\",\n    \"FileMetaData\",\n    \"ParquetDataset\",\n    \"ParquetFile\",\n    \"ParquetLogicalType\",\n    \"ParquetReader\",\n    \"ParquetSchema\",\n    \"ParquetWriter\",\n    \"RowGroupMetaData\",\n    \"SortingColumn\",\n    \"Statistics\",\n    \"read_metadata\",\n    \"read_pandas\",\n    \"read_schema\",\n    \"read_table\",\n    \"write_metadata\",\n    \"write_table\",\n    \"write_to_dataset\",\n    \"_filters_to_expression\",\n    \"filters_to_expression\",\n)\n", "python/pyarrow/parquet/encryption.py": "# pylint: disable=unused-wildcard-import, unused-import\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom pyarrow._parquet_encryption import (CryptoFactory,   # noqa\n                                         EncryptionConfiguration,\n                                         DecryptionConfiguration,\n                                         KmsConnectionConfig,\n                                         KmsClient)\n", "python/pyarrow/parquet/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# flake8: noqa\n\nfrom .core import *\n", "python/pyarrow/vendored/version.py": "# Vendored from https://github.com/pypa/packaging,\n# changeset b5878c977206f60302536db969a8cef420853ade\n\n# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of the\n# `packaging` repository for complete details.\n\nimport collections\nimport itertools\nimport re\nimport warnings\n\n__all__ = [\"parse\", \"Version\", \"LegacyVersion\",\n           \"InvalidVersion\", \"VERSION_PATTERN\"]\n\n\nclass InfinityType:\n    def __repr__(self):\n        return \"Infinity\"\n\n    def __hash__(self):\n        return hash(repr(self))\n\n    def __lt__(self, other):\n        return False\n\n    def __le__(self, other):\n        return False\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__)\n\n    def __ne__(self, other):\n        return not isinstance(other, self.__class__)\n\n    def __gt__(self, other):\n        return True\n\n    def __ge__(self, other):\n        return True\n\n    def __neg__(self):\n        return NegativeInfinity\n\n\nInfinity = InfinityType()\n\n\nclass NegativeInfinityType:\n    def __repr__(self):\n        return \"-Infinity\"\n\n    def __hash__(self):\n        return hash(repr(self))\n\n    def __lt__(self, other):\n        return True\n\n    def __le__(self, other):\n        return True\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__)\n\n    def __ne__(self, other):\n        return not isinstance(other, self.__class__)\n\n    def __gt__(self, other):\n        return False\n\n    def __ge__(self, other):\n        return False\n\n    def __neg__(self):\n        return Infinity\n\n\nNegativeInfinity = NegativeInfinityType()\n\n\n_Version = collections.namedtuple(\n    \"_Version\", [\"epoch\", \"release\", \"dev\", \"pre\", \"post\", \"local\"]\n)\n\n\ndef parse(version):\n    \"\"\"\n    Parse the given version string and return either a :class:`Version` object\n    or a :class:`LegacyVersion` object depending on if the given version is\n    a valid PEP 440 version or a legacy version.\n    \"\"\"\n    try:\n        return Version(version)\n    except InvalidVersion:\n        return LegacyVersion(version)\n\n\nclass InvalidVersion(ValueError):\n    \"\"\"\n    An invalid version was found, users should refer to PEP 440.\n    \"\"\"\n\n\nclass _BaseVersion:\n\n    def __hash__(self):\n        return hash(self._key)\n\n    # Please keep the duplicated `isinstance` check\n    # in the six comparisons hereunder\n    # unless you find a way to avoid adding overhead function calls.\n    def __lt__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key < other._key\n\n    def __le__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key <= other._key\n\n    def __eq__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key == other._key\n\n    def __ge__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key >= other._key\n\n    def __gt__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key > other._key\n\n    def __ne__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key != other._key\n\n\nclass LegacyVersion(_BaseVersion):\n    def __init__(self, version):\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n\n        warnings.warn(\n            \"Creating a LegacyVersion has been deprecated and will be \"\n            \"removed in the next major release\",\n            DeprecationWarning,\n        )\n\n    def __str__(self):\n        return self._version\n\n    def __repr__(self):\n        return f\"<LegacyVersion('{self}')>\"\n\n    @property\n    def public(self):\n        return self._version\n\n    @property\n    def base_version(self):\n        return self._version\n\n    @property\n    def epoch(self):\n        return -1\n\n    @property\n    def release(self):\n        return None\n\n    @property\n    def pre(self):\n        return None\n\n    @property\n    def post(self):\n        return None\n\n    @property\n    def dev(self):\n        return None\n\n    @property\n    def local(self):\n        return None\n\n    @property\n    def is_prerelease(self):\n        return False\n\n    @property\n    def is_postrelease(self):\n        return False\n\n    @property\n    def is_devrelease(self):\n        return False\n\n\n_legacy_version_component_re = re.compile(\n    r\"(\\d+ | [a-z]+ | \\.| -)\", re.VERBOSE)\n\n_legacy_version_replacement_map = {\n    \"pre\": \"c\",\n    \"preview\": \"c\",\n    \"-\": \"final-\",\n    \"rc\": \"c\",\n    \"dev\": \"@\",\n}\n\n\ndef _parse_version_parts(s):\n    for part in _legacy_version_component_re.split(s):\n        part = _legacy_version_replacement_map.get(part, part)\n\n        if not part or part == \".\":\n            continue\n\n        if part[:1] in \"0123456789\":\n            # pad for numeric comparison\n            yield part.zfill(8)\n        else:\n            yield \"*\" + part\n\n    # ensure that alpha/beta/candidate are before final\n    yield \"*final\"\n\n\ndef _legacy_cmpkey(version):\n\n    # We hardcode an epoch of -1 here. A PEP 440 version can only have a epoch\n    # greater than or equal to 0. This will effectively put the LegacyVersion,\n    # which uses the defacto standard originally implemented by setuptools,\n    # as before all PEP 440 versions.\n    epoch = -1\n\n    # This scheme is taken from pkg_resources.parse_version setuptools prior to\n    # it's adoption of the packaging library.\n    parts = []\n    for part in _parse_version_parts(version.lower()):\n        if part.startswith(\"*\"):\n            # remove \"-\" before a prerelease tag\n            if part < \"*final\":\n                while parts and parts[-1] == \"*final-\":\n                    parts.pop()\n\n            # remove trailing zeros from each series of numeric parts\n            while parts and parts[-1] == \"00000000\":\n                parts.pop()\n\n        parts.append(part)\n\n    return epoch, tuple(parts)\n\n\n# Deliberately not anchored to the start and end of the string, to make it\n# easier for 3rd party code to reuse\nVERSION_PATTERN = r\"\"\"\n    v?\n    (?:\n        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n        (?P<pre>                                          # pre-release\n            [-_\\.]?\n            (?P<pre_l>(a|b|c|rc|alpha|beta|pre|preview))\n            [-_\\.]?\n            (?P<pre_n>[0-9]+)?\n        )?\n        (?P<post>                                         # post release\n            (?:-(?P<post_n1>[0-9]+))\n            |\n            (?:\n                [-_\\.]?\n                (?P<post_l>post|rev|r)\n                [-_\\.]?\n                (?P<post_n2>[0-9]+)?\n            )\n        )?\n        (?P<dev>                                          # dev release\n            [-_\\.]?\n            (?P<dev_l>dev)\n            [-_\\.]?\n            (?P<dev_n>[0-9]+)?\n        )?\n    )\n    (?:\\+(?P<local>[a-z0-9]+(?:[-_\\.][a-z0-9]+)*))?       # local version\n\"\"\"\n\n\nclass Version(_BaseVersion):\n\n    _regex = re.compile(r\"^\\s*\" + VERSION_PATTERN +\n                        r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n\n    def __init__(self, version):\n\n        # Validate the version and parse it into pieces\n        match = self._regex.search(version)\n        if not match:\n            raise InvalidVersion(f\"Invalid version: '{version}'\")\n\n        # Store the parsed out pieces of the version\n        self._version = _Version(\n            epoch=int(match.group(\"epoch\")) if match.group(\"epoch\") else 0,\n            release=tuple(int(i) for i in match.group(\"release\").split(\".\")),\n            pre=_parse_letter_version(\n                match.group(\"pre_l\"), match.group(\"pre_n\")),\n            post=_parse_letter_version(\n                match.group(\"post_l\"), match.group(\n                    \"post_n1\") or match.group(\"post_n2\")\n            ),\n            dev=_parse_letter_version(\n                match.group(\"dev_l\"), match.group(\"dev_n\")),\n            local=_parse_local_version(match.group(\"local\")),\n        )\n\n        # Generate a key which will be used for sorting\n        self._key = _cmpkey(\n            self._version.epoch,\n            self._version.release,\n            self._version.pre,\n            self._version.post,\n            self._version.dev,\n            self._version.local,\n        )\n\n    def __repr__(self):\n        return f\"<Version('{self}')>\"\n\n    def __str__(self):\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(f\"{self.epoch}!\")\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        # Pre-release\n        if self.pre is not None:\n            parts.append(\"\".join(str(x) for x in self.pre))\n\n        # Post-release\n        if self.post is not None:\n            parts.append(f\".post{self.post}\")\n\n        # Development release\n        if self.dev is not None:\n            parts.append(f\".dev{self.dev}\")\n\n        # Local version segment\n        if self.local is not None:\n            parts.append(f\"+{self.local}\")\n\n        return \"\".join(parts)\n\n    @property\n    def epoch(self):\n        _epoch = self._version.epoch\n        return _epoch\n\n    @property\n    def release(self):\n        _release = self._version.release\n        return _release\n\n    @property\n    def pre(self):\n        _pre = self._version.pre\n        return _pre\n\n    @property\n    def post(self):\n        return self._version.post[1] if self._version.post else None\n\n    @property\n    def dev(self):\n        return self._version.dev[1] if self._version.dev else None\n\n    @property\n    def local(self):\n        if self._version.local:\n            return \".\".join(str(x) for x in self._version.local)\n        else:\n            return None\n\n    @property\n    def public(self):\n        return str(self).split(\"+\", 1)[0]\n\n    @property\n    def base_version(self):\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(f\"{self.epoch}!\")\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        return \"\".join(parts)\n\n    @property\n    def is_prerelease(self):\n        return self.dev is not None or self.pre is not None\n\n    @property\n    def is_postrelease(self):\n        return self.post is not None\n\n    @property\n    def is_devrelease(self):\n        return self.dev is not None\n\n    @property\n    def major(self):\n        return self.release[0] if len(self.release) >= 1 else 0\n\n    @property\n    def minor(self):\n        return self.release[1] if len(self.release) >= 2 else 0\n\n    @property\n    def micro(self):\n        return self.release[2] if len(self.release) >= 3 else 0\n\n\ndef _parse_letter_version(letter, number):\n\n    if letter:\n        # We consider there to be an implicit 0 in a pre-release if there is\n        # not a numeral associated with it.\n        if number is None:\n            number = 0\n\n        # We normalize any letters to their lower case form\n        letter = letter.lower()\n\n        # We consider some words to be alternate spellings of other words and\n        # in those cases we want to normalize the spellings to our preferred\n        # spelling.\n        if letter == \"alpha\":\n            letter = \"a\"\n        elif letter == \"beta\":\n            letter = \"b\"\n        elif letter in [\"c\", \"pre\", \"preview\"]:\n            letter = \"rc\"\n        elif letter in [\"rev\", \"r\"]:\n            letter = \"post\"\n\n        return letter, int(number)\n    if not letter and number:\n        # We assume if we are given a number, but we are not given a letter\n        # then this is using the implicit post release syntax (e.g. 1.0-1)\n        letter = \"post\"\n\n        return letter, int(number)\n\n    return None\n\n\n_local_version_separators = re.compile(r\"[\\._-]\")\n\n\ndef _parse_local_version(local):\n    \"\"\"\n    Takes a string like abc.1.twelve and turns it into (\"abc\", 1, \"twelve\").\n    \"\"\"\n    if local is not None:\n        return tuple(\n            part.lower() if not part.isdigit() else int(part)\n            for part in _local_version_separators.split(local)\n        )\n    return None\n\n\ndef _cmpkey(epoch, release, pre, post, dev, local):\n\n    # When we compare a release version, we want to compare it with all of the\n    # trailing zeros removed. So we'll use a reverse the list, drop all the now\n    # leading zeros until we come to something non zero, then take the rest\n    # re-reverse it back into the correct order and make it a tuple and use\n    # that for our sorting key.\n    _release = tuple(\n        reversed(list(itertools.dropwhile(lambda x: x == 0,\n                                          reversed(release))))\n    )\n\n    # We need to \"trick\" the sorting algorithm to put 1.0.dev0 before 1.0a0.\n    # We'll do this by abusing the pre segment, but we _only_ want to do this\n    # if there is not a pre or a post segment. If we have one of those then\n    # the normal sorting rules will handle this case correctly.\n    if pre is None and post is None and dev is not None:\n        _pre = NegativeInfinity\n    # Versions without a pre-release (except as noted above) should sort after\n    # those with one.\n    elif pre is None:\n        _pre = Infinity\n    else:\n        _pre = pre\n\n    # Versions without a post segment should sort before those with one.\n    if post is None:\n        _post = NegativeInfinity\n\n    else:\n        _post = post\n\n    # Versions without a development segment should sort after those with one.\n    if dev is None:\n        _dev = Infinity\n\n    else:\n        _dev = dev\n\n    if local is None:\n        # Versions without a local segment should sort before those with one.\n        _local = NegativeInfinity\n    else:\n        # Versions with a local segment need that segment parsed to implement\n        # the sorting rules in PEP440.\n        # - Alpha numeric segments sort before numeric segments\n        # - Alpha numeric segments sort lexicographically\n        # - Numeric segments sort numerically\n        # - Shorter versions sort before longer versions when the prefixes\n        #   match exactly\n        _local = tuple(\n            (i, \"\") if isinstance(i, int) else (NegativeInfinity, i)\n            for i in local\n        )\n\n    return epoch, _release, _pre, _post, _dev, _local\n", "python/pyarrow/vendored/docscrape.py": "# Vendored from https://github.com/numpy/numpydoc/,\n# changeset 4ae1e00e72e522c126403c1814f0b99dc5978622\n\n# This file is licensed under the BSD License. See the LICENSE.txt file\n# in the root of the `numpydoc` repository for complete details.\n\n\"\"\"Extract reference documentation from the NumPy source tree.\n\n\"\"\"\nimport inspect\nimport textwrap\nimport re\nimport pydoc\nfrom warnings import warn\nfrom collections import namedtuple\nfrom collections.abc import Callable, Mapping\nimport copy\nimport sys\n\n\ndef strip_blank_lines(l):\n    \"Remove leading and trailing blank lines from a list of lines\"\n    while l and not l[0].strip():\n        del l[0]\n    while l and not l[-1].strip():\n        del l[-1]\n    return l\n\n\nclass Reader:\n    \"\"\"A line-based string reader.\n\n    \"\"\"\n\n    def __init__(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data : str\n           String with lines separated by '\\\\n'.\n\n        \"\"\"\n        if isinstance(data, list):\n            self._str = data\n        else:\n            self._str = data.split('\\n')  # store string as list of lines\n\n        self.reset()\n\n    def __getitem__(self, n):\n        return self._str[n]\n\n    def reset(self):\n        self._l = 0  # current line nr\n\n    def read(self):\n        if not self.eof():\n            out = self[self._l]\n            self._l += 1\n            return out\n        else:\n            return ''\n\n    def seek_next_non_empty_line(self):\n        for l in self[self._l:]:\n            if l.strip():\n                break\n            else:\n                self._l += 1\n\n    def eof(self):\n        return self._l >= len(self._str)\n\n    def read_to_condition(self, condition_func):\n        start = self._l\n        for line in self[start:]:\n            if condition_func(line):\n                return self[start:self._l]\n            self._l += 1\n            if self.eof():\n                return self[start:self._l+1]\n        return []\n\n    def read_to_next_empty_line(self):\n        self.seek_next_non_empty_line()\n\n        def is_empty(line):\n            return not line.strip()\n\n        return self.read_to_condition(is_empty)\n\n    def read_to_next_unindented_line(self):\n        def is_unindented(line):\n            return (line.strip() and (len(line.lstrip()) == len(line)))\n        return self.read_to_condition(is_unindented)\n\n    def peek(self, n=0):\n        if self._l + n < len(self._str):\n            return self[self._l + n]\n        else:\n            return ''\n\n    def is_empty(self):\n        return not ''.join(self._str).strip()\n\n\nclass ParseError(Exception):\n    def __str__(self):\n        message = self.args[0]\n        if hasattr(self, 'docstring'):\n            message = \"%s in %r\" % (message, self.docstring)\n        return message\n\n\nParameter = namedtuple('Parameter', ['name', 'type', 'desc'])\n\n\nclass NumpyDocString(Mapping):\n    \"\"\"Parses a numpydoc string to an abstract representation\n\n    Instances define a mapping from section title to structured data.\n\n    \"\"\"\n\n    sections = {\n        'Signature': '',\n        'Summary': [''],\n        'Extended Summary': [],\n        'Parameters': [],\n        'Returns': [],\n        'Yields': [],\n        'Receives': [],\n        'Raises': [],\n        'Warns': [],\n        'Other Parameters': [],\n        'Attributes': [],\n        'Methods': [],\n        'See Also': [],\n        'Notes': [],\n        'Warnings': [],\n        'References': '',\n        'Examples': '',\n        'index': {}\n    }\n\n    def __init__(self, docstring, config=None):\n        orig_docstring = docstring\n        docstring = textwrap.dedent(docstring).split('\\n')\n\n        self._doc = Reader(docstring)\n        self._parsed_data = copy.deepcopy(self.sections)\n\n        try:\n            self._parse()\n        except ParseError as e:\n            e.docstring = orig_docstring\n            raise\n\n    def __getitem__(self, key):\n        return self._parsed_data[key]\n\n    def __setitem__(self, key, val):\n        if key not in self._parsed_data:\n            self._error_location(\"Unknown section %s\" % key, error=False)\n        else:\n            self._parsed_data[key] = val\n\n    def __iter__(self):\n        return iter(self._parsed_data)\n\n    def __len__(self):\n        return len(self._parsed_data)\n\n    def _is_at_section(self):\n        self._doc.seek_next_non_empty_line()\n\n        if self._doc.eof():\n            return False\n\n        l1 = self._doc.peek().strip()  # e.g. Parameters\n\n        if l1.startswith('.. index::'):\n            return True\n\n        l2 = self._doc.peek(1).strip()  # ---------- or ==========\n        if len(l2) >= 3 and (set(l2) in ({'-'}, {'='})) and len(l2) != len(l1):\n            snip = '\\n'.join(self._doc._str[:2])+'...'\n            self._error_location(\"potentially wrong underline length... \\n%s \\n%s in \\n%s\"\n                                 % (l1, l2, snip), error=False)\n        return l2.startswith('-'*len(l1)) or l2.startswith('='*len(l1))\n\n    def _strip(self, doc):\n        i = 0\n        j = 0\n        for i, line in enumerate(doc):\n            if line.strip():\n                break\n\n        for j, line in enumerate(doc[::-1]):\n            if line.strip():\n                break\n\n        return doc[i:len(doc)-j]\n\n    def _read_to_next_section(self):\n        section = self._doc.read_to_next_empty_line()\n\n        while not self._is_at_section() and not self._doc.eof():\n            if not self._doc.peek(-1).strip():  # previous line was empty\n                section += ['']\n\n            section += self._doc.read_to_next_empty_line()\n\n        return section\n\n    def _read_sections(self):\n        while not self._doc.eof():\n            data = self._read_to_next_section()\n            name = data[0].strip()\n\n            if name.startswith('..'):  # index section\n                yield name, data[1:]\n            elif len(data) < 2:\n                yield StopIteration\n            else:\n                yield name, self._strip(data[2:])\n\n    def _parse_param_list(self, content, single_element_is_type=False):\n        content = dedent_lines(content)\n        r = Reader(content)\n        params = []\n        while not r.eof():\n            header = r.read().strip()\n            if ' :' in header:\n                arg_name, arg_type = header.split(' :', maxsplit=1)\n                arg_name, arg_type = arg_name.strip(), arg_type.strip()\n            else:\n                if single_element_is_type:\n                    arg_name, arg_type = '', header\n                else:\n                    arg_name, arg_type = header, ''\n\n            desc = r.read_to_next_unindented_line()\n            desc = dedent_lines(desc)\n            desc = strip_blank_lines(desc)\n\n            params.append(Parameter(arg_name, arg_type, desc))\n\n        return params\n\n    # See also supports the following formats.\n    #\n    # <FUNCNAME>\n    # <FUNCNAME> SPACE* COLON SPACE+ <DESC> SPACE*\n    # <FUNCNAME> ( COMMA SPACE+ <FUNCNAME>)+ (COMMA | PERIOD)? SPACE*\n    # <FUNCNAME> ( COMMA SPACE+ <FUNCNAME>)* SPACE* COLON SPACE+ <DESC> SPACE*\n\n    # <FUNCNAME> is one of\n    #   <PLAIN_FUNCNAME>\n    #   COLON <ROLE> COLON BACKTICK <PLAIN_FUNCNAME> BACKTICK\n    # where\n    #   <PLAIN_FUNCNAME> is a legal function name, and\n    #   <ROLE> is any nonempty sequence of word characters.\n    # Examples: func_f1  :meth:`func_h1` :obj:`~baz.obj_r` :class:`class_j`\n    # <DESC> is a string describing the function.\n\n    _role = r\":(?P<role>(py:)?\\w+):\"\n    _funcbacktick = r\"`(?P<name>(?:~\\w+\\.)?[a-zA-Z0-9_\\.-]+)`\"\n    _funcplain = r\"(?P<name2>[a-zA-Z0-9_\\.-]+)\"\n    _funcname = r\"(\" + _role + _funcbacktick + r\"|\" + _funcplain + r\")\"\n    _funcnamenext = _funcname.replace('role', 'rolenext')\n    _funcnamenext = _funcnamenext.replace('name', 'namenext')\n    _description = r\"(?P<description>\\s*:(\\s+(?P<desc>\\S+.*))?)?\\s*$\"\n    _func_rgx = re.compile(r\"^\\s*\" + _funcname + r\"\\s*\")\n    _line_rgx = re.compile(\n        r\"^\\s*\" +\n        r\"(?P<allfuncs>\" +        # group for all function names\n        _funcname +\n        r\"(?P<morefuncs>([,]\\s+\" + _funcnamenext + r\")*)\" +\n        r\")\" +                     # end of \"allfuncs\"\n        # Some function lists have a trailing comma (or period)  '\\s*'\n        r\"(?P<trailing>[,\\.])?\" +\n        _description)\n\n    # Empty <DESC> elements are replaced with '..'\n    empty_description = '..'\n\n    def _parse_see_also(self, content):\n        \"\"\"\n        func_name : Descriptive text\n            continued text\n        another_func_name : Descriptive text\n        func_name1, func_name2, :meth:`func_name`, func_name3\n\n        \"\"\"\n\n        content = dedent_lines(content)\n\n        items = []\n\n        def parse_item_name(text):\n            \"\"\"Match ':role:`name`' or 'name'.\"\"\"\n            m = self._func_rgx.match(text)\n            if not m:\n                self._error_location(f\"Error parsing See Also entry {line!r}\")\n            role = m.group('role')\n            name = m.group('name') if role else m.group('name2')\n            return name, role, m.end()\n\n        rest = []\n        for line in content:\n            if not line.strip():\n                continue\n\n            line_match = self._line_rgx.match(line)\n            description = None\n            if line_match:\n                description = line_match.group('desc')\n                if line_match.group('trailing') and description:\n                    self._error_location(\n                        'Unexpected comma or period after function list at index %d of '\n                        'line \"%s\"' % (line_match.end('trailing'), line),\n                        error=False)\n            if not description and line.startswith(' '):\n                rest.append(line.strip())\n            elif line_match:\n                funcs = []\n                text = line_match.group('allfuncs')\n                while True:\n                    if not text.strip():\n                        break\n                    name, role, match_end = parse_item_name(text)\n                    funcs.append((name, role))\n                    text = text[match_end:].strip()\n                    if text and text[0] == ',':\n                        text = text[1:].strip()\n                rest = list(filter(None, [description]))\n                items.append((funcs, rest))\n            else:\n                self._error_location(f\"Error parsing See Also entry {line!r}\")\n        return items\n\n    def _parse_index(self, section, content):\n        \"\"\"\n        .. index: default\n           :refguide: something, else, and more\n\n        \"\"\"\n        def strip_each_in(lst):\n            return [s.strip() for s in lst]\n\n        out = {}\n        section = section.split('::')\n        if len(section) > 1:\n            out['default'] = strip_each_in(section[1].split(','))[0]\n        for line in content:\n            line = line.split(':')\n            if len(line) > 2:\n                out[line[1]] = strip_each_in(line[2].split(','))\n        return out\n\n    def _parse_summary(self):\n        \"\"\"Grab signature (if given) and summary\"\"\"\n        if self._is_at_section():\n            return\n\n        # If several signatures present, take the last one\n        while True:\n            summary = self._doc.read_to_next_empty_line()\n            summary_str = \" \".join([s.strip() for s in summary]).strip()\n            compiled = re.compile(r'^([\\w., ]+=)?\\s*[\\w\\.]+\\(.*\\)$')\n            if compiled.match(summary_str):\n                self['Signature'] = summary_str\n                if not self._is_at_section():\n                    continue\n            break\n\n        if summary is not None:\n            self['Summary'] = summary\n\n        if not self._is_at_section():\n            self['Extended Summary'] = self._read_to_next_section()\n\n    def _parse(self):\n        self._doc.reset()\n        self._parse_summary()\n\n        sections = list(self._read_sections())\n        section_names = set([section for section, content in sections])\n\n        has_returns = 'Returns' in section_names\n        has_yields = 'Yields' in section_names\n        # We could do more tests, but we are not. Arbitrarily.\n        if has_returns and has_yields:\n            msg = 'Docstring contains both a Returns and Yields section.'\n            raise ValueError(msg)\n        if not has_yields and 'Receives' in section_names:\n            msg = 'Docstring contains a Receives section but not Yields.'\n            raise ValueError(msg)\n\n        for (section, content) in sections:\n            if not section.startswith('..'):\n                section = (s.capitalize() for s in section.split(' '))\n                section = ' '.join(section)\n                if self.get(section):\n                    self._error_location(\"The section %s appears twice in  %s\"\n                                         % (section, '\\n'.join(self._doc._str)))\n\n            if section in ('Parameters', 'Other Parameters', 'Attributes',\n                           'Methods'):\n                self[section] = self._parse_param_list(content)\n            elif section in ('Returns', 'Yields', 'Raises', 'Warns', 'Receives'):\n                self[section] = self._parse_param_list(\n                    content, single_element_is_type=True)\n            elif section.startswith('.. index::'):\n                self['index'] = self._parse_index(section, content)\n            elif section == 'See Also':\n                self['See Also'] = self._parse_see_also(content)\n            else:\n                self[section] = content\n\n    @property\n    def _obj(self):\n        if hasattr(self, '_cls'):\n            return self._cls\n        elif hasattr(self, '_f'):\n            return self._f\n        return None\n\n    def _error_location(self, msg, error=True):\n        if self._obj is not None:\n            # we know where the docs came from:\n            try:\n                filename = inspect.getsourcefile(self._obj)\n            except TypeError:\n                filename = None\n            msg += f\" in the docstring of {self._obj.__name__}\"\n            msg += f\" in {filename}.\" if filename else \"\"\n        if error:\n            raise ValueError(msg)\n        else:\n            warn(msg)\n\n    # string conversion routines\n\n    def _str_header(self, name, symbol='-'):\n        return [name, len(name)*symbol]\n\n    def _str_indent(self, doc, indent=4):\n        return [' '*indent + line for line in doc]\n\n    def _str_signature(self):\n        if self['Signature']:\n            return [self['Signature'].replace('*', r'\\*')] + ['']\n        return ['']\n\n    def _str_summary(self):\n        if self['Summary']:\n            return self['Summary'] + ['']\n        return []\n\n    def _str_extended_summary(self):\n        if self['Extended Summary']:\n            return self['Extended Summary'] + ['']\n        return []\n\n    def _str_param_list(self, name):\n        out = []\n        if self[name]:\n            out += self._str_header(name)\n            for param in self[name]:\n                parts = []\n                if param.name:\n                    parts.append(param.name)\n                if param.type:\n                    parts.append(param.type)\n                out += [' : '.join(parts)]\n                if param.desc and ''.join(param.desc).strip():\n                    out += self._str_indent(param.desc)\n            out += ['']\n        return out\n\n    def _str_section(self, name):\n        out = []\n        if self[name]:\n            out += self._str_header(name)\n            out += self[name]\n            out += ['']\n        return out\n\n    def _str_see_also(self, func_role):\n        if not self['See Also']:\n            return []\n        out = []\n        out += self._str_header(\"See Also\")\n        out += ['']\n        last_had_desc = True\n        for funcs, desc in self['See Also']:\n            assert isinstance(funcs, list)\n            links = []\n            for func, role in funcs:\n                if role:\n                    link = ':%s:`%s`' % (role, func)\n                elif func_role:\n                    link = ':%s:`%s`' % (func_role, func)\n                else:\n                    link = \"`%s`_\" % func\n                links.append(link)\n            link = ', '.join(links)\n            out += [link]\n            if desc:\n                out += self._str_indent([' '.join(desc)])\n                last_had_desc = True\n            else:\n                last_had_desc = False\n                out += self._str_indent([self.empty_description])\n\n        if last_had_desc:\n            out += ['']\n        out += ['']\n        return out\n\n    def _str_index(self):\n        idx = self['index']\n        out = []\n        output_index = False\n        default_index = idx.get('default', '')\n        if default_index:\n            output_index = True\n        out += ['.. index:: %s' % default_index]\n        for section, references in idx.items():\n            if section == 'default':\n                continue\n            output_index = True\n            out += ['   :%s: %s' % (section, ', '.join(references))]\n        if output_index:\n            return out\n        return ''\n\n    def __str__(self, func_role=''):\n        out = []\n        out += self._str_signature()\n        out += self._str_summary()\n        out += self._str_extended_summary()\n        for param_list in ('Parameters', 'Returns', 'Yields', 'Receives',\n                           'Other Parameters', 'Raises', 'Warns'):\n            out += self._str_param_list(param_list)\n        out += self._str_section('Warnings')\n        out += self._str_see_also(func_role)\n        for s in ('Notes', 'References', 'Examples'):\n            out += self._str_section(s)\n        for param_list in ('Attributes', 'Methods'):\n            out += self._str_param_list(param_list)\n        out += self._str_index()\n        return '\\n'.join(out)\n\n\ndef dedent_lines(lines):\n    \"\"\"Deindent a list of lines maximally\"\"\"\n    return textwrap.dedent(\"\\n\".join(lines)).split(\"\\n\")\n\n\nclass FunctionDoc(NumpyDocString):\n    def __init__(self, func, role='func', doc=None, config=None):\n        self._f = func\n        self._role = role  # e.g. \"func\" or \"meth\"\n\n        if doc is None:\n            if func is None:\n                raise ValueError(\"No function or docstring given\")\n            doc = inspect.getdoc(func) or ''\n        if config is None:\n            config = {}\n        NumpyDocString.__init__(self, doc, config)\n\n    def get_func(self):\n        func_name = getattr(self._f, '__name__', self.__class__.__name__)\n        if inspect.isclass(self._f):\n            func = getattr(self._f, '__call__', self._f.__init__)\n        else:\n            func = self._f\n        return func, func_name\n\n    def __str__(self):\n        out = ''\n\n        func, func_name = self.get_func()\n\n        roles = {'func': 'function',\n                 'meth': 'method'}\n\n        if self._role:\n            if self._role not in roles:\n                print(\"Warning: invalid role %s\" % self._role)\n            out += '.. %s:: %s\\n    \\n\\n' % (roles.get(self._role, ''),\n                                             func_name)\n\n        out += super().__str__(func_role=self._role)\n        return out\n\n\nclass ObjDoc(NumpyDocString):\n    def __init__(self, obj, doc=None, config=None):\n        self._f = obj\n        if config is None:\n            config = {}\n        NumpyDocString.__init__(self, doc, config=config)\n\n\nclass ClassDoc(NumpyDocString):\n\n    extra_public_methods = ['__call__']\n\n    def __init__(self, cls, doc=None, modulename='', func_doc=FunctionDoc,\n                 config=None):\n        if not inspect.isclass(cls) and cls is not None:\n            raise ValueError(\"Expected a class or None, but got %r\" % cls)\n        self._cls = cls\n\n        if 'sphinx' in sys.modules:\n            from sphinx.ext.autodoc import ALL\n        else:\n            ALL = object()\n\n        if config is None:\n            config = {}\n        self.show_inherited_members = config.get(\n            'show_inherited_class_members', True)\n\n        if modulename and not modulename.endswith('.'):\n            modulename += '.'\n        self._mod = modulename\n\n        if doc is None:\n            if cls is None:\n                raise ValueError(\"No class or documentation string given\")\n            doc = pydoc.getdoc(cls)\n\n        NumpyDocString.__init__(self, doc)\n\n        _members = config.get('members', [])\n        if _members is ALL:\n            _members = None\n        _exclude = config.get('exclude-members', [])\n\n        if config.get('show_class_members', True) and _exclude is not ALL:\n            def splitlines_x(s):\n                if not s:\n                    return []\n                else:\n                    return s.splitlines()\n            for field, items in [('Methods', self.methods),\n                                 ('Attributes', self.properties)]:\n                if not self[field]:\n                    doc_list = []\n                    for name in sorted(items):\n                        if (name in _exclude or\n                                (_members and name not in _members)):\n                            continue\n                        try:\n                            doc_item = pydoc.getdoc(getattr(self._cls, name))\n                            doc_list.append(\n                                Parameter(name, '', splitlines_x(doc_item)))\n                        except AttributeError:\n                            pass  # method doesn't exist\n                    self[field] = doc_list\n\n    @property\n    def methods(self):\n        if self._cls is None:\n            return []\n        return [name for name, func in inspect.getmembers(self._cls)\n                if ((not name.startswith('_') or\n                     name in self.extra_public_methods) and\n                    isinstance(func, Callable) and\n                    self._is_show_member(name))]\n\n    @property\n    def properties(self):\n        if self._cls is None:\n            return []\n        return [name for name, func in inspect.getmembers(self._cls)\n                if (not name.startswith('_') and\n                    (func is None or isinstance(func, property) or\n                     inspect.isdatadescriptor(func)) and\n                    self._is_show_member(name))]\n\n    def _is_show_member(self, name):\n        if self.show_inherited_members:\n            return True  # show all class members\n        if name not in self._cls.__dict__:\n            return False  # class member is inherited, we do not show it\n        return True\n\n\ndef get_doc_object(obj, what=None, doc=None, config=None):\n    if what is None:\n        if inspect.isclass(obj):\n            what = 'class'\n        elif inspect.ismodule(obj):\n            what = 'module'\n        elif isinstance(obj, Callable):\n            what = 'function'\n        else:\n            what = 'object'\n    if config is None:\n        config = {}\n\n    if what == 'class':\n        return ClassDoc(obj, func_doc=FunctionDoc, doc=doc, config=config)\n    elif what in ('function', 'method'):\n        return FunctionDoc(obj, doc=doc, config=config)\n    else:\n        if doc is None:\n            doc = pydoc.getdoc(obj)\n        return ObjDoc(obj, doc, config=config)\n", "python/pyarrow/vendored/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "python/scripts/test_imports.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pyarrow as pa  # noqa\nimport sys\n\nassert 'pandas' not in sys.modules\n", "python/scripts/test_leak.py": "#!/usr/bin/env python\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pyarrow as pa\nimport numpy as np\nimport pandas as pd\nfrom pyarrow.tests.util import rands\nimport memory_profiler\nimport gc\nimport io\n\nMEGABYTE = 1 << 20\n\n\ndef assert_does_not_leak(f, iterations=10, check_interval=1, tolerance=5):\n    gc.collect()\n    baseline = memory_profiler.memory_usage()[0]\n    for i in range(iterations):\n        f()\n        if i % check_interval == 0:\n            gc.collect()\n            usage = memory_profiler.memory_usage()[0]\n            diff = usage - baseline\n            print(\"{0}: {1}\\r\".format(i, diff), end=\"\")\n            if diff > tolerance:\n                raise Exception(\"Memory increased by {0} megabytes after {1} \"\n                                \"iterations\".format(diff, i + 1))\n    gc.collect()\n    usage = memory_profiler.memory_usage()[0]\n    diff = usage - baseline\n    print(\"\\nMemory increased by {0} megabytes after {1} \"\n          \"iterations\".format(diff, iterations))\n\n\ndef test_leak1():\n    data = [pa.array(np.concatenate([np.random.randn(100000)] * 1000))]\n    table = pa.Table.from_arrays(data, ['foo'])\n\n    def func():\n        table.to_pandas()\n    assert_does_not_leak(func)\n\n\ndef test_leak2():\n    data = [pa.array(np.concatenate([np.random.randn(100000)] * 10))]\n    table = pa.Table.from_arrays(data, ['foo'])\n\n    def func():\n        df = table.to_pandas()\n\n        batch = pa.RecordBatch.from_pandas(df)\n\n        sink = io.BytesIO()\n        writer = pa.RecordBatchFileWriter(sink, batch.schema)\n        writer.write_batch(batch)\n        writer.close()\n\n        buf_reader = pa.BufferReader(sink.getvalue())\n        reader = pa.open_file(buf_reader)\n        reader.read_all()\n\n    assert_does_not_leak(func, iterations=50, tolerance=50)\n\n\ndef test_leak3():\n    import pyarrow.parquet as pq\n\n    df = pd.DataFrame({'a{0}'.format(i): [1, 2, 3, 4]\n                       for i in range(50)})\n    table = pa.Table.from_pandas(df, preserve_index=False)\n\n    writer = pq.ParquetWriter('leak_test_' + rands(5) + '.parquet',\n                              table.schema)\n\n    def func():\n        writer.write_table(table, row_group_size=len(table))\n\n    # This does not \"leak\" per se but we do want to have this use as little\n    # memory as possible\n    assert_does_not_leak(func, iterations=500,\n                         check_interval=50, tolerance=20)\n\n\ndef test_ARROW_8801():\n    x = pd.to_datetime(np.random.randint(0, 2**32, size=2**20, dtype=np.int64),\n                       unit='ms', utc=True)\n    table = pa.table(pd.DataFrame({'x': x}))\n\n    assert_does_not_leak(lambda: table.to_pandas(split_blocks=False),\n                         iterations=1000, check_interval=50, tolerance=1000)\n\n\nif __name__ == '__main__':\n    test_ARROW_8801()\n", "python/benchmarks/common.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport codecs\nimport decimal\nfrom functools import partial\nimport itertools\nimport sys\nimport unicodedata\n\nimport numpy as np\n\nimport pyarrow as pa\n\nKILOBYTE = 1 << 10\nMEGABYTE = KILOBYTE * KILOBYTE\n\nDEFAULT_NONE_PROB = 0.3\n\n\ndef _multiplicate_sequence(base, target_size):\n    q, r = divmod(target_size, len(base))\n    return [base] * q + [base[:r]]\n\n\ndef get_random_bytes(n, seed=42):\n    \"\"\"\n    Generate a random bytes object of size *n*.\n    Note the result might be compressible.\n    \"\"\"\n    rnd = np.random.RandomState(seed)\n    # Computing a huge random bytestring can be costly, so we get at most\n    # 100KB and duplicate the result as needed\n    base_size = 100003\n    q, r = divmod(n, base_size)\n    if q == 0:\n        result = rnd.bytes(r)\n    else:\n        base = rnd.bytes(base_size)\n        result = b''.join(_multiplicate_sequence(base, n))\n    assert len(result) == n\n    return result\n\n\ndef get_random_ascii(n, seed=42):\n    \"\"\"\n    Get a random ASCII-only unicode string of size *n*.\n    \"\"\"\n    arr = np.frombuffer(get_random_bytes(n, seed=seed), dtype=np.int8) & 0x7f\n    result, _ = codecs.ascii_decode(arr)\n    assert isinstance(result, str)\n    assert len(result) == n\n    return result\n\n\ndef _random_unicode_letters(n, seed=42):\n    \"\"\"\n    Generate a string of random unicode letters (slow).\n    \"\"\"\n    def _get_more_candidates():\n        return rnd.randint(0, sys.maxunicode, size=n).tolist()\n\n    rnd = np.random.RandomState(seed)\n    out = []\n    candidates = []\n\n    while len(out) < n:\n        if not candidates:\n            candidates = _get_more_candidates()\n        ch = chr(candidates.pop())\n        # XXX Do we actually care that the code points are valid?\n        if unicodedata.category(ch)[0] == 'L':\n            out.append(ch)\n    return out\n\n\n_1024_random_unicode_letters = _random_unicode_letters(1024)\n\n\ndef get_random_unicode(n, seed=42):\n    \"\"\"\n    Get a random non-ASCII unicode string of size *n*.\n    \"\"\"\n    indices = np.frombuffer(get_random_bytes(n * 2, seed=seed),\n                            dtype=np.int16) & 1023\n    unicode_arr = np.array(_1024_random_unicode_letters)[indices]\n\n    result = ''.join(unicode_arr.tolist())\n    assert len(result) == n, (len(result), len(unicode_arr))\n    return result\n\n\nclass BuiltinsGenerator(object):\n\n    def __init__(self, seed=42):\n        self.rnd = np.random.RandomState(seed)\n\n    def sprinkle(self, lst, prob, value):\n        \"\"\"\n        Sprinkle *value* entries in list *lst* with likelihood *prob*.\n        \"\"\"\n        for i, p in enumerate(self.rnd.random_sample(size=len(lst))):\n            if p < prob:\n                lst[i] = value\n\n    def sprinkle_nones(self, lst, prob):\n        \"\"\"\n        Sprinkle None entries in list *lst* with likelihood *prob*.\n        \"\"\"\n        self.sprinkle(lst, prob, None)\n\n    def generate_int_list(self, n, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of Python ints with *none_prob* probability of\n        an entry being None.\n        \"\"\"\n        data = list(range(n))\n        self.sprinkle_nones(data, none_prob)\n        return data\n\n    def generate_float_list(self, n, none_prob=DEFAULT_NONE_PROB,\n                            use_nan=False):\n        \"\"\"\n        Generate a list of Python floats with *none_prob* probability of\n        an entry being None (or NaN if *use_nan* is true).\n        \"\"\"\n        # Make sure we get Python floats, not np.float64\n        data = list(map(float, self.rnd.uniform(0.0, 1.0, n)))\n        assert len(data) == n\n        self.sprinkle(data, none_prob, value=float('nan') if use_nan else None)\n        return data\n\n    def generate_bool_list(self, n, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of Python bools with *none_prob* probability of\n        an entry being None.\n        \"\"\"\n        # Make sure we get Python bools, not np.bool_\n        data = [bool(x >= 0.5) for x in self.rnd.uniform(0.0, 1.0, n)]\n        assert len(data) == n\n        self.sprinkle_nones(data, none_prob)\n        return data\n\n    def generate_decimal_list(self, n, none_prob=DEFAULT_NONE_PROB,\n                              use_nan=False):\n        \"\"\"\n        Generate a list of Python Decimals with *none_prob* probability of\n        an entry being None (or NaN if *use_nan* is true).\n        \"\"\"\n        data = [decimal.Decimal('%.9f' % f)\n                for f in self.rnd.uniform(0.0, 1.0, n)]\n        assert len(data) == n\n        self.sprinkle(data, none_prob,\n                      value=decimal.Decimal('nan') if use_nan else None)\n        return data\n\n    def generate_object_list(self, n, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of generic Python objects with *none_prob*\n        probability of an entry being None.\n        \"\"\"\n        data = [object() for i in range(n)]\n        self.sprinkle_nones(data, none_prob)\n        return data\n\n    def _generate_varying_sequences(self, random_factory, n, min_size,\n                                    max_size, none_prob):\n        \"\"\"\n        Generate a list of *n* sequences of varying size between *min_size*\n        and *max_size*, with *none_prob* probability of an entry being None.\n        The base material for each sequence is obtained by calling\n        `random_factory(<some size>)`\n        \"\"\"\n        base_size = 10000\n        base = random_factory(base_size + max_size)\n        data = []\n        for i in range(n):\n            off = self.rnd.randint(base_size)\n            if min_size == max_size:\n                size = min_size\n            else:\n                size = self.rnd.randint(min_size, max_size + 1)\n            data.append(base[off:off + size])\n        self.sprinkle_nones(data, none_prob)\n        assert len(data) == n\n        return data\n\n    def generate_fixed_binary_list(self, n, size, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of bytestrings with a fixed *size*.\n        \"\"\"\n        return self._generate_varying_sequences(get_random_bytes, n,\n                                                size, size, none_prob)\n\n    def generate_varying_binary_list(self, n, min_size, max_size,\n                                     none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of bytestrings with a random size between\n        *min_size* and *max_size*.\n        \"\"\"\n        return self._generate_varying_sequences(get_random_bytes, n,\n                                                min_size, max_size, none_prob)\n\n    def generate_ascii_string_list(self, n, min_size, max_size,\n                                   none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of ASCII strings with a random size between\n        *min_size* and *max_size*.\n        \"\"\"\n        return self._generate_varying_sequences(get_random_ascii, n,\n                                                min_size, max_size, none_prob)\n\n    def generate_unicode_string_list(self, n, min_size, max_size,\n                                     none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of unicode strings with a random size between\n        *min_size* and *max_size*.\n        \"\"\"\n        return self._generate_varying_sequences(get_random_unicode, n,\n                                                min_size, max_size, none_prob)\n\n    def generate_int_list_list(self, n, min_size, max_size,\n                               none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of lists of Python ints with a random size between\n        *min_size* and *max_size*.\n        \"\"\"\n        return self._generate_varying_sequences(\n            partial(self.generate_int_list, none_prob=none_prob),\n            n, min_size, max_size, none_prob)\n\n    def generate_tuple_list(self, n, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of tuples with random values.\n        Each tuple has the form `(int value, float value, bool value)`\n        \"\"\"\n        dicts = self.generate_dict_list(n, none_prob=none_prob)\n        tuples = [(d.get('u'), d.get('v'), d.get('w'))\n                  if d is not None else None\n                  for d in dicts]\n        assert len(tuples) == n\n        return tuples\n\n    def generate_dict_list(self, n, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of dicts with random values.\n        Each dict has the form\n\n            `{'u': int value, 'v': float value, 'w': bool value}`\n        \"\"\"\n        ints = self.generate_int_list(n, none_prob=none_prob)\n        floats = self.generate_float_list(n, none_prob=none_prob)\n        bools = self.generate_bool_list(n, none_prob=none_prob)\n        dicts = []\n        # Keep half the Nones, omit the other half\n        keep_nones = itertools.cycle([True, False])\n        for u, v, w in zip(ints, floats, bools):\n            d = {}\n            if u is not None or next(keep_nones):\n                d['u'] = u\n            if v is not None or next(keep_nones):\n                d['v'] = v\n            if w is not None or next(keep_nones):\n                d['w'] = w\n            dicts.append(d)\n        self.sprinkle_nones(dicts, none_prob)\n        assert len(dicts) == n\n        return dicts\n\n    def get_type_and_builtins(self, n, type_name):\n        \"\"\"\n        Return a `(arrow type, list)` tuple where the arrow type\n        corresponds to the given logical *type_name*, and the list\n        is a list of *n* random-generated Python objects compatible\n        with the arrow type.\n        \"\"\"\n        size = None\n\n        if type_name in ('bool', 'decimal', 'ascii', 'unicode', 'int64 list'):\n            kind = type_name\n        elif type_name.startswith(('int', 'uint')):\n            kind = 'int'\n        elif type_name.startswith('float'):\n            kind = 'float'\n        elif type_name.startswith('struct'):\n            kind = 'struct'\n        elif type_name == 'binary':\n            kind = 'varying binary'\n        elif type_name.startswith('binary'):\n            kind = 'fixed binary'\n            size = int(type_name[6:])\n            assert size > 0\n        else:\n            raise ValueError(\"unrecognized type %r\" % (type_name,))\n\n        if kind in ('int', 'float'):\n            ty = getattr(pa, type_name)()\n        elif kind == 'bool':\n            ty = pa.bool_()\n        elif kind == 'decimal':\n            ty = pa.decimal128(9, 9)\n        elif kind == 'fixed binary':\n            ty = pa.binary(size)\n        elif kind == 'varying binary':\n            ty = pa.binary()\n        elif kind in ('ascii', 'unicode'):\n            ty = pa.string()\n        elif kind == 'int64 list':\n            ty = pa.list_(pa.int64())\n        elif kind == 'struct':\n            ty = pa.struct([pa.field('u', pa.int64()),\n                            pa.field('v', pa.float64()),\n                            pa.field('w', pa.bool_())])\n\n        factories = {\n            'int': self.generate_int_list,\n            'float': self.generate_float_list,\n            'bool': self.generate_bool_list,\n            'decimal': self.generate_decimal_list,\n            'fixed binary': partial(self.generate_fixed_binary_list,\n                                    size=size),\n            'varying binary': partial(self.generate_varying_binary_list,\n                                      min_size=3, max_size=40),\n            'ascii': partial(self.generate_ascii_string_list,\n                             min_size=3, max_size=40),\n            'unicode': partial(self.generate_unicode_string_list,\n                               min_size=3, max_size=40),\n            'int64 list': partial(self.generate_int_list_list,\n                                  min_size=0, max_size=20),\n            'struct': self.generate_dict_list,\n            'struct from tuples': self.generate_tuple_list,\n        }\n        data = factories[kind](n)\n        return ty, data\n", "python/benchmarks/array_ops.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pyarrow as pa\n\n\nclass ScalarAccess(object):\n    n = 10 ** 5\n\n    def setUp(self):\n        self._array = pa.array(list(range(self.n)), type=pa.int64())\n        self._array_items = list(self._array)\n\n    def time_getitem(self):\n        for i in range(self.n):\n            self._array[i]\n\n    def time_as_py(self):\n        for item in self._array_items:\n            item.as_py()\n", "python/benchmarks/microbenchmarks.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pyarrow.benchmark as pb\n\nfrom . import common\n\n\nclass PandasObjectIsNull(object):\n    size = 10 ** 5\n    types = ('int', 'float', 'object', 'decimal')\n\n    param_names = ['type']\n    params = [types]\n\n    def setup(self, type_name):\n        gen = common.BuiltinsGenerator()\n        if type_name == 'int':\n            lst = gen.generate_int_list(self.size)\n        elif type_name == 'float':\n            lst = gen.generate_float_list(self.size, use_nan=True)\n        elif type_name == 'object':\n            lst = gen.generate_object_list(self.size)\n        elif type_name == 'decimal':\n            lst = gen.generate_decimal_list(self.size)\n        else:\n            assert 0\n        self.lst = lst\n\n    def time_PandasObjectIsNull(self, *args):\n        pb.benchmark_PandasObjectIsNull(self.lst)\n", "python/benchmarks/convert_pandas.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport numpy as np\nimport pandas as pd\n\nimport pyarrow as pa\nfrom pyarrow.tests.util import rands\n\n\nclass PandasConversionsBase(object):\n    def setup(self, n, dtype):\n        if dtype == 'float64_nans':\n            arr = np.arange(n).astype('float64')\n            arr[arr % 10 == 0] = np.nan\n        else:\n            arr = np.arange(n).astype(dtype)\n        self.data = pd.DataFrame({'column': arr})\n\n\nclass PandasConversionsToArrow(PandasConversionsBase):\n    param_names = ('size', 'dtype')\n    params = ((10, 10 ** 6), ('int64', 'float64', 'float64_nans', 'str'))\n\n    def time_from_series(self, n, dtype):\n        pa.Table.from_pandas(self.data)\n\n\nclass PandasConversionsFromArrow(PandasConversionsBase):\n    param_names = ('size', 'dtype')\n    params = ((10, 10 ** 6), ('int64', 'float64', 'float64_nans', 'str'))\n\n    def setup(self, n, dtype):\n        super(PandasConversionsFromArrow, self).setup(n, dtype)\n        self.arrow_data = pa.Table.from_pandas(self.data)\n\n    def time_to_series(self, n, dtype):\n        self.arrow_data.to_pandas()\n\n\nclass ToPandasStrings(object):\n\n    param_names = ('uniqueness', 'total')\n    params = ((0.001, 0.01, 0.1, 0.5), (1000000,))\n    string_length = 25\n\n    def setup(self, uniqueness, total):\n        nunique = int(total * uniqueness)\n        unique_values = [rands(self.string_length) for i in range(nunique)]\n        values = unique_values * (total // nunique)\n        self.arr = pa.array(values, type=pa.string())\n        self.table = pa.Table.from_arrays([self.arr], ['f0'])\n\n    def time_to_pandas_dedup(self, *args):\n        self.arr.to_pandas()\n\n    def time_to_pandas_no_dedup(self, *args):\n        self.arr.to_pandas(deduplicate_objects=False)\n\n\nclass SerializeDeserializePandas(object):\n\n    def setup(self):\n        # 10 million length\n        n = 10000000\n        self.df = pd.DataFrame({'data': np.random.randn(n)})\n        self.serialized = pa.serialize_pandas(self.df)\n\n    def time_serialize_pandas(self):\n        pa.serialize_pandas(self.df)\n\n    def time_deserialize_pandas(self):\n        pa.deserialize_pandas(self.serialized)\n\n\nclass TableFromPandasMicroperformance(object):\n    # ARROW-4629\n\n    def setup(self):\n        ser = pd.Series(range(10000))\n        df = pd.DataFrame({col: ser.copy(deep=True) for col in range(100)})\n        # Simulate a real dataset by converting some columns to strings\n        self.df = df.astype({col: str for col in range(50)})\n\n    def time_Table_from_pandas(self):\n        for _ in range(50):\n            pa.Table.from_pandas(self.df, nthreads=1)\n", "python/benchmarks/parquet.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport numpy as np\n\nimport pyarrow as pa\ntry:\n    import pyarrow.parquet as pq\nexcept ImportError:\n    pq = None\nfrom pyarrow.tests.util import rands\n\n\nclass ParquetWriteBinary(object):\n\n    def setup(self):\n        nuniques = 100000\n        value_size = 50\n        length = 1000000\n        num_cols = 10\n\n        unique_values = np.array([rands(value_size) for\n                                  i in range(nuniques)], dtype='O')\n        values = unique_values[np.random.randint(0, nuniques, size=length)]\n        self.table = pa.table([pa.array(values) for i in range(num_cols)],\n                              names=['f{}'.format(i) for i in range(num_cols)])\n        self.table_df = self.table.to_pandas()\n\n    def time_write_binary_table(self):\n        out = pa.BufferOutputStream()\n        pq.write_table(self.table, out)\n\n    def time_write_binary_table_uncompressed(self):\n        out = pa.BufferOutputStream()\n        pq.write_table(self.table, out, compression='none')\n\n    def time_write_binary_table_no_dictionary(self):\n        out = pa.BufferOutputStream()\n        pq.write_table(self.table, out, use_dictionary=False)\n\n    def time_convert_pandas_and_write_binary_table(self):\n        out = pa.BufferOutputStream()\n        pq.write_table(pa.table(self.table_df), out)\n\n\ndef generate_dict_strings(string_size, nunique, length, random_order=True):\n    uniques = np.array([rands(string_size) for i in range(nunique)], dtype='O')\n    if random_order:\n        indices = np.random.randint(0, nunique, size=length).astype('i4')\n    else:\n        indices = np.arange(nunique).astype('i4').repeat(length // nunique)\n    return pa.DictionaryArray.from_arrays(indices, uniques)\n\n\ndef generate_dict_table(num_cols, string_size, nunique, length,\n                        random_order=True):\n    data = generate_dict_strings(string_size, nunique, length,\n                                 random_order=random_order)\n    return pa.table([\n        data for i in range(num_cols)\n    ], names=['f{}'.format(i) for i in range(num_cols)])\n\n\nclass ParquetWriteDictionaries(object):\n\n    param_names = ('nunique',)\n    params = [(1000), (100000)]\n\n    def setup(self, nunique):\n        self.num_cols = 10\n        self.value_size = 32\n        self.nunique = nunique\n        self.length = 10000000\n\n        self.table = generate_dict_table(self.num_cols, self.value_size,\n                                         self.nunique, self.length)\n        self.table_sequential = generate_dict_table(self.num_cols,\n                                                    self.value_size,\n                                                    self.nunique, self.length,\n                                                    random_order=False)\n\n    def time_write_random_order(self, nunique):\n        pq.write_table(self.table, pa.BufferOutputStream())\n\n    def time_write_sequential(self, nunique):\n        pq.write_table(self.table_sequential, pa.BufferOutputStream())\n\n\nclass ParquetManyColumns(object):\n\n    total_cells = 10000000\n    param_names = ('num_cols',)\n    params = [100, 1000, 10000]\n\n    def setup(self, num_cols):\n        num_rows = self.total_cells // num_cols\n        self.table = pa.table({'c' + str(i): np.random.randn(num_rows)\n                               for i in range(num_cols)})\n\n        out = pa.BufferOutputStream()\n        pq.write_table(self.table, out)\n        self.buf = out.getvalue()\n\n    def time_write(self, num_cols):\n        out = pa.BufferOutputStream()\n        pq.write_table(self.table, out)\n\n    def time_read(self, num_cols):\n        pq.read_table(self.buf)\n", "python/benchmarks/convert_builtins.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pyarrow as pa\n\nfrom . import common\n\n\n# TODO:\n# - test dates and times\n\n\nclass ConvertPyListToArray(object):\n    \"\"\"\n    Benchmark pa.array(list of values, type=...)\n    \"\"\"\n    size = 10 ** 5\n    types = ('int32', 'uint32', 'int64', 'uint64',\n             'float32', 'float64', 'bool', 'decimal',\n             'binary', 'binary10', 'ascii', 'unicode',\n             'int64 list', 'struct', 'struct from tuples')\n\n    param_names = ['type']\n    params = [types]\n\n    def setup(self, type_name):\n        gen = common.BuiltinsGenerator()\n        self.ty, self.data = gen.get_type_and_builtins(self.size, type_name)\n\n    def time_convert(self, *args):\n        pa.array(self.data, type=self.ty)\n\n\nclass InferPyListToArray(object):\n    \"\"\"\n    Benchmark pa.array(list of values) with type inference\n    \"\"\"\n    size = 10 ** 5\n    types = ('int64', 'float64', 'bool', 'decimal', 'binary', 'ascii',\n             'unicode', 'int64 list', 'struct')\n\n    param_names = ['type']\n    params = [types]\n\n    def setup(self, type_name):\n        gen = common.BuiltinsGenerator()\n        self.ty, self.data = gen.get_type_and_builtins(self.size, type_name)\n\n    def time_infer(self, *args):\n        arr = pa.array(self.data)\n        assert arr.type == self.ty\n\n\nclass ConvertArrayToPyList(object):\n    \"\"\"\n    Benchmark pa.array.to_pylist()\n    \"\"\"\n    size = 10 ** 5\n    types = ('int32', 'uint32', 'int64', 'uint64',\n             'float32', 'float64', 'bool', 'decimal',\n             'binary', 'binary10', 'ascii', 'unicode',\n             'int64 list', 'struct')\n\n    param_names = ['type']\n    params = [types]\n\n    def setup(self, type_name):\n        gen = common.BuiltinsGenerator()\n        self.ty, self.data = gen.get_type_and_builtins(self.size, type_name)\n        self.arr = pa.array(self.data, type=self.ty)\n\n    def time_convert(self, *args):\n        self.arr.to_pylist()\n", "python/benchmarks/io.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport time\nimport pyarrow as pa\n\n\nclass HighLatencyReader(object):\n\n    def __init__(self, raw, latency):\n        self.raw = raw\n        self.latency = latency\n\n    def close(self):\n        self.raw.close()\n\n    @property\n    def closed(self):\n        return self.raw.closed\n\n    def read(self, nbytes=None):\n        time.sleep(self.latency)\n        return self.raw.read(nbytes)\n\n\nclass HighLatencyWriter(object):\n\n    def __init__(self, raw, latency):\n        self.raw = raw\n        self.latency = latency\n\n    def close(self):\n        self.raw.close()\n\n    @property\n    def closed(self):\n        return self.raw.closed\n\n    def write(self, data):\n        time.sleep(self.latency)\n        self.raw.write(data)\n\n\nclass BufferedIOHighLatency(object):\n    \"\"\"Benchmark creating a parquet manifest.\"\"\"\n\n    increment = 1024\n    total_size = 16 * (1 << 20)  # 16 MB\n    buffer_size = 1 << 20  # 1 MB\n    latency = 0.1  # 100ms\n\n    param_names = ('latency',)\n    params = [0, 0.01, 0.1]\n\n    def time_buffered_writes(self, latency):\n        test_data = b'x' * self.increment\n        bytes_written = 0\n        out = pa.BufferOutputStream()\n        slow_out = HighLatencyWriter(out, latency)\n        buffered_out = pa.output_stream(slow_out, buffer_size=self.buffer_size)\n\n        while bytes_written < self.total_size:\n            buffered_out.write(test_data)\n            bytes_written += self.increment\n        buffered_out.flush()\n\n    def time_buffered_reads(self, latency):\n        bytes_read = 0\n        reader = pa.input_stream(pa.py_buffer(b'x' * self.total_size))\n        slow_reader = HighLatencyReader(reader, latency)\n        buffered_reader = pa.input_stream(slow_reader,\n                                          buffer_size=self.buffer_size)\n        while bytes_read < self.total_size:\n            buffered_reader.read(self.increment)\n            bytes_read += self.increment\n", "python/benchmarks/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "python/benchmarks/streaming.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n\nfrom . import common\nfrom .common import KILOBYTE, MEGABYTE\n\n\ndef generate_chunks(total_size, nchunks, ncols, dtype=np.dtype('int64')):\n    rowsize = total_size // nchunks // ncols\n    assert rowsize % dtype.itemsize == 0\n\n    def make_column(col, chunk):\n        return np.frombuffer(common.get_random_bytes(\n            rowsize, seed=col + 997 * chunk)).view(dtype)\n\n    return [pd.DataFrame({\n            'c' + str(col): make_column(col, chunk)\n            for col in range(ncols)})\n            for chunk in range(nchunks)]\n\n\nclass StreamReader(object):\n    \"\"\"\n    Benchmark in-memory streaming to a Pandas dataframe.\n    \"\"\"\n    total_size = 64 * MEGABYTE\n    ncols = 8\n    chunk_sizes = [16 * KILOBYTE, 256 * KILOBYTE, 8 * MEGABYTE]\n\n    param_names = ['chunk_size']\n    params = [chunk_sizes]\n\n    def setup(self, chunk_size):\n        # Note we're careful to stream different chunks instead of\n        # streaming N times the same chunk, so that we avoid operating\n        # entirely out of L1/L2.\n        chunks = generate_chunks(self.total_size,\n                                 nchunks=self.total_size // chunk_size,\n                                 ncols=self.ncols)\n        batches = [pa.RecordBatch.from_pandas(df)\n                   for df in chunks]\n        schema = batches[0].schema\n        sink = pa.BufferOutputStream()\n        stream_writer = pa.RecordBatchStreamWriter(sink, schema)\n        for batch in batches:\n            stream_writer.write_batch(batch)\n        self.source = sink.getvalue()\n\n    def time_read_to_dataframe(self, *args):\n        reader = pa.RecordBatchStreamReader(self.source)\n        table = reader.read_all()\n        df = table.to_pandas()  # noqa\n", "python/examples/flight/client.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"An example Flight CLI client.\"\"\"\n\nimport argparse\nimport sys\n\nimport pyarrow\nimport pyarrow.flight\nimport pyarrow.csv as csv\n\n\ndef list_flights(args, client, connection_args={}):\n    print('Flights\\n=======')\n    for flight in client.list_flights():\n        descriptor = flight.descriptor\n        if descriptor.descriptor_type == pyarrow.flight.DescriptorType.PATH:\n            print(\"Path:\", descriptor.path)\n        elif descriptor.descriptor_type == pyarrow.flight.DescriptorType.CMD:\n            print(\"Command:\", descriptor.command)\n        else:\n            print(\"Unknown descriptor type\")\n\n        print(\"Total records:\", end=\" \")\n        if flight.total_records >= 0:\n            print(flight.total_records)\n        else:\n            print(\"Unknown\")\n\n        print(\"Total bytes:\", end=\" \")\n        if flight.total_bytes >= 0:\n            print(flight.total_bytes)\n        else:\n            print(\"Unknown\")\n\n        print(\"Number of endpoints:\", len(flight.endpoints))\n        print(\"Schema:\")\n        print(flight.schema)\n        print('---')\n\n    print('\\nActions\\n=======')\n    for action in client.list_actions():\n        print(\"Type:\", action.type)\n        print(\"Description:\", action.description)\n        print('---')\n\n\ndef do_action(args, client, connection_args={}):\n    try:\n        buf = pyarrow.allocate_buffer(0)\n        action = pyarrow.flight.Action(args.action_type, buf)\n        print('Running action', args.action_type)\n        for result in client.do_action(action):\n            print(\"Got result\", result.body.to_pybytes())\n    except pyarrow.lib.ArrowIOError as e:\n        print(\"Error calling action:\", e)\n\n\ndef push_data(args, client, connection_args={}):\n    print('File Name:', args.file)\n    my_table = csv.read_csv(args.file)\n    print('Table rows=', str(len(my_table)))\n    df = my_table.to_pandas()\n    print(df.head())\n    writer, _ = client.do_put(\n        pyarrow.flight.FlightDescriptor.for_path(args.file), my_table.schema)\n    writer.write_table(my_table)\n    writer.close()\n\n\ndef get_flight(args, client, connection_args={}):\n    if args.path:\n        descriptor = pyarrow.flight.FlightDescriptor.for_path(*args.path)\n    else:\n        descriptor = pyarrow.flight.FlightDescriptor.for_command(args.command)\n\n    info = client.get_flight_info(descriptor)\n    for endpoint in info.endpoints:\n        print('Ticket:', endpoint.ticket)\n        for location in endpoint.locations:\n            print(location)\n            get_client = pyarrow.flight.FlightClient(location,\n                                                     **connection_args)\n            reader = get_client.do_get(endpoint.ticket)\n            df = reader.read_pandas()\n            print(df)\n\n\ndef _add_common_arguments(parser):\n    parser.add_argument('--tls', action='store_true',\n                        help='Enable transport-level security')\n    parser.add_argument('--tls-roots', default=None,\n                        help='Path to trusted TLS certificate(s)')\n    parser.add_argument(\"--mtls\", nargs=2, default=None,\n                        metavar=('CERTFILE', 'KEYFILE'),\n                        help=\"Enable transport-level security\")\n    parser.add_argument('host', type=str,\n                        help=\"Address or hostname to connect to\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    subcommands = parser.add_subparsers()\n\n    cmd_list = subcommands.add_parser('list')\n    cmd_list.set_defaults(action='list')\n    _add_common_arguments(cmd_list)\n    cmd_list.add_argument('-l', '--list', action='store_true',\n                          help=\"Print more details.\")\n\n    cmd_do = subcommands.add_parser('do')\n    cmd_do.set_defaults(action='do')\n    _add_common_arguments(cmd_do)\n    cmd_do.add_argument('action_type', type=str,\n                        help=\"The action type to run.\")\n\n    cmd_put = subcommands.add_parser('put')\n    cmd_put.set_defaults(action='put')\n    _add_common_arguments(cmd_put)\n    cmd_put.add_argument('file', type=str,\n                         help=\"CSV file to upload.\")\n\n    cmd_get = subcommands.add_parser('get')\n    cmd_get.set_defaults(action='get')\n    _add_common_arguments(cmd_get)\n    cmd_get_descriptor = cmd_get.add_mutually_exclusive_group(required=True)\n    cmd_get_descriptor.add_argument('-p', '--path', type=str, action='append',\n                                    help=\"The path for the descriptor.\")\n    cmd_get_descriptor.add_argument('-c', '--command', type=str,\n                                    help=\"The command for the descriptor.\")\n\n    args = parser.parse_args()\n    if not hasattr(args, 'action'):\n        parser.print_help()\n        sys.exit(1)\n\n    commands = {\n        'list': list_flights,\n        'do': do_action,\n        'get': get_flight,\n        'put': push_data,\n    }\n    host, port = args.host.split(':')\n    port = int(port)\n    scheme = \"grpc+tcp\"\n    connection_args = {}\n    if args.tls:\n        scheme = \"grpc+tls\"\n        if args.tls_roots:\n            with open(args.tls_roots, \"rb\") as root_certs:\n                connection_args[\"tls_root_certs\"] = root_certs.read()\n    if args.mtls:\n        with open(args.mtls[0], \"rb\") as cert_file:\n            tls_cert_chain = cert_file.read()\n        with open(args.mtls[1], \"rb\") as key_file:\n            tls_private_key = key_file.read()\n        connection_args[\"cert_chain\"] = tls_cert_chain\n        connection_args[\"private_key\"] = tls_private_key\n    client = pyarrow.flight.FlightClient(f\"{scheme}://{host}:{port}\",\n                                         **connection_args)\n    while True:\n        try:\n            action = pyarrow.flight.Action(\"healthcheck\", b\"\")\n            options = pyarrow.flight.FlightCallOptions(timeout=1)\n            list(client.do_action(action, options=options))\n            break\n        except pyarrow.ArrowIOError as e:\n            if \"Deadline\" in str(e):\n                print(\"Server is not ready, waiting...\")\n    commands[args.action](args, client, connection_args)\n\n\nif __name__ == '__main__':\n    main()\n", "python/examples/flight/middleware.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"Example of invisibly propagating a request ID with middleware.\"\"\"\n\nimport argparse\nimport sys\nimport threading\nimport uuid\n\nimport pyarrow as pa\nimport pyarrow.flight as flight\n\n\nclass TraceContext:\n    _locals = threading.local()\n    _locals.trace_id = None\n\n    @classmethod\n    def current_trace_id(cls):\n        if not getattr(cls._locals, \"trace_id\", None):\n            cls.set_trace_id(uuid.uuid4().hex)\n        return cls._locals.trace_id\n\n    @classmethod\n    def set_trace_id(cls, trace_id):\n        cls._locals.trace_id = trace_id\n\n\nTRACE_HEADER = \"x-tracing-id\"\n\n\nclass TracingServerMiddleware(flight.ServerMiddleware):\n    def __init__(self, trace_id):\n        self.trace_id = trace_id\n\n    def sending_headers(self):\n        return {\n            TRACE_HEADER: self.trace_id,\n        }\n\n\nclass TracingServerMiddlewareFactory(flight.ServerMiddlewareFactory):\n    def start_call(self, info, headers):\n        print(\"Starting new call:\", info)\n        if TRACE_HEADER in headers:\n            trace_id = headers[TRACE_HEADER][0]\n            print(\"Found trace header with value:\", trace_id)\n            TraceContext.set_trace_id(trace_id)\n        return TracingServerMiddleware(TraceContext.current_trace_id())\n\n\nclass TracingClientMiddleware(flight.ClientMiddleware):\n    def sending_headers(self):\n        print(\"Sending trace ID:\", TraceContext.current_trace_id())\n        return {\n            \"x-tracing-id\": TraceContext.current_trace_id(),\n        }\n\n    def received_headers(self, headers):\n        if TRACE_HEADER in headers:\n            trace_id = headers[TRACE_HEADER][0]\n            print(\"Found trace header with value:\", trace_id)\n            # Don't overwrite our trace ID\n\n\nclass TracingClientMiddlewareFactory(flight.ClientMiddlewareFactory):\n    def start_call(self, info):\n        print(\"Starting new call:\", info)\n        return TracingClientMiddleware()\n\n\nclass FlightServer(flight.FlightServerBase):\n    def __init__(self, delegate, **kwargs):\n        super().__init__(**kwargs)\n        if delegate:\n            self.delegate = flight.connect(\n                delegate,\n                middleware=(TracingClientMiddlewareFactory(),))\n        else:\n            self.delegate = None\n\n    def list_actions(self, context):\n        return [\n            (\"get-trace-id\", \"Get the trace context ID.\"),\n        ]\n\n    def do_action(self, context, action):\n        trace_middleware = context.get_middleware(\"trace\")\n        if trace_middleware:\n            TraceContext.set_trace_id(trace_middleware.trace_id)\n        if action.type == \"get-trace-id\":\n            if self.delegate:\n                for result in self.delegate.do_action(action):\n                    yield result\n            else:\n                trace_id = TraceContext.current_trace_id().encode(\"utf-8\")\n                print(\"Returning trace ID:\", trace_id)\n                buf = pa.py_buffer(trace_id)\n                yield pa.flight.Result(buf)\n        else:\n            raise KeyError(f\"Unknown action {action.type!r}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    subparsers = parser.add_subparsers(dest=\"command\")\n    client = subparsers.add_parser(\"client\", help=\"Run the client.\")\n    client.add_argument(\"server\")\n    client.add_argument(\"--request-id\", default=None)\n\n    server = subparsers.add_parser(\"server\", help=\"Run the server.\")\n    server.add_argument(\n        \"--listen\",\n        required=True,\n        help=\"The location to listen on (example: grpc://localhost:5050)\",\n    )\n    server.add_argument(\n        \"--delegate\",\n        required=False,\n        default=None,\n        help=(\"A location to delegate to. That is, this server will \"\n              \"simply call the given server for the response. Demonstrates \"\n              \"propagation of the trace ID between servers.\"),\n    )\n\n    args = parser.parse_args()\n    if not getattr(args, \"command\"):\n        parser.print_help()\n        return 1\n\n    if args.command == \"server\":\n        server = FlightServer(\n            args.delegate,\n            location=args.listen,\n            middleware={\"trace\": TracingServerMiddlewareFactory()})\n        server.serve()\n    elif args.command == \"client\":\n        client = flight.connect(\n            args.server,\n            middleware=(TracingClientMiddlewareFactory(),))\n        if args.request_id:\n            TraceContext.set_trace_id(args.request_id)\n        else:\n            TraceContext.set_trace_id(\"client-chosen-id\")\n\n        for result in client.do_action(flight.Action(\"get-trace-id\", b\"\")):\n            print(result.body.to_pybytes())\n\n\nif __name__ == \"__main__\":\n    sys.exit(main() or 0)\n", "python/examples/flight/server.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"An example Flight Python server.\"\"\"\n\nimport argparse\nimport ast\nimport threading\nimport time\n\nimport pyarrow\nimport pyarrow.flight\n\n\nclass FlightServer(pyarrow.flight.FlightServerBase):\n    def __init__(self, host=\"localhost\", location=None,\n                 tls_certificates=None, verify_client=False,\n                 root_certificates=None, auth_handler=None):\n        super(FlightServer, self).__init__(\n            location, auth_handler, tls_certificates, verify_client,\n            root_certificates)\n        self.flights = {}\n        self.host = host\n        self.tls_certificates = tls_certificates\n\n    @classmethod\n    def descriptor_to_key(self, descriptor):\n        return (descriptor.descriptor_type.value, descriptor.command,\n                tuple(descriptor.path or tuple()))\n\n    def _make_flight_info(self, key, descriptor, table):\n        if self.tls_certificates:\n            location = pyarrow.flight.Location.for_grpc_tls(\n                self.host, self.port)\n        else:\n            location = pyarrow.flight.Location.for_grpc_tcp(\n                self.host, self.port)\n        endpoints = [pyarrow.flight.FlightEndpoint(repr(key), [location]), ]\n\n        mock_sink = pyarrow.MockOutputStream()\n        stream_writer = pyarrow.RecordBatchStreamWriter(\n            mock_sink, table.schema)\n        stream_writer.write_table(table)\n        stream_writer.close()\n        data_size = mock_sink.size()\n\n        return pyarrow.flight.FlightInfo(table.schema,\n                                         descriptor, endpoints,\n                                         table.num_rows, data_size)\n\n    def list_flights(self, context, criteria):\n        for key, table in self.flights.items():\n            if key[1] is not None:\n                descriptor = \\\n                    pyarrow.flight.FlightDescriptor.for_command(key[1])\n            else:\n                descriptor = pyarrow.flight.FlightDescriptor.for_path(*key[2])\n\n            yield self._make_flight_info(key, descriptor, table)\n\n    def get_flight_info(self, context, descriptor):\n        key = FlightServer.descriptor_to_key(descriptor)\n        if key in self.flights:\n            table = self.flights[key]\n            return self._make_flight_info(key, descriptor, table)\n        raise KeyError('Flight not found.')\n\n    def do_put(self, context, descriptor, reader, writer):\n        key = FlightServer.descriptor_to_key(descriptor)\n        print(key)\n        self.flights[key] = reader.read_all()\n        print(self.flights[key])\n\n    def do_get(self, context, ticket):\n        key = ast.literal_eval(ticket.ticket.decode())\n        if key not in self.flights:\n            return None\n        return pyarrow.flight.RecordBatchStream(self.flights[key])\n\n    def list_actions(self, context):\n        return [\n            (\"clear\", \"Clear the stored flights.\"),\n            (\"shutdown\", \"Shut down this server.\"),\n        ]\n\n    def do_action(self, context, action):\n        if action.type == \"clear\":\n            raise NotImplementedError(\n                \"{} is not implemented.\".format(action.type))\n        elif action.type == \"healthcheck\":\n            pass\n        elif action.type == \"shutdown\":\n            yield pyarrow.flight.Result(pyarrow.py_buffer(b'Shutdown!'))\n            # Shut down on background thread to avoid blocking current\n            # request\n            threading.Thread(target=self._shutdown).start()\n        else:\n            raise KeyError(\"Unknown action {!r}\".format(action.type))\n\n    def _shutdown(self):\n        \"\"\"Shut down after a delay.\"\"\"\n        print(\"Server is shutting down...\")\n        time.sleep(2)\n        self.shutdown()\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\",\n                        help=\"Address or hostname to listen on\")\n    parser.add_argument(\"--port\", type=int, default=5005,\n                        help=\"Port number to listen on\")\n    parser.add_argument(\"--tls\", nargs=2, default=None,\n                        metavar=('CERTFILE', 'KEYFILE'),\n                        help=\"Enable transport-level security\")\n    parser.add_argument(\"--verify_client\", type=bool, default=False,\n                        help=\"enable mutual TLS and verify the client if True\")\n\n    args = parser.parse_args()\n    tls_certificates = []\n    scheme = \"grpc+tcp\"\n    if args.tls:\n        scheme = \"grpc+tls\"\n        with open(args.tls[0], \"rb\") as cert_file:\n            tls_cert_chain = cert_file.read()\n        with open(args.tls[1], \"rb\") as key_file:\n            tls_private_key = key_file.read()\n        tls_certificates.append((tls_cert_chain, tls_private_key))\n\n    location = \"{}://{}:{}\".format(scheme, args.host, args.port)\n\n    server = FlightServer(args.host, location,\n                          tls_certificates=tls_certificates,\n                          verify_client=args.verify_client)\n    print(\"Serving on\", location)\n    server.serve()\n\n\nif __name__ == '__main__':\n    main()\n", "python/examples/parquet_encryption/sample_vault_kms_client.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"A sample KmsClient implementation.\"\"\"\nimport argparse\nimport base64\nimport os\n\nimport requests\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport pyarrow.parquet.encryption as pe\n\n\nclass VaultClient(pe.KmsClient):\n    \"\"\"An example of a KmsClient implementation with master keys\n    managed by Hashicorp Vault KMS.\n    See Vault documentation: https://www.vaultproject.io/api/secret/transit\n    Not for production use!\n    \"\"\"\n    JSON_MEDIA_TYPE = \"application/json; charset=utf-8\"\n    DEFAULT_TRANSIT_ENGINE = \"/v1/transit/\"\n    WRAP_ENDPOINT = \"encrypt/\"\n    UNWRAP_ENDPOINT = \"decrypt/\"\n    TOKEN_HEADER = \"X-Vault-Token\"\n\n    def __init__(self, kms_connection_config):\n        \"\"\"Create a VaultClient instance.\n\n        Parameters\n        ----------\n        kms_connection_config : KmsConnectionConfig\n           configuration parameters to connect to vault,\n           e.g. URL and access token\n        \"\"\"\n        pe.KmsClient.__init__(self)\n        self.kms_url = kms_connection_config.kms_instance_url + \\\n            VaultClient.DEFAULT_TRANSIT_ENGINE\n        self.kms_connection_config = kms_connection_config\n\n    def wrap_key(self, key_bytes, master_key_identifier):\n        \"\"\"Call Vault to wrap key key_bytes with key\n        identified by master_key_identifier.\"\"\"\n        endpoint = self.kms_url + VaultClient.WRAP_ENDPOINT\n        headers = {VaultClient.TOKEN_HEADER:\n                   self.kms_connection_config.key_access_token}\n        r = requests.post(endpoint + master_key_identifier,\n                          headers=headers,\n                          data={'plaintext': base64.b64encode(key_bytes)})\n        r.raise_for_status()\n        r_dict = r.json()\n        wrapped_key = r_dict['data']['ciphertext']\n        return wrapped_key\n\n    def unwrap_key(self, wrapped_key, master_key_identifier):\n        \"\"\"Call Vault to unwrap wrapped_key with key\n        identified by master_key_identifier\"\"\"\n        endpoint = self.kms_url + VaultClient.UNWRAP_ENDPOINT\n        headers = {VaultClient.TOKEN_HEADER:\n                   self.kms_connection_config.key_access_token}\n        r = requests.post(endpoint + master_key_identifier,\n                          headers=headers,\n                          data={'ciphertext': wrapped_key})\n        r.raise_for_status()\n        r_dict = r.json()\n        plaintext = r_dict['data']['plaintext']\n        key_bytes = base64.b64decode(plaintext)\n        return key_bytes\n\n\ndef parquet_write_read_with_vault(parquet_filename):\n    \"\"\"An example for writing an encrypted parquet and reading an\n    encrypted parquet using master keys managed by Hashicorp Vault KMS.\n    Note that for this implementation requests dependency is needed\n    and environment properties VAULT_URL and VAULT_TOKEN should be set.\n    Please enable the transit engine.\n    \"\"\"\n    path = parquet_filename\n\n    table = pa.Table.from_pydict({\n        'a': pa.array([1, 2, 3]),\n        'b': pa.array(['a', 'b', 'c']),\n        'c': pa.array(['x', 'y', 'z'])\n    })\n\n    # Encrypt the footer with the footer key,\n    # encrypt column `a` with one key\n    # and column `b` with another key,\n    # keep `c` plaintext\n    footer_key_name = \"footer_key\"\n    col_a_key_name = \"col_a_key\"\n    col_b_key_name = \"col_b_key\"\n\n    encryption_config = pe.EncryptionConfiguration(\n        footer_key=footer_key_name,\n        column_keys={\n            col_a_key_name: [\"a\"],\n            col_b_key_name: [\"b\"],\n        })\n\n    kms_connection_config = pe.KmsConnectionConfig(\n        kms_instance_url=os.environ.get('VAULT_URL', ''),\n        key_access_token=os.environ.get('VAULT_TOKEN', ''),\n    )\n\n    def kms_factory(kms_connection_configuration):\n        return VaultClient(kms_connection_configuration)\n\n    # Write with encryption properties\n    crypto_factory = pe.CryptoFactory(kms_factory)\n    file_encryption_properties = crypto_factory.file_encryption_properties(\n        kms_connection_config, encryption_config)\n    with pq.ParquetWriter(path,\n                          table.schema,\n                          encryption_properties=file_encryption_properties) \\\n            as writer:\n        writer.write_table(table)\n\n    # Read with decryption properties\n    file_decryption_properties = crypto_factory.file_decryption_properties(\n        kms_connection_config)\n    result = pq.ParquetFile(\n        path, decryption_properties=file_decryption_properties)\n    result_table = result.read()\n    assert table.equals(result_table)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Write and read an encrypted parquet using master keys \"\n        \"managed by Hashicorp Vault.\\nBefore using please enable the \"\n        \"transit engine in Vault and set VAULT_URL and VAULT_TOKEN \"\n        \"environment variables.\")\n    parser.add_argument('--filename', dest='filename', type=str,\n                        default='/tmp/encrypted_table.vault.parquet',\n                        help='Filename of the parquet file to be created '\n                             '(default: /tmp/encrypted_table.vault.parquet')\n    args = parser.parse_args()\n    filename = args.filename\n    parquet_write_read_with_vault(filename)\n\n\nif __name__ == '__main__':\n    main()\n", "python/examples/dataset/write_dataset_encrypted.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport shutil\nimport os\nfrom datetime import timedelta\n\nimport pyarrow as pa\nimport pyarrow.dataset as ds\nimport pyarrow.parquet.encryption as pe\nfrom pyarrow.tests.parquet.encryption import InMemoryKmsClient\n\n\"\"\" A sample to demonstrate parquet dataset encryption and decryption\"\"\"\n\n# create a list of dictionaries that will represent our dataset\ntable = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n                  'n_legs': [2, 2, 4, 4, 5, 100],\n                  'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n                             \"Brittle stars\", \"Centipede\"]})\n\n# create a PyArrow dataset from the table\ndataset = ds.dataset(table)\n\nFOOTER_KEY = b\"0123456789112345\"\nFOOTER_KEY_NAME = \"footer_key\"\nCOL_KEY = b\"1234567890123450\"\nCOL_KEY_NAME = \"col_key\"\n\nencryption_config = pe.EncryptionConfiguration(\n    footer_key=FOOTER_KEY_NAME,\n    plaintext_footer=False,\n    # Use COL_KEY_NAME to encrypt `n_legs` and `animal` columns.\n    column_keys={\n        COL_KEY_NAME: [\"n_legs\", \"animal\"],\n    },\n    encryption_algorithm=\"AES_GCM_V1\",\n    # requires timedelta or an assertion is raised\n    cache_lifetime=timedelta(minutes=5.0),\n    data_key_length_bits=256)\n\nkms_connection_config = pe.KmsConnectionConfig(\n    custom_kms_conf={\n        FOOTER_KEY_NAME: FOOTER_KEY.decode(\"UTF-8\"),\n        COL_KEY_NAME: COL_KEY.decode(\"UTF-8\"),\n    }\n)\n\ndecryption_config = pe.DecryptionConfiguration(cache_lifetime=300)\n\n\ndef kms_factory(kms_connection_configuration):\n    return InMemoryKmsClient(kms_connection_configuration)\n\n\ncrypto_factory = pe.CryptoFactory(kms_factory)\nparquet_encryption_cfg = ds.ParquetEncryptionConfig(\n    crypto_factory, kms_connection_config, encryption_config)\nparquet_decryption_cfg = ds.ParquetDecryptionConfig(crypto_factory,\n                                                    kms_connection_config,\n                                                    decryption_config)\n\n# set encryption config for parquet fragment scan options\npq_scan_opts = ds.ParquetFragmentScanOptions()\npq_scan_opts.parquet_decryption_config = parquet_decryption_cfg\npformat = pa.dataset.ParquetFileFormat(default_fragment_scan_options=pq_scan_opts)\n\nif os.path.exists('sample_dataset'):\n    shutil.rmtree('sample_dataset')\n\nwrite_options = pformat.make_write_options(\n    encryption_config=parquet_encryption_cfg)\n\nds.write_dataset(data=dataset, base_dir=\"sample_dataset\",\n                 partitioning=['year'], format=pformat, file_options=write_options)\n# read the dataset back\ndataset = ds.dataset('sample_dataset', format=pformat)\n\n# print the dataset\nprint(dataset.to_table())\n", "csharp/test/Apache.Arrow.Compression.Tests/generate_resources.py": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements. See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nGenerates files required for tests where it's not possible to generate\nthem with dotnet Arrow.\n\"\"\"\n\nimport pyarrow as pa\nfrom pathlib import Path\n\n\ndef write_compressed_ipc_file(path: Path, compression: str, stream: bool = False):\n    schema = pa.schema([\n        pa.field('integers', pa.int32()),\n        pa.field('floats', pa.float32()),\n    ])\n\n    integers = pa.array(range(100), type=pa.int32())\n    floats = pa.array((x * 0.1 for x in range(100)), type=pa.float32())\n    batch = pa.record_batch([integers, floats], schema)\n\n    options = pa.ipc.IpcWriteOptions(compression=compression)\n\n    with pa.OSFile(path.as_posix(), 'wb') as sink:\n        if stream:\n            with pa.ipc.new_stream(sink, schema, options=options) as writer:\n                writer.write(batch)\n        else:\n            with pa.ipc.new_file(sink, schema, options=options) as writer:\n                writer.write(batch)\n\n\nif __name__ == '__main__':\n    resource_dir = Path(__file__).resolve().parent / 'Resources'\n\n    write_compressed_ipc_file(resource_dir / 'ipc_lz4_compression.arrow', 'lz4')\n    write_compressed_ipc_file(resource_dir / 'ipc_lz4_compression.arrow_stream', 'lz4', stream=True)\n    write_compressed_ipc_file(resource_dir / 'ipc_zstd_compression.arrow', 'zstd')\n    write_compressed_ipc_file(resource_dir / 'ipc_zstd_compression.arrow_stream', 'zstd', stream=True)\n", "docs/source/conf.py": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n\nimport datetime\nimport os\nimport sys\nimport warnings\nfrom unittest import mock\nfrom docutils.parsers.rst import Directive, directives\n\nsys.path.extend([\n    os.path.join(os.path.dirname(__file__),\n                 '..', '../..')\n\n])\n\n# -- Customization --------------------------------------------------------\n\ntry:\n    import pyarrow\n    exclude_patterns = []\n    pyarrow_version =  pyarrow.__version__\n\n    # Conditional API doc generation\n\n    # Sphinx has two features for conditional inclusion:\n    # - The \"only\" directive\n    #   https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#including-content-based-on-tags\n    # - The \"ifconfig\" extension\n    #   https://www.sphinx-doc.org/en/master/usage/extensions/ifconfig.html\n    #\n    # Both have issues, but \"ifconfig\" seems to work in this setting.\n\n    try:\n        import pyarrow.cuda\n        cuda_enabled = True\n    except ImportError:\n        cuda_enabled = False\n        # Mock pyarrow.cuda to avoid autodoc warnings.\n        # XXX I can't get autodoc_mock_imports to work, so mock manually instead\n        # (https://github.com/sphinx-doc/sphinx/issues/2174#issuecomment-453177550)\n        pyarrow.cuda = sys.modules['pyarrow.cuda'] = mock.Mock()\n\n    try:\n        import pyarrow.flight\n        flight_enabled = True\n    except ImportError:\n        flight_enabled = False\n        pyarrow.flight = sys.modules['pyarrow.flight'] = mock.Mock()\n\n    try:\n        import pyarrow.orc\n        orc_enabled = True\n    except ImportError:\n        orc_enabled = False\n        pyarrow.orc = sys.modules['pyarrow.orc'] = mock.Mock()\n\n    try:\n        import pyarrow.parquet.encryption\n        parquet_encryption_enabled = True\n    except ImportError:\n        parquet_encryption_enabled = False\n        pyarrow.parquet.encryption = sys.modules['pyarrow.parquet.encryption'] = mock.Mock()\nexcept (ImportError, LookupError):\n    exclude_patterns = ['python']\n    pyarrow_version =  \"\"\n    cuda_enabled = False\n    flight_enabled = False\n\n# Suppresses all warnings printed when sphinx is traversing the code (e.g.\n# deprecation warnings)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*pyarrow.*\")\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'breathe',\n    'IPython.sphinxext.ipython_console_highlighting',\n    'IPython.sphinxext.ipython_directive',\n    'myst_parser',\n    'numpydoc',\n    'sphinx_design',\n    'sphinx_copybutton',\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.doctest',\n    'sphinx.ext.ifconfig',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.viewcode',\n    'sphinxcontrib.mermaid',\n]\n\n# Show members for classes in .. autosummary\nautodoc_default_options = {\n    'members': None,\n    'special-members': '__dataframe__',\n    'undoc-members': None,\n    'show-inheritance': None,\n    'inherited-members': None\n}\n\n# Breathe configuration\nbreathe_projects = {\n    \"arrow_cpp\": os.environ.get(\"ARROW_CPP_DOXYGEN_XML\", \"../../cpp/apidoc/xml\"),\n}\nbreathe_default_project = \"arrow_cpp\"\n\n# Overridden conditionally below\nautodoc_mock_imports = []\n\n# copybutton configuration\ncopybutton_prompt_text = r\">>> |\\.\\.\\. |\\$ |In \\[\\d*\\]: | {2,5}\\.\\.\\.: \"\ncopybutton_prompt_is_regexp = True\ncopybutton_line_continuation_character = \"\\\\\"\n\n# ipython directive options\nipython_mplbackend = ''\n\n# MyST-Parser configuration\nmyst_enable_extensions = [\n    'amsmath',\n    'attrs_inline',\n    # 'colon_fence',\n    'deflist',\n    'dollarmath',\n    'fieldlist',\n    'html_admonition',\n    'html_image',\n    'linkify',\n    # 'replacements',\n    # 'smartquotes',\n    'strikethrough',\n    'substitution',\n    'tasklist',\n]\n\n# numpydoc configuration\nnumpydoc_xref_param_type = True\nnumpydoc_show_class_members = False\nnumpydoc_xref_ignore = {\n    \"or\", \"and\", \"of\", \"if\", \"default\", \"optional\", \"object\",\n    \"dicts\", \"rows\", \"Python\", \"source\", \"filesystem\",\n    \"dataset\", \"datasets\",\n    # TODO those one could be linked to a glossary or python docs?\n    \"file\", \"path\", \"paths\", \"mapping\", \"Mapping\", \"URI\", \"function\",\n    \"iterator\", \"Iterator\",\n    # TODO this term is used regularly, but isn't actually exposed (base class)\n    \"RecordBatchReader\",\n    # additional ignores that could be fixed by rewriting the docstrings\n    \"other\", \"supporting\", \"buffer\", \"protocol\",  # from Codec / pa.compress\n    \"depends\", \"on\", \"inputs\",  # pyarrow.compute\n    \"values\", \"coercible\", \"to\", \"arrays\",  # pa.chunked_array, Table methods\n    \"depending\",  # to_pandas\n}\nnumpydoc_xref_aliases = {\n    \"array-like\": \":func:`array-like <pyarrow.array>`\",\n    \"Array\": \"pyarrow.Array\",\n    \"Schema\": \"pyarrow.Schema\",\n    \"RecordBatch\": \"pyarrow.RecordBatch\",\n    \"Table\": \"pyarrow.Table\",\n    \"MemoryPool\": \"pyarrow.MemoryPool\",\n    \"NativeFile\": \"pyarrow.NativeFile\",\n    \"FileSystem\": \"pyarrow.fs.FileSystem\",\n    \"FileType\": \"pyarrow.fs.FileType\",\n}\n\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n\nsource_suffix = {\n    # We need to keep \"'.rst': 'restructuredtext'\" as the first item.\n    # This is a workaround of\n    # https://github.com/sphinx-doc/sphinx/issues/12147 .\n    #\n    # We can sort these items in alphabetical order with Sphinx 7.3.0\n    # or later that will include the fix of this problem.\n    '.rst': 'restructuredtext',\n    '.md': 'markdown',\n}\n\nautosummary_generate = True\n\n# The encoding of source files.\n#\n# source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'Apache Arrow'\ncopyright = (\n    f\"2016-{datetime.datetime.now().year} Apache Software Foundation.\\n\"\n    \"Apache Arrow, Arrow, Apache, the Apache feather logo, and the Apache Arrow \"\n    \"project logo are either registered trademarks or trademarks of The Apache \"\n    \"Software Foundation in the United States and other countries\"\n)\nauthor = u'Apache Software Foundation'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = os.environ.get('ARROW_DOCS_VERSION', pyarrow_version)\n\n# The full version, including alpha/beta/rc tags.\nrelease = os.environ.get('ARROW_DOCS_VERSION', pyarrow_version)\n\nif \"+\" in release:\n    release = release.split(\".dev\")[0] + \" (dev)\"\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = \"en\"\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#\n# today = ''\n#\n# Else, today_fmt is used as the format for a strftime call.\n#\n# today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = exclude_patterns + ['_build', 'Thumbs.db', '.DS_Store']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#\n# default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\nintersphinx_mapping = {\n    'python': ('https://docs.python.org/3', None),\n    'numpy': ('https://numpy.org/doc/stable/', None),\n    'pandas': ('https://pandas.pydata.org/docs/', None)\n}\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'pydata_sphinx_theme'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n\nswitcher_version = version\nif \".dev\" in version:\n    switcher_version = \"dev/\"\nelse:\n    # If we are not building dev version of the docs, we are building\n    # docs for the stable version\n    switcher_version = \"\"\n\nhtml_theme_options = {\n    \"show_toc_level\": 2,\n    \"use_edit_page_button\": True,\n    \"logo\": {\n      \"image_light\": \"_static/arrow.png\",\n      \"image_dark\": \"_static/arrow-dark.png\",\n    },\n    \"header_links_before_dropdown\": 2,\n    \"header_dropdown_text\": \"Implementations\",\n    \"navbar_end\": [\"version-switcher\", \"theme-switcher\", \"navbar-icon-links\"],\n    \"icon_links\": [\n        {\n            \"name\": \"GitHub\",\n            \"url\": \"https://github.com/apache/arrow\",\n            \"icon\": \"fa-brands fa-square-github\",\n        },\n        {\n            \"name\": \"X\",\n            \"url\": \"https://twitter.com/ApacheArrow\",\n            \"icon\": \"fa-brands fa-square-x-twitter\",\n        },\n    ],\n    \"show_version_warning_banner\": True,\n    \"switcher\": {\n        \"json_url\": \"/docs/_static/versions.json\",\n        \"version_match\": switcher_version,\n    },\n}\n\nhtml_context = {\n    \"github_user\": \"apache\",\n    \"github_repo\": \"arrow\",\n    \"github_version\": \"main\",\n    \"doc_path\": \"docs/source\",\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# \"<project> v<release> documentation\" by default.\n#\nhtml_title = u'Apache Arrow v{}'.format(version)\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#\n# html_logo = \"_static/arrow.png\"\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or\n# 32x32 pixels large.\n#\nhtml_favicon = \"_static/favicon.ico\"\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# Custom fixes to the RTD theme\nhtml_css_files = ['theme_overrides.css']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#\n# html_extra_path = []\n\n# If not None, a 'Last updated on:' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to '%b %d, %Y'.\n#\n# html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#\n# html_sidebars = {\n#    '**': ['sidebar-logo.html', 'sidebar-search-bs.html', 'sidebar-nav-bs.html'],\n# }\n\n# The base URL which points to the root of the HTML documentation,\n# used for canonical url\nhtml_baseurl = \"https://arrow.apache.org/docs/\"\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n#\n# html_domain_indices = True\n\n# If false, no index is generated.\n#\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#\nhtml_show_sourcelink = False\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#\n# html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#\n# html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n# html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'\n#   'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr', 'zh'\n#\n# html_search_language = 'en'\n\n# A dictionary with options for the search language support, empty by default.\n# 'ja' uses this config value.\n# 'zh' user can custom change `jieba` dictionary path.\n#\n# html_search_options = {'type': 'default'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#\n# html_search_scorer = 'scorer.js'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'arrowdoc'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n     # The paper size ('letterpaper' or 'a4paper').\n     #\n     # 'papersize': 'letterpaper',\n\n     # The font size ('10pt', '11pt' or '12pt').\n     #\n     # 'pointsize': '10pt',\n\n     # Additional stuff for the LaTeX preamble.\n     #\n     # 'preamble': '',\n\n     # Latex figure (float) alignment\n     #\n     # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, 'arrow.tex', u'Apache Arrow Documentation',\n     u'Apache Arrow Team', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#\n# latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n#\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#\n# latex_appendices = []\n\n# It false, will not define \\strong, \\code, \\titleref, \\crossref ... but only\n# \\sphinxstrong, ..., \\sphinxtitleref, ... To help avoid clash with user added\n# packages.\n#\n# latex_keep_old_macro_names = True\n\n# If false, no module index is generated.\n#\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'arrow', u'Apache Arrow Documentation',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, 'arrow', u'Apache Arrow Documentation',\n     author, 'Apache Arrow', 'One line description of project.',\n     'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n#\n# texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#\n# texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n#\n# texinfo_no_detailmenu = False\n\n# -- Options for mermaid output -------------------------------------------\n\nmermaid_output_format = 'svg'\n\ndef setup(app):\n    # Use a config value to indicate whether CUDA API docs can be generated.\n    # This will also rebuild appropriately when the value changes.\n    app.add_config_value('cuda_enabled', cuda_enabled, 'env')\n    app.add_config_value('flight_enabled', flight_enabled, 'env')\n    app.add_directive('arrow-computefuncs', ComputeFunctionsTableDirective)\n\n\nclass ComputeFunctionsTableDirective(Directive):\n    \"\"\"Generate a table of Arrow compute functions.\n\n    .. arrow-computefuncs::\n        :kind: hash_aggregate\n\n    The generated table will include function name,\n    description and option class reference.\n\n    The functions listed in the table can be restricted\n    with the :kind: option.\n    \"\"\"\n    has_content = True\n    option_spec = {\n        \"kind\": directives.unchanged\n    }\n\n    def run(self):\n        from docutils.statemachine import ViewList\n        from docutils import nodes\n        import pyarrow.compute as pc\n\n        result = ViewList()\n        function_kind = self.options.get('kind', None)\n\n        result.append(\".. csv-table::\", \"<computefuncs>\")\n        result.append(\"   :widths: 20, 60, 20\", \"<computefuncs>\")\n        result.append(\"   \", \"<computefuncs>\")\n        for fname in pc.list_functions():\n            func = pc.get_function(fname)\n            option_class = \"\"\n            if func._doc.options_class:\n                option_class = f\":class:`{func._doc.options_class}`\"\n            if not function_kind or func.kind == function_kind:\n                result.append(\n                    f'   \"{fname}\", \"{func._doc.summary}\", \"{option_class}\"',\n                    \"<computefuncs>\"\n                )\n\n        node = nodes.section()\n        node.document = self.state.document\n        self.state.nested_parse(result, 0, node)\n        return node.children\n", "cpp/gdb_arrow.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import namedtuple\nfrom collections.abc import Sequence\nimport datetime\nimport decimal\nimport enum\nfrom functools import lru_cache, partial\nimport itertools\nimport math\nimport operator\nimport struct\nimport sys\nimport warnings\n\nimport gdb\nfrom gdb.types import get_basic_type\n\n\nassert sys.version_info[0] >= 3, \"Arrow GDB extension needs Python 3+\"\n\n\n# gdb API docs at https://sourceware.org/gdb/onlinedocs/gdb/Python-API.html#Python-API\n\n\n_type_ids = [\n    'NA', 'BOOL', 'UINT8', 'INT8', 'UINT16', 'INT16', 'UINT32', 'INT32',\n    'UINT64', 'INT64', 'HALF_FLOAT', 'FLOAT', 'DOUBLE', 'STRING', 'BINARY',\n    'FIXED_SIZE_BINARY', 'DATE32', 'DATE64', 'TIMESTAMP', 'TIME32', 'TIME64',\n    'INTERVAL_MONTHS', 'INTERVAL_DAY_TIME', 'DECIMAL128', 'DECIMAL256',\n    'LIST', 'STRUCT', 'SPARSE_UNION', 'DENSE_UNION', 'DICTIONARY', 'MAP',\n    'EXTENSION', 'FIXED_SIZE_LIST', 'DURATION', 'LARGE_STRING',\n    'LARGE_BINARY', 'LARGE_LIST', 'INTERVAL_MONTH_DAY_NANO']\n\n# Mirror the C++ Type::type enum\nType = enum.IntEnum('Type', _type_ids, start=0)\n\n# Mirror the C++ TimeUnit::type enum\nTimeUnit = enum.IntEnum('TimeUnit', ['SECOND', 'MILLI', 'MICRO', 'NANO'],\n                        start=0)\n\ntype_id_to_struct_code = {\n    Type.INT8: 'b',\n    Type.INT16: 'h',\n    Type.INT32: 'i',\n    Type.INT64: 'q',\n    Type.UINT8: 'B',\n    Type.UINT16: 'H',\n    Type.UINT32: 'I',\n    Type.UINT64: 'Q',\n    Type.HALF_FLOAT: 'e',\n    Type.FLOAT: 'f',\n    Type.DOUBLE: 'd',\n    Type.DATE32: 'i',\n    Type.DATE64: 'q',\n    Type.TIME32: 'i',\n    Type.TIME64: 'q',\n    Type.INTERVAL_DAY_TIME: 'ii',\n    Type.INTERVAL_MONTHS: 'i',\n    Type.INTERVAL_MONTH_DAY_NANO: 'iiq',\n    Type.DURATION: 'q',\n    Type.TIMESTAMP: 'q',\n}\n\nTimeUnitTraits = namedtuple('TimeUnitTraits', ('multiplier',\n                                               'fractional_digits'))\n\ntime_unit_traits = {\n    TimeUnit.SECOND: TimeUnitTraits(1, 0),\n    TimeUnit.MILLI: TimeUnitTraits(1_000, 3),\n    TimeUnit.MICRO: TimeUnitTraits(1_000_000, 6),\n    TimeUnit.NANO: TimeUnitTraits(1_000_000_000, 9),\n}\n\n\ndef identity(v):\n    return v\n\n\ndef has_null_bitmap(type_id):\n    return type_id not in (Type.NA, Type.SPARSE_UNION, Type.DENSE_UNION)\n\n\n@lru_cache()\ndef byte_order():\n    \"\"\"\n    Get the target program (not the GDB host's) endianness.\n    \"\"\"\n    s = gdb.execute(\"show endian\", to_string=True).strip()\n    if 'big' in s:\n        return 'big'\n    elif 'little' in s:\n        return 'little'\n    warnings.warn('Could not determine target endianness '\n                  f'from GDB\\'s response:\\n\"\"\"{s}\"\"\"')\n    # Fall back to host endianness\n    return sys.byteorder\n\n\ndef for_evaluation(val, ty=None):\n    \"\"\"\n    Return a parsable form of gdb.Value `val`, optionally with gdb.Type `ty`.\n    \"\"\"\n    if ty is None:\n        ty = get_basic_type(val.type)\n    typename = str(ty)  # `ty.name` is sometimes None...\n    if '::' in typename and not typename.startswith('::'):\n        # ARROW-15652: expressions evaluated by GDB are evaluated in the\n        # scope of the C++ namespace of the currently selected frame.\n        # When inside a Parquet frame, `arrow::<some type>` would be looked\n        # up as `parquet::arrow::<some type>` and fail.\n        # Therefore, force the lookup to happen in the global namespace scope.\n        typename = f\"::{typename}\"\n    if ty.code == gdb.TYPE_CODE_PTR:\n        # It's already a pointer, can represent it directly\n        return f\"(({typename}) ({val}))\"\n    if val.address is None:\n        raise ValueError(f\"Cannot further evaluate rvalue: {val}\")\n    return f\"(* ({typename}*) ({val.address}))\"\n\n\ndef is_char_star(ty):\n    # Note that \"const char*\" can have TYPE_CODE_INT as target type...\n    ty = get_basic_type(ty)\n    return (ty.code == gdb.TYPE_CODE_PTR and\n            get_basic_type(ty.target()).code\n                in (gdb.TYPE_CODE_CHAR, gdb.TYPE_CODE_INT))\n\n\ndef deref(val):\n    \"\"\"\n    Dereference a raw or smart pointer.\n    \"\"\"\n    ty = get_basic_type(val.type)\n    if ty.code == gdb.TYPE_CODE_PTR:\n        return val.dereference()\n    if ty.name.startswith('std::'):\n        if \"shared\" in ty.name:\n            return SharedPtr(val).value\n        if \"unique\" in ty.name:\n            return UniquePtr(val).value\n    raise TypeError(f\"Cannot dereference value of type '{ty.name}'\")\n\n\n_string_literal_mapping = {\n    ord('\\\\'): r'\\\\',\n    ord('\\n'): r'\\n',\n    ord('\\r'): r'\\r',\n    ord('\\t'): r'\\t',\n    ord('\"'): r'\\\"',\n}\n\nfor c in range(0, 32):\n    if c not in _string_literal_mapping:\n        _string_literal_mapping[c] = f\"\\\\x{c:02x}\"\n\n\ndef string_literal(s):\n    \"\"\"\n    Format a Python string or gdb.Value for display as a literal.\n    \"\"\"\n    max_len = 50\n    if isinstance(s, gdb.Value):\n        s = s.string()\n    if len(s) > max_len:\n        s = s[:max_len]\n        return '\"' + s.translate(_string_literal_mapping) + '\" [continued]'\n    else:\n        return '\"' + s.translate(_string_literal_mapping) + '\"'\n\n\ndef bytes_literal(val, size=None):\n    \"\"\"\n    Format a gdb.Value for display as a literal containing possibly\n    unprintable characters.\n    \"\"\"\n    return val.lazy_string(length=size).value()\n\n\ndef utf8_literal(val, size=None):\n    \"\"\"\n    Format a gdb.Value for display as a utf-8 literal.\n    \"\"\"\n    if size is None:\n        s = val.string(encoding='utf8', errors='backslashreplace')\n    elif size != 0:\n        s = val.string(encoding='utf8', errors='backslashreplace', length=size)\n    else:\n        s = \"\"\n    return string_literal(s)\n\n\ndef half_float_value(val):\n    \"\"\"\n    Return a Python float of the given half-float (represented as a uint64_t\n    gdb.Value).\n    \"\"\"\n    buf = gdb.selected_inferior().read_memory(val.address, 2)\n    return struct.unpack(\"e\", buf)[0]\n\n\ndef load_atomic(val):\n    \"\"\"\n    Load a std::atomic<T>'s value.\n    \"\"\"\n    valty = val.type.template_argument(0)\n    # XXX This assumes std::atomic<T> has the same layout as a raw T.\n    return val.address.reinterpret_cast(valty.pointer()).dereference()\n\n\ndef load_null_count(val):\n    \"\"\"\n    Load a null count from a gdb.Value of an integer (either atomic or not).\n    \"\"\"\n    if get_basic_type(val.type).code != gdb.TYPE_CODE_INT:\n        val = load_atomic(val)\n    return val\n\n\ndef format_null_count(val):\n    \"\"\"\n    Format a null count value.\n    \"\"\"\n    if not isinstance(val, int):\n        null_count = int(load_null_count(val))\n    return (f\"null count {null_count}\" if null_count != -1\n            else \"unknown null count\")\n\n\ndef short_time_unit(val):\n    return ['s', 'ms', 'us', 'ns'][int(val)]\n\n\ndef format_month_interval(val):\n    \"\"\"\n    Format a MonthInterval value.\n    \"\"\"\n    return f\"{int(val)}M\"\n\n\ndef format_days_milliseconds(days, milliseconds):\n    return f\"{days}d{milliseconds}ms\"\n\n\ndef format_months_days_nanos(months, days, nanos):\n    return f\"{months}M{days}d{nanos}ns\"\n\n\n_date_base = datetime.date(1970, 1, 1).toordinal()\n\n\ndef format_date32(val):\n    \"\"\"\n    Format a date32 value.\n    \"\"\"\n    val = int(val)\n    try:\n        decoded = datetime.date.fromordinal(val + _date_base)\n    except ValueError:  # \"ordinal must be >= 1\"\n        return f\"{val}d [year <= 0]\"\n    else:\n        return f\"{val}d [{decoded}]\"\n\n\ndef format_date64(val):\n    \"\"\"\n    Format a date64 value.\n    \"\"\"\n    val = int(val)\n    days, remainder = divmod(val, 86400 * 1000)\n    if remainder:\n        return f\"{val}ms [non-multiple of 86400000]\"\n    try:\n        decoded = datetime.date.fromordinal(days + _date_base)\n    except ValueError:  # \"ordinal must be >= 1\"\n        return f\"{val}ms [year <= 0]\"\n    else:\n        return f\"{val}ms [{decoded}]\"\n\n\ndef format_timestamp(val, unit):\n    \"\"\"\n    Format a timestamp value.\n    \"\"\"\n    val = int(val)\n    unit = int(unit)\n    short_unit = short_time_unit(unit)\n    traits = time_unit_traits[unit]\n    seconds, subseconds = divmod(val, traits.multiplier)\n    try:\n        dt = datetime.datetime.utcfromtimestamp(seconds)\n    except (ValueError, OSError, OverflowError):\n        # value out of range for datetime.datetime\n        pretty = \"too large to represent\"\n    else:\n        pretty = dt.isoformat().replace('T', ' ')\n        if traits.fractional_digits > 0:\n            pretty += f\".{subseconds:0{traits.fractional_digits}d}\"\n    return f\"{val}{short_unit} [{pretty}]\"\n\n\ndef cast_to_concrete(val, ty):\n    return (val.reference_value().reinterpret_cast(ty.reference())\n            .referenced_value())\n\n\ndef scalar_class_from_type(name):\n    \"\"\"\n    Given a DataTypeClass class name (such as \"BooleanType\"), return the\n    corresponding Scalar class name.\n    \"\"\"\n    assert name.endswith(\"Type\")\n    return name[:-4] + \"Scalar\"\n\n\ndef array_class_from_type(name):\n    \"\"\"\n    Given a DataTypeClass class name (such as \"BooleanType\"), return the\n    corresponding Array class name.\n    \"\"\"\n    assert name.endswith(\"Type\")\n    return name[:-4] + \"Array\"\n\n\nclass CString:\n    \"\"\"\n    A `const char*` or similar value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n\n    def __bool__(self):\n        return int(data) != 0 and int(data[0]) != 0\n\n    @property\n    def data(self):\n        return self.val\n\n    def bytes_literal(self):\n        return self.val.lazy_string().value()\n\n    def string_literal(self):\n        # XXX use lazy_string() as well?\n        return string_literal(self.val)\n\n    def string(self):\n        return self.val.string()\n\n    def __format__(self, fmt):\n        return str(self.bytes_literal())\n\n\n# NOTE: gdb.parse_and_eval() is *slow* and calling it multiple times\n# may add noticeable latencies.  For standard C++ classes, we therefore\n# try to fetch their properties from libstdc++ internals (which hopefully\n# are stable), before falling back on calling the public API methods.\n\nclass SharedPtr:\n    \"\"\"\n    A `std::shared_ptr<T>` value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        try:\n            # libstdc++ internals\n            self._ptr = val['_M_ptr']\n        except gdb.error:\n            # fallback for other C++ standard libraries\n            self._ptr = gdb.parse_and_eval(f\"{for_evaluation(val)}.get()\")\n\n    def get(self):\n        \"\"\"\n        Return the underlying pointer (a T*).\n        \"\"\"\n        return self._ptr\n\n    @property\n    def value(self):\n        \"\"\"\n        The underlying value (a T).\n        \"\"\"\n        return self._ptr.dereference()\n\n\nclass UniquePtr:\n    \"\"\"\n    A `std::unique_ptr<T>` value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        ty = self.val.type.template_argument(0)\n        # XXX This assumes that the embedded T* pointer lies at the start\n        # of std::unique_ptr<T>.\n        self._ptr = self.val.address.reinterpret_cast(ty.pointer().pointer())\n\n    def get(self):\n        \"\"\"\n        Return the underlying pointer (a T*).\n        \"\"\"\n        return self._ptr\n\n    @property\n    def value(self):\n        \"\"\"\n        The underlying value (a T).\n        \"\"\"\n        return self._ptr.dereference()\n\n\nclass Variant:\n    \"\"\"\n    A `std::variant<...>`.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        try:\n            # libstdc++ internals\n            self.index = val['_M_index']\n        except gdb.error:\n            # fallback for other C++ standard libraries\n            self.index = gdb.parse_and_eval(f\"{for_evaluation(val)}.index()\")\n        try:\n            self.value_type = self.val.type.template_argument(self.index)\n        except RuntimeError:\n            # Index out of bounds\n            self.value_type = None\n\n    @property\n    def value(self):\n        if self.value_type is None:\n            return None\n        ptr = self.val.address\n        if ptr is not None:\n            return ptr.reinterpret_cast(self.value_type.pointer()\n                                        ).dereference()\n        return None\n\n\nclass StdString:\n    \"\"\"\n    A `std::string` (or possibly `std::string_view`) value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        try:\n            # libstdc++ internals\n            self._data = val['_M_dataplus']['_M_p']\n            self._size = val['_M_string_length']\n        except gdb.error:\n            # fallback for other C++ standard libraries\n            self._data = gdb.parse_and_eval(f\"{for_evaluation(val)}.c_str()\")\n            self._size = gdb.parse_and_eval(f\"{for_evaluation(val)}.size()\")\n\n    def __bool__(self):\n        return self._size != 0\n\n    @property\n    def data(self):\n        return self._data\n\n    @property\n    def size(self):\n        return self._size\n\n    def bytes_literal(self):\n        return self._data.lazy_string(length=self._size).value()\n\n    def string_literal(self):\n        # XXX use lazy_string() as well?\n        return string_literal(self._data)\n\n    def string(self):\n        return self._data.string()\n\n    def __format__(self, fmt):\n        return str(self.bytes_literal())\n\n\nclass StdVector(Sequence):\n    \"\"\"\n    A `std::vector<T>` value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        try:\n            # libstdc++ internals\n            impl = self.val['_M_impl']\n            self._data = impl['_M_start']\n            self._size = int(impl['_M_finish'] - self._data)\n        except gdb.error:\n            # fallback for other C++ standard libraries\n            self._data = int(gdb.parse_and_eval(\n                f\"{for_evaluation(self.val)}.data()\"))\n            self._size = int(gdb.parse_and_eval(\n                f\"{for_evaluation(self.val)}.size()\"))\n\n    def _check_index(self, index):\n        if index < 0 or index >= self._size:\n            raise IndexError(\n                f\"Index {index} out of bounds (should be in [0, {self._size - 1}])\")\n\n    def __len__(self):\n        return self._size\n\n    def __getitem__(self, index):\n        self._check_index(index)\n        return self._data[index]\n\n    def eval_at(self, index, eval_format):\n        \"\"\"\n        Run `eval_format` with the value at `index`.\n\n        For example, if `eval_format` is \"{}.get()\", this will evaluate\n        \"{self[index]}.get()\".\n        \"\"\"\n        self._check_index(index)\n        return gdb.parse_and_eval(\n            eval_format.format(for_evaluation(self._data[index])))\n\n    def iter_eval(self, eval_format):\n        data_eval = for_evaluation(self._data)\n        for i in range(self._size):\n            yield gdb.parse_and_eval(\n                eval_format.format(f\"{data_eval}[{i}]\"))\n\n    @property\n    def size(self):\n        return self._size\n\n\nclass StdPtrVector(StdVector):\n\n    def __getitem__(self, index):\n        return deref(super().__getitem__(index))\n\n\nclass FieldVector(StdVector):\n\n    def __getitem__(self, index):\n        \"\"\"\n        Dereference the Field object at this index.\n        \"\"\"\n        return Field(deref(super().__getitem__(index)))\n\n    def __str__(self):\n        l = [str(self[i]) for i in range(len(self))]\n        return \"{\" + \", \".join(l) + \"}\"\n\n\nclass Field:\n    \"\"\"\n    A arrow::Field value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n\n    @property\n    def name(self):\n        return StdString(self.val['name_'])\n\n    @property\n    def type(self):\n        return deref(self.val['type_'])\n\n    @property\n    def nullable(self):\n        return bool(self.val['nullable_'])\n\n    def __str__(self):\n        return str(self.val)\n\n\nclass FieldPtr(Field):\n    \"\"\"\n    A std::shared_ptr<arrow::Field> value.\n    \"\"\"\n\n    def __init__(self, val):\n        super().__init__(deref(val))\n\n\nclass Buffer:\n    \"\"\"\n    A arrow::Buffer value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        self.size = int(val['size_'])\n\n    @property\n    def data(self):\n        return self.val['data_']\n\n    def bytes_literal(self):\n        if self.size > 0:\n            return self.val['data_'].lazy_string(length=self.size).value()\n        else:\n            return '\"\"'\n\n    def bytes_view(self, offset=0, length=None):\n        \"\"\"\n        Return a view over the bytes of this buffer.\n        \"\"\"\n        if self.size > 0:\n            if length is None:\n                length = self.size\n            mem = gdb.selected_inferior().read_memory(\n                self.val['data_'] + offset, self.size)\n        else:\n            mem = memoryview(b\"\")\n        # Read individual bytes as unsigned integers rather than\n        # Python bytes objects\n        return mem.cast('B')\n\n    view = bytes_view\n\n\nclass BufferPtr:\n    \"\"\"\n    A arrow::Buffer* value (possibly null).\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        ptr = int(self.val)\n        self.buf = Buffer(val.dereference()) if ptr != 0 else None\n\n    @property\n    def data(self):\n        if self.buf is None:\n            return None\n        return self.buf.data\n\n    @property\n    def size(self):\n        if self.buf is None:\n            return None\n        return self.buf.size\n\n    def bytes_literal(self):\n        if self.buf is None:\n            return None\n        return self.buf.bytes_literal()\n\n\nclass TypedBuffer(Buffer):\n    \"\"\"\n    A buffer containing values of a given a struct format code.\n    \"\"\"\n    _boolean_format = object()\n\n    def __init__(self, val, mem_format):\n        super().__init__(val)\n        self.mem_format = mem_format\n        if not self.is_boolean:\n            self.byte_width = struct.calcsize('=' + self.mem_format)\n\n    @classmethod\n    def from_type_id(cls, val, type_id):\n        assert isinstance(type_id, int)\n        if type_id == Type.BOOL:\n            mem_format = cls._boolean_format\n        else:\n            mem_format = type_id_to_struct_code[type_id]\n        return cls(val, mem_format)\n\n    def view(self, offset=0, length=None):\n        \"\"\"\n        Return a view over the primitive values in this buffer.\n\n        The optional `offset` and `length` are expressed in primitive values,\n        not bytes.\n        \"\"\"\n        if self.is_boolean:\n            return Bitmap.from_buffer(self, offset, length)\n\n        byte_offset = offset * self.byte_width\n        if length is not None:\n            mem = self.bytes_view(byte_offset, length * self.byte_width)\n        else:\n            mem = self.bytes_view(byte_offset)\n        return TypedView(mem, self.mem_format)\n\n    @property\n    def is_boolean(self):\n        return self.mem_format is self._boolean_format\n\n\nclass TypedView(Sequence):\n    \"\"\"\n    View a bytes-compatible object as a sequence of objects described\n    by a struct format code.\n    \"\"\"\n\n    def __init__(self, mem, mem_format):\n        assert isinstance(mem, memoryview)\n        self.mem = mem\n        self.mem_format = mem_format\n        self.byte_width = struct.calcsize('=' + mem_format)\n        self.length = mem.nbytes // self.byte_width\n\n    def _check_index(self, index):\n        if not 0 <= index < self.length:\n            raise IndexError(\"Wrong index for bitmap\")\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        self._check_index(index)\n        w = self.byte_width\n        # Cannot use memoryview.cast() because the 'e' format for half-floats\n        # is poorly supported.\n        mem = self.mem[index * w:(index + 1) * w]\n        return struct.unpack('=' + self.mem_format, mem)\n\n\nclass Bitmap(Sequence):\n    \"\"\"\n    View a bytes-compatible object as a sequence of bools.\n    \"\"\"\n    _masks = [0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, 0x80]\n\n    def __init__(self, view, offset, length):\n        self.view = view\n        self.offset = offset\n        self.length = length\n\n    def _check_index(self, index):\n        if not 0 <= index < self.length:\n            raise IndexError(\"Wrong index for bitmap\")\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        self._check_index(index)\n        index += self.offset\n        byte_index, bit_index = divmod(index, 8)\n        byte = self.view[byte_index]\n        return byte & self._masks[bit_index] != 0\n\n    @classmethod\n    def from_buffer(cls, buf, offset, length):\n        assert isinstance(buf, Buffer)\n        byte_offset, bit_offset = divmod(offset, 8)\n        byte_length = math.ceil(length + offset / 8) - byte_offset\n        return cls(buf.bytes_view(byte_offset, byte_length),\n                   bit_offset, length)\n\n\nclass MappedView(Sequence):\n\n    def __init__(self, func, view):\n        self.view = view\n        self.func = func\n\n    def __len__(self):\n        return len(self.view)\n\n    def __getitem__(self, index):\n        return self.func(self.view[index])\n\n\nclass StarMappedView(Sequence):\n\n    def __init__(self, func, view):\n        self.view = view\n        self.func = func\n\n    def __len__(self):\n        return len(self.view)\n\n    def __getitem__(self, index):\n        return self.func(*self.view[index])\n\n\nclass NullBitmap(Bitmap):\n\n    def __getitem__(self, index):\n        self._check_index(index)\n        if self.view is None:\n            return True\n        return super().__getitem__(index)\n\n    @classmethod\n    def from_buffer(cls, buf, offset, length):\n        \"\"\"\n        Create a null bitmap from a Buffer (or None if missing,\n        in which case all values are True).\n        \"\"\"\n        if buf is None:\n            return cls(buf, offset, length)\n        return super().from_buffer(buf, offset, length)\n\n\nKeyValue = namedtuple('KeyValue', ('key', 'value'))\n\n\nclass Metadata(Sequence):\n    \"\"\"\n    A arrow::KeyValueMetadata value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        self.keys = StdVector(self.val['keys_'])\n        self.values = StdVector(self.val['values_'])\n\n    def __len__(self):\n        return len(self.keys)\n\n    def __getitem__(self, i):\n        return KeyValue(StdString(self.keys[i]), StdString(self.values[i]))\n\n\nclass MetadataPtr(Sequence):\n    \"\"\"\n    A shared_ptr<arrow::KeyValueMetadata> value, possibly null.\n    \"\"\"\n\n    def __init__(self, val):\n        self.ptr = SharedPtr(val).get()\n        self.is_null = int(self.ptr) == 0\n        self.md = None if self.is_null else Metadata(self.ptr.dereference())\n\n    def __len__(self):\n        return 0 if self.is_null else len(self.md)\n\n    def __getitem__(self, i):\n        if self.is_null:\n            raise IndexError\n        return self.md[i]\n\n\nDecimalTraits = namedtuple('DecimalTraits', ('bit_width', 'struct_format_le'))\n\ndecimal_traits = {\n    128: DecimalTraits(128, 'Qq'),\n    256: DecimalTraits(256, 'QQQq'),\n}\n\nclass BaseDecimal:\n    \"\"\"\n    Base class for arrow::BasicDecimal{128,256...} values.\n    \"\"\"\n\n    def __init__(self, address):\n        self.address = address\n\n    @classmethod\n    def from_value(cls, val):\n        \"\"\"\n        Create a decimal from a gdb.Value representing the corresponding\n        arrow::BasicDecimal{128,256...}.\n        \"\"\"\n        return cls(val['array_'].address)\n\n    @classmethod\n    def from_address(cls, address):\n        \"\"\"\n        Create a decimal from a gdb.Value representing the address of the\n        raw decimal storage.\n        \"\"\"\n        return cls(address)\n\n    @property\n    def words(self):\n        \"\"\"\n        The decimal words, from least to most significant.\n        \"\"\"\n        mem = gdb.selected_inferior().read_memory(self.address,\n                                                  self.traits.bit_width // 8)\n        fmt = self.traits.struct_format_le\n        if byte_order() == 'big':\n            fmt = fmt[::-1]\n        words = struct.unpack(f\"={fmt}\", mem)\n        if byte_order() == 'big':\n            words = words[::-1]\n        return words\n\n    def __int__(self):\n        \"\"\"\n        The underlying bigint value.\n        \"\"\"\n        v = 0\n        words = self.words\n        bits_per_word = self.traits.bit_width // len(words)\n        for w in reversed(words):\n            v = (v << bits_per_word) + w\n        return v\n\n    def format(self, precision, scale):\n        \"\"\"\n        Format as a decimal number with the given precision and scale.\n        \"\"\"\n        v = int(self)\n        with decimal.localcontext() as ctx:\n            ctx.prec = precision\n            ctx.capitals = False\n            return str(decimal.Decimal(v).scaleb(-scale))\n\n\nclass Decimal128(BaseDecimal):\n    traits = decimal_traits[128]\n\n\nclass Decimal256(BaseDecimal):\n    traits = decimal_traits[256]\n\n\ndecimal_bits_to_class = {\n    128: Decimal128,\n    256: Decimal256,\n}\n\ndecimal_type_to_class = {\n    f\"Decimal{bits}Type\": cls\n    for (bits, cls) in decimal_bits_to_class.items()\n}\n\n\nclass ExtensionType:\n    \"\"\"\n    A arrow::ExtensionType.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n\n    @property\n    def storage_type(self):\n        return deref(self.val['storage_type_'])\n\n    def to_string(self):\n        \"\"\"\n        The result of calling ToString(show_metadata=True).\n        \"\"\"\n        # XXX `show_metadata` is an optional argument, but gdb doesn't allow\n        # omitting it.\n        return StdString(gdb.parse_and_eval(\n            f\"{for_evaluation(self.val)}.ToString(true)\"))\n\n\nclass Schema:\n    \"\"\"\n    A arrow::Schema.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        impl = deref(self.val['impl_'])\n        self.fields = FieldVector(impl['fields_'])\n        self.metadata = MetadataPtr(impl['metadata_'])\n\n\nclass RecordBatch:\n    \"\"\"\n    A arrow::RecordBatch.\n    \"\"\"\n\n    def __init__(self, val):\n        # XXX this relies on RecordBatch always being a SimpleRecordBatch\n        # under the hood. What if users create their own RecordBatch\n        # implementation?\n        self.val = cast_to_concrete(val,\n                                    gdb.lookup_type(\"arrow::SimpleRecordBatch\"))\n        self.schema = Schema(deref(self.val['schema_']))\n        self.columns = StdPtrVector(self.val['columns_'])\n\n    @property\n    def num_rows(self):\n        return self.val['num_rows_']\n\n\nclass Table:\n    \"\"\"\n    A arrow::Table.\n    \"\"\"\n\n    def __init__(self, val):\n        # XXX this relies on Table always being a SimpleTable under the hood.\n        # What if users create their own Table implementation?\n        self.val = cast_to_concrete(val,\n                                    gdb.lookup_type(\"arrow::SimpleTable\"))\n        self.schema = Schema(deref(self.val['schema_']))\n        self.columns = StdPtrVector(self.val['columns_'])\n\n    @property\n    def num_rows(self):\n        return self.val['num_rows_']\n\n\ntype_reprs = {\n    'NullType': 'null',\n    'BooleanType': 'boolean',\n    'UInt8Type': 'uint8',\n    'Int8Type': 'int8',\n    'UInt16Type': 'uint16',\n    'Int16Type': 'int16',\n    'UInt32Type': 'uint32',\n    'Int32Type': 'int32',\n    'UInt64Type': 'uint64',\n    'Int64Type': 'int64',\n    'HalfFloatType': 'float16',\n    'FloatType': 'float32',\n    'DoubleType': 'float64',\n    'Date32Type': 'date32',\n    'Date64Type': 'date64',\n    'Time32Type': 'time32',\n    'Time64Type': 'time64',\n    'TimestampType': 'timestamp',\n    'MonthIntervalType': 'month_interval',\n    'DayTimeIntervalType': 'day_time_interval',\n    'MonthDayNanoIntervalType': 'month_day_nano_interval',\n    'DurationType': 'duration',\n    'Decimal128Type': 'decimal128',\n    'Decimal256Type': 'decimal256',\n    'StringType': 'utf8',\n    'LargeStringType': 'large_utf8',\n    'BinaryType': 'binary',\n    'LargeBinaryType': 'large_binary',\n    'FixedSizeBinaryType': 'fixed_size_binary',\n    'ListType': 'list',\n    'LargeListType': 'large_list',\n    'FixedSizeListType': 'fixed_size_list',\n    'MapType': 'map',\n    'StructType': 'struct_',\n    'SparseUnionType': 'sparse_union',\n    'DenseUnionType': 'dense_union',\n    'DictionaryType': 'dictionary',\n    }\n\n\nclass TypePrinter:\n    \"\"\"\n    Pretty-printer for arrow::DataTypeClass and subclasses.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.name = name\n        # Cast to concrete type class to access all derived methods\n        # and properties.\n        self.type = gdb.lookup_type(f\"arrow::{name}\")\n        self.val = cast_to_concrete(val, self.type)\n\n    @property\n    def fields(self):\n        return FieldVector(self.val['children_'])\n\n    def _format_type(self):\n        r = type_reprs.get(self.name, self.name)\n        return f\"arrow::{r}\"\n\n    def _for_evaluation(self):\n        return for_evaluation(self.val, self.type)\n\n\nclass PrimitiveTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for non-parametric types.\n    \"\"\"\n\n    def to_string(self):\n        return f\"{self._format_type()}()\"\n\n\nclass TimeTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for time and duration types.\n    \"\"\"\n\n    def _get_unit(self):\n        return self.val['unit_']\n\n    def to_string(self):\n        return f\"{self._format_type()}({self._get_unit()})\"\n\n\nclass TimestampTypePrinter(TimeTypePrinter):\n    \"\"\"\n    Pretty-printer for timestamp types.\n    \"\"\"\n\n    def to_string(self):\n        tz = StdString(self.val['timezone_'])\n        if tz:\n            return f'{self._format_type()}({self._get_unit()}, {tz})'\n        else:\n            return f'{self._format_type()}({self._get_unit()})'\n\n\nclass FixedSizeBinaryTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for fixed-size binary types.\n    \"\"\"\n\n    def to_string(self):\n        width = int(self.val['byte_width_'])\n        return f\"{self._format_type()}({width})\"\n\n\nclass DecimalTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for decimal types.\n    \"\"\"\n\n    def to_string(self):\n        precision = int(self.val['precision_'])\n        scale = int(self.val['scale_'])\n        return f\"{self._format_type()}({precision}, {scale})\"\n\n\nclass ListTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for list types.\n    \"\"\"\n\n    def _get_value_type(self):\n        fields = self.fields\n        if len(fields) != 1:\n            return None\n        return fields[0].type\n\n    def to_string(self):\n        child = self._get_value_type()\n        if child is None:\n            return f\"{self._format_type()}<uninitialized or corrupt>\"\n        else:\n            return f\"{self._format_type()}({child})\"\n\n\nclass FixedSizeListTypePrinter(ListTypePrinter):\n    \"\"\"\n    Pretty-printer for fixed-size list type.\n    \"\"\"\n\n    def to_string(self):\n        child = self._get_value_type()\n        if child is None:\n            return f\"{self._format_type()}<uninitialized or corrupt>\"\n        list_size = int(self.val['list_size_'])\n        return f\"{self._format_type()}({child}, {list_size})\"\n\n\nclass MapTypePrinter(ListTypePrinter):\n    \"\"\"\n    Pretty-printer for map types.\n    \"\"\"\n\n    def to_string(self):\n        struct_type = self._get_value_type()\n        if struct_type is None:\n            return f\"{self._format_type()}<uninitialized or corrupt>\"\n        struct_children = FieldVector(struct_type['children_'])\n        if len(struct_children) != 2:\n            return f\"{self._format_type()}<uninitialized or corrupt>\"\n        key_type = struct_children[0].type\n        item_type = struct_children[1].type\n        return (f\"{self._format_type()}({key_type}, {item_type}, \"\n                f\"keys_sorted={self.val['keys_sorted_']})\")\n\n\nclass DictionaryTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for dictionary types.\n    \"\"\"\n\n    def to_string(self):\n        index_type = deref(self.val['index_type_'])\n        value_type = deref(self.val['value_type_'])\n        ordered = self.val['ordered_']\n        return (f\"{self._format_type()}({index_type}, {value_type}, \"\n                f\"ordered={ordered})\")\n\n\nclass StructTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for struct types.\n    \"\"\"\n\n    def to_string(self):\n        return f\"{self._format_type()}({self.fields})\"\n\n\nclass UnionTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for union types.\n    \"\"\"\n\n    def to_string(self):\n        type_codes = StdVector(self.val['type_codes_'])\n        type_codes = \"{\" + \", \".join(str(x.cast(gdb.lookup_type('int')))\n                                     for x in type_codes) + \"}\"\n        return f\"{self._format_type()}(fields={self.fields}, type_codes={type_codes})\"\n\n\nclass ExtensionTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for extension types.\n    \"\"\"\n\n    def to_string(self):\n        ext_type = ExtensionType(self.val)\n        return (f\"{self._format_type()} {ext_type.to_string().string_literal()} \"\n                f\"with storage type {ext_type.storage_type}\")\n\n\nclass ScalarPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Scalar and subclasses.\n    \"\"\"\n\n    def __new__(cls, val):\n        # Lookup actual (derived) class to instantiate\n        type_id = int(deref(val['type'])['id_'])\n        type_class = lookup_type_class(type_id)\n        if type_class is not None:\n            cls = type_class.scalar_printer\n            assert issubclass(cls, ScalarPrinter)\n        self = object.__new__(cls)\n        self.type_class = type_class\n        self.type_name = type_class.name\n        self.name = scalar_class_from_type(self.type_name)\n        self.type_id = type_id\n        # Cast to concrete Scalar class to access derived attributes.\n        concrete_type = gdb.lookup_type(f\"arrow::{self.name}\")\n        self.val = cast_to_concrete(val, concrete_type)\n        self.is_valid = bool(self.val['is_valid'])\n        return self\n\n    @property\n    def type(self):\n        \"\"\"\n        The concrete DataTypeClass instance.\n        \"\"\"\n        concrete_type = gdb.lookup_type(f\"arrow::{self.type_name}\")\n        return cast_to_concrete(deref(self.val['type']),\n                                concrete_type)\n\n    def _format_type(self):\n        return f\"arrow::{self.name}\"\n\n    def _format_null(self):\n        if self.type_class.is_parametric:\n            return f\"{self._format_type()} of type {self.type}, null value\"\n        else:\n            return f\"{self._format_type()} of null value\"\n\n    def _for_evaluation(self):\n        return for_evaluation(self.val)\n\n\nclass NullScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::NullScalar.\n    \"\"\"\n\n    def to_string(self):\n        return self._format_type()\n\n\nclass NumericScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for numeric Arrow scalars.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        value = self.val['value']\n        if self.type_name == \"HalfFloatType\":\n            return (f\"{self._format_type()} \"\n                    f\"of value {half_float_value(value)} [{value}]\")\n        if self.type_name in (\"UInt8Type\", \"Int8Type\"):\n            value = value.cast(gdb.lookup_type('int'))\n        return f\"{self._format_type()} of value {value}\"\n\n\nclass TimeScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for Arrow time-like scalars.\n    \"\"\"\n\n    def to_string(self):\n        unit = short_time_unit(self.type['unit_'])\n        if not self.is_valid:\n            return f\"{self._format_type()} of null value [{unit}]\"\n        value = self.val['value']\n        return f\"{self._format_type()} of value {value}{unit}\"\n\n\nclass Date32ScalarPrinter(TimeScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::Date32Scalar.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        value = self.val['value']\n        return f\"{self._format_type()} of value {format_date32(value)}\"\n\n\nclass Date64ScalarPrinter(TimeScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::Date64Scalar.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        value = self.val['value']\n        return f\"{self._format_type()} of value {format_date64(value)}\"\n\n\nclass TimestampScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::TimestampScalar.\n    \"\"\"\n\n    def to_string(self):\n        unit = short_time_unit(self.type['unit_'])\n        tz = StdString(self.type['timezone_'])\n        tz = tz.string_literal() if tz.size != 0 else \"no timezone\"\n        if not self.is_valid:\n            return f\"{self._format_type()} of null value [{unit}, {tz}]\"\n        value = self.val['value']\n        return f\"{self._format_type()} of value {value}{unit} [{tz}]\"\n\n\nclass MonthIntervalScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::MonthIntervalScalarPrinter.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        value = self.val['value']\n        return f\"{self._format_type()} of value {format_month_interval(value)}\"\n\n\nclass DecimalScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::DecimalScalar and subclasses.\n    \"\"\"\n\n    @property\n    def decimal_class(self):\n        return decimal_type_to_class[self.type_name]\n\n    def to_string(self):\n        ty = self.type\n        precision = int(ty['precision_'])\n        scale = int(ty['scale_'])\n        suffix = f\"[precision={precision}, scale={scale}]\"\n        if not self.is_valid:\n            return f\"{self._format_type()} of null value {suffix}\"\n        value = self.decimal_class.from_value(self.val['value']\n                                              ).format(precision, scale)\n        return f\"{self._format_type()} of value {value} {suffix}\"\n\n\nclass BaseBinaryScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::BaseBinaryScalar and subclasses.\n    \"\"\"\n\n    def _format_buf(self, bufptr):\n        if 'String' in self.type_name:\n            return utf8_literal(bufptr.data, bufptr.size)\n        else:\n            return bufptr.bytes_literal()\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        bufptr = BufferPtr(SharedPtr(self.val['value']).get())\n        size = bufptr.size\n        if size is None:\n            return f\"{self._format_type()} of value <unallocated>\"\n        return (f\"{self._format_type()} of size {size}, \"\n                f\"value {self._format_buf(bufptr)}\")\n\n\nclass FixedSizeBinaryScalarPrinter(BaseBinaryScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::FixedSizeBinaryScalar.\n    \"\"\"\n\n    def to_string(self):\n        size = self.type['byte_width_']\n        bufptr = BufferPtr(SharedPtr(self.val['value']).get())\n        if bufptr.data is None:\n            return f\"{self._format_type()} of size {size}, <unallocated>\"\n        nullness = '' if self.is_valid else 'null with '\n        return (f\"{self._format_type()} of size {size}, \"\n                f\"{nullness}value {self._format_buf(bufptr)}\")\n\n\nclass DictionaryScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::DictionaryScalar.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        index = deref(self.val['value']['index'])\n        dictionary = deref(self.val['value']['dictionary'])\n        return (f\"{self._format_type()} of index {index}, \"\n                f\"dictionary {dictionary}\")\n\n\nclass BaseListScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::BaseListScalar and subclasses.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        value = deref(self.val['value'])\n        return f\"{self._format_type()} of value {value}\"\n\n\nclass StructScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::StructScalar.\n    \"\"\"\n\n    def display_hint(self):\n        return 'map'\n\n    def children(self):\n        if not self.is_valid:\n            return None\n        eval_fields = StdVector(self.type['children_'])\n        eval_values = StdVector(self.val['value'])\n        for field, value in zip(eval_fields, eval_values):\n            name = StdString(deref(field)['name_']).string_literal()\n            yield (\"name\", name)\n            yield (\"value\", deref(value))\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        return f\"{self._format_type()}\"\n\n\nclass SparseUnionScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::UnionScalar and subclasses.\n    \"\"\"\n\n    def to_string(self):\n        type_code = self.val['type_code'].cast(gdb.lookup_type('int'))\n        if not self.is_valid:\n            return (f\"{self._format_type()} of type {self.type}, \"\n                    f\"type code {type_code}, null value\")\n        eval_values = StdVector(self.val['value'])\n        child_id = self.val['child_id'].cast(gdb.lookup_type('int'))\n        return (f\"{self._format_type()} of type code {type_code}, \"\n                f\"value {deref(eval_values[child_id])}\")\n\n\n\nclass DenseUnionScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::UnionScalar and subclasses.\n    \"\"\"\n\n    def to_string(self):\n        type_code = self.val['type_code'].cast(gdb.lookup_type('int'))\n        if not self.is_valid:\n            return (f\"{self._format_type()} of type {self.type}, \"\n                    f\"type code {type_code}, null value\")\n        value = deref(self.val['value'])\n        return (f\"{self._format_type()} of type code {type_code}, \"\n                f\"value {value}\")\n\n\nclass MapScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::MapScalar.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n\n        array = deref(self.val['value'])\n        data = deref(array['data_'])\n        data_printer = ArrayDataPrinter(\"arrow::ArrayData\", data)\n        return (f\"{self._format_type()} of type {self.type}, \"\n                f\"value {data_printer._format_contents()}\")\n\n\nclass ExtensionScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::ExtensionScalar.\n    \"\"\"\n\n    def to_string(self):\n        ext_type = ExtensionType(self.type)\n        if not self.is_valid:\n            return (f\"{self._format_type()} of type \"\n                    f\"{ext_type.to_string().string_literal()}, null value\")\n        value = deref(self.val['value'])\n        return (f\"{self._format_type()} of type \"\n                f\"{ext_type.to_string().string_literal()}, value {value}\")\n\n\nclass ArrayDataPrinter:\n    \"\"\"\n    Pretty-printer for arrow::ArrayData.\n    \"\"\"\n\n    def __new__(cls, name, val):\n        # Lookup actual (derived) class to instantiate\n        type_id = int(deref(val['type'])['id_'])\n        type_class = lookup_type_class(type_id)\n        if type_class is not None:\n            cls = type_class.array_data_printer\n            assert issubclass(cls, ArrayDataPrinter)\n        self = object.__new__(cls)\n        self.name = name\n        self.val = val\n        self.type_class = type_class\n        self.type_name = type_class.name\n        self.type_id = type_id\n        self.offset = int(self.val['offset'])\n        self.length = int(self.val['length'])\n        return self\n\n    @property\n    def type(self):\n        \"\"\"\n        The concrete DataTypeClass instance.\n        \"\"\"\n        concrete_type = gdb.lookup_type(f\"arrow::{self.type_name}\")\n        return cast_to_concrete(deref(self.val['type']), concrete_type)\n\n    def _format_contents(self):\n        return (f\"length {self.length}, \"\n                f\"offset {self.offset}, \"\n                f\"{format_null_count(self.val['null_count'])}\")\n\n    def _buffer(self, index, type_id=None):\n        buffers = StdVector(self.val['buffers'])\n        bufptr = SharedPtr(buffers[index]).get()\n        if int(bufptr) == 0:\n            return None\n        if type_id is not None:\n            return TypedBuffer.from_type_id(bufptr.dereference(), type_id)\n        else:\n            return Buffer(bufptr.dereference())\n\n    def _buffer_values(self, index, type_id, length=None):\n        \"\"\"\n        Return a typed view of values in the buffer with the given index.\n\n        Values are returned as tuples since some types may decode to\n        multiple values (for example day_time_interval).\n        \"\"\"\n        buf = self._buffer(index, type_id)\n        if buf is None:\n            return None\n        if length is None:\n            length = self.length\n        return buf.view(self.offset, length)\n\n    def _unpacked_buffer_values(self, index, type_id, length=None):\n        \"\"\"\n        Like _buffer_values(), but assumes values are 1-tuples\n        and returns them unpacked.\n        \"\"\"\n        return StarMappedView(identity,\n                              self._buffer_values(index, type_id, length))\n\n    def _null_bitmap(self):\n        buf = self._buffer(0) if has_null_bitmap(self.type_id) else None\n        return NullBitmap.from_buffer(buf, self.offset, self.length)\n\n    def _null_child(self, i):\n        return str(i), \"null\"\n\n    def _valid_child(self, i, value):\n        return str(i), value\n\n    def display_hint(self):\n        return None\n\n    def children(self):\n        return ()\n\n    def to_string(self):\n        ty = self.type\n        return (f\"{self.name} of type {ty}, \"\n                f\"{self._format_contents()}\")\n\n\nclass NumericArrayDataPrinter(ArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for numeric data types.\n    \"\"\"\n    _format_value = staticmethod(identity)\n\n    def _values_view(self):\n        return StarMappedView(self._format_value,\n                              self._buffer_values(1, self.type_id))\n\n    def display_hint(self):\n        return \"array\"\n\n    def children(self):\n        if self.length == 0:\n            return\n        values = self._values_view()\n        null_bits = self._null_bitmap()\n        for i, (valid, value) in enumerate(zip(null_bits, values)):\n            if valid:\n                yield self._valid_child(i, str(value))\n            else:\n                yield self._null_child(i)\n\n\nclass BooleanArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for boolean.\n    \"\"\"\n\n    def _format_value(self, v):\n        return str(v).lower()\n\n    def _values_view(self):\n        return MappedView(self._format_value,\n                          self._buffer_values(1, self.type_id))\n\n\nclass Date32ArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for date32.\n    \"\"\"\n    _format_value = staticmethod(format_date32)\n\n\nclass Date64ArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for date64.\n    \"\"\"\n    _format_value = staticmethod(format_date64)\n\n\nclass TimeArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for time32 and time64.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.unit = self.type['unit_']\n        self.unit_string = short_time_unit(self.unit)\n\n    def _format_value(self, val):\n        return f\"{val}{self.unit_string}\"\n\n\nclass TimestampArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for timestamp.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.unit = self.type['unit_']\n\n    def _format_value(self, val):\n        return format_timestamp(val, self.unit)\n\n\nclass MonthIntervalArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for month_interval.\n    \"\"\"\n    _format_value = staticmethod(format_month_interval)\n\n\nclass DayTimeIntervalArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for day_time_interval.\n    \"\"\"\n    _format_value = staticmethod(format_days_milliseconds)\n\n\nclass MonthDayNanoIntervalArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for day_time_interval.\n    \"\"\"\n    _format_value = staticmethod(format_months_days_nanos)\n\n\nclass DecimalArrayDataPrinter(ArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for decimals.\n    \"\"\"\n\n    def __init__(self, name, val):\n        ty = self.type\n        self.precision = int(ty['precision_'])\n        self.scale = int(ty['scale_'])\n        self.decimal_class = decimal_type_to_class[self.type_name]\n        self.byte_width = self.decimal_class.traits.bit_width // 8\n\n    def display_hint(self):\n        return \"array\"\n\n    def children(self):\n        if self.length == 0:\n            return\n        null_bits = self._null_bitmap()\n        address = self._buffer(1).data + self.offset * self.byte_width\n        for i, valid in enumerate(null_bits):\n            if valid:\n                dec = self.decimal_class.from_address(address)\n                yield self._valid_child(\n                    i, dec.format(self.precision, self.scale))\n            else:\n                yield self._null_child(i)\n            address += self.byte_width\n\n\nclass FixedSizeBinaryArrayDataPrinter(ArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for fixed_size_binary.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.byte_width = self.type['byte_width_']\n\n    def display_hint(self):\n        return \"array\"\n\n    def children(self):\n        if self.length == 0:\n            return\n        null_bits = self._null_bitmap()\n        address = self._buffer(1).data + self.offset * self.byte_width\n        for i, valid in enumerate(null_bits):\n            if valid:\n                if self.byte_width:\n                    yield self._valid_child(\n                        i, bytes_literal(address, self.byte_width))\n                else:\n                    yield self._valid_child(i, '\"\"')\n            else:\n                yield self._null_child(i)\n            address += self.byte_width\n\n\nclass BinaryArrayDataPrinter(ArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for variable-sized binary.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.is_large = self.type_id in (Type.LARGE_BINARY, Type.LARGE_STRING)\n        self.is_utf8 = self.type_id in (Type.STRING, Type.LARGE_STRING)\n        self.format_string = utf8_literal if self.is_utf8 else bytes_literal\n\n    def display_hint(self):\n        return \"array\"\n\n    def children(self):\n        if self.length == 0:\n            return\n        null_bits = self._null_bitmap()\n        offsets = self._unpacked_buffer_values(\n            1, Type.INT64 if self.is_large else Type.INT32,\n            length=self.length + 1)\n        values = self._buffer(2).data\n        for i, valid in enumerate(null_bits):\n            if valid:\n                start = offsets[i]\n                size = offsets[i + 1] - start\n                if size:\n                    yield self._valid_child(\n                        i, self.format_string(values + start, size))\n                else:\n                    yield self._valid_child(i, '\"\"')\n            else:\n                yield self._null_child(i)\n\n\nclass ArrayPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Array and subclasses.\n    \"\"\"\n\n    def __init__(self, val):\n        data = deref(val['data_'])\n        self.data_printer = ArrayDataPrinter(\"arrow::ArrayData\", data)\n        self.name = array_class_from_type(self.data_printer.type_name)\n\n    def _format_contents(self):\n        return self.data_printer._format_contents()\n\n    def to_string(self):\n        if self.data_printer.type_class.is_parametric:\n            ty = self.data_printer.type\n            return f\"arrow::{self.name} of type {ty}, {self._format_contents()}\"\n        else:\n            return f\"arrow::{self.name} of {self._format_contents()}\"\n\n    def display_hint(self):\n        return self.data_printer.display_hint()\n\n    def children(self):\n        return self.data_printer.children()\n\n\nclass ChunkedArrayPrinter:\n    \"\"\"\n    Pretty-printer for arrow::ChunkedArray.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.name = name\n        self.val = val\n        self.chunks = StdVector(self.val['chunks_'])\n\n    def display_hint(self):\n        return \"array\"\n\n    def children(self):\n        for i, chunk in enumerate(self.chunks):\n            printer = ArrayPrinter(deref(chunk))\n            yield str(i), printer._format_contents()\n\n    def to_string(self):\n        ty = deref(self.val['type_'])\n        return (f\"{self.name} of type {ty}, length {self.val['length_']}, \"\n                f\"{format_null_count(self.val['null_count_'])} \"\n                f\"with {len(self.chunks)} chunks\")\n\n\nclass DataTypeClass:\n    array_data_printer = ArrayDataPrinter\n\n    def __init__(self, name):\n        self.name = name\n\n\nclass NullTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = NullScalarPrinter\n\n\nclass NumericTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = NumericScalarPrinter\n    array_data_printer = NumericArrayDataPrinter\n\n\nclass BooleanTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = NumericScalarPrinter\n    array_data_printer = BooleanArrayDataPrinter\n\n\nclass Date32TypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = Date32ScalarPrinter\n    array_data_printer = Date32ArrayDataPrinter\n\n\nclass Date64TypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = Date64ScalarPrinter\n    array_data_printer = Date64ArrayDataPrinter\n\n\nclass TimeTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = TimeTypePrinter\n    scalar_printer = TimeScalarPrinter\n    array_data_printer = TimeArrayDataPrinter\n\n\nclass TimestampTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = TimestampTypePrinter\n    scalar_printer = TimestampScalarPrinter\n    array_data_printer = TimestampArrayDataPrinter\n\n\nclass DurationTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = TimeTypePrinter\n    scalar_printer = TimeScalarPrinter\n    array_data_printer = TimeArrayDataPrinter\n\n\nclass MonthIntervalTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = MonthIntervalScalarPrinter\n    array_data_printer = MonthIntervalArrayDataPrinter\n\n\nclass DayTimeIntervalTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = NumericScalarPrinter\n    array_data_printer = DayTimeIntervalArrayDataPrinter\n\n\nclass MonthDayNanoIntervalTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = NumericScalarPrinter\n    array_data_printer = MonthDayNanoIntervalArrayDataPrinter\n\n\nclass DecimalTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = DecimalTypePrinter\n    scalar_printer = DecimalScalarPrinter\n    array_data_printer = DecimalArrayDataPrinter\n\n\nclass BaseBinaryTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = BaseBinaryScalarPrinter\n    array_data_printer = BinaryArrayDataPrinter\n\n\nclass FixedSizeBinaryTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = FixedSizeBinaryTypePrinter\n    scalar_printer = FixedSizeBinaryScalarPrinter\n    array_data_printer = FixedSizeBinaryArrayDataPrinter\n\n\nclass BaseListTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = ListTypePrinter\n    scalar_printer = BaseListScalarPrinter\n\n\nclass FixedSizeListTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = FixedSizeListTypePrinter\n    scalar_printer = BaseListScalarPrinter\n\n\nclass MapTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = MapTypePrinter\n    scalar_printer = MapScalarPrinter\n\n\nclass StructTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = StructTypePrinter\n    scalar_printer = StructScalarPrinter\n\n\nclass DenseUnionTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = UnionTypePrinter\n    scalar_printer = DenseUnionScalarPrinter\n\n\nclass SparseUnionTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = UnionTypePrinter\n    scalar_printer = SparseUnionScalarPrinter\n\n\nclass DictionaryTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = DictionaryTypePrinter\n    scalar_printer = DictionaryScalarPrinter\n\n\nclass ExtensionTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = ExtensionTypePrinter\n    scalar_printer = ExtensionScalarPrinter\n\n\nDataTypeTraits = namedtuple('DataTypeTraits', ('factory', 'name'))\n\n\ntype_traits_by_id = {\n    Type.NA: DataTypeTraits(NullTypeClass, 'NullType'),\n\n    Type.BOOL: DataTypeTraits(BooleanTypeClass, 'BooleanType'),\n\n    Type.UINT8: DataTypeTraits(NumericTypeClass, 'UInt8Type'),\n    Type.INT8: DataTypeTraits(NumericTypeClass, 'Int8Type'),\n    Type.UINT16: DataTypeTraits(NumericTypeClass, 'UInt16Type'),\n    Type.INT16: DataTypeTraits(NumericTypeClass, 'Int16Type'),\n    Type.UINT32: DataTypeTraits(NumericTypeClass, 'UInt32Type'),\n    Type.INT32: DataTypeTraits(NumericTypeClass, 'Int32Type'),\n    Type.UINT64: DataTypeTraits(NumericTypeClass, 'UInt64Type'),\n    Type.INT64: DataTypeTraits(NumericTypeClass, 'Int64Type'),\n    Type.HALF_FLOAT: DataTypeTraits(NumericTypeClass, 'HalfFloatType'),\n    Type.FLOAT: DataTypeTraits(NumericTypeClass, 'FloatType'),\n    Type.DOUBLE: DataTypeTraits(NumericTypeClass, 'DoubleType'),\n\n    Type.STRING: DataTypeTraits(BaseBinaryTypeClass, 'StringType'),\n    Type.BINARY: DataTypeTraits(BaseBinaryTypeClass, 'BinaryType'),\n    Type.LARGE_STRING: DataTypeTraits(BaseBinaryTypeClass, 'LargeStringType'),\n    Type.LARGE_BINARY: DataTypeTraits(BaseBinaryTypeClass, 'LargeBinaryType'),\n\n    Type.FIXED_SIZE_BINARY: DataTypeTraits(FixedSizeBinaryTypeClass,\n                                           'FixedSizeBinaryType'),\n\n    Type.DATE32: DataTypeTraits(Date32TypeClass, 'Date32Type'),\n    Type.DATE64: DataTypeTraits(Date64TypeClass, 'Date64Type'),\n    Type.TIMESTAMP: DataTypeTraits(TimestampTypeClass, 'TimestampType'),\n    Type.TIME32: DataTypeTraits(TimeTypeClass, 'Time32Type'),\n    Type.TIME64: DataTypeTraits(TimeTypeClass, 'Time64Type'),\n    Type.DURATION: DataTypeTraits(DurationTypeClass, 'DurationType'),\n    Type.INTERVAL_MONTHS: DataTypeTraits(MonthIntervalTypeClass,\n                                         'MonthIntervalType'),\n    Type.INTERVAL_DAY_TIME: DataTypeTraits(DayTimeIntervalTypeClass,\n                                           'DayTimeIntervalType'),\n    Type.INTERVAL_MONTH_DAY_NANO: DataTypeTraits(MonthDayNanoIntervalTypeClass,\n                                                 'MonthDayNanoIntervalType'),\n\n    Type.DECIMAL128: DataTypeTraits(DecimalTypeClass, 'Decimal128Type'),\n    Type.DECIMAL256: DataTypeTraits(DecimalTypeClass, 'Decimal256Type'),\n\n    Type.LIST: DataTypeTraits(BaseListTypeClass, 'ListType'),\n    Type.LARGE_LIST: DataTypeTraits(BaseListTypeClass, 'LargeListType'),\n    Type.FIXED_SIZE_LIST: DataTypeTraits(FixedSizeListTypeClass,\n                                         'FixedSizeListType'),\n    Type.MAP: DataTypeTraits(MapTypeClass, 'MapType'),\n\n    Type.STRUCT: DataTypeTraits(StructTypeClass, 'StructType'),\n    Type.SPARSE_UNION: DataTypeTraits(SparseUnionTypeClass, 'SparseUnionType'),\n    Type.DENSE_UNION: DataTypeTraits(DenseUnionTypeClass, 'DenseUnionType'),\n\n    Type.DICTIONARY: DataTypeTraits(DictionaryTypeClass, 'DictionaryType'),\n    Type.EXTENSION: DataTypeTraits(ExtensionTypeClass, 'ExtensionType'),\n}\n\nmax_type_id = len(type_traits_by_id) - 1\n\n\ndef lookup_type_class(type_id):\n    \"\"\"\n    Lookup a type class (an instance of DataTypeClass) by its type id.\n    \"\"\"\n    traits = type_traits_by_id.get(type_id)\n    if traits is not None:\n        return traits.factory(traits.name)\n    return None\n\n\nclass StatusPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Status.\n    \"\"\"\n    _status_codes_by_id = {\n        0: 'OK',\n        1: 'OutOfMemory',\n        2: 'KeyError',\n        3: 'TypeError',\n        4: 'Invalid',\n        5: 'IOError',\n        6: 'CapacityError',\n        7: 'IndexError',\n        8: 'Cancelled',\n        9: 'UnknownError',\n        10: 'NotImplemented',\n        11: 'SerializationError',\n        13: 'RError',\n        40: 'CodeGenError',\n        41: 'ExpressionValidationError',\n        42: 'ExecutionError',\n        45: 'AlreadyExists',\n    }\n\n    def __init__(self, name, val):\n        self.val = val\n\n    def _format_detail(self, state):\n        detail_ptr = SharedPtr(state['detail']).get()\n        if int(detail_ptr) == 0:\n            return None\n        detail_id = CString(gdb.parse_and_eval(\n            f\"{for_evaluation(detail_ptr)}->type_id()\"))\n        # Cannot use StdString as ToString() returns a rvalue\n        detail_msg = CString(gdb.parse_and_eval(\n            f\"{for_evaluation(detail_ptr)}->ToString().c_str()\"))\n        return f\"[{detail_id.string()}] {detail_msg.string_literal()}\"\n\n    def _format_error(self, state):\n        code = int(state['code'])\n        codename = self._status_codes_by_id.get(code)\n        if codename is not None:\n            s = f\"arrow::Status::{codename}(\"\n        else:\n            s = f\"arrow::Status(<unknown code {code}>, \"\n        s += StdString(state['msg']).string_literal()\n        detail_msg = self._format_detail(state)\n        if detail_msg is not None:\n            return s + f\", detail={detail_msg})\"\n        else:\n            return s + \")\"\n\n    def to_string(self):\n        state_ptr = self.val['state_']\n        if int(state_ptr) == 0:\n            return \"arrow::Status::OK()\"\n        return self._format_error(state_ptr.dereference())\n\n\nclass ResultPrinter(StatusPrinter):\n    \"\"\"\n    Pretty-printer for arrow::Result<T>.\n    \"\"\"\n\n    def to_string(self):\n        data_type = self.val.type.template_argument(0)\n        state_ptr = self.val['status_']['state_']\n        if int(state_ptr) != 0:\n            inner = self._format_error(state_ptr)\n        else:\n            data_ptr = self.val['storage_']['data_'].address\n            assert data_ptr\n            inner = data_ptr.reinterpret_cast(\n                data_type.pointer()).dereference()\n        return f\"arrow::Result<{data_type}>({inner})\"\n\n\nclass FieldPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Field.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n\n    def to_string(self):\n        f = Field(self.val)\n        nullable = f.nullable\n        if nullable:\n            return f'arrow::field({f.name}, {f.type})'\n        else:\n            return f'arrow::field({f.name}, {f.type}, nullable=false)'\n\n\nclass MetadataPrinter:\n    \"\"\"\n    Pretty-printer for arrow::KeyValueMetadata.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n        self.metadata = Metadata(self.val)\n\n    def display_hint(self):\n        return 'map'\n\n    def children(self):\n        for k, v in self.metadata:\n            yield (\"key\", k.bytes_literal())\n            yield (\"value\", v.bytes_literal())\n\n    def to_string(self):\n        return f\"arrow::KeyValueMetadata of size {len(self.metadata)}\"\n\n\nclass SchemaPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Schema.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n        self.schema = Schema(val)\n        # TODO endianness\n\n    def display_hint(self):\n        return 'map'\n\n    def children(self):\n        for field in self.schema.fields:\n            yield (\"name\", field.name.string_literal())\n            yield (\"type\", field.type)\n\n    def to_string(self):\n        num_fields = len(self.schema.fields)\n        md_items = len(self.schema.metadata)\n        if md_items > 0:\n            return (f\"arrow::Schema with {num_fields} fields \"\n                    f\"and {md_items} metadata items\")\n        else:\n            return f\"arrow::Schema with {num_fields} fields\"\n\n\nclass BaseColumnarPrinter:\n\n    def __init__(self, name, val, columnar):\n        self.name = name\n        self.val = val\n        self.columnar = columnar\n        self.schema = self.columnar.schema\n\n    def display_hint(self):\n        return 'map'\n\n    def children(self):\n        for field, col in zip(self.schema.fields,\n                              self.columnar.columns):\n            yield (\"name\", field.name.string_literal())\n            yield (\"value\", col)\n\n    def to_string(self):\n        num_fields = len(self.schema.fields)\n        num_rows = self.columnar.num_rows\n        md_items = len(self.schema.metadata)\n        if md_items > 0:\n            return (f\"arrow::{self.name} with {num_fields} columns, \"\n                    f\"{num_rows} rows, {md_items} metadata items\")\n        else:\n            return (f\"arrow::{self.name} with {num_fields} columns, \"\n                    f\"{num_rows} rows\")\n\n\nclass RecordBatchPrinter(BaseColumnarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::RecordBatch.\n    \"\"\"\n\n    def __init__(self, name, val):\n        BaseColumnarPrinter.__init__(self, \"RecordBatch\", val, RecordBatch(val))\n\n\nclass TablePrinter(BaseColumnarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::Table.\n    \"\"\"\n\n    def __init__(self, name, val):\n        BaseColumnarPrinter.__init__(self, \"Table\", val, Table(val))\n\n\nclass DatumPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Datum.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n        self.variant = Variant(val['value'])\n\n    def to_string(self):\n        if self.variant.index == 0:\n            # Datum::NONE\n            return \"arrow::Datum (empty)\"\n        if self.variant.value_type is None:\n            return \"arrow::Datum (uninitialized or corrupt?)\"\n        # All non-empty Datums contain a shared_ptr<T>\n        value = deref(self.variant.value)\n        return f\"arrow::Datum of value {value}\"\n\n\nclass BufferPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Buffer and subclasses.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.name = name\n        self.val = val\n\n    def to_string(self):\n        if bool(self.val['is_mutable_']):\n            mutable = 'mutable'\n        else:\n            mutable = 'read-only'\n        size = int(self.val['size_'])\n        if size == 0:\n            return f\"arrow::{self.name} of size 0, {mutable}\"\n        if not self.val['is_cpu_']:\n            return f\"arrow::{self.name} of size {size}, {mutable}, not on CPU\"\n        data = bytes_literal(self.val['data_'], size)\n        return f\"arrow::{self.name} of size {size}, {mutable}, {data}\"\n\n\nclass DayMillisecondsPrinter:\n    \"\"\"\n    Pretty-printer for arrow::DayTimeIntervalType::DayMilliseconds.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n\n    def to_string(self):\n        return format_days_milliseconds(self.val['days'],\n                                        self.val['milliseconds'])\n\n\nclass MonthDayNanosPrinter:\n    \"\"\"\n    Pretty-printer for arrow::MonthDayNanoIntervalType::MonthDayNanos.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n\n    def to_string(self):\n        return format_months_days_nanos(self.val['months'],\n                                        self.val['days'],\n                                        self.val['nanoseconds'])\n\n\nclass DecimalPrinter:\n    \"\"\"\n    Pretty-printer for Arrow decimal values.\n    \"\"\"\n\n    def __init__(self, bit_width, name, val):\n        self.name = name\n        self.val = val\n        self.bit_width = bit_width\n\n    def to_string(self):\n        dec = decimal_bits_to_class[self.bit_width].from_value(self.val)\n        return f\"{self.name}({int(dec)})\"\n\n\nprinters = {\n    \"arrow::ArrayData\": ArrayDataPrinter,\n    \"arrow::BasicDecimal128\": partial(DecimalPrinter, 128),\n    \"arrow::BasicDecimal256\": partial(DecimalPrinter, 256),\n    \"arrow::ChunkedArray\": ChunkedArrayPrinter,\n    \"arrow::Datum\": DatumPrinter,\n    \"arrow::DayTimeIntervalType::DayMilliseconds\": DayMillisecondsPrinter,\n    \"arrow::Decimal128\": partial(DecimalPrinter, 128),\n    \"arrow::Decimal256\": partial(DecimalPrinter, 256),\n    \"arrow::MonthDayNanoIntervalType::MonthDayNanos\": MonthDayNanosPrinter,\n    \"arrow::Field\": FieldPrinter,\n    \"arrow::KeyValueMetadata\": MetadataPrinter,\n    \"arrow::RecordBatch\": RecordBatchPrinter,\n    \"arrow::Result\": ResultPrinter,\n    \"arrow::Schema\": SchemaPrinter,\n    \"arrow::SimpleRecordBatch\": RecordBatchPrinter,\n    \"arrow::SimpleTable\": TablePrinter,\n    \"arrow::Status\": StatusPrinter,\n    \"arrow::Table\": TablePrinter,\n}\n\n\ndef arrow_pretty_print(val):\n    name = val.type.strip_typedefs().name\n    if name is None:\n        return\n    name = name.partition('<')[0]  # Remove template parameters\n    printer = printers.get(name)\n    if printer is not None:\n        return printer(name, val)\n\n    if not name.startswith(\"arrow::\"):\n        return\n    arrow_name = name[len(\"arrow::\"):]\n\n    if arrow_name.endswith(\"Buffer\"):\n        try:\n            val['data_']\n        except Exception:\n            # Not a Buffer?\n            pass\n        else:\n            return BufferPrinter(arrow_name, val)\n\n    elif arrow_name.endswith(\"Type\"):\n        # Look up dynamic type, as it may be hidden behind a DataTypeClass\n        # pointer or reference.\n        try:\n            type_id = int(val['id_'])\n        except Exception:\n            # Not a DataTypeClass?\n            pass\n        else:\n            type_class = lookup_type_class(type_id)\n            if type_class is not None:\n                return type_class.type_printer(type_class.name, val)\n\n    elif arrow_name.endswith(\"Array\"):\n        return ArrayPrinter(val)\n\n    elif arrow_name.endswith(\"Scalar\"):\n        try:\n            val['is_valid']\n        except Exception:\n            # Not a Scalar?\n            pass\n        else:\n            return ScalarPrinter(val)\n\n\ndef main():\n    # This pattern allows for two modes of use:\n    # - manual loading using `source gdb-arrow.py`: current_objfile()\n    #   will be None;\n    # - automatic loading from the GDB `scripts-directory`: current_objfile()\n    #   will be tied to the inferior being debugged.\n    objfile = gdb.current_objfile()\n    if objfile is None:\n        objfile = gdb\n\n    objfile.pretty_printers.append(arrow_pretty_print)\n\n\nif __name__ == '__main__':\n    main()\n", "cpp/tools/binary_symbol_explore.py": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport subprocess as sbp\nimport sys\n\ntry:\n    import pandas as pd\n    HAVE_PANDAS = True\nexcept ImportError:\n    HAVE_PANDAS = False\n\nSYMBOL_FILTERS = {\n    'std::chrono::duration': 'duration',\n    'std::__cxx11::basic_string': 'std::string',\n    'arrow::ArrayData': 'ArrayData',\n    'arrow::ArraySpan': 'ArraySpan',\n    'arrow::Datum': 'Datum',\n    'arrow::Scalar': 'Scalar',\n    'arrow::Status': 'Status',\n    'arrow::Type': 'Type',\n    'arrow::TimestampType': 'TsT',\n    'arrow::BinaryType': 'BinaryT',\n    'arrow::BooleanType': 'BoolT',\n    'arrow::StringType': 'StringT',\n    'arrow::LargeStringType': 'LStringT',\n    'arrow::DoubleType': 'DoubleT',\n    'arrow::FloatType': 'FloatT',\n    'arrow::Int64Type': 'Int64T',\n    'arrow::UInt64Type': 'UInt64T',\n    'arrow::LargeListType': 'LListT',\n    'arrow::ListType': 'ListT',\n    'arrow::FixedSizeListType': 'FSLT',\n    'arrow::compute::': 'ac::',\n    'ac::internal::': '',\n    'arrow::internal::': 'ai::',\n    '(anonymous namespace)::': '',\n    'internal::applicator::': '',\n    'internal::CastFunctor': 'CastFunctor',\n    'ac::KernelContext*': 'C*',\n    'ArrayData const&': 'A&',\n    'ArraySpan const&': 'A&',\n    'ArrayData*': 'O*',\n    'Scalar const&': 'S&',\n    'Datum const&': 'V&',\n    'Datum*': 'O*',\n    'ac::ExecBatch const&': 'B&',\n    'ac::ExecSpan const&': 'B&',\n    'ac::ExecValue const&': 'V&',\n    'ac::ExecResult*': 'O*',\n    'Type::type': 'T',\n}\n\n\ndef filter_symbol(symbol_name):\n    for token, replacement in SYMBOL_FILTERS.items():\n        symbol_name = symbol_name.replace(token, replacement)\n    return symbol_name\n\n\ndef get_symbols_and_sizes(object_file):\n    cmd = f\"nm --print-size --size-sort {object_file} | c++filt\"\n    output = sbp.check_output(cmd, shell=True).decode('utf-8')\n    symbol_sizes = []\n    for x in output.split('\\n'):\n        if len(x) == 0:\n            continue\n        _, hex_size, _, symbol_name = x.split(' ', 3)\n        symbol_name = filter_symbol(symbol_name)\n        symbol_sizes.append((symbol_name, int(hex_size, 16)))\n    return dict(symbol_sizes)\n\n\nif __name__ == '__main__':\n    base, contender = sys.argv[1], sys.argv[2]\n\n    base_results = get_symbols_and_sizes(base)\n    contender_results = get_symbols_and_sizes(contender)\n\n    all_symbols = set(base_results.keys()) | set(contender_results.keys())\n\n    diff_table = []\n    for name in all_symbols:\n        if name in base_results and name in contender_results:\n            base_size = base_results[name]\n            contender_size = contender_results[name]\n        elif name in base_results:\n            base_size = base_results[name]\n            contender_size = 0\n        else:\n            base_size = 0\n            contender_size = contender_results[name]\n        diff = contender_size - base_size\n        diff_table.append((name, base_size, contender_size, diff))\n    diff_table.sort(key=lambda x: x[3])\n\n    if HAVE_PANDAS:\n        diff = pd.DataFrame.from_records(diff_table,\n                                         columns=['symbol', 'base',\n                                                  'contender', 'diff'])\n        pd.options.display.max_rows = 1000\n        pd.options.display.max_colwidth = 150\n        print(diff[diff['diff'] < - 700])\n        print(diff[diff['diff'] > 700])\n    else:\n        # TODO\n        pass\n", "cpp/build-support/run_clang_format.py": "#!/usr/bin/env python3\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import print_function\nimport lintutils\nfrom subprocess import PIPE\nimport argparse\nimport difflib\nimport multiprocessing as mp\nimport sys\nfrom functools import partial\n\n\n# examine the output of clang-format and if changes are\n# present assemble a (unified)patch of the difference\ndef _check_one_file(filename, formatted):\n    with open(filename, \"rb\") as reader:\n        original = reader.read()\n\n    if formatted != original:\n        # Run the equivalent of diff -u\n        diff = list(difflib.unified_diff(\n            original.decode('utf8').splitlines(True),\n            formatted.decode('utf8').splitlines(True),\n            fromfile=filename,\n            tofile=\"{} (after clang format)\".format(\n                filename)))\n    else:\n        diff = None\n\n    return filename, diff\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Runs clang-format on all of the source \"\n        \"files. If --fix is specified enforce format by \"\n        \"modifying in place, otherwise compare the output \"\n        \"with the existing file and output any necessary \"\n        \"changes as a patch in unified diff format\")\n    parser.add_argument(\"--clang_format_binary\",\n                        required=True,\n                        help=\"Path to the clang-format binary\")\n    parser.add_argument(\"--exclude_globs\",\n                        help=\"Filename containing globs for files \"\n                        \"that should be excluded from the checks\")\n    parser.add_argument(\"--source_dir\",\n                        required=True,\n                        action=\"append\",\n                        help=\"Root directory of the source code\")\n    parser.add_argument(\"--fix\", default=False,\n                        action=\"store_true\",\n                        help=\"If specified, will re-format the source \"\n                        \"code instead of comparing the re-formatted \"\n                        \"output, defaults to %(default)s\")\n    parser.add_argument(\"--quiet\", default=False,\n                        action=\"store_true\",\n                        help=\"If specified, only print errors\")\n    arguments = parser.parse_args()\n\n    exclude_globs = []\n    if arguments.exclude_globs:\n        with open(arguments.exclude_globs) as f:\n            exclude_globs.extend(line.strip() for line in f)\n\n    formatted_filenames = []\n    for source_dir in arguments.source_dir:\n        for path in lintutils.get_sources(source_dir, exclude_globs):\n            formatted_filenames.append(str(path))\n\n    if arguments.fix:\n        if not arguments.quiet:\n            print(\"\\n\".join(map(lambda x: \"Formatting {}\".format(x),\n                                formatted_filenames)))\n\n        # Break clang-format invocations into chunks: each invocation formats\n        # 16 files. Wait for all processes to complete\n        results = lintutils.run_parallel([\n            [arguments.clang_format_binary, \"-i\"] + some\n            for some in lintutils.chunk(formatted_filenames, 16)\n        ])\n        for returncode, stdout, stderr in results:\n            # if any clang-format reported a parse error, bubble it\n            if returncode != 0:\n                sys.exit(returncode)\n\n    else:\n        # run an instance of clang-format for each source file in parallel,\n        # then wait for all processes to complete\n        results = lintutils.run_parallel([\n            [arguments.clang_format_binary, filename]\n            for filename in formatted_filenames\n        ], stdout=PIPE, stderr=PIPE)\n\n        checker_args = []\n        for filename, res in zip(formatted_filenames, results):\n            # if any clang-format reported a parse error, bubble it\n            returncode, stdout, stderr = res\n            if returncode != 0:\n                print(stderr)\n                sys.exit(returncode)\n            checker_args.append((filename, stdout))\n\n        error = False\n        pool = mp.Pool()\n        try:\n            # check the output from each invocation of clang-format in parallel\n            for filename, diff in pool.starmap(_check_one_file, checker_args):\n                if not arguments.quiet:\n                    print(\"Checking {}\".format(filename))\n                if diff:\n                    print(\"{} had clang-format style issues\".format(filename))\n                    # Print out the diff to stderr\n                    error = True\n                    # pad with a newline\n                    print(file=sys.stderr)\n                    sys.stderr.writelines(diff)\n        except Exception:\n            error = True\n            raise\n        finally:\n            pool.terminate()\n            pool.join()\n        sys.exit(1 if error else 0)\n", "cpp/build-support/cpplint.py": "#!/usr/bin/env python3\n#\n# Copyright (c) 2009 Google Inc. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#    * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#    * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"Does google-lint on c++ files.\n\nThe goal of this script is to identify places in the code that *may*\nbe in non-compliance with google style.  It does not attempt to fix\nup these problems -- the point is to educate.  It does also not\nattempt to find all problems, or to ensure that everything it does\nfind is legitimately a problem.\n\nIn particular, we can get very confused by /* and // inside strings!\nWe do a small hack, which is to ignore //'s with \"'s after them on the\nsame line, but it is far from perfect (in either direction).\n\"\"\"\n\n# cpplint predates fstrings\n# pylint: disable=consider-using-f-string\n\n# pylint: disable=invalid-name\n\nimport codecs\nimport copy\nimport getopt\nimport glob\nimport itertools\nimport math  # for log\nimport os\nimport re\nimport sre_compile\nimport string\nimport sys\nimport sysconfig\nimport unicodedata\nimport xml.etree.ElementTree\n\n# if empty, use defaults\n_valid_extensions = set([])\n\n__VERSION__ = '1.6.1'\n\ntry:\n  #  -- pylint: disable=used-before-assignment\n  xrange          # Python 2\nexcept NameError:\n  #  -- pylint: disable=redefined-builtin\n  xrange = range  # Python 3\n\n\n_USAGE = \"\"\"\nSyntax: cpplint.py [--verbose=#] [--output=emacs|eclipse|vs7|junit|sed|gsed]\n                   [--filter=-x,+y,...]\n                   [--counting=total|toplevel|detailed] [--root=subdir]\n                   [--repository=path]\n                   [--linelength=digits] [--headers=x,y,...]\n                   [--recursive]\n                   [--exclude=path]\n                   [--extensions=hpp,cpp,...]\n                   [--includeorder=default|standardcfirst]\n                   [--quiet]\n                   [--version]\n        <file> [file] ...\n\n  Style checker for C/C++ source files.\n  This is a fork of the Google style checker with minor extensions.\n\n  The style guidelines this tries to follow are those in\n    https://google.github.io/styleguide/cppguide.html\n\n  Every problem is given a confidence score from 1-5, with 5 meaning we are\n  certain of the problem, and 1 meaning it could be a legitimate construct.\n  This will miss some errors, and is not a substitute for a code review.\n\n  To suppress false-positive errors of a certain category, add a\n  'NOLINT(category)' comment to the line.  NOLINT or NOLINT(*)\n  suppresses errors of all categories on that line.\n\n  The files passed in will be linted; at least one file must be provided.\n  Default linted extensions are %s.\n  Other file types will be ignored.\n  Change the extensions with the --extensions flag.\n\n  Flags:\n\n    output=emacs|eclipse|vs7|junit|sed|gsed\n      By default, the output is formatted to ease emacs parsing.  Visual Studio\n      compatible output (vs7) may also be used.  Further support exists for\n      eclipse (eclipse), and JUnit (junit). XML parsers such as those used\n      in Jenkins and Bamboo may also be used.\n      The sed format outputs sed commands that should fix some of the errors.\n      Note that this requires gnu sed. If that is installed as gsed on your\n      system (common e.g. on macOS with homebrew) you can use the gsed output\n      format. Sed commands are written to stdout, not stderr, so you should be\n      able to pipe output straight to a shell to run the fixes.\n\n    verbose=#\n      Specify a number 0-5 to restrict errors to certain verbosity levels.\n      Errors with lower verbosity levels have lower confidence and are more\n      likely to be false positives.\n\n    quiet\n      Don't print anything if no errors are found.\n\n    filter=-x,+y,...\n      Specify a comma-separated list of category-filters to apply: only\n      error messages whose category names pass the filters will be printed.\n      (Category names are printed with the message and look like\n      \"[whitespace/indent]\".)  Filters are evaluated left to right.\n      \"-FOO\" means \"do not print categories that start with FOO\".\n      \"+FOO\" means \"do print categories that start with FOO\".\n\n      Examples: --filter=-whitespace,+whitespace/braces\n                --filter=-whitespace,-runtime/printf,+runtime/printf_format\n                --filter=-,+build/include_what_you_use\n\n      To see a list of all the categories used in cpplint, pass no arg:\n         --filter=\n\n    counting=total|toplevel|detailed\n      The total number of errors found is always printed. If\n      'toplevel' is provided, then the count of errors in each of\n      the top-level categories like 'build' and 'whitespace' will\n      also be printed. If 'detailed' is provided, then a count\n      is provided for each category like 'build/class'.\n\n    repository=path\n      The top level directory of the repository, used to derive the header\n      guard CPP variable. By default, this is determined by searching for a\n      path that contains .git, .hg, or .svn. When this flag is specified, the\n      given path is used instead. This option allows the header guard CPP\n      variable to remain consistent even if members of a team have different\n      repository root directories (such as when checking out a subdirectory\n      with SVN). In addition, users of non-mainstream version control systems\n      can use this flag to ensure readable header guard CPP variables.\n\n      Examples:\n        Assuming that Alice checks out ProjectName and Bob checks out\n        ProjectName/trunk and trunk contains src/chrome/ui/browser.h, then\n        with no --repository flag, the header guard CPP variable will be:\n\n        Alice => TRUNK_SRC_CHROME_BROWSER_UI_BROWSER_H_\n        Bob   => SRC_CHROME_BROWSER_UI_BROWSER_H_\n\n        If Alice uses the --repository=trunk flag and Bob omits the flag or\n        uses --repository=. then the header guard CPP variable will be:\n\n        Alice => SRC_CHROME_BROWSER_UI_BROWSER_H_\n        Bob   => SRC_CHROME_BROWSER_UI_BROWSER_H_\n\n    root=subdir\n      The root directory used for deriving header guard CPP variable.\n      This directory is relative to the top level directory of the repository\n      which by default is determined by searching for a directory that contains\n      .git, .hg, or .svn but can also be controlled with the --repository flag.\n      If the specified directory does not exist, this flag is ignored.\n\n      Examples:\n        Assuming that src is the top level directory of the repository (and\n        cwd=top/src), the header guard CPP variables for\n        src/chrome/browser/ui/browser.h are:\n\n        No flag => CHROME_BROWSER_UI_BROWSER_H_\n        --root=chrome => BROWSER_UI_BROWSER_H_\n        --root=chrome/browser => UI_BROWSER_H_\n        --root=.. => SRC_CHROME_BROWSER_UI_BROWSER_H_\n\n    linelength=digits\n      This is the allowed line length for the project. The default value is\n      80 characters.\n\n      Examples:\n        --linelength=120\n\n    recursive\n      Search for files to lint recursively. Each directory given in the list\n      of files to be linted is replaced by all files that descend from that\n      directory. Files with extensions not in the valid extensions list are\n      excluded.\n\n    exclude=path\n      Exclude the given path from the list of files to be linted. Relative\n      paths are evaluated relative to the current directory and shell globbing\n      is performed. This flag can be provided multiple times to exclude\n      multiple files.\n\n      Examples:\n        --exclude=one.cc\n        --exclude=src/*.cc\n        --exclude=src/*.cc --exclude=test/*.cc\n\n    extensions=extension,extension,...\n      The allowed file extensions that cpplint will check\n\n      Examples:\n        --extensions=%s\n\n    includeorder=default|standardcfirst\n      For the build/include_order rule, the default is to blindly assume angle\n      bracket includes with file extension are c-system-headers (default),\n      even knowing this will have false classifications.\n      The default is established at google.\n      standardcfirst means to instead use an allow-list of known c headers and\n      treat all others as separate group of \"other system headers\". The C headers\n      included are those of the C-standard lib and closely related ones.\n\n    headers=x,y,...\n      The header extensions that cpplint will treat as .h in checks. Values are\n      automatically added to --extensions list.\n     (by default, only files with extensions %s will be assumed to be headers)\n\n      Examples:\n        --headers=%s\n        --headers=hpp,hxx\n        --headers=hpp\n\n    cpplint.py supports per-directory configurations specified in CPPLINT.cfg\n    files. CPPLINT.cfg file can contain a number of key=value pairs.\n    Currently the following options are supported:\n\n      set noparent\n      filter=+filter1,-filter2,...\n      exclude_files=regex\n      linelength=80\n      root=subdir\n      headers=x,y,...\n\n    \"set noparent\" option prevents cpplint from traversing directory tree\n    upwards looking for more .cfg files in parent directories. This option\n    is usually placed in the top-level project directory.\n\n    The \"filter\" option is similar in function to --filter flag. It specifies\n    message filters in addition to the |_DEFAULT_FILTERS| and those specified\n    through --filter command-line flag.\n\n    \"exclude_files\" allows to specify a regular expression to be matched against\n    a file name. If the expression matches, the file is skipped and not run\n    through the linter.\n\n    \"linelength\" allows to specify the allowed line length for the project.\n\n    The \"root\" option is similar in function to the --root flag (see example\n    above). Paths are relative to the directory of the CPPLINT.cfg.\n\n    The \"headers\" option is similar in function to the --headers flag\n    (see example above).\n\n    CPPLINT.cfg has an effect on files in the same directory and all\n    sub-directories, unless overridden by a nested configuration file.\n\n      Example file:\n        filter=-build/include_order,+build/include_alpha\n        exclude_files=.*\\\\.cc\n\n    The above example disables build/include_order warning and enables\n    build/include_alpha as well as excludes all .cc from being\n    processed by linter, in the current directory (where the .cfg\n    file is located) and all sub-directories.\n\"\"\"\n\n# We categorize each error message we print.  Here are the categories.\n# We want an explicit list so we can list them all in cpplint --filter=.\n# If you add a new error message with a new category, add it to the list\n# here!  cpplint_unittest.py should tell you if you forget to do this.\n_ERROR_CATEGORIES = [\n    'build/class',\n    'build/c++11',\n    'build/c++14',\n    'build/c++tr1',\n    'build/deprecated',\n    'build/endif_comment',\n    'build/explicit_make_pair',\n    'build/forward_decl',\n    'build/header_guard',\n    'build/include',\n    'build/include_subdir',\n    'build/include_alpha',\n    'build/include_order',\n    'build/include_what_you_use',\n    'build/namespaces_headers',\n    'build/namespaces_literals',\n    'build/namespaces',\n    'build/printf_format',\n    'build/storage_class',\n    'legal/copyright',\n    'readability/alt_tokens',\n    'readability/braces',\n    'readability/casting',\n    'readability/check',\n    'readability/constructors',\n    'readability/fn_size',\n    'readability/inheritance',\n    'readability/multiline_comment',\n    'readability/multiline_string',\n    'readability/namespace',\n    'readability/nolint',\n    'readability/nul',\n    'readability/strings',\n    'readability/todo',\n    'readability/utf8',\n    'runtime/arrays',\n    'runtime/casting',\n    'runtime/explicit',\n    'runtime/int',\n    'runtime/init',\n    'runtime/invalid_increment',\n    'runtime/member_string_references',\n    'runtime/memset',\n    'runtime/indentation_namespace',\n    'runtime/operator',\n    'runtime/printf',\n    'runtime/printf_format',\n    'runtime/references',\n    'runtime/string',\n    'runtime/threadsafe_fn',\n    'runtime/vlog',\n    'whitespace/blank_line',\n    'whitespace/braces',\n    'whitespace/comma',\n    'whitespace/comments',\n    'whitespace/empty_conditional_body',\n    'whitespace/empty_if_body',\n    'whitespace/empty_loop_body',\n    'whitespace/end_of_line',\n    'whitespace/ending_newline',\n    'whitespace/forcolon',\n    'whitespace/indent',\n    'whitespace/line_length',\n    'whitespace/newline',\n    'whitespace/operators',\n    'whitespace/parens',\n    'whitespace/semicolon',\n    'whitespace/tab',\n    'whitespace/todo',\n    ]\n\n# keywords to use with --outputs which generate stdout for machine processing\n_MACHINE_OUTPUTS = [\n  'junit',\n  'sed',\n  'gsed'\n]\n\n# These error categories are no longer enforced by cpplint, but for backwards-\n# compatibility they may still appear in NOLINT comments.\n_LEGACY_ERROR_CATEGORIES = [\n    'readability/streams',\n    'readability/function',\n    ]\n\n# These prefixes for categories should be ignored since they relate to other\n# tools which also use the NOLINT syntax, e.g. clang-tidy.\n_OTHER_NOLINT_CATEGORY_PREFIXES = [\n    'clang-analyzer',\n    ]\n\n# The default state of the category filter. This is overridden by the --filter=\n# flag. By default all errors are on, so only add here categories that should be\n# off by default (i.e., categories that must be enabled by the --filter= flags).\n# All entries here should start with a '-' or '+', as in the --filter= flag.\n_DEFAULT_FILTERS = ['-build/include_alpha']\n\n# The default list of categories suppressed for C (not C++) files.\n_DEFAULT_C_SUPPRESSED_CATEGORIES = [\n    'readability/casting',\n    ]\n\n# The default list of categories suppressed for Linux Kernel files.\n_DEFAULT_KERNEL_SUPPRESSED_CATEGORIES = [\n    'whitespace/tab',\n    ]\n\n# We used to check for high-bit characters, but after much discussion we\n# decided those were OK, as long as they were in UTF-8 and didn't represent\n# hard-coded international strings, which belong in a separate i18n file.\n\n# C++ headers\n_CPP_HEADERS = frozenset([\n    # Legacy\n    'algobase.h',\n    'algo.h',\n    'alloc.h',\n    'builtinbuf.h',\n    'bvector.h',\n    'complex.h',\n    'defalloc.h',\n    'deque.h',\n    'editbuf.h',\n    'fstream.h',\n    'function.h',\n    'hash_map',\n    'hash_map.h',\n    'hash_set',\n    'hash_set.h',\n    'hashtable.h',\n    'heap.h',\n    'indstream.h',\n    'iomanip.h',\n    'iostream.h',\n    'istream.h',\n    'iterator.h',\n    'list.h',\n    'map.h',\n    'multimap.h',\n    'multiset.h',\n    'ostream.h',\n    'pair.h',\n    'parsestream.h',\n    'pfstream.h',\n    'procbuf.h',\n    'pthread_alloc',\n    'pthread_alloc.h',\n    'rope',\n    'rope.h',\n    'ropeimpl.h',\n    'set.h',\n    'slist',\n    'slist.h',\n    'stack.h',\n    'stdiostream.h',\n    'stl_alloc.h',\n    'stl_relops.h',\n    'streambuf.h',\n    'stream.h',\n    'strfile.h',\n    'strstream.h',\n    'tempbuf.h',\n    'tree.h',\n    'type_traits.h',\n    'vector.h',\n    # 17.6.1.2 C++ library headers\n    'algorithm',\n    'array',\n    'atomic',\n    'bitset',\n    'chrono',\n    'codecvt',\n    'complex',\n    'condition_variable',\n    'deque',\n    'exception',\n    'forward_list',\n    'fstream',\n    'functional',\n    'future',\n    'initializer_list',\n    'iomanip',\n    'ios',\n    'iosfwd',\n    'iostream',\n    'istream',\n    'iterator',\n    'limits',\n    'list',\n    'locale',\n    'map',\n    'memory',\n    'mutex',\n    'new',\n    'numeric',\n    'ostream',\n    'queue',\n    'random',\n    'ratio',\n    'regex',\n    'scoped_allocator',\n    'set',\n    'sstream',\n    'stack',\n    'stdexcept',\n    'streambuf',\n    'string',\n    'strstream',\n    'system_error',\n    'thread',\n    'tuple',\n    'typeindex',\n    'typeinfo',\n    'type_traits',\n    'unordered_map',\n    'unordered_set',\n    'utility',\n    'valarray',\n    'vector',\n    # 17.6.1.2 C++14 headers\n    'shared_mutex',\n    # 17.6.1.2 C++17 headers\n    'any',\n    'charconv',\n    'codecvt',\n    'execution',\n    'filesystem',\n    'memory_resource',\n    'optional',\n    'string_view',\n    'variant',\n    # 17.6.1.2 C++ headers for C library facilities\n    'cassert',\n    'ccomplex',\n    'cctype',\n    'cerrno',\n    'cfenv',\n    'cfloat',\n    'cinttypes',\n    'ciso646',\n    'climits',\n    'clocale',\n    'cmath',\n    'csetjmp',\n    'csignal',\n    'cstdalign',\n    'cstdarg',\n    'cstdbool',\n    'cstddef',\n    'cstdint',\n    'cstdio',\n    'cstdlib',\n    'cstring',\n    'ctgmath',\n    'ctime',\n    'cuchar',\n    'cwchar',\n    'cwctype',\n    ])\n\n# C headers\n_C_HEADERS = frozenset([\n    # System C headers\n    'assert.h',\n    'complex.h',\n    'ctype.h',\n    'errno.h',\n    'fenv.h',\n    'float.h',\n    'inttypes.h',\n    'iso646.h',\n    'limits.h',\n    'locale.h',\n    'math.h',\n    'setjmp.h',\n    'signal.h',\n    'stdalign.h',\n    'stdarg.h',\n    'stdatomic.h',\n    'stdbool.h',\n    'stddef.h',\n    'stdint.h',\n    'stdio.h',\n    'stdlib.h',\n    'stdnoreturn.h',\n    'string.h',\n    'tgmath.h',\n    'threads.h',\n    'time.h',\n    'uchar.h',\n    'wchar.h',\n    'wctype.h',\n    # additional POSIX C headers\n    'aio.h',\n    'arpa/inet.h',\n    'cpio.h',\n    'dirent.h',\n    'dlfcn.h',\n    'fcntl.h',\n    'fmtmsg.h',\n    'fnmatch.h',\n    'ftw.h',\n    'glob.h',\n    'grp.h',\n    'iconv.h',\n    'langinfo.h',\n    'libgen.h',\n    'monetary.h',\n    'mqueue.h',\n    'ndbm.h',\n    'net/if.h',\n    'netdb.h',\n    'netinet/in.h',\n    'netinet/tcp.h',\n    'nl_types.h',\n    'poll.h',\n    'pthread.h',\n    'pwd.h',\n    'regex.h',\n    'sched.h',\n    'search.h',\n    'semaphore.h',\n    'setjmp.h',\n    'signal.h',\n    'spawn.h',\n    'strings.h',\n    'stropts.h',\n    'syslog.h',\n    'tar.h',\n    'termios.h',\n    'trace.h',\n    'ulimit.h',\n    'unistd.h',\n    'utime.h',\n    'utmpx.h',\n    'wordexp.h',\n    # additional GNUlib headers\n    'a.out.h',\n    'aliases.h',\n    'alloca.h',\n    'ar.h',\n    'argp.h',\n    'argz.h',\n    'byteswap.h',\n    'crypt.h',\n    'endian.h',\n    'envz.h',\n    'err.h',\n    'error.h',\n    'execinfo.h',\n    'fpu_control.h',\n    'fstab.h',\n    'fts.h',\n    'getopt.h',\n    'gshadow.h',\n    'ieee754.h',\n    'ifaddrs.h',\n    'libintl.h',\n    'mcheck.h',\n    'mntent.h',\n    'obstack.h',\n    'paths.h',\n    'printf.h',\n    'pty.h',\n    'resolv.h',\n    'shadow.h',\n    'sysexits.h',\n    'ttyent.h',\n    # Additional linux glibc headers\n    'dlfcn.h',\n    'elf.h',\n    'features.h',\n    'gconv.h',\n    'gnu-versions.h',\n    'lastlog.h',\n    'libio.h',\n    'link.h',\n    'malloc.h',\n    'memory.h',\n    'netash/ash.h',\n    'netatalk/at.h',\n    'netax25/ax25.h',\n    'neteconet/ec.h',\n    'netipx/ipx.h',\n    'netiucv/iucv.h',\n    'netpacket/packet.h',\n    'netrom/netrom.h',\n    'netrose/rose.h',\n    'nfs/nfs.h',\n    'nl_types.h',\n    'nss.h',\n    're_comp.h',\n    'regexp.h',\n    'sched.h',\n    'sgtty.h',\n    'stab.h',\n    'stdc-predef.h',\n    'stdio_ext.h',\n    'syscall.h',\n    'termio.h',\n    'thread_db.h',\n    'ucontext.h',\n    'ustat.h',\n    'utmp.h',\n    'values.h',\n    'wait.h',\n    'xlocale.h',\n    # Hardware specific headers\n    'arm_neon.h',\n    'emmintrin.h',\n    'xmmintin.h',\n    ])\n\n# Folders of C libraries so commonly used in C++,\n# that they have parity with standard C libraries.\nC_STANDARD_HEADER_FOLDERS = frozenset([\n    # standard C library\n    \"sys\",\n    # glibc for linux\n    \"arpa\",\n    \"asm-generic\",\n    \"bits\",\n    \"gnu\",\n    \"net\",\n    \"netinet\",\n    \"protocols\",\n    \"rpc\",\n    \"rpcsvc\",\n    \"scsi\",\n    # linux kernel header\n    \"drm\",\n    \"linux\",\n    \"misc\",\n    \"mtd\",\n    \"rdma\",\n    \"sound\",\n    \"video\",\n    \"xen\",\n  ])\n\n# Type names\n_TYPES = re.compile(\n    r'^(?:'\n    # [dcl.type.simple]\n    r'(char(16_t|32_t)?)|wchar_t|'\n    r'bool|short|int|long|signed|unsigned|float|double|'\n    # [support.types]\n    r'(ptrdiff_t|size_t|max_align_t|nullptr_t)|'\n    # [cstdint.syn]\n    r'(u?int(_fast|_least)?(8|16|32|64)_t)|'\n    r'(u?int(max|ptr)_t)|'\n    r')$')\n\n\n# These headers are excluded from [build/include] and [build/include_order]\n# checks:\n# - Anything not following google file name conventions (containing an\n#   uppercase character, such as Python.h or nsStringAPI.h, for example).\n# - Lua headers.\n_THIRD_PARTY_HEADERS_PATTERN = re.compile(\n    r'^(?:[^/]*[A-Z][^/]*\\.h|lua\\.h|lauxlib\\.h|lualib\\.h)$')\n\n# Pattern for matching FileInfo.BaseName() against test file name\n_test_suffixes = ['_test', '_regtest', '_unittest']\n_TEST_FILE_SUFFIX = '(' + '|'.join(_test_suffixes) + r')$'\n\n# Pattern that matches only complete whitespace, possibly across multiple lines.\n_EMPTY_CONDITIONAL_BODY_PATTERN = re.compile(r'^\\s*$', re.DOTALL)\n\n# Assertion macros.  These are defined in base/logging.h and\n# testing/base/public/gunit.h.\n_CHECK_MACROS = [\n    'DCHECK', 'CHECK',\n    'EXPECT_TRUE', 'ASSERT_TRUE',\n    'EXPECT_FALSE', 'ASSERT_FALSE',\n    ]\n\n# Replacement macros for CHECK/DCHECK/EXPECT_TRUE/EXPECT_FALSE\n_CHECK_REPLACEMENT = dict([(macro_var, {}) for macro_var in _CHECK_MACROS])\n\nfor op, replacement in [('==', 'EQ'), ('!=', 'NE'),\n                        ('>=', 'GE'), ('>', 'GT'),\n                        ('<=', 'LE'), ('<', 'LT')]:\n  _CHECK_REPLACEMENT['DCHECK'][op] = 'DCHECK_%s' % replacement\n  _CHECK_REPLACEMENT['CHECK'][op] = 'CHECK_%s' % replacement\n  _CHECK_REPLACEMENT['EXPECT_TRUE'][op] = 'EXPECT_%s' % replacement\n  _CHECK_REPLACEMENT['ASSERT_TRUE'][op] = 'ASSERT_%s' % replacement\n\nfor op, inv_replacement in [('==', 'NE'), ('!=', 'EQ'),\n                            ('>=', 'LT'), ('>', 'LE'),\n                            ('<=', 'GT'), ('<', 'GE')]:\n  _CHECK_REPLACEMENT['EXPECT_FALSE'][op] = 'EXPECT_%s' % inv_replacement\n  _CHECK_REPLACEMENT['ASSERT_FALSE'][op] = 'ASSERT_%s' % inv_replacement\n\n# Alternative tokens and their replacements.  For full list, see section 2.5\n# Alternative tokens [lex.digraph] in the C++ standard.\n#\n# Digraphs (such as '%:') are not included here since it's a mess to\n# match those on a word boundary.\n_ALT_TOKEN_REPLACEMENT = {\n    'and': '&&',\n    'bitor': '|',\n    'or': '||',\n    'xor': '^',\n    'compl': '~',\n    'bitand': '&',\n    'and_eq': '&=',\n    'or_eq': '|=',\n    'xor_eq': '^=',\n    'not': '!',\n    'not_eq': '!='\n    }\n\n# Compile regular expression that matches all the above keywords.  The \"[ =()]\"\n# bit is meant to avoid matching these keywords outside of boolean expressions.\n#\n# False positives include C-style multi-line comments and multi-line strings\n# but those have always been troublesome for cpplint.\n_ALT_TOKEN_REPLACEMENT_PATTERN = re.compile(\n    r'[ =()](' + ('|'.join(_ALT_TOKEN_REPLACEMENT.keys())) + r')(?=[ (]|$)')\n\n\n# These constants define types of headers for use with\n# _IncludeState.CheckNextIncludeOrder().\n_C_SYS_HEADER = 1\n_CPP_SYS_HEADER = 2\n_OTHER_SYS_HEADER = 3\n_LIKELY_MY_HEADER = 4\n_POSSIBLE_MY_HEADER = 5\n_OTHER_HEADER = 6\n\n# These constants define the current inline assembly state\n_NO_ASM = 0       # Outside of inline assembly block\n_INSIDE_ASM = 1   # Inside inline assembly block\n_END_ASM = 2      # Last line of inline assembly block\n_BLOCK_ASM = 3    # The whole block is an inline assembly block\n\n# Match start of assembly blocks\n_MATCH_ASM = re.compile(r'^\\s*(?:asm|_asm|__asm|__asm__)'\n                        r'(?:\\s+(volatile|__volatile__))?'\n                        r'\\s*[{(]')\n\n# Match strings that indicate we're working on a C (not C++) file.\n_SEARCH_C_FILE = re.compile(r'\\b(?:LINT_C_FILE|'\n                            r'vim?:\\s*.*(\\s*|:)filetype=c(\\s*|:|$))')\n\n# Match string that indicates we're working on a Linux Kernel file.\n_SEARCH_KERNEL_FILE = re.compile(r'\\b(?:LINT_KERNEL_FILE)')\n\n# Commands for sed to fix the problem\n_SED_FIXUPS = {\n  'Remove spaces around =': r's/ = /=/',\n  'Remove spaces around !=': r's/ != /!=/',\n  'Remove space before ( in if (': r's/if (/if(/',\n  'Remove space before ( in for (': r's/for (/for(/',\n  'Remove space before ( in while (': r's/while (/while(/',\n  'Remove space before ( in switch (': r's/switch (/switch(/',\n  'Should have a space between // and comment': r's/\\/\\//\\/\\/ /',\n  'Missing space before {': r's/\\([^ ]\\){/\\1 {/',\n  'Tab found, replace by spaces': r's/\\t/  /g',\n  'Line ends in whitespace.  Consider deleting these extra spaces.': r's/\\s*$//',\n  'You don\\'t need a ; after a }': r's/};/}/',\n  'Missing space after ,': r's/,\\([^ ]\\)/, \\1/g',\n}\n\n_regexp_compile_cache = {}\n\n# {str, set(int)}: a map from error categories to sets of linenumbers\n# on which those errors are expected and should be suppressed.\n_error_suppressions = {}\n\n# The root directory used for deriving header guard CPP variable.\n# This is set by --root flag.\n_root = None\n_root_debug = False\n\n# The top level repository directory. If set, _root is calculated relative to\n# this directory instead of the directory containing version control artifacts.\n# This is set by the --repository flag.\n_repository = None\n\n# Files to exclude from linting. This is set by the --exclude flag.\n_excludes = None\n\n# Whether to suppress all PrintInfo messages, UNRELATED to --quiet flag\n_quiet = False\n\n# The allowed line length of files.\n# This is set by --linelength flag.\n_line_length = 80\n\n# This allows to use different include order rule than default\n_include_order = \"default\"\n\ntry:\n  #  -- pylint: disable=used-before-assignment\n  unicode\nexcept NameError:\n  #  -- pylint: disable=redefined-builtin\n  basestring = unicode = str\n\ntry:\n  #  -- pylint: disable=used-before-assignment\n  long\nexcept NameError:\n  #  -- pylint: disable=redefined-builtin\n  long = int\n\nif sys.version_info < (3,):\n  #  -- pylint: disable=no-member\n  # BINARY_TYPE = str\n  itervalues = dict.itervalues\n  iteritems = dict.iteritems\nelse:\n  # BINARY_TYPE = bytes\n  itervalues = dict.values\n  iteritems = dict.items\n\ndef unicode_escape_decode(x):\n  if sys.version_info < (3,):\n    return codecs.unicode_escape_decode(x)[0]\n  else:\n    return x\n\n# Treat all headers starting with 'h' equally: .h, .hpp, .hxx etc.\n# This is set by --headers flag.\n_hpp_headers = set([])\n\n# {str, bool}: a map from error categories to booleans which indicate if the\n# category should be suppressed for every line.\n_global_error_suppressions = {}\n\ndef ProcessHppHeadersOption(val):\n  global _hpp_headers\n  try:\n    _hpp_headers = {ext.strip() for ext in val.split(',')}\n  except ValueError:\n    PrintUsage('Header extensions must be comma separated list.')\n\ndef ProcessIncludeOrderOption(val):\n  if val is None or val == \"default\":\n    pass\n  elif val == \"standardcfirst\":\n    global _include_order\n    _include_order = val\n  else:\n    PrintUsage('Invalid includeorder value %s. Expected default|standardcfirst')\n\ndef IsHeaderExtension(file_extension):\n  return file_extension in GetHeaderExtensions()\n\ndef GetHeaderExtensions():\n  if _hpp_headers:\n    return _hpp_headers\n  if _valid_extensions:\n    return {h for h in _valid_extensions if 'h' in h}\n  return set(['h', 'hh', 'hpp', 'hxx', 'h++', 'cuh'])\n\n# The allowed extensions for file names\n# This is set by --extensions flag\ndef GetAllExtensions():\n  return GetHeaderExtensions().union(_valid_extensions or set(\n    ['c', 'cc', 'cpp', 'cxx', 'c++', 'cu']))\n\ndef ProcessExtensionsOption(val):\n  global _valid_extensions\n  try:\n    extensions = [ext.strip() for ext in val.split(',')]\n    _valid_extensions = set(extensions)\n  except ValueError:\n    PrintUsage('Extensions should be a comma-separated list of values;'\n               'for example: extensions=hpp,cpp\\n'\n               'This could not be parsed: \"%s\"' % (val,))\n\ndef GetNonHeaderExtensions():\n  return GetAllExtensions().difference(GetHeaderExtensions())\n\ndef ParseNolintSuppressions(filename, raw_line, linenum, error):\n  \"\"\"Updates the global list of line error-suppressions.\n\n  Parses any NOLINT comments on the current line, updating the global\n  error_suppressions store.  Reports an error if the NOLINT comment\n  was malformed.\n\n  Args:\n    filename: str, the name of the input file.\n    raw_line: str, the line of input text, with comments.\n    linenum: int, the number of the current line.\n    error: function, an error handler.\n  \"\"\"\n  matched = Search(r'\\bNOLINT(NEXTLINE)?\\b(\\([^)]+\\))?', raw_line)\n  if matched:\n    if matched.group(1):\n      suppressed_line = linenum + 1\n    else:\n      suppressed_line = linenum\n    category = matched.group(2)\n    if category in (None, '(*)'):  # => \"suppress all\"\n      _error_suppressions.setdefault(None, set()).add(suppressed_line)\n    else:\n      if category.startswith('(') and category.endswith(')'):\n        category = category[1:-1]\n        if category in _ERROR_CATEGORIES:\n          _error_suppressions.setdefault(category, set()).add(suppressed_line)\n        elif any(c for c in _OTHER_NOLINT_CATEGORY_PREFIXES if category.startswith(c)):\n          # Ignore any categories from other tools.\n          pass\n        elif category not in _LEGACY_ERROR_CATEGORIES:\n          error(filename, linenum, 'readability/nolint', 5,\n                'Unknown NOLINT error category: %s' % category)\n\n\ndef ProcessGlobalSuppressions(lines):\n  \"\"\"Updates the list of global error suppressions.\n\n  Parses any lint directives in the file that have global effect.\n\n  Args:\n    lines: An array of strings, each representing a line of the file, with the\n           last element being empty if the file is terminated with a newline.\n  \"\"\"\n  for line in lines:\n    if _SEARCH_C_FILE.search(line):\n      for category in _DEFAULT_C_SUPPRESSED_CATEGORIES:\n        _global_error_suppressions[category] = True\n    if _SEARCH_KERNEL_FILE.search(line):\n      for category in _DEFAULT_KERNEL_SUPPRESSED_CATEGORIES:\n        _global_error_suppressions[category] = True\n\n\ndef ResetNolintSuppressions():\n  \"\"\"Resets the set of NOLINT suppressions to empty.\"\"\"\n  _error_suppressions.clear()\n  _global_error_suppressions.clear()\n\n\ndef IsErrorSuppressedByNolint(category, linenum):\n  \"\"\"Returns true if the specified error category is suppressed on this line.\n\n  Consults the global error_suppressions map populated by\n  ParseNolintSuppressions/ProcessGlobalSuppressions/ResetNolintSuppressions.\n\n  Args:\n    category: str, the category of the error.\n    linenum: int, the current line number.\n  Returns:\n    bool, True iff the error should be suppressed due to a NOLINT comment or\n    global suppression.\n  \"\"\"\n  return (_global_error_suppressions.get(category, False) or\n          linenum in _error_suppressions.get(category, set()) or\n          linenum in _error_suppressions.get(None, set()))\n\n\ndef Match(pattern, s):\n  \"\"\"Matches the string with the pattern, caching the compiled regexp.\"\"\"\n  # The regexp compilation caching is inlined in both Match and Search for\n  # performance reasons; factoring it out into a separate function turns out\n  # to be noticeably expensive.\n  if pattern not in _regexp_compile_cache:\n    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)\n  return _regexp_compile_cache[pattern].match(s)\n\n\ndef ReplaceAll(pattern, rep, s):\n  \"\"\"Replaces instances of pattern in a string with a replacement.\n\n  The compiled regex is kept in a cache shared by Match and Search.\n\n  Args:\n    pattern: regex pattern\n    rep: replacement text\n    s: search string\n\n  Returns:\n    string with replacements made (or original string if no replacements)\n  \"\"\"\n  if pattern not in _regexp_compile_cache:\n    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)\n  return _regexp_compile_cache[pattern].sub(rep, s)\n\n\ndef Search(pattern, s):\n  \"\"\"Searches the string for the pattern, caching the compiled regexp.\"\"\"\n  if pattern not in _regexp_compile_cache:\n    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)\n  return _regexp_compile_cache[pattern].search(s)\n\n\ndef _IsSourceExtension(s):\n  \"\"\"File extension (excluding dot) matches a source file extension.\"\"\"\n  return s in GetNonHeaderExtensions()\n\n\nclass _IncludeState(object):\n  \"\"\"Tracks line numbers for includes, and the order in which includes appear.\n\n  include_list contains list of lists of (header, line number) pairs.\n  It's a lists of lists rather than just one flat list to make it\n  easier to update across preprocessor boundaries.\n\n  Call CheckNextIncludeOrder() once for each header in the file, passing\n  in the type constants defined above. Calls in an illegal order will\n  raise an _IncludeError with an appropriate error message.\n\n  \"\"\"\n  # self._section will move monotonically through this set. If it ever\n  # needs to move backwards, CheckNextIncludeOrder will raise an error.\n  _INITIAL_SECTION = 0\n  _MY_H_SECTION = 1\n  _C_SECTION = 2\n  _CPP_SECTION = 3\n  _OTHER_SYS_SECTION = 4\n  _OTHER_H_SECTION = 5\n\n  _TYPE_NAMES = {\n      _C_SYS_HEADER: 'C system header',\n      _CPP_SYS_HEADER: 'C++ system header',\n      _OTHER_SYS_HEADER: 'other system header',\n      _LIKELY_MY_HEADER: 'header this file implements',\n      _POSSIBLE_MY_HEADER: 'header this file may implement',\n      _OTHER_HEADER: 'other header',\n      }\n  _SECTION_NAMES = {\n      _INITIAL_SECTION: \"... nothing. (This can't be an error.)\",\n      _MY_H_SECTION: 'a header this file implements',\n      _C_SECTION: 'C system header',\n      _CPP_SECTION: 'C++ system header',\n      _OTHER_SYS_SECTION: 'other system header',\n      _OTHER_H_SECTION: 'other header',\n      }\n\n  def __init__(self):\n    self.include_list = [[]]\n    self._section = None\n    self._last_header = None\n    self.ResetSection('')\n\n  def FindHeader(self, header):\n    \"\"\"Check if a header has already been included.\n\n    Args:\n      header: header to check.\n    Returns:\n      Line number of previous occurrence, or -1 if the header has not\n      been seen before.\n    \"\"\"\n    for section_list in self.include_list:\n      for f in section_list:\n        if f[0] == header:\n          return f[1]\n    return -1\n\n  def ResetSection(self, directive):\n    \"\"\"Reset section checking for preprocessor directive.\n\n    Args:\n      directive: preprocessor directive (e.g. \"if\", \"else\").\n    \"\"\"\n    # The name of the current section.\n    self._section = self._INITIAL_SECTION\n    # The path of last found header.\n    self._last_header = ''\n\n    # Update list of includes.  Note that we never pop from the\n    # include list.\n    if directive in ('if', 'ifdef', 'ifndef'):\n      self.include_list.append([])\n    elif directive in ('else', 'elif'):\n      self.include_list[-1] = []\n\n  def SetLastHeader(self, header_path):\n    self._last_header = header_path\n\n  def CanonicalizeAlphabeticalOrder(self, header_path):\n    \"\"\"Returns a path canonicalized for alphabetical comparison.\n\n    - replaces \"-\" with \"_\" so they both cmp the same.\n    - removes '-inl' since we don't require them to be after the main header.\n    - lowercase everything, just in case.\n\n    Args:\n      header_path: Path to be canonicalized.\n\n    Returns:\n      Canonicalized path.\n    \"\"\"\n    return header_path.replace('-inl.h', '.h').replace('-', '_').lower()\n\n  def IsInAlphabeticalOrder(self, clean_lines, linenum, header_path):\n    \"\"\"Check if a header is in alphabetical order with the previous header.\n\n    Args:\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      header_path: Canonicalized header to be checked.\n\n    Returns:\n      Returns true if the header is in alphabetical order.\n    \"\"\"\n    # If previous section is different from current section, _last_header will\n    # be reset to empty string, so it's always less than current header.\n    #\n    # If previous line was a blank line, assume that the headers are\n    # intentionally sorted the way they are.\n    if (self._last_header > header_path and\n        Match(r'^\\s*#\\s*include\\b', clean_lines.elided[linenum - 1])):\n      return False\n    return True\n\n  def CheckNextIncludeOrder(self, header_type):\n    \"\"\"Returns a non-empty error message if the next header is out of order.\n\n    This function also updates the internal state to be ready to check\n    the next include.\n\n    Args:\n      header_type: One of the _XXX_HEADER constants defined above.\n\n    Returns:\n      The empty string if the header is in the right order, or an\n      error message describing what's wrong.\n\n    \"\"\"\n    error_message = ('Found %s after %s' %\n                     (self._TYPE_NAMES[header_type],\n                      self._SECTION_NAMES[self._section]))\n\n    last_section = self._section\n\n    if header_type == _C_SYS_HEADER:\n      if self._section <= self._C_SECTION:\n        self._section = self._C_SECTION\n      else:\n        self._last_header = ''\n        return error_message\n    elif header_type == _CPP_SYS_HEADER:\n      if self._section <= self._CPP_SECTION:\n        self._section = self._CPP_SECTION\n      else:\n        self._last_header = ''\n        return error_message\n    elif header_type == _OTHER_SYS_HEADER:\n      if self._section <= self._OTHER_SYS_SECTION:\n        self._section = self._OTHER_SYS_SECTION\n      else:\n        self._last_header = ''\n        return error_message\n    elif header_type == _LIKELY_MY_HEADER:\n      if self._section <= self._MY_H_SECTION:\n        self._section = self._MY_H_SECTION\n      else:\n        self._section = self._OTHER_H_SECTION\n    elif header_type == _POSSIBLE_MY_HEADER:\n      if self._section <= self._MY_H_SECTION:\n        self._section = self._MY_H_SECTION\n      else:\n        # This will always be the fallback because we're not sure\n        # enough that the header is associated with this file.\n        self._section = self._OTHER_H_SECTION\n    else:\n      assert header_type == _OTHER_HEADER\n      self._section = self._OTHER_H_SECTION\n\n    if last_section != self._section:\n      self._last_header = ''\n\n    return ''\n\n\nclass _CppLintState(object):\n  \"\"\"Maintains module-wide state..\"\"\"\n\n  def __init__(self):\n    self.verbose_level = 1  # global setting.\n    self.error_count = 0    # global count of reported errors\n    # filters to apply when emitting error messages\n    self.filters = _DEFAULT_FILTERS[:]\n    # backup of filter list. Used to restore the state after each file.\n    self._filters_backup = self.filters[:]\n    self.counting = 'total'  # In what way are we counting errors?\n    self.errors_by_category = {}  # string to int dict storing error counts\n    self.quiet = False  # Suppress non-error messages?\n\n    # output format:\n    # \"emacs\" - format that emacs can parse (default)\n    # \"eclipse\" - format that eclipse can parse\n    # \"vs7\" - format that Microsoft Visual Studio 7 can parse\n    # \"junit\" - format that Jenkins, Bamboo, etc can parse\n    # \"sed\" - returns a gnu sed command to fix the problem\n    # \"gsed\" - like sed, but names the command gsed, e.g. for macOS homebrew users\n    self.output_format = 'emacs'\n\n    # For JUnit output, save errors and failures until the end so that they\n    # can be written into the XML\n    self._junit_errors = []\n    self._junit_failures = []\n\n  def SetOutputFormat(self, output_format):\n    \"\"\"Sets the output format for errors.\"\"\"\n    self.output_format = output_format\n\n  def SetQuiet(self, quiet):\n    \"\"\"Sets the module's quiet settings, and returns the previous setting.\"\"\"\n    last_quiet = self.quiet\n    self.quiet = quiet\n    return last_quiet\n\n  def SetVerboseLevel(self, level):\n    \"\"\"Sets the module's verbosity, and returns the previous setting.\"\"\"\n    last_verbose_level = self.verbose_level\n    self.verbose_level = level\n    return last_verbose_level\n\n  def SetCountingStyle(self, counting_style):\n    \"\"\"Sets the module's counting options.\"\"\"\n    self.counting = counting_style\n\n  def SetFilters(self, filters):\n    \"\"\"Sets the error-message filters.\n\n    These filters are applied when deciding whether to emit a given\n    error message.\n\n    Args:\n      filters: A string of comma-separated filters (eg \"+whitespace/indent\").\n               Each filter should start with + or -; else we die.\n\n    Raises:\n      ValueError: The comma-separated filters did not all start with '+' or '-'.\n                  E.g. \"-,+whitespace,-whitespace/indent,whitespace/badfilter\"\n    \"\"\"\n    # Default filters always have less priority than the flag ones.\n    self.filters = _DEFAULT_FILTERS[:]\n    self.AddFilters(filters)\n\n  def AddFilters(self, filters):\n    \"\"\" Adds more filters to the existing list of error-message filters. \"\"\"\n    for filt in filters.split(','):\n      clean_filt = filt.strip()\n      if clean_filt:\n        self.filters.append(clean_filt)\n    for filt in self.filters:\n      if not (filt.startswith('+') or filt.startswith('-')):\n        raise ValueError('Every filter in --filters must start with + or -'\n                         ' (%s does not)' % filt)\n\n  def BackupFilters(self):\n    \"\"\" Saves the current filter list to backup storage.\"\"\"\n    self._filters_backup = self.filters[:]\n\n  def RestoreFilters(self):\n    \"\"\" Restores filters previously backed up.\"\"\"\n    self.filters = self._filters_backup[:]\n\n  def ResetErrorCounts(self):\n    \"\"\"Sets the module's error statistic back to zero.\"\"\"\n    self.error_count = 0\n    self.errors_by_category = {}\n\n  def IncrementErrorCount(self, category):\n    \"\"\"Bumps the module's error statistic.\"\"\"\n    self.error_count += 1\n    if self.counting in ('toplevel', 'detailed'):\n      if self.counting != 'detailed':\n        category = category.split('/')[0]\n      if category not in self.errors_by_category:\n        self.errors_by_category[category] = 0\n      self.errors_by_category[category] += 1\n\n  def PrintErrorCounts(self):\n    \"\"\"Print a summary of errors by category, and the total.\"\"\"\n    for category, count in sorted(iteritems(self.errors_by_category)):\n      self.PrintInfo('Category \\'%s\\' errors found: %d\\n' %\n                       (category, count))\n    if self.error_count > 0:\n      self.PrintInfo('Total errors found: %d\\n' % self.error_count)\n\n  def PrintInfo(self, message):\n    # _quiet does not represent --quiet flag.\n    # Hide infos from stdout to keep stdout pure for machine consumption\n    if not _quiet and self.output_format not in _MACHINE_OUTPUTS:\n      sys.stdout.write(message)\n\n  def PrintError(self, message):\n    if self.output_format == 'junit':\n      self._junit_errors.append(message)\n    else:\n      sys.stderr.write(message)\n\n  def AddJUnitFailure(self, filename, linenum, message, category, confidence):\n    self._junit_failures.append((filename, linenum, message, category,\n        confidence))\n\n  def FormatJUnitXML(self):\n    num_errors = len(self._junit_errors)\n    num_failures = len(self._junit_failures)\n\n    testsuite = xml.etree.ElementTree.Element('testsuite')\n    testsuite.attrib['errors'] = str(num_errors)\n    testsuite.attrib['failures'] = str(num_failures)\n    testsuite.attrib['name'] = 'cpplint'\n\n    if num_errors == 0 and num_failures == 0:\n      testsuite.attrib['tests'] = str(1)\n      xml.etree.ElementTree.SubElement(testsuite, 'testcase', name='passed')\n\n    else:\n      testsuite.attrib['tests'] = str(num_errors + num_failures)\n      if num_errors > 0:\n        testcase = xml.etree.ElementTree.SubElement(testsuite, 'testcase')\n        testcase.attrib['name'] = 'errors'\n        error = xml.etree.ElementTree.SubElement(testcase, 'error')\n        error.text = '\\n'.join(self._junit_errors)\n      if num_failures > 0:\n        # Group failures by file\n        failed_file_order = []\n        failures_by_file = {}\n        for failure in self._junit_failures:\n          failed_file = failure[0]\n          if failed_file not in failed_file_order:\n            failed_file_order.append(failed_file)\n            failures_by_file[failed_file] = []\n          failures_by_file[failed_file].append(failure)\n        # Create a testcase for each file\n        for failed_file in failed_file_order:\n          failures = failures_by_file[failed_file]\n          testcase = xml.etree.ElementTree.SubElement(testsuite, 'testcase')\n          testcase.attrib['name'] = failed_file\n          failure = xml.etree.ElementTree.SubElement(testcase, 'failure')\n          template = '{0}: {1} [{2}] [{3}]'\n          texts = [template.format(f[1], f[2], f[3], f[4]) for f in failures]\n          failure.text = '\\n'.join(texts)\n\n    xml_decl = '<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n'\n    return xml_decl + xml.etree.ElementTree.tostring(testsuite, 'utf-8').decode('utf-8')\n\n\n_cpplint_state = _CppLintState()\n\n\ndef _OutputFormat():\n  \"\"\"Gets the module's output format.\"\"\"\n  return _cpplint_state.output_format\n\n\ndef _SetOutputFormat(output_format):\n  \"\"\"Sets the module's output format.\"\"\"\n  _cpplint_state.SetOutputFormat(output_format)\n\ndef _Quiet():\n  \"\"\"Return's the module's quiet setting.\"\"\"\n  return _cpplint_state.quiet\n\ndef _SetQuiet(quiet):\n  \"\"\"Set the module's quiet status, and return previous setting.\"\"\"\n  return _cpplint_state.SetQuiet(quiet)\n\n\ndef _VerboseLevel():\n  \"\"\"Returns the module's verbosity setting.\"\"\"\n  return _cpplint_state.verbose_level\n\n\ndef _SetVerboseLevel(level):\n  \"\"\"Sets the module's verbosity, and returns the previous setting.\"\"\"\n  return _cpplint_state.SetVerboseLevel(level)\n\n\ndef _SetCountingStyle(level):\n  \"\"\"Sets the module's counting options.\"\"\"\n  _cpplint_state.SetCountingStyle(level)\n\n\ndef _Filters():\n  \"\"\"Returns the module's list of output filters, as a list.\"\"\"\n  return _cpplint_state.filters\n\n\ndef _SetFilters(filters):\n  \"\"\"Sets the module's error-message filters.\n\n  These filters are applied when deciding whether to emit a given\n  error message.\n\n  Args:\n    filters: A string of comma-separated filters (eg \"whitespace/indent\").\n             Each filter should start with + or -; else we die.\n  \"\"\"\n  _cpplint_state.SetFilters(filters)\n\ndef _AddFilters(filters):\n  \"\"\"Adds more filter overrides.\n\n  Unlike _SetFilters, this function does not reset the current list of filters\n  available.\n\n  Args:\n    filters: A string of comma-separated filters (eg \"whitespace/indent\").\n             Each filter should start with + or -; else we die.\n  \"\"\"\n  _cpplint_state.AddFilters(filters)\n\ndef _BackupFilters():\n  \"\"\" Saves the current filter list to backup storage.\"\"\"\n  _cpplint_state.BackupFilters()\n\ndef _RestoreFilters():\n  \"\"\" Restores filters previously backed up.\"\"\"\n  _cpplint_state.RestoreFilters()\n\nclass _FunctionState(object):\n  \"\"\"Tracks current function name and the number of lines in its body.\"\"\"\n\n  _NORMAL_TRIGGER = 250  # for --v=0, 500 for --v=1, etc.\n  _TEST_TRIGGER = 400    # about 50% more than _NORMAL_TRIGGER.\n\n  def __init__(self):\n    self.in_a_function = False\n    self.lines_in_function = 0\n    self.current_function = ''\n\n  def Begin(self, function_name):\n    \"\"\"Start analyzing function body.\n\n    Args:\n      function_name: The name of the function being tracked.\n    \"\"\"\n    self.in_a_function = True\n    self.lines_in_function = 0\n    self.current_function = function_name\n\n  def Count(self):\n    \"\"\"Count line in current function body.\"\"\"\n    if self.in_a_function:\n      self.lines_in_function += 1\n\n  def Check(self, error, filename, linenum):\n    \"\"\"Report if too many lines in function body.\n\n    Args:\n      error: The function to call with any errors found.\n      filename: The name of the current file.\n      linenum: The number of the line to check.\n    \"\"\"\n    if not self.in_a_function:\n      return\n\n    if Match(r'T(EST|est)', self.current_function):\n      base_trigger = self._TEST_TRIGGER\n    else:\n      base_trigger = self._NORMAL_TRIGGER\n    trigger = base_trigger * 2**_VerboseLevel()\n\n    if self.lines_in_function > trigger:\n      error_level = int(math.log(self.lines_in_function / base_trigger, 2))\n      # 50 => 0, 100 => 1, 200 => 2, 400 => 3, 800 => 4, 1600 => 5, ...\n      if error_level > 5:\n        error_level = 5\n      error(filename, linenum, 'readability/fn_size', error_level,\n            'Small and focused functions are preferred:'\n            ' %s has %d non-comment lines'\n            ' (error triggered by exceeding %d lines).'  % (\n                self.current_function, self.lines_in_function, trigger))\n\n  def End(self):\n    \"\"\"Stop analyzing function body.\"\"\"\n    self.in_a_function = False\n\n\nclass _IncludeError(Exception):\n  \"\"\"Indicates a problem with the include order in a file.\"\"\"\n  pass\n\n\nclass FileInfo(object):\n  \"\"\"Provides utility functions for filenames.\n\n  FileInfo provides easy access to the components of a file's path\n  relative to the project root.\n  \"\"\"\n\n  def __init__(self, filename):\n    self._filename = filename\n\n  def FullName(self):\n    \"\"\"Make Windows paths like Unix.\"\"\"\n    return os.path.abspath(self._filename).replace('\\\\', '/')\n\n  def RepositoryName(self):\n    r\"\"\"FullName after removing the local path to the repository.\n\n    If we have a real absolute path name here we can try to do something smart:\n    detecting the root of the checkout and truncating /path/to/checkout from\n    the name so that we get header guards that don't include things like\n    \"C:\\\\Documents and Settings\\\\...\" or \"/home/username/...\" in them and thus\n    people on different computers who have checked the source out to different\n    locations won't see bogus errors.\n    \"\"\"\n    fullname = self.FullName()\n\n    if os.path.exists(fullname):\n      project_dir = os.path.dirname(fullname)\n\n      # If the user specified a repository path, it exists, and the file is\n      # contained in it, use the specified repository path\n      if _repository:\n        repo = FileInfo(_repository).FullName()\n        root_dir = project_dir\n        while os.path.exists(root_dir):\n          # allow case-insensitive compare on Windows\n          if os.path.normcase(root_dir) == os.path.normcase(repo):\n            return os.path.relpath(fullname, root_dir).replace('\\\\', '/')\n          one_up_dir = os.path.dirname(root_dir)\n          if one_up_dir == root_dir:\n            break\n          root_dir = one_up_dir\n\n      if os.path.exists(os.path.join(project_dir, \".svn\")):\n        # If there's a .svn file in the current directory, we recursively look\n        # up the directory tree for the top of the SVN checkout\n        root_dir = project_dir\n        one_up_dir = os.path.dirname(root_dir)\n        while os.path.exists(os.path.join(one_up_dir, \".svn\")):\n          root_dir = os.path.dirname(root_dir)\n          one_up_dir = os.path.dirname(one_up_dir)\n\n        prefix = os.path.commonprefix([root_dir, project_dir])\n        return fullname[len(prefix) + 1:]\n\n      # Not SVN <= 1.6? Try to find a git, hg, or svn top level directory by\n      # searching up from the current path.\n      root_dir = current_dir = os.path.dirname(fullname)\n      while current_dir != os.path.dirname(current_dir):\n        if (os.path.exists(os.path.join(current_dir, \".git\")) or\n            os.path.exists(os.path.join(current_dir, \".hg\")) or\n            os.path.exists(os.path.join(current_dir, \".svn\"))):\n          root_dir = current_dir\n        current_dir = os.path.dirname(current_dir)\n\n      if (os.path.exists(os.path.join(root_dir, \".git\")) or\n          os.path.exists(os.path.join(root_dir, \".hg\")) or\n          os.path.exists(os.path.join(root_dir, \".svn\"))):\n        prefix = os.path.commonprefix([root_dir, project_dir])\n        return fullname[len(prefix) + 1:]\n\n    # Don't know what to do; header guard warnings may be wrong...\n    return fullname\n\n  def Split(self):\n    \"\"\"Splits the file into the directory, basename, and extension.\n\n    For 'chrome/browser/browser.cc', Split() would\n    return ('chrome/browser', 'browser', '.cc')\n\n    Returns:\n      A tuple of (directory, basename, extension).\n    \"\"\"\n\n    googlename = self.RepositoryName()\n    project, rest = os.path.split(googlename)\n    return (project,) + os.path.splitext(rest)\n\n  def BaseName(self):\n    \"\"\"File base name - text after the final slash, before the final period.\"\"\"\n    return self.Split()[1]\n\n  def Extension(self):\n    \"\"\"File extension - text following the final period, includes that period.\"\"\"\n    return self.Split()[2]\n\n  def NoExtension(self):\n    \"\"\"File has no source file extension.\"\"\"\n    return '/'.join(self.Split()[0:2])\n\n  def IsSource(self):\n    \"\"\"File has a source file extension.\"\"\"\n    return _IsSourceExtension(self.Extension()[1:])\n\n\ndef _ShouldPrintError(category, confidence, linenum):\n  \"\"\"If confidence >= verbose, category passes filter and is not suppressed.\"\"\"\n\n  # There are three ways we might decide not to print an error message:\n  # a \"NOLINT(category)\" comment appears in the source,\n  # the verbosity level isn't high enough, or the filters filter it out.\n  if IsErrorSuppressedByNolint(category, linenum):\n    return False\n\n  if confidence < _cpplint_state.verbose_level:\n    return False\n\n  is_filtered = False\n  for one_filter in _Filters():\n    if one_filter.startswith('-'):\n      if category.startswith(one_filter[1:]):\n        is_filtered = True\n    elif one_filter.startswith('+'):\n      if category.startswith(one_filter[1:]):\n        is_filtered = False\n    else:\n      assert False  # should have been checked for in SetFilter.\n  if is_filtered:\n    return False\n\n  return True\n\n\ndef Error(filename, linenum, category, confidence, message):\n  \"\"\"Logs the fact we've found a lint error.\n\n  We log where the error was found, and also our confidence in the error,\n  that is, how certain we are this is a legitimate style regression, and\n  not a misidentification or a use that's sometimes justified.\n\n  False positives can be suppressed by the use of\n  \"cpplint(category)\"  comments on the offending line.  These are\n  parsed into _error_suppressions.\n\n  Args:\n    filename: The name of the file containing the error.\n    linenum: The number of the line containing the error.\n    category: A string used to describe the \"category\" this bug\n      falls under: \"whitespace\", say, or \"runtime\".  Categories\n      may have a hierarchy separated by slashes: \"whitespace/indent\".\n    confidence: A number from 1-5 representing a confidence score for\n      the error, with 5 meaning that we are certain of the problem,\n      and 1 meaning that it could be a legitimate construct.\n    message: The error message.\n  \"\"\"\n  if _ShouldPrintError(category, confidence, linenum):\n    _cpplint_state.IncrementErrorCount(category)\n    if _cpplint_state.output_format == 'vs7':\n      _cpplint_state.PrintError('%s(%s): error cpplint: [%s] %s [%d]\\n' % (\n          filename, linenum, category, message, confidence))\n    elif _cpplint_state.output_format == 'eclipse':\n      sys.stderr.write('%s:%s: warning: %s  [%s] [%d]\\n' % (\n          filename, linenum, message, category, confidence))\n    elif _cpplint_state.output_format == 'junit':\n      _cpplint_state.AddJUnitFailure(filename, linenum, message, category,\n          confidence)\n    elif _cpplint_state.output_format in ['sed', 'gsed']:\n      if message in _SED_FIXUPS:\n        sys.stdout.write(_cpplint_state.output_format + \" -i '%s%s' %s # %s  [%s] [%d]\\n\" % (\n            linenum, _SED_FIXUPS[message], filename, message, category, confidence))\n      else:\n        sys.stderr.write('# %s:%s:  \"%s\"  [%s] [%d]\\n' % (\n            filename, linenum, message, category, confidence))\n    else:\n      final_message = '%s:%s:  %s  [%s] [%d]\\n' % (\n          filename, linenum, message, category, confidence)\n      sys.stderr.write(final_message)\n\n# Matches standard C++ escape sequences per 2.13.2.3 of the C++ standard.\n_RE_PATTERN_CLEANSE_LINE_ESCAPES = re.compile(\n    r'\\\\([abfnrtv?\"\\\\\\']|\\d+|x[0-9a-fA-F]+)')\n# Match a single C style comment on the same line.\n_RE_PATTERN_C_COMMENTS = r'/\\*(?:[^*]|\\*(?!/))*\\*/'\n# Matches multi-line C style comments.\n# This RE is a little bit more complicated than one might expect, because we\n# have to take care of space removals tools so we can handle comments inside\n# statements better.\n# The current rule is: We only clear spaces from both sides when we're at the\n# end of the line. Otherwise, we try to remove spaces from the right side,\n# if this doesn't work we try on left side but only if there's a non-character\n# on the right.\n_RE_PATTERN_CLEANSE_LINE_C_COMMENTS = re.compile(\n    r'(\\s*' + _RE_PATTERN_C_COMMENTS + r'\\s*$|' +\n    _RE_PATTERN_C_COMMENTS + r'\\s+|' +\n    r'\\s+' + _RE_PATTERN_C_COMMENTS + r'(?=\\W)|' +\n    _RE_PATTERN_C_COMMENTS + r')')\n\n\ndef IsCppString(line):\n  \"\"\"Does line terminate so, that the next symbol is in string constant.\n\n  This function does not consider comments at all.\n\n  Args:\n    line: is a partial line of code starting from the 0..n.\n\n  Returns:\n    True, if next character appended to 'line' is inside a\n    string constant.\n  \"\"\"\n\n  line = line.replace(r'\\\\', 'XX')  # after this, \\\\\" does not match to \\\"\n  return ((line.count('\"') - line.count(r'\\\"') - line.count(\"'\\\"'\")) & 1) == 1\n\n\ndef CleanseRawStrings(raw_lines):\n  \"\"\"Removes C++11 raw strings from lines.\n\n    Before:\n      static const char kData[] = R\"(\n          multi-line string\n          )\";\n\n    After:\n      static const char kData[] = \"\"\n          (replaced by blank line)\n          \"\";\n\n  Args:\n    raw_lines: list of raw lines.\n\n  Returns:\n    list of lines with C++11 raw strings replaced by empty strings.\n  \"\"\"\n\n  delimiter = None\n  lines_without_raw_strings = []\n  for line in raw_lines:\n    if delimiter:\n      # Inside a raw string, look for the end\n      end = line.find(delimiter)\n      if end >= 0:\n        # Found the end of the string, match leading space for this\n        # line and resume copying the original lines, and also insert\n        # a \"\" on the last line.\n        leading_space = Match(r'^(\\s*)\\S', line)\n        line = leading_space.group(1) + '\"\"' + line[end + len(delimiter):]\n        delimiter = None\n      else:\n        # Haven't found the end yet, append a blank line.\n        line = '\"\"'\n\n    # Look for beginning of a raw string, and replace them with\n    # empty strings.  This is done in a loop to handle multiple raw\n    # strings on the same line.\n    while delimiter is None:\n      # Look for beginning of a raw string.\n      # See 2.14.15 [lex.string] for syntax.\n      #\n      # Once we have matched a raw string, we check the prefix of the\n      # line to make sure that the line is not part of a single line\n      # comment.  It's done this way because we remove raw strings\n      # before removing comments as opposed to removing comments\n      # before removing raw strings.  This is because there are some\n      # cpplint checks that requires the comments to be preserved, but\n      # we don't want to check comments that are inside raw strings.\n      matched = Match(r'^(.*?)\\b(?:R|u8R|uR|UR|LR)\"([^\\s\\\\()]*)\\((.*)$', line)\n      if (matched and\n          not Match(r'^([^\\'\"]|\\'(\\\\.|[^\\'])*\\'|\"(\\\\.|[^\"])*\")*//',\n                    matched.group(1))):\n        delimiter = ')' + matched.group(2) + '\"'\n\n        end = matched.group(3).find(delimiter)\n        if end >= 0:\n          # Raw string ended on same line\n          line = (matched.group(1) + '\"\"' +\n                  matched.group(3)[end + len(delimiter):])\n          delimiter = None\n        else:\n          # Start of a multi-line raw string\n          line = matched.group(1) + '\"\"'\n      else:\n        break\n\n    lines_without_raw_strings.append(line)\n\n  # TODO(unknown): if delimiter is not None here, we might want to\n  # emit a warning for unterminated string.\n  return lines_without_raw_strings\n\n\ndef FindNextMultiLineCommentStart(lines, lineix):\n  \"\"\"Find the beginning marker for a multiline comment.\"\"\"\n  while lineix < len(lines):\n    if lines[lineix].strip().startswith('/*'):\n      # Only return this marker if the comment goes beyond this line\n      if lines[lineix].strip().find('*/', 2) < 0:\n        return lineix\n    lineix += 1\n  return len(lines)\n\n\ndef FindNextMultiLineCommentEnd(lines, lineix):\n  \"\"\"We are inside a comment, find the end marker.\"\"\"\n  while lineix < len(lines):\n    if lines[lineix].strip().endswith('*/'):\n      return lineix\n    lineix += 1\n  return len(lines)\n\n\ndef RemoveMultiLineCommentsFromRange(lines, begin, end):\n  \"\"\"Clears a range of lines for multi-line comments.\"\"\"\n  # Having // <empty> comments makes the lines non-empty, so we will not get\n  # unnecessary blank line warnings later in the code.\n  for i in range(begin, end):\n    lines[i] = '/**/'\n\n\ndef RemoveMultiLineComments(filename, lines, error):\n  \"\"\"Removes multiline (c-style) comments from lines.\"\"\"\n  lineix = 0\n  while lineix < len(lines):\n    lineix_begin = FindNextMultiLineCommentStart(lines, lineix)\n    if lineix_begin >= len(lines):\n      return\n    lineix_end = FindNextMultiLineCommentEnd(lines, lineix_begin)\n    if lineix_end >= len(lines):\n      error(filename, lineix_begin + 1, 'readability/multiline_comment', 5,\n            'Could not find end of multi-line comment')\n      return\n    RemoveMultiLineCommentsFromRange(lines, lineix_begin, lineix_end + 1)\n    lineix = lineix_end + 1\n\n\ndef CleanseComments(line):\n  \"\"\"Removes //-comments and single-line C-style /* */ comments.\n\n  Args:\n    line: A line of C++ source.\n\n  Returns:\n    The line with single-line comments removed.\n  \"\"\"\n  commentpos = line.find('//')\n  if commentpos != -1 and not IsCppString(line[:commentpos]):\n    line = line[:commentpos].rstrip()\n  # get rid of /* ... */\n  return _RE_PATTERN_CLEANSE_LINE_C_COMMENTS.sub('', line)\n\n\nclass CleansedLines(object):\n  \"\"\"Holds 4 copies of all lines with different preprocessing applied to them.\n\n  1) elided member contains lines without strings and comments.\n  2) lines member contains lines without comments.\n  3) raw_lines member contains all the lines without processing.\n  4) lines_without_raw_strings member is same as raw_lines, but with C++11 raw\n     strings removed.\n  All these members are of <type 'list'>, and of the same length.\n  \"\"\"\n\n  def __init__(self, lines):\n    self.elided = []\n    self.lines = []\n    self.raw_lines = lines\n    self.num_lines = len(lines)\n    self.lines_without_raw_strings = CleanseRawStrings(lines)\n    # # pylint: disable=consider-using-enumerate\n    for linenum in range(len(self.lines_without_raw_strings)):\n      self.lines.append(CleanseComments(\n          self.lines_without_raw_strings[linenum]))\n      elided = self._CollapseStrings(self.lines_without_raw_strings[linenum])\n      self.elided.append(CleanseComments(elided))\n\n  def NumLines(self):\n    \"\"\"Returns the number of lines represented.\"\"\"\n    return self.num_lines\n\n  @staticmethod\n  def _CollapseStrings(elided):\n    \"\"\"Collapses strings and chars on a line to simple \"\" or '' blocks.\n\n    We nix strings first so we're not fooled by text like '\"http://\"'\n\n    Args:\n      elided: The line being processed.\n\n    Returns:\n      The line with collapsed strings.\n    \"\"\"\n    if _RE_PATTERN_INCLUDE.match(elided):\n      return elided\n\n    # Remove escaped characters first to make quote/single quote collapsing\n    # basic.  Things that look like escaped characters shouldn't occur\n    # outside of strings and chars.\n    elided = _RE_PATTERN_CLEANSE_LINE_ESCAPES.sub('', elided)\n\n    # Replace quoted strings and digit separators.  Both single quotes\n    # and double quotes are processed in the same loop, otherwise\n    # nested quotes wouldn't work.\n    collapsed = ''\n    while True:\n      # Find the first quote character\n      match = Match(r'^([^\\'\"]*)([\\'\"])(.*)$', elided)\n      if not match:\n        collapsed += elided\n        break\n      head, quote, tail = match.groups()\n\n      if quote == '\"':\n        # Collapse double quoted strings\n        second_quote = tail.find('\"')\n        if second_quote >= 0:\n          collapsed += head + '\"\"'\n          elided = tail[second_quote + 1:]\n        else:\n          # Unmatched double quote, don't bother processing the rest\n          # of the line since this is probably a multiline string.\n          collapsed += elided\n          break\n      else:\n        # Found single quote, check nearby text to eliminate digit separators.\n        #\n        # There is no special handling for floating point here, because\n        # the integer/fractional/exponent parts would all be parsed\n        # correctly as long as there are digits on both sides of the\n        # separator.  So we are fine as long as we don't see something\n        # like \"0.'3\" (gcc 4.9.0 will not allow this literal).\n        if Search(r'\\b(?:0[bBxX]?|[1-9])[0-9a-fA-F]*$', head):\n          match_literal = Match(r'^((?:\\'?[0-9a-zA-Z_])*)(.*)$', \"'\" + tail)\n          collapsed += head + match_literal.group(1).replace(\"'\", '')\n          elided = match_literal.group(2)\n        else:\n          second_quote = tail.find('\\'')\n          if second_quote >= 0:\n            collapsed += head + \"''\"\n            elided = tail[second_quote + 1:]\n          else:\n            # Unmatched single quote\n            collapsed += elided\n            break\n\n    return collapsed\n\n\ndef FindEndOfExpressionInLine(line, startpos, stack):\n  \"\"\"Find the position just after the end of current parenthesized expression.\n\n  Args:\n    line: a CleansedLines line.\n    startpos: start searching at this position.\n    stack: nesting stack at startpos.\n\n  Returns:\n    On finding matching end: (index just after matching end, None)\n    On finding an unclosed expression: (-1, None)\n    Otherwise: (-1, new stack at end of this line)\n  \"\"\"\n  for i in xrange(startpos, len(line)):\n    char = line[i]\n    if char in '([{':\n      # Found start of parenthesized expression, push to expression stack\n      stack.append(char)\n    elif char == '<':\n      # Found potential start of template argument list\n      if i > 0 and line[i - 1] == '<':\n        # Left shift operator\n        if stack and stack[-1] == '<':\n          stack.pop()\n          if not stack:\n            return (-1, None)\n      elif i > 0 and Search(r'\\boperator\\s*$', line[0:i]):\n        # operator<, don't add to stack\n        continue\n      else:\n        # Tentative start of template argument list\n        stack.append('<')\n    elif char in ')]}':\n      # Found end of parenthesized expression.\n      #\n      # If we are currently expecting a matching '>', the pending '<'\n      # must have been an operator.  Remove them from expression stack.\n      while stack and stack[-1] == '<':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n      if ((stack[-1] == '(' and char == ')') or\n          (stack[-1] == '[' and char == ']') or\n          (stack[-1] == '{' and char == '}')):\n        stack.pop()\n        if not stack:\n          return (i + 1, None)\n      else:\n        # Mismatched parentheses\n        return (-1, None)\n    elif char == '>':\n      # Found potential end of template argument list.\n\n      # Ignore \"->\" and operator functions\n      if (i > 0 and\n          (line[i - 1] == '-' or Search(r'\\boperator\\s*$', line[0:i - 1]))):\n        continue\n\n      # Pop the stack if there is a matching '<'.  Otherwise, ignore\n      # this '>' since it must be an operator.\n      if stack:\n        if stack[-1] == '<':\n          stack.pop()\n          if not stack:\n            return (i + 1, None)\n    elif char == ';':\n      # Found something that look like end of statements.  If we are currently\n      # expecting a '>', the matching '<' must have been an operator, since\n      # template argument list should not contain statements.\n      while stack and stack[-1] == '<':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n\n  # Did not find end of expression or unbalanced parentheses on this line\n  return (-1, stack)\n\n\ndef CloseExpression(clean_lines, linenum, pos):\n  \"\"\"If input points to ( or { or [ or <, finds the position that closes it.\n\n  If lines[linenum][pos] points to a '(' or '{' or '[' or '<', finds the\n  linenum/pos that correspond to the closing of the expression.\n\n  TODO(unknown): cpplint spends a fair bit of time matching parentheses.\n  Ideally we would want to index all opening and closing parentheses once\n  and have CloseExpression be just a simple lookup, but due to preprocessor\n  tricks, this is not so easy.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    pos: A position on the line.\n\n  Returns:\n    A tuple (line, linenum, pos) pointer *past* the closing brace, or\n    (line, len(lines), -1) if we never find a close.  Note we ignore\n    strings and comments when matching; and the line we return is the\n    'cleansed' line at linenum.\n  \"\"\"\n\n  line = clean_lines.elided[linenum]\n  if (line[pos] not in '({[<') or Match(r'<[<=]', line[pos:]):\n    return (line, clean_lines.NumLines(), -1)\n\n  # Check first line\n  (end_pos, stack) = FindEndOfExpressionInLine(line, pos, [])\n  if end_pos > -1:\n    return (line, linenum, end_pos)\n\n  # Continue scanning forward\n  while stack and linenum < clean_lines.NumLines() - 1:\n    linenum += 1\n    line = clean_lines.elided[linenum]\n    (end_pos, stack) = FindEndOfExpressionInLine(line, 0, stack)\n    if end_pos > -1:\n      return (line, linenum, end_pos)\n\n  # Did not find end of expression before end of file, give up\n  return (line, clean_lines.NumLines(), -1)\n\n\ndef FindStartOfExpressionInLine(line, endpos, stack):\n  \"\"\"Find position at the matching start of current expression.\n\n  This is almost the reverse of FindEndOfExpressionInLine, but note\n  that the input position and returned position differs by 1.\n\n  Args:\n    line: a CleansedLines line.\n    endpos: start searching at this position.\n    stack: nesting stack at endpos.\n\n  Returns:\n    On finding matching start: (index at matching start, None)\n    On finding an unclosed expression: (-1, None)\n    Otherwise: (-1, new stack at beginning of this line)\n  \"\"\"\n  i = endpos\n  while i >= 0:\n    char = line[i]\n    if char in ')]}':\n      # Found end of expression, push to expression stack\n      stack.append(char)\n    elif char == '>':\n      # Found potential end of template argument list.\n      #\n      # Ignore it if it's a \"->\" or \">=\" or \"operator>\"\n      if (i > 0 and\n          (line[i - 1] == '-' or\n           Match(r'\\s>=\\s', line[i - 1:]) or\n           Search(r'\\boperator\\s*$', line[0:i]))):\n        i -= 1\n      else:\n        stack.append('>')\n    elif char == '<':\n      # Found potential start of template argument list\n      if i > 0 and line[i - 1] == '<':\n        # Left shift operator\n        i -= 1\n      else:\n        # If there is a matching '>', we can pop the expression stack.\n        # Otherwise, ignore this '<' since it must be an operator.\n        if stack and stack[-1] == '>':\n          stack.pop()\n          if not stack:\n            return (i, None)\n    elif char in '([{':\n      # Found start of expression.\n      #\n      # If there are any unmatched '>' on the stack, they must be\n      # operators.  Remove those.\n      while stack and stack[-1] == '>':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n      if ((char == '(' and stack[-1] == ')') or\n          (char == '[' and stack[-1] == ']') or\n          (char == '{' and stack[-1] == '}')):\n        stack.pop()\n        if not stack:\n          return (i, None)\n      else:\n        # Mismatched parentheses\n        return (-1, None)\n    elif char == ';':\n      # Found something that look like end of statements.  If we are currently\n      # expecting a '<', the matching '>' must have been an operator, since\n      # template argument list should not contain statements.\n      while stack and stack[-1] == '>':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n\n    i -= 1\n\n  return (-1, stack)\n\n\ndef ReverseCloseExpression(clean_lines, linenum, pos):\n  \"\"\"If input points to ) or } or ] or >, finds the position that opens it.\n\n  If lines[linenum][pos] points to a ')' or '}' or ']' or '>', finds the\n  linenum/pos that correspond to the opening of the expression.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    pos: A position on the line.\n\n  Returns:\n    A tuple (line, linenum, pos) pointer *at* the opening brace, or\n    (line, 0, -1) if we never find the matching opening brace.  Note\n    we ignore strings and comments when matching; and the line we\n    return is the 'cleansed' line at linenum.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  if line[pos] not in ')}]>':\n    return (line, 0, -1)\n\n  # Check last line\n  (start_pos, stack) = FindStartOfExpressionInLine(line, pos, [])\n  if start_pos > -1:\n    return (line, linenum, start_pos)\n\n  # Continue scanning backward\n  while stack and linenum > 0:\n    linenum -= 1\n    line = clean_lines.elided[linenum]\n    (start_pos, stack) = FindStartOfExpressionInLine(line, len(line) - 1, stack)\n    if start_pos > -1:\n      return (line, linenum, start_pos)\n\n  # Did not find start of expression before beginning of file, give up\n  return (line, 0, -1)\n\n\ndef CheckForCopyright(filename, lines, error):\n  \"\"\"Logs an error if no Copyright message appears at the top of the file.\"\"\"\n\n  # We'll say it should occur by line 10. Don't forget there's a\n  # placeholder line at the front.\n  for line in xrange(1, min(len(lines), 11)):\n    if re.search(r'Copyright', lines[line], re.I): break\n  else:                       # means no copyright line was found\n    error(filename, 0, 'legal/copyright', 5,\n          'No copyright message found.  '\n          'You should have a line: \"Copyright [year] <Copyright Owner>\"')\n\n\ndef GetIndentLevel(line):\n  \"\"\"Return the number of leading spaces in line.\n\n  Args:\n    line: A string to check.\n\n  Returns:\n    An integer count of leading spaces, possibly zero.\n  \"\"\"\n  indent = Match(r'^( *)\\S', line)\n  if indent:\n    return len(indent.group(1))\n  else:\n    return 0\n\ndef PathSplitToList(path):\n  \"\"\"Returns the path split into a list by the separator.\n\n  Args:\n    path: An absolute or relative path (e.g. '/a/b/c/' or '../a')\n\n  Returns:\n    A list of path components (e.g. ['a', 'b', 'c]).\n  \"\"\"\n  lst = []\n  while True:\n    (head, tail) = os.path.split(path)\n    if head == path:  # absolute paths end\n      lst.append(head)\n      break\n    if tail == path:  # relative paths end\n      lst.append(tail)\n      break\n\n    path = head\n    lst.append(tail)\n\n  lst.reverse()\n  return lst\n\ndef GetHeaderGuardCPPVariable(filename):\n  \"\"\"Returns the CPP variable that should be used as a header guard.\n\n  Args:\n    filename: The name of a C++ header file.\n\n  Returns:\n    The CPP variable that should be used as a header guard in the\n    named file.\n\n  \"\"\"\n\n  # Restores original filename in case that cpplint is invoked from Emacs's\n  # flymake.\n  filename = re.sub(r'_flymake\\.h$', '.h', filename)\n  filename = re.sub(r'/\\.flymake/([^/]*)$', r'/\\1', filename)\n  # Replace 'c++' with 'cpp'.\n  filename = filename.replace('C++', 'cpp').replace('c++', 'cpp')\n\n  fileinfo = FileInfo(filename)\n  file_path_from_root = fileinfo.RepositoryName()\n\n  def FixupPathFromRoot():\n    if _root_debug:\n      sys.stderr.write(\"\\n_root fixup, _root = '%s', repository name = '%s'\\n\"\n          % (_root, fileinfo.RepositoryName()))\n\n    # Process the file path with the --root flag if it was set.\n    if not _root:\n      if _root_debug:\n        sys.stderr.write(\"_root unspecified\\n\")\n      return file_path_from_root\n\n    def StripListPrefix(lst, prefix):\n      # f(['x', 'y'], ['w, z']) -> None  (not a valid prefix)\n      if lst[:len(prefix)] != prefix:\n        return None\n      # f(['a, 'b', 'c', 'd'], ['a', 'b']) -> ['c', 'd']\n      return lst[(len(prefix)):]\n\n    # root behavior:\n    #   --root=subdir , lstrips subdir from the header guard\n    maybe_path = StripListPrefix(PathSplitToList(file_path_from_root),\n                                 PathSplitToList(_root))\n\n    if _root_debug:\n      sys.stderr.write((\"_root lstrip (maybe_path=%s, file_path_from_root=%s,\" +\n          \" _root=%s)\\n\") % (maybe_path, file_path_from_root, _root))\n\n    if maybe_path:\n      return os.path.join(*maybe_path)\n\n    #   --root=.. , will prepend the outer directory to the header guard\n    full_path = fileinfo.FullName()\n    # adapt slashes for windows\n    root_abspath = os.path.abspath(_root).replace('\\\\', '/')\n\n    maybe_path = StripListPrefix(PathSplitToList(full_path),\n                                 PathSplitToList(root_abspath))\n\n    if _root_debug:\n      sys.stderr.write((\"_root prepend (maybe_path=%s, full_path=%s, \" +\n          \"root_abspath=%s)\\n\") % (maybe_path, full_path, root_abspath))\n\n    if maybe_path:\n      return os.path.join(*maybe_path)\n\n    if _root_debug:\n      sys.stderr.write(\"_root ignore, returning %s\\n\" % (file_path_from_root))\n\n    #   --root=FAKE_DIR is ignored\n    return file_path_from_root\n\n  file_path_from_root = FixupPathFromRoot()\n  return re.sub(r'[^a-zA-Z0-9]', '_', file_path_from_root).upper() + '_'\n\n\ndef CheckForHeaderGuard(filename, clean_lines, error):\n  \"\"\"Checks that the file contains a header guard.\n\n  Logs an error if no #ifndef header guard is present.  For other\n  headers, checks that the full pathname is used.\n\n  Args:\n    filename: The name of the C++ header file.\n    clean_lines: A CleansedLines instance containing the file.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Don't check for header guards if there are error suppression\n  # comments somewhere in this file.\n  #\n  # Because this is silencing a warning for a nonexistent line, we\n  # only support the very specific NOLINT(build/header_guard) syntax,\n  # and not the general NOLINT or NOLINT(*) syntax.\n  raw_lines = clean_lines.lines_without_raw_strings\n  for i in raw_lines:\n    if Search(r'//\\s*NOLINT\\(build/header_guard\\)', i):\n      return\n\n  # Allow pragma once instead of header guards\n  for i in raw_lines:\n    if Search(r'^\\s*#pragma\\s+once', i):\n      return\n\n  cppvar = GetHeaderGuardCPPVariable(filename)\n\n  ifndef = ''\n  ifndef_linenum = 0\n  define = ''\n  endif = ''\n  endif_linenum = 0\n  for linenum, line in enumerate(raw_lines):\n    linesplit = line.split()\n    if len(linesplit) >= 2:\n      # find the first occurrence of #ifndef and #define, save arg\n      if not ifndef and linesplit[0] == '#ifndef':\n        # set ifndef to the header guard presented on the #ifndef line.\n        ifndef = linesplit[1]\n        ifndef_linenum = linenum\n      if not define and linesplit[0] == '#define':\n        define = linesplit[1]\n    # find the last occurrence of #endif, save entire line\n    if line.startswith('#endif'):\n      endif = line\n      endif_linenum = linenum\n\n  if not ifndef or not define or ifndef != define:\n    error(filename, 0, 'build/header_guard', 5,\n          'No #ifndef header guard found, suggested CPP variable is: %s' %\n          cppvar)\n    return\n\n  # The guard should be PATH_FILE_H_, but we also allow PATH_FILE_H__\n  # for backward compatibility.\n  if ifndef != cppvar:\n    error_level = 0\n    if ifndef != cppvar + '_':\n      error_level = 5\n\n    ParseNolintSuppressions(filename, raw_lines[ifndef_linenum], ifndef_linenum,\n                            error)\n    error(filename, ifndef_linenum, 'build/header_guard', error_level,\n          '#ifndef header guard has wrong style, please use: %s' % cppvar)\n\n  # Check for \"//\" comments on endif line.\n  ParseNolintSuppressions(filename, raw_lines[endif_linenum], endif_linenum,\n                          error)\n  match = Match(r'#endif\\s*//\\s*' + cppvar + r'(_)?\\b', endif)\n  if match:\n    if match.group(1) == '_':\n      # Issue low severity warning for deprecated double trailing underscore\n      error(filename, endif_linenum, 'build/header_guard', 0,\n            '#endif line should be \"#endif  // %s\"' % cppvar)\n    return\n\n  # Didn't find the corresponding \"//\" comment.  If this file does not\n  # contain any \"//\" comments at all, it could be that the compiler\n  # only wants \"/**/\" comments, look for those instead.\n  no_single_line_comments = True\n  for i in xrange(1, len(raw_lines) - 1):\n    line = raw_lines[i]\n    if Match(r'^(?:(?:\\'(?:\\.|[^\\'])*\\')|(?:\"(?:\\.|[^\"])*\")|[^\\'\"])*//', line):\n      no_single_line_comments = False\n      break\n\n  if no_single_line_comments:\n    match = Match(r'#endif\\s*/\\*\\s*' + cppvar + r'(_)?\\s*\\*/', endif)\n    if match:\n      if match.group(1) == '_':\n        # Low severity warning for double trailing underscore\n        error(filename, endif_linenum, 'build/header_guard', 0,\n              '#endif line should be \"#endif  /* %s */\"' % cppvar)\n      return\n\n  # Didn't find anything\n  error(filename, endif_linenum, 'build/header_guard', 5,\n        '#endif line should be \"#endif  // %s\"' % cppvar)\n\n\ndef CheckHeaderFileIncluded(filename, include_state, error):\n  \"\"\"Logs an error if a source file does not include its header.\"\"\"\n\n  # Do not check test files\n  fileinfo = FileInfo(filename)\n  if Search(_TEST_FILE_SUFFIX, fileinfo.BaseName()):\n    return\n\n  for ext in GetHeaderExtensions():\n    basefilename = filename[0:len(filename) - len(fileinfo.Extension())]\n    headerfile = basefilename + '.' + ext\n    if not os.path.exists(headerfile):\n      continue\n    headername = FileInfo(headerfile).RepositoryName()\n    first_include = None\n    include_uses_unix_dir_aliases = False\n    for section_list in include_state.include_list:\n      for f in section_list:\n        include_text = f[0]\n        if \"./\" in include_text:\n          include_uses_unix_dir_aliases = True\n        if headername in include_text or include_text in headername:\n          return\n        if not first_include:\n          first_include = f[1]\n\n    message = '%s should include its header file %s' % (fileinfo.RepositoryName(), headername)\n    if include_uses_unix_dir_aliases:\n      message += \". Relative paths like . and .. are not allowed.\"\n\n    error(filename, first_include, 'build/include', 5, message)\n\n\ndef CheckForBadCharacters(filename, lines, error):\n  \"\"\"Logs an error for each line containing bad characters.\n\n  Two kinds of bad characters:\n\n  1. Unicode replacement characters: These indicate that either the file\n  contained invalid UTF-8 (likely) or Unicode replacement characters (which\n  it shouldn't).  Note that it's possible for this to throw off line\n  numbering if the invalid UTF-8 occurred adjacent to a newline.\n\n  2. NUL bytes.  These are problematic for some tools.\n\n  Args:\n    filename: The name of the current file.\n    lines: An array of strings, each representing a line of the file.\n    error: The function to call with any errors found.\n  \"\"\"\n  for linenum, line in enumerate(lines):\n    if unicode_escape_decode('\\ufffd') in line:\n      error(filename, linenum, 'readability/utf8', 5,\n            'Line contains invalid UTF-8 (or Unicode replacement character).')\n    if '\\0' in line:\n      error(filename, linenum, 'readability/nul', 5, 'Line contains NUL byte.')\n\n\ndef CheckForNewlineAtEOF(filename, lines, error):\n  \"\"\"Logs an error if there is no newline char at the end of the file.\n\n  Args:\n    filename: The name of the current file.\n    lines: An array of strings, each representing a line of the file.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # The array lines() was created by adding two newlines to the\n  # original file (go figure), then splitting on \\n.\n  # To verify that the file ends in \\n, we just have to make sure the\n  # last-but-two element of lines() exists and is empty.\n  if len(lines) < 3 or lines[-2]:\n    error(filename, len(lines) - 2, 'whitespace/ending_newline', 5,\n          'Could not find a newline character at the end of the file.')\n\n\ndef CheckForMultilineCommentsAndStrings(filename, clean_lines, linenum, error):\n  \"\"\"Logs an error if we see /* ... */ or \"...\" that extend past one line.\n\n  /* ... */ comments are legit inside macros, for one line.\n  Otherwise, we prefer // comments, so it's ok to warn about the\n  other.  Likewise, it's ok for strings to extend across multiple\n  lines, as long as a line continuation character (backslash)\n  terminates each line. Although not currently prohibited by the C++\n  style guide, it's ugly and unnecessary. We don't do well with either\n  in this lint program, so we warn about both.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Remove all \\\\ (escaped backslashes) from the line. They are OK, and the\n  # second (escaped) slash may trigger later \\\" detection erroneously.\n  line = line.replace('\\\\\\\\', '')\n\n  if line.count('/*') > line.count('*/'):\n    error(filename, linenum, 'readability/multiline_comment', 5,\n          'Complex multi-line /*...*/-style comment found. '\n          'Lint may give bogus warnings.  '\n          'Consider replacing these with //-style comments, '\n          'with #if 0...#endif, '\n          'or with more clearly structured multi-line comments.')\n\n  if (line.count('\"') - line.count('\\\\\"')) % 2:\n    error(filename, linenum, 'readability/multiline_string', 5,\n          'Multi-line string (\"...\") found.  This lint script doesn\\'t '\n          'do well with such strings, and may give bogus warnings.  '\n          'Use C++11 raw strings or concatenation instead.')\n\n\n# (non-threadsafe name, thread-safe alternative, validation pattern)\n#\n# The validation pattern is used to eliminate false positives such as:\n#  _rand();               // false positive due to substring match.\n#  ->rand();              // some member function rand().\n#  ACMRandom rand(seed);  // some variable named rand.\n#  ISAACRandom rand();    // another variable named rand.\n#\n# Basically we require the return value of these functions to be used\n# in some expression context on the same line by matching on some\n# operator before the function name.  This eliminates constructors and\n# member function calls.\n_UNSAFE_FUNC_PREFIX = r'(?:[-+*/=%^&|(<]\\s*|>\\s+)'\n_THREADING_LIST = (\n    ('asctime(', 'asctime_r(', _UNSAFE_FUNC_PREFIX + r'asctime\\([^)]+\\)'),\n    ('ctime(', 'ctime_r(', _UNSAFE_FUNC_PREFIX + r'ctime\\([^)]+\\)'),\n    ('getgrgid(', 'getgrgid_r(', _UNSAFE_FUNC_PREFIX + r'getgrgid\\([^)]+\\)'),\n    ('getgrnam(', 'getgrnam_r(', _UNSAFE_FUNC_PREFIX + r'getgrnam\\([^)]+\\)'),\n    ('getlogin(', 'getlogin_r(', _UNSAFE_FUNC_PREFIX + r'getlogin\\(\\)'),\n    ('getpwnam(', 'getpwnam_r(', _UNSAFE_FUNC_PREFIX + r'getpwnam\\([^)]+\\)'),\n    ('getpwuid(', 'getpwuid_r(', _UNSAFE_FUNC_PREFIX + r'getpwuid\\([^)]+\\)'),\n    ('gmtime(', 'gmtime_r(', _UNSAFE_FUNC_PREFIX + r'gmtime\\([^)]+\\)'),\n    ('localtime(', 'localtime_r(', _UNSAFE_FUNC_PREFIX + r'localtime\\([^)]+\\)'),\n    ('rand(', 'rand_r(', _UNSAFE_FUNC_PREFIX + r'rand\\(\\)'),\n    ('strtok(', 'strtok_r(',\n     _UNSAFE_FUNC_PREFIX + r'strtok\\([^)]+\\)'),\n    ('ttyname(', 'ttyname_r(', _UNSAFE_FUNC_PREFIX + r'ttyname\\([^)]+\\)'),\n    )\n\n\ndef CheckPosixThreading(filename, clean_lines, linenum, error):\n  \"\"\"Checks for calls to thread-unsafe functions.\n\n  Much code has been originally written without consideration of\n  multi-threading. Also, engineers are relying on their old experience;\n  they have learned posix before threading extensions were added. These\n  tests guide the engineers to use thread-safe functions (when using\n  posix directly).\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  for single_thread_func, multithread_safe_func, pattern in _THREADING_LIST:\n    # Additional pattern matching check to confirm that this is the\n    # function we are looking for\n    if Search(pattern, line):\n      error(filename, linenum, 'runtime/threadsafe_fn', 2,\n            'Consider using ' + multithread_safe_func +\n            '...) instead of ' + single_thread_func +\n            '...) for improved thread safety.')\n\n\ndef CheckVlogArguments(filename, clean_lines, linenum, error):\n  \"\"\"Checks that VLOG() is only used for defining a logging level.\n\n  For example, VLOG(2) is correct. VLOG(INFO), VLOG(WARNING), VLOG(ERROR), and\n  VLOG(FATAL) are not.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  if Search(r'\\bVLOG\\((INFO|ERROR|WARNING|DFATAL|FATAL)\\)', line):\n    error(filename, linenum, 'runtime/vlog', 5,\n          'VLOG() should be used with numeric verbosity level.  '\n          'Use LOG() if you want symbolic severity levels.')\n\n# Matches invalid increment: *count++, which moves pointer instead of\n# incrementing a value.\n_RE_PATTERN_INVALID_INCREMENT = re.compile(\n    r'^\\s*\\*\\w+(\\+\\+|--);')\n\n\ndef CheckInvalidIncrement(filename, clean_lines, linenum, error):\n  \"\"\"Checks for invalid increment *count++.\n\n  For example following function:\n  void increment_counter(int* count) {\n    *count++;\n  }\n  is invalid, because it effectively does count++, moving pointer, and should\n  be replaced with ++*count, (*count)++ or *count += 1.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  if _RE_PATTERN_INVALID_INCREMENT.match(line):\n    error(filename, linenum, 'runtime/invalid_increment', 5,\n          'Changing pointer instead of value (or unused value of operator*).')\n\n\ndef IsMacroDefinition(clean_lines, linenum):\n  if Search(r'^#define', clean_lines[linenum]):\n    return True\n\n  if linenum > 0 and Search(r'\\\\$', clean_lines[linenum - 1]):\n    return True\n\n  return False\n\n\ndef IsForwardClassDeclaration(clean_lines, linenum):\n  return Match(r'^\\s*(\\btemplate\\b)*.*class\\s+\\w+;\\s*$', clean_lines[linenum])\n\n\nclass _BlockInfo(object):\n  \"\"\"Stores information about a generic block of code.\"\"\"\n\n  def __init__(self, linenum, seen_open_brace):\n    self.starting_linenum = linenum\n    self.seen_open_brace = seen_open_brace\n    self.open_parentheses = 0\n    self.inline_asm = _NO_ASM\n    self.check_namespace_indentation = False\n\n  def CheckBegin(self, filename, clean_lines, linenum, error):\n    \"\"\"Run checks that applies to text up to the opening brace.\n\n    This is mostly for checking the text after the class identifier\n    and the \"{\", usually where the base class is specified.  For other\n    blocks, there isn't much to check, so we always pass.\n\n    Args:\n      filename: The name of the current file.\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      error: The function to call with any errors found.\n    \"\"\"\n    pass\n\n  def CheckEnd(self, filename, clean_lines, linenum, error):\n    \"\"\"Run checks that applies to text after the closing brace.\n\n    This is mostly used for checking end of namespace comments.\n\n    Args:\n      filename: The name of the current file.\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      error: The function to call with any errors found.\n    \"\"\"\n    pass\n\n  def IsBlockInfo(self):\n    \"\"\"Returns true if this block is a _BlockInfo.\n\n    This is convenient for verifying that an object is an instance of\n    a _BlockInfo, but not an instance of any of the derived classes.\n\n    Returns:\n      True for this class, False for derived classes.\n    \"\"\"\n    return self.__class__ == _BlockInfo\n\n\nclass _ExternCInfo(_BlockInfo):\n  \"\"\"Stores information about an 'extern \"C\"' block.\"\"\"\n\n  def __init__(self, linenum):\n    _BlockInfo.__init__(self, linenum, True)\n\n\nclass _ClassInfo(_BlockInfo):\n  \"\"\"Stores information about a class.\"\"\"\n\n  def __init__(self, name, class_or_struct, clean_lines, linenum):\n    _BlockInfo.__init__(self, linenum, False)\n    self.name = name\n    self.is_derived = False\n    self.check_namespace_indentation = True\n    if class_or_struct == 'struct':\n      self.access = 'public'\n      self.is_struct = True\n    else:\n      self.access = 'private'\n      self.is_struct = False\n\n    # Remember initial indentation level for this class.  Using raw_lines here\n    # instead of elided to account for leading comments.\n    self.class_indent = GetIndentLevel(clean_lines.raw_lines[linenum])\n\n    # Try to find the end of the class.  This will be confused by things like:\n    #   class A {\n    #   } *x = { ...\n    #\n    # But it's still good enough for CheckSectionSpacing.\n    self.last_line = 0\n    depth = 0\n    for i in range(linenum, clean_lines.NumLines()):\n      line = clean_lines.elided[i]\n      depth += line.count('{') - line.count('}')\n      if not depth:\n        self.last_line = i\n        break\n\n  def CheckBegin(self, filename, clean_lines, linenum, error):\n    # Look for a bare ':'\n    if Search('(^|[^:]):($|[^:])', clean_lines.elided[linenum]):\n      self.is_derived = True\n\n  def CheckEnd(self, filename, clean_lines, linenum, error):\n    # If there is a DISALLOW macro, it should appear near the end of\n    # the class.\n    seen_last_thing_in_class = False\n    for i in xrange(linenum - 1, self.starting_linenum, -1):\n      match = Search(\n          r'\\b(DISALLOW_COPY_AND_ASSIGN|DISALLOW_IMPLICIT_CONSTRUCTORS)\\(' +\n          self.name + r'\\)',\n          clean_lines.elided[i])\n      if match:\n        if seen_last_thing_in_class:\n          error(filename, i, 'readability/constructors', 3,\n                match.group(1) + ' should be the last thing in the class')\n        break\n\n      if not Match(r'^\\s*$', clean_lines.elided[i]):\n        seen_last_thing_in_class = True\n\n    # Check that closing brace is aligned with beginning of the class.\n    # Only do this if the closing brace is indented by only whitespaces.\n    # This means we will not check single-line class definitions.\n    indent = Match(r'^( *)\\}', clean_lines.elided[linenum])\n    if indent and len(indent.group(1)) != self.class_indent:\n      if self.is_struct:\n        parent = 'struct ' + self.name\n      else:\n        parent = 'class ' + self.name\n      error(filename, linenum, 'whitespace/indent', 3,\n            'Closing brace should be aligned with beginning of %s' % parent)\n\n\nclass _NamespaceInfo(_BlockInfo):\n  \"\"\"Stores information about a namespace.\"\"\"\n\n  def __init__(self, name, linenum):\n    _BlockInfo.__init__(self, linenum, False)\n    self.name = name or ''\n    self.check_namespace_indentation = True\n\n  def CheckEnd(self, filename, clean_lines, linenum, error):\n    \"\"\"Check end of namespace comments.\"\"\"\n    line = clean_lines.raw_lines[linenum]\n\n    # Check how many lines is enclosed in this namespace.  Don't issue\n    # warning for missing namespace comments if there aren't enough\n    # lines.  However, do apply checks if there is already an end of\n    # namespace comment and it's incorrect.\n    #\n    # TODO(unknown): We always want to check end of namespace comments\n    # if a namespace is large, but sometimes we also want to apply the\n    # check if a short namespace contained nontrivial things (something\n    # other than forward declarations).  There is currently no logic on\n    # deciding what these nontrivial things are, so this check is\n    # triggered by namespace size only, which works most of the time.\n    if (linenum - self.starting_linenum < 10\n        and not Match(r'^\\s*};*\\s*(//|/\\*).*\\bnamespace\\b', line)):\n      return\n\n    # Look for matching comment at end of namespace.\n    #\n    # Note that we accept C style \"/* */\" comments for terminating\n    # namespaces, so that code that terminate namespaces inside\n    # preprocessor macros can be cpplint clean.\n    #\n    # We also accept stuff like \"// end of namespace <name>.\" with the\n    # period at the end.\n    #\n    # Besides these, we don't accept anything else, otherwise we might\n    # get false negatives when existing comment is a substring of the\n    # expected namespace.\n    if self.name:\n      # Named namespace\n      if not Match((r'^\\s*};*\\s*(//|/\\*).*\\bnamespace\\s+' +\n                    re.escape(self.name) + r'[\\*/\\.\\\\\\s]*$'),\n                   line):\n        error(filename, linenum, 'readability/namespace', 5,\n              'Namespace should be terminated with \"// namespace %s\"' %\n              self.name)\n    else:\n      # Anonymous namespace\n      if not Match(r'^\\s*};*\\s*(//|/\\*).*\\bnamespace[\\*/\\.\\\\\\s]*$', line):\n        # If \"// namespace anonymous\" or \"// anonymous namespace (more text)\",\n        # mention \"// anonymous namespace\" as an acceptable form\n        if Match(r'^\\s*}.*\\b(namespace anonymous|anonymous namespace)\\b', line):\n          error(filename, linenum, 'readability/namespace', 5,\n                'Anonymous namespace should be terminated with \"// namespace\"'\n                ' or \"// anonymous namespace\"')\n        else:\n          error(filename, linenum, 'readability/namespace', 5,\n                'Anonymous namespace should be terminated with \"// namespace\"')\n\n\nclass _PreprocessorInfo(object):\n  \"\"\"Stores checkpoints of nesting stacks when #if/#else is seen.\"\"\"\n\n  def __init__(self, stack_before_if):\n    # The entire nesting stack before #if\n    self.stack_before_if = stack_before_if\n\n    # The entire nesting stack up to #else\n    self.stack_before_else = []\n\n    # Whether we have already seen #else or #elif\n    self.seen_else = False\n\n\nclass NestingState(object):\n  \"\"\"Holds states related to parsing braces.\"\"\"\n\n  def __init__(self):\n    # Stack for tracking all braces.  An object is pushed whenever we\n    # see a \"{\", and popped when we see a \"}\".  Only 3 types of\n    # objects are possible:\n    # - _ClassInfo: a class or struct.\n    # - _NamespaceInfo: a namespace.\n    # - _BlockInfo: some other type of block.\n    self.stack = []\n\n    # Top of the previous stack before each Update().\n    #\n    # Because the nesting_stack is updated at the end of each line, we\n    # had to do some convoluted checks to find out what is the current\n    # scope at the beginning of the line.  This check is simplified by\n    # saving the previous top of nesting stack.\n    #\n    # We could save the full stack, but we only need the top.  Copying\n    # the full nesting stack would slow down cpplint by ~10%.\n    self.previous_stack_top = []\n\n    # Stack of _PreprocessorInfo objects.\n    self.pp_stack = []\n\n  def SeenOpenBrace(self):\n    \"\"\"Check if we have seen the opening brace for the innermost block.\n\n    Returns:\n      True if we have seen the opening brace, False if the innermost\n      block is still expecting an opening brace.\n    \"\"\"\n    return (not self.stack) or self.stack[-1].seen_open_brace\n\n  def InNamespaceBody(self):\n    \"\"\"Check if we are currently one level inside a namespace body.\n\n    Returns:\n      True if top of the stack is a namespace block, False otherwise.\n    \"\"\"\n    return self.stack and isinstance(self.stack[-1], _NamespaceInfo)\n\n  def InExternC(self):\n    \"\"\"Check if we are currently one level inside an 'extern \"C\"' block.\n\n    Returns:\n      True if top of the stack is an extern block, False otherwise.\n    \"\"\"\n    return self.stack and isinstance(self.stack[-1], _ExternCInfo)\n\n  def InClassDeclaration(self):\n    \"\"\"Check if we are currently one level inside a class or struct declaration.\n\n    Returns:\n      True if top of the stack is a class/struct, False otherwise.\n    \"\"\"\n    return self.stack and isinstance(self.stack[-1], _ClassInfo)\n\n  def InAsmBlock(self):\n    \"\"\"Check if we are currently one level inside an inline ASM block.\n\n    Returns:\n      True if the top of the stack is a block containing inline ASM.\n    \"\"\"\n    return self.stack and self.stack[-1].inline_asm != _NO_ASM\n\n  def InTemplateArgumentList(self, clean_lines, linenum, pos):\n    \"\"\"Check if current position is inside template argument list.\n\n    Args:\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      pos: position just after the suspected template argument.\n    Returns:\n      True if (linenum, pos) is inside template arguments.\n    \"\"\"\n    while linenum < clean_lines.NumLines():\n      # Find the earliest character that might indicate a template argument\n      line = clean_lines.elided[linenum]\n      match = Match(r'^[^{};=\\[\\]\\.<>]*(.)', line[pos:])\n      if not match:\n        linenum += 1\n        pos = 0\n        continue\n      token = match.group(1)\n      pos += len(match.group(0))\n\n      # These things do not look like template argument list:\n      #   class Suspect {\n      #   class Suspect x; }\n      if token in ('{', '}', ';'): return False\n\n      # These things look like template argument list:\n      #   template <class Suspect>\n      #   template <class Suspect = default_value>\n      #   template <class Suspect[]>\n      #   template <class Suspect...>\n      if token in ('>', '=', '[', ']', '.'): return True\n\n      # Check if token is an unmatched '<'.\n      # If not, move on to the next character.\n      if token != '<':\n        pos += 1\n        if pos >= len(line):\n          linenum += 1\n          pos = 0\n        continue\n\n      # We can't be sure if we just find a single '<', and need to\n      # find the matching '>'.\n      (_, end_line, end_pos) = CloseExpression(clean_lines, linenum, pos - 1)\n      if end_pos < 0:\n        # Not sure if template argument list or syntax error in file\n        return False\n      linenum = end_line\n      pos = end_pos\n    return False\n\n  def UpdatePreprocessor(self, line):\n    \"\"\"Update preprocessor stack.\n\n    We need to handle preprocessors due to classes like this:\n      #ifdef SWIG\n      struct ResultDetailsPageElementExtensionPoint {\n      #else\n      struct ResultDetailsPageElementExtensionPoint : public Extension {\n      #endif\n\n    We make the following assumptions (good enough for most files):\n    - Preprocessor condition evaluates to true from #if up to first\n      #else/#elif/#endif.\n\n    - Preprocessor condition evaluates to false from #else/#elif up\n      to #endif.  We still perform lint checks on these lines, but\n      these do not affect nesting stack.\n\n    Args:\n      line: current line to check.\n    \"\"\"\n    if Match(r'^\\s*#\\s*(if|ifdef|ifndef)\\b', line):\n      # Beginning of #if block, save the nesting stack here.  The saved\n      # stack will allow us to restore the parsing state in the #else case.\n      self.pp_stack.append(_PreprocessorInfo(copy.deepcopy(self.stack)))\n    elif Match(r'^\\s*#\\s*(else|elif)\\b', line):\n      # Beginning of #else block\n      if self.pp_stack:\n        if not self.pp_stack[-1].seen_else:\n          # This is the first #else or #elif block.  Remember the\n          # whole nesting stack up to this point.  This is what we\n          # keep after the #endif.\n          self.pp_stack[-1].seen_else = True\n          self.pp_stack[-1].stack_before_else = copy.deepcopy(self.stack)\n\n        # Restore the stack to how it was before the #if\n        self.stack = copy.deepcopy(self.pp_stack[-1].stack_before_if)\n      else:\n        # TODO(unknown): unexpected #else, issue warning?\n        pass\n    elif Match(r'^\\s*#\\s*endif\\b', line):\n      # End of #if or #else blocks.\n      if self.pp_stack:\n        # If we saw an #else, we will need to restore the nesting\n        # stack to its former state before the #else, otherwise we\n        # will just continue from where we left off.\n        if self.pp_stack[-1].seen_else:\n          # Here we can just use a shallow copy since we are the last\n          # reference to it.\n          self.stack = self.pp_stack[-1].stack_before_else\n        # Drop the corresponding #if\n        self.pp_stack.pop()\n      else:\n        # TODO(unknown): unexpected #endif, issue warning?\n        pass\n\n  # TODO(unknown): Update() is too long, but we will refactor later.\n  def Update(self, filename, clean_lines, linenum, error):\n    \"\"\"Update nesting state with current line.\n\n    Args:\n      filename: The name of the current file.\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      error: The function to call with any errors found.\n    \"\"\"\n    line = clean_lines.elided[linenum]\n\n    # Remember top of the previous nesting stack.\n    #\n    # The stack is always pushed/popped and not modified in place, so\n    # we can just do a shallow copy instead of copy.deepcopy.  Using\n    # deepcopy would slow down cpplint by ~28%.\n    if self.stack:\n      self.previous_stack_top = self.stack[-1]\n    else:\n      self.previous_stack_top = None\n\n    # Update pp_stack\n    self.UpdatePreprocessor(line)\n\n    # Count parentheses.  This is to avoid adding struct arguments to\n    # the nesting stack.\n    if self.stack:\n      inner_block = self.stack[-1]\n      depth_change = line.count('(') - line.count(')')\n      inner_block.open_parentheses += depth_change\n\n      # Also check if we are starting or ending an inline assembly block.\n      if inner_block.inline_asm in (_NO_ASM, _END_ASM):\n        if (depth_change != 0 and\n            inner_block.open_parentheses == 1 and\n            _MATCH_ASM.match(line)):\n          # Enter assembly block\n          inner_block.inline_asm = _INSIDE_ASM\n        else:\n          # Not entering assembly block.  If previous line was _END_ASM,\n          # we will now shift to _NO_ASM state.\n          inner_block.inline_asm = _NO_ASM\n      elif (inner_block.inline_asm == _INSIDE_ASM and\n            inner_block.open_parentheses == 0):\n        # Exit assembly block\n        inner_block.inline_asm = _END_ASM\n\n    # Consume namespace declaration at the beginning of the line.  Do\n    # this in a loop so that we catch same line declarations like this:\n    #   namespace proto2 { namespace bridge { class MessageSet; } }\n    while True:\n      # Match start of namespace.  The \"\\b\\s*\" below catches namespace\n      # declarations even if it weren't followed by a whitespace, this\n      # is so that we don't confuse our namespace checker.  The\n      # missing spaces will be flagged by CheckSpacing.\n      namespace_decl_match = Match(r'^\\s*namespace\\b\\s*([:\\w]+)?(.*)$', line)\n      if not namespace_decl_match:\n        break\n\n      new_namespace = _NamespaceInfo(namespace_decl_match.group(1), linenum)\n      self.stack.append(new_namespace)\n\n      line = namespace_decl_match.group(2)\n      if line.find('{') != -1:\n        new_namespace.seen_open_brace = True\n        line = line[line.find('{') + 1:]\n\n    # Look for a class declaration in whatever is left of the line\n    # after parsing namespaces.  The regexp accounts for decorated classes\n    # such as in:\n    #   class LOCKABLE API Object {\n    #   };\n    class_decl_match = Match(\n        r'^(\\s*(?:template\\s*<[\\w\\s<>,:=]*>\\s*)?'\n        r'(class|struct)\\s+(?:[a-zA-Z0-9_]+\\s+)*(\\w+(?:::\\w+)*))'\n        r'(.*)$', line)\n    if (class_decl_match and\n        (not self.stack or self.stack[-1].open_parentheses == 0)):\n      # We do not want to accept classes that are actually template arguments:\n      #   template <class Ignore1,\n      #             class Ignore2 = Default<Args>,\n      #             template <Args> class Ignore3>\n      #   void Function() {};\n      #\n      # To avoid template argument cases, we scan forward and look for\n      # an unmatched '>'.  If we see one, assume we are inside a\n      # template argument list.\n      end_declaration = len(class_decl_match.group(1))\n      if not self.InTemplateArgumentList(clean_lines, linenum, end_declaration):\n        self.stack.append(_ClassInfo(\n            class_decl_match.group(3), class_decl_match.group(2),\n            clean_lines, linenum))\n        line = class_decl_match.group(4)\n\n    # If we have not yet seen the opening brace for the innermost block,\n    # run checks here.\n    if not self.SeenOpenBrace():\n      self.stack[-1].CheckBegin(filename, clean_lines, linenum, error)\n\n    # Update access control if we are inside a class/struct\n    if self.stack and isinstance(self.stack[-1], _ClassInfo):\n      classinfo = self.stack[-1]\n      access_match = Match(\n          r'^(.*)\\b(public|private|protected|signals)(\\s+(?:slots\\s*)?)?'\n          r':(?:[^:]|$)',\n          line)\n      if access_match:\n        classinfo.access = access_match.group(2)\n\n        # Check that access keywords are indented +1 space.  Skip this\n        # check if the keywords are not preceded by whitespaces.\n        indent = access_match.group(1)\n        if (len(indent) != classinfo.class_indent + 1 and\n            Match(r'^\\s*$', indent)):\n          if classinfo.is_struct:\n            parent = 'struct ' + classinfo.name\n          else:\n            parent = 'class ' + classinfo.name\n          slots = ''\n          if access_match.group(3):\n            slots = access_match.group(3)\n          error(filename, linenum, 'whitespace/indent', 3,\n                '%s%s: should be indented +1 space inside %s' % (\n                    access_match.group(2), slots, parent))\n\n    # Consume braces or semicolons from what's left of the line\n    while True:\n      # Match first brace, semicolon, or closed parenthesis.\n      matched = Match(r'^[^{;)}]*([{;)}])(.*)$', line)\n      if not matched:\n        break\n\n      token = matched.group(1)\n      if token == '{':\n        # If namespace or class hasn't seen a opening brace yet, mark\n        # namespace/class head as complete.  Push a new block onto the\n        # stack otherwise.\n        if not self.SeenOpenBrace():\n          self.stack[-1].seen_open_brace = True\n        elif Match(r'^extern\\s*\"[^\"]*\"\\s*\\{', line):\n          self.stack.append(_ExternCInfo(linenum))\n        else:\n          self.stack.append(_BlockInfo(linenum, True))\n          if _MATCH_ASM.match(line):\n            self.stack[-1].inline_asm = _BLOCK_ASM\n\n      elif token == ';' or token == ')':\n        # If we haven't seen an opening brace yet, but we already saw\n        # a semicolon, this is probably a forward declaration.  Pop\n        # the stack for these.\n        #\n        # Similarly, if we haven't seen an opening brace yet, but we\n        # already saw a closing parenthesis, then these are probably\n        # function arguments with extra \"class\" or \"struct\" keywords.\n        # Also pop these stack for these.\n        if not self.SeenOpenBrace():\n          self.stack.pop()\n      else:  # token == '}'\n        # Perform end of block checks and pop the stack.\n        if self.stack:\n          self.stack[-1].CheckEnd(filename, clean_lines, linenum, error)\n          self.stack.pop()\n      line = matched.group(2)\n\n  def InnermostClass(self):\n    \"\"\"Get class info on the top of the stack.\n\n    Returns:\n      A _ClassInfo object if we are inside a class, or None otherwise.\n    \"\"\"\n    for i in range(len(self.stack), 0, -1):\n      classinfo = self.stack[i - 1]\n      if isinstance(classinfo, _ClassInfo):\n        return classinfo\n    return None\n\n  def CheckCompletedBlocks(self, filename, error):\n    \"\"\"Checks that all classes and namespaces have been completely parsed.\n\n    Call this when all lines in a file have been processed.\n    Args:\n      filename: The name of the current file.\n      error: The function to call with any errors found.\n    \"\"\"\n    # Note: This test can result in false positives if #ifdef constructs\n    # get in the way of brace matching. See the testBuildClass test in\n    # cpplint_unittest.py for an example of this.\n    for obj in self.stack:\n      if isinstance(obj, _ClassInfo):\n        error(filename, obj.starting_linenum, 'build/class', 5,\n              'Failed to find complete declaration of class %s' %\n              obj.name)\n      elif isinstance(obj, _NamespaceInfo):\n        error(filename, obj.starting_linenum, 'build/namespaces', 5,\n              'Failed to find complete declaration of namespace %s' %\n              obj.name)\n\n\ndef CheckForNonStandardConstructs(filename, clean_lines, linenum,\n                                  nesting_state, error):\n  r\"\"\"Logs an error if we see certain non-ANSI constructs ignored by gcc-2.\n\n  Complain about several constructs which gcc-2 accepts, but which are\n  not standard C++.  Warning about these in lint is one way to ease the\n  transition to new compilers.\n  - put storage class first (e.g. \"static const\" instead of \"const static\").\n  - \"%lld\" instead of %qd\" in printf-type functions.\n  - \"%1$d\" is non-standard in printf-type functions.\n  - \"\\%\" is an undefined character escape sequence.\n  - text after #endif is not allowed.\n  - invalid inner-style forward declaration.\n  - >? and <? operators, and their >?= and <?= cousins.\n\n  Additionally, check for constructor/destructor style violations and reference\n  members, as it is very convenient to do so while checking for\n  gcc-2 compliance.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n  \"\"\"\n\n  # Remove comments from the line, but leave in strings for now.\n  line = clean_lines.lines[linenum]\n\n  if Search(r'printf\\s*\\(.*\".*%[-+ ]?\\d*q', line):\n    error(filename, linenum, 'runtime/printf_format', 3,\n          '%q in format strings is deprecated.  Use %ll instead.')\n\n  if Search(r'printf\\s*\\(.*\".*%\\d+\\$', line):\n    error(filename, linenum, 'runtime/printf_format', 2,\n          '%N$ formats are unconventional.  Try rewriting to avoid them.')\n\n  # Remove escaped backslashes before looking for undefined escapes.\n  line = line.replace('\\\\\\\\', '')\n\n  if Search(r'(\"|\\').*\\\\(%|\\[|\\(|{)', line):\n    error(filename, linenum, 'build/printf_format', 3,\n          '%, [, (, and { are undefined character escapes.  Unescape them.')\n\n  # For the rest, work with both comments and strings removed.\n  line = clean_lines.elided[linenum]\n\n  if Search(r'\\b(const|volatile|void|char|short|int|long'\n            r'|float|double|signed|unsigned'\n            r'|schar|u?int8|u?int16|u?int32|u?int64)'\n            r'\\s+(register|static|extern|typedef)\\b',\n            line):\n    error(filename, linenum, 'build/storage_class', 5,\n          'Storage-class specifier (static, extern, typedef, etc) should be '\n          'at the beginning of the declaration.')\n\n  if Match(r'\\s*#\\s*endif\\s*[^/\\s]+', line):\n    error(filename, linenum, 'build/endif_comment', 5,\n          'Uncommented text after #endif is non-standard.  Use a comment.')\n\n  if Match(r'\\s*class\\s+(\\w+\\s*::\\s*)+\\w+\\s*;', line):\n    error(filename, linenum, 'build/forward_decl', 5,\n          'Inner-style forward declarations are invalid.  Remove this line.')\n\n  if Search(r'(\\w+|[+-]?\\d+(\\.\\d*)?)\\s*(<|>)\\?=?\\s*(\\w+|[+-]?\\d+)(\\.\\d*)?',\n            line):\n    error(filename, linenum, 'build/deprecated', 3,\n          '>? and <? (max and min) operators are non-standard and deprecated.')\n\n  if Search(r'^\\s*const\\s*string\\s*&\\s*\\w+\\s*;', line):\n    # TODO(unknown): Could it be expanded safely to arbitrary references,\n    # without triggering too many false positives? The first\n    # attempt triggered 5 warnings for mostly benign code in the regtest, hence\n    # the restriction.\n    # Here's the original regexp, for the reference:\n    # type_name = r'\\w+((\\s*::\\s*\\w+)|(\\s*<\\s*\\w+?\\s*>))?'\n    # r'\\s*const\\s*' + type_name + '\\s*&\\s*\\w+\\s*;'\n    error(filename, linenum, 'runtime/member_string_references', 2,\n          'const string& members are dangerous. It is much better to use '\n          'alternatives, such as pointers or simple constants.')\n\n  # Everything else in this function operates on class declarations.\n  # Return early if the top of the nesting stack is not a class, or if\n  # the class head is not completed yet.\n  classinfo = nesting_state.InnermostClass()\n  if not classinfo or not classinfo.seen_open_brace:\n    return\n\n  # The class may have been declared with namespace or classname qualifiers.\n  # The constructor and destructor will not have those qualifiers.\n  base_classname = classinfo.name.split('::')[-1]\n\n  # Look for single-argument constructors that aren't marked explicit.\n  # Technically a valid construct, but against style.\n  explicit_constructor_match = Match(\n      r'\\s+(?:(?:inline|constexpr)\\s+)*(explicit\\s+)?'\n      r'(?:(?:inline|constexpr)\\s+)*%s\\s*'\n      r'\\(((?:[^()]|\\([^()]*\\))*)\\)'\n      % re.escape(base_classname),\n      line)\n\n  if explicit_constructor_match:\n    is_marked_explicit = explicit_constructor_match.group(1)\n\n    if not explicit_constructor_match.group(2):\n      constructor_args = []\n    else:\n      constructor_args = explicit_constructor_match.group(2).split(',')\n\n    # collapse arguments so that commas in template parameter lists and function\n    # argument parameter lists don't split arguments in two\n    i = 0\n    while i < len(constructor_args):\n      constructor_arg = constructor_args[i]\n      while (constructor_arg.count('<') > constructor_arg.count('>') or\n             constructor_arg.count('(') > constructor_arg.count(')')):\n        constructor_arg += ',' + constructor_args[i + 1]\n        del constructor_args[i + 1]\n      constructor_args[i] = constructor_arg\n      i += 1\n\n    variadic_args = [arg for arg in constructor_args if '&&...' in arg]\n    defaulted_args = [arg for arg in constructor_args if '=' in arg]\n    noarg_constructor = (not constructor_args or  # empty arg list\n                         # 'void' arg specifier\n                         (len(constructor_args) == 1 and\n                          constructor_args[0].strip() == 'void'))\n    onearg_constructor = ((len(constructor_args) == 1 and  # exactly one arg\n                           not noarg_constructor) or\n                          # all but at most one arg defaulted\n                          (len(constructor_args) >= 1 and\n                           not noarg_constructor and\n                           len(defaulted_args) >= len(constructor_args) - 1) or\n                          # variadic arguments with zero or one argument\n                          (len(constructor_args) <= 2 and\n                           len(variadic_args) >= 1))\n    initializer_list_constructor = bool(\n        onearg_constructor and\n        Search(r'\\bstd\\s*::\\s*initializer_list\\b', constructor_args[0]))\n    copy_constructor = bool(\n        onearg_constructor and\n        Match(r'((const\\s+(volatile\\s+)?)?|(volatile\\s+(const\\s+)?))?'\n              r'%s(\\s*<[^>]*>)?(\\s+const)?\\s*(?:<\\w+>\\s*)?&'\n              % re.escape(base_classname), constructor_args[0].strip()))\n\n    if (not is_marked_explicit and\n        onearg_constructor and\n        not initializer_list_constructor and\n        not copy_constructor):\n      if defaulted_args or variadic_args:\n        error(filename, linenum, 'runtime/explicit', 5,\n              'Constructors callable with one argument '\n              'should be marked explicit.')\n      else:\n        error(filename, linenum, 'runtime/explicit', 5,\n              'Single-parameter constructors should be marked explicit.')\n    elif is_marked_explicit and not onearg_constructor:\n      if noarg_constructor:\n        error(filename, linenum, 'runtime/explicit', 5,\n              'Zero-parameter constructors should not be marked explicit.')\n\n\ndef CheckSpacingForFunctionCall(filename, clean_lines, linenum, error):\n  \"\"\"Checks for the correctness of various spacing around function calls.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Since function calls often occur inside if/for/while/switch\n  # expressions - which have their own, more liberal conventions - we\n  # first see if we should be looking inside such an expression for a\n  # function call, to which we can apply more strict standards.\n  fncall = line    # if there's no control flow construct, look at whole line\n  for pattern in (r'\\bif\\s*\\((.*)\\)\\s*{',\n                  r'\\bfor\\s*\\((.*)\\)\\s*{',\n                  r'\\bwhile\\s*\\((.*)\\)\\s*[{;]',\n                  r'\\bswitch\\s*\\((.*)\\)\\s*{'):\n    match = Search(pattern, line)\n    if match:\n      fncall = match.group(1)    # look inside the parens for function calls\n      break\n\n  # Except in if/for/while/switch, there should never be space\n  # immediately inside parens (eg \"f( 3, 4 )\").  We make an exception\n  # for nested parens ( (a+b) + c ).  Likewise, there should never be\n  # a space before a ( when it's a function argument.  I assume it's a\n  # function argument when the char before the whitespace is legal in\n  # a function name (alnum + _) and we're not starting a macro. Also ignore\n  # pointers and references to arrays and functions coz they're too tricky:\n  # we use a very simple way to recognize these:\n  # \" (something)(maybe-something)\" or\n  # \" (something)(maybe-something,\" or\n  # \" (something)[something]\"\n  # Note that we assume the contents of [] to be short enough that\n  # they'll never need to wrap.\n  if (  # Ignore control structures.\n      not Search(r'\\b(if|elif|for|while|switch|return|new|delete|catch|sizeof)\\b',\n                 fncall) and\n      # Ignore pointers/references to functions.\n      not Search(r' \\([^)]+\\)\\([^)]*(\\)|,$)', fncall) and\n      # Ignore pointers/references to arrays.\n      not Search(r' \\([^)]+\\)\\[[^\\]]+\\]', fncall)):\n    if Search(r'\\w\\s*\\(\\s(?!\\s*\\\\$)', fncall):      # a ( used for a fn call\n      error(filename, linenum, 'whitespace/parens', 4,\n            'Extra space after ( in function call')\n    elif Search(r'\\(\\s+(?!(\\s*\\\\)|\\()', fncall):\n      error(filename, linenum, 'whitespace/parens', 2,\n            'Extra space after (')\n    if (Search(r'\\w\\s+\\(', fncall) and\n        not Search(r'_{0,2}asm_{0,2}\\s+_{0,2}volatile_{0,2}\\s+\\(', fncall) and\n        not Search(r'#\\s*define|typedef|using\\s+\\w+\\s*=', fncall) and\n        not Search(r'\\w\\s+\\((\\w+::)*\\*\\w+\\)\\(', fncall) and\n        not Search(r'\\bcase\\s+\\(', fncall)):\n      # TODO(unknown): Space after an operator function seem to be a common\n      # error, silence those for now by restricting them to highest verbosity.\n      if Search(r'\\boperator_*\\b', line):\n        error(filename, linenum, 'whitespace/parens', 0,\n              'Extra space before ( in function call')\n      else:\n        error(filename, linenum, 'whitespace/parens', 4,\n              'Extra space before ( in function call')\n    # If the ) is followed only by a newline or a { + newline, assume it's\n    # part of a control statement (if/while/etc), and don't complain\n    if Search(r'[^)]\\s+\\)\\s*[^{\\s]', fncall):\n      # If the closing parenthesis is preceded by only whitespaces,\n      # try to give a more descriptive error message.\n      if Search(r'^\\s+\\)', fncall):\n        error(filename, linenum, 'whitespace/parens', 2,\n              'Closing ) should be moved to the previous line')\n      else:\n        error(filename, linenum, 'whitespace/parens', 2,\n              'Extra space before )')\n\n\ndef IsBlankLine(line):\n  \"\"\"Returns true if the given line is blank.\n\n  We consider a line to be blank if the line is empty or consists of\n  only white spaces.\n\n  Args:\n    line: A line of a string.\n\n  Returns:\n    True, if the given line is blank.\n  \"\"\"\n  return not line or line.isspace()\n\n\ndef CheckForNamespaceIndentation(filename, nesting_state, clean_lines, line,\n                                 error):\n  is_namespace_indent_item = (\n      len(nesting_state.stack) > 1 and\n      nesting_state.stack[-1].check_namespace_indentation and\n      isinstance(nesting_state.previous_stack_top, _NamespaceInfo) and\n      nesting_state.previous_stack_top == nesting_state.stack[-2])\n\n  if ShouldCheckNamespaceIndentation(nesting_state, is_namespace_indent_item,\n                                     clean_lines.elided, line):\n    CheckItemIndentationInNamespace(filename, clean_lines.elided,\n                                    line, error)\n\n\ndef CheckForFunctionLengths(filename, clean_lines, linenum,\n                            function_state, error):\n  \"\"\"Reports for long function bodies.\n\n  For an overview why this is done, see:\n  https://google-styleguide.googlecode.com/svn/trunk/cppguide.xml#Write_Short_Functions\n\n  Uses a simplistic algorithm assuming other style guidelines\n  (especially spacing) are followed.\n  Only checks unindented functions, so class members are unchecked.\n  Trivial bodies are unchecked, so constructors with huge initializer lists\n  may be missed.\n  Blank/comment lines are not counted so as to avoid encouraging the removal\n  of vertical space and comments just to get through a lint check.\n  NOLINT *on the last line of a function* disables this check.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    function_state: Current function name and lines in body so far.\n    error: The function to call with any errors found.\n  \"\"\"\n  lines = clean_lines.lines\n  line = lines[linenum]\n  joined_line = ''\n\n  starting_func = False\n  regexp = r'(\\w(\\w|::|\\*|\\&|\\s)*)\\('  # decls * & space::name( ...\n  match_result = Match(regexp, line)\n  if match_result:\n    # If the name is all caps and underscores, figure it's a macro and\n    # ignore it, unless it's TEST or TEST_F.\n    function_name = match_result.group(1).split()[-1]\n    if function_name == 'TEST' or function_name == 'TEST_F' or (\n        not Match(r'[A-Z_]+$', function_name)):\n      starting_func = True\n\n  if starting_func:\n    body_found = False\n    for start_linenum in xrange(linenum, clean_lines.NumLines()):\n      start_line = lines[start_linenum]\n      joined_line += ' ' + start_line.lstrip()\n      if Search(r'(;|})', start_line):  # Declarations and trivial functions\n        body_found = True\n        break                              # ... ignore\n      if Search(r'{', start_line):\n        body_found = True\n        function = Search(r'((\\w|:)*)\\(', line).group(1)\n        if Match(r'TEST', function):    # Handle TEST... macros\n          parameter_regexp = Search(r'(\\(.*\\))', joined_line)\n          if parameter_regexp:             # Ignore bad syntax\n            function += parameter_regexp.group(1)\n        else:\n          function += '()'\n        function_state.Begin(function)\n        break\n    if not body_found:\n      # No body for the function (or evidence of a non-function) was found.\n      error(filename, linenum, 'readability/fn_size', 5,\n            'Lint failed to find start of function body.')\n  elif Match(r'^\\}\\s*$', line):  # function end\n    function_state.Check(error, filename, linenum)\n    function_state.End()\n  elif not Match(r'^\\s*$', line):\n    function_state.Count()  # Count non-blank/non-comment lines.\n\n\n_RE_PATTERN_TODO = re.compile(r'^//(\\s*)TODO(\\(.+?\\))?:?(\\s|$)?')\n\n\ndef CheckComment(line, filename, linenum, next_line_start, error):\n  \"\"\"Checks for common mistakes in comments.\n\n  Args:\n    line: The line in question.\n    filename: The name of the current file.\n    linenum: The number of the line to check.\n    next_line_start: The first non-whitespace column of the next line.\n    error: The function to call with any errors found.\n  \"\"\"\n  commentpos = line.find('//')\n  if commentpos != -1:\n    # Check if the // may be in quotes.  If so, ignore it\n    if re.sub(r'\\\\.', '', line[0:commentpos]).count('\"') % 2 == 0:\n      # Allow one space for new scopes, two spaces otherwise:\n      if (not (Match(r'^.*{ *//', line) and next_line_start == commentpos) and\n          ((commentpos >= 1 and\n            line[commentpos-1] not in string.whitespace) or\n           (commentpos >= 2 and\n            line[commentpos-2] not in string.whitespace))):\n        error(filename, linenum, 'whitespace/comments', 2,\n              'At least two spaces is best between code and comments')\n\n      # Checks for common mistakes in TODO comments.\n      comment = line[commentpos:]\n      match = _RE_PATTERN_TODO.match(comment)\n      if match:\n        # One whitespace is correct; zero whitespace is handled elsewhere.\n        leading_whitespace = match.group(1)\n        if len(leading_whitespace) > 1:\n          error(filename, linenum, 'whitespace/todo', 2,\n                'Too many spaces before TODO')\n\n        username = match.group(2)\n        if not username:\n          error(filename, linenum, 'readability/todo', 2,\n                'Missing username in TODO; it should look like '\n                '\"// TODO(my_username): Stuff.\"')\n\n        middle_whitespace = match.group(3)\n        # Comparisons made explicit for correctness -- pylint: disable=g-explicit-bool-comparison\n        if middle_whitespace != ' ' and middle_whitespace != '':\n          error(filename, linenum, 'whitespace/todo', 2,\n                'TODO(my_username) should be followed by a space')\n\n      # If the comment contains an alphanumeric character, there\n      # should be a space somewhere between it and the // unless\n      # it's a /// or //! Doxygen comment.\n      if (Match(r'//[^ ]*\\w', comment) and\n          not Match(r'(///|//\\!)(\\s+|$)', comment)):\n        error(filename, linenum, 'whitespace/comments', 4,\n              'Should have a space between // and comment')\n\n\ndef CheckSpacing(filename, clean_lines, linenum, nesting_state, error):\n  \"\"\"Checks for the correctness of various spacing issues in the code.\n\n  Things we check for: spaces around operators, spaces after\n  if/for/while/switch, no spaces around parens in function calls, two\n  spaces between code and comment, don't start a block with a blank\n  line, don't end a function with a blank line, don't add a blank line\n  after public/protected/private, don't have too many blank lines in a row.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Don't use \"elided\" lines here, otherwise we can't check commented lines.\n  # Don't want to use \"raw\" either, because we don't want to check inside C++11\n  # raw strings,\n  raw = clean_lines.lines_without_raw_strings\n  line = raw[linenum]\n\n  # Before nixing comments, check if the line is blank for no good\n  # reason.  This includes the first line after a block is opened, and\n  # blank lines at the end of a function (ie, right before a line like '}'\n  #\n  # Skip all the blank line checks if we are immediately inside a\n  # namespace body.  In other words, don't issue blank line warnings\n  # for this block:\n  #   namespace {\n  #\n  #   }\n  #\n  # A warning about missing end of namespace comments will be issued instead.\n  #\n  # Also skip blank line checks for 'extern \"C\"' blocks, which are formatted\n  # like namespaces.\n  if (IsBlankLine(line) and\n      not nesting_state.InNamespaceBody() and\n      not nesting_state.InExternC()):\n    elided = clean_lines.elided\n    prev_line = elided[linenum - 1]\n    prevbrace = prev_line.rfind('{')\n    # TODO(unknown): Don't complain if line before blank line, and line after,\n    #                both start with alnums and are indented the same amount.\n    #                This ignores whitespace at the start of a namespace block\n    #                because those are not usually indented.\n    if prevbrace != -1 and prev_line[prevbrace:].find('}') == -1:\n      # OK, we have a blank line at the start of a code block.  Before we\n      # complain, we check if it is an exception to the rule: The previous\n      # non-empty line has the parameters of a function header that are indented\n      # 4 spaces (because they did not fit in a 80 column line when placed on\n      # the same line as the function name).  We also check for the case where\n      # the previous line is indented 6 spaces, which may happen when the\n      # initializers of a constructor do not fit into a 80 column line.\n      exception = False\n      if Match(r' {6}\\w', prev_line):  # Initializer list?\n        # We are looking for the opening column of initializer list, which\n        # should be indented 4 spaces to cause 6 space indentation afterwards.\n        search_position = linenum-2\n        while (search_position >= 0\n               and Match(r' {6}\\w', elided[search_position])):\n          search_position -= 1\n        exception = (search_position >= 0\n                     and elided[search_position][:5] == '    :')\n      else:\n        # Search for the function arguments or an initializer list.  We use a\n        # simple heuristic here: If the line is indented 4 spaces; and we have a\n        # closing paren, without the opening paren, followed by an opening brace\n        # or colon (for initializer lists) we assume that it is the last line of\n        # a function header.  If we have a colon indented 4 spaces, it is an\n        # initializer list.\n        exception = (Match(r' {4}\\w[^\\(]*\\)\\s*(const\\s*)?(\\{\\s*$|:)',\n                           prev_line)\n                     or Match(r' {4}:', prev_line))\n\n      if not exception:\n        error(filename, linenum, 'whitespace/blank_line', 2,\n              'Redundant blank line at the start of a code block '\n              'should be deleted.')\n    # Ignore blank lines at the end of a block in a long if-else\n    # chain, like this:\n    #   if (condition1) {\n    #     // Something followed by a blank line\n    #\n    #   } else if (condition2) {\n    #     // Something else\n    #   }\n    if linenum + 1 < clean_lines.NumLines():\n      next_line = raw[linenum + 1]\n      if (next_line\n          and Match(r'\\s*}', next_line)\n          and next_line.find('} else ') == -1):\n        error(filename, linenum, 'whitespace/blank_line', 3,\n              'Redundant blank line at the end of a code block '\n              'should be deleted.')\n\n    matched = Match(r'\\s*(public|protected|private):', prev_line)\n    if matched:\n      error(filename, linenum, 'whitespace/blank_line', 3,\n            'Do not leave a blank line after \"%s:\"' % matched.group(1))\n\n  # Next, check comments\n  next_line_start = 0\n  if linenum + 1 < clean_lines.NumLines():\n    next_line = raw[linenum + 1]\n    next_line_start = len(next_line) - len(next_line.lstrip())\n  CheckComment(line, filename, linenum, next_line_start, error)\n\n  # get rid of comments and strings\n  line = clean_lines.elided[linenum]\n\n  # You shouldn't have spaces before your brackets, except for C++11 attributes\n  # or maybe after 'delete []', 'return []() {};', or 'auto [abc, ...] = ...;'.\n  if (Search(r'\\w\\s+\\[(?!\\[)', line) and\n      not Search(r'(?:auto&?|delete|return)\\s+\\[', line)):\n    error(filename, linenum, 'whitespace/braces', 5,\n          'Extra space before [')\n\n  # In range-based for, we wanted spaces before and after the colon, but\n  # not around \"::\" tokens that might appear.\n  if (Search(r'for *\\(.*[^:]:[^: ]', line) or\n      Search(r'for *\\(.*[^: ]:[^:]', line)):\n    error(filename, linenum, 'whitespace/forcolon', 2,\n          'Missing space around colon in range-based for loop')\n\n\ndef CheckOperatorSpacing(filename, clean_lines, linenum, error):\n  \"\"\"Checks for horizontal spacing around operators.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Don't try to do spacing checks for operator methods.  Do this by\n  # replacing the troublesome characters with something else,\n  # preserving column position for all other characters.\n  #\n  # The replacement is done repeatedly to avoid false positives from\n  # operators that call operators.\n  while True:\n    match = Match(r'^(.*\\boperator\\b)(\\S+)(\\s*\\(.*)$', line)\n    if match:\n      line = match.group(1) + ('_' * len(match.group(2))) + match.group(3)\n    else:\n      break\n\n  # We allow no-spaces around = within an if: \"if ( (a=Foo()) == 0 )\".\n  # Otherwise not.  Note we only check for non-spaces on *both* sides;\n  # sometimes people put non-spaces on one side when aligning ='s among\n  # many lines (not that this is behavior that I approve of...)\n  if ((Search(r'[\\w.]=', line) or\n       Search(r'=[\\w.]', line))\n      and not Search(r'\\b(if|while|for) ', line)\n      # Operators taken from [lex.operators] in C++11 standard.\n      and not Search(r'(>=|<=|==|!=|&=|\\^=|\\|=|\\+=|\\*=|\\/=|\\%=)', line)\n      and not Search(r'operator=', line)):\n    error(filename, linenum, 'whitespace/operators', 4,\n          'Missing spaces around =')\n\n  # It's ok not to have spaces around binary operators like + - * /, but if\n  # there's too little whitespace, we get concerned.  It's hard to tell,\n  # though, so we punt on this one for now.  TODO.\n\n  # You should always have whitespace around binary operators.\n  #\n  # Check <= and >= first to avoid false positives with < and >, then\n  # check non-include lines for spacing around < and >.\n  #\n  # If the operator is followed by a comma, assume it's be used in a\n  # macro context and don't do any checks.  This avoids false\n  # positives.\n  #\n  # Note that && is not included here.  This is because there are too\n  # many false positives due to RValue references.\n  match = Search(r'[^<>=!\\s](==|!=|<=|>=|\\|\\|)[^<>=!\\s,;\\)]', line)\n  if match:\n    error(filename, linenum, 'whitespace/operators', 3,\n          'Missing spaces around %s' % match.group(1))\n  elif not Match(r'#.*include', line):\n    # Look for < that is not surrounded by spaces.  This is only\n    # triggered if both sides are missing spaces, even though\n    # technically it should flag if at least one side is missing a\n    # space.  This is done to avoid some false positives with shifts.\n    match = Match(r'^(.*[^\\s<])<[^\\s=<,]', line)\n    if match:\n      (_, _, end_pos) = CloseExpression(\n          clean_lines, linenum, len(match.group(1)))\n      if end_pos <= -1:\n        error(filename, linenum, 'whitespace/operators', 3,\n              'Missing spaces around <')\n\n    # Look for > that is not surrounded by spaces.  Similar to the\n    # above, we only trigger if both sides are missing spaces to avoid\n    # false positives with shifts.\n    match = Match(r'^(.*[^-\\s>])>[^\\s=>,]', line)\n    if match:\n      (_, _, start_pos) = ReverseCloseExpression(\n          clean_lines, linenum, len(match.group(1)))\n      if start_pos <= -1:\n        error(filename, linenum, 'whitespace/operators', 3,\n              'Missing spaces around >')\n\n  # We allow no-spaces around << when used like this: 10<<20, but\n  # not otherwise (particularly, not when used as streams)\n  #\n  # We also allow operators following an opening parenthesis, since\n  # those tend to be macros that deal with operators.\n  match = Search(r'(operator|[^\\s(<])(?:L|UL|LL|ULL|l|ul|ll|ull)?<<([^\\s,=<])', line)\n  if (match and not (match.group(1).isdigit() and match.group(2).isdigit()) and\n      not (match.group(1) == 'operator' and match.group(2) == ';')):\n    error(filename, linenum, 'whitespace/operators', 3,\n          'Missing spaces around <<')\n\n  # We allow no-spaces around >> for almost anything.  This is because\n  # C++11 allows \">>\" to close nested templates, which accounts for\n  # most cases when \">>\" is not followed by a space.\n  #\n  # We still warn on \">>\" followed by alpha character, because that is\n  # likely due to \">>\" being used for right shifts, e.g.:\n  #   value >> alpha\n  #\n  # When \">>\" is used to close templates, the alphanumeric letter that\n  # follows would be part of an identifier, and there should still be\n  # a space separating the template type and the identifier.\n  #   type<type<type>> alpha\n  match = Search(r'>>[a-zA-Z_]', line)\n  if match:\n    error(filename, linenum, 'whitespace/operators', 3,\n          'Missing spaces around >>')\n\n  # There shouldn't be space around unary operators\n  match = Search(r'(!\\s|~\\s|[\\s]--[\\s;]|[\\s]\\+\\+[\\s;])', line)\n  if match:\n    error(filename, linenum, 'whitespace/operators', 4,\n          'Extra space for operator %s' % match.group(1))\n\n\ndef CheckParenthesisSpacing(filename, clean_lines, linenum, error):\n  \"\"\"Checks for horizontal spacing around parentheses.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # No spaces after an if, while, switch, or for\n  match = Search(r' (if\\(|for\\(|while\\(|switch\\()', line)\n  if match:\n    error(filename, linenum, 'whitespace/parens', 5,\n          'Missing space before ( in %s' % match.group(1))\n\n  # For if/for/while/switch, the left and right parens should be\n  # consistent about how many spaces are inside the parens, and\n  # there should either be zero or one spaces inside the parens.\n  # We don't want: \"if ( foo)\" or \"if ( foo   )\".\n  # Exception: \"for ( ; foo; bar)\" and \"for (foo; bar; )\" are allowed.\n  match = Search(r'\\b(if|for|while|switch)\\s*'\n                 r'\\(([ ]*)(.).*[^ ]+([ ]*)\\)\\s*{\\s*$',\n                 line)\n  if match:\n    if len(match.group(2)) != len(match.group(4)):\n      if not (match.group(3) == ';' and\n              len(match.group(2)) == 1 + len(match.group(4)) or\n              not match.group(2) and Search(r'\\bfor\\s*\\(.*; \\)', line)):\n        error(filename, linenum, 'whitespace/parens', 5,\n              'Mismatching spaces inside () in %s' % match.group(1))\n    if len(match.group(2)) not in [0, 1]:\n      error(filename, linenum, 'whitespace/parens', 5,\n            'Should have zero or one spaces inside ( and ) in %s' %\n            match.group(1))\n\n\ndef CheckCommaSpacing(filename, clean_lines, linenum, error):\n  \"\"\"Checks for horizontal spacing near commas and semicolons.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  raw = clean_lines.lines_without_raw_strings\n  line = clean_lines.elided[linenum]\n\n  # You should always have a space after a comma (either as fn arg or operator)\n  #\n  # This does not apply when the non-space character following the\n  # comma is another comma, since the only time when that happens is\n  # for empty macro arguments.\n  #\n  # We run this check in two passes: first pass on elided lines to\n  # verify that lines contain missing whitespaces, second pass on raw\n  # lines to confirm that those missing whitespaces are not due to\n  # elided comments.\n  if (Search(r',[^,\\s]', ReplaceAll(r'\\boperator\\s*,\\s*\\(', 'F(', line)) and\n      Search(r',[^,\\s]', raw[linenum])):\n    error(filename, linenum, 'whitespace/comma', 3,\n          'Missing space after ,')\n\n  # You should always have a space after a semicolon\n  # except for few corner cases\n  # TODO(unknown): clarify if 'if (1) { return 1;}' is requires one more\n  # space after ;\n  if Search(r';[^\\s};\\\\)/]', line):\n    error(filename, linenum, 'whitespace/semicolon', 3,\n          'Missing space after ;')\n\n\ndef _IsType(clean_lines, nesting_state, expr):\n  \"\"\"Check if expression looks like a type name, returns true if so.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    expr: The expression to check.\n  Returns:\n    True, if token looks like a type.\n  \"\"\"\n  # Keep only the last token in the expression\n  last_word = Match(r'^.*(\\b\\S+)$', expr)\n  if last_word:\n    token = last_word.group(1)\n  else:\n    token = expr\n\n  # Match native types and stdint types\n  if _TYPES.match(token):\n    return True\n\n  # Try a bit harder to match templated types.  Walk up the nesting\n  # stack until we find something that resembles a typename\n  # declaration for what we are looking for.\n  typename_pattern = (r'\\b(?:typename|class|struct)\\s+' + re.escape(token) +\n                      r'\\b')\n  block_index = len(nesting_state.stack) - 1\n  while block_index >= 0:\n    if isinstance(nesting_state.stack[block_index], _NamespaceInfo):\n      return False\n\n    # Found where the opening brace is.  We want to scan from this\n    # line up to the beginning of the function, minus a few lines.\n    #   template <typename Type1,  // stop scanning here\n    #             ...>\n    #   class C\n    #     : public ... {  // start scanning here\n    last_line = nesting_state.stack[block_index].starting_linenum\n\n    next_block_start = 0\n    if block_index > 0:\n      next_block_start = nesting_state.stack[block_index - 1].starting_linenum\n    first_line = last_line\n    while first_line >= next_block_start:\n      if clean_lines.elided[first_line].find('template') >= 0:\n        break\n      first_line -= 1\n    if first_line < next_block_start:\n      # Didn't find any \"template\" keyword before reaching the next block,\n      # there are probably no template things to check for this block\n      block_index -= 1\n      continue\n\n    # Look for typename in the specified range\n    for i in xrange(first_line, last_line + 1, 1):\n      if Search(typename_pattern, clean_lines.elided[i]):\n        return True\n    block_index -= 1\n\n  return False\n\n\ndef CheckBracesSpacing(filename, clean_lines, linenum, nesting_state, error):\n  \"\"\"Checks for horizontal spacing near commas.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Except after an opening paren, or after another opening brace (in case of\n  # an initializer list, for instance), you should have spaces before your\n  # braces when they are delimiting blocks, classes, namespaces etc.\n  # And since you should never have braces at the beginning of a line,\n  # this is an easy test.  Except that braces used for initialization don't\n  # follow the same rule; we often don't want spaces before those.\n  match = Match(r'^(.*[^ ({>]){', line)\n\n  if match:\n    # Try a bit harder to check for brace initialization.  This\n    # happens in one of the following forms:\n    #   Constructor() : initializer_list_{} { ... }\n    #   Constructor{}.MemberFunction()\n    #   Type variable{};\n    #   FunctionCall(type{}, ...);\n    #   LastArgument(..., type{});\n    #   LOG(INFO) << type{} << \" ...\";\n    #   map_of_type[{...}] = ...;\n    #   ternary = expr ? new type{} : nullptr;\n    #   OuterTemplate<InnerTemplateConstructor<Type>{}>\n    #\n    # We check for the character following the closing brace, and\n    # silence the warning if it's one of those listed above, i.e.\n    # \"{.;,)<>]:\".\n    #\n    # To account for nested initializer list, we allow any number of\n    # closing braces up to \"{;,)<\".  We can't simply silence the\n    # warning on first sight of closing brace, because that would\n    # cause false negatives for things that are not initializer lists.\n    #   Silence this:         But not this:\n    #     Outer{                if (...) {\n    #       Inner{...}            if (...){  // Missing space before {\n    #     };                    }\n    #\n    # There is a false negative with this approach if people inserted\n    # spurious semicolons, e.g. \"if (cond){};\", but we will catch the\n    # spurious semicolon with a separate check.\n    leading_text = match.group(1)\n    (endline, endlinenum, endpos) = CloseExpression(\n        clean_lines, linenum, len(match.group(1)))\n    trailing_text = ''\n    if endpos > -1:\n      trailing_text = endline[endpos:]\n    for offset in xrange(endlinenum + 1,\n                         min(endlinenum + 3, clean_lines.NumLines() - 1)):\n      trailing_text += clean_lines.elided[offset]\n    # We also suppress warnings for `uint64_t{expression}` etc., as the style\n    # guide recommends brace initialization for integral types to avoid\n    # overflow/truncation.\n    if (not Match(r'^[\\s}]*[{.;,)<>\\]:]', trailing_text)\n        and not _IsType(clean_lines, nesting_state, leading_text)):\n      error(filename, linenum, 'whitespace/braces', 5,\n            'Missing space before {')\n\n  # Make sure '} else {' has spaces.\n  if Search(r'}else', line):\n    error(filename, linenum, 'whitespace/braces', 5,\n          'Missing space before else')\n\n  # You shouldn't have a space before a semicolon at the end of the line.\n  # There's a special case for \"for\" since the style guide allows space before\n  # the semicolon there.\n  if Search(r':\\s*;\\s*$', line):\n    error(filename, linenum, 'whitespace/semicolon', 5,\n          'Semicolon defining empty statement. Use {} instead.')\n  elif Search(r'^\\s*;\\s*$', line):\n    error(filename, linenum, 'whitespace/semicolon', 5,\n          'Line contains only semicolon. If this should be an empty statement, '\n          'use {} instead.')\n  elif (Search(r'\\s+;\\s*$', line) and\n        not Search(r'\\bfor\\b', line)):\n    error(filename, linenum, 'whitespace/semicolon', 5,\n          'Extra space before last semicolon. If this should be an empty '\n          'statement, use {} instead.')\n\n\ndef IsDecltype(clean_lines, linenum, column):\n  \"\"\"Check if the token ending on (linenum, column) is decltype().\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: the number of the line to check.\n    column: end column of the token to check.\n  Returns:\n    True if this token is decltype() expression, False otherwise.\n  \"\"\"\n  (text, _, start_col) = ReverseCloseExpression(clean_lines, linenum, column)\n  if start_col < 0:\n    return False\n  if Search(r'\\bdecltype\\s*$', text[0:start_col]):\n    return True\n  return False\n\ndef CheckSectionSpacing(filename, clean_lines, class_info, linenum, error):\n  \"\"\"Checks for additional blank line issues related to sections.\n\n  Currently the only thing checked here is blank line before protected/private.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    class_info: A _ClassInfo objects.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  # Skip checks if the class is small, where small means 25 lines or less.\n  # 25 lines seems like a good cutoff since that's the usual height of\n  # terminals, and any class that can't fit in one screen can't really\n  # be considered \"small\".\n  #\n  # Also skip checks if we are on the first line.  This accounts for\n  # classes that look like\n  #   class Foo { public: ... };\n  #\n  # If we didn't find the end of the class, last_line would be zero,\n  # and the check will be skipped by the first condition.\n  if (class_info.last_line - class_info.starting_linenum <= 24 or\n      linenum <= class_info.starting_linenum):\n    return\n\n  matched = Match(r'\\s*(public|protected|private):', clean_lines.lines[linenum])\n  if matched:\n    # Issue warning if the line before public/protected/private was\n    # not a blank line, but don't do this if the previous line contains\n    # \"class\" or \"struct\".  This can happen two ways:\n    #  - We are at the beginning of the class.\n    #  - We are forward-declaring an inner class that is semantically\n    #    private, but needed to be public for implementation reasons.\n    # Also ignores cases where the previous line ends with a backslash as can be\n    # common when defining classes in C macros.\n    prev_line = clean_lines.lines[linenum - 1]\n    if (not IsBlankLine(prev_line) and\n        not Search(r'\\b(class|struct)\\b', prev_line) and\n        not Search(r'\\\\$', prev_line)):\n      # Try a bit harder to find the beginning of the class.  This is to\n      # account for multi-line base-specifier lists, e.g.:\n      #   class Derived\n      #       : public Base {\n      end_class_head = class_info.starting_linenum\n      for i in range(class_info.starting_linenum, linenum):\n        if Search(r'\\{\\s*$', clean_lines.lines[i]):\n          end_class_head = i\n          break\n      if end_class_head < linenum - 1:\n        error(filename, linenum, 'whitespace/blank_line', 3,\n              '\"%s:\" should be preceded by a blank line' % matched.group(1))\n\n\ndef GetPreviousNonBlankLine(clean_lines, linenum):\n  \"\"\"Return the most recent non-blank line and its line number.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file contents.\n    linenum: The number of the line to check.\n\n  Returns:\n    A tuple with two elements.  The first element is the contents of the last\n    non-blank line before the current line, or the empty string if this is the\n    first non-blank line.  The second is the line number of that line, or -1\n    if this is the first non-blank line.\n  \"\"\"\n\n  prevlinenum = linenum - 1\n  while prevlinenum >= 0:\n    prevline = clean_lines.elided[prevlinenum]\n    if not IsBlankLine(prevline):     # if not a blank line...\n      return (prevline, prevlinenum)\n    prevlinenum -= 1\n  return ('', -1)\n\n\ndef CheckBraces(filename, clean_lines, linenum, error):\n  \"\"\"Looks for misplaced braces (e.g. at the end of line).\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  line = clean_lines.elided[linenum]        # get rid of comments and strings\n\n  if Match(r'\\s*{\\s*$', line):\n    # We allow an open brace to start a line in the case where someone is using\n    # braces in a block to explicitly create a new scope, which is commonly used\n    # to control the lifetime of stack-allocated variables.  Braces are also\n    # used for brace initializers inside function calls.  We don't detect this\n    # perfectly: we just don't complain if the last non-whitespace character on\n    # the previous non-blank line is ',', ';', ':', '(', '{', or '}', or if the\n    # previous line starts a preprocessor block. We also allow a brace on the\n    # following line if it is part of an array initialization and would not fit\n    # within the 80 character limit of the preceding line.\n    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n    if (not Search(r'[,;:}{(]\\s*$', prevline) and\n        not Match(r'\\s*#', prevline) and\n        not (GetLineWidth(prevline) > _line_length - 2 and '[]' in prevline)):\n      error(filename, linenum, 'whitespace/braces', 4,\n            '{ should almost always be at the end of the previous line')\n\n  # An else clause should be on the same line as the preceding closing brace.\n  if Match(r'\\s*else\\b\\s*(?:if\\b|\\{|$)', line):\n    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n    if Match(r'\\s*}\\s*$', prevline):\n      error(filename, linenum, 'whitespace/newline', 4,\n            'An else should appear on the same line as the preceding }')\n\n  # If braces come on one side of an else, they should be on both.\n  # However, we have to worry about \"else if\" that spans multiple lines!\n  if Search(r'else if\\s*\\(', line):       # could be multi-line if\n    brace_on_left = bool(Search(r'}\\s*else if\\s*\\(', line))\n    # find the ( after the if\n    pos = line.find('else if')\n    pos = line.find('(', pos)\n    if pos > 0:\n      (endline, _, endpos) = CloseExpression(clean_lines, linenum, pos)\n      brace_on_right = endline[endpos:].find('{') != -1\n      if brace_on_left != brace_on_right:    # must be brace after if\n        error(filename, linenum, 'readability/braces', 5,\n              'If an else has a brace on one side, it should have it on both')\n  elif Search(r'}\\s*else[^{]*$', line) or Match(r'[^}]*else\\s*{', line):\n    error(filename, linenum, 'readability/braces', 5,\n          'If an else has a brace on one side, it should have it on both')\n\n  # Likewise, an else should never have the else clause on the same line\n  if Search(r'\\belse [^\\s{]', line) and not Search(r'\\belse if\\b', line):\n    error(filename, linenum, 'whitespace/newline', 4,\n          'Else clause should never be on same line as else (use 2 lines)')\n\n  # In the same way, a do/while should never be on one line\n  if Match(r'\\s*do [^\\s{]', line):\n    error(filename, linenum, 'whitespace/newline', 4,\n          'do/while clauses should not be on a single line')\n\n  # Check single-line if/else bodies. The style guide says 'curly braces are not\n  # required for single-line statements'. We additionally allow multi-line,\n  # single statements, but we reject anything with more than one semicolon in\n  # it. This means that the first semicolon after the if should be at the end of\n  # its line, and the line after that should have an indent level equal to or\n  # lower than the if. We also check for ambiguous if/else nesting without\n  # braces.\n  if_else_match = Search(r'\\b(if\\s*(|constexpr)\\s*\\(|else\\b)', line)\n  if if_else_match and not Match(r'\\s*#', line):\n    if_indent = GetIndentLevel(line)\n    endline, endlinenum, endpos = line, linenum, if_else_match.end()\n    if_match = Search(r'\\bif\\s*(|constexpr)\\s*\\(', line)\n    if if_match:\n      # This could be a multiline if condition, so find the end first.\n      pos = if_match.end() - 1\n      (endline, endlinenum, endpos) = CloseExpression(clean_lines, linenum, pos)\n    # Check for an opening brace, either directly after the if or on the next\n    # line. If found, this isn't a single-statement conditional.\n    if (not Match(r'\\s*{', endline[endpos:])\n        and not (Match(r'\\s*$', endline[endpos:])\n                 and endlinenum < (len(clean_lines.elided) - 1)\n                 and Match(r'\\s*{', clean_lines.elided[endlinenum + 1]))):\n      while (endlinenum < len(clean_lines.elided)\n             and ';' not in clean_lines.elided[endlinenum][endpos:]):\n        endlinenum += 1\n        endpos = 0\n      if endlinenum < len(clean_lines.elided):\n        endline = clean_lines.elided[endlinenum]\n        # We allow a mix of whitespace and closing braces (e.g. for one-liner\n        # methods) and a single \\ after the semicolon (for macros)\n        endpos = endline.find(';')\n        if not Match(r';[\\s}]*(\\\\?)$', endline[endpos:]):\n          # Semicolon isn't the last character, there's something trailing.\n          # Output a warning if the semicolon is not contained inside\n          # a lambda expression.\n          if not Match(r'^[^{};]*\\[[^\\[\\]]*\\][^{}]*\\{[^{}]*\\}\\s*\\)*[;,]\\s*$',\n                       endline):\n            error(filename, linenum, 'readability/braces', 4,\n                  'If/else bodies with multiple statements require braces')\n        elif endlinenum < len(clean_lines.elided) - 1:\n          # Make sure the next line is dedented\n          next_line = clean_lines.elided[endlinenum + 1]\n          next_indent = GetIndentLevel(next_line)\n          # With ambiguous nested if statements, this will error out on the\n          # if that *doesn't* match the else, regardless of whether it's the\n          # inner one or outer one.\n          if (if_match and Match(r'\\s*else\\b', next_line)\n              and next_indent != if_indent):\n            error(filename, linenum, 'readability/braces', 4,\n                  'Else clause should be indented at the same level as if. '\n                  'Ambiguous nested if/else chains require braces.')\n          elif next_indent > if_indent:\n            error(filename, linenum, 'readability/braces', 4,\n                  'If/else bodies with multiple statements require braces')\n\n\ndef CheckTrailingSemicolon(filename, clean_lines, linenum, error):\n  \"\"\"Looks for redundant trailing semicolon.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  line = clean_lines.elided[linenum]\n\n  # Block bodies should not be followed by a semicolon.  Due to C++11\n  # brace initialization, there are more places where semicolons are\n  # required than not, so we explicitly list the allowed rules rather\n  # than listing the disallowed ones.  These are the places where \"};\"\n  # should be replaced by just \"}\":\n  # 1. Some flavor of block following closing parenthesis:\n  #    for (;;) {};\n  #    while (...) {};\n  #    switch (...) {};\n  #    Function(...) {};\n  #    if (...) {};\n  #    if (...) else if (...) {};\n  #\n  # 2. else block:\n  #    if (...) else {};\n  #\n  # 3. const member function:\n  #    Function(...) const {};\n  #\n  # 4. Block following some statement:\n  #    x = 42;\n  #    {};\n  #\n  # 5. Block at the beginning of a function:\n  #    Function(...) {\n  #      {};\n  #    }\n  #\n  #    Note that naively checking for the preceding \"{\" will also match\n  #    braces inside multi-dimensional arrays, but this is fine since\n  #    that expression will not contain semicolons.\n  #\n  # 6. Block following another block:\n  #    while (true) {}\n  #    {};\n  #\n  # 7. End of namespaces:\n  #    namespace {};\n  #\n  #    These semicolons seems far more common than other kinds of\n  #    redundant semicolons, possibly due to people converting classes\n  #    to namespaces.  For now we do not warn for this case.\n  #\n  # Try matching case 1 first.\n  match = Match(r'^(.*\\)\\s*)\\{', line)\n  if match:\n    # Matched closing parenthesis (case 1).  Check the token before the\n    # matching opening parenthesis, and don't warn if it looks like a\n    # macro.  This avoids these false positives:\n    #  - macro that defines a base class\n    #  - multi-line macro that defines a base class\n    #  - macro that defines the whole class-head\n    #\n    # But we still issue warnings for macros that we know are safe to\n    # warn, specifically:\n    #  - TEST, TEST_F, TEST_P, MATCHER, MATCHER_P\n    #  - TYPED_TEST\n    #  - INTERFACE_DEF\n    #  - EXCLUSIVE_LOCKS_REQUIRED, SHARED_LOCKS_REQUIRED, LOCKS_EXCLUDED:\n    #\n    # We implement a list of safe macros instead of a list of\n    # unsafe macros, even though the latter appears less frequently in\n    # google code and would have been easier to implement.  This is because\n    # the downside for getting the allowed checks wrong means some extra\n    # semicolons, while the downside for getting disallowed checks wrong\n    # would result in compile errors.\n    #\n    # In addition to macros, we also don't want to warn on\n    #  - Compound literals\n    #  - Lambdas\n    #  - alignas specifier with anonymous structs\n    #  - decltype\n    closing_brace_pos = match.group(1).rfind(')')\n    opening_parenthesis = ReverseCloseExpression(\n        clean_lines, linenum, closing_brace_pos)\n    if opening_parenthesis[2] > -1:\n      line_prefix = opening_parenthesis[0][0:opening_parenthesis[2]]\n      macro = Search(r'\\b([A-Z_][A-Z0-9_]*)\\s*$', line_prefix)\n      func = Match(r'^(.*\\])\\s*$', line_prefix)\n      if ((macro and\n           macro.group(1) not in (\n               'TEST', 'TEST_F', 'MATCHER', 'MATCHER_P', 'TYPED_TEST',\n               'EXCLUSIVE_LOCKS_REQUIRED', 'SHARED_LOCKS_REQUIRED',\n               'LOCKS_EXCLUDED', 'INTERFACE_DEF')) or\n          (func and not Search(r'\\boperator\\s*\\[\\s*\\]', func.group(1))) or\n          Search(r'\\b(?:struct|union)\\s+alignas\\s*$', line_prefix) or\n          Search(r'\\bdecltype$', line_prefix) or\n          Search(r'\\s+=\\s*$', line_prefix)):\n        match = None\n    if (match and\n        opening_parenthesis[1] > 1 and\n        Search(r'\\]\\s*$', clean_lines.elided[opening_parenthesis[1] - 1])):\n      # Multi-line lambda-expression\n      match = None\n\n  else:\n    # Try matching cases 2-3.\n    match = Match(r'^(.*(?:else|\\)\\s*const)\\s*)\\{', line)\n    if not match:\n      # Try matching cases 4-6.  These are always matched on separate lines.\n      #\n      # Note that we can't simply concatenate the previous line to the\n      # current line and do a single match, otherwise we may output\n      # duplicate warnings for the blank line case:\n      #   if (cond) {\n      #     // blank line\n      #   }\n      prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n      if prevline and Search(r'[;{}]\\s*$', prevline):\n        match = Match(r'^(\\s*)\\{', line)\n\n  # Check matching closing brace\n  if match:\n    (endline, endlinenum, endpos) = CloseExpression(\n        clean_lines, linenum, len(match.group(1)))\n    if endpos > -1 and Match(r'^\\s*;', endline[endpos:]):\n      # Current {} pair is eligible for semicolon check, and we have found\n      # the redundant semicolon, output warning here.\n      #\n      # Note: because we are scanning forward for opening braces, and\n      # outputting warnings for the matching closing brace, if there are\n      # nested blocks with trailing semicolons, we will get the error\n      # messages in reversed order.\n\n      # We need to check the line forward for NOLINT\n      raw_lines = clean_lines.raw_lines\n      ParseNolintSuppressions(filename, raw_lines[endlinenum-1], endlinenum-1,\n                              error)\n      ParseNolintSuppressions(filename, raw_lines[endlinenum], endlinenum,\n                              error)\n\n      error(filename, endlinenum, 'readability/braces', 4,\n            \"You don't need a ; after a }\")\n\n\ndef CheckEmptyBlockBody(filename, clean_lines, linenum, error):\n  \"\"\"Look for empty loop/conditional body with only a single semicolon.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Search for loop keywords at the beginning of the line.  Because only\n  # whitespaces are allowed before the keywords, this will also ignore most\n  # do-while-loops, since those lines should start with closing brace.\n  #\n  # We also check \"if\" blocks here, since an empty conditional block\n  # is likely an error.\n  line = clean_lines.elided[linenum]\n  matched = Match(r'\\s*(for|while|if)\\s*\\(', line)\n  if matched:\n    # Find the end of the conditional expression.\n    (end_line, end_linenum, end_pos) = CloseExpression(\n        clean_lines, linenum, line.find('('))\n\n    # Output warning if what follows the condition expression is a semicolon.\n    # No warning for all other cases, including whitespace or newline, since we\n    # have a separate check for semicolons preceded by whitespace.\n    if end_pos >= 0 and Match(r';', end_line[end_pos:]):\n      if matched.group(1) == 'if':\n        error(filename, end_linenum, 'whitespace/empty_conditional_body', 5,\n              'Empty conditional bodies should use {}')\n      else:\n        error(filename, end_linenum, 'whitespace/empty_loop_body', 5,\n              'Empty loop bodies should use {} or continue')\n\n    # Check for if statements that have completely empty bodies (no comments)\n    # and no else clauses.\n    if end_pos >= 0 and matched.group(1) == 'if':\n      # Find the position of the opening { for the if statement.\n      # Return without logging an error if it has no brackets.\n      opening_linenum = end_linenum\n      opening_line_fragment = end_line[end_pos:]\n      # Loop until EOF or find anything that's not whitespace or opening {.\n      while not Search(r'^\\s*\\{', opening_line_fragment):\n        if Search(r'^(?!\\s*$)', opening_line_fragment):\n          # Conditional has no brackets.\n          return\n        opening_linenum += 1\n        if opening_linenum == len(clean_lines.elided):\n          # Couldn't find conditional's opening { or any code before EOF.\n          return\n        opening_line_fragment = clean_lines.elided[opening_linenum]\n      # Set opening_line (opening_line_fragment may not be entire opening line).\n      opening_line = clean_lines.elided[opening_linenum]\n\n      # Find the position of the closing }.\n      opening_pos = opening_line_fragment.find('{')\n      if opening_linenum == end_linenum:\n        # We need to make opening_pos relative to the start of the entire line.\n        opening_pos += end_pos\n      (closing_line, closing_linenum, closing_pos) = CloseExpression(\n          clean_lines, opening_linenum, opening_pos)\n      if closing_pos < 0:\n        return\n\n      # Now construct the body of the conditional. This consists of the portion\n      # of the opening line after the {, all lines until the closing line,\n      # and the portion of the closing line before the }.\n      if (clean_lines.raw_lines[opening_linenum] !=\n          CleanseComments(clean_lines.raw_lines[opening_linenum])):\n        # Opening line ends with a comment, so conditional isn't empty.\n        return\n      if closing_linenum > opening_linenum:\n        # Opening line after the {. Ignore comments here since we checked above.\n        bodylist = list(opening_line[opening_pos+1:])\n        # All lines until closing line, excluding closing line, with comments.\n        bodylist.extend(clean_lines.raw_lines[opening_linenum+1:closing_linenum])\n        # Closing line before the }. Won't (and can't) have comments.\n        bodylist.append(clean_lines.elided[closing_linenum][:closing_pos-1])\n        body = '\\n'.join(bodylist)\n      else:\n        # If statement has brackets and fits on a single line.\n        body = opening_line[opening_pos+1:closing_pos-1]\n\n      # Check if the body is empty\n      if not _EMPTY_CONDITIONAL_BODY_PATTERN.search(body):\n        return\n      # The body is empty. Now make sure there's not an else clause.\n      current_linenum = closing_linenum\n      current_line_fragment = closing_line[closing_pos:]\n      # Loop until EOF or find anything that's not whitespace or else clause.\n      while Search(r'^\\s*$|^(?=\\s*else)', current_line_fragment):\n        if Search(r'^(?=\\s*else)', current_line_fragment):\n          # Found an else clause, so don't log an error.\n          return\n        current_linenum += 1\n        if current_linenum == len(clean_lines.elided):\n          break\n        current_line_fragment = clean_lines.elided[current_linenum]\n\n      # The body is empty and there's no else clause until EOF or other code.\n      error(filename, end_linenum, 'whitespace/empty_if_body', 4,\n            ('If statement had no body and no else clause'))\n\n\ndef FindCheckMacro(line):\n  \"\"\"Find a replaceable CHECK-like macro.\n\n  Args:\n    line: line to search on.\n  Returns:\n    (macro name, start position), or (None, -1) if no replaceable\n    macro is found.\n  \"\"\"\n  for macro in _CHECK_MACROS:\n    i = line.find(macro)\n    if i >= 0:\n      # Find opening parenthesis.  Do a regular expression match here\n      # to make sure that we are matching the expected CHECK macro, as\n      # opposed to some other macro that happens to contain the CHECK\n      # substring.\n      matched = Match(r'^(.*\\b' + macro + r'\\s*)\\(', line)\n      if not matched:\n        continue\n      return (macro, len(matched.group(1)))\n  return (None, -1)\n\n\ndef CheckCheck(filename, clean_lines, linenum, error):\n  \"\"\"Checks the use of CHECK and EXPECT macros.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Decide the set of replacement macros that should be suggested\n  lines = clean_lines.elided\n  (check_macro, start_pos) = FindCheckMacro(lines[linenum])\n  if not check_macro:\n    return\n\n  # Find end of the boolean expression by matching parentheses\n  (last_line, end_line, end_pos) = CloseExpression(\n      clean_lines, linenum, start_pos)\n  if end_pos < 0:\n    return\n\n  # If the check macro is followed by something other than a\n  # semicolon, assume users will log their own custom error messages\n  # and don't suggest any replacements.\n  if not Match(r'\\s*;', last_line[end_pos:]):\n    return\n\n  if linenum == end_line:\n    expression = lines[linenum][start_pos + 1:end_pos - 1]\n  else:\n    expression = lines[linenum][start_pos + 1:]\n    for i in xrange(linenum + 1, end_line):\n      expression += lines[i]\n    expression += last_line[0:end_pos - 1]\n\n  # Parse expression so that we can take parentheses into account.\n  # This avoids false positives for inputs like \"CHECK((a < 4) == b)\",\n  # which is not replaceable by CHECK_LE.\n  lhs = ''\n  rhs = ''\n  operator = None\n  while expression:\n    matched = Match(r'^\\s*(<<|<<=|>>|>>=|->\\*|->|&&|\\|\\||'\n                    r'==|!=|>=|>|<=|<|\\()(.*)$', expression)\n    if matched:\n      token = matched.group(1)\n      if token == '(':\n        # Parenthesized operand\n        expression = matched.group(2)\n        (end, _) = FindEndOfExpressionInLine(expression, 0, ['('])\n        if end < 0:\n          return  # Unmatched parenthesis\n        lhs += '(' + expression[0:end]\n        expression = expression[end:]\n      elif token in ('&&', '||'):\n        # Logical and/or operators.  This means the expression\n        # contains more than one term, for example:\n        #   CHECK(42 < a && a < b);\n        #\n        # These are not replaceable with CHECK_LE, so bail out early.\n        return\n      elif token in ('<<', '<<=', '>>', '>>=', '->*', '->'):\n        # Non-relational operator\n        lhs += token\n        expression = matched.group(2)\n      else:\n        # Relational operator\n        operator = token\n        rhs = matched.group(2)\n        break\n    else:\n      # Unparenthesized operand.  Instead of appending to lhs one character\n      # at a time, we do another regular expression match to consume several\n      # characters at once if possible.  Trivial benchmark shows that this\n      # is more efficient when the operands are longer than a single\n      # character, which is generally the case.\n      matched = Match(r'^([^-=!<>()&|]+)(.*)$', expression)\n      if not matched:\n        matched = Match(r'^(\\s*\\S)(.*)$', expression)\n        if not matched:\n          break\n      lhs += matched.group(1)\n      expression = matched.group(2)\n\n  # Only apply checks if we got all parts of the boolean expression\n  if not (lhs and operator and rhs):\n    return\n\n  # Check that rhs do not contain logical operators.  We already know\n  # that lhs is fine since the loop above parses out && and ||.\n  if rhs.find('&&') > -1 or rhs.find('||') > -1:\n    return\n\n  # At least one of the operands must be a constant literal.  This is\n  # to avoid suggesting replacements for unprintable things like\n  # CHECK(variable != iterator)\n  #\n  # The following pattern matches decimal, hex integers, strings, and\n  # characters (in that order).\n  lhs = lhs.strip()\n  rhs = rhs.strip()\n  match_constant = r'^([-+]?(\\d+|0[xX][0-9a-fA-F]+)[lLuU]{0,3}|\".*\"|\\'.*\\')$'\n  if Match(match_constant, lhs) or Match(match_constant, rhs):\n    # Note: since we know both lhs and rhs, we can provide a more\n    # descriptive error message like:\n    #   Consider using CHECK_EQ(x, 42) instead of CHECK(x == 42)\n    # Instead of:\n    #   Consider using CHECK_EQ instead of CHECK(a == b)\n    #\n    # We are still keeping the less descriptive message because if lhs\n    # or rhs gets long, the error message might become unreadable.\n    error(filename, linenum, 'readability/check', 2,\n          'Consider using %s instead of %s(a %s b)' % (\n              _CHECK_REPLACEMENT[check_macro][operator],\n              check_macro, operator))\n\n\ndef CheckAltTokens(filename, clean_lines, linenum, error):\n  \"\"\"Check alternative keywords being used in boolean expressions.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Avoid preprocessor lines\n  if Match(r'^\\s*#', line):\n    return\n\n  # Last ditch effort to avoid multi-line comments.  This will not help\n  # if the comment started before the current line or ended after the\n  # current line, but it catches most of the false positives.  At least,\n  # it provides a way to workaround this warning for people who use\n  # multi-line comments in preprocessor macros.\n  #\n  # TODO(unknown): remove this once cpplint has better support for\n  # multi-line comments.\n  if line.find('/*') >= 0 or line.find('*/') >= 0:\n    return\n\n  for match in _ALT_TOKEN_REPLACEMENT_PATTERN.finditer(line):\n    error(filename, linenum, 'readability/alt_tokens', 2,\n          'Use operator %s instead of %s' % (\n              _ALT_TOKEN_REPLACEMENT[match.group(1)], match.group(1)))\n\n\ndef GetLineWidth(line):\n  \"\"\"Determines the width of the line in column positions.\n\n  Args:\n    line: A string, which may be a Unicode string.\n\n  Returns:\n    The width of the line in column positions, accounting for Unicode\n    combining characters and wide characters.\n  \"\"\"\n  if isinstance(line, unicode):\n    width = 0\n    for uc in unicodedata.normalize('NFC', line):\n      if unicodedata.east_asian_width(uc) in ('W', 'F'):\n        width += 2\n      elif not unicodedata.combining(uc):\n        # Issue 337\n        # https://mail.python.org/pipermail/python-list/2012-August/628809.html\n        if (sys.version_info.major, sys.version_info.minor) <= (3, 2):\n          # https://github.com/python/cpython/blob/2.7/Include/unicodeobject.h#L81\n          is_wide_build = sysconfig.get_config_var(\"Py_UNICODE_SIZE\") >= 4\n          # https://github.com/python/cpython/blob/2.7/Objects/unicodeobject.c#L564\n          is_low_surrogate = 0xDC00 <= ord(uc) <= 0xDFFF\n          if not is_wide_build and is_low_surrogate:\n            width -= 1\n\n        width += 1\n    return width\n  else:\n    return len(line)\n\n\ndef CheckStyle(filename, clean_lines, linenum, file_extension, nesting_state,\n               error):\n  \"\"\"Checks rules from the 'C++ style rules' section of cppguide.html.\n\n  Most of these rules are hard to test (naming, comment style), but we\n  do what we can.  In particular we check for 2-space indents, line lengths,\n  tab usage, spaces inside code, etc.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    file_extension: The extension (without the dot) of the filename.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Don't use \"elided\" lines here, otherwise we can't check commented lines.\n  # Don't want to use \"raw\" either, because we don't want to check inside C++11\n  # raw strings,\n  raw_lines = clean_lines.lines_without_raw_strings\n  line = raw_lines[linenum]\n  prev = raw_lines[linenum - 1] if linenum > 0 else ''\n\n  if line.find('\\t') != -1:\n    error(filename, linenum, 'whitespace/tab', 1,\n          'Tab found; better to use spaces')\n\n  # One or three blank spaces at the beginning of the line is weird; it's\n  # hard to reconcile that with 2-space indents.\n  # NOTE: here are the conditions rob pike used for his tests.  Mine aren't\n  # as sophisticated, but it may be worth becoming so:  RLENGTH==initial_spaces\n  # if(RLENGTH > 20) complain = 0;\n  # if(match($0, \" +(error|private|public|protected):\")) complain = 0;\n  # if(match(prev, \"&& *$\")) complain = 0;\n  # if(match(prev, \"\\\\|\\\\| *$\")) complain = 0;\n  # if(match(prev, \"[\\\",=><] *$\")) complain = 0;\n  # if(match($0, \" <<\")) complain = 0;\n  # if(match(prev, \" +for \\\\(\")) complain = 0;\n  # if(prevodd && match(prevprev, \" +for \\\\(\")) complain = 0;\n  scope_or_label_pattern = r'\\s*(?:public|private|protected|signals)(?:\\s+(?:slots\\s*)?)?:\\s*\\\\?$'\n  classinfo = nesting_state.InnermostClass()\n  initial_spaces = 0\n  cleansed_line = clean_lines.elided[linenum]\n  while initial_spaces < len(line) and line[initial_spaces] == ' ':\n    initial_spaces += 1\n  # There are certain situations we allow one space, notably for\n  # section labels, and also lines containing multi-line raw strings.\n  # We also don't check for lines that look like continuation lines\n  # (of lines ending in double quotes, commas, equals, or angle brackets)\n  # because the rules for how to indent those are non-trivial.\n  if (not Search(r'[\",=><] *$', prev) and\n      (initial_spaces == 1 or initial_spaces == 3) and\n      not Match(scope_or_label_pattern, cleansed_line) and\n      not (clean_lines.raw_lines[linenum] != line and\n           Match(r'^\\s*\"\"', line))):\n    error(filename, linenum, 'whitespace/indent', 3,\n          'Weird number of spaces at line-start.  '\n          'Are you using a 2-space indent?')\n\n  if line and line[-1].isspace():\n    error(filename, linenum, 'whitespace/end_of_line', 4,\n          'Line ends in whitespace.  Consider deleting these extra spaces.')\n\n  # Check if the line is a header guard.\n  is_header_guard = False\n  if IsHeaderExtension(file_extension):\n    cppvar = GetHeaderGuardCPPVariable(filename)\n    if (line.startswith('#ifndef %s' % cppvar) or\n        line.startswith('#define %s' % cppvar) or\n        line.startswith('#endif  // %s' % cppvar)):\n      is_header_guard = True\n  # #include lines and header guards can be long, since there's no clean way to\n  # split them.\n  #\n  # URLs can be long too.  It's possible to split these, but it makes them\n  # harder to cut&paste.\n  #\n  # The \"$Id:...$\" comment may also get very long without it being the\n  # developers fault.\n  #\n  # Doxygen documentation copying can get pretty long when using an overloaded\n  # function declaration\n  if (not line.startswith('#include') and not is_header_guard and\n      not Match(r'^\\s*//.*http(s?)://\\S*$', line) and\n      not Match(r'^\\s*//\\s*[^\\s]*$', line) and\n      not Match(r'^// \\$Id:.*#[0-9]+ \\$$', line) and\n      not Match(r'^\\s*/// [@\\\\](copydoc|copydetails|copybrief) .*$', line)):\n    line_width = GetLineWidth(line)\n    if line_width > _line_length:\n      error(filename, linenum, 'whitespace/line_length', 2,\n            'Lines should be <= %i characters long' % _line_length)\n\n  if (cleansed_line.count(';') > 1 and\n      # allow simple single line lambdas\n      not Match(r'^[^{};]*\\[[^\\[\\]]*\\][^{}]*\\{[^{}\\n\\r]*\\}',\n                line) and\n      # for loops are allowed two ;'s (and may run over two lines).\n      cleansed_line.find('for') == -1 and\n      (GetPreviousNonBlankLine(clean_lines, linenum)[0].find('for') == -1 or\n       GetPreviousNonBlankLine(clean_lines, linenum)[0].find(';') != -1) and\n      # It's ok to have many commands in a switch case that fits in 1 line\n      not ((cleansed_line.find('case ') != -1 or\n            cleansed_line.find('default:') != -1) and\n           cleansed_line.find('break;') != -1)):\n    error(filename, linenum, 'whitespace/newline', 0,\n          'More than one command on the same line')\n\n  # Some more style checks\n  CheckBraces(filename, clean_lines, linenum, error)\n  CheckTrailingSemicolon(filename, clean_lines, linenum, error)\n  CheckEmptyBlockBody(filename, clean_lines, linenum, error)\n  CheckSpacing(filename, clean_lines, linenum, nesting_state, error)\n  CheckOperatorSpacing(filename, clean_lines, linenum, error)\n  CheckParenthesisSpacing(filename, clean_lines, linenum, error)\n  CheckCommaSpacing(filename, clean_lines, linenum, error)\n  CheckBracesSpacing(filename, clean_lines, linenum, nesting_state, error)\n  CheckSpacingForFunctionCall(filename, clean_lines, linenum, error)\n  CheckCheck(filename, clean_lines, linenum, error)\n  CheckAltTokens(filename, clean_lines, linenum, error)\n  classinfo = nesting_state.InnermostClass()\n  if classinfo:\n    CheckSectionSpacing(filename, clean_lines, classinfo, linenum, error)\n\n\n_RE_PATTERN_INCLUDE = re.compile(r'^\\s*#\\s*include\\s*([<\"])([^>\"]*)[>\"].*$')\n# Matches the first component of a filename delimited by -s and _s. That is:\n#  _RE_FIRST_COMPONENT.match('foo').group(0) == 'foo'\n#  _RE_FIRST_COMPONENT.match('foo.cc').group(0) == 'foo'\n#  _RE_FIRST_COMPONENT.match('foo-bar_baz.cc').group(0) == 'foo'\n#  _RE_FIRST_COMPONENT.match('foo_bar-baz.cc').group(0) == 'foo'\n_RE_FIRST_COMPONENT = re.compile(r'^[^-_.]+')\n\n\ndef _DropCommonSuffixes(filename):\n  \"\"\"Drops common suffixes like _test.cc or -inl.h from filename.\n\n  For example:\n    >>> _DropCommonSuffixes('foo/foo-inl.h')\n    'foo/foo'\n    >>> _DropCommonSuffixes('foo/bar/foo.cc')\n    'foo/bar/foo'\n    >>> _DropCommonSuffixes('foo/foo_internal.h')\n    'foo/foo'\n    >>> _DropCommonSuffixes('foo/foo_unusualinternal.h')\n    'foo/foo_unusualinternal'\n\n  Args:\n    filename: The input filename.\n\n  Returns:\n    The filename with the common suffix removed.\n  \"\"\"\n  for suffix in itertools.chain(\n      ('%s.%s' % (test_suffix.lstrip('_'), ext)\n       for test_suffix, ext in itertools.product(_test_suffixes, GetNonHeaderExtensions())),\n      ('%s.%s' % (suffix, ext)\n       for suffix, ext in itertools.product(['inl', 'imp', 'internal'], GetHeaderExtensions()))):\n    if (filename.endswith(suffix) and len(filename) > len(suffix) and\n        filename[-len(suffix) - 1] in ('-', '_')):\n      return filename[:-len(suffix) - 1]\n  return os.path.splitext(filename)[0]\n\n\ndef _ClassifyInclude(fileinfo, include, used_angle_brackets, include_order=\"default\"):\n  \"\"\"Figures out what kind of header 'include' is.\n\n  Args:\n    fileinfo: The current file cpplint is running over. A FileInfo instance.\n    include: The path to a #included file.\n    used_angle_brackets: True if the #include used <> rather than \"\".\n    include_order: \"default\" or other value allowed in program arguments\n\n  Returns:\n    One of the _XXX_HEADER constants.\n\n  For example:\n    >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'stdio.h', True)\n    _C_SYS_HEADER\n    >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'string', True)\n    _CPP_SYS_HEADER\n    >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'foo/foo.h', True, \"standardcfirst\")\n    _OTHER_SYS_HEADER\n    >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'foo/foo.h', False)\n    _LIKELY_MY_HEADER\n    >>> _ClassifyInclude(FileInfo('foo/foo_unknown_extension.cc'),\n    ...                  'bar/foo_other_ext.h', False)\n    _POSSIBLE_MY_HEADER\n    >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'foo/bar.h', False)\n    _OTHER_HEADER\n  \"\"\"\n  # This is a list of all standard c++ header files, except\n  # those already checked for above.\n  is_cpp_header = include in _CPP_HEADERS\n\n  # Mark include as C header if in list or in a known folder for standard-ish C headers.\n  is_std_c_header = (include_order == \"default\") or (include in _C_HEADERS\n            # additional linux glibc header folders\n            or Search(r'(?:%s)\\/.*\\.h' % \"|\".join(C_STANDARD_HEADER_FOLDERS), include))\n\n  # Headers with C++ extensions shouldn't be considered C system headers\n  include_ext = os.path.splitext(include)[1]\n  is_system = used_angle_brackets and not include_ext in ['.hh', '.hpp', '.hxx', '.h++']\n\n  if is_system:\n    if is_cpp_header:\n      return _CPP_SYS_HEADER\n    if is_std_c_header:\n      return _C_SYS_HEADER\n    else:\n      return _OTHER_SYS_HEADER\n\n  # If the target file and the include we're checking share a\n  # basename when we drop common extensions, and the include\n  # lives in . , then it's likely to be owned by the target file.\n  target_dir, target_base = (\n      os.path.split(_DropCommonSuffixes(fileinfo.RepositoryName())))\n  include_dir, include_base = os.path.split(_DropCommonSuffixes(include))\n  target_dir_pub = os.path.normpath(target_dir + '/../public')\n  target_dir_pub = target_dir_pub.replace('\\\\', '/')\n  if target_base == include_base and (\n      include_dir == target_dir or\n      include_dir == target_dir_pub):\n    return _LIKELY_MY_HEADER\n\n  # If the target and include share some initial basename\n  # component, it's possible the target is implementing the\n  # include, so it's allowed to be first, but we'll never\n  # complain if it's not there.\n  target_first_component = _RE_FIRST_COMPONENT.match(target_base)\n  include_first_component = _RE_FIRST_COMPONENT.match(include_base)\n  if (target_first_component and include_first_component and\n      target_first_component.group(0) ==\n      include_first_component.group(0)):\n    return _POSSIBLE_MY_HEADER\n\n  return _OTHER_HEADER\n\n\n\ndef CheckIncludeLine(filename, clean_lines, linenum, include_state, error):\n  \"\"\"Check rules that are applicable to #include lines.\n\n  Strings on #include lines are NOT removed from elided line, to make\n  certain tasks easier. However, to prevent false positives, checks\n  applicable to #include lines in CheckLanguage must be put here.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    include_state: An _IncludeState instance in which the headers are inserted.\n    error: The function to call with any errors found.\n  \"\"\"\n  fileinfo = FileInfo(filename)\n  line = clean_lines.lines[linenum]\n\n  # \"include\" should use the new style \"foo/bar.h\" instead of just \"bar.h\"\n  # Only do this check if the included header follows google naming\n  # conventions.  If not, assume that it's a 3rd party API that\n  # requires special include conventions.\n  #\n  # We also make an exception for Lua headers, which follow google\n  # naming convention but not the include convention.\n  match = Match(r'#include\\s*\"([^/]+\\.(.*))\"', line)\n  if match:\n    if (IsHeaderExtension(match.group(2)) and\n        not _THIRD_PARTY_HEADERS_PATTERN.match(match.group(1))):\n      error(filename, linenum, 'build/include_subdir', 4,\n            'Include the directory when naming header files')\n\n  # we shouldn't include a file more than once. actually, there are a\n  # handful of instances where doing so is okay, but in general it's\n  # not.\n  match = _RE_PATTERN_INCLUDE.search(line)\n  if match:\n    include = match.group(2)\n    used_angle_brackets = (match.group(1) == '<')\n    duplicate_line = include_state.FindHeader(include)\n    if duplicate_line >= 0:\n      error(filename, linenum, 'build/include', 4,\n            '\"%s\" already included at %s:%s' %\n            (include, filename, duplicate_line))\n      return\n\n    for extension in GetNonHeaderExtensions():\n      if (include.endswith('.' + extension) and\n          os.path.dirname(fileinfo.RepositoryName()) != os.path.dirname(include)):\n        error(filename, linenum, 'build/include', 4,\n              'Do not include .' + extension + ' files from other packages')\n        return\n\n    # We DO want to include a 3rd party looking header if it matches the\n    # filename. Otherwise we get an erroneous error \"...should include its\n    # header\" error later.\n    third_src_header = False\n    for ext in GetHeaderExtensions():\n      basefilename = filename[0:len(filename) - len(fileinfo.Extension())]\n      headerfile = basefilename + '.' + ext\n      headername = FileInfo(headerfile).RepositoryName()\n      if headername in include or include in headername:\n        third_src_header = True\n        break\n\n    if third_src_header or not _THIRD_PARTY_HEADERS_PATTERN.match(include):\n      include_state.include_list[-1].append((include, linenum))\n\n      # We want to ensure that headers appear in the right order:\n      # 1) for foo.cc, foo.h  (preferred location)\n      # 2) c system files\n      # 3) cpp system files\n      # 4) for foo.cc, foo.h  (deprecated location)\n      # 5) other google headers\n      #\n      # We classify each include statement as one of those 5 types\n      # using a number of techniques. The include_state object keeps\n      # track of the highest type seen, and complains if we see a\n      # lower type after that.\n      error_message = include_state.CheckNextIncludeOrder(\n          _ClassifyInclude(fileinfo, include, used_angle_brackets, _include_order))\n      if error_message:\n        error(filename, linenum, 'build/include_order', 4,\n              '%s. Should be: %s.h, c system, c++ system, other.' %\n              (error_message, fileinfo.BaseName()))\n      canonical_include = include_state.CanonicalizeAlphabeticalOrder(include)\n      if not include_state.IsInAlphabeticalOrder(\n          clean_lines, linenum, canonical_include):\n        error(filename, linenum, 'build/include_alpha', 4,\n              'Include \"%s\" not in alphabetical order' % include)\n      include_state.SetLastHeader(canonical_include)\n\n\n\ndef _GetTextInside(text, start_pattern):\n  r\"\"\"Retrieves all the text between matching open and close parentheses.\n\n  Given a string of lines and a regular expression string, retrieve all the text\n  following the expression and between opening punctuation symbols like\n  (, [, or {, and the matching close-punctuation symbol. This properly nested\n  occurrences of the punctuations, so for the text like\n    printf(a(), b(c()));\n  a call to _GetTextInside(text, r'printf\\(') will return 'a(), b(c())'.\n  start_pattern must match string having an open punctuation symbol at the end.\n\n  Args:\n    text: The lines to extract text. Its comments and strings must be elided.\n           It can be single line and can span multiple lines.\n    start_pattern: The regexp string indicating where to start extracting\n                   the text.\n  Returns:\n    The extracted text.\n    None if either the opening string or ending punctuation could not be found.\n  \"\"\"\n  # TODO(unknown): Audit cpplint.py to see what places could be profitably\n  # rewritten to use _GetTextInside (and use inferior regexp matching today).\n\n  # Give opening punctuations to get the matching close-punctuations.\n  matching_punctuation = {'(': ')', '{': '}', '[': ']'}\n  closing_punctuation = set(itervalues(matching_punctuation))\n\n  # Find the position to start extracting text.\n  match = re.search(start_pattern, text, re.M)\n  if not match:  # start_pattern not found in text.\n    return None\n  start_position = match.end(0)\n\n  assert start_position > 0, (\n      'start_pattern must ends with an opening punctuation.')\n  assert text[start_position - 1] in matching_punctuation, (\n      'start_pattern must ends with an opening punctuation.')\n  # Stack of closing punctuations we expect to have in text after position.\n  punctuation_stack = [matching_punctuation[text[start_position - 1]]]\n  position = start_position\n  while punctuation_stack and position < len(text):\n    if text[position] == punctuation_stack[-1]:\n      punctuation_stack.pop()\n    elif text[position] in closing_punctuation:\n      # A closing punctuation without matching opening punctuations.\n      return None\n    elif text[position] in matching_punctuation:\n      punctuation_stack.append(matching_punctuation[text[position]])\n    position += 1\n  if punctuation_stack:\n    # Opening punctuations left without matching close-punctuations.\n    return None\n  # punctuations match.\n  return text[start_position:position - 1]\n\n\n# Patterns for matching call-by-reference parameters.\n#\n# Supports nested templates up to 2 levels deep using this messy pattern:\n#   < (?: < (?: < [^<>]*\n#               >\n#           |   [^<>] )*\n#         >\n#     |   [^<>] )*\n#   >\n_RE_PATTERN_IDENT = r'[_a-zA-Z]\\w*'  # =~ [[:alpha:]][[:alnum:]]*\n_RE_PATTERN_TYPE = (\n    r'(?:const\\s+)?(?:typename\\s+|class\\s+|struct\\s+|union\\s+|enum\\s+)?'\n    r'(?:\\w|'\n    r'\\s*<(?:<(?:<[^<>]*>|[^<>])*>|[^<>])*>|'\n    r'::)+')\n# A call-by-reference parameter ends with '& identifier'.\n_RE_PATTERN_REF_PARAM = re.compile(\n    r'(' + _RE_PATTERN_TYPE + r'(?:\\s*(?:\\bconst\\b|[*]))*\\s*'\n    r'&\\s*' + _RE_PATTERN_IDENT + r')\\s*(?:=[^,()]+)?[,)]')\n# A call-by-const-reference parameter either ends with 'const& identifier'\n# or looks like 'const type& identifier' when 'type' is atomic.\n_RE_PATTERN_CONST_REF_PARAM = (\n    r'(?:.*\\s*\\bconst\\s*&\\s*' + _RE_PATTERN_IDENT +\n    r'|const\\s+' + _RE_PATTERN_TYPE + r'\\s*&\\s*' + _RE_PATTERN_IDENT + r')')\n# Stream types.\n_RE_PATTERN_REF_STREAM_PARAM = (\n    r'(?:.*stream\\s*&\\s*' + _RE_PATTERN_IDENT + r')')\n\n\ndef CheckLanguage(filename, clean_lines, linenum, file_extension,\n                  include_state, nesting_state, error):\n  \"\"\"Checks rules from the 'C++ language rules' section of cppguide.html.\n\n  Some of these rules are hard to test (function overloading, using\n  uint32 inappropriately), but we do the best we can.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    file_extension: The extension (without the dot) of the filename.\n    include_state: An _IncludeState instance in which the headers are inserted.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n  # If the line is empty or consists of entirely a comment, no need to\n  # check it.\n  line = clean_lines.elided[linenum]\n  if not line:\n    return\n\n  match = _RE_PATTERN_INCLUDE.search(line)\n  if match:\n    CheckIncludeLine(filename, clean_lines, linenum, include_state, error)\n    return\n\n  # Reset include state across preprocessor directives.  This is meant\n  # to silence warnings for conditional includes.\n  match = Match(r'^\\s*#\\s*(if|ifdef|ifndef|elif|else|endif)\\b', line)\n  if match:\n    include_state.ResetSection(match.group(1))\n\n\n  # Perform other checks now that we are sure that this is not an include line\n  CheckCasts(filename, clean_lines, linenum, error)\n  CheckGlobalStatic(filename, clean_lines, linenum, error)\n  CheckPrintf(filename, clean_lines, linenum, error)\n\n  if IsHeaderExtension(file_extension):\n    # TODO(unknown): check that 1-arg constructors are explicit.\n    #                How to tell it's a constructor?\n    #                (handled in CheckForNonStandardConstructs for now)\n    # TODO(unknown): check that classes declare or disable copy/assign\n    #                (level 1 error)\n    pass\n\n  # Check if people are using the verboten C basic types.  The only exception\n  # we regularly allow is \"unsigned short port\" for port.\n  if Search(r'\\bshort port\\b', line):\n    if not Search(r'\\bunsigned short port\\b', line):\n      error(filename, linenum, 'runtime/int', 4,\n            'Use \"unsigned short\" for ports, not \"short\"')\n  else:\n    match = Search(r'\\b(short|long(?! +double)|long long)\\b', line)\n    if match:\n      error(filename, linenum, 'runtime/int', 4,\n            'Use int16/int64/etc, rather than the C type %s' % match.group(1))\n\n  # Check if some verboten operator overloading is going on\n  # TODO(unknown): catch out-of-line unary operator&:\n  #   class X {};\n  #   int operator&(const X& x) { return 42; }  // unary operator&\n  # The trick is it's hard to tell apart from binary operator&:\n  #   class Y { int operator&(const Y& x) { return 23; } }; // binary operator&\n  if Search(r'\\boperator\\s*&\\s*\\(\\s*\\)', line):\n    error(filename, linenum, 'runtime/operator', 4,\n          'Unary operator& is dangerous.  Do not use it.')\n\n  # Check for suspicious usage of \"if\" like\n  # } if (a == b) {\n  if Search(r'\\}\\s*if\\s*\\(', line):\n    error(filename, linenum, 'readability/braces', 4,\n          'Did you mean \"else if\"? If not, start a new line for \"if\".')\n\n  # Check for potential format string bugs like printf(foo).\n  # We constrain the pattern not to pick things like DocidForPrintf(foo).\n  # Not perfect but it can catch printf(foo.c_str()) and printf(foo->c_str())\n  # TODO(unknown): Catch the following case. Need to change the calling\n  # convention of the whole function to process multiple line to handle it.\n  #   printf(\n  #       boy_this_is_a_really_long_variable_that_cannot_fit_on_the_prev_line);\n  printf_args = _GetTextInside(line, r'(?i)\\b(string)?printf\\s*\\(')\n  if printf_args:\n    match = Match(r'([\\w.\\->()]+)$', printf_args)\n    if match and match.group(1) != '__VA_ARGS__':\n      function_name = re.search(r'\\b((?:string)?printf)\\s*\\(',\n                                line, re.I).group(1)\n      error(filename, linenum, 'runtime/printf', 4,\n            'Potential format string bug. Do %s(\"%%s\", %s) instead.'\n            % (function_name, match.group(1)))\n\n  # Check for potential memset bugs like memset(buf, sizeof(buf), 0).\n  match = Search(r'memset\\s*\\(([^,]*),\\s*([^,]*),\\s*0\\s*\\)', line)\n  if match and not Match(r\"^''|-?[0-9]+|0x[0-9A-Fa-f]$\", match.group(2)):\n    error(filename, linenum, 'runtime/memset', 4,\n          'Did you mean \"memset(%s, 0, %s)\"?'\n          % (match.group(1), match.group(2)))\n\n  if Search(r'\\busing namespace\\b', line):\n    if Search(r'\\bliterals\\b', line):\n      error(filename, linenum, 'build/namespaces_literals', 5,\n            'Do not use namespace using-directives.  '\n            'Use using-declarations instead.')\n    else:\n      error(filename, linenum, 'build/namespaces', 5,\n            'Do not use namespace using-directives.  '\n            'Use using-declarations instead.')\n\n  # Detect variable-length arrays.\n  match = Match(r'\\s*(.+::)?(\\w+) [a-z]\\w*\\[(.+)];', line)\n  if (match and match.group(2) != 'return' and match.group(2) != 'delete' and\n      match.group(3).find(']') == -1):\n    # Split the size using space and arithmetic operators as delimiters.\n    # If any of the resulting tokens are not compile time constants then\n    # report the error.\n    tokens = re.split(r'\\s|\\+|\\-|\\*|\\/|<<|>>]', match.group(3))\n    is_const = True\n    skip_next = False\n    for tok in tokens:\n      if skip_next:\n        skip_next = False\n        continue\n\n      if Search(r'sizeof\\(.+\\)', tok): continue\n      if Search(r'arraysize\\(\\w+\\)', tok): continue\n\n      tok = tok.lstrip('(')\n      tok = tok.rstrip(')')\n      if not tok: continue\n      if Match(r'\\d+', tok): continue\n      if Match(r'0[xX][0-9a-fA-F]+', tok): continue\n      if Match(r'k[A-Z0-9]\\w*', tok): continue\n      if Match(r'(.+::)?k[A-Z0-9]\\w*', tok): continue\n      if Match(r'(.+::)?[A-Z][A-Z0-9_]*', tok): continue\n      # A catch all for tricky sizeof cases, including 'sizeof expression',\n      # 'sizeof(*type)', 'sizeof(const type)', 'sizeof(struct StructName)'\n      # requires skipping the next token because we split on ' ' and '*'.\n      if tok.startswith('sizeof'):\n        skip_next = True\n        continue\n      is_const = False\n      break\n    if not is_const:\n      error(filename, linenum, 'runtime/arrays', 1,\n            'Do not use variable-length arrays.  Use an appropriately named '\n            \"('k' followed by CamelCase) compile-time constant for the size.\")\n\n  # Check for use of unnamed namespaces in header files.  Registration\n  # macros are typically OK, so we allow use of \"namespace {\" on lines\n  # that end with backslashes.\n  if (IsHeaderExtension(file_extension)\n      and Search(r'\\bnamespace\\s*{', line)\n      and line[-1] != '\\\\'):\n    error(filename, linenum, 'build/namespaces_headers', 4,\n          'Do not use unnamed namespaces in header files.  See '\n          'https://google-styleguide.googlecode.com/svn/trunk/cppguide.xml#Namespaces'\n          ' for more information.')\n\n\ndef CheckGlobalStatic(filename, clean_lines, linenum, error):\n  \"\"\"Check for unsafe global or static objects.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Match two lines at a time to support multiline declarations\n  if linenum + 1 < clean_lines.NumLines() and not Search(r'[;({]', line):\n    line += clean_lines.elided[linenum + 1].strip()\n\n  # Check for people declaring static/global STL strings at the top level.\n  # This is dangerous because the C++ language does not guarantee that\n  # globals with constructors are initialized before the first access, and\n  # also because globals can be destroyed when some threads are still running.\n  # TODO(unknown): Generalize this to also find static unique_ptr instances.\n  # TODO(unknown): File bugs for clang-tidy to find these.\n  match = Match(\n      r'((?:|static +)(?:|const +))(?::*std::)?string( +const)? +'\n      r'([a-zA-Z0-9_:]+)\\b(.*)',\n      line)\n\n  # Remove false positives:\n  # - String pointers (as opposed to values).\n  #    string *pointer\n  #    const string *pointer\n  #    string const *pointer\n  #    string *const pointer\n  #\n  # - Functions and template specializations.\n  #    string Function<Type>(...\n  #    string Class<Type>::Method(...\n  #\n  # - Operators.  These are matched separately because operator names\n  #   cross non-word boundaries, and trying to match both operators\n  #   and functions at the same time would decrease accuracy of\n  #   matching identifiers.\n  #    string Class::operator*()\n  if (match and\n      not Search(r'\\bstring\\b(\\s+const)?\\s*[\\*\\&]\\s*(const\\s+)?\\w', line) and\n      not Search(r'\\boperator\\W', line) and\n      not Match(r'\\s*(<.*>)?(::[a-zA-Z0-9_]+)*\\s*\\(([^\"]|$)', match.group(4))):\n    if Search(r'\\bconst\\b', line):\n      error(filename, linenum, 'runtime/string', 4,\n            'For a static/global string constant, use a C style string '\n            'instead: \"%schar%s %s[]\".' %\n            (match.group(1), match.group(2) or '', match.group(3)))\n    else:\n      error(filename, linenum, 'runtime/string', 4,\n            'Static/global string variables are not permitted.')\n\n  if (Search(r'\\b([A-Za-z0-9_]*_)\\(\\1\\)', line) or\n      Search(r'\\b([A-Za-z0-9_]*_)\\(CHECK_NOTNULL\\(\\1\\)\\)', line)):\n    error(filename, linenum, 'runtime/init', 4,\n          'You seem to be initializing a member variable with itself.')\n\n\ndef CheckPrintf(filename, clean_lines, linenum, error):\n  \"\"\"Check for printf related issues.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # When snprintf is used, the second argument shouldn't be a literal.\n  match = Search(r'snprintf\\s*\\(([^,]*),\\s*([0-9]*)\\s*,', line)\n  if match and match.group(2) != '0':\n    # If 2nd arg is zero, snprintf is used to calculate size.\n    error(filename, linenum, 'runtime/printf', 3,\n          'If you can, use sizeof(%s) instead of %s as the 2nd arg '\n          'to snprintf.' % (match.group(1), match.group(2)))\n\n  # Check if some verboten C functions are being used.\n  if Search(r'\\bsprintf\\s*\\(', line):\n    error(filename, linenum, 'runtime/printf', 5,\n          'Never use sprintf. Use snprintf instead.')\n  match = Search(r'\\b(strcpy|strcat)\\s*\\(', line)\n  if match:\n    error(filename, linenum, 'runtime/printf', 4,\n          'Almost always, snprintf is better than %s' % match.group(1))\n\n\ndef IsDerivedFunction(clean_lines, linenum):\n  \"\"\"Check if current line contains an inherited function.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if current line contains a function with \"override\"\n    virt-specifier.\n  \"\"\"\n  # Scan back a few lines for start of current function\n  for i in xrange(linenum, max(-1, linenum - 10), -1):\n    match = Match(r'^([^()]*\\w+)\\(', clean_lines.elided[i])\n    if match:\n      # Look for \"override\" after the matching closing parenthesis\n      line, _, closing_paren = CloseExpression(\n          clean_lines, i, len(match.group(1)))\n      return (closing_paren >= 0 and\n              Search(r'\\boverride\\b', line[closing_paren:]))\n  return False\n\n\ndef IsOutOfLineMethodDefinition(clean_lines, linenum):\n  \"\"\"Check if current line contains an out-of-line method definition.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if current line contains an out-of-line method definition.\n  \"\"\"\n  # Scan back a few lines for start of current function\n  for i in xrange(linenum, max(-1, linenum - 10), -1):\n    if Match(r'^([^()]*\\w+)\\(', clean_lines.elided[i]):\n      return Match(r'^[^()]*\\w+::\\w+\\(', clean_lines.elided[i]) is not None\n  return False\n\n\ndef IsInitializerList(clean_lines, linenum):\n  \"\"\"Check if current line is inside constructor initializer list.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if current line appears to be inside constructor initializer\n    list, False otherwise.\n  \"\"\"\n  for i in xrange(linenum, 1, -1):\n    line = clean_lines.elided[i]\n    if i == linenum:\n      remove_function_body = Match(r'^(.*)\\{\\s*$', line)\n      if remove_function_body:\n        line = remove_function_body.group(1)\n\n    if Search(r'\\s:\\s*\\w+[({]', line):\n      # A lone colon tend to indicate the start of a constructor\n      # initializer list.  It could also be a ternary operator, which\n      # also tend to appear in constructor initializer lists as\n      # opposed to parameter lists.\n      return True\n    if Search(r'\\}\\s*,\\s*$', line):\n      # A closing brace followed by a comma is probably the end of a\n      # brace-initialized member in constructor initializer list.\n      return True\n    if Search(r'[{};]\\s*$', line):\n      # Found one of the following:\n      # - A closing brace or semicolon, probably the end of the previous\n      #   function.\n      # - An opening brace, probably the start of current class or namespace.\n      #\n      # Current line is probably not inside an initializer list since\n      # we saw one of those things without seeing the starting colon.\n      return False\n\n  # Got to the beginning of the file without seeing the start of\n  # constructor initializer list.\n  return False\n\n\ndef CheckForNonConstReference(filename, clean_lines, linenum,\n                              nesting_state, error):\n  \"\"\"Check for non-const references.\n\n  Separate from CheckLanguage since it scans backwards from current\n  line, instead of scanning forward.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n  # Do nothing if there is no '&' on current line.\n  line = clean_lines.elided[linenum]\n  if '&' not in line:\n    return\n\n  # If a function is inherited, current function doesn't have much of\n  # a choice, so any non-const references should not be blamed on\n  # derived function.\n  if IsDerivedFunction(clean_lines, linenum):\n    return\n\n  # Don't warn on out-of-line method definitions, as we would warn on the\n  # in-line declaration, if it isn't marked with 'override'.\n  if IsOutOfLineMethodDefinition(clean_lines, linenum):\n    return\n\n  # Long type names may be broken across multiple lines, usually in one\n  # of these forms:\n  #   LongType\n  #       ::LongTypeContinued &identifier\n  #   LongType::\n  #       LongTypeContinued &identifier\n  #   LongType<\n  #       ...>::LongTypeContinued &identifier\n  #\n  # If we detected a type split across two lines, join the previous\n  # line to current line so that we can match const references\n  # accordingly.\n  #\n  # Note that this only scans back one line, since scanning back\n  # arbitrary number of lines would be expensive.  If you have a type\n  # that spans more than 2 lines, please use a typedef.\n  if linenum > 1:\n    previous = None\n    if Match(r'\\s*::(?:[\\w<>]|::)+\\s*&\\s*\\S', line):\n      # previous_line\\n + ::current_line\n      previous = Search(r'\\b((?:const\\s*)?(?:[\\w<>]|::)+[\\w<>])\\s*$',\n                        clean_lines.elided[linenum - 1])\n    elif Match(r'\\s*[a-zA-Z_]([\\w<>]|::)+\\s*&\\s*\\S', line):\n      # previous_line::\\n + current_line\n      previous = Search(r'\\b((?:const\\s*)?(?:[\\w<>]|::)+::)\\s*$',\n                        clean_lines.elided[linenum - 1])\n    if previous:\n      line = previous.group(1) + line.lstrip()\n    else:\n      # Check for templated parameter that is split across multiple lines\n      endpos = line.rfind('>')\n      if endpos > -1:\n        (_, startline, startpos) = ReverseCloseExpression(\n            clean_lines, linenum, endpos)\n        if startpos > -1 and startline < linenum:\n          # Found the matching < on an earlier line, collect all\n          # pieces up to current line.\n          line = ''\n          for i in xrange(startline, linenum + 1):\n            line += clean_lines.elided[i].strip()\n\n  # Check for non-const references in function parameters.  A single '&' may\n  # found in the following places:\n  #   inside expression: binary & for bitwise AND\n  #   inside expression: unary & for taking the address of something\n  #   inside declarators: reference parameter\n  # We will exclude the first two cases by checking that we are not inside a\n  # function body, including one that was just introduced by a trailing '{'.\n  # TODO(unknown): Doesn't account for 'catch(Exception& e)' [rare].\n  if (nesting_state.previous_stack_top and\n      not (isinstance(nesting_state.previous_stack_top, _ClassInfo) or\n           isinstance(nesting_state.previous_stack_top, _NamespaceInfo))):\n    # Not at toplevel, not within a class, and not within a namespace\n    return\n\n  # Avoid initializer lists.  We only need to scan back from the\n  # current line for something that starts with ':'.\n  #\n  # We don't need to check the current line, since the '&' would\n  # appear inside the second set of parentheses on the current line as\n  # opposed to the first set.\n  if linenum > 0:\n    for i in xrange(linenum - 1, max(0, linenum - 10), -1):\n      previous_line = clean_lines.elided[i]\n      if not Search(r'[),]\\s*$', previous_line):\n        break\n      if Match(r'^\\s*:\\s+\\S', previous_line):\n        return\n\n  # Avoid preprocessors\n  if Search(r'\\\\\\s*$', line):\n    return\n\n  # Avoid constructor initializer lists\n  if IsInitializerList(clean_lines, linenum):\n    return\n\n  # We allow non-const references in a few standard places, like functions\n  # called \"swap()\" or iostream operators like \"<<\" or \">>\".  Do not check\n  # those function parameters.\n  #\n  # We also accept & in static_assert, which looks like a function but\n  # it's actually a declaration expression.\n  allowed_functions = (r'(?:[sS]wap(?:<\\w:+>)?|'\n                           r'operator\\s*[<>][<>]|'\n                           r'static_assert|COMPILE_ASSERT'\n                           r')\\s*\\(')\n  if Search(allowed_functions, line):\n    return\n  elif not Search(r'\\S+\\([^)]*$', line):\n    # Don't see an allowed function on this line.  Actually we\n    # didn't see any function name on this line, so this is likely a\n    # multi-line parameter list.  Try a bit harder to catch this case.\n    for i in xrange(2):\n      if (linenum > i and\n          Search(allowed_functions, clean_lines.elided[linenum - i - 1])):\n        return\n\n  decls = ReplaceAll(r'{[^}]*}', ' ', line)  # exclude function body\n  for parameter in re.findall(_RE_PATTERN_REF_PARAM, decls):\n    if (not Match(_RE_PATTERN_CONST_REF_PARAM, parameter) and\n        not Match(_RE_PATTERN_REF_STREAM_PARAM, parameter)):\n      error(filename, linenum, 'runtime/references', 2,\n            'Is this a non-const reference? '\n            'If so, make const or use a pointer: ' +\n            ReplaceAll(' *<', '<', parameter))\n\n\ndef CheckCasts(filename, clean_lines, linenum, error):\n  \"\"\"Various cast related checks.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Check to see if they're using an conversion function cast.\n  # I just try to capture the most common basic types, though there are more.\n  # Parameterless conversion functions, such as bool(), are allowed as they are\n  # probably a member operator declaration or default constructor.\n  match = Search(\n      r'(\\bnew\\s+(?:const\\s+)?|\\S<\\s*(?:const\\s+)?)?\\b'\n      r'(int|float|double|bool|char|int32|uint32|int64|uint64)'\n      r'(\\([^)].*)', line)\n  expecting_function = ExpectingFunctionArgs(clean_lines, linenum)\n  if match and not expecting_function:\n    matched_type = match.group(2)\n\n    # matched_new_or_template is used to silence two false positives:\n    # - New operators\n    # - Template arguments with function types\n    #\n    # For template arguments, we match on types immediately following\n    # an opening bracket without any spaces.  This is a fast way to\n    # silence the common case where the function type is the first\n    # template argument.  False negative with less-than comparison is\n    # avoided because those operators are usually followed by a space.\n    #\n    #   function<double(double)>   // bracket + no space = false positive\n    #   value < double(42)         // bracket + space = true positive\n    matched_new_or_template = match.group(1)\n\n    # Avoid arrays by looking for brackets that come after the closing\n    # parenthesis.\n    if Match(r'\\([^()]+\\)\\s*\\[', match.group(3)):\n      return\n\n    # Other things to ignore:\n    # - Function pointers\n    # - Casts to pointer types\n    # - Placement new\n    # - Alias declarations\n    matched_funcptr = match.group(3)\n    if (matched_new_or_template is None and\n        not (matched_funcptr and\n             (Match(r'\\((?:[^() ]+::\\s*\\*\\s*)?[^() ]+\\)\\s*\\(',\n                    matched_funcptr) or\n              matched_funcptr.startswith('(*)'))) and\n        not Match(r'\\s*using\\s+\\S+\\s*=\\s*' + matched_type, line) and\n        not Search(r'new\\(\\S+\\)\\s*' + matched_type, line)):\n      error(filename, linenum, 'readability/casting', 4,\n            'Using deprecated casting style.  '\n            'Use static_cast<%s>(...) instead' %\n            matched_type)\n\n  if not expecting_function:\n    CheckCStyleCast(filename, clean_lines, linenum, 'static_cast',\n                    r'\\((int|float|double|bool|char|u?int(16|32|64)|size_t)\\)', error)\n\n  # This doesn't catch all cases. Consider (const char * const)\"hello\".\n  #\n  # (char *) \"foo\" should always be a const_cast (reinterpret_cast won't\n  # compile).\n  if CheckCStyleCast(filename, clean_lines, linenum, 'const_cast',\n                     r'\\((char\\s?\\*+\\s?)\\)\\s*\"', error):\n    pass\n  else:\n    # Check pointer casts for other than string constants\n    CheckCStyleCast(filename, clean_lines, linenum, 'reinterpret_cast',\n                    r'\\((\\w+\\s?\\*+\\s?)\\)', error)\n\n  # In addition, we look for people taking the address of a cast.  This\n  # is dangerous -- casts can assign to temporaries, so the pointer doesn't\n  # point where you think.\n  #\n  # Some non-identifier character is required before the '&' for the\n  # expression to be recognized as a cast.  These are casts:\n  #   expression = &static_cast<int*>(temporary());\n  #   function(&(int*)(temporary()));\n  #\n  # This is not a cast:\n  #   reference_type&(int* function_param);\n  match = Search(\n      r'(?:[^\\w]&\\(([^)*][^)]*)\\)[\\w(])|'\n      r'(?:[^\\w]&(static|dynamic|down|reinterpret)_cast\\b)', line)\n  if match:\n    # Try a better error message when the & is bound to something\n    # dereferenced by the casted pointer, as opposed to the casted\n    # pointer itself.\n    parenthesis_error = False\n    match = Match(r'^(.*&(?:static|dynamic|down|reinterpret)_cast\\b)<', line)\n    if match:\n      _, y1, x1 = CloseExpression(clean_lines, linenum, len(match.group(1)))\n      if x1 >= 0 and clean_lines.elided[y1][x1] == '(':\n        _, y2, x2 = CloseExpression(clean_lines, y1, x1)\n        if x2 >= 0:\n          extended_line = clean_lines.elided[y2][x2:]\n          if y2 < clean_lines.NumLines() - 1:\n            extended_line += clean_lines.elided[y2 + 1]\n          if Match(r'\\s*(?:->|\\[)', extended_line):\n            parenthesis_error = True\n\n    if parenthesis_error:\n      error(filename, linenum, 'readability/casting', 4,\n            ('Are you taking an address of something dereferenced '\n             'from a cast?  Wrapping the dereferenced expression in '\n             'parentheses will make the binding more obvious'))\n    else:\n      error(filename, linenum, 'runtime/casting', 4,\n            ('Are you taking an address of a cast?  '\n             'This is dangerous: could be a temp var.  '\n             'Take the address before doing the cast, rather than after'))\n\n\ndef CheckCStyleCast(filename, clean_lines, linenum, cast_type, pattern, error):\n  \"\"\"Checks for a C-style cast by looking for the pattern.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    cast_type: The string for the C++ cast to recommend.  This is either\n      reinterpret_cast, static_cast, or const_cast, depending.\n    pattern: The regular expression used to find C-style casts.\n    error: The function to call with any errors found.\n\n  Returns:\n    True if an error was emitted.\n    False otherwise.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  match = Search(pattern, line)\n  if not match:\n    return False\n\n  # Exclude lines with keywords that tend to look like casts\n  context = line[0:match.start(1) - 1]\n  if Match(r'.*\\b(?:sizeof|alignof|alignas|[_A-Z][_A-Z0-9]*)\\s*$', context):\n    return False\n\n  # Try expanding current context to see if we one level of\n  # parentheses inside a macro.\n  if linenum > 0:\n    for i in xrange(linenum - 1, max(0, linenum - 5), -1):\n      context = clean_lines.elided[i] + context\n  if Match(r'.*\\b[_A-Z][_A-Z0-9]*\\s*\\((?:\\([^()]*\\)|[^()])*$', context):\n    return False\n\n  # operator++(int) and operator--(int)\n  if (context.endswith(' operator++') or context.endswith(' operator--') or\n      context.endswith('::operator++') or context.endswith('::operator--')):\n    return False\n\n  # A single unnamed argument for a function tends to look like old style cast.\n  # If we see those, don't issue warnings for deprecated casts.\n  remainder = line[match.end(0):]\n  if Match(r'^\\s*(?:;|const\\b|throw\\b|final\\b|override\\b|[=>{),]|->)',\n           remainder):\n    return False\n\n  # At this point, all that should be left is actual casts.\n  error(filename, linenum, 'readability/casting', 4,\n        'Using C-style cast.  Use %s<%s>(...) instead' %\n        (cast_type, match.group(1)))\n\n  return True\n\n\ndef ExpectingFunctionArgs(clean_lines, linenum):\n  \"\"\"Checks whether where function type arguments are expected.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n\n  Returns:\n    True if the line at 'linenum' is inside something that expects arguments\n    of function types.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  return (Match(r'^\\s*MOCK_(CONST_)?METHOD\\d+(_T)?\\(', line) or\n          (linenum >= 2 and\n           (Match(r'^\\s*MOCK_(?:CONST_)?METHOD\\d+(?:_T)?\\((?:\\S+,)?\\s*$',\n                  clean_lines.elided[linenum - 1]) or\n            Match(r'^\\s*MOCK_(?:CONST_)?METHOD\\d+(?:_T)?\\(\\s*$',\n                  clean_lines.elided[linenum - 2]) or\n            Search(r'\\bstd::m?function\\s*\\<\\s*$',\n                   clean_lines.elided[linenum - 1]))))\n\n\n_HEADERS_CONTAINING_TEMPLATES = (\n    ('<deque>', ('deque',)),\n    ('<functional>', ('unary_function', 'binary_function',\n                      'plus', 'minus', 'multiplies', 'divides', 'modulus',\n                      'negate',\n                      'equal_to', 'not_equal_to', 'greater', 'less',\n                      'greater_equal', 'less_equal',\n                      'logical_and', 'logical_or', 'logical_not',\n                      'unary_negate', 'not1', 'binary_negate', 'not2',\n                      'bind1st', 'bind2nd',\n                      'pointer_to_unary_function',\n                      'pointer_to_binary_function',\n                      'ptr_fun',\n                      'mem_fun_t', 'mem_fun', 'mem_fun1_t', 'mem_fun1_ref_t',\n                      'mem_fun_ref_t',\n                      'const_mem_fun_t', 'const_mem_fun1_t',\n                      'const_mem_fun_ref_t', 'const_mem_fun1_ref_t',\n                      'mem_fun_ref',\n                     )),\n    ('<limits>', ('numeric_limits',)),\n    ('<list>', ('list',)),\n    ('<map>', ('multimap',)),\n    ('<memory>', ('allocator', 'make_shared', 'make_unique', 'shared_ptr',\n                  'unique_ptr', 'weak_ptr')),\n    ('<queue>', ('queue', 'priority_queue',)),\n    ('<set>', ('multiset',)),\n    ('<stack>', ('stack',)),\n    ('<string>', ('char_traits', 'basic_string',)),\n    ('<tuple>', ('tuple',)),\n    ('<unordered_map>', ('unordered_map', 'unordered_multimap')),\n    ('<unordered_set>', ('unordered_set', 'unordered_multiset')),\n    ('<utility>', ('pair',)),\n    ('<vector>', ('vector',)),\n\n    # gcc extensions.\n    # Note: std::hash is their hash, ::hash is our hash\n    ('<hash_map>', ('hash_map', 'hash_multimap',)),\n    ('<hash_set>', ('hash_set', 'hash_multiset',)),\n    ('<slist>', ('slist',)),\n    )\n\n_HEADERS_MAYBE_TEMPLATES = (\n    ('<algorithm>', ('copy', 'max', 'min', 'min_element', 'sort',\n                     'transform',\n                    )),\n    ('<utility>', ('forward', 'make_pair', 'move', 'swap')),\n    )\n\n_RE_PATTERN_STRING = re.compile(r'\\bstring\\b')\n\n_re_pattern_headers_maybe_templates = []\nfor _header, _templates in _HEADERS_MAYBE_TEMPLATES:\n  for _template in _templates:\n    # Match max<type>(..., ...), max(..., ...), but not foo->max, foo.max or\n    # 'type::max()'.\n    _re_pattern_headers_maybe_templates.append(\n        (re.compile(r'[^>.]\\b' + _template + r'(<.*?>)?\\([^\\)]'),\n            _template,\n            _header))\n# Match set<type>, but not foo->set<type>, foo.set<type>\n_re_pattern_headers_maybe_templates.append(\n    (re.compile(r'[^>.]\\bset\\s*\\<'),\n        'set<>',\n        '<set>'))\n# Match 'map<type> var' and 'std::map<type>(...)', but not 'map<type>(...)''\n_re_pattern_headers_maybe_templates.append(\n    (re.compile(r'(std\\b::\\bmap\\s*\\<)|(^(std\\b::\\b)map\\b\\(\\s*\\<)'),\n        'map<>',\n        '<map>'))\n\n# Other scripts may reach in and modify this pattern.\n_re_pattern_templates = []\nfor _header, _templates in _HEADERS_CONTAINING_TEMPLATES:\n  for _template in _templates:\n    _re_pattern_templates.append(\n        (re.compile(r'(\\<|\\b)' + _template + r'\\s*\\<'),\n         _template + '<>',\n         _header))\n\n\ndef FilesBelongToSameModule(filename_cc, filename_h):\n  \"\"\"Check if these two filenames belong to the same module.\n\n  The concept of a 'module' here is a as follows:\n  foo.h, foo-inl.h, foo.cc, foo_test.cc and foo_unittest.cc belong to the\n  same 'module' if they are in the same directory.\n  some/path/public/xyzzy and some/path/internal/xyzzy are also considered\n  to belong to the same module here.\n\n  If the filename_cc contains a longer path than the filename_h, for example,\n  '/absolute/path/to/base/sysinfo.cc', and this file would include\n  'base/sysinfo.h', this function also produces the prefix needed to open the\n  header. This is used by the caller of this function to more robustly open the\n  header file. We don't have access to the real include paths in this context,\n  so we need this guesswork here.\n\n  Known bugs: tools/base/bar.cc and base/bar.h belong to the same module\n  according to this implementation. Because of this, this function gives\n  some false positives. This should be sufficiently rare in practice.\n\n  Args:\n    filename_cc: is the path for the source (e.g. .cc) file\n    filename_h: is the path for the header path\n\n  Returns:\n    Tuple with a bool and a string:\n    bool: True if filename_cc and filename_h belong to the same module.\n    string: the additional prefix needed to open the header file.\n  \"\"\"\n  fileinfo_cc = FileInfo(filename_cc)\n  if not fileinfo_cc.Extension().lstrip('.') in GetNonHeaderExtensions():\n    return (False, '')\n\n  fileinfo_h = FileInfo(filename_h)\n  if not IsHeaderExtension(fileinfo_h.Extension().lstrip('.')):\n    return (False, '')\n\n  filename_cc = filename_cc[:-(len(fileinfo_cc.Extension()))]\n  matched_test_suffix = Search(_TEST_FILE_SUFFIX, fileinfo_cc.BaseName())\n  if matched_test_suffix:\n    filename_cc = filename_cc[:-len(matched_test_suffix.group(1))]\n\n  filename_cc = filename_cc.replace('/public/', '/')\n  filename_cc = filename_cc.replace('/internal/', '/')\n\n  filename_h = filename_h[:-(len(fileinfo_h.Extension()))]\n  if filename_h.endswith('-inl'):\n    filename_h = filename_h[:-len('-inl')]\n  filename_h = filename_h.replace('/public/', '/')\n  filename_h = filename_h.replace('/internal/', '/')\n\n  files_belong_to_same_module = filename_cc.endswith(filename_h)\n  common_path = ''\n  if files_belong_to_same_module:\n    common_path = filename_cc[:-len(filename_h)]\n  return files_belong_to_same_module, common_path\n\n\ndef UpdateIncludeState(filename, include_dict, io=codecs):\n  \"\"\"Fill up the include_dict with new includes found from the file.\n\n  Args:\n    filename: the name of the header to read.\n    include_dict: a dictionary in which the headers are inserted.\n    io: The io factory to use to read the file. Provided for testability.\n\n  Returns:\n    True if a header was successfully added. False otherwise.\n  \"\"\"\n  headerfile = None\n  try:\n    with io.open(filename, 'r', 'utf8', 'replace') as headerfile:\n      linenum = 0\n      for line in headerfile:\n        linenum += 1\n        clean_line = CleanseComments(line)\n        match = _RE_PATTERN_INCLUDE.search(clean_line)\n        if match:\n          include = match.group(2)\n          include_dict.setdefault(include, linenum)\n    return True\n  except IOError:\n    return False\n\n\n\ndef CheckForIncludeWhatYouUse(filename, clean_lines, include_state, error,\n                              io=codecs):\n  \"\"\"Reports for missing stl includes.\n\n  This function will output warnings to make sure you are including the headers\n  necessary for the stl containers and functions that you use. We only give one\n  reason to include a header. For example, if you use both equal_to<> and\n  less<> in a .h file, only one (the latter in the file) of these will be\n  reported as a reason to include the <functional>.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    include_state: An _IncludeState instance.\n    error: The function to call with any errors found.\n    io: The IO factory to use to read the header file. Provided for unittest\n        injection.\n  \"\"\"\n  required = {}  # A map of header name to linenumber and the template entity.\n                 # Example of required: { '<functional>': (1219, 'less<>') }\n\n  for linenum in xrange(clean_lines.NumLines()):\n    line = clean_lines.elided[linenum]\n    if not line or line[0] == '#':\n      continue\n\n    # String is special -- it is a non-templatized type in STL.\n    matched = _RE_PATTERN_STRING.search(line)\n    if matched:\n      # Don't warn about strings in non-STL namespaces:\n      # (We check only the first match per line; good enough.)\n      prefix = line[:matched.start()]\n      if prefix.endswith('std::') or not prefix.endswith('::'):\n        required['<string>'] = (linenum, 'string')\n\n    for pattern, template, header in _re_pattern_headers_maybe_templates:\n      if pattern.search(line):\n        required[header] = (linenum, template)\n\n    # The following function is just a speed up, no semantics are changed.\n    if not '<' in line:  # Reduces the cpu time usage by skipping lines.\n      continue\n\n    for pattern, template, header in _re_pattern_templates:\n      matched = pattern.search(line)\n      if matched:\n        # Don't warn about IWYU in non-STL namespaces:\n        # (We check only the first match per line; good enough.)\n        prefix = line[:matched.start()]\n        if prefix.endswith('std::') or not prefix.endswith('::'):\n          required[header] = (linenum, template)\n\n  # The policy is that if you #include something in foo.h you don't need to\n  # include it again in foo.cc. Here, we will look at possible includes.\n  # Let's flatten the include_state include_list and copy it into a dictionary.\n  include_dict = dict([item for sublist in include_state.include_list\n                       for item in sublist])\n\n  # Did we find the header for this file (if any) and successfully load it?\n  header_found = False\n\n  # Use the absolute path so that matching works properly.\n  abs_filename = FileInfo(filename).FullName()\n\n  # For Emacs's flymake.\n  # If cpplint is invoked from Emacs's flymake, a temporary file is generated\n  # by flymake and that file name might end with '_flymake.cc'. In that case,\n  # restore original file name here so that the corresponding header file can be\n  # found.\n  # e.g. If the file name is 'foo_flymake.cc', we should search for 'foo.h'\n  # instead of 'foo_flymake.h'\n  abs_filename = re.sub(r'_flymake\\.cc$', '.cc', abs_filename)\n\n  # include_dict is modified during iteration, so we iterate over a copy of\n  # the keys.\n  header_keys = list(include_dict.keys())\n  for header in header_keys:\n    (same_module, common_path) = FilesBelongToSameModule(abs_filename, header)\n    fullpath = common_path + header\n    if same_module and UpdateIncludeState(fullpath, include_dict, io):\n      header_found = True\n\n  # If we can't find the header file for a .cc, assume it's because we don't\n  # know where to look. In that case we'll give up as we're not sure they\n  # didn't include it in the .h file.\n  # TODO(unknown): Do a better job of finding .h files so we are confident that\n  # not having the .h file means there isn't one.\n  if not header_found:\n    for extension in GetNonHeaderExtensions():\n      if filename.endswith('.' + extension):\n        return\n\n  # All the lines have been processed, report the errors found.\n  for required_header_unstripped in sorted(required, key=required.__getitem__):\n    template = required[required_header_unstripped][1]\n    if required_header_unstripped.strip('<>\"') not in include_dict:\n      error(filename, required[required_header_unstripped][0],\n            'build/include_what_you_use', 4,\n            'Add #include ' + required_header_unstripped + ' for ' + template)\n\n\n_RE_PATTERN_EXPLICIT_MAKEPAIR = re.compile(r'\\bmake_pair\\s*<')\n\n\ndef CheckMakePairUsesDeduction(filename, clean_lines, linenum, error):\n  \"\"\"Check that make_pair's template arguments are deduced.\n\n  G++ 4.6 in C++11 mode fails badly if make_pair's template arguments are\n  specified explicitly, and such use isn't intended in any case.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  match = _RE_PATTERN_EXPLICIT_MAKEPAIR.search(line)\n  if match:\n    error(filename, linenum, 'build/explicit_make_pair',\n          4,  # 4 = high confidence\n          'For C++11-compatibility, omit template arguments from make_pair'\n          ' OR use pair directly OR if appropriate, construct a pair directly')\n\n\ndef CheckRedundantVirtual(filename, clean_lines, linenum, error):\n  \"\"\"Check if line contains a redundant \"virtual\" function-specifier.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  # Look for \"virtual\" on current line.\n  line = clean_lines.elided[linenum]\n  virtual = Match(r'^(.*)(\\bvirtual\\b)(.*)$', line)\n  if not virtual: return\n\n  # Ignore \"virtual\" keywords that are near access-specifiers.  These\n  # are only used in class base-specifier and do not apply to member\n  # functions.\n  if (Search(r'\\b(public|protected|private)\\s+$', virtual.group(1)) or\n      Match(r'^\\s+(public|protected|private)\\b', virtual.group(3))):\n    return\n\n  # Ignore the \"virtual\" keyword from virtual base classes.  Usually\n  # there is a column on the same line in these cases (virtual base\n  # classes are rare in google3 because multiple inheritance is rare).\n  if Match(r'^.*[^:]:[^:].*$', line): return\n\n  # Look for the next opening parenthesis.  This is the start of the\n  # parameter list (possibly on the next line shortly after virtual).\n  # TODO(unknown): doesn't work if there are virtual functions with\n  # decltype() or other things that use parentheses, but csearch suggests\n  # that this is rare.\n  end_col = -1\n  end_line = -1\n  start_col = len(virtual.group(2))\n  for start_line in xrange(linenum, min(linenum + 3, clean_lines.NumLines())):\n    line = clean_lines.elided[start_line][start_col:]\n    parameter_list = Match(r'^([^(]*)\\(', line)\n    if parameter_list:\n      # Match parentheses to find the end of the parameter list\n      (_, end_line, end_col) = CloseExpression(\n          clean_lines, start_line, start_col + len(parameter_list.group(1)))\n      break\n    start_col = 0\n\n  if end_col < 0:\n    return  # Couldn't find end of parameter list, give up\n\n  # Look for \"override\" or \"final\" after the parameter list\n  # (possibly on the next few lines).\n  for i in xrange(end_line, min(end_line + 3, clean_lines.NumLines())):\n    line = clean_lines.elided[i][end_col:]\n    match = Search(r'\\b(override|final)\\b', line)\n    if match:\n      error(filename, linenum, 'readability/inheritance', 4,\n            ('\"virtual\" is redundant since function is '\n             'already declared as \"%s\"' % match.group(1)))\n\n    # Set end_col to check whole lines after we are done with the\n    # first line.\n    end_col = 0\n    if Search(r'[^\\w]\\s*$', line):\n      break\n\n\ndef CheckRedundantOverrideOrFinal(filename, clean_lines, linenum, error):\n  \"\"\"Check if line contains a redundant \"override\" or \"final\" virt-specifier.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  # Look for closing parenthesis nearby.  We need one to confirm where\n  # the declarator ends and where the virt-specifier starts to avoid\n  # false positives.\n  line = clean_lines.elided[linenum]\n  declarator_end = line.rfind(')')\n  if declarator_end >= 0:\n    fragment = line[declarator_end:]\n  else:\n    if linenum > 1 and clean_lines.elided[linenum - 1].rfind(')') >= 0:\n      fragment = line\n    else:\n      return\n\n  # Check that at most one of \"override\" or \"final\" is present, not both\n  if Search(r'\\boverride\\b', fragment) and Search(r'\\bfinal\\b', fragment):\n    error(filename, linenum, 'readability/inheritance', 4,\n          ('\"override\" is redundant since function is '\n           'already declared as \"final\"'))\n\n\n\n\n# Returns true if we are at a new block, and it is directly\n# inside of a namespace.\ndef IsBlockInNameSpace(nesting_state, is_forward_declaration):\n  \"\"\"Checks that the new block is directly in a namespace.\n\n  Args:\n    nesting_state: The _NestingState object that contains info about our state.\n    is_forward_declaration: If the class is a forward declared class.\n  Returns:\n    Whether or not the new block is directly in a namespace.\n  \"\"\"\n  if is_forward_declaration:\n    return len(nesting_state.stack) >= 1 and (\n      isinstance(nesting_state.stack[-1], _NamespaceInfo))\n\n\n  return (len(nesting_state.stack) > 1 and\n          nesting_state.stack[-1].check_namespace_indentation and\n          isinstance(nesting_state.stack[-2], _NamespaceInfo))\n\n\ndef ShouldCheckNamespaceIndentation(nesting_state, is_namespace_indent_item,\n                                    raw_lines_no_comments, linenum):\n  \"\"\"This method determines if we should apply our namespace indentation check.\n\n  Args:\n    nesting_state: The current nesting state.\n    is_namespace_indent_item: If we just put a new class on the stack, True.\n      If the top of the stack is not a class, or we did not recently\n      add the class, False.\n    raw_lines_no_comments: The lines without the comments.\n    linenum: The current line number we are processing.\n\n  Returns:\n    True if we should apply our namespace indentation check. Currently, it\n    only works for classes and namespaces inside of a namespace.\n  \"\"\"\n\n  is_forward_declaration = IsForwardClassDeclaration(raw_lines_no_comments,\n                                                     linenum)\n\n  if not (is_namespace_indent_item or is_forward_declaration):\n    return False\n\n  # If we are in a macro, we do not want to check the namespace indentation.\n  if IsMacroDefinition(raw_lines_no_comments, linenum):\n    return False\n\n  return IsBlockInNameSpace(nesting_state, is_forward_declaration)\n\n\n# Call this method if the line is directly inside of a namespace.\n# If the line above is blank (excluding comments) or the start of\n# an inner namespace, it cannot be indented.\ndef CheckItemIndentationInNamespace(filename, raw_lines_no_comments, linenum,\n                                    error):\n  line = raw_lines_no_comments[linenum]\n  if Match(r'^\\s+', line):\n    error(filename, linenum, 'runtime/indentation_namespace', 4,\n          'Do not indent within a namespace')\n\n\ndef ProcessLine(filename, file_extension, clean_lines, line,\n                include_state, function_state, nesting_state, error,\n                extra_check_functions=None):\n  \"\"\"Processes a single line in the file.\n\n  Args:\n    filename: Filename of the file that is being processed.\n    file_extension: The extension (dot not included) of the file.\n    clean_lines: An array of strings, each representing a line of the file,\n                 with comments stripped.\n    line: Number of line being processed.\n    include_state: An _IncludeState instance in which the headers are inserted.\n    function_state: A _FunctionState instance which counts function lines, etc.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n    extra_check_functions: An array of additional check functions that will be\n                           run on each source line. Each function takes 4\n                           arguments: filename, clean_lines, line, error\n  \"\"\"\n  raw_lines = clean_lines.raw_lines\n  ParseNolintSuppressions(filename, raw_lines[line], line, error)\n  nesting_state.Update(filename, clean_lines, line, error)\n  CheckForNamespaceIndentation(filename, nesting_state, clean_lines, line,\n                               error)\n  if nesting_state.InAsmBlock(): return\n  CheckForFunctionLengths(filename, clean_lines, line, function_state, error)\n  CheckForMultilineCommentsAndStrings(filename, clean_lines, line, error)\n  CheckStyle(filename, clean_lines, line, file_extension, nesting_state, error)\n  CheckLanguage(filename, clean_lines, line, file_extension, include_state,\n                nesting_state, error)\n  CheckForNonConstReference(filename, clean_lines, line, nesting_state, error)\n  CheckForNonStandardConstructs(filename, clean_lines, line,\n                                nesting_state, error)\n  CheckVlogArguments(filename, clean_lines, line, error)\n  CheckPosixThreading(filename, clean_lines, line, error)\n  CheckInvalidIncrement(filename, clean_lines, line, error)\n  CheckMakePairUsesDeduction(filename, clean_lines, line, error)\n  CheckRedundantVirtual(filename, clean_lines, line, error)\n  CheckRedundantOverrideOrFinal(filename, clean_lines, line, error)\n  if extra_check_functions:\n    for check_fn in extra_check_functions:\n      check_fn(filename, clean_lines, line, error)\n\ndef FlagCxx11Features(filename, clean_lines, linenum, error):\n  \"\"\"Flag those c++11 features that we only allow in certain places.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  include = Match(r'\\s*#\\s*include\\s+[<\"]([^<\"]+)[\">]', line)\n\n  # Flag unapproved C++ TR1 headers.\n  if include and include.group(1).startswith('tr1/'):\n    error(filename, linenum, 'build/c++tr1', 5,\n          ('C++ TR1 headers such as <%s> are unapproved.') % include.group(1))\n\n  # Flag unapproved C++11 headers.\n  if include and include.group(1) in ('cfenv',\n                                      'condition_variable',\n                                      'fenv.h',\n                                      'future',\n                                      'mutex',\n                                      'thread',\n                                      'chrono',\n                                      'ratio',\n                                      'regex',\n                                      'system_error',\n                                     ):\n    error(filename, linenum, 'build/c++11', 5,\n          ('<%s> is an unapproved C++11 header.') % include.group(1))\n\n  # The only place where we need to worry about C++11 keywords and library\n  # features in preprocessor directives is in macro definitions.\n  if Match(r'\\s*#', line) and not Match(r'\\s*#\\s*define\\b', line): return\n\n  # These are classes and free functions.  The classes are always\n  # mentioned as std::*, but we only catch the free functions if\n  # they're not found by ADL.  They're alphabetical by header.\n  for top_name in (\n      # type_traits\n      'alignment_of',\n      'aligned_union',\n      ):\n    if Search(r'\\bstd::%s\\b' % top_name, line):\n      error(filename, linenum, 'build/c++11', 5,\n            ('std::%s is an unapproved C++11 class or function.  Send c-style '\n             'an example of where it would make your code more readable, and '\n             'they may let you use it.') % top_name)\n\n\ndef FlagCxx14Features(filename, clean_lines, linenum, error):\n  \"\"\"Flag those C++14 features that we restrict.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  include = Match(r'\\s*#\\s*include\\s+[<\"]([^<\"]+)[\">]', line)\n\n  # Flag unapproved C++14 headers.\n  if include and include.group(1) in ('scoped_allocator', 'shared_mutex'):\n    error(filename, linenum, 'build/c++14', 5,\n          ('<%s> is an unapproved C++14 header.') % include.group(1))\n\n\ndef ProcessFileData(filename, file_extension, lines, error,\n                    extra_check_functions=None):\n  \"\"\"Performs lint checks and reports any errors to the given error function.\n\n  Args:\n    filename: Filename of the file that is being processed.\n    file_extension: The extension (dot not included) of the file.\n    lines: An array of strings, each representing a line of the file, with the\n           last element being empty if the file is terminated with a newline.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n    extra_check_functions: An array of additional check functions that will be\n                           run on each source line. Each function takes 4\n                           arguments: filename, clean_lines, line, error\n  \"\"\"\n  lines = (['// marker so line numbers and indices both start at 1'] + lines +\n           ['// marker so line numbers end in a known way'])\n\n  include_state = _IncludeState()\n  function_state = _FunctionState()\n  nesting_state = NestingState()\n\n  ResetNolintSuppressions()\n\n  CheckForCopyright(filename, lines, error)\n  ProcessGlobalSuppressions(lines)\n  RemoveMultiLineComments(filename, lines, error)\n  clean_lines = CleansedLines(lines)\n\n  if IsHeaderExtension(file_extension):\n    CheckForHeaderGuard(filename, clean_lines, error)\n\n  for line in xrange(clean_lines.NumLines()):\n    ProcessLine(filename, file_extension, clean_lines, line,\n                include_state, function_state, nesting_state, error,\n                extra_check_functions)\n    FlagCxx11Features(filename, clean_lines, line, error)\n  nesting_state.CheckCompletedBlocks(filename, error)\n\n  CheckForIncludeWhatYouUse(filename, clean_lines, include_state, error)\n\n  # Check that the .cc file has included its header if it exists.\n  if _IsSourceExtension(file_extension):\n    CheckHeaderFileIncluded(filename, include_state, error)\n\n  # We check here rather than inside ProcessLine so that we see raw\n  # lines rather than \"cleaned\" lines.\n  CheckForBadCharacters(filename, lines, error)\n\n  CheckForNewlineAtEOF(filename, lines, error)\n\ndef ProcessConfigOverrides(filename):\n  \"\"\" Loads the configuration files and processes the config overrides.\n\n  Args:\n    filename: The name of the file being processed by the linter.\n\n  Returns:\n    False if the current |filename| should not be processed further.\n  \"\"\"\n\n  abs_filename = os.path.abspath(filename)\n  cfg_filters = []\n  keep_looking = True\n  while keep_looking:\n    abs_path, base_name = os.path.split(abs_filename)\n    if not base_name:\n      break  # Reached the root directory.\n\n    cfg_file = os.path.join(abs_path, \"CPPLINT.cfg\")\n    abs_filename = abs_path\n    if not os.path.isfile(cfg_file):\n      continue\n\n    try:\n      with codecs.open(cfg_file, 'r', 'utf8', 'replace') as file_handle:\n        for line in file_handle:\n          line, _, _ = line.partition('#')  # Remove comments.\n          if not line.strip():\n            continue\n\n          name, _, val = line.partition('=')\n          name = name.strip()\n          val = val.strip()\n          if name == 'set noparent':\n            keep_looking = False\n          elif name == 'filter':\n            cfg_filters.append(val)\n          elif name == 'exclude_files':\n            # When matching exclude_files pattern, use the base_name of\n            # the current file name or the directory name we are processing.\n            # For example, if we are checking for lint errors in /foo/bar/baz.cc\n            # and we found the .cfg file at /foo/CPPLINT.cfg, then the config\n            # file's \"exclude_files\" filter is meant to be checked against \"bar\"\n            # and not \"baz\" nor \"bar/baz.cc\".\n            if base_name:\n              pattern = re.compile(val)\n              if pattern.match(base_name):\n                if _cpplint_state.quiet:\n                  # Suppress \"Ignoring file\" warning when using --quiet.\n                  return False\n                _cpplint_state.PrintInfo('Ignoring \"%s\": file excluded by \"%s\". '\n                                 'File path component \"%s\" matches '\n                                 'pattern \"%s\"\\n' %\n                                 (filename, cfg_file, base_name, val))\n                return False\n          elif name == 'linelength':\n            global _line_length\n            try:\n              _line_length = int(val)\n            except ValueError:\n              _cpplint_state.PrintError('Line length must be numeric.')\n          elif name == 'extensions':\n            ProcessExtensionsOption(val)\n          elif name == 'root':\n            global _root\n            # root directories are specified relative to CPPLINT.cfg dir.\n            _root = os.path.join(os.path.dirname(cfg_file), val)\n          elif name == 'headers':\n            ProcessHppHeadersOption(val)\n          elif name == 'includeorder':\n            ProcessIncludeOrderOption(val)\n          else:\n            _cpplint_state.PrintError(\n                'Invalid configuration option (%s) in file %s\\n' %\n                (name, cfg_file))\n\n    except IOError:\n      _cpplint_state.PrintError(\n          \"Skipping config file '%s': Can't open for reading\\n\" % cfg_file)\n      keep_looking = False\n\n  # Apply all the accumulated filters in reverse order (top-level directory\n  # config options having the least priority).\n  for cfg_filter in reversed(cfg_filters):\n    _AddFilters(cfg_filter)\n\n  return True\n\n\ndef ProcessFile(filename, vlevel, extra_check_functions=None):\n  \"\"\"Does google-lint on a single file.\n\n  Args:\n    filename: The name of the file to parse.\n\n    vlevel: The level of errors to report.  Every error of confidence\n    >= verbose_level will be reported.  0 is a good default.\n\n    extra_check_functions: An array of additional check functions that will be\n                           run on each source line. Each function takes 4\n                           arguments: filename, clean_lines, line, error\n  \"\"\"\n\n  _SetVerboseLevel(vlevel)\n  _BackupFilters()\n  old_errors = _cpplint_state.error_count\n\n  if not ProcessConfigOverrides(filename):\n    _RestoreFilters()\n    return\n\n  lf_lines = []\n  crlf_lines = []\n  try:\n    # Support the UNIX convention of using \"-\" for stdin.  Note that\n    # we are not opening the file with universal newline support\n    # (which codecs doesn't support anyway), so the resulting lines do\n    # contain trailing '\\r' characters if we are reading a file that\n    # has CRLF endings.\n    # If after the split a trailing '\\r' is present, it is removed\n    # below.\n    if filename == '-':\n      lines = codecs.StreamReaderWriter(sys.stdin,\n                                        codecs.getreader('utf8'),\n                                        codecs.getwriter('utf8'),\n                                        'replace').read().split('\\n')\n    else:\n      with codecs.open(filename, 'r', 'utf8', 'replace') as target_file:\n        lines = target_file.read().split('\\n')\n\n    # Remove trailing '\\r'.\n    # The -1 accounts for the extra trailing blank line we get from split()\n    for linenum in range(len(lines) - 1):\n      if lines[linenum].endswith('\\r'):\n        lines[linenum] = lines[linenum].rstrip('\\r')\n        crlf_lines.append(linenum + 1)\n      else:\n        lf_lines.append(linenum + 1)\n\n  except IOError:\n    _cpplint_state.PrintError(\n        \"Skipping input '%s': Can't open for reading\\n\" % filename)\n    _RestoreFilters()\n    return\n\n  # Note, if no dot is found, this will give the entire filename as the ext.\n  file_extension = filename[filename.rfind('.') + 1:]\n\n  # When reading from stdin, the extension is unknown, so no cpplint tests\n  # should rely on the extension.\n  if filename != '-' and file_extension not in GetAllExtensions():\n    _cpplint_state.PrintError('Ignoring %s; not a valid file name '\n                     '(%s)\\n' % (filename, ', '.join(GetAllExtensions())))\n  else:\n    ProcessFileData(filename, file_extension, lines, Error,\n                    extra_check_functions)\n\n    # If end-of-line sequences are a mix of LF and CR-LF, issue\n    # warnings on the lines with CR.\n    #\n    # Don't issue any warnings if all lines are uniformly LF or CR-LF,\n    # since critique can handle these just fine, and the style guide\n    # doesn't dictate a particular end of line sequence.\n    #\n    # We can't depend on os.linesep to determine what the desired\n    # end-of-line sequence should be, since that will return the\n    # server-side end-of-line sequence.\n    if lf_lines and crlf_lines:\n      # Warn on every line with CR.  An alternative approach might be to\n      # check whether the file is mostly CRLF or just LF, and warn on the\n      # minority, we bias toward LF here since most tools prefer LF.\n      for linenum in crlf_lines:\n        Error(filename, linenum, 'whitespace/newline', 1,\n              'Unexpected \\\\r (^M) found; better to use only \\\\n')\n\n  # Suppress printing anything if --quiet was passed unless the error\n  # count has increased after processing this file.\n  if not _cpplint_state.quiet or old_errors != _cpplint_state.error_count:\n    _cpplint_state.PrintInfo('Done processing %s\\n' % filename)\n  _RestoreFilters()\n\n\ndef PrintUsage(message):\n  \"\"\"Prints a brief usage string and exits, optionally with an error message.\n\n  Args:\n    message: The optional error message.\n  \"\"\"\n  sys.stderr.write(_USAGE  % (sorted(list(GetAllExtensions())),\n       ','.join(sorted(list(GetAllExtensions()))),\n       sorted(GetHeaderExtensions()),\n       ','.join(sorted(GetHeaderExtensions()))))\n\n  if message:\n    sys.exit('\\nFATAL ERROR: ' + message)\n  else:\n    sys.exit(0)\n\ndef PrintVersion():\n  sys.stdout.write('Cpplint fork (https://github.com/cpplint/cpplint)\\n')\n  sys.stdout.write('cpplint ' + __VERSION__ + '\\n')\n  sys.stdout.write('Python ' + sys.version + '\\n')\n  sys.exit(0)\n\ndef PrintCategories():\n  \"\"\"Prints a list of all the error-categories used by error messages.\n\n  These are the categories used to filter messages via --filter.\n  \"\"\"\n  sys.stderr.write(''.join('  %s\\n' % cat for cat in _ERROR_CATEGORIES))\n  sys.exit(0)\n\n\ndef ParseArguments(args):\n  \"\"\"Parses the command line arguments.\n\n  This may set the output format and verbosity level as side-effects.\n\n  Args:\n    args: The command line arguments:\n\n  Returns:\n    The list of filenames to lint.\n  \"\"\"\n  try:\n    (opts, filenames) = getopt.getopt(args, '', ['help', 'output=', 'verbose=',\n                                                 'v=',\n                                                 'version',\n                                                 'counting=',\n                                                 'filter=',\n                                                 'root=',\n                                                 'repository=',\n                                                 'linelength=',\n                                                 'extensions=',\n                                                 'exclude=',\n                                                 'recursive',\n                                                 'headers=',\n                                                 'includeorder=',\n                                                 'quiet'])\n  except getopt.GetoptError:\n    PrintUsage('Invalid arguments.')\n\n  verbosity = _VerboseLevel()\n  output_format = _OutputFormat()\n  filters = ''\n  quiet = _Quiet()\n  counting_style = ''\n  recursive = False\n\n  for (opt, val) in opts:\n    if opt == '--help':\n      PrintUsage(None)\n    if opt == '--version':\n      PrintVersion()\n    elif opt == '--output':\n      if val not in ('emacs', 'vs7', 'eclipse', 'junit', 'sed', 'gsed'):\n        PrintUsage('The only allowed output formats are emacs, vs7, eclipse '\n                   'sed, gsed and junit.')\n      output_format = val\n    elif opt == '--quiet':\n      quiet = True\n    elif opt == '--verbose' or opt == '--v':\n      verbosity = int(val)\n    elif opt == '--filter':\n      filters = val\n      if not filters:\n        PrintCategories()\n    elif opt == '--counting':\n      if val not in ('total', 'toplevel', 'detailed'):\n        PrintUsage('Valid counting options are total, toplevel, and detailed')\n      counting_style = val\n    elif opt == '--root':\n      global _root\n      _root = val\n    elif opt == '--repository':\n      global _repository\n      _repository = val\n    elif opt == '--linelength':\n      global _line_length\n      try:\n        _line_length = int(val)\n      except ValueError:\n        PrintUsage('Line length must be digits.')\n    elif opt == '--exclude':\n      global _excludes\n      if not _excludes:\n        _excludes = set()\n      _excludes.update(glob.glob(val))\n    elif opt == '--extensions':\n      ProcessExtensionsOption(val)\n    elif opt == '--headers':\n      ProcessHppHeadersOption(val)\n    elif opt == '--recursive':\n      recursive = True\n    elif opt == '--includeorder':\n      ProcessIncludeOrderOption(val)\n\n  if not filenames:\n    PrintUsage('No files were specified.')\n\n  if recursive:\n    filenames = _ExpandDirectories(filenames)\n\n  if _excludes:\n    filenames = _FilterExcludedFiles(filenames)\n\n  _SetOutputFormat(output_format)\n  _SetQuiet(quiet)\n  _SetVerboseLevel(verbosity)\n  _SetFilters(filters)\n  _SetCountingStyle(counting_style)\n\n  filenames.sort()\n  return filenames\n\ndef _ExpandDirectories(filenames):\n  \"\"\"Searches a list of filenames and replaces directories in the list with\n  all files descending from those directories. Files with extensions not in\n  the valid extensions list are excluded.\n\n  Args:\n    filenames: A list of files or directories\n\n  Returns:\n    A list of all files that are members of filenames or descended from a\n    directory in filenames\n  \"\"\"\n  expanded = set()\n  for filename in filenames:\n    if not os.path.isdir(filename):\n      expanded.add(filename)\n      continue\n\n    for root, _, files in os.walk(filename):\n      for loopfile in files:\n        fullname = os.path.join(root, loopfile)\n        if fullname.startswith('.' + os.path.sep):\n          fullname = fullname[len('.' + os.path.sep):]\n        expanded.add(fullname)\n\n  filtered = []\n  for filename in expanded:\n    if os.path.splitext(filename)[1][1:] in GetAllExtensions():\n      filtered.append(filename)\n  return filtered\n\ndef _FilterExcludedFiles(fnames):\n  \"\"\"Filters out files listed in the --exclude command line switch. File paths\n  in the switch are evaluated relative to the current working directory\n  \"\"\"\n  exclude_paths = [os.path.abspath(f) for f in _excludes]\n  # because globbing does not work recursively, exclude all subpath of all excluded entries\n  return [f for f in fnames\n          if not any(e for e in exclude_paths\n                  if _IsParentOrSame(e, os.path.abspath(f)))]\n\ndef _IsParentOrSame(parent, child):\n  \"\"\"Return true if child is subdirectory of parent.\n  Assumes both paths are absolute and don't contain symlinks.\n  \"\"\"\n  parent = os.path.normpath(parent)\n  child = os.path.normpath(child)\n  if parent == child:\n    return True\n\n  prefix = os.path.commonprefix([parent, child])\n  if prefix != parent:\n    return False\n  # Note: os.path.commonprefix operates on character basis, so\n  # take extra care of situations like '/foo/ba' and '/foo/bar/baz'\n  child_suffix = child[len(prefix):]\n  child_suffix = child_suffix.lstrip(os.sep)\n  return child == os.path.join(prefix, child_suffix)\n\ndef main():\n  filenames = ParseArguments(sys.argv[1:])\n  backup_err = sys.stderr\n  try:\n    # Change stderr to write with replacement characters so we don't die\n    # if we try to print something containing non-ASCII characters.\n    sys.stderr = codecs.StreamReader(sys.stderr, 'replace')\n\n    _cpplint_state.ResetErrorCounts()\n    for filename in filenames:\n      ProcessFile(filename, _cpplint_state.verbose_level)\n    # If --quiet is passed, suppress printing error count unless there are errors.\n    if not _cpplint_state.quiet or _cpplint_state.error_count > 0:\n      _cpplint_state.PrintErrorCounts()\n\n    if _cpplint_state.output_format == 'junit':\n      sys.stderr.write(_cpplint_state.FormatJUnitXML())\n\n  finally:\n    sys.stderr = backup_err\n\n  sys.exit(_cpplint_state.error_count > 0)\n\n\nif __name__ == '__main__':\n  main()\n", "cpp/build-support/lintutils.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport multiprocessing as mp\nimport os\nfrom fnmatch import fnmatch\nfrom subprocess import Popen\n\n\ndef chunk(seq, n):\n    \"\"\"\n    divide a sequence into equal sized chunks\n    (the last chunk may be smaller, but won't be empty)\n    \"\"\"\n    chunks = []\n    some = []\n    for element in seq:\n        if len(some) == n:\n            chunks.append(some)\n            some = []\n        some.append(element)\n    if len(some) > 0:\n        chunks.append(some)\n    return chunks\n\n\ndef dechunk(chunks):\n    \"flatten chunks into a single list\"\n    seq = []\n    for chunk in chunks:\n        seq.extend(chunk)\n    return seq\n\n\ndef run_parallel(cmds, **kwargs):\n    \"\"\"\n    Run each of cmds (with shared **kwargs) using subprocess.Popen\n    then wait for all of them to complete.\n    Runs batches of multiprocessing.cpu_count() * 2 from cmds\n    returns a list of tuples containing each process'\n    returncode, stdout, stderr\n    \"\"\"\n    complete = []\n    for cmds_batch in chunk(cmds, mp.cpu_count() * 2):\n        procs_batch = [Popen(cmd, **kwargs) for cmd in cmds_batch]\n        for proc in procs_batch:\n            stdout, stderr = proc.communicate()\n            complete.append((proc.returncode, stdout, stderr))\n    return complete\n\n\n_source_extensions = '''\n.h\n.cc\n.cpp\n'''.split()\n\n\ndef get_sources(source_dir, exclude_globs=[]):\n    sources = []\n    for directory, subdirs, basenames in os.walk(source_dir):\n        for path in [os.path.join(directory, basename)\n                     for basename in basenames]:\n            # filter out non-source files\n            if os.path.splitext(path)[1] not in _source_extensions:\n                continue\n\n            path = os.path.abspath(path)\n\n            # filter out files that match the globs in the globs file\n            if any([fnmatch(path, glob) for glob in exclude_globs]):\n                continue\n\n            sources.append(path)\n    return sources\n\n\ndef stdout_pathcolonline(completed_process, filenames):\n    \"\"\"\n    given a completed process which may have reported some files as problematic\n    by printing the path name followed by ':' then a line number, examine\n    stdout and return the set of actually reported file names\n    \"\"\"\n    returncode, stdout, stderr = completed_process\n    bfilenames = set()\n    for filename in filenames:\n        bfilenames.add(filename.encode('utf-8') + b':')\n    problem_files = set()\n    for line in stdout.splitlines():\n        for filename in bfilenames:\n            if line.startswith(filename):\n                problem_files.add(filename.decode('utf-8'))\n                bfilenames.remove(filename)\n                break\n    return problem_files, stdout\n", "cpp/build-support/lint_cpp_cli.py": "#!/usr/bin/env python3\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport argparse\nimport re\nimport os\n\nparser = argparse.ArgumentParser(\n    description=\"Check for illegal headers for C++/CLI applications\")\nparser.add_argument(\"source_path\",\n                    help=\"Path to source code\")\narguments = parser.parse_args()\n\n\n_STRIP_COMMENT_REGEX = re.compile('(.+)?(?=//)')\n_NULLPTR_REGEX = re.compile(r'.*\\bnullptr\\b.*')\n_RETURN_NOT_OK_REGEX = re.compile(r'.*\\sRETURN_NOT_OK.*')\n_ASSIGN_OR_RAISE_REGEX = re.compile(r'.*\\sASSIGN_OR_RAISE.*')\n\n\ndef _paths(paths):\n    return [p.strip().replace('/', os.path.sep) for p in paths.splitlines()]\n\n\ndef _strip_comments(line):\n    m = _STRIP_COMMENT_REGEX.match(line)\n    if not m:\n        return line\n    else:\n        return m.group(0)\n\n\ndef lint_file(path):\n    fail_rules = [\n        # rule, error message, rule-specific exclusions list\n        (lambda x: '<mutex>' in x, 'Uses <mutex>', []),\n        (lambda x: '<iostream>' in x, 'Uses <iostream>', []),\n        (lambda x: re.match(_NULLPTR_REGEX, x), 'Uses nullptr', []),\n        (lambda x: re.match(_RETURN_NOT_OK_REGEX, x),\n         'Use ARROW_RETURN_NOT_OK in header files', _paths('''\\\n         arrow/status.h\n         test\n         arrow/util/hash.h\n         arrow/python/util''')),\n        (lambda x: re.match(_ASSIGN_OR_RAISE_REGEX, x),\n         'Use ARROW_ASSIGN_OR_RAISE in header files', _paths('''\\\n         arrow/result_internal.h\n         test\n         '''))\n\n    ]\n\n    with open(path) as f:\n        for i, line in enumerate(f):\n            stripped_line = _strip_comments(line)\n            for rule, why, rule_exclusions in fail_rules:\n                if any([True for excl in rule_exclusions if excl in path]):\n                    continue\n\n                if rule(stripped_line):\n                    yield path, why, i, line\n\n\nEXCLUSIONS = _paths('''\\\n    arrow/arrow-config.cmake\n    arrow/python/iterators.h\n    arrow/util/hashing.h\n    arrow/util/macros.h\n    arrow/util/parallel.h\n    arrow/vendored\n    arrow/visitor_inline.h\n    gandiva/cache.h\n    gandiva/jni\n    jni/\n    test\n    internal\n    _generated''')\n\n\ndef lint_files():\n    for dirpath, _, filenames in os.walk(arguments.source_path):\n        for filename in filenames:\n            full_path = os.path.join(dirpath, filename)\n\n            exclude = False\n            for exclusion in EXCLUSIONS:\n                if exclusion in full_path:\n                    exclude = True\n                    break\n\n            if exclude:\n                continue\n\n            # Lint file name, except for pkg-config templates\n            if not filename.endswith('.pc.in'):\n                if '-' in filename:\n                    why = (\"Please use underscores, not hyphens, \"\n                           \"in source file names\")\n                    yield full_path, why, 0, full_path\n\n            # Only run on header files\n            if filename.endswith('.h'):\n                for _ in lint_file(full_path):\n                    yield _\n\n\nif __name__ == '__main__':\n    failures = list(lint_files())\n    for path, why, i, line in failures:\n        print('File {0} failed C++/CLI lint check: {1}\\n'\n              'Line {2}: {3}'.format(path, why, i + 1, line))\n    if failures:\n        exit(1)\n", "cpp/build-support/asan_symbolize.py": "#!/usr/bin/env python3\n#===- lib/asan/scripts/asan_symbolize.py -----------------------------------===#\n#\n#                     The LLVM Compiler Infrastructure\n#\n# This file is distributed under the University of Illinois Open Source\n# License. See LICENSE.TXT for details.\n#\n#===------------------------------------------------------------------------===#\nimport bisect\nimport os\nimport re\nimport subprocess\nimport sys\n\nllvm_symbolizer = None\nsymbolizers = {}\nfiletypes = {}\nvmaddrs = {}\nDEBUG = False\n\n\n# FIXME: merge the code that calls fix_filename().\ndef fix_filename(file_name):\n  for path_to_cut in sys.argv[1:]:\n    file_name = re.sub('.*' + path_to_cut, '', file_name)\n  file_name = re.sub('.*asan_[a-z_]*.cc:[0-9]*', '_asan_rtl_', file_name)\n  file_name = re.sub('.*crtstuff.c:0', '???:0', file_name)\n  return file_name\n\n\nclass Symbolizer(object):\n  def __init__(self):\n    pass\n\n  def symbolize(self, addr, binary, offset):\n    \"\"\"Symbolize the given address (pair of binary and offset).\n\n    Overridden in subclasses.\n    Args:\n        addr: virtual address of an instruction.\n        binary: path to executable/shared object containing this instruction.\n        offset: instruction offset in the @binary.\n    Returns:\n        list of strings (one string for each inlined frame) describing\n        the code locations for this instruction (that is, function name, file\n        name, line and column numbers).\n    \"\"\"\n    return None\n\n\nclass LLVMSymbolizer(Symbolizer):\n  def __init__(self, symbolizer_path):\n    super(LLVMSymbolizer, self).__init__()\n    self.symbolizer_path = symbolizer_path\n    self.pipe = self.open_llvm_symbolizer()\n\n  def open_llvm_symbolizer(self):\n    if not os.path.exists(self.symbolizer_path):\n      return None\n    cmd = [self.symbolizer_path,\n           '--use-symbol-table=true',\n           '--demangle=false',\n           '--functions=true',\n           '--inlining=true']\n    if DEBUG:\n      print(' '.join(cmd))\n    return subprocess.Popen(cmd, stdin=subprocess.PIPE,\n                            stdout=subprocess.PIPE)\n\n  def symbolize(self, addr, binary, offset):\n    \"\"\"Overrides Symbolizer.symbolize.\"\"\"\n    if not self.pipe:\n      return None\n    result = []\n    try:\n      symbolizer_input = '%s %s' % (binary, offset)\n      if DEBUG:\n        print(symbolizer_input)\n      self.pipe.stdin.write(symbolizer_input)\n      self.pipe.stdin.write('\\n')\n      while True:\n        function_name = self.pipe.stdout.readline().rstrip()\n        if not function_name:\n          break\n        file_name = self.pipe.stdout.readline().rstrip()\n        file_name = fix_filename(file_name)\n        if (not function_name.startswith('??') and\n            not file_name.startswith('??')):\n          # Append only valid frames.\n          result.append('%s in %s %s' % (addr, function_name,\n                                         file_name))\n    except Exception:\n      result = []\n    if not result:\n      result = None\n    return result\n\n\ndef LLVMSymbolizerFactory(system):\n  symbolizer_path = os.getenv('LLVM_SYMBOLIZER_PATH')\n  if not symbolizer_path:\n    # Assume llvm-symbolizer is in PATH.\n    symbolizer_path = 'llvm-symbolizer'\n  return LLVMSymbolizer(symbolizer_path)\n\n\nclass Addr2LineSymbolizer(Symbolizer):\n  def __init__(self, binary):\n    super(Addr2LineSymbolizer, self).__init__()\n    self.binary = binary\n    self.pipe = self.open_addr2line()\n\n  def open_addr2line(self):\n    cmd = ['addr2line', '-f', '-e', self.binary]\n    if DEBUG:\n      print(' '.join(cmd))\n    return subprocess.Popen(cmd,\n                            stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n\n  def symbolize(self, addr, binary, offset):\n    \"\"\"Overrides Symbolizer.symbolize.\"\"\"\n    if self.binary != binary:\n      return None\n    try:\n      self.pipe.stdin.write(offset)\n      self.pipe.stdin.write('\\n')\n      function_name = self.pipe.stdout.readline().rstrip()\n      file_name = self.pipe.stdout.readline().rstrip()\n    except Exception:\n      function_name = ''\n      file_name = ''\n    file_name = fix_filename(file_name)\n    return ['%s in %s %s' % (addr, function_name, file_name)]\n\n\nclass DarwinSymbolizer(Symbolizer):\n  def __init__(self, addr, binary):\n    super(DarwinSymbolizer, self).__init__()\n    self.binary = binary\n    # Guess which arch we're running. 10 = len('0x') + 8 hex digits.\n    if len(addr) > 10:\n      self.arch = 'x86_64'\n    else:\n      self.arch = 'i386'\n    self.vmaddr = None\n    self.pipe = None\n\n  def write_addr_to_pipe(self, offset):\n    self.pipe.stdin.write('0x%x' % int(offset, 16))\n    self.pipe.stdin.write('\\n')\n\n  def open_atos(self):\n    if DEBUG:\n      print('atos -o %s -arch %s' % (self.binary, self.arch))\n    cmdline = ['atos', '-o', self.binary, '-arch', self.arch]\n    self.pipe = subprocess.Popen(cmdline,\n                                 stdin=subprocess.PIPE,\n                                 stdout=subprocess.PIPE,\n                                 stderr=subprocess.PIPE)\n\n  def symbolize(self, addr, binary, offset):\n    \"\"\"Overrides Symbolizer.symbolize.\"\"\"\n    if self.binary != binary:\n      return None\n    self.open_atos()\n    self.write_addr_to_pipe(offset)\n    self.pipe.stdin.close()\n    atos_line = self.pipe.stdout.readline().rstrip()\n    # A well-formed atos response looks like this:\n    #   foo(type1, type2) (in object.name) (filename.cc:80)\n    match = re.match(r'^(.*) \\(in (.*)\\) \\((.*:\\d*)\\)$', atos_line)\n    if DEBUG:\n      print('atos_line: {0}'.format(atos_line))\n    if match:\n      function_name = match.group(1)\n      function_name = re.sub(r'\\(.*?\\)', '', function_name)\n      file_name = fix_filename(match.group(3))\n      return ['%s in %s %s' % (addr, function_name, file_name)]\n    else:\n      return ['%s in %s' % (addr, atos_line)]\n\n\n# Chain several symbolizers so that if one symbolizer fails, we fall back\n# to the next symbolizer in chain.\nclass ChainSymbolizer(Symbolizer):\n  def __init__(self, symbolizer_list):\n    super(ChainSymbolizer, self).__init__()\n    self.symbolizer_list = symbolizer_list\n\n  def symbolize(self, addr, binary, offset):\n    \"\"\"Overrides Symbolizer.symbolize.\"\"\"\n    for symbolizer in self.symbolizer_list:\n      if symbolizer:\n        result = symbolizer.symbolize(addr, binary, offset)\n        if result:\n          return result\n    return None\n\n  def append_symbolizer(self, symbolizer):\n    self.symbolizer_list.append(symbolizer)\n\n\ndef BreakpadSymbolizerFactory(binary):\n  suffix = os.getenv('BREAKPAD_SUFFIX')\n  if suffix:\n    filename = binary + suffix\n    if os.access(filename, os.F_OK):\n      return BreakpadSymbolizer(filename)\n  return None\n\n\ndef SystemSymbolizerFactory(system, addr, binary):\n  if system == 'Darwin':\n    return DarwinSymbolizer(addr, binary)\n  elif system == 'Linux':\n    return Addr2LineSymbolizer(binary)\n\n\nclass BreakpadSymbolizer(Symbolizer):\n  def __init__(self, filename):\n    super(BreakpadSymbolizer, self).__init__()\n    self.filename = filename\n    lines = file(filename).readlines()\n    self.files = []\n    self.symbols = {}\n    self.address_list = []\n    self.addresses = {}\n    # MODULE mac x86_64 A7001116478B33F18FF9BEDE9F615F190 t\n    fragments = lines[0].rstrip().split()\n    self.arch = fragments[2]\n    self.debug_id = fragments[3]\n    self.binary = ' '.join(fragments[4:])\n    self.parse_lines(lines[1:])\n\n  def parse_lines(self, lines):\n    cur_function_addr = ''\n    for line in lines:\n      fragments = line.split()\n      if fragments[0] == 'FILE':\n        assert int(fragments[1]) == len(self.files)\n        self.files.append(' '.join(fragments[2:]))\n      elif fragments[0] == 'PUBLIC':\n        self.symbols[int(fragments[1], 16)] = ' '.join(fragments[3:])\n      elif fragments[0] in ['CFI', 'STACK']:\n        pass\n      elif fragments[0] == 'FUNC':\n        cur_function_addr = int(fragments[1], 16)\n        if not cur_function_addr in self.symbols.keys():\n          self.symbols[cur_function_addr] = ' '.join(fragments[4:])\n      else:\n        # Line starting with an address.\n        addr = int(fragments[0], 16)\n        self.address_list.append(addr)\n        # Tuple of symbol address, size, line, file number.\n        self.addresses[addr] = (cur_function_addr,\n                                int(fragments[1], 16),\n                                int(fragments[2]),\n                                int(fragments[3]))\n    self.address_list.sort()\n\n  def get_sym_file_line(self, addr):\n    key = None\n    if addr in self.addresses.keys():\n      key = addr\n    else:\n      index = bisect.bisect_left(self.address_list, addr)\n      if index == 0:\n        return None\n      else:\n        key = self.address_list[index - 1]\n    sym_id, size, line_no, file_no = self.addresses[key]\n    symbol = self.symbols[sym_id]\n    filename = self.files[file_no]\n    if addr < key + size:\n      return symbol, filename, line_no\n    else:\n      return None\n\n  def symbolize(self, addr, binary, offset):\n    if self.binary != binary:\n      return None\n    res = self.get_sym_file_line(int(offset, 16))\n    if res:\n      function_name, file_name, line_no = res\n      result = ['%s in %s %s:%d' % (\n          addr, function_name, file_name, line_no)]\n      print(result)\n      return result\n    else:\n      return None\n\n\nclass SymbolizationLoop(object):\n  def __init__(self, binary_name_filter=None):\n    # Used by clients who may want to supply a different binary name.\n    # E.g. in Chrome several binaries may share a single .dSYM.\n    self.binary_name_filter = binary_name_filter\n    self.system = os.uname()[0]\n    if self.system in ['Linux', 'Darwin']:\n      self.llvm_symbolizer = LLVMSymbolizerFactory(self.system)\n    else:\n      raise Exception('Unknown system')\n\n  def symbolize_address(self, addr, binary, offset):\n    # Use the chain of symbolizers:\n    # Breakpad symbolizer -> LLVM symbolizer -> addr2line/atos\n    # (fall back to next symbolizer if the previous one fails).\n    if not binary in symbolizers:\n      symbolizers[binary] = ChainSymbolizer(\n          [BreakpadSymbolizerFactory(binary), self.llvm_symbolizer])\n    result = symbolizers[binary].symbolize(addr, binary, offset)\n    if result is None:\n      # Initialize system symbolizer only if other symbolizers failed.\n      symbolizers[binary].append_symbolizer(\n          SystemSymbolizerFactory(self.system, addr, binary))\n      result = symbolizers[binary].symbolize(addr, binary, offset)\n    # The system symbolizer must produce some result.\n    assert result\n    return result\n\n  def print_symbolized_lines(self, symbolized_lines):\n    if not symbolized_lines:\n      print(self.current_line)\n    else:\n      for symbolized_frame in symbolized_lines:\n        print('    #' + str(self.frame_no) + ' ' + symbolized_frame.rstrip())\n        self.frame_no += 1\n\n  def process_stdin(self):\n    self.frame_no = 0\n\n    if sys.version_info[0] == 2:\n      sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n    else:\n      # Unbuffered output is not supported in Python 3\n      sys.stdout = os.fdopen(sys.stdout.fileno(), 'w')\n\n    while True:\n      line = sys.stdin.readline()\n      if not line: break\n      self.current_line = line.rstrip()\n      #0 0x7f6e35cf2e45  (/blah/foo.so+0x11fe45)\n      stack_trace_line_format = (\n          r'^( *#([0-9]+) *)(0x[0-9a-f]+) *\\((.*)\\+(0x[0-9a-f]+)\\)')\n      match = re.match(stack_trace_line_format, line)\n      if not match:\n        print(self.current_line)\n        continue\n      if DEBUG:\n        print(line)\n      _, frameno_str, addr, binary, offset = match.groups()\n      if frameno_str == '0':\n        # Assume that frame #0 is the first frame of new stack trace.\n        self.frame_no = 0\n      original_binary = binary\n      if self.binary_name_filter:\n        binary = self.binary_name_filter(binary)\n      symbolized_line = self.symbolize_address(addr, binary, offset)\n      if not symbolized_line:\n        if original_binary != binary:\n          symbolized_line = self.symbolize_address(addr, binary, offset)\n      self.print_symbolized_lines(symbolized_line)\n\n\nif __name__ == '__main__':\n  loop = SymbolizationLoop()\n  loop.process_stdin()\n", "cpp/build-support/run_clang_tidy.py": "#!/usr/bin/env python3\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import print_function\nimport argparse\nimport multiprocessing as mp\nimport lintutils\nfrom subprocess import PIPE\nimport sys\nfrom functools import partial\n\n\ndef _get_chunk_key(filenames):\n    # lists are not hashable so key on the first filename in a chunk\n    return filenames[0]\n\n\n# clang-tidy outputs complaints in '/path:line_number: complaint' format,\n# so we can scan its output to get a list of files to fix\ndef _check_some_files(completed_processes, filenames):\n    result = completed_processes[_get_chunk_key(filenames)]\n    return lintutils.stdout_pathcolonline(result, filenames)\n\n\ndef _check_all(cmd, filenames):\n    # each clang-tidy instance will process 16 files\n    chunks = lintutils.chunk(filenames, 16)\n    cmds = [cmd + some for some in chunks]\n    results = lintutils.run_parallel(cmds, stderr=PIPE, stdout=PIPE)\n    error = False\n    # record completed processes (keyed by the first filename in the input\n    # chunk) for lookup in _check_some_files\n    completed_processes = {\n        _get_chunk_key(some): result\n        for some, result in zip(chunks, results)\n    }\n    checker = partial(_check_some_files, completed_processes)\n    pool = mp.Pool()\n    try:\n        # check output of completed clang-tidy invocations in parallel\n        for problem_files, stdout in pool.imap(checker, chunks):\n            if problem_files:\n                msg = \"clang-tidy suggested fixes for {}\"\n                print(\"\\n\".join(map(msg.format, problem_files)))\n                error = True\n    except Exception:\n        error = True\n        raise\n    finally:\n        pool.terminate()\n        pool.join()\n\n    if error:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Runs clang-tidy on all \")\n    parser.add_argument(\"--clang_tidy_binary\",\n                        required=True,\n                        help=\"Path to the clang-tidy binary\")\n    parser.add_argument(\"--exclude_globs\",\n                        help=\"Filename containing globs for files \"\n                        \"that should be excluded from the checks\")\n    parser.add_argument(\"--compile_commands\",\n                        required=True,\n                        help=\"compile_commands.json to pass clang-tidy\")\n    parser.add_argument(\"--source_dir\",\n                        required=True,\n                        action=\"append\",\n                        help=\"Root directory of the source code\")\n    parser.add_argument(\"--fix\", default=False,\n                        action=\"store_true\",\n                        help=\"If specified, will attempt to fix the \"\n                        \"source code instead of recommending fixes, \"\n                        \"defaults to %(default)s\")\n    parser.add_argument(\"--quiet\", default=False,\n                        action=\"store_true\",\n                        help=\"If specified, only print errors\")\n    arguments = parser.parse_args()\n\n    exclude_globs = []\n    if arguments.exclude_globs:\n        for line in open(arguments.exclude_globs):\n            exclude_globs.append(line.strip())\n\n    linted_filenames = []\n    for source_dir in arguments.source_dir:\n        for path in lintutils.get_sources(source_dir, exclude_globs):\n            linted_filenames.append(path)\n\n    if not arguments.quiet:\n        msg = 'Tidying {}' if arguments.fix else 'Checking {}'\n        print(\"\\n\".join(map(msg.format, linted_filenames)))\n\n    cmd = [\n        arguments.clang_tidy_binary,\n        '-p',\n        arguments.compile_commands\n    ]\n    if arguments.fix:\n        cmd.append('-fix')\n        results = lintutils.run_parallel(\n            [cmd + some for some in lintutils.chunk(linted_filenames, 16)])\n        for returncode, stdout, stderr in results:\n            if returncode != 0:\n                sys.exit(returncode)\n\n    else:\n        _check_all(cmd, linted_filenames)\n", "cpp/build-support/run_cpplint.py": "#!/usr/bin/env python3\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import print_function\nimport lintutils\nfrom subprocess import PIPE, STDOUT\nimport argparse\nimport multiprocessing as mp\nimport sys\nimport platform\nfrom functools import partial\n\n\n# NOTE(wesm):\n#\n# * readability/casting is disabled as it aggressively warns about functions\n#   with names like \"int32\", so \"int32(x)\", where int32 is a function name,\n#   warns with\n_filters = '''\n-whitespace/comments\n-readability/casting\n-readability/todo\n-readability/alt_tokens\n-build/header_guard\n-build/c++11\n-build/include_what_you_use\n-runtime/references\n-build/include_order\n'''.split()\n\n\ndef _get_chunk_key(filenames):\n    # lists are not hashable so key on the first filename in a chunk\n    return filenames[0]\n\n\ndef _check_some_files(completed_processes, filenames):\n    # cpplint outputs complaints in '/path:line_number: complaint' format,\n    # so we can scan its output to get a list of files to fix\n    result = completed_processes[_get_chunk_key(filenames)]\n    return lintutils.stdout_pathcolonline(result, filenames)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Runs cpplint on all of the source files.\")\n    parser.add_argument(\"--cpplint_binary\",\n                        required=True,\n                        help=\"Path to the cpplint binary\")\n    parser.add_argument(\"--exclude_globs\",\n                        help=\"Filename containing globs for files \"\n                        \"that should be excluded from the checks\")\n    parser.add_argument(\"--source_dir\",\n                        required=True,\n                        action=\"append\",\n                        help=\"Root directory of the source code\")\n    parser.add_argument(\"--quiet\", default=False,\n                        action=\"store_true\",\n                        help=\"If specified, only print errors\")\n    arguments = parser.parse_args()\n\n    exclude_globs = []\n    if arguments.exclude_globs:\n        with open(arguments.exclude_globs) as f:\n            exclude_globs.extend(line.strip() for line in f)\n\n    linted_filenames = []\n    for source_dir in arguments.source_dir:\n        for path in lintutils.get_sources(source_dir, exclude_globs):\n            linted_filenames.append(str(path))\n\n    cmd = [\n        arguments.cpplint_binary,\n        '--verbose=2',\n        '--linelength=90',\n        '--filter=' + ','.join(_filters)\n    ]\n    if (arguments.cpplint_binary.endswith('.py') and\n            platform.system() == 'Windows'):\n        # Windows doesn't support executable scripts; execute with\n        # sys.executable\n        cmd.insert(0, sys.executable)\n    if arguments.quiet:\n        cmd.append('--quiet')\n    else:\n        print(\"\\n\".join(map(lambda x: \"Linting {}\".format(x),\n                            linted_filenames)))\n\n    # lint files in chunks: each invocation of cpplint will process 16 files\n    chunks = lintutils.chunk(linted_filenames, 16)\n    cmds = [cmd + some for some in chunks]\n    results = lintutils.run_parallel(cmds, stdout=PIPE, stderr=STDOUT)\n\n    error = False\n    # record completed processes (keyed by the first filename in the input\n    # chunk) for lookup in _check_some_files\n    completed_processes = {\n        _get_chunk_key(filenames): result\n        for filenames, result in zip(chunks, results)\n    }\n    checker = partial(_check_some_files, completed_processes)\n    pool = mp.Pool()\n    try:\n        # scan the outputs of various cpplint invocations in parallel to\n        # distill a list of problematic files\n        for problem_files, stdout in pool.imap(checker, chunks):\n            if problem_files:\n                if isinstance(stdout, bytes):\n                    stdout = stdout.decode('utf8')\n                print(stdout, file=sys.stderr)\n                error = True\n    except Exception:\n        error = True\n        raise\n    finally:\n        pool.terminate()\n        pool.join()\n\n    sys.exit(1 if error else 0)\n", "cpp/build-support/iwyu/iwyu_tool.py": "#!/usr/bin/env python\n\n# This file has been imported into the apache source tree from\n# the IWYU source tree as of version 0.8\n#   https://github.com/include-what-you-use/include-what-you-use/blob/master/iwyu_tool.py\n# and corresponding license has been added:\n#   https://github.com/include-what-you-use/include-what-you-use/blob/master/LICENSE.TXT\n#\n# ==============================================================================\n# LLVM Release License\n# ==============================================================================\n# University of Illinois/NCSA\n# Open Source License\n#\n# Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign.\n# All rights reserved.\n#\n# Developed by:\n#\n#     LLVM Team\n#\n#     University of Illinois at Urbana-Champaign\n#\n#     http://llvm.org\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal with\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n#     * Redistributions of source code must retain the above copyright notice,\n#       this list of conditions and the following disclaimers.\n#\n#     * Redistributions in binary form must reproduce the above copyright notice,\n#       this list of conditions and the following disclaimers in the\n#       documentation and/or other materials provided with the distribution.\n#\n#     * Neither the names of the LLVM Team, University of Illinois at\n#       Urbana-Champaign, nor the names of its contributors may be used to\n#       endorse or promote products derived from this Software without specific\n#       prior written permission.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\n# CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE\n# SOFTWARE.\n\n\"\"\" Driver to consume a Clang compilation database and invoke IWYU.\n\nExample usage with CMake:\n\n  # Unix systems\n  $ mkdir build && cd build\n  $ CC=\"clang\" CXX=\"clang++\" cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=ON ...\n  $ iwyu_tool.py -p .\n\n  # Windows systems\n  $ mkdir build && cd build\n  $ cmake -DCMAKE_CXX_COMPILER=\"%VCINSTALLDIR%/bin/cl.exe\" \\\n    -DCMAKE_C_COMPILER=\"%VCINSTALLDIR%/VC/bin/cl.exe\" \\\n    -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \\\n    -G Ninja ...\n  $ python iwyu_tool.py -p .\n\nSee iwyu_tool.py -h for more details on command-line arguments.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport argparse\nimport subprocess\nimport re\n\nimport logging\n\nlogging.basicConfig(filename='iwyu.log')\nLOGGER = logging.getLogger(\"iwyu\")\n\n\ndef iwyu_formatter(output):\n    \"\"\" Process iwyu's output, basically a no-op. \"\"\"\n    print('\\n'.join(output))\n\n\nCORRECT_RE = re.compile(r'^\\((.*?) has correct #includes/fwd-decls\\)$')\nSHOULD_ADD_RE = re.compile(r'^(.*?) should add these lines:$')\nSHOULD_REMOVE_RE = re.compile(r'^(.*?) should remove these lines:$')\nFULL_LIST_RE = re.compile(r'The full include-list for (.*?):$')\nEND_RE = re.compile(r'^---$')\nLINES_RE = re.compile(r'^- (.*?)  // lines ([0-9]+)-[0-9]+$')\n\n\nGENERAL, ADD, REMOVE, LIST = range(4)\n\n\ndef clang_formatter(output):\n    \"\"\" Process iwyu's output into something clang-like. \"\"\"\n    state = (GENERAL, None)\n    for line in output:\n        match = CORRECT_RE.match(line)\n        if match:\n            print('%s:1:1: note: #includes/fwd-decls are correct', match.groups(1))\n            continue\n        match = SHOULD_ADD_RE.match(line)\n        if match:\n            state = (ADD, match.group(1))\n            continue\n        match = SHOULD_REMOVE_RE.match(line)\n        if match:\n            state = (REMOVE, match.group(1))\n            continue\n        match = FULL_LIST_RE.match(line)\n        if match:\n            state = (LIST, match.group(1))\n        elif END_RE.match(line):\n            state = (GENERAL, None)\n        elif not line.strip():\n            continue\n        elif state[0] == GENERAL:\n            print(line)\n        elif state[0] == ADD:\n            print('%s:1:1: error: add the following line', state[1])\n            print(line)\n        elif state[0] == REMOVE:\n            match = LINES_RE.match(line)\n            line_no = match.group(2) if match else '1'\n            print('%s:%s:1: error: remove the following line', state[1], line_no)\n            print(match.group(1))\n\n\nDEFAULT_FORMAT = 'iwyu'\nFORMATTERS = {\n    'iwyu': iwyu_formatter,\n    'clang': clang_formatter\n}\n\n\ndef get_output(cwd, command):\n    \"\"\" Run the given command and return its output as a string. \"\"\"\n    process = subprocess.Popen(command,\n                               cwd=cwd,\n                               shell=True,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.STDOUT)\n    return process.communicate()[0].decode(\"utf-8\").splitlines()\n\n\ndef run_iwyu(cwd, compile_command, iwyu_args, verbose, formatter):\n    \"\"\" Rewrite compile_command to an IWYU command, and run it. \"\"\"\n    compiler, _, args = compile_command.partition(' ')\n    if compiler.endswith('cl.exe'):\n        # If the compiler name is cl.exe, let IWYU be cl-compatible\n        clang_args = ['--driver-mode=cl']\n    else:\n        clang_args = []\n\n    iwyu_args = ['-Xiwyu ' + a for a in iwyu_args]\n    command = ['include-what-you-use'] + clang_args + iwyu_args\n    command = '%s %s' % (' '.join(command), args.strip())\n\n    if verbose:\n        print('%s:', command)\n\n    formatter(get_output(cwd, command))\n\n\ndef main(compilation_db_path, source_files, verbose, formatter, iwyu_args):\n    \"\"\" Entry point. \"\"\"\n    # Canonicalize compilation database path\n    if os.path.isdir(compilation_db_path):\n        compilation_db_path = os.path.join(compilation_db_path,\n                                           'compile_commands.json')\n\n    compilation_db_path = os.path.realpath(compilation_db_path)\n    if not os.path.isfile(compilation_db_path):\n        print('ERROR: No such file or directory: \\'%s\\'', compilation_db_path)\n        return 1\n\n    # Read compilation db from disk\n    with open(compilation_db_path, 'r') as fileobj:\n        compilation_db = json.load(fileobj)\n\n    # expand symlinks\n    for entry in compilation_db:\n        entry['file'] = os.path.realpath(entry['file'])\n\n    # Cross-reference source files with compilation database\n    source_files = [os.path.realpath(s) for s in source_files]\n    if not source_files:\n        # No source files specified, analyze entire compilation database\n        entries = compilation_db\n    else:\n        # Source files specified, analyze the ones appearing in compilation db,\n        # warn for the rest.\n        entries = []\n        for source in source_files:\n            matches = [e for e in compilation_db if e['file'] == source]\n            if matches:\n                entries.extend(matches)\n            else:\n                print(\"{} not in compilation database\".format(source))\n                # TODO: As long as there is no complete compilation database available this check cannot be performed\n                pass\n                #print('WARNING: \\'%s\\' not found in compilation database.', source)\n\n    # Run analysis\n    try:\n        for entry in entries:\n            cwd, compile_command = entry['directory'], entry['command']\n            run_iwyu(cwd, compile_command, iwyu_args, verbose, formatter)\n    except OSError as why:\n        print('ERROR: Failed to launch include-what-you-use: %s', why)\n        return 1\n\n    return 0\n\n\ndef _bootstrap():\n    \"\"\" Parse arguments and dispatch to main(). \"\"\"\n    # This hackery is necessary to add the forwarded IWYU args to the\n    # usage and help strings.\n    def customize_usage(parser):\n        \"\"\" Rewrite the parser's format_usage. \"\"\"\n        original_format_usage = parser.format_usage\n        parser.format_usage = lambda: original_format_usage().rstrip() + \\\n                              ' -- [<IWYU args>]' + os.linesep\n\n    def customize_help(parser):\n        \"\"\" Rewrite the parser's format_help. \"\"\"\n        original_format_help = parser.format_help\n\n        def custom_help():\n            \"\"\" Customized help string, calls the adjusted format_usage. \"\"\"\n            helpmsg = original_format_help()\n            helplines = helpmsg.splitlines()\n            helplines[0] = parser.format_usage().rstrip()\n            return os.linesep.join(helplines) + os.linesep\n\n        parser.format_help = custom_help\n\n    # Parse arguments\n    parser = argparse.ArgumentParser(\n        description='Include-what-you-use compilation database driver.',\n        epilog='Assumes include-what-you-use is available on the PATH.')\n    customize_usage(parser)\n    customize_help(parser)\n\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='Print IWYU commands')\n    parser.add_argument('-o', '--output-format', type=str,\n                        choices=FORMATTERS.keys(), default=DEFAULT_FORMAT,\n                        help='Output format (default: %s)' % DEFAULT_FORMAT)\n    parser.add_argument('-p', metavar='<build-path>', required=True,\n                        help='Compilation database path', dest='dbpath')\n    parser.add_argument('source', nargs='*',\n                        help='Zero or more source files to run IWYU on. '\n                        'Defaults to all in compilation database.')\n\n    def partition_args(argv):\n        \"\"\" Split around '--' into driver args and IWYU args. \"\"\"\n        try:\n            double_dash = argv.index('--')\n            return argv[:double_dash], argv[double_dash+1:]\n        except ValueError:\n            return argv, []\n    argv, iwyu_args = partition_args(sys.argv[1:])\n    args = parser.parse_args(argv)\n\n    sys.exit(main(args.dbpath, args.source, args.verbose,\n                  FORMATTERS[args.output_format], iwyu_args))\n\n\nif __name__ == '__main__':\n    _bootstrap()\n", "cpp/build-support/fuzzing/pack_corpus.py": "#!/usr/bin/env python3\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Rename a bunch of corpus files to their SHA1 hashes, and\n# pack them into a ZIP archive.\n\nimport hashlib\nfrom pathlib import Path\nimport sys\nimport zipfile\n\n\ndef process_dir(corpus_dir, zip_output):\n    seen = set()\n\n    for child in corpus_dir.iterdir():\n        if not child.is_file():\n            raise IOError(\"Not a file: {0}\".format(child))\n        with child.open('rb') as f:\n            data = f.read()\n        arcname = hashlib.sha1(data).hexdigest()\n        if arcname in seen:\n            raise ValueError(\"Duplicate hash: {0} (in file {1})\"\n                             .format(arcname, child))\n        zip_output.writestr(str(arcname), data)\n        seen.add(arcname)\n\n\ndef main(corpus_dir, zip_output_name):\n    with zipfile.ZipFile(zip_output_name, 'w') as zip_output:\n        process_dir(Path(corpus_dir), zip_output)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: {0} <corpus dir> <output zip file>\".format(sys.argv[0]))\n        sys.exit(1)\n    main(sys.argv[1], sys.argv[2])\n", "cpp/src/gandiva/make_precompiled_bitcode.py": "#!/usr/bin/env python\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport sys\n\nmarker = b\"<DATA_CHARS>\"\n\ndef expand(data):\n    \"\"\"\n    Expand *data* as a initializer list of hexadecimal char escapes.\n    \"\"\"\n    expanded_data = \", \".join([hex(c) for c in bytearray(data)])\n    return expanded_data.encode('ascii')\n\n\ndef apply_template(template, data):\n    if template.count(marker) != 1:\n        raise ValueError(\"Invalid template\")\n    return template.replace(marker, expand(data))\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 4:\n        raise ValueError(\"Usage: {0} <template file> <data file> \"\n                         \"<output file>\".format(sys.argv[0]))\n    with open(sys.argv[1], \"rb\") as f:\n        template = f.read()\n    with open(sys.argv[2], \"rb\") as f:\n        data = f.read()\n\n    expanded_data = apply_template(template, data)\n    with open(sys.argv[3], \"wb\") as f:\n        f.write(expanded_data)\n", "cpp/src/arrow/acero/hash_join_graphs.py": "#!/bin/env python3\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n'''\nThis script takes a JSON file from a google benchmark run that measures rows/sec and generates graphs\nfor each benchmark.\n\nExample usage:\n1. Generate Benchmark Data:\nrelease/arrow-compute-hash-join-benchmark \\\n    --benchmark_counters_tabular=true \\\n    --benchmark_format=console \\\n    --benchmark_out=benchmark_data.json \\\n    --benchmark_out_format=json\n\n2. Visualize:\n../src/arrow/compute/exec/hash_join_graphs.py benchmarks_data.json\n'''\n\nimport math\nimport sys\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()\n\ndef try_as_numeric(val):\n    try:\n        return float(val)\n    except:\n        return str(val)\n\ndef is_multiplicative(lst):\n    if len(lst) < 3:\n        return False\n\n    if (lst[2] - lst[1]) == (lst[1] - lst[0]):\n        return False\n    \n    assert (lst[2] / lst[1]) == (lst[1] / lst[0])\n    return True\n\nclass Test:\n    def __init__(self):\n        self.times = []\n        self.args = {}\n\n    def get_argnames_by_cardinality_increasing(self):\n        key_cardinality = lambda x: len(set(self.args[x]))\n        def key_strings_first(x):\n            try:\n                as_float = float(self.args[x][0])\n                return True\n            except:\n                return False\n        by_cardinality = sorted(self.args.keys(), key=key_cardinality)\n        strings_first = sorted(by_cardinality, key=key_strings_first)\n        return strings_first\n\ndef organize_tests(filename):\n    tests = {}\n    with open(filename) as f:\n        df = json.load(f)\n        for idx, row in enumerate(df['benchmarks']):\n            test_name = row['name']\n            test_name_split = test_name.split('/')\n            if test_name_split[-1] == 'process_time':\n                test_name_split = test_name_split[:-1]\n                \n            base_name = test_name_split[0]\n            args = test_name_split[1:]\n            if base_name not in tests.keys():\n                tests[base_name] = Test()\n\n            tests[base_name].times.append(row['rows/sec'])\n            \n            if len(args) > 3:\n                raise('Test can have at most 3 parameters! Found', len(args), 'in test', test_name)\n            \n            nonnamed_args = [x for x in args if ':' not in x]\n            if len(nonnamed_args) > 1:\n                raise('Test name must have only one non-named parameter! Found', len(nonnamed_args), 'in test', test_name)\n\n            for arg in args:\n                arg_name = ''\n                arg_value = arg.strip('\\\"')\n                if ':' in arg:\n                    arg_split = arg.split(':')\n                    arg_name = arg_split[0]\n                    arg_value = arg_split[1].strip('\\\"')\n\n                arg_value = try_as_numeric(arg_value)\n                if arg_name not in tests[base_name].args.keys():\n                    tests[base_name].args[arg_name] = [arg_value]\n                else:\n                    tests[base_name].args[arg_name].append(arg_value)\n    return tests;\n\ndef construct_name(argname, argvalue):\n    if not argname:\n        return argvalue\n    return '%s: %s' % (argname, argvalue)\n\ndef plot_1d(test, argname, ax, label=None):\n    x_axis = test.args[argname]\n    y_axis = test.times\n    ax.plot(x_axis, y_axis, label=label)\n    if is_multiplicative(x_axis):\n        ax.set_xscale('log', base=(x_axis[1] / x_axis[0]))\n        ax.xaxis.set_major_formatter(plt.ScalarFormatter())\n    ax.legend()\n    ax.set_xlabel(argname)\n    ax.set_ylabel('rows/sec')\n\ndef plot_2d(test, sorted_argnames, ax, title):\n    assert len(sorted_argnames) == 2\n    lines = set(test.args[sorted_argnames[0]])\n    ax.set_title(title)\n    for line in sorted(lines, key=try_as_numeric):\n        indices = range(len(test.times))\n        indices = list(filter(lambda i: test.args[sorted_argnames[0]][i] == line, indices))\n        filtered_test = Test()\n        filtered_test.times = [test.times[i] for i in indices]\n        filtered_test.args[sorted_argnames[1]] = [test.args[sorted_argnames[1]][i] for i in indices]\n        plot_1d(filtered_test, sorted_argnames[1], ax, construct_name(sorted_argnames[0], line))\n\ndef plot_3d(test, sorted_argnames):\n    assert len(sorted_argnames) == 3\n    num_graphs = len(set(test.args[sorted_argnames[0]]))\n    num_rows = int(math.ceil(math.sqrt(num_graphs)))\n    num_cols = int(math.ceil(num_graphs / num_rows))\n    graphs = set(test.args[sorted_argnames[0]])\n\n    for j, graph in enumerate(sorted(graphs, key=try_as_numeric)):\n        ax = plt.subplot(num_rows, num_cols, j + 1)\n        filtered_test = Test()\n        indices = range(len(test.times))\n        indices = list(filter(lambda i: test.args[sorted_argnames[0]][i] == graph, indices))\n        filtered_test.times = [test.times[i] for i in indices]\n        filtered_test.args[sorted_argnames[1]] = [test.args[sorted_argnames[1]][i] for i in indices]\n        filtered_test.args[sorted_argnames[2]] = [test.args[sorted_argnames[2]][i] for i in indices]\n        plot_2d(filtered_test, sorted_argnames[1:], ax, construct_name(sorted_argnames[0], graph))\n\ndef main():\n    if len(sys.argv) != 2:\n        print('Usage: hash_join_graphs.py <data>.json')\n        print('This script expects there to be a counter called rows/sec as a field of every test in the JSON file.')\n        return\n\n    tests = organize_tests(sys.argv[1])\n\n    for i, test_name in enumerate(tests.keys()):\n        test = tests[test_name]\n        sorted_argnames = test.get_argnames_by_cardinality_increasing()\n        # Create a graph per lowest-cardinality arg\n        # Create a line per second-lowest-cardinality arg\n        # Use highest-cardinality arg as X axis\n        fig = plt.figure(i)\n        num_args = len(sorted_argnames)\n        if num_args == 3:\n            fig.suptitle(test_name)\n            plot_3d(test, sorted_argnames)\n            fig.subplots_adjust(hspace=0.4)\n        elif num_args == 2:\n            ax = plt.subplot()\n            plot_2d(test, sorted_argnames, ax, test_name)\n        else:\n            fig.suptitle(test_name)\n            ax = plt.subplot()\n            plot_1d(test, sorted_argnames[0], ax)\n        fig.set_size_inches(16, 9)\n        fig.savefig('%s.svg' % test_name, dpi=fig.dpi, bbox_inches='tight')\n        plt.show()\n\nif __name__ == '__main__':\n    main()\n", "cpp/src/arrow/util/bpacking_simd_codegen.py": "#!/usr/bin/env python\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Usage:\n#   python bpacking_simd_codegen.py 128 > bpacking_simd128_generated_internal.h\n#   python bpacking_simd_codegen.py 256 > bpacking_simd256_generated_internal.h\n#   python bpacking_simd_codegen.py 512 > bpacking_simd512_generated_internal.h\n\nfrom functools import partial\nimport sys\nfrom textwrap import dedent, indent\n\n\nclass UnpackGenerator:\n\n    def __init__(self, simd_width):\n        self.simd_width = simd_width\n        if simd_width % 32 != 0:\n            raise(\"SIMD bit width should be a multiple of 32\")\n        self.simd_byte_width = simd_width // 8\n\n    def print_unpack_bit0_func(self):\n        print(\n            \"inline static const uint32_t* unpack0_32(const uint32_t* in, uint32_t* out) {\")\n        print(\"  memset(out, 0x0, 32 * sizeof(*out));\")\n        print(\"  out += 32;\")\n        print(\"\")\n        print(\"  return in;\")\n        print(\"}\")\n\n\n    def print_unpack_bit32_func(self):\n        print(\n            \"inline static const uint32_t* unpack32_32(const uint32_t* in, uint32_t* out) {\")\n        print(\"  memcpy(out, in, 32 * sizeof(*out));\")\n        print(\"  in += 32;\")\n        print(\"  out += 32;\")\n        print(\"\")\n        print(\"  return in;\")\n        print(\"}\")\n\n    def print_unpack_bit_func(self, bit):\n        def p(code):\n            print(indent(code, prefix='  '))\n\n        shift = 0\n        shifts = []\n        in_index = 0\n        inls = []\n        mask = (1 << bit) - 1\n        bracket = \"{\"\n\n        print(f\"inline static const uint32_t* unpack{bit}_32(const uint32_t* in, uint32_t* out) {{\")\n        p(dedent(f\"\"\"\\\n            uint32_t mask = 0x{mask:0x};\n\n            simd_batch masks(mask);\n            simd_batch words, shifts;\n            simd_batch results;\n            \"\"\"))\n\n        def safe_load(index):\n            return f\"SafeLoad<uint32_t>(in + {index})\"\n\n        for i in range(32):\n            if shift + bit == 32:\n                shifts.append(shift)\n                inls.append(safe_load(in_index))\n                in_index += 1\n                shift = 0\n            elif shift + bit > 32:  # cross the boundary\n                inls.append(\n                    f\"{safe_load(in_index)} >> {shift} | {safe_load(in_index + 1)} << {32 - shift}\")\n                in_index += 1\n                shift = bit - (32 - shift)\n                shifts.append(0)  # zero shift\n            else:\n                shifts.append(shift)\n                inls.append(safe_load(in_index))\n                shift += bit\n\n        bytes_per_batch = self.simd_byte_width\n        words_per_batch = bytes_per_batch // 4\n\n        one_word_template = dedent(\"\"\"\\\n            words = simd_batch{{ {words} }};\n            shifts = simd_batch{{ {shifts} }};\n            results = (words >> shifts) & masks;\n            results.store_unaligned(out);\n            out += {words_per_batch};\n            \"\"\")\n\n        for start in range(0, 32, words_per_batch):\n            stop = start + words_per_batch;\n            p(f\"\"\"// extract {bit}-bit bundles {start} to {stop - 1}\"\"\")\n            p(one_word_template.format(\n                words=\", \".join(inls[start:stop]),\n                shifts=\", \".join(map(str, shifts[start:stop])),\n                words_per_batch=words_per_batch))\n\n        p(dedent(f\"\"\"\\\n            in += {bit};\n            return in;\"\"\"))\n        print(\"}\")\n\n\ndef print_copyright():\n    print(dedent(\"\"\"\\\n        // Licensed to the Apache Software Foundation (ASF) under one\n        // or more contributor license agreements.  See the NOTICE file\n        // distributed with this work for additional information\n        // regarding copyright ownership.  The ASF licenses this file\n        // to you under the Apache License, Version 2.0 (the\n        // \"License\"); you may not use this file except in compliance\n        // with the License.  You may obtain a copy of the License at\n        //\n        //   http://www.apache.org/licenses/LICENSE-2.0\n        //\n        // Unless required by applicable law or agreed to in writing,\n        // software distributed under the License is distributed on an\n        // \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n        // KIND, either express or implied.  See the License for the\n        // specific language governing permissions and limitations\n        // under the License.\n        \"\"\"))\n\n\ndef print_note():\n    print(\"// Automatically generated file; DO NOT EDIT.\")\n    print()\n\n\ndef main(simd_width):\n    print_copyright()\n    print_note()\n\n    struct_name = f\"UnpackBits{simd_width}\"\n\n    # NOTE: templating the UnpackBits struct on the dispatch level avoids\n    # potential name collisions if there are several UnpackBits generations\n    # with the same SIMD width on a given architecture.\n\n    print(dedent(f\"\"\"\\\n        #pragma once\n\n        #include <cstdint>\n        #include <cstring>\n\n        #include <xsimd/xsimd.hpp>\n\n        #include \"arrow/util/dispatch.h\"\n        #include \"arrow/util/ubsan.h\"\n\n        namespace arrow {{\n        namespace internal {{\n        namespace {{\n\n        using ::arrow::util::SafeLoad;\n\n        template <DispatchLevel level>\n        struct {struct_name} {{\n\n        using simd_batch = xsimd::make_sized_batch_t<uint32_t, {simd_width//32}>;\n        \"\"\"))\n\n    gen = UnpackGenerator(simd_width)\n    gen.print_unpack_bit0_func()\n    print()\n    for i in range(1, 32):\n        gen.print_unpack_bit_func(i)\n        print()\n    gen.print_unpack_bit32_func()\n    print()\n\n    print(dedent(f\"\"\"\\\n        }};  // struct {struct_name}\n\n        }}  // namespace\n        }}  // namespace internal\n        }}  // namespace arrow\n        \"\"\"))\n\n\nif __name__ == '__main__':\n    usage = f\"\"\"Usage: {__file__} <SIMD bit-width>\"\"\"\n    if len(sys.argv) != 2:\n        raise ValueError(usage)\n    try:\n        simd_width = int(sys.argv[1])\n    except ValueError:\n        raise ValueError(usage)\n\n    main(simd_width)\n", "cpp/src/arrow/util/bpacking64_codegen.py": "#!/bin/python\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# This script is modified from its original version in GitHub. Original source:\n# https://github.com/lemire/FrameOfReference/blob/146948b6058a976bc7767262ad3a2ce201486b93/scripts/turbopacking64.py\n\n# Usage:\n#   python bpacking64_codegen.py > bpacking64_default.h\n\ndef howmany(bit):\n    \"\"\" how many values are we going to pack? \"\"\"\n    return 32\n\n\ndef howmanywords(bit):\n    return (howmany(bit) * bit + 63)//64\n\n\ndef howmanybytes(bit):\n    return (howmany(bit) * bit + 7)//8\n\n\nprint('''// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\n// This file was generated by script which is modified from its original version in GitHub.\n// Original source:\n// https://github.com/lemire/FrameOfReference/blob/master/scripts/turbopacking64.py\n// The original copyright notice follows.\n\n// This code is released under the\n// Apache License Version 2.0 http://www.apache.org/licenses/.\n// (c) Daniel Lemire 2013\n\n#pragma once\n\n#include \"arrow/util/bit_util.h\"\n#include \"arrow/util/ubsan.h\"\n\nnamespace arrow {\nnamespace internal {\n''')\n\n\nprint(\"inline const uint8_t* unpack0_64(const uint8_t* in, uint64_t* out) {\")\nprint(\"  for(int k = 0; k < {0} ; k += 1) {{\".format(howmany(0)))\nprint(\"    out[k] = 0;\")\nprint(\"  }\")\nprint(\"  return in;\")\nprint(\"}\")\n\nfor bit in range(1, 65):\n    print(\"\")\n    print(\n        \"inline const uint8_t* unpack{0}_64(const uint8_t* in, uint64_t* out) {{\".format(bit))\n\n    if(bit < 64):\n        print(\"  const uint64_t mask = {0}ULL;\".format((1 << bit)-1))\n    maskstr = \" & mask\"\n    if (bit == 64):\n        maskstr = \"\"  # no need\n\n    for k in range(howmanywords(bit)-1):\n        print(\"  uint64_t w{0} = util::SafeLoadAs<uint64_t>(in);\".format(k))\n        print(\"  w{0} = arrow::BitUtil::FromLittleEndian(w{0});\".format(k))\n        print(\"  in += 8;\".format(k))\n    k = howmanywords(bit) - 1\n    if (bit % 2 == 0):\n        print(\"  uint64_t w{0} = util::SafeLoadAs<uint64_t>(in);\".format(k))\n        print(\"  w{0} = arrow::BitUtil::FromLittleEndian(w{0});\".format(k))\n        print(\"  in += 8;\".format(k))\n    else:\n        print(\"  uint64_t w{0} = util::SafeLoadAs<uint32_t>(in);\".format(k))\n        print(\"  w{0} = arrow::BitUtil::FromLittleEndian(w{0});\".format(k))\n        print(\"  in += 4;\".format(k))\n\n    for j in range(howmany(bit)):\n        firstword = j * bit // 64\n        secondword = (j * bit + bit - 1)//64\n        firstshift = (j*bit) % 64\n        firstshiftstr = \" >> {0}\".format(firstshift)\n        if(firstshift == 0):\n            firstshiftstr = \"\"  # no need\n        if(firstword == secondword):\n            if(firstshift + bit == 64):\n                print(\"  out[{0}] = w{1}{2};\".format(\n                    j, firstword, firstshiftstr, firstshift))\n            else:\n                print(\"  out[{0}] = (w{1}{2}){3};\".format(\n                    j, firstword, firstshiftstr, maskstr))\n        else:\n            secondshift = (64-firstshift)\n            print(\"  out[{0}] = ((w{1}{2}) | (w{3} << {4})){5};\".format(\n                j, firstword, firstshiftstr, firstword+1, secondshift, maskstr))\n    print(\"\")\n    print(\"  return in;\")\n    print(\"}\")\n\nprint('''\n}  // namespace internal\n}  // namespace arrow''')\n", "ci/scripts/generate_dataset.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nimport os\nimport shutil\nimport random\n\nimport pandas as pd\n\nif __name__ == \"__main__\":\n    # generate the test dataframe\n    data = {\n        \"total_amount\": list(),\n        \"fare_amount\": list()\n    }\n    for i in range(0, 500):\n        data['total_amount'].append(random.randint(1,11)*5)\n        data['fare_amount'].append(random.randint(1,11)*3)\n    df = pd.DataFrame(data)\n\n    # dump the dataframe to a parquet file\n    df.to_parquet(\"skyhook_test_data.parquet\")\n\n    # create the dataset by copying the parquet files\n    shutil.rmtree(\"nyc\", ignore_errors=True)\n    payment_type = [\"1\", \"2\", \"3\", \"4\"]\n    vendor_id = [\"1\", \"2\"]\n    for p in payment_type:\n        for v in vendor_id:\n            path = f\"nyc/payment_type={p}/VendorID={v}\"\n            os.makedirs(path, exist_ok=True)\n            shutil.copyfile(\"skyhook_test_data.parquet\", os.path.join(path, f\"{p}.{v}.parquet\"))\n", "ci/scripts/go_bench_adapt.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport json\nimport os\nimport uuid\nimport logging\nfrom pathlib import Path\nfrom typing import List\n\nfrom benchadapt import BenchmarkResult\nfrom benchadapt.adapters import BenchmarkAdapter\nfrom benchadapt.log import log\n\nlog.setLevel(logging.DEBUG)\n\nARROW_ROOT = Path(__file__).parent.parent.parent.resolve()\nSCRIPTS_PATH = ARROW_ROOT / \"ci\" / \"scripts\"\n\n# `github_commit_info` is meant to communicate GitHub-flavored commit\n# information to Conbench. See\n# https://github.com/conbench/conbench/blob/cf7931f/benchadapt/python/benchadapt/result.py#L66\n# for a specification.\ngithub_commit_info = {\"repository\": \"https://github.com/apache/arrow\"}\n\nif os.environ.get(\"CONBENCH_REF\") == \"main\":\n    # Assume GitHub Actions CI. The environment variable lookups below are\n    # expected to fail when not running in GitHub Actions.\n    github_commit_info = {\n        \"repository\": f'{os.environ[\"GITHUB_SERVER_URL\"]}/{os.environ[\"GITHUB_REPOSITORY\"]}',\n        \"commit\": os.environ[\"GITHUB_SHA\"],\n        \"pr_number\": None,  # implying default branch\n    }\n    run_reason = \"commit\"\nelse:\n    # Assume that the environment is not GitHub Actions CI. Error out if that\n    # assumption seems to be wrong.\n    assert os.getenv(\"GITHUB_ACTIONS\") is None\n\n    # This is probably a local dev environment, for testing. In this case, it\n    # does usually not make sense to provide commit information (not a\n    # controlled CI environment). Explicitly leave out \"commit\" and \"pr_number\" to\n    # reflect that (to not send commit information).\n\n    # Reflect 'local dev' scenario in run_reason. Allow user to (optionally)\n    # inject a custom piece of information into the run reason here, from\n    # environment.\n    run_reason = \"localdev\"\n    custom_reason_suffix = os.getenv(\"CONBENCH_CUSTOM_RUN_REASON\")\n    if custom_reason_suffix is not None:\n        run_reason += f\" {custom_reason_suffix.strip()}\"\n\n\nclass GoAdapter(BenchmarkAdapter):\n    result_file = \"bench_stats.json\"\n    command = [\"bash\", SCRIPTS_PATH / \"go_bench.sh\", ARROW_ROOT, \"-json\"]\n\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(command=self.command, *args, **kwargs)\n\n    def _transform_results(self) -> List[BenchmarkResult]:\n        with open(self.result_file, \"r\") as f:\n            raw_results = json.load(f)\n\n        run_id = uuid.uuid4().hex\n        parsed_results = []\n        for suite in raw_results[0][\"Suites\"]:\n            batch_id = uuid.uuid4().hex\n            pkg = suite[\"Pkg\"]\n\n            for benchmark in suite[\"Benchmarks\"]:\n                data = benchmark[\"Mem\"][\"MBPerSec\"] * 1e6\n                time = 1 / benchmark[\"NsPerOp\"] * 1e9\n\n                name = benchmark[\"Name\"].removeprefix(\"Benchmark\")\n                ncpu = name[name.rfind(\"-\") + 1 :]\n                pieces = name[: -(len(ncpu) + 1)].split(\"/\")\n\n                parsed = BenchmarkResult(\n                    run_id=run_id,\n                    batch_id=batch_id,\n                    stats={\n                        \"data\": [data],\n                        \"unit\": \"B/s\",\n                        \"times\": [time],\n                        \"time_unit\": \"i/s\",\n                        \"iterations\": benchmark[\"Runs\"],\n                    },\n                    context={\n                        \"benchmark_language\": \"Go\",\n                        \"goos\": suite[\"Goos\"],\n                        \"goarch\": suite[\"Goarch\"],\n                    },\n                    tags={\n                        \"pkg\": pkg,\n                        \"num_cpu\": ncpu,\n                        \"name\": pieces[0],\n                        \"params\": \"/\".join(pieces[1:]),\n                    },\n                    run_reason=run_reason,\n                    github=github_commit_info,\n                )\n                parsed.run_name = (\n                    f\"{parsed.run_reason}: {github_commit_info.get('commit')}\"\n                )\n                parsed_results.append(parsed)\n\n        return parsed_results\n\n\nif __name__ == \"__main__\":\n    go_adapter = GoAdapter(result_fields_override={\"info\": {}})\n    go_adapter()\n", "ci/conan/all/conanfile.py": "# MIT License\n#\n# Copyright (c) 2019 Conan.io\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom conan import ConanFile\nfrom conan.errors import ConanInvalidConfiguration, ConanException\nfrom conan.tools.build import check_min_cppstd, cross_building\nfrom conan.tools.cmake import CMake, CMakeDeps, CMakeToolchain, cmake_layout\nfrom conan.tools.files import apply_conandata_patches, copy, export_conandata_patches, get, rmdir\nfrom conan.tools.microsoft import is_msvc, is_msvc_static_runtime\nfrom conan.tools.scm import Version\n\nimport os\nimport glob\n\nrequired_conan_version = \">=1.53.0\"\n\nclass ArrowConan(ConanFile):\n    name = \"arrow\"\n    description = \"Apache Arrow is a cross-language development platform for in-memory data\"\n    license = (\"Apache-2.0\",)\n    url = \"https://github.com/conan-io/conan-center-index\"\n    homepage = \"https://arrow.apache.org/\"\n    topics = (\"memory\", \"gandiva\", \"parquet\", \"skyhook\", \"acero\", \"hdfs\", \"csv\", \"cuda\", \"gcs\", \"json\", \"hive\", \"s3\", \"grpc\")\n    package_type = \"library\"\n    settings = \"os\", \"arch\", \"compiler\", \"build_type\"\n    options = {\n        \"shared\": [True, False],\n        \"fPIC\": [True, False],\n        \"gandiva\":  [True, False],\n        \"parquet\": [\"auto\", True, False],\n        \"substrait\": [True, False],\n        \"skyhook\": [True, False],\n        \"acero\": [True, False],\n        \"cli\": [True, False],\n        \"compute\": [\"auto\", True, False],\n        \"dataset_modules\":  [\"auto\", True, False],\n        \"deprecated\": [True, False],\n        \"encryption\": [True, False],\n        \"filesystem_layer\":  [True, False],\n        \"hdfs_bridgs\": [True, False],\n        \"plasma\": [True, False, \"deprecated\"],\n        \"simd_level\": [None, \"default\", \"sse4_2\", \"avx2\", \"avx512\", \"neon\", ],\n        \"runtime_simd_level\": [None, \"sse4_2\", \"avx2\", \"avx512\", \"max\"],\n        \"with_backtrace\": [True, False],\n        \"with_boost\": [\"auto\", True, False],\n        \"with_csv\": [True, False],\n        \"with_cuda\": [True, False],\n        \"with_flight_rpc\":  [\"auto\", True, False],\n        \"with_flight_sql\":  [True, False],\n        \"with_gcs\": [True, False],\n        \"with_gflags\": [\"auto\", True, False],\n        \"with_glog\": [\"auto\", True, False],\n        \"with_grpc\": [\"auto\", True, False],\n        \"with_jemalloc\": [\"auto\", True, False],\n        \"with_mimalloc\": [True, False],\n        \"with_json\": [True, False],\n        \"with_thrift\": [\"auto\", True, False],\n        \"with_llvm\": [\"auto\", True, False],\n        \"with_openssl\": [\"auto\", True, False],\n        \"with_opentelemetry\": [True, False],\n        \"with_orc\": [True, False],\n        \"with_protobuf\": [\"auto\", True, False],\n        \"with_re2\": [\"auto\", True, False],\n        \"with_s3\": [True, False],\n        \"with_utf8proc\": [\"auto\", True, False],\n        \"with_brotli\": [True, False],\n        \"with_bz2\": [True, False],\n        \"with_lz4\": [True, False],\n        \"with_snappy\": [True, False],\n        \"with_zlib\": [True, False],\n        \"with_zstd\": [True, False],\n    }\n    default_options = {\n        \"shared\": False,\n        \"fPIC\": True,\n        \"gandiva\": False,\n        \"parquet\": False,\n        \"skyhook\": False,\n        \"substrait\": False,\n        \"acero\": False,\n        \"cli\": False,\n        \"compute\": False,\n        \"dataset_modules\": False,\n        \"deprecated\": True,\n        \"encryption\": False,\n        \"filesystem_layer\": False,\n        \"hdfs_bridgs\": False,\n        \"plasma\": \"deprecated\",\n        \"simd_level\": \"default\",\n        \"runtime_simd_level\": \"max\",\n        \"with_backtrace\": False,\n        \"with_boost\": False,\n        \"with_brotli\": False,\n        \"with_bz2\": False,\n        \"with_csv\": False,\n        \"with_cuda\": False,\n        \"with_flight_rpc\": False,\n        \"with_flight_sql\": False,\n        \"with_gcs\": False,\n        \"with_gflags\": False,\n        \"with_jemalloc\": False,\n        \"with_mimalloc\": False,\n        \"with_glog\": False,\n        \"with_grpc\": False,\n        \"with_json\": False,\n        \"with_thrift\": False,\n        \"with_llvm\": False,\n        \"with_openssl\": False,\n        \"with_opentelemetry\": False,\n        \"with_orc\": False,\n        \"with_protobuf\": False,\n        \"with_re2\": False,\n        \"with_s3\": False,\n        \"with_utf8proc\": False,\n        \"with_lz4\": False,\n        \"with_snappy\": False,\n        \"with_zlib\": False,\n        \"with_zstd\": False,\n    }\n    short_paths = True\n\n    @property\n    def _min_cppstd(self):\n        # arrow >= 10.0.0 requires C++17.\n        # https://github.com/apache/arrow/pull/13991\n        return \"11\" if Version(self.version) < \"10.0.0\" else \"17\"\n\n    @property\n    def _compilers_minimum_version(self):\n        return {\n            \"11\": {\n                \"clang\": \"3.9\",\n            },\n            \"17\": {\n                \"gcc\": \"8\",\n                \"clang\": \"7\",\n                \"apple-clang\": \"10\",\n                \"Visual Studio\": \"15\",\n                \"msvc\": \"191\",\n            },\n        }.get(self._min_cppstd, {})\n\n    def export_sources(self):\n        export_conandata_patches(self)\n        copy(self, \"conan_cmake_project_include.cmake\", self.recipe_folder, os.path.join(self.export_sources_folder, \"src\"))\n\n    def config_options(self):\n        if self.settings.os == \"Windows\":\n            del self.options.fPIC\n        if Version(self.version) < \"8.0.0\":\n            del self.options.substrait\n        if is_msvc(self):\n            self.options.with_boost = True\n\n    def configure(self):\n        if self.options.shared:\n            self.options.rm_safe(\"fPIC\")\n\n    def layout(self):\n        cmake_layout(self, src_folder=\"src\")\n\n    def _requires_rapidjson(self):\n        return self.options.with_json or self.options.encryption\n\n    def requirements(self):\n        if self.options.with_thrift:\n            self.requires(\"thrift/0.17.0\")\n        if self.options.with_protobuf:\n            self.requires(\"protobuf/3.21.9\")\n        if self.options.with_jemalloc:\n            self.requires(\"jemalloc/5.3.0\")\n        if self.options.with_mimalloc:\n            self.requires(\"mimalloc/1.7.6\")\n        if self.options.with_boost:\n            self.requires(\"boost/1.84.0\")\n        if self.options.with_gflags:\n            self.requires(\"gflags/2.2.2\")\n        if self.options.with_glog:\n            self.requires(\"glog/0.6.0\")\n        if self.options.get_safe(\"with_gcs\"):\n            self.requires(\"google-cloud-cpp/1.40.1\")\n        if self.options.with_grpc:\n            self.requires(\"grpc/1.50.0\")\n        if self._requires_rapidjson():\n            self.requires(\"rapidjson/1.1.0\")\n        if self.options.with_llvm:\n            self.requires(\"llvm-core/13.0.0\")\n        if self.options.with_openssl:\n            # aws-sdk-cpp requires openssl/1.1.1. it uses deprecated functions in openssl/3.0.0\n            if self.options.with_s3:\n                self.requires(\"openssl/1.1.1w\")\n            else:\n                self.requires(\"openssl/[>=1.1 <4]\")\n        if self.options.get_safe(\"with_opentelemetry\"):\n            self.requires(\"opentelemetry-cpp/1.7.0\")\n        if self.options.with_s3:\n            self.requires(\"aws-sdk-cpp/1.9.234\")\n        if self.options.with_brotli:\n            self.requires(\"brotli/1.1.0\")\n        if self.options.with_bz2:\n            self.requires(\"bzip2/1.0.8\")\n        if self.options.with_lz4:\n            self.requires(\"lz4/1.9.4\")\n        if self.options.with_snappy:\n            self.requires(\"snappy/1.1.9\")\n        if self.options.get_safe(\"simd_level\") != None or \\\n            self.options.get_safe(\"runtime_simd_level\") != None:\n            self.requires(\"xsimd/9.0.1\")\n        if self.options.with_zlib:\n            self.requires(\"zlib/[>=1.2.11 <2]\")\n        if self.options.with_zstd:\n            self.requires(\"zstd/1.5.5\")\n        if self.options.with_re2:\n            self.requires(\"re2/20230301\")\n        if self.options.with_utf8proc:\n            self.requires(\"utf8proc/2.8.0\")\n        if self.options.with_backtrace:\n            self.requires(\"libbacktrace/cci.20210118\")\n\n    def validate(self):\n        # Do not allow options with 'auto' value\n        # TODO: Remove \"auto\" from the possible values for these options\n        auto_options = [option for option, value in self.options.items() if value == \"auto\"]\n        if auto_options:\n            raise ConanException(\"Options with value 'auto' are deprecated. Please set them true/false or use its default value.\"\n                                 f\" Please change the following options: {auto_options}\")\n\n        # From https://github.com/conan-io/conan-center-index/pull/23163#issuecomment-2039808851\n        if self.options.gandiva:\n            if not self.options.with_re2:\n                raise ConanException(\"'with_re2' option should be True when'gandiva=True'\")\n            if not self.options.with_boost:\n                raise ConanException(\"'with_boost' option should be True when'gandiva=True'\")\n            if not self.options.with_utf8proc:\n                raise ConanException(\"'with_utf8proc' option should be True when'gandiva=True'\")\n\n        if self.settings.compiler.get_safe(\"cppstd\"):\n            check_min_cppstd(self, self._min_cppstd)\n\n        minimum_version = self._compilers_minimum_version.get(str(self.settings.compiler), False)\n        if minimum_version and Version(self.settings.compiler.version) < minimum_version:\n            raise ConanInvalidConfiguration(\n                f\"{self.ref} requires C++{self._min_cppstd}, which your compiler does not support.\"\n            )\n\n        if self.options.get_safe(\"skyhook\", False):\n            raise ConanInvalidConfiguration(\"CCI has no librados recipe (yet)\")\n        if self.options.with_cuda:\n            raise ConanInvalidConfiguration(\"CCI has no cuda recipe (yet)\")\n        if self.options.with_orc:\n            raise ConanInvalidConfiguration(\"CCI has no orc recipe (yet)\")\n        if self.options.with_s3 and not self.dependencies[\"aws-sdk-cpp\"].options.config:\n            raise ConanInvalidConfiguration(\"arrow:with_s3 requires aws-sdk-cpp:config is True.\")\n\n        if self.options.shared and self.options.with_jemalloc:\n            if self.dependencies[\"jemalloc\"].options.enable_cxx:\n                raise ConanInvalidConfiguration(\"jemmalloc.enable_cxx of a static jemalloc must be disabled\")\n\n\n    def build_requirements(self):\n        if Version(self.version) >= \"13.0.0\":\n            self.tool_requires(\"cmake/[>=3.16 <4]\")\n\n    def source(self):\n        # START\n        # This block should be removed when we update upstream:\n        # https://github.com/conan-io/conan-center-index/tree/master/recipes/arrow/\n        if not self.version in self.conan_data.get(\"sources\", {}):\n            import shutil\n            top_level = os.environ.get(\"ARROW_HOME\")\n            shutil.copytree(os.path.join(top_level, \"cpp\"),\n                            os.path.join(self.source_folder, \"cpp\"))\n            shutil.copytree(os.path.join(top_level, \"format\"),\n                            os.path.join(self.source_folder, \"format\"))\n            top_level_files = [\n                \".env\",\n                \"LICENSE.txt\",\n                \"NOTICE.txt\",\n            ]\n            for top_level_file in top_level_files:\n                shutil.copy(os.path.join(top_level, top_level_file),\n                            self.source_folder)\n            return\n        # END\n        get(self, **self.conan_data[\"sources\"][self.version],\n            filename=f\"apache-arrow-{self.version}.tar.gz\", strip_root=True)\n\n    def generate(self):\n        tc = CMakeToolchain(self)\n        if cross_building(self):\n            cmake_system_processor = {\n                \"armv8\": \"aarch64\",\n                \"armv8.3\": \"aarch64\",\n            }.get(str(self.settings.arch), str(self.settings.arch))\n            if cmake_system_processor == \"aarch64\":\n                tc.variables[\"ARROW_CPU_FLAG\"] = \"armv8\"\n        if is_msvc(self):\n            tc.variables[\"ARROW_USE_STATIC_CRT\"] = is_msvc_static_runtime(self)\n        tc.variables[\"ARROW_DEPENDENCY_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_PACKAGE_KIND\"] = \"conan\" # See https://github.com/conan-io/conan-center-index/pull/14903/files#r1057938314 for details\n        tc.variables[\"ARROW_GANDIVA\"] = bool(self.options.gandiva)\n        tc.variables[\"ARROW_PARQUET\"] = self.options.parquet\n        tc.variables[\"ARROW_SUBSTRAIT\"] = bool(self.options.get_safe(\"substrait\", False))\n        tc.variables[\"ARROW_ACERO\"] = bool(self.options.acero)\n        tc.variables[\"ARROW_DATASET\"] = self.options.dataset_modules\n        tc.variables[\"ARROW_FILESYSTEM\"] = bool(self.options.filesystem_layer)\n        tc.variables[\"PARQUET_REQUIRE_ENCRYPTION\"] = bool(self.options.encryption)\n        tc.variables[\"ARROW_HDFS\"] = bool(self.options.hdfs_bridgs)\n        tc.variables[\"ARROW_VERBOSE_THIRDPARTY_BUILD\"] = True\n        tc.variables[\"ARROW_BUILD_SHARED\"] = bool(self.options.shared)\n        tc.variables[\"ARROW_BUILD_STATIC\"] = not bool(self.options.shared)\n        tc.variables[\"ARROW_NO_DEPRECATED_API\"] = not bool(self.options.deprecated)\n        tc.variables[\"ARROW_FLIGHT\"] = self.options.with_flight_rpc\n        tc.variables[\"ARROW_FLIGHT_SQL\"] = bool(self.options.get_safe(\"with_flight_sql\", False))\n        tc.variables[\"ARROW_COMPUTE\"] = bool(self.options.compute)\n        tc.variables[\"ARROW_CSV\"] = bool(self.options.with_csv)\n        tc.variables[\"ARROW_CUDA\"] = bool(self.options.with_cuda)\n        tc.variables[\"ARROW_JEMALLOC\"] = self.options.with_jemalloc\n        tc.variables[\"jemalloc_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_MIMALLOC\"] = bool(self.options.with_mimalloc)\n        tc.variables[\"ARROW_JSON\"] = bool(self.options.with_json)\n        tc.variables[\"google_cloud_cpp_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_GCS\"] = bool(self.options.get_safe(\"with_gcs\", False))\n        tc.variables[\"BOOST_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"Protobuf_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_protobuf:\n            tc.variables[\"ARROW_PROTOBUF_USE_SHARED\"] = bool(self.dependencies[\"protobuf\"].options.shared)\n        tc.variables[\"gRPC_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_grpc:\n            tc.variables[\"ARROW_GRPC_USE_SHARED\"] = bool(self.dependencies[\"grpc\"].options.shared)\n\n        tc.variables[\"ARROW_USE_GLOG\"] = self.options.with_glog\n        tc.variables[\"GLOG_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_WITH_BACKTRACE\"] = bool(self.options.with_backtrace)\n        tc.variables[\"ARROW_WITH_BROTLI\"] = bool(self.options.with_brotli)\n        tc.variables[\"brotli_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_brotli:\n            tc.variables[\"ARROW_BROTLI_USE_SHARED\"] = bool(self.dependencies[\"brotli\"].options.shared)\n        tc.variables[\"gflags_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_gflags:\n            tc.variables[\"ARROW_GFLAGS_USE_SHARED\"] = bool(self.dependencies[\"gflags\"].options.shared)\n        tc.variables[\"ARROW_WITH_BZ2\"] = bool(self.options.with_bz2)\n        tc.variables[\"BZip2_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_bz2:\n            tc.variables[\"ARROW_BZ2_USE_SHARED\"] = bool(self.dependencies[\"bzip2\"].options.shared)\n        tc.variables[\"ARROW_WITH_LZ4\"] = bool(self.options.with_lz4)\n        tc.variables[\"lz4_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_lz4:\n            tc.variables[\"ARROW_LZ4_USE_SHARED\"] = bool(self.dependencies[\"lz4\"].options.shared)\n        tc.variables[\"ARROW_WITH_SNAPPY\"] = bool(self.options.with_snappy)\n        tc.variables[\"RapidJSON_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"Snappy_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_snappy:\n            tc.variables[\"ARROW_SNAPPY_USE_SHARED\"] = bool(self.dependencies[\"snappy\"].options.shared)\n        tc.variables[\"ARROW_WITH_ZLIB\"] = bool(self.options.with_zlib)\n        tc.variables[\"re2_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ZLIB_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"xsimd_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_WITH_ZSTD\"] = bool(self.options.with_zstd)\n        tc.variables[\"zstd_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_SIMD_LEVEL\"] = str(self.options.simd_level).upper()\n        tc.variables[\"ARROW_RUNTIME_SIMD_LEVEL\"] = str(self.options.runtime_simd_level).upper()\n        if self.options.with_zstd:\n            tc.variables[\"ARROW_ZSTD_USE_SHARED\"] = bool(self.dependencies[\"zstd\"].options.shared)\n        tc.variables[\"ORC_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_WITH_THRIFT\"] = bool(self.options.with_thrift)\n        tc.variables[\"Thrift_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_thrift:\n            tc.variables[\"THRIFT_VERSION\"] = bool(self.dependencies[\"thrift\"].ref.version) # a recent thrift does not require boost\n            tc.variables[\"ARROW_THRIFT_USE_SHARED\"] = bool(self.dependencies[\"thrift\"].options.shared)\n        tc.variables[\"ARROW_USE_OPENSSL\"] = self.options.with_openssl\n        if self.options.with_openssl:\n            tc.variables[\"OPENSSL_ROOT_DIR\"] = self.dependencies[\"openssl\"].package_folder.replace(\"\\\\\", \"/\")\n            tc.variables[\"ARROW_OPENSSL_USE_SHARED\"] = bool(self.dependencies[\"openssl\"].options.shared)\n        if self.options.with_boost:\n            tc.variables[\"ARROW_USE_BOOST\"] = True\n            tc.variables[\"ARROW_BOOST_USE_SHARED\"] = bool(self.dependencies[\"boost\"].options.shared)\n        tc.variables[\"ARROW_S3\"] = bool(self.options.with_s3)\n        tc.variables[\"AWSSDK_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_BUILD_UTILITIES\"] = bool(self.options.cli)\n        tc.variables[\"ARROW_BUILD_INTEGRATION\"] = False\n        tc.variables[\"ARROW_INSTALL_NAME_RPATH\"] = True\n        tc.variables[\"ARROW_BUILD_EXAMPLES\"] = False\n        tc.variables[\"ARROW_BUILD_TESTS\"] = False\n        tc.variables[\"ARROW_ENABLE_TIMING_TESTS\"] = False\n        tc.variables[\"ARROW_BUILD_BENCHMARKS\"] = False\n        tc.variables[\"LLVM_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_WITH_UTF8PROC\"] = self.options.with_utf8proc\n        tc.variables[\"ARROW_BOOST_REQUIRED\"] = self.options.with_boost\n        tc.variables[\"utf8proc_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_utf8proc:\n            tc.variables[\"ARROW_UTF8PROC_USE_SHARED\"] = bool(self.dependencies[\"utf8proc\"].options.shared)\n        tc.variables[\"BUILD_WARNING_LEVEL\"] = \"PRODUCTION\"\n        if is_msvc(self):\n            tc.variables[\"ARROW_USE_STATIC_CRT\"] = is_msvc_static_runtime(self)\n        if self.options.with_llvm:\n            tc.variables[\"LLVM_DIR\"] = self.dependencies[\"llvm-core\"].package_folder.replace(\"\\\\\", \"/\")\n\n        tc.cache_variables[\"CMAKE_PROJECT_arrow_INCLUDE\"] = os.path.join(self.source_folder, \"conan_cmake_project_include.cmake\")\n        tc.generate()\n\n        deps = CMakeDeps(self)\n        deps.generate()\n\n    def _patch_sources(self):\n        apply_conandata_patches(self)\n        if Version(self.version) < \"10.0.0\":\n            for filename in glob.glob(os.path.join(self.source_folder, \"cpp\", \"cmake_modules\", \"Find*.cmake\")):\n                if os.path.basename(filename) not in [\n                    \"FindArrow.cmake\",\n                    \"FindArrowAcero.cmake\",\n                    \"FindArrowCUDA.cmake\",\n                    \"FindArrowDataset.cmake\",\n                    \"FindArrowFlight.cmake\",\n                    \"FindArrowFlightSql.cmake\",\n                    \"FindArrowFlightTesting.cmake\",\n                    \"FindArrowPython.cmake\",\n                    \"FindArrowPythonFlight.cmake\",\n                    \"FindArrowSubstrait.cmake\",\n                    \"FindArrowTesting.cmake\",\n                    \"FindGandiva.cmake\",\n                    \"FindParquet.cmake\",\n                ]:\n                    os.remove(filename)\n\n    def build(self):\n        self._patch_sources()\n        cmake =CMake(self)\n        cmake.configure(build_script_folder=os.path.join(self.source_folder, \"cpp\"))\n        cmake.build()\n\n    def package(self):\n        copy(self, pattern=\"LICENSE.txt\", dst=os.path.join(self.package_folder, \"licenses\"), src=self.source_folder)\n        copy(self, pattern=\"NOTICE.txt\", dst=os.path.join(self.package_folder, \"licenses\"), src=self.source_folder)\n        cmake =CMake(self)\n        cmake.install()\n\n        rmdir(self, os.path.join(self.package_folder, \"lib\", \"cmake\"))\n        rmdir(self, os.path.join(self.package_folder, \"lib\", \"pkgconfig\"))\n        rmdir(self, os.path.join(self.package_folder, \"share\"))\n\n    def package_info(self):\n        # FIXME: fix CMake targets of components\n\n        self.cpp_info.set_property(\"cmake_file_name\", \"Arrow\")\n\n        suffix = \"_static\" if is_msvc(self) and not self.options.shared else \"\"\n\n        self.cpp_info.components[\"libarrow\"].set_property(\"pkg_config_name\", \"arrow\")\n        self.cpp_info.components[\"libarrow\"].libs = [f\"arrow{suffix}\"]\n        if not self.options.shared:\n            self.cpp_info.components[\"libarrow\"].defines = [\"ARROW_STATIC\"]\n            if self.settings.os in [\"Linux\", \"FreeBSD\"]:\n                self.cpp_info.components[\"libarrow\"].system_libs = [\"pthread\", \"m\", \"dl\", \"rt\"]\n\n        if self.options.parquet:\n            self.cpp_info.components[\"libparquet\"].set_property(\"pkg_config_name\", \"parquet\")\n            self.cpp_info.components[\"libparquet\"].libs = [f\"parquet{suffix}\"]\n            self.cpp_info.components[\"libparquet\"].requires = [\"libarrow\"]\n            if not self.options.shared:\n                self.cpp_info.components[\"libparquet\"].defines = [\"PARQUET_STATIC\"]\n\n        if self.options.get_safe(\"substrait\"):\n            self.cpp_info.components[\"libarrow_substrait\"].set_property(\"pkg_config_name\", \"arrow_substrait\")\n            self.cpp_info.components[\"libarrow_substrait\"].libs = [f\"arrow_substrait{suffix}\"]\n            self.cpp_info.components[\"libarrow_substrait\"].requires = [\"libparquet\", \"dataset\"]\n\n        # Plasma was deprecated in Arrow 12.0.0\n        del self.options.plasma\n\n        if self.options.acero:\n            self.cpp_info.components[\"libacero\"].libs = [f\"arrow_acero{suffix}\"]\n            self.cpp_info.components[\"libacero\"].names[\"cmake_find_package\"] = \"acero\"\n            self.cpp_info.components[\"libacero\"].names[\"cmake_find_package_multi\"] = \"acero\"\n            self.cpp_info.components[\"libacero\"].names[\"pkg_config\"] = \"acero\"\n            self.cpp_info.components[\"libacero\"].requires = [\"libarrow\"]\n\n        if self.options.gandiva:\n            self.cpp_info.components[\"libgandiva\"].set_property(\"pkg_config_name\", \"gandiva\")\n            self.cpp_info.components[\"libgandiva\"].libs = [f\"gandiva{suffix}\"]\n            self.cpp_info.components[\"libgandiva\"].requires = [\"libarrow\"]\n            if not self.options.shared:\n                self.cpp_info.components[\"libgandiva\"].defines = [\"GANDIVA_STATIC\"]\n\n        if self.options.with_flight_rpc:\n            self.cpp_info.components[\"libarrow_flight\"].set_property(\"pkg_config_name\", \"flight_rpc\")\n            self.cpp_info.components[\"libarrow_flight\"].libs = [f\"arrow_flight{suffix}\"]\n            self.cpp_info.components[\"libarrow_flight\"].requires = [\"libarrow\"]\n\n        if self.options.get_safe(\"with_flight_sql\"):\n            self.cpp_info.components[\"libarrow_flight_sql\"].set_property(\"pkg_config_name\", \"flight_sql\")\n            self.cpp_info.components[\"libarrow_flight_sql\"].libs = [f\"arrow_flight_sql{suffix}\"]\n            self.cpp_info.components[\"libarrow_flight_sql\"].requires = [\"libarrow\", \"libarrow_flight\"]\n\n        if self.options.dataset_modules:\n            self.cpp_info.components[\"dataset\"].libs = [\"arrow_dataset\"]\n            if self.options.parquet:\n                self.cpp_info.components[\"dataset\"].requires = [\"libparquet\"]\n\n        if self.options.cli and (self.options.with_cuda or self.options.with_flight_rpc or self.options.parquet):\n            binpath = os.path.join(self.package_folder, \"bin\")\n            self.output.info(f\"Appending PATH env var: {binpath}\")\n            self.env_info.PATH.append(binpath)\n\n        if self.options.with_boost:\n            if self.options.gandiva:\n                # FIXME: only filesystem component is used\n                self.cpp_info.components[\"libgandiva\"].requires.append(\"boost::boost\")\n            if self.options.parquet and self.settings.compiler == \"gcc\" and self.settings.compiler.version < Version(\"4.9\"):\n                self.cpp_info.components[\"libparquet\"].requires.append(\"boost::boost\")\n            # FIXME: only headers components is used\n            self.cpp_info.components[\"libarrow\"].requires.append(\"boost::boost\")\n        if self.options.with_openssl:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"openssl::openssl\")\n        if self.options.with_gflags:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"gflags::gflags\")\n        if self.options.with_glog:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"glog::glog\")\n        if self.options.with_jemalloc:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"jemalloc::jemalloc\")\n        if self.options.with_mimalloc:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"mimalloc::mimalloc\")\n        if self.options.with_re2:\n            if self.options.gandiva:\n                self.cpp_info.components[\"libgandiva\"].requires.append(\"re2::re2\")\n            if self.options.parquet:\n                self.cpp_info.components[\"libparquet\"].requires.append(\"re2::re2\")\n            self.cpp_info.components[\"libarrow\"].requires.append(\"re2::re2\")\n        if self.options.with_llvm:\n            self.cpp_info.components[\"libgandiva\"].requires.append(\"llvm-core::llvm-core\")\n        if self.options.with_protobuf:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"protobuf::protobuf\")\n        if self.options.with_utf8proc:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"utf8proc::utf8proc\")\n        if self.options.with_thrift:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"thrift::thrift\")\n        if self.options.with_backtrace:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"libbacktrace::libbacktrace\")\n        if self.options.with_cuda:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"cuda::cuda\")\n        if self._requires_rapidjson():\n            self.cpp_info.components[\"libarrow\"].requires.append(\"rapidjson::rapidjson\")\n        if self.options.with_s3:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"aws-sdk-cpp::s3\")\n        if self.options.get_safe(\"with_gcs\"):\n            self.cpp_info.components[\"libarrow\"].requires.append(\"google-cloud-cpp::storage\")\n        if self.options.with_orc:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"orc::orc\")\n        if self.options.get_safe(\"with_opentelemetry\"):\n            self.cpp_info.components[\"libarrow\"].requires.append(\"opentelemetry-cpp::opentelemetry-cpp\")\n        if self.options.with_brotli:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"brotli::brotli\")\n        if self.options.with_bz2:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"bzip2::bzip2\")\n        if self.options.with_lz4:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"lz4::lz4\")\n        if self.options.with_snappy:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"snappy::snappy\")\n        if self.options.get_safe(\"simd_level\") != None or self.options.get_safe(\"runtime_simd_level\") != None:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"xsimd::xsimd\")\n        if self.options.with_zlib:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"zlib::zlib\")\n        if self.options.with_zstd:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"zstd::zstd\")\n        if self.options.with_boost:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"boost::boost\")\n        if self.options.with_grpc:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"grpc::grpc\")\n        if self.options.with_flight_rpc:\n            self.cpp_info.components[\"libarrow_flight\"].requires.append(\"protobuf::protobuf\")\n\n        # TODO: to remove in conan v2\n        self.cpp_info.filenames[\"cmake_find_package\"] = \"Arrow\"\n        self.cpp_info.filenames[\"cmake_find_package_multi\"] = \"Arrow\"\n        self.cpp_info.components[\"libarrow\"].names[\"cmake_find_package\"] = \"arrow\"\n        self.cpp_info.components[\"libarrow\"].names[\"cmake_find_package_multi\"] = \"arrow\"\n        if self.options.parquet:\n            self.cpp_info.components[\"libparquet\"].names[\"cmake_find_package\"] = \"parquet\"\n            self.cpp_info.components[\"libparquet\"].names[\"cmake_find_package_multi\"] = \"parquet\"\n        if self.options.get_safe(\"substrait\"):\n            self.cpp_info.components[\"libarrow_substrait\"].names[\"cmake_find_package\"] = \"arrow_substrait\"\n            self.cpp_info.components[\"libarrow_substrait\"].names[\"cmake_find_package_multi\"] = \"arrow_substrait\"\n        if self.options.gandiva:\n            self.cpp_info.components[\"libgandiva\"].names[\"cmake_find_package\"] = \"gandiva\"\n            self.cpp_info.components[\"libgandiva\"].names[\"cmake_find_package_multi\"] = \"gandiva\"\n        if self.options.with_flight_rpc:\n            self.cpp_info.components[\"libarrow_flight\"].names[\"cmake_find_package\"] = \"flight_rpc\"\n            self.cpp_info.components[\"libarrow_flight\"].names[\"cmake_find_package_multi\"] = \"flight_rpc\"\n        if self.options.get_safe(\"with_flight_sql\"):\n            self.cpp_info.components[\"libarrow_flight_sql\"].names[\"cmake_find_package\"] = \"flight_sql\"\n            self.cpp_info.components[\"libarrow_flight_sql\"].names[\"cmake_find_package_multi\"] = \"flight_sql\"\n        if self.options.cli and (self.options.with_cuda or self.options.with_flight_rpc or self.options.parquet):\n            self.env_info.PATH.append(os.path.join(self.package_folder, \"bin\"))\n", "ci/conan/all/test_v1_package/conanfile.py": "# MIT License\n#\n# Copyright (c) 2019 Conan.io\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom conans import ConanFile, CMake\nfrom conan.tools.build import cross_building\nimport os\n\n\nclass TestPackageV1Conan(ConanFile):\n    settings = \"os\", \"arch\", \"compiler\", \"build_type\"\n    generators = \"cmake\", \"cmake_find_package_multi\"\n\n    def build(self):\n        cmake = CMake(self)\n        cmake.configure()\n        cmake.build()\n\n    def test(self):\n        if not cross_building(self):\n            bin_path = os.path.join(\"bin\", \"test_package\")\n            self.run(bin_path, run_environment=True)\n", "ci/conan/all/test_package/conanfile.py": "# MIT License\n#\n# Copyright (c) 2019 Conan.io\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom conan import ConanFile\nfrom conan.tools.build import can_run\nfrom conan.tools.cmake import cmake_layout, CMake\nimport os\n\n\n# It will become the standard on Conan 2.x\nclass TestPackageConan(ConanFile):\n    settings = \"os\", \"arch\", \"compiler\", \"build_type\"\n    generators = \"CMakeDeps\", \"CMakeToolchain\", \"VirtualRunEnv\"\n    test_type = \"explicit\"\n\n    def requirements(self):\n        self.requires(self.tested_reference_str)\n\n    def layout(self):\n        cmake_layout(self)\n\n    def build(self):\n        cmake = CMake(self)\n        cmake.configure()\n        cmake.build()\n\n    def test(self):\n        if can_run(self):\n            bin_path = os.path.join(self.cpp.build.bindirs[0], \"test_package\")\n            self.run(bin_path, env=\"conanrun\")\n"}