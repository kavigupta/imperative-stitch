{"cmake-format.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# cmake-format configuration file\n# Use `archery lint --cmake-format --fix` to reformat all cmake files in the\n# source tree\n\n# -----------------------------\n# Options affecting formatting.\n# -----------------------------\nwith section(\"format\"):\n    # How wide to allow formatted cmake files\n    line_width = 90\n\n    # How many spaces to tab for indent\n    tab_size = 2\n\n    # If a positional argument group contains more than this many arguments,\n    # then force it to a vertical layout.\n    max_pargs_hwrap = 4\n\n    # If the statement spelling length (including space and parenthesis) is\n    # smaller than this amount, then force reject nested layouts.\n    # This value only comes into play when considering whether or not to nest\n    # arguments below their parent. If the number of characters in the parent\n    # is less than this value, we will not nest.\n    min_prefix_chars = 32\n\n    # If true, separate flow control names from their parentheses with a space\n    separate_ctrl_name_with_space = False\n\n    # If true, separate function names from parentheses with a space\n    separate_fn_name_with_space = False\n\n    # If a statement is wrapped to more than one line, than dangle the closing\n    # parenthesis on it's own line\n    dangle_parens = False\n\n    # What style line endings to use in the output.\n    line_ending = 'unix'\n\n    # Format command names consistently as 'lower' or 'upper' case\n    command_case = 'lower'\n\n    # Format keywords consistently as 'lower' or 'upper' case\n    keyword_case = 'unchanged'\n\n# ------------------------------------------------\n# Options affecting comment reflow and formatting.\n# ------------------------------------------------\nwith section(\"markup\"):\n    # enable comment markup parsing and reflow\n    enable_markup = False\n\n    # If comment markup is enabled, don't reflow the first comment block in\n    # eachlistfile. Use this to preserve formatting of your\n    # copyright/licensestatements.\n    first_comment_is_literal = True\n\n    # If comment markup is enabled, don't reflow any comment block which\n    # matches this (regex) pattern. Default is `None` (disabled).\n    literal_comment_pattern = None\n", "dev/merge_arrow_pr.py": "#!/usr/bin/env python3\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Utility for creating well-formed pull request merges and pushing them to\n# Apache.\n#   usage: ./merge_arrow_pr.py  <pr-number>  (see config env vars below)\n#\n# This utility assumes:\n#   - you already have a local Arrow git clone\n#   - you have added remotes corresponding to both:\n#       (i) the GitHub Apache Arrow mirror\n#       (ii) the Apache git repo\n#\n# There are several pieces of authorization possibly needed via environment\n# variables.\n#\n# Configuration environment variables:\n#   - APACHE_JIRA_TOKEN: your Apache JIRA Personal Access Token\n#   - ARROW_GITHUB_API_TOKEN: a GitHub API token to use for API requests\n#   - ARROW_GITHUB_ORG: the GitHub organisation ('apache' by default)\n#   - DEBUG: use for testing to avoid pushing to apache (0 by default)\n\nimport configparser\nimport os\nimport pprint\nimport re\nimport subprocess\nimport sys\nimport requests\nimport getpass\n\nfrom six.moves import input\nimport six\n\ntry:\n    import jira.client\n    import jira.exceptions\nexcept ImportError:\n    print(\"Could not find jira library. \"\n          \"Run 'pip install jira' to install.\")\n    print(\"Exiting without trying to close the associated JIRA.\")\n    sys.exit(1)\n\n# Remote name which points to the GitHub site\nORG_NAME = (\n    os.environ.get(\"ARROW_GITHUB_ORG\") or\n    os.environ.get(\"PR_REMOTE_NAME\") or  # backward compatibility\n    \"apache\"\n)\nPROJECT_NAME = os.environ.get('ARROW_PROJECT_NAME') or \"arrow\"\n\n# For testing to avoid accidentally pushing to apache\nDEBUG = bool(int(os.environ.get(\"DEBUG\", 0)))\n\nif DEBUG:\n    print(\"**************** DEBUGGING ****************\")\n\n\nJIRA_API_BASE = \"https://issues.apache.org/jira\"\n\n\ndef get_json(url, headers=None):\n    response = requests.get(url, headers=headers)\n    if response.status_code != 200:\n        raise ValueError(response.json())\n    # GitHub returns a link header with the next, previous, last\n    # page if there is pagination on the response. See:\n    # https://docs.github.com/en/rest/guides/using-pagination-in-the-rest-api#using-link-headers\n    next_responses = None\n    if \"link\" in response.headers:\n        links = response.headers['link'].split(', ')\n        for link in links:\n            if 'rel=\"next\"' in link:\n                # Format: '<url>; rel=\"next\"'\n                next_url = link.split(\";\")[0][1:-1]\n                next_responses = get_json(next_url, headers)\n    responses = response.json()\n    if next_responses:\n        if isinstance(responses, list):\n            responses.extend(next_responses)\n        else:\n            raise ValueError('GitHub response was paginated and is not a list')\n    return responses\n\n\ndef run_cmd(cmd):\n    if isinstance(cmd, six.string_types):\n        cmd = cmd.split(' ')\n\n    try:\n        output = subprocess.check_output(cmd)\n    except subprocess.CalledProcessError as e:\n        # this avoids hiding the stdout / stderr of failed processes\n        print('Command failed: %s' % cmd)\n        print('With output:')\n        print('--------------')\n        print(e.output)\n        print('--------------')\n        raise e\n\n    if isinstance(output, six.binary_type):\n        output = output.decode('utf-8')\n    return output\n\n\n_REGEX_CI_DIRECTIVE = re.compile(r'\\[[^\\]]*\\]')\n\n\ndef strip_ci_directives(commit_message):\n    # Remove things like '[force ci]', '[skip appveyor]' from the assembled\n    # commit message\n    return _REGEX_CI_DIRECTIVE.sub('', commit_message)\n\n\ndef fix_version_from_branch(versions):\n    # Note: Assumes this is a sorted (newest->oldest) list of un-released\n    # versions\n    return versions[-1]\n\n\nMIGRATION_COMMENT_REGEX = re.compile(\n    r\"This issue has been migrated to \\[issue #(?P<issue_id>(\\d+))\"\n)\n\n\nclass JiraIssue(object):\n\n    def __init__(self, jira_con, jira_id, project, cmd):\n        self.jira_con = jira_con\n        self.jira_id = jira_id\n        self.project = project\n        self.cmd = cmd\n\n        try:\n            self.issue = jira_con.issue(jira_id)\n        except Exception as e:\n            self.cmd.fail(\"ASF JIRA could not find %s\\n%s\" % (jira_id, e))\n\n    @property\n    def current_fix_versions(self):\n        return self.issue.fields.fixVersions\n\n    @property\n    def current_versions(self):\n        # Only suggest versions starting with a number, like 0.x but not JS-0.x\n        all_versions = self.jira_con.project_versions(self.project)\n        unreleased_versions = [x for x in all_versions\n                               if not x.raw['released']]\n\n        mainline_versions = self._filter_mainline_versions(unreleased_versions)\n        return mainline_versions\n\n    def _filter_mainline_versions(self, versions):\n        if self.project == 'PARQUET':\n            mainline_regex = re.compile(r'cpp-\\d.*')\n        else:\n            mainline_regex = re.compile(r'\\d.*')\n\n        return [x for x in versions if mainline_regex.match(x.name)]\n\n    def resolve(self, fix_version, comment, *args):\n        fields = self.issue.fields\n        cur_status = fields.status.name\n\n        if cur_status == \"Resolved\" or cur_status == \"Closed\":\n            self.cmd.fail(\"JIRA issue %s already has status '%s'\"\n                          % (self.jira_id, cur_status))\n\n        resolve = [x for x in self.jira_con.transitions(self.jira_id)\n                   if x['name'] == \"Resolve Issue\"][0]\n\n        # ARROW-6915: do not overwrite existing fix versions corresponding to\n        # point releases\n        fix_versions = [v.raw for v in self.jira_con.project_versions(\n            self.project) if v.name == fix_version]\n        fix_version_names = set(x['name'] for x in fix_versions)\n        for version in self.current_fix_versions:\n            major, minor, patch = version.name.split('.')\n            if patch != '0' and version.name not in fix_version_names:\n                fix_versions.append(version.raw)\n\n        if DEBUG:\n            print(\"JIRA issue %s untouched -> %s\" %\n                  (self.jira_id, [v[\"name\"] for v in fix_versions]))\n        else:\n            self.jira_con.transition_issue(self.jira_id, resolve[\"id\"],\n                                           comment=comment,\n                                           fixVersions=fix_versions)\n            print(\"Successfully resolved %s!\" % (self.jira_id))\n\n        self.issue = self.jira_con.issue(self.jira_id)\n        self.show()\n\n    def show(self):\n        fields = self.issue.fields\n        print(format_issue_output(\"jira\", self.jira_id, fields.status.name,\n                                  fields.summary, fields.assignee,\n                                  fields.components))\n\n    def github_issue_id(self):\n        try:\n            last_jira_comment = self.issue.fields.comment.comments[-1].body\n        except Exception:\n            # If no comment found or other issues ignore\n            return None\n        matches = MIGRATION_COMMENT_REGEX.search(last_jira_comment)\n        if matches:\n            values = matches.groupdict()\n            return \"GH-\" + values['issue_id']\n\n\nclass GitHubIssue(object):\n\n    def __init__(self, github_api, github_id, cmd):\n        self.github_api = github_api\n        self.github_id = github_id\n        self.cmd = cmd\n\n        try:\n            self.issue = self.github_api.get_issue_data(github_id)\n        except Exception as e:\n            self.cmd.fail(\"GitHub could not find %s\\n%s\" % (github_id, e))\n\n    def get_label(self, prefix):\n        prefix = f\"{prefix}:\"\n        return [\n            lbl[\"name\"][len(prefix):].strip()\n            for lbl in self.issue[\"labels\"] if lbl[\"name\"].startswith(prefix)\n        ]\n\n    @property\n    def components(self):\n        return self.get_label(\"Component\")\n\n    @property\n    def assignees(self):\n        return [a[\"login\"] for a in self.issue[\"assignees\"]]\n\n    @property\n    def current_fix_versions(self):\n        try:\n            return self.issue.get(\"milestone\", {}).get(\"title\")\n        except AttributeError:\n            pass\n\n    @property\n    def current_versions(self):\n        all_versions = self.github_api.get_milestones()\n\n        unreleased_versions = [x for x in all_versions if x[\"state\"] == \"open\"]\n        unreleased_versions = [x[\"title\"] for x in unreleased_versions]\n\n        return unreleased_versions\n\n    def resolve(self, fix_version, comment, pr_body):\n        cur_status = self.issue[\"state\"]\n\n        if cur_status == \"closed\":\n            self.cmd.fail(\"GitHub issue %s already has status '%s'\"\n                          % (self.github_id, cur_status))\n\n        if DEBUG:\n            print(\"GitHub issue %s untouched -> %s\" %\n                  (self.github_id, fix_version))\n        else:\n            self.github_api.assign_milestone(self.github_id, fix_version)\n            if f\"Closes: #{self.github_id}\" not in pr_body:\n                self.github_api.close_issue(self.github_id, comment)\n            print(\"Successfully resolved %s!\" % (self.github_id))\n\n        self.issue = self.github_api.get_issue_data(self.github_id)\n        self.show()\n\n    def show(self):\n        issue = self.issue\n        print(format_issue_output(\"github\", self.github_id, issue[\"state\"],\n                                  issue[\"title\"], ', '.join(self.assignees),\n                                  self.components))\n\n\ndef get_candidate_fix_version(mainline_versions,\n                              maintenance_branches=()):\n\n    all_versions = [getattr(v, \"name\", v) for v in mainline_versions]\n\n    def version_tuple(x):\n        # Parquet versions are something like cpp-1.2.0\n        numeric_version = getattr(x, \"name\", x).split(\"-\", 1)[-1]\n        return tuple(int(_) for _ in numeric_version.split(\".\"))\n    all_versions = sorted(all_versions, key=version_tuple, reverse=True)\n\n    # Only suggest versions starting with a number, like 0.x but not JS-0.x\n    mainline_versions = all_versions\n    major_versions = [v for v in mainline_versions if v.endswith('.0.0')]\n\n    if len(mainline_versions) > len(major_versions):\n        # If there is a future major release, suggest that\n        mainline_versions = major_versions\n\n    mainline_versions = [v for v in mainline_versions\n                         if f\"maint-{v}\" not in maintenance_branches]\n    default_fix_versions = fix_version_from_branch(mainline_versions)\n\n    return default_fix_versions\n\n\ndef format_issue_output(issue_type, issue_id, status,\n                        summary, assignee, components):\n    if not assignee:\n        assignee = \"NOT ASSIGNED!!!\"\n    else:\n        assignee = getattr(assignee, \"displayName\", assignee)\n\n    if len(components) == 0:\n        components = 'NO COMPONENTS!!!'\n    else:\n        components = ', '.join((getattr(x, \"name\", x) for x in components))\n\n    if issue_type == \"jira\":\n        url = '/'.join((JIRA_API_BASE, 'browse', issue_id))\n    else:\n        url = (\n            f'https://github.com/{ORG_NAME}/{PROJECT_NAME}/issues/{issue_id}'\n        )\n\n    return \"\"\"=== {} {} ===\nSummary\\t\\t{}\nAssignee\\t{}\nComponents\\t{}\nStatus\\t\\t{}\nURL\\t\\t{}\"\"\".format(issue_type.upper(), issue_id, summary, assignee,\n                    components, status, url)\n\n\nclass GitHubAPI(object):\n\n    def __init__(self, project_name, cmd):\n        self.github_api = (\n            f\"https://api.github.com/repos/{ORG_NAME}/{project_name}\"\n        )\n\n        token = None\n        config = load_configuration()\n        if \"github\" in config.sections():\n            token = config[\"github\"][\"api_token\"]\n        if not token:\n            token = os.environ.get('ARROW_GITHUB_API_TOKEN')\n        if not token:\n            token = cmd.prompt('Env ARROW_GITHUB_API_TOKEN not set, '\n                               'please enter your GitHub API token '\n                               '(GitHub personal access token):')\n        headers = {\n            'Accept': 'application/vnd.github.v3+json',\n            'Authorization': 'token {0}'.format(token),\n        }\n        self.headers = headers\n\n    def get_milestones(self):\n        return get_json(\"%s/milestones\" % (self.github_api, ),\n                        headers=self.headers)\n\n    def get_milestone_number(self, version):\n        return next((\n            m[\"number\"] for m in self.get_milestones() if m[\"title\"] == version\n        ), None)\n\n    def get_issue_data(self, number):\n        return get_json(\"%s/issues/%s\" % (self.github_api, number),\n                        headers=self.headers)\n\n    def get_pr_data(self, number):\n        return get_json(\"%s/pulls/%s\" % (self.github_api, number),\n                        headers=self.headers)\n\n    def get_pr_commits(self, number):\n        return get_json(\"%s/pulls/%s/commits\" % (self.github_api, number),\n                        headers=self.headers)\n\n    def get_branches(self):\n        return get_json(\"%s/branches\" % (self.github_api),\n                        headers=self.headers)\n\n    def close_issue(self, number, comment):\n        issue_url = f'{self.github_api}/issues/{number}'\n        comment_url = f'{self.github_api}/issues/{number}/comments'\n\n        r = requests.post(comment_url, json={\n                          \"body\": comment}, headers=self.headers)\n        if not r.ok:\n            raise ValueError(\n                f\"Failed request: {comment_url}:{r.status_code} -> {r.json()}\")\n\n        r = requests.patch(\n            issue_url, json={\"state\": \"closed\"}, headers=self.headers)\n        if not r.ok:\n            raise ValueError(\n                f\"Failed request: {issue_url}:{r.status_code} -> {r.json()}\")\n\n    def assign_milestone(self, number, version):\n        url = f'{self.github_api}/issues/{number}'\n        milestone_number = self.get_milestone_number(version)\n        if not milestone_number:\n            raise ValueError(f\"Invalid version {version}, milestone not found\")\n        payload = {\n            'milestone': milestone_number\n        }\n        r = requests.patch(url, headers=self.headers, json=payload)\n        if not r.ok:\n            raise ValueError(\n                f\"Failed request: {url}:{r.status_code} -> {r.json()}\")\n        return r.json()\n\n    def merge_pr(self, number, commit_title, commit_message):\n        url = f'{self.github_api}/pulls/{number}/merge'\n        payload = {\n            'commit_title': commit_title,\n            'commit_message': commit_message,\n            'merge_method': 'squash',\n        }\n        response = requests.put(url, headers=self.headers, json=payload)\n        result = response.json()\n        if response.status_code == 200 and 'merged' in result:\n            self.clear_pr_state_labels(number)\n        else:\n            result['merged'] = False\n            result['message'] += f': {url}'\n        return result\n\n    def clear_pr_state_labels(self, number):\n        url = f'{self.github_api}/issues/{number}/labels'\n        response = requests.get(url, headers=self.headers)\n        labels = response.json()\n        for label in labels:\n            # All PR workflow state labels starts with \"awaiting\"\n            if label['name'].startswith('awaiting'):\n                label_url = f\"{url}/{label['name']}\"\n                requests.delete(label_url, headers=self.headers)\n\n\nclass CommandInput(object):\n    \"\"\"\n    Interface to input(...) to enable unit test mocks to be created\n    \"\"\"\n\n    def fail(self, msg):\n        raise Exception(msg)\n\n    def prompt(self, prompt):\n        return input(prompt)\n\n    def getpass(self, prompt):\n        return getpass.getpass(prompt)\n\n    def continue_maybe(self, prompt):\n        while True:\n            result = input(\"\\n%s (y/n): \" % prompt)\n            if result.lower() == \"y\":\n                return\n            elif result.lower() == \"n\":\n                self.fail(\"Okay, exiting\")\n            else:\n                prompt = \"Please input 'y' or 'n'\"\n\n\nclass PullRequest(object):\n    GITHUB_PR_TITLE_PATTERN = re.compile(r'^GH-([0-9]+)\\b.*$')\n    # We can merge PARQUET patches from JIRA or GH prefixed issues\n    JIRA_SUPPORTED_PROJECTS = ['PARQUET']\n    JIRA_PR_TITLE_REGEXEN = [\n        (project, re.compile(r'^(' + project + r'-[0-9]+)\\b.*$'))\n        for project in JIRA_SUPPORTED_PROJECTS\n    ]\n    JIRA_UNSUPPORTED_ARROW = re.compile(r'^(ARROW-[0-9]+)\\b.*$')\n\n    def __init__(self, cmd, github_api, git_remote, jira_con, number):\n        self.cmd = cmd\n        self._github_api = github_api\n        self.git_remote = git_remote\n        self.con = jira_con\n        self.number = number\n        self._pr_data = github_api.get_pr_data(number)\n        try:\n            self.url = self._pr_data[\"url\"]\n            self.title = self._pr_data[\"title\"]\n            self.body = self._pr_data[\"body\"]\n            self.target_ref = self._pr_data[\"base\"][\"ref\"]\n            self.user_login = self._pr_data[\"user\"][\"login\"]\n            self.base_ref = self._pr_data[\"head\"][\"ref\"]\n        except KeyError:\n            pprint.pprint(self._pr_data)\n            raise\n        self.description = \"%s/%s\" % (self.user_login, self.base_ref)\n\n        self.issue = self._get_issue()\n\n    def show(self):\n        print(\"\\n=== Pull Request #%s ===\" % self.number)\n        print(\"title\\t%s\\nsource\\t%s\\ntarget\\t%s\\nurl\\t%s\"\n              % (self.title, self.description, self.target_ref, self.url))\n        if self.issue is not None:\n            self.issue.show()\n        else:\n            print(\"Minor PR.  Please ensure it meets guidelines for minor.\\n\")\n\n    @property\n    def is_merged(self):\n        return bool(self._pr_data[\"merged\"])\n\n    @property\n    def is_mergeable(self):\n        return bool(self._pr_data[\"mergeable\"])\n\n    @property\n    def maintenance_branches(self):\n        return [x[\"name\"] for x in self._github_api.get_branches()\n                if x[\"name\"].startswith(\"maint-\")]\n\n    def _get_issue(self):\n        if self.title.startswith(\"MINOR:\"):\n            return None\n\n        m = self.GITHUB_PR_TITLE_PATTERN.search(self.title)\n        if m:\n            github_id = m.group(1)\n            return GitHubIssue(self._github_api, github_id, self.cmd)\n\n        m = self.JIRA_UNSUPPORTED_ARROW.search(self.title)\n        if m:\n            old_jira_id = m.group(1)\n            jira_issue = JiraIssue(self.con, old_jira_id, 'ARROW', self.cmd)\n            self.cmd.fail(\"PR titles with ARROW- prefixed tickets on JIRA \"\n                          \"are unsupported, update the PR title from \"\n                          f\"{old_jira_id}. Possible GitHub id could be: \"\n                          f\"{jira_issue.github_issue_id()}\")\n\n        for project, regex in self.JIRA_PR_TITLE_REGEXEN:\n            m = regex.search(self.title)\n            if m:\n                jira_id = m.group(1)\n                return JiraIssue(self.con, jira_id, project, self.cmd)\n\n        options = ' or '.join(\n            '{0}-XXX'.format(project)\n            for project in self.JIRA_SUPPORTED_PROJECTS + [\"GH\"]\n        )\n        self.cmd.fail(\"PR title should be prefixed by a GitHub ID or a \"\n                      \"Jira ID, like: {0}, but found {1}\".format(\n                          options, self.title))\n\n    def merge(self):\n        \"\"\"\n        merge the requested PR and return the merge hash\n        \"\"\"\n        commits = self._github_api.get_pr_commits(self.number)\n\n        def format_commit_author(commit):\n            author = commit['commit']['author']\n            name = author['name']\n            email = author['email']\n            return f'{name} <{email}>'\n        commit_authors = [format_commit_author(commit) for commit in commits]\n        co_authored_by_re = re.compile(\n            r'^Co-authored-by:\\s*(.*)', re.MULTILINE)\n\n        def extract_co_authors(commit):\n            message = commit['commit']['message']\n            return co_authored_by_re.findall(message)\n        commit_co_authors = []\n        for commit in commits:\n            commit_co_authors.extend(extract_co_authors(commit))\n\n        all_commit_authors = commit_authors + commit_co_authors\n        distinct_authors = sorted(set(all_commit_authors),\n                                  key=lambda x: commit_authors.count(x),\n                                  reverse=True)\n\n        for i, author in enumerate(distinct_authors):\n            print(\"Author {}: {}\".format(i + 1, author))\n\n        if len(distinct_authors) > 1:\n            primary_author, distinct_other_authors = get_primary_author(\n                self.cmd, distinct_authors)\n        else:\n            # If there is only one author, do not prompt for a lead author\n            primary_author = distinct_authors.pop()\n            distinct_other_authors = []\n\n        commit_title = f'{self.title} (#{self.number})'\n        commit_message_chunks = []\n        if self.body is not None:\n            # Remove comments (i.e. <-- comment -->) from the PR description.\n            body = re.sub(r\"<!--.*?-->\", \"\", self.body, flags=re.DOTALL)\n            # avoid github user name references by inserting a space after @\n            body = re.sub(r\"@(\\w+)\", \"@ \\\\1\", body)\n            commit_message_chunks.append(body)\n\n        committer_name = run_cmd(\"git config --get user.name\").strip()\n        committer_email = run_cmd(\"git config --get user.email\").strip()\n\n        authors = (\"Authored-by:\" if len(distinct_other_authors) == 0\n                   else \"Lead-authored-by:\")\n        authors += \" %s\" % primary_author\n        if len(distinct_authors) > 0:\n            authors += \"\\n\" + \"\\n\".join([\"Co-authored-by: %s\" % a\n                                         for a in distinct_other_authors])\n        authors += \"\\n\" + \"Signed-off-by: %s <%s>\" % (committer_name,\n                                                      committer_email)\n        commit_message_chunks.append(authors)\n\n        commit_message = \"\\n\\n\".join(commit_message_chunks)\n\n        # Normalize line ends and collapse extraneous newlines. We allow two\n        # consecutive newlines for paragraph breaks but not more.\n        commit_message = \"\\n\".join(commit_message.splitlines())\n        commit_message = re.sub(\"\\n{2,}\", \"\\n\\n\", commit_message)\n\n        if DEBUG:\n            print(\"*** Commit title ***\")\n            print(commit_title)\n            print()\n            print(\"*** Commit message ***\")\n            print(commit_message)\n\n        if DEBUG:\n            merge_hash = None\n        else:\n            result = self._github_api.merge_pr(self.number,\n                                               commit_title,\n                                               commit_message)\n            if not result['merged']:\n                message = result['message']\n                self.cmd.fail(f'Failed to merge pull request: {message}')\n            merge_hash = result['sha']\n\n        print(\"Pull request #%s merged!\" % self.number)\n        print(\"Merge hash: %s\" % merge_hash)\n\n\ndef get_primary_author(cmd, distinct_authors):\n    author_pat = re.compile(r'(.*) <(.*)>')\n\n    while True:\n        primary_author = cmd.prompt(\n            \"Enter primary author in the format of \"\n            \"\\\"name <email>\\\" [%s]: \" % distinct_authors[0])\n\n        if primary_author == \"\":\n            return distinct_authors[0], distinct_authors[1:]\n\n        if author_pat.match(primary_author):\n            break\n        print('Bad author \"{}\", please try again'.format(primary_author))\n\n    # When primary author is specified manually, de-dup it from\n    # author list and put it at the head of author list.\n    distinct_other_authors = [x for x in distinct_authors\n                              if x != primary_author]\n    return primary_author, distinct_other_authors\n\n\ndef prompt_for_fix_version(cmd, issue, maintenance_branches=()):\n    default_fix_version = get_candidate_fix_version(\n        mainline_versions=issue.current_versions,\n        maintenance_branches=maintenance_branches\n    )\n\n    current_fix_versions = issue.current_fix_versions\n    if (current_fix_versions and\n            current_fix_versions != default_fix_version):\n        print(\"\\n=== The assigned milestone is not the default ===\")\n        print(f\"Assigned milestone: {current_fix_versions}\")\n        print(f\"Current milestone: {default_fix_version}\")\n        if issue.issue[\"milestone\"].get(\"state\") == 'closed':\n            print(\"The assigned milestone state is closed. Contact the \")\n            print(\"Release Manager if it has to be added to a closed Release\")\n        print(\"Please ensure to assign the correct milestone.\")\n        # Default to existing assigned milestone\n        default_fix_version = current_fix_versions\n\n    issue_fix_version = cmd.prompt(\"Enter fix version [%s]: \"\n                                   % default_fix_version)\n    if issue_fix_version == \"\":\n        issue_fix_version = default_fix_version\n    issue_fix_version = issue_fix_version.strip()\n    return issue_fix_version\n\n\nCONFIG_FILE = \"~/.config/arrow/merge.conf\"\n\n\ndef load_configuration():\n    config = configparser.ConfigParser()\n    config.read(os.path.expanduser(CONFIG_FILE))\n    return config\n\n\ndef get_credentials(cmd):\n    token = None\n\n    config = load_configuration()\n    if \"jira\" in config.sections():\n        token = config[\"jira\"].get(\"token\")\n\n    # Fallback to environment variables\n    if not token:\n        token = os.environ.get(\"APACHE_JIRA_TOKEN\")\n\n    # Fallback to user tty prompt\n    if not token:\n        token = cmd.prompt(\"Env APACHE_JIRA_TOKEN not set, \"\n                           \"please enter your Jira API token \"\n                           \"(Jira personal access token):\")\n\n    return token\n\n\ndef connect_jira(cmd):\n    return jira.client.JIRA(options={'server': JIRA_API_BASE},\n                            token_auth=get_credentials(cmd))\n\n\ndef get_pr_num():\n    if len(sys.argv) == 2:\n        return sys.argv[1]\n\n    return input(\"Which pull request would you like to merge? (e.g. 34): \")\n\n\ndef cli():\n    # Location of your Arrow git clone\n    ARROW_HOME = os.path.abspath(os.path.dirname(__file__))\n    print(f\"ARROW_HOME = {ARROW_HOME}\")\n    print(f\"ORG_NAME = {ORG_NAME}\")\n    print(f\"PROJECT_NAME = {PROJECT_NAME}\")\n\n    cmd = CommandInput()\n\n    pr_num = get_pr_num()\n\n    os.chdir(ARROW_HOME)\n\n    github_api = GitHubAPI(PROJECT_NAME, cmd)\n\n    jira_con = connect_jira(cmd)\n    pr = PullRequest(cmd, github_api, ORG_NAME, jira_con, pr_num)\n\n    if pr.is_merged:\n        print(\"Pull request %s has already been merged\" % pr_num)\n        sys.exit(0)\n\n    if not pr.is_mergeable:\n        print(\"Pull request %s is not mergeable in its current form\" % pr_num)\n        sys.exit(1)\n\n    pr.show()\n\n    cmd.continue_maybe(\"Proceed with merging pull request #%s?\" % pr_num)\n\n    pr.merge()\n\n    if pr.issue is None:\n        print(\"Minor PR.  No issue to update.\\n\")\n        return\n\n    cmd.continue_maybe(\"Would you like to update the associated issue?\")\n    issue_comment = (\n        \"Issue resolved by pull request %s\\n%s\"\n        % (pr_num,\n           f\"https://github.com/{ORG_NAME}/{PROJECT_NAME}/pull/{pr_num}\")\n    )\n    fix_version = prompt_for_fix_version(cmd, pr.issue,\n                                         pr.maintenance_branches)\n    pr.issue.resolve(fix_version, issue_comment, pr.body)\n\n\nif __name__ == '__main__':\n    try:\n        cli()\n    except Exception:\n        raise\n", "dev/tasks/conda-recipes/clean.py": "import subprocess\nfrom typing import Set\n\nimport json\nimport pandas as pd\nimport sys\n\nfrom packaging.version import Version\n\n\nVERSIONS_TO_KEEP = 5\nDELETE_BEFORE = pd.Timestamp.now() - pd.Timedelta(days=30)\n\nPLATFORMS = [\n    \"linux-64\",\n    \"linux-aarch64\",\n    \"linux-ppc64le\",\n    \"osx-64\",\n    \"osx-arm64\",\n    \"win-64\",\n]\n\n\nclass CommandFailedException(Exception):\n\n    def __init__(self, cmdline, output):\n        self.cmdline = cmdline\n        self.output = output\n\n\ndef run_command(cmdline, **kwargs):\n    kwargs.setdefault('capture_output', True)\n    p = subprocess.run(cmdline, **kwargs)\n    if p.returncode != 0:\n        print(f\"Command {cmdline} returned non-zero exit status \"\n              f\"{p.returncode}\", file=sys.stderr)\n        output = \"\"\n        if p.stdout:\n            print(\"Stdout was:\\n\" + \"-\" * 70, file=sys.stderr)\n            output = p.stdout.decode().rstrip()\n            print(output, file=sys.stderr)\n            print(\"-\" * 70, file=sys.stderr)\n        if p.stderr:\n            print(\"Stderr was:\\n\" + \"-\" * 70, file=sys.stderr)\n            output = p.stderr.decode().rstrip()\n            print(p.stderr.decode().rstrip(), file=sys.stderr)\n            print(\"-\" * 70, file=sys.stderr)\n        raise CommandFailedException(cmdline=cmdline, output=output)\n    return p.stdout\n\n\ndef builds_to_delete(platform: str, to_delete: Set[str]) -> int:\n    try:\n        pkgs_json = run_command(\n            [\n                \"conda\",\n                \"search\",\n                \"--json\",\n                \"-c\",\n                \"arrow-nightlies\",\n                \"--override-channels\",\n                \"--subdir\",\n                platform\n            ],\n        )\n    except CommandFailedException as ex:\n        # If the command failed due to no packages found, return\n        # 0 builds to delete.\n        if \"PackagesNotFoundError\" in ex.output:\n            return 0\n        else:\n            sys.exit(1)\n\n    pkgs = json.loads(pkgs_json)\n    num_builds = 0\n\n    for package_name, builds in pkgs.items():\n        num_builds += len(builds)\n        builds = pd.DataFrame(builds)\n        builds[\"version\"] = builds[\"version\"].map(Version)\n        # May be NaN if package doesn't depend on Python\n        builds[\"py_version\"] = builds[\"build\"].str.extract(r'(py\\d+)')\n        builds[\"timestamp\"] = pd.to_datetime(builds['timestamp'], unit='ms')\n        builds[\"stale\"] = builds[\"timestamp\"] < DELETE_BEFORE\n        # Some packages can be present in several \"features\" (e.g. CUDA),\n        # others miss that column in which case we set a default value.\n        if \"track_features\" not in builds.columns:\n            if package_name == \"arrow-cpp-proc\":\n                # XXX arrow-cpp-proc puts the features in the build field...\n                builds[\"track_features\"] = builds[\"build\"]\n            else:\n                builds[\"track_features\"] = 0\n\n        # Detect old builds for each configuration:\n        # a product of (architecture, Python version, features).\n        for (subdir, python, features, stale), group in builds.groupby(\n                [\"subdir\", \"py_version\", \"track_features\", \"stale\"],\n                dropna=False):\n            del_candidates = []\n            if stale:\n                del_candidates = group\n            else:\n                group = group.sort_values(by=\"version\", ascending=False)\n                if len(group) > VERSIONS_TO_KEEP:\n                    del_candidates = group[VERSIONS_TO_KEEP:]\n\n            if len(del_candidates):\n                to_delete.update(\n                    f\"arrow-nightlies/{package_name}/\"\n                    + del_candidates[\"version\"].astype(str)\n                    + del_candidates[\"url\"].str.replace(\n                        \"https://conda.anaconda.org/arrow-nightlies\", \"\",\n                        regex=False\n                    )\n                )\n\n    return num_builds\n\n\nif __name__ == \"__main__\":\n    to_delete = set()\n    num_builds = 0\n    for platform in PLATFORMS:\n        num_builds += builds_to_delete(platform, to_delete)\n\n    to_delete = sorted(to_delete)\n\n    print(f\"{len(to_delete)} builds may be deleted out of {num_builds}\")\n    for name in to_delete:\n        print(f\"- {name}\")\n\n    if \"FORCE\" in sys.argv and len(to_delete) > 0:\n        print(\"Deleting ...\")\n        run_command([\"anaconda\", \"remove\", \"-f\"] + to_delete)\n", "dev/archery/setup.py": "#!/usr/bin/env python\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport functools\nimport operator\nimport sys\nfrom setuptools import setup, find_packages\n\n# pygit2>=1.14.0 requires python 3.9, so crossbow and all\n# both technically require python 3.9 \u2014 however we still need to\n# support 3.8 when using docker. When 3.8 is EOLed and we bump\n# to Python 3.9 this will resolve itself.\nif sys.version_info < (3, 8):\n    sys.exit('Python < 3.8 is not supported')\n\n# For pathlib.Path compatibility\njinja_req = 'jinja2>=2.11'\n\nextras = {\n    'benchmark': ['pandas'],\n    'crossbow': ['github3.py', jinja_req, 'pygit2>=1.14.0', 'requests',\n                 'ruamel.yaml', 'setuptools_scm<8.0.0'],\n    'crossbow-upload': ['github3.py', jinja_req, 'ruamel.yaml',\n                        'setuptools_scm'],\n    'docker': ['ruamel.yaml', 'python-dotenv'],\n    'integration': ['cffi'],\n    'integration-java': ['jpype1'],\n    'lint': ['numpydoc==1.1.0', 'autopep8', 'flake8==6.1.0', 'cython-lint',\n             'cmake_format==0.6.13', 'sphinx-lint==0.9.1'],\n    'numpydoc': ['numpydoc==1.1.0'],\n    'release': ['pygithub', jinja_req, 'jira', 'semver', 'gitpython'],\n}\nextras['bot'] = extras['crossbow'] + ['pygithub', 'jira']\nextras['all'] = list(set(functools.reduce(operator.add, extras.values())))\n\nsetup(\n    name='archery',\n    version=\"0.1.0\",\n    description='Apache Arrow Developers Tools',\n    url='http://github.com/apache/arrow',\n    maintainer='Arrow Developers',\n    maintainer_email='dev@arrow.apache.org',\n    packages=find_packages(),\n    include_package_data=True,\n    python_requires='>=3.8',\n    install_requires=['click>=7'],\n    tests_require=['pytest', 'responses'],\n    extras_require=extras,\n    entry_points='''\n        [console_scripts]\n        archery=archery.cli:archery\n    '''\n)\n", "dev/archery/archery/cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import namedtuple\nfrom io import StringIO\nimport click\nimport json\nimport logging\nimport os\nimport pathlib\nimport sys\n\nfrom .benchmark.codec import JsonEncoder\nfrom .benchmark.compare import RunnerComparator, DEFAULT_THRESHOLD\nfrom .benchmark.runner import CppBenchmarkRunner, JavaBenchmarkRunner\nfrom .compat import _import_pandas\nfrom .lang.cpp import CppCMakeDefinition, CppConfiguration\nfrom .utils.cli import ArrowBool, validate_arrow_sources, add_optional_command\nfrom .utils.lint import linter, python_numpydoc, LintValidationException\nfrom .utils.logger import logger, ctx as log_ctx\nfrom .utils.source import ArrowSources\nfrom .utils.tmpdir import tmpdir\n\n# Set default logging to INFO in command line.\nlogging.basicConfig(level=logging.INFO)\n\n\nBOOL = ArrowBool()\n\n\n@click.group(context_settings={\"help_option_names\": [\"-h\", \"--help\"]})\n@click.option(\"--debug\", type=BOOL, is_flag=True, default=False,\n              envvar='ARCHERY_DEBUG',\n              help=\"Increase logging with debugging output.\")\n@click.option(\"--pdb\", type=BOOL, is_flag=True, default=False,\n              help=\"Invoke pdb on uncaught exception.\")\n@click.option(\"-q\", \"--quiet\", type=BOOL, is_flag=True, default=False,\n              help=\"Silence executed commands.\")\n@click.pass_context\ndef archery(ctx, debug, pdb, quiet):\n    \"\"\" Apache Arrow developer utilities.\n\n    See sub-commands help with `archery <cmd> --help`.\n\n    \"\"\"\n    # Ensure ctx.obj exists\n    ctx.ensure_object(dict)\n\n    log_ctx.quiet = quiet\n    if debug:\n        logger.setLevel(logging.DEBUG)\n\n    ctx.obj['debug'] = debug\n\n    if pdb:\n        import pdb\n        sys.excepthook = lambda t, v, e: pdb.pm()\n\n\nbuild_dir_type = click.Path(dir_okay=True, file_okay=False, resolve_path=True)\n# Supported build types\nbuild_type = click.Choice([\"debug\", \"relwithdebinfo\", \"release\"],\n                          case_sensitive=False)\n# Supported warn levels\nwarn_level_type = click.Choice([\"everything\", \"checkin\", \"production\"],\n                               case_sensitive=False)\n\nsimd_level = click.Choice([\"NONE\", \"SSE4_2\", \"AVX2\", \"AVX512\"],\n                          case_sensitive=True)\n\n\ndef cpp_toolchain_options(cmd):\n    options = [\n        click.option(\"--cc\", metavar=\"<compiler>\", help=\"C compiler.\"),\n        click.option(\"--cxx\", metavar=\"<compiler>\", help=\"C++ compiler.\"),\n        click.option(\"--cxx-flags\", help=\"C++ compiler flags.\"),\n        click.option(\"--cpp-package-prefix\",\n                     help=(\"Value to pass for ARROW_PACKAGE_PREFIX and \"\n                           \"use ARROW_DEPENDENCY_SOURCE=SYSTEM\"))\n    ]\n    return _apply_options(cmd, options)\n\n\ndef java_toolchain_options(cmd):\n    options = [\n        click.option(\"--java-home\", metavar=\"<java_home>\",\n                     help=\"Path to Java Developers Kit.\"),\n        click.option(\"--java-options\", help=\"java compiler options.\"),\n    ]\n    return _apply_options(cmd, options)\n\n\ndef _apply_options(cmd, options):\n    for option in options:\n        cmd = option(cmd)\n    return cmd\n\n\n@archery.command(short_help=\"Initialize an Arrow C++ build\")\n@click.option(\"--src\", metavar=\"<arrow_src>\", default=None,\n              callback=validate_arrow_sources,\n              help=\"Specify Arrow source directory\")\n# toolchain\n@cpp_toolchain_options\n@click.option(\"--build-type\", default=None, type=build_type,\n              help=\"CMake's CMAKE_BUILD_TYPE\")\n@click.option(\"--build-static\", default=True, type=BOOL,\n              help=\"Build static libraries\")\n@click.option(\"--build-shared\", default=True, type=BOOL,\n              help=\"Build shared libraries\")\n@click.option(\"--build-unity\", default=True, type=BOOL,\n              help=\"Use CMAKE_UNITY_BUILD\")\n@click.option(\"--warn-level\", default=\"production\", type=warn_level_type,\n              help=\"Controls compiler warnings -W(no-)error.\")\n@click.option(\"--use-gold-linker\", default=True, type=BOOL,\n              help=\"Toggles ARROW_USE_LD_GOLD option.\")\n@click.option(\"--simd-level\", default=\"DEFAULT\", type=simd_level,\n              help=\"Toggles ARROW_SIMD_LEVEL option.\")\n# Tests and benchmarks\n@click.option(\"--with-tests\", default=True, type=BOOL,\n              help=\"Build with tests.\")\n@click.option(\"--with-benchmarks\", default=None, type=BOOL,\n              help=\"Build with benchmarks.\")\n@click.option(\"--with-examples\", default=None, type=BOOL,\n              help=\"Build with examples.\")\n@click.option(\"--with-integration\", default=None, type=BOOL,\n              help=\"Build with integration test executables.\")\n# Static checks\n@click.option(\"--use-asan\", default=None, type=BOOL,\n              help=\"Toggle ARROW_USE_ASAN sanitizer.\")\n@click.option(\"--use-tsan\", default=None, type=BOOL,\n              help=\"Toggle ARROW_USE_TSAN sanitizer.\")\n@click.option(\"--use-ubsan\", default=None, type=BOOL,\n              help=\"Toggle ARROW_USE_UBSAN sanitizer.\")\n@click.option(\"--with-fuzzing\", default=None, type=BOOL,\n              help=\"Toggle ARROW_FUZZING.\")\n# Components\n@click.option(\"--with-compute\", default=None, type=BOOL,\n              help=\"Build the Arrow compute module.\")\n@click.option(\"--with-csv\", default=None, type=BOOL,\n              help=\"Build the Arrow CSV parser module.\")\n@click.option(\"--with-cuda\", default=None, type=BOOL,\n              help=\"Build the Arrow CUDA extensions.\")\n@click.option(\"--with-dataset\", default=None, type=BOOL,\n              help=\"Build the Arrow dataset module.\")\n@click.option(\"--with-filesystem\", default=None, type=BOOL,\n              help=\"Build the Arrow filesystem layer.\")\n@click.option(\"--with-flight\", default=None, type=BOOL,\n              help=\"Build with Flight rpc support.\")\n@click.option(\"--with-gandiva\", default=None, type=BOOL,\n              help=\"Build with Gandiva expression compiler support.\")\n@click.option(\"--with-gcs\", default=None, type=BOOL,\n              help=\"Build Arrow with Google Cloud Storage (GCS) support.\")\n@click.option(\"--with-hdfs\", default=None, type=BOOL,\n              help=\"Build the Arrow HDFS bridge.\")\n@click.option(\"--with-hiveserver2\", default=None, type=BOOL,\n              help=\"Build the HiveServer2 client and arrow adapter.\")\n@click.option(\"--with-ipc\", default=None, type=BOOL,\n              help=\"Build the Arrow IPC extensions.\")\n@click.option(\"--with-json\", default=None, type=BOOL,\n              help=\"Build the Arrow JSON parser module.\")\n@click.option(\"--with-mimalloc\", default=None, type=BOOL,\n              help=\"Build the Arrow mimalloc based allocator.\")\n@click.option(\"--with-parquet\", default=None, type=BOOL,\n              help=\"Build with Parquet file support.\")\n@click.option(\"--with-python\", default=None, type=BOOL,\n              help=\"Build the Arrow CPython extensions.\")\n@click.option(\"--with-r\", default=None, type=BOOL,\n              help=\"Build the Arrow R extensions. This is not a CMake option, \"\n              \"it will toggle required options\")\n@click.option(\"--with-s3\", default=None, type=BOOL,\n              help=\"Build Arrow with S3 support.\")\n# Compressions\n@click.option(\"--with-brotli\", default=None, type=BOOL,\n              help=\"Build Arrow with brotli compression.\")\n@click.option(\"--with-bz2\", default=None, type=BOOL,\n              help=\"Build Arrow with bz2 compression.\")\n@click.option(\"--with-lz4\", default=None, type=BOOL,\n              help=\"Build Arrow with lz4 compression.\")\n@click.option(\"--with-snappy\", default=None, type=BOOL,\n              help=\"Build Arrow with snappy compression.\")\n@click.option(\"--with-zlib\", default=None, type=BOOL,\n              help=\"Build Arrow with zlib compression.\")\n@click.option(\"--with-zstd\", default=None, type=BOOL,\n              help=\"Build Arrow with zstd compression.\")\n# CMake extra feature\n@click.option(\"--cmake-extras\", type=str, multiple=True,\n              help=\"Extra flags/options to pass to cmake invocation. \"\n              \"Can be stacked\")\n@click.option(\"--install-prefix\", type=str,\n              help=\"Destination directory where files are installed. Expand to\"\n              \"CMAKE_INSTALL_PREFIX. Defaults to to $CONDA_PREFIX if the\"\n              \"variable exists.\")\n# misc\n@click.option(\"-f\", \"--force\", type=BOOL, is_flag=True, default=False,\n              help=\"Delete existing build directory if found.\")\n@click.option(\"--targets\", type=str, multiple=True,\n              help=\"Generator targets to run. Can be stacked.\")\n@click.argument(\"build_dir\", type=build_dir_type)\n@click.pass_context\ndef build(ctx, src, build_dir, force, targets, **kwargs):\n    \"\"\" Initialize a C++ build directory.\n\n    The build command creates a directory initialized with Arrow's cpp source\n    cmake and configuration. It can also optionally invoke the generator to\n    test the build (and used in scripts).\n\n    Note that archery will carry the caller environment. It will also not touch\n    an existing directory, one must use the `--force` option to remove the\n    existing directory.\n\n    Examples:\n\n    \\b\n    # Initialize build with clang8 and avx2 support in directory `clang8-build`\n    \\b\n    archery build --cc=clang-8 --cxx=clang++-8 --cxx-flags=-mavx2 clang8-build\n\n    \\b\n    # Builds and run test\n    archery build --targets=all --targets=test build\n    \"\"\"\n    # Arrow's cpp cmake configuration\n    conf = CppConfiguration(**kwargs)\n    # This is a closure around cmake invocation, e.g. calling `def.build()`\n    # yields a directory ready to be run with the generator\n    cmake_def = CppCMakeDefinition(src.cpp, conf)\n    # Create build directory\n    build = cmake_def.build(build_dir, force=force)\n\n    for target in targets:\n        build.run(target)\n\n\nLintCheck = namedtuple('LintCheck', ('option_name', 'help'))\n\nlint_checks = [\n    LintCheck('clang-format', \"Format C++ files with clang-format.\"),\n    LintCheck('clang-tidy', \"Lint C++ files with clang-tidy.\"),\n    LintCheck('cpplint', \"Lint C++ files with cpplint.\"),\n    LintCheck('iwyu', \"Lint changed C++ files with Include-What-You-Use.\"),\n    LintCheck('python',\n              \"Format and lint Python files with autopep8 and flake8.\"),\n    LintCheck('numpydoc', \"Lint Python files with numpydoc.\"),\n    LintCheck('cmake-format', \"Format CMake files with cmake-format.py.\"),\n    LintCheck('rat',\n              \"Check all sources files for license texts via Apache RAT.\"),\n    LintCheck('r', \"Lint R files.\"),\n    LintCheck('docker', \"Lint Dockerfiles with hadolint.\"),\n    LintCheck('docs', \"Lint docs with sphinx-lint.\"),\n]\n\n\ndef decorate_lint_command(cmd):\n    \"\"\"\n    Decorate the lint() command function to add individual per-check options.\n    \"\"\"\n    for check in lint_checks:\n        option = click.option(\"--{0}/--no-{0}\".format(check.option_name),\n                              default=None, help=check.help)\n        cmd = option(cmd)\n    return cmd\n\n\n@archery.command(short_help=\"Check Arrow source tree for errors\")\n@click.option(\"--src\", metavar=\"<arrow_src>\", default=None,\n              callback=validate_arrow_sources,\n              help=\"Specify Arrow source directory\")\n@click.option(\"--fix\", is_flag=True, type=BOOL, default=False,\n              help=\"Toggle fixing the lint errors if the linter supports it.\")\n@click.option(\"--iwyu_all\", is_flag=True, type=BOOL, default=False,\n              help=\"Run IWYU on all C++ files if enabled\")\n@click.option(\"-a\", \"--all\", is_flag=True, default=False,\n              help=\"Enable all checks.\")\n@click.argument(\"path\", required=False)\n@decorate_lint_command\n@click.pass_context\ndef lint(ctx, src, fix, iwyu_all, path, **checks):\n    if checks.pop('all'):\n        # \"--all\" is given => enable all non-selected checks\n        for k, v in checks.items():\n            if v is None:\n                checks[k] = True\n    if not any(checks.values()):\n        raise click.UsageError(\n            \"Need to enable at least one lint check (try --help)\")\n    try:\n        linter(src, fix, iwyu_all=iwyu_all, path=path, **checks)\n    except LintValidationException:\n        sys.exit(1)\n\n\ndef _flatten_numpydoc_rules(rules):\n    flattened = []\n    for rule in rules:\n        flattened.extend(filter(None, rule.split(',')))\n    return flattened\n\n\n@archery.command(short_help=\"Lint python docstring with NumpyDoc\")\n@click.argument('symbols', nargs=-1)\n@click.option(\"--src\", metavar=\"<arrow_src>\", default=None,\n              callback=validate_arrow_sources,\n              help=\"Specify Arrow source directory\")\n@click.option(\"--allow-rule\", \"-a\", multiple=True,\n              help=\"Allow only these rules (can be comma-separated)\")\n@click.option(\"--disallow-rule\", \"-d\", multiple=True,\n              help=\"Disallow these rules (can be comma-separated)\")\ndef numpydoc(src, symbols, allow_rule, disallow_rule):\n    \"\"\"\n    Pass list of modules or symbols as arguments to restrict the validation.\n\n    By default all modules of pyarrow are tried to be validated.\n\n    Examples\n    --------\n    archery numpydoc pyarrow.dataset\n    archery numpydoc pyarrow.csv pyarrow.json pyarrow.parquet\n    archery numpydoc pyarrow.array\n    \"\"\"\n    disallow_rule = disallow_rule or {'GL01', 'SA01', 'EX01', 'ES01'}\n    try:\n        results = python_numpydoc(\n            symbols, allow_rules=_flatten_numpydoc_rules(allow_rule),\n            disallow_rules=_flatten_numpydoc_rules(disallow_rule))\n        for result in results:\n            result.ok()\n    except LintValidationException:\n        sys.exit(1)\n\n\n@archery.group()\n@click.pass_context\ndef benchmark(ctx):\n    \"\"\" Arrow benchmarking.\n\n    Use the diff sub-command to benchmark revisions, and/or build directories.\n    \"\"\"\n    pass\n\n\ndef benchmark_common_options(cmd):\n    def check_language(ctx, param, value):\n        if value not in {\"cpp\", \"java\"}:\n            raise click.BadParameter(\"cpp or java is supported now\")\n        return value\n\n    options = [\n        click.option(\"--src\", metavar=\"<arrow_src>\", show_default=True,\n                     default=None, callback=validate_arrow_sources,\n                     help=\"Specify Arrow source directory\"),\n        click.option(\"--preserve\", type=BOOL, default=False, show_default=True,\n                     is_flag=True,\n                     help=\"Preserve workspace for investigation.\"),\n        click.option(\"--output\", metavar=\"<output>\",\n                     type=click.File(\"w\", encoding=\"utf8\"), default=None,\n                     help=\"Capture output result into file.\"),\n        click.option(\"--language\", metavar=\"<lang>\", type=str, default=\"cpp\",\n                     show_default=True, callback=check_language,\n                     help=\"Specify target language for the benchmark\"),\n        click.option(\"--build-extras\", type=str, multiple=True,\n                     help=\"Extra flags/options to pass to mvn build. \"\n                     \"Can be stacked. For language=java\"),\n        click.option(\"--benchmark-extras\", type=str, multiple=True,\n                     help=\"Extra flags/options to pass to mvn benchmark. \"\n                     \"Can be stacked. For language=java\"),\n        click.option(\"--cmake-extras\", type=str, multiple=True,\n                     help=\"Extra flags/options to pass to cmake invocation. \"\n                     \"Can be stacked. For language=cpp\"),\n        click.option(\"--cpp-benchmark-extras\", type=str, multiple=True,\n                     help=\"Extra flags/options to pass to C++ benchmark executables. \"\n                     \"Can be stacked. For language=cpp\"),\n    ]\n\n    cmd = java_toolchain_options(cmd)\n    cmd = cpp_toolchain_options(cmd)\n    return _apply_options(cmd, options)\n\n\ndef benchmark_filter_options(cmd):\n    options = [\n        click.option(\"--suite-filter\", metavar=\"<regex>\", show_default=True,\n                     type=str, default=None,\n                     help=\"Regex filtering benchmark suites.\"),\n        click.option(\"--benchmark-filter\", metavar=\"<regex>\",\n                     show_default=True, type=str, default=None,\n                     help=\"Regex filtering benchmarks.\")\n    ]\n    return _apply_options(cmd, options)\n\n\n@benchmark.command(name=\"list\", short_help=\"List benchmark suite\")\n@click.argument(\"rev_or_path\", metavar=\"[<rev_or_path>]\",\n                default=\"WORKSPACE\", required=False)\n@benchmark_common_options\n@click.pass_context\ndef benchmark_list(ctx, rev_or_path, src, preserve, output, cmake_extras,\n                   java_home, java_options, build_extras, benchmark_extras,\n                   cpp_benchmark_extras, language, **kwargs):\n    \"\"\" List benchmark suite.\n    \"\"\"\n    with tmpdir(preserve=preserve) as root:\n        logger.debug(\"Running benchmark {}\".format(rev_or_path))\n\n        if language == \"cpp\":\n            conf = CppBenchmarkRunner.default_configuration(\n                cmake_extras=cmake_extras, **kwargs)\n\n            runner_base = CppBenchmarkRunner.from_rev_or_path(\n                src, root, rev_or_path, conf,\n                benchmark_extras=cpp_benchmark_extras)\n\n        elif language == \"java\":\n            for key in {'cpp_package_prefix', 'cxx_flags', 'cxx', 'cc'}:\n                del kwargs[key]\n            conf = JavaBenchmarkRunner.default_configuration(\n                java_home=java_home, java_options=java_options,\n                build_extras=build_extras, benchmark_extras=benchmark_extras,\n                **kwargs)\n\n            runner_base = JavaBenchmarkRunner.from_rev_or_path(\n                src, root, rev_or_path, conf)\n\n        for b in runner_base.list_benchmarks:\n            click.echo(b, file=output or sys.stdout)\n\n\n@benchmark.command(name=\"run\", short_help=\"Run benchmark suite\")\n@click.argument(\"rev_or_path\", metavar=\"[<rev_or_path>]\",\n                default=\"WORKSPACE\", required=False)\n@benchmark_common_options\n@benchmark_filter_options\n@click.option(\"--repetitions\", type=int, default=-1,\n              help=(\"Number of repetitions of each benchmark. Increasing \"\n                    \"may improve result precision. \"\n                    \"[default: 1 for cpp, 5 for java]\"))\n@click.option(\"--repetition-min-time\", type=float, default=None,\n              help=(\"Minimum duration of each repetition in seconds. \"\n                    \"Currently only supported for language=cpp. \"\n                    \"[default: use runner-specific defaults]\"))\n@click.pass_context\ndef benchmark_run(ctx, rev_or_path, src, preserve, output, cmake_extras,\n                  java_home, java_options, build_extras, benchmark_extras,\n                  language, suite_filter, benchmark_filter, repetitions,\n                  repetition_min_time, cpp_benchmark_extras, **kwargs):\n    \"\"\" Run benchmark suite.\n\n    This command will run the benchmark suite for a single build. This is\n    used to capture (and/or publish) the results.\n\n    The caller can optionally specify a target which is either a git revision\n    (commit, tag, special values like HEAD) or a cmake build directory.\n\n    When a commit is referenced, a local clone of the arrow sources (specified\n    via --src) is performed and the proper branch is created. This is done in\n    a temporary directory which can be left intact with the `--preserve` flag.\n\n    The special token \"WORKSPACE\" is reserved to specify the current git\n    workspace. This imply that no clone will be performed.\n\n    Examples:\n\n    \\b\n    # Run the benchmarks on current git workspace\n    \\b\n    archery benchmark run\n\n    \\b\n    # Run the benchmarks on an existing build directory\n    \\b\n    archery benchmark run /build/cpp\n\n    \\b\n    # Run the benchmarks on current previous commit\n    \\b\n    archery benchmark run HEAD~1\n\n    \\b\n    # Run the benchmarks on current git workspace and output results as a JSON file.\n    \\b\n    archery benchmark run --output=run.json\n    \"\"\"\n    with tmpdir(preserve=preserve) as root:\n        logger.debug(\"Running benchmark {}\".format(rev_or_path))\n\n        if language == \"cpp\":\n            conf = CppBenchmarkRunner.default_configuration(\n                cmake_extras=cmake_extras, **kwargs)\n\n            repetitions = repetitions if repetitions != -1 else 1\n            runner_base = CppBenchmarkRunner.from_rev_or_path(\n                src, root, rev_or_path, conf,\n                repetitions=repetitions, repetition_min_time=repetition_min_time,\n                suite_filter=suite_filter, benchmark_filter=benchmark_filter,\n                benchmark_extras=cpp_benchmark_extras)\n\n        elif language == \"java\":\n            for key in {'cpp_package_prefix', 'cxx_flags', 'cxx', 'cc'}:\n                del kwargs[key]\n            conf = JavaBenchmarkRunner.default_configuration(\n                java_home=java_home, java_options=java_options,\n                build_extras=build_extras, benchmark_extras=benchmark_extras,\n                **kwargs)\n\n            repetitions = repetitions if repetitions != -1 else 5\n            runner_base = JavaBenchmarkRunner.from_rev_or_path(\n                src, root, rev_or_path, conf,\n                repetitions=repetitions,\n                benchmark_filter=benchmark_filter)\n\n        # XXX for some reason, the benchmark runner only does its work\n        # when asked to JSON-serialize the results, so produce a JSON\n        # output even when none is requested.\n        json_out = json.dumps(runner_base, cls=JsonEncoder)\n        if output is not None:\n            output.write(json_out)\n\n\n@benchmark.command(name=\"diff\", short_help=\"Compare benchmark suites\")\n@benchmark_common_options\n@benchmark_filter_options\n@click.option(\"--threshold\", type=float, default=DEFAULT_THRESHOLD,\n              show_default=True,\n              help=\"Regression failure threshold in percentage.\")\n@click.option(\"--repetitions\", type=int, default=1, show_default=True,\n              help=(\"Number of repetitions of each benchmark. Increasing \"\n                    \"may improve result precision. \"\n                    \"[default: 1 for cpp, 5 for java\"))\n@click.option(\"--no-counters\", type=BOOL, default=False, is_flag=True,\n              help=\"Hide counters field in diff report.\")\n@click.argument(\"contender\", metavar=\"[<contender>\",\n                default=ArrowSources.WORKSPACE, required=False)\n@click.argument(\"baseline\", metavar=\"[<baseline>]]\", default=\"origin/HEAD\",\n                required=False)\n@click.pass_context\ndef benchmark_diff(ctx, src, preserve, output, language, cmake_extras,\n                   suite_filter, benchmark_filter, repetitions, no_counters,\n                   java_home, java_options, build_extras, benchmark_extras,\n                   cpp_benchmark_extras, threshold, contender, baseline,\n                   **kwargs):\n    \"\"\"Compare (diff) benchmark runs.\n\n    This command acts like git-diff but for benchmark results.\n\n    The caller can optionally specify both the contender and the baseline. If\n    unspecified, the contender will default to the current workspace (like git)\n    and the baseline will default to the mainline development branch (i.e.\n    default git branch).\n\n    Each target (contender or baseline) can either be a git revision\n    (commit, tag, special values like HEAD) or a cmake build directory. This\n    allow comparing git commits, and/or different compilers and/or compiler\n    flags.\n\n    When a commit is referenced, a local clone of the arrow sources (specified\n    via --src) is performed and the proper branch is created. This is done in\n    a temporary directory which can be left intact with the `--preserve` flag.\n\n    The special token \"WORKSPACE\" is reserved to specify the current git\n    workspace. This imply that no clone will be performed.\n\n    Examples:\n\n    \\b\n    # Compare workspace (contender) against the mainline development branch\n    # (baseline)\n    \\b\n    archery benchmark diff\n\n    \\b\n    # Compare the mainline development branch (contender) against the latest\n    # version (baseline)\n    \\b\n    export LAST=$(git tag -l \"apache-arrow-[0-9]*\" | sort -rV | head -1)\n    \\b\n    archery benchmark diff <default-branch> \"$LAST\"\n\n    \\b\n    # Compare g++7 (contender) with clang++-8 (baseline) builds\n    \\b\n    archery build --with-benchmarks=true \\\\\n            --cxx-flags=-ftree-vectorize \\\\\n            --cc=gcc-7 --cxx=g++-7 gcc7-build\n    \\b\n    archery build --with-benchmarks=true \\\\\n            --cxx-flags=-flax-vector-conversions \\\\\n            --cc=clang-8 --cxx=clang++-8 clang8-build\n    \\b\n    archery benchmark diff gcc7-build clang8-build\n\n    \\b\n    # Compare default targets but scoped to the suites matching\n    # `^arrow-compute-aggregate` and benchmarks matching `(Sum|Mean)Kernel`.\n    \\b\n    archery benchmark diff --suite-filter=\"^arrow-compute-aggregate\" \\\\\n            --benchmark-filter=\"(Sum|Mean)Kernel\"\n\n    \\b\n    # Capture result in file `result.json`\n    \\b\n    archery benchmark diff --output=result.json\n    \\b\n    # Equivalently with no stdout clutter.\n    archery --quiet benchmark diff > result.json\n\n    \\b\n    # Comparing with a cached results from `archery benchmark run`\n    \\b\n    archery benchmark run --output=run.json HEAD~1\n    \\b\n    # This should not recompute the benchmark from run.json\n    archery --quiet benchmark diff WORKSPACE run.json > result.json\n    \"\"\"\n    with tmpdir(preserve=preserve) as root:\n        logger.debug(\"Comparing {} (contender) with {} (baseline)\"\n                     .format(contender, baseline))\n\n        if language == \"cpp\":\n            conf = CppBenchmarkRunner.default_configuration(\n                cmake_extras=cmake_extras, **kwargs)\n\n            repetitions = repetitions if repetitions != -1 else 1\n            runner_cont = CppBenchmarkRunner.from_rev_or_path(\n                src, root, contender, conf,\n                repetitions=repetitions,\n                suite_filter=suite_filter,\n                benchmark_filter=benchmark_filter,\n                benchmark_extras=cpp_benchmark_extras)\n            runner_base = CppBenchmarkRunner.from_rev_or_path(\n                src, root, baseline, conf,\n                repetitions=repetitions,\n                suite_filter=suite_filter,\n                benchmark_filter=benchmark_filter,\n                benchmark_extras=cpp_benchmark_extras)\n\n        elif language == \"java\":\n            for key in {'cpp_package_prefix', 'cxx_flags', 'cxx', 'cc'}:\n                del kwargs[key]\n            conf = JavaBenchmarkRunner.default_configuration(\n                java_home=java_home, java_options=java_options,\n                build_extras=build_extras, benchmark_extras=benchmark_extras,\n                **kwargs)\n\n            repetitions = repetitions if repetitions != -1 else 5\n            runner_cont = JavaBenchmarkRunner.from_rev_or_path(\n                src, root, contender, conf,\n                repetitions=repetitions,\n                benchmark_filter=benchmark_filter)\n            runner_base = JavaBenchmarkRunner.from_rev_or_path(\n                src, root, baseline, conf,\n                repetitions=repetitions,\n                benchmark_filter=benchmark_filter)\n\n        runner_comp = RunnerComparator(runner_cont, runner_base, threshold)\n\n        # TODO(kszucs): test that the output is properly formatted jsonlines\n        comparisons_json = _get_comparisons_as_json(runner_comp.comparisons)\n        ren_counters = language == \"java\"\n        formatted = _format_comparisons_with_pandas(comparisons_json,\n                                                    no_counters, ren_counters)\n        print(formatted, file=output or sys.stdout)\n\n\ndef _get_comparisons_as_json(comparisons):\n    buf = StringIO()\n    for comparator in comparisons:\n        json.dump(comparator, buf, cls=JsonEncoder)\n        buf.write(\"\\n\")\n\n    return buf.getvalue()\n\n\ndef _format_comparisons_with_pandas(comparisons_json, no_counters,\n                                    ren_counters):\n    pd = _import_pandas()\n    df = pd.read_json(StringIO(comparisons_json), lines=True)\n    # parse change % so we can sort by it\n    df['change %'] = df.pop('change').str[:-1].map(float)\n    first_regression = len(df) - df['regression'].sum()\n\n    fields = ['benchmark', 'baseline', 'contender', 'change %']\n    if not no_counters:\n        fields += ['counters']\n\n    df = df[fields]\n    if ren_counters:\n        df = df.rename(columns={'counters': 'configurations'})\n    df = df.sort_values(by='change %', ascending=False)\n\n    def labelled(title, df):\n        if len(df) == 0:\n            return ''\n        title += ': ({})'.format(len(df))\n        df_str = df.to_string(index=False)\n        bar = '-' * df_str.index('\\n')\n        return '\\n'.join([bar, title, bar, df_str])\n\n    return '\\n\\n'.join([labelled('Non-regressions', df[:first_regression]),\n                        labelled('Regressions', df[first_regression:])])\n\n\n# ----------------------------------------------------------------------\n# Integration testing\n\ndef _set_default(opt, default):\n    if opt is None:\n        return default\n    return opt\n\n\n@archery.command(short_help=\"Execute protocol and Flight integration tests\")\n@click.option('--with-all', is_flag=True, default=False,\n              help=('Include all known languages by default '\n                    'in integration tests'))\n@click.option('--random-seed', type=int, default=12345,\n              help=\"Seed for PRNG when generating test data\")\n@click.option('--with-cpp', type=bool, default=False,\n              help='Include C++ in integration tests')\n@click.option('--with-csharp', type=bool, default=False,\n              help='Include C# in integration tests')\n@click.option('--with-java', type=bool, default=False,\n              help='Include Java in integration tests')\n@click.option('--with-js', type=bool, default=False,\n              help='Include JavaScript in integration tests')\n@click.option('--with-go', type=bool, default=False,\n              help='Include Go in integration tests')\n@click.option('--with-nanoarrow', type=bool, default=False,\n              help='Include nanoarrow in integration tests',\n              envvar=\"ARCHERY_INTEGRATION_WITH_NANOARROW\")\n@click.option('--with-rust', type=bool, default=False,\n              help='Include Rust in integration tests',\n              envvar=\"ARCHERY_INTEGRATION_WITH_RUST\")\n@click.option('--write_generated_json', default=\"\",\n              help='Generate test JSON to indicated path')\n@click.option('--run-ipc', is_flag=True, default=False,\n              help='Run IPC integration tests')\n@click.option('--run-flight', is_flag=True, default=False,\n              help='Run Flight integration tests')\n@click.option('--run-c-data', is_flag=True, default=False,\n              help='Run C Data Interface integration tests')\n@click.option('--debug', is_flag=True, default=False,\n              help='Run executables in debug mode as relevant')\n@click.option('--serial', is_flag=True, default=False,\n              help='Run tests serially, rather than in parallel')\n@click.option('--tempdir', default=None,\n              help=('Directory to use for writing '\n                    'integration test temporary files'))\n@click.option('stop_on_error', '-x', '--stop-on-error',\n              is_flag=True, default=False,\n              help='Stop on first error')\n@click.option('--gold-dirs', multiple=True,\n              help=\"gold integration test file paths\")\n@click.option('-k', '--match',\n              help=(\"Substring for test names to include in run, \"\n                    \"e.g. -k primitive\"))\ndef integration(with_all=False, random_seed=12345, **args):\n    from .integration.runner import write_js_test_json, run_all_tests\n    import numpy as np\n\n    # FIXME(bkietz) Include help strings for individual testers.\n    # For example, CPPTester's ARROW_CPP_EXE_PATH environment variable.\n\n    # Make runs involving data generation deterministic\n    np.random.seed(random_seed)\n\n    gen_path = args['write_generated_json']\n\n    languages = ['cpp', 'csharp', 'java', 'js', 'go', 'nanoarrow', 'rust']\n    formats = ['ipc', 'flight', 'c_data']\n\n    enabled_languages = 0\n    for lang in languages:\n        param = f'with_{lang}'\n        if with_all:\n            args[param] = with_all\n        enabled_languages += args[param]\n\n    enabled_formats = 0\n    for fmt in formats:\n        param = f'run_{fmt}'\n        enabled_formats += args[param]\n\n    if gen_path:\n        # XXX See GH-37575: this option is only used by the JS test suite\n        # and might not be useful anymore.\n        os.makedirs(gen_path, exist_ok=True)\n        write_js_test_json(gen_path)\n    else:\n        if enabled_formats == 0:\n            raise click.UsageError(\n                \"Need to enable at least one format to test \"\n                \"(IPC, Flight, C Data Interface); try --help\")\n        if enabled_languages == 0:\n            raise click.UsageError(\n                \"Need to enable at least one language to test; try --help\")\n        run_all_tests(**args)\n\n\n@archery.command()\n@click.option('--arrow-token', envvar='ARROW_GITHUB_TOKEN',\n              help='OAuth token for responding comment in the arrow repo')\n@click.option('--committers-file', '-c', type=click.File('r', encoding='utf8'))\n@click.option('--event-name', '-n', required=True)\n@click.option('--event-payload', '-p', type=click.File('r', encoding='utf8'),\n              default='-', required=True)\ndef trigger_bot(arrow_token, committers_file, event_name, event_payload):\n    from .bot import CommentBot, PullRequestWorkflowBot, actions\n    from ruamel.yaml import YAML\n\n    event_payload = json.loads(event_payload.read())\n    if 'comment' in event_name:\n        bot = CommentBot(name='github-actions', handler=actions, token=arrow_token)\n        bot.handle(event_name, event_payload)\n    else:\n        committers = None\n        if committers_file:\n            committers = [committer['alias']\n                          for committer in YAML().load(committers_file)]\n        bot = PullRequestWorkflowBot(event_name, event_payload, token=arrow_token,\n                                     committers=committers)\n        bot.handle()\n\n\n@archery.group(\"linking\")\n@click.pass_obj\ndef linking(obj):\n    \"\"\"\n    Quick and dirty utilities for checking library linkage.\n    \"\"\"\n    pass\n\n\n@linking.command(\"check-dependencies\")\n@click.argument(\"paths\", nargs=-1)\n@click.option(\"--allow\", \"-a\", \"allowed\", multiple=True,\n              help=\"Name of the allowed libraries\")\n@click.option(\"--disallow\", \"-d\", \"disallowed\", multiple=True,\n              help=\"Name of the disallowed libraries\")\n@click.pass_obj\ndef linking_check_dependencies(obj, allowed, disallowed, paths):\n    from .linking import check_dynamic_library_dependencies, DependencyError\n\n    allowed, disallowed = set(allowed), set(disallowed)\n    try:\n        for path in map(pathlib.Path, paths):\n            check_dynamic_library_dependencies(path, allowed=allowed,\n                                               disallowed=disallowed)\n    except DependencyError as e:\n        raise click.ClickException(str(e))\n\n\nadd_optional_command(\"docker\", module=\".docker.cli\", function=\"docker\",\n                     parent=archery)\nadd_optional_command(\"release\", module=\".release.cli\", function=\"release\",\n                     parent=archery)\nadd_optional_command(\"crossbow\", module=\".crossbow.cli\", function=\"crossbow\",\n                     parent=archery)\n\n\nif __name__ == \"__main__\":\n    archery(obj={})\n", "dev/archery/archery/linking.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport platform\nimport subprocess\n\nfrom .utils.command import Command\n\n\n_ldd = Command(\"ldd\")\n_otool = Command(\"otool\")\n\n\nclass DependencyError(Exception):\n    pass\n\n\nclass DynamicLibrary:\n\n    def __init__(self, path):\n        self.path = path\n\n    def list_dependencies(self):\n        \"\"\"\n        List the full name of the library dependencies.\n        \"\"\"\n        system = platform.system()\n        if system == \"Linux\":\n            result = _ldd.run(self.path, stdout=subprocess.PIPE)\n            lines = result.stdout.splitlines()\n            return [ll.split(None, 1)[0].decode() for ll in lines]\n        elif system == \"Darwin\":\n            result = _otool.run(\"-L\", self.path, stdout=subprocess.PIPE)\n            lines = result.stdout.splitlines()\n            return [dl.split(None, 1)[0].decode() for dl in lines]\n        else:\n            raise ValueError(f\"{platform} is not supported\")\n\n    def list_dependency_names(self):\n        \"\"\"\n        List the truncated names of the dynamic library dependencies.\n        \"\"\"\n        names = []\n        for dependency in self.list_dependencies():\n            *_, library = dependency.rsplit(\"/\", 1)\n            name, *_ = library.split(\".\", 1)\n            names.append(name)\n        return names\n\n\ndef check_dynamic_library_dependencies(path, allowed, disallowed):\n    dylib = DynamicLibrary(path)\n    for dep in dylib.list_dependency_names():\n        if allowed and dep not in allowed:\n            raise DependencyError(\n                f\"Unexpected shared dependency found in {dylib.path}: `{dep}`\"\n            )\n        if disallowed and dep in disallowed:\n            raise DependencyError(\n                f\"Disallowed shared dependency found in {dylib.path}: `{dep}`\"\n            )\n", "dev/archery/archery/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "dev/archery/archery/compat.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pathlib\nimport sys\n\n\ndef _is_path_like(path):\n    return isinstance(path, str) or hasattr(path, '__fspath__')\n\n\ndef _ensure_path(path):\n    if isinstance(path, pathlib.Path):\n        return path\n    else:\n        return pathlib.Path(_stringify_path(path))\n\n\ndef _stringify_path(path):\n    \"\"\"\n    Convert *path* to a string or unicode path if possible.\n    \"\"\"\n    if isinstance(path, str):\n        return path\n\n    # checking whether path implements the filesystem protocol\n    try:\n        return path.__fspath__()\n    except AttributeError:\n        pass\n\n    raise TypeError(\"not a path-like object\")\n\n\ndef _import_pandas():\n    # ARROW-13425: avoid importing PyArrow from Pandas\n    sys.modules['pyarrow'] = None\n    import pandas as pd\n    return pd\n\n\ndef _get_module(obj, *, default=None):\n    \"\"\"\n    Try to find the name of the module *obj* is defined on.\n    \"\"\"\n    try:\n        return obj.__module__\n    except AttributeError:\n        # Might be a method/property descriptor as generated by Cython,\n        # look up the enclosing class.\n        try:\n            return obj.__objclass__.__module__\n        except AttributeError:\n            return default\n", "dev/archery/archery/bot.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport enum\nimport os\nimport shlex\nfrom pathlib import Path\nfrom functools import lru_cache, partial\nimport tempfile\n\nimport click\nimport github\n\nfrom .utils.git import git\nfrom .utils.logger import logger\nfrom .crossbow import Repo, Queue, Config, Target, Job, CommentReport\n\n\ndef cached_property(fn):\n    return property(lru_cache(maxsize=1)(fn))\n\n\nclass EventError(Exception):\n    pass\n\n\nclass CommandError(Exception):\n\n    def __init__(self, message):\n        self.message = message\n\n\nclass _CommandMixin:\n\n    def get_help_option(self, ctx):\n        def show_help(ctx, param, value):\n            if value and not ctx.resilient_parsing:\n                raise click.UsageError(ctx.get_help())\n        option = super().get_help_option(ctx)\n        option.callback = show_help\n        return option\n\n    def __call__(self, message, **kwargs):\n        args = shlex.split(message)\n        try:\n            with self.make_context(self.name, args=args, obj=kwargs) as ctx:\n                return self.invoke(ctx)\n        except click.ClickException as e:\n            raise CommandError(e.format_message())\n\n\nclass Command(_CommandMixin, click.Command):\n    pass\n\n\nclass Group(_CommandMixin, click.Group):\n\n    def command(self, *args, **kwargs):\n        kwargs.setdefault('cls', Command)\n        return super().command(*args, **kwargs)\n\n    def group(self, *args, **kwargs):\n        kwargs.setdefault('cls', Group)\n        return super().group(*args, **kwargs)\n\n    def parse_args(self, ctx, args):\n        if not args and self.no_args_is_help and not ctx.resilient_parsing:\n            raise click.UsageError(ctx.get_help())\n        return super().parse_args(ctx, args)\n\n\ncommand = partial(click.command, cls=Command)\ngroup = partial(click.group, cls=Group)\n\n\nLABEL_PREFIX = \"awaiting\"\n\n\n@enum.unique\nclass PullRequestState(enum.Enum):\n    \"\"\"State of a pull request.\"\"\"\n\n    review = f\"{LABEL_PREFIX} review\"\n    committer_review = f\"{LABEL_PREFIX} committer review\"\n    changes = f\"{LABEL_PREFIX} changes\"\n    change_review = f\"{LABEL_PREFIX} change review\"\n    merge = f\"{LABEL_PREFIX} merge\"\n\n\nCOMMITTER_ROLES = {'OWNER', 'MEMBER'}\n\n\nclass PullRequestWorkflowBot:\n\n    def __init__(self, event_name, event_payload, token=None, committers=None):\n        kwargs = {}\n        if token is not None:\n            kwargs[\"auth\"] = github.Auth.Token(token)\n        self.github = github.Github(**kwargs)\n        self.event_name = event_name\n        self.event_payload = event_payload\n        self.committers = committers\n\n    @cached_property\n    def pull(self):\n        \"\"\"\n        Returns a github.PullRequest object associated with the event.\n        \"\"\"\n        return self.repo.get_pull(self.event_payload['pull_request']['number'])\n\n    @cached_property\n    def repo(self):\n        return self.github.get_repo(self.event_payload['repository']['id'], lazy=True)\n\n    def is_committer(self, action):\n        \"\"\"\n        Returns whether the author of the action is a committer or not.\n        If the list of committer usernames is not available it will use the\n        author_association as a fallback mechanism.\n        \"\"\"\n        if self.committers:\n            return (self.event_payload[action]['user']['login'] in\n                    self.committers)\n        return (self.event_payload[action]['author_association'] in\n                COMMITTER_ROLES)\n\n    def handle(self):\n        current_state = None\n        try:\n            current_state = self.get_current_state()\n        except EventError:\n            # In case of error (more than one state) we clear state labels\n            # only possible if a label has been manually added.\n            self.clear_current_state()\n        next_state = self.compute_next_state(current_state)\n        if not current_state or current_state != next_state:\n            if current_state:\n                self.clear_current_state()\n            self.set_state(next_state)\n\n    def get_current_state(self):\n        \"\"\"\n        Returns a PullRequestState with the current PR state label\n        based on label starting with LABEL_PREFIX.\n        If more than one label is found raises EventError.\n        If no label is found returns None.\n        \"\"\"\n        states = [label.name for label in self.pull.get_labels()\n                  if label.name.startswith(LABEL_PREFIX)]\n        if len(states) > 1:\n            raise EventError(f\"PR cannot be on more than one states - {states}\")\n        elif states:\n            return PullRequestState(states[0])\n\n    def clear_current_state(self):\n        \"\"\"\n        Removes all existing labels starting with LABEL_PREFIX\n        \"\"\"\n        for label in self.pull.get_labels():\n            if label.name.startswith(LABEL_PREFIX):\n                self.pull.remove_from_labels(label)\n\n    def compute_next_state(self, current_state):\n        \"\"\"\n        Returns the expected next state based on the event and\n        the current state.\n        \"\"\"\n        if (self.event_name == \"pull_request_target\" and\n                self.event_payload['action'] == 'opened'):\n            if self.is_committer('pull_request'):\n                return PullRequestState.committer_review\n            else:\n                return PullRequestState.review\n        elif (self.event_name == \"pull_request_review\" and\n                self.event_payload[\"action\"] == \"submitted\"):\n            review_state = self.event_payload[\"review\"][\"state\"].lower()\n            if not self.is_committer('review'):\n                # Non-committer reviews cannot change state once committer has already\n                # reviewed, requested changes or approved\n                if current_state in (\n                        PullRequestState.change_review,\n                        PullRequestState.changes,\n                        PullRequestState.merge):\n                    return current_state\n                else:\n                    return PullRequestState.committer_review\n            if review_state == 'approved':\n                return PullRequestState.merge\n            else:\n                return PullRequestState.changes\n        elif (self.event_name == \"pull_request_target\" and\n              self.event_payload['action'] == 'synchronize' and\n              current_state == PullRequestState.changes):\n            return PullRequestState.change_review\n        # Default already opened PRs to Review state.\n        if current_state is None:\n            current_state = PullRequestState.review\n        return current_state\n\n    def set_state(self, state):\n        \"\"\"Sets the State label to the PR.\"\"\"\n        self.pull.add_to_labels(state.value)\n\n\nclass CommentBot:\n\n    def __init__(self, name, handler, token=None):\n        # TODO(kszucs): validate\n        assert isinstance(name, str)\n        assert callable(handler)\n        self.name = name\n        self.handler = handler\n        kwargs = {}\n        if token is not None:\n            kwargs[\"auth\"] = github.Auth.Token(token)\n        self.github = github.Github(**kwargs)\n\n    def parse_command(self, payload):\n        mention = '@{}'.format(self.name)\n        comment = payload['comment']\n\n        if payload['sender']['login'] == self.name:\n            raise EventError(\"Don't respond to itself\")\n        elif payload['action'] not in {'created', 'edited'}:\n            raise EventError(\"Don't respond to comment deletion\")\n        elif not comment['body'].lstrip().startswith(mention):\n            raise EventError(\"The bot is not mentioned\")\n\n        # Parse the comment, removing the bot mentioned (and everything\n        # before it)\n        command = payload['comment']['body'].split(mention)[-1]\n\n        # then split on newlines and keep only the first line\n        # (ignoring all other lines)\n        return command.split(\"\\n\")[0].strip()\n\n    def handle(self, event, payload):\n        try:\n            command = self.parse_command(payload)\n        except EventError as e:\n            logger.error(e)\n            # see the possible reasons in the validate method\n            return\n\n        if event == 'issue_comment':\n            return self.handle_issue_comment(command, payload)\n        elif event == 'pull_request_review_comment':\n            return self.handle_review_comment(command, payload)\n        else:\n            raise ValueError(\"Unexpected event type {}\".format(event))\n\n    def handle_issue_comment(self, command, payload):\n        repo = self.github.get_repo(payload['repository']['id'], lazy=True)\n        issue = repo.get_issue(payload['issue']['number'])\n\n        try:\n            pull = issue.as_pull_request()\n        except github.GithubException:\n            return issue.create_comment(\n                \"The comment bot only listens to pull request comments!\"\n            )\n\n        comment = pull.get_issue_comment(payload['comment']['id'])\n        try:\n            # Only allow users of apache org to submit commands, for more see\n            # https://developer.github.com/v4/enum/commentauthorassociation/\n            # Checking  privileges here enables the bot to respond\n            # without relying on the handler.\n            allowed_roles = {'OWNER', 'MEMBER', 'COLLABORATOR'}\n            if payload['comment']['author_association'] not in allowed_roles:\n                raise EventError(\n                    \"Only contributors can submit requests to this bot. \"\n                    \"Please ask someone from the community for help with \"\n                    \"getting the first commit in.\"\n                )\n            self.handler(command, issue=issue, pull_request=pull,\n                         comment=comment)\n        except Exception as e:\n            logger.exception(e)\n            url = \"{server}/{repo}/actions/runs/{run_id}\".format(\n                server=os.environ[\"GITHUB_SERVER_URL\"],\n                repo=os.environ[\"GITHUB_REPOSITORY\"],\n                run_id=os.environ[\"GITHUB_RUN_ID\"],\n            )\n            pull.create_issue_comment(\n                f\"```\\n{e}\\nThe Archery job run can be found at: {url}\\n```\")\n            comment.create_reaction('-1')\n        else:\n            comment.create_reaction('+1')\n\n    def handle_review_comment(self, payload):\n        raise NotImplementedError()\n\n\n@group(name='@github-actions')\n@click.pass_context\ndef actions(ctx):\n    \"\"\"Ursabot\"\"\"\n    ctx.ensure_object(dict)\n\n\n@actions.group()\n@click.option('--crossbow', '-c', default='ursacomputing/crossbow',\n              help='Crossbow repository on github to use')\n@click.pass_obj\ndef crossbow(obj, crossbow):\n    \"\"\"\n    Trigger crossbow builds for this pull request\n    \"\"\"\n    obj['crossbow_repo'] = crossbow\n\n\ndef _clone_arrow_and_crossbow(dest, crossbow_repo, arrow_repo_url, pr_number):\n    \"\"\"\n    Clone the repositories and initialize crossbow objects.\n\n    Parameters\n    ----------\n    dest : Path\n        Filesystem path to clone the repositories to.\n    crossbow_repo : str\n        GitHub repository name, like kszucs/crossbow.\n    arrow_repo_url : str\n        Target Apache Arrow repository's clone URL, such as\n        \"https://github.com/apache/arrow.git\".\n    pr_number : int\n        Target PR number.\n    \"\"\"\n    arrow_path = dest / 'arrow'\n    queue_path = dest / 'crossbow'\n\n    # we use unique branch name instead of fork's branch name to avoid\n    # branch name conflict such as 'main' (GH-39996)\n    local_branch = f'archery/pr-{pr_number}'\n    # 1. clone arrow and checkout the PR's branch\n    pr_ref = f'pull/{pr_number}/head:{local_branch}'\n    git.clone('--no-checkout', arrow_repo_url, str(arrow_path))\n    # fetch the PR's branch into the clone\n    git.fetch('origin', pr_ref, git_dir=arrow_path)\n    # checkout the PR's branch into the clone\n    git.checkout(local_branch, git_dir=arrow_path)\n\n    # 2. clone crossbow repository\n    crossbow_url = 'https://github.com/{}'.format(crossbow_repo)\n    git.clone(crossbow_url, str(queue_path))\n\n    # 3. initialize crossbow objects\n    github_token = os.environ['CROSSBOW_GITHUB_TOKEN']\n    arrow = Repo(arrow_path)\n    queue = Queue(queue_path, github_token=github_token, require_https=True)\n\n    return (arrow, queue)\n\n\n@crossbow.command()\n@click.argument('tasks', nargs=-1, required=False)\n@click.option('--group', '-g', 'groups', multiple=True,\n              help='Submit task groups as defined in tests.yml')\n@click.option('--param', '-p', 'params', multiple=True,\n              help='Additional task parameters for rendering the CI templates')\n@click.option('--arrow-version', '-v', default=None,\n              help='Set target version explicitly.')\n@click.option('--wait', default=60,\n              help='Wait the specified seconds before generating a report.')\n@click.pass_obj\ndef submit(obj, tasks, groups, params, arrow_version, wait):\n    \"\"\"\n    Submit crossbow testing tasks.\n\n    See groups defined in arrow/dev/tasks/tasks.yml\n    \"\"\"\n    crossbow_repo = obj['crossbow_repo']\n    pull_request = obj['pull_request']\n    with tempfile.TemporaryDirectory() as tmpdir:\n        tmpdir = Path(tmpdir)\n        arrow, queue = _clone_arrow_and_crossbow(\n            dest=Path(tmpdir),\n            crossbow_repo=crossbow_repo,\n            arrow_repo_url=pull_request.base.repo.clone_url,\n            pr_number=pull_request.number,\n        )\n        # load available tasks configuration and groups from yaml\n        config = Config.load_yaml(arrow.path / \"dev\" / \"tasks\" / \"tasks.yml\")\n        config.validate()\n\n        # initialize the crossbow build's target repository\n        target = Target.from_repo(arrow, version=arrow_version,\n                                  remote=pull_request.head.repo.clone_url,\n                                  branch=pull_request.head.ref)\n\n        # parse additional job parameters\n        params = dict([p.split(\"=\") for p in params])\n        params['pr_number'] = pull_request.number\n\n        # instantiate the job object\n        job = Job.from_config(config=config, target=target, tasks=tasks,\n                              groups=groups, params=params)\n\n        # add the job to the crossbow queue and push to the remote repository\n        queue.put(job, prefix=\"actions\", increment_job_id=False)\n        queue.push()\n\n        # render the response comment's content\n        report = CommentReport(job, crossbow_repo=crossbow_repo,\n                               wait_for_task=wait)\n\n        # send the response\n        pull_request.create_issue_comment(report.show())\n", "dev/archery/archery/lang/python.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom contextlib import contextmanager\nfrom enum import EnumMeta\nimport inspect\nimport tokenize\n\ntry:\n    from numpydoc.validate import Docstring, validate\nexcept ImportError:\n    have_numpydoc = False\nelse:\n    have_numpydoc = True\n\nfrom ..compat import _get_module\nfrom ..utils.logger import logger\nfrom ..utils.command import Command, capture_stdout, default_bin\n\n\nclass PythonCommand(Command):\n    def __init__(self, python_bin=None):\n        self.bin = default_bin(python_bin, \"python\")\n\n\nclass Flake8(Command):\n    def __init__(self, flake8_bin=None):\n        self.bin = default_bin(flake8_bin, \"flake8\")\n\n\nclass CythonLint(Command):\n    def __init__(self, cython_lint_bin=None):\n        self.bin = default_bin(cython_lint_bin, \"cython-lint\")\n\n\nclass Autopep8(Command):\n    def __init__(self, autopep8_bin=None):\n        self.bin = default_bin(autopep8_bin, \"autopep8\")\n\n    @capture_stdout()\n    def run_captured(self, *args, **kwargs):\n        return self.run(*args, **kwargs)\n\n\ndef _tokenize_signature(s):\n    lines = s.encode('ascii').splitlines()\n    generator = iter(lines).__next__\n    return tokenize.tokenize(generator)\n\n\ndef _convert_typehint(tokens):\n    names = []\n    opening_bracket_reached = False\n    for token in tokens:\n        # omit the tokens before the opening bracket\n        if not opening_bracket_reached:\n            if token.string == '(':\n                opening_bracket_reached = True\n            else:\n                continue\n\n        if token.type == 1:  # type 1 means NAME token\n            names.append(token)\n        else:\n            if len(names) == 1:\n                yield (names[0].type, names[0].string)\n            elif len(names) == 2:\n                # two \"NAME\" tokens follow each other which means a cython\n                # typehint like `bool argument`, so remove the typehint\n                # note that we could convert it to python typehints, but hints\n                # are not supported by _signature_fromstr\n                yield (names[1].type, names[1].string)\n            elif len(names) > 2:\n                raise ValueError('More than two NAME tokens follow each other')\n            names = []\n            yield (token.type, token.string)\n\n\ndef inspect_signature(obj):\n    \"\"\"\n    Custom signature inspection primarily for cython generated callables.\n\n    Cython puts the signatures to the first line of the docstrings, which we\n    can reuse to parse the python signature from, but some gymnastics are\n    required, like removing the cython typehints.\n\n    It converts the cython signature:\n        array(obj, type=None, mask=None, size=None, from_pandas=None,\n              bool safe=True, MemoryPool memory_pool=None)\n    To:\n        <Signature (obj, type=None, mask=None, size=None, from_pandas=None,\n                    safe=True, memory_pool=None)>\n    \"\"\"\n    cython_signature = obj.__doc__.splitlines()[0]\n    cython_tokens = _tokenize_signature(cython_signature)\n    python_tokens = _convert_typehint(cython_tokens)\n    python_signature = tokenize.untokenize(python_tokens)\n    return inspect._signature_fromstr(inspect.Signature, obj, python_signature)\n\n\nclass NumpyDoc:\n    IGNORE_VALIDATION_ERRORS_FOR_TYPE = {\n        # Enum function signatures should never be documented\n        EnumMeta: [\"PR01\"]\n    }\n\n    def __init__(self, symbols=None):\n        if not have_numpydoc:\n            raise RuntimeError(\n                'Numpydoc is not available, install with command: '\n                'pip install numpydoc==1.1.0'\n            )\n        self.symbols = set(symbols or {'pyarrow'})\n\n    def traverse(self, fn, obj, from_package):\n        \"\"\"Apply a function on publicly exposed API components.\n\n        Recursively iterates over the members of the passed object. It omits\n        any '_' prefixed and thirdparty (non pyarrow) symbols.\n\n        Parameters\n        ----------\n        fn : callable\n            A function to apply on all traversed objects.\n        obj : Any\n            The object to start from.\n        from_package : string\n            Predicate to only consider objects from this package.\n        \"\"\"\n        todo = [obj]\n        seen = set()\n\n        while todo:\n            obj = todo.pop()\n            if obj in seen:\n                continue\n            else:\n                seen.add(obj)\n\n            fn(obj)\n\n            for name in dir(obj):\n                if name.startswith('_'):\n                    continue\n\n                member = getattr(obj, name)\n                module = _get_module(member)\n                if module is None or not module.startswith(from_package):\n                    continue\n                # Is it a Cython-generated method? If so, try to detect\n                # whether it only has a implicitly-generated docstring,\n                # and no user-defined docstring following it.\n                # The generated docstring would lack description of method\n                # parameters and therefore fail Numpydoc validation.\n                if hasattr(member, '__objclass__'):\n                    doc = getattr(member, '__doc__', None)\n                    # The Cython-generated docstring would be a one-liner,\n                    # such as \"ReadOptions.equals(self, ReadOptions other)\".\n                    if (doc and '\\n' not in doc and f'.{name}(' in doc):\n                        continue\n                todo.append(member)\n\n    @contextmanager\n    def _apply_patches(self):\n        \"\"\"\n        Patch Docstring class to bypass loading already loaded python objects.\n        \"\"\"\n        orig_load_obj = Docstring._load_obj\n        orig_signature = inspect.signature\n\n        @staticmethod\n        def _load_obj(obj):\n            # By default it expects a qualname and import the object, but we\n            # have already loaded object after the API traversal.\n            if isinstance(obj, str):\n                return orig_load_obj(obj)\n            else:\n                return obj\n\n        def signature(obj):\n            # inspect.signature tries to parse __text_signature__ if other\n            # properties like __signature__ doesn't exists, but cython\n            # doesn't set that property despite that embedsignature cython\n            # directive is set. The only way to inspect a cython compiled\n            # callable's signature to parse it from __doc__ while\n            # embedsignature directive is set during the build phase.\n            # So path inspect.signature function to attempt to parse the first\n            # line of callable.__doc__ as a signature.\n            try:\n                return orig_signature(obj)\n            except Exception as orig_error:\n                try:\n                    return inspect_signature(obj)\n                except Exception:\n                    raise orig_error\n\n        try:\n            Docstring._load_obj = _load_obj\n            inspect.signature = signature\n            yield\n        finally:\n            Docstring._load_obj = orig_load_obj\n            inspect.signature = orig_signature\n\n    def validate(self, from_package='', allow_rules=None,\n                 disallow_rules=None):\n        results = []\n\n        def callback(obj):\n            try:\n                result = validate(obj)\n            except OSError as e:\n                symbol = f\"{_get_module(obj, default='')}.{obj.__name__}\"\n                logger.warning(f\"Unable to validate `{symbol}` due to `{e}`\")\n                return\n\n            errors = []\n            for errcode, errmsg in result.get('errors', []):\n                if allow_rules and errcode not in allow_rules:\n                    continue\n                if disallow_rules and errcode in disallow_rules:\n                    continue\n                if any(isinstance(obj, obj_type) and errcode in errcode_list\n                       for obj_type, errcode_list\n                       in NumpyDoc.IGNORE_VALIDATION_ERRORS_FOR_TYPE.items()):\n                    continue\n                errors.append((errcode, errmsg))\n\n            if len(errors):\n                result['errors'] = errors\n                results.append((obj, result))\n\n        with self._apply_patches():\n            for symbol in self.symbols:\n                try:\n                    obj = Docstring._load_obj(symbol)\n                except (ImportError, AttributeError):\n                    print('{} is not available for import'.format(symbol))\n                else:\n                    self.traverse(callback, obj, from_package=from_package)\n\n        return results\n", "dev/archery/archery/lang/java.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nfrom ..utils.command import Command, CommandStackMixin, default_bin\nfrom ..utils.maven import MavenDefinition\n\n\nclass Java(Command):\n    def __init__(self, java_bin=None):\n        self.bin = default_bin(java_bin, \"java\")\n\n\nclass Jar(CommandStackMixin, Java):\n    def __init__(self, jar, *args, **kwargs):\n        self.jar = jar\n        self.argv = (\"-jar\", jar)\n        Java.__init__(self, *args, **kwargs)\n\n\nclass JavaConfiguration:\n    def __init__(self,\n\n                 # toolchain\n                 java_home=None, java_options=None,\n                 # build & benchmark\n                 build_extras=None, benchmark_extras=None):\n        self.java_home = java_home\n        self.java_options = java_options\n\n        self.build_extras = list(build_extras) if build_extras else []\n        self.benchmark_extras = list(\n            benchmark_extras) if benchmark_extras else []\n\n    @property\n    def build_definitions(self):\n        return self.build_extras\n\n    @property\n    def benchmark_definitions(self):\n        return self.benchmark_extras\n\n    @property\n    def environment(self):\n        env = os.environ.copy()\n\n        if self.java_home:\n            env[\"JAVA_HOME\"] = self.java_home\n\n        if self.java_options:\n            env[\"JAVA_OPTIONS\"] = self.java_options\n\n        return env\n\n\nclass JavaMavenDefinition(MavenDefinition):\n    def __init__(self, source, conf, **kwargs):\n        self.configuration = conf\n        super().__init__(source, **kwargs,\n                         build_definitions=conf.build_definitions,\n                         benchmark_definitions=conf.benchmark_definitions,\n                         env=conf.environment)\n", "dev/archery/archery/lang/cpp.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nfrom ..utils.cmake import CMakeDefinition\n\n\ndef truthifier(value):\n    return \"ON\" if value else \"OFF\"\n\n\ndef or_else(value, default):\n    return value if value else default\n\n\ndef coalesce(value, fallback):\n    return fallback if value is None else value\n\n\nLLVM_VERSION = 7\n\n\nclass CppConfiguration:\n    def __init__(self,\n\n                 # toolchain\n                 cc=None, cxx=None, cxx_flags=None,\n                 build_type=None, warn_level=None,\n                 cpp_package_prefix=None, install_prefix=None, use_conda=None,\n                 build_static=True, build_shared=True, build_unity=True,\n                 # tests & examples\n                 with_tests=None, with_benchmarks=None, with_examples=None,\n                 with_integration=None,\n                 # static checks\n                 use_asan=None, use_tsan=None, use_ubsan=None,\n                 with_fuzzing=None,\n                 # Components\n                 with_compute=None, with_csv=None, with_cuda=None,\n                 with_dataset=None, with_filesystem=None, with_flight=None,\n                 with_gandiva=None, with_gcs=None, with_hdfs=None,\n                 with_hiveserver2=None,\n                 with_ipc=True, with_json=None,\n                 with_mimalloc=None, with_jemalloc=None,\n                 with_parquet=None, with_python=True,\n                 with_r=None, with_s3=None,\n                 # Compressions\n                 with_brotli=None, with_bz2=None, with_lz4=None,\n                 with_snappy=None, with_zlib=None, with_zstd=None,\n                 # extras\n                 with_lint_only=False,\n                 use_gold_linker=True,\n                 simd_level=\"DEFAULT\",\n                 cmake_extras=None):\n        self._cc = cc\n        self._cxx = cxx\n        self.cxx_flags = cxx_flags\n\n        self._build_type = build_type\n        self.warn_level = warn_level\n        self._install_prefix = install_prefix\n        self._package_prefix = cpp_package_prefix\n        self._use_conda = use_conda\n        self.build_static = build_static\n        self.build_shared = build_shared\n        self.build_unity = build_unity\n\n        self.with_tests = with_tests\n        self.with_benchmarks = with_benchmarks\n        self.with_examples = with_examples\n        self.with_integration = with_integration\n\n        self.use_asan = use_asan\n        self.use_tsan = use_tsan\n        self.use_ubsan = use_ubsan\n        self.with_fuzzing = with_fuzzing\n\n        self.with_compute = with_compute\n        self.with_csv = with_csv\n        self.with_cuda = with_cuda\n        self.with_dataset = with_dataset\n        self.with_filesystem = with_filesystem\n        self.with_flight = with_flight\n        self.with_gandiva = with_gandiva\n        self.with_gcs = with_gcs\n        self.with_hdfs = with_hdfs\n        self.with_hiveserver2 = with_hiveserver2\n        self.with_ipc = with_ipc\n        self.with_json = with_json\n        self.with_mimalloc = with_mimalloc\n        self.with_jemalloc = with_jemalloc\n        self.with_parquet = with_parquet\n        self.with_python = with_python\n        self.with_r = with_r\n        self.with_s3 = with_s3\n\n        self.with_brotli = with_brotli\n        self.with_bz2 = with_bz2\n        self.with_lz4 = with_lz4\n        self.with_snappy = with_snappy\n        self.with_zlib = with_zlib\n        self.with_zstd = with_zstd\n\n        self.with_lint_only = with_lint_only\n        self.use_gold_linker = use_gold_linker\n        self.simd_level = simd_level\n\n        self.cmake_extras = cmake_extras\n\n        # Fixup required dependencies by providing sane defaults if the caller\n        # didn't specify the option.\n        if self.with_r:\n            self.with_csv = coalesce(with_csv, True)\n            self.with_dataset = coalesce(with_dataset, True)\n            self.with_filesystem = coalesce(with_filesystem, True)\n            self.with_ipc = coalesce(with_ipc, True)\n            self.with_json = coalesce(with_json, True)\n            self.with_parquet = coalesce(with_parquet, True)\n\n        if self.with_python:\n            self.with_compute = coalesce(with_compute, True)\n            self.with_csv = coalesce(with_csv, True)\n            self.with_dataset = coalesce(with_dataset, True)\n            self.with_filesystem = coalesce(with_filesystem, True)\n            self.with_hdfs = coalesce(with_hdfs, True)\n            self.with_json = coalesce(with_json, True)\n            self.with_lz4 = coalesce(with_lz4, True)\n            self.with_zlib = coalesce(with_zlib, True)\n\n        if self.with_dataset:\n            self.with_filesystem = coalesce(with_filesystem, True)\n            self.with_parquet = coalesce(with_parquet, True)\n\n        if self.with_parquet:\n            self.with_snappy = coalesce(with_snappy, True)\n\n    @property\n    def build_type(self):\n        if self._build_type:\n            return self._build_type\n\n        if self.with_fuzzing:\n            return \"relwithdebinfo\"\n\n        return \"release\"\n\n    @property\n    def cc(self):\n        if self._cc:\n            return self._cc\n\n        if self.with_fuzzing:\n            return \"clang-{}\".format(LLVM_VERSION)\n\n        return None\n\n    @property\n    def cxx(self):\n        if self._cxx:\n            return self._cxx\n\n        if self.with_fuzzing:\n            return \"clang++-{}\".format(LLVM_VERSION)\n\n        return None\n\n    def _gen_defs(self):\n        if self.cxx_flags:\n            yield (\"ARROW_CXXFLAGS\", self.cxx_flags)\n\n        yield (\"CMAKE_EXPORT_COMPILE_COMMANDS\", truthifier(True))\n        yield (\"CMAKE_BUILD_TYPE\", self.build_type)\n\n        if not self.with_lint_only:\n            yield (\"BUILD_WARNING_LEVEL\",\n                   or_else(self.warn_level, \"production\"))\n\n        # if not ctx.quiet:\n        #   yield (\"ARROW_VERBOSE_THIRDPARTY_BUILD\", \"ON\")\n\n        maybe_prefix = self.install_prefix\n        if maybe_prefix:\n            yield (\"CMAKE_INSTALL_PREFIX\", maybe_prefix)\n\n        if self._package_prefix is not None:\n            yield (\"ARROW_DEPENDENCY_SOURCE\", \"SYSTEM\")\n            yield (\"ARROW_PACKAGE_PREFIX\", self._package_prefix)\n\n        yield (\"ARROW_BUILD_STATIC\", truthifier(self.build_static))\n        yield (\"ARROW_BUILD_SHARED\", truthifier(self.build_shared))\n        yield (\"CMAKE_UNITY_BUILD\", truthifier(self.build_unity))\n\n        # Tests and benchmarks\n        yield (\"ARROW_BUILD_TESTS\", truthifier(self.with_tests))\n        yield (\"ARROW_BUILD_BENCHMARKS\", truthifier(self.with_benchmarks))\n        yield (\"ARROW_BUILD_EXAMPLES\", truthifier(self.with_examples))\n        yield (\"ARROW_BUILD_INTEGRATION\", truthifier(self.with_integration))\n\n        # Static checks\n        yield (\"ARROW_USE_ASAN\", truthifier(self.use_asan))\n        yield (\"ARROW_USE_TSAN\", truthifier(self.use_tsan))\n        yield (\"ARROW_USE_UBSAN\", truthifier(self.use_ubsan))\n        yield (\"ARROW_FUZZING\", truthifier(self.with_fuzzing))\n\n        # Components\n        yield (\"ARROW_COMPUTE\", truthifier(self.with_compute))\n        yield (\"ARROW_CSV\", truthifier(self.with_csv))\n        yield (\"ARROW_CUDA\", truthifier(self.with_cuda))\n        yield (\"ARROW_DATASET\", truthifier(self.with_dataset))\n        yield (\"ARROW_FILESYSTEM\", truthifier(self.with_filesystem))\n        yield (\"ARROW_FLIGHT\", truthifier(self.with_flight))\n        yield (\"ARROW_GANDIVA\", truthifier(self.with_gandiva))\n        yield (\"ARROW_GCS\", truthifier(self.with_gcs))\n        yield (\"ARROW_HDFS\", truthifier(self.with_hdfs))\n        yield (\"ARROW_IPC\", truthifier(self.with_ipc))\n        yield (\"ARROW_JSON\", truthifier(self.with_json))\n        yield (\"ARROW_MIMALLOC\", truthifier(self.with_mimalloc))\n        yield (\"ARROW_JEMALLOC\", truthifier(self.with_jemalloc))\n        yield (\"ARROW_PARQUET\", truthifier(self.with_parquet))\n        yield (\"ARROW_S3\", truthifier(self.with_s3))\n\n        # Compressions\n        yield (\"ARROW_WITH_BROTLI\", truthifier(self.with_brotli))\n        yield (\"ARROW_WITH_BZ2\", truthifier(self.with_bz2))\n        yield (\"ARROW_WITH_LZ4\", truthifier(self.with_lz4))\n        yield (\"ARROW_WITH_SNAPPY\", truthifier(self.with_snappy))\n        yield (\"ARROW_WITH_ZLIB\", truthifier(self.with_zlib))\n        yield (\"ARROW_WITH_ZSTD\", truthifier(self.with_zstd))\n\n        yield (\"ARROW_LINT_ONLY\", truthifier(self.with_lint_only))\n\n        # Some configurations don't like gnu gold linker.\n        broken_with_gold_ld = [self.with_fuzzing, self.with_gandiva]\n        if self.use_gold_linker and not any(broken_with_gold_ld):\n            yield (\"ARROW_USE_LD_GOLD\", truthifier(self.use_gold_linker))\n        yield (\"ARROW_SIMD_LEVEL\", or_else(self.simd_level, \"DEFAULT\"))\n\n        # Detect custom conda toolchain\n        if self.use_conda:\n            for d, v in [('CMAKE_AR', 'AR'), ('CMAKE_RANLIB', 'RANLIB')]:\n                v = os.environ.get(v)\n                if v:\n                    yield (d, v)\n\n    @property\n    def install_prefix(self):\n        if self._install_prefix:\n            return self._install_prefix\n\n        if self.use_conda:\n            return os.environ.get(\"CONDA_PREFIX\")\n\n        return None\n\n    @property\n    def use_conda(self):\n        # If the user didn't specify a preference, guess via environment\n        if self._use_conda is None:\n            return os.environ.get(\"CONDA_PREFIX\") is not None\n\n        return self._use_conda\n\n    @property\n    def definitions(self):\n        extras = list(self.cmake_extras) if self.cmake_extras else []\n        definitions = [\"-D{}={}\".format(d[0], d[1]) for d in self._gen_defs()]\n        return definitions + extras\n\n    @property\n    def environment(self):\n        env = os.environ.copy()\n\n        if self.cc:\n            env[\"CC\"] = self.cc\n\n        if self.cxx:\n            env[\"CXX\"] = self.cxx\n\n        return env\n\n\nclass CppCMakeDefinition(CMakeDefinition):\n    def __init__(self, source, conf, **kwargs):\n        self.configuration = conf\n        super().__init__(source, **kwargs,\n                         definitions=conf.definitions, env=conf.environment,\n                         build_type=conf.build_type)\n", "dev/archery/archery/lang/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "dev/archery/archery/utils/command.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport shlex\nimport shutil\nimport subprocess\n\nfrom .logger import logger, ctx\n\n\ndef default_bin(name, default):\n    assert default\n    env_name = \"ARCHERY_{0}_BIN\".format(default.upper())\n    return name if name else os.environ.get(env_name, default)\n\n\n# Decorator running a command and returning stdout\nclass capture_stdout:\n    def __init__(self, strip=False, listify=False):\n        self.strip = strip\n        self.listify = listify\n\n    def __call__(self, f):\n        def strip_it(x):\n            return x.strip() if self.strip else x\n\n        def list_it(x):\n            return x.decode('utf-8').splitlines() if self.listify else x\n\n        def wrapper(*argv, **kwargs):\n            # Ensure stdout is captured\n            kwargs[\"stdout\"] = subprocess.PIPE\n            return list_it(strip_it(f(*argv, **kwargs).stdout))\n        return wrapper\n\n\nclass Command:\n    \"\"\"\n    A runnable command.\n\n    Class inheriting from the Command class must provide the bin\n    property/attribute.\n    \"\"\"\n\n    def __init__(self, bin):\n        self.bin = bin\n\n    def run(self, *argv, **kwargs):\n        assert hasattr(self, \"bin\")\n        invocation = shlex.split(self.bin)\n        invocation.extend(argv)\n\n        for key in [\"stdout\", \"stderr\"]:\n            # Preserve caller intention, otherwise silence\n            if key not in kwargs and ctx.quiet:\n                kwargs[key] = subprocess.PIPE\n\n        # Prefer safe by default\n        if \"check\" not in kwargs:\n            kwargs[\"check\"] = True\n\n        logger.debug(\"Executing `{}`\".format(invocation))\n        return subprocess.run(invocation, **kwargs)\n\n    @property\n    def available(self):\n        \"\"\"\n        Indicate if the command binary is found in PATH.\n        \"\"\"\n        binary = shlex.split(self.bin)[0]\n        return shutil.which(binary) is not None\n\n    def __call__(self, *argv, **kwargs):\n        return self.run(*argv, **kwargs)\n\n\nclass CommandStackMixin:\n    def run(self, *argv, **kwargs):\n        stacked_args = self.argv + argv\n        return super(CommandStackMixin, self).run(*stacked_args, **kwargs)\n\n\nclass Bash(Command):\n    def __init__(self, bash_bin=None):\n        self.bin = default_bin(bash_bin, \"bash\")\n", "dev/archery/archery/utils/rat.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport fnmatch\nimport re\nfrom xml.etree import ElementTree\n\nfrom ..lang.java import Jar\nfrom .cache import Cache\nfrom .command import capture_stdout\n\nRAT_VERSION = 0.13\nRAT_JAR_FILENAME = \"apache-rat-{}.jar\".format(RAT_VERSION)\nRAT_URL_ = \"https://repo1.maven.org/maven2/org/apache/rat/apache-rat\"\nRAT_URL = \"/\".join([RAT_URL_, str(RAT_VERSION), RAT_JAR_FILENAME])\n\n\nclass Rat(Jar):\n    def __init__(self):\n        jar = Cache().get_or_insert_from_url(RAT_JAR_FILENAME, RAT_URL)\n        Jar.__init__(self, jar)\n\n    @capture_stdout(strip=False)\n    def run_report(self, archive_path, **kwargs):\n        return self.run(\"--xml\", archive_path, **kwargs)\n\n    def report(self, archive_path, **kwargs):\n        return RatReport(self.run_report(archive_path, **kwargs))\n\n\ndef exclusion_from_globs(exclusions_path):\n    with open(exclusions_path, 'r') as exclusions_fd:\n        exclusions = [e.strip() for e in exclusions_fd]\n        return lambda path: any([fnmatch.fnmatch(path, e) for e in exclusions])\n\n\nclass RatReport:\n    def __init__(self, xml):\n        self.xml = xml\n        self.tree = ElementTree.fromstring(xml)\n\n    def __repr__(self):\n        return \"RatReport({})\".format(self.xml)\n\n    def validate(self, exclusion=None):\n        for r in self.tree.findall('resource'):\n            approvals = r.findall('license-approval')\n            if not approvals or approvals[0].attrib['name'] == 'true':\n                continue\n\n            clean_name = re.sub('^[^/]+/', '', r.attrib['name'])\n\n            if exclusion and exclusion(clean_name):\n                continue\n\n            yield clean_name\n", "dev/archery/archery/utils/cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport importlib\n\nimport click\n\nfrom .source import ArrowSources, InvalidArrowSource\n\n\nclass ArrowBool(click.types.BoolParamType):\n    \"\"\"\n    ArrowBool supports the 'ON' and 'OFF' values on top of the values\n    supported by BoolParamType. This is convenient to port script which exports\n    CMake options variables.\n    \"\"\"\n    name = \"boolean\"\n\n    def convert(self, value, param, ctx):\n        if isinstance(value, str):\n            lowered = value.lower()\n            if lowered == \"on\":\n                return True\n            elif lowered == \"off\":\n                return False\n\n        return super().convert(value, param, ctx)\n\n\ndef validate_arrow_sources(ctx, param, src):\n    \"\"\"\n    Ensure a directory contains Arrow cpp sources.\n    \"\"\"\n    try:\n        return ArrowSources.find(src)\n    except InvalidArrowSource as e:\n        raise click.BadParameter(str(e))\n\n\ndef add_optional_command(name, module, function, parent):\n    try:\n        module = importlib.import_module(module, package=\"archery\")\n        command = getattr(module, function)\n    except ImportError as exc:\n        error_message = exc.name\n\n        @parent.command(\n            name,\n            context_settings={\n                \"allow_extra_args\": True,\n                \"ignore_unknown_options\": True,\n            }\n        )\n        def command():\n            raise click.ClickException(\n                f\"Couldn't import command `{name}` due to {error_message}\"\n            )\n    else:\n        parent.add_command(command)\n", "dev/archery/archery/utils/cmake.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport re\nfrom shutil import rmtree, which\n\nfrom .command import Command, default_bin\n\n\nclass CMake(Command):\n    def __init__(self, cmake_bin=None):\n        self.bin = default_bin(cmake_bin, \"cmake\")\n\n    @staticmethod\n    def default_generator():\n        \"\"\" Infer default generator.\n\n        Gives precedence to ninja if there exists an executable named `ninja`\n        in the search path.\n        \"\"\"\n        found_ninja = which(\"ninja\")\n        return \"Ninja\" if found_ninja else \"Unix Makefiles\"\n\n\ncmake = CMake()\n\n\nclass CMakeDefinition:\n    \"\"\" CMakeDefinition captures the cmake invocation arguments.\n\n    It allows creating build directories with the same definition, e.g.\n    ```\n    build_1 = cmake_def.build(\"/tmp/build-1\")\n    build_2 = cmake_def.build(\"/tmp/build-2\")\n\n    ...\n\n    build1.all()\n    build2.all()\n    \"\"\"\n\n    def __init__(self, source, build_type=\"release\", generator=None,\n                 definitions=None, env=None):\n        \"\"\" Initialize a CMakeDefinition\n\n        Parameters\n        ----------\n        source : str\n                 Source directory where the top-level CMakeLists.txt is\n                 located. This is usually the root of the project.\n        generator : str, optional\n        definitions: list(str), optional\n        env : dict(str,str), optional\n              Environment to use when invoking cmake. This can be required to\n              work around cmake deficiencies, e.g. CC and CXX.\n        \"\"\"\n        self.source = os.path.abspath(source)\n        self.build_type = build_type\n        self.generator = generator if generator else cmake.default_generator()\n        self.definitions = definitions if definitions else []\n        self.env = env\n\n    @property\n    def arguments(self):\n        \"\"\"\" Return the arguments to cmake invocation. \"\"\"\n        arguments = [\n            \"-G{}\".format(self.generator),\n        ] + self.definitions + [\n            self.source\n        ]\n        return arguments\n\n    def build(self, build_dir, force=False, cmd_kwargs=None, **kwargs):\n        \"\"\" Invoke cmake into a build directory.\n\n        Parameters\n        ----------\n        build_dir : str\n                    Directory in which the CMake build will be instantiated.\n        force : bool\n                If the build folder exists, delete it before. Otherwise if it's\n                present, an error will be returned.\n        \"\"\"\n        if os.path.exists(build_dir):\n            # Extra safety to ensure we're deleting a build folder.\n            if not CMakeBuild.is_build_dir(build_dir):\n                raise FileExistsError(\n                    \"{} is not a cmake build\".format(build_dir)\n                )\n            if not force:\n                raise FileExistsError(\n                    \"{} exists use force=True\".format(build_dir)\n                )\n            rmtree(build_dir)\n\n        os.mkdir(build_dir)\n\n        cmd_kwargs = cmd_kwargs if cmd_kwargs else {}\n        cmake(*self.arguments, cwd=build_dir, env=self.env, **cmd_kwargs)\n        return CMakeBuild(build_dir, self.build_type, definition=self,\n                          **kwargs)\n\n    def __repr__(self):\n        return \"CMakeDefinition[source={}]\".format(self.source)\n\n\nCMAKE_BUILD_TYPE_RE = re.compile(\"CMAKE_BUILD_TYPE:STRING=([a-zA-Z]+)\")\n\n\nclass CMakeBuild(CMake):\n    \"\"\" CMakeBuild represents a build directory initialized by cmake.\n\n    The build instance can be used to build/test/install. It alleviates the\n    user to know which generator is used.\n    \"\"\"\n\n    def __init__(self, build_dir, build_type, definition=None):\n        \"\"\" Initialize a CMakeBuild.\n\n        The caller must ensure that cmake was invoked in the build directory.\n\n        Parameters\n        ----------\n        definition : CMakeDefinition\n                     The definition to build from.\n        build_dir : str\n                    The build directory to setup into.\n        \"\"\"\n        assert CMakeBuild.is_build_dir(build_dir)\n        super().__init__()\n        self.build_dir = os.path.abspath(build_dir)\n        self.build_type = build_type\n        self.definition = definition\n\n    @property\n    def binaries_dir(self):\n        return os.path.join(self.build_dir, self.build_type)\n\n    def run(self, *argv, verbose=False, **kwargs):\n        cmake_args = [\"--build\", self.build_dir, \"--\"]\n        extra = []\n        if verbose:\n            extra.append(\"-v\" if self.bin.endswith(\"ninja\") else \"VERBOSE=1\")\n        # Commands must be ran under the build directory\n        return super().run(*cmake_args, *extra,\n                           *argv, **kwargs, cwd=self.build_dir)\n\n    def all(self):\n        return self.run(\"all\")\n\n    def clean(self):\n        return self.run(\"clean\")\n\n    def install(self):\n        return self.run(\"install\")\n\n    def test(self):\n        return self.run(\"test\")\n\n    @staticmethod\n    def is_build_dir(path):\n        \"\"\" Indicate if a path is CMake build directory.\n\n        This method only checks for the existence of paths and does not do any\n        validation whatsoever.\n        \"\"\"\n        cmake_cache = os.path.join(path, \"CMakeCache.txt\")\n        cmake_files = os.path.join(path, \"CMakeFiles\")\n        return os.path.exists(cmake_cache) and os.path.exists(cmake_files)\n\n    @staticmethod\n    def from_path(path):\n        \"\"\" Instantiate a CMakeBuild from a path.\n\n        This is used to recover from an existing physical directory (created\n        with or without CMakeBuild).\n\n        Note that this method is not idempotent as the original definition will\n        be lost. Only build_type is recovered.\n        \"\"\"\n        if not CMakeBuild.is_build_dir(path):\n            raise ValueError(\"Not a valid CMakeBuild path: {}\".format(path))\n\n        build_type = None\n        # Infer build_type by looking at CMakeCache.txt and looking for a magic\n        # definition\n        cmake_cache_path = os.path.join(path, \"CMakeCache.txt\")\n        with open(cmake_cache_path, \"r\") as cmake_cache:\n            candidates = CMAKE_BUILD_TYPE_RE.findall(cmake_cache.read())\n            build_type = candidates[0].lower() if candidates else \"release\"\n\n        return CMakeBuild(path, build_type)\n\n    def __repr__(self):\n        return (\"CMakeBuild[\"\n                \"build = {},\"\n                \"build_type = {},\"\n                \"definition = {}]\".format(self.build_dir,\n                                          self.build_type,\n                                          self.definition))\n", "dev/archery/archery/utils/source.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nfrom pathlib import Path\nimport subprocess\nimport tempfile\n\nfrom .command import Command\nfrom .git import git\n\n\nARROW_ROOT_DEFAULT = os.environ.get(\n    'ARROW_ROOT',\n    Path(__file__).resolve().parents[4]\n)\n\n\ndef arrow_path(path):\n    \"\"\"\n    Return full path to a file given its path inside the Arrow repo.\n    \"\"\"\n    return os.path.join(ARROW_ROOT_DEFAULT, path)\n\n\nclass InvalidArrowSource(Exception):\n    pass\n\n\nclass ArrowSources:\n    \"\"\" ArrowSources is a companion class representing a directory containing\n    Apache Arrow's sources.\n    \"\"\"\n    # Note that WORKSPACE is a reserved git revision name by this module to\n    # reference the current git workspace. In other words, this indicates to\n    # ArrowSources.at_revision that no cloning/checkout is required.\n    WORKSPACE = \"WORKSPACE\"\n\n    def __init__(self, path):\n        \"\"\" Initialize an ArrowSources\n\n        The caller must ensure that path is valid arrow source directory (can\n        be checked with ArrowSources.valid)\n\n        Parameters\n        ----------\n        path : src\n        \"\"\"\n        path = Path(path)\n        # validate by checking a specific path in the arrow source tree\n        if not (path / 'cpp' / 'CMakeLists.txt').exists():\n            raise InvalidArrowSource(\n                \"No Arrow C++ sources found in {}.\".format(path)\n            )\n        self.path = path\n\n    @property\n    def archery(self):\n        \"\"\" Returns the archery directory of an Arrow sources. \"\"\"\n        return self.dev / \"archery\"\n\n    @property\n    def cpp(self):\n        \"\"\" Returns the cpp directory of an Arrow sources. \"\"\"\n        return self.path / \"cpp\"\n\n    @property\n    def dev(self):\n        \"\"\" Returns the dev directory of an Arrow sources. \"\"\"\n        return self.path / \"dev\"\n\n    @property\n    def java(self):\n        \"\"\" Returns the java directory of an Arrow sources. \"\"\"\n        return self.path / \"java\"\n\n    @property\n    def python(self):\n        \"\"\" Returns the python directory of an Arrow sources. \"\"\"\n        return self.path / \"python\"\n\n    @property\n    def pyarrow(self):\n        \"\"\" Returns the python/pyarrow directory of an Arrow sources. \"\"\"\n        return self.python / \"pyarrow\"\n\n    @property\n    def r(self):\n        \"\"\" Returns the r directory of an Arrow sources. \"\"\"\n        return self.path / \"r\"\n\n    @property\n    def git_backed(self):\n        \"\"\" Indicate if the sources are backed by git. \"\"\"\n        return (self.path / \".git\").exists()\n\n    @property\n    def git_dirty(self):\n        \"\"\" Indicate if the sources is a dirty git directory. \"\"\"\n        return self.git_backed and git.dirty(git_dir=self.path)\n\n    def archive(self, path, dereference=False, compressor=None, revision=None):\n        \"\"\" Saves a git archive at path. \"\"\"\n        if not self.git_backed:\n            raise ValueError(\"{} is not backed by git\".format(self))\n\n        rev = revision if revision else \"HEAD\"\n        archive = git.archive(\"--prefix=apache-arrow.tmp/\", rev,\n                              git_dir=self.path)\n        with tempfile.TemporaryDirectory() as tmp:\n            tmp = Path(tmp)\n            tar_path = tmp / \"apache-arrow.tar\"\n            with open(tar_path, \"wb\") as tar:\n                tar.write(archive)\n            Command(\"tar\").run(\"xf\", tar_path, \"-C\", tmp)\n            # Must use the same logic in dev/release/02-source.sh\n            Command(\"cp\").run(\"-R\", \"-L\", tmp /\n                              \"apache-arrow.tmp\", tmp / \"apache-arrow\")\n            Command(\"tar\").run(\"cf\", tar_path, \"-C\", tmp, \"apache-arrow\")\n            with open(tar_path, \"rb\") as tar:\n                archive = tar.read()\n\n        if compressor:\n            archive = compressor(archive)\n\n        with open(path, \"wb\") as archive_fd:\n            archive_fd.write(archive)\n\n    def at_revision(self, revision, clone_dir):\n        \"\"\" Return a copy of the current sources for a specified git revision.\n\n        This method may return the current object if no checkout is required.\n        The caller is responsible to remove the cloned repository directory.\n\n        The user can use the special WORKSPACE token to mean the current git\n        workspace (no checkout performed).\n\n        The second value of the returned tuple indicates if a clone was\n        performed.\n\n        Parameters\n        ----------\n        revision : str\n                   Revision to checkout sources at.\n        clone_dir : str\n                    Path to checkout the local clone.\n        \"\"\"\n        if not self.git_backed:\n            raise ValueError(\"{} is not backed by git\".format(self))\n\n        if revision == ArrowSources.WORKSPACE:\n            return self, False\n\n        # A local clone is required to leave the current sources intact such\n        # that builds depending on said sources are not invalidated (or worse\n        # slightly affected when re-invoking the generator).\n        # \"--local\" only works when dest dir is on same volume of source dir.\n        # \"--shared\" works even if dest dir is on different volume.\n        git.clone(\"--shared\", self.path, clone_dir)\n\n        # Revision can reference \"origin/\" (or any remotes) that are not found\n        # in the local clone. Thus, revisions are dereferenced in the source\n        # repository.\n        original_revision = git.rev_parse(revision)\n\n        git.checkout(original_revision, git_dir=clone_dir)\n\n        return ArrowSources(clone_dir), True\n\n    @staticmethod\n    def find(path=None):\n        \"\"\" Infer Arrow sources directory from various method.\n\n        The following guesses are done in order until a valid match is found:\n\n        1. Checks the given optional parameter.\n\n        2. Checks if the environment variable `ARROW_SRC` is defined and use\n           this.\n\n        3. Checks if the current working directory (cwd) is an Arrow source\n           directory.\n\n        4. Checks if this file (cli.py) is still in the original source\n           repository. If so, returns the relative path to the source\n           directory.\n        \"\"\"\n\n        # Explicit via environment\n        env = os.environ.get(\"ARROW_SRC\")\n\n        # Implicit via cwd\n        cwd = Path.cwd()\n\n        # Implicit via current file\n        try:\n            this = Path(__file__).parents[4]\n        except IndexError:\n            this = None\n\n        # Implicit via git repository (if archery is installed system wide)\n        try:\n            repo = git.repository_root(git_dir=cwd)\n        except subprocess.CalledProcessError:\n            # We're not inside a git repository.\n            repo = None\n\n        paths = list(filter(None, [path, env, cwd, this, repo]))\n        for p in paths:\n            try:\n                return ArrowSources(p)\n            except InvalidArrowSource:\n                pass\n\n        searched_paths = \"\\n\".join([\" - {}\".format(p) for p in paths])\n        raise InvalidArrowSource(\n            \"Unable to locate Arrow's source directory. \"\n            \"Searched paths are:\\n{}\".format(searched_paths)\n        )\n\n    def __repr__(self):\n        return os.fspath(self.path)\n", "dev/archery/archery/utils/report.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom abc import ABCMeta, abstractmethod\nimport datetime\n\nimport jinja2\n\n\ndef markdown_escape(s):\n    for char in ('*', '#', '_', '~', '`', '>'):\n        s = s.replace(char, '\\\\' + char)\n    return s\n\n\nclass Report(metaclass=ABCMeta):\n\n    def __init__(self, **kwargs):\n        for field in self.fields:\n            if field not in kwargs:\n                raise ValueError('Missing keyword argument {}'.format(field))\n        self._data = kwargs\n\n    def __getattr__(self, key):\n        return self._data[key]\n\n    @abstractmethod\n    def fields(self):\n        pass\n\n    @property\n    @abstractmethod\n    def templates(self):\n        pass\n\n\nclass JinjaReport(Report):\n\n    def __init__(self, **kwargs):\n        self.env = jinja2.Environment(\n            loader=jinja2.PackageLoader('archery', 'templates')\n        )\n        self.env.filters['md'] = markdown_escape\n        self.env.globals['today'] = datetime.date.today\n        super().__init__(**kwargs)\n\n    def render(self, template_name):\n        template_path = self.templates[template_name]\n        template = self.env.get_template(template_path)\n        return template.render(**self._data)\n", "dev/archery/archery/utils/git.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom .command import Command, capture_stdout, default_bin\nfrom ..compat import _stringify_path\n\n\n# Decorator prepending argv with the git sub-command found with the method\n# name.\ndef git_cmd(fn):\n    # function name is the subcommand\n    sub_cmd = fn.__name__.replace(\"_\", \"-\")\n\n    def wrapper(self, *argv, **kwargs):\n        return fn(self, sub_cmd, *argv, **kwargs)\n    return wrapper\n\n\nclass Git(Command):\n    def __init__(self, git_bin=None):\n        self.bin = default_bin(git_bin, \"git\")\n\n    def run_cmd(self, cmd, *argv, git_dir=None, **kwargs):\n        \"\"\" Inject flags before sub-command in argv. \"\"\"\n        opts = []\n        if git_dir is not None:\n            opts.extend([\"-C\", _stringify_path(git_dir)])\n\n        return self.run(*opts, cmd, *argv, **kwargs)\n\n    @capture_stdout(strip=False)\n    @git_cmd\n    def archive(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @git_cmd\n    def clone(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @git_cmd\n    def fetch(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @git_cmd\n    def checkout(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    def dirty(self, **kwargs):\n        return len(self.status(\"--short\", **kwargs)) > 0\n\n    @git_cmd\n    def log(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @capture_stdout(strip=True, listify=True)\n    @git_cmd\n    def ls_files(self, *argv, listify=False, **kwargs):\n        stdout = self.run_cmd(*argv, **kwargs)\n        return stdout\n\n    @capture_stdout(strip=True)\n    @git_cmd\n    def rev_parse(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @capture_stdout(strip=True)\n    @git_cmd\n    def status(self, *argv, **kwargs):\n        return self.run_cmd(*argv, **kwargs)\n\n    @capture_stdout(strip=True)\n    def head(self, **kwargs):\n        \"\"\" Return commit pointed by HEAD. \"\"\"\n        return self.rev_parse(\"HEAD\", **kwargs)\n\n    @capture_stdout(strip=True)\n    def current_branch(self, **kwargs):\n        return self.rev_parse(\"--abbrev-ref\", \"HEAD\", **kwargs)\n\n    def repository_root(self, git_dir=None, **kwargs):\n        \"\"\" Locates the repository's root path from a subdirectory. \"\"\"\n        stdout = self.rev_parse(\"--show-toplevel\", git_dir=git_dir, **kwargs)\n        return stdout.decode('utf-8')\n\n\ngit = Git()\n", "dev/archery/archery/utils/tmpdir.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom contextlib import contextmanager\nfrom tempfile import mkdtemp, TemporaryDirectory\n\n\n@contextmanager\ndef tmpdir(preserve=False, prefix=\"arrow-archery-\"):\n    if preserve:\n        yield mkdtemp(prefix=prefix)\n    else:\n        with TemporaryDirectory(prefix=prefix) as tmp:\n            yield tmp\n", "dev/archery/archery/utils/cache.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom pathlib import Path\nimport os\nfrom urllib.request import urlopen\n\nfrom .logger import logger\n\nARCHERY_CACHE_DIR = Path.home() / \".cache\" / \"archery\"\n\n\nclass Cache:\n    \"\"\" Cache stores downloaded objects, notably apache-rat.jar. \"\"\"\n\n    def __init__(self, path=ARCHERY_CACHE_DIR):\n        self.root = path\n\n        if not path.exists():\n            os.makedirs(path)\n\n    def key_path(self, key):\n        \"\"\" Return the full path of a key. \"\"\"\n        return self.root/key\n\n    def get(self, key):\n        \"\"\" Return the full path of a key if cached, None otherwise. \"\"\"\n        path = self.key_path(key)\n        return path if path.exists() else None\n\n    def delete(self, key):\n        \"\"\" Remove a key (and the file) from the cache. \"\"\"\n        path = self.get(key)\n        if path:\n            path.unlink()\n\n    def get_or_insert(self, key, create):\n        \"\"\"\n        Get or Insert a key from the cache. If the key is not found, the\n        `create` closure will be evaluated.\n\n        The `create` closure takes a single parameter, the path where the\n        object should be store. The file should only be created upon success.\n        \"\"\"\n        path = self.key_path(key)\n\n        if not path.exists():\n            create(path)\n\n        return path\n\n    def get_or_insert_from_url(self, key, url):\n        \"\"\"\n        Get or Insert a key from the cache. If the key is not found, the file\n        is downloaded from `url`.\n        \"\"\"\n        def download(path):\n            \"\"\" Tiny wrapper that download a file and save as key. \"\"\"\n            logger.debug(\"Downloading {} as {}\".format(url, path))\n            conn = urlopen(url)\n            # Ensure the download is completed before writing to disks.\n            content = conn.read()\n            with open(path, \"wb\") as path_fd:\n                path_fd.write(content)\n\n        return self.get_or_insert(key, download)\n", "dev/archery/archery/utils/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "dev/archery/archery/utils/logger.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport logging\n\n\"\"\" Global logger. \"\"\"\nlogger = logging.getLogger(\"archery\")\n\n\nclass LoggingContext:\n    def __init__(self, quiet=False):\n        self.quiet = quiet\n\n\nctx = LoggingContext()\n", "dev/archery/archery/utils/maven.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nfrom .command import Command, default_bin\n\n\nclass Maven(Command):\n    def __init__(self, maven_bin=None):\n        self.bin = default_bin(maven_bin, \"mvn\")\n\n\nmaven = Maven()\n\n\nclass MavenDefinition:\n    \"\"\" MavenDefinition captures the maven invocation arguments.\n\n    It allows creating build directories with the same definition, e.g.\n    ```\n    build_1 = maven_def.build(\"/tmp/build-1\")\n    build_2 = maven_def.build(\"/tmp/build-2\")\n\n    ...\n\n    build1.install()\n    build2.install()\n    \"\"\"\n\n    def __init__(self, source, build_definitions=None,\n                 benchmark_definitions=None, env=None):\n        \"\"\" Initialize a MavenDefinition\n\n        Parameters\n        ----------\n        source : str\n                 Source directory where the top-level pom.xml is\n                 located. This is usually the root of the project.\n        build_definitions: list(str), optional\n        benchmark_definitions: list(str), optional\n        \"\"\"\n        self.source = os.path.abspath(source)\n        self.build_definitions = build_definitions if build_definitions else []\n        self.benchmark_definitions =\\\n            benchmark_definitions if benchmark_definitions else []\n        self.env = env\n\n    @property\n    def build_arguments(self):\n        \"\"\"\" Return the arguments to maven invocation for build. \"\"\"\n        arguments = self.build_definitions + [\n            \"-B\", \"-DskipTests\", \"-Drat.skip=true\",\n            \"-Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.\"\n            \"Slf4jMavenTransferListener=warn\",\n            \"-T\", \"2C\", \"install\"\n        ]\n        return arguments\n\n    def build(self, build_dir, force=False, cmd_kwargs=None, **kwargs):\n        \"\"\" Invoke maven into a build directory.\n\n        Parameters\n        ----------\n        build_dir : str\n                    Directory in which the Maven build will be instantiated.\n        force : bool\n                not used now\n        \"\"\"\n        if os.path.exists(build_dir):\n            # Extra safety to ensure we're deleting a build folder.\n            if not MavenBuild.is_build_dir(build_dir):\n                raise FileExistsError(\n                    \"{} is not a maven build\".format(build_dir)\n                )\n\n        cmd_kwargs = cmd_kwargs if cmd_kwargs else {}\n        assert MavenBuild.is_build_dir(build_dir)\n        maven(*self.build_arguments, cwd=build_dir, env=self.env, **cmd_kwargs)\n        return MavenBuild(build_dir, definition=self, **kwargs)\n\n    @property\n    def list_arguments(self):\n        \"\"\"\" Return the arguments to maven invocation for list \"\"\"\n        arguments = [\n            \"-Dskip.perf.benchmarks=false\", \"-Dbenchmark.list=-lp\", \"install\"\n        ]\n        return arguments\n\n    @property\n    def benchmark_arguments(self):\n        \"\"\"\" Return the arguments to maven invocation for benchmark \"\"\"\n        arguments = self.benchmark_definitions + [\n            \"-Dskip.perf.benchmarks=false\", \"-Dbenchmark.fork=1\",\n            \"-Dbenchmark.jvmargs=\\\"-Darrow.enable_null_check_for_get=false \"\n            \"-Darrow.enable_unsafe_memory_access=true\\\"\",\n            \"install\"\n        ]\n        return arguments\n\n    def __repr__(self):\n        return \"MavenDefinition[source={}]\".format(self.source)\n\n\nclass MavenBuild(Maven):\n    \"\"\" MavenBuild represents a build directory initialized by maven.\n\n    The build instance can be used to build/test/install. It alleviates the\n    user to know which generator is used.\n    \"\"\"\n\n    def __init__(self, build_dir, definition=None):\n        \"\"\" Initialize a MavenBuild.\n\n        The caller must ensure that maven was invoked in the build directory.\n\n        Parameters\n        ----------\n        definition : MavenDefinition\n                     The definition to build from.\n        build_dir : str\n                    The build directory to setup into.\n        \"\"\"\n        assert MavenBuild.is_build_dir(build_dir)\n        super().__init__()\n        self.build_dir = os.path.abspath(build_dir)\n        self.definition = definition\n\n    @property\n    def binaries_dir(self):\n        return self.build_dir\n\n    def run(self, *argv, verbose=False, cwd=None, **kwargs):\n        extra = []\n        if verbose:\n            extra.append(\"-X\")\n        if cwd is None:\n            cwd = self.build_dir\n        # Commands must be ran under the directory where pom.xml exists\n        return super().run(*extra, *argv, **kwargs, cwd=cwd)\n\n    def build(self, *argv, verbose=False, **kwargs):\n        definition_args = self.definition.build_arguments\n        cwd = self.binaries_dir\n        return self.run(*argv, *definition_args, verbose=verbose, cwd=cwd,\n                        env=self.definition.env, **kwargs)\n\n    def list(self, *argv, verbose=False, **kwargs):\n        definition_args = self.definition.list_arguments\n        cwd = self.binaries_dir + \"/performance\"\n        return self.run(*argv, *definition_args, verbose=verbose, cwd=cwd,\n                        env=self.definition.env, **kwargs)\n\n    def benchmark(self, *argv, verbose=False, **kwargs):\n        definition_args = self.definition.benchmark_arguments\n        cwd = self.binaries_dir + \"/performance\"\n        return self.run(*argv, *definition_args, verbose=verbose, cwd=cwd,\n                        env=self.definition.env, **kwargs)\n\n    @staticmethod\n    def is_build_dir(path):\n        \"\"\" Indicate if a path is Maven top directory.\n\n        This method only checks for the existence of paths and does not do any\n        validation whatsoever.\n        \"\"\"\n        pom_xml = os.path.join(path, \"pom.xml\")\n        performance_dir = os.path.join(path, \"performance\")\n        return os.path.exists(pom_xml) and os.path.isdir(performance_dir)\n\n    @staticmethod\n    def from_path(path):\n        \"\"\" Instantiate a Maven from a path.\n\n        This is used to recover from an existing physical directory (created\n        with or without Maven).\n\n        Note that this method is not idempotent as the original definition will\n        be lost.\n        \"\"\"\n        if not MavenBuild.is_build_dir(path):\n            raise ValueError(\"Not a valid MavenBuild path: {}\".format(path))\n\n        return MavenBuild(path, definition=None)\n\n    def __repr__(self):\n        return (\"MavenBuild[\"\n                \"build = {},\"\n                \"definition = {}]\".format(self.build_dir,\n                                          self.definition))\n", "dev/archery/archery/utils/lint.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport fnmatch\nimport gzip\nimport os\nfrom pathlib import Path\n\nimport click\n\nfrom .command import Bash, Command, default_bin\nfrom ..compat import _get_module\nfrom .cmake import CMake\nfrom .git import git\nfrom .logger import logger\nfrom ..lang.cpp import CppCMakeDefinition, CppConfiguration\nfrom ..lang.python import Autopep8, Flake8, CythonLint, NumpyDoc, PythonCommand\nfrom .rat import Rat, exclusion_from_globs\nfrom .tmpdir import tmpdir\n\n\n_archery_install_msg = (\n    \"Please install archery using: `pip install -e dev/archery[lint]`. \"\n)\n\n\nclass LintValidationException(Exception):\n    pass\n\n\nclass LintResult:\n    def __init__(self, success, reason=None):\n        self.success = success\n\n    def ok(self):\n        if not self.success:\n            raise LintValidationException\n\n    @staticmethod\n    def from_cmd(command_result):\n        return LintResult(command_result.returncode == 0)\n\n\ndef cpp_linter(src, build_dir, clang_format=True, cpplint=True,\n               clang_tidy=False, iwyu=False, iwyu_all=False,\n               fix=False):\n    \"\"\" Run clang-format, cpplint and clang-tidy on cpp/ codebase. \"\"\"\n    logger.info(\"Running C++ linters\")\n\n    cmake = CMake()\n    if not cmake.available:\n        logger.error(\"cpp linter requested but cmake binary not found.\")\n        return\n\n    # A cmake build directory is required to populate `compile_commands.json`\n    # which in turn is required by clang-tidy. It also provides a convenient\n    # way to hide clang-format/clang-tidy invocation via the Generate\n    # (ninja/make) targets.\n\n    # ARROW_LINT_ONLY exits early but ignore building compile_command.json\n    lint_only = not (iwyu or clang_tidy)\n    cmake_args = {\"with_python\": False, \"with_lint_only\": lint_only}\n    cmake_def = CppCMakeDefinition(src.cpp, CppConfiguration(**cmake_args))\n\n    build = cmake_def.build(build_dir)\n    if clang_format:\n        target = \"format\" if fix else \"check-format\"\n        yield LintResult.from_cmd(build.run(target, check=False))\n\n    if cpplint:\n        yield LintResult.from_cmd(build.run(\"lint\", check=False))\n        yield LintResult.from_cmd(build.run(\"lint_cpp_cli\", check=False))\n\n    if clang_tidy:\n        yield LintResult.from_cmd(build.run(\"check-clang-tidy\", check=False))\n\n    if iwyu:\n        if iwyu_all:\n            iwyu_cmd = \"iwyu-all\"\n        else:\n            iwyu_cmd = \"iwyu\"\n        yield LintResult.from_cmd(build.run(iwyu_cmd, check=False))\n\n\nclass CMakeFormat(Command):\n\n    def __init__(self, paths, cmake_format_bin=None):\n        self.check_version()\n        self.bin = default_bin(cmake_format_bin, \"cmake-format\")\n        self.paths = paths\n\n    @classmethod\n    def from_patterns(cls, base_path, include_patterns, exclude_patterns):\n        paths = {\n            str(path.as_posix())\n            for pattern in include_patterns\n            for path in base_path.glob(pattern)\n        }\n        for pattern in exclude_patterns:\n            pattern = (base_path / pattern).as_posix()\n            paths -= set(fnmatch.filter(paths, str(pattern)))\n        return cls(paths)\n\n    @staticmethod\n    def check_version():\n        try:\n            # cmake_format is part of the cmakelang package\n            import cmakelang\n        except ImportError:\n            raise ImportError(\n\n            )\n        # pin a specific version of cmake_format, must be updated in setup.py\n        if cmakelang.__version__ != \"0.6.13\":\n            raise LintValidationException(\n                f\"Wrong version of cmake_format is detected. \"\n                f\"{_archery_install_msg}\"\n            )\n\n    def check(self):\n        return self.run(\"-l\", \"error\", \"--check\", *self.paths, check=False)\n\n    def fix(self):\n        return self.run(\"--in-place\", *self.paths, check=False)\n\n\ndef cmake_linter(src, fix=False):\n    \"\"\"\n    Run cmake-format on all CMakeFiles.txt\n    \"\"\"\n    logger.info(\"Running cmake-format linters\")\n\n    cmake_format = CMakeFormat.from_patterns(\n        src.path,\n        include_patterns=[\n            'ci/**/*.cmake',\n            'cpp/CMakeLists.txt',\n            'cpp/src/**/*.cmake',\n            'cpp/src/**/*.cmake.in',\n            'cpp/src/**/CMakeLists.txt',\n            'cpp/examples/**/CMakeLists.txt',\n            'cpp/cmake_modules/*.cmake',\n            'go/**/CMakeLists.txt',\n            'java/**/CMakeLists.txt',\n            'matlab/**/CMakeLists.txt',\n            'python/**/CMakeLists.txt',\n        ],\n        exclude_patterns=[\n            'cpp/cmake_modules/FindNumPy.cmake',\n            'cpp/cmake_modules/FindPythonLibsNew.cmake',\n            'cpp/cmake_modules/UseCython.cmake',\n            'cpp/src/arrow/util/*.h.cmake',\n        ]\n    )\n    method = cmake_format.fix if fix else cmake_format.check\n\n    yield LintResult.from_cmd(method())\n\n\ndef python_linter(src, fix=False):\n    \"\"\"Run Python linters on python/pyarrow, python/examples, setup.py\n    and dev/. \"\"\"\n    setup_py = os.path.join(src.python, \"setup.py\")\n    setup_cfg = os.path.join(src.python, \"setup.cfg\")\n\n    logger.info(\"Running Python formatter (autopep8)\")\n\n    autopep8 = Autopep8()\n    if not autopep8.available:\n        logger.error(\n            \"Python formatter requested but autopep8 binary not found. \"\n            f\"{_archery_install_msg}\")\n        return\n\n    # Gather files for autopep8\n    patterns = [\"python/benchmarks/**/*.py\",\n                \"python/examples/**/*.py\",\n                \"python/pyarrow/**/*.py\",\n                \"python/pyarrow/**/*.pyx\",\n                \"python/pyarrow/**/*.pxd\",\n                \"python/pyarrow/**/*.pxi\",\n                \"dev/*.py\",\n                \"dev/archery/**/*.py\",\n                \"dev/release/**/*.py\"]\n    files = [setup_py]\n    for pattern in patterns:\n        files += list(map(str, Path(src.path).glob(pattern)))\n\n    args = ['--global-config', setup_cfg, '--ignore-local-config']\n    if fix:\n        args += ['-j0', '--in-place']\n        args += sorted(files)\n        yield LintResult.from_cmd(autopep8(*args))\n    else:\n        # XXX `-j0` doesn't work well with `--exit-code`, so instead\n        # we capture the diff and check whether it's empty\n        # (https://github.com/hhatto/autopep8/issues/543)\n        args += ['-j0', '--diff']\n        args += sorted(files)\n        diff = autopep8.run_captured(*args)\n        if diff:\n            print(diff.decode('utf8'))\n            yield LintResult(success=False)\n        else:\n            yield LintResult(success=True)\n\n    # Run flake8 after autopep8 (the latter may have modified some files)\n    logger.info(\"Running Python linter (flake8)\")\n\n    flake8 = Flake8()\n    if not flake8.available:\n        logger.error(\n            \"Python linter requested but flake8 binary not found. \"\n            f\"{_archery_install_msg}\")\n        return\n\n    flake8_exclude = ['.venv*', 'vendored']\n\n    yield LintResult.from_cmd(\n        flake8(\"--extend-exclude=\" + ','.join(flake8_exclude),\n               \"--config=\" + os.path.join(src.python, \"setup.cfg\"),\n               setup_py, src.pyarrow, os.path.join(src.python, \"benchmarks\"),\n               os.path.join(src.python, \"examples\"), src.dev, check=False))\n\n    logger.info(\"Running Cython linter (cython-lint)\")\n\n    cython_lint = CythonLint()\n    if not cython_lint.available:\n        logger.error(\n            \"Cython linter requested but cython-lint binary not found. \"\n            f\"{_archery_install_msg}\")\n        return\n\n    # Gather files for cython-lint\n    patterns = [\"python/pyarrow/**/*.pyx\",\n                \"python/pyarrow/**/*.pxd\",\n                \"python/pyarrow/**/*.pxi\",\n                \"python/examples/**/*.pyx\",\n                \"python/examples/**/*.pxd\",\n                \"python/examples/**/*.pxi\",\n                ]\n    files = []\n    for pattern in patterns:\n        files += list(map(str, Path(src.path).glob(pattern)))\n    args = ['--no-pycodestyle']\n    args += sorted(files)\n    yield LintResult.from_cmd(cython_lint(*args))\n\n\ndef python_cpp_linter(src, clang_format=True, fix=False):\n    \"\"\"Run C++ linters on python/pyarrow/src/arrow/python.\"\"\"\n    cpp_src = os.path.join(src.python, \"pyarrow\", \"src\", \"arrow\", \"python\")\n\n    python = PythonCommand()\n\n    if clang_format:\n        logger.info(\"Running clang-format for python/pyarrow/src/arrow/python\")\n\n        if \"CLANG_TOOLS_PATH\" in os.environ:\n            clang_format_binary = os.path.join(\n                os.environ[\"CLANG_TOOLS_PATH\"], \"clang-format\")\n        else:\n            clang_format_binary = \"clang-format-14\"\n\n        run_clang_format = os.path.join(src.cpp, \"build-support\",\n                                        \"run_clang_format.py\")\n        args = [run_clang_format, \"--source_dir\", cpp_src,\n                \"--clang_format_binary\", clang_format_binary]\n        if fix:\n            args += [\"--fix\"]\n\n        yield LintResult.from_cmd(python.run(*args))\n\n\ndef python_numpydoc(symbols=None, allow_rules=None, disallow_rules=None):\n    \"\"\"Run numpydoc linter on python.\n\n    Pyarrow must be available for import.\n    \"\"\"\n    logger.info(\"Running Python docstring linters\")\n    # by default try to run on all pyarrow package\n    symbols = symbols or {\n        'pyarrow',\n        'pyarrow.compute',\n        'pyarrow.csv',\n        'pyarrow.dataset',\n        'pyarrow.feather',\n        # 'pyarrow.flight',\n        'pyarrow.fs',\n        'pyarrow.gandiva',\n        'pyarrow.ipc',\n        'pyarrow.json',\n        'pyarrow.orc',\n        'pyarrow.parquet',\n        'pyarrow.types',\n    }\n    try:\n        numpydoc = NumpyDoc(symbols)\n    except RuntimeError as e:\n        logger.error(str(e))\n        yield LintResult(success=False)\n        return\n\n    results = numpydoc.validate(\n        # limit the validation scope to the pyarrow package\n        from_package='pyarrow',\n        allow_rules=allow_rules,\n        disallow_rules=disallow_rules\n    )\n\n    if len(results) == 0:\n        yield LintResult(success=True)\n        return\n\n    number_of_violations = 0\n    for obj, result in results:\n        errors = result['errors']\n\n        # inspect doesn't play nice with cython generated source code,\n        # to use a hacky way to represent a proper __qualname__\n        doc = getattr(obj, '__doc__', '')\n        name = getattr(obj, '__name__', '')\n        qualname = getattr(obj, '__qualname__', '')\n        module = _get_module(obj, default='')\n        instance = getattr(obj, '__self__', '')\n        if instance:\n            klass = instance.__class__.__name__\n        else:\n            klass = ''\n\n        try:\n            cython_signature = doc.splitlines()[0]\n        except Exception:\n            cython_signature = ''\n\n        desc = '.'.join(filter(None, [module, klass, qualname or name]))\n\n        click.echo()\n        click.echo(click.style(desc, bold=True, fg='yellow'))\n        if cython_signature:\n            qualname_with_signature = '.'.join([module, cython_signature])\n            click.echo(\n                click.style(\n                    '-> {}'.format(qualname_with_signature),\n                    fg='yellow'\n                )\n            )\n\n        for error in errors:\n            number_of_violations += 1\n            click.echo('{}: {}'.format(*error))\n\n    msg = 'Total number of docstring violations: {}'.format(\n        number_of_violations\n    )\n    click.echo()\n    click.echo(click.style(msg, fg='red'))\n\n    yield LintResult(success=False)\n\n\ndef rat_linter(src, root):\n    \"\"\"Run apache-rat license linter.\"\"\"\n    logger.info(\"Running apache-rat linter\")\n\n    if src.git_dirty:\n        logger.warn(\"Due to the usage of git-archive, uncommitted files will\"\n                    \" not be checked for rat violations. \")\n\n    exclusion = exclusion_from_globs(\n        os.path.join(src.dev, \"release\", \"rat_exclude_files.txt\"))\n\n    # Creates a git-archive of ArrowSources, apache-rat expects a gzip\n    # compressed tar archive.\n    archive_path = os.path.join(root, \"apache-arrow.tar.gz\")\n    src.archive(archive_path, compressor=gzip.compress)\n    report = Rat().report(archive_path)\n\n    violations = list(report.validate(exclusion=exclusion))\n    for violation in violations:\n        print(\"apache-rat license violation: {}\".format(violation))\n\n    yield LintResult(len(violations) == 0)\n\n\ndef r_linter(src):\n    \"\"\"Run R linter.\"\"\"\n    logger.info(\"Running R linter\")\n    r_lint_sh = os.path.join(src.r, \"lint.sh\")\n    yield LintResult.from_cmd(Bash().run(r_lint_sh, check=False))\n\n\nclass Hadolint(Command):\n    def __init__(self, hadolint_bin=None):\n        self.bin = default_bin(hadolint_bin, \"hadolint\")\n\n\ndef is_docker_image(path):\n    dirname = os.path.dirname(path)\n    filename = os.path.basename(path)\n\n    excluded = dirname.startswith(\n        \"dev\") or dirname.startswith(\"python/manylinux\")\n\n    return filename.startswith(\"Dockerfile\") and not excluded\n\n\ndef docker_linter(src):\n    \"\"\"Run Hadolint docker linter.\"\"\"\n    logger.info(\"Running Docker linter\")\n\n    hadolint = Hadolint()\n\n    if not hadolint.available:\n        logger.error(\n            \"hadolint linter requested but hadolint binary not found.\")\n        return\n\n    for path in git.ls_files(git_dir=src.path):\n        if is_docker_image(path):\n            yield LintResult.from_cmd(hadolint.run(path, check=False,\n                                                   cwd=src.path))\n\n\nclass SphinxLint(Command):\n    def __init__(self, src, path=None, sphinx_lint_bin=None, disable=None, enable=None):\n        self.src = src\n        self.path = path\n        self.bin = default_bin(sphinx_lint_bin, \"sphinx-lint\")\n        self.disable = disable or \"all\"\n        self.enable = enable\n\n    def lint(self, *args, check=False):\n        docs_path = os.path.join(self.src.path, \"docs\")\n\n        args = []\n\n        if self.disable:\n            args.extend([\"--disable\", self.disable])\n\n        if self.enable:\n            args.extend([\"--enable\", self.enable])\n\n        if self.path is not None:\n            args.extend([self.path])\n        else:\n            args.extend([docs_path])\n\n        return self.run(*args, check=check)\n\n\ndef docs_linter(src, path=None):\n    \"\"\"Run sphinx-lint on docs.\"\"\"\n    logger.info(\"Running docs linter (sphinx-lint)\")\n\n    sphinx_lint = SphinxLint(\n        src,\n        path=path,\n        disable=\"all\",\n        enable=\"trailing-whitespace,missing-final-newline\"\n    )\n\n    if not sphinx_lint.available:\n        logger.error(\"sphinx-lint linter requested but sphinx-lint binary not found\")\n        return\n\n    yield LintResult.from_cmd(sphinx_lint.lint())\n\n\ndef linter(src, fix=False, path=None, *, clang_format=False, cpplint=False,\n           clang_tidy=False, iwyu=False, iwyu_all=False,\n           python=False, numpydoc=False, cmake_format=False, rat=False,\n           r=False, docker=False, docs=False):\n    \"\"\"Run all linters.\"\"\"\n    with tmpdir(prefix=\"arrow-lint-\") as root:\n        build_dir = os.path.join(root, \"cpp-build\")\n\n        # Linters yield LintResult without raising exceptions on failure.\n        # This allows running all linters in one pass and exposing all\n        # errors to the user.\n        results = []\n\n        if clang_format or cpplint or clang_tidy or iwyu:\n            results.extend(cpp_linter(src, build_dir,\n                                      clang_format=clang_format,\n                                      cpplint=cpplint,\n                                      clang_tidy=clang_tidy,\n                                      iwyu=iwyu,\n                                      iwyu_all=iwyu_all,\n                                      fix=fix))\n\n        if python:\n            results.extend(python_linter(src, fix=fix))\n\n        if python and clang_format:\n            results.extend(python_cpp_linter(src,\n                                             clang_format=clang_format,\n                                             fix=fix))\n\n        if numpydoc:\n            results.extend(python_numpydoc())\n\n        if cmake_format:\n            results.extend(cmake_linter(src, fix=fix))\n\n        if rat:\n            results.extend(rat_linter(src, root))\n\n        if r:\n            results.extend(r_linter(src))\n\n        if docker:\n            results.extend(docker_linter(src))\n\n        if docs:\n            results.extend(docs_linter(src, path))\n\n        # Raise error if one linter failed, ensuring calling code can exit with\n        # non-zero.\n        for result in results:\n            result.ok()\n", "dev/archery/archery/benchmark/compare.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\n# Define a global regression threshold as 5%. This is purely subjective and\n# flawed. This does not track cumulative regression.\nDEFAULT_THRESHOLD = 0.05\n\n\ndef items_per_seconds_fmt(value):\n    if value < 1000:\n        return \"{} items/sec\".format(value)\n    if value < 1000**2:\n        return \"{:.3f}K items/sec\".format(value / 1000)\n    if value < 1000**3:\n        return \"{:.3f}M items/sec\".format(value / 1000**2)\n    else:\n        return \"{:.3f}G items/sec\".format(value / 1000**3)\n\n\ndef bytes_per_seconds_fmt(value):\n    if value < 1024:\n        return \"{} bytes/sec\".format(value)\n    if value < 1024**2:\n        return \"{:.3f} KiB/sec\".format(value / 1024)\n    if value < 1024**3:\n        return \"{:.3f} MiB/sec\".format(value / 1024**2)\n    if value < 1024**4:\n        return \"{:.3f} GiB/sec\".format(value / 1024**3)\n    else:\n        return \"{:.3f} TiB/sec\".format(value / 1024**4)\n\n\ndef change_fmt(value):\n    return \"{:.3%}\".format(value)\n\n\ndef formatter_for_unit(unit):\n    if unit == \"bytes_per_second\":\n        return bytes_per_seconds_fmt\n    elif unit == \"items_per_second\":\n        return items_per_seconds_fmt\n    else:\n        return lambda x: x\n\n\nclass BenchmarkComparator:\n    \"\"\" Compares two benchmarks.\n\n    Encodes the logic of comparing two benchmarks and taking a decision on\n    if it induce a regression.\n    \"\"\"\n\n    def __init__(self, contender, baseline, threshold=DEFAULT_THRESHOLD,\n                 suite_name=None):\n        self.contender = contender\n        self.baseline = baseline\n        self.threshold = threshold\n        self.suite_name = suite_name\n\n    @property\n    def name(self):\n        return self.baseline.name\n\n    @property\n    def less_is_better(self):\n        return self.baseline.less_is_better\n\n    @property\n    def unit(self):\n        return self.baseline.unit\n\n    @property\n    def change(self):\n        new = self.contender.value\n        old = self.baseline.value\n\n        if old == 0 and new == 0:\n            return 0.0\n        if old == 0:\n            return 0.0\n\n        return float(new - old) / abs(old)\n\n    @property\n    def confidence(self):\n        \"\"\" Indicate if a comparison of benchmarks should be trusted. \"\"\"\n        return True\n\n    @property\n    def regression(self):\n        change = self.change\n        adjusted_change = change if self.less_is_better else -change\n        return (self.confidence and adjusted_change > self.threshold)\n\n    @property\n    def formatted(self):\n        fmt = formatter_for_unit(self.unit)\n        return {\n            \"benchmark\": self.name,\n            \"change\": change_fmt(self.change),\n            \"regression\": self.regression,\n            \"baseline\": fmt(self.baseline.value),\n            \"contender\": fmt(self.contender.value),\n            \"unit\": self.unit,\n            \"less_is_better\": self.less_is_better,\n            \"counters\": str(self.baseline.counters)\n        }\n\n    def compare(self, comparator=None):\n        return {\n            \"benchmark\": self.name,\n            \"change\": self.change,\n            \"regression\": self.regression,\n            \"baseline\": self.baseline.value,\n            \"contender\": self.contender.value,\n            \"unit\": self.unit,\n            \"less_is_better\": self.less_is_better,\n            \"counters\": self.baseline.counters\n        }\n\n    def __call__(self, **kwargs):\n        return self.compare(**kwargs)\n\n\ndef pairwise_compare(contender, baseline):\n    dict_contender = {e.name: e for e in contender}\n    dict_baseline = {e.name: e for e in baseline}\n\n    for name in (dict_contender.keys() & dict_baseline.keys()):\n        yield name, (dict_contender[name], dict_baseline[name])\n\n\nclass RunnerComparator:\n    \"\"\" Compares suites/benchmarks from runners.\n\n    It is up to the caller that ensure that runners are compatible (both from\n    the same language implementation).\n    \"\"\"\n\n    def __init__(self, contender, baseline, threshold=DEFAULT_THRESHOLD):\n        self.contender = contender\n        self.baseline = baseline\n        self.threshold = threshold\n\n    @property\n    def comparisons(self):\n        contender = self.contender.suites\n        baseline = self.baseline.suites\n        suites = pairwise_compare(contender, baseline)\n\n        for suite_name, (suite_cont, suite_base) in suites:\n            benchmarks = pairwise_compare(\n                suite_cont.benchmarks, suite_base.benchmarks)\n\n            for _, (bench_cont, bench_base) in benchmarks:\n                yield BenchmarkComparator(bench_cont, bench_base,\n                                          threshold=self.threshold,\n                                          suite_name=suite_name)\n", "dev/archery/archery/benchmark/google.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom itertools import filterfalse, groupby, tee\nimport json\nimport subprocess\nfrom tempfile import NamedTemporaryFile\n\nfrom .core import Benchmark\nfrom ..utils.command import Command\n\n\ndef partition(pred, iterable):\n    # adapted from python's examples\n    t1, t2 = tee(iterable)\n    return list(filter(pred, t1)), list(filterfalse(pred, t2))\n\n\nclass GoogleBenchmarkCommand(Command):\n    \"\"\" Run a google benchmark binary.\n\n    This assumes the binary supports the standard command line options,\n    notably `--benchmark_filter`, `--benchmark_format`, etc...\n    \"\"\"\n\n    def __init__(self, benchmark_bin, benchmark_filter=None, benchmark_extras=None):\n        self.bin = benchmark_bin\n        self.benchmark_filter = benchmark_filter\n        self.benchmark_extras = benchmark_extras or []\n\n    def list_benchmarks(self):\n        argv = [\"--benchmark_list_tests\"]\n        if self.benchmark_filter:\n            argv.append(\"--benchmark_filter={}\".format(self.benchmark_filter))\n        result = self.run(*argv, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE)\n        return str.splitlines(result.stdout.decode(\"utf-8\"))\n\n    def results(self, repetitions=1, repetition_min_time=None):\n        with NamedTemporaryFile() as out:\n            argv = [f\"--benchmark_repetitions={repetitions}\",\n                    f\"--benchmark_out={out.name}\",\n                    \"--benchmark_out_format=json\"]\n\n            if repetition_min_time is not None:\n                argv.append(f\"--benchmark_min_time={repetition_min_time:.6f}s\")\n\n            if self.benchmark_filter:\n                argv.append(f\"--benchmark_filter={self.benchmark_filter}\")\n\n            argv += self.benchmark_extras\n\n            self.run(*argv, check=True)\n            return json.load(out)\n\n\nclass GoogleBenchmarkObservation:\n    \"\"\" Represents one run of a single (google c++) benchmark.\n\n    Aggregates are reported by Google Benchmark executables alongside\n    other observations whenever repetitions are specified (with\n    `--benchmark_repetitions` on the bare benchmark, or with the\n    archery option `--repetitions`). Aggregate observations are not\n    included in `GoogleBenchmark.runs`.\n\n    RegressionSumKernel/32768/0                 1 us          1 us  25.8077GB/s\n    RegressionSumKernel/32768/0                 1 us          1 us  25.7066GB/s\n    RegressionSumKernel/32768/0                 1 us          1 us  25.1481GB/s\n    RegressionSumKernel/32768/0                 1 us          1 us  25.846GB/s\n    RegressionSumKernel/32768/0                 1 us          1 us  25.6453GB/s\n    RegressionSumKernel/32768/0_mean            1 us          1 us  25.6307GB/s\n    RegressionSumKernel/32768/0_median          1 us          1 us  25.7066GB/s\n    RegressionSumKernel/32768/0_stddev          0 us          0 us  288.046MB/s\n    \"\"\"\n\n    def __init__(self, name, real_time, cpu_time, time_unit, run_type,\n                 size=None, bytes_per_second=None, items_per_second=None,\n                 **counters):\n        self._name = name\n        self.real_time = real_time\n        self.cpu_time = cpu_time\n        self.time_unit = time_unit\n        self.run_type = run_type\n        self.size = size\n        self.bytes_per_second = bytes_per_second\n        self.items_per_second = items_per_second\n        self.counters = counters\n\n    @property\n    def is_aggregate(self):\n        \"\"\" Indicate if the observation is a run or an aggregate. \"\"\"\n        return self.run_type == \"aggregate\"\n\n    @property\n    def is_realtime(self):\n        \"\"\" Indicate if the preferred value is realtime instead of cputime. \"\"\"\n        return self.name.find(\"/real_time\") != -1\n\n    @property\n    def name(self):\n        name = self._name\n        return name.rsplit(\"_\", maxsplit=1)[0] if self.is_aggregate else name\n\n    @property\n    def time(self):\n        return self.real_time if self.is_realtime else self.cpu_time\n\n    @property\n    def value(self):\n        \"\"\" Return the benchmark value.\"\"\"\n        return self.bytes_per_second or self.items_per_second or self.time\n\n    @property\n    def unit(self):\n        if self.bytes_per_second:\n            return \"bytes_per_second\"\n        elif self.items_per_second:\n            return \"items_per_second\"\n        else:\n            return self.time_unit\n\n    def __repr__(self):\n        return str(self.value)\n\n\nclass GoogleBenchmark(Benchmark):\n    \"\"\" A set of GoogleBenchmarkObservations. \"\"\"\n\n    def __init__(self, name, runs):\n        \"\"\" Initialize a GoogleBenchmark.\n\n        Parameters\n        ----------\n        name: str\n              Name of the benchmark\n        runs: list(GoogleBenchmarkObservation)\n              Repetitions of GoogleBenchmarkObservation run.\n\n        \"\"\"\n        self.name = name\n        # exclude google benchmark aggregate artifacts\n        _, runs = partition(lambda b: b.is_aggregate, runs)\n        self.runs = sorted(runs, key=lambda b: b.value)\n        unit = self.runs[0].unit\n        time_unit = self.runs[0].time_unit\n        less_is_better = not unit.endswith(\"per_second\")\n        values = [b.value for b in self.runs]\n        times = [b.real_time for b in self.runs]\n        # Slight kludge to extract the UserCounters for each benchmark\n        counters = self.runs[0].counters\n        super().__init__(name, unit, less_is_better, values, time_unit, times,\n                         counters)\n\n    def __repr__(self):\n        return \"GoogleBenchmark[name={},runs={}]\".format(self.names, self.runs)\n\n    @classmethod\n    def from_json(cls, payload):\n        def group_key(x):\n            return x.name\n\n        benchmarks = map(lambda x: GoogleBenchmarkObservation(**x), payload)\n        groups = groupby(sorted(benchmarks, key=group_key), group_key)\n        return [cls(k, list(bs)) for k, bs in groups]\n", "dev/archery/archery/benchmark/core.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\ndef median(values):\n    n = len(values)\n    if n == 0:\n        raise ValueError(\"median requires at least one value\")\n    elif n % 2 == 0:\n        return (values[(n // 2) - 1] + values[n // 2]) / 2\n    else:\n        return values[n // 2]\n\n\nclass Benchmark:\n    def __init__(self, name, unit, less_is_better, values, time_unit,\n                 times, counters=None):\n        self.name = name\n        self.unit = unit\n        self.less_is_better = less_is_better\n        self.values = sorted(values)\n        self.time_unit = time_unit\n        self.times = sorted(times)\n        self.median = median(self.values)\n        self.counters = counters or {}\n\n    @property\n    def value(self):\n        return self.median\n\n    def __repr__(self):\n        return \"Benchmark[name={},value={}]\".format(self.name, self.value)\n\n\nclass BenchmarkSuite:\n    def __init__(self, name, benchmarks):\n        self.name = name\n        self.benchmarks = benchmarks\n\n    def __repr__(self):\n        return \"BenchmarkSuite[name={}, benchmarks={}]\".format(\n            self.name, self.benchmarks\n        )\n", "dev/archery/archery/benchmark/codec.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nimport json\n\nfrom ..benchmark.core import Benchmark, BenchmarkSuite\nfrom ..benchmark.runner import BenchmarkRunner, StaticBenchmarkRunner\nfrom ..benchmark.compare import BenchmarkComparator\n\n\nclass JsonEncoder(json.JSONEncoder):\n    def default(self, o):\n        if isinstance(o, Benchmark):\n            return BenchmarkCodec.encode(o)\n\n        if isinstance(o, BenchmarkSuite):\n            return BenchmarkSuiteCodec.encode(o)\n\n        if isinstance(o, BenchmarkRunner):\n            return BenchmarkRunnerCodec.encode(o)\n\n        if isinstance(o, BenchmarkComparator):\n            return BenchmarkComparatorCodec.encode(o)\n\n        return json.JSONEncoder.default(self, o)\n\n\nclass BenchmarkCodec:\n    @staticmethod\n    def encode(b):\n        return {\n            \"name\": b.name,\n            \"unit\": b.unit,\n            \"less_is_better\": b.less_is_better,\n            \"values\": b.values,\n            \"time_unit\": b.time_unit,\n            \"times\": b.times,\n            \"counters\": b.counters,\n        }\n\n    @staticmethod\n    def decode(dct, **kwargs):\n        return Benchmark(**dct, **kwargs)\n\n\nclass BenchmarkSuiteCodec:\n    @staticmethod\n    def encode(bs):\n        return {\n            \"name\": bs.name,\n            \"benchmarks\": [BenchmarkCodec.encode(b) for b in bs.benchmarks]\n        }\n\n    @staticmethod\n    def decode(dct, **kwargs):\n        benchmarks = [BenchmarkCodec.decode(b)\n                      for b in dct.pop(\"benchmarks\", [])]\n        return BenchmarkSuite(benchmarks=benchmarks, **dct, **kwargs)\n\n\nclass BenchmarkRunnerCodec:\n    @staticmethod\n    def encode(br):\n        return {\"suites\": [BenchmarkSuiteCodec.encode(s) for s in br.suites]}\n\n    @staticmethod\n    def decode(dct, **kwargs):\n        suites = [BenchmarkSuiteCodec.decode(s)\n                  for s in dct.pop(\"suites\", [])]\n        return StaticBenchmarkRunner(suites=suites, **dct, **kwargs)\n\n\nclass BenchmarkComparatorCodec:\n    @staticmethod\n    def encode(bc):\n        comparator = bc.formatted\n\n        suite_name = bc.suite_name\n        if suite_name:\n            comparator[\"suite\"] = suite_name\n\n        return comparator\n", "dev/archery/archery/benchmark/runner.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport glob\nimport json\nimport os\nimport re\n\nfrom .core import BenchmarkSuite\nfrom .google import GoogleBenchmarkCommand, GoogleBenchmark\nfrom .jmh import JavaMicrobenchmarkHarnessCommand, JavaMicrobenchmarkHarness\nfrom ..lang.cpp import CppCMakeDefinition, CppConfiguration\nfrom ..lang.java import JavaMavenDefinition, JavaConfiguration\nfrom ..utils.cmake import CMakeBuild\nfrom ..utils.maven import MavenBuild\nfrom ..utils.logger import logger\n\n\ndef regex_filter(re_expr):\n    if re_expr is None:\n        return lambda s: True\n    re_comp = re.compile(re_expr)\n    return lambda s: re_comp.search(s)\n\n\nDEFAULT_REPETITIONS = 1\n\n\nclass BenchmarkRunner:\n    def __init__(self, suite_filter=None, benchmark_filter=None,\n                 repetitions=DEFAULT_REPETITIONS, repetition_min_time=None):\n        self.suite_filter = suite_filter\n        self.benchmark_filter = benchmark_filter\n        self.repetitions = repetitions\n        self.repetition_min_time = repetition_min_time\n\n    @property\n    def suites(self):\n        raise NotImplementedError(\"BenchmarkRunner must implement suites\")\n\n    @staticmethod\n    def from_rev_or_path(src, root, rev_or_path, cmake_conf, **kwargs):\n        raise NotImplementedError(\n            \"BenchmarkRunner must implement from_rev_or_path\")\n\n\nclass StaticBenchmarkRunner(BenchmarkRunner):\n    \"\"\" Run suites from a (static) set of suites. \"\"\"\n\n    def __init__(self, suites, **kwargs):\n        self._suites = suites\n        super().__init__(**kwargs)\n\n    @property\n    def list_benchmarks(self):\n        for suite in self._suites:\n            for benchmark in suite.benchmarks:\n                yield \"{}.{}\".format(suite.name, benchmark.name)\n\n    @property\n    def suites(self):\n        suite_fn = regex_filter(self.suite_filter)\n        benchmark_fn = regex_filter(self.benchmark_filter)\n\n        for suite in (s for s in self._suites if suite_fn(s.name)):\n            benchmarks = [b for b in suite.benchmarks if benchmark_fn(b.name)]\n            yield BenchmarkSuite(suite.name, benchmarks)\n\n    @classmethod\n    def is_json_result(cls, path_or_str):\n        builder = None\n        try:\n            builder = cls.from_json(path_or_str)\n        except BaseException:\n            pass\n\n        return builder is not None\n\n    @staticmethod\n    def from_json(path_or_str, **kwargs):\n        # .codec imported here to break recursive imports\n        from .codec import BenchmarkRunnerCodec\n        if os.path.isfile(path_or_str):\n            with open(path_or_str) as f:\n                loaded = json.load(f)\n        else:\n            loaded = json.loads(path_or_str)\n        return BenchmarkRunnerCodec.decode(loaded, **kwargs)\n\n    def __repr__(self):\n        return \"BenchmarkRunner[suites={}]\".format(list(self.suites))\n\n\nclass CppBenchmarkRunner(BenchmarkRunner):\n    \"\"\" Run suites from a CMakeBuild. \"\"\"\n\n    def __init__(self, build, benchmark_extras, **kwargs):\n        \"\"\" Initialize a CppBenchmarkRunner. \"\"\"\n        self.build = build\n        self.benchmark_extras = benchmark_extras\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def default_configuration(**kwargs):\n        \"\"\" Returns the default benchmark configuration. \"\"\"\n        return CppConfiguration(\n            build_type=\"release\", with_tests=False, with_benchmarks=True,\n            with_compute=True,\n            with_csv=True,\n            with_dataset=True,\n            with_json=True,\n            with_parquet=True,\n            with_python=False,\n            with_brotli=True,\n            with_bz2=True,\n            with_lz4=True,\n            with_snappy=True,\n            with_zlib=True,\n            with_zstd=True,\n            **kwargs)\n\n    @property\n    def suites_binaries(self):\n        \"\"\" Returns a list of benchmark binaries for this build. \"\"\"\n        # Ensure build is up-to-date to run benchmarks\n        self.build()\n        # Not the best method, but works for now\n        glob_expr = os.path.join(self.build.binaries_dir, \"*-benchmark\")\n        return {os.path.basename(b): b for b in glob.glob(glob_expr)}\n\n    def suite(self, name, suite_bin):\n        \"\"\" Returns the resulting benchmarks for a given suite. \"\"\"\n        suite_cmd = GoogleBenchmarkCommand(suite_bin, self.benchmark_filter,\n                                           self.benchmark_extras)\n\n        # Ensure there will be data\n        benchmark_names = suite_cmd.list_benchmarks()\n        if not benchmark_names:\n            return None\n\n        results = suite_cmd.results(\n            repetitions=self.repetitions,\n            repetition_min_time=self.repetition_min_time)\n        benchmarks = GoogleBenchmark.from_json(results.get(\"benchmarks\"))\n        return BenchmarkSuite(name, benchmarks)\n\n    @property\n    def list_benchmarks(self):\n        for suite_name, suite_bin in self.suites_binaries.items():\n            suite_cmd = GoogleBenchmarkCommand(suite_bin)\n            for benchmark_name in suite_cmd.list_benchmarks():\n                yield \"{}.{}\".format(suite_name, benchmark_name)\n\n    @property\n    def suites(self):\n        \"\"\" Returns all suite for a runner. \"\"\"\n        suite_matcher = regex_filter(self.suite_filter)\n\n        suite_found = False\n        suite_and_binaries = self.suites_binaries\n        for suite_name in suite_and_binaries:\n            if not suite_matcher(suite_name):\n                logger.debug(\"Ignoring suite {}\".format(suite_name))\n                continue\n\n            suite_bin = suite_and_binaries[suite_name]\n            suite = self.suite(suite_name, suite_bin)\n\n            # Filter may exclude all benchmarks\n            if not suite:\n                logger.debug(\"Suite {} executed but no results\"\n                             .format(suite_name))\n                continue\n\n            suite_found = True\n            yield suite\n\n        if not suite_found:\n            raise ValueError(\"No benchmark matches the suite/benchmark filter\")\n\n    @staticmethod\n    def from_rev_or_path(src, root, rev_or_path, cmake_conf, **kwargs):\n        \"\"\" Returns a BenchmarkRunner from a path or a git revision.\n\n        First, it checks if `rev_or_path` is a valid path (or string) of a json\n        object that can deserialize to a BenchmarkRunner. If so, it initialize\n        a StaticBenchmarkRunner from it. This allows memoizing the result of a\n        run in a file or a string.\n\n        Second, it checks if `rev_or_path` points to a valid CMake build\n        directory.  If so, it creates a CppBenchmarkRunner with this existing\n        CMakeBuild.\n\n        Otherwise, it assumes `rev_or_path` is a revision and clone/checkout\n        the given revision and create a fresh CMakeBuild.\n        \"\"\"\n        build = None\n        if StaticBenchmarkRunner.is_json_result(rev_or_path):\n            kwargs.pop('benchmark_extras', None)\n            return StaticBenchmarkRunner.from_json(rev_or_path, **kwargs)\n        elif CMakeBuild.is_build_dir(rev_or_path):\n            build = CMakeBuild.from_path(rev_or_path)\n            return CppBenchmarkRunner(build, **kwargs)\n        else:\n            # Revisions can references remote via the `/` character, ensure\n            # that the revision is path friendly\n            path_rev = rev_or_path.replace(\"/\", \"_\")\n            root_rev = os.path.join(root, path_rev)\n            os.mkdir(root_rev)\n\n            clone_dir = os.path.join(root_rev, \"arrow\")\n            # Possibly checkout the sources at given revision, no need to\n            # perform cleanup on cloned repository as root_rev is reclaimed.\n            src_rev, _ = src.at_revision(rev_or_path, clone_dir)\n            cmake_def = CppCMakeDefinition(src_rev.cpp, cmake_conf)\n            build_dir = os.path.join(root_rev, \"build\")\n            return CppBenchmarkRunner(cmake_def.build(build_dir), **kwargs)\n\n\nclass JavaBenchmarkRunner(BenchmarkRunner):\n    \"\"\" Run suites for Java. \"\"\"\n\n    # default repetitions is 5 for Java microbenchmark harness\n    def __init__(self, build, **kwargs):\n        \"\"\" Initialize a JavaBenchmarkRunner. \"\"\"\n        self.build = build\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def default_configuration(**kwargs):\n        \"\"\" Returns the default benchmark configuration. \"\"\"\n        return JavaConfiguration(**kwargs)\n\n    def suite(self, name):\n        \"\"\" Returns the resulting benchmarks for a given suite. \"\"\"\n        # update .m2 directory, which installs target jars\n        self.build.build()\n\n        suite_cmd = JavaMicrobenchmarkHarnessCommand(\n            self.build, self.benchmark_filter)\n\n        # Ensure there will be data\n        benchmark_names = suite_cmd.list_benchmarks()\n        if not benchmark_names:\n            return None\n\n        # TODO: support `repetition_min_time`\n        results = suite_cmd.results(repetitions=self.repetitions)\n        benchmarks = JavaMicrobenchmarkHarness.from_json(results)\n        return BenchmarkSuite(name, benchmarks)\n\n    @property\n    def list_benchmarks(self):\n        \"\"\" Returns all suite names \"\"\"\n        # Ensure build is up-to-date to run benchmarks\n        self.build.build()\n\n        suite_cmd = JavaMicrobenchmarkHarnessCommand(self.build)\n        benchmark_names = suite_cmd.list_benchmarks()\n        for benchmark_name in benchmark_names:\n            yield \"{}\".format(benchmark_name)\n\n    @property\n    def suites(self):\n        \"\"\" Returns all suite for a runner. \"\"\"\n        suite_name = \"JavaBenchmark\"\n        suite = self.suite(suite_name)\n\n        # Filter may exclude all benchmarks\n        if not suite:\n            logger.debug(\"Suite {} executed but no results\"\n                         .format(suite_name))\n            return\n\n        yield suite\n\n    @staticmethod\n    def from_rev_or_path(src, root, rev_or_path, maven_conf, **kwargs):\n        \"\"\" Returns a BenchmarkRunner from a path or a git revision.\n\n        First, it checks if `rev_or_path` is a valid path (or string) of a json\n        object that can deserialize to a BenchmarkRunner. If so, it initialize\n        a StaticBenchmarkRunner from it. This allows memoizing the result of a\n        run in a file or a string.\n\n        Second, it checks if `rev_or_path` points to a valid Maven build\n        directory.  If so, it creates a JavaBenchmarkRunner with this existing\n        MavenBuild.\n\n        Otherwise, it assumes `rev_or_path` is a revision and clone/checkout\n        the given revision and create a fresh MavenBuild.\n        \"\"\"\n        if StaticBenchmarkRunner.is_json_result(rev_or_path):\n            return StaticBenchmarkRunner.from_json(rev_or_path, **kwargs)\n        elif MavenBuild.is_build_dir(rev_or_path):\n            maven_def = JavaMavenDefinition(rev_or_path, maven_conf)\n            return JavaBenchmarkRunner(maven_def.build(rev_or_path), **kwargs)\n        else:\n            # Revisions can references remote via the `/` character, ensure\n            # that the revision is path friendly\n            path_rev = rev_or_path.replace(\"/\", \"_\")\n            root_rev = os.path.join(root, path_rev)\n            os.mkdir(root_rev)\n\n            clone_dir = os.path.join(root_rev, \"arrow\")\n            # Possibly checkout the sources at given revision, no need to\n            # perform cleanup on cloned repository as root_rev is reclaimed.\n            src_rev, _ = src.at_revision(rev_or_path, clone_dir)\n            maven_def = JavaMavenDefinition(src_rev.java, maven_conf)\n            build_dir = os.path.join(root_rev, \"arrow/java\")\n            return JavaBenchmarkRunner(maven_def.build(build_dir), **kwargs)\n", "dev/archery/archery/benchmark/jmh.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom itertools import filterfalse, groupby, tee\nimport json\nimport subprocess\nfrom tempfile import NamedTemporaryFile\n\nfrom .core import Benchmark\nfrom ..utils.command import Command\nfrom ..utils.maven import Maven\n\n\ndef partition(pred, iterable):\n    # adapted from python's examples\n    t1, t2 = tee(iterable)\n    return list(filter(pred, t1)), list(filterfalse(pred, t2))\n\n\nclass JavaMicrobenchmarkHarnessCommand(Command):\n    \"\"\" Run a Java Micro Benchmark Harness\n\n    This assumes the binary supports the standard command line options,\n    notably `-Dbenchmark_filter`\n    \"\"\"\n\n    def __init__(self, build, benchmark_filter=None):\n        self.benchmark_filter = benchmark_filter\n        self.build = build\n        self.maven = Maven()\n\n    \"\"\" Extract benchmark names from output between \"Benchmarks:\" and \"[INFO]\".\n    Assume the following output:\n      ...\n      Benchmarks:\n      org.apache.arrow.vector.IntBenchmarks.setIntDirectly\n      ...\n      org.apache.arrow.vector.IntBenchmarks.setWithValueHolder\n      org.apache.arrow.vector.IntBenchmarks.setWithWriter\n      ...\n      [INFO]\n    \"\"\"\n\n    def list_benchmarks(self):\n        argv = []\n        if self.benchmark_filter:\n            argv.append(\"-Dbenchmark.filter={}\".format(self.benchmark_filter))\n        result = self.build.list(\n            *argv, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        lists = []\n        benchmarks = False\n        for line in str.splitlines(result.stdout.decode(\"utf-8\")):\n            if not benchmarks:\n                if line.startswith(\"Benchmarks:\"):\n                    benchmarks = True\n            else:\n                if line.startswith(\"org.apache.arrow\"):\n                    lists.append(line)\n                if line.startswith(\"[INFO]\"):\n                    break\n        return lists\n\n    def results(self, repetitions):\n        with NamedTemporaryFile(suffix=\".json\") as out:\n            argv = [\"-Dbenchmark.runs={}\".format(repetitions),\n                    \"-Dbenchmark.resultfile={}\".format(out.name),\n                    \"-Dbenchmark.resultformat=json\"]\n            if self.benchmark_filter:\n                argv.append(\n                    \"-Dbenchmark.filter={}\".format(self.benchmark_filter)\n                )\n\n            self.build.benchmark(*argv, check=True)\n            return json.load(out)\n\n\nclass JavaMicrobenchmarkHarnessObservation:\n    \"\"\" Represents one run of a single Java Microbenchmark Harness\n    \"\"\"\n\n    def __init__(self, benchmark, primaryMetric,\n                 forks, warmupIterations, measurementIterations, **counters):\n        self.name = benchmark\n        self.primaryMetric = primaryMetric\n        self.score = primaryMetric[\"score\"]\n        self.score_unit = primaryMetric[\"scoreUnit\"]\n        self.forks = forks\n        self.warmups = warmupIterations\n        self.runs = measurementIterations\n        self.counters = {\n            \"mode\": counters[\"mode\"],\n            \"threads\": counters[\"threads\"],\n            \"warmups\": warmupIterations,\n            \"warmupTime\": counters[\"warmupTime\"],\n            \"measurements\": measurementIterations,\n            \"measurementTime\": counters[\"measurementTime\"],\n            \"jvmArgs\": counters[\"jvmArgs\"]\n        }\n        self.reciprocal_value = True if self.score_unit.endswith(\n            \"/op\") else False\n        if self.score_unit.startswith(\"ops/\"):\n            idx = self.score_unit.find(\"/\")\n            self.normalizePerSec(self.score_unit[idx+1:])\n        elif self.score_unit.endswith(\"/op\"):\n            idx = self.score_unit.find(\"/\")\n            self.normalizePerSec(self.score_unit[:idx])\n        else:\n            self.normalizeFactor = 1\n\n    @property\n    def value(self):\n        \"\"\" Return the benchmark value.\"\"\"\n        val = 1 / self.score if self.reciprocal_value else self.score\n        return val * self.normalizeFactor\n\n    def normalizePerSec(self, unit):\n        if unit == \"ns\":\n            self.normalizeFactor = 1000 * 1000 * 1000\n        elif unit == \"us\":\n            self.normalizeFactor = 1000 * 1000\n        elif unit == \"ms\":\n            self.normalizeFactor = 1000\n        elif unit == \"min\":\n            self.normalizeFactor = 1 / 60\n        elif unit == \"hr\":\n            self.normalizeFactor = 1 / (60 * 60)\n        elif unit == \"day\":\n            self.normalizeFactor = 1 / (60 * 60 * 24)\n        else:\n            self.normalizeFactor = 1\n\n    @property\n    def unit(self):\n        if self.score_unit.startswith(\"ops/\"):\n            return \"items_per_second\"\n        elif self.score_unit.endswith(\"/op\"):\n            return \"items_per_second\"\n        else:\n            return \"?\"\n\n    def __repr__(self):\n        return str(self.value)\n\n\nclass JavaMicrobenchmarkHarness(Benchmark):\n    \"\"\" A set of JavaMicrobenchmarkHarnessObservations. \"\"\"\n\n    def __init__(self, name, runs):\n        \"\"\" Initialize a JavaMicrobenchmarkHarness.\n\n        Parameters\n        ----------\n        name: str\n              Name of the benchmark\n        forks: int\n        warmups: int\n        runs: int\n        runs: list(JavaMicrobenchmarkHarnessObservation)\n              Repetitions of JavaMicrobenchmarkHarnessObservation run.\n\n        \"\"\"\n        self.name = name\n        self.runs = sorted(runs, key=lambda b: b.value)\n        unit = self.runs[0].unit\n        time_unit = \"N/A\"\n        less_is_better = not unit.endswith(\"per_second\")\n        values = [b.value for b in self.runs]\n        times = []\n        # Slight kludge to extract the UserCounters for each benchmark\n        counters = self.runs[0].counters\n        super().__init__(name, unit, less_is_better, values, time_unit, times,\n                         counters)\n\n    def __repr__(self):\n        return \"JavaMicrobenchmark[name={},runs={}]\".format(\n            self.name, self.runs)\n\n    @classmethod\n    def from_json(cls, payload):\n        def group_key(x):\n            return x.name\n\n        benchmarks = map(\n            lambda x: JavaMicrobenchmarkHarnessObservation(**x), payload)\n        groups = groupby(sorted(benchmarks, key=group_key), group_key)\n        return [cls(k, list(bs)) for k, bs in groups]\n", "dev/archery/archery/benchmark/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "dev/archery/archery/integration/util.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport io\nimport random\nimport socket\nimport subprocess\nimport sys\nimport threading\nimport uuid\n\nimport numpy as np\n\n\ndef guid():\n    return uuid.uuid4().hex\n\n\n# SKIP categories\nSKIP_C_ARRAY = 'c_array'\nSKIP_C_SCHEMA = 'c_schema'\nSKIP_FLIGHT = 'flight'\nSKIP_IPC = 'ipc'\n\n\nclass _Printer:\n    \"\"\"\n    A print()-providing object that can override the stream output on\n    a per-thread basis.\n    \"\"\"\n\n    def __init__(self):\n        self._tls = threading.local()\n\n    def _get_stdout(self):\n        try:\n            return self._tls.stdout\n        except AttributeError:\n            self._tls.stdout = sys.stdout\n            self._tls.corked = False\n            return self._tls.stdout\n\n    def print(self, *args, **kwargs):\n        \"\"\"\n        A variant of print() that writes to a thread-local stream.\n        \"\"\"\n        print(*args, file=self._get_stdout(), **kwargs)\n\n    @property\n    def stdout(self):\n        \"\"\"\n        A thread-local stdout wrapper that may be temporarily buffered\n        using `cork()`.\n        \"\"\"\n        return self._get_stdout()\n\n    @contextlib.contextmanager\n    def cork(self):\n        \"\"\"\n        Temporarily buffer this thread's stream and write out its contents\n        at the end of the context manager.  Useful to avoid interleaved\n        output when multiple threads output progress information.\n        \"\"\"\n        outer_stdout = self._get_stdout()\n        assert not self._tls.corked, \"reentrant call\"\n        inner_stdout = self._tls.stdout = io.StringIO()\n        self._tls.corked = True\n        try:\n            yield\n        finally:\n            self._tls.stdout = outer_stdout\n            self._tls.corked = False\n            outer_stdout.write(inner_stdout.getvalue())\n            outer_stdout.flush()\n\n\nprinter = _Printer()\nlog = printer.print\n\n\n_RAND_CHARS = np.array(list(\"abcdefghijklmnop123456\u00c2rr\u00f4w\u00b5\u00a3\u00b0\u20ac\u77e2\"), dtype=\"U\")\n\n\ndef random_utf8(nchars):\n    \"\"\"\n    Generate one random UTF8 string.\n    \"\"\"\n    return ''.join(np.random.choice(_RAND_CHARS, nchars))\n\n\ndef random_bytes(nbytes):\n    \"\"\"\n    Generate one random binary string.\n    \"\"\"\n    # NOTE getrandbits(0) fails\n    if nbytes > 0:\n        return random.getrandbits(nbytes * 8).to_bytes(nbytes,\n                                                       byteorder='little')\n    else:\n        return b\"\"\n\n\ndef tobytes(o):\n    if isinstance(o, str):\n        return o.encode('utf8')\n    return o\n\n\ndef frombytes(o):\n    if isinstance(o, bytes):\n        return o.decode('utf8')\n    return o\n\n\ndef run_cmd(cmd, **kwargs):\n    if isinstance(cmd, str):\n        cmd = cmd.split(' ')\n\n    try:\n        kwargs.update(stderr=subprocess.STDOUT)\n        output = subprocess.check_output(cmd, **kwargs)\n    except subprocess.CalledProcessError as e:\n        # this avoids hiding the stdout / stderr of failed processes\n        sio = io.StringIO()\n        print('Command failed:', \" \".join(cmd), file=sio)\n        print('With output:', file=sio)\n        print('--------------', file=sio)\n        print(frombytes(e.output), file=sio)\n        print('--------------', file=sio)\n        raise RuntimeError(sio.getvalue())\n\n    return frombytes(output)\n\n\n# Adapted from CPython\ndef find_unused_port(family=socket.AF_INET, socktype=socket.SOCK_STREAM):\n    \"\"\"Returns an unused port that should be suitable for binding.  This is\n    achieved by creating a temporary socket with the same family and type as\n    the 'sock' parameter (default is AF_INET, SOCK_STREAM), and binding it to\n    the specified host address (defaults to 0.0.0.0) with the port set to 0,\n    eliciting an unused ephemeral port from the OS.  The temporary socket is\n    then closed and deleted, and the ephemeral port is returned.\n    \"\"\"\n    with socket.socket(family, socktype) as tempsock:\n        tempsock.bind(('', 0))\n        port = tempsock.getsockname()[1]\n    del tempsock\n    return port\n", "dev/archery/archery/integration/runner.py": "# licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import namedtuple\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nimport glob\nimport gzip\nimport itertools\nimport os\nimport sys\nimport tempfile\nimport traceback\nfrom typing import Callable, List, Optional\n\nfrom . import cdata\nfrom .scenario import Scenario\nfrom .tester import Tester, CDataExporter, CDataImporter\nfrom .tester_cpp import CppTester\nfrom .tester_go import GoTester\nfrom .tester_rust import RustTester\nfrom .tester_java import JavaTester\nfrom .tester_js import JSTester\nfrom .tester_csharp import CSharpTester\nfrom .tester_nanoarrow import NanoarrowTester\nfrom .util import guid, printer\nfrom .util import SKIP_C_ARRAY, SKIP_C_SCHEMA, SKIP_FLIGHT, SKIP_IPC\nfrom ..utils.source import ARROW_ROOT_DEFAULT\nfrom . import datagen\n\n\nFailure = namedtuple('Failure',\n                     ('test_case', 'producer', 'consumer', 'exc_info'))\n\nlog = printer.print\n\n\nclass Outcome:\n    def __init__(self):\n        self.failure = None\n        self.skipped = False\n\n\nclass IntegrationRunner(object):\n\n    def __init__(self, json_files,\n                 flight_scenarios: List[Scenario],\n                 testers: List[Tester], tempdir=None,\n                 debug=False, stop_on_error=True, gold_dirs=None,\n                 serial=False, match=None, **unused_kwargs):\n        self.json_files = json_files\n        self.flight_scenarios = flight_scenarios\n        self.testers = testers\n        self.temp_dir = tempdir or tempfile.mkdtemp()\n        self.debug = debug\n        self.stop_on_error = stop_on_error\n        self.serial = serial\n        self.gold_dirs = gold_dirs\n        self.failures: List[Outcome] = []\n        self.skips: List[Outcome] = []\n        self.match = match\n\n        if self.match is not None:\n            print(\"-- Only running tests with {} in their name\"\n                  .format(self.match))\n            self.json_files = [json_file for json_file in self.json_files\n                               if self.match in json_file.name]\n\n    def run_ipc(self):\n        \"\"\"\n        Run Arrow IPC integration tests for the matrix of enabled\n        implementations.\n        \"\"\"\n        for producer, consumer in itertools.product(\n                filter(lambda t: t.PRODUCER, self.testers),\n                filter(lambda t: t.CONSUMER, self.testers)):\n            self._compare_ipc_implementations(\n                producer, consumer, self._produce_consume,\n                self.json_files)\n        if self.gold_dirs:\n            for gold_dir, consumer in itertools.product(\n                    self.gold_dirs,\n                    filter(lambda t: t.CONSUMER, self.testers)):\n                log('\\n')\n                log('******************************************************')\n                log('Tests against golden files in {}'.format(gold_dir))\n                log('******************************************************')\n\n                def run_gold(_, consumer, test_case: datagen.File):\n                    return self._run_gold(gold_dir, consumer, test_case)\n                self._compare_ipc_implementations(\n                    consumer, consumer, run_gold,\n                    self._gold_tests(gold_dir))\n        log('\\n')\n\n    def run_flight(self):\n        \"\"\"\n        Run Arrow Flight integration tests for the matrix of enabled\n        implementations.\n        \"\"\"\n        servers = filter(lambda t: t.FLIGHT_SERVER, self.testers)\n        clients = filter(lambda t: (t.FLIGHT_CLIENT and t.CONSUMER),\n                         self.testers)\n        for server, client in itertools.product(servers, clients):\n            self._compare_flight_implementations(server, client)\n        log('\\n')\n\n    def run_c_data(self):\n        \"\"\"\n        Run Arrow C Data interface integration tests for the matrix of\n        enabled implementations.\n        \"\"\"\n        for producer, consumer in itertools.product(\n                filter(lambda t: t.C_DATA_SCHEMA_EXPORTER, self.testers),\n                filter(lambda t: t.C_DATA_SCHEMA_IMPORTER, self.testers)):\n            self._compare_c_data_implementations(producer, consumer)\n        log('\\n')\n\n    def _gold_tests(self, gold_dir):\n        prefix = os.path.basename(os.path.normpath(gold_dir))\n        SUFFIX = \".json.gz\"\n        golds = [jf for jf in os.listdir(gold_dir) if jf.endswith(SUFFIX)]\n        for json_path in golds:\n            name = json_path[json_path.index('_')+1: -len(SUFFIX)]\n            base_name = prefix + \"_\" + name + \".gold.json\"\n            out_path = os.path.join(self.temp_dir, base_name)\n            with gzip.open(os.path.join(gold_dir, json_path)) as i:\n                with open(out_path, \"wb\") as out:\n                    out.write(i.read())\n\n            # Find the generated file with the same name as this gold file\n            try:\n                equiv_json_file = next(f for f in self.json_files\n                                       if f.name == name)\n            except StopIteration:\n                equiv_json_file = None\n\n            skip_testers = set()\n            if name == 'union' and prefix == '0.17.1':\n                skip_testers.add(\"Java\")\n                skip_testers.add(\"JS\")\n            if prefix == '1.0.0-bigendian' or prefix == '1.0.0-littleendian':\n                skip_testers.add(\"C#\")\n                skip_testers.add(\"Java\")\n                skip_testers.add(\"JS\")\n                skip_testers.add(\"Rust\")\n            if prefix == '2.0.0-compression':\n                skip_testers.add(\"JS\")\n\n            # See https://github.com/apache/arrow/pull/9822 for how to\n            # disable specific compression type tests.\n\n            if prefix == '4.0.0-shareddict':\n                skip_testers.add(\"C#\")\n\n            quirks = set()\n            if prefix in {'0.14.1', '0.17.1',\n                          '1.0.0-bigendian', '1.0.0-littleendian'}:\n                # ARROW-13558: older versions generated decimal values that\n                # were out of range for the given precision.\n                quirks.add(\"no_decimal_validate\")\n                quirks.add(\"no_date64_validate\")\n                quirks.add(\"no_times_validate\")\n\n            json_file = datagen.File(name, schema=None, batches=None,\n                                     path=out_path,\n                                     skip_testers=skip_testers,\n                                     quirks=quirks)\n            if equiv_json_file is not None:\n                json_file.add_skips_from(equiv_json_file)\n            yield json_file\n\n    def _run_test_cases(self,\n                        case_runner: Callable[[datagen.File], Outcome],\n                        test_cases: List[datagen.File],\n                        *, serial: Optional[bool] = None) -> None:\n        \"\"\"\n        Populate self.failures with the outcomes of the\n        ``case_runner`` ran against ``test_cases``\n        \"\"\"\n        def case_wrapper(test_case):\n            if serial:\n                return case_runner(test_case)\n            with printer.cork():\n                return case_runner(test_case)\n\n        if serial is None:\n            serial = self.serial\n\n        if self.failures and self.stop_on_error:\n            return\n\n        if serial:\n            for outcome in map(case_wrapper, test_cases):\n                if outcome.failure is not None:\n                    self.failures.append(outcome.failure)\n                    if self.stop_on_error:\n                        break\n                elif outcome.skipped:\n                    self.skips.append(outcome)\n\n        else:\n            with ThreadPoolExecutor() as executor:\n                for outcome in executor.map(case_wrapper, test_cases):\n                    if outcome.failure is not None:\n                        self.failures.append(outcome.failure)\n                        if self.stop_on_error:\n                            break\n                    elif outcome.skipped:\n                        self.skips.append(outcome)\n\n    def _compare_ipc_implementations(\n        self,\n        producer: Tester,\n        consumer: Tester,\n        run_binaries: Callable[[Tester, Tester, datagen.File], None],\n        test_cases: List[datagen.File]\n    ):\n        \"\"\"\n        Compare Arrow IPC for two implementations (one producer, one consumer).\n        \"\"\"\n        log('##########################################################')\n        log('IPC: {0} producing, {1} consuming'\n            .format(producer.name, consumer.name))\n        log('##########################################################')\n\n        case_runner = partial(self._run_ipc_test_case,\n                              producer, consumer, run_binaries)\n        self._run_test_cases(case_runner, test_cases)\n\n    def _run_ipc_test_case(\n        self,\n        producer: Tester,\n        consumer: Tester,\n        run_binaries: Callable[[Tester, Tester, datagen.File], None],\n        test_case: datagen.File,\n    ) -> Outcome:\n        \"\"\"\n        Run one IPC test case.\n        \"\"\"\n        outcome = Outcome()\n\n        json_path = test_case.path\n        log('=' * 70)\n        log('Testing file {0}'.format(json_path))\n\n        if test_case.should_skip(producer.name, SKIP_IPC):\n            log(f'-- Skipping test because producer {producer.name} does '\n                f'not support IPC')\n            outcome.skipped = True\n\n        elif test_case.should_skip(consumer.name, SKIP_IPC):\n            log(f'-- Skipping test because consumer {consumer.name} does '\n                f'not support IPC')\n            outcome.skipped = True\n\n        else:\n            try:\n                run_binaries(producer, consumer, test_case)\n            except Exception:\n                traceback.print_exc(file=printer.stdout)\n                outcome.failure = Failure(test_case, producer, consumer,\n                                          sys.exc_info())\n\n        log('=' * 70)\n\n        return outcome\n\n    def _produce_consume(self,\n                         producer: Tester,\n                         consumer: Tester,\n                         test_case: datagen.File\n                         ) -> None:\n        \"\"\"\n        Given a producer and a consumer, run different combination of\n        tests for the ``test_case``\n        * read and write are consistent\n        * stream to file is consistent\n        \"\"\"\n        # Make the random access file\n        json_path = test_case.path\n        file_id = guid()[:8]\n        name = os.path.splitext(os.path.basename(json_path))[0]\n\n        producer_file_path = os.path.join(self.temp_dir, file_id + '_' +\n                                          name + '.json_as_file')\n        producer_stream_path = os.path.join(self.temp_dir, file_id + '_' +\n                                            name + '.producer_file_as_stream')\n        consumer_file_path = os.path.join(self.temp_dir, file_id + '_' +\n                                          name + '.consumer_stream_as_file')\n\n        log('-- Creating binary inputs')\n        producer.json_to_file(json_path, producer_file_path)\n\n        # Validate the file\n        log('-- Validating file')\n        consumer.validate(json_path, producer_file_path)\n\n        log('-- Validating stream')\n        producer.file_to_stream(producer_file_path, producer_stream_path)\n        consumer.stream_to_file(producer_stream_path, consumer_file_path)\n        consumer.validate(json_path, consumer_file_path)\n\n    def _run_gold(self,\n                  gold_dir: str,\n                  consumer: Tester,\n                  test_case: datagen.File) -> None:\n        \"\"\"\n        Given a directory with:\n        * an ``.arrow_file``\n        * a ``.stream``\n        associated to the json integration file at ``test_case.path``\n\n        verify that the consumer can read both and agrees with\n        what the json file contains; also run ``stream_to_file`` and\n        verify that the consumer produces an equivalent file from the\n        IPC stream.\n        \"\"\"\n        json_path = test_case.path\n\n        # Validate the file\n        log('-- Validating file')\n        producer_file_path = os.path.join(\n            gold_dir, \"generated_\" + test_case.name + \".arrow_file\")\n        consumer.validate(json_path, producer_file_path,\n                          quirks=test_case.quirks)\n\n        log('-- Validating stream')\n        consumer_stream_path = os.path.join(\n            gold_dir, \"generated_\" + test_case.name + \".stream\")\n        file_id = guid()[:8]\n        name = os.path.splitext(os.path.basename(json_path))[0]\n\n        consumer_file_path = os.path.join(self.temp_dir, file_id + '_' +\n                                          name + '.consumer_stream_as_file')\n\n        consumer.stream_to_file(consumer_stream_path, consumer_file_path)\n        consumer.validate(json_path, consumer_file_path,\n                          quirks=test_case.quirks)\n\n    def _compare_flight_implementations(\n        self,\n        producer: Tester,\n        consumer: Tester\n    ):\n        log('##########################################################')\n        log('Flight: {0} serving, {1} requesting'\n            .format(producer.name, consumer.name))\n        log('##########################################################')\n\n        case_runner = partial(self._run_flight_test_case, producer, consumer)\n        self._run_test_cases(\n            case_runner, self.json_files + self.flight_scenarios)\n\n    def _run_flight_test_case(self,\n                              producer: Tester,\n                              consumer: Tester,\n                              test_case: datagen.File) -> Outcome:\n        \"\"\"\n        Run one Flight test case.\n        \"\"\"\n        outcome = Outcome()\n\n        log('=' * 70)\n        log('Testing file {0}'.format(test_case.name))\n\n        if test_case.should_skip(producer.name, SKIP_FLIGHT):\n            log(f'-- Skipping test because producer {producer.name} does '\n                f'not support Flight')\n            outcome.skipped = True\n\n        elif test_case.should_skip(consumer.name, SKIP_FLIGHT):\n            log(f'-- Skipping test because consumer {consumer.name} does '\n                f'not support Flight')\n            outcome.skipped = True\n\n        else:\n            try:\n                if isinstance(test_case, Scenario):\n                    server = producer.flight_server(test_case.name)\n                    client_args = {'scenario_name': test_case.name}\n                else:\n                    server = producer.flight_server()\n                    client_args = {'json_path': test_case.path}\n\n                with server as port:\n                    # Have the client upload the file, then download and\n                    # compare\n                    consumer.flight_request(port, **client_args)\n            except Exception:\n                traceback.print_exc(file=printer.stdout)\n                outcome.failure = Failure(test_case, producer, consumer,\n                                          sys.exc_info())\n\n        log('=' * 70)\n\n        return outcome\n\n    def _compare_c_data_implementations(\n        self,\n        producer: Tester,\n        consumer: Tester\n    ):\n        log('##########################################################')\n        log(f'C Data Interface: '\n            f'{producer.name} exporting, {consumer.name} importing')\n        log('##########################################################')\n\n        # Serial execution is required for proper memory accounting\n        serial = True\n\n        with producer.make_c_data_exporter() as exporter:\n            with consumer.make_c_data_importer() as importer:\n                case_runner = partial(self._run_c_schema_test_case,\n                                      producer, consumer,\n                                      exporter, importer)\n                self._run_test_cases(case_runner, self.json_files, serial=serial)\n\n                if producer.C_DATA_ARRAY_EXPORTER and consumer.C_DATA_ARRAY_IMPORTER:\n                    case_runner = partial(self._run_c_array_test_cases,\n                                          producer, consumer,\n                                          exporter, importer)\n                    self._run_test_cases(case_runner, self.json_files, serial=serial)\n\n    def _run_c_schema_test_case(self,\n                                producer: Tester, consumer: Tester,\n                                exporter: CDataExporter,\n                                importer: CDataImporter,\n                                test_case: datagen.File) -> Outcome:\n        \"\"\"\n        Run one C ArrowSchema test case.\n        \"\"\"\n        outcome = Outcome()\n\n        def do_run():\n            json_path = test_case.path\n            ffi = cdata.ffi()\n            c_schema_ptr = ffi.new(\"struct ArrowSchema*\")\n            with cdata.check_memory_released(exporter, importer):\n                exporter.export_schema_from_json(json_path, c_schema_ptr)\n                importer.import_schema_and_compare_to_json(json_path, c_schema_ptr)\n\n        log('=' * 70)\n        log(f'Testing C ArrowSchema from file {test_case.name!r}')\n\n        if test_case.should_skip(producer.name, SKIP_C_SCHEMA):\n            log(f'-- Skipping test because producer {producer.name} does '\n                f'not support C ArrowSchema')\n            outcome.skipped = True\n\n        elif test_case.should_skip(consumer.name, SKIP_C_SCHEMA):\n            log(f'-- Skipping test because consumer {consumer.name} does '\n                f'not support C ArrowSchema')\n            outcome.skipped = True\n\n        else:\n            try:\n                do_run()\n            except Exception:\n                traceback.print_exc(file=printer.stdout)\n                outcome.failure = Failure(test_case, producer, consumer,\n                                          sys.exc_info())\n\n        log('=' * 70)\n\n        return outcome\n\n    def _run_c_array_test_cases(self,\n                                producer: Tester, consumer: Tester,\n                                exporter: CDataExporter,\n                                importer: CDataImporter,\n                                test_case: datagen.File) -> Outcome:\n        \"\"\"\n        Run one set C ArrowArray test cases.\n        \"\"\"\n        outcome = Outcome()\n\n        def do_run():\n            json_path = test_case.path\n            ffi = cdata.ffi()\n            c_array_ptr = ffi.new(\"struct ArrowArray*\")\n            for num_batch in range(test_case.num_batches):\n                log(f'... with record batch #{num_batch}')\n                with cdata.check_memory_released(exporter, importer):\n                    exporter.export_batch_from_json(json_path,\n                                                    num_batch,\n                                                    c_array_ptr)\n                    importer.import_batch_and_compare_to_json(json_path,\n                                                              num_batch,\n                                                              c_array_ptr)\n\n        log('=' * 70)\n        log(f'Testing C ArrowArray '\n            f'from file {test_case.name!r}')\n\n        if test_case.should_skip(producer.name, SKIP_C_ARRAY):\n            log(f'-- Skipping test because producer {producer.name} does '\n                f'not support C ArrowArray')\n            outcome.skipped = True\n\n        elif test_case.should_skip(consumer.name, SKIP_C_ARRAY):\n            log(f'-- Skipping test because consumer {consumer.name} does '\n                f'not support C ArrowArray')\n            outcome.skipped = True\n\n        else:\n            try:\n                do_run()\n            except Exception:\n                traceback.print_exc(file=printer.stdout)\n                outcome.failure = Failure(test_case, producer, consumer,\n                                          sys.exc_info())\n\n        log('=' * 70)\n\n        return outcome\n\n\ndef get_static_json_files():\n    glob_pattern = os.path.join(ARROW_ROOT_DEFAULT,\n                                'integration', 'data', '*.json')\n    return [\n        datagen.File(name=os.path.basename(p), path=p,\n                     schema=None, batches=None)\n        for p in glob.glob(glob_pattern)\n    ]\n\n\ndef run_all_tests(with_cpp=True, with_java=True, with_js=True,\n                  with_csharp=True, with_go=True, with_rust=False,\n                  with_nanoarrow=False, run_ipc=False, run_flight=False,\n                  run_c_data=False, tempdir=None, **kwargs):\n    tempdir = tempdir or tempfile.mkdtemp(prefix='arrow-integration-')\n\n    testers: List[Tester] = []\n\n    if with_cpp:\n        testers.append(CppTester(**kwargs))\n\n    if with_java:\n        testers.append(JavaTester(**kwargs))\n\n    if with_js:\n        testers.append(JSTester(**kwargs))\n\n    if with_csharp:\n        testers.append(CSharpTester(**kwargs))\n\n    if with_go:\n        testers.append(GoTester(**kwargs))\n\n    if with_nanoarrow:\n        testers.append(NanoarrowTester(**kwargs))\n\n    if with_rust:\n        testers.append(RustTester(**kwargs))\n\n    static_json_files = get_static_json_files()\n    generated_json_files = datagen.get_generated_json_files(tempdir=tempdir)\n    json_files = static_json_files + generated_json_files\n\n    # Additional integration test cases for Arrow Flight.\n    flight_scenarios = [\n        Scenario(\n            \"auth:basic_proto\",\n            description=\"Authenticate using the BasicAuth protobuf.\"),\n        Scenario(\n            \"middleware\",\n            description=\"Ensure headers are propagated via middleware.\",\n        ),\n        Scenario(\n            \"ordered\",\n            description=\"Ensure FlightInfo.ordered is supported.\",\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"expiration_time:do_get\",\n            description=(\"Ensure FlightEndpoint.expiration_time with \"\n                         \"DoGet is working as expected.\"),\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"expiration_time:list_actions\",\n            description=(\"Ensure FlightEndpoint.expiration_time related \"\n                         \"pre-defined actions is working with ListActions \"\n                         \"as expected.\"),\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"expiration_time:cancel_flight_info\",\n            description=(\"Ensure FlightEndpoint.expiration_time and \"\n                         \"CancelFlightInfo are working as expected.\"),\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"expiration_time:renew_flight_endpoint\",\n            description=(\"Ensure FlightEndpoint.expiration_time and \"\n                         \"RenewFlightEndpoint are working as expected.\"),\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"location:reuse_connection\",\n            description=\"Ensure arrow-flight-reuse-connection is accepted.\",\n            skip_testers={\"JS\", \"C#\", \"Rust\"},\n        ),\n        Scenario(\n            \"session_options\",\n            description=\"Ensure Flight SQL Sessions work as expected.\",\n            skip_testers={\"JS\", \"C#\", \"Rust\"}\n        ),\n        Scenario(\n            \"poll_flight_info\",\n            description=\"Ensure PollFlightInfo is supported.\",\n            skip_testers={\"JS\", \"C#\", \"Rust\"}\n        ),\n        Scenario(\n            \"app_metadata_flight_info_endpoint\",\n            description=\"Ensure support FlightInfo and Endpoint app_metadata\",\n            skip_testers={\"JS\", \"C#\", \"Rust\"}\n        ),\n        Scenario(\n            \"flight_sql\",\n            description=\"Ensure Flight SQL protocol is working as expected.\",\n            skip_testers={\"Rust\"}\n        ),\n        Scenario(\n            \"flight_sql:extension\",\n            description=\"Ensure Flight SQL extensions work as expected.\",\n            skip_testers={\"Rust\"}\n        ),\n        Scenario(\n            \"flight_sql:ingestion\",\n            description=\"Ensure Flight SQL ingestion works as expected.\",\n            skip_testers={\"JS\", \"C#\", \"Rust\", \"Java\"}\n        ),\n    ]\n\n    runner = IntegrationRunner(json_files, flight_scenarios, testers, **kwargs)\n    if run_ipc:\n        runner.run_ipc()\n    if run_flight:\n        runner.run_flight()\n    if run_c_data:\n        runner.run_c_data()\n\n    fail_count = 0\n    if runner.failures:\n        log(\"################# FAILURES #################\")\n        for test_case, producer, consumer, exc_info in runner.failures:\n            fail_count += 1\n            log(\"FAILED TEST:\", end=\" \")\n            log(test_case.name, producer.name, \"producing, \",\n                consumer.name, \"consuming\")\n            if exc_info:\n                exc_type, exc_value, exc_tb = exc_info\n                log(f'{exc_type}: {exc_value}')\n            log()\n\n    log(f\"{fail_count} failures, {len(runner.skips)} skips\")\n    if fail_count > 0:\n        sys.exit(1)\n\n\ndef write_js_test_json(directory):\n    datagen.generate_primitive_case([], name='primitive_no_batches').write(\n        os.path.join(directory, 'primitive-no-batches.json')\n    )\n    datagen.generate_primitive_case([17, 20], name='primitive').write(\n        os.path.join(directory, 'primitive.json')\n    )\n    datagen.generate_primitive_case([0, 0, 0], name='primitive_zerolength').write(\n        os.path.join(directory, 'primitive-empty.json')\n    )\n    # datagen.generate_primitive_large_offsets_case([17, 20]).write(\n    #     os.path.join(directory, 'primitive-large-offsets.json')\n    # )\n    datagen.generate_null_case([10, 0]).write(\n        os.path.join(directory, 'null.json')\n    )\n    datagen.generate_null_trivial_case([0, 0]).write(\n        os.path.join(directory, 'null-trivial.json')\n    )\n    datagen.generate_decimal128_case().write(\n        os.path.join(directory, 'decimal128.json')\n    )\n    # datagen.generate_decimal256_case().write(\n    #     os.path.join(directory, 'decimal256.json')\n    # )\n    datagen.generate_datetime_case().write(\n        os.path.join(directory, 'datetime.json')\n    )\n    # datagen.generate_duration_case().write(\n    #     os.path.join(directory, 'duration.json')\n    # )\n    # datagen.generate_interval_case().write(\n    #     os.path.join(directory, 'interval.json')\n    # )\n    # datagen.generate_month_day_nano_interval_case().write(\n    #     os.path.join(directory, 'month_day_nano_interval.json')\n    # )\n    datagen.generate_map_case().write(\n        os.path.join(directory, 'map.json')\n    )\n    datagen.generate_non_canonical_map_case().write(\n        os.path.join(directory, 'non_canonical_map.json')\n    )\n    datagen.generate_nested_case().write(\n        os.path.join(directory, 'nested.json')\n    )\n    datagen.generate_recursive_nested_case().write(\n        os.path.join(directory, 'recursive-nested.json')\n    )\n    # datagen.generate_nested_large_offsets_case().write(\n    #     os.path.join(directory, 'nested-large-offsets.json')\n    # )\n    datagen.generate_unions_case().write(\n        os.path.join(directory, 'unions.json')\n    )\n    datagen.generate_custom_metadata_case().write(\n        os.path.join(directory, 'custom-metadata.json')\n    )\n    # datagen.generate_duplicate_fieldnames_case().write(\n    #     os.path.join(directory, 'duplicate-fieldnames.json')\n    # )\n    datagen.generate_dictionary_case().write(\n        os.path.join(directory, 'dictionary.json')\n    )\n    datagen.generate_dictionary_unsigned_case().write(\n        os.path.join(directory, 'dictionary-unsigned.json')\n    )\n    datagen.generate_nested_dictionary_case().write(\n        os.path.join(directory, 'dictionary-nested.json')\n    )\n    # datagen.generate_run_end_encoded_case().write(\n    #     os.path.join(directory, 'run_end_encoded.json')\n    # )\n    datagen.generate_extension_case().write(\n        os.path.join(directory, 'extension.json')\n    )\n", "dev/archery/archery/integration/scenario.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nclass Scenario:\n    \"\"\"\n    An integration test scenario for Arrow Flight.\n\n    Does not correspond to a particular IPC JSON file.\n    \"\"\"\n\n    def __init__(self, name, description, skip_testers=None):\n        self.name = name\n        self.description = description\n        self.skipped_testers = skip_testers or set()\n\n    def should_skip(self, tester, format):\n        return tester in self.skipped_testers\n", "dev/archery/archery/integration/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "dev/archery/archery/release/reports.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nfrom ..utils.report import JinjaReport\n\n\nclass ReleaseCuration(JinjaReport):\n    templates = {\n        'console': 'release_curation.txt.j2'\n    }\n    fields = [\n        'release',\n        'within',\n        'outside',\n        'noissue',\n        'parquet',\n        'nopatch',\n        'minimal',\n        'minor'\n    ]\n\n\nclass ReleaseChangelog(JinjaReport):\n    templates = {\n        'markdown': 'release_changelog.md.j2',\n        'html': 'release_changelog.html.j2'\n    }\n    fields = [\n        'release',\n        'categories'\n    ]\n", "dev/archery/archery/release/cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pathlib\n\nimport click\n\nfrom ..utils.cli import validate_arrow_sources\nfrom .core import IssueTracker, Release\n\n\n@click.group('release')\n@click.option(\"--src\", metavar=\"<arrow_src>\", default=None,\n              callback=validate_arrow_sources,\n              help=\"Specify Arrow source directory.\")\n@click.option('--github-token', '-t', default=None,\n              envvar=\"CROSSBOW_GITHUB_TOKEN\",\n              help='OAuth token for GitHub authentication')\n@click.pass_obj\ndef release(obj, src, github_token):\n    \"\"\"Release related commands.\"\"\"\n\n    obj['issue_tracker'] = IssueTracker(github_token=github_token)\n    obj['repo'] = src.path\n\n\n@release.command('curate', help=\"Lists release related issues.\")\n@click.argument('version')\n@click.option('--minimal/--full', '-m/-f',\n              help=\"Only show actionable issues.\", default=False)\n@click.pass_obj\ndef release_curate(obj, version, minimal):\n    \"\"\"Release curation.\"\"\"\n    release = Release(version, repo=obj['repo'],\n                      issue_tracker=obj['issue_tracker'])\n    curation = release.curate(minimal)\n\n    click.echo(curation.render('console'))\n\n\n@release.group('changelog')\ndef release_changelog():\n    \"\"\"Release changelog.\"\"\"\n    pass\n\n\n@release_changelog.command('add')\n@click.argument('version')\n@click.pass_obj\ndef release_changelog_add(obj, version):\n    \"\"\"Prepend the changelog with the current release\"\"\"\n    repo, issue_tracker = obj['repo'], obj['issue_tracker']\n\n    # just handle the current version\n    release = Release(version, repo=repo, issue_tracker=issue_tracker)\n    if release.is_released:\n        raise ValueError('This version has been already released!')\n\n    changelog = release.changelog()\n    changelog_path = pathlib.Path(repo) / 'CHANGELOG.md'\n\n    current_content = changelog_path.read_text()\n    new_content = changelog.render('markdown') + current_content\n\n    changelog_path.write_text(new_content)\n    click.echo(\"CHANGELOG.md is updated!\")\n\n\n@release_changelog.command('generate')\n@click.argument('version')\n@click.argument('output', type=click.File('w', encoding='utf8'), default='-')\n@click.pass_obj\ndef release_changelog_generate(obj, version, output):\n    \"\"\"Generate the changelog of a specific release.\"\"\"\n    repo, issue_tracker = obj['repo'], obj['issue_tracker']\n\n    # just handle the current version\n    release = Release(version, repo=repo, issue_tracker=issue_tracker)\n\n    changelog = release.changelog()\n    output.write(changelog.render('markdown'))\n\n\n@release_changelog.command('regenerate')\n@click.pass_obj\ndef release_changelog_regenerate(obj):\n    \"\"\"Regenerate the whole CHANGELOG.md file\"\"\"\n    issue_tracker, repo = obj['issue_tracker'], obj['repo']\n    changelogs = []\n    issue_tracker = IssueTracker(issue_tracker=issue_tracker)\n\n    for version in issue_tracker.project_versions():\n        if not version.released:\n            continue\n        release = Release(version, repo=repo,\n                          issue_tracker=issue_tracker)\n        click.echo('Querying changelog for version: {}'.format(version))\n        changelogs.append(release.changelog())\n\n    click.echo('Rendering new CHANGELOG.md file...')\n    changelog_path = pathlib.Path(repo) / 'CHANGELOG.md'\n    with changelog_path.open('w') as fp:\n        for cl in changelogs:\n            fp.write(cl.render('markdown'))\n\n\n@release.command('cherry-pick')\n@click.argument('version')\n@click.option('--dry-run/--execute', default=True,\n              help=\"Display the git commands instead of executing them.\")\n@click.option('--recreate/--continue', default=True,\n              help=\"Recreate the maintenance branch or only apply unapplied \"\n                   \"patches.\")\n@click.pass_obj\ndef release_cherry_pick(obj, version, dry_run, recreate):\n    \"\"\"\n    Cherry pick commits.\n    \"\"\"\n    issue_tracker = obj['issue_tracker']\n    release = Release(version,\n                      repo=obj['repo'], issue_tracker=issue_tracker)\n\n    if not dry_run:\n        release.cherry_pick_commits(recreate_branch=recreate)\n    else:\n        click.echo(f'git checkout -b {release.branch} {release.base_branch}')\n        for commit in release.commits_to_pick():\n            click.echo('git cherry-pick {}'.format(commit.hexsha))\n", "dev/archery/archery/release/core.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom abc import abstractmethod\nfrom collections import defaultdict\nimport functools\nimport os\nimport pathlib\nimport re\nimport warnings\n\nfrom git import Repo\nfrom github import Github\nfrom jira import JIRA\nfrom semver import VersionInfo as SemVer\n\nfrom ..utils.source import ArrowSources\nfrom ..utils.logger import logger\nfrom .reports import ReleaseCuration, ReleaseChangelog\n\n\ndef cached_property(fn):\n    return property(functools.lru_cache(maxsize=1)(fn))\n\n\nclass Version(SemVer):\n\n    __slots__ = ('released', 'release_date')\n\n    def __init__(self, released=False, release_date=None, **kwargs):\n        super().__init__(**kwargs)\n        self.released = released\n        self.release_date = release_date\n\n    @classmethod\n    def parse(cls, version, **kwargs):\n        return cls(**SemVer.parse(version).to_dict(), **kwargs)\n\n    @classmethod\n    def from_jira(cls, jira_version):\n        return cls.parse(\n            jira_version.name,\n            released=jira_version.released,\n            release_date=getattr(jira_version, 'releaseDate', None)\n        )\n\n    @classmethod\n    def from_milestone(cls, milestone):\n        return cls.parse(\n            milestone.title,\n            released=milestone.state == \"closed\",\n            release_date=milestone.due_on\n        )\n\n\nclass Issue:\n\n    def __init__(self, key, type, summary, github_issue=None):\n        self.key = key\n        self.type = type\n        self.summary = summary\n        self.github_issue_id = getattr(github_issue, \"number\", None)\n        self._github_issue = github_issue\n\n    @classmethod\n    def from_jira(cls, jira_issue):\n        return cls(\n            key=jira_issue.key,\n            type=jira_issue.fields.issuetype.name,\n            summary=jira_issue.fields.summary\n        )\n\n    @classmethod\n    def from_github(cls, github_issue):\n        return cls(\n            key=github_issue.number,\n            type=next(\n                iter(\n                    [\n                        label.name for label in github_issue.labels\n                        if label.name.startswith(\"Type:\")\n                    ]\n                ), None),\n            summary=github_issue.title,\n            github_issue=github_issue\n        )\n\n    @property\n    def project(self):\n        if isinstance(self.key, int):\n            return 'GH'\n        return self.key.split('-')[0]\n\n    @property\n    def number(self):\n        if isinstance(self.key, str):\n            return int(self.key.split('-')[1])\n        else:\n            return self.key\n\n    @cached_property\n    def is_pr(self):\n        return bool(self._github_issue and self._github_issue.pull_request)\n\n\nclass Jira(JIRA):\n\n    def __init__(self, url='https://issues.apache.org/jira'):\n        super().__init__(url)\n\n    def issue(self, key):\n        return Issue.from_jira(super().issue(key))\n\n\nclass IssueTracker:\n\n    def __init__(self, github_token=None):\n        github = Github(github_token)\n        self.github_repo = github.get_repo('apache/arrow')\n\n    def project_version(self, version_string):\n        for milestone in self.project_versions():\n            if milestone == version_string:\n                return milestone\n\n    def project_versions(self):\n        versions = []\n        milestones = self.github_repo.get_milestones(state=\"all\")\n        for milestone in milestones:\n            try:\n                versions.append(Version.from_milestone(milestone))\n            except ValueError:\n                # ignore invalid semantic versions like JS-0.4.0\n                continue\n        return sorted(versions, reverse=True)\n\n    def _milestone_from_semver(self, semver):\n        milestones = self.github_repo.get_milestones(state=\"all\")\n        for milestone in milestones:\n            try:\n                if milestone.title == semver:\n                    return milestone\n            except ValueError:\n                # ignore invalid semantic versions like JS-0.3.0\n                continue\n\n    def project_issues(self, version):\n        issues = self.github_repo.get_issues(\n            milestone=self._milestone_from_semver(version),\n            state=\"all\")\n        return list(map(Issue.from_github, issues))\n\n    def issue(self, key):\n        return Issue.from_github(self.github_repo.get_issue(key))\n\n\n_TITLE_REGEX = re.compile(\n    r\"(?P<issue>(?P<project>(ARROW|PARQUET|GH))\\-(?P<issue_id>(\\d+)))?\\s*:?\\s*\"\n    r\"(?P<minor>(MINOR))?\\s*:?\\s*\"\n    r\"(?P<components>\\[.*\\])?\\s*(?P<summary>.*)\"\n)\n_COMPONENT_REGEX = re.compile(r\"\\[([^\\[\\]]+)\\]\")\n\n\nclass CommitTitle:\n\n    def __init__(self, summary, project=None, issue=None, minor=None,\n                 components=None, issue_id=None):\n        self.project = project\n        self.issue = issue\n        self.issue_id = issue_id\n        self.components = components or []\n        self.summary = summary\n        self.minor = bool(minor)\n\n    def __str__(self):\n        return self.to_string()\n\n    def __eq__(self, other):\n        return (\n            self.summary == other.summary and\n            self.project == other.project and\n            self.issue == other.issue and\n            self.minor == other.minor and\n            self.components == other.components\n        )\n\n    def __hash__(self):\n        return hash(\n            (self.summary, self.project, self.issue, tuple(self.components))\n        )\n\n    @classmethod\n    def parse(cls, headline):\n        matches = _TITLE_REGEX.match(headline)\n        if matches is None:\n            warnings.warn(\n                \"Unable to parse commit message `{}`\".format(headline)\n            )\n            return CommitTitle(headline)\n\n        values = matches.groupdict()\n        components = values.get('components') or ''\n        components = _COMPONENT_REGEX.findall(components)\n\n        return CommitTitle(\n            values['summary'],\n            project=values.get('project'),\n            issue=values.get('issue'),\n            issue_id=values.get('issue_id'),\n            minor=values.get('minor'),\n            components=components\n        )\n\n    def to_string(self, with_issue=True, with_components=True):\n        out = \"\"\n        if with_issue and self.issue:\n            out += \"{}: \".format(self.issue)\n        if with_components and self.components:\n            for component in self.components:\n                out += \"[{}]\".format(component)\n            out += \" \"\n        out += self.summary\n        return out\n\n\nclass Commit:\n\n    def __init__(self, wrapped):\n        self._title = CommitTitle.parse(wrapped.summary)\n        self._wrapped = wrapped\n\n    def __getattr__(self, attr):\n        if hasattr(self._title, attr):\n            return getattr(self._title, attr)\n        else:\n            return getattr(self._wrapped, attr)\n\n    def __repr__(self):\n        template = '<Commit sha={!r} issue={!r} components={!r} summary={!r}>'\n        return template.format(self.hexsha, self.issue, self.components,\n                               self.summary)\n\n    @property\n    def url(self):\n        return 'https://github.com/apache/arrow/commit/{}'.format(self.hexsha)\n\n    @property\n    def title(self):\n        return self._title\n\n\nclass Release:\n\n    def __new__(self, version, repo=None, github_token=None,\n                issue_tracker=None):\n        if isinstance(version, str):\n            version = Version.parse(version)\n        elif not isinstance(version, Version):\n            raise TypeError(version)\n\n        # decide the type of the release based on the version number\n        if version.patch == 0:\n            if version.minor == 0:\n                klass = MajorRelease\n            elif version.major == 0:\n                # handle minor releases before 1.0 as major releases\n                klass = MajorRelease\n            else:\n                klass = MinorRelease\n        else:\n            klass = PatchRelease\n\n        return super().__new__(klass)\n\n    def __init__(self, version, repo, issue_tracker):\n        if repo is None:\n            arrow = ArrowSources.find()\n            repo = Repo(arrow.path)\n        elif isinstance(repo, (str, pathlib.Path)):\n            repo = Repo(repo)\n        elif not isinstance(repo, Repo):\n            raise TypeError(\"`repo` argument must be a path or a valid Repo \"\n                            \"instance\")\n\n        if isinstance(version, str):\n            version = issue_tracker.project_version(version)\n\n        elif not isinstance(version, Version):\n            raise TypeError(version)\n\n        self.version = version\n        self.repo = repo\n        self.issue_tracker = issue_tracker\n\n    def __repr__(self):\n        if self.version.released:\n            status = \"released_at={self.version.release_date!r}\"\n        else:\n            status = \"pending\"\n        return f\"<{self.__class__.__name__} {self.version!r} {status}>\"\n\n    @property\n    def is_released(self):\n        return self.version.released\n\n    @property\n    def tag(self):\n        return f\"apache-arrow-{self.version}\"\n\n    @property\n    @abstractmethod\n    def branch(self):\n        \"\"\"\n        Target branch that serves as the base for the release.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def siblings(self):\n        \"\"\"\n        Releases to consider when calculating previous and next releases.\n        \"\"\"\n        ...\n\n    @cached_property\n    def previous(self):\n        # select all non-patch releases\n        position = self.siblings.index(self.version)\n        try:\n            previous = self.siblings[position + 1]\n        except IndexError:\n            # first release doesn't have a previous one\n            return None\n        else:\n            return Release(previous, repo=self.repo,\n                           issue_tracker=self.issue_tracker)\n\n    @cached_property\n    def next(self):\n        # select all non-patch releases\n        position = self.siblings.index(self.version)\n        if position <= 0:\n            raise ValueError(\"There is no upcoming release set in JIRA after \"\n                             f\"version {self.version}\")\n        upcoming = self.siblings[position - 1]\n        return Release(upcoming, repo=self.repo,\n                       issue_tracker=self.issue_tracker)\n\n    @cached_property\n    def issues(self):\n        issues = self.issue_tracker.project_issues(\n            self.version\n        )\n        return {i.key: i for i in issues}\n\n    @cached_property\n    def github_issue_ids(self):\n        return {v.github_issue_id for v in self.issues.values()\n                if v.github_issue_id}\n\n    @cached_property\n    def commits(self):\n        \"\"\"\n        All commits applied between two versions.\n        \"\"\"\n        if self.previous is None:\n            # first release\n            lower = ''\n        else:\n            lower = self.repo.tags[self.previous.tag]\n\n        if self.version.released:\n            try:\n                upper = self.repo.tags[self.tag]\n            except IndexError:\n                warnings.warn(f\"Release tag `{self.tag}` doesn't exist.\")\n                return []\n        else:\n            try:\n                upper = self.repo.branches[self.branch]\n            except IndexError:\n                warnings.warn(f\"Release branch `{self.branch}` doesn't exist.\")\n                return []\n\n        commit_range = f\"{lower}..{upper}\"\n        return list(map(Commit, self.repo.iter_commits(commit_range)))\n\n    @cached_property\n    def jira_instance(self):\n        return Jira()\n\n    @cached_property\n    def default_branch(self):\n        default_branch_name = os.getenv(\"ARCHERY_DEFAULT_BRANCH\")\n\n        if default_branch_name is None:\n            # Set up repo object\n            arrow = ArrowSources.find()\n            repo = Repo(arrow.path)\n            origin = repo.remotes[\"origin\"]\n            origin_refs = origin.refs\n\n            try:\n                # Get git.RemoteReference object to origin/HEAD\n                # If the reference does not exist, a KeyError will be thrown\n                origin_head = origin_refs[\"HEAD\"]\n\n                # Get git.RemoteReference object to origin/default-branch-name\n                origin_head_reference = origin_head.reference\n\n                # Get string value of remote head reference, should return\n                # \"origin/main\" or \"origin/master\"\n                origin_head_name = origin_head_reference.name\n                origin_head_name_tokenized = origin_head_name.split(\"/\")\n\n                # The last token is the default branch name\n                default_branch_name = origin_head_name_tokenized[-1]\n            except (KeyError, IndexError):\n                # Use a hard-coded default value to set default_branch_name\n                default_branch_name = \"main\"\n                warnings.warn('Unable to determine default branch name: '\n                              'ARCHERY_DEFAULT_BRANCH environment variable is '\n                              'not set. Git repository does not contain a '\n                              '\\'refs/remotes/origin/HEAD\\'reference. Setting '\n                              'the default branch name to ' +\n                              default_branch_name, RuntimeWarning)\n\n        return default_branch_name\n\n    def curate(self, minimal=False):\n        # handle commits with parquet issue key specially\n        release_issues = self.issues\n        within, outside, noissue, parquet, minor = [], [], [], [], []\n        for c in self.commits:\n            if c.issue is None:\n                if c.title.minor:\n                    minor.append(c)\n                else:\n                    noissue.append(c)\n            elif c.project == 'GH':\n                if int(c.issue_id) in release_issues:\n                    within.append((release_issues[int(c.issue_id)], c))\n                else:\n                    outside.append(\n                        (self.issue_tracker.issue(int(c.issue_id)), c))\n            elif c.project == 'ARROW':\n                if c.issue in release_issues:\n                    within.append((release_issues[c.issue], c))\n                else:\n                    outside.append((self.jira_instance.issue(c.issue), c))\n            elif c.project == 'PARQUET':\n                parquet.append((self.jira_instance.issue(c.issue), c))\n            else:\n                warnings.warn(\n                    f'Issue {c.issue} does not pertain to GH' +\n                    ', ARROW or PARQUET')\n                outside.append((c.issue, c))\n\n        # remaining jira tickets\n        within_keys = {i.key for i, c in within}\n        # Take into account that some issues milestoned are prs\n        nopatch = [issue for key, issue in release_issues.items()\n                   if key not in within_keys and issue.is_pr is False]\n\n        return ReleaseCuration(release=self, within=within, outside=outside,\n                               noissue=noissue, parquet=parquet,\n                               nopatch=nopatch, minimal=minimal, minor=minor)\n\n    def changelog(self):\n        issue_commit_pairs = []\n\n        # get organized report for the release\n        curation = self.curate()\n\n        # jira tickets having patches in the release\n        issue_commit_pairs.extend(curation.within)\n        # parquet patches in the release\n        issue_commit_pairs.extend(curation.parquet)\n\n        # jira tickets without patches\n        for issue in curation.nopatch:\n            issue_commit_pairs.append((issue, None))\n\n        # organize issues into categories\n        issue_types = {\n            'Bug': 'Bug Fixes',\n            'Improvement': 'New Features and Improvements',\n            'New Feature': 'New Features and Improvements',\n            'Sub-task': 'New Features and Improvements',\n            'Task': 'New Features and Improvements',\n            'Test': 'Bug Fixes',\n            'Wish': 'New Features and Improvements',\n            'Type: bug': 'Bug Fixes',\n            'Type: enhancement': 'New Features and Improvements',\n            'Type: task': 'New Features and Improvements',\n            'Type: test': 'Bug Fixes',\n            'Type: usage': 'New Features and Improvements',\n        }\n        categories = defaultdict(list)\n        for issue, commit in issue_commit_pairs:\n            try:\n                categories[issue_types[issue.type]].append((issue, commit))\n            except KeyError:\n                # If issue or pr don't have a type assume task.\n                # Currently the label for type is not mandatory on GitHub.\n                categories[issue_types['Type: task']].append((issue, commit))\n\n        # sort issues by the issue key in ascending order\n        for issues in categories.values():\n            issues.sort(key=lambda pair: (pair[0].project, pair[0].number))\n\n        return ReleaseChangelog(release=self, categories=categories)\n\n    def commits_to_pick(self, exclude_already_applied=True):\n        # collect commits applied on the default branch since the root of the\n        # maintenance branch (the previous major release)\n        commit_range = f\"{self.previous.tag}..{self.default_branch}\"\n\n        # keeping the original order of the commits helps to minimize the merge\n        # conflicts during cherry-picks\n        commits = map(Commit, self.repo.iter_commits(commit_range))\n\n        # exclude patches that have been already applied to the maintenance\n        # branch, we cannot identify patches based on sha because it changes\n        # after the cherry pick so use commit title instead\n        if exclude_already_applied:\n            already_applied = {c.title for c in self.commits}\n        else:\n            already_applied = set()\n\n        # iterate over the commits applied on the main branch and filter out\n        # the ones that are included in the jira release\n        patches_to_pick = []\n        for c in commits:\n            key = c.issue\n            # For the release we assume all issues that have to be\n            # cherry-picked are merged with the GH issue id instead of the\n            # JIRA ARROW one. That's why we use github_issues along with\n            # issues. This is only to correct the mapping for migrated issues.\n            if c.issue and c.issue.startswith(\"GH-\"):\n                key = int(c.issue_id)\n            if ((key in self.github_issue_ids or key in self.issues) and\n                    c.title not in already_applied):\n                patches_to_pick.append(c)\n        return reversed(patches_to_pick)\n\n    def cherry_pick_commits(self, recreate_branch=True):\n        if recreate_branch:\n            # delete, create and checkout the maintenance branch based off of\n            # the previous tag\n            if self.branch in self.repo.branches:\n                logger.info(f\"Deleting branch {self.branch}\")\n                self.repo.git.branch('-D', self.branch)\n            logger.info(\n                f\"Creating branch {self.branch} from {self.base_branch} branch\"\n            )\n            self.repo.git.checkout(self.base_branch, b=self.branch)\n        else:\n            # just checkout the already existing maintenance branch\n            logger.info(f\"Checking out branch {self.branch}\")\n            self.repo.git.checkout(self.branch)\n\n        # cherry pick the commits based on the jira tickets\n        for commit in self.commits_to_pick():\n            logger.info(f\"Cherry-picking commit {commit.hexsha}\")\n            self.repo.git.cherry_pick(commit.hexsha)\n\n\nclass MajorRelease(Release):\n\n    @property\n    def branch(self):\n        return f\"maint-{self.version}\"\n\n    @property\n    def base_branch(self):\n        return self.default_branch\n\n    @cached_property\n    def siblings(self):\n        \"\"\"\n        Filter only the major releases.\n        \"\"\"\n        # handle minor releases before 1.0 as major releases\n        return [v for v in self.issue_tracker.project_versions()\n                if v.patch == 0 and (v.major == 0 or v.minor == 0)]\n\n\nclass MinorRelease(Release):\n\n    @property\n    def branch(self):\n        return f\"maint-{self.version.major}.x.x\"\n\n    @property\n    def base_branch(self):\n        return self.previous.tag\n\n    @cached_property\n    def siblings(self):\n        \"\"\"\n        Filter the major and minor releases.\n        \"\"\"\n        return [v for v in self.issue_tracker.project_versions()\n                if v.patch == 0]\n\n\nclass PatchRelease(Release):\n\n    @property\n    def branch(self):\n        return f\"maint-{self.version.major}.{self.version.minor}.x\"\n\n    @property\n    def base_branch(self):\n        return self.previous.tag\n\n    @cached_property\n    def siblings(self):\n        \"\"\"\n        No filtering, consider all releases.\n        \"\"\"\n        return self.issue_tracker.project_versions()\n", "dev/archery/archery/release/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom .core import Release, MajorRelease, MinorRelease, PatchRelease  # noqa\n", "dev/archery/archery/crossbow/reports.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport collections\nimport csv\nimport operator\nimport fnmatch\nimport functools\nimport time\n\nimport click\nimport requests\n\nfrom archery.utils.report import JinjaReport\n\n\n# TODO(kszucs): use archery.report.JinjaReport instead\nclass Report:\n\n    ROW_HEADERS = [\n        \"task_name\",\n        \"task_status\",\n        \"build_links\",\n        \"crossbow_branch_url\",\n        \"ci_system\",\n        \"extra_params\",\n        \"template\",\n        \"arrow_commit\",\n    ]\n\n    def __init__(self, job, task_filters=None, wait_for_task=None):\n        self.job = job\n\n        tasks = sorted(job.tasks.items())\n        if task_filters:\n            filtered = set()\n            for pattern in task_filters:\n                filtered |= set(fnmatch.filter(job.tasks.keys(), pattern))\n\n            tasks = [(name, task) for name, task in tasks if name in filtered]\n\n        self._tasks = dict(tasks)\n        self._wait_for_task = wait_for_task\n\n    @property\n    def repo_url(self):\n        url = self.job.queue.remote_url\n        return url[:-4] if url.endswith('.git') else url\n\n    def url(self, query):\n        return '{}/branches/all?query={}'.format(self.repo_url, query)\n\n    def branch_url(self, branch):\n        return '{}/tree/{}'.format(self.repo_url, branch)\n\n    def task_url(self, task):\n        build_links = task.status().build_links\n        # Only wait if the link to the actual build is not present\n        # and refresh task status.\n        if not build_links and self._wait_for_task:\n            time.sleep(self._wait_for_task)\n            build_links = task.status(force_query=True).build_links\n        if build_links:\n            # show link to the actual build, some CI providers implement\n            # the statuses API others implement the checks API, retrieve any.\n            return build_links[0]\n        else:\n            # show link to the branch if no status build link was found.\n            return self.branch_url(task.branch)\n\n    @property\n    @functools.lru_cache(maxsize=1)\n    def tasks_by_state(self):\n        tasks_by_state = collections.defaultdict(dict)\n        for task_name, task in self.job.tasks.items():\n            state = task.status().combined_state\n            tasks_by_state[state][task_name] = task\n        return tasks_by_state\n\n    @property\n    def contains_failures(self):\n        return any(self.tasks_by_state[state] for state in (\n            \"error\", \"failure\"))\n\n    @property\n    def tasks(self):\n        return self._tasks\n\n    def show(self):\n        raise NotImplementedError()\n\n    @property\n    def rows(self):\n        \"\"\"\n        Produces a generator that allow us to iterate over\n        the job tasks as a list of rows.\n        Row headers are defined at Report.ROW_HEADERS.\n        \"\"\"\n        for task_name, task in sorted(self.job.tasks.items()):\n            task_status = task.status()\n            row = [\n                task_name,\n                task_status.combined_state,\n                task_status.build_links,\n                self.branch_url(task.branch),\n                task.ci,\n                # We want this to be serialized as a dict instead\n                # of an orderedict.\n                {k: v for k, v in task.params.items()},\n                task.template,\n                # Arrow repository commit\n                self.job.target.head\n            ]\n            yield row\n\n\nclass ConsoleReport(Report):\n    \"\"\"Report the status of a Job to the console using click\"\"\"\n\n    # output table's header template\n    HEADER = '[{state:>7}] {branch:<52} {content:>16}'\n    DETAILS = ' \u2514 {url}'\n\n    # output table's row template for assets\n    ARTIFACT_NAME = '{artifact:>69} '\n    ARTIFACT_STATE = '[{state:>7}]'\n\n    # state color mapping to highlight console output\n    COLORS = {\n        # from CombinedStatus\n        'error': 'red',\n        'failure': 'red',\n        'pending': 'yellow',\n        'success': 'green',\n        # custom state messages\n        'ok': 'green',\n        'missing': 'red'\n    }\n\n    def lead(self, state, branch, n_uploaded, n_expected):\n        line = self.HEADER.format(\n            state=state.upper(),\n            branch=branch,\n            content='uploaded {} / {}'.format(n_uploaded, n_expected)\n        )\n        return click.style(line, fg=self.COLORS[state.lower()])\n\n    def header(self):\n        header = self.HEADER.format(\n            state='state',\n            branch='Task / Branch',\n            content='Artifacts'\n        )\n        delimiter = '-' * len(header)\n        return '{}\\n{}'.format(header, delimiter)\n\n    def artifact(self, state, pattern, asset):\n        if asset is None:\n            artifact = pattern\n            state = 'pending' if state == 'pending' else 'missing'\n        else:\n            artifact = asset.name\n            state = 'ok'\n\n        name_ = self.ARTIFACT_NAME.format(artifact=artifact)\n        state_ = click.style(\n            self.ARTIFACT_STATE.format(state=state.upper()),\n            self.COLORS[state]\n        )\n        return name_ + state_\n\n    def show(self, outstream, asset_callback=None, validate_patterns=True):\n        echo = functools.partial(click.echo, file=outstream)\n\n        # write table's header\n        echo(self.header())\n\n        # write table's body\n        for task_name, task in self.tasks.items():\n            # write summary of the uploaded vs total assets\n            status = task.status()\n            assets = task.assets(validate_patterns=validate_patterns)\n\n            # mapping of artifact pattern to asset or None of not uploaded\n            n_expected = len(task.artifacts)\n            n_uploaded = len(assets.uploaded_assets())\n            echo(self.lead(status.combined_state, task_name, n_uploaded,\n                           n_expected))\n\n            # show link to the actual build, some of the CI providers implement\n            # the statuses API others implement the checks API, so display both\n            for link in status.build_links:\n                echo(self.DETAILS.format(url=link))\n\n            # write per asset status\n            for artifact_pattern, asset in assets.items():\n                if asset_callback is not None:\n                    asset_callback(task_name, task, asset)\n                echo(self.artifact(status.combined_state, artifact_pattern,\n                                   asset))\n\n\nclass ChatReport(JinjaReport):\n    templates = {\n        'text': 'chat_nightly_report.txt.j2',\n    }\n    fields = [\n        'report',\n        'extra_message_success',\n        'extra_message_failure',\n    ]\n\n\nclass ReportUtils:\n\n    @classmethod\n    def send_message(cls, webhook, message):\n        resp = requests.post(webhook, json={\n            \"blocks\": [\n                {\n                    \"type\": \"section\",\n                    \"text\": {\n                            \"type\": \"mrkdwn\",\n                            \"text\": message\n                    }\n                }\n            ]\n        }\n        )\n        return resp\n\n    @classmethod\n    def send_email(cls, smtp_user, smtp_password, smtp_server, smtp_port,\n                   recipient_email, message):\n        import smtplib\n\n        server = smtplib.SMTP_SSL(smtp_server, smtp_port)\n        server.ehlo()\n        server.login(smtp_user, smtp_password)\n        server.sendmail(smtp_user, recipient_email, message)\n        server.close()\n\n    @classmethod\n    def write_csv(cls, report, add_headers=True):\n        with open(f'{report.job.branch}.csv', 'w') as csvfile:\n            task_writer = csv.writer(csvfile)\n            if add_headers:\n                task_writer.writerow(report.ROW_HEADERS)\n            task_writer.writerows(report.rows)\n\n\nclass EmailReport(JinjaReport):\n    templates = {\n        'nightly_report': 'email_nightly_report.txt.j2',\n        'token_expiration': 'email_token_expiration.txt.j2',\n    }\n    fields = [\n        'report',\n        'sender_name',\n        'sender_email',\n        'recipient_email',\n    ]\n\n\nclass CommentReport(Report):\n\n    _markdown_badge = '[![{title}]({badge})]({{url}})'\n\n    badges = {\n        'github': _markdown_badge.format(\n            title='GitHub Actions',\n            badge=(\n                'https://github.com/{repo}/actions/workflows/crossbow.yml/'\n                'badge.svg?branch={branch}'\n            ),\n        ),\n        'azure': _markdown_badge.format(\n            title='Azure',\n            badge=(\n                'https://dev.azure.com/{repo}/_apis/build/status/'\n                '{repo_dotted}?branchName={branch}'\n            )\n        ),\n        'travis': _markdown_badge.format(\n            title='Travis CI',\n            badge='https://img.shields.io/travis/{repo}/{branch}.svg'\n        ),\n        'circle': _markdown_badge.format(\n            title='CircleCI',\n            badge=(\n                'https://img.shields.io/circleci/build/github'\n                '/{repo}/{branch}.svg'\n            )\n        ),\n        'appveyor': _markdown_badge.format(\n            title='AppVeyor',\n            badge='https://img.shields.io/appveyor/ci/{repo}/{branch}.svg'\n        ),\n        'drone': _markdown_badge.format(\n            title='Drone',\n            badge='https://img.shields.io/drone/build/{repo}/{branch}.svg'\n        ),\n    }\n\n    def __init__(self, job, crossbow_repo, wait_for_task=None):\n        self.crossbow_repo = crossbow_repo\n        super().__init__(job, wait_for_task=wait_for_task)\n\n    def show(self):\n        url = 'https://github.com/{repo}/branches/all?query={branch}'\n        sha = self.job.target.head\n\n        msg = 'Revision: {}\\n\\n'.format(sha)\n        msg += 'Submitted crossbow builds: [{repo} @ {branch}]'\n        msg += '({})\\n'.format(url)\n        msg += '\\n|Task|Status|\\n|----|------|'\n\n        tasks = sorted(self.job.tasks.items(), key=operator.itemgetter(0))\n        for key, task in tasks:\n            branch = task.branch\n\n            try:\n                template = self.badges[task.ci]\n                badge = template.format(\n                    repo=self.crossbow_repo,\n                    repo_dotted=self.crossbow_repo.replace('/', '.'),\n                    branch=branch,\n                    url=self.task_url(task)\n                )\n            except KeyError:\n                badge = 'unsupported CI service `{}`'.format(task.ci)\n\n            msg += '\\n|{}|{}|'.format(key, badge)\n\n        return msg.format(repo=self.crossbow_repo, branch=self.job.branch)\n", "dev/archery/archery/crossbow/cli.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom datetime import date\nfrom pathlib import Path\nimport time\nimport sys\n\nimport click\n\nfrom .core import Config, Repo, Queue, Target, Job, CrossbowError\nfrom .reports import (ChatReport, Report, ReportUtils, ConsoleReport,\n                      EmailReport, CommentReport)\nfrom ..utils.source import ArrowSources\n\n\n_default_arrow_path = ArrowSources.find().path\n_default_queue_path = _default_arrow_path.parent / \"crossbow\"\n_default_config_path = _default_arrow_path / \"dev\" / \"tasks\" / \"tasks.yml\"\n\n\n@click.group()\n@click.option('--github-token', '-t', default=None,\n              envvar=\"CROSSBOW_GITHUB_TOKEN\",\n              help='OAuth token for GitHub authentication')\n@click.option('--arrow-path', '-a',\n              type=click.Path(), default=_default_arrow_path,\n              help='Arrow\\'s repository path. Defaults to the repository of '\n                   'this script')\n@click.option('--queue-path', '-q',\n              envvar=\"CROSSBOW_QUEUE_PATH\",\n              type=click.Path(), default=_default_queue_path,\n              help='The repository path used for scheduling the tasks. '\n                   'Defaults to crossbow directory placed next to arrow')\n@click.option('--queue-remote', '-qr', default=None,\n              help='Force to use this remote URL for the Queue repository')\n@click.option('--output-file', metavar='<output>',\n              type=click.File('w', encoding='utf8'), default='-',\n              help='Capture output result into file.')\n@click.pass_context\ndef crossbow(ctx, github_token, arrow_path, queue_path, queue_remote,\n             output_file):\n    \"\"\"\n    Schedule packaging tasks or nightly builds on CI services.\n    \"\"\"\n    ctx.ensure_object(dict)\n    ctx.obj['output'] = output_file\n    ctx.obj['arrow'] = Repo(arrow_path)\n    ctx.obj['queue'] = Queue(queue_path, remote_url=queue_remote,\n                             github_token=github_token, require_https=True)\n\n\n@crossbow.command()\n@click.option('--config-path', '-c',\n              type=click.Path(exists=True), default=_default_config_path,\n              help='Task configuration yml. Defaults to tasks.yml')\n@click.pass_obj\ndef check_config(obj, config_path):\n    # load available tasks configuration and groups from yaml\n    config = Config.load_yaml(config_path)\n    config.validate()\n\n    output = obj['output']\n    config.show(output)\n\n\n@crossbow.command()\n@click.argument('tasks', nargs=-1, required=False)\n@click.option('--group', '-g', 'groups', multiple=True,\n              help='Submit task groups as defined in task.yml')\n@click.option('--param', '-p', 'params', multiple=True,\n              help='Additional task parameters for rendering the CI templates')\n@click.option('--job-prefix', default='build',\n              help='Arbitrary prefix for branch names, e.g. nightly')\n@click.option('--config-path', '-c',\n              type=click.Path(exists=True), default=_default_config_path,\n              help='Task configuration yml. Defaults to tasks.yml')\n@click.option('--arrow-version', '-v', default=None,\n              help='Set target version explicitly.')\n@click.option('--arrow-remote', '-r', default=None,\n              help='Set GitHub remote explicitly, which is going to be cloned '\n                   'on the CI services. Note, that no validation happens '\n                   'locally. Examples: https://github.com/apache/arrow or '\n                   'https://github.com/kszucs/arrow.')\n@click.option('--arrow-branch', '-b', default=None,\n              help='Give the branch name explicitly, e.g. ARROW-1949.')\n@click.option('--arrow-sha', '-t', default=None,\n              help='Set commit SHA or Tag name explicitly, e.g. f67a515, '\n                   'apache-arrow-0.11.1.')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.option('--dry-run/--commit', default=False,\n              help='Just display the rendered CI configurations without '\n                   'committing them')\n@click.option('--no-push/--push', default=False,\n              help='Don\\'t push the changes')\n@click.pass_obj\ndef submit(obj, tasks, groups, params, job_prefix, config_path, arrow_version,\n           arrow_remote, arrow_branch, arrow_sha, fetch, dry_run, no_push):\n    output = obj['output']\n    queue, arrow = obj['queue'], obj['arrow']\n\n    # load available tasks configuration and groups from yaml\n    config = Config.load_yaml(config_path)\n    try:\n        config.validate()\n    except CrossbowError as e:\n        raise click.ClickException(str(e))\n\n    # Override the detected repo url / remote, branch and sha - this aims to\n    # make release procedure a bit simpler.\n    # Note, that the target revision's crossbow templates must be\n    # compatible with the locally checked out version of crossbow (which is\n    # in case of the release procedure), because the templates still\n    # contain some business logic (dependency installation, deployments)\n    # which will be reduced to a single command in the future.\n    target = Target.from_repo(arrow, remote=arrow_remote, branch=arrow_branch,\n                              head=arrow_sha, version=arrow_version)\n\n    # parse additional job parameters\n    params = dict([p.split(\"=\") for p in params])\n\n    # instantiate the job object\n    try:\n        job = Job.from_config(config=config, target=target, tasks=tasks,\n                              groups=groups, params=params)\n    except CrossbowError as e:\n        raise click.ClickException(str(e))\n\n    job.show(output)\n    if dry_run:\n        return\n\n    if fetch:\n        queue.fetch()\n    queue.put(job, prefix=job_prefix)\n\n    if no_push:\n        click.echo('Branches and commits created but not pushed: `{}`'\n                   .format(job.branch))\n    else:\n        queue.push()\n        click.echo('Pushed job identifier is: `{}`'.format(job.branch))\n\n\n@crossbow.command()\n@click.option('--base-branch', default=None,\n              help='Set base branch for the PR.')\n@click.option('--create-pr', is_flag=True, default=False,\n              help='Create GitHub Pull Request')\n@click.option('--head-branch', default=None,\n              help='Give the branch name explicitly, e.g. release-9.0.0-rc0')\n@click.option('--pr-body', default=None,\n              help='Set body for the PR.')\n@click.option('--pr-title', default=None,\n              help='Set title for the PR.')\n@click.option('--remote', default=None,\n              help='Set GitHub remote explicitly, which is going to be used '\n                   'for the PR. Note, that no validation happens '\n                   'locally. Examples: https://github.com/apache/arrow or '\n                   'https://github.com/raulcd/arrow.')\n@click.option('--rc', default=None,\n              help='Release Candidate number.')\n@click.option('--version', default=None,\n              help='Release version.')\n@click.option('--verify-binaries', is_flag=True, default=False,\n              help='Trigger the verify binaries jobs')\n@click.option('--verify-source', is_flag=True, default=False,\n              help='Trigger the verify source jobs')\n@click.option('--verify-wheels', is_flag=True, default=False,\n              help='Trigger the verify wheels jobs')\n@click.pass_obj\ndef verify_release_candidate(obj, base_branch, create_pr,\n                             head_branch, pr_body, pr_title, remote,\n                             rc, version, verify_binaries, verify_source,\n                             verify_wheels):\n    # The verify-release-candidate command will create a PR (or find one)\n    # and add the verify-rc* comment to trigger the verify tasks\n\n    # Redefine Arrow repo to use the correct arrow remote.\n    arrow = Repo(path=obj['arrow'].path, remote_url=remote)\n\n    response = arrow.github_pr(title=pr_title, head=head_branch,\n                               base=base_branch, body=pr_body,\n                               github_token=obj['queue'].github_token,\n                               create=create_pr)\n\n    # If we want to trigger any verification job we add a comment to the PR.\n    verify_flags = [verify_source, verify_binaries, verify_wheels]\n    if any(verify_flags):\n        command = \"@github-actions crossbow submit\"\n        verify_groups = [\"verify-rc-source\",\n                         \"verify-rc-binaries\", \"verify-rc-wheels\"]\n        job_groups = \"\"\n        for flag, group in zip(verify_flags, verify_groups):\n            if flag:\n                job_groups += f\" --group {group}\"\n        response.create_comment(\n            f\"{command} {job_groups} --param \" +\n            f\"release={version} --param rc={rc}\")\n\n\n@crossbow.command()\n@click.argument('task', required=True)\n@click.option('--config-path', '-c',\n              type=click.Path(exists=True), default=_default_config_path,\n              help='Task configuration yml. Defaults to tasks.yml')\n@click.option('--arrow-version', '-v', default=None,\n              help='Set target version explicitly.')\n@click.option('--arrow-remote', '-r', default=None,\n              help='Set GitHub remote explicitly, which is going to be cloned '\n                   'on the CI services. Note, that no validation happens '\n                   'locally. Examples: https://github.com/apache/arrow or '\n                   'https://github.com/kszucs/arrow.')\n@click.option('--arrow-branch', '-b', default=None,\n              help='Give the branch name explicitly, e.g. ARROW-1949.')\n@click.option('--arrow-sha', '-t', default=None,\n              help='Set commit SHA or Tag name explicitly, e.g. f67a515, '\n                   'apache-arrow-0.11.1.')\n@click.option('--param', '-p', 'params', multiple=True,\n              help='Additional task parameters for rendering the CI templates')\n@click.pass_obj\ndef render(obj, task, config_path, arrow_version, arrow_remote, arrow_branch,\n           arrow_sha, params):\n    \"\"\"\n    Utility command to check the rendered CI templates.\n    \"\"\"\n    from .core import _flatten\n\n    def highlight(code):\n        try:\n            from pygments import highlight\n            from pygments.lexers import YamlLexer\n            from pygments.formatters import TerminalFormatter\n            return highlight(code, YamlLexer(), TerminalFormatter())\n        except ImportError:\n            return code\n\n    arrow = obj['arrow']\n\n    target = Target.from_repo(arrow, remote=arrow_remote, branch=arrow_branch,\n                              head=arrow_sha, version=arrow_version)\n    config = Config.load_yaml(config_path)\n    params = dict([p.split(\"=\") for p in params])\n    params[\"queue_remote_url\"] = \"https://github.com/org/crossbow\"\n    job = Job.from_config(config=config, target=target, tasks=[task],\n                          params=params)\n\n    for task_name, rendered_files in job.render_tasks().items():\n        for path, content in _flatten(rendered_files).items():\n            click.echo('#' * 80)\n            click.echo('### {:^72} ###'.format(\"/\".join(path)))\n            click.echo('#' * 80)\n            click.echo(highlight(content))\n\n\n@crossbow.command()\n@click.argument('job-name', required=True)\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.option('--task-filter', '-f', 'task_filters', multiple=True,\n              help='Glob pattern for filtering relevant tasks')\n@click.option('--validate/--no-validate', default=False,\n              help='Return non-zero exit code '\n                   'if there is any non-success task')\n@click.pass_obj\ndef status(obj, job_name, fetch, task_filters, validate):\n    output = obj['output']\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n    job = queue.get(job_name)\n\n    success = True\n\n    def asset_callback(task_name, task, asset):\n        nonlocal success\n        if task.status().combined_state in {'error', 'failure'}:\n            success = False\n        if asset is None:\n            success = False\n\n    report = ConsoleReport(job, task_filters=task_filters)\n    report.show(output, asset_callback=asset_callback)\n    if validate and not success:\n        sys.exit(1)\n\n\n@crossbow.command()\n@click.option('--arrow-remote', '-r', default=None,\n              help='Set GitHub remote explicitly, which is going to be cloned '\n                   'on the CI services. Note, that no validation happens '\n                   'locally. Examples: \"https://github.com/apache/arrow\" or '\n                   '\"raulcd/arrow\".')\n@click.option('--crossbow', '-c', default='ursacomputing/crossbow',\n              help='Crossbow repository on github to use')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.option('--job-name', required=True)\n@click.option('--pr-title', required=True,\n              help='Track the job submitted on PR with given title')\n@click.pass_obj\ndef report_pr(obj, arrow_remote, crossbow, fetch, job_name, pr_title):\n    arrow = obj['arrow']\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n    job = queue.get(job_name)\n\n    report = CommentReport(job, crossbow_repo=crossbow)\n    target_arrow = Repo(path=arrow.path, remote_url=arrow_remote)\n    pull_request = target_arrow.github_pr(title=pr_title,\n                                          github_token=queue.github_token,\n                                          create=False)\n    # render the response comment's content on the PR\n    pull_request.create_comment(report.show())\n    click.echo(f'Job is tracked on PR {pull_request.html_url}')\n\n\n@crossbow.command()\n@click.argument('prefix', required=True)\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.pass_obj\ndef latest_prefix(obj, prefix, fetch):\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n    latest = queue.latest_for_prefix(prefix)\n    click.echo(latest.branch)\n\n\n@crossbow.command()\n@click.argument('job-name', required=True)\n@click.option('--sender-name', '-n',\n              help='Name to use for report e-mail.')\n@click.option('--sender-email', '-e',\n              help='E-mail to use for report e-mail.')\n@click.option('--recipient-email', '-r',\n              help='Where to send the e-mail report')\n@click.option('--smtp-user', '-u',\n              help='E-mail address to use for SMTP login')\n@click.option('--smtp-password', '-P',\n              help='SMTP password to use for report e-mail.')\n@click.option('--smtp-server', '-s', default='smtp.gmail.com',\n              help='SMTP server to use for report e-mail.')\n@click.option('--smtp-port', '-p', default=465,\n              help='SMTP port to use for report e-mail.')\n@click.option('--poll/--no-poll', default=False,\n              help='Wait for completion if there are tasks pending')\n@click.option('--poll-max-minutes', default=180,\n              help='Maximum amount of time waiting for job completion')\n@click.option('--poll-interval-minutes', default=10,\n              help='Number of minutes to wait to check job status again')\n@click.option('--send/--dry-run', default=False,\n              help='Just display the report, don\\'t send it')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.pass_obj\ndef report(obj, job_name, sender_name, sender_email, recipient_email,\n           smtp_user, smtp_password, smtp_server, smtp_port, poll,\n           poll_max_minutes, poll_interval_minutes, send, fetch):\n    \"\"\"\n    Send an e-mail report showing success/failure of tasks in a Crossbow run\n    \"\"\"\n    output = obj['output']\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n\n    job = queue.get(job_name)\n    email_report = EmailReport(\n        report=Report(job),\n        sender_name=sender_name,\n        sender_email=sender_email,\n        recipient_email=recipient_email\n    )\n\n    if poll:\n        job.wait_until_finished(\n            poll_max_minutes=poll_max_minutes,\n            poll_interval_minutes=poll_interval_minutes\n        )\n\n    if send:\n        ReportUtils.send_email(\n            smtp_user=smtp_user,\n            smtp_password=smtp_password,\n            smtp_server=smtp_server,\n            smtp_port=smtp_port,\n            recipient_email=recipient_email,\n            message=email_report.render(\"nightly_report\")\n        )\n    else:\n        output.write(email_report.render(\"nightly_report\"))\n\n\n@crossbow.command()\n@click.argument('job-name', required=True)\n@click.option('--send/--dry-run', default=False,\n              help='Just display the report, don\\'t send it')\n@click.option('--webhook', '-w',\n              help='Zulip/Slack Webhook address to send the report to')\n@click.option('--extra-message-success', '-s', default=None,\n              help='Extra message, will be appended if no failures.')\n@click.option('--extra-message-failure', '-f', default=None,\n              help='Extra message, will be appended if there are failures.')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.pass_obj\ndef report_chat(obj, job_name, send, webhook, extra_message_success,\n                extra_message_failure, fetch):\n    \"\"\"\n    Send a chat report to a webhook showing success/failure\n    of tasks in a Crossbow run.\n    \"\"\"\n    output = obj['output']\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n\n    job = queue.get(job_name)\n    report_chat = ChatReport(report=Report(job),\n                             extra_message_success=extra_message_success,\n                             extra_message_failure=extra_message_failure)\n    if send:\n        ReportUtils.send_message(webhook, report_chat.render(\"text\"))\n    else:\n        output.write(report_chat.render(\"text\"))\n\n\n@crossbow.command()\n@click.argument('job-name', required=True)\n@click.option('--save/--dry-run', default=False,\n              help='Just display the report, don\\'t save it')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.pass_obj\ndef report_csv(obj, job_name, save, fetch):\n    \"\"\"\n    Generates a CSV report with the different tasks information\n    from a Crossbow run.\n    \"\"\"\n    output = obj['output']\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n\n    job = queue.get(job_name)\n    report = Report(job)\n    if save:\n        ReportUtils.write_csv(report)\n    else:\n        output.write(\"\\n\".join([str(row) for row in report.rows]))\n\n\n@crossbow.command()\n@click.argument('job-name', required=True)\n@click.option('-t', '--target-dir',\n              default=_default_arrow_path / 'packages',\n              type=click.Path(file_okay=False, dir_okay=True),\n              help='Directory to download the build artifacts')\n@click.option('--dry-run/--execute', default=False,\n              help='Just display process, don\\'t download anything')\n@click.option('--fetch/--no-fetch', default=True,\n              help='Fetch references (branches and tags) from the remote')\n@click.option('--task-filter', '-f', 'task_filters', multiple=True,\n              help='Glob pattern for filtering relevant tasks')\n@click.option('--validate-patterns/--skip-pattern-validation', default=True,\n              help='Whether to validate artifact name patterns or not')\n@click.pass_obj\ndef download_artifacts(obj, job_name, target_dir, dry_run, fetch,\n                       validate_patterns, task_filters):\n    \"\"\"Download build artifacts from GitHub releases\"\"\"\n    output = obj['output']\n\n    # fetch the queue repository\n    queue = obj['queue']\n    if fetch:\n        queue.fetch()\n\n    # query the job's artifacts\n    job = queue.get(job_name)\n\n    # create directory to download the assets to\n    target_dir = Path(target_dir).absolute() / job_name\n    target_dir.mkdir(parents=True, exist_ok=True)\n\n    # download the assets while showing the job status\n    def asset_callback(task_name, task, asset):\n        if asset is not None:\n            path = target_dir / task_name / asset.name\n            path.parent.mkdir(exist_ok=True)\n\n            def need_download():\n                if dry_run:\n                    return False\n                if not path.exists():\n                    return True\n                if path.stat().st_size != asset.size:\n                    return True\n                return False\n\n            if need_download():\n                import github3\n                max_n_retries = 5\n                n_retries = 0\n                while True:\n                    try:\n                        asset.download(path)\n                    except github3.exceptions.GitHubException as error:\n                        n_retries += 1\n                        if n_retries == max_n_retries:\n                            raise\n                        wait_seconds = 60\n                        click.echo(f'Failed to download {path}')\n                        click.echo(f'Retry #{n_retries} after {wait_seconds}s')\n                        click.echo(error)\n                        time.sleep(wait_seconds)\n                    else:\n                        break\n\n    click.echo('Downloading {}\\'s artifacts.'.format(job_name))\n    click.echo('Destination directory is {}'.format(target_dir))\n    click.echo()\n\n    report = ConsoleReport(job, task_filters=task_filters)\n    report.show(\n        output,\n        asset_callback=asset_callback,\n        validate_patterns=validate_patterns\n    )\n\n\n@crossbow.command()\n@click.argument('patterns', nargs=-1, required=True)\n@click.option('--sha', required=True, help='Target committish')\n@click.option('--tag', required=True, help='Target tag')\n@click.option('--method', default='curl', help='Use cURL to upload')\n@click.pass_obj\ndef upload_artifacts(obj, tag, sha, patterns, method):\n    queue = obj['queue']\n    queue.github_overwrite_release_assets(\n        tag_name=tag, target_commitish=sha, method=method, patterns=patterns\n    )\n\n\n@crossbow.command()\n@click.option('--dry-run/--execute', default=False,\n              help='Just display process, don\\'t download anything')\n@click.option('--days', default=90,\n              help='Branches older than this amount of days will be deleted')\n@click.option('--maximum', default=1000,\n              help='Maximum limit of branches to delete for a single run')\n@click.pass_obj\ndef delete_old_branches(obj, dry_run, days, maximum):\n    \"\"\"\n    Deletes branches on queue repository (crossbow) that are older than number\n    of days.\n    With a maximum number of branches to be deleted. This is required to avoid\n    triggering GitHub protection limits.\n    \"\"\"\n    queue = obj['queue']\n    ts = time.time() - days * 24 * 3600\n    refs = []\n    for ref in queue.repo.listall_reference_objects():\n        commit = ref.peel()\n        if commit.commit_time < ts and not ref.name.startswith(\n                \"refs/remotes/origin/pr/\"):\n            # Check if reference is a remote reference to point\n            # to the remote head.\n            ref_name = ref.name\n            if ref_name.startswith(\"refs/remotes/origin\"):\n                ref_name = ref_name.replace(\"remotes/origin\", \"heads\")\n            refs.append(f\":{ref_name}\")\n\n    def batch_gen(iterable, step):\n        total_length = len(iterable)\n        to_delete = min(total_length, maximum)\n        print(f\"Total number of references to be deleted: {to_delete}\")\n        for index in range(0, to_delete, step):\n            yield iterable[index:min(index + step, to_delete)]\n\n    for batch in batch_gen(refs, 50):\n        if not dry_run:\n            queue.push(batch)\n        else:\n            print(batch)\n\n\n@crossbow.command()\n@click.option('--days', default=30,\n              help='Notification will be sent if expiration date is '\n                   'closer than the number of days.')\n@click.option('--sender-name', '-n',\n              help='Name to use for report e-mail.')\n@click.option('--sender-email', '-e',\n              help='E-mail to use for report e-mail.')\n@click.option('--recipient-email', '-r',\n              help='Where to send the e-mail report')\n@click.option('--smtp-user', '-u',\n              help='E-mail address to use for SMTP login')\n@click.option('--smtp-password', '-P',\n              help='SMTP password to use for report e-mail.')\n@click.option('--smtp-server', '-s', default='smtp.gmail.com',\n              help='SMTP server to use for report e-mail.')\n@click.option('--smtp-port', '-p', default=465,\n              help='SMTP port to use for report e-mail.')\n@click.option('--send/--dry-run', default=False,\n              help='Just display the report, don\\'t send it')\n@click.pass_obj\ndef notify_token_expiration(obj, days, sender_name, sender_email,\n                            recipient_email, smtp_user, smtp_password,\n                            smtp_server, smtp_port, send):\n    \"\"\"\n    Check if token is close to expiration and send email notifying.\n    \"\"\"\n    output = obj['output']\n    queue = obj['queue']\n\n    token_expiration_date = queue.token_expiration_date()\n    days_left = 0\n    if token_expiration_date:\n        days_left = (token_expiration_date - date.today()).days\n        if days_left > days:\n            output.write(\"Notification not sent. \" +\n                         f\"Token will expire in {days_left} days.\")\n            return\n\n    class TokenExpirationReport:\n        def __init__(self, token_expiration_date, days_left):\n            self.token_expiration_date = token_expiration_date\n            self.days_left = days_left\n\n    email_report = EmailReport(\n        report=TokenExpirationReport(\n            token_expiration_date or \"ALREADY_EXPIRED\", days_left),\n        sender_name=sender_name,\n        sender_email=sender_email,\n        recipient_email=recipient_email\n    )\n\n    message = email_report.render(\"token_expiration\").strip()\n    if send:\n        ReportUtils.send_email(\n            smtp_user=smtp_user,\n            smtp_password=smtp_password,\n            smtp_server=smtp_server,\n            smtp_port=smtp_port,\n            recipient_email=recipient_email,\n            message=message\n        )\n    else:\n        output.write(message)\n", "dev/archery/archery/crossbow/core.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport re\nimport fnmatch\nimport glob\nimport time\nimport logging\nimport mimetypes\nimport subprocess\nimport textwrap\nimport uuid\nfrom io import StringIO\nfrom pathlib import Path\nfrom datetime import date\nimport warnings\n\nimport jinja2\nfrom ruamel.yaml import YAML\n\ntry:\n    import github3\n    _have_github3 = True\nexcept ImportError:\n    github3 = object\n    _have_github3 = False\n\ntry:\n    import pygit2\nexcept ImportError:\n    PygitRemoteCallbacks = object\n    GitError = Exception\nelse:\n    PygitRemoteCallbacks = pygit2.RemoteCallbacks\n    GitError = pygit2.GitError\n\nfrom ..utils.source import ArrowSources\n\n\nfor pkg in [\"requests\", \"urllib3\", \"github3\"]:\n    logging.getLogger(pkg).setLevel(logging.WARNING)\n\nlogger = logging.getLogger(\"crossbow\")\n\n\nclass CrossbowError(Exception):\n    pass\n\n\ndef _flatten(mapping):\n    \"\"\"Converts a hierarchical mapping to a flat dictionary\"\"\"\n    result = {}\n    for k, v in mapping.items():\n        if isinstance(v, dict):\n            for ik, iv in _flatten(v).items():\n                ik = ik if isinstance(ik, tuple) else (ik,)\n                result[(k,) + ik] = iv\n        elif isinstance(v, list):\n            for ik, iv in enumerate(_flatten(v)):\n                ik = ik if isinstance(ik, tuple) else (ik,)\n                result[(k,) + ik] = iv\n        else:\n            result[(k,)] = v\n    return result\n\n\ndef _unflatten(mapping):\n    \"\"\"Converts a flat tuple => object mapping to hierarchical one\"\"\"\n    result = {}\n    for path, value in mapping.items():\n        parents, leaf = path[:-1], path[-1]\n        # create the hierarchy until we reach the leaf value\n        temp = result\n        for parent in parents:\n            temp.setdefault(parent, {})\n            temp = temp[parent]\n        # set the leaf value\n        temp[leaf] = value\n\n    return result\n\n\ndef _unflatten_tree(files):\n    \"\"\"Converts a flat path => object mapping to a hierarchical directories\n\n    Input:\n        {\n            'path/to/file.a': a_content,\n            'path/to/file.b': b_content,\n            'path/file.c': c_content\n        }\n    Output:\n        {\n            'path': {\n                'to': {\n                    'file.a': a_content,\n                    'file.b': b_content\n                },\n                'file.c': c_content\n            }\n        }\n    \"\"\"\n    files = {tuple(k.split('/')): v for k, v in files.items()}\n    return _unflatten(files)\n\n\ndef _render_jinja_template(searchpath, template, params):\n    def format_all(items, pattern):\n        return [pattern.format(item) for item in items]\n\n    loader = jinja2.FileSystemLoader(searchpath)\n    env = jinja2.Environment(loader=loader, trim_blocks=True,\n                             lstrip_blocks=True,\n                             undefined=jinja2.StrictUndefined)\n    env.filters['format_all'] = format_all\n    template = env.get_template(template)\n    return template.render(**params)\n\n\n# configurations for setting up branch skipping\n# - appveyor has a feature to skip builds without an appveyor.yml\n# - travis reads from the default branch and applies the rules\n# - circle requires the configuration to be present on all branch, even ones\n#   that are configured to be skipped\n# - azure skips branches without azure-pipelines.yml by default\n# - github skips branches without .github/workflows/ by default\n\n_default_travis_yml = \"\"\"\nbranches:\n  only:\n    - master\n    - /.*-travis-.*/\n\nos: linux\ndist: trusty\nlanguage: generic\n\"\"\"\n\n_default_circle_yml = \"\"\"\nversion: 2\n\njobs:\n  build:\n    machine: true\n\nworkflows:\n  version: 2\n  build:\n    jobs:\n      - build:\n          filters:\n            branches:\n              only:\n                - /.*-circle-.*/\n\"\"\"\n\n_default_tree = {\n    '.travis.yml': _default_travis_yml,\n    '.circleci/config.yml': _default_circle_yml\n}\n\n\nclass GitRemoteCallbacks(PygitRemoteCallbacks):\n\n    def __init__(self, token):\n        self.token = token\n        self.attempts = 0\n        super().__init__()\n\n    def push_update_reference(self, refname, message):\n        pass\n\n    def update_tips(self, refname, old, new):\n        pass\n\n    def credentials(self, url, username_from_url, allowed_types):\n        # its a libgit2 bug, that it infinitely retries the authentication\n        self.attempts += 1\n\n        if self.attempts >= 5:\n            # pygit2 doesn't propagate the exception properly\n            msg = 'Wrong oauth personal access token'\n            print(msg)\n            raise CrossbowError(msg)\n\n        if (allowed_types &\n                pygit2.credentials.CredentialType.USERPASS_PLAINTEXT):\n            return pygit2.UserPass('x-oauth-basic', self.token)\n        else:\n            return None\n\n\ndef _git_ssh_to_https(url):\n    return url.replace('git@github.com:', 'https://github.com/')\n\n\ndef _parse_github_user_repo(remote_url):\n    # TODO: use a proper URL parser instead?\n    m = re.match(r'.*\\/([^\\/]+)\\/([^\\/\\.]+)(\\.git|/)?$', remote_url)\n    if m is None:\n        # Perhaps it's simply \"username/reponame\"?\n        m = re.match(r'^(\\w+)/(\\w+)$', remote_url)\n        if m is None:\n            raise CrossbowError(\n                f\"Unable to parse the github owner and repository from the \"\n                f\"repository's remote url {remote_url!r}\"\n            )\n    user, repo = m.group(1), m.group(2)\n    return user, repo\n\n\nclass Repo:\n    \"\"\"\n    Base class for interaction with local git repositories\n\n    A high level wrapper used for both reading revision information from\n    arrow's repository and pushing continuous integration tasks to the queue\n    repository.\n\n    Parameters\n    ----------\n    require_https : boolean, default False\n        Raise exception for SSH origin URLs\n    \"\"\"\n\n    def __init__(self, path, github_token=None, remote_url=None,\n                 require_https=False):\n        self.path = Path(path)\n        self.github_token = github_token\n        self.require_https = require_https\n        self._remote_url = remote_url\n        self._pygit_repo = None\n        self._github_repo = None  # set by as_github_repo()\n        self._updated_refs = []\n\n    def __str__(self):\n        tpl = textwrap.dedent('''\n            Repo: {remote}@{branch}\n            Commit: {head}\n        ''')\n        return tpl.format(\n            remote=self.remote_url,\n            branch=self.branch.branch_name,\n            head=self.head\n        )\n\n    @property\n    def repo(self):\n        if self._pygit_repo is None:\n            self._pygit_repo = pygit2.Repository(str(self.path))\n        return self._pygit_repo\n\n    @property\n    def origin(self):\n        remote = self.repo.remotes['origin']\n        if self.require_https and remote.url.startswith('git@github.com'):\n            raise CrossbowError(\"Change SSH origin URL to HTTPS to use \"\n                                \"Crossbow: {}\".format(remote.url))\n        return remote\n\n    def fetch(self, retry=3):\n        refspec = '+refs/heads/*:refs/remotes/origin/*'\n        attempt = 1\n        while True:\n            try:\n                self.origin.fetch([refspec])\n                break\n            except GitError as e:\n                if retry and attempt < retry:\n                    attempt += 1\n                else:\n                    raise e\n\n    def push(self, refs=None, github_token=None):\n        github_token = github_token or self.github_token\n        if github_token is None:\n            raise RuntimeError(\n                'Could not determine GitHub token. Please set the '\n                'CROSSBOW_GITHUB_TOKEN environment variable to a '\n                'valid GitHub access token or pass one to --github-token.'\n            )\n        callbacks = GitRemoteCallbacks(github_token)\n        refs = refs or []\n        try:\n            self.origin.push(refs + self._updated_refs, callbacks=callbacks)\n        except pygit2.GitError:\n            raise RuntimeError('Failed to push updated references, '\n                               'potentially because of credential issues: {}'\n                               .format(self._updated_refs))\n        else:\n            self.updated_refs = []\n\n    @property\n    def head(self):\n        \"\"\"Currently checked out commit's sha\"\"\"\n        return self.repo.head\n\n    @property\n    def branch(self):\n        \"\"\"Currently checked out branch\"\"\"\n        try:\n            return self.repo.branches[self.repo.head.shorthand]\n        except KeyError:\n            raise CrossbowError(\n                'Cannot determine the current branch of the Arrow repository '\n                'to clone or push to, perhaps it is in detached HEAD state. '\n                'Please checkout a branch.'\n            )\n\n    @property\n    def remote(self):\n        \"\"\"Currently checked out branch's remote counterpart\"\"\"\n        try:\n            return self.repo.remotes[self.branch.upstream.remote_name]\n        except (AttributeError, KeyError):\n            raise CrossbowError(\n                'Cannot determine git remote for the Arrow repository to '\n                'clone or push to, try to push the `{}` branch first to have '\n                'a remote tracking counterpart.'.format(self.branch.name)\n            )\n\n    @property\n    def remote_url(self):\n        \"\"\"Currently checked out branch's remote counterpart URL\n\n        If an SSH github url is set, it will be replaced by the https\n        equivalent usable with GitHub OAuth token.\n        \"\"\"\n        return self._remote_url or _git_ssh_to_https(self.remote.url)\n\n    @property\n    def user_name(self):\n        try:\n            return next(self.repo.config.get_multivar('user.name'))\n        except StopIteration:\n            return os.environ.get('GIT_COMMITTER_NAME', 'unknown')\n\n    @property\n    def user_email(self):\n        try:\n            return next(self.repo.config.get_multivar('user.email'))\n        except StopIteration:\n            return os.environ.get('GIT_COMMITTER_EMAIL', 'unknown')\n\n    @property\n    def signature(self):\n        return pygit2.Signature(self.user_name, self.user_email,\n                                int(time.time()))\n\n    @property\n    def default_branch_name(self):\n        default_branch_name = os.getenv(\"ARCHERY_DEFAULT_BRANCH\")\n\n        if default_branch_name is None:\n            try:\n                ref_obj = self.repo.references[\"refs/remotes/origin/HEAD\"]\n                target_name = ref_obj.target\n                target_name_tokenized = target_name.split(\"/\")\n                default_branch_name = target_name_tokenized[-1]\n            except KeyError:\n                default_branch_name = \"main\"\n                warnings.warn('Unable to determine default branch name: '\n                              'ARCHERY_DEFAULT_BRANCH environment variable is '\n                              'not set. Git repository does not contain a '\n                              '\\'refs/remotes/origin/HEAD\\'reference. Setting '\n                              'the default branch name to ' +\n                              default_branch_name, RuntimeWarning)\n\n        return default_branch_name\n\n    def create_tree(self, files):\n        builder = self.repo.TreeBuilder()\n\n        for filename, content in files.items():\n            if isinstance(content, dict):\n                # create a subtree\n                tree_id = self.create_tree(content)\n                builder.insert(filename, tree_id, pygit2.GIT_FILEMODE_TREE)\n            else:\n                # create a file\n                blob_id = self.repo.create_blob(content)\n                builder.insert(filename, blob_id, pygit2.GIT_FILEMODE_BLOB)\n\n        tree_id = builder.write()\n        return tree_id\n\n    def create_commit(self, files, parents=None, message='',\n                      reference_name=None):\n        if parents is None:\n            # by default use the main branch as the base of the new branch\n            # required to reuse github actions cache across crossbow tasks\n            commit, _ = self.repo.resolve_refish(self.default_branch_name)\n            parents = [commit.id]\n        tree_id = self.create_tree(files)\n\n        author = committer = self.signature\n        commit_id = self.repo.create_commit(reference_name, author, committer,\n                                            message, tree_id, parents)\n        return self.repo[commit_id]\n\n    def create_branch(self, branch_name, files, parents=None, message='',\n                      signature=None):\n        # create commit with the passed tree\n        commit = self.create_commit(files, parents=parents, message=message)\n\n        # create branch pointing to the previously created commit\n        branch = self.repo.create_branch(branch_name, commit)\n\n        # append to the pushable references\n        self._updated_refs.append('refs/heads/{}'.format(branch_name))\n\n        return branch\n\n    def create_tag(self, tag_name, commit_id, message=''):\n        git_object_commit = (\n            pygit2.GIT_OBJECT_COMMIT\n            if getattr(pygit2, 'GIT_OBJECT_COMMIT')\n            else pygit2.GIT_OBJ_COMMIT\n        )\n        tag_id = self.repo.create_tag(tag_name, commit_id,\n                                      git_object_commit,\n                                      self.signature,\n                                      message)\n\n        # append to the pushable references\n        self._updated_refs.append('refs/tags/{}'.format(tag_name))\n\n        return self.repo[tag_id]\n\n    def file_contents(self, commit_id, file):\n        commit = self.repo[commit_id]\n        entry = commit.tree[file]\n        blob = self.repo[entry.id]\n        return blob.data\n\n    def _github_login(self, github_token):\n        \"\"\"Returns a logged in github3.GitHub instance\"\"\"\n        if not _have_github3:\n            raise ImportError('Must install github3.py')\n        github_token = github_token or self.github_token\n        session = github3.session.GitHubSession(\n            default_connect_timeout=10,\n            default_read_timeout=30\n        )\n        github = github3.GitHub(session=session)\n        github.login(token=github_token)\n        return github\n\n    def as_github_repo(self, github_token=None):\n        \"\"\"Converts it to a repository object which wraps the GitHub API\"\"\"\n        if self._github_repo is None:\n            github = self._github_login(github_token)\n            username, reponame = _parse_github_user_repo(self.remote_url)\n            self._github_repo = github.repository(username, reponame)\n        return self._github_repo\n\n    def token_expiration_date(self, github_token=None):\n        \"\"\"Returns the expiration date for the github_token provided\"\"\"\n        github = self._github_login(github_token)\n        # github3 hides the headers from us. Use the _get method\n        # to access the response headers.\n        resp = github._get(github.session.base_url)\n        # Response in the form '2023-01-23 10:40:28 UTC'\n        date_string = resp.headers.get(\n            'github-authentication-token-expiration')\n        if date_string:\n            return date.fromisoformat(date_string.split()[0])\n\n    def github_commit(self, sha):\n        repo = self.as_github_repo()\n        return repo.commit(sha)\n\n    def github_release(self, tag):\n        repo = self.as_github_repo()\n        try:\n            return repo.release_from_tag(tag)\n        except github3.exceptions.NotFoundError:\n            return None\n\n    def github_upload_asset_requests(self, release, path, name, mime,\n                                     max_retries=None, retry_backoff=None):\n        if max_retries is None:\n            max_retries = int(os.environ.get('CROSSBOW_MAX_RETRIES', 8))\n        if retry_backoff is None:\n            retry_backoff = int(os.environ.get('CROSSBOW_RETRY_BACKOFF', 5))\n\n        for i in range(max_retries):\n            try:\n                with open(path, 'rb') as fp:\n                    result = release.upload_asset(name=name, asset=fp,\n                                                  content_type=mime)\n            except github3.exceptions.ResponseError as e:\n                logger.error('Attempt {} has failed with message: {}.'\n                             .format(i + 1, str(e)))\n                logger.error('Error message {}'.format(e.msg))\n                logger.error('List of errors provided by GitHub:')\n                for err in e.errors:\n                    logger.error(' - {}'.format(err))\n\n                if e.code == 422:\n                    # 422 Validation Failed, probably raised because\n                    # ReleaseAsset already exists, so try to remove it before\n                    # reattempting the asset upload\n                    for asset in release.assets():\n                        if asset.name == name:\n                            logger.info('Release asset {} already exists, '\n                                        'removing it...'.format(name))\n                            asset.delete()\n                            logger.info('Asset {} removed.'.format(name))\n                            break\n            except github3.exceptions.ConnectionError as e:\n                logger.error('Attempt {} has failed with message: {}.'\n                             .format(i + 1, str(e)))\n            else:\n                logger.info('Attempt {} has finished.'.format(i + 1))\n                return result\n\n            time.sleep(retry_backoff)\n\n        raise RuntimeError('GitHub asset uploading has failed!')\n\n    def github_upload_asset_curl(self, release, path, name, mime):\n        upload_url, _ = release.upload_url.split('{?')\n        upload_url += '?name={}'.format(name)\n\n        command = [\n            'curl',\n            '--fail',\n            '-H', \"Authorization: token {}\".format(self.github_token),\n            '-H', \"Content-Type: {}\".format(mime),\n            '--data-binary', '@{}'.format(path),\n            upload_url\n        ]\n        return subprocess.run(command, shell=False, check=True)\n\n    def github_overwrite_release_assets(self, tag_name, target_commitish,\n                                        patterns, method='requests'):\n        # Since github has changed something the asset uploading via requests\n        # got instable, so prefer the cURL alternative.\n        # Potential cause:\n        #    sigmavirus24/github3.py/issues/779#issuecomment-379470626\n        repo = self.as_github_repo()\n        if not tag_name:\n            raise CrossbowError('Empty tag name')\n        if not target_commitish:\n            raise CrossbowError('Empty target commit for the release tag')\n\n        # remove the whole release if it already exists\n        try:\n            release = repo.release_from_tag(tag_name)\n        except github3.exceptions.NotFoundError:\n            pass\n        else:\n            release.delete()\n\n        release = repo.create_release(tag_name, target_commitish)\n        for pattern in patterns:\n            for path in glob.glob(pattern, recursive=True):\n                name = os.path.basename(path)\n                size = os.path.getsize(path)\n                mime = mimetypes.guess_type(name)[0] or 'application/zip'\n\n                logger.info(\n                    'Uploading asset `{}` with mimetype {} and size {}...'\n                    .format(name, mime, size)\n                )\n\n                if method == 'requests':\n                    self.github_upload_asset_requests(release, path, name=name,\n                                                      mime=mime)\n                elif method == 'curl':\n                    self.github_upload_asset_curl(release, path, name=name,\n                                                  mime=mime)\n                else:\n                    raise CrossbowError(\n                        'Unsupported upload method {}'.format(method)\n                    )\n\n    def github_pr(self, title, head=None, base=None, body=None,\n                  github_token=None, create=False):\n        if create:\n            # Default value for base is the default_branch_name\n            base = self.default_branch_name if base is None else base\n        github_token = github_token or self.github_token\n        repo = self.as_github_repo(github_token=github_token)\n        if create:\n            return repo.create_pull(title=title, base=base, head=head,\n                                    body=body)\n        else:\n            # Retrieve open PR for base and head.\n            # There should be a single open one with that title.\n            for pull in repo.pull_requests(state=\"open\", head=head,\n                                           base=base):\n                if title in pull.title:\n                    return pull\n            raise CrossbowError(\n                f\"Pull request with Title: {title!r} not found \"\n                f\"in repository {repo.full_name!r}\"\n            )\n\n\nclass Queue(Repo):\n\n    def _latest_prefix_id(self, prefix):\n        pattern = re.compile(r'[\\w\\/-]*{}-(\\d+)'.format(prefix))\n        matches = list(filter(None, map(pattern.match, self.repo.branches)))\n        if matches:\n            latest = max(int(m.group(1)) for m in matches)\n        else:\n            latest = -1\n        return latest\n\n    def _prefix_contains_date(self, prefix):\n        prefix_date_pattern = re.compile(r'[\\w\\/-]*-(\\d+)-(\\d+)-(\\d+)')\n        match_prefix = prefix_date_pattern.match(prefix)\n        if match_prefix:\n            return match_prefix.group(0)[-10:]\n\n    def _latest_prefix_date(self, prefix):\n        pattern = re.compile(r'[\\w\\/-]*{}-(\\d+)-(\\d+)-(\\d+)'.format(prefix))\n        matches = list(filter(None, map(pattern.match, self.repo.branches)))\n        if matches:\n            latest = sorted([m.group(0) for m in matches])[-1]\n            # slice the trailing date part (YYYY-MM-DD)\n            latest = latest[-10:]\n        else:\n            latest = -1\n        return latest\n\n    def _next_job_id(self, prefix):\n        \"\"\"Auto increments the branch's identifier based on the prefix\"\"\"\n        latest_id = self._latest_prefix_id(prefix)\n        return '{}-{}'.format(prefix, latest_id + 1)\n\n    def _new_hex_id(self, prefix):\n        \"\"\"Append a new id to branch's identifier based on the prefix\"\"\"\n        hex_id = uuid.uuid4().hex[:10]\n        return '{}-{}'.format(prefix, hex_id)\n\n    def latest_for_prefix(self, prefix):\n        prefix_date = self._prefix_contains_date(prefix)\n        if prefix.startswith(\"nightly\") and not prefix_date:\n            latest_id = self._latest_prefix_date(prefix)\n            if not latest_id:\n                raise RuntimeError(\n                    f\"No job has been submitted with prefix '{prefix}'' yet\"\n                )\n            latest_id += \"-0\"\n        else:\n            latest_id = self._latest_prefix_id(prefix)\n            if latest_id < 0:\n                raise RuntimeError(\n                    f\"No job has been submitted with prefix '{prefix}' yet\"\n                )\n        job_name = '{}-{}'.format(prefix, latest_id)\n        return self.get(job_name)\n\n    def date_of(self, job):\n        # it'd be better to bound to the queue repository on deserialization\n        # and reorganize these methods to Job\n        branch_name = 'origin/{}'.format(job.branch)\n        branch = self.repo.branches[branch_name]\n        commit = self.repo[branch.target]\n        return date.fromtimestamp(commit.commit_time)\n\n    def jobs(self, pattern):\n        \"\"\"Return jobs sorted by its identifier in reverse order\"\"\"\n        job_names = []\n        for name in self.repo.branches.remote:\n            origin, name = name.split('/', 1)\n            result = re.match(pattern, name)\n            if result:\n                job_names.append(name)\n\n        for name in sorted(job_names, reverse=True):\n            yield self.get(name)\n\n    def get(self, job_name):\n        branch_name = 'origin/{}'.format(job_name)\n        branch = self.repo.branches[branch_name]\n        try:\n            content = self.file_contents(branch.target, 'job.yml')\n        except KeyError:\n            raise CrossbowError(\n                'No job is found with name: {}'.format(job_name)\n            )\n\n        buffer = StringIO(content.decode('utf-8'))\n        job = yaml.load(buffer)\n        job.queue = self\n        return job\n\n    def put(self, job, prefix='build', increment_job_id=True):\n        if not isinstance(job, Job):\n            raise CrossbowError('`job` must be an instance of Job')\n        if job.branch is not None:\n            raise CrossbowError('`job.branch` is automatically generated, '\n                                'thus it must be blank')\n\n        job.queue = self\n        if increment_job_id:\n            # auto increment and set next job id, e.g. build-85\n            job.branch = self._next_job_id(prefix)\n        else:\n            # set new branch to something unique, e.g. build-41d017af40\n            job.branch = self._new_hex_id(prefix)\n\n        # create tasks' branches\n        for task_name, task in job.tasks.items():\n            # adding CI's name to the end of the branch in order to use skip\n            # patterns on travis and circleci\n            task.branch = '{}-{}-{}'.format(job.branch, task.ci, task_name)\n            params = {\n                **job.params,\n                \"arrow\": job.target,\n                \"job\": job,\n                \"queue_remote_url\": self.remote_url\n            }\n            files = task.render_files(job.template_searchpath, params=params)\n            branch = self.create_branch(task.branch, files=files)\n            self.create_tag(task.tag, branch.target)\n            task.commit = str(branch.target)\n\n        # create job's branch with its description\n        return self.create_branch(job.branch, files=job.render_files())\n\n\ndef get_version(root, **kwargs):\n    \"\"\"\n    Parse function for setuptools_scm that ignores tags for non-C++\n    subprojects, e.g. apache-arrow-js-XXX tags.\n    \"\"\"\n    from setuptools_scm.git import parse as parse_git_version\n\n    # query the calculated version based on the git tags\n    kwargs['describe_command'] = (\n        'git describe --dirty --tags --long --match \"apache-arrow-[0-9]*.*\"'\n    )\n    version = parse_git_version(root, **kwargs)\n    tag = str(version.tag)\n\n    # We may get a development tag for the next version, such as \"5.0.0.dev0\",\n    # or the tag of an already released version, such as \"4.0.0\".\n    # In the latter case, we need to increment the version so that the computed\n    # version comes after any patch release (the next feature version after\n    # 4.0.0 is 5.0.0).\n    pattern = r\"^(\\d+)\\.(\\d+)\\.(\\d+)\"\n    match = re.match(pattern, tag)\n    major, minor, patch = map(int, match.groups())\n    if 'dev' not in tag:\n        major += 1\n\n    return \"{}.{}.{}.dev{}\".format(major, minor, patch, version.distance or 0)\n\n\nclass Serializable:\n\n    @classmethod\n    def to_yaml(cls, representer, data):\n        tag = '!{}'.format(cls.__name__)\n        dct = {k: v for k, v in data.__dict__.items() if not k.startswith('_')}\n        return representer.represent_mapping(tag, dct)\n\n\nclass Target(Serializable):\n    \"\"\"\n    Describes target repository and revision the builds run against\n\n    This serializable data container holding information about arrow's\n    git remote, branch, sha and version number as well as some metadata\n    (currently only an email address where the notification should be sent).\n    \"\"\"\n\n    def __init__(self, head, branch, remote, version, r_version, email=None):\n        self.head = head\n        self.email = email\n        self.branch = branch\n        self.remote = remote\n        self.github_repo = \"/\".join(_parse_github_user_repo(remote))\n        self.version = version\n        self.r_version = r_version\n        self.no_rc_version = re.sub(r'-rc\\d+\\Z', '', version)\n        self.no_rc_r_version = re.sub(r'-rc\\d+\\Z', '', r_version)\n        # Semantic Versioning 1.0.0: https://semver.org/spec/v1.0.0.html\n        #\n        # > A pre-release version number MAY be denoted by appending an\n        # > arbitrary string immediately following the patch version and a\n        # > dash. The string MUST be comprised of only alphanumerics plus\n        # > dash [0-9A-Za-z-].\n        #\n        # Example:\n        #\n        #   '0.16.1.dev10' ->\n        #   '0.16.1-dev10'\n        self.no_rc_semver_version = \\\n            re.sub(r'\\.(dev\\d+)\\Z', r'-\\1', self.no_rc_version)\n        # Substitute dev version for SNAPSHOT\n        #\n        # Example:\n        #\n        # '10.0.0.dev235' ->\n        # '10.0.0-SNAPSHOT'\n        self.no_rc_snapshot_version = re.sub(\n            r'\\.(dev\\d+)$', '-SNAPSHOT', self.no_rc_version)\n\n    @classmethod\n    def from_repo(cls, repo, head=None, branch=None, remote=None, version=None,\n                  email=None):\n        \"\"\"Initialize from a repository\n\n        Optionally override detected remote, branch, head, and/or version.\n        \"\"\"\n        assert isinstance(repo, Repo)\n\n        if head is None:\n            head = str(repo.head.target)\n        if branch is None:\n            branch = repo.branch.branch_name\n        if remote is None:\n            remote = repo.remote_url\n        if version is None:\n            version = get_version(repo.path)\n        if email is None:\n            email = repo.user_email\n\n        version_dev_match = re.match(r\".*\\.dev(\\d+)$\", version)\n        if version_dev_match:\n            with open(f\"{repo.path}/r/DESCRIPTION\") as description_file:\n                description = description_file.read()\n                r_version_pattern = re.compile(r\"^Version:\\s*(.*)$\",\n                                               re.MULTILINE)\n                r_version = re.findall(r_version_pattern, description)[0]\n            if r_version:\n                version_dev = int(version_dev_match[1])\n                # \"1_0000_00_00 +\" is for generating a greater version\n                # than YYYYMMDD. For example, 1_0000_00_01\n                # (version_dev == 1 case) is greater than 2022_10_16.\n                #\n                # Why do we need a greater version than YYYYMMDD? It's\n                # for keeping backward compatibility. We used\n                # MAJOR.MINOR.PATCH.YYYYMMDD as our nightly package\n                # version. (See also ARROW-16403). If we use \"9000 +\n                # version_dev\" here, a developer that used\n                # 9.0.0.20221016 can't upgrade to the later nightly\n                # package unless we release 10.0.0. Because 9.0.0.9234\n                # or something is less than 9.0.0.20221016.\n                r_version_dev = 1_0000_00_00 + version_dev\n                # version: 10.0.0.dev234\n                # r_version: 9.0.0.9000\n                # -> 9.0.0.100000234\n                r_version = re.sub(r\"\\.9000\\Z\", f\".{r_version_dev}\", r_version)\n            else:\n                r_version = version\n        else:\n            r_version = version\n\n        return cls(head=head, email=email, branch=branch, remote=remote,\n                   version=version, r_version=r_version)\n\n    def is_default_branch(self):\n        return self.branch == 'main'\n\n\nclass Task(Serializable):\n    \"\"\"\n    Describes a build task and metadata required to render CI templates\n\n    A task is represented as a single git commit and branch containing jinja2\n    rendered files (currently appveyor.yml or .travis.yml configurations).\n\n    A task can't be directly submitted to a queue, must belong to a job.\n    Each task's unique identifier is its branch name, which is generated after\n    submitting the job to a queue.\n    \"\"\"\n\n    def __init__(self, name, ci, template, artifacts=None, params=None):\n        assert ci in {\n            'circle',\n            'travis',\n            'appveyor',\n            'azure',\n            'github',\n            'drone',\n        }\n        self.name = name\n        self.ci = ci\n        self.template = template\n        self.artifacts = artifacts or []\n        self.params = params or {}\n        self.branch = None  # filled after adding to a queue\n        self.commit = None  # filled after adding to a queue\n        self._queue = None  # set by the queue object after put or get\n        self._status = None  # status cache\n        self._assets = None  # assets cache\n\n    def render_files(self, searchpath, params=None):\n        params = {**self.params, **(params or {}), \"task\": self}\n        try:\n            rendered = _render_jinja_template(searchpath, self.template,\n                                              params=params)\n        except jinja2.TemplateError as e:\n            raise RuntimeError(\n                'Failed to render template `{}` with {}: {}'.format(\n                    self.template, e.__class__.__name__, str(e)\n                )\n            )\n\n        tree = {**_default_tree, self.filename: rendered}\n        return _unflatten_tree(tree)\n\n    @property\n    def tag(self):\n        return self.branch\n\n    @property\n    def filename(self):\n        config_files = {\n            'circle': '.circleci/config.yml',\n            'travis': '.travis.yml',\n            'appveyor': 'appveyor.yml',\n            'azure': 'azure-pipelines.yml',\n            'github': '.github/workflows/crossbow.yml',\n            'drone': '.drone.yml',\n        }\n        return config_files[self.ci]\n\n    def status(self, force_query=False):\n        _status = getattr(self, '_status', None)\n        if force_query or _status is None:\n            github_commit = self._queue.github_commit(self.commit)\n            self._status = TaskStatus(github_commit)\n        return self._status\n\n    def assets(self, force_query=False, validate_patterns=True):\n        _assets = getattr(self, '_assets', None)\n        if force_query or _assets is None:\n            github_release = self._queue.github_release(self.tag)\n            self._assets = TaskAssets(github_release,\n                                      artifact_patterns=self.artifacts,\n                                      validate_patterns=validate_patterns)\n        return self._assets\n\n\nclass TaskStatus:\n    \"\"\"\n    Combine the results from status and checks API to a single state.\n\n    Azure pipelines uses checks API which doesn't provide a combined\n    interface like status API does, so we need to manually combine\n    both the commit statuses and the commit checks coming from\n    different API endpoint\n\n    Status.state: error, failure, pending or success, default pending\n    CheckRun.status: queued, in_progress or completed, default: queued\n    CheckRun.conclusion: success, failure, neutral, cancelled, timed_out\n                            or action_required, only set if\n                            CheckRun.status == 'completed'\n\n    1. Convert CheckRun's status and conclusion to one of Status.state\n    2. Merge the states based on the following rules:\n        - failure if any of the contexts report as error or failure\n        - pending if there are no statuses or a context is pending\n        - success if the latest status for all contexts is success\n        error otherwise.\n\n    Parameters\n    ----------\n    commit : github3.Commit\n        Commit to query the combined status for.\n\n    Returns\n    -------\n    TaskStatus(\n        combined_state='error|failure|pending|success',\n        github_status='original github status object',\n        github_check_runs='github checks associated with the commit',\n        total_count='number of statuses and checks'\n    )\n    \"\"\"\n\n    def __init__(self, commit):\n        status = commit.status()\n        check_runs = list(commit.check_runs())\n        states = [s.state for s in status.statuses]\n\n        for check in check_runs:\n            if check.status == 'completed':\n                if check.conclusion in {'success', 'failure'}:\n                    states.append(check.conclusion)\n                elif check.conclusion in {'cancelled', 'timed_out',\n                                          'action_required'}:\n                    states.append('error')\n                # omit `neutral` conclusion\n            else:\n                states.append('pending')\n\n        # it could be more effective, but the following is more descriptive\n        combined_state = 'error'\n        if len(states):\n            if any(state in {'error', 'failure'} for state in states):\n                combined_state = 'failure'\n            elif any(state == 'pending' for state in states):\n                combined_state = 'pending'\n            elif all(state == 'success' for state in states):\n                combined_state = 'success'\n\n        # show link to the actual build, some of the CI providers implement\n        # the statuses API others implement the checks API, so display both\n        build_links = [s.target_url for s in status.statuses]\n        build_links += [c.html_url for c in check_runs]\n\n        self.combined_state = combined_state\n        self.github_status = status\n        self.github_check_runs = check_runs\n        self.total_count = len(states)\n        self.build_links = build_links\n\n\nclass TaskAssets(dict):\n\n    def __init__(self, github_release, artifact_patterns,\n                 validate_patterns=True):\n        # HACK(kszucs): don't expect uploaded assets of no artifacts were\n        # defined for the tasks in order to spare a bit of github rate limit\n        if not artifact_patterns:\n            return\n\n        if github_release is None:\n            github_assets = {}  # no assets have been uploaded for the task\n        else:\n            github_assets = {a.name: a for a in github_release.assets()}\n\n        if not validate_patterns:\n            # shortcut to avoid pattern validation and just set all artifacts\n            return self.update(github_assets)\n\n        for pattern in artifact_patterns:\n            # artifact can be a regex pattern\n            compiled = re.compile(f\"^{pattern}$\")\n            matches = list(\n                filter(None, map(compiled.match, github_assets.keys()))\n            )\n            num_matches = len(matches)\n\n            # validate artifact pattern matches single asset\n            if num_matches == 0:\n                self[pattern] = None\n            elif num_matches == 1:\n                self[pattern] = github_assets[matches[0].group(0)]\n            else:\n                raise CrossbowError(\n                    'Only a single asset should match pattern `{}`, there are '\n                    'multiple ones: {}'.format(pattern, ', '.join(matches))\n                )\n\n    def missing_patterns(self):\n        return [pattern for pattern, asset in self.items() if asset is None]\n\n    def uploaded_assets(self):\n        return [asset for asset in self.values() if asset is not None]\n\n\nclass Job(Serializable):\n    \"\"\"Describes multiple tasks against a single target repository\"\"\"\n\n    def __init__(self, target, tasks, params=None, template_searchpath=None):\n        if not tasks:\n            raise ValueError('no tasks were provided for the job')\n        if not all(isinstance(task, Task) for task in tasks.values()):\n            raise ValueError('each `tasks` mus be an instance of Task')\n        if not isinstance(target, Target):\n            raise ValueError('`target` must be an instance of Target')\n        if not isinstance(params, dict):\n            raise ValueError('`params` must be an instance of dict')\n\n        self.target = target\n        self.tasks = tasks\n        self.params = params or {}  # additional parameters for the tasks\n        self.branch = None  # filled after adding to a queue\n        self._queue = None  # set by the queue object after put or get\n        if template_searchpath is None:\n            self._template_searchpath = ArrowSources.find().path\n        else:\n            self._template_searchpath = template_searchpath\n\n    def render_files(self):\n        with StringIO() as buf:\n            yaml.dump(self, buf)\n            content = buf.getvalue()\n        tree = {**_default_tree, \"job.yml\": content}\n        return _unflatten_tree(tree)\n\n    def render_tasks(self, params=None):\n        result = {}\n        params = {\n            **self.params,\n            \"arrow\": self.target,\n            \"job\": self,\n            **(params or {})\n        }\n        for task_name, task in self.tasks.items():\n            files = task.render_files(self._template_searchpath, params)\n            result[task_name] = files\n        return result\n\n    @property\n    def template_searchpath(self):\n        return self._template_searchpath\n\n    @property\n    def queue(self):\n        assert isinstance(self._queue, Queue)\n        return self._queue\n\n    @queue.setter\n    def queue(self, queue):\n        assert isinstance(queue, Queue)\n        self._queue = queue\n        for task in self.tasks.values():\n            task._queue = queue\n\n    @property\n    def email(self):\n        return os.environ.get('CROSSBOW_EMAIL', self.target.email)\n\n    @property\n    def date(self):\n        return self.queue.date_of(self)\n\n    def show(self, stream=None):\n        return yaml.dump(self, stream=stream)\n\n    @classmethod\n    def from_config(cls, config, target, tasks=None, groups=None, params=None):\n        \"\"\"\n        Instantiate a job from based on a config.\n\n        Parameters\n        ----------\n        config : dict\n            Deserialized content of tasks.yml\n        target : Target\n            Describes target repository and revision the builds run against.\n        tasks : Optional[List[str]], default None\n            List of glob patterns for matching task names.\n        groups : Optional[List[str]], default None\n            List of exact group names matching predefined task sets in the\n            config.\n        params : Optional[Dict[str, str]], default None\n            Additional rendering parameters for the task templates.\n\n        Returns\n        -------\n        Job\n\n        Raises\n        ------\n        Exception:\n            If invalid groups or tasks has been passed.\n        \"\"\"\n        task_definitions = config.select(tasks, groups=groups)\n\n        # instantiate the tasks\n        tasks = {}\n        versions = {\n            'version': target.version,\n            'no_rc_version': target.no_rc_version,\n            'no_rc_semver_version': target.no_rc_semver_version,\n            'no_rc_snapshot_version': target.no_rc_snapshot_version,\n            'r_version': target.r_version,\n            'no_rc_r_version': target.no_rc_r_version,\n        }\n        for task_name, task in task_definitions.items():\n            task = task.copy()\n            artifacts = task.pop('artifacts', None) or []  # because of yaml\n            artifacts = [fn.format(**versions) for fn in artifacts]\n            tasks[task_name] = Task(task_name, artifacts=artifacts, **task)\n        return cls(target=target, tasks=tasks, params=params,\n                   template_searchpath=config.template_searchpath)\n\n    def is_finished(self):\n        for task in self.tasks.values():\n            status = task.status(force_query=True)\n            if status.combined_state == 'pending':\n                return False\n        return True\n\n    def wait_until_finished(self, poll_max_minutes=120,\n                            poll_interval_minutes=10):\n        started_at = time.time()\n        while True:\n            if self.is_finished():\n                break\n\n            waited_for_minutes = (time.time() - started_at) / 60\n            if waited_for_minutes > poll_max_minutes:\n                msg = ('Exceeded the maximum amount of time waiting for job '\n                       'to finish, waited for {} minutes.')\n                raise RuntimeError(msg.format(waited_for_minutes))\n\n            logger.info('Waiting {} minutes and then checking again'\n                        .format(poll_interval_minutes))\n            time.sleep(poll_interval_minutes * 60)\n\n\nclass Config(dict):\n\n    def __init__(self, tasks, template_searchpath):\n        super().__init__(tasks)\n        self.template_searchpath = template_searchpath\n\n    @classmethod\n    def load_yaml(cls, path):\n        path = Path(path)\n        searchpath = path.parent\n        rendered = _render_jinja_template(searchpath, template=path.name,\n                                          params={})\n        config = yaml.load(rendered)\n        return cls(config, template_searchpath=searchpath)\n\n    def show(self, stream=None):\n        return yaml.dump(dict(self), stream=stream)\n\n    def select(self, tasks=None, groups=None):\n        config_groups = dict(self['groups'])\n        config_tasks = dict(self['tasks'])\n        valid_groups = set(config_groups.keys())\n        valid_tasks = set(config_tasks.keys())\n        group_allowlist = list(groups or [])\n        task_allowlist = list(tasks or [])\n\n        # validate that the passed groups are defined in the config\n        requested_groups = set(group_allowlist)\n        invalid_groups = requested_groups - valid_groups\n        if invalid_groups:\n            msg = 'Invalid group(s) {!r}. Must be one of {!r}'.format(\n                invalid_groups, valid_groups\n            )\n            raise CrossbowError(msg)\n\n        # treat the task names as glob patterns to select tasks more easily\n        requested_tasks = set()\n        for pattern in task_allowlist:\n            matches = fnmatch.filter(valid_tasks, pattern)\n            if len(matches):\n                requested_tasks.update(matches)\n            else:\n                raise CrossbowError(\n                    \"Unable to match any tasks for `{}`\".format(pattern)\n                )\n\n        requested_group_tasks = set()\n        for group in group_allowlist:\n            # separate the patterns from the blocklist patterns\n            task_patterns = list(config_groups[group])\n            task_blocklist_patterns = [\n                x.strip(\"~\") for x in task_patterns if x.startswith(\"~\")]\n            task_patterns = [x for x in task_patterns if not x.startswith(\"~\")]\n\n            # treat the task names as glob patterns to select tasks more easily\n            for pattern in task_patterns:\n                matches = fnmatch.filter(valid_tasks, pattern)\n                if len(matches):\n                    requested_group_tasks.update(matches)\n                else:\n                    raise CrossbowError(\n                        \"Unable to match any tasks for `{}`\".format(pattern)\n                    )\n\n            # remove any tasks that are negated with ~task-name\n            for block_pattern in task_blocklist_patterns:\n                matches = fnmatch.filter(valid_tasks, block_pattern)\n                if len(matches):\n                    requested_group_tasks = requested_group_tasks.difference(\n                        matches)\n                else:\n                    raise CrossbowError(\n                        \"Unable to match any tasks for `{}`\".format(pattern)\n                    )\n\n        requested_tasks = requested_tasks.union(requested_group_tasks)\n\n        # validate that the passed and matched tasks are defined in the config\n        invalid_tasks = requested_tasks - valid_tasks\n        if invalid_tasks:\n            msg = 'Invalid task(s) {!r}. Must be one of {!r}'.format(\n                invalid_tasks, valid_tasks\n            )\n            raise CrossbowError(msg)\n\n        return {\n            task_name: config_tasks[task_name] for task_name in requested_tasks\n        }\n\n    def validate(self):\n        # validate that the task groups are properly referring to the tasks\n        for group_name, group in self['groups'].items():\n            for pattern in group:\n                # remove the negation character for blocklisted tasks\n                pattern = pattern.strip(\"~\")\n                tasks = self.select(tasks=[pattern])\n                if not tasks:\n                    raise CrossbowError(\n                        \"The pattern `{}` defined for task group `{}` is not \"\n                        \"matching any of the tasks defined in the \"\n                        \"configuration file.\".format(pattern, group_name)\n                    )\n\n        # validate that the tasks are constructible\n        for task_name, task in self['tasks'].items():\n            try:\n                Task(task_name, **task)\n            except Exception as e:\n                raise CrossbowError(\n                    'Unable to construct a task object from the '\n                    'definition  of task `{}`. The original error message '\n                    'is: `{}`'.format(task_name, str(e))\n                )\n\n        # Get the default branch name from the repository\n        arrow_source_dir = ArrowSources.find()\n        repo = Repo(arrow_source_dir.path)\n\n        # validate that the defined tasks are renderable, in order to to that\n        # define the required object with dummy data\n        target = Target(\n            head='e279a7e06e61c14868ca7d71dea795420aea6539',\n            branch=repo.default_branch_name,\n            remote='https://github.com/apache/arrow',\n            version='1.0.0dev123',\n            r_version='0.13.0.100000123',\n            email='dummy@example.ltd'\n        )\n        job = Job.from_config(config=self,\n                              target=target,\n                              tasks=self['tasks'],\n                              groups=self['groups'],\n                              params={})\n\n        for task_name, task in self['tasks'].items():\n            task = Task(task_name, **task)\n            files = task.render_files(\n                self.template_searchpath,\n                params=dict(\n                    arrow=target,\n                    job=job,\n                    queue_remote_url='https://github.com/org/crossbow'\n                )\n            )\n            if not files:\n                raise CrossbowError('No files have been rendered for task `{}`'\n                                    .format(task_name))\n\n\n# configure yaml serializer\nyaml = YAML()\nyaml.register_class(Job)\nyaml.register_class(Task)\nyaml.register_class(Target)\nyaml.register_class(Queue)\nyaml.register_class(TaskStatus)\n", "dev/archery/archery/crossbow/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom .core import Config, Repo, Queue, Target, Job  # noqa\nfrom .reports import CommentReport, ConsoleReport, EmailReport  # noqa\n", "dev/release/check-rat-report.py": "#!/usr/bin/env python\n##############################################################################\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n##############################################################################\nimport fnmatch\nimport re\nimport sys\nimport xml.etree.ElementTree as ET\n\nif len(sys.argv) != 3:\n    sys.stderr.write(\"Usage: %s exclude_globs.lst rat_report.xml\\n\" %\n                     sys.argv[0])\n    sys.exit(1)\n\nexclude_globs_filename = sys.argv[1]\nxml_filename = sys.argv[2]\n\nglobs = [line.strip() for line in open(exclude_globs_filename, \"r\")]\n\ntree = ET.parse(xml_filename)\nroot = tree.getroot()\nresources = root.findall('resource')\n\nall_ok = True\nfor r in resources:\n    approvals = r.findall('license-approval')\n    if not approvals or approvals[0].attrib['name'] == 'true':\n        continue\n    clean_name = re.sub('^[^/]+/', '', r.attrib['name'])\n    excluded = False\n    for g in globs:\n        if fnmatch.fnmatch(clean_name, g):\n            excluded = True\n            break\n    if not excluded:\n        sys.stdout.write(\"NOT APPROVED: %s (%s): %s\\n\" % (\n            clean_name, r.attrib['name'], approvals[0].attrib['name']))\n        all_ok = False\n\nif not all_ok:\n    sys.exit(1)\n\nprint('OK')\nsys.exit(0)\n", "dev/release/download_rc_binaries.py": "#!/usr/bin/env python\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Download release binaries.\"\"\"\n\nimport argparse\nimport concurrent.futures as cf\nimport functools\nimport json\nimport os\nimport random\nimport re\nimport subprocess\nimport time\nimport urllib.request\n\n\nDEFAULT_PARALLEL_DOWNLOADS = 8\n\n\nclass Downloader:\n\n    def get_file_list(self, prefix, filter=None):\n        def traverse(directory, files, directories):\n            url = f'{self.URL_ROOT}/{directory}'\n            response = urllib.request.urlopen(url).read().decode()\n            paths = re.findall('<a href=\"(.+?)\"', response)\n            for path in paths:\n                path = re.sub(f'^{re.escape(url)}',\n                              '',\n                              path)\n                if path == '../':\n                    continue\n                resolved_path = f'{directory}{path}'\n                if filter and not filter(path):\n                    continue\n                if path.endswith('/'):\n                    directories.append(resolved_path)\n                else:\n                    files.append(resolved_path)\n        files = []\n        if prefix != '' and not prefix.endswith('/'):\n            prefix += '/'\n        directories = [prefix]\n        while len(directories) > 0:\n            directory = directories.pop()\n            traverse(directory, files, directories)\n        return files\n\n    def download_files(self, files, dest=None, num_parallel=None,\n                       re_match=None):\n        \"\"\"\n        Download files from Bintray in parallel. If file already exists, will\n        overwrite if the checksum does not match what Bintray says it should be\n\n        Parameters\n        ----------\n        files : List[Dict]\n            File listing from Bintray\n        dest : str, default None\n            Defaults to current working directory\n        num_parallel : int, default 8\n            Number of files to download in parallel. If set to None, uses\n            default\n        \"\"\"\n        if dest is None:\n            dest = os.getcwd()\n        if num_parallel is None:\n            num_parallel = DEFAULT_PARALLEL_DOWNLOADS\n\n        if re_match is not None:\n            regex = re.compile(re_match)\n            files = [x for x in files if regex.match(x)]\n\n        if num_parallel == 1:\n            for path in files:\n                self._download_file(dest, path)\n        else:\n            parallel_map_terminate_early(\n                functools.partial(self._download_file, dest),\n                files,\n                num_parallel\n            )\n\n    def _download_file(self, dest, path):\n        base, filename = os.path.split(path)\n\n        dest_dir = os.path.join(dest, base)\n        os.makedirs(dest_dir, exist_ok=True)\n\n        dest_path = os.path.join(dest_dir, filename)\n\n        print(\"Downloading {} to {}\".format(path, dest_path))\n\n        url = f'{self.URL_ROOT}/{path}'\n        self._download_url(url, dest_path)\n\n    def _download_url(self, url, dest_path, *, extra_args=None):\n        cmd = [\n            \"curl\",\n            \"--fail\",\n            \"--location\",\n            \"--retry\",\n            \"5\",\n            *(extra_args or []),\n            \"--output\",\n            dest_path,\n            url,\n        ]\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE)\n        stdout, stderr = proc.communicate()\n        if proc.returncode != 0:\n            try:\n                # Don't leave possibly partial file around\n                os.remove(dest_path)\n            except IOError:\n                pass\n            raise Exception(f\"Downloading {url} failed\\n\"\n                            f\"stdout: {stdout}\\nstderr: {stderr}\")\n\n    def _curl_version(self):\n        cmd = [\"curl\", \"--version\"]\n        out = subprocess.run(cmd, capture_output=True, check=True).stdout\n        match = re.search(r\"curl (\\d+)\\.(\\d+)\\.(\\d+) \", out.decode())\n        return (int(match.group(1)), int(match.group(2)), int(match.group(3)))\n\n\nclass Artifactory(Downloader):\n    URL_ROOT = \"https://apache.jfrog.io/artifactory/arrow\"\n\n\nclass Maven(Downloader):\n    URL_ROOT = \"https://repository.apache.org\" + \\\n        \"/content/repositories/staging/org/apache/arrow\"\n\n\nclass GitHub(Downloader):\n    def __init__(self, repository, tag):\n        super().__init__()\n        if repository is None:\n            raise ValueError(\"--repository is required\")\n        if tag is None:\n            raise ValueError(\"--tag is required\")\n        self._repository = repository\n        self._tag = tag\n\n    def get_file_list(self, prefix, filter=None):\n        url = (f\"https://api.github.com/repos/{self._repository}/\"\n               f\"releases/tags/{self._tag}\")\n        print(\"Fetching release from\", url)\n        request = urllib.request.Request(\n            url,\n            method=\"GET\",\n            headers={\n                \"Accept\": \"application/vnd.github+json\",\n            },\n        )\n        raw_response = urllib.request.urlopen(request).read().decode()\n        response = json.loads(raw_response)\n\n        files = []\n        for asset in response[\"assets\"]:\n            if filter and not filter(asset[\"name\"]):\n                continue\n            # Don't use the API URL since it has a fairly strict rate\n            # limit unless logged in, and we have a lot of tiny\n            # artifacts\n            url = (\n                f\"https://github.com/{self._repository}/\"\n                f\"releases/download/{self._tag}/{asset['name']}\"\n            )\n            files.append((asset[\"name\"], url))\n        return files\n\n    def _download_file(self, dest, asset):\n        name, url = asset\n\n        os.makedirs(dest, exist_ok=True)\n        dest_path = os.path.join(dest, name)\n        print(f\"Downloading {url} to {dest_path}\")\n\n        if os.path.isfile(dest_path):\n            print(\"Already downloaded\", dest_path)\n            return\n\n        delay = random.randint(0, 3)\n        print(f\"Waiting {delay} seconds to avoid rate limit\")\n        time.sleep(delay)\n\n        extra_args = [\n            \"--header\",\n            \"Accept: application/octet-stream\",\n        ]\n        if self._curl_version() >= (7, 71, 0):\n            # Also retry 403s\n            extra_args.append(\"--retry-all-errors\")\n        self._download_url(\n            url,\n            dest_path,\n            extra_args=extra_args\n        )\n\n\ndef parallel_map_terminate_early(f, iterable, num_parallel):\n    tasks = []\n    with cf.ProcessPoolExecutor(num_parallel) as pool:\n        for v in iterable:\n            tasks.append(pool.submit(functools.partial(f, v)))\n\n        for task in cf.as_completed(tasks):\n            if task.exception() is not None:\n                e = task.exception()\n                for task in tasks:\n                    task.cancel()\n                raise e\n\n\nARROW_REPOSITORY_PACKAGE_TYPES = [\n    'almalinux',\n    'amazon-linux',\n    'centos',\n    'debian',\n    'ubuntu',\n]\nARROW_STANDALONE_PACKAGE_TYPES = ['nuget', 'python']\nARROW_PACKAGE_TYPES = \\\n    ARROW_REPOSITORY_PACKAGE_TYPES + \\\n    ARROW_STANDALONE_PACKAGE_TYPES\n\n\ndef download_rc_binaries(version, rc_number, re_match=None, dest=None,\n                         num_parallel=None, target_package_type=None,\n                         repository=None, tag=None):\n    version_string = '{}-rc{}'.format(version, rc_number)\n    version_pattern = re.compile(r'\\d+\\.\\d+\\.\\d+')\n    if target_package_type:\n        package_types = [target_package_type]\n    else:\n        package_types = ARROW_PACKAGE_TYPES\n    for package_type in package_types:\n        def is_target(path):\n            match = version_pattern.search(path)\n            if not match:\n                return True\n            return match[0] == version\n        filter = is_target\n\n        if package_type == 'jars':\n            downloader = Maven()\n            prefix = ''\n        elif package_type == 'github':\n            downloader = GitHub(repository, tag)\n            prefix = ''\n            filter = None\n        elif package_type in ARROW_REPOSITORY_PACKAGE_TYPES:\n            downloader = Artifactory()\n            prefix = f'{package_type}-rc'\n        else:\n            downloader = Artifactory()\n            prefix = f'{package_type}-rc/{version_string}'\n            filter = None\n        files = downloader.get_file_list(prefix, filter=filter)\n        downloader.download_files(files, re_match=re_match, dest=dest,\n                                  num_parallel=num_parallel)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='Download release candidate binaries'\n    )\n    parser.add_argument('version', type=str, help='The version number')\n    parser.add_argument('rc_number', type=int,\n                        help='The release candidate number, e.g. 0, 1, etc')\n    parser.add_argument('-e', '--regexp', type=str, default=None,\n                        help=('Regular expression to match on file names '\n                              'to only download certain files'))\n    parser.add_argument('--dest', type=str, default=os.getcwd(),\n                        help='The output folder for the downloaded files')\n    parser.add_argument('--num_parallel', type=int,\n                        default=DEFAULT_PARALLEL_DOWNLOADS,\n                        help='The number of concurrent downloads to do')\n    parser.add_argument('--package_type', type=str, default=None,\n                        help='The package type to be downloaded')\n    parser.add_argument('--repository', type=str,\n                        help=('The repository to pull from '\n                              '(required if --package_type=github)'))\n    parser.add_argument('--tag', type=str,\n                        help=('The release tag to download '\n                              '(required if --package_type=github)'))\n    args = parser.parse_args()\n\n    download_rc_binaries(\n        args.version,\n        args.rc_number,\n        dest=args.dest,\n        re_match=args.regexp,\n        num_parallel=args.num_parallel,\n        target_package_type=args.package_type,\n        repository=args.repository,\n        tag=args.tag,\n    )\n", "r/inst/demo_flight_server.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"\n    An example Flight Python server.\n    See https://github.com/apache/arrow/blob/main/python/examples/flight/server.py\n\"\"\"\n\nimport ast\nimport threading\nimport time\n\nimport pyarrow\nimport pyarrow.flight\n\n\nclass DemoFlightServer(pyarrow.flight.FlightServerBase):\n    def __init__(self, host=\"localhost\", port=5005):\n        if isinstance(port, float):\n            # Because R is looser with integer vs. float\n            port = int(port)\n        location = \"grpc+tcp://{}:{}\".format(host, port)\n        super(DemoFlightServer, self).__init__(location)\n        self.flights = {}\n        self.host = host\n\n    @classmethod\n    def descriptor_to_key(self, descriptor):\n        return (descriptor.descriptor_type.value, descriptor.command,\n                tuple(descriptor.path or tuple()))\n\n    def _make_flight_info(self, key, descriptor, table):\n        location = pyarrow.flight.Location.for_grpc_tcp(self.host, self.port)\n        endpoints = [pyarrow.flight.FlightEndpoint(repr(key), [location]), ]\n\n        mock_sink = pyarrow.MockOutputStream()\n        stream_writer = pyarrow.RecordBatchStreamWriter(\n            mock_sink, table.schema)\n        stream_writer.write_table(table)\n        stream_writer.close()\n        data_size = mock_sink.size()\n\n        return pyarrow.flight.FlightInfo(table.schema,\n                                         descriptor, endpoints,\n                                         table.num_rows, data_size)\n\n    def list_flights(self, context, criteria):\n        print(\"list_flights\")\n        for key, table in self.flights.items():\n            if key[1] is not None:\n                descriptor = \\\n                    pyarrow.flight.FlightDescriptor.for_command(key[1])\n            else:\n                descriptor = pyarrow.flight.FlightDescriptor.for_path(*key[2])\n\n            yield self._make_flight_info(key, descriptor, table)\n\n    def get_flight_info(self, context, descriptor):\n        print(\"get_flight_info\")\n        key = DemoFlightServer.descriptor_to_key(descriptor)\n        if key in self.flights:\n            table = self.flights[key]\n            return self._make_flight_info(key, descriptor, table)\n        raise KeyError('Flight not found.')\n\n    def do_put(self, context, descriptor, reader, writer):\n        print(\"do_put\")\n        key = DemoFlightServer.descriptor_to_key(descriptor)\n        print(key)\n        self.flights[key] = reader.read_all()\n        print(self.flights[key])\n\n    def do_get(self, context, ticket):\n        print(\"do_get\")\n        key = ast.literal_eval(ticket.ticket.decode())\n        if key not in self.flights:\n            return None\n        return pyarrow.flight.RecordBatchStream(self.flights[key])\n\n    def list_actions(self, context):\n        print(\"list_actions\")\n        return [\n            (\"clear\", \"Clear the stored flights.\"),\n            (\"shutdown\", \"Shut down this server.\"),\n        ]\n\n    def do_action(self, context, action):\n        print(\"do_action\")\n        if action.type == \"clear\":\n            raise NotImplementedError(\n                \"{} is not implemented.\".format(action.type))\n        elif action.type == \"healthcheck\":\n            pass\n        elif action.type == \"shutdown\":\n            yield pyarrow.flight.Result(pyarrow.py_buffer(b'Shutdown!'))\n            # Shut down on background thread to avoid blocking current\n            # request\n            threading.Thread(target=self._shutdown).start()\n        else:\n            raise KeyError(\"Unknown action {!r}\".format(action.type))\n\n    def _shutdown(self):\n        \"\"\"Shut down after a delay.\"\"\"\n        print(\"Server is shutting down...\")\n        time.sleep(2)\n        self.shutdown()\n", "c_glib/tool/generate-version-header.py": "#!/usr/bin/env python3\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nimport argparse\nfrom io import TextIOBase\nfrom pathlib import Path\nimport re\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n            description=\"Generate C header with version macros\")\n    parser.add_argument(\n            \"--library\",\n            required=True,\n            help=\"The library name to use in macro prefixes\")\n    parser.add_argument(\n            \"--version\",\n            required=True,\n            help=\"The library version number\")\n    parser.add_argument(\n            \"--input\",\n            type=Path,\n            required=True,\n            help=\"Path to the input template file\")\n    parser.add_argument(\n            \"--output\",\n            type=Path,\n            required=True,\n            help=\"Path to the output file to generate\")\n\n    args = parser.parse_args()\n\n    with open(args.input, \"r\", encoding=\"utf-8\") as input_file, \\\n            open(args.output, \"w\", encoding=\"utf-8\") as output_file:\n        write_header(\n                input_file, output_file, args.library, args.version)\n\n\ndef write_header(\n        input_file: TextIOBase,\n        output_file: TextIOBase,\n        library_name: str,\n        version: str):\n    if \"-\" in version:\n        version, version_tag = version.split(\"-\")\n    else:\n        version_tag = \"\"\n    version_major, version_minor, version_micro = [int(v) for v in version.split(\".\")]\n\n    encoded_versions = generate_encoded_versions(library_name)\n    visibility_macros = generate_visibility_macros(library_name)\n    availability_macros = generate_availability_macros(library_name)\n\n    replacements = {\n            \"VERSION_MAJOR\": str(version_major),\n            \"VERSION_MINOR\": str(version_minor),\n            \"VERSION_MICRO\": str(version_micro),\n            \"VERSION_TAG\": version_tag,\n            \"ENCODED_VERSIONS\": encoded_versions,\n            \"VISIBILITY_MACROS\": visibility_macros,\n            \"AVAILABILITY_MACROS\": availability_macros,\n    }\n\n    output_file.write(re.sub(\n        r\"@([A-Z_]+)@\", lambda match: replacements[match[1]], input_file.read()))\n\n\ndef generate_visibility_macros(library: str) -> str:\n    return f\"\"\"#if (defined(_WIN32) || defined(__CYGWIN__)) && defined(_MSC_VER) && \\\n  !defined({library}_STATIC_COMPILATION)\n#  define {library}_EXPORT __declspec(dllexport)\n#  define {library}_IMPORT __declspec(dllimport)\n#else\n#  define {library}_EXPORT\n#  define {library}_IMPORT\n#endif\n\n#ifdef {library}_COMPILATION\n#  define {library}_API {library}_EXPORT\n#else\n#  define {library}_API {library}_IMPORT\n#endif\n\n#define {library}_EXTERN {library}_API extern\"\"\"\n\n\ndef generate_encoded_versions(library: str) -> str:\n    macros = []\n\n    for major_version, minor_version in ALL_VERSIONS:\n        macros.append(f\"\"\"/**\n * {library}_VERSION_{major_version}_{minor_version}:\n *\n * You can use this macro value for compile time API version check.\n *\n * Since: {major_version}.{minor_version}.0\n */\n#define {library}_VERSION_{major_version}_{minor_version} G_ENCODE_VERSION({major_version}, {minor_version})\"\"\")  # noqa: E501\n\n    return \"\\n\\n\".join(macros)\n\n\ndef generate_availability_macros(library: str) -> str:\n    macros = [f\"\"\"#define {library}_AVAILABLE_IN_ALL {library}_EXTERN\"\"\"]\n\n    for major_version, minor_version in ALL_VERSIONS:\n        macros.append(f\"\"\"#if {library}_VERSION_MIN_REQUIRED >= {library}_VERSION_{major_version}_{minor_version}\n#  define {library}_DEPRECATED_IN_{major_version}_{minor_version}               {library}_DEPRECATED\n#  define {library}_DEPRECATED_IN_{major_version}_{minor_version}_FOR(function) {library}_DEPRECATED_FOR(function)\n#else\n#  define {library}_DEPRECATED_IN_{major_version}_{minor_version}\n#  define {library}_DEPRECATED_IN_{major_version}_{minor_version}_FOR(function)\n#endif\n\n#if {library}_VERSION_MAX_ALLOWED < {library}_VERSION_{major_version}_{minor_version}\n#  define {library}_AVAILABLE_IN_{major_version}_{minor_version} {library}_EXTERN {library}_UNAVAILABLE({major_version}, {minor_version})\n#else\n#  define {library}_AVAILABLE_IN_{major_version}_{minor_version} {library}_EXTERN\n#endif\"\"\")  # noqa: E501\n\n    return \"\\n\\n\".join(macros)\n\n\nALL_VERSIONS = [\n        (17, 0),\n        (16, 0),\n        (15, 0),\n        (14, 0),\n        (13, 0),\n        (12, 0),\n        (11, 0),\n        (10, 0),\n        (9, 0),\n        (8, 0),\n        (7, 0),\n        (6, 0),\n        (5, 0),\n        (4, 0),\n        (3, 0),\n        (2, 0),\n        (1, 0),\n        (0, 17),\n        (0, 16),\n        (0, 15),\n        (0, 14),\n        (0, 13),\n        (0, 12),\n        (0, 11),\n        (0, 10),\n]\n\n\nif __name__ == '__main__':\n    main()\n", "python/setup.py": "#!/usr/bin/env python\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport os\nimport os.path\nfrom os.path import join as pjoin\nimport re\nimport shlex\nimport sys\n\nif sys.version_info >= (3, 10):\n    import sysconfig\nelse:\n    # Get correct EXT_SUFFIX on Windows (https://bugs.python.org/issue39825)\n    from distutils import sysconfig\n\nimport pkg_resources\nfrom setuptools import setup, Extension, Distribution, find_namespace_packages\n\nfrom Cython.Distutils import build_ext as _build_ext\nimport Cython\n\n# Check if we're running 64-bit Python\nis_64_bit = sys.maxsize > 2**32\n\nif Cython.__version__ < '0.29.31':\n    raise Exception(\n        'Please update your Cython version. Supported Cython >= 0.29.31')\n\nsetup_dir = os.path.abspath(os.path.dirname(__file__))\n\next_suffix = sysconfig.get_config_var('EXT_SUFFIX')\n\n\n@contextlib.contextmanager\ndef changed_dir(dirname):\n    oldcwd = os.getcwd()\n    os.chdir(dirname)\n    try:\n        yield\n    finally:\n        os.chdir(oldcwd)\n\n\ndef strtobool(val):\n    \"\"\"Convert a string representation of truth to true (1) or false (0).\n\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n    'val' is anything else.\n    \"\"\"\n    # Copied from distutils\n    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))\n\n\nclass build_ext(_build_ext):\n    _found_names = ()\n\n    def build_extensions(self):\n        numpy_incl = pkg_resources.resource_filename('numpy', 'core/include')\n\n        self.extensions = [ext for ext in self.extensions\n                           if ext.name != '__dummy__']\n\n        for ext in self.extensions:\n            if (hasattr(ext, 'include_dirs') and\n                    numpy_incl not in ext.include_dirs):\n                ext.include_dirs.append(numpy_incl)\n        _build_ext.build_extensions(self)\n\n    def run(self):\n        self._run_cmake()\n        _build_ext.run(self)\n\n    # adapted from cmake_build_ext in dynd-python\n    # github.com/libdynd/dynd-python\n\n    description = \"Build the C-extensions for arrow\"\n    user_options = ([('cmake-generator=', None, 'CMake generator'),\n                     ('extra-cmake-args=', None, 'extra arguments for CMake'),\n                     ('build-type=', None,\n                      'build type (debug or release), default release'),\n                     ('boost-namespace=', None,\n                      'namespace of boost (default: boost)'),\n                     ('with-cuda', None, 'build the Cuda extension'),\n                     ('with-flight', None, 'build the Flight extension'),\n                     ('with-substrait', None, 'build the Substrait extension'),\n                     ('with-acero', None, 'build the Acero Engine extension'),\n                     ('with-dataset', None, 'build the Dataset extension'),\n                     ('with-parquet', None, 'build the Parquet extension'),\n                     ('with-parquet-encryption', None,\n                      'build the Parquet encryption extension'),\n                     ('with-azure', None,\n                      'build the Azure Blob Storage extension'),\n                     ('with-gcs', None,\n                      'build the Google Cloud Storage (GCS) extension'),\n                     ('with-s3', None, 'build the Amazon S3 extension'),\n                     ('with-static-parquet', None, 'link parquet statically'),\n                     ('with-static-boost', None, 'link boost statically'),\n                     ('with-orc', None, 'build the ORC extension'),\n                     ('with-gandiva', None, 'build the Gandiva extension'),\n                     ('generate-coverage', None,\n                      'enable Cython code coverage'),\n                     ('bundle-boost', None,\n                      'bundle the (shared) Boost libraries'),\n                     ('bundle-cython-cpp', None,\n                      'bundle generated Cython C++ code '\n                      '(used for code coverage)'),\n                     ('bundle-arrow-cpp', None,\n                      'bundle the Arrow C++ libraries'),\n                     ('bundle-arrow-cpp-headers', None,\n                      'bundle the Arrow C++ headers')] +\n                    _build_ext.user_options)\n\n    def initialize_options(self):\n        _build_ext.initialize_options(self)\n        self.cmake_generator = os.environ.get('PYARROW_CMAKE_GENERATOR')\n        if not self.cmake_generator and sys.platform == 'win32':\n            self.cmake_generator = 'Visual Studio 15 2017 Win64'\n        self.extra_cmake_args = os.environ.get('PYARROW_CMAKE_OPTIONS', '')\n        self.build_type = os.environ.get('PYARROW_BUILD_TYPE',\n                                         'release').lower()\n\n        self.cmake_cxxflags = os.environ.get('PYARROW_CXXFLAGS', '')\n\n        if sys.platform == 'win32':\n            # Cannot do debug builds in Windows unless Python itself is a debug\n            # build\n            if not hasattr(sys, 'gettotalrefcount'):\n                self.build_type = 'release'\n\n        self.with_azure = None\n        self.with_gcs = None\n        self.with_s3 = None\n        self.with_hdfs = None\n        self.with_cuda = None\n        self.with_substrait = None\n        self.with_flight = None\n        self.with_acero = None\n        self.with_dataset = None\n        self.with_parquet = None\n        self.with_parquet_encryption = None\n        self.with_orc = None\n        self.with_gandiva = None\n\n        self.generate_coverage = strtobool(\n            os.environ.get('PYARROW_GENERATE_COVERAGE', '0'))\n        self.bundle_arrow_cpp = strtobool(\n            os.environ.get('PYARROW_BUNDLE_ARROW_CPP', '0'))\n        self.bundle_cython_cpp = strtobool(\n            os.environ.get('PYARROW_BUNDLE_CYTHON_CPP', '0'))\n\n    CYTHON_MODULE_NAMES = [\n        'lib',\n        '_fs',\n        '_csv',\n        '_json',\n        '_compute',\n        '_cuda',\n        '_flight',\n        '_dataset',\n        '_dataset_orc',\n        '_dataset_parquet',\n        '_acero',\n        '_feather',\n        '_parquet',\n        '_parquet_encryption',\n        '_pyarrow_cpp_tests',\n        '_orc',\n        '_azurefs',\n        '_gcsfs',\n        '_s3fs',\n        '_substrait',\n        '_hdfs',\n        'gandiva']\n\n    def _run_cmake(self):\n        # check if build_type is correctly passed / set\n        if self.build_type.lower() not in ('release', 'debug',\n                                           'relwithdebinfo'):\n            raise ValueError(\"--build-type (or PYARROW_BUILD_TYPE) needs to \"\n                             \"be 'release', 'debug' or 'relwithdebinfo'\")\n\n        # The directory containing this setup.py\n        source = os.path.dirname(os.path.abspath(__file__))\n\n        # The staging directory for the module being built\n        build_cmd = self.get_finalized_command('build')\n        saved_cwd = os.getcwd()\n        build_temp = pjoin(saved_cwd, build_cmd.build_temp)\n        build_lib = pjoin(saved_cwd, build_cmd.build_lib)\n\n        if not os.path.isdir(build_temp):\n            self.mkpath(build_temp)\n\n        if self.inplace:\n            # a bit hacky\n            build_lib = saved_cwd\n\n        install_prefix = pjoin(build_lib, \"pyarrow\")\n\n        # Change to the build directory\n        with changed_dir(build_temp):\n            # Detect if we built elsewhere\n            if os.path.isfile('CMakeCache.txt'):\n                cachefile = open('CMakeCache.txt', 'r')\n                cachedir = re.search('CMAKE_CACHEFILE_DIR:INTERNAL=(.*)',\n                                     cachefile.read()).group(1)\n                cachefile.close()\n                if (cachedir != build_temp):\n                    build_base = pjoin(saved_cwd, build_cmd.build_base)\n                    print(f\"-- Skipping build. Temp build {build_temp} does \"\n                          f\"not match cached dir {cachedir}\")\n                    print(\"---- For a clean build you might want to delete \"\n                          f\"{build_base}.\")\n                    return\n\n            cmake_options = [\n                f'-DCMAKE_INSTALL_PREFIX={install_prefix}',\n                f'-DPYTHON_EXECUTABLE={sys.executable}',\n                f'-DPython3_EXECUTABLE={sys.executable}',\n                f'-DPYARROW_CXXFLAGS={self.cmake_cxxflags}',\n            ]\n\n            def append_cmake_bool(value, varname):\n                cmake_options.append('-D{0}={1}'.format(\n                    varname, 'on' if value else 'off'))\n\n            def append_cmake_component(flag, varname):\n                # only pass this to cmake is the user pass the --with-component\n                # flag to setup.py build_ext\n                if flag is not None:\n                    append_cmake_bool(flag, varname)\n\n            if self.cmake_generator:\n                cmake_options += ['-G', self.cmake_generator]\n\n            append_cmake_component(self.with_cuda, 'PYARROW_CUDA')\n            append_cmake_component(self.with_substrait, 'PYARROW_SUBSTRAIT')\n            append_cmake_component(self.with_flight, 'PYARROW_FLIGHT')\n            append_cmake_component(self.with_gandiva, 'PYARROW_GANDIVA')\n            append_cmake_component(self.with_acero, 'PYARROW_ACERO')\n            append_cmake_component(self.with_dataset, 'PYARROW_DATASET')\n            append_cmake_component(self.with_orc, 'PYARROW_ORC')\n            append_cmake_component(self.with_parquet, 'PYARROW_PARQUET')\n            append_cmake_component(self.with_parquet_encryption,\n                                   'PYARROW_PARQUET_ENCRYPTION')\n            append_cmake_component(self.with_azure, 'PYARROW_AZURE')\n            append_cmake_component(self.with_gcs, 'PYARROW_GCS')\n            append_cmake_component(self.with_s3, 'PYARROW_S3')\n            append_cmake_component(self.with_hdfs, 'PYARROW_HDFS')\n\n            append_cmake_bool(self.bundle_arrow_cpp,\n                              'PYARROW_BUNDLE_ARROW_CPP')\n            append_cmake_bool(self.bundle_cython_cpp,\n                              'PYARROW_BUNDLE_CYTHON_CPP')\n            append_cmake_bool(self.generate_coverage,\n                              'PYARROW_GENERATE_COVERAGE')\n\n            cmake_options.append(\n                f'-DCMAKE_BUILD_TYPE={self.build_type.lower()}')\n\n            extra_cmake_args = shlex.split(self.extra_cmake_args)\n\n            build_tool_args = []\n            if sys.platform == 'win32':\n                if not is_64_bit:\n                    raise RuntimeError('Not supported on 32-bit Windows')\n            else:\n                build_tool_args.append('--')\n                if os.environ.get('PYARROW_BUILD_VERBOSE', '0') == '1':\n                    cmake_options.append('-DCMAKE_VERBOSE_MAKEFILE=ON')\n                parallel = os.environ.get('PYARROW_PARALLEL')\n                if parallel:\n                    build_tool_args.append(f'-j{parallel}')\n\n            # Generate the build files\n            print(\"-- Running cmake for PyArrow\")\n            self.spawn(['cmake'] + extra_cmake_args + cmake_options + [source])\n            print(\"-- Finished cmake for PyArrow\")\n\n            print(\"-- Running cmake --build for PyArrow\")\n            self.spawn(['cmake', '--build', '.', '--config', self.build_type] +\n                       build_tool_args)\n            print(\"-- Finished cmake --build for PyArrow\")\n\n            print(\"-- Running cmake --build --target install for PyArrow\")\n            self.spawn(['cmake', '--build', '.', '--config', self.build_type] +\n                       ['--target', 'install'] + build_tool_args)\n            print(\"-- Finished cmake --build --target install for PyArrow\")\n\n            self._found_names = []\n            for name in self.CYTHON_MODULE_NAMES:\n                built_path = pjoin(install_prefix, name + ext_suffix)\n                if os.path.exists(built_path):\n                    self._found_names.append(name)\n\n    def _get_build_dir(self):\n        # Get the package directory from build_py\n        build_py = self.get_finalized_command('build_py')\n        return build_py.get_package_dir('pyarrow')\n\n    def _get_cmake_ext_path(self, name):\n        # This is the name of the arrow C-extension\n        filename = name + ext_suffix\n        return pjoin(self._get_build_dir(), filename)\n\n    def get_ext_generated_cpp_source(self, name):\n        if sys.platform == 'win32':\n            head, tail = os.path.split(name)\n            return pjoin(head, tail + \".cpp\")\n        else:\n            return pjoin(name + \".cpp\")\n\n    def get_ext_built_api_header(self, name):\n        if sys.platform == 'win32':\n            head, tail = os.path.split(name)\n            return pjoin(head, tail + \"_api.h\")\n        else:\n            return pjoin(name + \"_api.h\")\n\n    def get_names(self):\n        return self._found_names\n\n    def get_outputs(self):\n        # Just the C extensions\n        # regular_exts = _build_ext.get_outputs(self)\n        return [self._get_cmake_ext_path(name)\n                for name in self.get_names()]\n\n\nclass BinaryDistribution(Distribution):\n    def has_ext_modules(foo):\n        return True\n\n\nif strtobool(os.environ.get('PYARROW_INSTALL_TESTS', '1')):\n    packages = find_namespace_packages(include=['pyarrow*'])\n    exclude_package_data = {}\nelse:\n    packages = find_namespace_packages(include=['pyarrow*'],\n                                       exclude=[\"pyarrow.tests*\"])\n    # setuptools adds back importable packages even when excluded.\n    # https://github.com/pypa/setuptools/issues/3260\n    # https://github.com/pypa/setuptools/issues/3340#issuecomment-1219383976\n    exclude_package_data = {\"pyarrow\": [\"tests*\"]}\n\n\nsetup(\n    packages=packages,\n    exclude_package_data=exclude_package_data,\n    distclass=BinaryDistribution,\n    # Dummy extension to trigger build_ext\n    ext_modules=[Extension('__dummy__', sources=[])],\n    cmdclass={\n        'build_ext': build_ext\n    },\n)\n", "python/pyarrow/cuda.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# flake8: noqa\n\n\nfrom pyarrow._cuda import (Context, IpcMemHandle, CudaBuffer,\n                           HostBuffer, BufferReader, BufferWriter,\n                           new_host_buffer,\n                           serialize_record_batch, read_message,\n                           read_record_batch)\n", "python/pyarrow/ipc.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Arrow file and stream reader/writer classes, and other messaging tools\n\nimport os\n\nimport pyarrow as pa\n\nfrom pyarrow.lib import (IpcReadOptions, IpcWriteOptions, ReadStats, WriteStats,  # noqa\n                         Message, MessageReader,\n                         RecordBatchReader, _ReadPandasMixin,\n                         MetadataVersion,\n                         read_message, read_record_batch, read_schema,\n                         read_tensor, write_tensor,\n                         get_record_batch_size, get_tensor_size)\nimport pyarrow.lib as lib\n\n\nclass RecordBatchStreamReader(lib._RecordBatchStreamReader):\n    \"\"\"\n    Reader for the Arrow streaming binary format.\n\n    Parameters\n    ----------\n    source : bytes/buffer-like, pyarrow.NativeFile, or file-like Python object\n        Either an in-memory buffer, or a readable file object.\n        If you want to use memory map use MemoryMappedFile as source.\n    options : pyarrow.ipc.IpcReadOptions\n        Options for IPC deserialization.\n        If None, default values will be used.\n    memory_pool : MemoryPool, default None\n        If None, default memory pool is used.\n    \"\"\"\n\n    def __init__(self, source, *, options=None, memory_pool=None):\n        options = _ensure_default_ipc_read_options(options)\n        self._open(source, options=options, memory_pool=memory_pool)\n\n\n_ipc_writer_class_doc = \"\"\"\\\nParameters\n----------\nsink : str, pyarrow.NativeFile, or file-like Python object\n    Either a file path, or a writable file object.\nschema : pyarrow.Schema\n    The Arrow schema for data to be written to the file.\nuse_legacy_format : bool, default None\n    Deprecated in favor of setting options. Cannot be provided with\n    options.\n\n    If None, False will be used unless this default is overridden by\n    setting the environment variable ARROW_PRE_0_15_IPC_FORMAT=1\noptions : pyarrow.ipc.IpcWriteOptions\n    Options for IPC serialization.\n\n    If None, default values will be used: the legacy format will not\n    be used unless overridden by setting the environment variable\n    ARROW_PRE_0_15_IPC_FORMAT=1, and the V5 metadata version will be\n    used unless overridden by setting the environment variable\n    ARROW_PRE_1_0_METADATA_VERSION=1.\"\"\"\n\n\nclass RecordBatchStreamWriter(lib._RecordBatchStreamWriter):\n    __doc__ = \"\"\"Writer for the Arrow streaming binary format\n\n{}\"\"\".format(_ipc_writer_class_doc)\n\n    def __init__(self, sink, schema, *, use_legacy_format=None, options=None):\n        options = _get_legacy_format_default(use_legacy_format, options)\n        self._open(sink, schema, options=options)\n\n\nclass RecordBatchFileReader(lib._RecordBatchFileReader):\n    \"\"\"\n    Class for reading Arrow record batch data from the Arrow binary file format\n\n    Parameters\n    ----------\n    source : bytes/buffer-like, pyarrow.NativeFile, or file-like Python object\n        Either an in-memory buffer, or a readable file object.\n        If you want to use memory map use MemoryMappedFile as source.\n    footer_offset : int, default None\n        If the file is embedded in some larger file, this is the byte offset to\n        the very end of the file data\n    options : pyarrow.ipc.IpcReadOptions\n        Options for IPC serialization.\n        If None, default values will be used.\n    memory_pool : MemoryPool, default None\n        If None, default memory pool is used.\n    \"\"\"\n\n    def __init__(self, source, footer_offset=None, *, options=None,\n                 memory_pool=None):\n        options = _ensure_default_ipc_read_options(options)\n        self._open(source, footer_offset=footer_offset,\n                   options=options, memory_pool=memory_pool)\n\n\nclass RecordBatchFileWriter(lib._RecordBatchFileWriter):\n\n    __doc__ = \"\"\"Writer to create the Arrow binary file format\n\n{}\"\"\".format(_ipc_writer_class_doc)\n\n    def __init__(self, sink, schema, *, use_legacy_format=None, options=None):\n        options = _get_legacy_format_default(use_legacy_format, options)\n        self._open(sink, schema, options=options)\n\n\ndef _get_legacy_format_default(use_legacy_format, options):\n    if use_legacy_format is not None and options is not None:\n        raise ValueError(\n            \"Can provide at most one of options and use_legacy_format\")\n    elif options:\n        if not isinstance(options, IpcWriteOptions):\n            raise TypeError(\"expected IpcWriteOptions, got {}\"\n                            .format(type(options)))\n        return options\n\n    metadata_version = MetadataVersion.V5\n    if use_legacy_format is None:\n        use_legacy_format = \\\n            bool(int(os.environ.get('ARROW_PRE_0_15_IPC_FORMAT', '0')))\n    if bool(int(os.environ.get('ARROW_PRE_1_0_METADATA_VERSION', '0'))):\n        metadata_version = MetadataVersion.V4\n    return IpcWriteOptions(use_legacy_format=use_legacy_format,\n                           metadata_version=metadata_version)\n\n\ndef _ensure_default_ipc_read_options(options):\n    if options and not isinstance(options, IpcReadOptions):\n        raise TypeError(\n            \"expected IpcReadOptions, got {}\".format(type(options))\n        )\n    return options or IpcReadOptions()\n\n\ndef new_stream(sink, schema, *, use_legacy_format=None, options=None):\n    return RecordBatchStreamWriter(sink, schema,\n                                   use_legacy_format=use_legacy_format,\n                                   options=options)\n\n\nnew_stream.__doc__ = \"\"\"\\\nCreate an Arrow columnar IPC stream writer instance\n\n{}\n\nReturns\n-------\nwriter : RecordBatchStreamWriter\n    A writer for the given sink\n\"\"\".format(_ipc_writer_class_doc)\n\n\ndef open_stream(source, *, options=None, memory_pool=None):\n    \"\"\"\n    Create reader for Arrow streaming format.\n\n    Parameters\n    ----------\n    source : bytes/buffer-like, pyarrow.NativeFile, or file-like Python object\n        Either an in-memory buffer, or a readable file object.\n    options : pyarrow.ipc.IpcReadOptions\n        Options for IPC serialization.\n        If None, default values will be used.\n    memory_pool : MemoryPool, default None\n        If None, default memory pool is used.\n\n    Returns\n    -------\n    reader : RecordBatchStreamReader\n        A reader for the given source\n    \"\"\"\n    return RecordBatchStreamReader(source, options=options,\n                                   memory_pool=memory_pool)\n\n\ndef new_file(sink, schema, *, use_legacy_format=None, options=None):\n    return RecordBatchFileWriter(sink, schema,\n                                 use_legacy_format=use_legacy_format,\n                                 options=options)\n\n\nnew_file.__doc__ = \"\"\"\\\nCreate an Arrow columnar IPC file writer instance\n\n{}\n\nReturns\n-------\nwriter : RecordBatchFileWriter\n    A writer for the given sink\n\"\"\".format(_ipc_writer_class_doc)\n\n\ndef open_file(source, footer_offset=None, *, options=None, memory_pool=None):\n    \"\"\"\n    Create reader for Arrow file format.\n\n    Parameters\n    ----------\n    source : bytes/buffer-like, pyarrow.NativeFile, or file-like Python object\n        Either an in-memory buffer, or a readable file object.\n    footer_offset : int, default None\n        If the file is embedded in some larger file, this is the byte offset to\n        the very end of the file data.\n    options : pyarrow.ipc.IpcReadOptions\n        Options for IPC serialization.\n        If None, default values will be used.\n    memory_pool : MemoryPool, default None\n        If None, default memory pool is used.\n\n    Returns\n    -------\n    reader : RecordBatchFileReader\n        A reader for the given source\n    \"\"\"\n    return RecordBatchFileReader(\n        source, footer_offset=footer_offset,\n        options=options, memory_pool=memory_pool)\n\n\ndef serialize_pandas(df, *, nthreads=None, preserve_index=None):\n    \"\"\"\n    Serialize a pandas DataFrame into a buffer protocol compatible object.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n    nthreads : int, default None\n        Number of threads to use for conversion to Arrow, default all CPUs.\n    preserve_index : bool, default None\n        The default of None will store the index as a column, except for\n        RangeIndex which is stored as metadata only. If True, always\n        preserve the pandas index data as a column. If False, no index\n        information is saved and the result will have a default RangeIndex.\n\n    Returns\n    -------\n    buf : buffer\n        An object compatible with the buffer protocol.\n    \"\"\"\n    batch = pa.RecordBatch.from_pandas(df, nthreads=nthreads,\n                                       preserve_index=preserve_index)\n    sink = pa.BufferOutputStream()\n    with pa.RecordBatchStreamWriter(sink, batch.schema) as writer:\n        writer.write_batch(batch)\n    return sink.getvalue()\n\n\ndef deserialize_pandas(buf, *, use_threads=True):\n    \"\"\"Deserialize a buffer protocol compatible object into a pandas DataFrame.\n\n    Parameters\n    ----------\n    buf : buffer\n        An object compatible with the buffer protocol.\n    use_threads : bool, default True\n        Whether to parallelize the conversion using multiple threads.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The buffer deserialized as pandas DataFrame\n    \"\"\"\n    buffer_reader = pa.BufferReader(buf)\n    with pa.RecordBatchStreamReader(buffer_reader) as reader:\n        table = reader.read_all()\n    return table.to_pandas(use_threads=use_threads)\n", "python/pyarrow/orc.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nfrom numbers import Integral\nimport warnings\n\nfrom pyarrow.lib import Table\nimport pyarrow._orc as _orc\nfrom pyarrow.fs import _resolve_filesystem_and_path\n\n\nclass ORCFile:\n    \"\"\"\n    Reader interface for a single ORC file\n\n    Parameters\n    ----------\n    source : str or pyarrow.NativeFile\n        Readable source. For passing Python file objects or byte buffers,\n        see pyarrow.io.PythonFileInterface or pyarrow.io.BufferReader.\n    \"\"\"\n\n    def __init__(self, source):\n        self.reader = _orc.ORCReader()\n        self.reader.open(source)\n\n    @property\n    def metadata(self):\n        \"\"\"The file metadata, as an arrow KeyValueMetadata\"\"\"\n        return self.reader.metadata()\n\n    @property\n    def schema(self):\n        \"\"\"The file schema, as an arrow schema\"\"\"\n        return self.reader.schema()\n\n    @property\n    def nrows(self):\n        \"\"\"The number of rows in the file\"\"\"\n        return self.reader.nrows()\n\n    @property\n    def nstripes(self):\n        \"\"\"The number of stripes in the file\"\"\"\n        return self.reader.nstripes()\n\n    @property\n    def file_version(self):\n        \"\"\"Format version of the ORC file, must be 0.11 or 0.12\"\"\"\n        return self.reader.file_version()\n\n    @property\n    def software_version(self):\n        \"\"\"Software instance and version that wrote this file\"\"\"\n        return self.reader.software_version()\n\n    @property\n    def compression(self):\n        \"\"\"Compression codec of the file\"\"\"\n        return self.reader.compression()\n\n    @property\n    def compression_size(self):\n        \"\"\"Number of bytes to buffer for the compression codec in the file\"\"\"\n        return self.reader.compression_size()\n\n    @property\n    def writer(self):\n        \"\"\"Name of the writer that wrote this file.\n        If the writer is unknown then its Writer ID\n        (a number) is returned\"\"\"\n        return self.reader.writer()\n\n    @property\n    def writer_version(self):\n        \"\"\"Version of the writer\"\"\"\n        return self.reader.writer_version()\n\n    @property\n    def row_index_stride(self):\n        \"\"\"Number of rows per an entry in the row index or 0\n        if there is no row index\"\"\"\n        return self.reader.row_index_stride()\n\n    @property\n    def nstripe_statistics(self):\n        \"\"\"Number of stripe statistics\"\"\"\n        return self.reader.nstripe_statistics()\n\n    @property\n    def content_length(self):\n        \"\"\"Length of the data stripes in the file in bytes\"\"\"\n        return self.reader.content_length()\n\n    @property\n    def stripe_statistics_length(self):\n        \"\"\"The number of compressed bytes in the file stripe statistics\"\"\"\n        return self.reader.stripe_statistics_length()\n\n    @property\n    def file_footer_length(self):\n        \"\"\"The number of compressed bytes in the file footer\"\"\"\n        return self.reader.file_footer_length()\n\n    @property\n    def file_postscript_length(self):\n        \"\"\"The number of bytes in the file postscript\"\"\"\n        return self.reader.file_postscript_length()\n\n    @property\n    def file_length(self):\n        \"\"\"The number of bytes in the file\"\"\"\n        return self.reader.file_length()\n\n    def _select_names(self, columns=None):\n        if columns is None:\n            return None\n\n        schema = self.schema\n        names = []\n        for col in columns:\n            if isinstance(col, Integral):\n                col = int(col)\n                if 0 <= col < len(schema):\n                    col = schema[col].name\n                    names.append(col)\n                else:\n                    raise ValueError(\"Column indices must be in 0 <= ind < %d,\"\n                                     \" got %d\" % (len(schema), col))\n            else:\n                return columns\n\n        return names\n\n    def read_stripe(self, n, columns=None):\n        \"\"\"Read a single stripe from the file.\n\n        Parameters\n        ----------\n        n : int\n            The stripe index\n        columns : list\n            If not None, only these columns will be read from the stripe. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'\n\n        Returns\n        -------\n        pyarrow.RecordBatch\n            Content of the stripe as a RecordBatch.\n        \"\"\"\n        columns = self._select_names(columns)\n        return self.reader.read_stripe(n, columns=columns)\n\n    def read(self, columns=None):\n        \"\"\"Read the whole file.\n\n        Parameters\n        ----------\n        columns : list\n            If not None, only these columns will be read from the file. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'. Output always follows the\n            ordering of the file and not the `columns` list.\n\n        Returns\n        -------\n        pyarrow.Table\n            Content of the file as a Table.\n        \"\"\"\n        columns = self._select_names(columns)\n        return self.reader.read(columns=columns)\n\n\n_orc_writer_args_docs = \"\"\"file_version : {\"0.11\", \"0.12\"}, default \"0.12\"\n    Determine which ORC file version to use.\n    `Hive 0.11 / ORC v0 <https://orc.apache.org/specification/ORCv0/>`_\n    is the older version\n    while `Hive 0.12 / ORC v1 <https://orc.apache.org/specification/ORCv1/>`_\n    is the newer one.\nbatch_size : int, default 1024\n    Number of rows the ORC writer writes at a time.\nstripe_size : int, default 64 * 1024 * 1024\n    Size of each ORC stripe in bytes.\ncompression : string, default 'uncompressed'\n    The compression codec.\n    Valid values: {'UNCOMPRESSED', 'SNAPPY', 'ZLIB', 'LZ4', 'ZSTD'}\n    Note that LZ0 is currently not supported.\ncompression_block_size : int, default 64 * 1024\n    Size of each compression block in bytes.\ncompression_strategy : string, default 'speed'\n    The compression strategy i.e. speed vs size reduction.\n    Valid values: {'SPEED', 'COMPRESSION'}\nrow_index_stride : int, default 10000\n    The row index stride i.e. the number of rows per\n    an entry in the row index.\npadding_tolerance : double, default 0.0\n    The padding tolerance.\ndictionary_key_size_threshold : double, default 0.0\n    The dictionary key size threshold. 0 to disable dictionary encoding.\n    1 to always enable dictionary encoding.\nbloom_filter_columns : None, set-like or list-like, default None\n    Columns that use the bloom filter.\nbloom_filter_fpp : double, default 0.05\n    Upper limit of the false-positive rate of the bloom filter.\n\"\"\"\n\n\nclass ORCWriter:\n    __doc__ = \"\"\"\nWriter interface for a single ORC file\n\nParameters\n----------\nwhere : str or pyarrow.io.NativeFile\n    Writable target. For passing Python file objects or byte buffers,\n    see pyarrow.io.PythonFileInterface, pyarrow.io.BufferOutputStream\n    or pyarrow.io.FixedSizeBufferWriter.\n{}\n\"\"\".format(_orc_writer_args_docs)\n\n    is_open = False\n\n    def __init__(self, where, *,\n                 file_version='0.12',\n                 batch_size=1024,\n                 stripe_size=64 * 1024 * 1024,\n                 compression='uncompressed',\n                 compression_block_size=65536,\n                 compression_strategy='speed',\n                 row_index_stride=10000,\n                 padding_tolerance=0.0,\n                 dictionary_key_size_threshold=0.0,\n                 bloom_filter_columns=None,\n                 bloom_filter_fpp=0.05,\n                 ):\n        self.writer = _orc.ORCWriter()\n        self.writer.open(\n            where,\n            file_version=file_version,\n            batch_size=batch_size,\n            stripe_size=stripe_size,\n            compression=compression,\n            compression_block_size=compression_block_size,\n            compression_strategy=compression_strategy,\n            row_index_stride=row_index_stride,\n            padding_tolerance=padding_tolerance,\n            dictionary_key_size_threshold=dictionary_key_size_threshold,\n            bloom_filter_columns=bloom_filter_columns,\n            bloom_filter_fpp=bloom_filter_fpp\n        )\n        self.is_open = True\n\n    def __del__(self):\n        self.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n    def write(self, table):\n        \"\"\"\n        Write the table into an ORC file. The schema of the table must\n        be equal to the schema used when opening the ORC file.\n\n        Parameters\n        ----------\n        table : pyarrow.Table\n            The table to be written into the ORC file\n        \"\"\"\n        assert self.is_open\n        self.writer.write(table)\n\n    def close(self):\n        \"\"\"\n        Close the ORC file\n        \"\"\"\n        if self.is_open:\n            self.writer.close()\n            self.is_open = False\n\n\ndef read_table(source, columns=None, filesystem=None):\n    filesystem, path = _resolve_filesystem_and_path(source, filesystem)\n    if filesystem is not None:\n        source = filesystem.open_input_file(path)\n\n    if columns is not None and len(columns) == 0:\n        result = ORCFile(source).read().select(columns)\n    else:\n        result = ORCFile(source).read(columns=columns)\n\n    return result\n\n\nread_table.__doc__ = \"\"\"\nRead a Table from an ORC file.\n\nParameters\n----------\nsource : str, pyarrow.NativeFile, or file-like object\n    If a string passed, can be a single file name. For file-like objects,\n    only read a single file. Use pyarrow.BufferReader to read a file\n    contained in a bytes or buffer-like object.\ncolumns : list\n    If not None, only these columns will be read from the file. A column\n    name may be a prefix of a nested field, e.g. 'a' will select 'a.b',\n    'a.c', and 'a.d.e'. Output always follows the ordering of the file and\n    not the `columns` list. If empty, no columns will be read. Note\n    that the table will still have the correct num_rows set despite having\n    no columns.\nfilesystem : FileSystem, default None\n    If nothing passed, will be inferred based on path.\n    Path will try to be found in the local on-disk filesystem otherwise\n    it will be parsed as an URI to determine the filesystem.\n\"\"\"\n\n\ndef write_table(table, where, *,\n                file_version='0.12',\n                batch_size=1024,\n                stripe_size=64 * 1024 * 1024,\n                compression='uncompressed',\n                compression_block_size=65536,\n                compression_strategy='speed',\n                row_index_stride=10000,\n                padding_tolerance=0.0,\n                dictionary_key_size_threshold=0.0,\n                bloom_filter_columns=None,\n                bloom_filter_fpp=0.05):\n    if isinstance(where, Table):\n        warnings.warn(\n            \"The order of the arguments has changed. Pass as \"\n            \"'write_table(table, where)' instead. The old order will raise \"\n            \"an error in the future.\", FutureWarning, stacklevel=2\n        )\n        table, where = where, table\n    with ORCWriter(\n        where,\n        file_version=file_version,\n        batch_size=batch_size,\n        stripe_size=stripe_size,\n        compression=compression,\n        compression_block_size=compression_block_size,\n        compression_strategy=compression_strategy,\n        row_index_stride=row_index_stride,\n        padding_tolerance=padding_tolerance,\n        dictionary_key_size_threshold=dictionary_key_size_threshold,\n        bloom_filter_columns=bloom_filter_columns,\n        bloom_filter_fpp=bloom_filter_fpp\n    ) as writer:\n        writer.write(table)\n\n\nwrite_table.__doc__ = \"\"\"\nWrite a table into an ORC file.\n\nParameters\n----------\ntable : pyarrow.lib.Table\n    The table to be written into the ORC file\nwhere : str or pyarrow.io.NativeFile\n    Writable target. For passing Python file objects or byte buffers,\n    see pyarrow.io.PythonFileInterface, pyarrow.io.BufferOutputStream\n    or pyarrow.io.FixedSizeBufferWriter.\n{}\n\"\"\".format(_orc_writer_args_docs)\n", "python/pyarrow/benchmark.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# flake8: noqa\n\n\nfrom pyarrow.lib import benchmark_PandasObjectIsNull\n", "python/pyarrow/flight.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\ntry:\n    from pyarrow._flight import (  # noqa:F401\n        connect,\n        Action,\n        ActionType,\n        BasicAuth,\n        CallInfo,\n        CertKeyPair,\n        ClientAuthHandler,\n        ClientMiddleware,\n        ClientMiddlewareFactory,\n        DescriptorType,\n        FlightCallOptions,\n        FlightCancelledError,\n        FlightClient,\n        FlightDataStream,\n        FlightDescriptor,\n        FlightEndpoint,\n        FlightError,\n        FlightInfo,\n        FlightInternalError,\n        FlightMetadataReader,\n        FlightMetadataWriter,\n        FlightMethod,\n        FlightServerBase,\n        FlightServerError,\n        FlightStreamChunk,\n        FlightStreamReader,\n        FlightStreamWriter,\n        FlightTimedOutError,\n        FlightUnauthenticatedError,\n        FlightUnauthorizedError,\n        FlightUnavailableError,\n        FlightWriteSizeExceededError,\n        GeneratorStream,\n        Location,\n        MetadataRecordBatchReader,\n        MetadataRecordBatchWriter,\n        RecordBatchStream,\n        Result,\n        SchemaResult,\n        ServerAuthHandler,\n        ServerCallContext,\n        ServerMiddleware,\n        ServerMiddlewareFactory,\n        Ticket,\n        TracingServerMiddlewareFactory,\n    )\nexcept ImportError as exc:\n    raise ImportError(\n        f\"The pyarrow installation is not built with support for 'flight' ({str(exc)})\"\n    ) from None\n", "python/pyarrow/feather.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nimport os\n\nfrom pyarrow.pandas_compat import _pandas_api  # noqa\nfrom pyarrow.lib import (Codec, Table,  # noqa\n                         concat_tables, schema)\nimport pyarrow.lib as ext\nfrom pyarrow import _feather\nfrom pyarrow._feather import FeatherError  # noqa: F401\n\n\nclass FeatherDataset:\n    \"\"\"\n    Encapsulates details of reading a list of Feather files.\n\n    Parameters\n    ----------\n    path_or_paths : List[str]\n        A list of file names\n    validate_schema : bool, default True\n        Check that individual file schemas are all the same / compatible\n    \"\"\"\n\n    def __init__(self, path_or_paths, validate_schema=True):\n        self.paths = path_or_paths\n        self.validate_schema = validate_schema\n\n    def read_table(self, columns=None):\n        \"\"\"\n        Read multiple feather files as a single pyarrow.Table\n\n        Parameters\n        ----------\n        columns : List[str]\n            Names of columns to read from the file\n\n        Returns\n        -------\n        pyarrow.Table\n            Content of the file as a table (of columns)\n        \"\"\"\n        _fil = read_table(self.paths[0], columns=columns)\n        self._tables = [_fil]\n        self.schema = _fil.schema\n\n        for path in self.paths[1:]:\n            table = read_table(path, columns=columns)\n            if self.validate_schema:\n                self.validate_schemas(path, table)\n            self._tables.append(table)\n        return concat_tables(self._tables)\n\n    def validate_schemas(self, piece, table):\n        if not self.schema.equals(table.schema):\n            raise ValueError('Schema in {!s} was different. \\n'\n                             '{!s}\\n\\nvs\\n\\n{!s}'\n                             .format(piece, self.schema,\n                                     table.schema))\n\n    def read_pandas(self, columns=None, use_threads=True):\n        \"\"\"\n        Read multiple Parquet files as a single pandas DataFrame\n\n        Parameters\n        ----------\n        columns : List[str]\n            Names of columns to read from the file\n        use_threads : bool, default True\n            Use multiple threads when converting to pandas\n\n        Returns\n        -------\n        pandas.DataFrame\n            Content of the file as a pandas DataFrame (of columns)\n        \"\"\"\n        return self.read_table(columns=columns).to_pandas(\n            use_threads=use_threads)\n\n\ndef check_chunked_overflow(name, col):\n    if col.num_chunks == 1:\n        return\n\n    if col.type in (ext.binary(), ext.string()):\n        raise ValueError(\"Column '{}' exceeds 2GB maximum capacity of \"\n                         \"a Feather binary column. This restriction may be \"\n                         \"lifted in the future\".format(name))\n    else:\n        # TODO(wesm): Not sure when else this might be reached\n        raise ValueError(\"Column '{}' of type {} was chunked on conversion \"\n                         \"to Arrow and cannot be currently written to \"\n                         \"Feather format\".format(name, str(col.type)))\n\n\n_FEATHER_SUPPORTED_CODECS = {'lz4', 'zstd', 'uncompressed'}\n\n\ndef write_feather(df, dest, compression=None, compression_level=None,\n                  chunksize=None, version=2):\n    \"\"\"\n    Write a pandas.DataFrame to Feather format.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pyarrow.Table\n        Data to write out as Feather format.\n    dest : str\n        Local destination path.\n    compression : string, default None\n        Can be one of {\"zstd\", \"lz4\", \"uncompressed\"}. The default of None uses\n        LZ4 for V2 files if it is available, otherwise uncompressed.\n    compression_level : int, default None\n        Use a compression level particular to the chosen compressor. If None\n        use the default compression level\n    chunksize : int, default None\n        For V2 files, the internal maximum size of Arrow RecordBatch chunks\n        when writing the Arrow IPC file format. None means use the default,\n        which is currently 64K\n    version : int, default 2\n        Feather file version. Version 2 is the current. Version 1 is the more\n        limited legacy format\n    \"\"\"\n    if _pandas_api.have_pandas:\n        if (_pandas_api.has_sparse and\n                isinstance(df, _pandas_api.pd.SparseDataFrame)):\n            df = df.to_dense()\n\n    if _pandas_api.is_data_frame(df):\n        # Feather v1 creates a new column in the resultant Table to\n        # store index information if index type is not RangeIndex\n\n        if version == 1:\n            preserve_index = False\n        elif version == 2:\n            preserve_index = None\n        else:\n            raise ValueError(\"Version value should either be 1 or 2\")\n\n        table = Table.from_pandas(df, preserve_index=preserve_index)\n\n        if version == 1:\n            # Version 1 does not chunking\n            for i, name in enumerate(table.schema.names):\n                col = table[i]\n                check_chunked_overflow(name, col)\n    else:\n        table = df\n\n    if version == 1:\n        if len(table.column_names) > len(set(table.column_names)):\n            raise ValueError(\"cannot serialize duplicate column names\")\n\n        if compression is not None:\n            raise ValueError(\"Feather V1 files do not support compression \"\n                             \"option\")\n\n        if chunksize is not None:\n            raise ValueError(\"Feather V1 files do not support chunksize \"\n                             \"option\")\n    else:\n        if compression is None and Codec.is_available('lz4_frame'):\n            compression = 'lz4'\n        elif (compression is not None and\n              compression not in _FEATHER_SUPPORTED_CODECS):\n            raise ValueError('compression=\"{}\" not supported, must be '\n                             'one of {}'.format(compression,\n                                                _FEATHER_SUPPORTED_CODECS))\n\n    try:\n        _feather.write_feather(table, dest, compression=compression,\n                               compression_level=compression_level,\n                               chunksize=chunksize, version=version)\n    except Exception:\n        if isinstance(dest, str):\n            try:\n                os.remove(dest)\n            except os.error:\n                pass\n        raise\n\n\ndef read_feather(source, columns=None, use_threads=True,\n                 memory_map=False, **kwargs):\n    \"\"\"\n    Read a pandas.DataFrame from Feather format. To read as pyarrow.Table use\n    feather.read_table.\n\n    Parameters\n    ----------\n    source : str file path, or file-like object\n        You can use MemoryMappedFile as source, for explicitly use memory map.\n    columns : sequence, optional\n        Only read a specific set of columns. If not provided, all columns are\n        read.\n    use_threads : bool, default True\n        Whether to parallelize reading using multiple threads. If false the\n        restriction is used in the conversion to Pandas as well as in the\n        reading from Feather format.\n    memory_map : boolean, default False\n        Use memory mapping when opening file on disk, when source is a str.\n    **kwargs\n        Additional keyword arguments passed on to `pyarrow.Table.to_pandas`.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The contents of the Feather file as a pandas.DataFrame\n    \"\"\"\n    return (read_table(\n        source, columns=columns, memory_map=memory_map,\n        use_threads=use_threads).to_pandas(use_threads=use_threads, **kwargs))\n\n\ndef read_table(source, columns=None, memory_map=False, use_threads=True):\n    \"\"\"\n    Read a pyarrow.Table from Feather format\n\n    Parameters\n    ----------\n    source : str file path, or file-like object\n        You can use MemoryMappedFile as source, for explicitly use memory map.\n    columns : sequence, optional\n        Only read a specific set of columns. If not provided, all columns are\n        read.\n    memory_map : boolean, default False\n        Use memory mapping when opening file on disk, when source is a str\n    use_threads : bool, default True\n        Whether to parallelize reading using multiple threads.\n\n    Returns\n    -------\n    table : pyarrow.Table\n        The contents of the Feather file as a pyarrow.Table\n    \"\"\"\n    reader = _feather.FeatherReader(\n        source, use_memory_map=memory_map, use_threads=use_threads)\n\n    if columns is None:\n        return reader.read()\n\n    column_types = [type(column) for column in columns]\n    if all(map(lambda t: t == int, column_types)):\n        table = reader.read_indices(columns)\n    elif all(map(lambda t: t == str, column_types)):\n        table = reader.read_names(columns)\n    else:\n        column_type_names = [t.__name__ for t in column_types]\n        raise TypeError(\"Columns must be indices or names. \"\n                        \"Got columns {} of types {}\"\n                        .format(columns, column_type_names))\n\n    # Feather v1 already respects the column selection\n    if reader.version < 3:\n        return table\n    # Feather v2 reads with sorted / deduplicated selection\n    elif sorted(set(columns)) == columns:\n        return table\n    else:\n        # follow exact order / selection of names\n        return table.select(columns)\n", "python/pyarrow/fs.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"\nFileSystem abstraction to interact with various local and remote filesystems.\n\"\"\"\n\nfrom pyarrow.util import _is_path_like, _stringify_path\n\nfrom pyarrow._fs import (  # noqa\n    FileSelector,\n    FileType,\n    FileInfo,\n    FileSystem,\n    LocalFileSystem,\n    SubTreeFileSystem,\n    _MockFileSystem,\n    FileSystemHandler,\n    PyFileSystem,\n    _copy_files,\n    _copy_files_selector,\n)\n\n# For backward compatibility.\nFileStats = FileInfo\n\n_not_imported = []\ntry:\n    from pyarrow._azurefs import AzureFileSystem  # noqa\nexcept ImportError:\n    _not_imported.append(\"AzureFileSystem\")\n\ntry:\n    from pyarrow._hdfs import HadoopFileSystem  # noqa\nexcept ImportError:\n    _not_imported.append(\"HadoopFileSystem\")\n\ntry:\n    from pyarrow._gcsfs import GcsFileSystem  # noqa\nexcept ImportError:\n    _not_imported.append(\"GcsFileSystem\")\n\ntry:\n    from pyarrow._s3fs import (  # noqa\n        AwsDefaultS3RetryStrategy, AwsStandardS3RetryStrategy,\n        S3FileSystem, S3LogLevel, S3RetryStrategy, ensure_s3_initialized,\n        finalize_s3, ensure_s3_finalized, initialize_s3, resolve_s3_region)\nexcept ImportError:\n    _not_imported.append(\"S3FileSystem\")\nelse:\n    # GH-38364: we don't initialize S3 eagerly as that could lead\n    # to crashes at shutdown even when S3 isn't used.\n    # Instead, S3 is initialized lazily using `ensure_s3_initialized`\n    # in assorted places.\n    import atexit\n    atexit.register(ensure_s3_finalized)\n\n\ndef __getattr__(name):\n    if name in _not_imported:\n        raise ImportError(\n            \"The pyarrow installation is not built with support for \"\n            \"'{0}'\".format(name)\n        )\n\n    raise AttributeError(\n        \"module 'pyarrow.fs' has no attribute '{0}'\".format(name)\n    )\n\n\ndef _filesystem_from_str(uri):\n    # instantiate the file system from an uri, if the uri has a path\n    # component then it will be treated as a path prefix\n    filesystem, prefix = FileSystem.from_uri(uri)\n    prefix = filesystem.normalize_path(prefix)\n    if prefix:\n        # validate that the prefix is pointing to a directory\n        prefix_info = filesystem.get_file_info([prefix])[0]\n        if prefix_info.type != FileType.Directory:\n            raise ValueError(\n                \"The path component of the filesystem URI must point to a \"\n                \"directory but it has a type: `{}`. The path component \"\n                \"is `{}` and the given filesystem URI is `{}`\".format(\n                    prefix_info.type.name, prefix_info.path, uri\n                )\n            )\n        filesystem = SubTreeFileSystem(prefix, filesystem)\n    return filesystem\n\n\ndef _ensure_filesystem(filesystem, *, use_mmap=False):\n    if isinstance(filesystem, FileSystem):\n        return filesystem\n    elif isinstance(filesystem, str):\n        if use_mmap:\n            raise ValueError(\n                \"Specifying to use memory mapping not supported for \"\n                \"filesystem specified as an URI string\"\n            )\n        return _filesystem_from_str(filesystem)\n\n    # handle fsspec-compatible filesystems\n    try:\n        import fsspec\n    except ImportError:\n        pass\n    else:\n        if isinstance(filesystem, fsspec.AbstractFileSystem):\n            if type(filesystem).__name__ == 'LocalFileSystem':\n                # In case its a simple LocalFileSystem, use native arrow one\n                return LocalFileSystem(use_mmap=use_mmap)\n            return PyFileSystem(FSSpecHandler(filesystem))\n\n    raise TypeError(\n        \"Unrecognized filesystem: {}. `filesystem` argument must be a \"\n        \"FileSystem instance or a valid file system URI'\".format(\n            type(filesystem))\n    )\n\n\ndef _resolve_filesystem_and_path(path, filesystem=None, *, memory_map=False):\n    \"\"\"\n    Return filesystem/path from path which could be an URI or a plain\n    filesystem path.\n    \"\"\"\n    if not _is_path_like(path):\n        if filesystem is not None:\n            raise ValueError(\n                \"'filesystem' passed but the specified path is file-like, so\"\n                \" there is nothing to open with 'filesystem'.\"\n            )\n        return filesystem, path\n\n    if filesystem is not None:\n        filesystem = _ensure_filesystem(filesystem, use_mmap=memory_map)\n        if isinstance(filesystem, LocalFileSystem):\n            path = _stringify_path(path)\n        elif not isinstance(path, str):\n            raise TypeError(\n                \"Expected string path; path-like objects are only allowed \"\n                \"with a local filesystem\"\n            )\n        path = filesystem.normalize_path(path)\n        return filesystem, path\n\n    path = _stringify_path(path)\n\n    # if filesystem is not given, try to automatically determine one\n    # first check if the file exists as a local (relative) file path\n    # if not then try to parse the path as an URI\n    filesystem = LocalFileSystem(use_mmap=memory_map)\n\n    try:\n        file_info = filesystem.get_file_info(path)\n    except ValueError:  # ValueError means path is likely an URI\n        file_info = None\n        exists_locally = False\n    else:\n        exists_locally = (file_info.type != FileType.NotFound)\n\n    # if the file or directory doesn't exists locally, then assume that\n    # the path is an URI describing the file system as well\n    if not exists_locally:\n        try:\n            filesystem, path = FileSystem.from_uri(path)\n        except ValueError as e:\n            # neither an URI nor a locally existing path, so assume that\n            # local path was given and propagate a nicer file not found error\n            # instead of a more confusing scheme parsing error\n            if \"empty scheme\" not in str(e) \\\n                    and \"Cannot parse URI\" not in str(e):\n                raise\n    else:\n        path = filesystem.normalize_path(path)\n\n    return filesystem, path\n\n\ndef copy_files(source, destination,\n               source_filesystem=None, destination_filesystem=None,\n               *, chunk_size=1024*1024, use_threads=True):\n    \"\"\"\n    Copy files between FileSystems.\n\n    This functions allows you to recursively copy directories of files from\n    one file system to another, such as from S3 to your local machine.\n\n    Parameters\n    ----------\n    source : string\n        Source file path or URI to a single file or directory.\n        If a directory, files will be copied recursively from this path.\n    destination : string\n        Destination file path or URI. If `source` is a file, `destination`\n        is also interpreted as the destination file (not directory).\n        Directories will be created as necessary.\n    source_filesystem : FileSystem, optional\n        Source filesystem, needs to be specified if `source` is not a URI,\n        otherwise inferred.\n    destination_filesystem : FileSystem, optional\n        Destination filesystem, needs to be specified if `destination` is not\n        a URI, otherwise inferred.\n    chunk_size : int, default 1MB\n        The maximum size of block to read before flushing to the\n        destination file. A larger chunk_size will use more memory while\n        copying but may help accommodate high latency FileSystems.\n    use_threads : bool, default True\n        Whether to use multiple threads to accelerate copying.\n\n    Examples\n    --------\n    Inspect an S3 bucket's files:\n\n    >>> s3, path = fs.FileSystem.from_uri(\n    ...            \"s3://registry.opendata.aws/roda/ndjson/\")\n    >>> selector = fs.FileSelector(path)\n    >>> s3.get_file_info(selector)\n    [<FileInfo for 'registry.opendata.aws/roda/ndjson/index.ndjson':...]\n\n    Copy one file from S3 bucket to a local directory:\n\n    >>> fs.copy_files(\"s3://registry.opendata.aws/roda/ndjson/index.ndjson\",\n    ...               \"file:///{}/index_copy.ndjson\".format(local_path))\n\n    >>> fs.LocalFileSystem().get_file_info(str(local_path)+\n    ...                                    '/index_copy.ndjson')\n    <FileInfo for '.../index_copy.ndjson': type=FileType.File, size=...>\n\n    Copy file using a FileSystem object:\n\n    >>> fs.copy_files(\"registry.opendata.aws/roda/ndjson/index.ndjson\",\n    ...               \"file:///{}/index_copy.ndjson\".format(local_path),\n    ...               source_filesystem=fs.S3FileSystem())\n    \"\"\"\n    source_fs, source_path = _resolve_filesystem_and_path(\n        source, source_filesystem\n    )\n    destination_fs, destination_path = _resolve_filesystem_and_path(\n        destination, destination_filesystem\n    )\n\n    file_info = source_fs.get_file_info(source_path)\n    if file_info.type == FileType.Directory:\n        source_sel = FileSelector(source_path, recursive=True)\n        _copy_files_selector(source_fs, source_sel,\n                             destination_fs, destination_path,\n                             chunk_size, use_threads)\n    else:\n        _copy_files(source_fs, source_path,\n                    destination_fs, destination_path,\n                    chunk_size, use_threads)\n\n\nclass FSSpecHandler(FileSystemHandler):\n    \"\"\"\n    Handler for fsspec-based Python filesystems.\n\n    https://filesystem-spec.readthedocs.io/en/latest/index.html\n\n    Parameters\n    ----------\n    fs : FSSpec-compliant filesystem instance\n\n    Examples\n    --------\n    >>> PyFileSystem(FSSpecHandler(fsspec_fs)) # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, fs):\n        self.fs = fs\n\n    def __eq__(self, other):\n        if isinstance(other, FSSpecHandler):\n            return self.fs == other.fs\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, FSSpecHandler):\n            return self.fs != other.fs\n        return NotImplemented\n\n    def get_type_name(self):\n        protocol = self.fs.protocol\n        if isinstance(protocol, list):\n            protocol = protocol[0]\n        return \"fsspec+{0}\".format(protocol)\n\n    def normalize_path(self, path):\n        return path\n\n    @staticmethod\n    def _create_file_info(path, info):\n        size = info[\"size\"]\n        if info[\"type\"] == \"file\":\n            ftype = FileType.File\n        elif info[\"type\"] == \"directory\":\n            ftype = FileType.Directory\n            # some fsspec filesystems include a file size for directories\n            size = None\n        else:\n            ftype = FileType.Unknown\n        return FileInfo(path, ftype, size=size, mtime=info.get(\"mtime\", None))\n\n    def get_file_info(self, paths):\n        infos = []\n        for path in paths:\n            try:\n                info = self.fs.info(path)\n            except FileNotFoundError:\n                infos.append(FileInfo(path, FileType.NotFound))\n            else:\n                infos.append(self._create_file_info(path, info))\n        return infos\n\n    def get_file_info_selector(self, selector):\n        if not self.fs.isdir(selector.base_dir):\n            if self.fs.exists(selector.base_dir):\n                raise NotADirectoryError(selector.base_dir)\n            else:\n                if selector.allow_not_found:\n                    return []\n                else:\n                    raise FileNotFoundError(selector.base_dir)\n\n        if selector.recursive:\n            maxdepth = None\n        else:\n            maxdepth = 1\n\n        infos = []\n        selected_files = self.fs.find(\n            selector.base_dir, maxdepth=maxdepth, withdirs=True, detail=True\n        )\n        for path, info in selected_files.items():\n            _path = path.strip(\"/\")\n            base_dir = selector.base_dir.strip(\"/\")\n            # Need to exclude base directory from selected files if present\n            # (fsspec filesystems, see GH-37555)\n            if _path != base_dir:\n                infos.append(self._create_file_info(path, info))\n\n        return infos\n\n    def create_dir(self, path, recursive):\n        # mkdir also raises FileNotFoundError when base directory is not found\n        try:\n            self.fs.mkdir(path, create_parents=recursive)\n        except FileExistsError:\n            pass\n\n    def delete_dir(self, path):\n        self.fs.rm(path, recursive=True)\n\n    def _delete_dir_contents(self, path, missing_dir_ok):\n        try:\n            subpaths = self.fs.listdir(path, detail=False)\n        except FileNotFoundError:\n            if missing_dir_ok:\n                return\n            raise\n        for subpath in subpaths:\n            if self.fs.isdir(subpath):\n                self.fs.rm(subpath, recursive=True)\n            elif self.fs.isfile(subpath):\n                self.fs.rm(subpath)\n\n    def delete_dir_contents(self, path, missing_dir_ok):\n        if path.strip(\"/\") == \"\":\n            raise ValueError(\n                \"delete_dir_contents called on path '\", path, \"'\")\n        self._delete_dir_contents(path, missing_dir_ok)\n\n    def delete_root_dir_contents(self):\n        self._delete_dir_contents(\"/\")\n\n    def delete_file(self, path):\n        # fs.rm correctly raises IsADirectoryError when `path` is a directory\n        # instead of a file and `recursive` is not set to True\n        if not self.fs.exists(path):\n            raise FileNotFoundError(path)\n        self.fs.rm(path)\n\n    def move(self, src, dest):\n        self.fs.mv(src, dest, recursive=True)\n\n    def copy_file(self, src, dest):\n        # fs.copy correctly raises IsADirectoryError when `src` is a directory\n        # instead of a file\n        self.fs.copy(src, dest)\n\n    # TODO can we read/pass metadata (e.g. Content-Type) in the methods below?\n\n    def open_input_stream(self, path):\n        from pyarrow import PythonFile\n\n        if not self.fs.isfile(path):\n            raise FileNotFoundError(path)\n\n        return PythonFile(self.fs.open(path, mode=\"rb\"), mode=\"r\")\n\n    def open_input_file(self, path):\n        from pyarrow import PythonFile\n\n        if not self.fs.isfile(path):\n            raise FileNotFoundError(path)\n\n        return PythonFile(self.fs.open(path, mode=\"rb\"), mode=\"r\")\n\n    def open_output_stream(self, path, metadata):\n        from pyarrow import PythonFile\n\n        return PythonFile(self.fs.open(path, mode=\"wb\"), mode=\"w\")\n\n    def open_append_stream(self, path, metadata):\n        from pyarrow import PythonFile\n\n        return PythonFile(self.fs.open(path, mode=\"ab\"), mode=\"w\")\n", "python/pyarrow/compute.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom pyarrow._compute import (  # noqa\n    Function,\n    FunctionOptions,\n    FunctionRegistry,\n    HashAggregateFunction,\n    HashAggregateKernel,\n    Kernel,\n    ScalarAggregateFunction,\n    ScalarAggregateKernel,\n    ScalarFunction,\n    ScalarKernel,\n    VectorFunction,\n    VectorKernel,\n    # Option classes\n    ArraySortOptions,\n    AssumeTimezoneOptions,\n    CastOptions,\n    CountOptions,\n    CumulativeOptions,\n    CumulativeSumOptions,\n    DayOfWeekOptions,\n    DictionaryEncodeOptions,\n    RunEndEncodeOptions,\n    ElementWiseAggregateOptions,\n    ExtractRegexOptions,\n    FilterOptions,\n    IndexOptions,\n    JoinOptions,\n    ListSliceOptions,\n    ListFlattenOptions,\n    MakeStructOptions,\n    MapLookupOptions,\n    MatchSubstringOptions,\n    ModeOptions,\n    NullOptions,\n    PadOptions,\n    PairwiseOptions,\n    PartitionNthOptions,\n    QuantileOptions,\n    RandomOptions,\n    RankOptions,\n    ReplaceSliceOptions,\n    ReplaceSubstringOptions,\n    RoundBinaryOptions,\n    RoundOptions,\n    RoundTemporalOptions,\n    RoundToMultipleOptions,\n    ScalarAggregateOptions,\n    SelectKOptions,\n    SetLookupOptions,\n    SliceOptions,\n    SortOptions,\n    SplitOptions,\n    SplitPatternOptions,\n    StrftimeOptions,\n    StrptimeOptions,\n    StructFieldOptions,\n    TakeOptions,\n    TDigestOptions,\n    TrimOptions,\n    Utf8NormalizeOptions,\n    VarianceOptions,\n    WeekOptions,\n    # Functions\n    call_function,\n    function_registry,\n    get_function,\n    list_functions,\n    # Udf\n    call_tabular_function,\n    register_scalar_function,\n    register_tabular_function,\n    register_aggregate_function,\n    register_vector_function,\n    UdfContext,\n    # Expressions\n    Expression,\n)\n\nfrom collections import namedtuple\nimport inspect\nfrom textwrap import dedent\nimport warnings\n\nimport pyarrow as pa\nfrom pyarrow import _compute_docstrings\nfrom pyarrow.vendored import docscrape\n\n\ndef _get_arg_names(func):\n    return func._doc.arg_names\n\n\n_OptionsClassDoc = namedtuple('_OptionsClassDoc', ('params',))\n\n\ndef _scrape_options_class_doc(options_class):\n    if not options_class.__doc__:\n        return None\n    doc = docscrape.NumpyDocString(options_class.__doc__)\n    return _OptionsClassDoc(doc['Parameters'])\n\n\ndef _decorate_compute_function(wrapper, exposed_name, func, options_class):\n    # Decorate the given compute function wrapper with useful metadata\n    # and documentation.\n    cpp_doc = func._doc\n\n    wrapper.__arrow_compute_function__ = dict(\n        name=func.name,\n        arity=func.arity,\n        options_class=cpp_doc.options_class,\n        options_required=cpp_doc.options_required)\n    wrapper.__name__ = exposed_name\n    wrapper.__qualname__ = exposed_name\n\n    doc_pieces = []\n\n    # 1. One-line summary\n    summary = cpp_doc.summary\n    if not summary:\n        arg_str = \"arguments\" if func.arity > 1 else \"argument\"\n        summary = (\"Call compute function {!r} with the given {}\"\n                   .format(func.name, arg_str))\n\n    doc_pieces.append(f\"{summary}.\\n\\n\")\n\n    # 2. Multi-line description\n    description = cpp_doc.description\n    if description:\n        doc_pieces.append(f\"{description}\\n\\n\")\n\n    doc_addition = _compute_docstrings.function_doc_additions.get(func.name)\n\n    # 3. Parameter description\n    doc_pieces.append(dedent(\"\"\"\\\n        Parameters\n        ----------\n        \"\"\"))\n\n    # 3a. Compute function parameters\n    arg_names = _get_arg_names(func)\n    for arg_name in arg_names:\n        if func.kind in ('vector', 'scalar_aggregate'):\n            arg_type = 'Array-like'\n        else:\n            arg_type = 'Array-like or scalar-like'\n        doc_pieces.append(f\"{arg_name} : {arg_type}\\n\")\n        doc_pieces.append(\"    Argument to compute function.\\n\")\n\n    # 3b. Compute function option values\n    if options_class is not None:\n        options_class_doc = _scrape_options_class_doc(options_class)\n        if options_class_doc:\n            for p in options_class_doc.params:\n                doc_pieces.append(f\"{p.name} : {p.type}\\n\")\n                for s in p.desc:\n                    doc_pieces.append(f\"    {s}\\n\")\n        else:\n            warnings.warn(f\"Options class {options_class.__name__} \"\n                          f\"does not have a docstring\", RuntimeWarning)\n            options_sig = inspect.signature(options_class)\n            for p in options_sig.parameters.values():\n                doc_pieces.append(dedent(\"\"\"\\\n                {0} : optional\n                    Parameter for {1} constructor. Either `options`\n                    or `{0}` can be passed, but not both at the same time.\n                \"\"\".format(p.name, options_class.__name__)))\n        doc_pieces.append(dedent(f\"\"\"\\\n            options : pyarrow.compute.{options_class.__name__}, optional\n                Alternative way of passing options.\n            \"\"\"))\n\n    doc_pieces.append(dedent(\"\"\"\\\n        memory_pool : pyarrow.MemoryPool, optional\n            If not passed, will allocate memory from the default memory pool.\n        \"\"\"))\n\n    # 4. Custom addition (e.g. examples)\n    if doc_addition is not None:\n        doc_pieces.append(\"\\n{}\\n\".format(dedent(doc_addition).strip(\"\\n\")))\n\n    wrapper.__doc__ = \"\".join(doc_pieces)\n    return wrapper\n\n\ndef _get_options_class(func):\n    class_name = func._doc.options_class\n    if not class_name:\n        return None\n    try:\n        return globals()[class_name]\n    except KeyError:\n        warnings.warn(\"Python binding for {} not exposed\"\n                      .format(class_name), RuntimeWarning)\n        return None\n\n\ndef _handle_options(name, options_class, options, args, kwargs):\n    if args or kwargs:\n        if options is not None:\n            raise TypeError(\n                \"Function {!r} called with both an 'options' argument \"\n                \"and additional arguments\"\n                .format(name))\n        return options_class(*args, **kwargs)\n\n    if options is not None:\n        if isinstance(options, dict):\n            return options_class(**options)\n        elif isinstance(options, options_class):\n            return options\n        raise TypeError(\n            \"Function {!r} expected a {} parameter, got {}\"\n            .format(name, options_class, type(options)))\n\n    return None\n\n\ndef _make_generic_wrapper(func_name, func, options_class, arity):\n    if options_class is None:\n        def wrapper(*args, memory_pool=None):\n            if arity is not Ellipsis and len(args) != arity:\n                raise TypeError(\n                    f\"{func_name} takes {arity} positional argument(s), \"\n                    f\"but {len(args)} were given\"\n                )\n            if args and isinstance(args[0], Expression):\n                return Expression._call(func_name, list(args))\n            return func.call(args, None, memory_pool)\n    else:\n        def wrapper(*args, memory_pool=None, options=None, **kwargs):\n            if arity is not Ellipsis:\n                if len(args) < arity:\n                    raise TypeError(\n                        f\"{func_name} takes {arity} positional argument(s), \"\n                        f\"but {len(args)} were given\"\n                    )\n                option_args = args[arity:]\n                args = args[:arity]\n            else:\n                option_args = ()\n            options = _handle_options(func_name, options_class, options,\n                                      option_args, kwargs)\n            if args and isinstance(args[0], Expression):\n                return Expression._call(func_name, list(args), options)\n            return func.call(args, options, memory_pool)\n    return wrapper\n\n\ndef _make_signature(arg_names, var_arg_names, options_class):\n    from inspect import Parameter\n    params = []\n    for name in arg_names:\n        params.append(Parameter(name, Parameter.POSITIONAL_ONLY))\n    for name in var_arg_names:\n        params.append(Parameter(name, Parameter.VAR_POSITIONAL))\n    if options_class is not None:\n        options_sig = inspect.signature(options_class)\n        for p in options_sig.parameters.values():\n            assert p.kind in (Parameter.POSITIONAL_OR_KEYWORD,\n                              Parameter.KEYWORD_ONLY)\n            if var_arg_names:\n                # Cannot have a positional argument after a *args\n                p = p.replace(kind=Parameter.KEYWORD_ONLY)\n            params.append(p)\n        params.append(Parameter(\"options\", Parameter.KEYWORD_ONLY,\n                                default=None))\n    params.append(Parameter(\"memory_pool\", Parameter.KEYWORD_ONLY,\n                            default=None))\n    return inspect.Signature(params)\n\n\ndef _wrap_function(name, func):\n    options_class = _get_options_class(func)\n    arg_names = _get_arg_names(func)\n    has_vararg = arg_names and arg_names[-1].startswith('*')\n    if has_vararg:\n        var_arg_names = [arg_names.pop().lstrip('*')]\n    else:\n        var_arg_names = []\n\n    wrapper = _make_generic_wrapper(\n        name, func, options_class, arity=func.arity)\n    wrapper.__signature__ = _make_signature(arg_names, var_arg_names,\n                                            options_class)\n    return _decorate_compute_function(wrapper, name, func, options_class)\n\n\ndef _make_global_functions():\n    \"\"\"\n    Make global functions wrapping each compute function.\n\n    Note that some of the automatically-generated wrappers may be overridden\n    by custom versions below.\n    \"\"\"\n    g = globals()\n    reg = function_registry()\n\n    # Avoid clashes with Python keywords\n    rewrites = {'and': 'and_',\n                'or': 'or_'}\n\n    for cpp_name in reg.list_functions():\n        name = rewrites.get(cpp_name, cpp_name)\n        func = reg.get_function(cpp_name)\n        if func.kind == \"hash_aggregate\":\n            # Hash aggregate functions are not callable,\n            # so let's not expose them at module level.\n            continue\n        if func.kind == \"scalar_aggregate\" and func.arity == 0:\n            # Nullary scalar aggregate functions are not callable\n            # directly so let's not expose them at module level.\n            continue\n        assert name not in g, name\n        g[cpp_name] = g[name] = _wrap_function(name, func)\n\n\n_make_global_functions()\n\n\ndef cast(arr, target_type=None, safe=None, options=None, memory_pool=None):\n    \"\"\"\n    Cast array values to another data type. Can also be invoked as an array\n    instance method.\n\n    Parameters\n    ----------\n    arr : Array-like\n    target_type : DataType or str\n        Type to cast to\n    safe : bool, default True\n        Check for overflows or other unsafe conversions\n    options : CastOptions, default None\n        Additional checks pass by CastOptions\n    memory_pool : MemoryPool, optional\n        memory pool to use for allocations during function execution.\n\n    Examples\n    --------\n    >>> from datetime import datetime\n    >>> import pyarrow as pa\n    >>> arr = pa.array([datetime(2010, 1, 1), datetime(2015, 1, 1)])\n    >>> arr.type\n    TimestampType(timestamp[us])\n\n    You can use ``pyarrow.DataType`` objects to specify the target type:\n\n    >>> cast(arr, pa.timestamp('ms'))\n    <pyarrow.lib.TimestampArray object at ...>\n    [\n      2010-01-01 00:00:00.000,\n      2015-01-01 00:00:00.000\n    ]\n\n    >>> cast(arr, pa.timestamp('ms')).type\n    TimestampType(timestamp[ms])\n\n    Alternatively, it is also supported to use the string aliases for these\n    types:\n\n    >>> arr.cast('timestamp[ms]')\n    <pyarrow.lib.TimestampArray object at ...>\n    [\n      2010-01-01 00:00:00.000,\n      2015-01-01 00:00:00.000\n    ]\n    >>> arr.cast('timestamp[ms]').type\n    TimestampType(timestamp[ms])\n\n    Returns\n    -------\n    casted : Array\n        The cast result as a new Array\n    \"\"\"\n    safe_vars_passed = (safe is not None) or (target_type is not None)\n\n    if safe_vars_passed and (options is not None):\n        raise ValueError(\"Must either pass values for 'target_type' and 'safe'\"\n                         \" or pass a value for 'options'\")\n\n    if options is None:\n        target_type = pa.types.lib.ensure_type(target_type)\n        if safe is False:\n            options = CastOptions.unsafe(target_type)\n        else:\n            options = CastOptions.safe(target_type)\n    return call_function(\"cast\", [arr], options, memory_pool)\n\n\ndef index(data, value, start=None, end=None, *, memory_pool=None):\n    \"\"\"\n    Find the index of the first occurrence of a given value.\n\n    Parameters\n    ----------\n    data : Array-like\n    value : Scalar-like object\n        The value to search for.\n    start : int, optional\n    end : int, optional\n    memory_pool : MemoryPool, optional\n        If not passed, will allocate memory from the default memory pool.\n\n    Returns\n    -------\n    index : int\n        the index, or -1 if not found\n    \"\"\"\n    if start is not None:\n        if end is not None:\n            data = data.slice(start, end - start)\n        else:\n            data = data.slice(start)\n    elif end is not None:\n        data = data.slice(0, end)\n\n    if not isinstance(value, pa.Scalar):\n        value = pa.scalar(value, type=data.type)\n    elif data.type != value.type:\n        value = pa.scalar(value.as_py(), type=data.type)\n    options = IndexOptions(value=value)\n    result = call_function('index', [data], options, memory_pool)\n    if start is not None and result.as_py() >= 0:\n        result = pa.scalar(result.as_py() + start, type=pa.int64())\n    return result\n\n\ndef take(data, indices, *, boundscheck=True, memory_pool=None):\n    \"\"\"\n    Select values (or records) from array- or table-like data given integer\n    selection indices.\n\n    The result will be of the same type(s) as the input, with elements taken\n    from the input array (or record batch / table fields) at the given\n    indices. If an index is null then the corresponding value in the output\n    will be null.\n\n    Parameters\n    ----------\n    data : Array, ChunkedArray, RecordBatch, or Table\n    indices : Array, ChunkedArray\n        Must be of integer type\n    boundscheck : boolean, default True\n        Whether to boundscheck the indices. If False and there is an out of\n        bounds index, will likely cause the process to crash.\n    memory_pool : MemoryPool, optional\n        If not passed, will allocate memory from the default memory pool.\n\n    Returns\n    -------\n    result : depends on inputs\n        Selected values for the given indices\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> arr = pa.array([\"a\", \"b\", \"c\", None, \"e\", \"f\"])\n    >>> indices = pa.array([0, None, 4, 3])\n    >>> arr.take(indices)\n    <pyarrow.lib.StringArray object at ...>\n    [\n      \"a\",\n      null,\n      \"e\",\n      null\n    ]\n    \"\"\"\n    options = TakeOptions(boundscheck=boundscheck)\n    return call_function('take', [data, indices], options, memory_pool)\n\n\ndef fill_null(values, fill_value):\n    \"\"\"Replace each null element in values with a corresponding\n    element from fill_value.\n\n    If fill_value is scalar-like, then every null element in values\n    will be replaced with fill_value. If fill_value is array-like,\n    then the i-th element in values will be replaced with the i-th\n    element in fill_value.\n\n    The fill_value's type must be the same as that of values, or it\n    must be able to be implicitly casted to the array's type.\n\n    This is an alias for :func:`coalesce`.\n\n    Parameters\n    ----------\n    values : Array, ChunkedArray, or Scalar-like object\n        Each null element is replaced with the corresponding value\n        from fill_value.\n    fill_value : Array, ChunkedArray, or Scalar-like object\n        If not same type as values, will attempt to cast.\n\n    Returns\n    -------\n    result : depends on inputs\n        Values with all null elements replaced\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> arr = pa.array([1, 2, None, 3], type=pa.int8())\n    >>> fill_value = pa.scalar(5, type=pa.int8())\n    >>> arr.fill_null(fill_value)\n    <pyarrow.lib.Int8Array object at ...>\n    [\n      1,\n      2,\n      5,\n      3\n    ]\n    >>> arr = pa.array([1, 2, None, 4, None])\n    >>> arr.fill_null(pa.array([10, 20, 30, 40, 50]))\n    <pyarrow.lib.Int64Array object at ...>\n    [\n      1,\n      2,\n      30,\n      4,\n      50\n    ]\n    \"\"\"\n    if not isinstance(fill_value, (pa.Array, pa.ChunkedArray, pa.Scalar)):\n        fill_value = pa.scalar(fill_value, type=values.type)\n    elif values.type != fill_value.type:\n        fill_value = pa.scalar(fill_value.as_py(), type=values.type)\n\n    return call_function(\"coalesce\", [values, fill_value])\n\n\ndef top_k_unstable(values, k, sort_keys=None, *, memory_pool=None):\n    \"\"\"\n    Select the indices of the top-k ordered elements from array- or table-like\n    data.\n\n    This is a specialization for :func:`select_k_unstable`. Output is not\n    guaranteed to be stable.\n\n    Parameters\n    ----------\n    values : Array, ChunkedArray, RecordBatch, or Table\n        Data to sort and get top indices from.\n    k : int\n        The number of `k` elements to keep.\n    sort_keys : List-like\n        Column key names to order by when input is table-like data.\n    memory_pool : MemoryPool, optional\n        If not passed, will allocate memory from the default memory pool.\n\n    Returns\n    -------\n    result : Array\n        Indices of the top-k ordered elements\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> import pyarrow.compute as pc\n    >>> arr = pa.array([\"a\", \"b\", \"c\", None, \"e\", \"f\"])\n    >>> pc.top_k_unstable(arr, k=3)\n    <pyarrow.lib.UInt64Array object at ...>\n    [\n      5,\n      4,\n      2\n    ]\n    \"\"\"\n    if sort_keys is None:\n        sort_keys = []\n    if isinstance(values, (pa.Array, pa.ChunkedArray)):\n        sort_keys.append((\"dummy\", \"descending\"))\n    else:\n        sort_keys = map(lambda key_name: (key_name, \"descending\"), sort_keys)\n    options = SelectKOptions(k, sort_keys)\n    return call_function(\"select_k_unstable\", [values], options, memory_pool)\n\n\ndef bottom_k_unstable(values, k, sort_keys=None, *, memory_pool=None):\n    \"\"\"\n    Select the indices of the bottom-k ordered elements from\n    array- or table-like data.\n\n    This is a specialization for :func:`select_k_unstable`. Output is not\n    guaranteed to be stable.\n\n    Parameters\n    ----------\n    values : Array, ChunkedArray, RecordBatch, or Table\n        Data to sort and get bottom indices from.\n    k : int\n        The number of `k` elements to keep.\n    sort_keys : List-like\n        Column key names to order by when input is table-like data.\n    memory_pool : MemoryPool, optional\n        If not passed, will allocate memory from the default memory pool.\n\n    Returns\n    -------\n    result : Array of indices\n        Indices of the bottom-k ordered elements\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> import pyarrow.compute as pc\n    >>> arr = pa.array([\"a\", \"b\", \"c\", None, \"e\", \"f\"])\n    >>> pc.bottom_k_unstable(arr, k=3)\n    <pyarrow.lib.UInt64Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    if sort_keys is None:\n        sort_keys = []\n    if isinstance(values, (pa.Array, pa.ChunkedArray)):\n        sort_keys.append((\"dummy\", \"ascending\"))\n    else:\n        sort_keys = map(lambda key_name: (key_name, \"ascending\"), sort_keys)\n    options = SelectKOptions(k, sort_keys)\n    return call_function(\"select_k_unstable\", [values], options, memory_pool)\n\n\ndef random(n, *, initializer='system', options=None, memory_pool=None):\n    \"\"\"\n    Generate numbers in the range [0, 1).\n\n    Generated values are uniformly-distributed, double-precision\n    in range [0, 1). Algorithm and seed can be changed via RandomOptions.\n\n    Parameters\n    ----------\n    n : int\n        Number of values to generate, must be greater than or equal to 0\n    initializer : int or str\n        How to initialize the underlying random generator.\n        If an integer is given, it is used as a seed.\n        If \"system\" is given, the random generator is initialized with\n        a system-specific source of (hopefully true) randomness.\n        Other values are invalid.\n    options : pyarrow.compute.RandomOptions, optional\n        Alternative way of passing options.\n    memory_pool : pyarrow.MemoryPool, optional\n        If not passed, will allocate memory from the default memory pool.\n    \"\"\"\n    options = RandomOptions(initializer=initializer)\n    return call_function(\"random\", [], options, memory_pool, length=n)\n\n\ndef field(*name_or_index):\n    \"\"\"Reference a column of the dataset.\n\n    Stores only the field's name. Type and other information is known only when\n    the expression is bound to a dataset having an explicit scheme.\n\n    Nested references are allowed by passing multiple names or a tuple of\n    names. For example ``('foo', 'bar')`` references the field named \"bar\"\n    inside the field named \"foo\".\n\n    Parameters\n    ----------\n    *name_or_index : string, multiple strings, tuple or int\n        The name or index of the (possibly nested) field the expression\n        references to.\n\n    Returns\n    -------\n    field_expr : Expression\n        Reference to the given field\n\n    Examples\n    --------\n    >>> import pyarrow.compute as pc\n    >>> pc.field(\"a\")\n    <pyarrow.compute.Expression a>\n    >>> pc.field(1)\n    <pyarrow.compute.Expression FieldPath(1)>\n    >>> pc.field((\"a\", \"b\"))\n    <pyarrow.compute.Expression FieldRef.Nested(FieldRef.Name(a) ...\n    >>> pc.field(\"a\", \"b\")\n    <pyarrow.compute.Expression FieldRef.Nested(FieldRef.Name(a) ...\n    \"\"\"\n    n = len(name_or_index)\n    if n == 1:\n        if isinstance(name_or_index[0], (str, int)):\n            return Expression._field(name_or_index[0])\n        elif isinstance(name_or_index[0], tuple):\n            return Expression._nested_field(name_or_index[0])\n        else:\n            raise TypeError(\n                \"field reference should be str, multiple str, tuple or \"\n                f\"integer, got {type(name_or_index[0])}\"\n            )\n    # In case of multiple strings not supplied in a tuple\n    else:\n        return Expression._nested_field(name_or_index)\n\n\ndef scalar(value):\n    \"\"\"Expression representing a scalar value.\n\n    Parameters\n    ----------\n    value : bool, int, float or string\n        Python value of the scalar. Note that only a subset of types are\n        currently supported.\n\n    Returns\n    -------\n    scalar_expr : Expression\n        An Expression representing the scalar value\n    \"\"\"\n    return Expression._scalar(value)\n", "python/pyarrow/util.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Miscellaneous utility code\n\nimport os\nimport contextlib\nimport functools\nimport gc\nimport socket\nimport sys\nimport textwrap\nimport types\nimport warnings\n\n\n_DEPR_MSG = (\n    \"pyarrow.{} is deprecated as of {}, please use pyarrow.{} instead.\"\n)\n\n\ndef doc(*docstrings, **params):\n    \"\"\"\n    A decorator that takes docstring templates, concatenates them, and finally\n    performs string substitution on them.\n    This decorator will add a variable \"_docstring_components\" to the wrapped\n    callable to keep track of the original docstring template for potential future use.\n    If the docstring is a template, it will be saved as a string.\n    Otherwise, it will be saved as a callable and the docstring will be obtained via\n    the __doc__ attribute.\n    This decorator cannot be used on Cython classes due to a CPython constraint,\n    which enforces the __doc__ attribute to be read-only.\n    See https://github.com/python/cpython/issues/91309\n\n    Parameters\n    ----------\n    *docstrings : None, str, or callable\n        The string / docstring / docstring template to be prepended in order\n        before the default docstring under the callable.\n    **params\n        The key/value pairs used to format the docstring template.\n    \"\"\"\n\n    def decorator(decorated):\n        docstring_components = []\n\n        # collect docstrings and docstring templates\n        for docstring in docstrings:\n            if docstring is None:\n                continue\n            if hasattr(docstring, \"_docstring_components\"):\n                docstring_components.extend(\n                    docstring._docstring_components\n                )\n            elif isinstance(docstring, str) or docstring.__doc__:\n                docstring_components.append(docstring)\n\n        # append the callable's docstring last\n        if decorated.__doc__:\n            docstring_components.append(textwrap.dedent(decorated.__doc__))\n\n        params_applied = [\n            component.format(**params)\n            if isinstance(component, str) and len(params) > 0\n            else component\n            for component in docstring_components\n        ]\n\n        decorated.__doc__ = \"\".join(\n            [\n                component\n                if isinstance(component, str)\n                else textwrap.dedent(component.__doc__ or \"\")\n                for component in params_applied\n            ]\n        )\n\n        decorated._docstring_components = (\n            docstring_components\n        )\n        return decorated\n\n    return decorator\n\n\ndef _deprecate_api(old_name, new_name, api, next_version, type=FutureWarning):\n    msg = _DEPR_MSG.format(old_name, next_version, new_name)\n\n    def wrapper(*args, **kwargs):\n        warnings.warn(msg, type)\n        return api(*args, **kwargs)\n    return wrapper\n\n\ndef _deprecate_class(old_name, new_class, next_version,\n                     instancecheck=True):\n    \"\"\"\n    Raise warning if a deprecated class is used in an isinstance check.\n    \"\"\"\n    class _DeprecatedMeta(type):\n        def __instancecheck__(self, other):\n            warnings.warn(\n                _DEPR_MSG.format(old_name, next_version, new_class.__name__),\n                FutureWarning,\n                stacklevel=2\n            )\n            return isinstance(other, new_class)\n\n    return _DeprecatedMeta(old_name, (new_class,), {})\n\n\ndef _is_iterable(obj):\n    try:\n        iter(obj)\n        return True\n    except TypeError:\n        return False\n\n\ndef _is_path_like(path):\n    return isinstance(path, str) or hasattr(path, '__fspath__')\n\n\ndef _stringify_path(path):\n    \"\"\"\n    Convert *path* to a string or unicode path if possible.\n    \"\"\"\n    if isinstance(path, str):\n        return os.path.expanduser(path)\n\n    # checking whether path implements the filesystem protocol\n    try:\n        return os.path.expanduser(path.__fspath__())\n    except AttributeError:\n        pass\n\n    raise TypeError(\"not a path-like object\")\n\n\ndef product(seq):\n    \"\"\"\n    Return a product of sequence items.\n    \"\"\"\n    return functools.reduce(lambda a, b: a*b, seq, 1)\n\n\ndef get_contiguous_span(shape, strides, itemsize):\n    \"\"\"\n    Return a contiguous span of N-D array data.\n\n    Parameters\n    ----------\n    shape : tuple\n    strides : tuple\n    itemsize : int\n      Specify array shape data\n\n    Returns\n    -------\n    start, end : int\n      The span end points.\n    \"\"\"\n    if not strides:\n        start = 0\n        end = itemsize * product(shape)\n    else:\n        start = 0\n        end = itemsize\n        for i, dim in enumerate(shape):\n            if dim == 0:\n                start = end = 0\n                break\n            stride = strides[i]\n            if stride > 0:\n                end += stride * (dim - 1)\n            elif stride < 0:\n                start += stride * (dim - 1)\n        if end - start != itemsize * product(shape):\n            raise ValueError('array data is non-contiguous')\n    return start, end\n\n\ndef find_free_port():\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    with contextlib.closing(sock) as sock:\n        sock.bind(('', 0))\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        return sock.getsockname()[1]\n\n\ndef guid():\n    from uuid import uuid4\n    return uuid4().hex\n\n\ndef _break_traceback_cycle_from_frame(frame):\n    # Clear local variables in all inner frames, so as to break the\n    # reference cycle.\n    this_frame = sys._getframe(0)\n    refs = gc.get_referrers(frame)\n    while refs:\n        for frame in refs:\n            if frame is not this_frame and isinstance(frame, types.FrameType):\n                break\n        else:\n            # No frame found in referrers (finished?)\n            break\n        refs = None\n        # Clear the frame locals, to try and break the cycle (it is\n        # somewhere along the chain of execution frames).\n        frame.clear()\n        # To visit the inner frame, we need to find it among the\n        # referrers of this frame (while `frame.f_back` would let\n        # us visit the outer frame).\n        refs = gc.get_referrers(frame)\n    refs = frame = this_frame = None\n\n\ndef download_tzdata_on_windows():\n    r\"\"\"\n    Download and extract latest IANA timezone database into the\n    location expected by Arrow which is %USERPROFILE%\\Downloads\\tzdata.\n    \"\"\"\n    if sys.platform != 'win32':\n        raise TypeError(f\"Timezone database is already provided by {sys.platform}\")\n\n    import tarfile\n\n    tzdata_path = os.path.expandvars(r\"%USERPROFILE%\\Downloads\\tzdata\")\n    tzdata_compressed = os.path.join(tzdata_path, \"tzdata.tar.gz\")\n    os.makedirs(tzdata_path, exist_ok=True)\n\n    from urllib.request import urlopen\n    with urlopen('https://data.iana.org/time-zones/tzdata-latest.tar.gz') as response:\n        with open(tzdata_compressed, 'wb') as f:\n            f.write(response.read())\n\n    assert os.path.exists(tzdata_compressed)\n\n    tarfile.open(tzdata_compressed).extractall(tzdata_path)\n\n    with urlopen('https://raw.githubusercontent.com/unicode-org/cldr/master/common/supplemental/windowsZones.xml') as response_zones:   # noqa\n        with open(os.path.join(tzdata_path, \"windowsZones.xml\"), 'wb') as f:\n            f.write(response_zones.read())\n", "python/pyarrow/substrait.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\ntry:\n    from pyarrow._substrait import (  # noqa\n        BoundExpressions,\n        get_supported_functions,\n        run_query,\n        deserialize_expressions,\n        serialize_expressions\n    )\nexcept ImportError as exc:\n    raise ImportError(\n        \"The pyarrow installation is not built with support \"\n        f\"for 'substrait' ({str(exc)})\"\n    ) from None\n", "python/pyarrow/json.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nfrom pyarrow._json import ReadOptions, ParseOptions, read_json  # noqa\n", "python/pyarrow/types.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Tools for dealing with Arrow type metadata in Python\n\n\nfrom pyarrow.lib import (is_boolean_value,  # noqa\n                         is_integer_value,\n                         is_float_value)\n\nimport pyarrow.lib as lib\nfrom pyarrow.util import doc\n\n\n_SIGNED_INTEGER_TYPES = {lib.Type_INT8, lib.Type_INT16, lib.Type_INT32,\n                         lib.Type_INT64}\n_UNSIGNED_INTEGER_TYPES = {lib.Type_UINT8, lib.Type_UINT16, lib.Type_UINT32,\n                           lib.Type_UINT64}\n_INTEGER_TYPES = _SIGNED_INTEGER_TYPES | _UNSIGNED_INTEGER_TYPES\n_FLOATING_TYPES = {lib.Type_HALF_FLOAT, lib.Type_FLOAT, lib.Type_DOUBLE}\n_DECIMAL_TYPES = {lib.Type_DECIMAL128, lib.Type_DECIMAL256}\n_DATE_TYPES = {lib.Type_DATE32, lib.Type_DATE64}\n_TIME_TYPES = {lib.Type_TIME32, lib.Type_TIME64}\n_INTERVAL_TYPES = {lib.Type_INTERVAL_MONTH_DAY_NANO}\n_TEMPORAL_TYPES = ({lib.Type_TIMESTAMP,\n                    lib.Type_DURATION} | _TIME_TYPES | _DATE_TYPES |\n                   _INTERVAL_TYPES)\n_UNION_TYPES = {lib.Type_SPARSE_UNION, lib.Type_DENSE_UNION}\n_NESTED_TYPES = {lib.Type_LIST, lib.Type_FIXED_SIZE_LIST, lib.Type_LARGE_LIST,\n                 lib.Type_LIST_VIEW, lib.Type_LARGE_LIST_VIEW,\n                 lib.Type_STRUCT, lib.Type_MAP} | _UNION_TYPES\n\n\n@doc(datatype=\"null\")\ndef is_null(t):\n    \"\"\"\n    Return True if value is an instance of type: {datatype}.\n\n    Parameters\n    ----------\n    t : DataType\n    \"\"\"\n    return t.id == lib.Type_NA\n\n\n@doc(is_null, datatype=\"boolean\")\ndef is_boolean(t):\n    return t.id == lib.Type_BOOL\n\n\n@doc(is_null, datatype=\"any integer\")\ndef is_integer(t):\n    return t.id in _INTEGER_TYPES\n\n\n@doc(is_null, datatype=\"signed integer\")\ndef is_signed_integer(t):\n    return t.id in _SIGNED_INTEGER_TYPES\n\n\n@doc(is_null, datatype=\"unsigned integer\")\ndef is_unsigned_integer(t):\n    return t.id in _UNSIGNED_INTEGER_TYPES\n\n\n@doc(is_null, datatype=\"int8\")\ndef is_int8(t):\n    return t.id == lib.Type_INT8\n\n\n@doc(is_null, datatype=\"int16\")\ndef is_int16(t):\n    return t.id == lib.Type_INT16\n\n\n@doc(is_null, datatype=\"int32\")\ndef is_int32(t):\n    return t.id == lib.Type_INT32\n\n\n@doc(is_null, datatype=\"int64\")\ndef is_int64(t):\n    return t.id == lib.Type_INT64\n\n\n@doc(is_null, datatype=\"uint8\")\ndef is_uint8(t):\n    return t.id == lib.Type_UINT8\n\n\n@doc(is_null, datatype=\"uint16\")\ndef is_uint16(t):\n    return t.id == lib.Type_UINT16\n\n\n@doc(is_null, datatype=\"uint32\")\ndef is_uint32(t):\n    return t.id == lib.Type_UINT32\n\n\n@doc(is_null, datatype=\"uint64\")\ndef is_uint64(t):\n    return t.id == lib.Type_UINT64\n\n\n@doc(is_null, datatype=\"floating point numeric\")\ndef is_floating(t):\n    return t.id in _FLOATING_TYPES\n\n\n@doc(is_null, datatype=\"float16 (half-precision)\")\ndef is_float16(t):\n    return t.id == lib.Type_HALF_FLOAT\n\n\n@doc(is_null, datatype=\"float32 (single precision)\")\ndef is_float32(t):\n    return t.id == lib.Type_FLOAT\n\n\n@doc(is_null, datatype=\"float64 (double precision)\")\ndef is_float64(t):\n    return t.id == lib.Type_DOUBLE\n\n\n@doc(is_null, datatype=\"list\")\ndef is_list(t):\n    return t.id == lib.Type_LIST\n\n\n@doc(is_null, datatype=\"large list\")\ndef is_large_list(t):\n    return t.id == lib.Type_LARGE_LIST\n\n\n@doc(is_null, datatype=\"fixed size list\")\ndef is_fixed_size_list(t):\n    return t.id == lib.Type_FIXED_SIZE_LIST\n\n\n@doc(is_null, datatype=\"list view\")\ndef is_list_view(t):\n    return t.id == lib.Type_LIST_VIEW\n\n\n@doc(is_null, datatype=\"large list view\")\ndef is_large_list_view(t):\n    return t.id == lib.Type_LARGE_LIST_VIEW\n\n\n@doc(is_null, datatype=\"struct\")\ndef is_struct(t):\n    return t.id == lib.Type_STRUCT\n\n\n@doc(is_null, datatype=\"union\")\ndef is_union(t):\n    return t.id in _UNION_TYPES\n\n\n@doc(is_null, datatype=\"nested type\")\ndef is_nested(t):\n    return t.id in _NESTED_TYPES\n\n\n@doc(is_null, datatype=\"run-end encoded\")\ndef is_run_end_encoded(t):\n    return t.id == lib.Type_RUN_END_ENCODED\n\n\n@doc(is_null, datatype=\"date, time, timestamp or duration\")\ndef is_temporal(t):\n    return t.id in _TEMPORAL_TYPES\n\n\n@doc(is_null, datatype=\"timestamp\")\ndef is_timestamp(t):\n    return t.id == lib.Type_TIMESTAMP\n\n\n@doc(is_null, datatype=\"duration\")\ndef is_duration(t):\n    return t.id == lib.Type_DURATION\n\n\n@doc(is_null, datatype=\"time\")\ndef is_time(t):\n    return t.id in _TIME_TYPES\n\n\n@doc(is_null, datatype=\"time32\")\ndef is_time32(t):\n    return t.id == lib.Type_TIME32\n\n\n@doc(is_null, datatype=\"time64\")\ndef is_time64(t):\n    return t.id == lib.Type_TIME64\n\n\n@doc(is_null, datatype=\"variable-length binary\")\ndef is_binary(t):\n    return t.id == lib.Type_BINARY\n\n\n@doc(is_null, datatype=\"large variable-length binary\")\ndef is_large_binary(t):\n    return t.id == lib.Type_LARGE_BINARY\n\n\n@doc(method=\"is_string\")\ndef is_unicode(t):\n    \"\"\"\n    Alias for {method}.\n\n    Parameters\n    ----------\n    t : DataType\n    \"\"\"\n    return is_string(t)\n\n\n@doc(is_null, datatype=\"string (utf8 unicode)\")\ndef is_string(t):\n    return t.id == lib.Type_STRING\n\n\n@doc(is_unicode, method=\"is_large_string\")\ndef is_large_unicode(t):\n    return is_large_string(t)\n\n\n@doc(is_null, datatype=\"large string (utf8 unicode)\")\ndef is_large_string(t):\n    return t.id == lib.Type_LARGE_STRING\n\n\n@doc(is_null, datatype=\"fixed size binary\")\ndef is_fixed_size_binary(t):\n    return t.id == lib.Type_FIXED_SIZE_BINARY\n\n\n@doc(is_null, datatype=\"variable-length binary view\")\ndef is_binary_view(t):\n    return t.id == lib.Type_BINARY_VIEW\n\n\n@doc(is_null, datatype=\"variable-length string (utf-8) view\")\ndef is_string_view(t):\n    return t.id == lib.Type_STRING_VIEW\n\n\n@doc(is_null, datatype=\"date\")\ndef is_date(t):\n    return t.id in _DATE_TYPES\n\n\n@doc(is_null, datatype=\"date32 (days)\")\ndef is_date32(t):\n    return t.id == lib.Type_DATE32\n\n\n@doc(is_null, datatype=\"date64 (milliseconds)\")\ndef is_date64(t):\n    return t.id == lib.Type_DATE64\n\n\n@doc(is_null, datatype=\"map\")\ndef is_map(t):\n    return t.id == lib.Type_MAP\n\n\n@doc(is_null, datatype=\"decimal\")\ndef is_decimal(t):\n    return t.id in _DECIMAL_TYPES\n\n\n@doc(is_null, datatype=\"decimal128\")\ndef is_decimal128(t):\n    return t.id == lib.Type_DECIMAL128\n\n\n@doc(is_null, datatype=\"decimal256\")\ndef is_decimal256(t):\n    return t.id == lib.Type_DECIMAL256\n\n\n@doc(is_null, datatype=\"dictionary-encoded\")\ndef is_dictionary(t):\n    return t.id == lib.Type_DICTIONARY\n\n\n@doc(is_null, datatype=\"interval\")\ndef is_interval(t):\n    return t.id == lib.Type_INTERVAL_MONTH_DAY_NANO\n\n\n@doc(is_null, datatype=\"primitive type\")\ndef is_primitive(t):\n    return lib._is_primitive(t.id)\n", "python/pyarrow/pandas_compat.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nimport ast\nfrom collections.abc import Sequence\nfrom concurrent import futures\n# import threading submodule upfront to avoid partially initialized\n# module bug (ARROW-11983)\nimport concurrent.futures.thread  # noqa\nfrom copy import deepcopy\nimport decimal\nfrom itertools import zip_longest\nimport json\nimport operator\nimport re\nimport warnings\n\nimport numpy as np\n\nimport pyarrow as pa\nfrom pyarrow.lib import _pandas_api, frombytes  # noqa\n\n\n_logical_type_map = {}\n\n\ndef get_logical_type_map():\n    global _logical_type_map\n\n    if not _logical_type_map:\n        _logical_type_map.update({\n            pa.lib.Type_NA: 'empty',\n            pa.lib.Type_BOOL: 'bool',\n            pa.lib.Type_INT8: 'int8',\n            pa.lib.Type_INT16: 'int16',\n            pa.lib.Type_INT32: 'int32',\n            pa.lib.Type_INT64: 'int64',\n            pa.lib.Type_UINT8: 'uint8',\n            pa.lib.Type_UINT16: 'uint16',\n            pa.lib.Type_UINT32: 'uint32',\n            pa.lib.Type_UINT64: 'uint64',\n            pa.lib.Type_HALF_FLOAT: 'float16',\n            pa.lib.Type_FLOAT: 'float32',\n            pa.lib.Type_DOUBLE: 'float64',\n            pa.lib.Type_DATE32: 'date',\n            pa.lib.Type_DATE64: 'date',\n            pa.lib.Type_TIME32: 'time',\n            pa.lib.Type_TIME64: 'time',\n            pa.lib.Type_BINARY: 'bytes',\n            pa.lib.Type_FIXED_SIZE_BINARY: 'bytes',\n            pa.lib.Type_STRING: 'unicode',\n        })\n    return _logical_type_map\n\n\ndef get_logical_type(arrow_type):\n    logical_type_map = get_logical_type_map()\n\n    try:\n        return logical_type_map[arrow_type.id]\n    except KeyError:\n        if isinstance(arrow_type, pa.lib.DictionaryType):\n            return 'categorical'\n        elif isinstance(arrow_type, pa.lib.ListType):\n            return 'list[{}]'.format(get_logical_type(arrow_type.value_type))\n        elif isinstance(arrow_type, pa.lib.TimestampType):\n            return 'datetimetz' if arrow_type.tz is not None else 'datetime'\n        elif isinstance(arrow_type, pa.lib.Decimal128Type):\n            return 'decimal'\n        return 'object'\n\n\n_numpy_logical_type_map = {\n    np.bool_: 'bool',\n    np.int8: 'int8',\n    np.int16: 'int16',\n    np.int32: 'int32',\n    np.int64: 'int64',\n    np.uint8: 'uint8',\n    np.uint16: 'uint16',\n    np.uint32: 'uint32',\n    np.uint64: 'uint64',\n    np.float32: 'float32',\n    np.float64: 'float64',\n    'datetime64[D]': 'date',\n    np.str_: 'string',\n    np.bytes_: 'bytes',\n}\n\n\ndef get_logical_type_from_numpy(pandas_collection):\n    try:\n        return _numpy_logical_type_map[pandas_collection.dtype.type]\n    except KeyError:\n        if hasattr(pandas_collection.dtype, 'tz'):\n            return 'datetimetz'\n        # See https://github.com/pandas-dev/pandas/issues/24739 (infer_dtype will\n        # result in \"datetime64\" without unit, while pandas astype requires a unit)\n        if str(pandas_collection.dtype).startswith('datetime64'):\n            return str(pandas_collection.dtype)\n        result = _pandas_api.infer_dtype(pandas_collection)\n        if result == 'string':\n            return 'unicode'\n        return result\n\n\ndef get_extension_dtype_info(column):\n    dtype = column.dtype\n    if str(dtype) == 'category':\n        cats = getattr(column, 'cat', column)\n        assert cats is not None\n        metadata = {\n            'num_categories': len(cats.categories),\n            'ordered': cats.ordered,\n        }\n        physical_dtype = str(cats.codes.dtype)\n    elif hasattr(dtype, 'tz'):\n        metadata = {'timezone': pa.lib.tzinfo_to_string(dtype.tz)}\n        physical_dtype = 'datetime64[ns]'\n    else:\n        metadata = None\n        physical_dtype = str(dtype)\n    return physical_dtype, metadata\n\n\ndef get_column_metadata(column, name, arrow_type, field_name):\n    \"\"\"Construct the metadata for a given column\n\n    Parameters\n    ----------\n    column : pandas.Series or pandas.Index\n    name : str\n    arrow_type : pyarrow.DataType\n    field_name : str\n        Equivalent to `name` when `column` is a `Series`, otherwise if `column`\n        is a pandas Index then `field_name` will not be the same as `name`.\n        This is the name of the field in the arrow Table's schema.\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    logical_type = get_logical_type(arrow_type)\n\n    string_dtype, extra_metadata = get_extension_dtype_info(column)\n    if logical_type == 'decimal':\n        extra_metadata = {\n            'precision': arrow_type.precision,\n            'scale': arrow_type.scale,\n        }\n        string_dtype = 'object'\n\n    if name is not None and not isinstance(name, str):\n        raise TypeError(\n            'Column name must be a string. Got column {} of type {}'.format(\n                name, type(name).__name__\n            )\n        )\n\n    assert field_name is None or isinstance(field_name, str), \\\n        str(type(field_name))\n    return {\n        'name': name,\n        'field_name': 'None' if field_name is None else field_name,\n        'pandas_type': logical_type,\n        'numpy_type': string_dtype,\n        'metadata': extra_metadata,\n    }\n\n\ndef construct_metadata(columns_to_convert, df, column_names, index_levels,\n                       index_descriptors, preserve_index, types):\n    \"\"\"Returns a dictionary containing enough metadata to reconstruct a pandas\n    DataFrame as an Arrow Table, including index columns.\n\n    Parameters\n    ----------\n    columns_to_convert : list[pd.Series]\n    df : pandas.DataFrame\n    index_levels : List[pd.Index]\n    index_descriptors : List[Dict]\n    preserve_index : bool\n    types : List[pyarrow.DataType]\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    num_serialized_index_levels = len([descr for descr in index_descriptors\n                                       if not isinstance(descr, dict)])\n    # Use ntypes instead of Python shorthand notation [:-len(x)] as [:-0]\n    # behaves differently to what we want.\n    ntypes = len(types)\n    df_types = types[:ntypes - num_serialized_index_levels]\n    index_types = types[ntypes - num_serialized_index_levels:]\n\n    column_metadata = []\n    for col, sanitized_name, arrow_type in zip(columns_to_convert,\n                                               column_names, df_types):\n        metadata = get_column_metadata(col, name=sanitized_name,\n                                       arrow_type=arrow_type,\n                                       field_name=sanitized_name)\n        column_metadata.append(metadata)\n\n    index_column_metadata = []\n    if preserve_index is not False:\n        non_str_index_names = []\n        for level, arrow_type, descriptor in zip(index_levels, index_types,\n                                                 index_descriptors):\n            if isinstance(descriptor, dict):\n                # The index is represented in a non-serialized fashion,\n                # e.g. RangeIndex\n                continue\n\n            if level.name is not None and not isinstance(level.name, str):\n                non_str_index_names.append(level.name)\n\n            metadata = get_column_metadata(\n                level,\n                name=_column_name_to_strings(level.name),\n                arrow_type=arrow_type,\n                field_name=descriptor,\n            )\n            index_column_metadata.append(metadata)\n\n        if len(non_str_index_names) > 0:\n            warnings.warn(\n                f\"The DataFrame has non-str index name `{non_str_index_names}`\"\n                \" which will be converted to string\"\n                \" and not roundtrip correctly.\",\n                UserWarning, stacklevel=4)\n\n        column_indexes = []\n\n        levels = getattr(df.columns, 'levels', [df.columns])\n        names = getattr(df.columns, 'names', [df.columns.name])\n        for level, name in zip(levels, names):\n            metadata = _get_simple_index_descriptor(level, name)\n            column_indexes.append(metadata)\n    else:\n        index_descriptors = index_column_metadata = column_indexes = []\n\n    return {\n        b'pandas': json.dumps({\n            'index_columns': index_descriptors,\n            'column_indexes': column_indexes,\n            'columns': column_metadata + index_column_metadata,\n            'creator': {\n                'library': 'pyarrow',\n                'version': pa.__version__\n            },\n            'pandas_version': _pandas_api.version\n        }).encode('utf8')\n    }\n\n\ndef _get_simple_index_descriptor(level, name):\n    string_dtype, extra_metadata = get_extension_dtype_info(level)\n    pandas_type = get_logical_type_from_numpy(level)\n    if 'mixed' in pandas_type:\n        warnings.warn(\n            \"The DataFrame has column names of mixed type. They will be \"\n            \"converted to strings and not roundtrip correctly.\",\n            UserWarning, stacklevel=4)\n    if pandas_type == 'unicode':\n        assert not extra_metadata\n        extra_metadata = {'encoding': 'UTF-8'}\n    return {\n        'name': name,\n        'field_name': name,\n        'pandas_type': pandas_type,\n        'numpy_type': string_dtype,\n        'metadata': extra_metadata,\n    }\n\n\ndef _column_name_to_strings(name):\n    \"\"\"Convert a column name (or level) to either a string or a recursive\n    collection of strings.\n\n    Parameters\n    ----------\n    name : str or tuple\n\n    Returns\n    -------\n    value : str or tuple\n\n    Examples\n    --------\n    >>> name = 'foo'\n    >>> _column_name_to_strings(name)\n    'foo'\n    >>> name = ('foo', 'bar')\n    >>> _column_name_to_strings(name)\n    \"('foo', 'bar')\"\n    >>> import pandas as pd\n    >>> name = (1, pd.Timestamp('2017-02-01 00:00:00'))\n    >>> _column_name_to_strings(name)\n    \"('1', '2017-02-01 00:00:00')\"\n    \"\"\"\n    if isinstance(name, str):\n        return name\n    elif isinstance(name, bytes):\n        # XXX: should we assume that bytes in Python 3 are UTF-8?\n        return name.decode('utf8')\n    elif isinstance(name, tuple):\n        return str(tuple(map(_column_name_to_strings, name)))\n    elif isinstance(name, Sequence):\n        raise TypeError(\"Unsupported type for MultiIndex level\")\n    elif name is None:\n        return None\n    return str(name)\n\n\ndef _index_level_name(index, i, column_names):\n    \"\"\"Return the name of an index level or a default name if `index.name` is\n    None or is already a column name.\n\n    Parameters\n    ----------\n    index : pandas.Index\n    i : int\n\n    Returns\n    -------\n    name : str\n    \"\"\"\n    if index.name is not None and index.name not in column_names:\n        return _column_name_to_strings(index.name)\n    else:\n        return '__index_level_{:d}__'.format(i)\n\n\ndef _get_columns_to_convert(df, schema, preserve_index, columns):\n    columns = _resolve_columns_of_interest(df, schema, columns)\n\n    if not df.columns.is_unique:\n        raise ValueError(\n            'Duplicate column names found: {}'.format(list(df.columns))\n        )\n\n    if schema is not None:\n        return _get_columns_to_convert_given_schema(df, schema, preserve_index)\n\n    column_names = []\n\n    index_levels = (\n        _get_index_level_values(df.index) if preserve_index is not False\n        else []\n    )\n\n    columns_to_convert = []\n    convert_fields = []\n\n    for name in columns:\n        col = df[name]\n        name = _column_name_to_strings(name)\n\n        if _pandas_api.is_sparse(col):\n            raise TypeError(\n                \"Sparse pandas data (column {}) not supported.\".format(name))\n\n        columns_to_convert.append(col)\n        convert_fields.append(None)\n        column_names.append(name)\n\n    index_descriptors = []\n    index_column_names = []\n    for i, index_level in enumerate(index_levels):\n        name = _index_level_name(index_level, i, column_names)\n        if (isinstance(index_level, _pandas_api.pd.RangeIndex) and\n                preserve_index is None):\n            descr = _get_range_index_descriptor(index_level)\n        else:\n            columns_to_convert.append(index_level)\n            convert_fields.append(None)\n            descr = name\n            index_column_names.append(name)\n        index_descriptors.append(descr)\n\n    all_names = column_names + index_column_names\n\n    # all_names : all of the columns in the resulting table including the data\n    # columns and serialized index columns\n    # column_names : the names of the data columns\n    # index_column_names : the names of the serialized index columns\n    # index_descriptors : descriptions of each index to be used for\n    # reconstruction\n    # index_levels : the extracted index level values\n    # columns_to_convert : assembled raw data (both data columns and indexes)\n    # to be converted to Arrow format\n    # columns_fields : specified column to use for coercion / casting\n    # during serialization, if a Schema was provided\n    return (all_names, column_names, index_column_names, index_descriptors,\n            index_levels, columns_to_convert, convert_fields)\n\n\ndef _get_columns_to_convert_given_schema(df, schema, preserve_index):\n    \"\"\"\n    Specialized version of _get_columns_to_convert in case a Schema is\n    specified.\n    In that case, the Schema is used as the single point of truth for the\n    table structure (types, which columns are included, order of columns, ...).\n    \"\"\"\n    column_names = []\n    columns_to_convert = []\n    convert_fields = []\n    index_descriptors = []\n    index_column_names = []\n    index_levels = []\n\n    for name in schema.names:\n        try:\n            col = df[name]\n            is_index = False\n        except KeyError:\n            try:\n                col = _get_index_level(df, name)\n            except (KeyError, IndexError):\n                # name not found as index level\n                raise KeyError(\n                    \"name '{}' present in the specified schema is not found \"\n                    \"in the columns or index\".format(name))\n            if preserve_index is False:\n                raise ValueError(\n                    \"name '{}' present in the specified schema corresponds \"\n                    \"to the index, but 'preserve_index=False' was \"\n                    \"specified\".format(name))\n            elif (preserve_index is None and\n                    isinstance(col, _pandas_api.pd.RangeIndex)):\n                raise ValueError(\n                    \"name '{}' is present in the schema, but it is a \"\n                    \"RangeIndex which will not be converted as a column \"\n                    \"in the Table, but saved as metadata-only not in \"\n                    \"columns. Specify 'preserve_index=True' to force it \"\n                    \"being added as a column, or remove it from the \"\n                    \"specified schema\".format(name))\n            is_index = True\n\n        name = _column_name_to_strings(name)\n\n        if _pandas_api.is_sparse(col):\n            raise TypeError(\n                \"Sparse pandas data (column {}) not supported.\".format(name))\n\n        field = schema.field(name)\n        columns_to_convert.append(col)\n        convert_fields.append(field)\n        column_names.append(name)\n\n        if is_index:\n            index_column_names.append(name)\n            index_descriptors.append(name)\n            index_levels.append(col)\n\n    all_names = column_names + index_column_names\n\n    return (all_names, column_names, index_column_names, index_descriptors,\n            index_levels, columns_to_convert, convert_fields)\n\n\ndef _get_index_level(df, name):\n    \"\"\"\n    Get the index level of a DataFrame given 'name' (column name in an arrow\n    Schema).\n    \"\"\"\n    key = name\n    if name not in df.index.names and _is_generated_index_name(name):\n        # we know we have an autogenerated name => extract number and get\n        # the index level positionally\n        key = int(name[len(\"__index_level_\"):-2])\n    return df.index.get_level_values(key)\n\n\ndef _level_name(name):\n    # preserve type when default serializable, otherwise str it\n    try:\n        json.dumps(name)\n        return name\n    except TypeError:\n        return str(name)\n\n\ndef _get_range_index_descriptor(level):\n    # public start/stop/step attributes added in pandas 0.25.0\n    return {\n        'kind': 'range',\n        'name': _level_name(level.name),\n        'start': _pandas_api.get_rangeindex_attribute(level, 'start'),\n        'stop': _pandas_api.get_rangeindex_attribute(level, 'stop'),\n        'step': _pandas_api.get_rangeindex_attribute(level, 'step')\n    }\n\n\ndef _get_index_level_values(index):\n    n = len(getattr(index, 'levels', [index]))\n    return [index.get_level_values(i) for i in range(n)]\n\n\ndef _resolve_columns_of_interest(df, schema, columns):\n    if schema is not None and columns is not None:\n        raise ValueError('Schema and columns arguments are mutually '\n                         'exclusive, pass only one of them')\n    elif schema is not None:\n        columns = schema.names\n    elif columns is not None:\n        columns = [c for c in columns if c in df.columns]\n    else:\n        columns = df.columns\n\n    return columns\n\n\ndef dataframe_to_types(df, preserve_index, columns=None):\n    (all_names,\n     column_names,\n     _,\n     index_descriptors,\n     index_columns,\n     columns_to_convert,\n     _) = _get_columns_to_convert(df, None, preserve_index, columns)\n\n    types = []\n    # If pandas knows type, skip conversion\n    for c in columns_to_convert:\n        values = c.values\n        if _pandas_api.is_categorical(values):\n            type_ = pa.array(c, from_pandas=True).type\n        elif _pandas_api.is_extension_array_dtype(values):\n            empty = c.head(0) if isinstance(\n                c, _pandas_api.pd.Series) else c[:0]\n            type_ = pa.array(empty, from_pandas=True).type\n        else:\n            values, type_ = get_datetimetz_type(values, c.dtype, None)\n            type_ = pa.lib._ndarray_to_arrow_type(values, type_)\n            if type_ is None:\n                type_ = pa.array(c, from_pandas=True).type\n        types.append(type_)\n\n    metadata = construct_metadata(\n        columns_to_convert, df, column_names, index_columns,\n        index_descriptors, preserve_index, types\n    )\n\n    return all_names, types, metadata\n\n\ndef dataframe_to_arrays(df, schema, preserve_index, nthreads=1, columns=None,\n                        safe=True):\n    (all_names,\n     column_names,\n     index_column_names,\n     index_descriptors,\n     index_columns,\n     columns_to_convert,\n     convert_fields) = _get_columns_to_convert(df, schema, preserve_index,\n                                               columns)\n\n    # NOTE(wesm): If nthreads=None, then we use a heuristic to decide whether\n    # using a thread pool is worth it. Currently the heuristic is whether the\n    # nrows > 100 * ncols and ncols > 1.\n    if nthreads is None:\n        nrows, ncols = len(df), len(df.columns)\n        if nrows > ncols * 100 and ncols > 1:\n            nthreads = pa.cpu_count()\n        else:\n            nthreads = 1\n\n    def convert_column(col, field):\n        if field is None:\n            field_nullable = True\n            type_ = None\n        else:\n            field_nullable = field.nullable\n            type_ = field.type\n\n        try:\n            result = pa.array(col, type=type_, from_pandas=True, safe=safe)\n        except (pa.ArrowInvalid,\n                pa.ArrowNotImplementedError,\n                pa.ArrowTypeError) as e:\n            e.args += (\"Conversion failed for column {!s} with type {!s}\"\n                       .format(col.name, col.dtype),)\n            raise e\n        if not field_nullable and result.null_count > 0:\n            raise ValueError(\"Field {} was non-nullable but pandas column \"\n                             \"had {} null values\".format(str(field),\n                                                         result.null_count))\n        return result\n\n    def _can_definitely_zero_copy(arr):\n        return (isinstance(arr, np.ndarray) and\n                arr.flags.contiguous and\n                issubclass(arr.dtype.type, np.integer))\n\n    if nthreads == 1:\n        arrays = [convert_column(c, f)\n                  for c, f in zip(columns_to_convert, convert_fields)]\n    else:\n        arrays = []\n        with futures.ThreadPoolExecutor(nthreads) as executor:\n            for c, f in zip(columns_to_convert, convert_fields):\n                if _can_definitely_zero_copy(c.values):\n                    arrays.append(convert_column(c, f))\n                else:\n                    arrays.append(executor.submit(convert_column, c, f))\n\n        for i, maybe_fut in enumerate(arrays):\n            if isinstance(maybe_fut, futures.Future):\n                arrays[i] = maybe_fut.result()\n\n    types = [x.type for x in arrays]\n\n    if schema is None:\n        fields = []\n        for name, type_ in zip(all_names, types):\n            name = name if name is not None else 'None'\n            fields.append(pa.field(name, type_))\n        schema = pa.schema(fields)\n\n    pandas_metadata = construct_metadata(\n        columns_to_convert, df, column_names, index_columns,\n        index_descriptors, preserve_index, types\n    )\n    metadata = deepcopy(schema.metadata) if schema.metadata else dict()\n    metadata.update(pandas_metadata)\n    schema = schema.with_metadata(metadata)\n\n    # If dataframe is empty but with RangeIndex ->\n    # remember the length of the indexes\n    n_rows = None\n    if len(arrays) == 0:\n        try:\n            kind = index_descriptors[0][\"kind\"]\n            if kind == \"range\":\n                start = index_descriptors[0][\"start\"]\n                stop = index_descriptors[0][\"stop\"]\n                step = index_descriptors[0][\"step\"]\n                n_rows = len(range(start, stop, step))\n        except IndexError:\n            pass\n\n    return arrays, schema, n_rows\n\n\ndef get_datetimetz_type(values, dtype, type_):\n    if values.dtype.type != np.datetime64:\n        return values, type_\n\n    if _pandas_api.is_datetimetz(dtype) and type_ is None:\n        # If no user type passed, construct a tz-aware timestamp type\n        tz = dtype.tz\n        unit = dtype.unit\n        type_ = pa.timestamp(unit, tz)\n    elif type_ is None:\n        # Trust the NumPy dtype\n        type_ = pa.from_numpy_dtype(values.dtype)\n\n    return values, type_\n\n# ----------------------------------------------------------------------\n# Converting pyarrow.Table efficiently to pandas.DataFrame\n\n\ndef _reconstruct_block(item, columns=None, extension_columns=None, return_block=True):\n    \"\"\"\n    Construct a pandas Block from the `item` dictionary coming from pyarrow's\n    serialization or returned by arrow::python::ConvertTableToPandas.\n\n    This function takes care of converting dictionary types to pandas\n    categorical, Timestamp-with-timezones to the proper pandas Block, and\n    conversion to pandas ExtensionBlock\n\n    Parameters\n    ----------\n    item : dict\n        For basic types, this is a dictionary in the form of\n        {'block': np.ndarray of values, 'placement': pandas block placement}.\n        Additional keys are present for other types (dictionary, timezone,\n        object).\n    columns :\n        Column names of the table being constructed, used for extension types\n    extension_columns : dict\n        Dictionary of {column_name: pandas_dtype} that includes all columns\n        and corresponding dtypes that will be converted to a pandas\n        ExtensionBlock.\n\n    Returns\n    -------\n    pandas Block\n\n    \"\"\"\n    import pandas.core.internals as _int\n\n    block_arr = item.get('block', None)\n    placement = item['placement']\n    if 'dictionary' in item:\n        arr = _pandas_api.categorical_type.from_codes(\n            block_arr, categories=item['dictionary'],\n            ordered=item['ordered'])\n    elif 'timezone' in item:\n        unit, _ = np.datetime_data(block_arr.dtype)\n        dtype = make_datetimetz(unit, item['timezone'])\n        if _pandas_api.is_ge_v21():\n            arr = _pandas_api.pd.array(\n                block_arr.view(\"int64\"), dtype=dtype, copy=False\n            )\n        else:\n            arr = block_arr\n            if return_block:\n                block = _int.make_block(block_arr, placement=placement,\n                                        klass=_int.DatetimeTZBlock,\n                                        dtype=dtype)\n                return block\n    elif 'py_array' in item:\n        # create ExtensionBlock\n        arr = item['py_array']\n        assert len(placement) == 1\n        name = columns[placement[0]]\n        pandas_dtype = extension_columns[name]\n        if not hasattr(pandas_dtype, '__from_arrow__'):\n            raise ValueError(\"This column does not support to be converted \"\n                             \"to a pandas ExtensionArray\")\n        arr = pandas_dtype.__from_arrow__(arr)\n    else:\n        arr = block_arr\n\n    if return_block:\n        return _int.make_block(arr, placement=placement)\n    else:\n        return arr, placement\n\n\ndef make_datetimetz(unit, tz):\n    if _pandas_api.is_v1():\n        unit = 'ns'  # ARROW-3789: Coerce date/timestamp types to datetime64[ns]\n    tz = pa.lib.string_to_tzinfo(tz)\n    return _pandas_api.datetimetz_type(unit, tz=tz)\n\n\ndef table_to_dataframe(\n    options, table, categories=None, ignore_metadata=False, types_mapper=None\n):\n    all_columns = []\n    column_indexes = []\n    pandas_metadata = table.schema.pandas_metadata\n\n    if not ignore_metadata and pandas_metadata is not None:\n        all_columns = pandas_metadata['columns']\n        column_indexes = pandas_metadata.get('column_indexes', [])\n        index_descriptors = pandas_metadata['index_columns']\n        table = _add_any_metadata(table, pandas_metadata)\n        table, index = _reconstruct_index(table, index_descriptors,\n                                          all_columns, types_mapper)\n        ext_columns_dtypes = _get_extension_dtypes(\n            table, all_columns, types_mapper)\n    else:\n        index = _pandas_api.pd.RangeIndex(table.num_rows)\n        ext_columns_dtypes = _get_extension_dtypes(table, [], types_mapper)\n\n    _check_data_column_metadata_consistency(all_columns)\n    columns = _deserialize_column_index(table, all_columns, column_indexes)\n\n    column_names = table.column_names\n    result = pa.lib.table_to_blocks(options, table, categories,\n                                    list(ext_columns_dtypes.keys()))\n    if _pandas_api.is_ge_v3():\n        from pandas.api.internals import create_dataframe_from_blocks\n\n        blocks = [\n            _reconstruct_block(\n                item, column_names, ext_columns_dtypes, return_block=False)\n            for item in result\n        ]\n        df = create_dataframe_from_blocks(blocks, index=index, columns=columns)\n        return df\n    else:\n        from pandas.core.internals import BlockManager\n        from pandas import DataFrame\n\n        blocks = [\n            _reconstruct_block(item, column_names, ext_columns_dtypes)\n            for item in result\n        ]\n        axes = [columns, index]\n        mgr = BlockManager(blocks, axes)\n        if _pandas_api.is_ge_v21():\n            df = DataFrame._from_mgr(mgr, mgr.axes)\n        else:\n            df = DataFrame(mgr)\n        return df\n\n\n# Set of the string repr of all numpy dtypes that can be stored in a pandas\n# dataframe (complex not included since not supported by Arrow)\n_pandas_supported_numpy_types = {\n    \"int8\", \"int16\", \"int32\", \"int64\",\n    \"uint8\", \"uint16\", \"uint32\", \"uint64\",\n    \"float16\", \"float32\", \"float64\",\n    \"object\", \"bool\"\n}\n\n\ndef _get_extension_dtypes(table, columns_metadata, types_mapper=None):\n    \"\"\"\n    Based on the stored column pandas metadata and the extension types\n    in the arrow schema, infer which columns should be converted to a\n    pandas extension dtype.\n\n    The 'numpy_type' field in the column metadata stores the string\n    representation of the original pandas dtype (and, despite its name,\n    not the 'pandas_type' field).\n    Based on this string representation, a pandas/numpy dtype is constructed\n    and then we can check if this dtype supports conversion from arrow.\n\n    \"\"\"\n    ext_columns = {}\n\n    # older pandas version that does not yet support extension dtypes\n    if _pandas_api.extension_dtype is None:\n        return ext_columns\n\n    # infer the extension columns from the pandas metadata\n    for col_meta in columns_metadata:\n        try:\n            name = col_meta['field_name']\n        except KeyError:\n            name = col_meta['name']\n        dtype = col_meta['numpy_type']\n\n        if dtype not in _pandas_supported_numpy_types:\n            # pandas_dtype is expensive, so avoid doing this for types\n            # that are certainly numpy dtypes\n            pandas_dtype = _pandas_api.pandas_dtype(dtype)\n            if isinstance(pandas_dtype, _pandas_api.extension_dtype):\n                if hasattr(pandas_dtype, \"__from_arrow__\"):\n                    ext_columns[name] = pandas_dtype\n\n    # infer from extension type in the schema\n    for field in table.schema:\n        typ = field.type\n        if isinstance(typ, pa.BaseExtensionType):\n            try:\n                pandas_dtype = typ.to_pandas_dtype()\n            except NotImplementedError:\n                pass\n            else:\n                ext_columns[field.name] = pandas_dtype\n\n    # use the specified mapping of built-in arrow types to pandas dtypes\n    if types_mapper:\n        for field in table.schema:\n            typ = field.type\n            pandas_dtype = types_mapper(typ)\n            if pandas_dtype is not None:\n                ext_columns[field.name] = pandas_dtype\n\n    return ext_columns\n\n\ndef _check_data_column_metadata_consistency(all_columns):\n    # It can never be the case in a released version of pyarrow that\n    # c['name'] is None *and* 'field_name' is not a key in the column metadata,\n    # because the change to allow c['name'] to be None and the change to add\n    # 'field_name' are in the same release (0.8.0)\n    assert all(\n        (c['name'] is None and 'field_name' in c) or c['name'] is not None\n        for c in all_columns\n    )\n\n\ndef _deserialize_column_index(block_table, all_columns, column_indexes):\n    if all_columns:\n        columns_name_dict = {\n            c.get('field_name', _column_name_to_strings(c['name'])): c['name']\n            for c in all_columns\n        }\n        columns_values = [\n            columns_name_dict.get(name, name) for name in block_table.column_names\n        ]\n    else:\n        columns_values = block_table.column_names\n\n    # Construct the base index\n    if len(column_indexes) > 1:\n        # If we're passed multiple column indexes then evaluate with\n        # ast.literal_eval, since the column index values show up as a list of\n        # tuples\n        columns = _pandas_api.pd.MultiIndex.from_tuples(\n            list(map(ast.literal_eval, columns_values)),\n            names=[col_index['name'] for col_index in column_indexes],\n        )\n    else:\n        columns = _pandas_api.pd.Index(\n            columns_values, name=column_indexes[0][\"name\"] if column_indexes else None\n        )\n\n    # if we're reconstructing the index\n    if len(column_indexes) > 0:\n        columns = _reconstruct_columns_from_metadata(columns, column_indexes)\n\n    return columns\n\n\ndef _reconstruct_index(table, index_descriptors, all_columns, types_mapper=None):\n    # 0. 'field_name' is the name of the column in the arrow Table\n    # 1. 'name' is the user-facing name of the column, that is, it came from\n    #    pandas\n    # 2. 'field_name' and 'name' differ for index columns\n    # 3. We fall back on c['name'] for backwards compatibility\n    field_name_to_metadata = {\n        c.get('field_name', c['name']): c\n        for c in all_columns\n    }\n\n    # Build up a list of index columns and names while removing those columns\n    # from the original table\n    index_arrays = []\n    index_names = []\n    result_table = table\n    for descr in index_descriptors:\n        if isinstance(descr, str):\n            result_table, index_level, index_name = _extract_index_level(\n                table, result_table, descr, field_name_to_metadata, types_mapper)\n            if index_level is None:\n                # ARROW-1883: the serialized index column was not found\n                continue\n        elif descr['kind'] == 'range':\n            index_name = descr['name']\n            index_level = _pandas_api.pd.RangeIndex(descr['start'],\n                                                    descr['stop'],\n                                                    step=descr['step'],\n                                                    name=index_name)\n            if len(index_level) != len(table):\n                # Possibly the result of munged metadata\n                continue\n        else:\n            raise ValueError(\"Unrecognized index kind: {}\"\n                             .format(descr['kind']))\n        index_arrays.append(index_level)\n        index_names.append(index_name)\n\n    pd = _pandas_api.pd\n\n    # Reconstruct the row index\n    if len(index_arrays) > 1:\n        index = pd.MultiIndex.from_arrays(index_arrays, names=index_names)\n    elif len(index_arrays) == 1:\n        index = index_arrays[0]\n        if not isinstance(index, pd.Index):\n            # Box anything that wasn't boxed above\n            index = pd.Index(index, name=index_names[0])\n    else:\n        index = pd.RangeIndex(table.num_rows)\n\n    return result_table, index\n\n\ndef _extract_index_level(table, result_table, field_name,\n                         field_name_to_metadata, types_mapper=None):\n    logical_name = field_name_to_metadata[field_name]['name']\n    index_name = _backwards_compatible_index_name(field_name, logical_name)\n    i = table.schema.get_field_index(field_name)\n\n    if i == -1:\n        # The serialized index column was removed by the user\n        return result_table, None, None\n\n    col = table.column(i)\n    index_level = col.to_pandas(types_mapper=types_mapper)\n    index_level.name = None\n    result_table = result_table.remove_column(\n        result_table.schema.get_field_index(field_name)\n    )\n    return result_table, index_level, index_name\n\n\ndef _backwards_compatible_index_name(raw_name, logical_name):\n    \"\"\"Compute the name of an index column that is compatible with older\n    versions of :mod:`pyarrow`.\n\n    Parameters\n    ----------\n    raw_name : str\n    logical_name : str\n\n    Returns\n    -------\n    result : str\n\n    Notes\n    -----\n    * Part of :func:`~pyarrow.pandas_compat.table_to_blockmanager`\n    \"\"\"\n    # Part of table_to_blockmanager\n    if raw_name == logical_name and _is_generated_index_name(raw_name):\n        return None\n    else:\n        return logical_name\n\n\ndef _is_generated_index_name(name):\n    pattern = r'^__index_level_\\d+__$'\n    return re.match(pattern, name) is not None\n\n\n_pandas_logical_type_map = {\n    'date': 'datetime64[D]',\n    'datetime': 'datetime64[ns]',\n    'datetimetz': 'datetime64[ns]',\n    'unicode': np.str_,\n    'bytes': np.bytes_,\n    'string': np.str_,\n    'integer': np.int64,\n    'floating': np.float64,\n    'decimal': np.object_,\n    'empty': np.object_,\n}\n\n\ndef _pandas_type_to_numpy_type(pandas_type):\n    \"\"\"Get the numpy dtype that corresponds to a pandas type.\n\n    Parameters\n    ----------\n    pandas_type : str\n        The result of a call to pandas.lib.infer_dtype.\n\n    Returns\n    -------\n    dtype : np.dtype\n        The dtype that corresponds to `pandas_type`.\n    \"\"\"\n    try:\n        return _pandas_logical_type_map[pandas_type]\n    except KeyError:\n        if 'mixed' in pandas_type:\n            # catching 'mixed', 'mixed-integer' and 'mixed-integer-float'\n            return np.object_\n        return np.dtype(pandas_type)\n\n\ndef _reconstruct_columns_from_metadata(columns, column_indexes):\n    \"\"\"Construct a pandas MultiIndex from `columns` and column index metadata\n    in `column_indexes`.\n\n    Parameters\n    ----------\n    columns : List[pd.Index]\n        The columns coming from a pyarrow.Table\n    column_indexes : List[Dict[str, str]]\n        The column index metadata deserialized from the JSON schema metadata\n        in a :class:`~pyarrow.Table`.\n\n    Returns\n    -------\n    result : MultiIndex\n        The index reconstructed using `column_indexes` metadata with levels of\n        the correct type.\n\n    Notes\n    -----\n    * Part of :func:`~pyarrow.pandas_compat.table_to_blockmanager`\n    \"\"\"\n    pd = _pandas_api.pd\n    # Get levels and labels, and provide sane defaults if the index has a\n    # single level to avoid if/else spaghetti.\n    levels = getattr(columns, 'levels', None) or [columns]\n    labels = getattr(columns, 'codes', None) or [None]\n\n    # Convert each level to the dtype provided in the metadata\n    levels_dtypes = [\n        (level, col_index.get('pandas_type', str(level.dtype)),\n         col_index.get('numpy_type', None))\n        for level, col_index in zip_longest(\n            levels, column_indexes, fillvalue={}\n        )\n    ]\n\n    new_levels = []\n    encoder = operator.methodcaller('encode', 'UTF-8')\n\n    for level, pandas_dtype, numpy_dtype in levels_dtypes:\n        dtype = _pandas_type_to_numpy_type(pandas_dtype)\n        # Since our metadata is UTF-8 encoded, Python turns things that were\n        # bytes into unicode strings when json.loads-ing them. We need to\n        # convert them back to bytes to preserve metadata.\n        if dtype == np.bytes_:\n            level = level.map(encoder)\n        # ARROW-13756: if index is timezone aware DataTimeIndex\n        if pandas_dtype == \"datetimetz\":\n            tz = pa.lib.string_to_tzinfo(\n                column_indexes[0]['metadata']['timezone'])\n            level = pd.to_datetime(level, utc=True).tz_convert(tz)\n            if _pandas_api.is_ge_v3():\n                # with pandas 3+, to_datetime returns a unit depending on the string\n                # data, so we restore it to the original unit from the metadata\n                level = level.as_unit(np.datetime_data(dtype)[0])\n        # GH-41503: if the column index was decimal, restore to decimal\n        elif pandas_dtype == \"decimal\":\n            level = _pandas_api.pd.Index([decimal.Decimal(i) for i in level])\n        elif level.dtype != dtype:\n            level = level.astype(dtype)\n        # ARROW-9096: if original DataFrame was upcast we keep that\n        if level.dtype != numpy_dtype and pandas_dtype != \"datetimetz\":\n            level = level.astype(numpy_dtype)\n\n        new_levels.append(level)\n\n    if len(new_levels) > 1:\n        return pd.MultiIndex(new_levels, labels, names=columns.names)\n    else:\n        return pd.Index(new_levels[0], dtype=new_levels[0].dtype, name=columns.name)\n\n\ndef _add_any_metadata(table, pandas_metadata):\n    modified_columns = {}\n    modified_fields = {}\n\n    schema = table.schema\n\n    index_columns = pandas_metadata['index_columns']\n    # only take index columns into account if they are an actual table column\n    index_columns = [idx_col for idx_col in index_columns\n                     if isinstance(idx_col, str)]\n    n_index_levels = len(index_columns)\n    n_columns = len(pandas_metadata['columns']) - n_index_levels\n\n    # Add time zones\n    for i, col_meta in enumerate(pandas_metadata['columns']):\n\n        raw_name = col_meta.get('field_name')\n        if not raw_name:\n            # deal with metadata written with arrow < 0.8 or fastparquet\n            raw_name = col_meta['name']\n            if i >= n_columns:\n                # index columns\n                raw_name = index_columns[i - n_columns]\n            if raw_name is None:\n                raw_name = 'None'\n\n        idx = schema.get_field_index(raw_name)\n        if idx != -1:\n            if col_meta['pandas_type'] == 'datetimetz':\n                col = table[idx]\n                if not isinstance(col.type, pa.lib.TimestampType):\n                    continue\n                metadata = col_meta['metadata']\n                if not metadata:\n                    continue\n                metadata_tz = metadata.get('timezone')\n                if metadata_tz and metadata_tz != col.type.tz:\n                    converted = col.to_pandas()\n                    tz_aware_type = pa.timestamp('ns', tz=metadata_tz)\n                    with_metadata = pa.Array.from_pandas(converted,\n                                                         type=tz_aware_type)\n\n                    modified_fields[idx] = pa.field(schema[idx].name,\n                                                    tz_aware_type)\n                    modified_columns[idx] = with_metadata\n\n    if len(modified_columns) > 0:\n        columns = []\n        fields = []\n        for i in range(len(table.schema)):\n            if i in modified_columns:\n                columns.append(modified_columns[i])\n                fields.append(modified_fields[i])\n            else:\n                columns.append(table[i])\n                fields.append(table.schema[i])\n        return pa.Table.from_arrays(columns, schema=pa.schema(fields))\n    else:\n        return table\n\n\n# ----------------------------------------------------------------------\n# Helper functions used in lib\n\n\ndef make_tz_aware(series, tz):\n    \"\"\"\n    Make a datetime64 Series timezone-aware for the given tz\n    \"\"\"\n    tz = pa.lib.string_to_tzinfo(tz)\n    series = (series.dt.tz_localize('utc')\n                    .dt.tz_convert(tz))\n    return series\n", "python/pyarrow/csv.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nfrom pyarrow._csv import (  # noqa\n    ReadOptions, ParseOptions, ConvertOptions, ISO8601,\n    open_csv, read_csv, CSVStreamingReader, write_csv,\n    WriteOptions, CSVWriter, InvalidRow)\n", "python/pyarrow/jvm.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"\nFunctions to interact with Arrow memory allocated by Arrow Java.\n\nThese functions convert the objects holding the metadata, the actual\ndata is not copied at all.\n\nThis will only work with a JVM running in the same process such as provided\nthrough jpype. Modules that talk to a remote JVM like py4j will not work as the\nmemory addresses reported by them are not reachable in the python process.\n\"\"\"\n\nimport pyarrow as pa\n\n\nclass _JvmBufferNanny:\n    \"\"\"\n    An object that keeps a org.apache.arrow.memory.ArrowBuf's underlying\n    memory alive.\n    \"\"\"\n    ref_manager = None\n\n    def __init__(self, jvm_buf):\n        ref_manager = jvm_buf.getReferenceManager()\n        # Will raise a java.lang.IllegalArgumentException if the buffer\n        # is already freed.  It seems that exception cannot easily be\n        # caught...\n        ref_manager.retain()\n        self.ref_manager = ref_manager\n\n    def __del__(self):\n        if self.ref_manager is not None:\n            self.ref_manager.release()\n\n\ndef jvm_buffer(jvm_buf):\n    \"\"\"\n    Construct an Arrow buffer from org.apache.arrow.memory.ArrowBuf\n\n    Parameters\n    ----------\n\n    jvm_buf: org.apache.arrow.memory.ArrowBuf\n        Arrow Buffer representation on the JVM.\n\n    Returns\n    -------\n    pyarrow.Buffer\n        Python Buffer that references the JVM memory.\n    \"\"\"\n    nanny = _JvmBufferNanny(jvm_buf)\n    address = jvm_buf.memoryAddress()\n    size = jvm_buf.capacity()\n    return pa.foreign_buffer(address, size, base=nanny)\n\n\ndef _from_jvm_int_type(jvm_type):\n    \"\"\"\n    Convert a JVM int type to its Python equivalent.\n\n    Parameters\n    ----------\n    jvm_type : org.apache.arrow.vector.types.pojo.ArrowType$Int\n\n    Returns\n    -------\n    typ : pyarrow.DataType\n    \"\"\"\n\n    bit_width = jvm_type.getBitWidth()\n    if jvm_type.getIsSigned():\n        if bit_width == 8:\n            return pa.int8()\n        elif bit_width == 16:\n            return pa.int16()\n        elif bit_width == 32:\n            return pa.int32()\n        elif bit_width == 64:\n            return pa.int64()\n    else:\n        if bit_width == 8:\n            return pa.uint8()\n        elif bit_width == 16:\n            return pa.uint16()\n        elif bit_width == 32:\n            return pa.uint32()\n        elif bit_width == 64:\n            return pa.uint64()\n\n\ndef _from_jvm_float_type(jvm_type):\n    \"\"\"\n    Convert a JVM float type to its Python equivalent.\n\n    Parameters\n    ----------\n    jvm_type: org.apache.arrow.vector.types.pojo.ArrowType$FloatingPoint\n\n    Returns\n    -------\n    typ: pyarrow.DataType\n    \"\"\"\n    precision = jvm_type.getPrecision().toString()\n    if precision == 'HALF':\n        return pa.float16()\n    elif precision == 'SINGLE':\n        return pa.float32()\n    elif precision == 'DOUBLE':\n        return pa.float64()\n\n\ndef _from_jvm_time_type(jvm_type):\n    \"\"\"\n    Convert a JVM time type to its Python equivalent.\n\n    Parameters\n    ----------\n    jvm_type: org.apache.arrow.vector.types.pojo.ArrowType$Time\n\n    Returns\n    -------\n    typ: pyarrow.DataType\n    \"\"\"\n    time_unit = jvm_type.getUnit().toString()\n    if time_unit == 'SECOND':\n        assert jvm_type.getBitWidth() == 32\n        return pa.time32('s')\n    elif time_unit == 'MILLISECOND':\n        assert jvm_type.getBitWidth() == 32\n        return pa.time32('ms')\n    elif time_unit == 'MICROSECOND':\n        assert jvm_type.getBitWidth() == 64\n        return pa.time64('us')\n    elif time_unit == 'NANOSECOND':\n        assert jvm_type.getBitWidth() == 64\n        return pa.time64('ns')\n\n\ndef _from_jvm_timestamp_type(jvm_type):\n    \"\"\"\n    Convert a JVM timestamp type to its Python equivalent.\n\n    Parameters\n    ----------\n    jvm_type: org.apache.arrow.vector.types.pojo.ArrowType$Timestamp\n\n    Returns\n    -------\n    typ: pyarrow.DataType\n    \"\"\"\n    time_unit = jvm_type.getUnit().toString()\n    timezone = jvm_type.getTimezone()\n    if timezone is not None:\n        timezone = str(timezone)\n    if time_unit == 'SECOND':\n        return pa.timestamp('s', tz=timezone)\n    elif time_unit == 'MILLISECOND':\n        return pa.timestamp('ms', tz=timezone)\n    elif time_unit == 'MICROSECOND':\n        return pa.timestamp('us', tz=timezone)\n    elif time_unit == 'NANOSECOND':\n        return pa.timestamp('ns', tz=timezone)\n\n\ndef _from_jvm_date_type(jvm_type):\n    \"\"\"\n    Convert a JVM date type to its Python equivalent\n\n    Parameters\n    ----------\n    jvm_type: org.apache.arrow.vector.types.pojo.ArrowType$Date\n\n    Returns\n    -------\n    typ: pyarrow.DataType\n    \"\"\"\n    day_unit = jvm_type.getUnit().toString()\n    if day_unit == 'DAY':\n        return pa.date32()\n    elif day_unit == 'MILLISECOND':\n        return pa.date64()\n\n\ndef field(jvm_field):\n    \"\"\"\n    Construct a Field from a org.apache.arrow.vector.types.pojo.Field\n    instance.\n\n    Parameters\n    ----------\n    jvm_field: org.apache.arrow.vector.types.pojo.Field\n\n    Returns\n    -------\n    pyarrow.Field\n    \"\"\"\n    name = str(jvm_field.getName())\n    jvm_type = jvm_field.getType()\n\n    typ = None\n    if not jvm_type.isComplex():\n        type_str = jvm_type.getTypeID().toString()\n        if type_str == 'Null':\n            typ = pa.null()\n        elif type_str == 'Int':\n            typ = _from_jvm_int_type(jvm_type)\n        elif type_str == 'FloatingPoint':\n            typ = _from_jvm_float_type(jvm_type)\n        elif type_str == 'Utf8':\n            typ = pa.string()\n        elif type_str == 'Binary':\n            typ = pa.binary()\n        elif type_str == 'FixedSizeBinary':\n            typ = pa.binary(jvm_type.getByteWidth())\n        elif type_str == 'Bool':\n            typ = pa.bool_()\n        elif type_str == 'Time':\n            typ = _from_jvm_time_type(jvm_type)\n        elif type_str == 'Timestamp':\n            typ = _from_jvm_timestamp_type(jvm_type)\n        elif type_str == 'Date':\n            typ = _from_jvm_date_type(jvm_type)\n        elif type_str == 'Decimal':\n            typ = pa.decimal128(jvm_type.getPrecision(), jvm_type.getScale())\n        else:\n            raise NotImplementedError(\n                \"Unsupported JVM type: {}\".format(type_str))\n    else:\n        # TODO: The following JVM types are not implemented:\n        #       Struct, List, FixedSizeList, Union, Dictionary\n        raise NotImplementedError(\n            \"JVM field conversion only implemented for primitive types.\")\n\n    nullable = jvm_field.isNullable()\n    jvm_metadata = jvm_field.getMetadata()\n    if jvm_metadata.isEmpty():\n        metadata = None\n    else:\n        metadata = {str(entry.getKey()): str(entry.getValue())\n                    for entry in jvm_metadata.entrySet()}\n    return pa.field(name, typ, nullable, metadata)\n\n\ndef schema(jvm_schema):\n    \"\"\"\n    Construct a Schema from a org.apache.arrow.vector.types.pojo.Schema\n    instance.\n\n    Parameters\n    ----------\n    jvm_schema: org.apache.arrow.vector.types.pojo.Schema\n\n    Returns\n    -------\n    pyarrow.Schema\n    \"\"\"\n    fields = jvm_schema.getFields()\n    fields = [field(f) for f in fields]\n    jvm_metadata = jvm_schema.getCustomMetadata()\n    if jvm_metadata.isEmpty():\n        metadata = None\n    else:\n        metadata = {str(entry.getKey()): str(entry.getValue())\n                    for entry in jvm_metadata.entrySet()}\n    return pa.schema(fields, metadata)\n\n\ndef array(jvm_array):\n    \"\"\"\n    Construct an (Python) Array from its JVM equivalent.\n\n    Parameters\n    ----------\n    jvm_array : org.apache.arrow.vector.ValueVector\n\n    Returns\n    -------\n    array : Array\n    \"\"\"\n    if jvm_array.getField().getType().isComplex():\n        minor_type_str = jvm_array.getMinorType().toString()\n        raise NotImplementedError(\n            \"Cannot convert JVM Arrow array of type {},\"\n            \" complex types not yet implemented.\".format(minor_type_str))\n    dtype = field(jvm_array.getField()).type\n    buffers = [jvm_buffer(buf)\n               for buf in list(jvm_array.getBuffers(False))]\n\n    # If JVM has an empty Vector, buffer list will be empty so create manually\n    if len(buffers) == 0:\n        return pa.array([], type=dtype)\n\n    length = jvm_array.getValueCount()\n    null_count = jvm_array.getNullCount()\n    return pa.Array.from_buffers(dtype, length, buffers, null_count)\n\n\ndef record_batch(jvm_vector_schema_root):\n    \"\"\"\n    Construct a (Python) RecordBatch from a JVM VectorSchemaRoot\n\n    Parameters\n    ----------\n    jvm_vector_schema_root : org.apache.arrow.vector.VectorSchemaRoot\n\n    Returns\n    -------\n    record_batch: pyarrow.RecordBatch\n    \"\"\"\n    pa_schema = schema(jvm_vector_schema_root.getSchema())\n\n    arrays = []\n    for name in pa_schema.names:\n        arrays.append(array(jvm_vector_schema_root.getVector(name)))\n\n    return pa.RecordBatch.from_arrays(\n        arrays,\n        pa_schema.names,\n        metadata=pa_schema.metadata\n    )\n", "python/pyarrow/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# flake8: noqa\n\n\"\"\"\nPyArrow is the python implementation of Apache Arrow.\n\nApache Arrow is a cross-language development platform for in-memory data.\nIt specifies a standardized language-independent columnar memory format for\nflat and hierarchical data, organized for efficient analytic operations on\nmodern hardware. It also provides computational libraries and zero-copy\nstreaming messaging and interprocess communication.\n\nFor more information see the official page at https://arrow.apache.org\n\"\"\"\n\nimport gc as _gc\nimport importlib as _importlib\nimport os as _os\nimport platform as _platform\nimport sys as _sys\nimport warnings as _warnings\n\ntry:\n    from ._generated_version import version as __version__\nexcept ImportError:\n    # Package is not installed, parse git tag at runtime\n    try:\n        import setuptools_scm\n        # Code duplicated from setup.py to avoid a dependency on each other\n\n        def parse_git(root, **kwargs):\n            \"\"\"\n            Parse function for setuptools_scm that ignores tags for non-C++\n            subprojects, e.g. apache-arrow-js-XXX tags.\n            \"\"\"\n            from setuptools_scm.git import parse\n            kwargs['describe_command'] = \\\n                \"git describe --dirty --tags --long --match 'apache-arrow-[0-9]*.*'\"\n            return parse(root, **kwargs)\n        __version__ = setuptools_scm.get_version('../',\n                                                 parse=parse_git)\n    except ImportError:\n        __version__ = None\n\n# ARROW-8684: Disable GC while initializing Cython extension module,\n# to workaround Cython bug in https://github.com/cython/cython/issues/3603\n_gc_enabled = _gc.isenabled()\n_gc.disable()\nimport pyarrow.lib as _lib\nif _gc_enabled:\n    _gc.enable()\n\nfrom pyarrow.lib import (BuildInfo, RuntimeInfo, set_timezone_db_path,\n                         MonthDayNano, VersionInfo, cpp_build_info,\n                         cpp_version, cpp_version_info, runtime_info,\n                         cpu_count, set_cpu_count, enable_signal_handlers,\n                         io_thread_count, set_io_thread_count)\n\n\ndef show_versions():\n    \"\"\"\n    Print various version information, to help with error reporting.\n    \"\"\"\n    def print_entry(label, value):\n        print(f\"{label: <26}: {value: <8}\")\n\n    print(\"pyarrow version info\\n--------------------\")\n    print_entry(\"Package kind\", cpp_build_info.package_kind\n                if len(cpp_build_info.package_kind) > 0\n                else \"not indicated\")\n    print_entry(\"Arrow C++ library version\", cpp_build_info.version)\n    print_entry(\"Arrow C++ compiler\",\n                f\"{cpp_build_info.compiler_id} {cpp_build_info.compiler_version}\")\n    print_entry(\"Arrow C++ compiler flags\", cpp_build_info.compiler_flags)\n    print_entry(\"Arrow C++ git revision\", cpp_build_info.git_id)\n    print_entry(\"Arrow C++ git description\", cpp_build_info.git_description)\n    print_entry(\"Arrow C++ build type\", cpp_build_info.build_type)\n\n\ndef _module_is_available(module):\n    try:\n        _importlib.import_module(f'pyarrow.{module}')\n    except ImportError:\n        return False\n    else:\n        return True\n\n\ndef _filesystem_is_available(fs):\n    try:\n        import pyarrow.fs\n    except ImportError:\n        return False\n\n    try:\n        getattr(pyarrow.fs, fs)\n    except (ImportError, AttributeError):\n        return False\n    else:\n        return True\n\n\ndef show_info():\n    \"\"\"\n    Print detailed version and platform information, for error reporting\n    \"\"\"\n    show_versions()\n\n    def print_entry(label, value):\n        print(f\"  {label: <20}: {value: <8}\")\n\n    print(\"\\nPlatform:\")\n    print_entry(\"OS / Arch\", f\"{_platform.system()} {_platform.machine()}\")\n    print_entry(\"SIMD Level\", runtime_info().simd_level)\n    print_entry(\"Detected SIMD Level\", runtime_info().detected_simd_level)\n\n    pool = default_memory_pool()\n    print(\"\\nMemory:\")\n    print_entry(\"Default backend\", pool.backend_name)\n    print_entry(\"Bytes allocated\", f\"{pool.bytes_allocated()} bytes\")\n    print_entry(\"Max memory\", f\"{pool.max_memory()} bytes\")\n    print_entry(\"Supported Backends\", ', '.join(supported_memory_backends()))\n\n    print(\"\\nOptional modules:\")\n    modules = [\"csv\", \"cuda\", \"dataset\", \"feather\", \"flight\", \"fs\", \"gandiva\", \"json\",\n               \"orc\", \"parquet\"]\n    for module in modules:\n        status = \"Enabled\" if _module_is_available(module) else \"-\"\n        print(f\"  {module: <20}: {status: <8}\")\n\n    print(\"\\nFilesystems:\")\n    filesystems = [\"AzureFileSystem\", \"GcsFileSystem\",\n                   \"HadoopFileSystem\", \"S3FileSystem\"]\n    for fs in filesystems:\n        status = \"Enabled\" if _filesystem_is_available(fs) else \"-\"\n        print(f\"  {fs: <20}: {status: <8}\")\n\n    print(\"\\nCompression Codecs:\")\n    codecs = [\"brotli\", \"bz2\", \"gzip\", \"lz4_frame\", \"lz4\", \"snappy\", \"zstd\"]\n    for codec in codecs:\n        status = \"Enabled\" if Codec.is_available(codec) else \"-\"\n        print(f\"  {codec: <20}: {status: <8}\")\n\n\nfrom pyarrow.lib import (null, bool_,\n                         int8, int16, int32, int64,\n                         uint8, uint16, uint32, uint64,\n                         time32, time64, timestamp, date32, date64, duration,\n                         month_day_nano_interval,\n                         float16, float32, float64,\n                         binary, string, utf8, binary_view, string_view,\n                         large_binary, large_string, large_utf8,\n                         decimal128, decimal256,\n                         list_, large_list, list_view, large_list_view,\n                         map_, struct,\n                         union, sparse_union, dense_union,\n                         dictionary,\n                         run_end_encoded,\n                         fixed_shape_tensor,\n                         field,\n                         type_for_alias,\n                         DataType, DictionaryType, StructType,\n                         ListType, LargeListType, FixedSizeListType,\n                         ListViewType, LargeListViewType,\n                         MapType, UnionType, SparseUnionType, DenseUnionType,\n                         TimestampType, Time32Type, Time64Type, DurationType,\n                         FixedSizeBinaryType, Decimal128Type, Decimal256Type,\n                         BaseExtensionType, ExtensionType,\n                         RunEndEncodedType, FixedShapeTensorType,\n                         PyExtensionType, UnknownExtensionType,\n                         register_extension_type, unregister_extension_type,\n                         DictionaryMemo,\n                         KeyValueMetadata,\n                         Field,\n                         Schema,\n                         schema,\n                         unify_schemas,\n                         Array, Tensor,\n                         array, chunked_array, record_batch, nulls, repeat,\n                         SparseCOOTensor, SparseCSRMatrix, SparseCSCMatrix,\n                         SparseCSFTensor,\n                         infer_type, from_numpy_dtype,\n                         NullArray,\n                         NumericArray, IntegerArray, FloatingPointArray,\n                         BooleanArray,\n                         Int8Array, UInt8Array,\n                         Int16Array, UInt16Array,\n                         Int32Array, UInt32Array,\n                         Int64Array, UInt64Array,\n                         HalfFloatArray, FloatArray, DoubleArray,\n                         ListArray, LargeListArray, FixedSizeListArray,\n                         ListViewArray, LargeListViewArray,\n                         MapArray, UnionArray,\n                         BinaryArray, StringArray,\n                         LargeBinaryArray, LargeStringArray,\n                         BinaryViewArray, StringViewArray,\n                         FixedSizeBinaryArray,\n                         DictionaryArray,\n                         Date32Array, Date64Array, TimestampArray,\n                         Time32Array, Time64Array, DurationArray,\n                         MonthDayNanoIntervalArray,\n                         Decimal128Array, Decimal256Array, StructArray, ExtensionArray,\n                         RunEndEncodedArray, FixedShapeTensorArray,\n                         scalar, NA, _NULL as NULL, Scalar,\n                         NullScalar, BooleanScalar,\n                         Int8Scalar, Int16Scalar, Int32Scalar, Int64Scalar,\n                         UInt8Scalar, UInt16Scalar, UInt32Scalar, UInt64Scalar,\n                         HalfFloatScalar, FloatScalar, DoubleScalar,\n                         Decimal128Scalar, Decimal256Scalar,\n                         ListScalar, LargeListScalar, FixedSizeListScalar,\n                         ListViewScalar, LargeListViewScalar,\n                         Date32Scalar, Date64Scalar,\n                         Time32Scalar, Time64Scalar,\n                         TimestampScalar, DurationScalar,\n                         MonthDayNanoIntervalScalar,\n                         BinaryScalar, LargeBinaryScalar, BinaryViewScalar,\n                         StringScalar, LargeStringScalar, StringViewScalar,\n                         FixedSizeBinaryScalar, DictionaryScalar,\n                         MapScalar, StructScalar, UnionScalar,\n                         RunEndEncodedScalar, ExtensionScalar)\n\n# Buffers, allocation\nfrom pyarrow.lib import (DeviceAllocationType, Device, MemoryManager,\n                         default_cpu_memory_manager)\n\nfrom pyarrow.lib import (Buffer, ResizableBuffer, foreign_buffer, py_buffer,\n                         Codec, compress, decompress, allocate_buffer)\n\nfrom pyarrow.lib import (MemoryPool, LoggingMemoryPool, ProxyMemoryPool,\n                         total_allocated_bytes, set_memory_pool,\n                         default_memory_pool, system_memory_pool,\n                         jemalloc_memory_pool, mimalloc_memory_pool,\n                         logging_memory_pool, proxy_memory_pool,\n                         log_memory_allocations, jemalloc_set_decay_ms,\n                         supported_memory_backends)\n\n# I/O\nfrom pyarrow.lib import (NativeFile, PythonFile,\n                         BufferedInputStream, BufferedOutputStream, CacheOptions,\n                         CompressedInputStream, CompressedOutputStream,\n                         TransformInputStream, transcoding_input_stream,\n                         FixedSizeBufferWriter,\n                         BufferReader, BufferOutputStream,\n                         OSFile, MemoryMappedFile, memory_map,\n                         create_memory_map, MockOutputStream,\n                         input_stream, output_stream,\n                         have_libhdfs)\n\nfrom pyarrow.lib import (ChunkedArray, RecordBatch, Table, table,\n                         concat_arrays, concat_tables, TableGroupBy,\n                         RecordBatchReader)\n\n# Exceptions\nfrom pyarrow.lib import (ArrowCancelled,\n                         ArrowCapacityError,\n                         ArrowException,\n                         ArrowKeyError,\n                         ArrowIndexError,\n                         ArrowInvalid,\n                         ArrowIOError,\n                         ArrowMemoryError,\n                         ArrowNotImplementedError,\n                         ArrowTypeError,\n                         ArrowSerializationError)\n\nfrom pyarrow.ipc import serialize_pandas, deserialize_pandas\nimport pyarrow.ipc as ipc\n\nimport pyarrow.types as types\n\n\n# ----------------------------------------------------------------------\n# Deprecations\n\nfrom pyarrow.util import _deprecate_api, _deprecate_class\n\n\n# TODO: Deprecate these somehow in the pyarrow namespace\nfrom pyarrow.ipc import (Message, MessageReader, MetadataVersion,\n                         RecordBatchFileReader, RecordBatchFileWriter,\n                         RecordBatchStreamReader, RecordBatchStreamWriter)\n\n# ----------------------------------------------------------------------\n# Returning absolute path to the pyarrow include directory (if bundled, e.g. in\n# wheels)\n\n\ndef get_include():\n    \"\"\"\n    Return absolute path to directory containing Arrow C++ include\n    headers. Similar to numpy.get_include\n    \"\"\"\n    return _os.path.join(_os.path.dirname(__file__), 'include')\n\n\ndef _get_pkg_config_executable():\n    return _os.environ.get('PKG_CONFIG', 'pkg-config')\n\n\ndef _has_pkg_config(pkgname):\n    import subprocess\n    try:\n        return subprocess.call([_get_pkg_config_executable(),\n                                '--exists', pkgname]) == 0\n    except FileNotFoundError:\n        return False\n\n\ndef _read_pkg_config_variable(pkgname, cli_args):\n    import subprocess\n    cmd = [_get_pkg_config_executable(), pkgname] + cli_args\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE)\n    out, err = proc.communicate()\n    if proc.returncode != 0:\n        raise RuntimeError(\"pkg-config failed: \" + err.decode('utf8'))\n    return out.rstrip().decode('utf8')\n\n\ndef get_libraries():\n    \"\"\"\n    Return list of library names to include in the `libraries` argument for C\n    or Cython extensions using pyarrow\n    \"\"\"\n    return ['arrow_python', 'arrow']\n\n\ndef create_library_symlinks():\n    \"\"\"\n    With Linux and macOS wheels, the bundled shared libraries have an embedded\n    ABI version like libarrow.so.17 or libarrow.17.dylib and so linking to them\n    with -larrow won't work unless we create symlinks at locations like\n    site-packages/pyarrow/libarrow.so. This unfortunate workaround addresses\n    prior problems we had with shipping two copies of the shared libraries to\n    permit third party projects like turbodbc to build their C++ extensions\n    against the pyarrow wheels.\n\n    This function must only be invoked once and only when the shared libraries\n    are bundled with the Python package, which should only apply to wheel-based\n    installs. It requires write access to the site-packages/pyarrow directory\n    and so depending on your system may need to be run with root.\n    \"\"\"\n    import glob\n    if _sys.platform == 'win32':\n        return\n    package_cwd = _os.path.dirname(__file__)\n\n    if _sys.platform == 'linux':\n        bundled_libs = glob.glob(_os.path.join(package_cwd, '*.so.*'))\n\n        def get_symlink_path(hard_path):\n            return hard_path.rsplit('.', 1)[0]\n    else:\n        bundled_libs = glob.glob(_os.path.join(package_cwd, '*.*.dylib'))\n\n        def get_symlink_path(hard_path):\n            return '.'.join((hard_path.rsplit('.', 2)[0], 'dylib'))\n\n    for lib_hard_path in bundled_libs:\n        symlink_path = get_symlink_path(lib_hard_path)\n        if _os.path.exists(symlink_path):\n            continue\n        try:\n            _os.symlink(lib_hard_path, symlink_path)\n        except PermissionError:\n            print(\"Tried creating symlink {}. If you need to link to \"\n                  \"bundled shared libraries, run \"\n                  \"pyarrow.create_library_symlinks() as root\")\n\n\ndef get_library_dirs():\n    \"\"\"\n    Return lists of directories likely to contain Arrow C++ libraries for\n    linking C or Cython extensions using pyarrow\n    \"\"\"\n    package_cwd = _os.path.dirname(__file__)\n    library_dirs = [package_cwd]\n\n    def append_library_dir(library_dir):\n        if library_dir not in library_dirs:\n            library_dirs.append(library_dir)\n\n    # Search library paths via pkg-config. This is necessary if the user\n    # installed libarrow and the other shared libraries manually and they\n    # are not shipped inside the pyarrow package (see also ARROW-2976).\n    pkg_config_executable = _os.environ.get('PKG_CONFIG') or 'pkg-config'\n    for pkgname in [\"arrow\", \"arrow_python\"]:\n        if _has_pkg_config(pkgname):\n            library_dir = _read_pkg_config_variable(pkgname,\n                                                    [\"--libs-only-L\"])\n            # pkg-config output could be empty if Arrow is installed\n            # as a system package.\n            if library_dir:\n                if not library_dir.startswith(\"-L\"):\n                    raise ValueError(\n                        \"pkg-config --libs-only-L returned unexpected \"\n                        \"value {!r}\".format(library_dir))\n                append_library_dir(library_dir[2:])\n\n    if _sys.platform == 'win32':\n        # TODO(wesm): Is this necessary, or does setuptools within a conda\n        # installation add Library\\lib to the linker path for MSVC?\n        python_base_install = _os.path.dirname(_sys.executable)\n        library_dir = _os.path.join(python_base_install, 'Library', 'lib')\n\n        if _os.path.exists(_os.path.join(library_dir, 'arrow.lib')):\n            append_library_dir(library_dir)\n\n    # ARROW-4074: Allow for ARROW_HOME to be set to some other directory\n    if _os.environ.get('ARROW_HOME'):\n        append_library_dir(_os.path.join(_os.environ['ARROW_HOME'], 'lib'))\n    else:\n        # Python wheels bundle the Arrow libraries in the pyarrow directory.\n        append_library_dir(_os.path.dirname(_os.path.abspath(__file__)))\n\n    return library_dirs\n", "python/pyarrow/acero.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# ---------------------------------------------------------------------\n# Implement Internal ExecPlan bindings\n\n# cython: profile=False\n# distutils: language = c++\n# cython: language_level = 3\n\nfrom pyarrow.lib import Table\nfrom pyarrow.compute import Expression, field\n\ntry:\n    from pyarrow._acero import (  # noqa\n        Declaration,\n        ExecNodeOptions,\n        TableSourceNodeOptions,\n        FilterNodeOptions,\n        ProjectNodeOptions,\n        AggregateNodeOptions,\n        OrderByNodeOptions,\n        HashJoinNodeOptions,\n        AsofJoinNodeOptions,\n    )\nexcept ImportError as exc:\n    raise ImportError(\n        f\"The pyarrow installation is not built with support for 'acero' ({str(exc)})\"\n    ) from None\n\n\ntry:\n    import pyarrow.dataset as ds\n    from pyarrow._dataset import ScanNodeOptions\nexcept ImportError:\n    class DatasetModuleStub:\n        class Dataset:\n            pass\n\n        class InMemoryDataset:\n            pass\n    ds = DatasetModuleStub\n\n\ndef _dataset_to_decl(dataset, use_threads=True):\n    decl = Declaration(\"scan\", ScanNodeOptions(dataset, use_threads=use_threads))\n\n    # Get rid of special dataset columns\n    # \"__fragment_index\", \"__batch_index\", \"__last_in_fragment\", \"__filename\"\n    projections = [field(f) for f in dataset.schema.names]\n    decl = Declaration.from_sequence(\n        [decl, Declaration(\"project\", ProjectNodeOptions(projections))]\n    )\n\n    filter_expr = dataset._scan_options.get(\"filter\")\n    if filter_expr is not None:\n        # Filters applied in CScanNodeOptions are \"best effort\" for the scan node itself\n        # so we always need to inject an additional Filter node to apply them for real.\n        decl = Declaration.from_sequence(\n            [decl, Declaration(\"filter\", FilterNodeOptions(filter_expr))]\n        )\n\n    return decl\n\n\ndef _perform_join(join_type, left_operand, left_keys,\n                  right_operand, right_keys,\n                  left_suffix=None, right_suffix=None,\n                  use_threads=True, coalesce_keys=False,\n                  output_type=Table):\n    \"\"\"\n    Perform join of two tables or datasets.\n\n    The result will be an output table with the result of the join operation\n\n    Parameters\n    ----------\n    join_type : str\n        One of supported join types.\n    left_operand : Table or Dataset\n        The left operand for the join operation.\n    left_keys : str or list[str]\n        The left key (or keys) on which the join operation should be performed.\n    right_operand : Table or Dataset\n        The right operand for the join operation.\n    right_keys : str or list[str]\n        The right key (or keys) on which the join operation should be performed.\n    left_suffix : str, default None\n        Which suffix to add to left column names. This prevents confusion\n        when the columns in left and right operands have colliding names.\n    right_suffix : str, default None\n        Which suffix to add to the right column names. This prevents confusion\n        when the columns in left and right operands have colliding names.\n    use_threads : bool, default True\n        Whether to use multithreading or not.\n    coalesce_keys : bool, default False\n        If the duplicated keys should be omitted from one of the sides\n        in the join result.\n    output_type: Table or InMemoryDataset\n        The output type for the exec plan result.\n\n    Returns\n    -------\n    result_table : Table or InMemoryDataset\n    \"\"\"\n    if not isinstance(left_operand, (Table, ds.Dataset)):\n        raise TypeError(f\"Expected Table or Dataset, got {type(left_operand)}\")\n    if not isinstance(right_operand, (Table, ds.Dataset)):\n        raise TypeError(f\"Expected Table or Dataset, got {type(right_operand)}\")\n\n    # Prepare left and right tables Keys to send them to the C++ function\n    left_keys_order = {}\n    if not isinstance(left_keys, (tuple, list)):\n        left_keys = [left_keys]\n    for idx, key in enumerate(left_keys):\n        left_keys_order[key] = idx\n\n    right_keys_order = {}\n    if not isinstance(right_keys, (list, tuple)):\n        right_keys = [right_keys]\n    for idx, key in enumerate(right_keys):\n        right_keys_order[key] = idx\n\n    # By default expose all columns on both left and right table\n    left_columns = left_operand.schema.names\n    right_columns = right_operand.schema.names\n\n    # Pick the join type\n    if join_type == \"left semi\" or join_type == \"left anti\":\n        right_columns = []\n    elif join_type == \"right semi\" or join_type == \"right anti\":\n        left_columns = []\n    elif join_type == \"inner\" or join_type == \"left outer\":\n        right_columns = [\n            col for col in right_columns if col not in right_keys_order\n        ]\n    elif join_type == \"right outer\":\n        left_columns = [\n            col for col in left_columns if col not in left_keys_order\n        ]\n\n    # Turn the columns to vectors of FieldRefs\n    # and set aside indices of keys.\n    left_column_keys_indices = {}\n    for idx, colname in enumerate(left_columns):\n        if colname in left_keys:\n            left_column_keys_indices[colname] = idx\n    right_column_keys_indices = {}\n    for idx, colname in enumerate(right_columns):\n        if colname in right_keys:\n            right_column_keys_indices[colname] = idx\n\n    # Add the join node to the execplan\n    if isinstance(left_operand, ds.Dataset):\n        left_source = _dataset_to_decl(left_operand, use_threads=use_threads)\n    else:\n        left_source = Declaration(\"table_source\", TableSourceNodeOptions(left_operand))\n    if isinstance(right_operand, ds.Dataset):\n        right_source = _dataset_to_decl(right_operand, use_threads=use_threads)\n    else:\n        right_source = Declaration(\n            \"table_source\", TableSourceNodeOptions(right_operand)\n        )\n\n    if coalesce_keys:\n        join_opts = HashJoinNodeOptions(\n            join_type, left_keys, right_keys, left_columns, right_columns,\n            output_suffix_for_left=left_suffix or \"\",\n            output_suffix_for_right=right_suffix or \"\",\n        )\n    else:\n        join_opts = HashJoinNodeOptions(\n            join_type, left_keys, right_keys,\n            output_suffix_for_left=left_suffix or \"\",\n            output_suffix_for_right=right_suffix or \"\",\n        )\n    decl = Declaration(\n        \"hashjoin\", options=join_opts, inputs=[left_source, right_source]\n    )\n\n    if coalesce_keys and join_type == \"full outer\":\n        # In case of full outer joins, the join operation will output all columns\n        # so that we can coalesce the keys and exclude duplicates in a subsequent\n        # projection.\n        left_columns_set = set(left_columns)\n        right_columns_set = set(right_columns)\n        # Where the right table columns start.\n        right_operand_index = len(left_columns)\n        projected_col_names = []\n        projections = []\n        for idx, col in enumerate(left_columns + right_columns):\n            if idx < len(left_columns) and col in left_column_keys_indices:\n                # Include keys only once and coalesce left+right table keys.\n                projected_col_names.append(col)\n                # Get the index of the right key that is being paired\n                # with this left key. We do so by retrieving the name\n                # of the right key that is in the same position in the provided keys\n                # and then looking up the index for that name in the right table.\n                right_key_index = right_column_keys_indices[\n                    right_keys[left_keys_order[col]]]\n                projections.append(\n                    Expression._call(\"coalesce\", [\n                        Expression._field(idx), Expression._field(\n                            right_operand_index+right_key_index)\n                    ])\n                )\n            elif idx >= right_operand_index and col in right_column_keys_indices:\n                # Do not include right table keys. As they would lead to duplicated keys\n                continue\n            else:\n                # For all the other columns include them as they are.\n                # Just recompute the suffixes that the join produced as the projection\n                # would lose them otherwise.\n                if (\n                    left_suffix and idx < right_operand_index\n                    and col in right_columns_set\n                ):\n                    col += left_suffix\n                if (\n                    right_suffix and idx >= right_operand_index\n                    and col in left_columns_set\n                ):\n                    col += right_suffix\n                projected_col_names.append(col)\n                projections.append(\n                    Expression._field(idx)\n                )\n        projection = Declaration(\n            \"project\", ProjectNodeOptions(projections, projected_col_names)\n        )\n        decl = Declaration.from_sequence([decl, projection])\n\n    result_table = decl.to_table(use_threads=use_threads)\n\n    if output_type == Table:\n        return result_table\n    elif output_type == ds.InMemoryDataset:\n        return ds.InMemoryDataset(result_table)\n    else:\n        raise TypeError(\"Unsupported output type\")\n\n\ndef _perform_join_asof(left_operand, left_on, left_by,\n                       right_operand, right_on, right_by,\n                       tolerance, use_threads=True,\n                       output_type=Table):\n    \"\"\"\n    Perform asof join of two tables or datasets.\n\n    The result will be an output table with the result of the join operation\n\n    Parameters\n    ----------\n    left_operand : Table or Dataset\n        The left operand for the join operation.\n    left_on : str\n        The left key (or keys) on which the join operation should be performed.\n    left_by: str or list[str]\n        The left key (or keys) on which the join operation should be performed.\n    right_operand : Table or Dataset\n        The right operand for the join operation.\n    right_on : str or list[str]\n        The right key (or keys) on which the join operation should be performed.\n    right_by: str or list[str]\n        The right key (or keys) on which the join operation should be performed.\n    tolerance : int\n        The tolerance to use for the asof join. The tolerance is interpreted in\n        the same units as the \"on\" key.\n    output_type: Table or InMemoryDataset\n        The output type for the exec plan result.\n\n    Returns\n    -------\n    result_table : Table or InMemoryDataset\n    \"\"\"\n    if not isinstance(left_operand, (Table, ds.Dataset)):\n        raise TypeError(f\"Expected Table or Dataset, got {type(left_operand)}\")\n    if not isinstance(right_operand, (Table, ds.Dataset)):\n        raise TypeError(f\"Expected Table or Dataset, got {type(right_operand)}\")\n\n    if not isinstance(left_by, (tuple, list)):\n        left_by = [left_by]\n    if not isinstance(right_by, (tuple, list)):\n        right_by = [right_by]\n\n    # AsofJoin does not return on or by columns for right_operand.\n    right_columns = [\n        col for col in right_operand.schema.names\n        if col not in [right_on] + right_by\n    ]\n    columns_collisions = set(left_operand.schema.names) & set(right_columns)\n    if columns_collisions:\n        raise ValueError(\n            \"Columns {} present in both tables. AsofJoin does not support \"\n            \"column collisions.\".format(columns_collisions),\n        )\n\n    # Add the join node to the execplan\n    if isinstance(left_operand, ds.Dataset):\n        left_source = _dataset_to_decl(left_operand, use_threads=use_threads)\n    else:\n        left_source = Declaration(\n            \"table_source\", TableSourceNodeOptions(left_operand),\n        )\n    if isinstance(right_operand, ds.Dataset):\n        right_source = _dataset_to_decl(right_operand, use_threads=use_threads)\n    else:\n        right_source = Declaration(\n            \"table_source\", TableSourceNodeOptions(right_operand)\n        )\n\n    join_opts = AsofJoinNodeOptions(\n        left_on, left_by, right_on, right_by, tolerance\n    )\n    decl = Declaration(\n        \"asofjoin\", options=join_opts, inputs=[left_source, right_source]\n    )\n\n    result_table = decl.to_table(use_threads=use_threads)\n\n    if output_type == Table:\n        return result_table\n    elif output_type == ds.InMemoryDataset:\n        return ds.InMemoryDataset(result_table)\n    else:\n        raise TypeError(\"Unsupported output type\")\n\n\ndef _filter_table(table, expression):\n    \"\"\"Filter rows of a table based on the provided expression.\n\n    The result will be an output table with only the rows matching\n    the provided expression.\n\n    Parameters\n    ----------\n    table : Table or Dataset\n        Table or Dataset that should be filtered.\n    expression : Expression\n        The expression on which rows should be filtered.\n\n    Returns\n    -------\n    Table\n    \"\"\"\n    decl = Declaration.from_sequence([\n        Declaration(\"table_source\", options=TableSourceNodeOptions(table)),\n        Declaration(\"filter\", options=FilterNodeOptions(expression))\n    ])\n    return decl.to_table(use_threads=True)\n\n\ndef _sort_source(table_or_dataset, sort_keys, output_type=Table, **kwargs):\n\n    if isinstance(table_or_dataset, ds.Dataset):\n        data_source = _dataset_to_decl(table_or_dataset, use_threads=True)\n    else:\n        data_source = Declaration(\n            \"table_source\", TableSourceNodeOptions(table_or_dataset)\n        )\n\n    order_by = Declaration(\"order_by\", OrderByNodeOptions(sort_keys, **kwargs))\n\n    decl = Declaration.from_sequence([data_source, order_by])\n    result_table = decl.to_table(use_threads=True)\n\n    if output_type == Table:\n        return result_table\n    elif output_type == ds.InMemoryDataset:\n        return ds.InMemoryDataset(result_table)\n    else:\n        raise TypeError(\"Unsupported output type\")\n\n\ndef _group_by(table, aggregates, keys, use_threads=True):\n\n    decl = Declaration.from_sequence([\n        Declaration(\"table_source\", TableSourceNodeOptions(table)),\n        Declaration(\"aggregate\", AggregateNodeOptions(aggregates, keys=keys))\n    ])\n    return decl.to_table(use_threads=use_threads)\n", "python/pyarrow/cffi.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import absolute_import\n\nimport cffi\n\nc_source = \"\"\"\n    struct ArrowSchema {\n      // Array type description\n      const char* format;\n      const char* name;\n      const char* metadata;\n      int64_t flags;\n      int64_t n_children;\n      struct ArrowSchema** children;\n      struct ArrowSchema* dictionary;\n\n      // Release callback\n      void (*release)(struct ArrowSchema*);\n      // Opaque producer-specific data\n      void* private_data;\n    };\n\n    struct ArrowArray {\n      // Array data description\n      int64_t length;\n      int64_t null_count;\n      int64_t offset;\n      int64_t n_buffers;\n      int64_t n_children;\n      const void** buffers;\n      struct ArrowArray** children;\n      struct ArrowArray* dictionary;\n\n      // Release callback\n      void (*release)(struct ArrowArray*);\n      // Opaque producer-specific data\n      void* private_data;\n    };\n\n    struct ArrowArrayStream {\n      int (*get_schema)(struct ArrowArrayStream*, struct ArrowSchema* out);\n      int (*get_next)(struct ArrowArrayStream*, struct ArrowArray* out);\n\n      const char* (*get_last_error)(struct ArrowArrayStream*);\n\n      // Release callback\n      void (*release)(struct ArrowArrayStream*);\n      // Opaque producer-specific data\n      void* private_data;\n    };\n\n    typedef int32_t ArrowDeviceType;\n\n    struct ArrowDeviceArray {\n      struct ArrowArray array;\n      int64_t device_id;\n      ArrowDeviceType device_type;\n      void* sync_event;\n      int64_t reserved[3];\n    };\n    \"\"\"\n\n# TODO use out-of-line mode for faster import and avoid C parsing\nffi = cffi.FFI()\nffi.cdef(c_source)\n", "python/pyarrow/interchange/column.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import annotations\n\nimport enum\nfrom typing import (\n    Any,\n    Dict,\n    Iterable,\n    Optional,\n    Tuple,\n)\n\nimport sys\nif sys.version_info >= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nimport pyarrow as pa\nimport pyarrow.compute as pc\nfrom pyarrow.interchange.buffer import _PyArrowBuffer\n\n\nclass DtypeKind(enum.IntEnum):\n    \"\"\"\n    Integer enum for data types.\n\n    Attributes\n    ----------\n    INT : int\n        Matches to signed integer data type.\n    UINT : int\n        Matches to unsigned integer data type.\n    FLOAT : int\n        Matches to floating point data type.\n    BOOL : int\n        Matches to boolean data type.\n    STRING : int\n        Matches to string data type (UTF-8 encoded).\n    DATETIME : int\n        Matches to datetime data type.\n    CATEGORICAL : int\n        Matches to categorical data type.\n    \"\"\"\n\n    INT = 0\n    UINT = 1\n    FLOAT = 2\n    BOOL = 20\n    STRING = 21  # UTF-8\n    DATETIME = 22\n    CATEGORICAL = 23\n\n\nDtype = Tuple[DtypeKind, int, str, str]  # see Column.dtype\n\n\n_PYARROW_KINDS = {\n    pa.int8(): (DtypeKind.INT, \"c\"),\n    pa.int16(): (DtypeKind.INT, \"s\"),\n    pa.int32(): (DtypeKind.INT, \"i\"),\n    pa.int64(): (DtypeKind.INT, \"l\"),\n    pa.uint8(): (DtypeKind.UINT, \"C\"),\n    pa.uint16(): (DtypeKind.UINT, \"S\"),\n    pa.uint32(): (DtypeKind.UINT, \"I\"),\n    pa.uint64(): (DtypeKind.UINT, \"L\"),\n    pa.float16(): (DtypeKind.FLOAT, \"e\"),\n    pa.float32(): (DtypeKind.FLOAT, \"f\"),\n    pa.float64(): (DtypeKind.FLOAT, \"g\"),\n    pa.bool_(): (DtypeKind.BOOL, \"b\"),\n    pa.string(): (DtypeKind.STRING, \"u\"),\n    pa.large_string(): (DtypeKind.STRING, \"U\"),\n}\n\n\nclass ColumnNullType(enum.IntEnum):\n    \"\"\"\n    Integer enum for null type representation.\n\n    Attributes\n    ----------\n    NON_NULLABLE : int\n        Non-nullable column.\n    USE_NAN : int\n        Use explicit float NaN value.\n    USE_SENTINEL : int\n        Sentinel value besides NaN.\n    USE_BITMASK : int\n        The bit is set/unset representing a null on a certain position.\n    USE_BYTEMASK : int\n        The byte is set/unset representing a null on a certain position.\n    \"\"\"\n\n    NON_NULLABLE = 0\n    USE_NAN = 1\n    USE_SENTINEL = 2\n    USE_BITMASK = 3\n    USE_BYTEMASK = 4\n\n\nclass ColumnBuffers(TypedDict):\n    # first element is a buffer containing the column data;\n    # second element is the data buffer's associated dtype\n    data: Tuple[_PyArrowBuffer, Dtype]\n\n    # first element is a buffer containing mask values indicating missing data;\n    # second element is the mask value buffer's associated dtype.\n    # None if the null representation is not a bit or byte mask\n    validity: Optional[Tuple[_PyArrowBuffer, Dtype]]\n\n    # first element is a buffer containing the offset values for\n    # variable-size binary data (e.g., variable-length strings);\n    # second element is the offsets buffer's associated dtype.\n    # None if the data buffer does not have an associated offsets buffer\n    offsets: Optional[Tuple[_PyArrowBuffer, Dtype]]\n\n\nclass CategoricalDescription(TypedDict):\n    # whether the ordering of dictionary indices is semantically meaningful\n    is_ordered: bool\n    # whether a dictionary-style mapping of categorical values to other objects\n    # exists\n    is_dictionary: bool\n    # Python-level only (e.g. ``{int: str}``).\n    # None if not a dictionary-style categorical.\n    categories: Optional[_PyArrowColumn]\n\n\nclass Endianness:\n    \"\"\"Enum indicating the byte-order of a data-type.\"\"\"\n\n    LITTLE = \"<\"\n    BIG = \">\"\n    NATIVE = \"=\"\n    NA = \"|\"\n\n\nclass NoBufferPresent(Exception):\n    \"\"\"Exception to signal that there is no requested buffer.\"\"\"\n\n\nclass _PyArrowColumn:\n    \"\"\"\n    A column object, with only the methods and properties required by the\n    interchange protocol defined.\n\n    A column can contain one or more chunks. Each chunk can contain up to three\n    buffers - a data buffer, a mask buffer (depending on null representation),\n    and an offsets buffer (if variable-size binary; e.g., variable-length\n    strings).\n\n    TBD: Arrow has a separate \"null\" dtype, and has no separate mask concept.\n         Instead, it seems to use \"children\" for both columns with a bit mask,\n         and for nested dtypes. Unclear whether this is elegant or confusing.\n         This design requires checking the null representation explicitly.\n\n         The Arrow design requires checking:\n         1. the ARROW_FLAG_NULLABLE (for sentinel values)\n         2. if a column has two children, combined with one of those children\n            having a null dtype.\n\n         Making the mask concept explicit seems useful. One null dtype would\n         not be enough to cover both bit and byte masks, so that would mean\n         even more checking if we did it the Arrow way.\n\n    TBD: there's also the \"chunk\" concept here, which is implicit in Arrow as\n         multiple buffers per array (= column here). Semantically it may make\n         sense to have both: chunks were meant for example for lazy evaluation\n         of data which doesn't fit in memory, while multiple buffers per column\n         could also come from doing a selection operation on a single\n         contiguous buffer.\n\n         Given these concepts, one would expect chunks to be all of the same\n         size (say a 10,000 row dataframe could have 10 chunks of 1,000 rows),\n         while multiple buffers could have data-dependent lengths. Not an issue\n         in pandas if one column is backed by a single NumPy array, but in\n         Arrow it seems possible.\n         Are multiple chunks *and* multiple buffers per column necessary for\n         the purposes of this interchange protocol, or must producers either\n         reuse the chunk concept for this or copy the data?\n\n    Note: this Column object can only be produced by ``__dataframe__``, so\n          doesn't need its own version or ``__column__`` protocol.\n    \"\"\"\n\n    def __init__(\n        self, column: pa.Array | pa.ChunkedArray, allow_copy: bool = True\n    ) -> None:\n        \"\"\"\n        Handles PyArrow Arrays and ChunkedArrays.\n        \"\"\"\n        # Store the column as a private attribute\n        if isinstance(column, pa.ChunkedArray):\n            if column.num_chunks == 1:\n                column = column.chunk(0)\n            else:\n                if not allow_copy:\n                    raise RuntimeError(\n                        \"Chunks will be combined and a copy is required which \"\n                        \"is forbidden by allow_copy=False\"\n                    )\n                column = column.combine_chunks()\n\n        self._allow_copy = allow_copy\n\n        if pa.types.is_boolean(column.type):\n            if not allow_copy:\n                raise RuntimeError(\n                    \"Boolean column will be casted to uint8 and a copy \"\n                    \"is required which is forbidden by allow_copy=False\"\n                )\n            self._dtype = self._dtype_from_arrowdtype(column.type, 8)\n            self._col = pc.cast(column, pa.uint8())\n        else:\n            self._col = column\n            dtype = self._col.type\n            try:\n                bit_width = dtype.bit_width\n            except ValueError:\n                # in case of a variable-length strings, considered as array\n                # of bytes (8 bits)\n                bit_width = 8\n            self._dtype = self._dtype_from_arrowdtype(dtype, bit_width)\n\n    def size(self) -> int:\n        \"\"\"\n        Size of the column, in elements.\n\n        Corresponds to DataFrame.num_rows() if column is a single chunk;\n        equal to size of this current chunk otherwise.\n\n        Is a method rather than a property because it may cause a (potentially\n        expensive) computation for some dataframe implementations.\n        \"\"\"\n        return len(self._col)\n\n    @property\n    def offset(self) -> int:\n        \"\"\"\n        Offset of first element.\n\n        May be > 0 if using chunks; for example for a column with N chunks of\n        equal size M (only the last chunk may be shorter),\n        ``offset = n * M``, ``n = 0 .. N-1``.\n        \"\"\"\n        return self._col.offset\n\n    @property\n    def dtype(self) -> Tuple[DtypeKind, int, str, str]:\n        \"\"\"\n        Dtype description as a tuple ``(kind, bit-width, format string,\n        endianness)``.\n\n        Bit-width : the number of bits as an integer\n        Format string : data type description format string in Apache Arrow C\n                        Data Interface format.\n        Endianness : current only native endianness (``=``) is supported\n\n        Notes:\n            - Kind specifiers are aligned with DLPack where possible (hence the\n              jump to 20, leave enough room for future extension)\n            - Masks must be specified as boolean with either bit width 1 (for\n              bit masks) or 8 (for byte masks).\n            - Dtype width in bits was preferred over bytes\n            - Endianness isn't too useful, but included now in case in the\n              future we need to support non-native endianness\n            - Went with Apache Arrow format strings over NumPy format strings\n              because they're more complete from a dataframe perspective\n            - Format strings are mostly useful for datetime specification, and\n              for categoricals.\n            - For categoricals, the format string describes the type of the\n              categorical in the data buffer. In case of a separate encoding of\n              the categorical (e.g. an integer to string mapping), this can\n              be derived from ``self.describe_categorical``.\n            - Data types not included: complex, Arrow-style null, binary,\n              decimal, and nested (list, struct, map, union) dtypes.\n        \"\"\"\n        return self._dtype\n\n    def _dtype_from_arrowdtype(\n        self, dtype: pa.DataType, bit_width: int\n    ) -> Tuple[DtypeKind, int, str, str]:\n        \"\"\"\n        See `self.dtype` for details.\n        \"\"\"\n        # Note: 'c' (complex) not handled yet (not in array spec v1).\n        #       'b', 'B' (bytes), 'S', 'a', (old-style string) 'V' (void)\n        #       not handled datetime and timedelta both map to datetime\n        #       (is timedelta handled?)\n\n        if pa.types.is_timestamp(dtype):\n            kind = DtypeKind.DATETIME\n            ts = dtype.unit[0]\n            tz = dtype.tz if dtype.tz else \"\"\n            f_string = \"ts{ts}:{tz}\".format(ts=ts, tz=tz)\n            return kind, bit_width, f_string, Endianness.NATIVE\n        elif pa.types.is_dictionary(dtype):\n            kind = DtypeKind.CATEGORICAL\n            arr = self._col\n            indices_dtype = arr.indices.type\n            _, f_string = _PYARROW_KINDS.get(indices_dtype)\n            return kind, bit_width, f_string, Endianness.NATIVE\n        else:\n            kind, f_string = _PYARROW_KINDS.get(dtype, (None, None))\n            if kind is None:\n                raise ValueError(\n                    f\"Data type {dtype} not supported by interchange protocol\")\n\n            return kind, bit_width, f_string, Endianness.NATIVE\n\n    @property\n    def describe_categorical(self) -> CategoricalDescription:\n        \"\"\"\n        If the dtype is categorical, there are two options:\n        - There are only values in the data buffer.\n        - There is a separate non-categorical Column encoding categorical\n          values.\n\n        Raises TypeError if the dtype is not categorical\n\n        Returns the dictionary with description on how to interpret the\n        data buffer:\n            - \"is_ordered\" : bool, whether the ordering of dictionary indices\n                             is semantically meaningful.\n            - \"is_dictionary\" : bool, whether a mapping of\n                                categorical values to other objects exists\n            - \"categories\" : Column representing the (implicit) mapping of\n                             indices to category values (e.g. an array of\n                             cat1, cat2, ...). None if not a dictionary-style\n                             categorical.\n\n        TBD: are there any other in-memory representations that are needed?\n        \"\"\"\n        arr = self._col\n        if not pa.types.is_dictionary(arr.type):\n            raise TypeError(\n                \"describe_categorical only works on a column with \"\n                \"categorical dtype!\"\n            )\n\n        return {\n            \"is_ordered\": self._col.type.ordered,\n            \"is_dictionary\": True,\n            \"categories\": _PyArrowColumn(arr.dictionary),\n        }\n\n    @property\n    def describe_null(self) -> Tuple[ColumnNullType, Any]:\n        \"\"\"\n        Return the missing value (or \"null\") representation the column dtype\n        uses, as a tuple ``(kind, value)``.\n\n        Value : if kind is \"sentinel value\", the actual value. If kind is a bit\n        mask or a byte mask, the value (0 or 1) indicating a missing value.\n        None otherwise.\n        \"\"\"\n        # In case of no missing values, we need to set ColumnNullType to\n        # non nullable as in the current __dataframe__ protocol bit/byte masks\n        # cannot be None\n        if self.null_count == 0:\n            return ColumnNullType.NON_NULLABLE, None\n        else:\n            return ColumnNullType.USE_BITMASK, 0\n\n    @property\n    def null_count(self) -> int:\n        \"\"\"\n        Number of null elements, if known.\n\n        Note: Arrow uses -1 to indicate \"unknown\", but None seems cleaner.\n        \"\"\"\n        arrow_null_count = self._col.null_count\n        n = arrow_null_count if arrow_null_count != -1 else None\n        return n\n\n    @property\n    def metadata(self) -> Dict[str, Any]:\n        \"\"\"\n        The metadata for the column. See `DataFrame.metadata` for more details.\n        \"\"\"\n        pass\n\n    def num_chunks(self) -> int:\n        \"\"\"\n        Return the number of chunks the column consists of.\n        \"\"\"\n        return 1\n\n    def get_chunks(\n        self, n_chunks: Optional[int] = None\n    ) -> Iterable[_PyArrowColumn]:\n        \"\"\"\n        Return an iterator yielding the chunks.\n\n        See `DataFrame.get_chunks` for details on ``n_chunks``.\n        \"\"\"\n        if n_chunks and n_chunks > 1:\n            chunk_size = self.size() // n_chunks\n            if self.size() % n_chunks != 0:\n                chunk_size += 1\n\n            array = self._col\n            i = 0\n            for start in range(0, chunk_size * n_chunks, chunk_size):\n                yield _PyArrowColumn(\n                    array.slice(start, chunk_size), self._allow_copy\n                )\n                i += 1\n        else:\n            yield self\n\n    def get_buffers(self) -> ColumnBuffers:\n        \"\"\"\n        Return a dictionary containing the underlying buffers.\n\n        The returned dictionary has the following contents:\n\n            - \"data\": a two-element tuple whose first element is a buffer\n                      containing the data and whose second element is the data\n                      buffer's associated dtype.\n            - \"validity\": a two-element tuple whose first element is a buffer\n                          containing mask values indicating missing data and\n                          whose second element is the mask value buffer's\n                          associated dtype. None if the null representation is\n                          not a bit or byte mask.\n            - \"offsets\": a two-element tuple whose first element is a buffer\n                         containing the offset values for variable-size binary\n                         data (e.g., variable-length strings) and whose second\n                         element is the offsets buffer's associated dtype. None\n                         if the data buffer does not have an associated offsets\n                         buffer.\n        \"\"\"\n        buffers: ColumnBuffers = {\n            \"data\": self._get_data_buffer(),\n            \"validity\": None,\n            \"offsets\": None,\n        }\n\n        try:\n            buffers[\"validity\"] = self._get_validity_buffer()\n        except NoBufferPresent:\n            pass\n\n        try:\n            buffers[\"offsets\"] = self._get_offsets_buffer()\n        except NoBufferPresent:\n            pass\n\n        return buffers\n\n    def _get_data_buffer(\n        self,\n    ) -> Tuple[_PyArrowBuffer, Any]:  # Any is for self.dtype tuple\n        \"\"\"\n        Return the buffer containing the data and the buffer's\n        associated dtype.\n        \"\"\"\n        array = self._col\n        dtype = self.dtype\n\n        # In case of dictionary arrays, use indices\n        # to define a buffer, codes are transferred through\n        # describe_categorical()\n        if pa.types.is_dictionary(array.type):\n            array = array.indices\n            dtype = _PyArrowColumn(array).dtype\n\n        n = len(array.buffers())\n        if n == 2:\n            return _PyArrowBuffer(array.buffers()[1]), dtype\n        elif n == 3:\n            return _PyArrowBuffer(array.buffers()[2]), dtype\n\n    def _get_validity_buffer(self) -> Tuple[_PyArrowBuffer, Any]:\n        \"\"\"\n        Return the buffer containing the mask values indicating missing data\n        and the buffer's associated dtype.\n        Raises NoBufferPresent if null representation is not a bit or byte\n        mask.\n        \"\"\"\n        # Define the dtype of the returned buffer\n        dtype = (DtypeKind.BOOL, 1, \"b\", Endianness.NATIVE)\n        array = self._col\n        buff = array.buffers()[0]\n        if buff:\n            return _PyArrowBuffer(buff), dtype\n        else:\n            raise NoBufferPresent(\n                \"There are no missing values so \"\n                \"does not have a separate mask\")\n\n    def _get_offsets_buffer(self) -> Tuple[_PyArrowBuffer, Any]:\n        \"\"\"\n        Return the buffer containing the offset values for variable-size binary\n        data (e.g., variable-length strings) and the buffer's associated dtype.\n        Raises NoBufferPresent if the data buffer does not have an associated\n        offsets buffer.\n        \"\"\"\n        array = self._col\n        n = len(array.buffers())\n        if n == 2:\n            raise NoBufferPresent(\n                \"This column has a fixed-length dtype so \"\n                \"it does not have an offsets buffer\"\n            )\n        elif n == 3:\n            # Define the dtype of the returned buffer\n            dtype = self._col.type\n            if pa.types.is_large_string(dtype):\n                dtype = (DtypeKind.INT, 64, \"l\", Endianness.NATIVE)\n            else:\n                dtype = (DtypeKind.INT, 32, \"i\", Endianness.NATIVE)\n            return _PyArrowBuffer(array.buffers()[1]), dtype\n", "python/pyarrow/interchange/buffer.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import annotations\nimport enum\n\nimport pyarrow as pa\n\n\nclass DlpackDeviceType(enum.IntEnum):\n    \"\"\"Integer enum for device type codes matching DLPack.\"\"\"\n\n    CPU = 1\n    CUDA = 2\n    CPU_PINNED = 3\n    OPENCL = 4\n    VULKAN = 7\n    METAL = 8\n    VPI = 9\n    ROCM = 10\n\n\nclass _PyArrowBuffer:\n    \"\"\"\n    Data in the buffer is guaranteed to be contiguous in memory.\n\n    Note that there is no dtype attribute present, a buffer can be thought of\n    as simply a block of memory. However, if the column that the buffer is\n    attached to has a dtype that's supported by DLPack and ``__dlpack__`` is\n    implemented, then that dtype information will be contained in the return\n    value from ``__dlpack__``.\n\n    This distinction is useful to support both data exchange via DLPack on a\n    buffer and (b) dtypes like variable-length strings which do not have a\n    fixed number of bytes per element.\n    \"\"\"\n\n    def __init__(self, x: pa.Buffer, allow_copy: bool = True) -> None:\n        \"\"\"\n        Handle PyArrow Buffers.\n        \"\"\"\n        self._x = x\n\n    @property\n    def bufsize(self) -> int:\n        \"\"\"\n        Buffer size in bytes.\n        \"\"\"\n        return self._x.size\n\n    @property\n    def ptr(self) -> int:\n        \"\"\"\n        Pointer to start of the buffer as an integer.\n        \"\"\"\n        return self._x.address\n\n    def __dlpack__(self):\n        \"\"\"\n        Produce DLPack capsule (see array API standard).\n\n        Raises:\n            - TypeError : if the buffer contains unsupported dtypes.\n            - NotImplementedError : if DLPack support is not implemented\n\n        Useful to have to connect to array libraries. Support optional because\n        it's not completely trivial to implement for a Python-only library.\n        \"\"\"\n        raise NotImplementedError(\"__dlpack__\")\n\n    def __dlpack_device__(self) -> tuple[DlpackDeviceType, int | None]:\n        \"\"\"\n        Device type and device ID for where the data in the buffer resides.\n        Uses device type codes matching DLPack.\n        Note: must be implemented even if ``__dlpack__`` is not.\n        \"\"\"\n        if self._x.is_cpu:\n            return (DlpackDeviceType.CPU, None)\n        else:\n            raise NotImplementedError(\"__dlpack_device__\")\n\n    def __repr__(self) -> str:\n        return (\n            \"PyArrowBuffer(\" +\n            str(\n                {\n                    \"bufsize\": self.bufsize,\n                    \"ptr\": self.ptr,\n                    \"device\": self.__dlpack_device__()[0].name,\n                }\n            ) +\n            \")\"\n        )\n", "python/pyarrow/interchange/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# flake8: noqa\n\nfrom .from_dataframe import from_dataframe\n", "python/pyarrow/parquet/core.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\nfrom collections import defaultdict\nfrom contextlib import nullcontext\nfrom functools import reduce\n\nimport inspect\nimport json\nimport os\nimport re\nimport operator\nimport warnings\n\nimport pyarrow as pa\n\ntry:\n    import pyarrow._parquet as _parquet\nexcept ImportError as exc:\n    raise ImportError(\n        \"The pyarrow installation is not built with support \"\n        f\"for the Parquet file format ({str(exc)})\"\n    ) from None\n\nfrom pyarrow._parquet import (ParquetReader, Statistics,  # noqa\n                              FileMetaData, RowGroupMetaData,\n                              ColumnChunkMetaData,\n                              ParquetSchema, ColumnSchema,\n                              ParquetLogicalType,\n                              FileEncryptionProperties,\n                              FileDecryptionProperties,\n                              SortingColumn)\nfrom pyarrow.fs import (LocalFileSystem, FileSystem, FileType,\n                        _resolve_filesystem_and_path, _ensure_filesystem)\nfrom pyarrow.util import guid, _is_path_like, _stringify_path, _deprecate_api\n\n\ndef _check_contains_null(val):\n    if isinstance(val, bytes):\n        for byte in val:\n            if isinstance(byte, bytes):\n                compare_to = chr(0)\n            else:\n                compare_to = 0\n            if byte == compare_to:\n                return True\n    elif isinstance(val, str):\n        return '\\x00' in val\n    return False\n\n\ndef _check_filters(filters, check_null_strings=True):\n    \"\"\"\n    Check if filters are well-formed.\n    \"\"\"\n    if filters is not None:\n        if len(filters) == 0 or any(len(f) == 0 for f in filters):\n            raise ValueError(\"Malformed filters\")\n        if isinstance(filters[0][0], str):\n            # We have encountered the situation where we have one nesting level\n            # too few:\n            #   We have [(,,), ..] instead of [[(,,), ..]]\n            filters = [filters]\n        if check_null_strings:\n            for conjunction in filters:\n                for col, op, val in conjunction:\n                    if (\n                        isinstance(val, list) and\n                        all(_check_contains_null(v) for v in val) or\n                        _check_contains_null(val)\n                    ):\n                        raise NotImplementedError(\n                            \"Null-terminated binary strings are not supported \"\n                            \"as filter values.\"\n                        )\n    return filters\n\n\n_DNF_filter_doc = \"\"\"Predicates are expressed using an ``Expression`` or using\n    the disjunctive normal form (DNF), like ``[[('x', '=', 0), ...], ...]``.\n    DNF allows arbitrary boolean logical combinations of single column predicates.\n    The innermost tuples each describe a single column predicate. The list of inner\n    predicates is interpreted as a conjunction (AND), forming a more selective and\n    multiple column predicate. Finally, the most outer list combines these filters\n    as a disjunction (OR).\n\n    Predicates may also be passed as List[Tuple]. This form is interpreted\n    as a single conjunction. To express OR in predicates, one must\n    use the (preferred) List[List[Tuple]] notation.\n\n    Each tuple has format: (``key``, ``op``, ``value``) and compares the\n    ``key`` with the ``value``.\n    The supported ``op`` are:  ``=`` or ``==``, ``!=``, ``<``, ``>``, ``<=``,\n    ``>=``, ``in`` and ``not in``. If the ``op`` is ``in`` or ``not in``, the\n    ``value`` must be a collection such as a ``list``, a ``set`` or a\n    ``tuple``.\n\n    Examples:\n\n    Using the ``Expression`` API:\n\n    .. code-block:: python\n\n        import pyarrow.compute as pc\n        pc.field('x') = 0\n        pc.field('y').isin(['a', 'b', 'c'])\n        ~pc.field('y').isin({'a', 'b'})\n\n    Using the DNF format:\n\n    .. code-block:: python\n\n        ('x', '=', 0)\n        ('y', 'in', ['a', 'b', 'c'])\n        ('z', 'not in', {'a','b'})\n\n    \"\"\"\n\n\ndef filters_to_expression(filters):\n    \"\"\"\n    Check if filters are well-formed and convert to an ``Expression``.\n\n    Parameters\n    ----------\n    filters : List[Tuple] or List[List[Tuple]]\n\n    Notes\n    -----\n    See internal ``pyarrow._DNF_filter_doc`` attribute for more details.\n\n    Examples\n    --------\n\n    >>> filters_to_expression([('foo', '==', 'bar')])\n    <pyarrow.compute.Expression (foo == \"bar\")>\n\n    Returns\n    -------\n    pyarrow.compute.Expression\n        An Expression representing the filters\n    \"\"\"\n    import pyarrow.dataset as ds\n\n    if isinstance(filters, ds.Expression):\n        return filters\n\n    filters = _check_filters(filters, check_null_strings=False)\n\n    def convert_single_predicate(col, op, val):\n        field = ds.field(col)\n\n        if op == \"=\" or op == \"==\":\n            return field == val\n        elif op == \"!=\":\n            return field != val\n        elif op == '<':\n            return field < val\n        elif op == '>':\n            return field > val\n        elif op == '<=':\n            return field <= val\n        elif op == '>=':\n            return field >= val\n        elif op == 'in':\n            return field.isin(val)\n        elif op == 'not in':\n            return ~field.isin(val)\n        else:\n            raise ValueError(\n                '\"{0}\" is not a valid operator in predicates.'.format(\n                    (col, op, val)))\n\n    disjunction_members = []\n\n    for conjunction in filters:\n        conjunction_members = [\n            convert_single_predicate(col, op, val)\n            for col, op, val in conjunction\n        ]\n\n        disjunction_members.append(reduce(operator.and_, conjunction_members))\n\n    return reduce(operator.or_, disjunction_members)\n\n\n_filters_to_expression = _deprecate_api(\n    \"_filters_to_expression\", \"filters_to_expression\",\n    filters_to_expression, \"10.0.0\", DeprecationWarning)\n\n\n# ----------------------------------------------------------------------\n# Reading a single Parquet file\n\n\nclass ParquetFile:\n    \"\"\"\n    Reader interface for a single Parquet file.\n\n    Parameters\n    ----------\n    source : str, pathlib.Path, pyarrow.NativeFile, or file-like object\n        Readable source. For passing bytes or buffer-like file containing a\n        Parquet file, use pyarrow.BufferReader.\n    metadata : FileMetaData, default None\n        Use existing metadata object, rather than reading from file.\n    common_metadata : FileMetaData, default None\n        Will be used in reads for pandas schema metadata if not found in the\n        main file's metadata, no other uses at the moment.\n    read_dictionary : list\n        List of column names to read directly as DictionaryArray.\n    memory_map : bool, default False\n        If the source is a file path, use a memory map to read file, which can\n        improve performance in some environments.\n    buffer_size : int, default 0\n        If positive, perform read buffering when deserializing individual\n        column chunks. Otherwise IO calls are unbuffered.\n    pre_buffer : bool, default False\n        Coalesce and issue file reads in parallel to improve performance on\n        high-latency filesystems (e.g. S3). If True, Arrow will use a\n        background I/O thread pool.\n    coerce_int96_timestamp_unit : str, default None\n        Cast timestamps that are stored in INT96 format to a particular\n        resolution (e.g. 'ms'). Setting to None is equivalent to 'ns'\n        and therefore INT96 timestamps will be inferred as timestamps\n        in nanoseconds.\n    decryption_properties : FileDecryptionProperties, default None\n        File decryption properties for Parquet Modular Encryption.\n    thrift_string_size_limit : int, default None\n        If not None, override the maximum total string size allocated\n        when decoding Thrift structures. The default limit should be\n        sufficient for most Parquet files.\n    thrift_container_size_limit : int, default None\n        If not None, override the maximum total size of containers allocated\n        when decoding Thrift structures. The default limit should be\n        sufficient for most Parquet files.\n    filesystem : FileSystem, default None\n        If nothing passed, will be inferred based on path.\n        Path will try to be found in the local on-disk filesystem otherwise\n        it will be parsed as an URI to determine the filesystem.\n    page_checksum_verification : bool, default False\n        If True, verify the checksum for each page read from the file.\n\n    Examples\n    --------\n\n    Generate an example PyArrow Table and write it to Parquet file:\n\n    >>> import pyarrow as pa\n    >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n    ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n    ...                              \"Brittle stars\", \"Centipede\"]})\n\n    >>> import pyarrow.parquet as pq\n    >>> pq.write_table(table, 'example.parquet')\n\n    Create a ``ParquetFile`` object from the Parquet file:\n\n    >>> parquet_file = pq.ParquetFile('example.parquet')\n\n    Read the data:\n\n    >>> parquet_file.read()\n    pyarrow.Table\n    n_legs: int64\n    animal: string\n    ----\n    n_legs: [[2,2,4,4,5,100]]\n    animal: [[\"Flamingo\",\"Parrot\",\"Dog\",\"Horse\",\"Brittle stars\",\"Centipede\"]]\n\n    Create a ParquetFile object with \"animal\" column as DictionaryArray:\n\n    >>> parquet_file = pq.ParquetFile('example.parquet',\n    ...                               read_dictionary=[\"animal\"])\n    >>> parquet_file.read()\n    pyarrow.Table\n    n_legs: int64\n    animal: dictionary<values=string, indices=int32, ordered=0>\n    ----\n    n_legs: [[2,2,4,4,5,100]]\n    animal: [  -- dictionary:\n    [\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]  -- indices:\n    [0,1,2,3,4,5]]\n    \"\"\"\n\n    def __init__(self, source, *, metadata=None, common_metadata=None,\n                 read_dictionary=None, memory_map=False, buffer_size=0,\n                 pre_buffer=False, coerce_int96_timestamp_unit=None,\n                 decryption_properties=None, thrift_string_size_limit=None,\n                 thrift_container_size_limit=None, filesystem=None,\n                 page_checksum_verification=False):\n\n        self._close_source = getattr(source, 'closed', True)\n\n        filesystem, source = _resolve_filesystem_and_path(\n            source, filesystem, memory_map=memory_map)\n        if filesystem is not None:\n            source = filesystem.open_input_file(source)\n            self._close_source = True  # We opened it here, ensure we close it.\n\n        self.reader = ParquetReader()\n        self.reader.open(\n            source, use_memory_map=memory_map,\n            buffer_size=buffer_size, pre_buffer=pre_buffer,\n            read_dictionary=read_dictionary, metadata=metadata,\n            coerce_int96_timestamp_unit=coerce_int96_timestamp_unit,\n            decryption_properties=decryption_properties,\n            thrift_string_size_limit=thrift_string_size_limit,\n            thrift_container_size_limit=thrift_container_size_limit,\n            page_checksum_verification=page_checksum_verification,\n        )\n        self.common_metadata = common_metadata\n        self._nested_paths_by_prefix = self._build_nested_paths()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n    def _build_nested_paths(self):\n        paths = self.reader.column_paths\n\n        result = defaultdict(list)\n\n        for i, path in enumerate(paths):\n            key = path[0]\n            rest = path[1:]\n            while True:\n                result[key].append(i)\n\n                if not rest:\n                    break\n\n                key = '.'.join((key, rest[0]))\n                rest = rest[1:]\n\n        return result\n\n    @property\n    def metadata(self):\n        \"\"\"\n        Return the Parquet metadata.\n        \"\"\"\n        return self.reader.metadata\n\n    @property\n    def schema(self):\n        \"\"\"\n        Return the Parquet schema, unconverted to Arrow types\n        \"\"\"\n        return self.metadata.schema\n\n    @property\n    def schema_arrow(self):\n        \"\"\"\n        Return the inferred Arrow schema, converted from the whole Parquet\n        file's schema\n\n        Examples\n        --------\n        Generate an example Parquet file:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        Read the Arrow schema:\n\n        >>> parquet_file.schema_arrow\n        n_legs: int64\n        animal: string\n        \"\"\"\n        return self.reader.schema_arrow\n\n    @property\n    def num_row_groups(self):\n        \"\"\"\n        Return the number of row groups of the Parquet file.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        >>> parquet_file.num_row_groups\n        1\n        \"\"\"\n        return self.reader.num_row_groups\n\n    def close(self, force: bool = False):\n        if self._close_source or force:\n            self.reader.close()\n\n    @property\n    def closed(self) -> bool:\n        return self.reader.closed\n\n    def read_row_group(self, i, columns=None, use_threads=True,\n                       use_pandas_metadata=False):\n        \"\"\"\n        Read a single row group from a Parquet file.\n\n        Parameters\n        ----------\n        i : int\n            Index of the individual row group that we want to read.\n        columns : list\n            If not None, only these columns will be read from the row group. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'.\n        use_threads : bool, default True\n            Perform multi-threaded column reads.\n        use_pandas_metadata : bool, default False\n            If True and file has custom pandas schema metadata, ensure that\n            index columns are also loaded.\n\n        Returns\n        -------\n        pyarrow.table.Table\n            Content of the row group as a table (of columns)\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        >>> parquet_file.read_row_group(0)\n        pyarrow.Table\n        n_legs: int64\n        animal: string\n        ----\n        n_legs: [[2,2,4,4,5,100]]\n        animal: [[\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]]\n        \"\"\"\n        column_indices = self._get_column_indices(\n            columns, use_pandas_metadata=use_pandas_metadata)\n        return self.reader.read_row_group(i, column_indices=column_indices,\n                                          use_threads=use_threads)\n\n    def read_row_groups(self, row_groups, columns=None, use_threads=True,\n                        use_pandas_metadata=False):\n        \"\"\"\n        Read a multiple row groups from a Parquet file.\n\n        Parameters\n        ----------\n        row_groups : list\n            Only these row groups will be read from the file.\n        columns : list\n            If not None, only these columns will be read from the row group. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'.\n        use_threads : bool, default True\n            Perform multi-threaded column reads.\n        use_pandas_metadata : bool, default False\n            If True and file has custom pandas schema metadata, ensure that\n            index columns are also loaded.\n\n        Returns\n        -------\n        pyarrow.table.Table\n            Content of the row groups as a table (of columns).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        >>> parquet_file.read_row_groups([0,0])\n        pyarrow.Table\n        n_legs: int64\n        animal: string\n        ----\n        n_legs: [[2,2,4,4,5,...,2,4,4,5,100]]\n        animal: [[\"Flamingo\",\"Parrot\",\"Dog\",...,\"Brittle stars\",\"Centipede\"]]\n        \"\"\"\n        column_indices = self._get_column_indices(\n            columns, use_pandas_metadata=use_pandas_metadata)\n        return self.reader.read_row_groups(row_groups,\n                                           column_indices=column_indices,\n                                           use_threads=use_threads)\n\n    def iter_batches(self, batch_size=65536, row_groups=None, columns=None,\n                     use_threads=True, use_pandas_metadata=False):\n        \"\"\"\n        Read streaming batches from a Parquet file.\n\n        Parameters\n        ----------\n        batch_size : int, default 64K\n            Maximum number of records to yield per batch. Batches may be\n            smaller if there aren't enough rows in the file.\n        row_groups : list\n            Only these row groups will be read from the file.\n        columns : list\n            If not None, only these columns will be read from the file. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'.\n        use_threads : boolean, default True\n            Perform multi-threaded column reads.\n        use_pandas_metadata : boolean, default False\n            If True and file has custom pandas schema metadata, ensure that\n            index columns are also loaded.\n\n        Yields\n        ------\n        pyarrow.RecordBatch\n            Contents of each batch as a record batch\n\n        Examples\n        --------\n        Generate an example Parquet file:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n        >>> for i in parquet_file.iter_batches():\n        ...     print(\"RecordBatch\")\n        ...     print(i.to_pandas())\n        ...\n        RecordBatch\n           n_legs         animal\n        0       2       Flamingo\n        1       2         Parrot\n        2       4            Dog\n        3       4          Horse\n        4       5  Brittle stars\n        5     100      Centipede\n        \"\"\"\n        if row_groups is None:\n            row_groups = range(0, self.metadata.num_row_groups)\n        column_indices = self._get_column_indices(\n            columns, use_pandas_metadata=use_pandas_metadata)\n\n        batches = self.reader.iter_batches(batch_size,\n                                           row_groups=row_groups,\n                                           column_indices=column_indices,\n                                           use_threads=use_threads)\n        return batches\n\n    def read(self, columns=None, use_threads=True, use_pandas_metadata=False):\n        \"\"\"\n        Read a Table from Parquet format.\n\n        Parameters\n        ----------\n        columns : list\n            If not None, only these columns will be read from the file. A\n            column name may be a prefix of a nested field, e.g. 'a' will select\n            'a.b', 'a.c', and 'a.d.e'.\n        use_threads : bool, default True\n            Perform multi-threaded column reads.\n        use_pandas_metadata : bool, default False\n            If True and file has custom pandas schema metadata, ensure that\n            index columns are also loaded.\n\n        Returns\n        -------\n        pyarrow.table.Table\n            Content of the file as a table (of columns).\n\n        Examples\n        --------\n        Generate an example Parquet file:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        Read a Table:\n\n        >>> parquet_file.read(columns=[\"animal\"])\n        pyarrow.Table\n        animal: string\n        ----\n        animal: [[\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]]\n        \"\"\"\n        column_indices = self._get_column_indices(\n            columns, use_pandas_metadata=use_pandas_metadata)\n        return self.reader.read_all(column_indices=column_indices,\n                                    use_threads=use_threads)\n\n    def scan_contents(self, columns=None, batch_size=65536):\n        \"\"\"\n        Read contents of file for the given columns and batch size.\n\n        Notes\n        -----\n        This function's primary purpose is benchmarking.\n        The scan is executed on a single thread.\n\n        Parameters\n        ----------\n        columns : list of integers, default None\n            Select columns to read, if None scan all columns.\n        batch_size : int, default 64K\n            Number of rows to read at a time internally.\n\n        Returns\n        -------\n        num_rows : int\n            Number of rows in file\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'example.parquet')\n        >>> parquet_file = pq.ParquetFile('example.parquet')\n\n        >>> parquet_file.scan_contents()\n        6\n        \"\"\"\n        column_indices = self._get_column_indices(columns)\n        return self.reader.scan_contents(column_indices,\n                                         batch_size=batch_size)\n\n    def _get_column_indices(self, column_names, use_pandas_metadata=False):\n        if column_names is None:\n            return None\n\n        indices = []\n\n        for name in column_names:\n            if name in self._nested_paths_by_prefix:\n                indices.extend(self._nested_paths_by_prefix[name])\n\n        if use_pandas_metadata:\n            file_keyvalues = self.metadata.metadata\n            common_keyvalues = (self.common_metadata.metadata\n                                if self.common_metadata is not None\n                                else None)\n\n            if file_keyvalues and b'pandas' in file_keyvalues:\n                index_columns = _get_pandas_index_columns(file_keyvalues)\n            elif common_keyvalues and b'pandas' in common_keyvalues:\n                index_columns = _get_pandas_index_columns(common_keyvalues)\n            else:\n                index_columns = []\n\n            if indices is not None and index_columns:\n                indices += [self.reader.column_name_idx(descr)\n                            for descr in index_columns\n                            if not isinstance(descr, dict)]\n\n        return indices\n\n\n_SPARK_DISALLOWED_CHARS = re.compile('[ ,;{}()\\n\\t=]')\n\n\ndef _sanitized_spark_field_name(name):\n    return _SPARK_DISALLOWED_CHARS.sub('_', name)\n\n\ndef _sanitize_schema(schema, flavor):\n    if 'spark' in flavor:\n        sanitized_fields = []\n\n        schema_changed = False\n\n        for field in schema:\n            name = field.name\n            sanitized_name = _sanitized_spark_field_name(name)\n\n            if sanitized_name != name:\n                schema_changed = True\n                sanitized_field = pa.field(sanitized_name, field.type,\n                                           field.nullable, field.metadata)\n                sanitized_fields.append(sanitized_field)\n            else:\n                sanitized_fields.append(field)\n\n        new_schema = pa.schema(sanitized_fields, metadata=schema.metadata)\n        return new_schema, schema_changed\n    else:\n        return schema, False\n\n\ndef _sanitize_table(table, new_schema, flavor):\n    # TODO: This will not handle prohibited characters in nested field names\n    if 'spark' in flavor:\n        column_data = [table[i] for i in range(table.num_columns)]\n        return pa.Table.from_arrays(column_data, schema=new_schema)\n    else:\n        return table\n\n\n_parquet_writer_arg_docs = \"\"\"version : {\"1.0\", \"2.4\", \"2.6\"}, default \"2.6\"\n    Determine which Parquet logical types are available for use, whether the\n    reduced set from the Parquet 1.x.x format or the expanded logical types\n    added in later format versions.\n    Files written with version='2.4' or '2.6' may not be readable in all\n    Parquet implementations, so version='1.0' is likely the choice that\n    maximizes file compatibility.\n    UINT32 and some logical types are only available with version '2.4'.\n    Nanosecond timestamps are only available with version '2.6'.\n    Other features such as compression algorithms or the new serialized\n    data page format must be enabled separately (see 'compression' and\n    'data_page_version').\nuse_dictionary : bool or list, default True\n    Specify if we should use dictionary encoding in general or only for\n    some columns.\n    When encoding the column, if the dictionary size is too large, the\n    column will fallback to ``PLAIN`` encoding. Specially, ``BOOLEAN`` type\n    doesn't support dictionary encoding.\ncompression : str or dict, default 'snappy'\n    Specify the compression codec, either on a general basis or per-column.\n    Valid values: {'NONE', 'SNAPPY', 'GZIP', 'BROTLI', 'LZ4', 'ZSTD'}.\nwrite_statistics : bool or list, default True\n    Specify if we should write statistics in general (default is True) or only\n    for some columns.\nuse_deprecated_int96_timestamps : bool, default None\n    Write timestamps to INT96 Parquet format. Defaults to False unless enabled\n    by flavor argument. This take priority over the coerce_timestamps option.\ncoerce_timestamps : str, default None\n    Cast timestamps to a particular resolution. If omitted, defaults are chosen\n    depending on `version`. For ``version='1.0'`` and ``version='2.4'``,\n    nanoseconds are cast to microseconds ('us'), while for\n    ``version='2.6'`` (the default), they are written natively without loss\n    of resolution.  Seconds are always cast to milliseconds ('ms') by default,\n    as Parquet does not have any temporal type with seconds resolution.\n    If the casting results in loss of data, it will raise an exception\n    unless ``allow_truncated_timestamps=True`` is given.\n    Valid values: {None, 'ms', 'us'}\nallow_truncated_timestamps : bool, default False\n    Allow loss of data when coercing timestamps to a particular\n    resolution. E.g. if microsecond or nanosecond data is lost when coercing to\n    'ms', do not raise an exception. Passing ``allow_truncated_timestamp=True``\n    will NOT result in the truncation exception being ignored unless\n    ``coerce_timestamps`` is not None.\ndata_page_size : int, default None\n    Set a target threshold for the approximate encoded size of data\n    pages within a column chunk (in bytes). If None, use the default data page\n    size of 1MByte.\nflavor : {'spark'}, default None\n    Sanitize schema or set other compatibility options to work with\n    various target systems.\nfilesystem : FileSystem, default None\n    If nothing passed, will be inferred from `where` if path-like, else\n    `where` is already a file-like object so no filesystem is needed.\ncompression_level : int or dict, default None\n    Specify the compression level for a codec, either on a general basis or\n    per-column. If None is passed, arrow selects the compression level for\n    the compression codec in use. The compression level has a different\n    meaning for each codec, so you have to read the documentation of the\n    codec you are using.\n    An exception is thrown if the compression codec does not allow specifying\n    a compression level.\nuse_byte_stream_split : bool or list, default False\n    Specify if the byte_stream_split encoding should be used in general or\n    only for some columns. If both dictionary and byte_stream_stream are\n    enabled, then dictionary is preferred.\n    The byte_stream_split encoding is valid for integer, floating-point\n    and fixed-size binary data types (including decimals); it should be\n    combined with a compression codec so as to achieve size reduction.\ncolumn_encoding : string or dict, default None\n    Specify the encoding scheme on a per column basis.\n    Can only be used when ``use_dictionary`` is set to False, and\n    cannot be used in combination with ``use_byte_stream_split``.\n    Currently supported values: {'PLAIN', 'BYTE_STREAM_SPLIT',\n    'DELTA_BINARY_PACKED', 'DELTA_LENGTH_BYTE_ARRAY', 'DELTA_BYTE_ARRAY'}.\n    Certain encodings are only compatible with certain data types.\n    Please refer to the encodings section of `Reading and writing Parquet\n    files <https://arrow.apache.org/docs/cpp/parquet.html#encodings>`_.\ndata_page_version : {\"1.0\", \"2.0\"}, default \"1.0\"\n    The serialized Parquet data page format version to write, defaults to\n    1.0. This does not impact the file schema logical types and Arrow to\n    Parquet type casting behavior; for that use the \"version\" option.\nuse_compliant_nested_type : bool, default True\n    Whether to write compliant Parquet nested type (lists) as defined\n    `here <https://github.com/apache/parquet-format/blob/master/\n    LogicalTypes.md#nested-types>`_, defaults to ``True``.\n    For ``use_compliant_nested_type=True``, this will write into a list\n    with 3-level structure where the middle level, named ``list``,\n    is a repeated group with a single field named ``element``::\n\n        <list-repetition> group <name> (LIST) {\n            repeated group list {\n                  <element-repetition> <element-type> element;\n            }\n        }\n\n    For ``use_compliant_nested_type=False``, this will also write into a list\n    with 3-level structure, where the name of the single field of the middle\n    level ``list`` is taken from the element name for nested columns in Arrow,\n    which defaults to ``item``::\n\n        <list-repetition> group <name> (LIST) {\n            repeated group list {\n                <element-repetition> <element-type> item;\n            }\n        }\nencryption_properties : FileEncryptionProperties, default None\n    File encryption properties for Parquet Modular Encryption.\n    If None, no encryption will be done.\n    The encryption properties can be created using:\n    ``CryptoFactory.file_encryption_properties()``.\nwrite_batch_size : int, default None\n    Number of values to write to a page at a time. If None, use the default of\n    1024. ``write_batch_size`` is complementary to ``data_page_size``. If pages\n    are exceeding the ``data_page_size`` due to large column values, lowering\n    the batch size can help keep page sizes closer to the intended size.\ndictionary_pagesize_limit : int, default None\n    Specify the dictionary page size limit per row group. If None, use the\n    default 1MB.\nstore_schema : bool, default True\n    By default, the Arrow schema is serialized and stored in the Parquet\n    file metadata (in the \"ARROW:schema\" key). When reading the file,\n    if this key is available, it will be used to more faithfully recreate\n    the original Arrow data. For example, for tz-aware timestamp columns\n    it will restore the timezone (Parquet only stores the UTC values without\n    timezone), or columns with duration type will be restored from the int64\n    Parquet column.\nwrite_page_index : bool, default False\n    Whether to write a page index in general for all columns.\n    Writing statistics to the page index disables the old method of writing\n    statistics to each data page header. The page index makes statistics-based\n    filtering more efficient than the page header, as it gathers all the\n    statistics for a Parquet file in a single place, avoiding scattered I/O.\n    Note that the page index is not yet used on the read size by PyArrow.\nwrite_page_checksum : bool, default False\n    Whether to write page checksums in general for all columns.\n    Page checksums enable detection of data corruption, which might occur during\n    transmission or in the storage.\nsorting_columns : Sequence of SortingColumn, default None\n    Specify the sort order of the data being written. The writer does not sort\n    the data nor does it verify that the data is sorted. The sort order is\n    written to the row group metadata, which can then be used by readers.\n\"\"\"\n\n_parquet_writer_example_doc = \"\"\"\\\nGenerate an example PyArrow Table and RecordBatch:\n\n>>> import pyarrow as pa\n>>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n...                              \"Brittle stars\", \"Centipede\"]})\n>>> batch = pa.record_batch([[2, 2, 4, 4, 5, 100],\n...                         [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n...                          \"Brittle stars\", \"Centipede\"]],\n...                         names=['n_legs', 'animal'])\n\ncreate a ParquetWriter object:\n\n>>> import pyarrow.parquet as pq\n>>> writer = pq.ParquetWriter('example.parquet', table.schema)\n\nand write the Table into the Parquet file:\n\n>>> writer.write_table(table)\n>>> writer.close()\n\n>>> pq.read_table('example.parquet').to_pandas()\n   n_legs         animal\n0       2       Flamingo\n1       2         Parrot\n2       4            Dog\n3       4          Horse\n4       5  Brittle stars\n5     100      Centipede\n\ncreate a ParquetWriter object for the RecordBatch:\n\n>>> writer2 = pq.ParquetWriter('example2.parquet', batch.schema)\n\nand write the RecordBatch into the Parquet file:\n\n>>> writer2.write_batch(batch)\n>>> writer2.close()\n\n>>> pq.read_table('example2.parquet').to_pandas()\n   n_legs         animal\n0       2       Flamingo\n1       2         Parrot\n2       4            Dog\n3       4          Horse\n4       5  Brittle stars\n5     100      Centipede\n\"\"\"\n\n\nclass ParquetWriter:\n\n    __doc__ = \"\"\"\nClass for incrementally building a Parquet file for Arrow tables.\n\nParameters\n----------\nwhere : path or file-like object\nschema : pyarrow.Schema\n{}\nwriter_engine_version : unused\n**options : dict\n    If options contains a key `metadata_collector` then the\n    corresponding value is assumed to be a list (or any object with\n    `.append` method) that will be filled with the file metadata instance\n    of the written file.\n\nExamples\n--------\n{}\n\"\"\".format(_parquet_writer_arg_docs, _parquet_writer_example_doc)\n\n    def __init__(self, where, schema, filesystem=None,\n                 flavor=None,\n                 version='2.6',\n                 use_dictionary=True,\n                 compression='snappy',\n                 write_statistics=True,\n                 use_deprecated_int96_timestamps=None,\n                 compression_level=None,\n                 use_byte_stream_split=False,\n                 column_encoding=None,\n                 writer_engine_version=None,\n                 data_page_version='1.0',\n                 use_compliant_nested_type=True,\n                 encryption_properties=None,\n                 write_batch_size=None,\n                 dictionary_pagesize_limit=None,\n                 store_schema=True,\n                 write_page_index=False,\n                 write_page_checksum=False,\n                 sorting_columns=None,\n                 **options):\n        if use_deprecated_int96_timestamps is None:\n            # Use int96 timestamps for Spark\n            if flavor is not None and 'spark' in flavor:\n                use_deprecated_int96_timestamps = True\n            else:\n                use_deprecated_int96_timestamps = False\n\n        self.flavor = flavor\n        if flavor is not None:\n            schema, self.schema_changed = _sanitize_schema(schema, flavor)\n        else:\n            self.schema_changed = False\n\n        self.schema = schema\n        self.where = where\n\n        # If we open a file using a filesystem, store file handle so we can be\n        # sure to close it when `self.close` is called.\n        self.file_handle = None\n\n        filesystem, path = _resolve_filesystem_and_path(where, filesystem)\n        if filesystem is not None:\n            # ARROW-10480: do not auto-detect compression.  While\n            # a filename like foo.parquet.gz is nonconforming, it\n            # shouldn't implicitly apply compression.\n            sink = self.file_handle = filesystem.open_output_stream(\n                path, compression=None)\n        else:\n            sink = where\n        self._metadata_collector = options.pop('metadata_collector', None)\n        engine_version = 'V2'\n        self.writer = _parquet.ParquetWriter(\n            sink, schema,\n            version=version,\n            compression=compression,\n            use_dictionary=use_dictionary,\n            write_statistics=write_statistics,\n            use_deprecated_int96_timestamps=use_deprecated_int96_timestamps,\n            compression_level=compression_level,\n            use_byte_stream_split=use_byte_stream_split,\n            column_encoding=column_encoding,\n            writer_engine_version=engine_version,\n            data_page_version=data_page_version,\n            use_compliant_nested_type=use_compliant_nested_type,\n            encryption_properties=encryption_properties,\n            write_batch_size=write_batch_size,\n            dictionary_pagesize_limit=dictionary_pagesize_limit,\n            store_schema=store_schema,\n            write_page_index=write_page_index,\n            write_page_checksum=write_page_checksum,\n            sorting_columns=sorting_columns,\n            **options)\n        self.is_open = True\n\n    def __del__(self):\n        if getattr(self, 'is_open', False):\n            self.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n        # return false since we want to propagate exceptions\n        return False\n\n    def write(self, table_or_batch, row_group_size=None):\n        \"\"\"\n        Write RecordBatch or Table to the Parquet file.\n\n        Parameters\n        ----------\n        table_or_batch : {RecordBatch, Table}\n        row_group_size : int, default None\n            Maximum number of rows in each written row group. If None,\n            the row group size will be the minimum of the input\n            table or batch length and 1024 * 1024.\n        \"\"\"\n        if isinstance(table_or_batch, pa.RecordBatch):\n            self.write_batch(table_or_batch, row_group_size)\n        elif isinstance(table_or_batch, pa.Table):\n            self.write_table(table_or_batch, row_group_size)\n        else:\n            raise TypeError(type(table_or_batch))\n\n    def write_batch(self, batch, row_group_size=None):\n        \"\"\"\n        Write RecordBatch to the Parquet file.\n\n        Parameters\n        ----------\n        batch : RecordBatch\n        row_group_size : int, default None\n            Maximum number of rows in written row group. If None, the\n            row group size will be the minimum of the RecordBatch\n            size and 1024 * 1024.  If set larger than 64Mi then 64Mi\n            will be used instead.\n        \"\"\"\n        table = pa.Table.from_batches([batch], batch.schema)\n        self.write_table(table, row_group_size)\n\n    def write_table(self, table, row_group_size=None):\n        \"\"\"\n        Write Table to the Parquet file.\n\n        Parameters\n        ----------\n        table : Table\n        row_group_size : int, default None\n            Maximum number of rows in each written row group. If None,\n            the row group size will be the minimum of the Table size\n            and 1024 * 1024.  If set larger than 64Mi then 64Mi will\n            be used instead.\n\n        \"\"\"\n        if self.schema_changed:\n            table = _sanitize_table(table, self.schema, self.flavor)\n        assert self.is_open\n\n        if not table.schema.equals(self.schema, check_metadata=False):\n            msg = ('Table schema does not match schema used to create file: '\n                   '\\ntable:\\n{!s} vs. \\nfile:\\n{!s}'\n                   .format(table.schema, self.schema))\n            raise ValueError(msg)\n\n        self.writer.write_table(table, row_group_size=row_group_size)\n\n    def close(self):\n        \"\"\"\n        Close the connection to the Parquet file.\n        \"\"\"\n        if self.is_open:\n            self.writer.close()\n            self.is_open = False\n            if self._metadata_collector is not None:\n                self._metadata_collector.append(self.writer.metadata)\n        if self.file_handle is not None:\n            self.file_handle.close()\n\n    def add_key_value_metadata(self, key_value_metadata):\n        \"\"\"\n        Add key-value metadata to the file.\n        This will overwrite any existing metadata with the same key.\n\n        Parameters\n        ----------\n        key_value_metadata : dict\n            Keys and values must be string-like / coercible to bytes.\n        \"\"\"\n        assert self.is_open\n        self.writer.add_key_value_metadata(key_value_metadata)\n\n\ndef _get_pandas_index_columns(keyvalues):\n    return (json.loads(keyvalues[b'pandas'].decode('utf8'))\n            ['index_columns'])\n\n\nEXCLUDED_PARQUET_PATHS = {'_SUCCESS'}\n\n\n_read_docstring_common = \"\"\"\\\nread_dictionary : list, default None\n    List of names or column paths (for nested types) to read directly\n    as DictionaryArray. Only supported for BYTE_ARRAY storage. To read\n    a flat column as dictionary-encoded pass the column name. For\n    nested types, you must pass the full column \"path\", which could be\n    something like level1.level2.list.item. Refer to the Parquet\n    file's schema to obtain the paths.\nmemory_map : bool, default False\n    If the source is a file path, use a memory map to read file, which can\n    improve performance in some environments.\nbuffer_size : int, default 0\n    If positive, perform read buffering when deserializing individual\n    column chunks. Otherwise IO calls are unbuffered.\npartitioning : pyarrow.dataset.Partitioning or str or list of str, \\\ndefault \"hive\"\n    The partitioning scheme for a partitioned dataset. The default of \"hive\"\n    assumes directory names with key=value pairs like \"/year=2009/month=11\".\n    In addition, a scheme like \"/2009/11\" is also supported, in which case\n    you need to specify the field names or a full schema. See the\n    ``pyarrow.dataset.partitioning()`` function for more details.\"\"\"\n\n\n_parquet_dataset_example = \"\"\"\\\nGenerate an example PyArrow Table and write it to a partitioned dataset:\n\n>>> import pyarrow as pa\n>>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n...                   'n_legs': [2, 2, 4, 4, 5, 100],\n...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n...                              \"Brittle stars\", \"Centipede\"]})\n>>> import pyarrow.parquet as pq\n>>> pq.write_to_dataset(table, root_path='dataset_v2',\n...                     partition_cols=['year'])\n\ncreate a ParquetDataset object from the dataset source:\n\n>>> dataset = pq.ParquetDataset('dataset_v2/')\n\nand read the data:\n\n>>> dataset.read().to_pandas()\n   n_legs         animal  year\n0       5  Brittle stars  2019\n1       2       Flamingo  2020\n2       4            Dog  2021\n3     100      Centipede  2021\n4       2         Parrot  2022\n5       4          Horse  2022\n\ncreate a ParquetDataset object with filter:\n\n>>> dataset = pq.ParquetDataset('dataset_v2/',\n...                             filters=[('n_legs','=',4)])\n>>> dataset.read().to_pandas()\n   n_legs animal  year\n0       4    Dog  2021\n1       4  Horse  2022\n\"\"\"\n\n\nclass ParquetDataset:\n    __doc__ = \"\"\"\nEncapsulates details of reading a complete Parquet dataset possibly\nconsisting of multiple files and partitions in subdirectories.\n\nParameters\n----------\npath_or_paths : str or List[str]\n    A directory name, single file name, or list of file names.\nfilesystem : FileSystem, default None\n    If nothing passed, will be inferred based on path.\n    Path will try to be found in the local on-disk filesystem otherwise\n    it will be parsed as an URI to determine the filesystem.\nschema : pyarrow.parquet.Schema\n    Optionally provide the Schema for the Dataset, in which case it will\n    not be inferred from the source.\nfilters : pyarrow.compute.Expression or List[Tuple] or List[List[Tuple]], default None\n    Rows which do not match the filter predicate will be removed from scanned\n    data. Partition keys embedded in a nested directory structure will be\n    exploited to avoid loading files at all if they contain no matching rows.\n    Within-file level filtering and different partitioning schemes are supported.\n\n    {1}\n{0}\nignore_prefixes : list, optional\n    Files matching any of these prefixes will be ignored by the\n    discovery process.\n    This is matched to the basename of a path.\n    By default this is ['.', '_'].\n    Note that discovery happens only if a directory is passed as source.\npre_buffer : bool, default True\n    Coalesce and issue file reads in parallel to improve performance on\n    high-latency filesystems (e.g. S3, GCS). If True, Arrow will use a\n    background I/O thread pool. If using a filesystem layer that itself\n    performs readahead (e.g. fsspec's S3FS), disable readahead for best\n    results. Set to False if you want to prioritize minimal memory usage\n    over maximum speed.\ncoerce_int96_timestamp_unit : str, default None\n    Cast timestamps that are stored in INT96 format to a particular resolution\n    (e.g. 'ms'). Setting to None is equivalent to 'ns' and therefore INT96\n    timestamps will be inferred as timestamps in nanoseconds.\ndecryption_properties : FileDecryptionProperties or None\n    File-level decryption properties.\n    The decryption properties can be created using\n    ``CryptoFactory.file_decryption_properties()``.\nthrift_string_size_limit : int, default None\n    If not None, override the maximum total string size allocated\n    when decoding Thrift structures. The default limit should be\n    sufficient for most Parquet files.\nthrift_container_size_limit : int, default None\n    If not None, override the maximum total size of containers allocated\n    when decoding Thrift structures. The default limit should be\n    sufficient for most Parquet files.\npage_checksum_verification : bool, default False\n    If True, verify the page checksum for each page read from the file.\nuse_legacy_dataset : bool, optional\n    Deprecated and has no effect from PyArrow version 15.0.0.\n\nExamples\n--------\n{2}\n\"\"\".format(_read_docstring_common, _DNF_filter_doc, _parquet_dataset_example)\n\n    def __init__(self, path_or_paths, filesystem=None, schema=None, *, filters=None,\n                 read_dictionary=None, memory_map=False, buffer_size=None,\n                 partitioning=\"hive\", ignore_prefixes=None, pre_buffer=True,\n                 coerce_int96_timestamp_unit=None,\n                 decryption_properties=None, thrift_string_size_limit=None,\n                 thrift_container_size_limit=None,\n                 page_checksum_verification=False,\n                 use_legacy_dataset=None):\n\n        if use_legacy_dataset is not None:\n            warnings.warn(\n                \"Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 \"\n                \"and will be removed in a future version.\",\n                FutureWarning, stacklevel=2)\n\n        import pyarrow.dataset as ds\n\n        # map format arguments\n        read_options = {\n            \"pre_buffer\": pre_buffer,\n            \"coerce_int96_timestamp_unit\": coerce_int96_timestamp_unit,\n            \"thrift_string_size_limit\": thrift_string_size_limit,\n            \"thrift_container_size_limit\": thrift_container_size_limit,\n            \"page_checksum_verification\": page_checksum_verification,\n        }\n        if buffer_size:\n            read_options.update(use_buffered_stream=True,\n                                buffer_size=buffer_size)\n        if read_dictionary is not None:\n            read_options.update(dictionary_columns=read_dictionary)\n\n        if decryption_properties is not None:\n            read_options.update(decryption_properties=decryption_properties)\n\n        self._filter_expression = None\n        if filters is not None:\n            self._filter_expression = filters_to_expression(filters)\n\n        # map old filesystems to new one\n        if filesystem is not None:\n            filesystem = _ensure_filesystem(\n                filesystem, use_mmap=memory_map)\n        elif filesystem is None and memory_map:\n            # if memory_map is specified, assume local file system (string\n            # path can in principle be URI for any filesystem)\n            filesystem = LocalFileSystem(use_mmap=memory_map)\n\n        # This needs to be checked after _ensure_filesystem, because that\n        # handles the case of an fsspec LocalFileSystem\n        if (\n            hasattr(path_or_paths, \"__fspath__\") and\n            filesystem is not None and\n            not isinstance(filesystem, LocalFileSystem)\n        ):\n            raise TypeError(\n                \"Path-like objects with __fspath__ must only be used with \"\n                f\"local file systems, not {type(filesystem)}\"\n            )\n\n        # check for single fragment dataset or dataset directory\n        single_file = None\n        self._base_dir = None\n        if not isinstance(path_or_paths, list):\n            if _is_path_like(path_or_paths):\n                path_or_paths = _stringify_path(path_or_paths)\n                if filesystem is None:\n                    # path might be a URI describing the FileSystem as well\n                    try:\n                        filesystem, path_or_paths = FileSystem.from_uri(\n                            path_or_paths)\n                    except ValueError:\n                        filesystem = LocalFileSystem(use_mmap=memory_map)\n                finfo = filesystem.get_file_info(path_or_paths)\n                if finfo.type == FileType.Directory:\n                    self._base_dir = path_or_paths\n            else:\n                single_file = path_or_paths\n\n        parquet_format = ds.ParquetFileFormat(**read_options)\n\n        if single_file is not None:\n            fragment = parquet_format.make_fragment(single_file, filesystem)\n\n            self._dataset = ds.FileSystemDataset(\n                [fragment], schema=schema or fragment.physical_schema,\n                format=parquet_format,\n                filesystem=fragment.filesystem\n            )\n            return\n\n        # check partitioning to enable dictionary encoding\n        if partitioning == \"hive\":\n            partitioning = ds.HivePartitioning.discover(\n                infer_dictionary=True)\n\n        self._dataset = ds.dataset(path_or_paths, filesystem=filesystem,\n                                   schema=schema, format=parquet_format,\n                                   partitioning=partitioning,\n                                   ignore_prefixes=ignore_prefixes)\n\n    def equals(self, other):\n        if not isinstance(other, ParquetDataset):\n            raise TypeError('`other` must be an instance of ParquetDataset')\n\n        return (self.schema == other.schema and\n                self._dataset.format == other._dataset.format and\n                self.filesystem == other.filesystem and\n                # self.fragments == other.fragments and\n                self.files == other.files)\n\n    def __eq__(self, other):\n        try:\n            return self.equals(other)\n        except TypeError:\n            return NotImplemented\n\n    @property\n    def schema(self):\n        \"\"\"\n        Schema of the Dataset.\n\n        Examples\n        --------\n        Generate an example dataset:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n        ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_to_dataset(table, root_path='dataset_v2_schema',\n        ...                     partition_cols=['year'])\n        >>> dataset = pq.ParquetDataset('dataset_v2_schema/')\n\n        Read the schema:\n\n        >>> dataset.schema\n        n_legs: int64\n        animal: string\n        year: dictionary<values=int32, indices=int32, ordered=0>\n        \"\"\"\n        return self._dataset.schema\n\n    def read(self, columns=None, use_threads=True, use_pandas_metadata=False):\n        \"\"\"\n        Read (multiple) Parquet files as a single pyarrow.Table.\n\n        Parameters\n        ----------\n        columns : List[str]\n            Names of columns to read from the dataset. The partition fields\n            are not automatically included.\n        use_threads : bool, default True\n            Perform multi-threaded column reads.\n        use_pandas_metadata : bool, default False\n            If True and file has custom pandas schema metadata, ensure that\n            index columns are also loaded.\n\n        Returns\n        -------\n        pyarrow.Table\n            Content of the file as a table (of columns).\n\n        Examples\n        --------\n        Generate an example dataset:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n        ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_to_dataset(table, root_path='dataset_v2_read',\n        ...                     partition_cols=['year'])\n        >>> dataset = pq.ParquetDataset('dataset_v2_read/')\n\n        Read the dataset:\n\n        >>> dataset.read(columns=[\"n_legs\"])\n        pyarrow.Table\n        n_legs: int64\n        ----\n        n_legs: [[5],[2],[4,100],[2,4]]\n        \"\"\"\n        # if use_pandas_metadata, we need to include index columns in the\n        # column selection, to be able to restore those in the pandas DataFrame\n        metadata = self.schema.metadata or {}\n\n        if use_pandas_metadata:\n            # if the dataset schema metadata itself doesn't have pandas\n            # then try to get this from common file (for backwards compat)\n            if b\"pandas\" not in metadata:\n                common_metadata = self._get_common_pandas_metadata()\n                if common_metadata:\n                    metadata = common_metadata\n\n        if columns is not None and use_pandas_metadata:\n            if metadata and b'pandas' in metadata:\n                # RangeIndex can be represented as dict instead of column name\n                index_columns = [\n                    col for col in _get_pandas_index_columns(metadata)\n                    if not isinstance(col, dict)\n                ]\n                columns = (\n                    list(columns) + list(set(index_columns) - set(columns))\n                )\n\n        table = self._dataset.to_table(\n            columns=columns, filter=self._filter_expression,\n            use_threads=use_threads\n        )\n\n        # if use_pandas_metadata, restore the pandas metadata (which gets\n        # lost if doing a specific `columns` selection in to_table)\n        if use_pandas_metadata:\n            if metadata and b\"pandas\" in metadata:\n                new_metadata = table.schema.metadata or {}\n                new_metadata.update({b\"pandas\": metadata[b\"pandas\"]})\n                table = table.replace_schema_metadata(new_metadata)\n\n        return table\n\n    def _get_common_pandas_metadata(self):\n\n        if not self._base_dir:\n            return None\n\n        metadata = None\n        for name in [\"_common_metadata\", \"_metadata\"]:\n            metadata_path = os.path.join(str(self._base_dir), name)\n            finfo = self.filesystem.get_file_info(metadata_path)\n            if finfo.is_file:\n                pq_meta = read_metadata(\n                    metadata_path, filesystem=self.filesystem)\n                metadata = pq_meta.metadata\n                if metadata and b'pandas' in metadata:\n                    break\n\n        return metadata\n\n    def read_pandas(self, **kwargs):\n        \"\"\"\n        Read dataset including pandas metadata, if any. Other arguments passed\n        through to :func:`read`, see docstring for further details.\n\n        Parameters\n        ----------\n        **kwargs : optional\n            Additional options for :func:`read`\n\n        Examples\n        --------\n        Generate an example parquet file:\n\n        >>> import pyarrow as pa\n        >>> import pandas as pd\n        >>> df = pd.DataFrame({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n        ...                    'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                    'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                    \"Brittle stars\", \"Centipede\"]})\n        >>> table = pa.Table.from_pandas(df)\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_table(table, 'table_V2.parquet')\n        >>> dataset = pq.ParquetDataset('table_V2.parquet')\n\n        Read the dataset with pandas metadata:\n\n        >>> dataset.read_pandas(columns=[\"n_legs\"])\n        pyarrow.Table\n        n_legs: int64\n        ----\n        n_legs: [[2,2,4,4,5,100]]\n\n        >>> dataset.read_pandas(columns=[\"n_legs\"]).schema.pandas_metadata\n        {'index_columns': [{'kind': 'range', 'name': None, 'start': 0, ...}\n        \"\"\"\n        return self.read(use_pandas_metadata=True, **kwargs)\n\n    @property\n    def fragments(self):\n        \"\"\"\n        A list of the Dataset source fragments or pieces with absolute\n        file paths.\n\n        Examples\n        --------\n        Generate an example dataset:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n        ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_to_dataset(table, root_path='dataset_v2_fragments',\n        ...                     partition_cols=['year'])\n        >>> dataset = pq.ParquetDataset('dataset_v2_fragments/')\n\n        List the fragments:\n\n        >>> dataset.fragments\n        [<pyarrow.dataset.ParquetFileFragment path=dataset_v2_fragments/...\n        \"\"\"\n        return list(self._dataset.get_fragments())\n\n    @property\n    def files(self):\n        \"\"\"\n        A list of absolute Parquet file paths in the Dataset source.\n\n        Examples\n        --------\n        Generate an example dataset:\n\n        >>> import pyarrow as pa\n        >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n        ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n        ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n        ...                              \"Brittle stars\", \"Centipede\"]})\n        >>> import pyarrow.parquet as pq\n        >>> pq.write_to_dataset(table, root_path='dataset_v2_files',\n        ...                     partition_cols=['year'])\n        >>> dataset = pq.ParquetDataset('dataset_v2_files/')\n\n        List the files:\n\n        >>> dataset.files\n        ['dataset_v2_files/year=2019/...-0.parquet', ...\n        \"\"\"\n        return self._dataset.files\n\n    @property\n    def filesystem(self):\n        \"\"\"\n        The filesystem type of the Dataset source.\n        \"\"\"\n        return self._dataset.filesystem\n\n    @property\n    def partitioning(self):\n        \"\"\"\n        The partitioning of the Dataset source, if discovered.\n        \"\"\"\n        return self._dataset.partitioning\n\n\n_read_table_docstring = \"\"\"\n{0}\n\nParameters\n----------\nsource : str, pyarrow.NativeFile, or file-like object\n    If a string passed, can be a single file name or directory name. For\n    file-like objects, only read a single file. Use pyarrow.BufferReader to\n    read a file contained in a bytes or buffer-like object.\ncolumns : list\n    If not None, only these columns will be read from the file. A column\n    name may be a prefix of a nested field, e.g. 'a' will select 'a.b',\n    'a.c', and 'a.d.e'. If empty, no columns will be read. Note\n    that the table will still have the correct num_rows set despite having\n    no columns.\nuse_threads : bool, default True\n    Perform multi-threaded column reads.\nschema : Schema, optional\n    Optionally provide the Schema for the parquet dataset, in which case it\n    will not be inferred from the source.\n{1}\nfilesystem : FileSystem, default None\n    If nothing passed, will be inferred based on path.\n    Path will try to be found in the local on-disk filesystem otherwise\n    it will be parsed as an URI to determine the filesystem.\nfilters : pyarrow.compute.Expression or List[Tuple] or List[List[Tuple]], default None\n    Rows which do not match the filter predicate will be removed from scanned\n    data. Partition keys embedded in a nested directory structure will be\n    exploited to avoid loading files at all if they contain no matching rows.\n    Within-file level filtering and different partitioning schemes are supported.\n\n    {3}\nuse_legacy_dataset : bool, optional\n    Deprecated and has no effect from PyArrow version 15.0.0.\nignore_prefixes : list, optional\n    Files matching any of these prefixes will be ignored by the\n    discovery process.\n    This is matched to the basename of a path.\n    By default this is ['.', '_'].\n    Note that discovery happens only if a directory is passed as source.\npre_buffer : bool, default True\n    Coalesce and issue file reads in parallel to improve performance on\n    high-latency filesystems (e.g. S3). If True, Arrow will use a\n    background I/O thread pool. If using a filesystem layer that itself\n    performs readahead (e.g. fsspec's S3FS), disable readahead for best\n    results.\ncoerce_int96_timestamp_unit : str, default None\n    Cast timestamps that are stored in INT96 format to a particular\n    resolution (e.g. 'ms'). Setting to None is equivalent to 'ns'\n    and therefore INT96 timestamps will be inferred as timestamps\n    in nanoseconds.\ndecryption_properties : FileDecryptionProperties or None\n    File-level decryption properties.\n    The decryption properties can be created using\n    ``CryptoFactory.file_decryption_properties()``.\nthrift_string_size_limit : int, default None\n    If not None, override the maximum total string size allocated\n    when decoding Thrift structures. The default limit should be\n    sufficient for most Parquet files.\nthrift_container_size_limit : int, default None\n    If not None, override the maximum total size of containers allocated\n    when decoding Thrift structures. The default limit should be\n    sufficient for most Parquet files.\npage_checksum_verification : bool, default False\n    If True, verify the checksum for each page read from the file.\n\nReturns\n-------\n{2}\n\n{4}\n\"\"\"\n\n_read_table_example = \"\"\"\\\n\nExamples\n--------\n\nGenerate an example PyArrow Table and write it to a partitioned dataset:\n\n>>> import pyarrow as pa\n>>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n...                   'n_legs': [2, 2, 4, 4, 5, 100],\n...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n...                              \"Brittle stars\", \"Centipede\"]})\n>>> import pyarrow.parquet as pq\n>>> pq.write_to_dataset(table, root_path='dataset_name_2',\n...                     partition_cols=['year'])\n\nRead the data:\n\n>>> pq.read_table('dataset_name_2').to_pandas()\n   n_legs         animal  year\n0       5  Brittle stars  2019\n1       2       Flamingo  2020\n2       4            Dog  2021\n3     100      Centipede  2021\n4       2         Parrot  2022\n5       4          Horse  2022\n\n\nRead only a subset of columns:\n\n>>> pq.read_table('dataset_name_2', columns=[\"n_legs\", \"animal\"])\npyarrow.Table\nn_legs: int64\nanimal: string\n----\nn_legs: [[5],[2],[4,100],[2,4]]\nanimal: [[\"Brittle stars\"],[\"Flamingo\"],[\"Dog\",\"Centipede\"],[\"Parrot\",\"Horse\"]]\n\nRead a subset of columns and read one column as DictionaryArray:\n\n>>> pq.read_table('dataset_name_2', columns=[\"n_legs\", \"animal\"],\n...               read_dictionary=[\"animal\"])\npyarrow.Table\nn_legs: int64\nanimal: dictionary<values=string, indices=int32, ordered=0>\n----\nn_legs: [[5],[2],[4,100],[2,4]]\nanimal: [  -- dictionary:\n[\"Brittle stars\"]  -- indices:\n[0],  -- dictionary:\n[\"Flamingo\"]  -- indices:\n[0],  -- dictionary:\n[\"Dog\",\"Centipede\"]  -- indices:\n[0,1],  -- dictionary:\n[\"Parrot\",\"Horse\"]  -- indices:\n[0,1]]\n\nRead the table with filter:\n\n>>> pq.read_table('dataset_name_2', columns=[\"n_legs\", \"animal\"],\n...               filters=[('n_legs','<',4)]).to_pandas()\n   n_legs    animal\n0       2  Flamingo\n1       2    Parrot\n\nRead data from a single Parquet file:\n\n>>> pq.write_table(table, 'example.parquet')\n>>> pq.read_table('dataset_name_2').to_pandas()\n   n_legs         animal  year\n0       5  Brittle stars  2019\n1       2       Flamingo  2020\n2       4            Dog  2021\n3     100      Centipede  2021\n4       2         Parrot  2022\n5       4          Horse  2022\n\"\"\"\n\n\ndef read_table(source, *, columns=None, use_threads=True,\n               schema=None, use_pandas_metadata=False, read_dictionary=None,\n               memory_map=False, buffer_size=0, partitioning=\"hive\",\n               filesystem=None, filters=None, use_legacy_dataset=None,\n               ignore_prefixes=None, pre_buffer=True,\n               coerce_int96_timestamp_unit=None,\n               decryption_properties=None, thrift_string_size_limit=None,\n               thrift_container_size_limit=None,\n               page_checksum_verification=False):\n\n    if use_legacy_dataset is not None:\n        warnings.warn(\n            \"Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 \"\n            \"and will be removed in a future version.\",\n            FutureWarning, stacklevel=2)\n\n    try:\n        dataset = ParquetDataset(\n            source,\n            schema=schema,\n            filesystem=filesystem,\n            partitioning=partitioning,\n            memory_map=memory_map,\n            read_dictionary=read_dictionary,\n            buffer_size=buffer_size,\n            filters=filters,\n            ignore_prefixes=ignore_prefixes,\n            pre_buffer=pre_buffer,\n            coerce_int96_timestamp_unit=coerce_int96_timestamp_unit,\n            decryption_properties=decryption_properties,\n            thrift_string_size_limit=thrift_string_size_limit,\n            thrift_container_size_limit=thrift_container_size_limit,\n            page_checksum_verification=page_checksum_verification,\n        )\n    except ImportError:\n        # fall back on ParquetFile for simple cases when pyarrow.dataset\n        # module is not available\n        if filters is not None:\n            raise ValueError(\n                \"the 'filters' keyword is not supported when the \"\n                \"pyarrow.dataset module is not available\"\n            )\n        if partitioning != \"hive\":\n            raise ValueError(\n                \"the 'partitioning' keyword is not supported when the \"\n                \"pyarrow.dataset module is not available\"\n            )\n        if schema is not None:\n            raise ValueError(\n                \"the 'schema' argument is not supported when the \"\n                \"pyarrow.dataset module is not available\"\n            )\n        filesystem, path = _resolve_filesystem_and_path(source, filesystem)\n        if filesystem is not None:\n            source = filesystem.open_input_file(path)\n        # TODO test that source is not a directory or a list\n        dataset = ParquetFile(\n            source, read_dictionary=read_dictionary,\n            memory_map=memory_map, buffer_size=buffer_size,\n            pre_buffer=pre_buffer,\n            coerce_int96_timestamp_unit=coerce_int96_timestamp_unit,\n            decryption_properties=decryption_properties,\n            thrift_string_size_limit=thrift_string_size_limit,\n            thrift_container_size_limit=thrift_container_size_limit,\n            page_checksum_verification=page_checksum_verification,\n        )\n\n    return dataset.read(columns=columns, use_threads=use_threads,\n                        use_pandas_metadata=use_pandas_metadata)\n\n\nread_table.__doc__ = _read_table_docstring.format(\n    \"\"\"Read a Table from Parquet format\"\"\",\n    \"\\n\".join((\"\"\"use_pandas_metadata : bool, default False\n    If True and file has custom pandas schema metadata, ensure that\n    index columns are also loaded.\"\"\", _read_docstring_common)),\n    \"\"\"pyarrow.Table\n    Content of the file as a table (of columns)\"\"\",\n    _DNF_filter_doc, _read_table_example)\n\n\ndef read_pandas(source, columns=None, **kwargs):\n    return read_table(\n        source, columns=columns, use_pandas_metadata=True, **kwargs\n    )\n\n\nread_pandas.__doc__ = _read_table_docstring.format(\n    'Read a Table from Parquet format, also reading DataFrame\\n'\n    'index values if known in the file metadata',\n    \"\\n\".join((_read_docstring_common,\n               \"\"\"**kwargs\n    additional options for :func:`read_table`\"\"\")),\n    \"\"\"pyarrow.Table\n    Content of the file as a Table of Columns, including DataFrame\n    indexes as columns\"\"\",\n    _DNF_filter_doc, \"\")\n\n\ndef write_table(table, where, row_group_size=None, version='2.6',\n                use_dictionary=True, compression='snappy',\n                write_statistics=True,\n                use_deprecated_int96_timestamps=None,\n                coerce_timestamps=None,\n                allow_truncated_timestamps=False,\n                data_page_size=None, flavor=None,\n                filesystem=None,\n                compression_level=None,\n                use_byte_stream_split=False,\n                column_encoding=None,\n                data_page_version='1.0',\n                use_compliant_nested_type=True,\n                encryption_properties=None,\n                write_batch_size=None,\n                dictionary_pagesize_limit=None,\n                store_schema=True,\n                write_page_index=False,\n                write_page_checksum=False,\n                sorting_columns=None,\n                **kwargs):\n    # Implementor's note: when adding keywords here / updating defaults, also\n    # update it in write_to_dataset and _dataset_parquet.pyx ParquetFileWriteOptions\n    row_group_size = kwargs.pop('chunk_size', row_group_size)\n    use_int96 = use_deprecated_int96_timestamps\n    try:\n        with ParquetWriter(\n                where, table.schema,\n                filesystem=filesystem,\n                version=version,\n                flavor=flavor,\n                use_dictionary=use_dictionary,\n                write_statistics=write_statistics,\n                coerce_timestamps=coerce_timestamps,\n                data_page_size=data_page_size,\n                allow_truncated_timestamps=allow_truncated_timestamps,\n                compression=compression,\n                use_deprecated_int96_timestamps=use_int96,\n                compression_level=compression_level,\n                use_byte_stream_split=use_byte_stream_split,\n                column_encoding=column_encoding,\n                data_page_version=data_page_version,\n                use_compliant_nested_type=use_compliant_nested_type,\n                encryption_properties=encryption_properties,\n                write_batch_size=write_batch_size,\n                dictionary_pagesize_limit=dictionary_pagesize_limit,\n                store_schema=store_schema,\n                write_page_index=write_page_index,\n                write_page_checksum=write_page_checksum,\n                sorting_columns=sorting_columns,\n                **kwargs) as writer:\n            writer.write_table(table, row_group_size=row_group_size)\n    except Exception:\n        if _is_path_like(where):\n            try:\n                os.remove(_stringify_path(where))\n            except os.error:\n                pass\n        raise\n\n\n_write_table_example = \"\"\"\\\nGenerate an example PyArrow Table:\n\n>>> import pyarrow as pa\n>>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n...                              \"Brittle stars\", \"Centipede\"]})\n\nand write the Table into Parquet file:\n\n>>> import pyarrow.parquet as pq\n>>> pq.write_table(table, 'example.parquet')\n\nDefining row group size for the Parquet file:\n\n>>> pq.write_table(table, 'example.parquet', row_group_size=3)\n\nDefining row group compression (default is Snappy):\n\n>>> pq.write_table(table, 'example.parquet', compression='none')\n\nDefining row group compression and encoding per-column:\n\n>>> pq.write_table(table, 'example.parquet',\n...                compression={'n_legs': 'snappy', 'animal': 'gzip'},\n...                use_dictionary=['n_legs', 'animal'])\n\nDefining column encoding per-column:\n\n>>> pq.write_table(table, 'example.parquet',\n...                column_encoding={'animal':'PLAIN'},\n...                use_dictionary=False)\n\"\"\"\n\nwrite_table.__doc__ = \"\"\"\nWrite a Table to Parquet format.\n\nParameters\n----------\ntable : pyarrow.Table\nwhere : string or pyarrow.NativeFile\nrow_group_size : int\n    Maximum number of rows in each written row group. If None, the\n    row group size will be the minimum of the Table size and\n    1024 * 1024.\n{}\n**kwargs : optional\n    Additional options for ParquetWriter\n\nExamples\n--------\n{}\n\"\"\".format(_parquet_writer_arg_docs, _write_table_example)\n\n\ndef write_to_dataset(table, root_path, partition_cols=None,\n                     filesystem=None, use_legacy_dataset=None,\n                     schema=None, partitioning=None,\n                     basename_template=None, use_threads=None,\n                     file_visitor=None, existing_data_behavior=None,\n                     **kwargs):\n    \"\"\"Wrapper around dataset.write_dataset for writing a Table to\n    Parquet format by partitions.\n    For each combination of partition columns and values,\n    a subdirectories are created in the following\n    manner:\n\n    root_dir/\n      group1=value1\n        group2=value1\n          <uuid>.parquet\n        group2=value2\n          <uuid>.parquet\n      group1=valueN\n        group2=value1\n          <uuid>.parquet\n        group2=valueN\n          <uuid>.parquet\n\n    Parameters\n    ----------\n    table : pyarrow.Table\n    root_path : str, pathlib.Path\n        The root directory of the dataset.\n    partition_cols : list,\n        Column names by which to partition the dataset.\n        Columns are partitioned in the order they are given.\n    filesystem : FileSystem, default None\n        If nothing passed, will be inferred based on path.\n        Path will try to be found in the local on-disk filesystem otherwise\n        it will be parsed as an URI to determine the filesystem.\n    use_legacy_dataset : bool, optional\n        Deprecated and has no effect from PyArrow version 15.0.0.\n    schema : Schema, optional\n        This Schema of the dataset.\n    partitioning : Partitioning or list[str], optional\n        The partitioning scheme specified with the\n        ``pyarrow.dataset.partitioning()`` function or a list of field names.\n        When providing a list of field names, you can use\n        ``partitioning_flavor`` to drive which partitioning type should be\n        used.\n    basename_template : str, optional\n        A template string used to generate basenames of written data files.\n        The token '{i}' will be replaced with an automatically incremented\n        integer. If not specified, it defaults to \"guid-{i}.parquet\".\n    use_threads : bool, default True\n        Write files in parallel. If enabled, then maximum parallelism will be\n        used determined by the number of available CPU cores.\n    file_visitor : function\n        If set, this function will be called with a WrittenFile instance\n        for each file created during the call.  This object will have both\n        a path attribute and a metadata attribute.\n\n        The path attribute will be a string containing the path to\n        the created file.\n\n        The metadata attribute will be the parquet metadata of the file.\n        This metadata will have the file path attribute set and can be used\n        to build a _metadata file.  The metadata attribute will be None if\n        the format is not parquet.\n\n        Example visitor which simple collects the filenames created::\n\n            visited_paths = []\n\n            def file_visitor(written_file):\n                visited_paths.append(written_file.path)\n\n    existing_data_behavior : 'overwrite_or_ignore' | 'error' | \\\n'delete_matching'\n        Controls how the dataset will handle data that already exists in\n        the destination. The default behaviour is 'overwrite_or_ignore'.\n\n        'overwrite_or_ignore' will ignore any existing data and will\n        overwrite files with the same name as an output file.  Other\n        existing files will be ignored.  This behavior, in combination\n        with a unique basename_template for each write, will allow for\n        an append workflow.\n\n        'error' will raise an error if any data exists in the destination.\n\n        'delete_matching' is useful when you are writing a partitioned\n        dataset.  The first time each partition directory is encountered\n        the entire directory will be deleted.  This allows you to overwrite\n        old partitions completely.\n    **kwargs : dict,\n        Used as additional kwargs for :func:`pyarrow.dataset.write_dataset`\n        function for matching kwargs, and remainder to\n        :func:`pyarrow.dataset.ParquetFileFormat.make_write_options`.\n        See the docstring of :func:`write_table` and\n        :func:`pyarrow.dataset.write_dataset` for the available options.\n        Using `metadata_collector` in kwargs allows one to collect the\n        file metadata instances of dataset pieces. The file paths in the\n        ColumnChunkMetaData will be set relative to `root_path`.\n\n    Examples\n    --------\n    Generate an example PyArrow Table:\n\n    >>> import pyarrow as pa\n    >>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n    ...                   'n_legs': [2, 2, 4, 4, 5, 100],\n    ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n    ...                              \"Brittle stars\", \"Centipede\"]})\n\n    and write it to a partitioned dataset:\n\n    >>> import pyarrow.parquet as pq\n    >>> pq.write_to_dataset(table, root_path='dataset_name_3',\n    ...                     partition_cols=['year'])\n    >>> pq.ParquetDataset('dataset_name_3').files\n    ['dataset_name_3/year=2019/...-0.parquet', ...\n\n    Write a single Parquet file into the root folder:\n\n    >>> pq.write_to_dataset(table, root_path='dataset_name_4')\n    >>> pq.ParquetDataset('dataset_name_4/').files\n    ['dataset_name_4/...-0.parquet']\n    \"\"\"\n    if use_legacy_dataset is not None:\n        warnings.warn(\n            \"Passing 'use_legacy_dataset' is deprecated as of pyarrow 15.0.0 \"\n            \"and will be removed in a future version.\",\n            FutureWarning, stacklevel=2)\n\n    metadata_collector = kwargs.pop('metadata_collector', None)\n\n    # Check for conflicting keywords\n    msg_confl = (\n        \"The '{1}' argument is not supported. \"\n        \"Use only '{0}' instead.\"\n    )\n    if partition_cols is not None and partitioning is not None:\n        raise ValueError(msg_confl.format(\"partitioning\",\n                                          \"partition_cols\"))\n\n    if metadata_collector is not None and file_visitor is not None:\n        raise ValueError(msg_confl.format(\"file_visitor\",\n                                          \"metadata_collector\"))\n\n    import pyarrow.dataset as ds\n\n    # extract write_dataset specific options\n    # reset assumed to go to make_write_options\n    write_dataset_kwargs = dict()\n    for key in inspect.signature(ds.write_dataset).parameters:\n        if key in kwargs:\n            write_dataset_kwargs[key] = kwargs.pop(key)\n    write_dataset_kwargs['max_rows_per_group'] = kwargs.pop(\n        'row_group_size', kwargs.pop(\"chunk_size\", None)\n    )\n\n    if metadata_collector is not None:\n        def file_visitor(written_file):\n            metadata_collector.append(written_file.metadata)\n\n    # map format arguments\n    parquet_format = ds.ParquetFileFormat()\n    write_options = parquet_format.make_write_options(**kwargs)\n\n    # map old filesystems to new one\n    if filesystem is not None:\n        filesystem = _ensure_filesystem(filesystem)\n\n    if partition_cols:\n        part_schema = table.select(partition_cols).schema\n        partitioning = ds.partitioning(part_schema, flavor=\"hive\")\n\n    if basename_template is None:\n        basename_template = guid() + '-{i}.parquet'\n\n    if existing_data_behavior is None:\n        existing_data_behavior = 'overwrite_or_ignore'\n\n    ds.write_dataset(\n        table, root_path, filesystem=filesystem,\n        format=parquet_format, file_options=write_options, schema=schema,\n        partitioning=partitioning, use_threads=use_threads,\n        file_visitor=file_visitor,\n        basename_template=basename_template,\n        existing_data_behavior=existing_data_behavior,\n        **write_dataset_kwargs)\n    return\n\n\ndef write_metadata(schema, where, metadata_collector=None, filesystem=None,\n                   **kwargs):\n    \"\"\"\n    Write metadata-only Parquet file from schema. This can be used with\n    `write_to_dataset` to generate `_common_metadata` and `_metadata` sidecar\n    files.\n\n    Parameters\n    ----------\n    schema : pyarrow.Schema\n    where : string or pyarrow.NativeFile\n    metadata_collector : list\n        where to collect metadata information.\n    filesystem : FileSystem, default None\n        If nothing passed, will be inferred from `where` if path-like, else\n        `where` is already a file-like object so no filesystem is needed.\n    **kwargs : dict,\n        Additional kwargs for ParquetWriter class. See docstring for\n        `ParquetWriter` for more information.\n\n    Examples\n    --------\n    Generate example data:\n\n    >>> import pyarrow as pa\n    >>> table = pa.table({'n_legs': [2, 2, 4, 4, 5, 100],\n    ...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n    ...                              \"Brittle stars\", \"Centipede\"]})\n\n    Write a dataset and collect metadata information.\n\n    >>> metadata_collector = []\n    >>> import pyarrow.parquet as pq\n    >>> pq.write_to_dataset(\n    ...     table, 'dataset_metadata',\n    ...      metadata_collector=metadata_collector)\n\n    Write the `_common_metadata` parquet file without row groups statistics.\n\n    >>> pq.write_metadata(\n    ...     table.schema, 'dataset_metadata/_common_metadata')\n\n    Write the `_metadata` parquet file with row groups statistics.\n\n    >>> pq.write_metadata(\n    ...     table.schema, 'dataset_metadata/_metadata',\n    ...     metadata_collector=metadata_collector)\n    \"\"\"\n    filesystem, where = _resolve_filesystem_and_path(where, filesystem)\n\n    if hasattr(where, \"seek\"):  # file-like\n        cursor_position = where.tell()\n\n    writer = ParquetWriter(where, schema, filesystem, **kwargs)\n    writer.close()\n\n    if metadata_collector is not None:\n        # ParquetWriter doesn't expose the metadata until it's written. Write\n        # it and read it again.\n        metadata = read_metadata(where, filesystem=filesystem)\n        if hasattr(where, \"seek\"):\n            where.seek(cursor_position)  # file-like, set cursor back.\n\n        for m in metadata_collector:\n            metadata.append_row_groups(m)\n        if filesystem is not None:\n            with filesystem.open_output_stream(where) as f:\n                metadata.write_metadata_file(f)\n        else:\n            metadata.write_metadata_file(where)\n\n\ndef read_metadata(where, memory_map=False, decryption_properties=None,\n                  filesystem=None):\n    \"\"\"\n    Read FileMetaData from footer of a single Parquet file.\n\n    Parameters\n    ----------\n    where : str (file path) or file-like object\n    memory_map : bool, default False\n        Create memory map when the source is a file path.\n    decryption_properties : FileDecryptionProperties, default None\n        Decryption properties for reading encrypted Parquet files.\n    filesystem : FileSystem, default None\n        If nothing passed, will be inferred based on path.\n        Path will try to be found in the local on-disk filesystem otherwise\n        it will be parsed as an URI to determine the filesystem.\n\n    Returns\n    -------\n    metadata : FileMetaData\n        The metadata of the Parquet file\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> import pyarrow.parquet as pq\n    >>> table = pa.table({'n_legs': [4, 5, 100],\n    ...                   'animal': [\"Dog\", \"Brittle stars\", \"Centipede\"]})\n    >>> pq.write_table(table, 'example.parquet')\n\n    >>> pq.read_metadata('example.parquet')\n    <pyarrow._parquet.FileMetaData object at ...>\n      created_by: parquet-cpp-arrow version ...\n      num_columns: 2\n      num_rows: 3\n      num_row_groups: 1\n      format_version: 2.6\n      serialized_size: ...\n    \"\"\"\n    filesystem, where = _resolve_filesystem_and_path(where, filesystem)\n    file_ctx = nullcontext()\n    if filesystem is not None:\n        file_ctx = where = filesystem.open_input_file(where)\n\n    with file_ctx:\n        file = ParquetFile(where, memory_map=memory_map,\n                           decryption_properties=decryption_properties)\n        return file.metadata\n\n\ndef read_schema(where, memory_map=False, decryption_properties=None,\n                filesystem=None):\n    \"\"\"\n    Read effective Arrow schema from Parquet file metadata.\n\n    Parameters\n    ----------\n    where : str (file path) or file-like object\n    memory_map : bool, default False\n        Create memory map when the source is a file path.\n    decryption_properties : FileDecryptionProperties, default None\n        Decryption properties for reading encrypted Parquet files.\n    filesystem : FileSystem, default None\n        If nothing passed, will be inferred based on path.\n        Path will try to be found in the local on-disk filesystem otherwise\n        it will be parsed as an URI to determine the filesystem.\n\n    Returns\n    -------\n    schema : pyarrow.Schema\n        The schema of the Parquet file\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> import pyarrow.parquet as pq\n    >>> table = pa.table({'n_legs': [4, 5, 100],\n    ...                   'animal': [\"Dog\", \"Brittle stars\", \"Centipede\"]})\n    >>> pq.write_table(table, 'example.parquet')\n\n    >>> pq.read_schema('example.parquet')\n    n_legs: int64\n    animal: string\n    \"\"\"\n    filesystem, where = _resolve_filesystem_and_path(where, filesystem)\n    file_ctx = nullcontext()\n    if filesystem is not None:\n        file_ctx = where = filesystem.open_input_file(where)\n\n    with file_ctx:\n        file = ParquetFile(\n            where, memory_map=memory_map,\n            decryption_properties=decryption_properties)\n        return file.schema.to_arrow_schema()\n\n\n__all__ = (\n    \"ColumnChunkMetaData\",\n    \"ColumnSchema\",\n    \"FileDecryptionProperties\",\n    \"FileEncryptionProperties\",\n    \"FileMetaData\",\n    \"ParquetDataset\",\n    \"ParquetFile\",\n    \"ParquetLogicalType\",\n    \"ParquetReader\",\n    \"ParquetSchema\",\n    \"ParquetWriter\",\n    \"RowGroupMetaData\",\n    \"SortingColumn\",\n    \"Statistics\",\n    \"read_metadata\",\n    \"read_pandas\",\n    \"read_schema\",\n    \"read_table\",\n    \"write_metadata\",\n    \"write_table\",\n    \"write_to_dataset\",\n    \"_filters_to_expression\",\n    \"filters_to_expression\",\n)\n", "python/pyarrow/parquet/encryption.py": "# pylint: disable=unused-wildcard-import, unused-import\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom pyarrow._parquet_encryption import (CryptoFactory,   # noqa\n                                         EncryptionConfiguration,\n                                         DecryptionConfiguration,\n                                         KmsConnectionConfig,\n                                         KmsClient)\n", "python/pyarrow/parquet/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# flake8: noqa\n\nfrom .core import *\n", "python/pyarrow/vendored/version.py": "# Vendored from https://github.com/pypa/packaging,\n# changeset b5878c977206f60302536db969a8cef420853ade\n\n# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of the\n# `packaging` repository for complete details.\n\nimport collections\nimport itertools\nimport re\nimport warnings\n\n__all__ = [\"parse\", \"Version\", \"LegacyVersion\",\n           \"InvalidVersion\", \"VERSION_PATTERN\"]\n\n\nclass InfinityType:\n    def __repr__(self):\n        return \"Infinity\"\n\n    def __hash__(self):\n        return hash(repr(self))\n\n    def __lt__(self, other):\n        return False\n\n    def __le__(self, other):\n        return False\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__)\n\n    def __ne__(self, other):\n        return not isinstance(other, self.__class__)\n\n    def __gt__(self, other):\n        return True\n\n    def __ge__(self, other):\n        return True\n\n    def __neg__(self):\n        return NegativeInfinity\n\n\nInfinity = InfinityType()\n\n\nclass NegativeInfinityType:\n    def __repr__(self):\n        return \"-Infinity\"\n\n    def __hash__(self):\n        return hash(repr(self))\n\n    def __lt__(self, other):\n        return True\n\n    def __le__(self, other):\n        return True\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__)\n\n    def __ne__(self, other):\n        return not isinstance(other, self.__class__)\n\n    def __gt__(self, other):\n        return False\n\n    def __ge__(self, other):\n        return False\n\n    def __neg__(self):\n        return Infinity\n\n\nNegativeInfinity = NegativeInfinityType()\n\n\n_Version = collections.namedtuple(\n    \"_Version\", [\"epoch\", \"release\", \"dev\", \"pre\", \"post\", \"local\"]\n)\n\n\ndef parse(version):\n    \"\"\"\n    Parse the given version string and return either a :class:`Version` object\n    or a :class:`LegacyVersion` object depending on if the given version is\n    a valid PEP 440 version or a legacy version.\n    \"\"\"\n    try:\n        return Version(version)\n    except InvalidVersion:\n        return LegacyVersion(version)\n\n\nclass InvalidVersion(ValueError):\n    \"\"\"\n    An invalid version was found, users should refer to PEP 440.\n    \"\"\"\n\n\nclass _BaseVersion:\n\n    def __hash__(self):\n        return hash(self._key)\n\n    # Please keep the duplicated `isinstance` check\n    # in the six comparisons hereunder\n    # unless you find a way to avoid adding overhead function calls.\n    def __lt__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key < other._key\n\n    def __le__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key <= other._key\n\n    def __eq__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key == other._key\n\n    def __ge__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key >= other._key\n\n    def __gt__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key > other._key\n\n    def __ne__(self, other):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key != other._key\n\n\nclass LegacyVersion(_BaseVersion):\n    def __init__(self, version):\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n\n        warnings.warn(\n            \"Creating a LegacyVersion has been deprecated and will be \"\n            \"removed in the next major release\",\n            DeprecationWarning,\n        )\n\n    def __str__(self):\n        return self._version\n\n    def __repr__(self):\n        return f\"<LegacyVersion('{self}')>\"\n\n    @property\n    def public(self):\n        return self._version\n\n    @property\n    def base_version(self):\n        return self._version\n\n    @property\n    def epoch(self):\n        return -1\n\n    @property\n    def release(self):\n        return None\n\n    @property\n    def pre(self):\n        return None\n\n    @property\n    def post(self):\n        return None\n\n    @property\n    def dev(self):\n        return None\n\n    @property\n    def local(self):\n        return None\n\n    @property\n    def is_prerelease(self):\n        return False\n\n    @property\n    def is_postrelease(self):\n        return False\n\n    @property\n    def is_devrelease(self):\n        return False\n\n\n_legacy_version_component_re = re.compile(\n    r\"(\\d+ | [a-z]+ | \\.| -)\", re.VERBOSE)\n\n_legacy_version_replacement_map = {\n    \"pre\": \"c\",\n    \"preview\": \"c\",\n    \"-\": \"final-\",\n    \"rc\": \"c\",\n    \"dev\": \"@\",\n}\n\n\ndef _parse_version_parts(s):\n    for part in _legacy_version_component_re.split(s):\n        part = _legacy_version_replacement_map.get(part, part)\n\n        if not part or part == \".\":\n            continue\n\n        if part[:1] in \"0123456789\":\n            # pad for numeric comparison\n            yield part.zfill(8)\n        else:\n            yield \"*\" + part\n\n    # ensure that alpha/beta/candidate are before final\n    yield \"*final\"\n\n\ndef _legacy_cmpkey(version):\n\n    # We hardcode an epoch of -1 here. A PEP 440 version can only have a epoch\n    # greater than or equal to 0. This will effectively put the LegacyVersion,\n    # which uses the defacto standard originally implemented by setuptools,\n    # as before all PEP 440 versions.\n    epoch = -1\n\n    # This scheme is taken from pkg_resources.parse_version setuptools prior to\n    # it's adoption of the packaging library.\n    parts = []\n    for part in _parse_version_parts(version.lower()):\n        if part.startswith(\"*\"):\n            # remove \"-\" before a prerelease tag\n            if part < \"*final\":\n                while parts and parts[-1] == \"*final-\":\n                    parts.pop()\n\n            # remove trailing zeros from each series of numeric parts\n            while parts and parts[-1] == \"00000000\":\n                parts.pop()\n\n        parts.append(part)\n\n    return epoch, tuple(parts)\n\n\n# Deliberately not anchored to the start and end of the string, to make it\n# easier for 3rd party code to reuse\nVERSION_PATTERN = r\"\"\"\n    v?\n    (?:\n        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n        (?P<pre>                                          # pre-release\n            [-_\\.]?\n            (?P<pre_l>(a|b|c|rc|alpha|beta|pre|preview))\n            [-_\\.]?\n            (?P<pre_n>[0-9]+)?\n        )?\n        (?P<post>                                         # post release\n            (?:-(?P<post_n1>[0-9]+))\n            |\n            (?:\n                [-_\\.]?\n                (?P<post_l>post|rev|r)\n                [-_\\.]?\n                (?P<post_n2>[0-9]+)?\n            )\n        )?\n        (?P<dev>                                          # dev release\n            [-_\\.]?\n            (?P<dev_l>dev)\n            [-_\\.]?\n            (?P<dev_n>[0-9]+)?\n        )?\n    )\n    (?:\\+(?P<local>[a-z0-9]+(?:[-_\\.][a-z0-9]+)*))?       # local version\n\"\"\"\n\n\nclass Version(_BaseVersion):\n\n    _regex = re.compile(r\"^\\s*\" + VERSION_PATTERN +\n                        r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n\n    def __init__(self, version):\n\n        # Validate the version and parse it into pieces\n        match = self._regex.search(version)\n        if not match:\n            raise InvalidVersion(f\"Invalid version: '{version}'\")\n\n        # Store the parsed out pieces of the version\n        self._version = _Version(\n            epoch=int(match.group(\"epoch\")) if match.group(\"epoch\") else 0,\n            release=tuple(int(i) for i in match.group(\"release\").split(\".\")),\n            pre=_parse_letter_version(\n                match.group(\"pre_l\"), match.group(\"pre_n\")),\n            post=_parse_letter_version(\n                match.group(\"post_l\"), match.group(\n                    \"post_n1\") or match.group(\"post_n2\")\n            ),\n            dev=_parse_letter_version(\n                match.group(\"dev_l\"), match.group(\"dev_n\")),\n            local=_parse_local_version(match.group(\"local\")),\n        )\n\n        # Generate a key which will be used for sorting\n        self._key = _cmpkey(\n            self._version.epoch,\n            self._version.release,\n            self._version.pre,\n            self._version.post,\n            self._version.dev,\n            self._version.local,\n        )\n\n    def __repr__(self):\n        return f\"<Version('{self}')>\"\n\n    def __str__(self):\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(f\"{self.epoch}!\")\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        # Pre-release\n        if self.pre is not None:\n            parts.append(\"\".join(str(x) for x in self.pre))\n\n        # Post-release\n        if self.post is not None:\n            parts.append(f\".post{self.post}\")\n\n        # Development release\n        if self.dev is not None:\n            parts.append(f\".dev{self.dev}\")\n\n        # Local version segment\n        if self.local is not None:\n            parts.append(f\"+{self.local}\")\n\n        return \"\".join(parts)\n\n    @property\n    def epoch(self):\n        _epoch = self._version.epoch\n        return _epoch\n\n    @property\n    def release(self):\n        _release = self._version.release\n        return _release\n\n    @property\n    def pre(self):\n        _pre = self._version.pre\n        return _pre\n\n    @property\n    def post(self):\n        return self._version.post[1] if self._version.post else None\n\n    @property\n    def dev(self):\n        return self._version.dev[1] if self._version.dev else None\n\n    @property\n    def local(self):\n        if self._version.local:\n            return \".\".join(str(x) for x in self._version.local)\n        else:\n            return None\n\n    @property\n    def public(self):\n        return str(self).split(\"+\", 1)[0]\n\n    @property\n    def base_version(self):\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(f\"{self.epoch}!\")\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        return \"\".join(parts)\n\n    @property\n    def is_prerelease(self):\n        return self.dev is not None or self.pre is not None\n\n    @property\n    def is_postrelease(self):\n        return self.post is not None\n\n    @property\n    def is_devrelease(self):\n        return self.dev is not None\n\n    @property\n    def major(self):\n        return self.release[0] if len(self.release) >= 1 else 0\n\n    @property\n    def minor(self):\n        return self.release[1] if len(self.release) >= 2 else 0\n\n    @property\n    def micro(self):\n        return self.release[2] if len(self.release) >= 3 else 0\n\n\ndef _parse_letter_version(letter, number):\n\n    if letter:\n        # We consider there to be an implicit 0 in a pre-release if there is\n        # not a numeral associated with it.\n        if number is None:\n            number = 0\n\n        # We normalize any letters to their lower case form\n        letter = letter.lower()\n\n        # We consider some words to be alternate spellings of other words and\n        # in those cases we want to normalize the spellings to our preferred\n        # spelling.\n        if letter == \"alpha\":\n            letter = \"a\"\n        elif letter == \"beta\":\n            letter = \"b\"\n        elif letter in [\"c\", \"pre\", \"preview\"]:\n            letter = \"rc\"\n        elif letter in [\"rev\", \"r\"]:\n            letter = \"post\"\n\n        return letter, int(number)\n    if not letter and number:\n        # We assume if we are given a number, but we are not given a letter\n        # then this is using the implicit post release syntax (e.g. 1.0-1)\n        letter = \"post\"\n\n        return letter, int(number)\n\n    return None\n\n\n_local_version_separators = re.compile(r\"[\\._-]\")\n\n\ndef _parse_local_version(local):\n    \"\"\"\n    Takes a string like abc.1.twelve and turns it into (\"abc\", 1, \"twelve\").\n    \"\"\"\n    if local is not None:\n        return tuple(\n            part.lower() if not part.isdigit() else int(part)\n            for part in _local_version_separators.split(local)\n        )\n    return None\n\n\ndef _cmpkey(epoch, release, pre, post, dev, local):\n\n    # When we compare a release version, we want to compare it with all of the\n    # trailing zeros removed. So we'll use a reverse the list, drop all the now\n    # leading zeros until we come to something non zero, then take the rest\n    # re-reverse it back into the correct order and make it a tuple and use\n    # that for our sorting key.\n    _release = tuple(\n        reversed(list(itertools.dropwhile(lambda x: x == 0,\n                                          reversed(release))))\n    )\n\n    # We need to \"trick\" the sorting algorithm to put 1.0.dev0 before 1.0a0.\n    # We'll do this by abusing the pre segment, but we _only_ want to do this\n    # if there is not a pre or a post segment. If we have one of those then\n    # the normal sorting rules will handle this case correctly.\n    if pre is None and post is None and dev is not None:\n        _pre = NegativeInfinity\n    # Versions without a pre-release (except as noted above) should sort after\n    # those with one.\n    elif pre is None:\n        _pre = Infinity\n    else:\n        _pre = pre\n\n    # Versions without a post segment should sort before those with one.\n    if post is None:\n        _post = NegativeInfinity\n\n    else:\n        _post = post\n\n    # Versions without a development segment should sort after those with one.\n    if dev is None:\n        _dev = Infinity\n\n    else:\n        _dev = dev\n\n    if local is None:\n        # Versions without a local segment should sort before those with one.\n        _local = NegativeInfinity\n    else:\n        # Versions with a local segment need that segment parsed to implement\n        # the sorting rules in PEP440.\n        # - Alpha numeric segments sort before numeric segments\n        # - Alpha numeric segments sort lexicographically\n        # - Numeric segments sort numerically\n        # - Shorter versions sort before longer versions when the prefixes\n        #   match exactly\n        _local = tuple(\n            (i, \"\") if isinstance(i, int) else (NegativeInfinity, i)\n            for i in local\n        )\n\n    return epoch, _release, _pre, _post, _dev, _local\n", "python/pyarrow/vendored/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "python/benchmarks/common.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport codecs\nimport decimal\nfrom functools import partial\nimport itertools\nimport sys\nimport unicodedata\n\nimport numpy as np\n\nimport pyarrow as pa\n\nKILOBYTE = 1 << 10\nMEGABYTE = KILOBYTE * KILOBYTE\n\nDEFAULT_NONE_PROB = 0.3\n\n\ndef _multiplicate_sequence(base, target_size):\n    q, r = divmod(target_size, len(base))\n    return [base] * q + [base[:r]]\n\n\ndef get_random_bytes(n, seed=42):\n    \"\"\"\n    Generate a random bytes object of size *n*.\n    Note the result might be compressible.\n    \"\"\"\n    rnd = np.random.RandomState(seed)\n    # Computing a huge random bytestring can be costly, so we get at most\n    # 100KB and duplicate the result as needed\n    base_size = 100003\n    q, r = divmod(n, base_size)\n    if q == 0:\n        result = rnd.bytes(r)\n    else:\n        base = rnd.bytes(base_size)\n        result = b''.join(_multiplicate_sequence(base, n))\n    assert len(result) == n\n    return result\n\n\ndef get_random_ascii(n, seed=42):\n    \"\"\"\n    Get a random ASCII-only unicode string of size *n*.\n    \"\"\"\n    arr = np.frombuffer(get_random_bytes(n, seed=seed), dtype=np.int8) & 0x7f\n    result, _ = codecs.ascii_decode(arr)\n    assert isinstance(result, str)\n    assert len(result) == n\n    return result\n\n\ndef _random_unicode_letters(n, seed=42):\n    \"\"\"\n    Generate a string of random unicode letters (slow).\n    \"\"\"\n    def _get_more_candidates():\n        return rnd.randint(0, sys.maxunicode, size=n).tolist()\n\n    rnd = np.random.RandomState(seed)\n    out = []\n    candidates = []\n\n    while len(out) < n:\n        if not candidates:\n            candidates = _get_more_candidates()\n        ch = chr(candidates.pop())\n        # XXX Do we actually care that the code points are valid?\n        if unicodedata.category(ch)[0] == 'L':\n            out.append(ch)\n    return out\n\n\n_1024_random_unicode_letters = _random_unicode_letters(1024)\n\n\ndef get_random_unicode(n, seed=42):\n    \"\"\"\n    Get a random non-ASCII unicode string of size *n*.\n    \"\"\"\n    indices = np.frombuffer(get_random_bytes(n * 2, seed=seed),\n                            dtype=np.int16) & 1023\n    unicode_arr = np.array(_1024_random_unicode_letters)[indices]\n\n    result = ''.join(unicode_arr.tolist())\n    assert len(result) == n, (len(result), len(unicode_arr))\n    return result\n\n\nclass BuiltinsGenerator(object):\n\n    def __init__(self, seed=42):\n        self.rnd = np.random.RandomState(seed)\n\n    def sprinkle(self, lst, prob, value):\n        \"\"\"\n        Sprinkle *value* entries in list *lst* with likelihood *prob*.\n        \"\"\"\n        for i, p in enumerate(self.rnd.random_sample(size=len(lst))):\n            if p < prob:\n                lst[i] = value\n\n    def sprinkle_nones(self, lst, prob):\n        \"\"\"\n        Sprinkle None entries in list *lst* with likelihood *prob*.\n        \"\"\"\n        self.sprinkle(lst, prob, None)\n\n    def generate_int_list(self, n, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of Python ints with *none_prob* probability of\n        an entry being None.\n        \"\"\"\n        data = list(range(n))\n        self.sprinkle_nones(data, none_prob)\n        return data\n\n    def generate_float_list(self, n, none_prob=DEFAULT_NONE_PROB,\n                            use_nan=False):\n        \"\"\"\n        Generate a list of Python floats with *none_prob* probability of\n        an entry being None (or NaN if *use_nan* is true).\n        \"\"\"\n        # Make sure we get Python floats, not np.float64\n        data = list(map(float, self.rnd.uniform(0.0, 1.0, n)))\n        assert len(data) == n\n        self.sprinkle(data, none_prob, value=float('nan') if use_nan else None)\n        return data\n\n    def generate_bool_list(self, n, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of Python bools with *none_prob* probability of\n        an entry being None.\n        \"\"\"\n        # Make sure we get Python bools, not np.bool_\n        data = [bool(x >= 0.5) for x in self.rnd.uniform(0.0, 1.0, n)]\n        assert len(data) == n\n        self.sprinkle_nones(data, none_prob)\n        return data\n\n    def generate_decimal_list(self, n, none_prob=DEFAULT_NONE_PROB,\n                              use_nan=False):\n        \"\"\"\n        Generate a list of Python Decimals with *none_prob* probability of\n        an entry being None (or NaN if *use_nan* is true).\n        \"\"\"\n        data = [decimal.Decimal('%.9f' % f)\n                for f in self.rnd.uniform(0.0, 1.0, n)]\n        assert len(data) == n\n        self.sprinkle(data, none_prob,\n                      value=decimal.Decimal('nan') if use_nan else None)\n        return data\n\n    def generate_object_list(self, n, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of generic Python objects with *none_prob*\n        probability of an entry being None.\n        \"\"\"\n        data = [object() for i in range(n)]\n        self.sprinkle_nones(data, none_prob)\n        return data\n\n    def _generate_varying_sequences(self, random_factory, n, min_size,\n                                    max_size, none_prob):\n        \"\"\"\n        Generate a list of *n* sequences of varying size between *min_size*\n        and *max_size*, with *none_prob* probability of an entry being None.\n        The base material for each sequence is obtained by calling\n        `random_factory(<some size>)`\n        \"\"\"\n        base_size = 10000\n        base = random_factory(base_size + max_size)\n        data = []\n        for i in range(n):\n            off = self.rnd.randint(base_size)\n            if min_size == max_size:\n                size = min_size\n            else:\n                size = self.rnd.randint(min_size, max_size + 1)\n            data.append(base[off:off + size])\n        self.sprinkle_nones(data, none_prob)\n        assert len(data) == n\n        return data\n\n    def generate_fixed_binary_list(self, n, size, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of bytestrings with a fixed *size*.\n        \"\"\"\n        return self._generate_varying_sequences(get_random_bytes, n,\n                                                size, size, none_prob)\n\n    def generate_varying_binary_list(self, n, min_size, max_size,\n                                     none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of bytestrings with a random size between\n        *min_size* and *max_size*.\n        \"\"\"\n        return self._generate_varying_sequences(get_random_bytes, n,\n                                                min_size, max_size, none_prob)\n\n    def generate_ascii_string_list(self, n, min_size, max_size,\n                                   none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of ASCII strings with a random size between\n        *min_size* and *max_size*.\n        \"\"\"\n        return self._generate_varying_sequences(get_random_ascii, n,\n                                                min_size, max_size, none_prob)\n\n    def generate_unicode_string_list(self, n, min_size, max_size,\n                                     none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of unicode strings with a random size between\n        *min_size* and *max_size*.\n        \"\"\"\n        return self._generate_varying_sequences(get_random_unicode, n,\n                                                min_size, max_size, none_prob)\n\n    def generate_int_list_list(self, n, min_size, max_size,\n                               none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of lists of Python ints with a random size between\n        *min_size* and *max_size*.\n        \"\"\"\n        return self._generate_varying_sequences(\n            partial(self.generate_int_list, none_prob=none_prob),\n            n, min_size, max_size, none_prob)\n\n    def generate_tuple_list(self, n, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of tuples with random values.\n        Each tuple has the form `(int value, float value, bool value)`\n        \"\"\"\n        dicts = self.generate_dict_list(n, none_prob=none_prob)\n        tuples = [(d.get('u'), d.get('v'), d.get('w'))\n                  if d is not None else None\n                  for d in dicts]\n        assert len(tuples) == n\n        return tuples\n\n    def generate_dict_list(self, n, none_prob=DEFAULT_NONE_PROB):\n        \"\"\"\n        Generate a list of dicts with random values.\n        Each dict has the form\n\n            `{'u': int value, 'v': float value, 'w': bool value}`\n        \"\"\"\n        ints = self.generate_int_list(n, none_prob=none_prob)\n        floats = self.generate_float_list(n, none_prob=none_prob)\n        bools = self.generate_bool_list(n, none_prob=none_prob)\n        dicts = []\n        # Keep half the Nones, omit the other half\n        keep_nones = itertools.cycle([True, False])\n        for u, v, w in zip(ints, floats, bools):\n            d = {}\n            if u is not None or next(keep_nones):\n                d['u'] = u\n            if v is not None or next(keep_nones):\n                d['v'] = v\n            if w is not None or next(keep_nones):\n                d['w'] = w\n            dicts.append(d)\n        self.sprinkle_nones(dicts, none_prob)\n        assert len(dicts) == n\n        return dicts\n\n    def get_type_and_builtins(self, n, type_name):\n        \"\"\"\n        Return a `(arrow type, list)` tuple where the arrow type\n        corresponds to the given logical *type_name*, and the list\n        is a list of *n* random-generated Python objects compatible\n        with the arrow type.\n        \"\"\"\n        size = None\n\n        if type_name in ('bool', 'decimal', 'ascii', 'unicode', 'int64 list'):\n            kind = type_name\n        elif type_name.startswith(('int', 'uint')):\n            kind = 'int'\n        elif type_name.startswith('float'):\n            kind = 'float'\n        elif type_name.startswith('struct'):\n            kind = 'struct'\n        elif type_name == 'binary':\n            kind = 'varying binary'\n        elif type_name.startswith('binary'):\n            kind = 'fixed binary'\n            size = int(type_name[6:])\n            assert size > 0\n        else:\n            raise ValueError(\"unrecognized type %r\" % (type_name,))\n\n        if kind in ('int', 'float'):\n            ty = getattr(pa, type_name)()\n        elif kind == 'bool':\n            ty = pa.bool_()\n        elif kind == 'decimal':\n            ty = pa.decimal128(9, 9)\n        elif kind == 'fixed binary':\n            ty = pa.binary(size)\n        elif kind == 'varying binary':\n            ty = pa.binary()\n        elif kind in ('ascii', 'unicode'):\n            ty = pa.string()\n        elif kind == 'int64 list':\n            ty = pa.list_(pa.int64())\n        elif kind == 'struct':\n            ty = pa.struct([pa.field('u', pa.int64()),\n                            pa.field('v', pa.float64()),\n                            pa.field('w', pa.bool_())])\n\n        factories = {\n            'int': self.generate_int_list,\n            'float': self.generate_float_list,\n            'bool': self.generate_bool_list,\n            'decimal': self.generate_decimal_list,\n            'fixed binary': partial(self.generate_fixed_binary_list,\n                                    size=size),\n            'varying binary': partial(self.generate_varying_binary_list,\n                                      min_size=3, max_size=40),\n            'ascii': partial(self.generate_ascii_string_list,\n                             min_size=3, max_size=40),\n            'unicode': partial(self.generate_unicode_string_list,\n                               min_size=3, max_size=40),\n            'int64 list': partial(self.generate_int_list_list,\n                                  min_size=0, max_size=20),\n            'struct': self.generate_dict_list,\n            'struct from tuples': self.generate_tuple_list,\n        }\n        data = factories[kind](n)\n        return ty, data\n", "python/benchmarks/array_ops.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pyarrow as pa\n\n\nclass ScalarAccess(object):\n    n = 10 ** 5\n\n    def setUp(self):\n        self._array = pa.array(list(range(self.n)), type=pa.int64())\n        self._array_items = list(self._array)\n\n    def time_getitem(self):\n        for i in range(self.n):\n            self._array[i]\n\n    def time_as_py(self):\n        for item in self._array_items:\n            item.as_py()\n", "python/benchmarks/microbenchmarks.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pyarrow.benchmark as pb\n\nfrom . import common\n\n\nclass PandasObjectIsNull(object):\n    size = 10 ** 5\n    types = ('int', 'float', 'object', 'decimal')\n\n    param_names = ['type']\n    params = [types]\n\n    def setup(self, type_name):\n        gen = common.BuiltinsGenerator()\n        if type_name == 'int':\n            lst = gen.generate_int_list(self.size)\n        elif type_name == 'float':\n            lst = gen.generate_float_list(self.size, use_nan=True)\n        elif type_name == 'object':\n            lst = gen.generate_object_list(self.size)\n        elif type_name == 'decimal':\n            lst = gen.generate_decimal_list(self.size)\n        else:\n            assert 0\n        self.lst = lst\n\n    def time_PandasObjectIsNull(self, *args):\n        pb.benchmark_PandasObjectIsNull(self.lst)\n", "python/benchmarks/convert_pandas.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport numpy as np\nimport pandas as pd\n\nimport pyarrow as pa\nfrom pyarrow.tests.util import rands\n\n\nclass PandasConversionsBase(object):\n    def setup(self, n, dtype):\n        if dtype == 'float64_nans':\n            arr = np.arange(n).astype('float64')\n            arr[arr % 10 == 0] = np.nan\n        else:\n            arr = np.arange(n).astype(dtype)\n        self.data = pd.DataFrame({'column': arr})\n\n\nclass PandasConversionsToArrow(PandasConversionsBase):\n    param_names = ('size', 'dtype')\n    params = ((10, 10 ** 6), ('int64', 'float64', 'float64_nans', 'str'))\n\n    def time_from_series(self, n, dtype):\n        pa.Table.from_pandas(self.data)\n\n\nclass PandasConversionsFromArrow(PandasConversionsBase):\n    param_names = ('size', 'dtype')\n    params = ((10, 10 ** 6), ('int64', 'float64', 'float64_nans', 'str'))\n\n    def setup(self, n, dtype):\n        super(PandasConversionsFromArrow, self).setup(n, dtype)\n        self.arrow_data = pa.Table.from_pandas(self.data)\n\n    def time_to_series(self, n, dtype):\n        self.arrow_data.to_pandas()\n\n\nclass ToPandasStrings(object):\n\n    param_names = ('uniqueness', 'total')\n    params = ((0.001, 0.01, 0.1, 0.5), (1000000,))\n    string_length = 25\n\n    def setup(self, uniqueness, total):\n        nunique = int(total * uniqueness)\n        unique_values = [rands(self.string_length) for i in range(nunique)]\n        values = unique_values * (total // nunique)\n        self.arr = pa.array(values, type=pa.string())\n        self.table = pa.Table.from_arrays([self.arr], ['f0'])\n\n    def time_to_pandas_dedup(self, *args):\n        self.arr.to_pandas()\n\n    def time_to_pandas_no_dedup(self, *args):\n        self.arr.to_pandas(deduplicate_objects=False)\n\n\nclass SerializeDeserializePandas(object):\n\n    def setup(self):\n        # 10 million length\n        n = 10000000\n        self.df = pd.DataFrame({'data': np.random.randn(n)})\n        self.serialized = pa.serialize_pandas(self.df)\n\n    def time_serialize_pandas(self):\n        pa.serialize_pandas(self.df)\n\n    def time_deserialize_pandas(self):\n        pa.deserialize_pandas(self.serialized)\n\n\nclass TableFromPandasMicroperformance(object):\n    # ARROW-4629\n\n    def setup(self):\n        ser = pd.Series(range(10000))\n        df = pd.DataFrame({col: ser.copy(deep=True) for col in range(100)})\n        # Simulate a real dataset by converting some columns to strings\n        self.df = df.astype({col: str for col in range(50)})\n\n    def time_Table_from_pandas(self):\n        for _ in range(50):\n            pa.Table.from_pandas(self.df, nthreads=1)\n", "python/benchmarks/parquet.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport numpy as np\n\nimport pyarrow as pa\ntry:\n    import pyarrow.parquet as pq\nexcept ImportError:\n    pq = None\nfrom pyarrow.tests.util import rands\n\n\nclass ParquetWriteBinary(object):\n\n    def setup(self):\n        nuniques = 100000\n        value_size = 50\n        length = 1000000\n        num_cols = 10\n\n        unique_values = np.array([rands(value_size) for\n                                  i in range(nuniques)], dtype='O')\n        values = unique_values[np.random.randint(0, nuniques, size=length)]\n        self.table = pa.table([pa.array(values) for i in range(num_cols)],\n                              names=['f{}'.format(i) for i in range(num_cols)])\n        self.table_df = self.table.to_pandas()\n\n    def time_write_binary_table(self):\n        out = pa.BufferOutputStream()\n        pq.write_table(self.table, out)\n\n    def time_write_binary_table_uncompressed(self):\n        out = pa.BufferOutputStream()\n        pq.write_table(self.table, out, compression='none')\n\n    def time_write_binary_table_no_dictionary(self):\n        out = pa.BufferOutputStream()\n        pq.write_table(self.table, out, use_dictionary=False)\n\n    def time_convert_pandas_and_write_binary_table(self):\n        out = pa.BufferOutputStream()\n        pq.write_table(pa.table(self.table_df), out)\n\n\ndef generate_dict_strings(string_size, nunique, length, random_order=True):\n    uniques = np.array([rands(string_size) for i in range(nunique)], dtype='O')\n    if random_order:\n        indices = np.random.randint(0, nunique, size=length).astype('i4')\n    else:\n        indices = np.arange(nunique).astype('i4').repeat(length // nunique)\n    return pa.DictionaryArray.from_arrays(indices, uniques)\n\n\ndef generate_dict_table(num_cols, string_size, nunique, length,\n                        random_order=True):\n    data = generate_dict_strings(string_size, nunique, length,\n                                 random_order=random_order)\n    return pa.table([\n        data for i in range(num_cols)\n    ], names=['f{}'.format(i) for i in range(num_cols)])\n\n\nclass ParquetWriteDictionaries(object):\n\n    param_names = ('nunique',)\n    params = [(1000), (100000)]\n\n    def setup(self, nunique):\n        self.num_cols = 10\n        self.value_size = 32\n        self.nunique = nunique\n        self.length = 10000000\n\n        self.table = generate_dict_table(self.num_cols, self.value_size,\n                                         self.nunique, self.length)\n        self.table_sequential = generate_dict_table(self.num_cols,\n                                                    self.value_size,\n                                                    self.nunique, self.length,\n                                                    random_order=False)\n\n    def time_write_random_order(self, nunique):\n        pq.write_table(self.table, pa.BufferOutputStream())\n\n    def time_write_sequential(self, nunique):\n        pq.write_table(self.table_sequential, pa.BufferOutputStream())\n\n\nclass ParquetManyColumns(object):\n\n    total_cells = 10000000\n    param_names = ('num_cols',)\n    params = [100, 1000, 10000]\n\n    def setup(self, num_cols):\n        num_rows = self.total_cells // num_cols\n        self.table = pa.table({'c' + str(i): np.random.randn(num_rows)\n                               for i in range(num_cols)})\n\n        out = pa.BufferOutputStream()\n        pq.write_table(self.table, out)\n        self.buf = out.getvalue()\n\n    def time_write(self, num_cols):\n        out = pa.BufferOutputStream()\n        pq.write_table(self.table, out)\n\n    def time_read(self, num_cols):\n        pq.read_table(self.buf)\n", "python/benchmarks/convert_builtins.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport pyarrow as pa\n\nfrom . import common\n\n\n# TODO:\n# - test dates and times\n\n\nclass ConvertPyListToArray(object):\n    \"\"\"\n    Benchmark pa.array(list of values, type=...)\n    \"\"\"\n    size = 10 ** 5\n    types = ('int32', 'uint32', 'int64', 'uint64',\n             'float32', 'float64', 'bool', 'decimal',\n             'binary', 'binary10', 'ascii', 'unicode',\n             'int64 list', 'struct', 'struct from tuples')\n\n    param_names = ['type']\n    params = [types]\n\n    def setup(self, type_name):\n        gen = common.BuiltinsGenerator()\n        self.ty, self.data = gen.get_type_and_builtins(self.size, type_name)\n\n    def time_convert(self, *args):\n        pa.array(self.data, type=self.ty)\n\n\nclass InferPyListToArray(object):\n    \"\"\"\n    Benchmark pa.array(list of values) with type inference\n    \"\"\"\n    size = 10 ** 5\n    types = ('int64', 'float64', 'bool', 'decimal', 'binary', 'ascii',\n             'unicode', 'int64 list', 'struct')\n\n    param_names = ['type']\n    params = [types]\n\n    def setup(self, type_name):\n        gen = common.BuiltinsGenerator()\n        self.ty, self.data = gen.get_type_and_builtins(self.size, type_name)\n\n    def time_infer(self, *args):\n        arr = pa.array(self.data)\n        assert arr.type == self.ty\n\n\nclass ConvertArrayToPyList(object):\n    \"\"\"\n    Benchmark pa.array.to_pylist()\n    \"\"\"\n    size = 10 ** 5\n    types = ('int32', 'uint32', 'int64', 'uint64',\n             'float32', 'float64', 'bool', 'decimal',\n             'binary', 'binary10', 'ascii', 'unicode',\n             'int64 list', 'struct')\n\n    param_names = ['type']\n    params = [types]\n\n    def setup(self, type_name):\n        gen = common.BuiltinsGenerator()\n        self.ty, self.data = gen.get_type_and_builtins(self.size, type_name)\n        self.arr = pa.array(self.data, type=self.ty)\n\n    def time_convert(self, *args):\n        self.arr.to_pylist()\n", "python/benchmarks/io.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport time\nimport pyarrow as pa\n\n\nclass HighLatencyReader(object):\n\n    def __init__(self, raw, latency):\n        self.raw = raw\n        self.latency = latency\n\n    def close(self):\n        self.raw.close()\n\n    @property\n    def closed(self):\n        return self.raw.closed\n\n    def read(self, nbytes=None):\n        time.sleep(self.latency)\n        return self.raw.read(nbytes)\n\n\nclass HighLatencyWriter(object):\n\n    def __init__(self, raw, latency):\n        self.raw = raw\n        self.latency = latency\n\n    def close(self):\n        self.raw.close()\n\n    @property\n    def closed(self):\n        return self.raw.closed\n\n    def write(self, data):\n        time.sleep(self.latency)\n        self.raw.write(data)\n\n\nclass BufferedIOHighLatency(object):\n    \"\"\"Benchmark creating a parquet manifest.\"\"\"\n\n    increment = 1024\n    total_size = 16 * (1 << 20)  # 16 MB\n    buffer_size = 1 << 20  # 1 MB\n    latency = 0.1  # 100ms\n\n    param_names = ('latency',)\n    params = [0, 0.01, 0.1]\n\n    def time_buffered_writes(self, latency):\n        test_data = b'x' * self.increment\n        bytes_written = 0\n        out = pa.BufferOutputStream()\n        slow_out = HighLatencyWriter(out, latency)\n        buffered_out = pa.output_stream(slow_out, buffer_size=self.buffer_size)\n\n        while bytes_written < self.total_size:\n            buffered_out.write(test_data)\n            bytes_written += self.increment\n        buffered_out.flush()\n\n    def time_buffered_reads(self, latency):\n        bytes_read = 0\n        reader = pa.input_stream(pa.py_buffer(b'x' * self.total_size))\n        slow_reader = HighLatencyReader(reader, latency)\n        buffered_reader = pa.input_stream(slow_reader,\n                                          buffer_size=self.buffer_size)\n        while bytes_read < self.total_size:\n            buffered_reader.read(self.increment)\n            bytes_read += self.increment\n", "python/benchmarks/__init__.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n", "python/benchmarks/streaming.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n\nfrom . import common\nfrom .common import KILOBYTE, MEGABYTE\n\n\ndef generate_chunks(total_size, nchunks, ncols, dtype=np.dtype('int64')):\n    rowsize = total_size // nchunks // ncols\n    assert rowsize % dtype.itemsize == 0\n\n    def make_column(col, chunk):\n        return np.frombuffer(common.get_random_bytes(\n            rowsize, seed=col + 997 * chunk)).view(dtype)\n\n    return [pd.DataFrame({\n            'c' + str(col): make_column(col, chunk)\n            for col in range(ncols)})\n            for chunk in range(nchunks)]\n\n\nclass StreamReader(object):\n    \"\"\"\n    Benchmark in-memory streaming to a Pandas dataframe.\n    \"\"\"\n    total_size = 64 * MEGABYTE\n    ncols = 8\n    chunk_sizes = [16 * KILOBYTE, 256 * KILOBYTE, 8 * MEGABYTE]\n\n    param_names = ['chunk_size']\n    params = [chunk_sizes]\n\n    def setup(self, chunk_size):\n        # Note we're careful to stream different chunks instead of\n        # streaming N times the same chunk, so that we avoid operating\n        # entirely out of L1/L2.\n        chunks = generate_chunks(self.total_size,\n                                 nchunks=self.total_size // chunk_size,\n                                 ncols=self.ncols)\n        batches = [pa.RecordBatch.from_pandas(df)\n                   for df in chunks]\n        schema = batches[0].schema\n        sink = pa.BufferOutputStream()\n        stream_writer = pa.RecordBatchStreamWriter(sink, schema)\n        for batch in batches:\n            stream_writer.write_batch(batch)\n        self.source = sink.getvalue()\n\n    def time_read_to_dataframe(self, *args):\n        reader = pa.RecordBatchStreamReader(self.source)\n        table = reader.read_all()\n        df = table.to_pandas()  # noqa\n", "python/examples/flight/client.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"An example Flight CLI client.\"\"\"\n\nimport argparse\nimport sys\n\nimport pyarrow\nimport pyarrow.flight\nimport pyarrow.csv as csv\n\n\ndef list_flights(args, client, connection_args={}):\n    print('Flights\\n=======')\n    for flight in client.list_flights():\n        descriptor = flight.descriptor\n        if descriptor.descriptor_type == pyarrow.flight.DescriptorType.PATH:\n            print(\"Path:\", descriptor.path)\n        elif descriptor.descriptor_type == pyarrow.flight.DescriptorType.CMD:\n            print(\"Command:\", descriptor.command)\n        else:\n            print(\"Unknown descriptor type\")\n\n        print(\"Total records:\", end=\" \")\n        if flight.total_records >= 0:\n            print(flight.total_records)\n        else:\n            print(\"Unknown\")\n\n        print(\"Total bytes:\", end=\" \")\n        if flight.total_bytes >= 0:\n            print(flight.total_bytes)\n        else:\n            print(\"Unknown\")\n\n        print(\"Number of endpoints:\", len(flight.endpoints))\n        print(\"Schema:\")\n        print(flight.schema)\n        print('---')\n\n    print('\\nActions\\n=======')\n    for action in client.list_actions():\n        print(\"Type:\", action.type)\n        print(\"Description:\", action.description)\n        print('---')\n\n\ndef do_action(args, client, connection_args={}):\n    try:\n        buf = pyarrow.allocate_buffer(0)\n        action = pyarrow.flight.Action(args.action_type, buf)\n        print('Running action', args.action_type)\n        for result in client.do_action(action):\n            print(\"Got result\", result.body.to_pybytes())\n    except pyarrow.lib.ArrowIOError as e:\n        print(\"Error calling action:\", e)\n\n\ndef push_data(args, client, connection_args={}):\n    print('File Name:', args.file)\n    my_table = csv.read_csv(args.file)\n    print('Table rows=', str(len(my_table)))\n    df = my_table.to_pandas()\n    print(df.head())\n    writer, _ = client.do_put(\n        pyarrow.flight.FlightDescriptor.for_path(args.file), my_table.schema)\n    writer.write_table(my_table)\n    writer.close()\n\n\ndef get_flight(args, client, connection_args={}):\n    if args.path:\n        descriptor = pyarrow.flight.FlightDescriptor.for_path(*args.path)\n    else:\n        descriptor = pyarrow.flight.FlightDescriptor.for_command(args.command)\n\n    info = client.get_flight_info(descriptor)\n    for endpoint in info.endpoints:\n        print('Ticket:', endpoint.ticket)\n        for location in endpoint.locations:\n            print(location)\n            get_client = pyarrow.flight.FlightClient(location,\n                                                     **connection_args)\n            reader = get_client.do_get(endpoint.ticket)\n            df = reader.read_pandas()\n            print(df)\n\n\ndef _add_common_arguments(parser):\n    parser.add_argument('--tls', action='store_true',\n                        help='Enable transport-level security')\n    parser.add_argument('--tls-roots', default=None,\n                        help='Path to trusted TLS certificate(s)')\n    parser.add_argument(\"--mtls\", nargs=2, default=None,\n                        metavar=('CERTFILE', 'KEYFILE'),\n                        help=\"Enable transport-level security\")\n    parser.add_argument('host', type=str,\n                        help=\"Address or hostname to connect to\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    subcommands = parser.add_subparsers()\n\n    cmd_list = subcommands.add_parser('list')\n    cmd_list.set_defaults(action='list')\n    _add_common_arguments(cmd_list)\n    cmd_list.add_argument('-l', '--list', action='store_true',\n                          help=\"Print more details.\")\n\n    cmd_do = subcommands.add_parser('do')\n    cmd_do.set_defaults(action='do')\n    _add_common_arguments(cmd_do)\n    cmd_do.add_argument('action_type', type=str,\n                        help=\"The action type to run.\")\n\n    cmd_put = subcommands.add_parser('put')\n    cmd_put.set_defaults(action='put')\n    _add_common_arguments(cmd_put)\n    cmd_put.add_argument('file', type=str,\n                         help=\"CSV file to upload.\")\n\n    cmd_get = subcommands.add_parser('get')\n    cmd_get.set_defaults(action='get')\n    _add_common_arguments(cmd_get)\n    cmd_get_descriptor = cmd_get.add_mutually_exclusive_group(required=True)\n    cmd_get_descriptor.add_argument('-p', '--path', type=str, action='append',\n                                    help=\"The path for the descriptor.\")\n    cmd_get_descriptor.add_argument('-c', '--command', type=str,\n                                    help=\"The command for the descriptor.\")\n\n    args = parser.parse_args()\n    if not hasattr(args, 'action'):\n        parser.print_help()\n        sys.exit(1)\n\n    commands = {\n        'list': list_flights,\n        'do': do_action,\n        'get': get_flight,\n        'put': push_data,\n    }\n    host, port = args.host.split(':')\n    port = int(port)\n    scheme = \"grpc+tcp\"\n    connection_args = {}\n    if args.tls:\n        scheme = \"grpc+tls\"\n        if args.tls_roots:\n            with open(args.tls_roots, \"rb\") as root_certs:\n                connection_args[\"tls_root_certs\"] = root_certs.read()\n    if args.mtls:\n        with open(args.mtls[0], \"rb\") as cert_file:\n            tls_cert_chain = cert_file.read()\n        with open(args.mtls[1], \"rb\") as key_file:\n            tls_private_key = key_file.read()\n        connection_args[\"cert_chain\"] = tls_cert_chain\n        connection_args[\"private_key\"] = tls_private_key\n    client = pyarrow.flight.FlightClient(f\"{scheme}://{host}:{port}\",\n                                         **connection_args)\n    while True:\n        try:\n            action = pyarrow.flight.Action(\"healthcheck\", b\"\")\n            options = pyarrow.flight.FlightCallOptions(timeout=1)\n            list(client.do_action(action, options=options))\n            break\n        except pyarrow.ArrowIOError as e:\n            if \"Deadline\" in str(e):\n                print(\"Server is not ready, waiting...\")\n    commands[args.action](args, client, connection_args)\n\n\nif __name__ == '__main__':\n    main()\n", "python/examples/flight/middleware.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"Example of invisibly propagating a request ID with middleware.\"\"\"\n\nimport argparse\nimport sys\nimport threading\nimport uuid\n\nimport pyarrow as pa\nimport pyarrow.flight as flight\n\n\nclass TraceContext:\n    _locals = threading.local()\n    _locals.trace_id = None\n\n    @classmethod\n    def current_trace_id(cls):\n        if not getattr(cls._locals, \"trace_id\", None):\n            cls.set_trace_id(uuid.uuid4().hex)\n        return cls._locals.trace_id\n\n    @classmethod\n    def set_trace_id(cls, trace_id):\n        cls._locals.trace_id = trace_id\n\n\nTRACE_HEADER = \"x-tracing-id\"\n\n\nclass TracingServerMiddleware(flight.ServerMiddleware):\n    def __init__(self, trace_id):\n        self.trace_id = trace_id\n\n    def sending_headers(self):\n        return {\n            TRACE_HEADER: self.trace_id,\n        }\n\n\nclass TracingServerMiddlewareFactory(flight.ServerMiddlewareFactory):\n    def start_call(self, info, headers):\n        print(\"Starting new call:\", info)\n        if TRACE_HEADER in headers:\n            trace_id = headers[TRACE_HEADER][0]\n            print(\"Found trace header with value:\", trace_id)\n            TraceContext.set_trace_id(trace_id)\n        return TracingServerMiddleware(TraceContext.current_trace_id())\n\n\nclass TracingClientMiddleware(flight.ClientMiddleware):\n    def sending_headers(self):\n        print(\"Sending trace ID:\", TraceContext.current_trace_id())\n        return {\n            \"x-tracing-id\": TraceContext.current_trace_id(),\n        }\n\n    def received_headers(self, headers):\n        if TRACE_HEADER in headers:\n            trace_id = headers[TRACE_HEADER][0]\n            print(\"Found trace header with value:\", trace_id)\n            # Don't overwrite our trace ID\n\n\nclass TracingClientMiddlewareFactory(flight.ClientMiddlewareFactory):\n    def start_call(self, info):\n        print(\"Starting new call:\", info)\n        return TracingClientMiddleware()\n\n\nclass FlightServer(flight.FlightServerBase):\n    def __init__(self, delegate, **kwargs):\n        super().__init__(**kwargs)\n        if delegate:\n            self.delegate = flight.connect(\n                delegate,\n                middleware=(TracingClientMiddlewareFactory(),))\n        else:\n            self.delegate = None\n\n    def list_actions(self, context):\n        return [\n            (\"get-trace-id\", \"Get the trace context ID.\"),\n        ]\n\n    def do_action(self, context, action):\n        trace_middleware = context.get_middleware(\"trace\")\n        if trace_middleware:\n            TraceContext.set_trace_id(trace_middleware.trace_id)\n        if action.type == \"get-trace-id\":\n            if self.delegate:\n                for result in self.delegate.do_action(action):\n                    yield result\n            else:\n                trace_id = TraceContext.current_trace_id().encode(\"utf-8\")\n                print(\"Returning trace ID:\", trace_id)\n                buf = pa.py_buffer(trace_id)\n                yield pa.flight.Result(buf)\n        else:\n            raise KeyError(f\"Unknown action {action.type!r}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    subparsers = parser.add_subparsers(dest=\"command\")\n    client = subparsers.add_parser(\"client\", help=\"Run the client.\")\n    client.add_argument(\"server\")\n    client.add_argument(\"--request-id\", default=None)\n\n    server = subparsers.add_parser(\"server\", help=\"Run the server.\")\n    server.add_argument(\n        \"--listen\",\n        required=True,\n        help=\"The location to listen on (example: grpc://localhost:5050)\",\n    )\n    server.add_argument(\n        \"--delegate\",\n        required=False,\n        default=None,\n        help=(\"A location to delegate to. That is, this server will \"\n              \"simply call the given server for the response. Demonstrates \"\n              \"propagation of the trace ID between servers.\"),\n    )\n\n    args = parser.parse_args()\n    if not getattr(args, \"command\"):\n        parser.print_help()\n        return 1\n\n    if args.command == \"server\":\n        server = FlightServer(\n            args.delegate,\n            location=args.listen,\n            middleware={\"trace\": TracingServerMiddlewareFactory()})\n        server.serve()\n    elif args.command == \"client\":\n        client = flight.connect(\n            args.server,\n            middleware=(TracingClientMiddlewareFactory(),))\n        if args.request_id:\n            TraceContext.set_trace_id(args.request_id)\n        else:\n            TraceContext.set_trace_id(\"client-chosen-id\")\n\n        for result in client.do_action(flight.Action(\"get-trace-id\", b\"\")):\n            print(result.body.to_pybytes())\n\n\nif __name__ == \"__main__\":\n    sys.exit(main() or 0)\n", "python/examples/flight/server.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"An example Flight Python server.\"\"\"\n\nimport argparse\nimport ast\nimport threading\nimport time\n\nimport pyarrow\nimport pyarrow.flight\n\n\nclass FlightServer(pyarrow.flight.FlightServerBase):\n    def __init__(self, host=\"localhost\", location=None,\n                 tls_certificates=None, verify_client=False,\n                 root_certificates=None, auth_handler=None):\n        super(FlightServer, self).__init__(\n            location, auth_handler, tls_certificates, verify_client,\n            root_certificates)\n        self.flights = {}\n        self.host = host\n        self.tls_certificates = tls_certificates\n\n    @classmethod\n    def descriptor_to_key(self, descriptor):\n        return (descriptor.descriptor_type.value, descriptor.command,\n                tuple(descriptor.path or tuple()))\n\n    def _make_flight_info(self, key, descriptor, table):\n        if self.tls_certificates:\n            location = pyarrow.flight.Location.for_grpc_tls(\n                self.host, self.port)\n        else:\n            location = pyarrow.flight.Location.for_grpc_tcp(\n                self.host, self.port)\n        endpoints = [pyarrow.flight.FlightEndpoint(repr(key), [location]), ]\n\n        mock_sink = pyarrow.MockOutputStream()\n        stream_writer = pyarrow.RecordBatchStreamWriter(\n            mock_sink, table.schema)\n        stream_writer.write_table(table)\n        stream_writer.close()\n        data_size = mock_sink.size()\n\n        return pyarrow.flight.FlightInfo(table.schema,\n                                         descriptor, endpoints,\n                                         table.num_rows, data_size)\n\n    def list_flights(self, context, criteria):\n        for key, table in self.flights.items():\n            if key[1] is not None:\n                descriptor = \\\n                    pyarrow.flight.FlightDescriptor.for_command(key[1])\n            else:\n                descriptor = pyarrow.flight.FlightDescriptor.for_path(*key[2])\n\n            yield self._make_flight_info(key, descriptor, table)\n\n    def get_flight_info(self, context, descriptor):\n        key = FlightServer.descriptor_to_key(descriptor)\n        if key in self.flights:\n            table = self.flights[key]\n            return self._make_flight_info(key, descriptor, table)\n        raise KeyError('Flight not found.')\n\n    def do_put(self, context, descriptor, reader, writer):\n        key = FlightServer.descriptor_to_key(descriptor)\n        print(key)\n        self.flights[key] = reader.read_all()\n        print(self.flights[key])\n\n    def do_get(self, context, ticket):\n        key = ast.literal_eval(ticket.ticket.decode())\n        if key not in self.flights:\n            return None\n        return pyarrow.flight.RecordBatchStream(self.flights[key])\n\n    def list_actions(self, context):\n        return [\n            (\"clear\", \"Clear the stored flights.\"),\n            (\"shutdown\", \"Shut down this server.\"),\n        ]\n\n    def do_action(self, context, action):\n        if action.type == \"clear\":\n            raise NotImplementedError(\n                \"{} is not implemented.\".format(action.type))\n        elif action.type == \"healthcheck\":\n            pass\n        elif action.type == \"shutdown\":\n            yield pyarrow.flight.Result(pyarrow.py_buffer(b'Shutdown!'))\n            # Shut down on background thread to avoid blocking current\n            # request\n            threading.Thread(target=self._shutdown).start()\n        else:\n            raise KeyError(\"Unknown action {!r}\".format(action.type))\n\n    def _shutdown(self):\n        \"\"\"Shut down after a delay.\"\"\"\n        print(\"Server is shutting down...\")\n        time.sleep(2)\n        self.shutdown()\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\",\n                        help=\"Address or hostname to listen on\")\n    parser.add_argument(\"--port\", type=int, default=5005,\n                        help=\"Port number to listen on\")\n    parser.add_argument(\"--tls\", nargs=2, default=None,\n                        metavar=('CERTFILE', 'KEYFILE'),\n                        help=\"Enable transport-level security\")\n    parser.add_argument(\"--verify_client\", type=bool, default=False,\n                        help=\"enable mutual TLS and verify the client if True\")\n\n    args = parser.parse_args()\n    tls_certificates = []\n    scheme = \"grpc+tcp\"\n    if args.tls:\n        scheme = \"grpc+tls\"\n        with open(args.tls[0], \"rb\") as cert_file:\n            tls_cert_chain = cert_file.read()\n        with open(args.tls[1], \"rb\") as key_file:\n            tls_private_key = key_file.read()\n        tls_certificates.append((tls_cert_chain, tls_private_key))\n\n    location = \"{}://{}:{}\".format(scheme, args.host, args.port)\n\n    server = FlightServer(args.host, location,\n                          tls_certificates=tls_certificates,\n                          verify_client=args.verify_client)\n    print(\"Serving on\", location)\n    server.serve()\n\n\nif __name__ == '__main__':\n    main()\n", "python/examples/parquet_encryption/sample_vault_kms_client.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"A sample KmsClient implementation.\"\"\"\nimport argparse\nimport base64\nimport os\n\nimport requests\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport pyarrow.parquet.encryption as pe\n\n\nclass VaultClient(pe.KmsClient):\n    \"\"\"An example of a KmsClient implementation with master keys\n    managed by Hashicorp Vault KMS.\n    See Vault documentation: https://www.vaultproject.io/api/secret/transit\n    Not for production use!\n    \"\"\"\n    JSON_MEDIA_TYPE = \"application/json; charset=utf-8\"\n    DEFAULT_TRANSIT_ENGINE = \"/v1/transit/\"\n    WRAP_ENDPOINT = \"encrypt/\"\n    UNWRAP_ENDPOINT = \"decrypt/\"\n    TOKEN_HEADER = \"X-Vault-Token\"\n\n    def __init__(self, kms_connection_config):\n        \"\"\"Create a VaultClient instance.\n\n        Parameters\n        ----------\n        kms_connection_config : KmsConnectionConfig\n           configuration parameters to connect to vault,\n           e.g. URL and access token\n        \"\"\"\n        pe.KmsClient.__init__(self)\n        self.kms_url = kms_connection_config.kms_instance_url + \\\n            VaultClient.DEFAULT_TRANSIT_ENGINE\n        self.kms_connection_config = kms_connection_config\n\n    def wrap_key(self, key_bytes, master_key_identifier):\n        \"\"\"Call Vault to wrap key key_bytes with key\n        identified by master_key_identifier.\"\"\"\n        endpoint = self.kms_url + VaultClient.WRAP_ENDPOINT\n        headers = {VaultClient.TOKEN_HEADER:\n                   self.kms_connection_config.key_access_token}\n        r = requests.post(endpoint + master_key_identifier,\n                          headers=headers,\n                          data={'plaintext': base64.b64encode(key_bytes)})\n        r.raise_for_status()\n        r_dict = r.json()\n        wrapped_key = r_dict['data']['ciphertext']\n        return wrapped_key\n\n    def unwrap_key(self, wrapped_key, master_key_identifier):\n        \"\"\"Call Vault to unwrap wrapped_key with key\n        identified by master_key_identifier\"\"\"\n        endpoint = self.kms_url + VaultClient.UNWRAP_ENDPOINT\n        headers = {VaultClient.TOKEN_HEADER:\n                   self.kms_connection_config.key_access_token}\n        r = requests.post(endpoint + master_key_identifier,\n                          headers=headers,\n                          data={'ciphertext': wrapped_key})\n        r.raise_for_status()\n        r_dict = r.json()\n        plaintext = r_dict['data']['plaintext']\n        key_bytes = base64.b64decode(plaintext)\n        return key_bytes\n\n\ndef parquet_write_read_with_vault(parquet_filename):\n    \"\"\"An example for writing an encrypted parquet and reading an\n    encrypted parquet using master keys managed by Hashicorp Vault KMS.\n    Note that for this implementation requests dependency is needed\n    and environment properties VAULT_URL and VAULT_TOKEN should be set.\n    Please enable the transit engine.\n    \"\"\"\n    path = parquet_filename\n\n    table = pa.Table.from_pydict({\n        'a': pa.array([1, 2, 3]),\n        'b': pa.array(['a', 'b', 'c']),\n        'c': pa.array(['x', 'y', 'z'])\n    })\n\n    # Encrypt the footer with the footer key,\n    # encrypt column `a` with one key\n    # and column `b` with another key,\n    # keep `c` plaintext\n    footer_key_name = \"footer_key\"\n    col_a_key_name = \"col_a_key\"\n    col_b_key_name = \"col_b_key\"\n\n    encryption_config = pe.EncryptionConfiguration(\n        footer_key=footer_key_name,\n        column_keys={\n            col_a_key_name: [\"a\"],\n            col_b_key_name: [\"b\"],\n        })\n\n    kms_connection_config = pe.KmsConnectionConfig(\n        kms_instance_url=os.environ.get('VAULT_URL', ''),\n        key_access_token=os.environ.get('VAULT_TOKEN', ''),\n    )\n\n    def kms_factory(kms_connection_configuration):\n        return VaultClient(kms_connection_configuration)\n\n    # Write with encryption properties\n    crypto_factory = pe.CryptoFactory(kms_factory)\n    file_encryption_properties = crypto_factory.file_encryption_properties(\n        kms_connection_config, encryption_config)\n    with pq.ParquetWriter(path,\n                          table.schema,\n                          encryption_properties=file_encryption_properties) \\\n            as writer:\n        writer.write_table(table)\n\n    # Read with decryption properties\n    file_decryption_properties = crypto_factory.file_decryption_properties(\n        kms_connection_config)\n    result = pq.ParquetFile(\n        path, decryption_properties=file_decryption_properties)\n    result_table = result.read()\n    assert table.equals(result_table)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Write and read an encrypted parquet using master keys \"\n        \"managed by Hashicorp Vault.\\nBefore using please enable the \"\n        \"transit engine in Vault and set VAULT_URL and VAULT_TOKEN \"\n        \"environment variables.\")\n    parser.add_argument('--filename', dest='filename', type=str,\n                        default='/tmp/encrypted_table.vault.parquet',\n                        help='Filename of the parquet file to be created '\n                             '(default: /tmp/encrypted_table.vault.parquet')\n    args = parser.parse_args()\n    filename = args.filename\n    parquet_write_read_with_vault(filename)\n\n\nif __name__ == '__main__':\n    main()\n", "cpp/gdb_arrow.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom collections import namedtuple\nfrom collections.abc import Sequence\nimport datetime\nimport decimal\nimport enum\nfrom functools import lru_cache, partial\nimport itertools\nimport math\nimport operator\nimport struct\nimport sys\nimport warnings\n\nimport gdb\nfrom gdb.types import get_basic_type\n\n\nassert sys.version_info[0] >= 3, \"Arrow GDB extension needs Python 3+\"\n\n\n# gdb API docs at https://sourceware.org/gdb/onlinedocs/gdb/Python-API.html#Python-API\n\n\n_type_ids = [\n    'NA', 'BOOL', 'UINT8', 'INT8', 'UINT16', 'INT16', 'UINT32', 'INT32',\n    'UINT64', 'INT64', 'HALF_FLOAT', 'FLOAT', 'DOUBLE', 'STRING', 'BINARY',\n    'FIXED_SIZE_BINARY', 'DATE32', 'DATE64', 'TIMESTAMP', 'TIME32', 'TIME64',\n    'INTERVAL_MONTHS', 'INTERVAL_DAY_TIME', 'DECIMAL128', 'DECIMAL256',\n    'LIST', 'STRUCT', 'SPARSE_UNION', 'DENSE_UNION', 'DICTIONARY', 'MAP',\n    'EXTENSION', 'FIXED_SIZE_LIST', 'DURATION', 'LARGE_STRING',\n    'LARGE_BINARY', 'LARGE_LIST', 'INTERVAL_MONTH_DAY_NANO']\n\n# Mirror the C++ Type::type enum\nType = enum.IntEnum('Type', _type_ids, start=0)\n\n# Mirror the C++ TimeUnit::type enum\nTimeUnit = enum.IntEnum('TimeUnit', ['SECOND', 'MILLI', 'MICRO', 'NANO'],\n                        start=0)\n\ntype_id_to_struct_code = {\n    Type.INT8: 'b',\n    Type.INT16: 'h',\n    Type.INT32: 'i',\n    Type.INT64: 'q',\n    Type.UINT8: 'B',\n    Type.UINT16: 'H',\n    Type.UINT32: 'I',\n    Type.UINT64: 'Q',\n    Type.HALF_FLOAT: 'e',\n    Type.FLOAT: 'f',\n    Type.DOUBLE: 'd',\n    Type.DATE32: 'i',\n    Type.DATE64: 'q',\n    Type.TIME32: 'i',\n    Type.TIME64: 'q',\n    Type.INTERVAL_DAY_TIME: 'ii',\n    Type.INTERVAL_MONTHS: 'i',\n    Type.INTERVAL_MONTH_DAY_NANO: 'iiq',\n    Type.DURATION: 'q',\n    Type.TIMESTAMP: 'q',\n}\n\nTimeUnitTraits = namedtuple('TimeUnitTraits', ('multiplier',\n                                               'fractional_digits'))\n\ntime_unit_traits = {\n    TimeUnit.SECOND: TimeUnitTraits(1, 0),\n    TimeUnit.MILLI: TimeUnitTraits(1_000, 3),\n    TimeUnit.MICRO: TimeUnitTraits(1_000_000, 6),\n    TimeUnit.NANO: TimeUnitTraits(1_000_000_000, 9),\n}\n\n\ndef identity(v):\n    return v\n\n\ndef has_null_bitmap(type_id):\n    return type_id not in (Type.NA, Type.SPARSE_UNION, Type.DENSE_UNION)\n\n\n@lru_cache()\ndef byte_order():\n    \"\"\"\n    Get the target program (not the GDB host's) endianness.\n    \"\"\"\n    s = gdb.execute(\"show endian\", to_string=True).strip()\n    if 'big' in s:\n        return 'big'\n    elif 'little' in s:\n        return 'little'\n    warnings.warn('Could not determine target endianness '\n                  f'from GDB\\'s response:\\n\"\"\"{s}\"\"\"')\n    # Fall back to host endianness\n    return sys.byteorder\n\n\ndef for_evaluation(val, ty=None):\n    \"\"\"\n    Return a parsable form of gdb.Value `val`, optionally with gdb.Type `ty`.\n    \"\"\"\n    if ty is None:\n        ty = get_basic_type(val.type)\n    typename = str(ty)  # `ty.name` is sometimes None...\n    if '::' in typename and not typename.startswith('::'):\n        # ARROW-15652: expressions evaluated by GDB are evaluated in the\n        # scope of the C++ namespace of the currently selected frame.\n        # When inside a Parquet frame, `arrow::<some type>` would be looked\n        # up as `parquet::arrow::<some type>` and fail.\n        # Therefore, force the lookup to happen in the global namespace scope.\n        typename = f\"::{typename}\"\n    if ty.code == gdb.TYPE_CODE_PTR:\n        # It's already a pointer, can represent it directly\n        return f\"(({typename}) ({val}))\"\n    if val.address is None:\n        raise ValueError(f\"Cannot further evaluate rvalue: {val}\")\n    return f\"(* ({typename}*) ({val.address}))\"\n\n\ndef is_char_star(ty):\n    # Note that \"const char*\" can have TYPE_CODE_INT as target type...\n    ty = get_basic_type(ty)\n    return (ty.code == gdb.TYPE_CODE_PTR and\n            get_basic_type(ty.target()).code\n                in (gdb.TYPE_CODE_CHAR, gdb.TYPE_CODE_INT))\n\n\ndef deref(val):\n    \"\"\"\n    Dereference a raw or smart pointer.\n    \"\"\"\n    ty = get_basic_type(val.type)\n    if ty.code == gdb.TYPE_CODE_PTR:\n        return val.dereference()\n    if ty.name.startswith('std::'):\n        if \"shared\" in ty.name:\n            return SharedPtr(val).value\n        if \"unique\" in ty.name:\n            return UniquePtr(val).value\n    raise TypeError(f\"Cannot dereference value of type '{ty.name}'\")\n\n\n_string_literal_mapping = {\n    ord('\\\\'): r'\\\\',\n    ord('\\n'): r'\\n',\n    ord('\\r'): r'\\r',\n    ord('\\t'): r'\\t',\n    ord('\"'): r'\\\"',\n}\n\nfor c in range(0, 32):\n    if c not in _string_literal_mapping:\n        _string_literal_mapping[c] = f\"\\\\x{c:02x}\"\n\n\ndef string_literal(s):\n    \"\"\"\n    Format a Python string or gdb.Value for display as a literal.\n    \"\"\"\n    max_len = 50\n    if isinstance(s, gdb.Value):\n        s = s.string()\n    if len(s) > max_len:\n        s = s[:max_len]\n        return '\"' + s.translate(_string_literal_mapping) + '\" [continued]'\n    else:\n        return '\"' + s.translate(_string_literal_mapping) + '\"'\n\n\ndef bytes_literal(val, size=None):\n    \"\"\"\n    Format a gdb.Value for display as a literal containing possibly\n    unprintable characters.\n    \"\"\"\n    return val.lazy_string(length=size).value()\n\n\ndef utf8_literal(val, size=None):\n    \"\"\"\n    Format a gdb.Value for display as a utf-8 literal.\n    \"\"\"\n    if size is None:\n        s = val.string(encoding='utf8', errors='backslashreplace')\n    elif size != 0:\n        s = val.string(encoding='utf8', errors='backslashreplace', length=size)\n    else:\n        s = \"\"\n    return string_literal(s)\n\n\ndef half_float_value(val):\n    \"\"\"\n    Return a Python float of the given half-float (represented as a uint64_t\n    gdb.Value).\n    \"\"\"\n    buf = gdb.selected_inferior().read_memory(val.address, 2)\n    return struct.unpack(\"e\", buf)[0]\n\n\ndef load_atomic(val):\n    \"\"\"\n    Load a std::atomic<T>'s value.\n    \"\"\"\n    valty = val.type.template_argument(0)\n    # XXX This assumes std::atomic<T> has the same layout as a raw T.\n    return val.address.reinterpret_cast(valty.pointer()).dereference()\n\n\ndef load_null_count(val):\n    \"\"\"\n    Load a null count from a gdb.Value of an integer (either atomic or not).\n    \"\"\"\n    if get_basic_type(val.type).code != gdb.TYPE_CODE_INT:\n        val = load_atomic(val)\n    return val\n\n\ndef format_null_count(val):\n    \"\"\"\n    Format a null count value.\n    \"\"\"\n    if not isinstance(val, int):\n        null_count = int(load_null_count(val))\n    return (f\"null count {null_count}\" if null_count != -1\n            else \"unknown null count\")\n\n\ndef short_time_unit(val):\n    return ['s', 'ms', 'us', 'ns'][int(val)]\n\n\ndef format_month_interval(val):\n    \"\"\"\n    Format a MonthInterval value.\n    \"\"\"\n    return f\"{int(val)}M\"\n\n\ndef format_days_milliseconds(days, milliseconds):\n    return f\"{days}d{milliseconds}ms\"\n\n\ndef format_months_days_nanos(months, days, nanos):\n    return f\"{months}M{days}d{nanos}ns\"\n\n\n_date_base = datetime.date(1970, 1, 1).toordinal()\n\n\ndef format_date32(val):\n    \"\"\"\n    Format a date32 value.\n    \"\"\"\n    val = int(val)\n    try:\n        decoded = datetime.date.fromordinal(val + _date_base)\n    except ValueError:  # \"ordinal must be >= 1\"\n        return f\"{val}d [year <= 0]\"\n    else:\n        return f\"{val}d [{decoded}]\"\n\n\ndef format_date64(val):\n    \"\"\"\n    Format a date64 value.\n    \"\"\"\n    val = int(val)\n    days, remainder = divmod(val, 86400 * 1000)\n    if remainder:\n        return f\"{val}ms [non-multiple of 86400000]\"\n    try:\n        decoded = datetime.date.fromordinal(days + _date_base)\n    except ValueError:  # \"ordinal must be >= 1\"\n        return f\"{val}ms [year <= 0]\"\n    else:\n        return f\"{val}ms [{decoded}]\"\n\n\ndef format_timestamp(val, unit):\n    \"\"\"\n    Format a timestamp value.\n    \"\"\"\n    val = int(val)\n    unit = int(unit)\n    short_unit = short_time_unit(unit)\n    traits = time_unit_traits[unit]\n    seconds, subseconds = divmod(val, traits.multiplier)\n    try:\n        dt = datetime.datetime.utcfromtimestamp(seconds)\n    except (ValueError, OSError, OverflowError):\n        # value out of range for datetime.datetime\n        pretty = \"too large to represent\"\n    else:\n        pretty = dt.isoformat().replace('T', ' ')\n        if traits.fractional_digits > 0:\n            pretty += f\".{subseconds:0{traits.fractional_digits}d}\"\n    return f\"{val}{short_unit} [{pretty}]\"\n\n\ndef cast_to_concrete(val, ty):\n    return (val.reference_value().reinterpret_cast(ty.reference())\n            .referenced_value())\n\n\ndef scalar_class_from_type(name):\n    \"\"\"\n    Given a DataTypeClass class name (such as \"BooleanType\"), return the\n    corresponding Scalar class name.\n    \"\"\"\n    assert name.endswith(\"Type\")\n    return name[:-4] + \"Scalar\"\n\n\ndef array_class_from_type(name):\n    \"\"\"\n    Given a DataTypeClass class name (such as \"BooleanType\"), return the\n    corresponding Array class name.\n    \"\"\"\n    assert name.endswith(\"Type\")\n    return name[:-4] + \"Array\"\n\n\nclass CString:\n    \"\"\"\n    A `const char*` or similar value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n\n    def __bool__(self):\n        return int(data) != 0 and int(data[0]) != 0\n\n    @property\n    def data(self):\n        return self.val\n\n    def bytes_literal(self):\n        return self.val.lazy_string().value()\n\n    def string_literal(self):\n        # XXX use lazy_string() as well?\n        return string_literal(self.val)\n\n    def string(self):\n        return self.val.string()\n\n    def __format__(self, fmt):\n        return str(self.bytes_literal())\n\n\n# NOTE: gdb.parse_and_eval() is *slow* and calling it multiple times\n# may add noticeable latencies.  For standard C++ classes, we therefore\n# try to fetch their properties from libstdc++ internals (which hopefully\n# are stable), before falling back on calling the public API methods.\n\nclass SharedPtr:\n    \"\"\"\n    A `std::shared_ptr<T>` value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        try:\n            # libstdc++ internals\n            self._ptr = val['_M_ptr']\n        except gdb.error:\n            # fallback for other C++ standard libraries\n            self._ptr = gdb.parse_and_eval(f\"{for_evaluation(val)}.get()\")\n\n    def get(self):\n        \"\"\"\n        Return the underlying pointer (a T*).\n        \"\"\"\n        return self._ptr\n\n    @property\n    def value(self):\n        \"\"\"\n        The underlying value (a T).\n        \"\"\"\n        return self._ptr.dereference()\n\n\nclass UniquePtr:\n    \"\"\"\n    A `std::unique_ptr<T>` value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        ty = self.val.type.template_argument(0)\n        # XXX This assumes that the embedded T* pointer lies at the start\n        # of std::unique_ptr<T>.\n        self._ptr = self.val.address.reinterpret_cast(ty.pointer().pointer())\n\n    def get(self):\n        \"\"\"\n        Return the underlying pointer (a T*).\n        \"\"\"\n        return self._ptr\n\n    @property\n    def value(self):\n        \"\"\"\n        The underlying value (a T).\n        \"\"\"\n        return self._ptr.dereference()\n\n\nclass Variant:\n    \"\"\"\n    A `std::variant<...>`.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        try:\n            # libstdc++ internals\n            self.index = val['_M_index']\n        except gdb.error:\n            # fallback for other C++ standard libraries\n            self.index = gdb.parse_and_eval(f\"{for_evaluation(val)}.index()\")\n        try:\n            self.value_type = self.val.type.template_argument(self.index)\n        except RuntimeError:\n            # Index out of bounds\n            self.value_type = None\n\n    @property\n    def value(self):\n        if self.value_type is None:\n            return None\n        ptr = self.val.address\n        if ptr is not None:\n            return ptr.reinterpret_cast(self.value_type.pointer()\n                                        ).dereference()\n        return None\n\n\nclass StdString:\n    \"\"\"\n    A `std::string` (or possibly `std::string_view`) value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        try:\n            # libstdc++ internals\n            self._data = val['_M_dataplus']['_M_p']\n            self._size = val['_M_string_length']\n        except gdb.error:\n            # fallback for other C++ standard libraries\n            self._data = gdb.parse_and_eval(f\"{for_evaluation(val)}.c_str()\")\n            self._size = gdb.parse_and_eval(f\"{for_evaluation(val)}.size()\")\n\n    def __bool__(self):\n        return self._size != 0\n\n    @property\n    def data(self):\n        return self._data\n\n    @property\n    def size(self):\n        return self._size\n\n    def bytes_literal(self):\n        return self._data.lazy_string(length=self._size).value()\n\n    def string_literal(self):\n        # XXX use lazy_string() as well?\n        return string_literal(self._data)\n\n    def string(self):\n        return self._data.string()\n\n    def __format__(self, fmt):\n        return str(self.bytes_literal())\n\n\nclass StdVector(Sequence):\n    \"\"\"\n    A `std::vector<T>` value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        try:\n            # libstdc++ internals\n            impl = self.val['_M_impl']\n            self._data = impl['_M_start']\n            self._size = int(impl['_M_finish'] - self._data)\n        except gdb.error:\n            # fallback for other C++ standard libraries\n            self._data = int(gdb.parse_and_eval(\n                f\"{for_evaluation(self.val)}.data()\"))\n            self._size = int(gdb.parse_and_eval(\n                f\"{for_evaluation(self.val)}.size()\"))\n\n    def _check_index(self, index):\n        if index < 0 or index >= self._size:\n            raise IndexError(\n                f\"Index {index} out of bounds (should be in [0, {self._size - 1}])\")\n\n    def __len__(self):\n        return self._size\n\n    def __getitem__(self, index):\n        self._check_index(index)\n        return self._data[index]\n\n    def eval_at(self, index, eval_format):\n        \"\"\"\n        Run `eval_format` with the value at `index`.\n\n        For example, if `eval_format` is \"{}.get()\", this will evaluate\n        \"{self[index]}.get()\".\n        \"\"\"\n        self._check_index(index)\n        return gdb.parse_and_eval(\n            eval_format.format(for_evaluation(self._data[index])))\n\n    def iter_eval(self, eval_format):\n        data_eval = for_evaluation(self._data)\n        for i in range(self._size):\n            yield gdb.parse_and_eval(\n                eval_format.format(f\"{data_eval}[{i}]\"))\n\n    @property\n    def size(self):\n        return self._size\n\n\nclass StdPtrVector(StdVector):\n\n    def __getitem__(self, index):\n        return deref(super().__getitem__(index))\n\n\nclass FieldVector(StdVector):\n\n    def __getitem__(self, index):\n        \"\"\"\n        Dereference the Field object at this index.\n        \"\"\"\n        return Field(deref(super().__getitem__(index)))\n\n    def __str__(self):\n        l = [str(self[i]) for i in range(len(self))]\n        return \"{\" + \", \".join(l) + \"}\"\n\n\nclass Field:\n    \"\"\"\n    A arrow::Field value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n\n    @property\n    def name(self):\n        return StdString(self.val['name_'])\n\n    @property\n    def type(self):\n        return deref(self.val['type_'])\n\n    @property\n    def nullable(self):\n        return bool(self.val['nullable_'])\n\n    def __str__(self):\n        return str(self.val)\n\n\nclass FieldPtr(Field):\n    \"\"\"\n    A std::shared_ptr<arrow::Field> value.\n    \"\"\"\n\n    def __init__(self, val):\n        super().__init__(deref(val))\n\n\nclass Buffer:\n    \"\"\"\n    A arrow::Buffer value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        self.size = int(val['size_'])\n\n    @property\n    def data(self):\n        return self.val['data_']\n\n    def bytes_literal(self):\n        if self.size > 0:\n            return self.val['data_'].lazy_string(length=self.size).value()\n        else:\n            return '\"\"'\n\n    def bytes_view(self, offset=0, length=None):\n        \"\"\"\n        Return a view over the bytes of this buffer.\n        \"\"\"\n        if self.size > 0:\n            if length is None:\n                length = self.size\n            mem = gdb.selected_inferior().read_memory(\n                self.val['data_'] + offset, self.size)\n        else:\n            mem = memoryview(b\"\")\n        # Read individual bytes as unsigned integers rather than\n        # Python bytes objects\n        return mem.cast('B')\n\n    view = bytes_view\n\n\nclass BufferPtr:\n    \"\"\"\n    A arrow::Buffer* value (possibly null).\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        ptr = int(self.val)\n        self.buf = Buffer(val.dereference()) if ptr != 0 else None\n\n    @property\n    def data(self):\n        if self.buf is None:\n            return None\n        return self.buf.data\n\n    @property\n    def size(self):\n        if self.buf is None:\n            return None\n        return self.buf.size\n\n    def bytes_literal(self):\n        if self.buf is None:\n            return None\n        return self.buf.bytes_literal()\n\n\nclass TypedBuffer(Buffer):\n    \"\"\"\n    A buffer containing values of a given a struct format code.\n    \"\"\"\n    _boolean_format = object()\n\n    def __init__(self, val, mem_format):\n        super().__init__(val)\n        self.mem_format = mem_format\n        if not self.is_boolean:\n            self.byte_width = struct.calcsize('=' + self.mem_format)\n\n    @classmethod\n    def from_type_id(cls, val, type_id):\n        assert isinstance(type_id, int)\n        if type_id == Type.BOOL:\n            mem_format = cls._boolean_format\n        else:\n            mem_format = type_id_to_struct_code[type_id]\n        return cls(val, mem_format)\n\n    def view(self, offset=0, length=None):\n        \"\"\"\n        Return a view over the primitive values in this buffer.\n\n        The optional `offset` and `length` are expressed in primitive values,\n        not bytes.\n        \"\"\"\n        if self.is_boolean:\n            return Bitmap.from_buffer(self, offset, length)\n\n        byte_offset = offset * self.byte_width\n        if length is not None:\n            mem = self.bytes_view(byte_offset, length * self.byte_width)\n        else:\n            mem = self.bytes_view(byte_offset)\n        return TypedView(mem, self.mem_format)\n\n    @property\n    def is_boolean(self):\n        return self.mem_format is self._boolean_format\n\n\nclass TypedView(Sequence):\n    \"\"\"\n    View a bytes-compatible object as a sequence of objects described\n    by a struct format code.\n    \"\"\"\n\n    def __init__(self, mem, mem_format):\n        assert isinstance(mem, memoryview)\n        self.mem = mem\n        self.mem_format = mem_format\n        self.byte_width = struct.calcsize('=' + mem_format)\n        self.length = mem.nbytes // self.byte_width\n\n    def _check_index(self, index):\n        if not 0 <= index < self.length:\n            raise IndexError(\"Wrong index for bitmap\")\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        self._check_index(index)\n        w = self.byte_width\n        # Cannot use memoryview.cast() because the 'e' format for half-floats\n        # is poorly supported.\n        mem = self.mem[index * w:(index + 1) * w]\n        return struct.unpack('=' + self.mem_format, mem)\n\n\nclass Bitmap(Sequence):\n    \"\"\"\n    View a bytes-compatible object as a sequence of bools.\n    \"\"\"\n    _masks = [0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, 0x80]\n\n    def __init__(self, view, offset, length):\n        self.view = view\n        self.offset = offset\n        self.length = length\n\n    def _check_index(self, index):\n        if not 0 <= index < self.length:\n            raise IndexError(\"Wrong index for bitmap\")\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        self._check_index(index)\n        index += self.offset\n        byte_index, bit_index = divmod(index, 8)\n        byte = self.view[byte_index]\n        return byte & self._masks[bit_index] != 0\n\n    @classmethod\n    def from_buffer(cls, buf, offset, length):\n        assert isinstance(buf, Buffer)\n        byte_offset, bit_offset = divmod(offset, 8)\n        byte_length = math.ceil(length + offset / 8) - byte_offset\n        return cls(buf.bytes_view(byte_offset, byte_length),\n                   bit_offset, length)\n\n\nclass MappedView(Sequence):\n\n    def __init__(self, func, view):\n        self.view = view\n        self.func = func\n\n    def __len__(self):\n        return len(self.view)\n\n    def __getitem__(self, index):\n        return self.func(self.view[index])\n\n\nclass StarMappedView(Sequence):\n\n    def __init__(self, func, view):\n        self.view = view\n        self.func = func\n\n    def __len__(self):\n        return len(self.view)\n\n    def __getitem__(self, index):\n        return self.func(*self.view[index])\n\n\nclass NullBitmap(Bitmap):\n\n    def __getitem__(self, index):\n        self._check_index(index)\n        if self.view is None:\n            return True\n        return super().__getitem__(index)\n\n    @classmethod\n    def from_buffer(cls, buf, offset, length):\n        \"\"\"\n        Create a null bitmap from a Buffer (or None if missing,\n        in which case all values are True).\n        \"\"\"\n        if buf is None:\n            return cls(buf, offset, length)\n        return super().from_buffer(buf, offset, length)\n\n\nKeyValue = namedtuple('KeyValue', ('key', 'value'))\n\n\nclass Metadata(Sequence):\n    \"\"\"\n    A arrow::KeyValueMetadata value.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        self.keys = StdVector(self.val['keys_'])\n        self.values = StdVector(self.val['values_'])\n\n    def __len__(self):\n        return len(self.keys)\n\n    def __getitem__(self, i):\n        return KeyValue(StdString(self.keys[i]), StdString(self.values[i]))\n\n\nclass MetadataPtr(Sequence):\n    \"\"\"\n    A shared_ptr<arrow::KeyValueMetadata> value, possibly null.\n    \"\"\"\n\n    def __init__(self, val):\n        self.ptr = SharedPtr(val).get()\n        self.is_null = int(self.ptr) == 0\n        self.md = None if self.is_null else Metadata(self.ptr.dereference())\n\n    def __len__(self):\n        return 0 if self.is_null else len(self.md)\n\n    def __getitem__(self, i):\n        if self.is_null:\n            raise IndexError\n        return self.md[i]\n\n\nDecimalTraits = namedtuple('DecimalTraits', ('bit_width', 'struct_format_le'))\n\ndecimal_traits = {\n    128: DecimalTraits(128, 'Qq'),\n    256: DecimalTraits(256, 'QQQq'),\n}\n\nclass BaseDecimal:\n    \"\"\"\n    Base class for arrow::BasicDecimal{128,256...} values.\n    \"\"\"\n\n    def __init__(self, address):\n        self.address = address\n\n    @classmethod\n    def from_value(cls, val):\n        \"\"\"\n        Create a decimal from a gdb.Value representing the corresponding\n        arrow::BasicDecimal{128,256...}.\n        \"\"\"\n        return cls(val['array_'].address)\n\n    @classmethod\n    def from_address(cls, address):\n        \"\"\"\n        Create a decimal from a gdb.Value representing the address of the\n        raw decimal storage.\n        \"\"\"\n        return cls(address)\n\n    @property\n    def words(self):\n        \"\"\"\n        The decimal words, from least to most significant.\n        \"\"\"\n        mem = gdb.selected_inferior().read_memory(self.address,\n                                                  self.traits.bit_width // 8)\n        fmt = self.traits.struct_format_le\n        if byte_order() == 'big':\n            fmt = fmt[::-1]\n        words = struct.unpack(f\"={fmt}\", mem)\n        if byte_order() == 'big':\n            words = words[::-1]\n        return words\n\n    def __int__(self):\n        \"\"\"\n        The underlying bigint value.\n        \"\"\"\n        v = 0\n        words = self.words\n        bits_per_word = self.traits.bit_width // len(words)\n        for w in reversed(words):\n            v = (v << bits_per_word) + w\n        return v\n\n    def format(self, precision, scale):\n        \"\"\"\n        Format as a decimal number with the given precision and scale.\n        \"\"\"\n        v = int(self)\n        with decimal.localcontext() as ctx:\n            ctx.prec = precision\n            ctx.capitals = False\n            return str(decimal.Decimal(v).scaleb(-scale))\n\n\nclass Decimal128(BaseDecimal):\n    traits = decimal_traits[128]\n\n\nclass Decimal256(BaseDecimal):\n    traits = decimal_traits[256]\n\n\ndecimal_bits_to_class = {\n    128: Decimal128,\n    256: Decimal256,\n}\n\ndecimal_type_to_class = {\n    f\"Decimal{bits}Type\": cls\n    for (bits, cls) in decimal_bits_to_class.items()\n}\n\n\nclass ExtensionType:\n    \"\"\"\n    A arrow::ExtensionType.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n\n    @property\n    def storage_type(self):\n        return deref(self.val['storage_type_'])\n\n    def to_string(self):\n        \"\"\"\n        The result of calling ToString(show_metadata=True).\n        \"\"\"\n        # XXX `show_metadata` is an optional argument, but gdb doesn't allow\n        # omitting it.\n        return StdString(gdb.parse_and_eval(\n            f\"{for_evaluation(self.val)}.ToString(true)\"))\n\n\nclass Schema:\n    \"\"\"\n    A arrow::Schema.\n    \"\"\"\n\n    def __init__(self, val):\n        self.val = val\n        impl = deref(self.val['impl_'])\n        self.fields = FieldVector(impl['fields_'])\n        self.metadata = MetadataPtr(impl['metadata_'])\n\n\nclass RecordBatch:\n    \"\"\"\n    A arrow::RecordBatch.\n    \"\"\"\n\n    def __init__(self, val):\n        # XXX this relies on RecordBatch always being a SimpleRecordBatch\n        # under the hood. What if users create their own RecordBatch\n        # implementation?\n        self.val = cast_to_concrete(val,\n                                    gdb.lookup_type(\"arrow::SimpleRecordBatch\"))\n        self.schema = Schema(deref(self.val['schema_']))\n        self.columns = StdPtrVector(self.val['columns_'])\n\n    @property\n    def num_rows(self):\n        return self.val['num_rows_']\n\n\nclass Table:\n    \"\"\"\n    A arrow::Table.\n    \"\"\"\n\n    def __init__(self, val):\n        # XXX this relies on Table always being a SimpleTable under the hood.\n        # What if users create their own Table implementation?\n        self.val = cast_to_concrete(val,\n                                    gdb.lookup_type(\"arrow::SimpleTable\"))\n        self.schema = Schema(deref(self.val['schema_']))\n        self.columns = StdPtrVector(self.val['columns_'])\n\n    @property\n    def num_rows(self):\n        return self.val['num_rows_']\n\n\ntype_reprs = {\n    'NullType': 'null',\n    'BooleanType': 'boolean',\n    'UInt8Type': 'uint8',\n    'Int8Type': 'int8',\n    'UInt16Type': 'uint16',\n    'Int16Type': 'int16',\n    'UInt32Type': 'uint32',\n    'Int32Type': 'int32',\n    'UInt64Type': 'uint64',\n    'Int64Type': 'int64',\n    'HalfFloatType': 'float16',\n    'FloatType': 'float32',\n    'DoubleType': 'float64',\n    'Date32Type': 'date32',\n    'Date64Type': 'date64',\n    'Time32Type': 'time32',\n    'Time64Type': 'time64',\n    'TimestampType': 'timestamp',\n    'MonthIntervalType': 'month_interval',\n    'DayTimeIntervalType': 'day_time_interval',\n    'MonthDayNanoIntervalType': 'month_day_nano_interval',\n    'DurationType': 'duration',\n    'Decimal128Type': 'decimal128',\n    'Decimal256Type': 'decimal256',\n    'StringType': 'utf8',\n    'LargeStringType': 'large_utf8',\n    'BinaryType': 'binary',\n    'LargeBinaryType': 'large_binary',\n    'FixedSizeBinaryType': 'fixed_size_binary',\n    'ListType': 'list',\n    'LargeListType': 'large_list',\n    'FixedSizeListType': 'fixed_size_list',\n    'MapType': 'map',\n    'StructType': 'struct_',\n    'SparseUnionType': 'sparse_union',\n    'DenseUnionType': 'dense_union',\n    'DictionaryType': 'dictionary',\n    }\n\n\nclass TypePrinter:\n    \"\"\"\n    Pretty-printer for arrow::DataTypeClass and subclasses.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.name = name\n        # Cast to concrete type class to access all derived methods\n        # and properties.\n        self.type = gdb.lookup_type(f\"arrow::{name}\")\n        self.val = cast_to_concrete(val, self.type)\n\n    @property\n    def fields(self):\n        return FieldVector(self.val['children_'])\n\n    def _format_type(self):\n        r = type_reprs.get(self.name, self.name)\n        return f\"arrow::{r}\"\n\n    def _for_evaluation(self):\n        return for_evaluation(self.val, self.type)\n\n\nclass PrimitiveTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for non-parametric types.\n    \"\"\"\n\n    def to_string(self):\n        return f\"{self._format_type()}()\"\n\n\nclass TimeTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for time and duration types.\n    \"\"\"\n\n    def _get_unit(self):\n        return self.val['unit_']\n\n    def to_string(self):\n        return f\"{self._format_type()}({self._get_unit()})\"\n\n\nclass TimestampTypePrinter(TimeTypePrinter):\n    \"\"\"\n    Pretty-printer for timestamp types.\n    \"\"\"\n\n    def to_string(self):\n        tz = StdString(self.val['timezone_'])\n        if tz:\n            return f'{self._format_type()}({self._get_unit()}, {tz})'\n        else:\n            return f'{self._format_type()}({self._get_unit()})'\n\n\nclass FixedSizeBinaryTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for fixed-size binary types.\n    \"\"\"\n\n    def to_string(self):\n        width = int(self.val['byte_width_'])\n        return f\"{self._format_type()}({width})\"\n\n\nclass DecimalTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for decimal types.\n    \"\"\"\n\n    def to_string(self):\n        precision = int(self.val['precision_'])\n        scale = int(self.val['scale_'])\n        return f\"{self._format_type()}({precision}, {scale})\"\n\n\nclass ListTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for list types.\n    \"\"\"\n\n    def _get_value_type(self):\n        fields = self.fields\n        if len(fields) != 1:\n            return None\n        return fields[0].type\n\n    def to_string(self):\n        child = self._get_value_type()\n        if child is None:\n            return f\"{self._format_type()}<uninitialized or corrupt>\"\n        else:\n            return f\"{self._format_type()}({child})\"\n\n\nclass FixedSizeListTypePrinter(ListTypePrinter):\n    \"\"\"\n    Pretty-printer for fixed-size list type.\n    \"\"\"\n\n    def to_string(self):\n        child = self._get_value_type()\n        if child is None:\n            return f\"{self._format_type()}<uninitialized or corrupt>\"\n        list_size = int(self.val['list_size_'])\n        return f\"{self._format_type()}({child}, {list_size})\"\n\n\nclass MapTypePrinter(ListTypePrinter):\n    \"\"\"\n    Pretty-printer for map types.\n    \"\"\"\n\n    def to_string(self):\n        struct_type = self._get_value_type()\n        if struct_type is None:\n            return f\"{self._format_type()}<uninitialized or corrupt>\"\n        struct_children = FieldVector(struct_type['children_'])\n        if len(struct_children) != 2:\n            return f\"{self._format_type()}<uninitialized or corrupt>\"\n        key_type = struct_children[0].type\n        item_type = struct_children[1].type\n        return (f\"{self._format_type()}({key_type}, {item_type}, \"\n                f\"keys_sorted={self.val['keys_sorted_']})\")\n\n\nclass DictionaryTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for dictionary types.\n    \"\"\"\n\n    def to_string(self):\n        index_type = deref(self.val['index_type_'])\n        value_type = deref(self.val['value_type_'])\n        ordered = self.val['ordered_']\n        return (f\"{self._format_type()}({index_type}, {value_type}, \"\n                f\"ordered={ordered})\")\n\n\nclass StructTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for struct types.\n    \"\"\"\n\n    def to_string(self):\n        return f\"{self._format_type()}({self.fields})\"\n\n\nclass UnionTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for union types.\n    \"\"\"\n\n    def to_string(self):\n        type_codes = StdVector(self.val['type_codes_'])\n        type_codes = \"{\" + \", \".join(str(x.cast(gdb.lookup_type('int')))\n                                     for x in type_codes) + \"}\"\n        return f\"{self._format_type()}(fields={self.fields}, type_codes={type_codes})\"\n\n\nclass ExtensionTypePrinter(TypePrinter):\n    \"\"\"\n    Pretty-printer for extension types.\n    \"\"\"\n\n    def to_string(self):\n        ext_type = ExtensionType(self.val)\n        return (f\"{self._format_type()} {ext_type.to_string().string_literal()} \"\n                f\"with storage type {ext_type.storage_type}\")\n\n\nclass ScalarPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Scalar and subclasses.\n    \"\"\"\n\n    def __new__(cls, val):\n        # Lookup actual (derived) class to instantiate\n        type_id = int(deref(val['type'])['id_'])\n        type_class = lookup_type_class(type_id)\n        if type_class is not None:\n            cls = type_class.scalar_printer\n            assert issubclass(cls, ScalarPrinter)\n        self = object.__new__(cls)\n        self.type_class = type_class\n        self.type_name = type_class.name\n        self.name = scalar_class_from_type(self.type_name)\n        self.type_id = type_id\n        # Cast to concrete Scalar class to access derived attributes.\n        concrete_type = gdb.lookup_type(f\"arrow::{self.name}\")\n        self.val = cast_to_concrete(val, concrete_type)\n        self.is_valid = bool(self.val['is_valid'])\n        return self\n\n    @property\n    def type(self):\n        \"\"\"\n        The concrete DataTypeClass instance.\n        \"\"\"\n        concrete_type = gdb.lookup_type(f\"arrow::{self.type_name}\")\n        return cast_to_concrete(deref(self.val['type']),\n                                concrete_type)\n\n    def _format_type(self):\n        return f\"arrow::{self.name}\"\n\n    def _format_null(self):\n        if self.type_class.is_parametric:\n            return f\"{self._format_type()} of type {self.type}, null value\"\n        else:\n            return f\"{self._format_type()} of null value\"\n\n    def _for_evaluation(self):\n        return for_evaluation(self.val)\n\n\nclass NullScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::NullScalar.\n    \"\"\"\n\n    def to_string(self):\n        return self._format_type()\n\n\nclass NumericScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for numeric Arrow scalars.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        value = self.val['value']\n        if self.type_name == \"HalfFloatType\":\n            return (f\"{self._format_type()} \"\n                    f\"of value {half_float_value(value)} [{value}]\")\n        if self.type_name in (\"UInt8Type\", \"Int8Type\"):\n            value = value.cast(gdb.lookup_type('int'))\n        return f\"{self._format_type()} of value {value}\"\n\n\nclass TimeScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for Arrow time-like scalars.\n    \"\"\"\n\n    def to_string(self):\n        unit = short_time_unit(self.type['unit_'])\n        if not self.is_valid:\n            return f\"{self._format_type()} of null value [{unit}]\"\n        value = self.val['value']\n        return f\"{self._format_type()} of value {value}{unit}\"\n\n\nclass Date32ScalarPrinter(TimeScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::Date32Scalar.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        value = self.val['value']\n        return f\"{self._format_type()} of value {format_date32(value)}\"\n\n\nclass Date64ScalarPrinter(TimeScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::Date64Scalar.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        value = self.val['value']\n        return f\"{self._format_type()} of value {format_date64(value)}\"\n\n\nclass TimestampScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::TimestampScalar.\n    \"\"\"\n\n    def to_string(self):\n        unit = short_time_unit(self.type['unit_'])\n        tz = StdString(self.type['timezone_'])\n        tz = tz.string_literal() if tz.size != 0 else \"no timezone\"\n        if not self.is_valid:\n            return f\"{self._format_type()} of null value [{unit}, {tz}]\"\n        value = self.val['value']\n        return f\"{self._format_type()} of value {value}{unit} [{tz}]\"\n\n\nclass MonthIntervalScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::MonthIntervalScalarPrinter.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        value = self.val['value']\n        return f\"{self._format_type()} of value {format_month_interval(value)}\"\n\n\nclass DecimalScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::DecimalScalar and subclasses.\n    \"\"\"\n\n    @property\n    def decimal_class(self):\n        return decimal_type_to_class[self.type_name]\n\n    def to_string(self):\n        ty = self.type\n        precision = int(ty['precision_'])\n        scale = int(ty['scale_'])\n        suffix = f\"[precision={precision}, scale={scale}]\"\n        if not self.is_valid:\n            return f\"{self._format_type()} of null value {suffix}\"\n        value = self.decimal_class.from_value(self.val['value']\n                                              ).format(precision, scale)\n        return f\"{self._format_type()} of value {value} {suffix}\"\n\n\nclass BaseBinaryScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::BaseBinaryScalar and subclasses.\n    \"\"\"\n\n    def _format_buf(self, bufptr):\n        if 'String' in self.type_name:\n            return utf8_literal(bufptr.data, bufptr.size)\n        else:\n            return bufptr.bytes_literal()\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        bufptr = BufferPtr(SharedPtr(self.val['value']).get())\n        size = bufptr.size\n        if size is None:\n            return f\"{self._format_type()} of value <unallocated>\"\n        return (f\"{self._format_type()} of size {size}, \"\n                f\"value {self._format_buf(bufptr)}\")\n\n\nclass FixedSizeBinaryScalarPrinter(BaseBinaryScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::FixedSizeBinaryScalar.\n    \"\"\"\n\n    def to_string(self):\n        size = self.type['byte_width_']\n        bufptr = BufferPtr(SharedPtr(self.val['value']).get())\n        if bufptr.data is None:\n            return f\"{self._format_type()} of size {size}, <unallocated>\"\n        nullness = '' if self.is_valid else 'null with '\n        return (f\"{self._format_type()} of size {size}, \"\n                f\"{nullness}value {self._format_buf(bufptr)}\")\n\n\nclass DictionaryScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::DictionaryScalar.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        index = deref(self.val['value']['index'])\n        dictionary = deref(self.val['value']['dictionary'])\n        return (f\"{self._format_type()} of index {index}, \"\n                f\"dictionary {dictionary}\")\n\n\nclass BaseListScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::BaseListScalar and subclasses.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        value = deref(self.val['value'])\n        return f\"{self._format_type()} of value {value}\"\n\n\nclass StructScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::StructScalar.\n    \"\"\"\n\n    def display_hint(self):\n        return 'map'\n\n    def children(self):\n        if not self.is_valid:\n            return None\n        eval_fields = StdVector(self.type['children_'])\n        eval_values = StdVector(self.val['value'])\n        for field, value in zip(eval_fields, eval_values):\n            name = StdString(deref(field)['name_']).string_literal()\n            yield (\"name\", name)\n            yield (\"value\", deref(value))\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n        return f\"{self._format_type()}\"\n\n\nclass SparseUnionScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::UnionScalar and subclasses.\n    \"\"\"\n\n    def to_string(self):\n        type_code = self.val['type_code'].cast(gdb.lookup_type('int'))\n        if not self.is_valid:\n            return (f\"{self._format_type()} of type {self.type}, \"\n                    f\"type code {type_code}, null value\")\n        eval_values = StdVector(self.val['value'])\n        child_id = self.val['child_id'].cast(gdb.lookup_type('int'))\n        return (f\"{self._format_type()} of type code {type_code}, \"\n                f\"value {deref(eval_values[child_id])}\")\n\n\n\nclass DenseUnionScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::UnionScalar and subclasses.\n    \"\"\"\n\n    def to_string(self):\n        type_code = self.val['type_code'].cast(gdb.lookup_type('int'))\n        if not self.is_valid:\n            return (f\"{self._format_type()} of type {self.type}, \"\n                    f\"type code {type_code}, null value\")\n        value = deref(self.val['value'])\n        return (f\"{self._format_type()} of type code {type_code}, \"\n                f\"value {value}\")\n\n\nclass MapScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::MapScalar.\n    \"\"\"\n\n    def to_string(self):\n        if not self.is_valid:\n            return self._format_null()\n\n        array = deref(self.val['value'])\n        data = deref(array['data_'])\n        data_printer = ArrayDataPrinter(\"arrow::ArrayData\", data)\n        return (f\"{self._format_type()} of type {self.type}, \"\n                f\"value {data_printer._format_contents()}\")\n\n\nclass ExtensionScalarPrinter(ScalarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::ExtensionScalar.\n    \"\"\"\n\n    def to_string(self):\n        ext_type = ExtensionType(self.type)\n        if not self.is_valid:\n            return (f\"{self._format_type()} of type \"\n                    f\"{ext_type.to_string().string_literal()}, null value\")\n        value = deref(self.val['value'])\n        return (f\"{self._format_type()} of type \"\n                f\"{ext_type.to_string().string_literal()}, value {value}\")\n\n\nclass ArrayDataPrinter:\n    \"\"\"\n    Pretty-printer for arrow::ArrayData.\n    \"\"\"\n\n    def __new__(cls, name, val):\n        # Lookup actual (derived) class to instantiate\n        type_id = int(deref(val['type'])['id_'])\n        type_class = lookup_type_class(type_id)\n        if type_class is not None:\n            cls = type_class.array_data_printer\n            assert issubclass(cls, ArrayDataPrinter)\n        self = object.__new__(cls)\n        self.name = name\n        self.val = val\n        self.type_class = type_class\n        self.type_name = type_class.name\n        self.type_id = type_id\n        self.offset = int(self.val['offset'])\n        self.length = int(self.val['length'])\n        return self\n\n    @property\n    def type(self):\n        \"\"\"\n        The concrete DataTypeClass instance.\n        \"\"\"\n        concrete_type = gdb.lookup_type(f\"arrow::{self.type_name}\")\n        return cast_to_concrete(deref(self.val['type']), concrete_type)\n\n    def _format_contents(self):\n        return (f\"length {self.length}, \"\n                f\"offset {self.offset}, \"\n                f\"{format_null_count(self.val['null_count'])}\")\n\n    def _buffer(self, index, type_id=None):\n        buffers = StdVector(self.val['buffers'])\n        bufptr = SharedPtr(buffers[index]).get()\n        if int(bufptr) == 0:\n            return None\n        if type_id is not None:\n            return TypedBuffer.from_type_id(bufptr.dereference(), type_id)\n        else:\n            return Buffer(bufptr.dereference())\n\n    def _buffer_values(self, index, type_id, length=None):\n        \"\"\"\n        Return a typed view of values in the buffer with the given index.\n\n        Values are returned as tuples since some types may decode to\n        multiple values (for example day_time_interval).\n        \"\"\"\n        buf = self._buffer(index, type_id)\n        if buf is None:\n            return None\n        if length is None:\n            length = self.length\n        return buf.view(self.offset, length)\n\n    def _unpacked_buffer_values(self, index, type_id, length=None):\n        \"\"\"\n        Like _buffer_values(), but assumes values are 1-tuples\n        and returns them unpacked.\n        \"\"\"\n        return StarMappedView(identity,\n                              self._buffer_values(index, type_id, length))\n\n    def _null_bitmap(self):\n        buf = self._buffer(0) if has_null_bitmap(self.type_id) else None\n        return NullBitmap.from_buffer(buf, self.offset, self.length)\n\n    def _null_child(self, i):\n        return str(i), \"null\"\n\n    def _valid_child(self, i, value):\n        return str(i), value\n\n    def display_hint(self):\n        return None\n\n    def children(self):\n        return ()\n\n    def to_string(self):\n        ty = self.type\n        return (f\"{self.name} of type {ty}, \"\n                f\"{self._format_contents()}\")\n\n\nclass NumericArrayDataPrinter(ArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for numeric data types.\n    \"\"\"\n    _format_value = staticmethod(identity)\n\n    def _values_view(self):\n        return StarMappedView(self._format_value,\n                              self._buffer_values(1, self.type_id))\n\n    def display_hint(self):\n        return \"array\"\n\n    def children(self):\n        if self.length == 0:\n            return\n        values = self._values_view()\n        null_bits = self._null_bitmap()\n        for i, (valid, value) in enumerate(zip(null_bits, values)):\n            if valid:\n                yield self._valid_child(i, str(value))\n            else:\n                yield self._null_child(i)\n\n\nclass BooleanArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for boolean.\n    \"\"\"\n\n    def _format_value(self, v):\n        return str(v).lower()\n\n    def _values_view(self):\n        return MappedView(self._format_value,\n                          self._buffer_values(1, self.type_id))\n\n\nclass Date32ArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for date32.\n    \"\"\"\n    _format_value = staticmethod(format_date32)\n\n\nclass Date64ArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for date64.\n    \"\"\"\n    _format_value = staticmethod(format_date64)\n\n\nclass TimeArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for time32 and time64.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.unit = self.type['unit_']\n        self.unit_string = short_time_unit(self.unit)\n\n    def _format_value(self, val):\n        return f\"{val}{self.unit_string}\"\n\n\nclass TimestampArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for timestamp.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.unit = self.type['unit_']\n\n    def _format_value(self, val):\n        return format_timestamp(val, self.unit)\n\n\nclass MonthIntervalArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for month_interval.\n    \"\"\"\n    _format_value = staticmethod(format_month_interval)\n\n\nclass DayTimeIntervalArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for day_time_interval.\n    \"\"\"\n    _format_value = staticmethod(format_days_milliseconds)\n\n\nclass MonthDayNanoIntervalArrayDataPrinter(NumericArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for day_time_interval.\n    \"\"\"\n    _format_value = staticmethod(format_months_days_nanos)\n\n\nclass DecimalArrayDataPrinter(ArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for decimals.\n    \"\"\"\n\n    def __init__(self, name, val):\n        ty = self.type\n        self.precision = int(ty['precision_'])\n        self.scale = int(ty['scale_'])\n        self.decimal_class = decimal_type_to_class[self.type_name]\n        self.byte_width = self.decimal_class.traits.bit_width // 8\n\n    def display_hint(self):\n        return \"array\"\n\n    def children(self):\n        if self.length == 0:\n            return\n        null_bits = self._null_bitmap()\n        address = self._buffer(1).data + self.offset * self.byte_width\n        for i, valid in enumerate(null_bits):\n            if valid:\n                dec = self.decimal_class.from_address(address)\n                yield self._valid_child(\n                    i, dec.format(self.precision, self.scale))\n            else:\n                yield self._null_child(i)\n            address += self.byte_width\n\n\nclass FixedSizeBinaryArrayDataPrinter(ArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for fixed_size_binary.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.byte_width = self.type['byte_width_']\n\n    def display_hint(self):\n        return \"array\"\n\n    def children(self):\n        if self.length == 0:\n            return\n        null_bits = self._null_bitmap()\n        address = self._buffer(1).data + self.offset * self.byte_width\n        for i, valid in enumerate(null_bits):\n            if valid:\n                if self.byte_width:\n                    yield self._valid_child(\n                        i, bytes_literal(address, self.byte_width))\n                else:\n                    yield self._valid_child(i, '\"\"')\n            else:\n                yield self._null_child(i)\n            address += self.byte_width\n\n\nclass BinaryArrayDataPrinter(ArrayDataPrinter):\n    \"\"\"\n    ArrayDataPrinter specialization for variable-sized binary.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.is_large = self.type_id in (Type.LARGE_BINARY, Type.LARGE_STRING)\n        self.is_utf8 = self.type_id in (Type.STRING, Type.LARGE_STRING)\n        self.format_string = utf8_literal if self.is_utf8 else bytes_literal\n\n    def display_hint(self):\n        return \"array\"\n\n    def children(self):\n        if self.length == 0:\n            return\n        null_bits = self._null_bitmap()\n        offsets = self._unpacked_buffer_values(\n            1, Type.INT64 if self.is_large else Type.INT32,\n            length=self.length + 1)\n        values = self._buffer(2).data\n        for i, valid in enumerate(null_bits):\n            if valid:\n                start = offsets[i]\n                size = offsets[i + 1] - start\n                if size:\n                    yield self._valid_child(\n                        i, self.format_string(values + start, size))\n                else:\n                    yield self._valid_child(i, '\"\"')\n            else:\n                yield self._null_child(i)\n\n\nclass ArrayPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Array and subclasses.\n    \"\"\"\n\n    def __init__(self, val):\n        data = deref(val['data_'])\n        self.data_printer = ArrayDataPrinter(\"arrow::ArrayData\", data)\n        self.name = array_class_from_type(self.data_printer.type_name)\n\n    def _format_contents(self):\n        return self.data_printer._format_contents()\n\n    def to_string(self):\n        if self.data_printer.type_class.is_parametric:\n            ty = self.data_printer.type\n            return f\"arrow::{self.name} of type {ty}, {self._format_contents()}\"\n        else:\n            return f\"arrow::{self.name} of {self._format_contents()}\"\n\n    def display_hint(self):\n        return self.data_printer.display_hint()\n\n    def children(self):\n        return self.data_printer.children()\n\n\nclass ChunkedArrayPrinter:\n    \"\"\"\n    Pretty-printer for arrow::ChunkedArray.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.name = name\n        self.val = val\n        self.chunks = StdVector(self.val['chunks_'])\n\n    def display_hint(self):\n        return \"array\"\n\n    def children(self):\n        for i, chunk in enumerate(self.chunks):\n            printer = ArrayPrinter(deref(chunk))\n            yield str(i), printer._format_contents()\n\n    def to_string(self):\n        ty = deref(self.val['type_'])\n        return (f\"{self.name} of type {ty}, length {self.val['length_']}, \"\n                f\"{format_null_count(self.val['null_count_'])} \"\n                f\"with {len(self.chunks)} chunks\")\n\n\nclass DataTypeClass:\n    array_data_printer = ArrayDataPrinter\n\n    def __init__(self, name):\n        self.name = name\n\n\nclass NullTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = NullScalarPrinter\n\n\nclass NumericTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = NumericScalarPrinter\n    array_data_printer = NumericArrayDataPrinter\n\n\nclass BooleanTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = NumericScalarPrinter\n    array_data_printer = BooleanArrayDataPrinter\n\n\nclass Date32TypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = Date32ScalarPrinter\n    array_data_printer = Date32ArrayDataPrinter\n\n\nclass Date64TypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = Date64ScalarPrinter\n    array_data_printer = Date64ArrayDataPrinter\n\n\nclass TimeTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = TimeTypePrinter\n    scalar_printer = TimeScalarPrinter\n    array_data_printer = TimeArrayDataPrinter\n\n\nclass TimestampTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = TimestampTypePrinter\n    scalar_printer = TimestampScalarPrinter\n    array_data_printer = TimestampArrayDataPrinter\n\n\nclass DurationTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = TimeTypePrinter\n    scalar_printer = TimeScalarPrinter\n    array_data_printer = TimeArrayDataPrinter\n\n\nclass MonthIntervalTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = MonthIntervalScalarPrinter\n    array_data_printer = MonthIntervalArrayDataPrinter\n\n\nclass DayTimeIntervalTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = NumericScalarPrinter\n    array_data_printer = DayTimeIntervalArrayDataPrinter\n\n\nclass MonthDayNanoIntervalTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = NumericScalarPrinter\n    array_data_printer = MonthDayNanoIntervalArrayDataPrinter\n\n\nclass DecimalTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = DecimalTypePrinter\n    scalar_printer = DecimalScalarPrinter\n    array_data_printer = DecimalArrayDataPrinter\n\n\nclass BaseBinaryTypeClass(DataTypeClass):\n    is_parametric = False\n    type_printer = PrimitiveTypePrinter\n    scalar_printer = BaseBinaryScalarPrinter\n    array_data_printer = BinaryArrayDataPrinter\n\n\nclass FixedSizeBinaryTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = FixedSizeBinaryTypePrinter\n    scalar_printer = FixedSizeBinaryScalarPrinter\n    array_data_printer = FixedSizeBinaryArrayDataPrinter\n\n\nclass BaseListTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = ListTypePrinter\n    scalar_printer = BaseListScalarPrinter\n\n\nclass FixedSizeListTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = FixedSizeListTypePrinter\n    scalar_printer = BaseListScalarPrinter\n\n\nclass MapTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = MapTypePrinter\n    scalar_printer = MapScalarPrinter\n\n\nclass StructTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = StructTypePrinter\n    scalar_printer = StructScalarPrinter\n\n\nclass DenseUnionTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = UnionTypePrinter\n    scalar_printer = DenseUnionScalarPrinter\n\n\nclass SparseUnionTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = UnionTypePrinter\n    scalar_printer = SparseUnionScalarPrinter\n\n\nclass DictionaryTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = DictionaryTypePrinter\n    scalar_printer = DictionaryScalarPrinter\n\n\nclass ExtensionTypeClass(DataTypeClass):\n    is_parametric = True\n    type_printer = ExtensionTypePrinter\n    scalar_printer = ExtensionScalarPrinter\n\n\nDataTypeTraits = namedtuple('DataTypeTraits', ('factory', 'name'))\n\n\ntype_traits_by_id = {\n    Type.NA: DataTypeTraits(NullTypeClass, 'NullType'),\n\n    Type.BOOL: DataTypeTraits(BooleanTypeClass, 'BooleanType'),\n\n    Type.UINT8: DataTypeTraits(NumericTypeClass, 'UInt8Type'),\n    Type.INT8: DataTypeTraits(NumericTypeClass, 'Int8Type'),\n    Type.UINT16: DataTypeTraits(NumericTypeClass, 'UInt16Type'),\n    Type.INT16: DataTypeTraits(NumericTypeClass, 'Int16Type'),\n    Type.UINT32: DataTypeTraits(NumericTypeClass, 'UInt32Type'),\n    Type.INT32: DataTypeTraits(NumericTypeClass, 'Int32Type'),\n    Type.UINT64: DataTypeTraits(NumericTypeClass, 'UInt64Type'),\n    Type.INT64: DataTypeTraits(NumericTypeClass, 'Int64Type'),\n    Type.HALF_FLOAT: DataTypeTraits(NumericTypeClass, 'HalfFloatType'),\n    Type.FLOAT: DataTypeTraits(NumericTypeClass, 'FloatType'),\n    Type.DOUBLE: DataTypeTraits(NumericTypeClass, 'DoubleType'),\n\n    Type.STRING: DataTypeTraits(BaseBinaryTypeClass, 'StringType'),\n    Type.BINARY: DataTypeTraits(BaseBinaryTypeClass, 'BinaryType'),\n    Type.LARGE_STRING: DataTypeTraits(BaseBinaryTypeClass, 'LargeStringType'),\n    Type.LARGE_BINARY: DataTypeTraits(BaseBinaryTypeClass, 'LargeBinaryType'),\n\n    Type.FIXED_SIZE_BINARY: DataTypeTraits(FixedSizeBinaryTypeClass,\n                                           'FixedSizeBinaryType'),\n\n    Type.DATE32: DataTypeTraits(Date32TypeClass, 'Date32Type'),\n    Type.DATE64: DataTypeTraits(Date64TypeClass, 'Date64Type'),\n    Type.TIMESTAMP: DataTypeTraits(TimestampTypeClass, 'TimestampType'),\n    Type.TIME32: DataTypeTraits(TimeTypeClass, 'Time32Type'),\n    Type.TIME64: DataTypeTraits(TimeTypeClass, 'Time64Type'),\n    Type.DURATION: DataTypeTraits(DurationTypeClass, 'DurationType'),\n    Type.INTERVAL_MONTHS: DataTypeTraits(MonthIntervalTypeClass,\n                                         'MonthIntervalType'),\n    Type.INTERVAL_DAY_TIME: DataTypeTraits(DayTimeIntervalTypeClass,\n                                           'DayTimeIntervalType'),\n    Type.INTERVAL_MONTH_DAY_NANO: DataTypeTraits(MonthDayNanoIntervalTypeClass,\n                                                 'MonthDayNanoIntervalType'),\n\n    Type.DECIMAL128: DataTypeTraits(DecimalTypeClass, 'Decimal128Type'),\n    Type.DECIMAL256: DataTypeTraits(DecimalTypeClass, 'Decimal256Type'),\n\n    Type.LIST: DataTypeTraits(BaseListTypeClass, 'ListType'),\n    Type.LARGE_LIST: DataTypeTraits(BaseListTypeClass, 'LargeListType'),\n    Type.FIXED_SIZE_LIST: DataTypeTraits(FixedSizeListTypeClass,\n                                         'FixedSizeListType'),\n    Type.MAP: DataTypeTraits(MapTypeClass, 'MapType'),\n\n    Type.STRUCT: DataTypeTraits(StructTypeClass, 'StructType'),\n    Type.SPARSE_UNION: DataTypeTraits(SparseUnionTypeClass, 'SparseUnionType'),\n    Type.DENSE_UNION: DataTypeTraits(DenseUnionTypeClass, 'DenseUnionType'),\n\n    Type.DICTIONARY: DataTypeTraits(DictionaryTypeClass, 'DictionaryType'),\n    Type.EXTENSION: DataTypeTraits(ExtensionTypeClass, 'ExtensionType'),\n}\n\nmax_type_id = len(type_traits_by_id) - 1\n\n\ndef lookup_type_class(type_id):\n    \"\"\"\n    Lookup a type class (an instance of DataTypeClass) by its type id.\n    \"\"\"\n    traits = type_traits_by_id.get(type_id)\n    if traits is not None:\n        return traits.factory(traits.name)\n    return None\n\n\nclass StatusPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Status.\n    \"\"\"\n    _status_codes_by_id = {\n        0: 'OK',\n        1: 'OutOfMemory',\n        2: 'KeyError',\n        3: 'TypeError',\n        4: 'Invalid',\n        5: 'IOError',\n        6: 'CapacityError',\n        7: 'IndexError',\n        8: 'Cancelled',\n        9: 'UnknownError',\n        10: 'NotImplemented',\n        11: 'SerializationError',\n        13: 'RError',\n        40: 'CodeGenError',\n        41: 'ExpressionValidationError',\n        42: 'ExecutionError',\n        45: 'AlreadyExists',\n    }\n\n    def __init__(self, name, val):\n        self.val = val\n\n    def _format_detail(self, state):\n        detail_ptr = SharedPtr(state['detail']).get()\n        if int(detail_ptr) == 0:\n            return None\n        detail_id = CString(gdb.parse_and_eval(\n            f\"{for_evaluation(detail_ptr)}->type_id()\"))\n        # Cannot use StdString as ToString() returns a rvalue\n        detail_msg = CString(gdb.parse_and_eval(\n            f\"{for_evaluation(detail_ptr)}->ToString().c_str()\"))\n        return f\"[{detail_id.string()}] {detail_msg.string_literal()}\"\n\n    def _format_error(self, state):\n        code = int(state['code'])\n        codename = self._status_codes_by_id.get(code)\n        if codename is not None:\n            s = f\"arrow::Status::{codename}(\"\n        else:\n            s = f\"arrow::Status(<unknown code {code}>, \"\n        s += StdString(state['msg']).string_literal()\n        detail_msg = self._format_detail(state)\n        if detail_msg is not None:\n            return s + f\", detail={detail_msg})\"\n        else:\n            return s + \")\"\n\n    def to_string(self):\n        state_ptr = self.val['state_']\n        if int(state_ptr) == 0:\n            return \"arrow::Status::OK()\"\n        return self._format_error(state_ptr.dereference())\n\n\nclass ResultPrinter(StatusPrinter):\n    \"\"\"\n    Pretty-printer for arrow::Result<T>.\n    \"\"\"\n\n    def to_string(self):\n        data_type = self.val.type.template_argument(0)\n        state_ptr = self.val['status_']['state_']\n        if int(state_ptr) != 0:\n            inner = self._format_error(state_ptr)\n        else:\n            data_ptr = self.val['storage_']['data_'].address\n            assert data_ptr\n            inner = data_ptr.reinterpret_cast(\n                data_type.pointer()).dereference()\n        return f\"arrow::Result<{data_type}>({inner})\"\n\n\nclass FieldPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Field.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n\n    def to_string(self):\n        f = Field(self.val)\n        nullable = f.nullable\n        if nullable:\n            return f'arrow::field({f.name}, {f.type})'\n        else:\n            return f'arrow::field({f.name}, {f.type}, nullable=false)'\n\n\nclass MetadataPrinter:\n    \"\"\"\n    Pretty-printer for arrow::KeyValueMetadata.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n        self.metadata = Metadata(self.val)\n\n    def display_hint(self):\n        return 'map'\n\n    def children(self):\n        for k, v in self.metadata:\n            yield (\"key\", k.bytes_literal())\n            yield (\"value\", v.bytes_literal())\n\n    def to_string(self):\n        return f\"arrow::KeyValueMetadata of size {len(self.metadata)}\"\n\n\nclass SchemaPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Schema.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n        self.schema = Schema(val)\n        # TODO endianness\n\n    def display_hint(self):\n        return 'map'\n\n    def children(self):\n        for field in self.schema.fields:\n            yield (\"name\", field.name.string_literal())\n            yield (\"type\", field.type)\n\n    def to_string(self):\n        num_fields = len(self.schema.fields)\n        md_items = len(self.schema.metadata)\n        if md_items > 0:\n            return (f\"arrow::Schema with {num_fields} fields \"\n                    f\"and {md_items} metadata items\")\n        else:\n            return f\"arrow::Schema with {num_fields} fields\"\n\n\nclass BaseColumnarPrinter:\n\n    def __init__(self, name, val, columnar):\n        self.name = name\n        self.val = val\n        self.columnar = columnar\n        self.schema = self.columnar.schema\n\n    def display_hint(self):\n        return 'map'\n\n    def children(self):\n        for field, col in zip(self.schema.fields,\n                              self.columnar.columns):\n            yield (\"name\", field.name.string_literal())\n            yield (\"value\", col)\n\n    def to_string(self):\n        num_fields = len(self.schema.fields)\n        num_rows = self.columnar.num_rows\n        md_items = len(self.schema.metadata)\n        if md_items > 0:\n            return (f\"arrow::{self.name} with {num_fields} columns, \"\n                    f\"{num_rows} rows, {md_items} metadata items\")\n        else:\n            return (f\"arrow::{self.name} with {num_fields} columns, \"\n                    f\"{num_rows} rows\")\n\n\nclass RecordBatchPrinter(BaseColumnarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::RecordBatch.\n    \"\"\"\n\n    def __init__(self, name, val):\n        BaseColumnarPrinter.__init__(self, \"RecordBatch\", val, RecordBatch(val))\n\n\nclass TablePrinter(BaseColumnarPrinter):\n    \"\"\"\n    Pretty-printer for arrow::Table.\n    \"\"\"\n\n    def __init__(self, name, val):\n        BaseColumnarPrinter.__init__(self, \"Table\", val, Table(val))\n\n\nclass DatumPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Datum.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n        self.variant = Variant(val['value'])\n\n    def to_string(self):\n        if self.variant.index == 0:\n            # Datum::NONE\n            return \"arrow::Datum (empty)\"\n        if self.variant.value_type is None:\n            return \"arrow::Datum (uninitialized or corrupt?)\"\n        # All non-empty Datums contain a shared_ptr<T>\n        value = deref(self.variant.value)\n        return f\"arrow::Datum of value {value}\"\n\n\nclass BufferPrinter:\n    \"\"\"\n    Pretty-printer for arrow::Buffer and subclasses.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.name = name\n        self.val = val\n\n    def to_string(self):\n        if bool(self.val['is_mutable_']):\n            mutable = 'mutable'\n        else:\n            mutable = 'read-only'\n        size = int(self.val['size_'])\n        if size == 0:\n            return f\"arrow::{self.name} of size 0, {mutable}\"\n        if not self.val['is_cpu_']:\n            return f\"arrow::{self.name} of size {size}, {mutable}, not on CPU\"\n        data = bytes_literal(self.val['data_'], size)\n        return f\"arrow::{self.name} of size {size}, {mutable}, {data}\"\n\n\nclass DayMillisecondsPrinter:\n    \"\"\"\n    Pretty-printer for arrow::DayTimeIntervalType::DayMilliseconds.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n\n    def to_string(self):\n        return format_days_milliseconds(self.val['days'],\n                                        self.val['milliseconds'])\n\n\nclass MonthDayNanosPrinter:\n    \"\"\"\n    Pretty-printer for arrow::MonthDayNanoIntervalType::MonthDayNanos.\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.val = val\n\n    def to_string(self):\n        return format_months_days_nanos(self.val['months'],\n                                        self.val['days'],\n                                        self.val['nanoseconds'])\n\n\nclass DecimalPrinter:\n    \"\"\"\n    Pretty-printer for Arrow decimal values.\n    \"\"\"\n\n    def __init__(self, bit_width, name, val):\n        self.name = name\n        self.val = val\n        self.bit_width = bit_width\n\n    def to_string(self):\n        dec = decimal_bits_to_class[self.bit_width].from_value(self.val)\n        return f\"{self.name}({int(dec)})\"\n\n\nprinters = {\n    \"arrow::ArrayData\": ArrayDataPrinter,\n    \"arrow::BasicDecimal128\": partial(DecimalPrinter, 128),\n    \"arrow::BasicDecimal256\": partial(DecimalPrinter, 256),\n    \"arrow::ChunkedArray\": ChunkedArrayPrinter,\n    \"arrow::Datum\": DatumPrinter,\n    \"arrow::DayTimeIntervalType::DayMilliseconds\": DayMillisecondsPrinter,\n    \"arrow::Decimal128\": partial(DecimalPrinter, 128),\n    \"arrow::Decimal256\": partial(DecimalPrinter, 256),\n    \"arrow::MonthDayNanoIntervalType::MonthDayNanos\": MonthDayNanosPrinter,\n    \"arrow::Field\": FieldPrinter,\n    \"arrow::KeyValueMetadata\": MetadataPrinter,\n    \"arrow::RecordBatch\": RecordBatchPrinter,\n    \"arrow::Result\": ResultPrinter,\n    \"arrow::Schema\": SchemaPrinter,\n    \"arrow::SimpleRecordBatch\": RecordBatchPrinter,\n    \"arrow::SimpleTable\": TablePrinter,\n    \"arrow::Status\": StatusPrinter,\n    \"arrow::Table\": TablePrinter,\n}\n\n\ndef arrow_pretty_print(val):\n    name = val.type.strip_typedefs().name\n    if name is None:\n        return\n    name = name.partition('<')[0]  # Remove template parameters\n    printer = printers.get(name)\n    if printer is not None:\n        return printer(name, val)\n\n    if not name.startswith(\"arrow::\"):\n        return\n    arrow_name = name[len(\"arrow::\"):]\n\n    if arrow_name.endswith(\"Buffer\"):\n        try:\n            val['data_']\n        except Exception:\n            # Not a Buffer?\n            pass\n        else:\n            return BufferPrinter(arrow_name, val)\n\n    elif arrow_name.endswith(\"Type\"):\n        # Look up dynamic type, as it may be hidden behind a DataTypeClass\n        # pointer or reference.\n        try:\n            type_id = int(val['id_'])\n        except Exception:\n            # Not a DataTypeClass?\n            pass\n        else:\n            type_class = lookup_type_class(type_id)\n            if type_class is not None:\n                return type_class.type_printer(type_class.name, val)\n\n    elif arrow_name.endswith(\"Array\"):\n        return ArrayPrinter(val)\n\n    elif arrow_name.endswith(\"Scalar\"):\n        try:\n            val['is_valid']\n        except Exception:\n            # Not a Scalar?\n            pass\n        else:\n            return ScalarPrinter(val)\n\n\ndef main():\n    # This pattern allows for two modes of use:\n    # - manual loading using `source gdb-arrow.py`: current_objfile()\n    #   will be None;\n    # - automatic loading from the GDB `scripts-directory`: current_objfile()\n    #   will be tied to the inferior being debugged.\n    objfile = gdb.current_objfile()\n    if objfile is None:\n        objfile = gdb\n\n    objfile.pretty_printers.append(arrow_pretty_print)\n\n\nif __name__ == '__main__':\n    main()\n", "cpp/tools/binary_symbol_explore.py": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport subprocess as sbp\nimport sys\n\ntry:\n    import pandas as pd\n    HAVE_PANDAS = True\nexcept ImportError:\n    HAVE_PANDAS = False\n\nSYMBOL_FILTERS = {\n    'std::chrono::duration': 'duration',\n    'std::__cxx11::basic_string': 'std::string',\n    'arrow::ArrayData': 'ArrayData',\n    'arrow::ArraySpan': 'ArraySpan',\n    'arrow::Datum': 'Datum',\n    'arrow::Scalar': 'Scalar',\n    'arrow::Status': 'Status',\n    'arrow::Type': 'Type',\n    'arrow::TimestampType': 'TsT',\n    'arrow::BinaryType': 'BinaryT',\n    'arrow::BooleanType': 'BoolT',\n    'arrow::StringType': 'StringT',\n    'arrow::LargeStringType': 'LStringT',\n    'arrow::DoubleType': 'DoubleT',\n    'arrow::FloatType': 'FloatT',\n    'arrow::Int64Type': 'Int64T',\n    'arrow::UInt64Type': 'UInt64T',\n    'arrow::LargeListType': 'LListT',\n    'arrow::ListType': 'ListT',\n    'arrow::FixedSizeListType': 'FSLT',\n    'arrow::compute::': 'ac::',\n    'ac::internal::': '',\n    'arrow::internal::': 'ai::',\n    '(anonymous namespace)::': '',\n    'internal::applicator::': '',\n    'internal::CastFunctor': 'CastFunctor',\n    'ac::KernelContext*': 'C*',\n    'ArrayData const&': 'A&',\n    'ArraySpan const&': 'A&',\n    'ArrayData*': 'O*',\n    'Scalar const&': 'S&',\n    'Datum const&': 'V&',\n    'Datum*': 'O*',\n    'ac::ExecBatch const&': 'B&',\n    'ac::ExecSpan const&': 'B&',\n    'ac::ExecValue const&': 'V&',\n    'ac::ExecResult*': 'O*',\n    'Type::type': 'T',\n}\n\n\ndef filter_symbol(symbol_name):\n    for token, replacement in SYMBOL_FILTERS.items():\n        symbol_name = symbol_name.replace(token, replacement)\n    return symbol_name\n\n\ndef get_symbols_and_sizes(object_file):\n    cmd = f\"nm --print-size --size-sort {object_file} | c++filt\"\n    output = sbp.check_output(cmd, shell=True).decode('utf-8')\n    symbol_sizes = []\n    for x in output.split('\\n'):\n        if len(x) == 0:\n            continue\n        _, hex_size, _, symbol_name = x.split(' ', 3)\n        symbol_name = filter_symbol(symbol_name)\n        symbol_sizes.append((symbol_name, int(hex_size, 16)))\n    return dict(symbol_sizes)\n\n\nif __name__ == '__main__':\n    base, contender = sys.argv[1], sys.argv[2]\n\n    base_results = get_symbols_and_sizes(base)\n    contender_results = get_symbols_and_sizes(contender)\n\n    all_symbols = set(base_results.keys()) | set(contender_results.keys())\n\n    diff_table = []\n    for name in all_symbols:\n        if name in base_results and name in contender_results:\n            base_size = base_results[name]\n            contender_size = contender_results[name]\n        elif name in base_results:\n            base_size = base_results[name]\n            contender_size = 0\n        else:\n            base_size = 0\n            contender_size = contender_results[name]\n        diff = contender_size - base_size\n        diff_table.append((name, base_size, contender_size, diff))\n    diff_table.sort(key=lambda x: x[3])\n\n    if HAVE_PANDAS:\n        diff = pd.DataFrame.from_records(diff_table,\n                                         columns=['symbol', 'base',\n                                                  'contender', 'diff'])\n        pd.options.display.max_rows = 1000\n        pd.options.display.max_colwidth = 150\n        print(diff[diff['diff'] < - 700])\n        print(diff[diff['diff'] > 700])\n    else:\n        # TODO\n        pass\n", "cpp/build-support/run_clang_format.py": "#!/usr/bin/env python3\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import print_function\nimport lintutils\nfrom subprocess import PIPE\nimport argparse\nimport difflib\nimport multiprocessing as mp\nimport sys\nfrom functools import partial\n\n\n# examine the output of clang-format and if changes are\n# present assemble a (unified)patch of the difference\ndef _check_one_file(filename, formatted):\n    with open(filename, \"rb\") as reader:\n        original = reader.read()\n\n    if formatted != original:\n        # Run the equivalent of diff -u\n        diff = list(difflib.unified_diff(\n            original.decode('utf8').splitlines(True),\n            formatted.decode('utf8').splitlines(True),\n            fromfile=filename,\n            tofile=\"{} (after clang format)\".format(\n                filename)))\n    else:\n        diff = None\n\n    return filename, diff\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Runs clang-format on all of the source \"\n        \"files. If --fix is specified enforce format by \"\n        \"modifying in place, otherwise compare the output \"\n        \"with the existing file and output any necessary \"\n        \"changes as a patch in unified diff format\")\n    parser.add_argument(\"--clang_format_binary\",\n                        required=True,\n                        help=\"Path to the clang-format binary\")\n    parser.add_argument(\"--exclude_globs\",\n                        help=\"Filename containing globs for files \"\n                        \"that should be excluded from the checks\")\n    parser.add_argument(\"--source_dir\",\n                        required=True,\n                        action=\"append\",\n                        help=\"Root directory of the source code\")\n    parser.add_argument(\"--fix\", default=False,\n                        action=\"store_true\",\n                        help=\"If specified, will re-format the source \"\n                        \"code instead of comparing the re-formatted \"\n                        \"output, defaults to %(default)s\")\n    parser.add_argument(\"--quiet\", default=False,\n                        action=\"store_true\",\n                        help=\"If specified, only print errors\")\n    arguments = parser.parse_args()\n\n    exclude_globs = []\n    if arguments.exclude_globs:\n        with open(arguments.exclude_globs) as f:\n            exclude_globs.extend(line.strip() for line in f)\n\n    formatted_filenames = []\n    for source_dir in arguments.source_dir:\n        for path in lintutils.get_sources(source_dir, exclude_globs):\n            formatted_filenames.append(str(path))\n\n    if arguments.fix:\n        if not arguments.quiet:\n            print(\"\\n\".join(map(lambda x: \"Formatting {}\".format(x),\n                                formatted_filenames)))\n\n        # Break clang-format invocations into chunks: each invocation formats\n        # 16 files. Wait for all processes to complete\n        results = lintutils.run_parallel([\n            [arguments.clang_format_binary, \"-i\"] + some\n            for some in lintutils.chunk(formatted_filenames, 16)\n        ])\n        for returncode, stdout, stderr in results:\n            # if any clang-format reported a parse error, bubble it\n            if returncode != 0:\n                sys.exit(returncode)\n\n    else:\n        # run an instance of clang-format for each source file in parallel,\n        # then wait for all processes to complete\n        results = lintutils.run_parallel([\n            [arguments.clang_format_binary, filename]\n            for filename in formatted_filenames\n        ], stdout=PIPE, stderr=PIPE)\n\n        checker_args = []\n        for filename, res in zip(formatted_filenames, results):\n            # if any clang-format reported a parse error, bubble it\n            returncode, stdout, stderr = res\n            if returncode != 0:\n                print(stderr)\n                sys.exit(returncode)\n            checker_args.append((filename, stdout))\n\n        error = False\n        pool = mp.Pool()\n        try:\n            # check the output from each invocation of clang-format in parallel\n            for filename, diff in pool.starmap(_check_one_file, checker_args):\n                if not arguments.quiet:\n                    print(\"Checking {}\".format(filename))\n                if diff:\n                    print(\"{} had clang-format style issues\".format(filename))\n                    # Print out the diff to stderr\n                    error = True\n                    # pad with a newline\n                    print(file=sys.stderr)\n                    sys.stderr.writelines(diff)\n        except Exception:\n            error = True\n            raise\n        finally:\n            pool.terminate()\n            pool.join()\n        sys.exit(1 if error else 0)\n", "cpp/build-support/cpplint.py": "#!/usr/bin/env python3\n#\n# Copyright (c) 2009 Google Inc. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#    * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#    * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"Does google-lint on c++ files.\n\nThe goal of this script is to identify places in the code that *may*\nbe in non-compliance with google style.  It does not attempt to fix\nup these problems -- the point is to educate.  It does also not\nattempt to find all problems, or to ensure that everything it does\nfind is legitimately a problem.\n\nIn particular, we can get very confused by /* and // inside strings!\nWe do a small hack, which is to ignore //'s with \"'s after them on the\nsame line, but it is far from perfect (in either direction).\n\"\"\"\n\n# cpplint predates fstrings\n# pylint: disable=consider-using-f-string\n\n# pylint: disable=invalid-name\n\nimport codecs\nimport copy\nimport getopt\nimport glob\nimport itertools\nimport math  # for log\nimport os\nimport re\nimport sre_compile\nimport string\nimport sys\nimport sysconfig\nimport unicodedata\nimport xml.etree.ElementTree\n\n# if empty, use defaults\n_valid_extensions = set([])\n\n__VERSION__ = '1.6.1'\n\ntry:\n  #  -- pylint: disable=used-before-assignment\n  xrange          # Python 2\nexcept NameError:\n  #  -- pylint: disable=redefined-builtin\n  xrange = range  # Python 3\n\n\n_USAGE = \"\"\"\nSyntax: cpplint.py [--verbose=#] [--output=emacs|eclipse|vs7|junit|sed|gsed]\n                   [--filter=-x,+y,...]\n                   [--counting=total|toplevel|detailed] [--root=subdir]\n                   [--repository=path]\n                   [--linelength=digits] [--headers=x,y,...]\n                   [--recursive]\n                   [--exclude=path]\n                   [--extensions=hpp,cpp,...]\n                   [--includeorder=default|standardcfirst]\n                   [--quiet]\n                   [--version]\n        <file> [file] ...\n\n  Style checker for C/C++ source files.\n  This is a fork of the Google style checker with minor extensions.\n\n  The style guidelines this tries to follow are those in\n    https://google.github.io/styleguide/cppguide.html\n\n  Every problem is given a confidence score from 1-5, with 5 meaning we are\n  certain of the problem, and 1 meaning it could be a legitimate construct.\n  This will miss some errors, and is not a substitute for a code review.\n\n  To suppress false-positive errors of a certain category, add a\n  'NOLINT(category)' comment to the line.  NOLINT or NOLINT(*)\n  suppresses errors of all categories on that line.\n\n  The files passed in will be linted; at least one file must be provided.\n  Default linted extensions are %s.\n  Other file types will be ignored.\n  Change the extensions with the --extensions flag.\n\n  Flags:\n\n    output=emacs|eclipse|vs7|junit|sed|gsed\n      By default, the output is formatted to ease emacs parsing.  Visual Studio\n      compatible output (vs7) may also be used.  Further support exists for\n      eclipse (eclipse), and JUnit (junit). XML parsers such as those used\n      in Jenkins and Bamboo may also be used.\n      The sed format outputs sed commands that should fix some of the errors.\n      Note that this requires gnu sed. If that is installed as gsed on your\n      system (common e.g. on macOS with homebrew) you can use the gsed output\n      format. Sed commands are written to stdout, not stderr, so you should be\n      able to pipe output straight to a shell to run the fixes.\n\n    verbose=#\n      Specify a number 0-5 to restrict errors to certain verbosity levels.\n      Errors with lower verbosity levels have lower confidence and are more\n      likely to be false positives.\n\n    quiet\n      Don't print anything if no errors are found.\n\n    filter=-x,+y,...\n      Specify a comma-separated list of category-filters to apply: only\n      error messages whose category names pass the filters will be printed.\n      (Category names are printed with the message and look like\n      \"[whitespace/indent]\".)  Filters are evaluated left to right.\n      \"-FOO\" means \"do not print categories that start with FOO\".\n      \"+FOO\" means \"do print categories that start with FOO\".\n\n      Examples: --filter=-whitespace,+whitespace/braces\n                --filter=-whitespace,-runtime/printf,+runtime/printf_format\n                --filter=-,+build/include_what_you_use\n\n      To see a list of all the categories used in cpplint, pass no arg:\n         --filter=\n\n    counting=total|toplevel|detailed\n      The total number of errors found is always printed. If\n      'toplevel' is provided, then the count of errors in each of\n      the top-level categories like 'build' and 'whitespace' will\n      also be printed. If 'detailed' is provided, then a count\n      is provided for each category like 'build/class'.\n\n    repository=path\n      The top level directory of the repository, used to derive the header\n      guard CPP variable. By default, this is determined by searching for a\n      path that contains .git, .hg, or .svn. When this flag is specified, the\n      given path is used instead. This option allows the header guard CPP\n      variable to remain consistent even if members of a team have different\n      repository root directories (such as when checking out a subdirectory\n      with SVN). In addition, users of non-mainstream version control systems\n      can use this flag to ensure readable header guard CPP variables.\n\n      Examples:\n        Assuming that Alice checks out ProjectName and Bob checks out\n        ProjectName/trunk and trunk contains src/chrome/ui/browser.h, then\n        with no --repository flag, the header guard CPP variable will be:\n\n        Alice => TRUNK_SRC_CHROME_BROWSER_UI_BROWSER_H_\n        Bob   => SRC_CHROME_BROWSER_UI_BROWSER_H_\n\n        If Alice uses the --repository=trunk flag and Bob omits the flag or\n        uses --repository=. then the header guard CPP variable will be:\n\n        Alice => SRC_CHROME_BROWSER_UI_BROWSER_H_\n        Bob   => SRC_CHROME_BROWSER_UI_BROWSER_H_\n\n    root=subdir\n      The root directory used for deriving header guard CPP variable.\n      This directory is relative to the top level directory of the repository\n      which by default is determined by searching for a directory that contains\n      .git, .hg, or .svn but can also be controlled with the --repository flag.\n      If the specified directory does not exist, this flag is ignored.\n\n      Examples:\n        Assuming that src is the top level directory of the repository (and\n        cwd=top/src), the header guard CPP variables for\n        src/chrome/browser/ui/browser.h are:\n\n        No flag => CHROME_BROWSER_UI_BROWSER_H_\n        --root=chrome => BROWSER_UI_BROWSER_H_\n        --root=chrome/browser => UI_BROWSER_H_\n        --root=.. => SRC_CHROME_BROWSER_UI_BROWSER_H_\n\n    linelength=digits\n      This is the allowed line length for the project. The default value is\n      80 characters.\n\n      Examples:\n        --linelength=120\n\n    recursive\n      Search for files to lint recursively. Each directory given in the list\n      of files to be linted is replaced by all files that descend from that\n      directory. Files with extensions not in the valid extensions list are\n      excluded.\n\n    exclude=path\n      Exclude the given path from the list of files to be linted. Relative\n      paths are evaluated relative to the current directory and shell globbing\n      is performed. This flag can be provided multiple times to exclude\n      multiple files.\n\n      Examples:\n        --exclude=one.cc\n        --exclude=src/*.cc\n        --exclude=src/*.cc --exclude=test/*.cc\n\n    extensions=extension,extension,...\n      The allowed file extensions that cpplint will check\n\n      Examples:\n        --extensions=%s\n\n    includeorder=default|standardcfirst\n      For the build/include_order rule, the default is to blindly assume angle\n      bracket includes with file extension are c-system-headers (default),\n      even knowing this will have false classifications.\n      The default is established at google.\n      standardcfirst means to instead use an allow-list of known c headers and\n      treat all others as separate group of \"other system headers\". The C headers\n      included are those of the C-standard lib and closely related ones.\n\n    headers=x,y,...\n      The header extensions that cpplint will treat as .h in checks. Values are\n      automatically added to --extensions list.\n     (by default, only files with extensions %s will be assumed to be headers)\n\n      Examples:\n        --headers=%s\n        --headers=hpp,hxx\n        --headers=hpp\n\n    cpplint.py supports per-directory configurations specified in CPPLINT.cfg\n    files. CPPLINT.cfg file can contain a number of key=value pairs.\n    Currently the following options are supported:\n\n      set noparent\n      filter=+filter1,-filter2,...\n      exclude_files=regex\n      linelength=80\n      root=subdir\n      headers=x,y,...\n\n    \"set noparent\" option prevents cpplint from traversing directory tree\n    upwards looking for more .cfg files in parent directories. This option\n    is usually placed in the top-level project directory.\n\n    The \"filter\" option is similar in function to --filter flag. It specifies\n    message filters in addition to the |_DEFAULT_FILTERS| and those specified\n    through --filter command-line flag.\n\n    \"exclude_files\" allows to specify a regular expression to be matched against\n    a file name. If the expression matches, the file is skipped and not run\n    through the linter.\n\n    \"linelength\" allows to specify the allowed line length for the project.\n\n    The \"root\" option is similar in function to the --root flag (see example\n    above). Paths are relative to the directory of the CPPLINT.cfg.\n\n    The \"headers\" option is similar in function to the --headers flag\n    (see example above).\n\n    CPPLINT.cfg has an effect on files in the same directory and all\n    sub-directories, unless overridden by a nested configuration file.\n\n      Example file:\n        filter=-build/include_order,+build/include_alpha\n        exclude_files=.*\\\\.cc\n\n    The above example disables build/include_order warning and enables\n    build/include_alpha as well as excludes all .cc from being\n    processed by linter, in the current directory (where the .cfg\n    file is located) and all sub-directories.\n\"\"\"\n\n# We categorize each error message we print.  Here are the categories.\n# We want an explicit list so we can list them all in cpplint --filter=.\n# If you add a new error message with a new category, add it to the list\n# here!  cpplint_unittest.py should tell you if you forget to do this.\n_ERROR_CATEGORIES = [\n    'build/class',\n    'build/c++11',\n    'build/c++14',\n    'build/c++tr1',\n    'build/deprecated',\n    'build/endif_comment',\n    'build/explicit_make_pair',\n    'build/forward_decl',\n    'build/header_guard',\n    'build/include',\n    'build/include_subdir',\n    'build/include_alpha',\n    'build/include_order',\n    'build/include_what_you_use',\n    'build/namespaces_headers',\n    'build/namespaces_literals',\n    'build/namespaces',\n    'build/printf_format',\n    'build/storage_class',\n    'legal/copyright',\n    'readability/alt_tokens',\n    'readability/braces',\n    'readability/casting',\n    'readability/check',\n    'readability/constructors',\n    'readability/fn_size',\n    'readability/inheritance',\n    'readability/multiline_comment',\n    'readability/multiline_string',\n    'readability/namespace',\n    'readability/nolint',\n    'readability/nul',\n    'readability/strings',\n    'readability/todo',\n    'readability/utf8',\n    'runtime/arrays',\n    'runtime/casting',\n    'runtime/explicit',\n    'runtime/int',\n    'runtime/init',\n    'runtime/invalid_increment',\n    'runtime/member_string_references',\n    'runtime/memset',\n    'runtime/indentation_namespace',\n    'runtime/operator',\n    'runtime/printf',\n    'runtime/printf_format',\n    'runtime/references',\n    'runtime/string',\n    'runtime/threadsafe_fn',\n    'runtime/vlog',\n    'whitespace/blank_line',\n    'whitespace/braces',\n    'whitespace/comma',\n    'whitespace/comments',\n    'whitespace/empty_conditional_body',\n    'whitespace/empty_if_body',\n    'whitespace/empty_loop_body',\n    'whitespace/end_of_line',\n    'whitespace/ending_newline',\n    'whitespace/forcolon',\n    'whitespace/indent',\n    'whitespace/line_length',\n    'whitespace/newline',\n    'whitespace/operators',\n    'whitespace/parens',\n    'whitespace/semicolon',\n    'whitespace/tab',\n    'whitespace/todo',\n    ]\n\n# keywords to use with --outputs which generate stdout for machine processing\n_MACHINE_OUTPUTS = [\n  'junit',\n  'sed',\n  'gsed'\n]\n\n# These error categories are no longer enforced by cpplint, but for backwards-\n# compatibility they may still appear in NOLINT comments.\n_LEGACY_ERROR_CATEGORIES = [\n    'readability/streams',\n    'readability/function',\n    ]\n\n# These prefixes for categories should be ignored since they relate to other\n# tools which also use the NOLINT syntax, e.g. clang-tidy.\n_OTHER_NOLINT_CATEGORY_PREFIXES = [\n    'clang-analyzer',\n    ]\n\n# The default state of the category filter. This is overridden by the --filter=\n# flag. By default all errors are on, so only add here categories that should be\n# off by default (i.e., categories that must be enabled by the --filter= flags).\n# All entries here should start with a '-' or '+', as in the --filter= flag.\n_DEFAULT_FILTERS = ['-build/include_alpha']\n\n# The default list of categories suppressed for C (not C++) files.\n_DEFAULT_C_SUPPRESSED_CATEGORIES = [\n    'readability/casting',\n    ]\n\n# The default list of categories suppressed for Linux Kernel files.\n_DEFAULT_KERNEL_SUPPRESSED_CATEGORIES = [\n    'whitespace/tab',\n    ]\n\n# We used to check for high-bit characters, but after much discussion we\n# decided those were OK, as long as they were in UTF-8 and didn't represent\n# hard-coded international strings, which belong in a separate i18n file.\n\n# C++ headers\n_CPP_HEADERS = frozenset([\n    # Legacy\n    'algobase.h',\n    'algo.h',\n    'alloc.h',\n    'builtinbuf.h',\n    'bvector.h',\n    'complex.h',\n    'defalloc.h',\n    'deque.h',\n    'editbuf.h',\n    'fstream.h',\n    'function.h',\n    'hash_map',\n    'hash_map.h',\n    'hash_set',\n    'hash_set.h',\n    'hashtable.h',\n    'heap.h',\n    'indstream.h',\n    'iomanip.h',\n    'iostream.h',\n    'istream.h',\n    'iterator.h',\n    'list.h',\n    'map.h',\n    'multimap.h',\n    'multiset.h',\n    'ostream.h',\n    'pair.h',\n    'parsestream.h',\n    'pfstream.h',\n    'procbuf.h',\n    'pthread_alloc',\n    'pthread_alloc.h',\n    'rope',\n    'rope.h',\n    'ropeimpl.h',\n    'set.h',\n    'slist',\n    'slist.h',\n    'stack.h',\n    'stdiostream.h',\n    'stl_alloc.h',\n    'stl_relops.h',\n    'streambuf.h',\n    'stream.h',\n    'strfile.h',\n    'strstream.h',\n    'tempbuf.h',\n    'tree.h',\n    'type_traits.h',\n    'vector.h',\n    # 17.6.1.2 C++ library headers\n    'algorithm',\n    'array',\n    'atomic',\n    'bitset',\n    'chrono',\n    'codecvt',\n    'complex',\n    'condition_variable',\n    'deque',\n    'exception',\n    'forward_list',\n    'fstream',\n    'functional',\n    'future',\n    'initializer_list',\n    'iomanip',\n    'ios',\n    'iosfwd',\n    'iostream',\n    'istream',\n    'iterator',\n    'limits',\n    'list',\n    'locale',\n    'map',\n    'memory',\n    'mutex',\n    'new',\n    'numeric',\n    'ostream',\n    'queue',\n    'random',\n    'ratio',\n    'regex',\n    'scoped_allocator',\n    'set',\n    'sstream',\n    'stack',\n    'stdexcept',\n    'streambuf',\n    'string',\n    'strstream',\n    'system_error',\n    'thread',\n    'tuple',\n    'typeindex',\n    'typeinfo',\n    'type_traits',\n    'unordered_map',\n    'unordered_set',\n    'utility',\n    'valarray',\n    'vector',\n    # 17.6.1.2 C++14 headers\n    'shared_mutex',\n    # 17.6.1.2 C++17 headers\n    'any',\n    'charconv',\n    'codecvt',\n    'execution',\n    'filesystem',\n    'memory_resource',\n    'optional',\n    'string_view',\n    'variant',\n    # 17.6.1.2 C++ headers for C library facilities\n    'cassert',\n    'ccomplex',\n    'cctype',\n    'cerrno',\n    'cfenv',\n    'cfloat',\n    'cinttypes',\n    'ciso646',\n    'climits',\n    'clocale',\n    'cmath',\n    'csetjmp',\n    'csignal',\n    'cstdalign',\n    'cstdarg',\n    'cstdbool',\n    'cstddef',\n    'cstdint',\n    'cstdio',\n    'cstdlib',\n    'cstring',\n    'ctgmath',\n    'ctime',\n    'cuchar',\n    'cwchar',\n    'cwctype',\n    ])\n\n# C headers\n_C_HEADERS = frozenset([\n    # System C headers\n    'assert.h',\n    'complex.h',\n    'ctype.h',\n    'errno.h',\n    'fenv.h',\n    'float.h',\n    'inttypes.h',\n    'iso646.h',\n    'limits.h',\n    'locale.h',\n    'math.h',\n    'setjmp.h',\n    'signal.h',\n    'stdalign.h',\n    'stdarg.h',\n    'stdatomic.h',\n    'stdbool.h',\n    'stddef.h',\n    'stdint.h',\n    'stdio.h',\n    'stdlib.h',\n    'stdnoreturn.h',\n    'string.h',\n    'tgmath.h',\n    'threads.h',\n    'time.h',\n    'uchar.h',\n    'wchar.h',\n    'wctype.h',\n    # additional POSIX C headers\n    'aio.h',\n    'arpa/inet.h',\n    'cpio.h',\n    'dirent.h',\n    'dlfcn.h',\n    'fcntl.h',\n    'fmtmsg.h',\n    'fnmatch.h',\n    'ftw.h',\n    'glob.h',\n    'grp.h',\n    'iconv.h',\n    'langinfo.h',\n    'libgen.h',\n    'monetary.h',\n    'mqueue.h',\n    'ndbm.h',\n    'net/if.h',\n    'netdb.h',\n    'netinet/in.h',\n    'netinet/tcp.h',\n    'nl_types.h',\n    'poll.h',\n    'pthread.h',\n    'pwd.h',\n    'regex.h',\n    'sched.h',\n    'search.h',\n    'semaphore.h',\n    'setjmp.h',\n    'signal.h',\n    'spawn.h',\n    'strings.h',\n    'stropts.h',\n    'syslog.h',\n    'tar.h',\n    'termios.h',\n    'trace.h',\n    'ulimit.h',\n    'unistd.h',\n    'utime.h',\n    'utmpx.h',\n    'wordexp.h',\n    # additional GNUlib headers\n    'a.out.h',\n    'aliases.h',\n    'alloca.h',\n    'ar.h',\n    'argp.h',\n    'argz.h',\n    'byteswap.h',\n    'crypt.h',\n    'endian.h',\n    'envz.h',\n    'err.h',\n    'error.h',\n    'execinfo.h',\n    'fpu_control.h',\n    'fstab.h',\n    'fts.h',\n    'getopt.h',\n    'gshadow.h',\n    'ieee754.h',\n    'ifaddrs.h',\n    'libintl.h',\n    'mcheck.h',\n    'mntent.h',\n    'obstack.h',\n    'paths.h',\n    'printf.h',\n    'pty.h',\n    'resolv.h',\n    'shadow.h',\n    'sysexits.h',\n    'ttyent.h',\n    # Additional linux glibc headers\n    'dlfcn.h',\n    'elf.h',\n    'features.h',\n    'gconv.h',\n    'gnu-versions.h',\n    'lastlog.h',\n    'libio.h',\n    'link.h',\n    'malloc.h',\n    'memory.h',\n    'netash/ash.h',\n    'netatalk/at.h',\n    'netax25/ax25.h',\n    'neteconet/ec.h',\n    'netipx/ipx.h',\n    'netiucv/iucv.h',\n    'netpacket/packet.h',\n    'netrom/netrom.h',\n    'netrose/rose.h',\n    'nfs/nfs.h',\n    'nl_types.h',\n    'nss.h',\n    're_comp.h',\n    'regexp.h',\n    'sched.h',\n    'sgtty.h',\n    'stab.h',\n    'stdc-predef.h',\n    'stdio_ext.h',\n    'syscall.h',\n    'termio.h',\n    'thread_db.h',\n    'ucontext.h',\n    'ustat.h',\n    'utmp.h',\n    'values.h',\n    'wait.h',\n    'xlocale.h',\n    # Hardware specific headers\n    'arm_neon.h',\n    'emmintrin.h',\n    'xmmintin.h',\n    ])\n\n# Folders of C libraries so commonly used in C++,\n# that they have parity with standard C libraries.\nC_STANDARD_HEADER_FOLDERS = frozenset([\n    # standard C library\n    \"sys\",\n    # glibc for linux\n    \"arpa\",\n    \"asm-generic\",\n    \"bits\",\n    \"gnu\",\n    \"net\",\n    \"netinet\",\n    \"protocols\",\n    \"rpc\",\n    \"rpcsvc\",\n    \"scsi\",\n    # linux kernel header\n    \"drm\",\n    \"linux\",\n    \"misc\",\n    \"mtd\",\n    \"rdma\",\n    \"sound\",\n    \"video\",\n    \"xen\",\n  ])\n\n# Type names\n_TYPES = re.compile(\n    r'^(?:'\n    # [dcl.type.simple]\n    r'(char(16_t|32_t)?)|wchar_t|'\n    r'bool|short|int|long|signed|unsigned|float|double|'\n    # [support.types]\n    r'(ptrdiff_t|size_t|max_align_t|nullptr_t)|'\n    # [cstdint.syn]\n    r'(u?int(_fast|_least)?(8|16|32|64)_t)|'\n    r'(u?int(max|ptr)_t)|'\n    r')$')\n\n\n# These headers are excluded from [build/include] and [build/include_order]\n# checks:\n# - Anything not following google file name conventions (containing an\n#   uppercase character, such as Python.h or nsStringAPI.h, for example).\n# - Lua headers.\n_THIRD_PARTY_HEADERS_PATTERN = re.compile(\n    r'^(?:[^/]*[A-Z][^/]*\\.h|lua\\.h|lauxlib\\.h|lualib\\.h)$')\n\n# Pattern for matching FileInfo.BaseName() against test file name\n_test_suffixes = ['_test', '_regtest', '_unittest']\n_TEST_FILE_SUFFIX = '(' + '|'.join(_test_suffixes) + r')$'\n\n# Pattern that matches only complete whitespace, possibly across multiple lines.\n_EMPTY_CONDITIONAL_BODY_PATTERN = re.compile(r'^\\s*$', re.DOTALL)\n\n# Assertion macros.  These are defined in base/logging.h and\n# testing/base/public/gunit.h.\n_CHECK_MACROS = [\n    'DCHECK', 'CHECK',\n    'EXPECT_TRUE', 'ASSERT_TRUE',\n    'EXPECT_FALSE', 'ASSERT_FALSE',\n    ]\n\n# Replacement macros for CHECK/DCHECK/EXPECT_TRUE/EXPECT_FALSE\n_CHECK_REPLACEMENT = dict([(macro_var, {}) for macro_var in _CHECK_MACROS])\n\nfor op, replacement in [('==', 'EQ'), ('!=', 'NE'),\n                        ('>=', 'GE'), ('>', 'GT'),\n                        ('<=', 'LE'), ('<', 'LT')]:\n  _CHECK_REPLACEMENT['DCHECK'][op] = 'DCHECK_%s' % replacement\n  _CHECK_REPLACEMENT['CHECK'][op] = 'CHECK_%s' % replacement\n  _CHECK_REPLACEMENT['EXPECT_TRUE'][op] = 'EXPECT_%s' % replacement\n  _CHECK_REPLACEMENT['ASSERT_TRUE'][op] = 'ASSERT_%s' % replacement\n\nfor op, inv_replacement in [('==', 'NE'), ('!=', 'EQ'),\n                            ('>=', 'LT'), ('>', 'LE'),\n                            ('<=', 'GT'), ('<', 'GE')]:\n  _CHECK_REPLACEMENT['EXPECT_FALSE'][op] = 'EXPECT_%s' % inv_replacement\n  _CHECK_REPLACEMENT['ASSERT_FALSE'][op] = 'ASSERT_%s' % inv_replacement\n\n# Alternative tokens and their replacements.  For full list, see section 2.5\n# Alternative tokens [lex.digraph] in the C++ standard.\n#\n# Digraphs (such as '%:') are not included here since it's a mess to\n# match those on a word boundary.\n_ALT_TOKEN_REPLACEMENT = {\n    'and': '&&',\n    'bitor': '|',\n    'or': '||',\n    'xor': '^',\n    'compl': '~',\n    'bitand': '&',\n    'and_eq': '&=',\n    'or_eq': '|=',\n    'xor_eq': '^=',\n    'not': '!',\n    'not_eq': '!='\n    }\n\n# Compile regular expression that matches all the above keywords.  The \"[ =()]\"\n# bit is meant to avoid matching these keywords outside of boolean expressions.\n#\n# False positives include C-style multi-line comments and multi-line strings\n# but those have always been troublesome for cpplint.\n_ALT_TOKEN_REPLACEMENT_PATTERN = re.compile(\n    r'[ =()](' + ('|'.join(_ALT_TOKEN_REPLACEMENT.keys())) + r')(?=[ (]|$)')\n\n\n# These constants define types of headers for use with\n# _IncludeState.CheckNextIncludeOrder().\n_C_SYS_HEADER = 1\n_CPP_SYS_HEADER = 2\n_OTHER_SYS_HEADER = 3\n_LIKELY_MY_HEADER = 4\n_POSSIBLE_MY_HEADER = 5\n_OTHER_HEADER = 6\n\n# These constants define the current inline assembly state\n_NO_ASM = 0       # Outside of inline assembly block\n_INSIDE_ASM = 1   # Inside inline assembly block\n_END_ASM = 2      # Last line of inline assembly block\n_BLOCK_ASM = 3    # The whole block is an inline assembly block\n\n# Match start of assembly blocks\n_MATCH_ASM = re.compile(r'^\\s*(?:asm|_asm|__asm|__asm__)'\n                        r'(?:\\s+(volatile|__volatile__))?'\n                        r'\\s*[{(]')\n\n# Match strings that indicate we're working on a C (not C++) file.\n_SEARCH_C_FILE = re.compile(r'\\b(?:LINT_C_FILE|'\n                            r'vim?:\\s*.*(\\s*|:)filetype=c(\\s*|:|$))')\n\n# Match string that indicates we're working on a Linux Kernel file.\n_SEARCH_KERNEL_FILE = re.compile(r'\\b(?:LINT_KERNEL_FILE)')\n\n# Commands for sed to fix the problem\n_SED_FIXUPS = {\n  'Remove spaces around =': r's/ = /=/',\n  'Remove spaces around !=': r's/ != /!=/',\n  'Remove space before ( in if (': r's/if (/if(/',\n  'Remove space before ( in for (': r's/for (/for(/',\n  'Remove space before ( in while (': r's/while (/while(/',\n  'Remove space before ( in switch (': r's/switch (/switch(/',\n  'Should have a space between // and comment': r's/\\/\\//\\/\\/ /',\n  'Missing space before {': r's/\\([^ ]\\){/\\1 {/',\n  'Tab found, replace by spaces': r's/\\t/  /g',\n  'Line ends in whitespace.  Consider deleting these extra spaces.': r's/\\s*$//',\n  'You don\\'t need a ; after a }': r's/};/}/',\n  'Missing space after ,': r's/,\\([^ ]\\)/, \\1/g',\n}\n\n_regexp_compile_cache = {}\n\n# {str, set(int)}: a map from error categories to sets of linenumbers\n# on which those errors are expected and should be suppressed.\n_error_suppressions = {}\n\n# The root directory used for deriving header guard CPP variable.\n# This is set by --root flag.\n_root = None\n_root_debug = False\n\n# The top level repository directory. If set, _root is calculated relative to\n# this directory instead of the directory containing version control artifacts.\n# This is set by the --repository flag.\n_repository = None\n\n# Files to exclude from linting. This is set by the --exclude flag.\n_excludes = None\n\n# Whether to suppress all PrintInfo messages, UNRELATED to --quiet flag\n_quiet = False\n\n# The allowed line length of files.\n# This is set by --linelength flag.\n_line_length = 80\n\n# This allows to use different include order rule than default\n_include_order = \"default\"\n\ntry:\n  #  -- pylint: disable=used-before-assignment\n  unicode\nexcept NameError:\n  #  -- pylint: disable=redefined-builtin\n  basestring = unicode = str\n\ntry:\n  #  -- pylint: disable=used-before-assignment\n  long\nexcept NameError:\n  #  -- pylint: disable=redefined-builtin\n  long = int\n\nif sys.version_info < (3,):\n  #  -- pylint: disable=no-member\n  # BINARY_TYPE = str\n  itervalues = dict.itervalues\n  iteritems = dict.iteritems\nelse:\n  # BINARY_TYPE = bytes\n  itervalues = dict.values\n  iteritems = dict.items\n\ndef unicode_escape_decode(x):\n  if sys.version_info < (3,):\n    return codecs.unicode_escape_decode(x)[0]\n  else:\n    return x\n\n# Treat all headers starting with 'h' equally: .h, .hpp, .hxx etc.\n# This is set by --headers flag.\n_hpp_headers = set([])\n\n# {str, bool}: a map from error categories to booleans which indicate if the\n# category should be suppressed for every line.\n_global_error_suppressions = {}\n\ndef ProcessHppHeadersOption(val):\n  global _hpp_headers\n  try:\n    _hpp_headers = {ext.strip() for ext in val.split(',')}\n  except ValueError:\n    PrintUsage('Header extensions must be comma separated list.')\n\ndef ProcessIncludeOrderOption(val):\n  if val is None or val == \"default\":\n    pass\n  elif val == \"standardcfirst\":\n    global _include_order\n    _include_order = val\n  else:\n    PrintUsage('Invalid includeorder value %s. Expected default|standardcfirst')\n\ndef IsHeaderExtension(file_extension):\n  return file_extension in GetHeaderExtensions()\n\ndef GetHeaderExtensions():\n  if _hpp_headers:\n    return _hpp_headers\n  if _valid_extensions:\n    return {h for h in _valid_extensions if 'h' in h}\n  return set(['h', 'hh', 'hpp', 'hxx', 'h++', 'cuh'])\n\n# The allowed extensions for file names\n# This is set by --extensions flag\ndef GetAllExtensions():\n  return GetHeaderExtensions().union(_valid_extensions or set(\n    ['c', 'cc', 'cpp', 'cxx', 'c++', 'cu']))\n\ndef ProcessExtensionsOption(val):\n  global _valid_extensions\n  try:\n    extensions = [ext.strip() for ext in val.split(',')]\n    _valid_extensions = set(extensions)\n  except ValueError:\n    PrintUsage('Extensions should be a comma-separated list of values;'\n               'for example: extensions=hpp,cpp\\n'\n               'This could not be parsed: \"%s\"' % (val,))\n\ndef GetNonHeaderExtensions():\n  return GetAllExtensions().difference(GetHeaderExtensions())\n\ndef ParseNolintSuppressions(filename, raw_line, linenum, error):\n  \"\"\"Updates the global list of line error-suppressions.\n\n  Parses any NOLINT comments on the current line, updating the global\n  error_suppressions store.  Reports an error if the NOLINT comment\n  was malformed.\n\n  Args:\n    filename: str, the name of the input file.\n    raw_line: str, the line of input text, with comments.\n    linenum: int, the number of the current line.\n    error: function, an error handler.\n  \"\"\"\n  matched = Search(r'\\bNOLINT(NEXTLINE)?\\b(\\([^)]+\\))?', raw_line)\n  if matched:\n    if matched.group(1):\n      suppressed_line = linenum + 1\n    else:\n      suppressed_line = linenum\n    category = matched.group(2)\n    if category in (None, '(*)'):  # => \"suppress all\"\n      _error_suppressions.setdefault(None, set()).add(suppressed_line)\n    else:\n      if category.startswith('(') and category.endswith(')'):\n        category = category[1:-1]\n        if category in _ERROR_CATEGORIES:\n          _error_suppressions.setdefault(category, set()).add(suppressed_line)\n        elif any(c for c in _OTHER_NOLINT_CATEGORY_PREFIXES if category.startswith(c)):\n          # Ignore any categories from other tools.\n          pass\n        elif category not in _LEGACY_ERROR_CATEGORIES:\n          error(filename, linenum, 'readability/nolint', 5,\n                'Unknown NOLINT error category: %s' % category)\n\n\ndef ProcessGlobalSuppressions(lines):\n  \"\"\"Updates the list of global error suppressions.\n\n  Parses any lint directives in the file that have global effect.\n\n  Args:\n    lines: An array of strings, each representing a line of the file, with the\n           last element being empty if the file is terminated with a newline.\n  \"\"\"\n  for line in lines:\n    if _SEARCH_C_FILE.search(line):\n      for category in _DEFAULT_C_SUPPRESSED_CATEGORIES:\n        _global_error_suppressions[category] = True\n    if _SEARCH_KERNEL_FILE.search(line):\n      for category in _DEFAULT_KERNEL_SUPPRESSED_CATEGORIES:\n        _global_error_suppressions[category] = True\n\n\ndef ResetNolintSuppressions():\n  \"\"\"Resets the set of NOLINT suppressions to empty.\"\"\"\n  _error_suppressions.clear()\n  _global_error_suppressions.clear()\n\n\ndef IsErrorSuppressedByNolint(category, linenum):\n  \"\"\"Returns true if the specified error category is suppressed on this line.\n\n  Consults the global error_suppressions map populated by\n  ParseNolintSuppressions/ProcessGlobalSuppressions/ResetNolintSuppressions.\n\n  Args:\n    category: str, the category of the error.\n    linenum: int, the current line number.\n  Returns:\n    bool, True iff the error should be suppressed due to a NOLINT comment or\n    global suppression.\n  \"\"\"\n  return (_global_error_suppressions.get(category, False) or\n          linenum in _error_suppressions.get(category, set()) or\n          linenum in _error_suppressions.get(None, set()))\n\n\ndef Match(pattern, s):\n  \"\"\"Matches the string with the pattern, caching the compiled regexp.\"\"\"\n  # The regexp compilation caching is inlined in both Match and Search for\n  # performance reasons; factoring it out into a separate function turns out\n  # to be noticeably expensive.\n  if pattern not in _regexp_compile_cache:\n    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)\n  return _regexp_compile_cache[pattern].match(s)\n\n\ndef ReplaceAll(pattern, rep, s):\n  \"\"\"Replaces instances of pattern in a string with a replacement.\n\n  The compiled regex is kept in a cache shared by Match and Search.\n\n  Args:\n    pattern: regex pattern\n    rep: replacement text\n    s: search string\n\n  Returns:\n    string with replacements made (or original string if no replacements)\n  \"\"\"\n  if pattern not in _regexp_compile_cache:\n    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)\n  return _regexp_compile_cache[pattern].sub(rep, s)\n\n\ndef Search(pattern, s):\n  \"\"\"Searches the string for the pattern, caching the compiled regexp.\"\"\"\n  if pattern not in _regexp_compile_cache:\n    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)\n  return _regexp_compile_cache[pattern].search(s)\n\n\ndef _IsSourceExtension(s):\n  \"\"\"File extension (excluding dot) matches a source file extension.\"\"\"\n  return s in GetNonHeaderExtensions()\n\n\nclass _IncludeState(object):\n  \"\"\"Tracks line numbers for includes, and the order in which includes appear.\n\n  include_list contains list of lists of (header, line number) pairs.\n  It's a lists of lists rather than just one flat list to make it\n  easier to update across preprocessor boundaries.\n\n  Call CheckNextIncludeOrder() once for each header in the file, passing\n  in the type constants defined above. Calls in an illegal order will\n  raise an _IncludeError with an appropriate error message.\n\n  \"\"\"\n  # self._section will move monotonically through this set. If it ever\n  # needs to move backwards, CheckNextIncludeOrder will raise an error.\n  _INITIAL_SECTION = 0\n  _MY_H_SECTION = 1\n  _C_SECTION = 2\n  _CPP_SECTION = 3\n  _OTHER_SYS_SECTION = 4\n  _OTHER_H_SECTION = 5\n\n  _TYPE_NAMES = {\n      _C_SYS_HEADER: 'C system header',\n      _CPP_SYS_HEADER: 'C++ system header',\n      _OTHER_SYS_HEADER: 'other system header',\n      _LIKELY_MY_HEADER: 'header this file implements',\n      _POSSIBLE_MY_HEADER: 'header this file may implement',\n      _OTHER_HEADER: 'other header',\n      }\n  _SECTION_NAMES = {\n      _INITIAL_SECTION: \"... nothing. (This can't be an error.)\",\n      _MY_H_SECTION: 'a header this file implements',\n      _C_SECTION: 'C system header',\n      _CPP_SECTION: 'C++ system header',\n      _OTHER_SYS_SECTION: 'other system header',\n      _OTHER_H_SECTION: 'other header',\n      }\n\n  def __init__(self):\n    self.include_list = [[]]\n    self._section = None\n    self._last_header = None\n    self.ResetSection('')\n\n  def FindHeader(self, header):\n    \"\"\"Check if a header has already been included.\n\n    Args:\n      header: header to check.\n    Returns:\n      Line number of previous occurrence, or -1 if the header has not\n      been seen before.\n    \"\"\"\n    for section_list in self.include_list:\n      for f in section_list:\n        if f[0] == header:\n          return f[1]\n    return -1\n\n  def ResetSection(self, directive):\n    \"\"\"Reset section checking for preprocessor directive.\n\n    Args:\n      directive: preprocessor directive (e.g. \"if\", \"else\").\n    \"\"\"\n    # The name of the current section.\n    self._section = self._INITIAL_SECTION\n    # The path of last found header.\n    self._last_header = ''\n\n    # Update list of includes.  Note that we never pop from the\n    # include list.\n    if directive in ('if', 'ifdef', 'ifndef'):\n      self.include_list.append([])\n    elif directive in ('else', 'elif'):\n      self.include_list[-1] = []\n\n  def SetLastHeader(self, header_path):\n    self._last_header = header_path\n\n  def CanonicalizeAlphabeticalOrder(self, header_path):\n    \"\"\"Returns a path canonicalized for alphabetical comparison.\n\n    - replaces \"-\" with \"_\" so they both cmp the same.\n    - removes '-inl' since we don't require them to be after the main header.\n    - lowercase everything, just in case.\n\n    Args:\n      header_path: Path to be canonicalized.\n\n    Returns:\n      Canonicalized path.\n    \"\"\"\n    return header_path.replace('-inl.h', '.h').replace('-', '_').lower()\n\n  def IsInAlphabeticalOrder(self, clean_lines, linenum, header_path):\n    \"\"\"Check if a header is in alphabetical order with the previous header.\n\n    Args:\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      header_path: Canonicalized header to be checked.\n\n    Returns:\n      Returns true if the header is in alphabetical order.\n    \"\"\"\n    # If previous section is different from current section, _last_header will\n    # be reset to empty string, so it's always less than current header.\n    #\n    # If previous line was a blank line, assume that the headers are\n    # intentionally sorted the way they are.\n    if (self._last_header > header_path and\n        Match(r'^\\s*#\\s*include\\b', clean_lines.elided[linenum - 1])):\n      return False\n    return True\n\n  def CheckNextIncludeOrder(self, header_type):\n    \"\"\"Returns a non-empty error message if the next header is out of order.\n\n    This function also updates the internal state to be ready to check\n    the next include.\n\n    Args:\n      header_type: One of the _XXX_HEADER constants defined above.\n\n    Returns:\n      The empty string if the header is in the right order, or an\n      error message describing what's wrong.\n\n    \"\"\"\n    error_message = ('Found %s after %s' %\n                     (self._TYPE_NAMES[header_type],\n                      self._SECTION_NAMES[self._section]))\n\n    last_section = self._section\n\n    if header_type == _C_SYS_HEADER:\n      if self._section <= self._C_SECTION:\n        self._section = self._C_SECTION\n      else:\n        self._last_header = ''\n        return error_message\n    elif header_type == _CPP_SYS_HEADER:\n      if self._section <= self._CPP_SECTION:\n        self._section = self._CPP_SECTION\n      else:\n        self._last_header = ''\n        return error_message\n    elif header_type == _OTHER_SYS_HEADER:\n      if self._section <= self._OTHER_SYS_SECTION:\n        self._section = self._OTHER_SYS_SECTION\n      else:\n        self._last_header = ''\n        return error_message\n    elif header_type == _LIKELY_MY_HEADER:\n      if self._section <= self._MY_H_SECTION:\n        self._section = self._MY_H_SECTION\n      else:\n        self._section = self._OTHER_H_SECTION\n    elif header_type == _POSSIBLE_MY_HEADER:\n      if self._section <= self._MY_H_SECTION:\n        self._section = self._MY_H_SECTION\n      else:\n        # This will always be the fallback because we're not sure\n        # enough that the header is associated with this file.\n        self._section = self._OTHER_H_SECTION\n    else:\n      assert header_type == _OTHER_HEADER\n      self._section = self._OTHER_H_SECTION\n\n    if last_section != self._section:\n      self._last_header = ''\n\n    return ''\n\n\nclass _CppLintState(object):\n  \"\"\"Maintains module-wide state..\"\"\"\n\n  def __init__(self):\n    self.verbose_level = 1  # global setting.\n    self.error_count = 0    # global count of reported errors\n    # filters to apply when emitting error messages\n    self.filters = _DEFAULT_FILTERS[:]\n    # backup of filter list. Used to restore the state after each file.\n    self._filters_backup = self.filters[:]\n    self.counting = 'total'  # In what way are we counting errors?\n    self.errors_by_category = {}  # string to int dict storing error counts\n    self.quiet = False  # Suppress non-error messages?\n\n    # output format:\n    # \"emacs\" - format that emacs can parse (default)\n    # \"eclipse\" - format that eclipse can parse\n    # \"vs7\" - format that Microsoft Visual Studio 7 can parse\n    # \"junit\" - format that Jenkins, Bamboo, etc can parse\n    # \"sed\" - returns a gnu sed command to fix the problem\n    # \"gsed\" - like sed, but names the command gsed, e.g. for macOS homebrew users\n    self.output_format = 'emacs'\n\n    # For JUnit output, save errors and failures until the end so that they\n    # can be written into the XML\n    self._junit_errors = []\n    self._junit_failures = []\n\n  def SetOutputFormat(self, output_format):\n    \"\"\"Sets the output format for errors.\"\"\"\n    self.output_format = output_format\n\n  def SetQuiet(self, quiet):\n    \"\"\"Sets the module's quiet settings, and returns the previous setting.\"\"\"\n    last_quiet = self.quiet\n    self.quiet = quiet\n    return last_quiet\n\n  def SetVerboseLevel(self, level):\n    \"\"\"Sets the module's verbosity, and returns the previous setting.\"\"\"\n    last_verbose_level = self.verbose_level\n    self.verbose_level = level\n    return last_verbose_level\n\n  def SetCountingStyle(self, counting_style):\n    \"\"\"Sets the module's counting options.\"\"\"\n    self.counting = counting_style\n\n  def SetFilters(self, filters):\n    \"\"\"Sets the error-message filters.\n\n    These filters are applied when deciding whether to emit a given\n    error message.\n\n    Args:\n      filters: A string of comma-separated filters (eg \"+whitespace/indent\").\n               Each filter should start with + or -; else we die.\n\n    Raises:\n      ValueError: The comma-separated filters did not all start with '+' or '-'.\n                  E.g. \"-,+whitespace,-whitespace/indent,whitespace/badfilter\"\n    \"\"\"\n    # Default filters always have less priority than the flag ones.\n    self.filters = _DEFAULT_FILTERS[:]\n    self.AddFilters(filters)\n\n  def AddFilters(self, filters):\n    \"\"\" Adds more filters to the existing list of error-message filters. \"\"\"\n    for filt in filters.split(','):\n      clean_filt = filt.strip()\n      if clean_filt:\n        self.filters.append(clean_filt)\n    for filt in self.filters:\n      if not (filt.startswith('+') or filt.startswith('-')):\n        raise ValueError('Every filter in --filters must start with + or -'\n                         ' (%s does not)' % filt)\n\n  def BackupFilters(self):\n    \"\"\" Saves the current filter list to backup storage.\"\"\"\n    self._filters_backup = self.filters[:]\n\n  def RestoreFilters(self):\n    \"\"\" Restores filters previously backed up.\"\"\"\n    self.filters = self._filters_backup[:]\n\n  def ResetErrorCounts(self):\n    \"\"\"Sets the module's error statistic back to zero.\"\"\"\n    self.error_count = 0\n    self.errors_by_category = {}\n\n  def IncrementErrorCount(self, category):\n    \"\"\"Bumps the module's error statistic.\"\"\"\n    self.error_count += 1\n    if self.counting in ('toplevel', 'detailed'):\n      if self.counting != 'detailed':\n        category = category.split('/')[0]\n      if category not in self.errors_by_category:\n        self.errors_by_category[category] = 0\n      self.errors_by_category[category] += 1\n\n  def PrintErrorCounts(self):\n    \"\"\"Print a summary of errors by category, and the total.\"\"\"\n    for category, count in sorted(iteritems(self.errors_by_category)):\n      self.PrintInfo('Category \\'%s\\' errors found: %d\\n' %\n                       (category, count))\n    if self.error_count > 0:\n      self.PrintInfo('Total errors found: %d\\n' % self.error_count)\n\n  def PrintInfo(self, message):\n    # _quiet does not represent --quiet flag.\n    # Hide infos from stdout to keep stdout pure for machine consumption\n    if not _quiet and self.output_format not in _MACHINE_OUTPUTS:\n      sys.stdout.write(message)\n\n  def PrintError(self, message):\n    if self.output_format == 'junit':\n      self._junit_errors.append(message)\n    else:\n      sys.stderr.write(message)\n\n  def AddJUnitFailure(self, filename, linenum, message, category, confidence):\n    self._junit_failures.append((filename, linenum, message, category,\n        confidence))\n\n  def FormatJUnitXML(self):\n    num_errors = len(self._junit_errors)\n    num_failures = len(self._junit_failures)\n\n    testsuite = xml.etree.ElementTree.Element('testsuite')\n    testsuite.attrib['errors'] = str(num_errors)\n    testsuite.attrib['failures'] = str(num_failures)\n    testsuite.attrib['name'] = 'cpplint'\n\n    if num_errors == 0 and num_failures == 0:\n      testsuite.attrib['tests'] = str(1)\n      xml.etree.ElementTree.SubElement(testsuite, 'testcase', name='passed')\n\n    else:\n      testsuite.attrib['tests'] = str(num_errors + num_failures)\n      if num_errors > 0:\n        testcase = xml.etree.ElementTree.SubElement(testsuite, 'testcase')\n        testcase.attrib['name'] = 'errors'\n        error = xml.etree.ElementTree.SubElement(testcase, 'error')\n        error.text = '\\n'.join(self._junit_errors)\n      if num_failures > 0:\n        # Group failures by file\n        failed_file_order = []\n        failures_by_file = {}\n        for failure in self._junit_failures:\n          failed_file = failure[0]\n          if failed_file not in failed_file_order:\n            failed_file_order.append(failed_file)\n            failures_by_file[failed_file] = []\n          failures_by_file[failed_file].append(failure)\n        # Create a testcase for each file\n        for failed_file in failed_file_order:\n          failures = failures_by_file[failed_file]\n          testcase = xml.etree.ElementTree.SubElement(testsuite, 'testcase')\n          testcase.attrib['name'] = failed_file\n          failure = xml.etree.ElementTree.SubElement(testcase, 'failure')\n          template = '{0}: {1} [{2}] [{3}]'\n          texts = [template.format(f[1], f[2], f[3], f[4]) for f in failures]\n          failure.text = '\\n'.join(texts)\n\n    xml_decl = '<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n'\n    return xml_decl + xml.etree.ElementTree.tostring(testsuite, 'utf-8').decode('utf-8')\n\n\n_cpplint_state = _CppLintState()\n\n\ndef _OutputFormat():\n  \"\"\"Gets the module's output format.\"\"\"\n  return _cpplint_state.output_format\n\n\ndef _SetOutputFormat(output_format):\n  \"\"\"Sets the module's output format.\"\"\"\n  _cpplint_state.SetOutputFormat(output_format)\n\ndef _Quiet():\n  \"\"\"Return's the module's quiet setting.\"\"\"\n  return _cpplint_state.quiet\n\ndef _SetQuiet(quiet):\n  \"\"\"Set the module's quiet status, and return previous setting.\"\"\"\n  return _cpplint_state.SetQuiet(quiet)\n\n\ndef _VerboseLevel():\n  \"\"\"Returns the module's verbosity setting.\"\"\"\n  return _cpplint_state.verbose_level\n\n\ndef _SetVerboseLevel(level):\n  \"\"\"Sets the module's verbosity, and returns the previous setting.\"\"\"\n  return _cpplint_state.SetVerboseLevel(level)\n\n\ndef _SetCountingStyle(level):\n  \"\"\"Sets the module's counting options.\"\"\"\n  _cpplint_state.SetCountingStyle(level)\n\n\ndef _Filters():\n  \"\"\"Returns the module's list of output filters, as a list.\"\"\"\n  return _cpplint_state.filters\n\n\ndef _SetFilters(filters):\n  \"\"\"Sets the module's error-message filters.\n\n  These filters are applied when deciding whether to emit a given\n  error message.\n\n  Args:\n    filters: A string of comma-separated filters (eg \"whitespace/indent\").\n             Each filter should start with + or -; else we die.\n  \"\"\"\n  _cpplint_state.SetFilters(filters)\n\ndef _AddFilters(filters):\n  \"\"\"Adds more filter overrides.\n\n  Unlike _SetFilters, this function does not reset the current list of filters\n  available.\n\n  Args:\n    filters: A string of comma-separated filters (eg \"whitespace/indent\").\n             Each filter should start with + or -; else we die.\n  \"\"\"\n  _cpplint_state.AddFilters(filters)\n\ndef _BackupFilters():\n  \"\"\" Saves the current filter list to backup storage.\"\"\"\n  _cpplint_state.BackupFilters()\n\ndef _RestoreFilters():\n  \"\"\" Restores filters previously backed up.\"\"\"\n  _cpplint_state.RestoreFilters()\n\nclass _FunctionState(object):\n  \"\"\"Tracks current function name and the number of lines in its body.\"\"\"\n\n  _NORMAL_TRIGGER = 250  # for --v=0, 500 for --v=1, etc.\n  _TEST_TRIGGER = 400    # about 50% more than _NORMAL_TRIGGER.\n\n  def __init__(self):\n    self.in_a_function = False\n    self.lines_in_function = 0\n    self.current_function = ''\n\n  def Begin(self, function_name):\n    \"\"\"Start analyzing function body.\n\n    Args:\n      function_name: The name of the function being tracked.\n    \"\"\"\n    self.in_a_function = True\n    self.lines_in_function = 0\n    self.current_function = function_name\n\n  def Count(self):\n    \"\"\"Count line in current function body.\"\"\"\n    if self.in_a_function:\n      self.lines_in_function += 1\n\n  def Check(self, error, filename, linenum):\n    \"\"\"Report if too many lines in function body.\n\n    Args:\n      error: The function to call with any errors found.\n      filename: The name of the current file.\n      linenum: The number of the line to check.\n    \"\"\"\n    if not self.in_a_function:\n      return\n\n    if Match(r'T(EST|est)', self.current_function):\n      base_trigger = self._TEST_TRIGGER\n    else:\n      base_trigger = self._NORMAL_TRIGGER\n    trigger = base_trigger * 2**_VerboseLevel()\n\n    if self.lines_in_function > trigger:\n      error_level = int(math.log(self.lines_in_function / base_trigger, 2))\n      # 50 => 0, 100 => 1, 200 => 2, 400 => 3, 800 => 4, 1600 => 5, ...\n      if error_level > 5:\n        error_level = 5\n      error(filename, linenum, 'readability/fn_size', error_level,\n            'Small and focused functions are preferred:'\n            ' %s has %d non-comment lines'\n            ' (error triggered by exceeding %d lines).'  % (\n                self.current_function, self.lines_in_function, trigger))\n\n  def End(self):\n    \"\"\"Stop analyzing function body.\"\"\"\n    self.in_a_function = False\n\n\nclass _IncludeError(Exception):\n  \"\"\"Indicates a problem with the include order in a file.\"\"\"\n  pass\n\n\nclass FileInfo(object):\n  \"\"\"Provides utility functions for filenames.\n\n  FileInfo provides easy access to the components of a file's path\n  relative to the project root.\n  \"\"\"\n\n  def __init__(self, filename):\n    self._filename = filename\n\n  def FullName(self):\n    \"\"\"Make Windows paths like Unix.\"\"\"\n    return os.path.abspath(self._filename).replace('\\\\', '/')\n\n  def RepositoryName(self):\n    r\"\"\"FullName after removing the local path to the repository.\n\n    If we have a real absolute path name here we can try to do something smart:\n    detecting the root of the checkout and truncating /path/to/checkout from\n    the name so that we get header guards that don't include things like\n    \"C:\\\\Documents and Settings\\\\...\" or \"/home/username/...\" in them and thus\n    people on different computers who have checked the source out to different\n    locations won't see bogus errors.\n    \"\"\"\n    fullname = self.FullName()\n\n    if os.path.exists(fullname):\n      project_dir = os.path.dirname(fullname)\n\n      # If the user specified a repository path, it exists, and the file is\n      # contained in it, use the specified repository path\n      if _repository:\n        repo = FileInfo(_repository).FullName()\n        root_dir = project_dir\n        while os.path.exists(root_dir):\n          # allow case-insensitive compare on Windows\n          if os.path.normcase(root_dir) == os.path.normcase(repo):\n            return os.path.relpath(fullname, root_dir).replace('\\\\', '/')\n          one_up_dir = os.path.dirname(root_dir)\n          if one_up_dir == root_dir:\n            break\n          root_dir = one_up_dir\n\n      if os.path.exists(os.path.join(project_dir, \".svn\")):\n        # If there's a .svn file in the current directory, we recursively look\n        # up the directory tree for the top of the SVN checkout\n        root_dir = project_dir\n        one_up_dir = os.path.dirname(root_dir)\n        while os.path.exists(os.path.join(one_up_dir, \".svn\")):\n          root_dir = os.path.dirname(root_dir)\n          one_up_dir = os.path.dirname(one_up_dir)\n\n        prefix = os.path.commonprefix([root_dir, project_dir])\n        return fullname[len(prefix) + 1:]\n\n      # Not SVN <= 1.6? Try to find a git, hg, or svn top level directory by\n      # searching up from the current path.\n      root_dir = current_dir = os.path.dirname(fullname)\n      while current_dir != os.path.dirname(current_dir):\n        if (os.path.exists(os.path.join(current_dir, \".git\")) or\n            os.path.exists(os.path.join(current_dir, \".hg\")) or\n            os.path.exists(os.path.join(current_dir, \".svn\"))):\n          root_dir = current_dir\n        current_dir = os.path.dirname(current_dir)\n\n      if (os.path.exists(os.path.join(root_dir, \".git\")) or\n          os.path.exists(os.path.join(root_dir, \".hg\")) or\n          os.path.exists(os.path.join(root_dir, \".svn\"))):\n        prefix = os.path.commonprefix([root_dir, project_dir])\n        return fullname[len(prefix) + 1:]\n\n    # Don't know what to do; header guard warnings may be wrong...\n    return fullname\n\n  def Split(self):\n    \"\"\"Splits the file into the directory, basename, and extension.\n\n    For 'chrome/browser/browser.cc', Split() would\n    return ('chrome/browser', 'browser', '.cc')\n\n    Returns:\n      A tuple of (directory, basename, extension).\n    \"\"\"\n\n    googlename = self.RepositoryName()\n    project, rest = os.path.split(googlename)\n    return (project,) + os.path.splitext(rest)\n\n  def BaseName(self):\n    \"\"\"File base name - text after the final slash, before the final period.\"\"\"\n    return self.Split()[1]\n\n  def Extension(self):\n    \"\"\"File extension - text following the final period, includes that period.\"\"\"\n    return self.Split()[2]\n\n  def NoExtension(self):\n    \"\"\"File has no source file extension.\"\"\"\n    return '/'.join(self.Split()[0:2])\n\n  def IsSource(self):\n    \"\"\"File has a source file extension.\"\"\"\n    return _IsSourceExtension(self.Extension()[1:])\n\n\ndef _ShouldPrintError(category, confidence, linenum):\n  \"\"\"If confidence >= verbose, category passes filter and is not suppressed.\"\"\"\n\n  # There are three ways we might decide not to print an error message:\n  # a \"NOLINT(category)\" comment appears in the source,\n  # the verbosity level isn't high enough, or the filters filter it out.\n  if IsErrorSuppressedByNolint(category, linenum):\n    return False\n\n  if confidence < _cpplint_state.verbose_level:\n    return False\n\n  is_filtered = False\n  for one_filter in _Filters():\n    if one_filter.startswith('-'):\n      if category.startswith(one_filter[1:]):\n        is_filtered = True\n    elif one_filter.startswith('+'):\n      if category.startswith(one_filter[1:]):\n        is_filtered = False\n    else:\n      assert False  # should have been checked for in SetFilter.\n  if is_filtered:\n    return False\n\n  return True\n\n\ndef Error(filename, linenum, category, confidence, message):\n  \"\"\"Logs the fact we've found a lint error.\n\n  We log where the error was found, and also our confidence in the error,\n  that is, how certain we are this is a legitimate style regression, and\n  not a misidentification or a use that's sometimes justified.\n\n  False positives can be suppressed by the use of\n  \"cpplint(category)\"  comments on the offending line.  These are\n  parsed into _error_suppressions.\n\n  Args:\n    filename: The name of the file containing the error.\n    linenum: The number of the line containing the error.\n    category: A string used to describe the \"category\" this bug\n      falls under: \"whitespace\", say, or \"runtime\".  Categories\n      may have a hierarchy separated by slashes: \"whitespace/indent\".\n    confidence: A number from 1-5 representing a confidence score for\n      the error, with 5 meaning that we are certain of the problem,\n      and 1 meaning that it could be a legitimate construct.\n    message: The error message.\n  \"\"\"\n  if _ShouldPrintError(category, confidence, linenum):\n    _cpplint_state.IncrementErrorCount(category)\n    if _cpplint_state.output_format == 'vs7':\n      _cpplint_state.PrintError('%s(%s): error cpplint: [%s] %s [%d]\\n' % (\n          filename, linenum, category, message, confidence))\n    elif _cpplint_state.output_format == 'eclipse':\n      sys.stderr.write('%s:%s: warning: %s  [%s] [%d]\\n' % (\n          filename, linenum, message, category, confidence))\n    elif _cpplint_state.output_format == 'junit':\n      _cpplint_state.AddJUnitFailure(filename, linenum, message, category,\n          confidence)\n    elif _cpplint_state.output_format in ['sed', 'gsed']:\n      if message in _SED_FIXUPS:\n        sys.stdout.write(_cpplint_state.output_format + \" -i '%s%s' %s # %s  [%s] [%d]\\n\" % (\n            linenum, _SED_FIXUPS[message], filename, message, category, confidence))\n      else:\n        sys.stderr.write('# %s:%s:  \"%s\"  [%s] [%d]\\n' % (\n            filename, linenum, message, category, confidence))\n    else:\n      final_message = '%s:%s:  %s  [%s] [%d]\\n' % (\n          filename, linenum, message, category, confidence)\n      sys.stderr.write(final_message)\n\n# Matches standard C++ escape sequences per 2.13.2.3 of the C++ standard.\n_RE_PATTERN_CLEANSE_LINE_ESCAPES = re.compile(\n    r'\\\\([abfnrtv?\"\\\\\\']|\\d+|x[0-9a-fA-F]+)')\n# Match a single C style comment on the same line.\n_RE_PATTERN_C_COMMENTS = r'/\\*(?:[^*]|\\*(?!/))*\\*/'\n# Matches multi-line C style comments.\n# This RE is a little bit more complicated than one might expect, because we\n# have to take care of space removals tools so we can handle comments inside\n# statements better.\n# The current rule is: We only clear spaces from both sides when we're at the\n# end of the line. Otherwise, we try to remove spaces from the right side,\n# if this doesn't work we try on left side but only if there's a non-character\n# on the right.\n_RE_PATTERN_CLEANSE_LINE_C_COMMENTS = re.compile(\n    r'(\\s*' + _RE_PATTERN_C_COMMENTS + r'\\s*$|' +\n    _RE_PATTERN_C_COMMENTS + r'\\s+|' +\n    r'\\s+' + _RE_PATTERN_C_COMMENTS + r'(?=\\W)|' +\n    _RE_PATTERN_C_COMMENTS + r')')\n\n\ndef IsCppString(line):\n  \"\"\"Does line terminate so, that the next symbol is in string constant.\n\n  This function does not consider comments at all.\n\n  Args:\n    line: is a partial line of code starting from the 0..n.\n\n  Returns:\n    True, if next character appended to 'line' is inside a\n    string constant.\n  \"\"\"\n\n  line = line.replace(r'\\\\', 'XX')  # after this, \\\\\" does not match to \\\"\n  return ((line.count('\"') - line.count(r'\\\"') - line.count(\"'\\\"'\")) & 1) == 1\n\n\ndef CleanseRawStrings(raw_lines):\n  \"\"\"Removes C++11 raw strings from lines.\n\n    Before:\n      static const char kData[] = R\"(\n          multi-line string\n          )\";\n\n    After:\n      static const char kData[] = \"\"\n          (replaced by blank line)\n          \"\";\n\n  Args:\n    raw_lines: list of raw lines.\n\n  Returns:\n    list of lines with C++11 raw strings replaced by empty strings.\n  \"\"\"\n\n  delimiter = None\n  lines_without_raw_strings = []\n  for line in raw_lines:\n    if delimiter:\n      # Inside a raw string, look for the end\n      end = line.find(delimiter)\n      if end >= 0:\n        # Found the end of the string, match leading space for this\n        # line and resume copying the original lines, and also insert\n        # a \"\" on the last line.\n        leading_space = Match(r'^(\\s*)\\S', line)\n        line = leading_space.group(1) + '\"\"' + line[end + len(delimiter):]\n        delimiter = None\n      else:\n        # Haven't found the end yet, append a blank line.\n        line = '\"\"'\n\n    # Look for beginning of a raw string, and replace them with\n    # empty strings.  This is done in a loop to handle multiple raw\n    # strings on the same line.\n    while delimiter is None:\n      # Look for beginning of a raw string.\n      # See 2.14.15 [lex.string] for syntax.\n      #\n      # Once we have matched a raw string, we check the prefix of the\n      # line to make sure that the line is not part of a single line\n      # comment.  It's done this way because we remove raw strings\n      # before removing comments as opposed to removing comments\n      # before removing raw strings.  This is because there are some\n      # cpplint checks that requires the comments to be preserved, but\n      # we don't want to check comments that are inside raw strings.\n      matched = Match(r'^(.*?)\\b(?:R|u8R|uR|UR|LR)\"([^\\s\\\\()]*)\\((.*)$', line)\n      if (matched and\n          not Match(r'^([^\\'\"]|\\'(\\\\.|[^\\'])*\\'|\"(\\\\.|[^\"])*\")*//',\n                    matched.group(1))):\n        delimiter = ')' + matched.group(2) + '\"'\n\n        end = matched.group(3).find(delimiter)\n        if end >= 0:\n          # Raw string ended on same line\n          line = (matched.group(1) + '\"\"' +\n                  matched.group(3)[end + len(delimiter):])\n          delimiter = None\n        else:\n          # Start of a multi-line raw string\n          line = matched.group(1) + '\"\"'\n      else:\n        break\n\n    lines_without_raw_strings.append(line)\n\n  # TODO(unknown): if delimiter is not None here, we might want to\n  # emit a warning for unterminated string.\n  return lines_without_raw_strings\n\n\ndef FindNextMultiLineCommentStart(lines, lineix):\n  \"\"\"Find the beginning marker for a multiline comment.\"\"\"\n  while lineix < len(lines):\n    if lines[lineix].strip().startswith('/*'):\n      # Only return this marker if the comment goes beyond this line\n      if lines[lineix].strip().find('*/', 2) < 0:\n        return lineix\n    lineix += 1\n  return len(lines)\n\n\ndef FindNextMultiLineCommentEnd(lines, lineix):\n  \"\"\"We are inside a comment, find the end marker.\"\"\"\n  while lineix < len(lines):\n    if lines[lineix].strip().endswith('*/'):\n      return lineix\n    lineix += 1\n  return len(lines)\n\n\ndef RemoveMultiLineCommentsFromRange(lines, begin, end):\n  \"\"\"Clears a range of lines for multi-line comments.\"\"\"\n  # Having // <empty> comments makes the lines non-empty, so we will not get\n  # unnecessary blank line warnings later in the code.\n  for i in range(begin, end):\n    lines[i] = '/**/'\n\n\ndef RemoveMultiLineComments(filename, lines, error):\n  \"\"\"Removes multiline (c-style) comments from lines.\"\"\"\n  lineix = 0\n  while lineix < len(lines):\n    lineix_begin = FindNextMultiLineCommentStart(lines, lineix)\n    if lineix_begin >= len(lines):\n      return\n    lineix_end = FindNextMultiLineCommentEnd(lines, lineix_begin)\n    if lineix_end >= len(lines):\n      error(filename, lineix_begin + 1, 'readability/multiline_comment', 5,\n            'Could not find end of multi-line comment')\n      return\n    RemoveMultiLineCommentsFromRange(lines, lineix_begin, lineix_end + 1)\n    lineix = lineix_end + 1\n\n\ndef CleanseComments(line):\n  \"\"\"Removes //-comments and single-line C-style /* */ comments.\n\n  Args:\n    line: A line of C++ source.\n\n  Returns:\n    The line with single-line comments removed.\n  \"\"\"\n  commentpos = line.find('//')\n  if commentpos != -1 and not IsCppString(line[:commentpos]):\n    line = line[:commentpos].rstrip()\n  # get rid of /* ... */\n  return _RE_PATTERN_CLEANSE_LINE_C_COMMENTS.sub('', line)\n\n\nclass CleansedLines(object):\n  \"\"\"Holds 4 copies of all lines with different preprocessing applied to them.\n\n  1) elided member contains lines without strings and comments.\n  2) lines member contains lines without comments.\n  3) raw_lines member contains all the lines without processing.\n  4) lines_without_raw_strings member is same as raw_lines, but with C++11 raw\n     strings removed.\n  All these members are of <type 'list'>, and of the same length.\n  \"\"\"\n\n  def __init__(self, lines):\n    self.elided = []\n    self.lines = []\n    self.raw_lines = lines\n    self.num_lines = len(lines)\n    self.lines_without_raw_strings = CleanseRawStrings(lines)\n    # # pylint: disable=consider-using-enumerate\n    for linenum in range(len(self.lines_without_raw_strings)):\n      self.lines.append(CleanseComments(\n          self.lines_without_raw_strings[linenum]))\n      elided = self._CollapseStrings(self.lines_without_raw_strings[linenum])\n      self.elided.append(CleanseComments(elided))\n\n  def NumLines(self):\n    \"\"\"Returns the number of lines represented.\"\"\"\n    return self.num_lines\n\n  @staticmethod\n  def _CollapseStrings(elided):\n    \"\"\"Collapses strings and chars on a line to simple \"\" or '' blocks.\n\n    We nix strings first so we're not fooled by text like '\"http://\"'\n\n    Args:\n      elided: The line being processed.\n\n    Returns:\n      The line with collapsed strings.\n    \"\"\"\n    if _RE_PATTERN_INCLUDE.match(elided):\n      return elided\n\n    # Remove escaped characters first to make quote/single quote collapsing\n    # basic.  Things that look like escaped characters shouldn't occur\n    # outside of strings and chars.\n    elided = _RE_PATTERN_CLEANSE_LINE_ESCAPES.sub('', elided)\n\n    # Replace quoted strings and digit separators.  Both single quotes\n    # and double quotes are processed in the same loop, otherwise\n    # nested quotes wouldn't work.\n    collapsed = ''\n    while True:\n      # Find the first quote character\n      match = Match(r'^([^\\'\"]*)([\\'\"])(.*)$', elided)\n      if not match:\n        collapsed += elided\n        break\n      head, quote, tail = match.groups()\n\n      if quote == '\"':\n        # Collapse double quoted strings\n        second_quote = tail.find('\"')\n        if second_quote >= 0:\n          collapsed += head + '\"\"'\n          elided = tail[second_quote + 1:]\n        else:\n          # Unmatched double quote, don't bother processing the rest\n          # of the line since this is probably a multiline string.\n          collapsed += elided\n          break\n      else:\n        # Found single quote, check nearby text to eliminate digit separators.\n        #\n        # There is no special handling for floating point here, because\n        # the integer/fractional/exponent parts would all be parsed\n        # correctly as long as there are digits on both sides of the\n        # separator.  So we are fine as long as we don't see something\n        # like \"0.'3\" (gcc 4.9.0 will not allow this literal).\n        if Search(r'\\b(?:0[bBxX]?|[1-9])[0-9a-fA-F]*$', head):\n          match_literal = Match(r'^((?:\\'?[0-9a-zA-Z_])*)(.*)$', \"'\" + tail)\n          collapsed += head + match_literal.group(1).replace(\"'\", '')\n          elided = match_literal.group(2)\n        else:\n          second_quote = tail.find('\\'')\n          if second_quote >= 0:\n            collapsed += head + \"''\"\n            elided = tail[second_quote + 1:]\n          else:\n            # Unmatched single quote\n            collapsed += elided\n            break\n\n    return collapsed\n\n\ndef FindEndOfExpressionInLine(line, startpos, stack):\n  \"\"\"Find the position just after the end of current parenthesized expression.\n\n  Args:\n    line: a CleansedLines line.\n    startpos: start searching at this position.\n    stack: nesting stack at startpos.\n\n  Returns:\n    On finding matching end: (index just after matching end, None)\n    On finding an unclosed expression: (-1, None)\n    Otherwise: (-1, new stack at end of this line)\n  \"\"\"\n  for i in xrange(startpos, len(line)):\n    char = line[i]\n    if char in '([{':\n      # Found start of parenthesized expression, push to expression stack\n      stack.append(char)\n    elif char == '<':\n      # Found potential start of template argument list\n      if i > 0 and line[i - 1] == '<':\n        # Left shift operator\n        if stack and stack[-1] == '<':\n          stack.pop()\n          if not stack:\n            return (-1, None)\n      elif i > 0 and Search(r'\\boperator\\s*$', line[0:i]):\n        # operator<, don't add to stack\n        continue\n      else:\n        # Tentative start of template argument list\n        stack.append('<')\n    elif char in ')]}':\n      # Found end of parenthesized expression.\n      #\n      # If we are currently expecting a matching '>', the pending '<'\n      # must have been an operator.  Remove them from expression stack.\n      while stack and stack[-1] == '<':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n      if ((stack[-1] == '(' and char == ')') or\n          (stack[-1] == '[' and char == ']') or\n          (stack[-1] == '{' and char == '}')):\n        stack.pop()\n        if not stack:\n          return (i + 1, None)\n      else:\n        # Mismatched parentheses\n        return (-1, None)\n    elif char == '>':\n      # Found potential end of template argument list.\n\n      # Ignore \"->\" and operator functions\n      if (i > 0 and\n          (line[i - 1] == '-' or Search(r'\\boperator\\s*$', line[0:i - 1]))):\n        continue\n\n      # Pop the stack if there is a matching '<'.  Otherwise, ignore\n      # this '>' since it must be an operator.\n      if stack:\n        if stack[-1] == '<':\n          stack.pop()\n          if not stack:\n            return (i + 1, None)\n    elif char == ';':\n      # Found something that look like end of statements.  If we are currently\n      # expecting a '>', the matching '<' must have been an operator, since\n      # template argument list should not contain statements.\n      while stack and stack[-1] == '<':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n\n  # Did not find end of expression or unbalanced parentheses on this line\n  return (-1, stack)\n\n\ndef CloseExpression(clean_lines, linenum, pos):\n  \"\"\"If input points to ( or { or [ or <, finds the position that closes it.\n\n  If lines[linenum][pos] points to a '(' or '{' or '[' or '<', finds the\n  linenum/pos that correspond to the closing of the expression.\n\n  TODO(unknown): cpplint spends a fair bit of time matching parentheses.\n  Ideally we would want to index all opening and closing parentheses once\n  and have CloseExpression be just a simple lookup, but due to preprocessor\n  tricks, this is not so easy.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    pos: A position on the line.\n\n  Returns:\n    A tuple (line, linenum, pos) pointer *past* the closing brace, or\n    (line, len(lines), -1) if we never find a close.  Note we ignore\n    strings and comments when matching; and the line we return is the\n    'cleansed' line at linenum.\n  \"\"\"\n\n  line = clean_lines.elided[linenum]\n  if (line[pos] not in '({[<') or Match(r'<[<=]', line[pos:]):\n    return (line, clean_lines.NumLines(), -1)\n\n  # Check first line\n  (end_pos, stack) = FindEndOfExpressionInLine(line, pos, [])\n  if end_pos > -1:\n    return (line, linenum, end_pos)\n\n  # Continue scanning forward\n  while stack and linenum < clean_lines.NumLines() - 1:\n    linenum += 1\n    line = clean_lines.elided[linenum]\n    (end_pos, stack) = FindEndOfExpressionInLine(line, 0, stack)\n    if end_pos > -1:\n      return (line, linenum, end_pos)\n\n  # Did not find end of expression before end of file, give up\n  return (line, clean_lines.NumLines(), -1)\n\n\ndef FindStartOfExpressionInLine(line, endpos, stack):\n  \"\"\"Find position at the matching start of current expression.\n\n  This is almost the reverse of FindEndOfExpressionInLine, but note\n  that the input position and returned position differs by 1.\n\n  Args:\n    line: a CleansedLines line.\n    endpos: start searching at this position.\n    stack: nesting stack at endpos.\n\n  Returns:\n    On finding matching start: (index at matching start, None)\n    On finding an unclosed expression: (-1, None)\n    Otherwise: (-1, new stack at beginning of this line)\n  \"\"\"\n  i = endpos\n  while i >= 0:\n    char = line[i]\n    if char in ')]}':\n      # Found end of expression, push to expression stack\n      stack.append(char)\n    elif char == '>':\n      # Found potential end of template argument list.\n      #\n      # Ignore it if it's a \"->\" or \">=\" or \"operator>\"\n      if (i > 0 and\n          (line[i - 1] == '-' or\n           Match(r'\\s>=\\s', line[i - 1:]) or\n           Search(r'\\boperator\\s*$', line[0:i]))):\n        i -= 1\n      else:\n        stack.append('>')\n    elif char == '<':\n      # Found potential start of template argument list\n      if i > 0 and line[i - 1] == '<':\n        # Left shift operator\n        i -= 1\n      else:\n        # If there is a matching '>', we can pop the expression stack.\n        # Otherwise, ignore this '<' since it must be an operator.\n        if stack and stack[-1] == '>':\n          stack.pop()\n          if not stack:\n            return (i, None)\n    elif char in '([{':\n      # Found start of expression.\n      #\n      # If there are any unmatched '>' on the stack, they must be\n      # operators.  Remove those.\n      while stack and stack[-1] == '>':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n      if ((char == '(' and stack[-1] == ')') or\n          (char == '[' and stack[-1] == ']') or\n          (char == '{' and stack[-1] == '}')):\n        stack.pop()\n        if not stack:\n          return (i, None)\n      else:\n        # Mismatched parentheses\n        return (-1, None)\n    elif char == ';':\n      # Found something that look like end of statements.  If we are currently\n      # expecting a '<', the matching '>' must have been an operator, since\n      # template argument list should not contain statements.\n      while stack and stack[-1] == '>':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n\n    i -= 1\n\n  return (-1, stack)\n\n\ndef ReverseCloseExpression(clean_lines, linenum, pos):\n  \"\"\"If input points to ) or } or ] or >, finds the position that opens it.\n\n  If lines[linenum][pos] points to a ')' or '}' or ']' or '>', finds the\n  linenum/pos that correspond to the opening of the expression.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    pos: A position on the line.\n\n  Returns:\n    A tuple (line, linenum, pos) pointer *at* the opening brace, or\n    (line, 0, -1) if we never find the matching opening brace.  Note\n    we ignore strings and comments when matching; and the line we\n    return is the 'cleansed' line at linenum.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  if line[pos] not in ')}]>':\n    return (line, 0, -1)\n\n  # Check last line\n  (start_pos, stack) = FindStartOfExpressionInLine(line, pos, [])\n  if start_pos > -1:\n    return (line, linenum, start_pos)\n\n  # Continue scanning backward\n  while stack and linenum > 0:\n    linenum -= 1\n    line = clean_lines.elided[linenum]\n    (start_pos, stack) = FindStartOfExpressionInLine(line, len(line) - 1, stack)\n    if start_pos > -1:\n      return (line, linenum, start_pos)\n\n  # Did not find start of expression before beginning of file, give up\n  return (line, 0, -1)\n\n\ndef CheckForCopyright(filename, lines, error):\n  \"\"\"Logs an error if no Copyright message appears at the top of the file.\"\"\"\n\n  # We'll say it should occur by line 10. Don't forget there's a\n  # placeholder line at the front.\n  for line in xrange(1, min(len(lines), 11)):\n    if re.search(r'Copyright', lines[line], re.I): break\n  else:                       # means no copyright line was found\n    error(filename, 0, 'legal/copyright', 5,\n          'No copyright message found.  '\n          'You should have a line: \"Copyright [year] <Copyright Owner>\"')\n\n\ndef GetIndentLevel(line):\n  \"\"\"Return the number of leading spaces in line.\n\n  Args:\n    line: A string to check.\n\n  Returns:\n    An integer count of leading spaces, possibly zero.\n  \"\"\"\n  indent = Match(r'^( *)\\S', line)\n  if indent:\n    return len(indent.group(1))\n  else:\n    return 0\n\ndef PathSplitToList(path):\n  \"\"\"Returns the path split into a list by the separator.\n\n  Args:\n    path: An absolute or relative path (e.g. '/a/b/c/' or '../a')\n\n  Returns:\n    A list of path components (e.g. ['a', 'b', 'c]).\n  \"\"\"\n  lst = []\n  while True:\n    (head, tail) = os.path.split(path)\n    if head == path:  # absolute paths end\n      lst.append(head)\n      break\n    if tail == path:  # relative paths end\n      lst.append(tail)\n      break\n\n    path = head\n    lst.append(tail)\n\n  lst.reverse()\n  return lst\n\ndef GetHeaderGuardCPPVariable(filename):\n  \"\"\"Returns the CPP variable that should be used as a header guard.\n\n  Args:\n    filename: The name of a C++ header file.\n\n  Returns:\n    The CPP variable that should be used as a header guard in the\n    named file.\n\n  \"\"\"\n\n  # Restores original filename in case that cpplint is invoked from Emacs's\n  # flymake.\n  filename = re.sub(r'_flymake\\.h$', '.h', filename)\n  filename = re.sub(r'/\\.flymake/([^/]*)$', r'/\\1', filename)\n  # Replace 'c++' with 'cpp'.\n  filename = filename.replace('C++', 'cpp').replace('c++', 'cpp')\n\n  fileinfo = FileInfo(filename)\n  file_path_from_root = fileinfo.RepositoryName()\n\n  def FixupPathFromRoot():\n    if _root_debug:\n      sys.stderr.write(\"\\n_root fixup, _root = '%s', repository name = '%s'\\n\"\n          % (_root, fileinfo.RepositoryName()))\n\n    # Process the file path with the --root flag if it was set.\n    if not _root:\n      if _root_debug:\n        sys.stderr.write(\"_root unspecified\\n\")\n      return file_path_from_root\n\n    def StripListPrefix(lst, prefix):\n      # f(['x', 'y'], ['w, z']) -> None  (not a valid prefix)\n      if lst[:len(prefix)] != prefix:\n        return None\n      # f(['a, 'b', 'c', 'd'], ['a', 'b']) -> ['c', 'd']\n      return lst[(len(prefix)):]\n\n    # root behavior:\n    #   --root=subdir , lstrips subdir from the header guard\n    maybe_path = StripListPrefix(PathSplitToList(file_path_from_root),\n                                 PathSplitToList(_root))\n\n    if _root_debug:\n      sys.stderr.write((\"_root lstrip (maybe_path=%s, file_path_from_root=%s,\" +\n          \" _root=%s)\\n\") % (maybe_path, file_path_from_root, _root))\n\n    if maybe_path:\n      return os.path.join(*maybe_path)\n\n    #   --root=.. , will prepend the outer directory to the header guard\n    full_path = fileinfo.FullName()\n    # adapt slashes for windows\n    root_abspath = os.path.abspath(_root).replace('\\\\', '/')\n\n    maybe_path = StripListPrefix(PathSplitToList(full_path),\n                                 PathSplitToList(root_abspath))\n\n    if _root_debug:\n      sys.stderr.write((\"_root prepend (maybe_path=%s, full_path=%s, \" +\n          \"root_abspath=%s)\\n\") % (maybe_path, full_path, root_abspath))\n\n    if maybe_path:\n      return os.path.join(*maybe_path)\n\n    if _root_debug:\n      sys.stderr.write(\"_root ignore, returning %s\\n\" % (file_path_from_root))\n\n    #   --root=FAKE_DIR is ignored\n    return file_path_from_root\n\n  file_path_from_root = FixupPathFromRoot()\n  return re.sub(r'[^a-zA-Z0-9]', '_', file_path_from_root).upper() + '_'\n\n\ndef CheckForHeaderGuard(filename, clean_lines, error):\n  \"\"\"Checks that the file contains a header guard.\n\n  Logs an error if no #ifndef header guard is present.  For other\n  headers, checks that the full pathname is used.\n\n  Args:\n    filename: The name of the C++ header file.\n    clean_lines: A CleansedLines instance containing the file.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Don't check for header guards if there are error suppression\n  # comments somewhere in this file.\n  #\n  # Because this is silencing a warning for a nonexistent line, we\n  # only support the very specific NOLINT(build/header_guard) syntax,\n  # and not the general NOLINT or NOLINT(*) syntax.\n  raw_lines = clean_lines.lines_without_raw_strings\n  for i in raw_lines:\n    if Search(r'//\\s*NOLINT\\(build/header_guard\\)', i):\n      return\n\n  # Allow pragma once instead of header guards\n  for i in raw_lines:\n    if Search(r'^\\s*#pragma\\s+once', i):\n      return\n\n  cppvar = GetHeaderGuardCPPVariable(filename)\n\n  ifndef = ''\n  ifndef_linenum = 0\n  define = ''\n  endif = ''\n  endif_linenum = 0\n  for linenum, line in enumerate(raw_lines):\n    linesplit = line.split()\n    if len(linesplit) >= 2:\n      # find the first occurrence of #ifndef and #define, save arg\n      if not ifndef and linesplit[0] == '#ifndef':\n        # set ifndef to the header guard presented on the #ifndef line.\n        ifndef = linesplit[1]\n        ifndef_linenum = linenum\n      if not define and linesplit[0] == '#define':\n        define = linesplit[1]\n    # find the last occurrence of #endif, save entire line\n    if line.startswith('#endif'):\n      endif = line\n      endif_linenum = linenum\n\n  if not ifndef or not define or ifndef != define:\n    error(filename, 0, 'build/header_guard', 5,\n          'No #ifndef header guard found, suggested CPP variable is: %s' %\n          cppvar)\n    return\n\n  # The guard should be PATH_FILE_H_, but we also allow PATH_FILE_H__\n  # for backward compatibility.\n  if ifndef != cppvar:\n    error_level = 0\n    if ifndef != cppvar + '_':\n      error_level = 5\n\n    ParseNolintSuppressions(filename, raw_lines[ifndef_linenum], ifndef_linenum,\n                            error)\n    error(filename, ifndef_linenum, 'build/header_guard', error_level,\n          '#ifndef header guard has wrong style, please use: %s' % cppvar)\n\n  # Check for \"//\" comments on endif line.\n  ParseNolintSuppressions(filename, raw_lines[endif_linenum], endif_linenum,\n                          error)\n  match = Match(r'#endif\\s*//\\s*' + cppvar + r'(_)?\\b', endif)\n  if match:\n    if match.group(1) == '_':\n      # Issue low severity warning for deprecated double trailing underscore\n      error(filename, endif_linenum, 'build/header_guard', 0,\n            '#endif line should be \"#endif  // %s\"' % cppvar)\n    return\n\n  # Didn't find the corresponding \"//\" comment.  If this file does not\n  # contain any \"//\" comments at all, it could be that the compiler\n  # only wants \"/**/\" comments, look for those instead.\n  no_single_line_comments = True\n  for i in xrange(1, len(raw_lines) - 1):\n    line = raw_lines[i]\n    if Match(r'^(?:(?:\\'(?:\\.|[^\\'])*\\')|(?:\"(?:\\.|[^\"])*\")|[^\\'\"])*//', line):\n      no_single_line_comments = False\n      break\n\n  if no_single_line_comments:\n    match = Match(r'#endif\\s*/\\*\\s*' + cppvar + r'(_)?\\s*\\*/', endif)\n    if match:\n      if match.group(1) == '_':\n        # Low severity warning for double trailing underscore\n        error(filename, endif_linenum, 'build/header_guard', 0,\n              '#endif line should be \"#endif  /* %s */\"' % cppvar)\n      return\n\n  # Didn't find anything\n  error(filename, endif_linenum, 'build/header_guard', 5,\n        '#endif line should be \"#endif  // %s\"' % cppvar)\n\n\ndef CheckHeaderFileIncluded(filename, include_state, error):\n  \"\"\"Logs an error if a source file does not include its header.\"\"\"\n\n  # Do not check test files\n  fileinfo = FileInfo(filename)\n  if Search(_TEST_FILE_SUFFIX, fileinfo.BaseName()):\n    return\n\n  for ext in GetHeaderExtensions():\n    basefilename = filename[0:len(filename) - len(fileinfo.Extension())]\n    headerfile = basefilename + '.' + ext\n    if not os.path.exists(headerfile):\n      continue\n    headername = FileInfo(headerfile).RepositoryName()\n    first_include = None\n    include_uses_unix_dir_aliases = False\n    for section_list in include_state.include_list:\n      for f in section_list:\n        include_text = f[0]\n        if \"./\" in include_text:\n          include_uses_unix_dir_aliases = True\n        if headername in include_text or include_text in headername:\n          return\n        if not first_include:\n          first_include = f[1]\n\n    message = '%s should include its header file %s' % (fileinfo.RepositoryName(), headername)\n    if include_uses_unix_dir_aliases:\n      message += \". Relative paths like . and .. are not allowed.\"\n\n    error(filename, first_include, 'build/include', 5, message)\n\n\ndef CheckForBadCharacters(filename, lines, error):\n  \"\"\"Logs an error for each line containing bad characters.\n\n  Two kinds of bad characters:\n\n  1. Unicode replacement characters: These indicate that either the file\n  contained invalid UTF-8 (likely) or Unicode replacement characters (which\n  it shouldn't).  Note that it's possible for this to throw off line\n  numbering if the invalid UTF-8 occurred adjacent to a newline.\n\n  2. NUL bytes.  These are problematic for some tools.\n\n  Args:\n    filename: The name of the current file.\n    lines: An array of strings, each representing a line of the file.\n    error: The function to call with any errors found.\n  \"\"\"\n  for linenum, line in enumerate(lines):\n    if unicode_escape_decode('\\ufffd') in line:\n      error(filename, linenum, 'readability/utf8', 5,\n            'Line contains invalid UTF-8 (or Unicode replacement character).')\n    if '\\0' in line:\n      error(filename, linenum, 'readability/nul', 5, 'Line contains NUL byte.')\n\n\ndef CheckForNewlineAtEOF(filename, lines, error):\n  \"\"\"Logs an error if there is no newline char at the end of the file.\n\n  Args:\n    filename: The name of the current file.\n    lines: An array of strings, each representing a line of the file.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # The array lines() was created by adding two newlines to the\n  # original file (go figure), then splitting on \\n.\n  # To verify that the file ends in \\n, we just have to make sure the\n  # last-but-two element of lines() exists and is empty.\n  if len(lines) < 3 or lines[-2]:\n    error(filename, len(lines) - 2, 'whitespace/ending_newline', 5,\n          'Could not find a newline character at the end of the file.')\n\n\ndef CheckForMultilineCommentsAndStrings(filename, clean_lines, linenum, error):\n  \"\"\"Logs an error if we see /* ... */ or \"...\" that extend past one line.\n\n  /* ... */ comments are legit inside macros, for one line.\n  Otherwise, we prefer // comments, so it's ok to warn about the\n  other.  Likewise, it's ok for strings to extend across multiple\n  lines, as long as a line continuation character (backslash)\n  terminates each line. Although not currently prohibited by the C++\n  style guide, it's ugly and unnecessary. We don't do well with either\n  in this lint program, so we warn about both.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Remove all \\\\ (escaped backslashes) from the line. They are OK, and the\n  # second (escaped) slash may trigger later \\\" detection erroneously.\n  line = line.replace('\\\\\\\\', '')\n\n  if line.count('/*') > line.count('*/'):\n    error(filename, linenum, 'readability/multiline_comment', 5,\n          'Complex multi-line /*...*/-style comment found. '\n          'Lint may give bogus warnings.  '\n          'Consider replacing these with //-style comments, '\n          'with #if 0...#endif, '\n          'or with more clearly structured multi-line comments.')\n\n  if (line.count('\"') - line.count('\\\\\"')) % 2:\n    error(filename, linenum, 'readability/multiline_string', 5,\n          'Multi-line string (\"...\") found.  This lint script doesn\\'t '\n          'do well with such strings, and may give bogus warnings.  '\n          'Use C++11 raw strings or concatenation instead.')\n\n\n# (non-threadsafe name, thread-safe alternative, validation pattern)\n#\n# The validation pattern is used to eliminate false positives such as:\n#  _rand();               // false positive due to substring match.\n#  ->rand();              // some member function rand().\n#  ACMRandom rand(seed);  // some variable named rand.\n#  ISAACRandom rand();    // another variable named rand.\n#\n# Basically we require the return value of these functions to be used\n# in some expression context on the same line by matching on some\n# operator before the function name.  This eliminates constructors and\n# member function calls.\n_UNSAFE_FUNC_PREFIX = r'(?:[-+*/=%^&|(<]\\s*|>\\s+)'\n_THREADING_LIST = (\n    ('asctime(', 'asctime_r(', _UNSAFE_FUNC_PREFIX + r'asctime\\([^)]+\\)'),\n    ('ctime(', 'ctime_r(', _UNSAFE_FUNC_PREFIX + r'ctime\\([^)]+\\)'),\n    ('getgrgid(', 'getgrgid_r(', _UNSAFE_FUNC_PREFIX + r'getgrgid\\([^)]+\\)'),\n    ('getgrnam(', 'getgrnam_r(', _UNSAFE_FUNC_PREFIX + r'getgrnam\\([^)]+\\)'),\n    ('getlogin(', 'getlogin_r(', _UNSAFE_FUNC_PREFIX + r'getlogin\\(\\)'),\n    ('getpwnam(', 'getpwnam_r(', _UNSAFE_FUNC_PREFIX + r'getpwnam\\([^)]+\\)'),\n    ('getpwuid(', 'getpwuid_r(', _UNSAFE_FUNC_PREFIX + r'getpwuid\\([^)]+\\)'),\n    ('gmtime(', 'gmtime_r(', _UNSAFE_FUNC_PREFIX + r'gmtime\\([^)]+\\)'),\n    ('localtime(', 'localtime_r(', _UNSAFE_FUNC_PREFIX + r'localtime\\([^)]+\\)'),\n    ('rand(', 'rand_r(', _UNSAFE_FUNC_PREFIX + r'rand\\(\\)'),\n    ('strtok(', 'strtok_r(',\n     _UNSAFE_FUNC_PREFIX + r'strtok\\([^)]+\\)'),\n    ('ttyname(', 'ttyname_r(', _UNSAFE_FUNC_PREFIX + r'ttyname\\([^)]+\\)'),\n    )\n\n\ndef CheckPosixThreading(filename, clean_lines, linenum, error):\n  \"\"\"Checks for calls to thread-unsafe functions.\n\n  Much code has been originally written without consideration of\n  multi-threading. Also, engineers are relying on their old experience;\n  they have learned posix before threading extensions were added. These\n  tests guide the engineers to use thread-safe functions (when using\n  posix directly).\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  for single_thread_func, multithread_safe_func, pattern in _THREADING_LIST:\n    # Additional pattern matching check to confirm that this is the\n    # function we are looking for\n    if Search(pattern, line):\n      error(filename, linenum, 'runtime/threadsafe_fn', 2,\n            'Consider using ' + multithread_safe_func +\n            '...) instead of ' + single_thread_func +\n            '...) for improved thread safety.')\n\n\ndef CheckVlogArguments(filename, clean_lines, linenum, error):\n  \"\"\"Checks that VLOG() is only used for defining a logging level.\n\n  For example, VLOG(2) is correct. VLOG(INFO), VLOG(WARNING), VLOG(ERROR), and\n  VLOG(FATAL) are not.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  if Search(r'\\bVLOG\\((INFO|ERROR|WARNING|DFATAL|FATAL)\\)', line):\n    error(filename, linenum, 'runtime/vlog', 5,\n          'VLOG() should be used with numeric verbosity level.  '\n          'Use LOG() if you want symbolic severity levels.')\n\n# Matches invalid increment: *count++, which moves pointer instead of\n# incrementing a value.\n_RE_PATTERN_INVALID_INCREMENT = re.compile(\n    r'^\\s*\\*\\w+(\\+\\+|--);')\n\n\ndef CheckInvalidIncrement(filename, clean_lines, linenum, error):\n  \"\"\"Checks for invalid increment *count++.\n\n  For example following function:\n  void increment_counter(int* count) {\n    *count++;\n  }\n  is invalid, because it effectively does count++, moving pointer, and should\n  be replaced with ++*count, (*count)++ or *count += 1.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  if _RE_PATTERN_INVALID_INCREMENT.match(line):\n    error(filename, linenum, 'runtime/invalid_increment', 5,\n          'Changing pointer instead of value (or unused value of operator*).')\n\n\ndef IsMacroDefinition(clean_lines, linenum):\n  if Search(r'^#define', clean_lines[linenum]):\n    return True\n\n  if linenum > 0 and Search(r'\\\\$', clean_lines[linenum - 1]):\n    return True\n\n  return False\n\n\ndef IsForwardClassDeclaration(clean_lines, linenum):\n  return Match(r'^\\s*(\\btemplate\\b)*.*class\\s+\\w+;\\s*$', clean_lines[linenum])\n\n\nclass _BlockInfo(object):\n  \"\"\"Stores information about a generic block of code.\"\"\"\n\n  def __init__(self, linenum, seen_open_brace):\n    self.starting_linenum = linenum\n    self.seen_open_brace = seen_open_brace\n    self.open_parentheses = 0\n    self.inline_asm = _NO_ASM\n    self.check_namespace_indentation = False\n\n  def CheckBegin(self, filename, clean_lines, linenum, error):\n    \"\"\"Run checks that applies to text up to the opening brace.\n\n    This is mostly for checking the text after the class identifier\n    and the \"{\", usually where the base class is specified.  For other\n    blocks, there isn't much to check, so we always pass.\n\n    Args:\n      filename: The name of the current file.\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      error: The function to call with any errors found.\n    \"\"\"\n    pass\n\n  def CheckEnd(self, filename, clean_lines, linenum, error):\n    \"\"\"Run checks that applies to text after the closing brace.\n\n    This is mostly used for checking end of namespace comments.\n\n    Args:\n      filename: The name of the current file.\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      error: The function to call with any errors found.\n    \"\"\"\n    pass\n\n  def IsBlockInfo(self):\n    \"\"\"Returns true if this block is a _BlockInfo.\n\n    This is convenient for verifying that an object is an instance of\n    a _BlockInfo, but not an instance of any of the derived classes.\n\n    Returns:\n      True for this class, False for derived classes.\n    \"\"\"\n    return self.__class__ == _BlockInfo\n\n\nclass _ExternCInfo(_BlockInfo):\n  \"\"\"Stores information about an 'extern \"C\"' block.\"\"\"\n\n  def __init__(self, linenum):\n    _BlockInfo.__init__(self, linenum, True)\n\n\nclass _ClassInfo(_BlockInfo):\n  \"\"\"Stores information about a class.\"\"\"\n\n  def __init__(self, name, class_or_struct, clean_lines, linenum):\n    _BlockInfo.__init__(self, linenum, False)\n    self.name = name\n    self.is_derived = False\n    self.check_namespace_indentation = True\n    if class_or_struct == 'struct':\n      self.access = 'public'\n      self.is_struct = True\n    else:\n      self.access = 'private'\n      self.is_struct = False\n\n    # Remember initial indentation level for this class.  Using raw_lines here\n    # instead of elided to account for leading comments.\n    self.class_indent = GetIndentLevel(clean_lines.raw_lines[linenum])\n\n    # Try to find the end of the class.  This will be confused by things like:\n    #   class A {\n    #   } *x = { ...\n    #\n    # But it's still good enough for CheckSectionSpacing.\n    self.last_line = 0\n    depth = 0\n    for i in range(linenum, clean_lines.NumLines()):\n      line = clean_lines.elided[i]\n      depth += line.count('{') - line.count('}')\n      if not depth:\n        self.last_line = i\n        break\n\n  def CheckBegin(self, filename, clean_lines, linenum, error):\n    # Look for a bare ':'\n    if Search('(^|[^:]):($|[^:])', clean_lines.elided[linenum]):\n      self.is_derived = True\n\n  def CheckEnd(self, filename, clean_lines, linenum, error):\n    # If there is a DISALLOW macro, it should appear near the end of\n    # the class.\n    seen_last_thing_in_class = False\n    for i in xrange(linenum - 1, self.starting_linenum, -1):\n      match = Search(\n          r'\\b(DISALLOW_COPY_AND_ASSIGN|DISALLOW_IMPLICIT_CONSTRUCTORS)\\(' +\n          self.name + r'\\)',\n          clean_lines.elided[i])\n      if match:\n        if seen_last_thing_in_class:\n          error(filename, i, 'readability/constructors', 3,\n                match.group(1) + ' should be the last thing in the class')\n        break\n\n      if not Match(r'^\\s*$', clean_lines.elided[i]):\n        seen_last_thing_in_class = True\n\n    # Check that closing brace is aligned with beginning of the class.\n    # Only do this if the closing brace is indented by only whitespaces.\n    # This means we will not check single-line class definitions.\n    indent = Match(r'^( *)\\}', clean_lines.elided[linenum])\n    if indent and len(indent.group(1)) != self.class_indent:\n      if self.is_struct:\n        parent = 'struct ' + self.name\n      else:\n        parent = 'class ' + self.name\n      error(filename, linenum, 'whitespace/indent', 3,\n            'Closing brace should be aligned with beginning of %s' % parent)\n\n\nclass _NamespaceInfo(_BlockInfo):\n  \"\"\"Stores information about a namespace.\"\"\"\n\n  def __init__(self, name, linenum):\n    _BlockInfo.__init__(self, linenum, False)\n    self.name = name or ''\n    self.check_namespace_indentation = True\n\n  def CheckEnd(self, filename, clean_lines, linenum, error):\n    \"\"\"Check end of namespace comments.\"\"\"\n    line = clean_lines.raw_lines[linenum]\n\n    # Check how many lines is enclosed in this namespace.  Don't issue\n    # warning for missing namespace comments if there aren't enough\n    # lines.  However, do apply checks if there is already an end of\n    # namespace comment and it's incorrect.\n    #\n    # TODO(unknown): We always want to check end of namespace comments\n    # if a namespace is large, but sometimes we also want to apply the\n    # check if a short namespace contained nontrivial things (something\n    # other than forward declarations).  There is currently no logic on\n    # deciding what these nontrivial things are, so this check is\n    # triggered by namespace size only, which works most of the time.\n    if (linenum - self.starting_linenum < 10\n        and not Match(r'^\\s*};*\\s*(//|/\\*).*\\bnamespace\\b', line)):\n      return\n\n    # Look for matching comment at end of namespace.\n    #\n    # Note that we accept C style \"/* */\" comments for terminating\n    # namespaces, so that code that terminate namespaces inside\n    # preprocessor macros can be cpplint clean.\n    #\n    # We also accept stuff like \"// end of namespace <name>.\" with the\n    # period at the end.\n    #\n    # Besides these, we don't accept anything else, otherwise we might\n    # get false negatives when existing comment is a substring of the\n    # expected namespace.\n    if self.name:\n      # Named namespace\n      if not Match((r'^\\s*};*\\s*(//|/\\*).*\\bnamespace\\s+' +\n                    re.escape(self.name) + r'[\\*/\\.\\\\\\s]*$'),\n                   line):\n        error(filename, linenum, 'readability/namespace', 5,\n              'Namespace should be terminated with \"// namespace %s\"' %\n              self.name)\n    else:\n      # Anonymous namespace\n      if not Match(r'^\\s*};*\\s*(//|/\\*).*\\bnamespace[\\*/\\.\\\\\\s]*$', line):\n        # If \"// namespace anonymous\" or \"// anonymous namespace (more text)\",\n        # mention \"// anonymous namespace\" as an acceptable form\n        if Match(r'^\\s*}.*\\b(namespace anonymous|anonymous namespace)\\b', line):\n          error(filename, linenum, 'readability/namespace', 5,\n                'Anonymous namespace should be terminated with \"// namespace\"'\n                ' or \"// anonymous namespace\"')\n        else:\n          error(filename, linenum, 'readability/namespace', 5,\n                'Anonymous namespace should be terminated with \"// namespace\"')\n\n\nclass _PreprocessorInfo(object):\n  \"\"\"Stores checkpoints of nesting stacks when #if/#else is seen.\"\"\"\n\n  def __init__(self, stack_before_if):\n    # The entire nesting stack before #if\n    self.stack_before_if = stack_before_if\n\n    # The entire nesting stack up to #else\n    self.stack_before_else = []\n\n    # Whether we have already seen #else or #elif\n    self.seen_else = False\n\n\nclass NestingState(object):\n  \"\"\"Holds states related to parsing braces.\"\"\"\n\n  def __init__(self):\n    # Stack for tracking all braces.  An object is pushed whenever we\n    # see a \"{\", and popped when we see a \"}\".  Only 3 types of\n    # objects are possible:\n    # - _ClassInfo: a class or struct.\n    # - _NamespaceInfo: a namespace.\n    # - _BlockInfo: some other type of block.\n    self.stack = []\n\n    # Top of the previous stack before each Update().\n    #\n    # Because the nesting_stack is updated at the end of each line, we\n    # had to do some convoluted checks to find out what is the current\n    # scope at the beginning of the line.  This check is simplified by\n    # saving the previous top of nesting stack.\n    #\n    # We could save the full stack, but we only need the top.  Copying\n    # the full nesting stack would slow down cpplint by ~10%.\n    self.previous_stack_top = []\n\n    # Stack of _PreprocessorInfo objects.\n    self.pp_stack = []\n\n  def SeenOpenBrace(self):\n    \"\"\"Check if we have seen the opening brace for the innermost block.\n\n    Returns:\n      True if we have seen the opening brace, False if the innermost\n      block is still expecting an opening brace.\n    \"\"\"\n    return (not self.stack) or self.stack[-1].seen_open_brace\n\n  def InNamespaceBody(self):\n    \"\"\"Check if we are currently one level inside a namespace body.\n\n    Returns:\n      True if top of the stack is a namespace block, False otherwise.\n    \"\"\"\n    return self.stack and isinstance(self.stack[-1], _NamespaceInfo)\n\n  def InExternC(self):\n    \"\"\"Check if we are currently one level inside an 'extern \"C\"' block.\n\n    Returns:\n      True if top of the stack is an extern block, False otherwise.\n    \"\"\"\n    return self.stack and isinstance(self.stack[-1], _ExternCInfo)\n\n  def InClassDeclaration(self):\n    \"\"\"Check if we are currently one level inside a class or struct declaration.\n\n    Returns:\n      True if top of the stack is a class/struct, False otherwise.\n    \"\"\"\n    return self.stack and isinstance(self.stack[-1], _ClassInfo)\n\n  def InAsmBlock(self):\n    \"\"\"Check if we are currently one level inside an inline ASM block.\n\n    Returns:\n      True if the top of the stack is a block containing inline ASM.\n    \"\"\"\n    return self.stack and self.stack[-1].inline_asm != _NO_ASM\n\n  def InTemplateArgumentList(self, clean_lines, linenum, pos):\n    \"\"\"Check if current position is inside template argument list.\n\n    Args:\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      pos: position just after the suspected template argument.\n    Returns:\n      True if (linenum, pos) is inside template arguments.\n    \"\"\"\n    while linenum < clean_lines.NumLines():\n      # Find the earliest character that might indicate a template argument\n      line = clean_lines.elided[linenum]\n      match = Match(r'^[^{};=\\[\\]\\.<>]*(.)', line[pos:])\n      if not match:\n        linenum += 1\n        pos = 0\n        continue\n      token = match.group(1)\n      pos += len(match.group(0))\n\n      # These things do not look like template argument list:\n      #   class Suspect {\n      #   class Suspect x; }\n      if token in ('{', '}', ';'): return False\n\n      # These things look like template argument list:\n      #   template <class Suspect>\n      #   template <class Suspect = default_value>\n      #   template <class Suspect[]>\n      #   template <class Suspect...>\n      if token in ('>', '=', '[', ']', '.'): return True\n\n      # Check if token is an unmatched '<'.\n      # If not, move on to the next character.\n      if token != '<':\n        pos += 1\n        if pos >= len(line):\n          linenum += 1\n          pos = 0\n        continue\n\n      # We can't be sure if we just find a single '<', and need to\n      # find the matching '>'.\n      (_, end_line, end_pos) = CloseExpression(clean_lines, linenum, pos - 1)\n      if end_pos < 0:\n        # Not sure if template argument list or syntax error in file\n        return False\n      linenum = end_line\n      pos = end_pos\n    return False\n\n  def UpdatePreprocessor(self, line):\n    \"\"\"Update preprocessor stack.\n\n    We need to handle preprocessors due to classes like this:\n      #ifdef SWIG\n      struct ResultDetailsPageElementExtensionPoint {\n      #else\n      struct ResultDetailsPageElementExtensionPoint : public Extension {\n      #endif\n\n    We make the following assumptions (good enough for most files):\n    - Preprocessor condition evaluates to true from #if up to first\n      #else/#elif/#endif.\n\n    - Preprocessor condition evaluates to false from #else/#elif up\n      to #endif.  We still perform lint checks on these lines, but\n      these do not affect nesting stack.\n\n    Args:\n      line: current line to check.\n    \"\"\"\n    if Match(r'^\\s*#\\s*(if|ifdef|ifndef)\\b', line):\n      # Beginning of #if block, save the nesting stack here.  The saved\n      # stack will allow us to restore the parsing state in the #else case.\n      self.pp_stack.append(_PreprocessorInfo(copy.deepcopy(self.stack)))\n    elif Match(r'^\\s*#\\s*(else|elif)\\b', line):\n      # Beginning of #else block\n      if self.pp_stack:\n        if not self.pp_stack[-1].seen_else:\n          # This is the first #else or #elif block.  Remember the\n          # whole nesting stack up to this point.  This is what we\n          # keep after the #endif.\n          self.pp_stack[-1].seen_else = True\n          self.pp_stack[-1].stack_before_else = copy.deepcopy(self.stack)\n\n        # Restore the stack to how it was before the #if\n        self.stack = copy.deepcopy(self.pp_stack[-1].stack_before_if)\n      else:\n        # TODO(unknown): unexpected #else, issue warning?\n        pass\n    elif Match(r'^\\s*#\\s*endif\\b', line):\n      # End of #if or #else blocks.\n      if self.pp_stack:\n        # If we saw an #else, we will need to restore the nesting\n        # stack to its former state before the #else, otherwise we\n        # will just continue from where we left off.\n        if self.pp_stack[-1].seen_else:\n          # Here we can just use a shallow copy since we are the last\n          # reference to it.\n          self.stack = self.pp_stack[-1].stack_before_else\n        # Drop the corresponding #if\n        self.pp_stack.pop()\n      else:\n        # TODO(unknown): unexpected #endif, issue warning?\n        pass\n\n  # TODO(unknown): Update() is too long, but we will refactor later.\n  def Update(self, filename, clean_lines, linenum, error):\n    \"\"\"Update nesting state with current line.\n\n    Args:\n      filename: The name of the current file.\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      error: The function to call with any errors found.\n    \"\"\"\n    line = clean_lines.elided[linenum]\n\n    # Remember top of the previous nesting stack.\n    #\n    # The stack is always pushed/popped and not modified in place, so\n    # we can just do a shallow copy instead of copy.deepcopy.  Using\n    # deepcopy would slow down cpplint by ~28%.\n    if self.stack:\n      self.previous_stack_top = self.stack[-1]\n    else:\n      self.previous_stack_top = None\n\n    # Update pp_stack\n    self.UpdatePreprocessor(line)\n\n    # Count parentheses.  This is to avoid adding struct arguments to\n    # the nesting stack.\n    if self.stack:\n      inner_block = self.stack[-1]\n      depth_change = line.count('(') - line.count(')')\n      inner_block.open_parentheses += depth_change\n\n      # Also check if we are starting or ending an inline assembly block.\n      if inner_block.inline_asm in (_NO_ASM, _END_ASM):\n        if (depth_change != 0 and\n            inner_block.open_parentheses == 1 and\n            _MATCH_ASM.match(line)):\n          # Enter assembly block\n          inner_block.inline_asm = _INSIDE_ASM\n        else:\n          # Not entering assembly block.  If previous line was _END_ASM,\n          # we will now shift to _NO_ASM state.\n          inner_block.inline_asm = _NO_ASM\n      elif (inner_block.inline_asm == _INSIDE_ASM and\n            inner_block.open_parentheses == 0):\n        # Exit assembly block\n        inner_block.inline_asm = _END_ASM\n\n    # Consume namespace declaration at the beginning of the line.  Do\n    # this in a loop so that we catch same line declarations like this:\n    #   namespace proto2 { namespace bridge { class MessageSet; } }\n    while True:\n      # Match start of namespace.  The \"\\b\\s*\" below catches namespace\n      # declarations even if it weren't followed by a whitespace, this\n      # is so that we don't confuse our namespace checker.  The\n      # missing spaces will be flagged by CheckSpacing.\n      namespace_decl_match = Match(r'^\\s*namespace\\b\\s*([:\\w]+)?(.*)$', line)\n      if not namespace_decl_match:\n        break\n\n      new_namespace = _NamespaceInfo(namespace_decl_match.group(1), linenum)\n      self.stack.append(new_namespace)\n\n      line = namespace_decl_match.group(2)\n      if line.find('{') != -1:\n        new_namespace.seen_open_brace = True\n        line = line[line.find('{') + 1:]\n\n    # Look for a class declaration in whatever is left of the line\n    # after parsing namespaces.  The regexp accounts for decorated classes\n    # such as in:\n    #   class LOCKABLE API Object {\n    #   };\n    class_decl_match = Match(\n        r'^(\\s*(?:template\\s*<[\\w\\s<>,:=]*>\\s*)?'\n        r'(class|struct)\\s+(?:[a-zA-Z0-9_]+\\s+)*(\\w+(?:::\\w+)*))'\n        r'(.*)$', line)\n    if (class_decl_match and\n        (not self.stack or self.stack[-1].open_parentheses == 0)):\n      # We do not want to accept classes that are actually template arguments:\n      #   template <class Ignore1,\n      #             class Ignore2 = Default<Args>,\n      #             template <Args> class Ignore3>\n      #   void Function() {};\n      #\n      # To avoid template argument cases, we scan forward and look for\n      # an unmatched '>'.  If we see one, assume we are inside a\n      # template argument list.\n      end_declaration = len(class_decl_match.group(1))\n      if not self.InTemplateArgumentList(clean_lines, linenum, end_declaration):\n        self.stack.append(_ClassInfo(\n            class_decl_match.group(3), class_decl_match.group(2),\n            clean_lines, linenum))\n        line = class_decl_match.group(4)\n\n    # If we have not yet seen the opening brace for the innermost block,\n    # run checks here.\n    if not self.SeenOpenBrace():\n      self.stack[-1].CheckBegin(filename, clean_lines, linenum, error)\n\n    # Update access control if we are inside a class/struct\n    if self.stack and isinstance(self.stack[-1], _ClassInfo):\n      classinfo = self.stack[-1]\n      access_match = Match(\n          r'^(.*)\\b(public|private|protected|signals)(\\s+(?:slots\\s*)?)?'\n          r':(?:[^:]|$)',\n          line)\n      if access_match:\n        classinfo.access = access_match.group(2)\n\n        # Check that access keywords are indented +1 space.  Skip this\n        # check if the keywords are not preceded by whitespaces.\n        indent = access_match.group(1)\n        if (len(indent) != classinfo.class_indent + 1 and\n            Match(r'^\\s*$', indent)):\n          if classinfo.is_struct:\n            parent = 'struct ' + classinfo.name\n          else:\n            parent = 'class ' + classinfo.name\n          slots = ''\n          if access_match.group(3):\n            slots = access_match.group(3)\n          error(filename, linenum, 'whitespace/indent', 3,\n                '%s%s: should be indented +1 space inside %s' % (\n                    access_match.group(2), slots, parent))\n\n    # Consume braces or semicolons from what's left of the line\n    while True:\n      # Match first brace, semicolon, or closed parenthesis.\n      matched = Match(r'^[^{;)}]*([{;)}])(.*)$', line)\n      if not matched:\n        break\n\n      token = matched.group(1)\n      if token == '{':\n        # If namespace or class hasn't seen a opening brace yet, mark\n        # namespace/class head as complete.  Push a new block onto the\n        # stack otherwise.\n        if not self.SeenOpenBrace():\n          self.stack[-1].seen_open_brace = True\n        elif Match(r'^extern\\s*\"[^\"]*\"\\s*\\{', line):\n          self.stack.append(_ExternCInfo(linenum))\n        else:\n          self.stack.append(_BlockInfo(linenum, True))\n          if _MATCH_ASM.match(line):\n            self.stack[-1].inline_asm = _BLOCK_ASM\n\n      elif token == ';' or token == ')':\n        # If we haven't seen an opening brace yet, but we already saw\n        # a semicolon, this is probably a forward declaration.  Pop\n        # the stack for these.\n        #\n        # Similarly, if we haven't seen an opening brace yet, but we\n        # already saw a closing parenthesis, then these are probably\n        # function arguments with extra \"class\" or \"struct\" keywords.\n        # Also pop these stack for these.\n        if not self.SeenOpenBrace():\n          self.stack.pop()\n      else:  # token == '}'\n        # Perform end of block checks and pop the stack.\n        if self.stack:\n          self.stack[-1].CheckEnd(filename, clean_lines, linenum, error)\n          self.stack.pop()\n      line = matched.group(2)\n\n  def InnermostClass(self):\n    \"\"\"Get class info on the top of the stack.\n\n    Returns:\n      A _ClassInfo object if we are inside a class, or None otherwise.\n    \"\"\"\n    for i in range(len(self.stack), 0, -1):\n      classinfo = self.stack[i - 1]\n      if isinstance(classinfo, _ClassInfo):\n        return classinfo\n    return None\n\n  def CheckCompletedBlocks(self, filename, error):\n    \"\"\"Checks that all classes and namespaces have been completely parsed.\n\n    Call this when all lines in a file have been processed.\n    Args:\n      filename: The name of the current file.\n      error: The function to call with any errors found.\n    \"\"\"\n    # Note: This test can result in false positives if #ifdef constructs\n    # get in the way of brace matching. See the testBuildClass test in\n    # cpplint_unittest.py for an example of this.\n    for obj in self.stack:\n      if isinstance(obj, _ClassInfo):\n        error(filename, obj.starting_linenum, 'build/class', 5,\n              'Failed to find complete declaration of class %s' %\n              obj.name)\n      elif isinstance(obj, _NamespaceInfo):\n        error(filename, obj.starting_linenum, 'build/namespaces', 5,\n              'Failed to find complete declaration of namespace %s' %\n              obj.name)\n\n\ndef CheckForNonStandardConstructs(filename, clean_lines, linenum,\n                                  nesting_state, error):\n  r\"\"\"Logs an error if we see certain non-ANSI constructs ignored by gcc-2.\n\n  Complain about several constructs which gcc-2 accepts, but which are\n  not standard C++.  Warning about these in lint is one way to ease the\n  transition to new compilers.\n  - put storage class first (e.g. \"static const\" instead of \"const static\").\n  - \"%lld\" instead of %qd\" in printf-type functions.\n  - \"%1$d\" is non-standard in printf-type functions.\n  - \"\\%\" is an undefined character escape sequence.\n  - text after #endif is not allowed.\n  - invalid inner-style forward declaration.\n  - >? and <? operators, and their >?= and <?= cousins.\n\n  Additionally, check for constructor/destructor style violations and reference\n  members, as it is very convenient to do so while checking for\n  gcc-2 compliance.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n  \"\"\"\n\n  # Remove comments from the line, but leave in strings for now.\n  line = clean_lines.lines[linenum]\n\n  if Search(r'printf\\s*\\(.*\".*%[-+ ]?\\d*q', line):\n    error(filename, linenum, 'runtime/printf_format', 3,\n          '%q in format strings is deprecated.  Use %ll instead.')\n\n  if Search(r'printf\\s*\\(.*\".*%\\d+\\$', line):\n    error(filename, linenum, 'runtime/printf_format', 2,\n          '%N$ formats are unconventional.  Try rewriting to avoid them.')\n\n  # Remove escaped backslashes before looking for undefined escapes.\n  line = line.replace('\\\\\\\\', '')\n\n  if Search(r'(\"|\\').*\\\\(%|\\[|\\(|{)', line):\n    error(filename, linenum, 'build/printf_format', 3,\n          '%, [, (, and { are undefined character escapes.  Unescape them.')\n\n  # For the rest, work with both comments and strings removed.\n  line = clean_lines.elided[linenum]\n\n  if Search(r'\\b(const|volatile|void|char|short|int|long'\n            r'|float|double|signed|unsigned'\n            r'|schar|u?int8|u?int16|u?int32|u?int64)'\n            r'\\s+(register|static|extern|typedef)\\b',\n            line):\n    error(filename, linenum, 'build/storage_class', 5,\n          'Storage-class specifier (static, extern, typedef, etc) should be '\n          'at the beginning of the declaration.')\n\n  if Match(r'\\s*#\\s*endif\\s*[^/\\s]+', line):\n    error(filename, linenum, 'build/endif_comment', 5,\n          'Uncommented text after #endif is non-standard.  Use a comment.')\n\n  if Match(r'\\s*class\\s+(\\w+\\s*::\\s*)+\\w+\\s*;', line):\n    error(filename, linenum, 'build/forward_decl', 5,\n          'Inner-style forward declarations are invalid.  Remove this line.')\n\n  if Search(r'(\\w+|[+-]?\\d+(\\.\\d*)?)\\s*(<|>)\\?=?\\s*(\\w+|[+-]?\\d+)(\\.\\d*)?',\n            line):\n    error(filename, linenum, 'build/deprecated', 3,\n          '>? and <? (max and min) operators are non-standard and deprecated.')\n\n  if Search(r'^\\s*const\\s*string\\s*&\\s*\\w+\\s*;', line):\n    # TODO(unknown): Could it be expanded safely to arbitrary references,\n    # without triggering too many false positives? The first\n    # attempt triggered 5 warnings for mostly benign code in the regtest, hence\n    # the restriction.\n    # Here's the original regexp, for the reference:\n    # type_name = r'\\w+((\\s*::\\s*\\w+)|(\\s*<\\s*\\w+?\\s*>))?'\n    # r'\\s*const\\s*' + type_name + '\\s*&\\s*\\w+\\s*;'\n    error(filename, linenum, 'runtime/member_string_references', 2,\n          'const string& members are dangerous. It is much better to use '\n          'alternatives, such as pointers or simple constants.')\n\n  # Everything else in this function operates on class declarations.\n  # Return early if the top of the nesting stack is not a class, or if\n  # the class head is not completed yet.\n  classinfo = nesting_state.InnermostClass()\n  if not classinfo or not classinfo.seen_open_brace:\n    return\n\n  # The class may have been declared with namespace or classname qualifiers.\n  # The constructor and destructor will not have those qualifiers.\n  base_classname = classinfo.name.split('::')[-1]\n\n  # Look for single-argument constructors that aren't marked explicit.\n  # Technically a valid construct, but against style.\n  explicit_constructor_match = Match(\n      r'\\s+(?:(?:inline|constexpr)\\s+)*(explicit\\s+)?'\n      r'(?:(?:inline|constexpr)\\s+)*%s\\s*'\n      r'\\(((?:[^()]|\\([^()]*\\))*)\\)'\n      % re.escape(base_classname),\n      line)\n\n  if explicit_constructor_match:\n    is_marked_explicit = explicit_constructor_match.group(1)\n\n    if not explicit_constructor_match.group(2):\n      constructor_args = []\n    else:\n      constructor_args = explicit_constructor_match.group(2).split(',')\n\n    # collapse arguments so that commas in template parameter lists and function\n    # argument parameter lists don't split arguments in two\n    i = 0\n    while i < len(constructor_args):\n      constructor_arg = constructor_args[i]\n      while (constructor_arg.count('<') > constructor_arg.count('>') or\n             constructor_arg.count('(') > constructor_arg.count(')')):\n        constructor_arg += ',' + constructor_args[i + 1]\n        del constructor_args[i + 1]\n      constructor_args[i] = constructor_arg\n      i += 1\n\n    variadic_args = [arg for arg in constructor_args if '&&...' in arg]\n    defaulted_args = [arg for arg in constructor_args if '=' in arg]\n    noarg_constructor = (not constructor_args or  # empty arg list\n                         # 'void' arg specifier\n                         (len(constructor_args) == 1 and\n                          constructor_args[0].strip() == 'void'))\n    onearg_constructor = ((len(constructor_args) == 1 and  # exactly one arg\n                           not noarg_constructor) or\n                          # all but at most one arg defaulted\n                          (len(constructor_args) >= 1 and\n                           not noarg_constructor and\n                           len(defaulted_args) >= len(constructor_args) - 1) or\n                          # variadic arguments with zero or one argument\n                          (len(constructor_args) <= 2 and\n                           len(variadic_args) >= 1))\n    initializer_list_constructor = bool(\n        onearg_constructor and\n        Search(r'\\bstd\\s*::\\s*initializer_list\\b', constructor_args[0]))\n    copy_constructor = bool(\n        onearg_constructor and\n        Match(r'((const\\s+(volatile\\s+)?)?|(volatile\\s+(const\\s+)?))?'\n              r'%s(\\s*<[^>]*>)?(\\s+const)?\\s*(?:<\\w+>\\s*)?&'\n              % re.escape(base_classname), constructor_args[0].strip()))\n\n    if (not is_marked_explicit and\n        onearg_constructor and\n        not initializer_list_constructor and\n        not copy_constructor):\n      if defaulted_args or variadic_args:\n        error(filename, linenum, 'runtime/explicit', 5,\n              'Constructors callable with one argument '\n              'should be marked explicit.')\n      else:\n        error(filename, linenum, 'runtime/explicit', 5,\n              'Single-parameter constructors should be marked explicit.')\n    elif is_marked_explicit and not onearg_constructor:\n      if noarg_constructor:\n        error(filename, linenum, 'runtime/explicit', 5,\n              'Zero-parameter constructors should not be marked explicit.')\n\n\ndef CheckSpacingForFunctionCall(filename, clean_lines, linenum, error):\n  \"\"\"Checks for the correctness of various spacing around function calls.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Since function calls often occur inside if/for/while/switch\n  # expressions - which have their own, more liberal conventions - we\n  # first see if we should be looking inside such an expression for a\n  # function call, to which we can apply more strict standards.\n  fncall = line    # if there's no control flow construct, look at whole line\n  for pattern in (r'\\bif\\s*\\((.*)\\)\\s*{',\n                  r'\\bfor\\s*\\((.*)\\)\\s*{',\n                  r'\\bwhile\\s*\\((.*)\\)\\s*[{;]',\n                  r'\\bswitch\\s*\\((.*)\\)\\s*{'):\n    match = Search(pattern, line)\n    if match:\n      fncall = match.group(1)    # look inside the parens for function calls\n      break\n\n  # Except in if/for/while/switch, there should never be space\n  # immediately inside parens (eg \"f( 3, 4 )\").  We make an exception\n  # for nested parens ( (a+b) + c ).  Likewise, there should never be\n  # a space before a ( when it's a function argument.  I assume it's a\n  # function argument when the char before the whitespace is legal in\n  # a function name (alnum + _) and we're not starting a macro. Also ignore\n  # pointers and references to arrays and functions coz they're too tricky:\n  # we use a very simple way to recognize these:\n  # \" (something)(maybe-something)\" or\n  # \" (something)(maybe-something,\" or\n  # \" (something)[something]\"\n  # Note that we assume the contents of [] to be short enough that\n  # they'll never need to wrap.\n  if (  # Ignore control structures.\n      not Search(r'\\b(if|elif|for|while|switch|return|new|delete|catch|sizeof)\\b',\n                 fncall) and\n      # Ignore pointers/references to functions.\n      not Search(r' \\([^)]+\\)\\([^)]*(\\)|,$)', fncall) and\n      # Ignore pointers/references to arrays.\n      not Search(r' \\([^)]+\\)\\[[^\\]]+\\]', fncall)):\n    if Search(r'\\w\\s*\\(\\s(?!\\s*\\\\$)', fncall):      # a ( used for a fn call\n      error(filename, linenum, 'whitespace/parens', 4,\n            'Extra space after ( in function call')\n    elif Search(r'\\(\\s+(?!(\\s*\\\\)|\\()', fncall):\n      error(filename, linenum, 'whitespace/parens', 2,\n            'Extra space after (')\n    if (Search(r'\\w\\s+\\(', fncall) and\n        not Search(r'_{0,2}asm_{0,2}\\s+_{0,2}volatile_{0,2}\\s+\\(', fncall) and\n        not Search(r'#\\s*define|typedef|using\\s+\\w+\\s*=', fncall) and\n        not Search(r'\\w\\s+\\((\\w+::)*\\*\\w+\\)\\(', fncall) and\n        not Search(r'\\bcase\\s+\\(', fncall)):\n      # TODO(unknown): Space after an operator function seem to be a common\n      # error, silence those for now by restricting them to highest verbosity.\n      if Search(r'\\boperator_*\\b', line):\n        error(filename, linenum, 'whitespace/parens', 0,\n              'Extra space before ( in function call')\n      else:\n        error(filename, linenum, 'whitespace/parens', 4,\n              'Extra space before ( in function call')\n    # If the ) is followed only by a newline or a { + newline, assume it's\n    # part of a control statement (if/while/etc), and don't complain\n    if Search(r'[^)]\\s+\\)\\s*[^{\\s]', fncall):\n      # If the closing parenthesis is preceded by only whitespaces,\n      # try to give a more descriptive error message.\n      if Search(r'^\\s+\\)', fncall):\n        error(filename, linenum, 'whitespace/parens', 2,\n              'Closing ) should be moved to the previous line')\n      else:\n        error(filename, linenum, 'whitespace/parens', 2,\n              'Extra space before )')\n\n\ndef IsBlankLine(line):\n  \"\"\"Returns true if the given line is blank.\n\n  We consider a line to be blank if the line is empty or consists of\n  only white spaces.\n\n  Args:\n    line: A line of a string.\n\n  Returns:\n    True, if the given line is blank.\n  \"\"\"\n  return not line or line.isspace()\n\n\ndef CheckForNamespaceIndentation(filename, nesting_state, clean_lines, line,\n                                 error):\n  is_namespace_indent_item = (\n      len(nesting_state.stack) > 1 and\n      nesting_state.stack[-1].check_namespace_indentation and\n      isinstance(nesting_state.previous_stack_top, _NamespaceInfo) and\n      nesting_state.previous_stack_top == nesting_state.stack[-2])\n\n  if ShouldCheckNamespaceIndentation(nesting_state, is_namespace_indent_item,\n                                     clean_lines.elided, line):\n    CheckItemIndentationInNamespace(filename, clean_lines.elided,\n                                    line, error)\n\n\ndef CheckForFunctionLengths(filename, clean_lines, linenum,\n                            function_state, error):\n  \"\"\"Reports for long function bodies.\n\n  For an overview why this is done, see:\n  https://google-styleguide.googlecode.com/svn/trunk/cppguide.xml#Write_Short_Functions\n\n  Uses a simplistic algorithm assuming other style guidelines\n  (especially spacing) are followed.\n  Only checks unindented functions, so class members are unchecked.\n  Trivial bodies are unchecked, so constructors with huge initializer lists\n  may be missed.\n  Blank/comment lines are not counted so as to avoid encouraging the removal\n  of vertical space and comments just to get through a lint check.\n  NOLINT *on the last line of a function* disables this check.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    function_state: Current function name and lines in body so far.\n    error: The function to call with any errors found.\n  \"\"\"\n  lines = clean_lines.lines\n  line = lines[linenum]\n  joined_line = ''\n\n  starting_func = False\n  regexp = r'(\\w(\\w|::|\\*|\\&|\\s)*)\\('  # decls * & space::name( ...\n  match_result = Match(regexp, line)\n  if match_result:\n    # If the name is all caps and underscores, figure it's a macro and\n    # ignore it, unless it's TEST or TEST_F.\n    function_name = match_result.group(1).split()[-1]\n    if function_name == 'TEST' or function_name == 'TEST_F' or (\n        not Match(r'[A-Z_]+$', function_name)):\n      starting_func = True\n\n  if starting_func:\n    body_found = False\n    for start_linenum in xrange(linenum, clean_lines.NumLines()):\n      start_line = lines[start_linenum]\n      joined_line += ' ' + start_line.lstrip()\n      if Search(r'(;|})', start_line):  # Declarations and trivial functions\n        body_found = True\n        break                              # ... ignore\n      if Search(r'{', start_line):\n        body_found = True\n        function = Search(r'((\\w|:)*)\\(', line).group(1)\n        if Match(r'TEST', function):    # Handle TEST... macros\n          parameter_regexp = Search(r'(\\(.*\\))', joined_line)\n          if parameter_regexp:             # Ignore bad syntax\n            function += parameter_regexp.group(1)\n        else:\n          function += '()'\n        function_state.Begin(function)\n        break\n    if not body_found:\n      # No body for the function (or evidence of a non-function) was found.\n      error(filename, linenum, 'readability/fn_size', 5,\n            'Lint failed to find start of function body.')\n  elif Match(r'^\\}\\s*$', line):  # function end\n    function_state.Check(error, filename, linenum)\n    function_state.End()\n  elif not Match(r'^\\s*$', line):\n    function_state.Count()  # Count non-blank/non-comment lines.\n\n\n_RE_PATTERN_TODO = re.compile(r'^//(\\s*)TODO(\\(.+?\\))?:?(\\s|$)?')\n\n\ndef CheckComment(line, filename, linenum, next_line_start, error):\n  \"\"\"Checks for common mistakes in comments.\n\n  Args:\n    line: The line in question.\n    filename: The name of the current file.\n    linenum: The number of the line to check.\n    next_line_start: The first non-whitespace column of the next line.\n    error: The function to call with any errors found.\n  \"\"\"\n  commentpos = line.find('//')\n  if commentpos != -1:\n    # Check if the // may be in quotes.  If so, ignore it\n    if re.sub(r'\\\\.', '', line[0:commentpos]).count('\"') % 2 == 0:\n      # Allow one space for new scopes, two spaces otherwise:\n      if (not (Match(r'^.*{ *//', line) and next_line_start == commentpos) and\n          ((commentpos >= 1 and\n            line[commentpos-1] not in string.whitespace) or\n           (commentpos >= 2 and\n            line[commentpos-2] not in string.whitespace))):\n        error(filename, linenum, 'whitespace/comments', 2,\n              'At least two spaces is best between code and comments')\n\n      # Checks for common mistakes in TODO comments.\n      comment = line[commentpos:]\n      match = _RE_PATTERN_TODO.match(comment)\n      if match:\n        # One whitespace is correct; zero whitespace is handled elsewhere.\n        leading_whitespace = match.group(1)\n        if len(leading_whitespace) > 1:\n          error(filename, linenum, 'whitespace/todo', 2,\n                'Too many spaces before TODO')\n\n        username = match.group(2)\n        if not username:\n          error(filename, linenum, 'readability/todo', 2,\n                'Missing username in TODO; it should look like '\n                '\"// TODO(my_username): Stuff.\"')\n\n        middle_whitespace = match.group(3)\n        # Comparisons made explicit for correctness -- pylint: disable=g-explicit-bool-comparison\n        if middle_whitespace != ' ' and middle_whitespace != '':\n          error(filename, linenum, 'whitespace/todo', 2,\n                'TODO(my_username) should be followed by a space')\n\n      # If the comment contains an alphanumeric character, there\n      # should be a space somewhere between it and the // unless\n      # it's a /// or //! Doxygen comment.\n      if (Match(r'//[^ ]*\\w', comment) and\n          not Match(r'(///|//\\!)(\\s+|$)', comment)):\n        error(filename, linenum, 'whitespace/comments', 4,\n              'Should have a space between // and comment')\n\n\ndef CheckSpacing(filename, clean_lines, linenum, nesting_state, error):\n  \"\"\"Checks for the correctness of various spacing issues in the code.\n\n  Things we check for: spaces around operators, spaces after\n  if/for/while/switch, no spaces around parens in function calls, two\n  spaces between code and comment, don't start a block with a blank\n  line, don't end a function with a blank line, don't add a blank line\n  after public/protected/private, don't have too many blank lines in a row.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Don't use \"elided\" lines here, otherwise we can't check commented lines.\n  # Don't want to use \"raw\" either, because we don't want to check inside C++11\n  # raw strings,\n  raw = clean_lines.lines_without_raw_strings\n  line = raw[linenum]\n\n  # Before nixing comments, check if the line is blank for no good\n  # reason.  This includes the first line after a block is opened, and\n  # blank lines at the end of a function (ie, right before a line like '}'\n  #\n  # Skip all the blank line checks if we are immediately inside a\n  # namespace body.  In other words, don't issue blank line warnings\n  # for this block:\n  #   namespace {\n  #\n  #   }\n  #\n  # A warning about missing end of namespace comments will be issued instead.\n  #\n  # Also skip blank line checks for 'extern \"C\"' blocks, which are formatted\n  # like namespaces.\n  if (IsBlankLine(line) and\n      not nesting_state.InNamespaceBody() and\n      not nesting_state.InExternC()):\n    elided = clean_lines.elided\n    prev_line = elided[linenum - 1]\n    prevbrace = prev_line.rfind('{')\n    # TODO(unknown): Don't complain if line before blank line, and line after,\n    #                both start with alnums and are indented the same amount.\n    #                This ignores whitespace at the start of a namespace block\n    #                because those are not usually indented.\n    if prevbrace != -1 and prev_line[prevbrace:].find('}') == -1:\n      # OK, we have a blank line at the start of a code block.  Before we\n      # complain, we check if it is an exception to the rule: The previous\n      # non-empty line has the parameters of a function header that are indented\n      # 4 spaces (because they did not fit in a 80 column line when placed on\n      # the same line as the function name).  We also check for the case where\n      # the previous line is indented 6 spaces, which may happen when the\n      # initializers of a constructor do not fit into a 80 column line.\n      exception = False\n      if Match(r' {6}\\w', prev_line):  # Initializer list?\n        # We are looking for the opening column of initializer list, which\n        # should be indented 4 spaces to cause 6 space indentation afterwards.\n        search_position = linenum-2\n        while (search_position >= 0\n               and Match(r' {6}\\w', elided[search_position])):\n          search_position -= 1\n        exception = (search_position >= 0\n                     and elided[search_position][:5] == '    :')\n      else:\n        # Search for the function arguments or an initializer list.  We use a\n        # simple heuristic here: If the line is indented 4 spaces; and we have a\n        # closing paren, without the opening paren, followed by an opening brace\n        # or colon (for initializer lists) we assume that it is the last line of\n        # a function header.  If we have a colon indented 4 spaces, it is an\n        # initializer list.\n        exception = (Match(r' {4}\\w[^\\(]*\\)\\s*(const\\s*)?(\\{\\s*$|:)',\n                           prev_line)\n                     or Match(r' {4}:', prev_line))\n\n      if not exception:\n        error(filename, linenum, 'whitespace/blank_line', 2,\n              'Redundant blank line at the start of a code block '\n              'should be deleted.')\n    # Ignore blank lines at the end of a block in a long if-else\n    # chain, like this:\n    #   if (condition1) {\n    #     // Something followed by a blank line\n    #\n    #   } else if (condition2) {\n    #     // Something else\n    #   }\n    if linenum + 1 < clean_lines.NumLines():\n      next_line = raw[linenum + 1]\n      if (next_line\n          and Match(r'\\s*}', next_line)\n          and next_line.find('} else ') == -1):\n        error(filename, linenum, 'whitespace/blank_line', 3,\n              'Redundant blank line at the end of a code block '\n              'should be deleted.')\n\n    matched = Match(r'\\s*(public|protected|private):', prev_line)\n    if matched:\n      error(filename, linenum, 'whitespace/blank_line', 3,\n            'Do not leave a blank line after \"%s:\"' % matched.group(1))\n\n  # Next, check comments\n  next_line_start = 0\n  if linenum + 1 < clean_lines.NumLines():\n    next_line = raw[linenum + 1]\n    next_line_start = len(next_line) - len(next_line.lstrip())\n  CheckComment(line, filename, linenum, next_line_start, error)\n\n  # get rid of comments and strings\n  line = clean_lines.elided[linenum]\n\n  # You shouldn't have spaces before your brackets, except for C++11 attributes\n  # or maybe after 'delete []', 'return []() {};', or 'auto [abc, ...] = ...;'.\n  if (Search(r'\\w\\s+\\[(?!\\[)', line) and\n      not Search(r'(?:auto&?|delete|return)\\s+\\[', line)):\n    error(filename, linenum, 'whitespace/braces', 5,\n          'Extra space before [')\n\n  # In range-based for, we wanted spaces before and after the colon, but\n  # not around \"::\" tokens that might appear.\n  if (Search(r'for *\\(.*[^:]:[^: ]', line) or\n      Search(r'for *\\(.*[^: ]:[^:]', line)):\n    error(filename, linenum, 'whitespace/forcolon', 2,\n          'Missing space around colon in range-based for loop')\n\n\ndef CheckOperatorSpacing(filename, clean_lines, linenum, error):\n  \"\"\"Checks for horizontal spacing around operators.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Don't try to do spacing checks for operator methods.  Do this by\n  # replacing the troublesome characters with something else,\n  # preserving column position for all other characters.\n  #\n  # The replacement is done repeatedly to avoid false positives from\n  # operators that call operators.\n  while True:\n    match = Match(r'^(.*\\boperator\\b)(\\S+)(\\s*\\(.*)$', line)\n    if match:\n      line = match.group(1) + ('_' * len(match.group(2))) + match.group(3)\n    else:\n      break\n\n  # We allow no-spaces around = within an if: \"if ( (a=Foo()) == 0 )\".\n  # Otherwise not.  Note we only check for non-spaces on *both* sides;\n  # sometimes people put non-spaces on one side when aligning ='s among\n  # many lines (not that this is behavior that I approve of...)\n  if ((Search(r'[\\w.]=', line) or\n       Search(r'=[\\w.]', line))\n      and not Search(r'\\b(if|while|for) ', line)\n      # Operators taken from [lex.operators] in C++11 standard.\n      and not Search(r'(>=|<=|==|!=|&=|\\^=|\\|=|\\+=|\\*=|\\/=|\\%=)', line)\n      and not Search(r'operator=', line)):\n    error(filename, linenum, 'whitespace/operators', 4,\n          'Missing spaces around =')\n\n  # It's ok not to have spaces around binary operators like + - * /, but if\n  # there's too little whitespace, we get concerned.  It's hard to tell,\n  # though, so we punt on this one for now.  TODO.\n\n  # You should always have whitespace around binary operators.\n  #\n  # Check <= and >= first to avoid false positives with < and >, then\n  # check non-include lines for spacing around < and >.\n  #\n  # If the operator is followed by a comma, assume it's be used in a\n  # macro context and don't do any checks.  This avoids false\n  # positives.\n  #\n  # Note that && is not included here.  This is because there are too\n  # many false positives due to RValue references.\n  match = Search(r'[^<>=!\\s](==|!=|<=|>=|\\|\\|)[^<>=!\\s,;\\)]', line)\n  if match:\n    error(filename, linenum, 'whitespace/operators', 3,\n          'Missing spaces around %s' % match.group(1))\n  elif not Match(r'#.*include', line):\n    # Look for < that is not surrounded by spaces.  This is only\n    # triggered if both sides are missing spaces, even though\n    # technically it should flag if at least one side is missing a\n    # space.  This is done to avoid some false positives with shifts.\n    match = Match(r'^(.*[^\\s<])<[^\\s=<,]', line)\n    if match:\n      (_, _, end_pos) = CloseExpression(\n          clean_lines, linenum, len(match.group(1)))\n      if end_pos <= -1:\n        error(filename, linenum, 'whitespace/operators', 3,\n              'Missing spaces around <')\n\n    # Look for > that is not surrounded by spaces.  Similar to the\n    # above, we only trigger if both sides are missing spaces to avoid\n    # false positives with shifts.\n    match = Match(r'^(.*[^-\\s>])>[^\\s=>,]', line)\n    if match:\n      (_, _, start_pos) = ReverseCloseExpression(\n          clean_lines, linenum, len(match.group(1)))\n      if start_pos <= -1:\n        error(filename, linenum, 'whitespace/operators', 3,\n              'Missing spaces around >')\n\n  # We allow no-spaces around << when used like this: 10<<20, but\n  # not otherwise (particularly, not when used as streams)\n  #\n  # We also allow operators following an opening parenthesis, since\n  # those tend to be macros that deal with operators.\n  match = Search(r'(operator|[^\\s(<])(?:L|UL|LL|ULL|l|ul|ll|ull)?<<([^\\s,=<])', line)\n  if (match and not (match.group(1).isdigit() and match.group(2).isdigit()) and\n      not (match.group(1) == 'operator' and match.group(2) == ';')):\n    error(filename, linenum, 'whitespace/operators', 3,\n          'Missing spaces around <<')\n\n  # We allow no-spaces around >> for almost anything.  This is because\n  # C++11 allows \">>\" to close nested templates, which accounts for\n  # most cases when \">>\" is not followed by a space.\n  #\n  # We still warn on \">>\" followed by alpha character, because that is\n  # likely due to \">>\" being used for right shifts, e.g.:\n  #   value >> alpha\n  #\n  # When \">>\" is used to close templates, the alphanumeric letter that\n  # follows would be part of an identifier, and there should still be\n  # a space separating the template type and the identifier.\n  #   type<type<type>> alpha\n  match = Search(r'>>[a-zA-Z_]', line)\n  if match:\n    error(filename, linenum, 'whitespace/operators', 3,\n          'Missing spaces around >>')\n\n  # There shouldn't be space around unary operators\n  match = Search(r'(!\\s|~\\s|[\\s]--[\\s;]|[\\s]\\+\\+[\\s;])', line)\n  if match:\n    error(filename, linenum, 'whitespace/operators', 4,\n          'Extra space for operator %s' % match.group(1))\n\n\ndef CheckParenthesisSpacing(filename, clean_lines, linenum, error):\n  \"\"\"Checks for horizontal spacing around parentheses.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # No spaces after an if, while, switch, or for\n  match = Search(r' (if\\(|for\\(|while\\(|switch\\()', line)\n  if match:\n    error(filename, linenum, 'whitespace/parens', 5,\n          'Missing space before ( in %s' % match.group(1))\n\n  # For if/for/while/switch, the left and right parens should be\n  # consistent about how many spaces are inside the parens, and\n  # there should either be zero or one spaces inside the parens.\n  # We don't want: \"if ( foo)\" or \"if ( foo   )\".\n  # Exception: \"for ( ; foo; bar)\" and \"for (foo; bar; )\" are allowed.\n  match = Search(r'\\b(if|for|while|switch)\\s*'\n                 r'\\(([ ]*)(.).*[^ ]+([ ]*)\\)\\s*{\\s*$',\n                 line)\n  if match:\n    if len(match.group(2)) != len(match.group(4)):\n      if not (match.group(3) == ';' and\n              len(match.group(2)) == 1 + len(match.group(4)) or\n              not match.group(2) and Search(r'\\bfor\\s*\\(.*; \\)', line)):\n        error(filename, linenum, 'whitespace/parens', 5,\n              'Mismatching spaces inside () in %s' % match.group(1))\n    if len(match.group(2)) not in [0, 1]:\n      error(filename, linenum, 'whitespace/parens', 5,\n            'Should have zero or one spaces inside ( and ) in %s' %\n            match.group(1))\n\n\ndef CheckCommaSpacing(filename, clean_lines, linenum, error):\n  \"\"\"Checks for horizontal spacing near commas and semicolons.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  raw = clean_lines.lines_without_raw_strings\n  line = clean_lines.elided[linenum]\n\n  # You should always have a space after a comma (either as fn arg or operator)\n  #\n  # This does not apply when the non-space character following the\n  # comma is another comma, since the only time when that happens is\n  # for empty macro arguments.\n  #\n  # We run this check in two passes: first pass on elided lines to\n  # verify that lines contain missing whitespaces, second pass on raw\n  # lines to confirm that those missing whitespaces are not due to\n  # elided comments.\n  if (Search(r',[^,\\s]', ReplaceAll(r'\\boperator\\s*,\\s*\\(', 'F(', line)) and\n      Search(r',[^,\\s]', raw[linenum])):\n    error(filename, linenum, 'whitespace/comma', 3,\n          'Missing space after ,')\n\n  # You should always have a space after a semicolon\n  # except for few corner cases\n  # TODO(unknown): clarify if 'if (1) { return 1;}' is requires one more\n  # space after ;\n  if Search(r';[^\\s};\\\\)/]', line):\n    error(filename, linenum, 'whitespace/semicolon', 3,\n          'Missing space after ;')\n\n\ndef _IsType(clean_lines, nesting_state, expr):\n  \"\"\"Check if expression looks like a type name, returns true if so.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    expr: The expression to check.\n  Returns:\n    True, if token looks like a type.\n  \"\"\"\n  # Keep only the last token in the expression\n  last_word = Match(r'^.*(\\b\\S+)$', expr)\n  if last_word:\n    token = last_word.group(1)\n  else:\n    token = expr\n\n  # Match native types and stdint types\n  if _TYPES.match(token):\n    return True\n\n  # Try a bit harder to match templated types.  Walk up the nesting\n  # stack until we find something that resembles a typename\n  # declaration for what we are looking for.\n  typename_pattern = (r'\\b(?:typename|class|struct)\\s+' + re.escape(token) +\n                      r'\\b')\n  block_index = len(nesting_state.stack) - 1\n  while block_index >= 0:\n    if isinstance(nesting_state.stack[block_index], _NamespaceInfo):\n      return False\n\n    # Found where the opening brace is.  We want to scan from this\n    # line up to the beginning of the function, minus a few lines.\n    #   template <typename Type1,  // stop scanning here\n    #             ...>\n    #   class C\n    #     : public ... {  // start scanning here\n    last_line = nesting_state.stack[block_index].starting_linenum\n\n    next_block_start = 0\n    if block_index > 0:\n      next_block_start = nesting_state.stack[block_index - 1].starting_linenum\n    first_line = last_line\n    while first_line >= next_block_start:\n      if clean_lines.elided[first_line].find('template') >= 0:\n        break\n      first_line -= 1\n    if first_line < next_block_start:\n      # Didn't find any \"template\" keyword before reaching the next block,\n      # there are probably no template things to check for this block\n      block_index -= 1\n      continue\n\n    # Look for typename in the specified range\n    for i in xrange(first_line, last_line + 1, 1):\n      if Search(typename_pattern, clean_lines.elided[i]):\n        return True\n    block_index -= 1\n\n  return False\n\n\ndef CheckBracesSpacing(filename, clean_lines, linenum, nesting_state, error):\n  \"\"\"Checks for horizontal spacing near commas.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Except after an opening paren, or after another opening brace (in case of\n  # an initializer list, for instance), you should have spaces before your\n  # braces when they are delimiting blocks, classes, namespaces etc.\n  # And since you should never have braces at the beginning of a line,\n  # this is an easy test.  Except that braces used for initialization don't\n  # follow the same rule; we often don't want spaces before those.\n  match = Match(r'^(.*[^ ({>]){', line)\n\n  if match:\n    # Try a bit harder to check for brace initialization.  This\n    # happens in one of the following forms:\n    #   Constructor() : initializer_list_{} { ... }\n    #   Constructor{}.MemberFunction()\n    #   Type variable{};\n    #   FunctionCall(type{}, ...);\n    #   LastArgument(..., type{});\n    #   LOG(INFO) << type{} << \" ...\";\n    #   map_of_type[{...}] = ...;\n    #   ternary = expr ? new type{} : nullptr;\n    #   OuterTemplate<InnerTemplateConstructor<Type>{}>\n    #\n    # We check for the character following the closing brace, and\n    # silence the warning if it's one of those listed above, i.e.\n    # \"{.;,)<>]:\".\n    #\n    # To account for nested initializer list, we allow any number of\n    # closing braces up to \"{;,)<\".  We can't simply silence the\n    # warning on first sight of closing brace, because that would\n    # cause false negatives for things that are not initializer lists.\n    #   Silence this:         But not this:\n    #     Outer{                if (...) {\n    #       Inner{...}            if (...){  // Missing space before {\n    #     };                    }\n    #\n    # There is a false negative with this approach if people inserted\n    # spurious semicolons, e.g. \"if (cond){};\", but we will catch the\n    # spurious semicolon with a separate check.\n    leading_text = match.group(1)\n    (endline, endlinenum, endpos) = CloseExpression(\n        clean_lines, linenum, len(match.group(1)))\n    trailing_text = ''\n    if endpos > -1:\n      trailing_text = endline[endpos:]\n    for offset in xrange(endlinenum + 1,\n                         min(endlinenum + 3, clean_lines.NumLines() - 1)):\n      trailing_text += clean_lines.elided[offset]\n    # We also suppress warnings for `uint64_t{expression}` etc., as the style\n    # guide recommends brace initialization for integral types to avoid\n    # overflow/truncation.\n    if (not Match(r'^[\\s}]*[{.;,)<>\\]:]', trailing_text)\n        and not _IsType(clean_lines, nesting_state, leading_text)):\n      error(filename, linenum, 'whitespace/braces', 5,\n            'Missing space before {')\n\n  # Make sure '} else {' has spaces.\n  if Search(r'}else', line):\n    error(filename, linenum, 'whitespace/braces', 5,\n          'Missing space before else')\n\n  # You shouldn't have a space before a semicolon at the end of the line.\n  # There's a special case for \"for\" since the style guide allows space before\n  # the semicolon there.\n  if Search(r':\\s*;\\s*$', line):\n    error(filename, linenum, 'whitespace/semicolon', 5,\n          'Semicolon defining empty statement. Use {} instead.')\n  elif Search(r'^\\s*;\\s*$', line):\n    error(filename, linenum, 'whitespace/semicolon', 5,\n          'Line contains only semicolon. If this should be an empty statement, '\n          'use {} instead.')\n  elif (Search(r'\\s+;\\s*$', line) and\n        not Search(r'\\bfor\\b', line)):\n    error(filename, linenum, 'whitespace/semicolon', 5,\n          'Extra space before last semicolon. If this should be an empty '\n          'statement, use {} instead.')\n\n\ndef IsDecltype(clean_lines, linenum, column):\n  \"\"\"Check if the token ending on (linenum, column) is decltype().\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: the number of the line to check.\n    column: end column of the token to check.\n  Returns:\n    True if this token is decltype() expression, False otherwise.\n  \"\"\"\n  (text, _, start_col) = ReverseCloseExpression(clean_lines, linenum, column)\n  if start_col < 0:\n    return False\n  if Search(r'\\bdecltype\\s*$', text[0:start_col]):\n    return True\n  return False\n\ndef CheckSectionSpacing(filename, clean_lines, class_info, linenum, error):\n  \"\"\"Checks for additional blank line issues related to sections.\n\n  Currently the only thing checked here is blank line before protected/private.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    class_info: A _ClassInfo objects.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  # Skip checks if the class is small, where small means 25 lines or less.\n  # 25 lines seems like a good cutoff since that's the usual height of\n  # terminals, and any class that can't fit in one screen can't really\n  # be considered \"small\".\n  #\n  # Also skip checks if we are on the first line.  This accounts for\n  # classes that look like\n  #   class Foo { public: ... };\n  #\n  # If we didn't find the end of the class, last_line would be zero,\n  # and the check will be skipped by the first condition.\n  if (class_info.last_line - class_info.starting_linenum <= 24 or\n      linenum <= class_info.starting_linenum):\n    return\n\n  matched = Match(r'\\s*(public|protected|private):', clean_lines.lines[linenum])\n  if matched:\n    # Issue warning if the line before public/protected/private was\n    # not a blank line, but don't do this if the previous line contains\n    # \"class\" or \"struct\".  This can happen two ways:\n    #  - We are at the beginning of the class.\n    #  - We are forward-declaring an inner class that is semantically\n    #    private, but needed to be public for implementation reasons.\n    # Also ignores cases where the previous line ends with a backslash as can be\n    # common when defining classes in C macros.\n    prev_line = clean_lines.lines[linenum - 1]\n    if (not IsBlankLine(prev_line) and\n        not Search(r'\\b(class|struct)\\b', prev_line) and\n        not Search(r'\\\\$', prev_line)):\n      # Try a bit harder to find the beginning of the class.  This is to\n      # account for multi-line base-specifier lists, e.g.:\n      #   class Derived\n      #       : public Base {\n      end_class_head = class_info.starting_linenum\n      for i in range(class_info.starting_linenum, linenum):\n        if Search(r'\\{\\s*$', clean_lines.lines[i]):\n          end_class_head = i\n          break\n      if end_class_head < linenum - 1:\n        error(filename, linenum, 'whitespace/blank_line', 3,\n              '\"%s:\" should be preceded by a blank line' % matched.group(1))\n\n\ndef GetPreviousNonBlankLine(clean_lines, linenum):\n  \"\"\"Return the most recent non-blank line and its line number.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file contents.\n    linenum: The number of the line to check.\n\n  Returns:\n    A tuple with two elements.  The first element is the contents of the last\n    non-blank line before the current line, or the empty string if this is the\n    first non-blank line.  The second is the line number of that line, or -1\n    if this is the first non-blank line.\n  \"\"\"\n\n  prevlinenum = linenum - 1\n  while prevlinenum >= 0:\n    prevline = clean_lines.elided[prevlinenum]\n    if not IsBlankLine(prevline):     # if not a blank line...\n      return (prevline, prevlinenum)\n    prevlinenum -= 1\n  return ('', -1)\n\n\ndef CheckBraces(filename, clean_lines, linenum, error):\n  \"\"\"Looks for misplaced braces (e.g. at the end of line).\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  line = clean_lines.elided[linenum]        # get rid of comments and strings\n\n  if Match(r'\\s*{\\s*$', line):\n    # We allow an open brace to start a line in the case where someone is using\n    # braces in a block to explicitly create a new scope, which is commonly used\n    # to control the lifetime of stack-allocated variables.  Braces are also\n    # used for brace initializers inside function calls.  We don't detect this\n    # perfectly: we just don't complain if the last non-whitespace character on\n    # the previous non-blank line is ',', ';', ':', '(', '{', or '}', or if the\n    # previous line starts a preprocessor block. We also allow a brace on the\n    # following line if it is part of an array initialization and would not fit\n    # within the 80 character limit of the preceding line.\n    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n    if (not Search(r'[,;:}{(]\\s*$', prevline) and\n        not Match(r'\\s*#', prevline) and\n        not (GetLineWidth(prevline) > _line_length - 2 and '[]' in prevline)):\n      error(filename, linenum, 'whitespace/braces', 4,\n            '{ should almost always be at the end of the previous line')\n\n  # An else clause should be on the same line as the preceding closing brace.\n  if Match(r'\\s*else\\b\\s*(?:if\\b|\\{|$)', line):\n    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n    if Match(r'\\s*}\\s*$', prevline):\n      error(filename, linenum, 'whitespace/newline', 4,\n            'An else should appear on the same line as the preceding }')\n\n  # If braces come on one side of an else, they should be on both.\n  # However, we have to worry about \"else if\" that spans multiple lines!\n  if Search(r'else if\\s*\\(', line):       # could be multi-line if\n    brace_on_left = bool(Search(r'}\\s*else if\\s*\\(', line))\n    # find the ( after the if\n    pos = line.find('else if')\n    pos = line.find('(', pos)\n    if pos > 0:\n      (endline, _, endpos) = CloseExpression(clean_lines, linenum, pos)\n      brace_on_right = endline[endpos:].find('{') != -1\n      if brace_on_left != brace_on_right:    # must be brace after if\n        error(filename, linenum, 'readability/braces', 5,\n              'If an else has a brace on one side, it should have it on both')\n  elif Search(r'}\\s*else[^{]*$', line) or Match(r'[^}]*else\\s*{', line):\n    error(filename, linenum, 'readability/braces', 5,\n          'If an else has a brace on one side, it should have it on both')\n\n  # Likewise, an else should never have the else clause on the same line\n  if Search(r'\\belse [^\\s{]', line) and not Search(r'\\belse if\\b', line):\n    error(filename, linenum, 'whitespace/newline', 4,\n          'Else clause should never be on same line as else (use 2 lines)')\n\n  # In the same way, a do/while should never be on one line\n  if Match(r'\\s*do [^\\s{]', line):\n    error(filename, linenum, 'whitespace/newline', 4,\n          'do/while clauses should not be on a single line')\n\n  # Check single-line if/else bodies. The style guide says 'curly braces are not\n  # required for single-line statements'. We additionally allow multi-line,\n  # single statements, but we reject anything with more than one semicolon in\n  # it. This means that the first semicolon after the if should be at the end of\n  # its line, and the line after that should have an indent level equal to or\n  # lower than the if. We also check for ambiguous if/else nesting without\n  # braces.\n  if_else_match = Search(r'\\b(if\\s*(|constexpr)\\s*\\(|else\\b)', line)\n  if if_else_match and not Match(r'\\s*#', line):\n    if_indent = GetIndentLevel(line)\n    endline, endlinenum, endpos = line, linenum, if_else_match.end()\n    if_match = Search(r'\\bif\\s*(|constexpr)\\s*\\(', line)\n    if if_match:\n      # This could be a multiline if condition, so find the end first.\n      pos = if_match.end() - 1\n      (endline, endlinenum, endpos) = CloseExpression(clean_lines, linenum, pos)\n    # Check for an opening brace, either directly after the if or on the next\n    # line. If found, this isn't a single-statement conditional.\n    if (not Match(r'\\s*{', endline[endpos:])\n        and not (Match(r'\\s*$', endline[endpos:])\n                 and endlinenum < (len(clean_lines.elided) - 1)\n                 and Match(r'\\s*{', clean_lines.elided[endlinenum + 1]))):\n      while (endlinenum < len(clean_lines.elided)\n             and ';' not in clean_lines.elided[endlinenum][endpos:]):\n        endlinenum += 1\n        endpos = 0\n      if endlinenum < len(clean_lines.elided):\n        endline = clean_lines.elided[endlinenum]\n        # We allow a mix of whitespace and closing braces (e.g. for one-liner\n        # methods) and a single \\ after the semicolon (for macros)\n        endpos = endline.find(';')\n        if not Match(r';[\\s}]*(\\\\?)$', endline[endpos:]):\n          # Semicolon isn't the last character, there's something trailing.\n          # Output a warning if the semicolon is not contained inside\n          # a lambda expression.\n          if not Match(r'^[^{};]*\\[[^\\[\\]]*\\][^{}]*\\{[^{}]*\\}\\s*\\)*[;,]\\s*$',\n                       endline):\n            error(filename, linenum, 'readability/braces', 4,\n                  'If/else bodies with multiple statements require braces')\n        elif endlinenum < len(clean_lines.elided) - 1:\n          # Make sure the next line is dedented\n          next_line = clean_lines.elided[endlinenum + 1]\n          next_indent = GetIndentLevel(next_line)\n          # With ambiguous nested if statements, this will error out on the\n          # if that *doesn't* match the else, regardless of whether it's the\n          # inner one or outer one.\n          if (if_match and Match(r'\\s*else\\b', next_line)\n              and next_indent != if_indent):\n            error(filename, linenum, 'readability/braces', 4,\n                  'Else clause should be indented at the same level as if. '\n                  'Ambiguous nested if/else chains require braces.')\n          elif next_indent > if_indent:\n            error(filename, linenum, 'readability/braces', 4,\n                  'If/else bodies with multiple statements require braces')\n\n\ndef CheckTrailingSemicolon(filename, clean_lines, linenum, error):\n  \"\"\"Looks for redundant trailing semicolon.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  line = clean_lines.elided[linenum]\n\n  # Block bodies should not be followed by a semicolon.  Due to C++11\n  # brace initialization, there are more places where semicolons are\n  # required than not, so we explicitly list the allowed rules rather\n  # than listing the disallowed ones.  These are the places where \"};\"\n  # should be replaced by just \"}\":\n  # 1. Some flavor of block following closing parenthesis:\n  #    for (;;) {};\n  #    while (...) {};\n  #    switch (...) {};\n  #    Function(...) {};\n  #    if (...) {};\n  #    if (...) else if (...) {};\n  #\n  # 2. else block:\n  #    if (...) else {};\n  #\n  # 3. const member function:\n  #    Function(...) const {};\n  #\n  # 4. Block following some statement:\n  #    x = 42;\n  #    {};\n  #\n  # 5. Block at the beginning of a function:\n  #    Function(...) {\n  #      {};\n  #    }\n  #\n  #    Note that naively checking for the preceding \"{\" will also match\n  #    braces inside multi-dimensional arrays, but this is fine since\n  #    that expression will not contain semicolons.\n  #\n  # 6. Block following another block:\n  #    while (true) {}\n  #    {};\n  #\n  # 7. End of namespaces:\n  #    namespace {};\n  #\n  #    These semicolons seems far more common than other kinds of\n  #    redundant semicolons, possibly due to people converting classes\n  #    to namespaces.  For now we do not warn for this case.\n  #\n  # Try matching case 1 first.\n  match = Match(r'^(.*\\)\\s*)\\{', line)\n  if match:\n    # Matched closing parenthesis (case 1).  Check the token before the\n    # matching opening parenthesis, and don't warn if it looks like a\n    # macro.  This avoids these false positives:\n    #  - macro that defines a base class\n    #  - multi-line macro that defines a base class\n    #  - macro that defines the whole class-head\n    #\n    # But we still issue warnings for macros that we know are safe to\n    # warn, specifically:\n    #  - TEST, TEST_F, TEST_P, MATCHER, MATCHER_P\n    #  - TYPED_TEST\n    #  - INTERFACE_DEF\n    #  - EXCLUSIVE_LOCKS_REQUIRED, SHARED_LOCKS_REQUIRED, LOCKS_EXCLUDED:\n    #\n    # We implement a list of safe macros instead of a list of\n    # unsafe macros, even though the latter appears less frequently in\n    # google code and would have been easier to implement.  This is because\n    # the downside for getting the allowed checks wrong means some extra\n    # semicolons, while the downside for getting disallowed checks wrong\n    # would result in compile errors.\n    #\n    # In addition to macros, we also don't want to warn on\n    #  - Compound literals\n    #  - Lambdas\n    #  - alignas specifier with anonymous structs\n    #  - decltype\n    closing_brace_pos = match.group(1).rfind(')')\n    opening_parenthesis = ReverseCloseExpression(\n        clean_lines, linenum, closing_brace_pos)\n    if opening_parenthesis[2] > -1:\n      line_prefix = opening_parenthesis[0][0:opening_parenthesis[2]]\n      macro = Search(r'\\b([A-Z_][A-Z0-9_]*)\\s*$', line_prefix)\n      func = Match(r'^(.*\\])\\s*$', line_prefix)\n      if ((macro and\n           macro.group(1) not in (\n               'TEST', 'TEST_F', 'MATCHER', 'MATCHER_P', 'TYPED_TEST',\n               'EXCLUSIVE_LOCKS_REQUIRED', 'SHARED_LOCKS_REQUIRED',\n               'LOCKS_EXCLUDED', 'INTERFACE_DEF')) or\n          (func and not Search(r'\\boperator\\s*\\[\\s*\\]', func.group(1))) or\n          Search(r'\\b(?:struct|union)\\s+alignas\\s*$', line_prefix) or\n          Search(r'\\bdecltype$', line_prefix) or\n          Search(r'\\s+=\\s*$', line_prefix)):\n        match = None\n    if (match and\n        opening_parenthesis[1] > 1 and\n        Search(r'\\]\\s*$', clean_lines.elided[opening_parenthesis[1] - 1])):\n      # Multi-line lambda-expression\n      match = None\n\n  else:\n    # Try matching cases 2-3.\n    match = Match(r'^(.*(?:else|\\)\\s*const)\\s*)\\{', line)\n    if not match:\n      # Try matching cases 4-6.  These are always matched on separate lines.\n      #\n      # Note that we can't simply concatenate the previous line to the\n      # current line and do a single match, otherwise we may output\n      # duplicate warnings for the blank line case:\n      #   if (cond) {\n      #     // blank line\n      #   }\n      prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n      if prevline and Search(r'[;{}]\\s*$', prevline):\n        match = Match(r'^(\\s*)\\{', line)\n\n  # Check matching closing brace\n  if match:\n    (endline, endlinenum, endpos) = CloseExpression(\n        clean_lines, linenum, len(match.group(1)))\n    if endpos > -1 and Match(r'^\\s*;', endline[endpos:]):\n      # Current {} pair is eligible for semicolon check, and we have found\n      # the redundant semicolon, output warning here.\n      #\n      # Note: because we are scanning forward for opening braces, and\n      # outputting warnings for the matching closing brace, if there are\n      # nested blocks with trailing semicolons, we will get the error\n      # messages in reversed order.\n\n      # We need to check the line forward for NOLINT\n      raw_lines = clean_lines.raw_lines\n      ParseNolintSuppressions(filename, raw_lines[endlinenum-1], endlinenum-1,\n                              error)\n      ParseNolintSuppressions(filename, raw_lines[endlinenum], endlinenum,\n                              error)\n\n      error(filename, endlinenum, 'readability/braces', 4,\n            \"You don't need a ; after a }\")\n\n\ndef CheckEmptyBlockBody(filename, clean_lines, linenum, error):\n  \"\"\"Look for empty loop/conditional body with only a single semicolon.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Search for loop keywords at the beginning of the line.  Because only\n  # whitespaces are allowed before the keywords, this will also ignore most\n  # do-while-loops, since those lines should start with closing brace.\n  #\n  # We also check \"if\" blocks here, since an empty conditional block\n  # is likely an error.\n  line = clean_lines.elided[linenum]\n  matched = Match(r'\\s*(for|while|if)\\s*\\(', line)\n  if matched:\n    # Find the end of the conditional expression.\n    (end_line, end_linenum, end_pos) = CloseExpression(\n        clean_lines, linenum, line.find('('))\n\n    # Output warning if what follows the condition expression is a semicolon.\n    # No warning for all other cases, including whitespace or newline, since we\n    # have a separate check for semicolons preceded by whitespace.\n    if end_pos >= 0 and Match(r';', end_line[end_pos:]):\n      if matched.group(1) == 'if':\n        error(filename, end_linenum, 'whitespace/empty_conditional_body', 5,\n              'Empty conditional bodies should use {}')\n      else:\n        error(filename, end_linenum, 'whitespace/empty_loop_body', 5,\n              'Empty loop bodies should use {} or continue')\n\n    # Check for if statements that have completely empty bodies (no comments)\n    # and no else clauses.\n    if end_pos >= 0 and matched.group(1) == 'if':\n      # Find the position of the opening { for the if statement.\n      # Return without logging an error if it has no brackets.\n      opening_linenum = end_linenum\n      opening_line_fragment = end_line[end_pos:]\n      # Loop until EOF or find anything that's not whitespace or opening {.\n      while not Search(r'^\\s*\\{', opening_line_fragment):\n        if Search(r'^(?!\\s*$)', opening_line_fragment):\n          # Conditional has no brackets.\n          return\n        opening_linenum += 1\n        if opening_linenum == len(clean_lines.elided):\n          # Couldn't find conditional's opening { or any code before EOF.\n          return\n        opening_line_fragment = clean_lines.elided[opening_linenum]\n      # Set opening_line (opening_line_fragment may not be entire opening line).\n      opening_line = clean_lines.elided[opening_linenum]\n\n      # Find the position of the closing }.\n      opening_pos = opening_line_fragment.find('{')\n      if opening_linenum == end_linenum:\n        # We need to make opening_pos relative to the start of the entire line.\n        opening_pos += end_pos\n      (closing_line, closing_linenum, closing_pos) = CloseExpression(\n          clean_lines, opening_linenum, opening_pos)\n      if closing_pos < 0:\n        return\n\n      # Now construct the body of the conditional. This consists of the portion\n      # of the opening line after the {, all lines until the closing line,\n      # and the portion of the closing line before the }.\n      if (clean_lines.raw_lines[opening_linenum] !=\n          CleanseComments(clean_lines.raw_lines[opening_linenum])):\n        # Opening line ends with a comment, so conditional isn't empty.\n        return\n      if closing_linenum > opening_linenum:\n        # Opening line after the {. Ignore comments here since we checked above.\n        bodylist = list(opening_line[opening_pos+1:])\n        # All lines until closing line, excluding closing line, with comments.\n        bodylist.extend(clean_lines.raw_lines[opening_linenum+1:closing_linenum])\n        # Closing line before the }. Won't (and can't) have comments.\n        bodylist.append(clean_lines.elided[closing_linenum][:closing_pos-1])\n        body = '\\n'.join(bodylist)\n      else:\n        # If statement has brackets and fits on a single line.\n        body = opening_line[opening_pos+1:closing_pos-1]\n\n      # Check if the body is empty\n      if not _EMPTY_CONDITIONAL_BODY_PATTERN.search(body):\n        return\n      # The body is empty. Now make sure there's not an else clause.\n      current_linenum = closing_linenum\n      current_line_fragment = closing_line[closing_pos:]\n      # Loop until EOF or find anything that's not whitespace or else clause.\n      while Search(r'^\\s*$|^(?=\\s*else)', current_line_fragment):\n        if Search(r'^(?=\\s*else)', current_line_fragment):\n          # Found an else clause, so don't log an error.\n          return\n        current_linenum += 1\n        if current_linenum == len(clean_lines.elided):\n          break\n        current_line_fragment = clean_lines.elided[current_linenum]\n\n      # The body is empty and there's no else clause until EOF or other code.\n      error(filename, end_linenum, 'whitespace/empty_if_body', 4,\n            ('If statement had no body and no else clause'))\n\n\ndef FindCheckMacro(line):\n  \"\"\"Find a replaceable CHECK-like macro.\n\n  Args:\n    line: line to search on.\n  Returns:\n    (macro name, start position), or (None, -1) if no replaceable\n    macro is found.\n  \"\"\"\n  for macro in _CHECK_MACROS:\n    i = line.find(macro)\n    if i >= 0:\n      # Find opening parenthesis.  Do a regular expression match here\n      # to make sure that we are matching the expected CHECK macro, as\n      # opposed to some other macro that happens to contain the CHECK\n      # substring.\n      matched = Match(r'^(.*\\b' + macro + r'\\s*)\\(', line)\n      if not matched:\n        continue\n      return (macro, len(matched.group(1)))\n  return (None, -1)\n\n\ndef CheckCheck(filename, clean_lines, linenum, error):\n  \"\"\"Checks the use of CHECK and EXPECT macros.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Decide the set of replacement macros that should be suggested\n  lines = clean_lines.elided\n  (check_macro, start_pos) = FindCheckMacro(lines[linenum])\n  if not check_macro:\n    return\n\n  # Find end of the boolean expression by matching parentheses\n  (last_line, end_line, end_pos) = CloseExpression(\n      clean_lines, linenum, start_pos)\n  if end_pos < 0:\n    return\n\n  # If the check macro is followed by something other than a\n  # semicolon, assume users will log their own custom error messages\n  # and don't suggest any replacements.\n  if not Match(r'\\s*;', last_line[end_pos:]):\n    return\n\n  if linenum == end_line:\n    expression = lines[linenum][start_pos + 1:end_pos - 1]\n  else:\n    expression = lines[linenum][start_pos + 1:]\n    for i in xrange(linenum + 1, end_line):\n      expression += lines[i]\n    expression += last_line[0:end_pos - 1]\n\n  # Parse expression so that we can take parentheses into account.\n  # This avoids false positives for inputs like \"CHECK((a < 4) == b)\",\n  # which is not replaceable by CHECK_LE.\n  lhs = ''\n  rhs = ''\n  operator = None\n  while expression:\n    matched = Match(r'^\\s*(<<|<<=|>>|>>=|->\\*|->|&&|\\|\\||'\n                    r'==|!=|>=|>|<=|<|\\()(.*)$', expression)\n    if matched:\n      token = matched.group(1)\n      if token == '(':\n        # Parenthesized operand\n        expression = matched.group(2)\n        (end, _) = FindEndOfExpressionInLine(expression, 0, ['('])\n        if end < 0:\n          return  # Unmatched parenthesis\n        lhs += '(' + expression[0:end]\n        expression = expression[end:]\n      elif token in ('&&', '||'):\n        # Logical and/or operators.  This means the expression\n        # contains more than one term, for example:\n        #   CHECK(42 < a && a < b);\n        #\n        # These are not replaceable with CHECK_LE, so bail out early.\n        return\n      elif token in ('<<', '<<=', '>>', '>>=', '->*', '->'):\n        # Non-relational operator\n        lhs += token\n        expression = matched.group(2)\n      else:\n        # Relational operator\n        operator = token\n        rhs = matched.group(2)\n        break\n    else:\n      # Unparenthesized operand.  Instead of appending to lhs one character\n      # at a time, we do another regular expression match to consume several\n      # characters at once if possible.  Trivial benchmark shows that this\n      # is more efficient when the operands are longer than a single\n      # character, which is generally the case.\n      matched = Match(r'^([^-=!<>()&|]+)(.*)$', expression)\n      if not matched:\n        matched = Match(r'^(\\s*\\S)(.*)$', expression)\n        if not matched:\n          break\n      lhs += matched.group(1)\n      expression = matched.group(2)\n\n  # Only apply checks if we got all parts of the boolean expression\n  if not (lhs and operator and rhs):\n    return\n\n  # Check that rhs do not contain logical operators.  We already know\n  # that lhs is fine since the loop above parses out && and ||.\n  if rhs.find('&&') > -1 or rhs.find('||') > -1:\n    return\n\n  # At least one of the operands must be a constant literal.  This is\n  # to avoid suggesting replacements for unprintable things like\n  # CHECK(variable != iterator)\n  #\n  # The following pattern matches decimal, hex integers, strings, and\n  # characters (in that order).\n  lhs = lhs.strip()\n  rhs = rhs.strip()\n  match_constant = r'^([-+]?(\\d+|0[xX][0-9a-fA-F]+)[lLuU]{0,3}|\".*\"|\\'.*\\')$'\n  if Match(match_constant, lhs) or Match(match_constant, rhs):\n    # Note: since we know both lhs and rhs, we can provide a more\n    # descriptive error message like:\n    #   Consider using CHECK_EQ(x, 42) instead of CHECK(x == 42)\n    # Instead of:\n    #   Consider using CHECK_EQ instead of CHECK(a == b)\n    #\n    # We are still keeping the less descriptive message because if lhs\n    # or rhs gets long, the error message might become unreadable.\n    error(filename, linenum, 'readability/check', 2,\n          'Consider using %s instead of %s(a %s b)' % (\n              _CHECK_REPLACEMENT[check_macro][operator],\n              check_macro, operator))\n\n\ndef CheckAltTokens(filename, clean_lines, linenum, error):\n  \"\"\"Check alternative keywords being used in boolean expressions.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Avoid preprocessor lines\n  if Match(r'^\\s*#', line):\n    return\n\n  # Last ditch effort to avoid multi-line comments.  This will not help\n  # if the comment started before the current line or ended after the\n  # current line, but it catches most of the false positives.  At least,\n  # it provides a way to workaround this warning for people who use\n  # multi-line comments in preprocessor macros.\n  #\n  # TODO(unknown): remove this once cpplint has better support for\n  # multi-line comments.\n  if line.find('/*') >= 0 or line.find('*/') >= 0:\n    return\n\n  for match in _ALT_TOKEN_REPLACEMENT_PATTERN.finditer(line):\n    error(filename, linenum, 'readability/alt_tokens', 2,\n          'Use operator %s instead of %s' % (\n              _ALT_TOKEN_REPLACEMENT[match.group(1)], match.group(1)))\n\n\ndef GetLineWidth(line):\n  \"\"\"Determines the width of the line in column positions.\n\n  Args:\n    line: A string, which may be a Unicode string.\n\n  Returns:\n    The width of the line in column positions, accounting for Unicode\n    combining characters and wide characters.\n  \"\"\"\n  if isinstance(line, unicode):\n    width = 0\n    for uc in unicodedata.normalize('NFC', line):\n      if unicodedata.east_asian_width(uc) in ('W', 'F'):\n        width += 2\n      elif not unicodedata.combining(uc):\n        # Issue 337\n        # https://mail.python.org/pipermail/python-list/2012-August/628809.html\n        if (sys.version_info.major, sys.version_info.minor) <= (3, 2):\n          # https://github.com/python/cpython/blob/2.7/Include/unicodeobject.h#L81\n          is_wide_build = sysconfig.get_config_var(\"Py_UNICODE_SIZE\") >= 4\n          # https://github.com/python/cpython/blob/2.7/Objects/unicodeobject.c#L564\n          is_low_surrogate = 0xDC00 <= ord(uc) <= 0xDFFF\n          if not is_wide_build and is_low_surrogate:\n            width -= 1\n\n        width += 1\n    return width\n  else:\n    return len(line)\n\n\ndef CheckStyle(filename, clean_lines, linenum, file_extension, nesting_state,\n               error):\n  \"\"\"Checks rules from the 'C++ style rules' section of cppguide.html.\n\n  Most of these rules are hard to test (naming, comment style), but we\n  do what we can.  In particular we check for 2-space indents, line lengths,\n  tab usage, spaces inside code, etc.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    file_extension: The extension (without the dot) of the filename.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Don't use \"elided\" lines here, otherwise we can't check commented lines.\n  # Don't want to use \"raw\" either, because we don't want to check inside C++11\n  # raw strings,\n  raw_lines = clean_lines.lines_without_raw_strings\n  line = raw_lines[linenum]\n  prev = raw_lines[linenum - 1] if linenum > 0 else ''\n\n  if line.find('\\t') != -1:\n    error(filename, linenum, 'whitespace/tab', 1,\n          'Tab found; better to use spaces')\n\n  # One or three blank spaces at the beginning of the line is weird; it's\n  # hard to reconcile that with 2-space indents.\n  # NOTE: here are the conditions rob pike used for his tests.  Mine aren't\n  # as sophisticated, but it may be worth becoming so:  RLENGTH==initial_spaces\n  # if(RLENGTH > 20) complain = 0;\n  # if(match($0, \" +(error|private|public|protected):\")) complain = 0;\n  # if(match(prev, \"&& *$\")) complain = 0;\n  # if(match(prev, \"\\\\|\\\\| *$\")) complain = 0;\n  # if(match(prev, \"[\\\",=><] *$\")) complain = 0;\n  # if(match($0, \" <<\")) complain = 0;\n  # if(match(prev, \" +for \\\\(\")) complain = 0;\n  # if(prevodd && match(prevprev, \" +for \\\\(\")) complain = 0;\n  scope_or_label_pattern = r'\\s*(?:public|private|protected|signals)(?:\\s+(?:slots\\s*)?)?:\\s*\\\\?$'\n  classinfo = nesting_state.InnermostClass()\n  initial_spaces = 0\n  cleansed_line = clean_lines.elided[linenum]\n  while initial_spaces < len(line) and line[initial_spaces] == ' ':\n    initial_spaces += 1\n  # There are certain situations we allow one space, notably for\n  # section labels, and also lines containing multi-line raw strings.\n  # We also don't check for lines that look like continuation lines\n  # (of lines ending in double quotes, commas, equals, or angle brackets)\n  # because the rules for how to indent those are non-trivial.\n  if (not Search(r'[\",=><] *$', prev) and\n      (initial_spaces == 1 or initial_spaces == 3) and\n      not Match(scope_or_label_pattern, cleansed_line) and\n      not (clean_lines.raw_lines[linenum] != line and\n           Match(r'^\\s*\"\"', line))):\n    error(filename, linenum, 'whitespace/indent', 3,\n          'Weird number of spaces at line-start.  '\n          'Are you using a 2-space indent?')\n\n  if line and line[-1].isspace():\n    error(filename, linenum, 'whitespace/end_of_line', 4,\n          'Line ends in whitespace.  Consider deleting these extra spaces.')\n\n  # Check if the line is a header guard.\n  is_header_guard = False\n  if IsHeaderExtension(file_extension):\n    cppvar = GetHeaderGuardCPPVariable(filename)\n    if (line.startswith('#ifndef %s' % cppvar) or\n        line.startswith('#define %s' % cppvar) or\n        line.startswith('#endif  // %s' % cppvar)):\n      is_header_guard = True\n  # #include lines and header guards can be long, since there's no clean way to\n  # split them.\n  #\n  # URLs can be long too.  It's possible to split these, but it makes them\n  # harder to cut&paste.\n  #\n  # The \"$Id:...$\" comment may also get very long without it being the\n  # developers fault.\n  #\n  # Doxygen documentation copying can get pretty long when using an overloaded\n  # function declaration\n  if (not line.startswith('#include') and not is_header_guard and\n      not Match(r'^\\s*//.*http(s?)://\\S*$', line) and\n      not Match(r'^\\s*//\\s*[^\\s]*$', line) and\n      not Match(r'^// \\$Id:.*#[0-9]+ \\$$', line) and\n      not Match(r'^\\s*/// [@\\\\](copydoc|copydetails|copybrief) .*$', line)):\n    line_width = GetLineWidth(line)\n    if line_width > _line_length:\n      error(filename, linenum, 'whitespace/line_length', 2,\n            'Lines should be <= %i characters long' % _line_length)\n\n  if (cleansed_line.count(';') > 1 and\n      # allow simple single line lambdas\n      not Match(r'^[^{};]*\\[[^\\[\\]]*\\][^{}]*\\{[^{}\\n\\r]*\\}',\n                line) and\n      # for loops are allowed two ;'s (and may run over two lines).\n      cleansed_line.find('for') == -1 and\n      (GetPreviousNonBlankLine(clean_lines, linenum)[0].find('for') == -1 or\n       GetPreviousNonBlankLine(clean_lines, linenum)[0].find(';') != -1) and\n      # It's ok to have many commands in a switch case that fits in 1 line\n      not ((cleansed_line.find('case ') != -1 or\n            cleansed_line.find('default:') != -1) and\n           cleansed_line.find('break;') != -1)):\n    error(filename, linenum, 'whitespace/newline', 0,\n          'More than one command on the same line')\n\n  # Some more style checks\n  CheckBraces(filename, clean_lines, linenum, error)\n  CheckTrailingSemicolon(filename, clean_lines, linenum, error)\n  CheckEmptyBlockBody(filename, clean_lines, linenum, error)\n  CheckSpacing(filename, clean_lines, linenum, nesting_state, error)\n  CheckOperatorSpacing(filename, clean_lines, linenum, error)\n  CheckParenthesisSpacing(filename, clean_lines, linenum, error)\n  CheckCommaSpacing(filename, clean_lines, linenum, error)\n  CheckBracesSpacing(filename, clean_lines, linenum, nesting_state, error)\n  CheckSpacingForFunctionCall(filename, clean_lines, linenum, error)\n  CheckCheck(filename, clean_lines, linenum, error)\n  CheckAltTokens(filename, clean_lines, linenum, error)\n  classinfo = nesting_state.InnermostClass()\n  if classinfo:\n    CheckSectionSpacing(filename, clean_lines, classinfo, linenum, error)\n\n\n_RE_PATTERN_INCLUDE = re.compile(r'^\\s*#\\s*include\\s*([<\"])([^>\"]*)[>\"].*$')\n# Matches the first component of a filename delimited by -s and _s. That is:\n#  _RE_FIRST_COMPONENT.match('foo').group(0) == 'foo'\n#  _RE_FIRST_COMPONENT.match('foo.cc').group(0) == 'foo'\n#  _RE_FIRST_COMPONENT.match('foo-bar_baz.cc').group(0) == 'foo'\n#  _RE_FIRST_COMPONENT.match('foo_bar-baz.cc').group(0) == 'foo'\n_RE_FIRST_COMPONENT = re.compile(r'^[^-_.]+')\n\n\ndef _DropCommonSuffixes(filename):\n  \"\"\"Drops common suffixes like _test.cc or -inl.h from filename.\n\n  For example:\n    >>> _DropCommonSuffixes('foo/foo-inl.h')\n    'foo/foo'\n    >>> _DropCommonSuffixes('foo/bar/foo.cc')\n    'foo/bar/foo'\n    >>> _DropCommonSuffixes('foo/foo_internal.h')\n    'foo/foo'\n    >>> _DropCommonSuffixes('foo/foo_unusualinternal.h')\n    'foo/foo_unusualinternal'\n\n  Args:\n    filename: The input filename.\n\n  Returns:\n    The filename with the common suffix removed.\n  \"\"\"\n  for suffix in itertools.chain(\n      ('%s.%s' % (test_suffix.lstrip('_'), ext)\n       for test_suffix, ext in itertools.product(_test_suffixes, GetNonHeaderExtensions())),\n      ('%s.%s' % (suffix, ext)\n       for suffix, ext in itertools.product(['inl', 'imp', 'internal'], GetHeaderExtensions()))):\n    if (filename.endswith(suffix) and len(filename) > len(suffix) and\n        filename[-len(suffix) - 1] in ('-', '_')):\n      return filename[:-len(suffix) - 1]\n  return os.path.splitext(filename)[0]\n\n\ndef _ClassifyInclude(fileinfo, include, used_angle_brackets, include_order=\"default\"):\n  \"\"\"Figures out what kind of header 'include' is.\n\n  Args:\n    fileinfo: The current file cpplint is running over. A FileInfo instance.\n    include: The path to a #included file.\n    used_angle_brackets: True if the #include used <> rather than \"\".\n    include_order: \"default\" or other value allowed in program arguments\n\n  Returns:\n    One of the _XXX_HEADER constants.\n\n  For example:\n    >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'stdio.h', True)\n    _C_SYS_HEADER\n    >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'string', True)\n    _CPP_SYS_HEADER\n    >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'foo/foo.h', True, \"standardcfirst\")\n    _OTHER_SYS_HEADER\n    >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'foo/foo.h', False)\n    _LIKELY_MY_HEADER\n    >>> _ClassifyInclude(FileInfo('foo/foo_unknown_extension.cc'),\n    ...                  'bar/foo_other_ext.h', False)\n    _POSSIBLE_MY_HEADER\n    >>> _ClassifyInclude(FileInfo('foo/foo.cc'), 'foo/bar.h', False)\n    _OTHER_HEADER\n  \"\"\"\n  # This is a list of all standard c++ header files, except\n  # those already checked for above.\n  is_cpp_header = include in _CPP_HEADERS\n\n  # Mark include as C header if in list or in a known folder for standard-ish C headers.\n  is_std_c_header = (include_order == \"default\") or (include in _C_HEADERS\n            # additional linux glibc header folders\n            or Search(r'(?:%s)\\/.*\\.h' % \"|\".join(C_STANDARD_HEADER_FOLDERS), include))\n\n  # Headers with C++ extensions shouldn't be considered C system headers\n  include_ext = os.path.splitext(include)[1]\n  is_system = used_angle_brackets and not include_ext in ['.hh', '.hpp', '.hxx', '.h++']\n\n  if is_system:\n    if is_cpp_header:\n      return _CPP_SYS_HEADER\n    if is_std_c_header:\n      return _C_SYS_HEADER\n    else:\n      return _OTHER_SYS_HEADER\n\n  # If the target file and the include we're checking share a\n  # basename when we drop common extensions, and the include\n  # lives in . , then it's likely to be owned by the target file.\n  target_dir, target_base = (\n      os.path.split(_DropCommonSuffixes(fileinfo.RepositoryName())))\n  include_dir, include_base = os.path.split(_DropCommonSuffixes(include))\n  target_dir_pub = os.path.normpath(target_dir + '/../public')\n  target_dir_pub = target_dir_pub.replace('\\\\', '/')\n  if target_base == include_base and (\n      include_dir == target_dir or\n      include_dir == target_dir_pub):\n    return _LIKELY_MY_HEADER\n\n  # If the target and include share some initial basename\n  # component, it's possible the target is implementing the\n  # include, so it's allowed to be first, but we'll never\n  # complain if it's not there.\n  target_first_component = _RE_FIRST_COMPONENT.match(target_base)\n  include_first_component = _RE_FIRST_COMPONENT.match(include_base)\n  if (target_first_component and include_first_component and\n      target_first_component.group(0) ==\n      include_first_component.group(0)):\n    return _POSSIBLE_MY_HEADER\n\n  return _OTHER_HEADER\n\n\n\ndef CheckIncludeLine(filename, clean_lines, linenum, include_state, error):\n  \"\"\"Check rules that are applicable to #include lines.\n\n  Strings on #include lines are NOT removed from elided line, to make\n  certain tasks easier. However, to prevent false positives, checks\n  applicable to #include lines in CheckLanguage must be put here.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    include_state: An _IncludeState instance in which the headers are inserted.\n    error: The function to call with any errors found.\n  \"\"\"\n  fileinfo = FileInfo(filename)\n  line = clean_lines.lines[linenum]\n\n  # \"include\" should use the new style \"foo/bar.h\" instead of just \"bar.h\"\n  # Only do this check if the included header follows google naming\n  # conventions.  If not, assume that it's a 3rd party API that\n  # requires special include conventions.\n  #\n  # We also make an exception for Lua headers, which follow google\n  # naming convention but not the include convention.\n  match = Match(r'#include\\s*\"([^/]+\\.(.*))\"', line)\n  if match:\n    if (IsHeaderExtension(match.group(2)) and\n        not _THIRD_PARTY_HEADERS_PATTERN.match(match.group(1))):\n      error(filename, linenum, 'build/include_subdir', 4,\n            'Include the directory when naming header files')\n\n  # we shouldn't include a file more than once. actually, there are a\n  # handful of instances where doing so is okay, but in general it's\n  # not.\n  match = _RE_PATTERN_INCLUDE.search(line)\n  if match:\n    include = match.group(2)\n    used_angle_brackets = (match.group(1) == '<')\n    duplicate_line = include_state.FindHeader(include)\n    if duplicate_line >= 0:\n      error(filename, linenum, 'build/include', 4,\n            '\"%s\" already included at %s:%s' %\n            (include, filename, duplicate_line))\n      return\n\n    for extension in GetNonHeaderExtensions():\n      if (include.endswith('.' + extension) and\n          os.path.dirname(fileinfo.RepositoryName()) != os.path.dirname(include)):\n        error(filename, linenum, 'build/include', 4,\n              'Do not include .' + extension + ' files from other packages')\n        return\n\n    # We DO want to include a 3rd party looking header if it matches the\n    # filename. Otherwise we get an erroneous error \"...should include its\n    # header\" error later.\n    third_src_header = False\n    for ext in GetHeaderExtensions():\n      basefilename = filename[0:len(filename) - len(fileinfo.Extension())]\n      headerfile = basefilename + '.' + ext\n      headername = FileInfo(headerfile).RepositoryName()\n      if headername in include or include in headername:\n        third_src_header = True\n        break\n\n    if third_src_header or not _THIRD_PARTY_HEADERS_PATTERN.match(include):\n      include_state.include_list[-1].append((include, linenum))\n\n      # We want to ensure that headers appear in the right order:\n      # 1) for foo.cc, foo.h  (preferred location)\n      # 2) c system files\n      # 3) cpp system files\n      # 4) for foo.cc, foo.h  (deprecated location)\n      # 5) other google headers\n      #\n      # We classify each include statement as one of those 5 types\n      # using a number of techniques. The include_state object keeps\n      # track of the highest type seen, and complains if we see a\n      # lower type after that.\n      error_message = include_state.CheckNextIncludeOrder(\n          _ClassifyInclude(fileinfo, include, used_angle_brackets, _include_order))\n      if error_message:\n        error(filename, linenum, 'build/include_order', 4,\n              '%s. Should be: %s.h, c system, c++ system, other.' %\n              (error_message, fileinfo.BaseName()))\n      canonical_include = include_state.CanonicalizeAlphabeticalOrder(include)\n      if not include_state.IsInAlphabeticalOrder(\n          clean_lines, linenum, canonical_include):\n        error(filename, linenum, 'build/include_alpha', 4,\n              'Include \"%s\" not in alphabetical order' % include)\n      include_state.SetLastHeader(canonical_include)\n\n\n\ndef _GetTextInside(text, start_pattern):\n  r\"\"\"Retrieves all the text between matching open and close parentheses.\n\n  Given a string of lines and a regular expression string, retrieve all the text\n  following the expression and between opening punctuation symbols like\n  (, [, or {, and the matching close-punctuation symbol. This properly nested\n  occurrences of the punctuations, so for the text like\n    printf(a(), b(c()));\n  a call to _GetTextInside(text, r'printf\\(') will return 'a(), b(c())'.\n  start_pattern must match string having an open punctuation symbol at the end.\n\n  Args:\n    text: The lines to extract text. Its comments and strings must be elided.\n           It can be single line and can span multiple lines.\n    start_pattern: The regexp string indicating where to start extracting\n                   the text.\n  Returns:\n    The extracted text.\n    None if either the opening string or ending punctuation could not be found.\n  \"\"\"\n  # TODO(unknown): Audit cpplint.py to see what places could be profitably\n  # rewritten to use _GetTextInside (and use inferior regexp matching today).\n\n  # Give opening punctuations to get the matching close-punctuations.\n  matching_punctuation = {'(': ')', '{': '}', '[': ']'}\n  closing_punctuation = set(itervalues(matching_punctuation))\n\n  # Find the position to start extracting text.\n  match = re.search(start_pattern, text, re.M)\n  if not match:  # start_pattern not found in text.\n    return None\n  start_position = match.end(0)\n\n  assert start_position > 0, (\n      'start_pattern must ends with an opening punctuation.')\n  assert text[start_position - 1] in matching_punctuation, (\n      'start_pattern must ends with an opening punctuation.')\n  # Stack of closing punctuations we expect to have in text after position.\n  punctuation_stack = [matching_punctuation[text[start_position - 1]]]\n  position = start_position\n  while punctuation_stack and position < len(text):\n    if text[position] == punctuation_stack[-1]:\n      punctuation_stack.pop()\n    elif text[position] in closing_punctuation:\n      # A closing punctuation without matching opening punctuations.\n      return None\n    elif text[position] in matching_punctuation:\n      punctuation_stack.append(matching_punctuation[text[position]])\n    position += 1\n  if punctuation_stack:\n    # Opening punctuations left without matching close-punctuations.\n    return None\n  # punctuations match.\n  return text[start_position:position - 1]\n\n\n# Patterns for matching call-by-reference parameters.\n#\n# Supports nested templates up to 2 levels deep using this messy pattern:\n#   < (?: < (?: < [^<>]*\n#               >\n#           |   [^<>] )*\n#         >\n#     |   [^<>] )*\n#   >\n_RE_PATTERN_IDENT = r'[_a-zA-Z]\\w*'  # =~ [[:alpha:]][[:alnum:]]*\n_RE_PATTERN_TYPE = (\n    r'(?:const\\s+)?(?:typename\\s+|class\\s+|struct\\s+|union\\s+|enum\\s+)?'\n    r'(?:\\w|'\n    r'\\s*<(?:<(?:<[^<>]*>|[^<>])*>|[^<>])*>|'\n    r'::)+')\n# A call-by-reference parameter ends with '& identifier'.\n_RE_PATTERN_REF_PARAM = re.compile(\n    r'(' + _RE_PATTERN_TYPE + r'(?:\\s*(?:\\bconst\\b|[*]))*\\s*'\n    r'&\\s*' + _RE_PATTERN_IDENT + r')\\s*(?:=[^,()]+)?[,)]')\n# A call-by-const-reference parameter either ends with 'const& identifier'\n# or looks like 'const type& identifier' when 'type' is atomic.\n_RE_PATTERN_CONST_REF_PARAM = (\n    r'(?:.*\\s*\\bconst\\s*&\\s*' + _RE_PATTERN_IDENT +\n    r'|const\\s+' + _RE_PATTERN_TYPE + r'\\s*&\\s*' + _RE_PATTERN_IDENT + r')')\n# Stream types.\n_RE_PATTERN_REF_STREAM_PARAM = (\n    r'(?:.*stream\\s*&\\s*' + _RE_PATTERN_IDENT + r')')\n\n\ndef CheckLanguage(filename, clean_lines, linenum, file_extension,\n                  include_state, nesting_state, error):\n  \"\"\"Checks rules from the 'C++ language rules' section of cppguide.html.\n\n  Some of these rules are hard to test (function overloading, using\n  uint32 inappropriately), but we do the best we can.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    file_extension: The extension (without the dot) of the filename.\n    include_state: An _IncludeState instance in which the headers are inserted.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n  # If the line is empty or consists of entirely a comment, no need to\n  # check it.\n  line = clean_lines.elided[linenum]\n  if not line:\n    return\n\n  match = _RE_PATTERN_INCLUDE.search(line)\n  if match:\n    CheckIncludeLine(filename, clean_lines, linenum, include_state, error)\n    return\n\n  # Reset include state across preprocessor directives.  This is meant\n  # to silence warnings for conditional includes.\n  match = Match(r'^\\s*#\\s*(if|ifdef|ifndef|elif|else|endif)\\b', line)\n  if match:\n    include_state.ResetSection(match.group(1))\n\n\n  # Perform other checks now that we are sure that this is not an include line\n  CheckCasts(filename, clean_lines, linenum, error)\n  CheckGlobalStatic(filename, clean_lines, linenum, error)\n  CheckPrintf(filename, clean_lines, linenum, error)\n\n  if IsHeaderExtension(file_extension):\n    # TODO(unknown): check that 1-arg constructors are explicit.\n    #                How to tell it's a constructor?\n    #                (handled in CheckForNonStandardConstructs for now)\n    # TODO(unknown): check that classes declare or disable copy/assign\n    #                (level 1 error)\n    pass\n\n  # Check if people are using the verboten C basic types.  The only exception\n  # we regularly allow is \"unsigned short port\" for port.\n  if Search(r'\\bshort port\\b', line):\n    if not Search(r'\\bunsigned short port\\b', line):\n      error(filename, linenum, 'runtime/int', 4,\n            'Use \"unsigned short\" for ports, not \"short\"')\n  else:\n    match = Search(r'\\b(short|long(?! +double)|long long)\\b', line)\n    if match:\n      error(filename, linenum, 'runtime/int', 4,\n            'Use int16/int64/etc, rather than the C type %s' % match.group(1))\n\n  # Check if some verboten operator overloading is going on\n  # TODO(unknown): catch out-of-line unary operator&:\n  #   class X {};\n  #   int operator&(const X& x) { return 42; }  // unary operator&\n  # The trick is it's hard to tell apart from binary operator&:\n  #   class Y { int operator&(const Y& x) { return 23; } }; // binary operator&\n  if Search(r'\\boperator\\s*&\\s*\\(\\s*\\)', line):\n    error(filename, linenum, 'runtime/operator', 4,\n          'Unary operator& is dangerous.  Do not use it.')\n\n  # Check for suspicious usage of \"if\" like\n  # } if (a == b) {\n  if Search(r'\\}\\s*if\\s*\\(', line):\n    error(filename, linenum, 'readability/braces', 4,\n          'Did you mean \"else if\"? If not, start a new line for \"if\".')\n\n  # Check for potential format string bugs like printf(foo).\n  # We constrain the pattern not to pick things like DocidForPrintf(foo).\n  # Not perfect but it can catch printf(foo.c_str()) and printf(foo->c_str())\n  # TODO(unknown): Catch the following case. Need to change the calling\n  # convention of the whole function to process multiple line to handle it.\n  #   printf(\n  #       boy_this_is_a_really_long_variable_that_cannot_fit_on_the_prev_line);\n  printf_args = _GetTextInside(line, r'(?i)\\b(string)?printf\\s*\\(')\n  if printf_args:\n    match = Match(r'([\\w.\\->()]+)$', printf_args)\n    if match and match.group(1) != '__VA_ARGS__':\n      function_name = re.search(r'\\b((?:string)?printf)\\s*\\(',\n                                line, re.I).group(1)\n      error(filename, linenum, 'runtime/printf', 4,\n            'Potential format string bug. Do %s(\"%%s\", %s) instead.'\n            % (function_name, match.group(1)))\n\n  # Check for potential memset bugs like memset(buf, sizeof(buf), 0).\n  match = Search(r'memset\\s*\\(([^,]*),\\s*([^,]*),\\s*0\\s*\\)', line)\n  if match and not Match(r\"^''|-?[0-9]+|0x[0-9A-Fa-f]$\", match.group(2)):\n    error(filename, linenum, 'runtime/memset', 4,\n          'Did you mean \"memset(%s, 0, %s)\"?'\n          % (match.group(1), match.group(2)))\n\n  if Search(r'\\busing namespace\\b', line):\n    if Search(r'\\bliterals\\b', line):\n      error(filename, linenum, 'build/namespaces_literals', 5,\n            'Do not use namespace using-directives.  '\n            'Use using-declarations instead.')\n    else:\n      error(filename, linenum, 'build/namespaces', 5,\n            'Do not use namespace using-directives.  '\n            'Use using-declarations instead.')\n\n  # Detect variable-length arrays.\n  match = Match(r'\\s*(.+::)?(\\w+) [a-z]\\w*\\[(.+)];', line)\n  if (match and match.group(2) != 'return' and match.group(2) != 'delete' and\n      match.group(3).find(']') == -1):\n    # Split the size using space and arithmetic operators as delimiters.\n    # If any of the resulting tokens are not compile time constants then\n    # report the error.\n    tokens = re.split(r'\\s|\\+|\\-|\\*|\\/|<<|>>]', match.group(3))\n    is_const = True\n    skip_next = False\n    for tok in tokens:\n      if skip_next:\n        skip_next = False\n        continue\n\n      if Search(r'sizeof\\(.+\\)', tok): continue\n      if Search(r'arraysize\\(\\w+\\)', tok): continue\n\n      tok = tok.lstrip('(')\n      tok = tok.rstrip(')')\n      if not tok: continue\n      if Match(r'\\d+', tok): continue\n      if Match(r'0[xX][0-9a-fA-F]+', tok): continue\n      if Match(r'k[A-Z0-9]\\w*', tok): continue\n      if Match(r'(.+::)?k[A-Z0-9]\\w*', tok): continue\n      if Match(r'(.+::)?[A-Z][A-Z0-9_]*', tok): continue\n      # A catch all for tricky sizeof cases, including 'sizeof expression',\n      # 'sizeof(*type)', 'sizeof(const type)', 'sizeof(struct StructName)'\n      # requires skipping the next token because we split on ' ' and '*'.\n      if tok.startswith('sizeof'):\n        skip_next = True\n        continue\n      is_const = False\n      break\n    if not is_const:\n      error(filename, linenum, 'runtime/arrays', 1,\n            'Do not use variable-length arrays.  Use an appropriately named '\n            \"('k' followed by CamelCase) compile-time constant for the size.\")\n\n  # Check for use of unnamed namespaces in header files.  Registration\n  # macros are typically OK, so we allow use of \"namespace {\" on lines\n  # that end with backslashes.\n  if (IsHeaderExtension(file_extension)\n      and Search(r'\\bnamespace\\s*{', line)\n      and line[-1] != '\\\\'):\n    error(filename, linenum, 'build/namespaces_headers', 4,\n          'Do not use unnamed namespaces in header files.  See '\n          'https://google-styleguide.googlecode.com/svn/trunk/cppguide.xml#Namespaces'\n          ' for more information.')\n\n\ndef CheckGlobalStatic(filename, clean_lines, linenum, error):\n  \"\"\"Check for unsafe global or static objects.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Match two lines at a time to support multiline declarations\n  if linenum + 1 < clean_lines.NumLines() and not Search(r'[;({]', line):\n    line += clean_lines.elided[linenum + 1].strip()\n\n  # Check for people declaring static/global STL strings at the top level.\n  # This is dangerous because the C++ language does not guarantee that\n  # globals with constructors are initialized before the first access, and\n  # also because globals can be destroyed when some threads are still running.\n  # TODO(unknown): Generalize this to also find static unique_ptr instances.\n  # TODO(unknown): File bugs for clang-tidy to find these.\n  match = Match(\n      r'((?:|static +)(?:|const +))(?::*std::)?string( +const)? +'\n      r'([a-zA-Z0-9_:]+)\\b(.*)',\n      line)\n\n  # Remove false positives:\n  # - String pointers (as opposed to values).\n  #    string *pointer\n  #    const string *pointer\n  #    string const *pointer\n  #    string *const pointer\n  #\n  # - Functions and template specializations.\n  #    string Function<Type>(...\n  #    string Class<Type>::Method(...\n  #\n  # - Operators.  These are matched separately because operator names\n  #   cross non-word boundaries, and trying to match both operators\n  #   and functions at the same time would decrease accuracy of\n  #   matching identifiers.\n  #    string Class::operator*()\n  if (match and\n      not Search(r'\\bstring\\b(\\s+const)?\\s*[\\*\\&]\\s*(const\\s+)?\\w', line) and\n      not Search(r'\\boperator\\W', line) and\n      not Match(r'\\s*(<.*>)?(::[a-zA-Z0-9_]+)*\\s*\\(([^\"]|$)', match.group(4))):\n    if Search(r'\\bconst\\b', line):\n      error(filename, linenum, 'runtime/string', 4,\n            'For a static/global string constant, use a C style string '\n            'instead: \"%schar%s %s[]\".' %\n            (match.group(1), match.group(2) or '', match.group(3)))\n    else:\n      error(filename, linenum, 'runtime/string', 4,\n            'Static/global string variables are not permitted.')\n\n  if (Search(r'\\b([A-Za-z0-9_]*_)\\(\\1\\)', line) or\n      Search(r'\\b([A-Za-z0-9_]*_)\\(CHECK_NOTNULL\\(\\1\\)\\)', line)):\n    error(filename, linenum, 'runtime/init', 4,\n          'You seem to be initializing a member variable with itself.')\n\n\ndef CheckPrintf(filename, clean_lines, linenum, error):\n  \"\"\"Check for printf related issues.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # When snprintf is used, the second argument shouldn't be a literal.\n  match = Search(r'snprintf\\s*\\(([^,]*),\\s*([0-9]*)\\s*,', line)\n  if match and match.group(2) != '0':\n    # If 2nd arg is zero, snprintf is used to calculate size.\n    error(filename, linenum, 'runtime/printf', 3,\n          'If you can, use sizeof(%s) instead of %s as the 2nd arg '\n          'to snprintf.' % (match.group(1), match.group(2)))\n\n  # Check if some verboten C functions are being used.\n  if Search(r'\\bsprintf\\s*\\(', line):\n    error(filename, linenum, 'runtime/printf', 5,\n          'Never use sprintf. Use snprintf instead.')\n  match = Search(r'\\b(strcpy|strcat)\\s*\\(', line)\n  if match:\n    error(filename, linenum, 'runtime/printf', 4,\n          'Almost always, snprintf is better than %s' % match.group(1))\n\n\ndef IsDerivedFunction(clean_lines, linenum):\n  \"\"\"Check if current line contains an inherited function.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if current line contains a function with \"override\"\n    virt-specifier.\n  \"\"\"\n  # Scan back a few lines for start of current function\n  for i in xrange(linenum, max(-1, linenum - 10), -1):\n    match = Match(r'^([^()]*\\w+)\\(', clean_lines.elided[i])\n    if match:\n      # Look for \"override\" after the matching closing parenthesis\n      line, _, closing_paren = CloseExpression(\n          clean_lines, i, len(match.group(1)))\n      return (closing_paren >= 0 and\n              Search(r'\\boverride\\b', line[closing_paren:]))\n  return False\n\n\ndef IsOutOfLineMethodDefinition(clean_lines, linenum):\n  \"\"\"Check if current line contains an out-of-line method definition.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if current line contains an out-of-line method definition.\n  \"\"\"\n  # Scan back a few lines for start of current function\n  for i in xrange(linenum, max(-1, linenum - 10), -1):\n    if Match(r'^([^()]*\\w+)\\(', clean_lines.elided[i]):\n      return Match(r'^[^()]*\\w+::\\w+\\(', clean_lines.elided[i]) is not None\n  return False\n\n\ndef IsInitializerList(clean_lines, linenum):\n  \"\"\"Check if current line is inside constructor initializer list.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if current line appears to be inside constructor initializer\n    list, False otherwise.\n  \"\"\"\n  for i in xrange(linenum, 1, -1):\n    line = clean_lines.elided[i]\n    if i == linenum:\n      remove_function_body = Match(r'^(.*)\\{\\s*$', line)\n      if remove_function_body:\n        line = remove_function_body.group(1)\n\n    if Search(r'\\s:\\s*\\w+[({]', line):\n      # A lone colon tend to indicate the start of a constructor\n      # initializer list.  It could also be a ternary operator, which\n      # also tend to appear in constructor initializer lists as\n      # opposed to parameter lists.\n      return True\n    if Search(r'\\}\\s*,\\s*$', line):\n      # A closing brace followed by a comma is probably the end of a\n      # brace-initialized member in constructor initializer list.\n      return True\n    if Search(r'[{};]\\s*$', line):\n      # Found one of the following:\n      # - A closing brace or semicolon, probably the end of the previous\n      #   function.\n      # - An opening brace, probably the start of current class or namespace.\n      #\n      # Current line is probably not inside an initializer list since\n      # we saw one of those things without seeing the starting colon.\n      return False\n\n  # Got to the beginning of the file without seeing the start of\n  # constructor initializer list.\n  return False\n\n\ndef CheckForNonConstReference(filename, clean_lines, linenum,\n                              nesting_state, error):\n  \"\"\"Check for non-const references.\n\n  Separate from CheckLanguage since it scans backwards from current\n  line, instead of scanning forward.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n  # Do nothing if there is no '&' on current line.\n  line = clean_lines.elided[linenum]\n  if '&' not in line:\n    return\n\n  # If a function is inherited, current function doesn't have much of\n  # a choice, so any non-const references should not be blamed on\n  # derived function.\n  if IsDerivedFunction(clean_lines, linenum):\n    return\n\n  # Don't warn on out-of-line method definitions, as we would warn on the\n  # in-line declaration, if it isn't marked with 'override'.\n  if IsOutOfLineMethodDefinition(clean_lines, linenum):\n    return\n\n  # Long type names may be broken across multiple lines, usually in one\n  # of these forms:\n  #   LongType\n  #       ::LongTypeContinued &identifier\n  #   LongType::\n  #       LongTypeContinued &identifier\n  #   LongType<\n  #       ...>::LongTypeContinued &identifier\n  #\n  # If we detected a type split across two lines, join the previous\n  # line to current line so that we can match const references\n  # accordingly.\n  #\n  # Note that this only scans back one line, since scanning back\n  # arbitrary number of lines would be expensive.  If you have a type\n  # that spans more than 2 lines, please use a typedef.\n  if linenum > 1:\n    previous = None\n    if Match(r'\\s*::(?:[\\w<>]|::)+\\s*&\\s*\\S', line):\n      # previous_line\\n + ::current_line\n      previous = Search(r'\\b((?:const\\s*)?(?:[\\w<>]|::)+[\\w<>])\\s*$',\n                        clean_lines.elided[linenum - 1])\n    elif Match(r'\\s*[a-zA-Z_]([\\w<>]|::)+\\s*&\\s*\\S', line):\n      # previous_line::\\n + current_line\n      previous = Search(r'\\b((?:const\\s*)?(?:[\\w<>]|::)+::)\\s*$',\n                        clean_lines.elided[linenum - 1])\n    if previous:\n      line = previous.group(1) + line.lstrip()\n    else:\n      # Check for templated parameter that is split across multiple lines\n      endpos = line.rfind('>')\n      if endpos > -1:\n        (_, startline, startpos) = ReverseCloseExpression(\n            clean_lines, linenum, endpos)\n        if startpos > -1 and startline < linenum:\n          # Found the matching < on an earlier line, collect all\n          # pieces up to current line.\n          line = ''\n          for i in xrange(startline, linenum + 1):\n            line += clean_lines.elided[i].strip()\n\n  # Check for non-const references in function parameters.  A single '&' may\n  # found in the following places:\n  #   inside expression: binary & for bitwise AND\n  #   inside expression: unary & for taking the address of something\n  #   inside declarators: reference parameter\n  # We will exclude the first two cases by checking that we are not inside a\n  # function body, including one that was just introduced by a trailing '{'.\n  # TODO(unknown): Doesn't account for 'catch(Exception& e)' [rare].\n  if (nesting_state.previous_stack_top and\n      not (isinstance(nesting_state.previous_stack_top, _ClassInfo) or\n           isinstance(nesting_state.previous_stack_top, _NamespaceInfo))):\n    # Not at toplevel, not within a class, and not within a namespace\n    return\n\n  # Avoid initializer lists.  We only need to scan back from the\n  # current line for something that starts with ':'.\n  #\n  # We don't need to check the current line, since the '&' would\n  # appear inside the second set of parentheses on the current line as\n  # opposed to the first set.\n  if linenum > 0:\n    for i in xrange(linenum - 1, max(0, linenum - 10), -1):\n      previous_line = clean_lines.elided[i]\n      if not Search(r'[),]\\s*$', previous_line):\n        break\n      if Match(r'^\\s*:\\s+\\S', previous_line):\n        return\n\n  # Avoid preprocessors\n  if Search(r'\\\\\\s*$', line):\n    return\n\n  # Avoid constructor initializer lists\n  if IsInitializerList(clean_lines, linenum):\n    return\n\n  # We allow non-const references in a few standard places, like functions\n  # called \"swap()\" or iostream operators like \"<<\" or \">>\".  Do not check\n  # those function parameters.\n  #\n  # We also accept & in static_assert, which looks like a function but\n  # it's actually a declaration expression.\n  allowed_functions = (r'(?:[sS]wap(?:<\\w:+>)?|'\n                           r'operator\\s*[<>][<>]|'\n                           r'static_assert|COMPILE_ASSERT'\n                           r')\\s*\\(')\n  if Search(allowed_functions, line):\n    return\n  elif not Search(r'\\S+\\([^)]*$', line):\n    # Don't see an allowed function on this line.  Actually we\n    # didn't see any function name on this line, so this is likely a\n    # multi-line parameter list.  Try a bit harder to catch this case.\n    for i in xrange(2):\n      if (linenum > i and\n          Search(allowed_functions, clean_lines.elided[linenum - i - 1])):\n        return\n\n  decls = ReplaceAll(r'{[^}]*}', ' ', line)  # exclude function body\n  for parameter in re.findall(_RE_PATTERN_REF_PARAM, decls):\n    if (not Match(_RE_PATTERN_CONST_REF_PARAM, parameter) and\n        not Match(_RE_PATTERN_REF_STREAM_PARAM, parameter)):\n      error(filename, linenum, 'runtime/references', 2,\n            'Is this a non-const reference? '\n            'If so, make const or use a pointer: ' +\n            ReplaceAll(' *<', '<', parameter))\n\n\ndef CheckCasts(filename, clean_lines, linenum, error):\n  \"\"\"Various cast related checks.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Check to see if they're using an conversion function cast.\n  # I just try to capture the most common basic types, though there are more.\n  # Parameterless conversion functions, such as bool(), are allowed as they are\n  # probably a member operator declaration or default constructor.\n  match = Search(\n      r'(\\bnew\\s+(?:const\\s+)?|\\S<\\s*(?:const\\s+)?)?\\b'\n      r'(int|float|double|bool|char|int32|uint32|int64|uint64)'\n      r'(\\([^)].*)', line)\n  expecting_function = ExpectingFunctionArgs(clean_lines, linenum)\n  if match and not expecting_function:\n    matched_type = match.group(2)\n\n    # matched_new_or_template is used to silence two false positives:\n    # - New operators\n    # - Template arguments with function types\n    #\n    # For template arguments, we match on types immediately following\n    # an opening bracket without any spaces.  This is a fast way to\n    # silence the common case where the function type is the first\n    # template argument.  False negative with less-than comparison is\n    # avoided because those operators are usually followed by a space.\n    #\n    #   function<double(double)>   // bracket + no space = false positive\n    #   value < double(42)         // bracket + space = true positive\n    matched_new_or_template = match.group(1)\n\n    # Avoid arrays by looking for brackets that come after the closing\n    # parenthesis.\n    if Match(r'\\([^()]+\\)\\s*\\[', match.group(3)):\n      return\n\n    # Other things to ignore:\n    # - Function pointers\n    # - Casts to pointer types\n    # - Placement new\n    # - Alias declarations\n    matched_funcptr = match.group(3)\n    if (matched_new_or_template is None and\n        not (matched_funcptr and\n             (Match(r'\\((?:[^() ]+::\\s*\\*\\s*)?[^() ]+\\)\\s*\\(',\n                    matched_funcptr) or\n              matched_funcptr.startswith('(*)'))) and\n        not Match(r'\\s*using\\s+\\S+\\s*=\\s*' + matched_type, line) and\n        not Search(r'new\\(\\S+\\)\\s*' + matched_type, line)):\n      error(filename, linenum, 'readability/casting', 4,\n            'Using deprecated casting style.  '\n            'Use static_cast<%s>(...) instead' %\n            matched_type)\n\n  if not expecting_function:\n    CheckCStyleCast(filename, clean_lines, linenum, 'static_cast',\n                    r'\\((int|float|double|bool|char|u?int(16|32|64)|size_t)\\)', error)\n\n  # This doesn't catch all cases. Consider (const char * const)\"hello\".\n  #\n  # (char *) \"foo\" should always be a const_cast (reinterpret_cast won't\n  # compile).\n  if CheckCStyleCast(filename, clean_lines, linenum, 'const_cast',\n                     r'\\((char\\s?\\*+\\s?)\\)\\s*\"', error):\n    pass\n  else:\n    # Check pointer casts for other than string constants\n    CheckCStyleCast(filename, clean_lines, linenum, 'reinterpret_cast',\n                    r'\\((\\w+\\s?\\*+\\s?)\\)', error)\n\n  # In addition, we look for people taking the address of a cast.  This\n  # is dangerous -- casts can assign to temporaries, so the pointer doesn't\n  # point where you think.\n  #\n  # Some non-identifier character is required before the '&' for the\n  # expression to be recognized as a cast.  These are casts:\n  #   expression = &static_cast<int*>(temporary());\n  #   function(&(int*)(temporary()));\n  #\n  # This is not a cast:\n  #   reference_type&(int* function_param);\n  match = Search(\n      r'(?:[^\\w]&\\(([^)*][^)]*)\\)[\\w(])|'\n      r'(?:[^\\w]&(static|dynamic|down|reinterpret)_cast\\b)', line)\n  if match:\n    # Try a better error message when the & is bound to something\n    # dereferenced by the casted pointer, as opposed to the casted\n    # pointer itself.\n    parenthesis_error = False\n    match = Match(r'^(.*&(?:static|dynamic|down|reinterpret)_cast\\b)<', line)\n    if match:\n      _, y1, x1 = CloseExpression(clean_lines, linenum, len(match.group(1)))\n      if x1 >= 0 and clean_lines.elided[y1][x1] == '(':\n        _, y2, x2 = CloseExpression(clean_lines, y1, x1)\n        if x2 >= 0:\n          extended_line = clean_lines.elided[y2][x2:]\n          if y2 < clean_lines.NumLines() - 1:\n            extended_line += clean_lines.elided[y2 + 1]\n          if Match(r'\\s*(?:->|\\[)', extended_line):\n            parenthesis_error = True\n\n    if parenthesis_error:\n      error(filename, linenum, 'readability/casting', 4,\n            ('Are you taking an address of something dereferenced '\n             'from a cast?  Wrapping the dereferenced expression in '\n             'parentheses will make the binding more obvious'))\n    else:\n      error(filename, linenum, 'runtime/casting', 4,\n            ('Are you taking an address of a cast?  '\n             'This is dangerous: could be a temp var.  '\n             'Take the address before doing the cast, rather than after'))\n\n\ndef CheckCStyleCast(filename, clean_lines, linenum, cast_type, pattern, error):\n  \"\"\"Checks for a C-style cast by looking for the pattern.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    cast_type: The string for the C++ cast to recommend.  This is either\n      reinterpret_cast, static_cast, or const_cast, depending.\n    pattern: The regular expression used to find C-style casts.\n    error: The function to call with any errors found.\n\n  Returns:\n    True if an error was emitted.\n    False otherwise.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  match = Search(pattern, line)\n  if not match:\n    return False\n\n  # Exclude lines with keywords that tend to look like casts\n  context = line[0:match.start(1) - 1]\n  if Match(r'.*\\b(?:sizeof|alignof|alignas|[_A-Z][_A-Z0-9]*)\\s*$', context):\n    return False\n\n  # Try expanding current context to see if we one level of\n  # parentheses inside a macro.\n  if linenum > 0:\n    for i in xrange(linenum - 1, max(0, linenum - 5), -1):\n      context = clean_lines.elided[i] + context\n  if Match(r'.*\\b[_A-Z][_A-Z0-9]*\\s*\\((?:\\([^()]*\\)|[^()])*$', context):\n    return False\n\n  # operator++(int) and operator--(int)\n  if (context.endswith(' operator++') or context.endswith(' operator--') or\n      context.endswith('::operator++') or context.endswith('::operator--')):\n    return False\n\n  # A single unnamed argument for a function tends to look like old style cast.\n  # If we see those, don't issue warnings for deprecated casts.\n  remainder = line[match.end(0):]\n  if Match(r'^\\s*(?:;|const\\b|throw\\b|final\\b|override\\b|[=>{),]|->)',\n           remainder):\n    return False\n\n  # At this point, all that should be left is actual casts.\n  error(filename, linenum, 'readability/casting', 4,\n        'Using C-style cast.  Use %s<%s>(...) instead' %\n        (cast_type, match.group(1)))\n\n  return True\n\n\ndef ExpectingFunctionArgs(clean_lines, linenum):\n  \"\"\"Checks whether where function type arguments are expected.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n\n  Returns:\n    True if the line at 'linenum' is inside something that expects arguments\n    of function types.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  return (Match(r'^\\s*MOCK_(CONST_)?METHOD\\d+(_T)?\\(', line) or\n          (linenum >= 2 and\n           (Match(r'^\\s*MOCK_(?:CONST_)?METHOD\\d+(?:_T)?\\((?:\\S+,)?\\s*$',\n                  clean_lines.elided[linenum - 1]) or\n            Match(r'^\\s*MOCK_(?:CONST_)?METHOD\\d+(?:_T)?\\(\\s*$',\n                  clean_lines.elided[linenum - 2]) or\n            Search(r'\\bstd::m?function\\s*\\<\\s*$',\n                   clean_lines.elided[linenum - 1]))))\n\n\n_HEADERS_CONTAINING_TEMPLATES = (\n    ('<deque>', ('deque',)),\n    ('<functional>', ('unary_function', 'binary_function',\n                      'plus', 'minus', 'multiplies', 'divides', 'modulus',\n                      'negate',\n                      'equal_to', 'not_equal_to', 'greater', 'less',\n                      'greater_equal', 'less_equal',\n                      'logical_and', 'logical_or', 'logical_not',\n                      'unary_negate', 'not1', 'binary_negate', 'not2',\n                      'bind1st', 'bind2nd',\n                      'pointer_to_unary_function',\n                      'pointer_to_binary_function',\n                      'ptr_fun',\n                      'mem_fun_t', 'mem_fun', 'mem_fun1_t', 'mem_fun1_ref_t',\n                      'mem_fun_ref_t',\n                      'const_mem_fun_t', 'const_mem_fun1_t',\n                      'const_mem_fun_ref_t', 'const_mem_fun1_ref_t',\n                      'mem_fun_ref',\n                     )),\n    ('<limits>', ('numeric_limits',)),\n    ('<list>', ('list',)),\n    ('<map>', ('multimap',)),\n    ('<memory>', ('allocator', 'make_shared', 'make_unique', 'shared_ptr',\n                  'unique_ptr', 'weak_ptr')),\n    ('<queue>', ('queue', 'priority_queue',)),\n    ('<set>', ('multiset',)),\n    ('<stack>', ('stack',)),\n    ('<string>', ('char_traits', 'basic_string',)),\n    ('<tuple>', ('tuple',)),\n    ('<unordered_map>', ('unordered_map', 'unordered_multimap')),\n    ('<unordered_set>', ('unordered_set', 'unordered_multiset')),\n    ('<utility>', ('pair',)),\n    ('<vector>', ('vector',)),\n\n    # gcc extensions.\n    # Note: std::hash is their hash, ::hash is our hash\n    ('<hash_map>', ('hash_map', 'hash_multimap',)),\n    ('<hash_set>', ('hash_set', 'hash_multiset',)),\n    ('<slist>', ('slist',)),\n    )\n\n_HEADERS_MAYBE_TEMPLATES = (\n    ('<algorithm>', ('copy', 'max', 'min', 'min_element', 'sort',\n                     'transform',\n                    )),\n    ('<utility>', ('forward', 'make_pair', 'move', 'swap')),\n    )\n\n_RE_PATTERN_STRING = re.compile(r'\\bstring\\b')\n\n_re_pattern_headers_maybe_templates = []\nfor _header, _templates in _HEADERS_MAYBE_TEMPLATES:\n  for _template in _templates:\n    # Match max<type>(..., ...), max(..., ...), but not foo->max, foo.max or\n    # 'type::max()'.\n    _re_pattern_headers_maybe_templates.append(\n        (re.compile(r'[^>.]\\b' + _template + r'(<.*?>)?\\([^\\)]'),\n            _template,\n            _header))\n# Match set<type>, but not foo->set<type>, foo.set<type>\n_re_pattern_headers_maybe_templates.append(\n    (re.compile(r'[^>.]\\bset\\s*\\<'),\n        'set<>',\n        '<set>'))\n# Match 'map<type> var' and 'std::map<type>(...)', but not 'map<type>(...)''\n_re_pattern_headers_maybe_templates.append(\n    (re.compile(r'(std\\b::\\bmap\\s*\\<)|(^(std\\b::\\b)map\\b\\(\\s*\\<)'),\n        'map<>',\n        '<map>'))\n\n# Other scripts may reach in and modify this pattern.\n_re_pattern_templates = []\nfor _header, _templates in _HEADERS_CONTAINING_TEMPLATES:\n  for _template in _templates:\n    _re_pattern_templates.append(\n        (re.compile(r'(\\<|\\b)' + _template + r'\\s*\\<'),\n         _template + '<>',\n         _header))\n\n\ndef FilesBelongToSameModule(filename_cc, filename_h):\n  \"\"\"Check if these two filenames belong to the same module.\n\n  The concept of a 'module' here is a as follows:\n  foo.h, foo-inl.h, foo.cc, foo_test.cc and foo_unittest.cc belong to the\n  same 'module' if they are in the same directory.\n  some/path/public/xyzzy and some/path/internal/xyzzy are also considered\n  to belong to the same module here.\n\n  If the filename_cc contains a longer path than the filename_h, for example,\n  '/absolute/path/to/base/sysinfo.cc', and this file would include\n  'base/sysinfo.h', this function also produces the prefix needed to open the\n  header. This is used by the caller of this function to more robustly open the\n  header file. We don't have access to the real include paths in this context,\n  so we need this guesswork here.\n\n  Known bugs: tools/base/bar.cc and base/bar.h belong to the same module\n  according to this implementation. Because of this, this function gives\n  some false positives. This should be sufficiently rare in practice.\n\n  Args:\n    filename_cc: is the path for the source (e.g. .cc) file\n    filename_h: is the path for the header path\n\n  Returns:\n    Tuple with a bool and a string:\n    bool: True if filename_cc and filename_h belong to the same module.\n    string: the additional prefix needed to open the header file.\n  \"\"\"\n  fileinfo_cc = FileInfo(filename_cc)\n  if not fileinfo_cc.Extension().lstrip('.') in GetNonHeaderExtensions():\n    return (False, '')\n\n  fileinfo_h = FileInfo(filename_h)\n  if not IsHeaderExtension(fileinfo_h.Extension().lstrip('.')):\n    return (False, '')\n\n  filename_cc = filename_cc[:-(len(fileinfo_cc.Extension()))]\n  matched_test_suffix = Search(_TEST_FILE_SUFFIX, fileinfo_cc.BaseName())\n  if matched_test_suffix:\n    filename_cc = filename_cc[:-len(matched_test_suffix.group(1))]\n\n  filename_cc = filename_cc.replace('/public/', '/')\n  filename_cc = filename_cc.replace('/internal/', '/')\n\n  filename_h = filename_h[:-(len(fileinfo_h.Extension()))]\n  if filename_h.endswith('-inl'):\n    filename_h = filename_h[:-len('-inl')]\n  filename_h = filename_h.replace('/public/', '/')\n  filename_h = filename_h.replace('/internal/', '/')\n\n  files_belong_to_same_module = filename_cc.endswith(filename_h)\n  common_path = ''\n  if files_belong_to_same_module:\n    common_path = filename_cc[:-len(filename_h)]\n  return files_belong_to_same_module, common_path\n\n\ndef UpdateIncludeState(filename, include_dict, io=codecs):\n  \"\"\"Fill up the include_dict with new includes found from the file.\n\n  Args:\n    filename: the name of the header to read.\n    include_dict: a dictionary in which the headers are inserted.\n    io: The io factory to use to read the file. Provided for testability.\n\n  Returns:\n    True if a header was successfully added. False otherwise.\n  \"\"\"\n  headerfile = None\n  try:\n    with io.open(filename, 'r', 'utf8', 'replace') as headerfile:\n      linenum = 0\n      for line in headerfile:\n        linenum += 1\n        clean_line = CleanseComments(line)\n        match = _RE_PATTERN_INCLUDE.search(clean_line)\n        if match:\n          include = match.group(2)\n          include_dict.setdefault(include, linenum)\n    return True\n  except IOError:\n    return False\n\n\n\ndef CheckForIncludeWhatYouUse(filename, clean_lines, include_state, error,\n                              io=codecs):\n  \"\"\"Reports for missing stl includes.\n\n  This function will output warnings to make sure you are including the headers\n  necessary for the stl containers and functions that you use. We only give one\n  reason to include a header. For example, if you use both equal_to<> and\n  less<> in a .h file, only one (the latter in the file) of these will be\n  reported as a reason to include the <functional>.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    include_state: An _IncludeState instance.\n    error: The function to call with any errors found.\n    io: The IO factory to use to read the header file. Provided for unittest\n        injection.\n  \"\"\"\n  required = {}  # A map of header name to linenumber and the template entity.\n                 # Example of required: { '<functional>': (1219, 'less<>') }\n\n  for linenum in xrange(clean_lines.NumLines()):\n    line = clean_lines.elided[linenum]\n    if not line or line[0] == '#':\n      continue\n\n    # String is special -- it is a non-templatized type in STL.\n    matched = _RE_PATTERN_STRING.search(line)\n    if matched:\n      # Don't warn about strings in non-STL namespaces:\n      # (We check only the first match per line; good enough.)\n      prefix = line[:matched.start()]\n      if prefix.endswith('std::') or not prefix.endswith('::'):\n        required['<string>'] = (linenum, 'string')\n\n    for pattern, template, header in _re_pattern_headers_maybe_templates:\n      if pattern.search(line):\n        required[header] = (linenum, template)\n\n    # The following function is just a speed up, no semantics are changed.\n    if not '<' in line:  # Reduces the cpu time usage by skipping lines.\n      continue\n\n    for pattern, template, header in _re_pattern_templates:\n      matched = pattern.search(line)\n      if matched:\n        # Don't warn about IWYU in non-STL namespaces:\n        # (We check only the first match per line; good enough.)\n        prefix = line[:matched.start()]\n        if prefix.endswith('std::') or not prefix.endswith('::'):\n          required[header] = (linenum, template)\n\n  # The policy is that if you #include something in foo.h you don't need to\n  # include it again in foo.cc. Here, we will look at possible includes.\n  # Let's flatten the include_state include_list and copy it into a dictionary.\n  include_dict = dict([item for sublist in include_state.include_list\n                       for item in sublist])\n\n  # Did we find the header for this file (if any) and successfully load it?\n  header_found = False\n\n  # Use the absolute path so that matching works properly.\n  abs_filename = FileInfo(filename).FullName()\n\n  # For Emacs's flymake.\n  # If cpplint is invoked from Emacs's flymake, a temporary file is generated\n  # by flymake and that file name might end with '_flymake.cc'. In that case,\n  # restore original file name here so that the corresponding header file can be\n  # found.\n  # e.g. If the file name is 'foo_flymake.cc', we should search for 'foo.h'\n  # instead of 'foo_flymake.h'\n  abs_filename = re.sub(r'_flymake\\.cc$', '.cc', abs_filename)\n\n  # include_dict is modified during iteration, so we iterate over a copy of\n  # the keys.\n  header_keys = list(include_dict.keys())\n  for header in header_keys:\n    (same_module, common_path) = FilesBelongToSameModule(abs_filename, header)\n    fullpath = common_path + header\n    if same_module and UpdateIncludeState(fullpath, include_dict, io):\n      header_found = True\n\n  # If we can't find the header file for a .cc, assume it's because we don't\n  # know where to look. In that case we'll give up as we're not sure they\n  # didn't include it in the .h file.\n  # TODO(unknown): Do a better job of finding .h files so we are confident that\n  # not having the .h file means there isn't one.\n  if not header_found:\n    for extension in GetNonHeaderExtensions():\n      if filename.endswith('.' + extension):\n        return\n\n  # All the lines have been processed, report the errors found.\n  for required_header_unstripped in sorted(required, key=required.__getitem__):\n    template = required[required_header_unstripped][1]\n    if required_header_unstripped.strip('<>\"') not in include_dict:\n      error(filename, required[required_header_unstripped][0],\n            'build/include_what_you_use', 4,\n            'Add #include ' + required_header_unstripped + ' for ' + template)\n\n\n_RE_PATTERN_EXPLICIT_MAKEPAIR = re.compile(r'\\bmake_pair\\s*<')\n\n\ndef CheckMakePairUsesDeduction(filename, clean_lines, linenum, error):\n  \"\"\"Check that make_pair's template arguments are deduced.\n\n  G++ 4.6 in C++11 mode fails badly if make_pair's template arguments are\n  specified explicitly, and such use isn't intended in any case.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n  match = _RE_PATTERN_EXPLICIT_MAKEPAIR.search(line)\n  if match:\n    error(filename, linenum, 'build/explicit_make_pair',\n          4,  # 4 = high confidence\n          'For C++11-compatibility, omit template arguments from make_pair'\n          ' OR use pair directly OR if appropriate, construct a pair directly')\n\n\ndef CheckRedundantVirtual(filename, clean_lines, linenum, error):\n  \"\"\"Check if line contains a redundant \"virtual\" function-specifier.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  # Look for \"virtual\" on current line.\n  line = clean_lines.elided[linenum]\n  virtual = Match(r'^(.*)(\\bvirtual\\b)(.*)$', line)\n  if not virtual: return\n\n  # Ignore \"virtual\" keywords that are near access-specifiers.  These\n  # are only used in class base-specifier and do not apply to member\n  # functions.\n  if (Search(r'\\b(public|protected|private)\\s+$', virtual.group(1)) or\n      Match(r'^\\s+(public|protected|private)\\b', virtual.group(3))):\n    return\n\n  # Ignore the \"virtual\" keyword from virtual base classes.  Usually\n  # there is a column on the same line in these cases (virtual base\n  # classes are rare in google3 because multiple inheritance is rare).\n  if Match(r'^.*[^:]:[^:].*$', line): return\n\n  # Look for the next opening parenthesis.  This is the start of the\n  # parameter list (possibly on the next line shortly after virtual).\n  # TODO(unknown): doesn't work if there are virtual functions with\n  # decltype() or other things that use parentheses, but csearch suggests\n  # that this is rare.\n  end_col = -1\n  end_line = -1\n  start_col = len(virtual.group(2))\n  for start_line in xrange(linenum, min(linenum + 3, clean_lines.NumLines())):\n    line = clean_lines.elided[start_line][start_col:]\n    parameter_list = Match(r'^([^(]*)\\(', line)\n    if parameter_list:\n      # Match parentheses to find the end of the parameter list\n      (_, end_line, end_col) = CloseExpression(\n          clean_lines, start_line, start_col + len(parameter_list.group(1)))\n      break\n    start_col = 0\n\n  if end_col < 0:\n    return  # Couldn't find end of parameter list, give up\n\n  # Look for \"override\" or \"final\" after the parameter list\n  # (possibly on the next few lines).\n  for i in xrange(end_line, min(end_line + 3, clean_lines.NumLines())):\n    line = clean_lines.elided[i][end_col:]\n    match = Search(r'\\b(override|final)\\b', line)\n    if match:\n      error(filename, linenum, 'readability/inheritance', 4,\n            ('\"virtual\" is redundant since function is '\n             'already declared as \"%s\"' % match.group(1)))\n\n    # Set end_col to check whole lines after we are done with the\n    # first line.\n    end_col = 0\n    if Search(r'[^\\w]\\s*$', line):\n      break\n\n\ndef CheckRedundantOverrideOrFinal(filename, clean_lines, linenum, error):\n  \"\"\"Check if line contains a redundant \"override\" or \"final\" virt-specifier.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  # Look for closing parenthesis nearby.  We need one to confirm where\n  # the declarator ends and where the virt-specifier starts to avoid\n  # false positives.\n  line = clean_lines.elided[linenum]\n  declarator_end = line.rfind(')')\n  if declarator_end >= 0:\n    fragment = line[declarator_end:]\n  else:\n    if linenum > 1 and clean_lines.elided[linenum - 1].rfind(')') >= 0:\n      fragment = line\n    else:\n      return\n\n  # Check that at most one of \"override\" or \"final\" is present, not both\n  if Search(r'\\boverride\\b', fragment) and Search(r'\\bfinal\\b', fragment):\n    error(filename, linenum, 'readability/inheritance', 4,\n          ('\"override\" is redundant since function is '\n           'already declared as \"final\"'))\n\n\n\n\n# Returns true if we are at a new block, and it is directly\n# inside of a namespace.\ndef IsBlockInNameSpace(nesting_state, is_forward_declaration):\n  \"\"\"Checks that the new block is directly in a namespace.\n\n  Args:\n    nesting_state: The _NestingState object that contains info about our state.\n    is_forward_declaration: If the class is a forward declared class.\n  Returns:\n    Whether or not the new block is directly in a namespace.\n  \"\"\"\n  if is_forward_declaration:\n    return len(nesting_state.stack) >= 1 and (\n      isinstance(nesting_state.stack[-1], _NamespaceInfo))\n\n\n  return (len(nesting_state.stack) > 1 and\n          nesting_state.stack[-1].check_namespace_indentation and\n          isinstance(nesting_state.stack[-2], _NamespaceInfo))\n\n\ndef ShouldCheckNamespaceIndentation(nesting_state, is_namespace_indent_item,\n                                    raw_lines_no_comments, linenum):\n  \"\"\"This method determines if we should apply our namespace indentation check.\n\n  Args:\n    nesting_state: The current nesting state.\n    is_namespace_indent_item: If we just put a new class on the stack, True.\n      If the top of the stack is not a class, or we did not recently\n      add the class, False.\n    raw_lines_no_comments: The lines without the comments.\n    linenum: The current line number we are processing.\n\n  Returns:\n    True if we should apply our namespace indentation check. Currently, it\n    only works for classes and namespaces inside of a namespace.\n  \"\"\"\n\n  is_forward_declaration = IsForwardClassDeclaration(raw_lines_no_comments,\n                                                     linenum)\n\n  if not (is_namespace_indent_item or is_forward_declaration):\n    return False\n\n  # If we are in a macro, we do not want to check the namespace indentation.\n  if IsMacroDefinition(raw_lines_no_comments, linenum):\n    return False\n\n  return IsBlockInNameSpace(nesting_state, is_forward_declaration)\n\n\n# Call this method if the line is directly inside of a namespace.\n# If the line above is blank (excluding comments) or the start of\n# an inner namespace, it cannot be indented.\ndef CheckItemIndentationInNamespace(filename, raw_lines_no_comments, linenum,\n                                    error):\n  line = raw_lines_no_comments[linenum]\n  if Match(r'^\\s+', line):\n    error(filename, linenum, 'runtime/indentation_namespace', 4,\n          'Do not indent within a namespace')\n\n\ndef ProcessLine(filename, file_extension, clean_lines, line,\n                include_state, function_state, nesting_state, error,\n                extra_check_functions=None):\n  \"\"\"Processes a single line in the file.\n\n  Args:\n    filename: Filename of the file that is being processed.\n    file_extension: The extension (dot not included) of the file.\n    clean_lines: An array of strings, each representing a line of the file,\n                 with comments stripped.\n    line: Number of line being processed.\n    include_state: An _IncludeState instance in which the headers are inserted.\n    function_state: A _FunctionState instance which counts function lines, etc.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n    extra_check_functions: An array of additional check functions that will be\n                           run on each source line. Each function takes 4\n                           arguments: filename, clean_lines, line, error\n  \"\"\"\n  raw_lines = clean_lines.raw_lines\n  ParseNolintSuppressions(filename, raw_lines[line], line, error)\n  nesting_state.Update(filename, clean_lines, line, error)\n  CheckForNamespaceIndentation(filename, nesting_state, clean_lines, line,\n                               error)\n  if nesting_state.InAsmBlock(): return\n  CheckForFunctionLengths(filename, clean_lines, line, function_state, error)\n  CheckForMultilineCommentsAndStrings(filename, clean_lines, line, error)\n  CheckStyle(filename, clean_lines, line, file_extension, nesting_state, error)\n  CheckLanguage(filename, clean_lines, line, file_extension, include_state,\n                nesting_state, error)\n  CheckForNonConstReference(filename, clean_lines, line, nesting_state, error)\n  CheckForNonStandardConstructs(filename, clean_lines, line,\n                                nesting_state, error)\n  CheckVlogArguments(filename, clean_lines, line, error)\n  CheckPosixThreading(filename, clean_lines, line, error)\n  CheckInvalidIncrement(filename, clean_lines, line, error)\n  CheckMakePairUsesDeduction(filename, clean_lines, line, error)\n  CheckRedundantVirtual(filename, clean_lines, line, error)\n  CheckRedundantOverrideOrFinal(filename, clean_lines, line, error)\n  if extra_check_functions:\n    for check_fn in extra_check_functions:\n      check_fn(filename, clean_lines, line, error)\n\ndef FlagCxx11Features(filename, clean_lines, linenum, error):\n  \"\"\"Flag those c++11 features that we only allow in certain places.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  include = Match(r'\\s*#\\s*include\\s+[<\"]([^<\"]+)[\">]', line)\n\n  # Flag unapproved C++ TR1 headers.\n  if include and include.group(1).startswith('tr1/'):\n    error(filename, linenum, 'build/c++tr1', 5,\n          ('C++ TR1 headers such as <%s> are unapproved.') % include.group(1))\n\n  # Flag unapproved C++11 headers.\n  if include and include.group(1) in ('cfenv',\n                                      'condition_variable',\n                                      'fenv.h',\n                                      'future',\n                                      'mutex',\n                                      'thread',\n                                      'chrono',\n                                      'ratio',\n                                      'regex',\n                                      'system_error',\n                                     ):\n    error(filename, linenum, 'build/c++11', 5,\n          ('<%s> is an unapproved C++11 header.') % include.group(1))\n\n  # The only place where we need to worry about C++11 keywords and library\n  # features in preprocessor directives is in macro definitions.\n  if Match(r'\\s*#', line) and not Match(r'\\s*#\\s*define\\b', line): return\n\n  # These are classes and free functions.  The classes are always\n  # mentioned as std::*, but we only catch the free functions if\n  # they're not found by ADL.  They're alphabetical by header.\n  for top_name in (\n      # type_traits\n      'alignment_of',\n      'aligned_union',\n      ):\n    if Search(r'\\bstd::%s\\b' % top_name, line):\n      error(filename, linenum, 'build/c++11', 5,\n            ('std::%s is an unapproved C++11 class or function.  Send c-style '\n             'an example of where it would make your code more readable, and '\n             'they may let you use it.') % top_name)\n\n\ndef FlagCxx14Features(filename, clean_lines, linenum, error):\n  \"\"\"Flag those C++14 features that we restrict.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  include = Match(r'\\s*#\\s*include\\s+[<\"]([^<\"]+)[\">]', line)\n\n  # Flag unapproved C++14 headers.\n  if include and include.group(1) in ('scoped_allocator', 'shared_mutex'):\n    error(filename, linenum, 'build/c++14', 5,\n          ('<%s> is an unapproved C++14 header.') % include.group(1))\n\n\ndef ProcessFileData(filename, file_extension, lines, error,\n                    extra_check_functions=None):\n  \"\"\"Performs lint checks and reports any errors to the given error function.\n\n  Args:\n    filename: Filename of the file that is being processed.\n    file_extension: The extension (dot not included) of the file.\n    lines: An array of strings, each representing a line of the file, with the\n           last element being empty if the file is terminated with a newline.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n    extra_check_functions: An array of additional check functions that will be\n                           run on each source line. Each function takes 4\n                           arguments: filename, clean_lines, line, error\n  \"\"\"\n  lines = (['// marker so line numbers and indices both start at 1'] + lines +\n           ['// marker so line numbers end in a known way'])\n\n  include_state = _IncludeState()\n  function_state = _FunctionState()\n  nesting_state = NestingState()\n\n  ResetNolintSuppressions()\n\n  CheckForCopyright(filename, lines, error)\n  ProcessGlobalSuppressions(lines)\n  RemoveMultiLineComments(filename, lines, error)\n  clean_lines = CleansedLines(lines)\n\n  if IsHeaderExtension(file_extension):\n    CheckForHeaderGuard(filename, clean_lines, error)\n\n  for line in xrange(clean_lines.NumLines()):\n    ProcessLine(filename, file_extension, clean_lines, line,\n                include_state, function_state, nesting_state, error,\n                extra_check_functions)\n    FlagCxx11Features(filename, clean_lines, line, error)\n  nesting_state.CheckCompletedBlocks(filename, error)\n\n  CheckForIncludeWhatYouUse(filename, clean_lines, include_state, error)\n\n  # Check that the .cc file has included its header if it exists.\n  if _IsSourceExtension(file_extension):\n    CheckHeaderFileIncluded(filename, include_state, error)\n\n  # We check here rather than inside ProcessLine so that we see raw\n  # lines rather than \"cleaned\" lines.\n  CheckForBadCharacters(filename, lines, error)\n\n  CheckForNewlineAtEOF(filename, lines, error)\n\ndef ProcessConfigOverrides(filename):\n  \"\"\" Loads the configuration files and processes the config overrides.\n\n  Args:\n    filename: The name of the file being processed by the linter.\n\n  Returns:\n    False if the current |filename| should not be processed further.\n  \"\"\"\n\n  abs_filename = os.path.abspath(filename)\n  cfg_filters = []\n  keep_looking = True\n  while keep_looking:\n    abs_path, base_name = os.path.split(abs_filename)\n    if not base_name:\n      break  # Reached the root directory.\n\n    cfg_file = os.path.join(abs_path, \"CPPLINT.cfg\")\n    abs_filename = abs_path\n    if not os.path.isfile(cfg_file):\n      continue\n\n    try:\n      with codecs.open(cfg_file, 'r', 'utf8', 'replace') as file_handle:\n        for line in file_handle:\n          line, _, _ = line.partition('#')  # Remove comments.\n          if not line.strip():\n            continue\n\n          name, _, val = line.partition('=')\n          name = name.strip()\n          val = val.strip()\n          if name == 'set noparent':\n            keep_looking = False\n          elif name == 'filter':\n            cfg_filters.append(val)\n          elif name == 'exclude_files':\n            # When matching exclude_files pattern, use the base_name of\n            # the current file name or the directory name we are processing.\n            # For example, if we are checking for lint errors in /foo/bar/baz.cc\n            # and we found the .cfg file at /foo/CPPLINT.cfg, then the config\n            # file's \"exclude_files\" filter is meant to be checked against \"bar\"\n            # and not \"baz\" nor \"bar/baz.cc\".\n            if base_name:\n              pattern = re.compile(val)\n              if pattern.match(base_name):\n                if _cpplint_state.quiet:\n                  # Suppress \"Ignoring file\" warning when using --quiet.\n                  return False\n                _cpplint_state.PrintInfo('Ignoring \"%s\": file excluded by \"%s\". '\n                                 'File path component \"%s\" matches '\n                                 'pattern \"%s\"\\n' %\n                                 (filename, cfg_file, base_name, val))\n                return False\n          elif name == 'linelength':\n            global _line_length\n            try:\n              _line_length = int(val)\n            except ValueError:\n              _cpplint_state.PrintError('Line length must be numeric.')\n          elif name == 'extensions':\n            ProcessExtensionsOption(val)\n          elif name == 'root':\n            global _root\n            # root directories are specified relative to CPPLINT.cfg dir.\n            _root = os.path.join(os.path.dirname(cfg_file), val)\n          elif name == 'headers':\n            ProcessHppHeadersOption(val)\n          elif name == 'includeorder':\n            ProcessIncludeOrderOption(val)\n          else:\n            _cpplint_state.PrintError(\n                'Invalid configuration option (%s) in file %s\\n' %\n                (name, cfg_file))\n\n    except IOError:\n      _cpplint_state.PrintError(\n          \"Skipping config file '%s': Can't open for reading\\n\" % cfg_file)\n      keep_looking = False\n\n  # Apply all the accumulated filters in reverse order (top-level directory\n  # config options having the least priority).\n  for cfg_filter in reversed(cfg_filters):\n    _AddFilters(cfg_filter)\n\n  return True\n\n\ndef ProcessFile(filename, vlevel, extra_check_functions=None):\n  \"\"\"Does google-lint on a single file.\n\n  Args:\n    filename: The name of the file to parse.\n\n    vlevel: The level of errors to report.  Every error of confidence\n    >= verbose_level will be reported.  0 is a good default.\n\n    extra_check_functions: An array of additional check functions that will be\n                           run on each source line. Each function takes 4\n                           arguments: filename, clean_lines, line, error\n  \"\"\"\n\n  _SetVerboseLevel(vlevel)\n  _BackupFilters()\n  old_errors = _cpplint_state.error_count\n\n  if not ProcessConfigOverrides(filename):\n    _RestoreFilters()\n    return\n\n  lf_lines = []\n  crlf_lines = []\n  try:\n    # Support the UNIX convention of using \"-\" for stdin.  Note that\n    # we are not opening the file with universal newline support\n    # (which codecs doesn't support anyway), so the resulting lines do\n    # contain trailing '\\r' characters if we are reading a file that\n    # has CRLF endings.\n    # If after the split a trailing '\\r' is present, it is removed\n    # below.\n    if filename == '-':\n      lines = codecs.StreamReaderWriter(sys.stdin,\n                                        codecs.getreader('utf8'),\n                                        codecs.getwriter('utf8'),\n                                        'replace').read().split('\\n')\n    else:\n      with codecs.open(filename, 'r', 'utf8', 'replace') as target_file:\n        lines = target_file.read().split('\\n')\n\n    # Remove trailing '\\r'.\n    # The -1 accounts for the extra trailing blank line we get from split()\n    for linenum in range(len(lines) - 1):\n      if lines[linenum].endswith('\\r'):\n        lines[linenum] = lines[linenum].rstrip('\\r')\n        crlf_lines.append(linenum + 1)\n      else:\n        lf_lines.append(linenum + 1)\n\n  except IOError:\n    _cpplint_state.PrintError(\n        \"Skipping input '%s': Can't open for reading\\n\" % filename)\n    _RestoreFilters()\n    return\n\n  # Note, if no dot is found, this will give the entire filename as the ext.\n  file_extension = filename[filename.rfind('.') + 1:]\n\n  # When reading from stdin, the extension is unknown, so no cpplint tests\n  # should rely on the extension.\n  if filename != '-' and file_extension not in GetAllExtensions():\n    _cpplint_state.PrintError('Ignoring %s; not a valid file name '\n                     '(%s)\\n' % (filename, ', '.join(GetAllExtensions())))\n  else:\n    ProcessFileData(filename, file_extension, lines, Error,\n                    extra_check_functions)\n\n    # If end-of-line sequences are a mix of LF and CR-LF, issue\n    # warnings on the lines with CR.\n    #\n    # Don't issue any warnings if all lines are uniformly LF or CR-LF,\n    # since critique can handle these just fine, and the style guide\n    # doesn't dictate a particular end of line sequence.\n    #\n    # We can't depend on os.linesep to determine what the desired\n    # end-of-line sequence should be, since that will return the\n    # server-side end-of-line sequence.\n    if lf_lines and crlf_lines:\n      # Warn on every line with CR.  An alternative approach might be to\n      # check whether the file is mostly CRLF or just LF, and warn on the\n      # minority, we bias toward LF here since most tools prefer LF.\n      for linenum in crlf_lines:\n        Error(filename, linenum, 'whitespace/newline', 1,\n              'Unexpected \\\\r (^M) found; better to use only \\\\n')\n\n  # Suppress printing anything if --quiet was passed unless the error\n  # count has increased after processing this file.\n  if not _cpplint_state.quiet or old_errors != _cpplint_state.error_count:\n    _cpplint_state.PrintInfo('Done processing %s\\n' % filename)\n  _RestoreFilters()\n\n\ndef PrintUsage(message):\n  \"\"\"Prints a brief usage string and exits, optionally with an error message.\n\n  Args:\n    message: The optional error message.\n  \"\"\"\n  sys.stderr.write(_USAGE  % (sorted(list(GetAllExtensions())),\n       ','.join(sorted(list(GetAllExtensions()))),\n       sorted(GetHeaderExtensions()),\n       ','.join(sorted(GetHeaderExtensions()))))\n\n  if message:\n    sys.exit('\\nFATAL ERROR: ' + message)\n  else:\n    sys.exit(0)\n\ndef PrintVersion():\n  sys.stdout.write('Cpplint fork (https://github.com/cpplint/cpplint)\\n')\n  sys.stdout.write('cpplint ' + __VERSION__ + '\\n')\n  sys.stdout.write('Python ' + sys.version + '\\n')\n  sys.exit(0)\n\ndef PrintCategories():\n  \"\"\"Prints a list of all the error-categories used by error messages.\n\n  These are the categories used to filter messages via --filter.\n  \"\"\"\n  sys.stderr.write(''.join('  %s\\n' % cat for cat in _ERROR_CATEGORIES))\n  sys.exit(0)\n\n\ndef ParseArguments(args):\n  \"\"\"Parses the command line arguments.\n\n  This may set the output format and verbosity level as side-effects.\n\n  Args:\n    args: The command line arguments:\n\n  Returns:\n    The list of filenames to lint.\n  \"\"\"\n  try:\n    (opts, filenames) = getopt.getopt(args, '', ['help', 'output=', 'verbose=',\n                                                 'v=',\n                                                 'version',\n                                                 'counting=',\n                                                 'filter=',\n                                                 'root=',\n                                                 'repository=',\n                                                 'linelength=',\n                                                 'extensions=',\n                                                 'exclude=',\n                                                 'recursive',\n                                                 'headers=',\n                                                 'includeorder=',\n                                                 'quiet'])\n  except getopt.GetoptError:\n    PrintUsage('Invalid arguments.')\n\n  verbosity = _VerboseLevel()\n  output_format = _OutputFormat()\n  filters = ''\n  quiet = _Quiet()\n  counting_style = ''\n  recursive = False\n\n  for (opt, val) in opts:\n    if opt == '--help':\n      PrintUsage(None)\n    if opt == '--version':\n      PrintVersion()\n    elif opt == '--output':\n      if val not in ('emacs', 'vs7', 'eclipse', 'junit', 'sed', 'gsed'):\n        PrintUsage('The only allowed output formats are emacs, vs7, eclipse '\n                   'sed, gsed and junit.')\n      output_format = val\n    elif opt == '--quiet':\n      quiet = True\n    elif opt == '--verbose' or opt == '--v':\n      verbosity = int(val)\n    elif opt == '--filter':\n      filters = val\n      if not filters:\n        PrintCategories()\n    elif opt == '--counting':\n      if val not in ('total', 'toplevel', 'detailed'):\n        PrintUsage('Valid counting options are total, toplevel, and detailed')\n      counting_style = val\n    elif opt == '--root':\n      global _root\n      _root = val\n    elif opt == '--repository':\n      global _repository\n      _repository = val\n    elif opt == '--linelength':\n      global _line_length\n      try:\n        _line_length = int(val)\n      except ValueError:\n        PrintUsage('Line length must be digits.')\n    elif opt == '--exclude':\n      global _excludes\n      if not _excludes:\n        _excludes = set()\n      _excludes.update(glob.glob(val))\n    elif opt == '--extensions':\n      ProcessExtensionsOption(val)\n    elif opt == '--headers':\n      ProcessHppHeadersOption(val)\n    elif opt == '--recursive':\n      recursive = True\n    elif opt == '--includeorder':\n      ProcessIncludeOrderOption(val)\n\n  if not filenames:\n    PrintUsage('No files were specified.')\n\n  if recursive:\n    filenames = _ExpandDirectories(filenames)\n\n  if _excludes:\n    filenames = _FilterExcludedFiles(filenames)\n\n  _SetOutputFormat(output_format)\n  _SetQuiet(quiet)\n  _SetVerboseLevel(verbosity)\n  _SetFilters(filters)\n  _SetCountingStyle(counting_style)\n\n  filenames.sort()\n  return filenames\n\ndef _ExpandDirectories(filenames):\n  \"\"\"Searches a list of filenames and replaces directories in the list with\n  all files descending from those directories. Files with extensions not in\n  the valid extensions list are excluded.\n\n  Args:\n    filenames: A list of files or directories\n\n  Returns:\n    A list of all files that are members of filenames or descended from a\n    directory in filenames\n  \"\"\"\n  expanded = set()\n  for filename in filenames:\n    if not os.path.isdir(filename):\n      expanded.add(filename)\n      continue\n\n    for root, _, files in os.walk(filename):\n      for loopfile in files:\n        fullname = os.path.join(root, loopfile)\n        if fullname.startswith('.' + os.path.sep):\n          fullname = fullname[len('.' + os.path.sep):]\n        expanded.add(fullname)\n\n  filtered = []\n  for filename in expanded:\n    if os.path.splitext(filename)[1][1:] in GetAllExtensions():\n      filtered.append(filename)\n  return filtered\n\ndef _FilterExcludedFiles(fnames):\n  \"\"\"Filters out files listed in the --exclude command line switch. File paths\n  in the switch are evaluated relative to the current working directory\n  \"\"\"\n  exclude_paths = [os.path.abspath(f) for f in _excludes]\n  # because globbing does not work recursively, exclude all subpath of all excluded entries\n  return [f for f in fnames\n          if not any(e for e in exclude_paths\n                  if _IsParentOrSame(e, os.path.abspath(f)))]\n\ndef _IsParentOrSame(parent, child):\n  \"\"\"Return true if child is subdirectory of parent.\n  Assumes both paths are absolute and don't contain symlinks.\n  \"\"\"\n  parent = os.path.normpath(parent)\n  child = os.path.normpath(child)\n  if parent == child:\n    return True\n\n  prefix = os.path.commonprefix([parent, child])\n  if prefix != parent:\n    return False\n  # Note: os.path.commonprefix operates on character basis, so\n  # take extra care of situations like '/foo/ba' and '/foo/bar/baz'\n  child_suffix = child[len(prefix):]\n  child_suffix = child_suffix.lstrip(os.sep)\n  return child == os.path.join(prefix, child_suffix)\n\ndef main():\n  filenames = ParseArguments(sys.argv[1:])\n  backup_err = sys.stderr\n  try:\n    # Change stderr to write with replacement characters so we don't die\n    # if we try to print something containing non-ASCII characters.\n    sys.stderr = codecs.StreamReader(sys.stderr, 'replace')\n\n    _cpplint_state.ResetErrorCounts()\n    for filename in filenames:\n      ProcessFile(filename, _cpplint_state.verbose_level)\n    # If --quiet is passed, suppress printing error count unless there are errors.\n    if not _cpplint_state.quiet or _cpplint_state.error_count > 0:\n      _cpplint_state.PrintErrorCounts()\n\n    if _cpplint_state.output_format == 'junit':\n      sys.stderr.write(_cpplint_state.FormatJUnitXML())\n\n  finally:\n    sys.stderr = backup_err\n\n  sys.exit(_cpplint_state.error_count > 0)\n\n\nif __name__ == '__main__':\n  main()\n", "cpp/build-support/lintutils.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport multiprocessing as mp\nimport os\nfrom fnmatch import fnmatch\nfrom subprocess import Popen\n\n\ndef chunk(seq, n):\n    \"\"\"\n    divide a sequence into equal sized chunks\n    (the last chunk may be smaller, but won't be empty)\n    \"\"\"\n    chunks = []\n    some = []\n    for element in seq:\n        if len(some) == n:\n            chunks.append(some)\n            some = []\n        some.append(element)\n    if len(some) > 0:\n        chunks.append(some)\n    return chunks\n\n\ndef dechunk(chunks):\n    \"flatten chunks into a single list\"\n    seq = []\n    for chunk in chunks:\n        seq.extend(chunk)\n    return seq\n\n\ndef run_parallel(cmds, **kwargs):\n    \"\"\"\n    Run each of cmds (with shared **kwargs) using subprocess.Popen\n    then wait for all of them to complete.\n    Runs batches of multiprocessing.cpu_count() * 2 from cmds\n    returns a list of tuples containing each process'\n    returncode, stdout, stderr\n    \"\"\"\n    complete = []\n    for cmds_batch in chunk(cmds, mp.cpu_count() * 2):\n        procs_batch = [Popen(cmd, **kwargs) for cmd in cmds_batch]\n        for proc in procs_batch:\n            stdout, stderr = proc.communicate()\n            complete.append((proc.returncode, stdout, stderr))\n    return complete\n\n\n_source_extensions = '''\n.h\n.cc\n.cpp\n'''.split()\n\n\ndef get_sources(source_dir, exclude_globs=[]):\n    sources = []\n    for directory, subdirs, basenames in os.walk(source_dir):\n        for path in [os.path.join(directory, basename)\n                     for basename in basenames]:\n            # filter out non-source files\n            if os.path.splitext(path)[1] not in _source_extensions:\n                continue\n\n            path = os.path.abspath(path)\n\n            # filter out files that match the globs in the globs file\n            if any([fnmatch(path, glob) for glob in exclude_globs]):\n                continue\n\n            sources.append(path)\n    return sources\n\n\ndef stdout_pathcolonline(completed_process, filenames):\n    \"\"\"\n    given a completed process which may have reported some files as problematic\n    by printing the path name followed by ':' then a line number, examine\n    stdout and return the set of actually reported file names\n    \"\"\"\n    returncode, stdout, stderr = completed_process\n    bfilenames = set()\n    for filename in filenames:\n        bfilenames.add(filename.encode('utf-8') + b':')\n    problem_files = set()\n    for line in stdout.splitlines():\n        for filename in bfilenames:\n            if line.startswith(filename):\n                problem_files.add(filename.decode('utf-8'))\n                bfilenames.remove(filename)\n                break\n    return problem_files, stdout\n", "cpp/build-support/lint_cpp_cli.py": "#!/usr/bin/env python3\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport argparse\nimport re\nimport os\n\nparser = argparse.ArgumentParser(\n    description=\"Check for illegal headers for C++/CLI applications\")\nparser.add_argument(\"source_path\",\n                    help=\"Path to source code\")\narguments = parser.parse_args()\n\n\n_STRIP_COMMENT_REGEX = re.compile('(.+)?(?=//)')\n_NULLPTR_REGEX = re.compile(r'.*\\bnullptr\\b.*')\n_RETURN_NOT_OK_REGEX = re.compile(r'.*\\sRETURN_NOT_OK.*')\n_ASSIGN_OR_RAISE_REGEX = re.compile(r'.*\\sASSIGN_OR_RAISE.*')\n\n\ndef _paths(paths):\n    return [p.strip().replace('/', os.path.sep) for p in paths.splitlines()]\n\n\ndef _strip_comments(line):\n    m = _STRIP_COMMENT_REGEX.match(line)\n    if not m:\n        return line\n    else:\n        return m.group(0)\n\n\ndef lint_file(path):\n    fail_rules = [\n        # rule, error message, rule-specific exclusions list\n        (lambda x: '<mutex>' in x, 'Uses <mutex>', []),\n        (lambda x: '<iostream>' in x, 'Uses <iostream>', []),\n        (lambda x: re.match(_NULLPTR_REGEX, x), 'Uses nullptr', []),\n        (lambda x: re.match(_RETURN_NOT_OK_REGEX, x),\n         'Use ARROW_RETURN_NOT_OK in header files', _paths('''\\\n         arrow/status.h\n         test\n         arrow/util/hash.h\n         arrow/python/util''')),\n        (lambda x: re.match(_ASSIGN_OR_RAISE_REGEX, x),\n         'Use ARROW_ASSIGN_OR_RAISE in header files', _paths('''\\\n         arrow/result_internal.h\n         test\n         '''))\n\n    ]\n\n    with open(path) as f:\n        for i, line in enumerate(f):\n            stripped_line = _strip_comments(line)\n            for rule, why, rule_exclusions in fail_rules:\n                if any([True for excl in rule_exclusions if excl in path]):\n                    continue\n\n                if rule(stripped_line):\n                    yield path, why, i, line\n\n\nEXCLUSIONS = _paths('''\\\n    arrow/arrow-config.cmake\n    arrow/python/iterators.h\n    arrow/util/hashing.h\n    arrow/util/macros.h\n    arrow/util/parallel.h\n    arrow/vendored\n    arrow/visitor_inline.h\n    gandiva/cache.h\n    gandiva/jni\n    jni/\n    test\n    internal\n    _generated''')\n\n\ndef lint_files():\n    for dirpath, _, filenames in os.walk(arguments.source_path):\n        for filename in filenames:\n            full_path = os.path.join(dirpath, filename)\n\n            exclude = False\n            for exclusion in EXCLUSIONS:\n                if exclusion in full_path:\n                    exclude = True\n                    break\n\n            if exclude:\n                continue\n\n            # Lint file name, except for pkg-config templates\n            if not filename.endswith('.pc.in'):\n                if '-' in filename:\n                    why = (\"Please use underscores, not hyphens, \"\n                           \"in source file names\")\n                    yield full_path, why, 0, full_path\n\n            # Only run on header files\n            if filename.endswith('.h'):\n                for _ in lint_file(full_path):\n                    yield _\n\n\nif __name__ == '__main__':\n    failures = list(lint_files())\n    for path, why, i, line in failures:\n        print('File {0} failed C++/CLI lint check: {1}\\n'\n              'Line {2}: {3}'.format(path, why, i + 1, line))\n    if failures:\n        exit(1)\n", "cpp/build-support/asan_symbolize.py": "#!/usr/bin/env python3\n#===- lib/asan/scripts/asan_symbolize.py -----------------------------------===#\n#\n#                     The LLVM Compiler Infrastructure\n#\n# This file is distributed under the University of Illinois Open Source\n# License. See LICENSE.TXT for details.\n#\n#===------------------------------------------------------------------------===#\nimport bisect\nimport os\nimport re\nimport subprocess\nimport sys\n\nllvm_symbolizer = None\nsymbolizers = {}\nfiletypes = {}\nvmaddrs = {}\nDEBUG = False\n\n\n# FIXME: merge the code that calls fix_filename().\ndef fix_filename(file_name):\n  for path_to_cut in sys.argv[1:]:\n    file_name = re.sub('.*' + path_to_cut, '', file_name)\n  file_name = re.sub('.*asan_[a-z_]*.cc:[0-9]*', '_asan_rtl_', file_name)\n  file_name = re.sub('.*crtstuff.c:0', '???:0', file_name)\n  return file_name\n\n\nclass Symbolizer(object):\n  def __init__(self):\n    pass\n\n  def symbolize(self, addr, binary, offset):\n    \"\"\"Symbolize the given address (pair of binary and offset).\n\n    Overridden in subclasses.\n    Args:\n        addr: virtual address of an instruction.\n        binary: path to executable/shared object containing this instruction.\n        offset: instruction offset in the @binary.\n    Returns:\n        list of strings (one string for each inlined frame) describing\n        the code locations for this instruction (that is, function name, file\n        name, line and column numbers).\n    \"\"\"\n    return None\n\n\nclass LLVMSymbolizer(Symbolizer):\n  def __init__(self, symbolizer_path):\n    super(LLVMSymbolizer, self).__init__()\n    self.symbolizer_path = symbolizer_path\n    self.pipe = self.open_llvm_symbolizer()\n\n  def open_llvm_symbolizer(self):\n    if not os.path.exists(self.symbolizer_path):\n      return None\n    cmd = [self.symbolizer_path,\n           '--use-symbol-table=true',\n           '--demangle=false',\n           '--functions=true',\n           '--inlining=true']\n    if DEBUG:\n      print(' '.join(cmd))\n    return subprocess.Popen(cmd, stdin=subprocess.PIPE,\n                            stdout=subprocess.PIPE)\n\n  def symbolize(self, addr, binary, offset):\n    \"\"\"Overrides Symbolizer.symbolize.\"\"\"\n    if not self.pipe:\n      return None\n    result = []\n    try:\n      symbolizer_input = '%s %s' % (binary, offset)\n      if DEBUG:\n        print(symbolizer_input)\n      self.pipe.stdin.write(symbolizer_input)\n      self.pipe.stdin.write('\\n')\n      while True:\n        function_name = self.pipe.stdout.readline().rstrip()\n        if not function_name:\n          break\n        file_name = self.pipe.stdout.readline().rstrip()\n        file_name = fix_filename(file_name)\n        if (not function_name.startswith('??') and\n            not file_name.startswith('??')):\n          # Append only valid frames.\n          result.append('%s in %s %s' % (addr, function_name,\n                                         file_name))\n    except Exception:\n      result = []\n    if not result:\n      result = None\n    return result\n\n\ndef LLVMSymbolizerFactory(system):\n  symbolizer_path = os.getenv('LLVM_SYMBOLIZER_PATH')\n  if not symbolizer_path:\n    # Assume llvm-symbolizer is in PATH.\n    symbolizer_path = 'llvm-symbolizer'\n  return LLVMSymbolizer(symbolizer_path)\n\n\nclass Addr2LineSymbolizer(Symbolizer):\n  def __init__(self, binary):\n    super(Addr2LineSymbolizer, self).__init__()\n    self.binary = binary\n    self.pipe = self.open_addr2line()\n\n  def open_addr2line(self):\n    cmd = ['addr2line', '-f', '-e', self.binary]\n    if DEBUG:\n      print(' '.join(cmd))\n    return subprocess.Popen(cmd,\n                            stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n\n  def symbolize(self, addr, binary, offset):\n    \"\"\"Overrides Symbolizer.symbolize.\"\"\"\n    if self.binary != binary:\n      return None\n    try:\n      self.pipe.stdin.write(offset)\n      self.pipe.stdin.write('\\n')\n      function_name = self.pipe.stdout.readline().rstrip()\n      file_name = self.pipe.stdout.readline().rstrip()\n    except Exception:\n      function_name = ''\n      file_name = ''\n    file_name = fix_filename(file_name)\n    return ['%s in %s %s' % (addr, function_name, file_name)]\n\n\nclass DarwinSymbolizer(Symbolizer):\n  def __init__(self, addr, binary):\n    super(DarwinSymbolizer, self).__init__()\n    self.binary = binary\n    # Guess which arch we're running. 10 = len('0x') + 8 hex digits.\n    if len(addr) > 10:\n      self.arch = 'x86_64'\n    else:\n      self.arch = 'i386'\n    self.vmaddr = None\n    self.pipe = None\n\n  def write_addr_to_pipe(self, offset):\n    self.pipe.stdin.write('0x%x' % int(offset, 16))\n    self.pipe.stdin.write('\\n')\n\n  def open_atos(self):\n    if DEBUG:\n      print('atos -o %s -arch %s' % (self.binary, self.arch))\n    cmdline = ['atos', '-o', self.binary, '-arch', self.arch]\n    self.pipe = subprocess.Popen(cmdline,\n                                 stdin=subprocess.PIPE,\n                                 stdout=subprocess.PIPE,\n                                 stderr=subprocess.PIPE)\n\n  def symbolize(self, addr, binary, offset):\n    \"\"\"Overrides Symbolizer.symbolize.\"\"\"\n    if self.binary != binary:\n      return None\n    self.open_atos()\n    self.write_addr_to_pipe(offset)\n    self.pipe.stdin.close()\n    atos_line = self.pipe.stdout.readline().rstrip()\n    # A well-formed atos response looks like this:\n    #   foo(type1, type2) (in object.name) (filename.cc:80)\n    match = re.match(r'^(.*) \\(in (.*)\\) \\((.*:\\d*)\\)$', atos_line)\n    if DEBUG:\n      print('atos_line: {0}'.format(atos_line))\n    if match:\n      function_name = match.group(1)\n      function_name = re.sub(r'\\(.*?\\)', '', function_name)\n      file_name = fix_filename(match.group(3))\n      return ['%s in %s %s' % (addr, function_name, file_name)]\n    else:\n      return ['%s in %s' % (addr, atos_line)]\n\n\n# Chain several symbolizers so that if one symbolizer fails, we fall back\n# to the next symbolizer in chain.\nclass ChainSymbolizer(Symbolizer):\n  def __init__(self, symbolizer_list):\n    super(ChainSymbolizer, self).__init__()\n    self.symbolizer_list = symbolizer_list\n\n  def symbolize(self, addr, binary, offset):\n    \"\"\"Overrides Symbolizer.symbolize.\"\"\"\n    for symbolizer in self.symbolizer_list:\n      if symbolizer:\n        result = symbolizer.symbolize(addr, binary, offset)\n        if result:\n          return result\n    return None\n\n  def append_symbolizer(self, symbolizer):\n    self.symbolizer_list.append(symbolizer)\n\n\ndef BreakpadSymbolizerFactory(binary):\n  suffix = os.getenv('BREAKPAD_SUFFIX')\n  if suffix:\n    filename = binary + suffix\n    if os.access(filename, os.F_OK):\n      return BreakpadSymbolizer(filename)\n  return None\n\n\ndef SystemSymbolizerFactory(system, addr, binary):\n  if system == 'Darwin':\n    return DarwinSymbolizer(addr, binary)\n  elif system == 'Linux':\n    return Addr2LineSymbolizer(binary)\n\n\nclass BreakpadSymbolizer(Symbolizer):\n  def __init__(self, filename):\n    super(BreakpadSymbolizer, self).__init__()\n    self.filename = filename\n    lines = file(filename).readlines()\n    self.files = []\n    self.symbols = {}\n    self.address_list = []\n    self.addresses = {}\n    # MODULE mac x86_64 A7001116478B33F18FF9BEDE9F615F190 t\n    fragments = lines[0].rstrip().split()\n    self.arch = fragments[2]\n    self.debug_id = fragments[3]\n    self.binary = ' '.join(fragments[4:])\n    self.parse_lines(lines[1:])\n\n  def parse_lines(self, lines):\n    cur_function_addr = ''\n    for line in lines:\n      fragments = line.split()\n      if fragments[0] == 'FILE':\n        assert int(fragments[1]) == len(self.files)\n        self.files.append(' '.join(fragments[2:]))\n      elif fragments[0] == 'PUBLIC':\n        self.symbols[int(fragments[1], 16)] = ' '.join(fragments[3:])\n      elif fragments[0] in ['CFI', 'STACK']:\n        pass\n      elif fragments[0] == 'FUNC':\n        cur_function_addr = int(fragments[1], 16)\n        if not cur_function_addr in self.symbols.keys():\n          self.symbols[cur_function_addr] = ' '.join(fragments[4:])\n      else:\n        # Line starting with an address.\n        addr = int(fragments[0], 16)\n        self.address_list.append(addr)\n        # Tuple of symbol address, size, line, file number.\n        self.addresses[addr] = (cur_function_addr,\n                                int(fragments[1], 16),\n                                int(fragments[2]),\n                                int(fragments[3]))\n    self.address_list.sort()\n\n  def get_sym_file_line(self, addr):\n    key = None\n    if addr in self.addresses.keys():\n      key = addr\n    else:\n      index = bisect.bisect_left(self.address_list, addr)\n      if index == 0:\n        return None\n      else:\n        key = self.address_list[index - 1]\n    sym_id, size, line_no, file_no = self.addresses[key]\n    symbol = self.symbols[sym_id]\n    filename = self.files[file_no]\n    if addr < key + size:\n      return symbol, filename, line_no\n    else:\n      return None\n\n  def symbolize(self, addr, binary, offset):\n    if self.binary != binary:\n      return None\n    res = self.get_sym_file_line(int(offset, 16))\n    if res:\n      function_name, file_name, line_no = res\n      result = ['%s in %s %s:%d' % (\n          addr, function_name, file_name, line_no)]\n      print(result)\n      return result\n    else:\n      return None\n\n\nclass SymbolizationLoop(object):\n  def __init__(self, binary_name_filter=None):\n    # Used by clients who may want to supply a different binary name.\n    # E.g. in Chrome several binaries may share a single .dSYM.\n    self.binary_name_filter = binary_name_filter\n    self.system = os.uname()[0]\n    if self.system in ['Linux', 'Darwin']:\n      self.llvm_symbolizer = LLVMSymbolizerFactory(self.system)\n    else:\n      raise Exception('Unknown system')\n\n  def symbolize_address(self, addr, binary, offset):\n    # Use the chain of symbolizers:\n    # Breakpad symbolizer -> LLVM symbolizer -> addr2line/atos\n    # (fall back to next symbolizer if the previous one fails).\n    if not binary in symbolizers:\n      symbolizers[binary] = ChainSymbolizer(\n          [BreakpadSymbolizerFactory(binary), self.llvm_symbolizer])\n    result = symbolizers[binary].symbolize(addr, binary, offset)\n    if result is None:\n      # Initialize system symbolizer only if other symbolizers failed.\n      symbolizers[binary].append_symbolizer(\n          SystemSymbolizerFactory(self.system, addr, binary))\n      result = symbolizers[binary].symbolize(addr, binary, offset)\n    # The system symbolizer must produce some result.\n    assert result\n    return result\n\n  def print_symbolized_lines(self, symbolized_lines):\n    if not symbolized_lines:\n      print(self.current_line)\n    else:\n      for symbolized_frame in symbolized_lines:\n        print('    #' + str(self.frame_no) + ' ' + symbolized_frame.rstrip())\n        self.frame_no += 1\n\n  def process_stdin(self):\n    self.frame_no = 0\n\n    if sys.version_info[0] == 2:\n      sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n    else:\n      # Unbuffered output is not supported in Python 3\n      sys.stdout = os.fdopen(sys.stdout.fileno(), 'w')\n\n    while True:\n      line = sys.stdin.readline()\n      if not line: break\n      self.current_line = line.rstrip()\n      #0 0x7f6e35cf2e45  (/blah/foo.so+0x11fe45)\n      stack_trace_line_format = (\n          r'^( *#([0-9]+) *)(0x[0-9a-f]+) *\\((.*)\\+(0x[0-9a-f]+)\\)')\n      match = re.match(stack_trace_line_format, line)\n      if not match:\n        print(self.current_line)\n        continue\n      if DEBUG:\n        print(line)\n      _, frameno_str, addr, binary, offset = match.groups()\n      if frameno_str == '0':\n        # Assume that frame #0 is the first frame of new stack trace.\n        self.frame_no = 0\n      original_binary = binary\n      if self.binary_name_filter:\n        binary = self.binary_name_filter(binary)\n      symbolized_line = self.symbolize_address(addr, binary, offset)\n      if not symbolized_line:\n        if original_binary != binary:\n          symbolized_line = self.symbolize_address(addr, binary, offset)\n      self.print_symbolized_lines(symbolized_line)\n\n\nif __name__ == '__main__':\n  loop = SymbolizationLoop()\n  loop.process_stdin()\n", "cpp/build-support/run_clang_tidy.py": "#!/usr/bin/env python3\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import print_function\nimport argparse\nimport multiprocessing as mp\nimport lintutils\nfrom subprocess import PIPE\nimport sys\nfrom functools import partial\n\n\ndef _get_chunk_key(filenames):\n    # lists are not hashable so key on the first filename in a chunk\n    return filenames[0]\n\n\n# clang-tidy outputs complaints in '/path:line_number: complaint' format,\n# so we can scan its output to get a list of files to fix\ndef _check_some_files(completed_processes, filenames):\n    result = completed_processes[_get_chunk_key(filenames)]\n    return lintutils.stdout_pathcolonline(result, filenames)\n\n\ndef _check_all(cmd, filenames):\n    # each clang-tidy instance will process 16 files\n    chunks = lintutils.chunk(filenames, 16)\n    cmds = [cmd + some for some in chunks]\n    results = lintutils.run_parallel(cmds, stderr=PIPE, stdout=PIPE)\n    error = False\n    # record completed processes (keyed by the first filename in the input\n    # chunk) for lookup in _check_some_files\n    completed_processes = {\n        _get_chunk_key(some): result\n        for some, result in zip(chunks, results)\n    }\n    checker = partial(_check_some_files, completed_processes)\n    pool = mp.Pool()\n    try:\n        # check output of completed clang-tidy invocations in parallel\n        for problem_files, stdout in pool.imap(checker, chunks):\n            if problem_files:\n                msg = \"clang-tidy suggested fixes for {}\"\n                print(\"\\n\".join(map(msg.format, problem_files)))\n                error = True\n    except Exception:\n        error = True\n        raise\n    finally:\n        pool.terminate()\n        pool.join()\n\n    if error:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Runs clang-tidy on all \")\n    parser.add_argument(\"--clang_tidy_binary\",\n                        required=True,\n                        help=\"Path to the clang-tidy binary\")\n    parser.add_argument(\"--exclude_globs\",\n                        help=\"Filename containing globs for files \"\n                        \"that should be excluded from the checks\")\n    parser.add_argument(\"--compile_commands\",\n                        required=True,\n                        help=\"compile_commands.json to pass clang-tidy\")\n    parser.add_argument(\"--source_dir\",\n                        required=True,\n                        action=\"append\",\n                        help=\"Root directory of the source code\")\n    parser.add_argument(\"--fix\", default=False,\n                        action=\"store_true\",\n                        help=\"If specified, will attempt to fix the \"\n                        \"source code instead of recommending fixes, \"\n                        \"defaults to %(default)s\")\n    parser.add_argument(\"--quiet\", default=False,\n                        action=\"store_true\",\n                        help=\"If specified, only print errors\")\n    arguments = parser.parse_args()\n\n    exclude_globs = []\n    if arguments.exclude_globs:\n        for line in open(arguments.exclude_globs):\n            exclude_globs.append(line.strip())\n\n    linted_filenames = []\n    for source_dir in arguments.source_dir:\n        for path in lintutils.get_sources(source_dir, exclude_globs):\n            linted_filenames.append(path)\n\n    if not arguments.quiet:\n        msg = 'Tidying {}' if arguments.fix else 'Checking {}'\n        print(\"\\n\".join(map(msg.format, linted_filenames)))\n\n    cmd = [\n        arguments.clang_tidy_binary,\n        '-p',\n        arguments.compile_commands\n    ]\n    if arguments.fix:\n        cmd.append('-fix')\n        results = lintutils.run_parallel(\n            [cmd + some for some in lintutils.chunk(linted_filenames, 16)])\n        for returncode, stdout, stderr in results:\n            if returncode != 0:\n                sys.exit(returncode)\n\n    else:\n        _check_all(cmd, linted_filenames)\n", "cpp/build-support/run_cpplint.py": "#!/usr/bin/env python3\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import print_function\nimport lintutils\nfrom subprocess import PIPE, STDOUT\nimport argparse\nimport multiprocessing as mp\nimport sys\nimport platform\nfrom functools import partial\n\n\n# NOTE(wesm):\n#\n# * readability/casting is disabled as it aggressively warns about functions\n#   with names like \"int32\", so \"int32(x)\", where int32 is a function name,\n#   warns with\n_filters = '''\n-whitespace/comments\n-readability/casting\n-readability/todo\n-readability/alt_tokens\n-build/header_guard\n-build/c++11\n-build/include_what_you_use\n-runtime/references\n-build/include_order\n'''.split()\n\n\ndef _get_chunk_key(filenames):\n    # lists are not hashable so key on the first filename in a chunk\n    return filenames[0]\n\n\ndef _check_some_files(completed_processes, filenames):\n    # cpplint outputs complaints in '/path:line_number: complaint' format,\n    # so we can scan its output to get a list of files to fix\n    result = completed_processes[_get_chunk_key(filenames)]\n    return lintutils.stdout_pathcolonline(result, filenames)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Runs cpplint on all of the source files.\")\n    parser.add_argument(\"--cpplint_binary\",\n                        required=True,\n                        help=\"Path to the cpplint binary\")\n    parser.add_argument(\"--exclude_globs\",\n                        help=\"Filename containing globs for files \"\n                        \"that should be excluded from the checks\")\n    parser.add_argument(\"--source_dir\",\n                        required=True,\n                        action=\"append\",\n                        help=\"Root directory of the source code\")\n    parser.add_argument(\"--quiet\", default=False,\n                        action=\"store_true\",\n                        help=\"If specified, only print errors\")\n    arguments = parser.parse_args()\n\n    exclude_globs = []\n    if arguments.exclude_globs:\n        with open(arguments.exclude_globs) as f:\n            exclude_globs.extend(line.strip() for line in f)\n\n    linted_filenames = []\n    for source_dir in arguments.source_dir:\n        for path in lintutils.get_sources(source_dir, exclude_globs):\n            linted_filenames.append(str(path))\n\n    cmd = [\n        arguments.cpplint_binary,\n        '--verbose=2',\n        '--linelength=90',\n        '--filter=' + ','.join(_filters)\n    ]\n    if (arguments.cpplint_binary.endswith('.py') and\n            platform.system() == 'Windows'):\n        # Windows doesn't support executable scripts; execute with\n        # sys.executable\n        cmd.insert(0, sys.executable)\n    if arguments.quiet:\n        cmd.append('--quiet')\n    else:\n        print(\"\\n\".join(map(lambda x: \"Linting {}\".format(x),\n                            linted_filenames)))\n\n    # lint files in chunks: each invocation of cpplint will process 16 files\n    chunks = lintutils.chunk(linted_filenames, 16)\n    cmds = [cmd + some for some in chunks]\n    results = lintutils.run_parallel(cmds, stdout=PIPE, stderr=STDOUT)\n\n    error = False\n    # record completed processes (keyed by the first filename in the input\n    # chunk) for lookup in _check_some_files\n    completed_processes = {\n        _get_chunk_key(filenames): result\n        for filenames, result in zip(chunks, results)\n    }\n    checker = partial(_check_some_files, completed_processes)\n    pool = mp.Pool()\n    try:\n        # scan the outputs of various cpplint invocations in parallel to\n        # distill a list of problematic files\n        for problem_files, stdout in pool.imap(checker, chunks):\n            if problem_files:\n                if isinstance(stdout, bytes):\n                    stdout = stdout.decode('utf8')\n                print(stdout, file=sys.stderr)\n                error = True\n    except Exception:\n        error = True\n        raise\n    finally:\n        pool.terminate()\n        pool.join()\n\n    sys.exit(1 if error else 0)\n", "cpp/build-support/iwyu/iwyu_tool.py": "#!/usr/bin/env python\n\n# This file has been imported into the apache source tree from\n# the IWYU source tree as of version 0.8\n#   https://github.com/include-what-you-use/include-what-you-use/blob/master/iwyu_tool.py\n# and corresponding license has been added:\n#   https://github.com/include-what-you-use/include-what-you-use/blob/master/LICENSE.TXT\n#\n# ==============================================================================\n# LLVM Release License\n# ==============================================================================\n# University of Illinois/NCSA\n# Open Source License\n#\n# Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign.\n# All rights reserved.\n#\n# Developed by:\n#\n#     LLVM Team\n#\n#     University of Illinois at Urbana-Champaign\n#\n#     http://llvm.org\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal with\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n# of the Software, and to permit persons to whom the Software is furnished to do\n# so, subject to the following conditions:\n#\n#     * Redistributions of source code must retain the above copyright notice,\n#       this list of conditions and the following disclaimers.\n#\n#     * Redistributions in binary form must reproduce the above copyright notice,\n#       this list of conditions and the following disclaimers in the\n#       documentation and/or other materials provided with the distribution.\n#\n#     * Neither the names of the LLVM Team, University of Illinois at\n#       Urbana-Champaign, nor the names of its contributors may be used to\n#       endorse or promote products derived from this Software without specific\n#       prior written permission.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\n# CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE\n# SOFTWARE.\n\n\"\"\" Driver to consume a Clang compilation database and invoke IWYU.\n\nExample usage with CMake:\n\n  # Unix systems\n  $ mkdir build && cd build\n  $ CC=\"clang\" CXX=\"clang++\" cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=ON ...\n  $ iwyu_tool.py -p .\n\n  # Windows systems\n  $ mkdir build && cd build\n  $ cmake -DCMAKE_CXX_COMPILER=\"%VCINSTALLDIR%/bin/cl.exe\" \\\n    -DCMAKE_C_COMPILER=\"%VCINSTALLDIR%/VC/bin/cl.exe\" \\\n    -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \\\n    -G Ninja ...\n  $ python iwyu_tool.py -p .\n\nSee iwyu_tool.py -h for more details on command-line arguments.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport argparse\nimport subprocess\nimport re\n\nimport logging\n\nlogging.basicConfig(filename='iwyu.log')\nLOGGER = logging.getLogger(\"iwyu\")\n\n\ndef iwyu_formatter(output):\n    \"\"\" Process iwyu's output, basically a no-op. \"\"\"\n    print('\\n'.join(output))\n\n\nCORRECT_RE = re.compile(r'^\\((.*?) has correct #includes/fwd-decls\\)$')\nSHOULD_ADD_RE = re.compile(r'^(.*?) should add these lines:$')\nSHOULD_REMOVE_RE = re.compile(r'^(.*?) should remove these lines:$')\nFULL_LIST_RE = re.compile(r'The full include-list for (.*?):$')\nEND_RE = re.compile(r'^---$')\nLINES_RE = re.compile(r'^- (.*?)  // lines ([0-9]+)-[0-9]+$')\n\n\nGENERAL, ADD, REMOVE, LIST = range(4)\n\n\ndef clang_formatter(output):\n    \"\"\" Process iwyu's output into something clang-like. \"\"\"\n    state = (GENERAL, None)\n    for line in output:\n        match = CORRECT_RE.match(line)\n        if match:\n            print('%s:1:1: note: #includes/fwd-decls are correct', match.groups(1))\n            continue\n        match = SHOULD_ADD_RE.match(line)\n        if match:\n            state = (ADD, match.group(1))\n            continue\n        match = SHOULD_REMOVE_RE.match(line)\n        if match:\n            state = (REMOVE, match.group(1))\n            continue\n        match = FULL_LIST_RE.match(line)\n        if match:\n            state = (LIST, match.group(1))\n        elif END_RE.match(line):\n            state = (GENERAL, None)\n        elif not line.strip():\n            continue\n        elif state[0] == GENERAL:\n            print(line)\n        elif state[0] == ADD:\n            print('%s:1:1: error: add the following line', state[1])\n            print(line)\n        elif state[0] == REMOVE:\n            match = LINES_RE.match(line)\n            line_no = match.group(2) if match else '1'\n            print('%s:%s:1: error: remove the following line', state[1], line_no)\n            print(match.group(1))\n\n\nDEFAULT_FORMAT = 'iwyu'\nFORMATTERS = {\n    'iwyu': iwyu_formatter,\n    'clang': clang_formatter\n}\n\n\ndef get_output(cwd, command):\n    \"\"\" Run the given command and return its output as a string. \"\"\"\n    process = subprocess.Popen(command,\n                               cwd=cwd,\n                               shell=True,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.STDOUT)\n    return process.communicate()[0].decode(\"utf-8\").splitlines()\n\n\ndef run_iwyu(cwd, compile_command, iwyu_args, verbose, formatter):\n    \"\"\" Rewrite compile_command to an IWYU command, and run it. \"\"\"\n    compiler, _, args = compile_command.partition(' ')\n    if compiler.endswith('cl.exe'):\n        # If the compiler name is cl.exe, let IWYU be cl-compatible\n        clang_args = ['--driver-mode=cl']\n    else:\n        clang_args = []\n\n    iwyu_args = ['-Xiwyu ' + a for a in iwyu_args]\n    command = ['include-what-you-use'] + clang_args + iwyu_args\n    command = '%s %s' % (' '.join(command), args.strip())\n\n    if verbose:\n        print('%s:', command)\n\n    formatter(get_output(cwd, command))\n\n\ndef main(compilation_db_path, source_files, verbose, formatter, iwyu_args):\n    \"\"\" Entry point. \"\"\"\n    # Canonicalize compilation database path\n    if os.path.isdir(compilation_db_path):\n        compilation_db_path = os.path.join(compilation_db_path,\n                                           'compile_commands.json')\n\n    compilation_db_path = os.path.realpath(compilation_db_path)\n    if not os.path.isfile(compilation_db_path):\n        print('ERROR: No such file or directory: \\'%s\\'', compilation_db_path)\n        return 1\n\n    # Read compilation db from disk\n    with open(compilation_db_path, 'r') as fileobj:\n        compilation_db = json.load(fileobj)\n\n    # expand symlinks\n    for entry in compilation_db:\n        entry['file'] = os.path.realpath(entry['file'])\n\n    # Cross-reference source files with compilation database\n    source_files = [os.path.realpath(s) for s in source_files]\n    if not source_files:\n        # No source files specified, analyze entire compilation database\n        entries = compilation_db\n    else:\n        # Source files specified, analyze the ones appearing in compilation db,\n        # warn for the rest.\n        entries = []\n        for source in source_files:\n            matches = [e for e in compilation_db if e['file'] == source]\n            if matches:\n                entries.extend(matches)\n            else:\n                print(\"{} not in compilation database\".format(source))\n                # TODO: As long as there is no complete compilation database available this check cannot be performed\n                pass\n                #print('WARNING: \\'%s\\' not found in compilation database.', source)\n\n    # Run analysis\n    try:\n        for entry in entries:\n            cwd, compile_command = entry['directory'], entry['command']\n            run_iwyu(cwd, compile_command, iwyu_args, verbose, formatter)\n    except OSError as why:\n        print('ERROR: Failed to launch include-what-you-use: %s', why)\n        return 1\n\n    return 0\n\n\ndef _bootstrap():\n    \"\"\" Parse arguments and dispatch to main(). \"\"\"\n    # This hackery is necessary to add the forwarded IWYU args to the\n    # usage and help strings.\n    def customize_usage(parser):\n        \"\"\" Rewrite the parser's format_usage. \"\"\"\n        original_format_usage = parser.format_usage\n        parser.format_usage = lambda: original_format_usage().rstrip() + \\\n                              ' -- [<IWYU args>]' + os.linesep\n\n    def customize_help(parser):\n        \"\"\" Rewrite the parser's format_help. \"\"\"\n        original_format_help = parser.format_help\n\n        def custom_help():\n            \"\"\" Customized help string, calls the adjusted format_usage. \"\"\"\n            helpmsg = original_format_help()\n            helplines = helpmsg.splitlines()\n            helplines[0] = parser.format_usage().rstrip()\n            return os.linesep.join(helplines) + os.linesep\n\n        parser.format_help = custom_help\n\n    # Parse arguments\n    parser = argparse.ArgumentParser(\n        description='Include-what-you-use compilation database driver.',\n        epilog='Assumes include-what-you-use is available on the PATH.')\n    customize_usage(parser)\n    customize_help(parser)\n\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='Print IWYU commands')\n    parser.add_argument('-o', '--output-format', type=str,\n                        choices=FORMATTERS.keys(), default=DEFAULT_FORMAT,\n                        help='Output format (default: %s)' % DEFAULT_FORMAT)\n    parser.add_argument('-p', metavar='<build-path>', required=True,\n                        help='Compilation database path', dest='dbpath')\n    parser.add_argument('source', nargs='*',\n                        help='Zero or more source files to run IWYU on. '\n                        'Defaults to all in compilation database.')\n\n    def partition_args(argv):\n        \"\"\" Split around '--' into driver args and IWYU args. \"\"\"\n        try:\n            double_dash = argv.index('--')\n            return argv[:double_dash], argv[double_dash+1:]\n        except ValueError:\n            return argv, []\n    argv, iwyu_args = partition_args(sys.argv[1:])\n    args = parser.parse_args(argv)\n\n    sys.exit(main(args.dbpath, args.source, args.verbose,\n                  FORMATTERS[args.output_format], iwyu_args))\n\n\nif __name__ == '__main__':\n    _bootstrap()\n", "cpp/build-support/fuzzing/pack_corpus.py": "#!/usr/bin/env python3\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Rename a bunch of corpus files to their SHA1 hashes, and\n# pack them into a ZIP archive.\n\nimport hashlib\nfrom pathlib import Path\nimport sys\nimport zipfile\n\n\ndef process_dir(corpus_dir, zip_output):\n    seen = set()\n\n    for child in corpus_dir.iterdir():\n        if not child.is_file():\n            raise IOError(\"Not a file: {0}\".format(child))\n        with child.open('rb') as f:\n            data = f.read()\n        arcname = hashlib.sha1(data).hexdigest()\n        if arcname in seen:\n            raise ValueError(\"Duplicate hash: {0} (in file {1})\"\n                             .format(arcname, child))\n        zip_output.writestr(str(arcname), data)\n        seen.add(arcname)\n\n\ndef main(corpus_dir, zip_output_name):\n    with zipfile.ZipFile(zip_output_name, 'w') as zip_output:\n        process_dir(Path(corpus_dir), zip_output)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: {0} <corpus dir> <output zip file>\".format(sys.argv[0]))\n        sys.exit(1)\n    main(sys.argv[1], sys.argv[2])\n", "cpp/src/gandiva/make_precompiled_bitcode.py": "#!/usr/bin/env python\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport sys\n\nmarker = b\"<DATA_CHARS>\"\n\ndef expand(data):\n    \"\"\"\n    Expand *data* as a initializer list of hexadecimal char escapes.\n    \"\"\"\n    expanded_data = \", \".join([hex(c) for c in bytearray(data)])\n    return expanded_data.encode('ascii')\n\n\ndef apply_template(template, data):\n    if template.count(marker) != 1:\n        raise ValueError(\"Invalid template\")\n    return template.replace(marker, expand(data))\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 4:\n        raise ValueError(\"Usage: {0} <template file> <data file> \"\n                         \"<output file>\".format(sys.argv[0]))\n    with open(sys.argv[1], \"rb\") as f:\n        template = f.read()\n    with open(sys.argv[2], \"rb\") as f:\n        data = f.read()\n\n    expanded_data = apply_template(template, data)\n    with open(sys.argv[3], \"wb\") as f:\n        f.write(expanded_data)\n", "cpp/src/arrow/acero/hash_join_graphs.py": "#!/bin/env python3\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n'''\nThis script takes a JSON file from a google benchmark run that measures rows/sec and generates graphs\nfor each benchmark.\n\nExample usage:\n1. Generate Benchmark Data:\nrelease/arrow-compute-hash-join-benchmark \\\n    --benchmark_counters_tabular=true \\\n    --benchmark_format=console \\\n    --benchmark_out=benchmark_data.json \\\n    --benchmark_out_format=json\n\n2. Visualize:\n../src/arrow/compute/exec/hash_join_graphs.py benchmarks_data.json\n'''\n\nimport math\nimport sys\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()\n\ndef try_as_numeric(val):\n    try:\n        return float(val)\n    except:\n        return str(val)\n\ndef is_multiplicative(lst):\n    if len(lst) < 3:\n        return False\n\n    if (lst[2] - lst[1]) == (lst[1] - lst[0]):\n        return False\n    \n    assert (lst[2] / lst[1]) == (lst[1] / lst[0])\n    return True\n\nclass Test:\n    def __init__(self):\n        self.times = []\n        self.args = {}\n\n    def get_argnames_by_cardinality_increasing(self):\n        key_cardinality = lambda x: len(set(self.args[x]))\n        def key_strings_first(x):\n            try:\n                as_float = float(self.args[x][0])\n                return True\n            except:\n                return False\n        by_cardinality = sorted(self.args.keys(), key=key_cardinality)\n        strings_first = sorted(by_cardinality, key=key_strings_first)\n        return strings_first\n\ndef organize_tests(filename):\n    tests = {}\n    with open(filename) as f:\n        df = json.load(f)\n        for idx, row in enumerate(df['benchmarks']):\n            test_name = row['name']\n            test_name_split = test_name.split('/')\n            if test_name_split[-1] == 'process_time':\n                test_name_split = test_name_split[:-1]\n                \n            base_name = test_name_split[0]\n            args = test_name_split[1:]\n            if base_name not in tests.keys():\n                tests[base_name] = Test()\n\n            tests[base_name].times.append(row['rows/sec'])\n            \n            if len(args) > 3:\n                raise('Test can have at most 3 parameters! Found', len(args), 'in test', test_name)\n            \n            nonnamed_args = [x for x in args if ':' not in x]\n            if len(nonnamed_args) > 1:\n                raise('Test name must have only one non-named parameter! Found', len(nonnamed_args), 'in test', test_name)\n\n            for arg in args:\n                arg_name = ''\n                arg_value = arg.strip('\\\"')\n                if ':' in arg:\n                    arg_split = arg.split(':')\n                    arg_name = arg_split[0]\n                    arg_value = arg_split[1].strip('\\\"')\n\n                arg_value = try_as_numeric(arg_value)\n                if arg_name not in tests[base_name].args.keys():\n                    tests[base_name].args[arg_name] = [arg_value]\n                else:\n                    tests[base_name].args[arg_name].append(arg_value)\n    return tests;\n\ndef construct_name(argname, argvalue):\n    if not argname:\n        return argvalue\n    return '%s: %s' % (argname, argvalue)\n\ndef plot_1d(test, argname, ax, label=None):\n    x_axis = test.args[argname]\n    y_axis = test.times\n    ax.plot(x_axis, y_axis, label=label)\n    if is_multiplicative(x_axis):\n        ax.set_xscale('log', base=(x_axis[1] / x_axis[0]))\n        ax.xaxis.set_major_formatter(plt.ScalarFormatter())\n    ax.legend()\n    ax.set_xlabel(argname)\n    ax.set_ylabel('rows/sec')\n\ndef plot_2d(test, sorted_argnames, ax, title):\n    assert len(sorted_argnames) == 2\n    lines = set(test.args[sorted_argnames[0]])\n    ax.set_title(title)\n    for line in sorted(lines, key=try_as_numeric):\n        indices = range(len(test.times))\n        indices = list(filter(lambda i: test.args[sorted_argnames[0]][i] == line, indices))\n        filtered_test = Test()\n        filtered_test.times = [test.times[i] for i in indices]\n        filtered_test.args[sorted_argnames[1]] = [test.args[sorted_argnames[1]][i] for i in indices]\n        plot_1d(filtered_test, sorted_argnames[1], ax, construct_name(sorted_argnames[0], line))\n\ndef plot_3d(test, sorted_argnames):\n    assert len(sorted_argnames) == 3\n    num_graphs = len(set(test.args[sorted_argnames[0]]))\n    num_rows = int(math.ceil(math.sqrt(num_graphs)))\n    num_cols = int(math.ceil(num_graphs / num_rows))\n    graphs = set(test.args[sorted_argnames[0]])\n\n    for j, graph in enumerate(sorted(graphs, key=try_as_numeric)):\n        ax = plt.subplot(num_rows, num_cols, j + 1)\n        filtered_test = Test()\n        indices = range(len(test.times))\n        indices = list(filter(lambda i: test.args[sorted_argnames[0]][i] == graph, indices))\n        filtered_test.times = [test.times[i] for i in indices]\n        filtered_test.args[sorted_argnames[1]] = [test.args[sorted_argnames[1]][i] for i in indices]\n        filtered_test.args[sorted_argnames[2]] = [test.args[sorted_argnames[2]][i] for i in indices]\n        plot_2d(filtered_test, sorted_argnames[1:], ax, construct_name(sorted_argnames[0], graph))\n\ndef main():\n    if len(sys.argv) != 2:\n        print('Usage: hash_join_graphs.py <data>.json')\n        print('This script expects there to be a counter called rows/sec as a field of every test in the JSON file.')\n        return\n\n    tests = organize_tests(sys.argv[1])\n\n    for i, test_name in enumerate(tests.keys()):\n        test = tests[test_name]\n        sorted_argnames = test.get_argnames_by_cardinality_increasing()\n        # Create a graph per lowest-cardinality arg\n        # Create a line per second-lowest-cardinality arg\n        # Use highest-cardinality arg as X axis\n        fig = plt.figure(i)\n        num_args = len(sorted_argnames)\n        if num_args == 3:\n            fig.suptitle(test_name)\n            plot_3d(test, sorted_argnames)\n            fig.subplots_adjust(hspace=0.4)\n        elif num_args == 2:\n            ax = plt.subplot()\n            plot_2d(test, sorted_argnames, ax, test_name)\n        else:\n            fig.suptitle(test_name)\n            ax = plt.subplot()\n            plot_1d(test, sorted_argnames[0], ax)\n        fig.set_size_inches(16, 9)\n        fig.savefig('%s.svg' % test_name, dpi=fig.dpi, bbox_inches='tight')\n        plt.show()\n\nif __name__ == '__main__':\n    main()\n", "cpp/src/arrow/util/bpacking_simd_codegen.py": "#!/usr/bin/env python\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Usage:\n#   python bpacking_simd_codegen.py 128 > bpacking_simd128_generated_internal.h\n#   python bpacking_simd_codegen.py 256 > bpacking_simd256_generated_internal.h\n#   python bpacking_simd_codegen.py 512 > bpacking_simd512_generated_internal.h\n\nfrom functools import partial\nimport sys\nfrom textwrap import dedent, indent\n\n\nclass UnpackGenerator:\n\n    def __init__(self, simd_width):\n        self.simd_width = simd_width\n        if simd_width % 32 != 0:\n            raise(\"SIMD bit width should be a multiple of 32\")\n        self.simd_byte_width = simd_width // 8\n\n    def print_unpack_bit0_func(self):\n        print(\n            \"inline static const uint32_t* unpack0_32(const uint32_t* in, uint32_t* out) {\")\n        print(\"  memset(out, 0x0, 32 * sizeof(*out));\")\n        print(\"  out += 32;\")\n        print(\"\")\n        print(\"  return in;\")\n        print(\"}\")\n\n\n    def print_unpack_bit32_func(self):\n        print(\n            \"inline static const uint32_t* unpack32_32(const uint32_t* in, uint32_t* out) {\")\n        print(\"  memcpy(out, in, 32 * sizeof(*out));\")\n        print(\"  in += 32;\")\n        print(\"  out += 32;\")\n        print(\"\")\n        print(\"  return in;\")\n        print(\"}\")\n\n    def print_unpack_bit_func(self, bit):\n        def p(code):\n            print(indent(code, prefix='  '))\n\n        shift = 0\n        shifts = []\n        in_index = 0\n        inls = []\n        mask = (1 << bit) - 1\n        bracket = \"{\"\n\n        print(f\"inline static const uint32_t* unpack{bit}_32(const uint32_t* in, uint32_t* out) {{\")\n        p(dedent(f\"\"\"\\\n            uint32_t mask = 0x{mask:0x};\n\n            simd_batch masks(mask);\n            simd_batch words, shifts;\n            simd_batch results;\n            \"\"\"))\n\n        def safe_load(index):\n            return f\"SafeLoad<uint32_t>(in + {index})\"\n\n        for i in range(32):\n            if shift + bit == 32:\n                shifts.append(shift)\n                inls.append(safe_load(in_index))\n                in_index += 1\n                shift = 0\n            elif shift + bit > 32:  # cross the boundary\n                inls.append(\n                    f\"{safe_load(in_index)} >> {shift} | {safe_load(in_index + 1)} << {32 - shift}\")\n                in_index += 1\n                shift = bit - (32 - shift)\n                shifts.append(0)  # zero shift\n            else:\n                shifts.append(shift)\n                inls.append(safe_load(in_index))\n                shift += bit\n\n        bytes_per_batch = self.simd_byte_width\n        words_per_batch = bytes_per_batch // 4\n\n        one_word_template = dedent(\"\"\"\\\n            words = simd_batch{{ {words} }};\n            shifts = simd_batch{{ {shifts} }};\n            results = (words >> shifts) & masks;\n            results.store_unaligned(out);\n            out += {words_per_batch};\n            \"\"\")\n\n        for start in range(0, 32, words_per_batch):\n            stop = start + words_per_batch;\n            p(f\"\"\"// extract {bit}-bit bundles {start} to {stop - 1}\"\"\")\n            p(one_word_template.format(\n                words=\", \".join(inls[start:stop]),\n                shifts=\", \".join(map(str, shifts[start:stop])),\n                words_per_batch=words_per_batch))\n\n        p(dedent(f\"\"\"\\\n            in += {bit};\n            return in;\"\"\"))\n        print(\"}\")\n\n\ndef print_copyright():\n    print(dedent(\"\"\"\\\n        // Licensed to the Apache Software Foundation (ASF) under one\n        // or more contributor license agreements.  See the NOTICE file\n        // distributed with this work for additional information\n        // regarding copyright ownership.  The ASF licenses this file\n        // to you under the Apache License, Version 2.0 (the\n        // \"License\"); you may not use this file except in compliance\n        // with the License.  You may obtain a copy of the License at\n        //\n        //   http://www.apache.org/licenses/LICENSE-2.0\n        //\n        // Unless required by applicable law or agreed to in writing,\n        // software distributed under the License is distributed on an\n        // \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n        // KIND, either express or implied.  See the License for the\n        // specific language governing permissions and limitations\n        // under the License.\n        \"\"\"))\n\n\ndef print_note():\n    print(\"// Automatically generated file; DO NOT EDIT.\")\n    print()\n\n\ndef main(simd_width):\n    print_copyright()\n    print_note()\n\n    struct_name = f\"UnpackBits{simd_width}\"\n\n    # NOTE: templating the UnpackBits struct on the dispatch level avoids\n    # potential name collisions if there are several UnpackBits generations\n    # with the same SIMD width on a given architecture.\n\n    print(dedent(f\"\"\"\\\n        #pragma once\n\n        #include <cstdint>\n        #include <cstring>\n\n        #include <xsimd/xsimd.hpp>\n\n        #include \"arrow/util/dispatch.h\"\n        #include \"arrow/util/ubsan.h\"\n\n        namespace arrow {{\n        namespace internal {{\n        namespace {{\n\n        using ::arrow::util::SafeLoad;\n\n        template <DispatchLevel level>\n        struct {struct_name} {{\n\n        using simd_batch = xsimd::make_sized_batch_t<uint32_t, {simd_width//32}>;\n        \"\"\"))\n\n    gen = UnpackGenerator(simd_width)\n    gen.print_unpack_bit0_func()\n    print()\n    for i in range(1, 32):\n        gen.print_unpack_bit_func(i)\n        print()\n    gen.print_unpack_bit32_func()\n    print()\n\n    print(dedent(f\"\"\"\\\n        }};  // struct {struct_name}\n\n        }}  // namespace\n        }}  // namespace internal\n        }}  // namespace arrow\n        \"\"\"))\n\n\nif __name__ == '__main__':\n    usage = f\"\"\"Usage: {__file__} <SIMD bit-width>\"\"\"\n    if len(sys.argv) != 2:\n        raise ValueError(usage)\n    try:\n        simd_width = int(sys.argv[1])\n    except ValueError:\n        raise ValueError(usage)\n\n    main(simd_width)\n", "cpp/src/arrow/util/bpacking64_codegen.py": "#!/bin/python\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# This script is modified from its original version in GitHub. Original source:\n# https://github.com/lemire/FrameOfReference/blob/146948b6058a976bc7767262ad3a2ce201486b93/scripts/turbopacking64.py\n\n# Usage:\n#   python bpacking64_codegen.py > bpacking64_default.h\n\ndef howmany(bit):\n    \"\"\" how many values are we going to pack? \"\"\"\n    return 32\n\n\ndef howmanywords(bit):\n    return (howmany(bit) * bit + 63)//64\n\n\ndef howmanybytes(bit):\n    return (howmany(bit) * bit + 7)//8\n\n\nprint('''// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\n// This file was generated by script which is modified from its original version in GitHub.\n// Original source:\n// https://github.com/lemire/FrameOfReference/blob/master/scripts/turbopacking64.py\n// The original copyright notice follows.\n\n// This code is released under the\n// Apache License Version 2.0 http://www.apache.org/licenses/.\n// (c) Daniel Lemire 2013\n\n#pragma once\n\n#include \"arrow/util/bit_util.h\"\n#include \"arrow/util/ubsan.h\"\n\nnamespace arrow {\nnamespace internal {\n''')\n\n\nprint(\"inline const uint8_t* unpack0_64(const uint8_t* in, uint64_t* out) {\")\nprint(\"  for(int k = 0; k < {0} ; k += 1) {{\".format(howmany(0)))\nprint(\"    out[k] = 0;\")\nprint(\"  }\")\nprint(\"  return in;\")\nprint(\"}\")\n\nfor bit in range(1, 65):\n    print(\"\")\n    print(\n        \"inline const uint8_t* unpack{0}_64(const uint8_t* in, uint64_t* out) {{\".format(bit))\n\n    if(bit < 64):\n        print(\"  const uint64_t mask = {0}ULL;\".format((1 << bit)-1))\n    maskstr = \" & mask\"\n    if (bit == 64):\n        maskstr = \"\"  # no need\n\n    for k in range(howmanywords(bit)-1):\n        print(\"  uint64_t w{0} = util::SafeLoadAs<uint64_t>(in);\".format(k))\n        print(\"  w{0} = arrow::BitUtil::FromLittleEndian(w{0});\".format(k))\n        print(\"  in += 8;\".format(k))\n    k = howmanywords(bit) - 1\n    if (bit % 2 == 0):\n        print(\"  uint64_t w{0} = util::SafeLoadAs<uint64_t>(in);\".format(k))\n        print(\"  w{0} = arrow::BitUtil::FromLittleEndian(w{0});\".format(k))\n        print(\"  in += 8;\".format(k))\n    else:\n        print(\"  uint64_t w{0} = util::SafeLoadAs<uint32_t>(in);\".format(k))\n        print(\"  w{0} = arrow::BitUtil::FromLittleEndian(w{0});\".format(k))\n        print(\"  in += 4;\".format(k))\n\n    for j in range(howmany(bit)):\n        firstword = j * bit // 64\n        secondword = (j * bit + bit - 1)//64\n        firstshift = (j*bit) % 64\n        firstshiftstr = \" >> {0}\".format(firstshift)\n        if(firstshift == 0):\n            firstshiftstr = \"\"  # no need\n        if(firstword == secondword):\n            if(firstshift + bit == 64):\n                print(\"  out[{0}] = w{1}{2};\".format(\n                    j, firstword, firstshiftstr, firstshift))\n            else:\n                print(\"  out[{0}] = (w{1}{2}){3};\".format(\n                    j, firstword, firstshiftstr, maskstr))\n        else:\n            secondshift = (64-firstshift)\n            print(\"  out[{0}] = ((w{1}{2}) | (w{3} << {4})){5};\".format(\n                j, firstword, firstshiftstr, firstword+1, secondshift, maskstr))\n    print(\"\")\n    print(\"  return in;\")\n    print(\"}\")\n\nprint('''\n}  // namespace internal\n}  // namespace arrow''')\n", "ci/scripts/go_bench_adapt.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport json\nimport os\nimport uuid\nimport logging\nfrom pathlib import Path\nfrom typing import List\n\nfrom benchadapt import BenchmarkResult\nfrom benchadapt.adapters import BenchmarkAdapter\nfrom benchadapt.log import log\n\nlog.setLevel(logging.DEBUG)\n\nARROW_ROOT = Path(__file__).parent.parent.parent.resolve()\nSCRIPTS_PATH = ARROW_ROOT / \"ci\" / \"scripts\"\n\n# `github_commit_info` is meant to communicate GitHub-flavored commit\n# information to Conbench. See\n# https://github.com/conbench/conbench/blob/cf7931f/benchadapt/python/benchadapt/result.py#L66\n# for a specification.\ngithub_commit_info = {\"repository\": \"https://github.com/apache/arrow\"}\n\nif os.environ.get(\"CONBENCH_REF\") == \"main\":\n    # Assume GitHub Actions CI. The environment variable lookups below are\n    # expected to fail when not running in GitHub Actions.\n    github_commit_info = {\n        \"repository\": f'{os.environ[\"GITHUB_SERVER_URL\"]}/{os.environ[\"GITHUB_REPOSITORY\"]}',\n        \"commit\": os.environ[\"GITHUB_SHA\"],\n        \"pr_number\": None,  # implying default branch\n    }\n    run_reason = \"commit\"\nelse:\n    # Assume that the environment is not GitHub Actions CI. Error out if that\n    # assumption seems to be wrong.\n    assert os.getenv(\"GITHUB_ACTIONS\") is None\n\n    # This is probably a local dev environment, for testing. In this case, it\n    # does usually not make sense to provide commit information (not a\n    # controlled CI environment). Explicitly leave out \"commit\" and \"pr_number\" to\n    # reflect that (to not send commit information).\n\n    # Reflect 'local dev' scenario in run_reason. Allow user to (optionally)\n    # inject a custom piece of information into the run reason here, from\n    # environment.\n    run_reason = \"localdev\"\n    custom_reason_suffix = os.getenv(\"CONBENCH_CUSTOM_RUN_REASON\")\n    if custom_reason_suffix is not None:\n        run_reason += f\" {custom_reason_suffix.strip()}\"\n\n\nclass GoAdapter(BenchmarkAdapter):\n    result_file = \"bench_stats.json\"\n    command = [\"bash\", SCRIPTS_PATH / \"go_bench.sh\", ARROW_ROOT, \"-json\"]\n\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(command=self.command, *args, **kwargs)\n\n    def _transform_results(self) -> List[BenchmarkResult]:\n        with open(self.result_file, \"r\") as f:\n            raw_results = json.load(f)\n\n        run_id = uuid.uuid4().hex\n        parsed_results = []\n        for suite in raw_results[0][\"Suites\"]:\n            batch_id = uuid.uuid4().hex\n            pkg = suite[\"Pkg\"]\n\n            for benchmark in suite[\"Benchmarks\"]:\n                data = benchmark[\"Mem\"][\"MBPerSec\"] * 1e6\n                time = 1 / benchmark[\"NsPerOp\"] * 1e9\n\n                name = benchmark[\"Name\"].removeprefix(\"Benchmark\")\n                ncpu = name[name.rfind(\"-\") + 1 :]\n                pieces = name[: -(len(ncpu) + 1)].split(\"/\")\n\n                parsed = BenchmarkResult(\n                    run_id=run_id,\n                    batch_id=batch_id,\n                    stats={\n                        \"data\": [data],\n                        \"unit\": \"B/s\",\n                        \"times\": [time],\n                        \"time_unit\": \"i/s\",\n                        \"iterations\": benchmark[\"Runs\"],\n                    },\n                    context={\n                        \"benchmark_language\": \"Go\",\n                        \"goos\": suite[\"Goos\"],\n                        \"goarch\": suite[\"Goarch\"],\n                    },\n                    tags={\n                        \"pkg\": pkg,\n                        \"num_cpu\": ncpu,\n                        \"name\": pieces[0],\n                        \"params\": \"/\".join(pieces[1:]),\n                    },\n                    run_reason=run_reason,\n                    github=github_commit_info,\n                )\n                parsed.run_name = (\n                    f\"{parsed.run_reason}: {github_commit_info.get('commit')}\"\n                )\n                parsed_results.append(parsed)\n\n        return parsed_results\n\n\nif __name__ == \"__main__\":\n    go_adapter = GoAdapter(result_fields_override={\"info\": {}})\n    go_adapter()\n", "ci/conan/all/conanfile.py": "# MIT License\n#\n# Copyright (c) 2019 Conan.io\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom conan import ConanFile\nfrom conan.errors import ConanInvalidConfiguration, ConanException\nfrom conan.tools.build import check_min_cppstd, cross_building\nfrom conan.tools.cmake import CMake, CMakeDeps, CMakeToolchain, cmake_layout\nfrom conan.tools.files import apply_conandata_patches, copy, export_conandata_patches, get, rmdir\nfrom conan.tools.microsoft import is_msvc, is_msvc_static_runtime\nfrom conan.tools.scm import Version\n\nimport os\nimport glob\n\nrequired_conan_version = \">=1.53.0\"\n\nclass ArrowConan(ConanFile):\n    name = \"arrow\"\n    description = \"Apache Arrow is a cross-language development platform for in-memory data\"\n    license = (\"Apache-2.0\",)\n    url = \"https://github.com/conan-io/conan-center-index\"\n    homepage = \"https://arrow.apache.org/\"\n    topics = (\"memory\", \"gandiva\", \"parquet\", \"skyhook\", \"acero\", \"hdfs\", \"csv\", \"cuda\", \"gcs\", \"json\", \"hive\", \"s3\", \"grpc\")\n    package_type = \"library\"\n    settings = \"os\", \"arch\", \"compiler\", \"build_type\"\n    options = {\n        \"shared\": [True, False],\n        \"fPIC\": [True, False],\n        \"gandiva\":  [True, False],\n        \"parquet\": [\"auto\", True, False],\n        \"substrait\": [True, False],\n        \"skyhook\": [True, False],\n        \"acero\": [True, False],\n        \"cli\": [True, False],\n        \"compute\": [\"auto\", True, False],\n        \"dataset_modules\":  [\"auto\", True, False],\n        \"deprecated\": [True, False],\n        \"encryption\": [True, False],\n        \"filesystem_layer\":  [True, False],\n        \"hdfs_bridgs\": [True, False],\n        \"plasma\": [True, False, \"deprecated\"],\n        \"simd_level\": [None, \"default\", \"sse4_2\", \"avx2\", \"avx512\", \"neon\", ],\n        \"runtime_simd_level\": [None, \"sse4_2\", \"avx2\", \"avx512\", \"max\"],\n        \"with_backtrace\": [True, False],\n        \"with_boost\": [\"auto\", True, False],\n        \"with_csv\": [True, False],\n        \"with_cuda\": [True, False],\n        \"with_flight_rpc\":  [\"auto\", True, False],\n        \"with_flight_sql\":  [True, False],\n        \"with_gcs\": [True, False],\n        \"with_gflags\": [\"auto\", True, False],\n        \"with_glog\": [\"auto\", True, False],\n        \"with_grpc\": [\"auto\", True, False],\n        \"with_jemalloc\": [\"auto\", True, False],\n        \"with_mimalloc\": [True, False],\n        \"with_json\": [True, False],\n        \"with_thrift\": [\"auto\", True, False],\n        \"with_llvm\": [\"auto\", True, False],\n        \"with_openssl\": [\"auto\", True, False],\n        \"with_opentelemetry\": [True, False],\n        \"with_orc\": [True, False],\n        \"with_protobuf\": [\"auto\", True, False],\n        \"with_re2\": [\"auto\", True, False],\n        \"with_s3\": [True, False],\n        \"with_utf8proc\": [\"auto\", True, False],\n        \"with_brotli\": [True, False],\n        \"with_bz2\": [True, False],\n        \"with_lz4\": [True, False],\n        \"with_snappy\": [True, False],\n        \"with_zlib\": [True, False],\n        \"with_zstd\": [True, False],\n    }\n    default_options = {\n        \"shared\": False,\n        \"fPIC\": True,\n        \"gandiva\": False,\n        \"parquet\": False,\n        \"skyhook\": False,\n        \"substrait\": False,\n        \"acero\": False,\n        \"cli\": False,\n        \"compute\": False,\n        \"dataset_modules\": False,\n        \"deprecated\": True,\n        \"encryption\": False,\n        \"filesystem_layer\": False,\n        \"hdfs_bridgs\": False,\n        \"plasma\": \"deprecated\",\n        \"simd_level\": \"default\",\n        \"runtime_simd_level\": \"max\",\n        \"with_backtrace\": False,\n        \"with_boost\": False,\n        \"with_brotli\": False,\n        \"with_bz2\": False,\n        \"with_csv\": False,\n        \"with_cuda\": False,\n        \"with_flight_rpc\": False,\n        \"with_flight_sql\": False,\n        \"with_gcs\": False,\n        \"with_gflags\": False,\n        \"with_jemalloc\": False,\n        \"with_mimalloc\": False,\n        \"with_glog\": False,\n        \"with_grpc\": False,\n        \"with_json\": False,\n        \"with_thrift\": False,\n        \"with_llvm\": False,\n        \"with_openssl\": False,\n        \"with_opentelemetry\": False,\n        \"with_orc\": False,\n        \"with_protobuf\": False,\n        \"with_re2\": False,\n        \"with_s3\": False,\n        \"with_utf8proc\": False,\n        \"with_lz4\": False,\n        \"with_snappy\": False,\n        \"with_zlib\": False,\n        \"with_zstd\": False,\n    }\n    short_paths = True\n\n    @property\n    def _min_cppstd(self):\n        # arrow >= 10.0.0 requires C++17.\n        # https://github.com/apache/arrow/pull/13991\n        return \"11\" if Version(self.version) < \"10.0.0\" else \"17\"\n\n    @property\n    def _compilers_minimum_version(self):\n        return {\n            \"11\": {\n                \"clang\": \"3.9\",\n            },\n            \"17\": {\n                \"gcc\": \"8\",\n                \"clang\": \"7\",\n                \"apple-clang\": \"10\",\n                \"Visual Studio\": \"15\",\n                \"msvc\": \"191\",\n            },\n        }.get(self._min_cppstd, {})\n\n    def export_sources(self):\n        export_conandata_patches(self)\n        copy(self, \"conan_cmake_project_include.cmake\", self.recipe_folder, os.path.join(self.export_sources_folder, \"src\"))\n\n    def config_options(self):\n        if self.settings.os == \"Windows\":\n            del self.options.fPIC\n        if Version(self.version) < \"8.0.0\":\n            del self.options.substrait\n        if is_msvc(self):\n            self.options.with_boost = True\n\n    def configure(self):\n        if self.options.shared:\n            self.options.rm_safe(\"fPIC\")\n\n    def layout(self):\n        cmake_layout(self, src_folder=\"src\")\n\n    def _requires_rapidjson(self):\n        return self.options.with_json or self.options.encryption\n\n    def requirements(self):\n        if self.options.with_thrift:\n            self.requires(\"thrift/0.17.0\")\n        if self.options.with_protobuf:\n            self.requires(\"protobuf/3.21.9\")\n        if self.options.with_jemalloc:\n            self.requires(\"jemalloc/5.3.0\")\n        if self.options.with_mimalloc:\n            self.requires(\"mimalloc/1.7.6\")\n        if self.options.with_boost:\n            self.requires(\"boost/1.84.0\")\n        if self.options.with_gflags:\n            self.requires(\"gflags/2.2.2\")\n        if self.options.with_glog:\n            self.requires(\"glog/0.6.0\")\n        if self.options.get_safe(\"with_gcs\"):\n            self.requires(\"google-cloud-cpp/1.40.1\")\n        if self.options.with_grpc:\n            self.requires(\"grpc/1.50.0\")\n        if self._requires_rapidjson():\n            self.requires(\"rapidjson/1.1.0\")\n        if self.options.with_llvm:\n            self.requires(\"llvm-core/13.0.0\")\n        if self.options.with_openssl:\n            # aws-sdk-cpp requires openssl/1.1.1. it uses deprecated functions in openssl/3.0.0\n            if self.options.with_s3:\n                self.requires(\"openssl/1.1.1w\")\n            else:\n                self.requires(\"openssl/[>=1.1 <4]\")\n        if self.options.get_safe(\"with_opentelemetry\"):\n            self.requires(\"opentelemetry-cpp/1.7.0\")\n        if self.options.with_s3:\n            self.requires(\"aws-sdk-cpp/1.9.234\")\n        if self.options.with_brotli:\n            self.requires(\"brotli/1.1.0\")\n        if self.options.with_bz2:\n            self.requires(\"bzip2/1.0.8\")\n        if self.options.with_lz4:\n            self.requires(\"lz4/1.9.4\")\n        if self.options.with_snappy:\n            self.requires(\"snappy/1.1.9\")\n        if self.options.get_safe(\"simd_level\") != None or \\\n            self.options.get_safe(\"runtime_simd_level\") != None:\n            self.requires(\"xsimd/9.0.1\")\n        if self.options.with_zlib:\n            self.requires(\"zlib/[>=1.2.11 <2]\")\n        if self.options.with_zstd:\n            self.requires(\"zstd/1.5.5\")\n        if self.options.with_re2:\n            self.requires(\"re2/20230301\")\n        if self.options.with_utf8proc:\n            self.requires(\"utf8proc/2.8.0\")\n        if self.options.with_backtrace:\n            self.requires(\"libbacktrace/cci.20210118\")\n\n    def validate(self):\n        # Do not allow options with 'auto' value\n        # TODO: Remove \"auto\" from the possible values for these options\n        auto_options = [option for option, value in self.options.items() if value == \"auto\"]\n        if auto_options:\n            raise ConanException(\"Options with value 'auto' are deprecated. Please set them true/false or use its default value.\"\n                                 f\" Please change the following options: {auto_options}\")\n\n        # From https://github.com/conan-io/conan-center-index/pull/23163#issuecomment-2039808851\n        if self.options.gandiva:\n            if not self.options.with_re2:\n                raise ConanException(\"'with_re2' option should be True when'gandiva=True'\")\n            if not self.options.with_boost:\n                raise ConanException(\"'with_boost' option should be True when'gandiva=True'\")\n            if not self.options.with_utf8proc:\n                raise ConanException(\"'with_utf8proc' option should be True when'gandiva=True'\")\n\n        if self.settings.compiler.get_safe(\"cppstd\"):\n            check_min_cppstd(self, self._min_cppstd)\n\n        minimum_version = self._compilers_minimum_version.get(str(self.settings.compiler), False)\n        if minimum_version and Version(self.settings.compiler.version) < minimum_version:\n            raise ConanInvalidConfiguration(\n                f\"{self.ref} requires C++{self._min_cppstd}, which your compiler does not support.\"\n            )\n\n        if self.options.get_safe(\"skyhook\", False):\n            raise ConanInvalidConfiguration(\"CCI has no librados recipe (yet)\")\n        if self.options.with_cuda:\n            raise ConanInvalidConfiguration(\"CCI has no cuda recipe (yet)\")\n        if self.options.with_orc:\n            raise ConanInvalidConfiguration(\"CCI has no orc recipe (yet)\")\n        if self.options.with_s3 and not self.dependencies[\"aws-sdk-cpp\"].options.config:\n            raise ConanInvalidConfiguration(\"arrow:with_s3 requires aws-sdk-cpp:config is True.\")\n\n        if self.options.shared and self.options.with_jemalloc:\n            if self.dependencies[\"jemalloc\"].options.enable_cxx:\n                raise ConanInvalidConfiguration(\"jemmalloc.enable_cxx of a static jemalloc must be disabled\")\n\n\n    def build_requirements(self):\n        if Version(self.version) >= \"13.0.0\":\n            self.tool_requires(\"cmake/[>=3.16 <4]\")\n\n    def source(self):\n        # START\n        # This block should be removed when we update upstream:\n        # https://github.com/conan-io/conan-center-index/tree/master/recipes/arrow/\n        if not self.version in self.conan_data.get(\"sources\", {}):\n            import shutil\n            top_level = os.environ.get(\"ARROW_HOME\")\n            shutil.copytree(os.path.join(top_level, \"cpp\"),\n                            os.path.join(self.source_folder, \"cpp\"))\n            shutil.copytree(os.path.join(top_level, \"format\"),\n                            os.path.join(self.source_folder, \"format\"))\n            top_level_files = [\n                \".env\",\n                \"LICENSE.txt\",\n                \"NOTICE.txt\",\n            ]\n            for top_level_file in top_level_files:\n                shutil.copy(os.path.join(top_level, top_level_file),\n                            self.source_folder)\n            return\n        # END\n        get(self, **self.conan_data[\"sources\"][self.version],\n            filename=f\"apache-arrow-{self.version}.tar.gz\", strip_root=True)\n\n    def generate(self):\n        tc = CMakeToolchain(self)\n        if cross_building(self):\n            cmake_system_processor = {\n                \"armv8\": \"aarch64\",\n                \"armv8.3\": \"aarch64\",\n            }.get(str(self.settings.arch), str(self.settings.arch))\n            if cmake_system_processor == \"aarch64\":\n                tc.variables[\"ARROW_CPU_FLAG\"] = \"armv8\"\n        if is_msvc(self):\n            tc.variables[\"ARROW_USE_STATIC_CRT\"] = is_msvc_static_runtime(self)\n        tc.variables[\"ARROW_DEPENDENCY_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_PACKAGE_KIND\"] = \"conan\" # See https://github.com/conan-io/conan-center-index/pull/14903/files#r1057938314 for details\n        tc.variables[\"ARROW_GANDIVA\"] = bool(self.options.gandiva)\n        tc.variables[\"ARROW_PARQUET\"] = self.options.parquet\n        tc.variables[\"ARROW_SUBSTRAIT\"] = bool(self.options.get_safe(\"substrait\", False))\n        tc.variables[\"ARROW_ACERO\"] = bool(self.options.acero)\n        tc.variables[\"ARROW_DATASET\"] = self.options.dataset_modules\n        tc.variables[\"ARROW_FILESYSTEM\"] = bool(self.options.filesystem_layer)\n        tc.variables[\"PARQUET_REQUIRE_ENCRYPTION\"] = bool(self.options.encryption)\n        tc.variables[\"ARROW_HDFS\"] = bool(self.options.hdfs_bridgs)\n        tc.variables[\"ARROW_VERBOSE_THIRDPARTY_BUILD\"] = True\n        tc.variables[\"ARROW_BUILD_SHARED\"] = bool(self.options.shared)\n        tc.variables[\"ARROW_BUILD_STATIC\"] = not bool(self.options.shared)\n        tc.variables[\"ARROW_NO_DEPRECATED_API\"] = not bool(self.options.deprecated)\n        tc.variables[\"ARROW_FLIGHT\"] = self.options.with_flight_rpc\n        tc.variables[\"ARROW_FLIGHT_SQL\"] = bool(self.options.get_safe(\"with_flight_sql\", False))\n        tc.variables[\"ARROW_COMPUTE\"] = bool(self.options.compute)\n        tc.variables[\"ARROW_CSV\"] = bool(self.options.with_csv)\n        tc.variables[\"ARROW_CUDA\"] = bool(self.options.with_cuda)\n        tc.variables[\"ARROW_JEMALLOC\"] = self.options.with_jemalloc\n        tc.variables[\"jemalloc_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_MIMALLOC\"] = bool(self.options.with_mimalloc)\n        tc.variables[\"ARROW_JSON\"] = bool(self.options.with_json)\n        tc.variables[\"google_cloud_cpp_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_GCS\"] = bool(self.options.get_safe(\"with_gcs\", False))\n        tc.variables[\"BOOST_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"Protobuf_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_protobuf:\n            tc.variables[\"ARROW_PROTOBUF_USE_SHARED\"] = bool(self.dependencies[\"protobuf\"].options.shared)\n        tc.variables[\"gRPC_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_grpc:\n            tc.variables[\"ARROW_GRPC_USE_SHARED\"] = bool(self.dependencies[\"grpc\"].options.shared)\n\n        tc.variables[\"ARROW_USE_GLOG\"] = self.options.with_glog\n        tc.variables[\"GLOG_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_WITH_BACKTRACE\"] = bool(self.options.with_backtrace)\n        tc.variables[\"ARROW_WITH_BROTLI\"] = bool(self.options.with_brotli)\n        tc.variables[\"brotli_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_brotli:\n            tc.variables[\"ARROW_BROTLI_USE_SHARED\"] = bool(self.dependencies[\"brotli\"].options.shared)\n        tc.variables[\"gflags_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_gflags:\n            tc.variables[\"ARROW_GFLAGS_USE_SHARED\"] = bool(self.dependencies[\"gflags\"].options.shared)\n        tc.variables[\"ARROW_WITH_BZ2\"] = bool(self.options.with_bz2)\n        tc.variables[\"BZip2_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_bz2:\n            tc.variables[\"ARROW_BZ2_USE_SHARED\"] = bool(self.dependencies[\"bzip2\"].options.shared)\n        tc.variables[\"ARROW_WITH_LZ4\"] = bool(self.options.with_lz4)\n        tc.variables[\"lz4_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_lz4:\n            tc.variables[\"ARROW_LZ4_USE_SHARED\"] = bool(self.dependencies[\"lz4\"].options.shared)\n        tc.variables[\"ARROW_WITH_SNAPPY\"] = bool(self.options.with_snappy)\n        tc.variables[\"RapidJSON_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"Snappy_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_snappy:\n            tc.variables[\"ARROW_SNAPPY_USE_SHARED\"] = bool(self.dependencies[\"snappy\"].options.shared)\n        tc.variables[\"ARROW_WITH_ZLIB\"] = bool(self.options.with_zlib)\n        tc.variables[\"re2_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ZLIB_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"xsimd_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_WITH_ZSTD\"] = bool(self.options.with_zstd)\n        tc.variables[\"zstd_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_SIMD_LEVEL\"] = str(self.options.simd_level).upper()\n        tc.variables[\"ARROW_RUNTIME_SIMD_LEVEL\"] = str(self.options.runtime_simd_level).upper()\n        if self.options.with_zstd:\n            tc.variables[\"ARROW_ZSTD_USE_SHARED\"] = bool(self.dependencies[\"zstd\"].options.shared)\n        tc.variables[\"ORC_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_WITH_THRIFT\"] = bool(self.options.with_thrift)\n        tc.variables[\"Thrift_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_thrift:\n            tc.variables[\"THRIFT_VERSION\"] = bool(self.dependencies[\"thrift\"].ref.version) # a recent thrift does not require boost\n            tc.variables[\"ARROW_THRIFT_USE_SHARED\"] = bool(self.dependencies[\"thrift\"].options.shared)\n        tc.variables[\"ARROW_USE_OPENSSL\"] = self.options.with_openssl\n        if self.options.with_openssl:\n            tc.variables[\"OPENSSL_ROOT_DIR\"] = self.dependencies[\"openssl\"].package_folder.replace(\"\\\\\", \"/\")\n            tc.variables[\"ARROW_OPENSSL_USE_SHARED\"] = bool(self.dependencies[\"openssl\"].options.shared)\n        if self.options.with_boost:\n            tc.variables[\"ARROW_USE_BOOST\"] = True\n            tc.variables[\"ARROW_BOOST_USE_SHARED\"] = bool(self.dependencies[\"boost\"].options.shared)\n        tc.variables[\"ARROW_S3\"] = bool(self.options.with_s3)\n        tc.variables[\"AWSSDK_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_BUILD_UTILITIES\"] = bool(self.options.cli)\n        tc.variables[\"ARROW_BUILD_INTEGRATION\"] = False\n        tc.variables[\"ARROW_INSTALL_NAME_RPATH\"] = True\n        tc.variables[\"ARROW_BUILD_EXAMPLES\"] = False\n        tc.variables[\"ARROW_BUILD_TESTS\"] = False\n        tc.variables[\"ARROW_ENABLE_TIMING_TESTS\"] = False\n        tc.variables[\"ARROW_BUILD_BENCHMARKS\"] = False\n        tc.variables[\"LLVM_SOURCE\"] = \"SYSTEM\"\n        tc.variables[\"ARROW_WITH_UTF8PROC\"] = self.options.with_utf8proc\n        tc.variables[\"ARROW_BOOST_REQUIRED\"] = self.options.with_boost\n        tc.variables[\"utf8proc_SOURCE\"] = \"SYSTEM\"\n        if self.options.with_utf8proc:\n            tc.variables[\"ARROW_UTF8PROC_USE_SHARED\"] = bool(self.dependencies[\"utf8proc\"].options.shared)\n        tc.variables[\"BUILD_WARNING_LEVEL\"] = \"PRODUCTION\"\n        if is_msvc(self):\n            tc.variables[\"ARROW_USE_STATIC_CRT\"] = is_msvc_static_runtime(self)\n        if self.options.with_llvm:\n            tc.variables[\"LLVM_DIR\"] = self.dependencies[\"llvm-core\"].package_folder.replace(\"\\\\\", \"/\")\n\n        tc.cache_variables[\"CMAKE_PROJECT_arrow_INCLUDE\"] = os.path.join(self.source_folder, \"conan_cmake_project_include.cmake\")\n        tc.generate()\n\n        deps = CMakeDeps(self)\n        deps.generate()\n\n    def _patch_sources(self):\n        apply_conandata_patches(self)\n        if Version(self.version) < \"10.0.0\":\n            for filename in glob.glob(os.path.join(self.source_folder, \"cpp\", \"cmake_modules\", \"Find*.cmake\")):\n                if os.path.basename(filename) not in [\n                    \"FindArrow.cmake\",\n                    \"FindArrowAcero.cmake\",\n                    \"FindArrowCUDA.cmake\",\n                    \"FindArrowDataset.cmake\",\n                    \"FindArrowFlight.cmake\",\n                    \"FindArrowFlightSql.cmake\",\n                    \"FindArrowFlightTesting.cmake\",\n                    \"FindArrowPython.cmake\",\n                    \"FindArrowPythonFlight.cmake\",\n                    \"FindArrowSubstrait.cmake\",\n                    \"FindArrowTesting.cmake\",\n                    \"FindGandiva.cmake\",\n                    \"FindParquet.cmake\",\n                ]:\n                    os.remove(filename)\n\n    def build(self):\n        self._patch_sources()\n        cmake =CMake(self)\n        cmake.configure(build_script_folder=os.path.join(self.source_folder, \"cpp\"))\n        cmake.build()\n\n    def package(self):\n        copy(self, pattern=\"LICENSE.txt\", dst=os.path.join(self.package_folder, \"licenses\"), src=self.source_folder)\n        copy(self, pattern=\"NOTICE.txt\", dst=os.path.join(self.package_folder, \"licenses\"), src=self.source_folder)\n        cmake =CMake(self)\n        cmake.install()\n\n        rmdir(self, os.path.join(self.package_folder, \"lib\", \"cmake\"))\n        rmdir(self, os.path.join(self.package_folder, \"lib\", \"pkgconfig\"))\n        rmdir(self, os.path.join(self.package_folder, \"share\"))\n\n    def package_info(self):\n        # FIXME: fix CMake targets of components\n\n        self.cpp_info.set_property(\"cmake_file_name\", \"Arrow\")\n\n        suffix = \"_static\" if is_msvc(self) and not self.options.shared else \"\"\n\n        self.cpp_info.components[\"libarrow\"].set_property(\"pkg_config_name\", \"arrow\")\n        self.cpp_info.components[\"libarrow\"].libs = [f\"arrow{suffix}\"]\n        if not self.options.shared:\n            self.cpp_info.components[\"libarrow\"].defines = [\"ARROW_STATIC\"]\n            if self.settings.os in [\"Linux\", \"FreeBSD\"]:\n                self.cpp_info.components[\"libarrow\"].system_libs = [\"pthread\", \"m\", \"dl\", \"rt\"]\n\n        if self.options.parquet:\n            self.cpp_info.components[\"libparquet\"].set_property(\"pkg_config_name\", \"parquet\")\n            self.cpp_info.components[\"libparquet\"].libs = [f\"parquet{suffix}\"]\n            self.cpp_info.components[\"libparquet\"].requires = [\"libarrow\"]\n            if not self.options.shared:\n                self.cpp_info.components[\"libparquet\"].defines = [\"PARQUET_STATIC\"]\n\n        if self.options.get_safe(\"substrait\"):\n            self.cpp_info.components[\"libarrow_substrait\"].set_property(\"pkg_config_name\", \"arrow_substrait\")\n            self.cpp_info.components[\"libarrow_substrait\"].libs = [f\"arrow_substrait{suffix}\"]\n            self.cpp_info.components[\"libarrow_substrait\"].requires = [\"libparquet\", \"dataset\"]\n\n        # Plasma was deprecated in Arrow 12.0.0\n        del self.options.plasma\n\n        if self.options.acero:\n            self.cpp_info.components[\"libacero\"].libs = [f\"arrow_acero{suffix}\"]\n            self.cpp_info.components[\"libacero\"].names[\"cmake_find_package\"] = \"acero\"\n            self.cpp_info.components[\"libacero\"].names[\"cmake_find_package_multi\"] = \"acero\"\n            self.cpp_info.components[\"libacero\"].names[\"pkg_config\"] = \"acero\"\n            self.cpp_info.components[\"libacero\"].requires = [\"libarrow\"]\n\n        if self.options.gandiva:\n            self.cpp_info.components[\"libgandiva\"].set_property(\"pkg_config_name\", \"gandiva\")\n            self.cpp_info.components[\"libgandiva\"].libs = [f\"gandiva{suffix}\"]\n            self.cpp_info.components[\"libgandiva\"].requires = [\"libarrow\"]\n            if not self.options.shared:\n                self.cpp_info.components[\"libgandiva\"].defines = [\"GANDIVA_STATIC\"]\n\n        if self.options.with_flight_rpc:\n            self.cpp_info.components[\"libarrow_flight\"].set_property(\"pkg_config_name\", \"flight_rpc\")\n            self.cpp_info.components[\"libarrow_flight\"].libs = [f\"arrow_flight{suffix}\"]\n            self.cpp_info.components[\"libarrow_flight\"].requires = [\"libarrow\"]\n\n        if self.options.get_safe(\"with_flight_sql\"):\n            self.cpp_info.components[\"libarrow_flight_sql\"].set_property(\"pkg_config_name\", \"flight_sql\")\n            self.cpp_info.components[\"libarrow_flight_sql\"].libs = [f\"arrow_flight_sql{suffix}\"]\n            self.cpp_info.components[\"libarrow_flight_sql\"].requires = [\"libarrow\", \"libarrow_flight\"]\n\n        if self.options.dataset_modules:\n            self.cpp_info.components[\"dataset\"].libs = [\"arrow_dataset\"]\n            if self.options.parquet:\n                self.cpp_info.components[\"dataset\"].requires = [\"libparquet\"]\n\n        if self.options.cli and (self.options.with_cuda or self.options.with_flight_rpc or self.options.parquet):\n            binpath = os.path.join(self.package_folder, \"bin\")\n            self.output.info(f\"Appending PATH env var: {binpath}\")\n            self.env_info.PATH.append(binpath)\n\n        if self.options.with_boost:\n            if self.options.gandiva:\n                # FIXME: only filesystem component is used\n                self.cpp_info.components[\"libgandiva\"].requires.append(\"boost::boost\")\n            if self.options.parquet and self.settings.compiler == \"gcc\" and self.settings.compiler.version < Version(\"4.9\"):\n                self.cpp_info.components[\"libparquet\"].requires.append(\"boost::boost\")\n            # FIXME: only headers components is used\n            self.cpp_info.components[\"libarrow\"].requires.append(\"boost::boost\")\n        if self.options.with_openssl:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"openssl::openssl\")\n        if self.options.with_gflags:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"gflags::gflags\")\n        if self.options.with_glog:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"glog::glog\")\n        if self.options.with_jemalloc:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"jemalloc::jemalloc\")\n        if self.options.with_mimalloc:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"mimalloc::mimalloc\")\n        if self.options.with_re2:\n            if self.options.gandiva:\n                self.cpp_info.components[\"libgandiva\"].requires.append(\"re2::re2\")\n            if self.options.parquet:\n                self.cpp_info.components[\"libparquet\"].requires.append(\"re2::re2\")\n            self.cpp_info.components[\"libarrow\"].requires.append(\"re2::re2\")\n        if self.options.with_llvm:\n            self.cpp_info.components[\"libgandiva\"].requires.append(\"llvm-core::llvm-core\")\n        if self.options.with_protobuf:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"protobuf::protobuf\")\n        if self.options.with_utf8proc:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"utf8proc::utf8proc\")\n        if self.options.with_thrift:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"thrift::thrift\")\n        if self.options.with_backtrace:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"libbacktrace::libbacktrace\")\n        if self.options.with_cuda:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"cuda::cuda\")\n        if self._requires_rapidjson():\n            self.cpp_info.components[\"libarrow\"].requires.append(\"rapidjson::rapidjson\")\n        if self.options.with_s3:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"aws-sdk-cpp::s3\")\n        if self.options.get_safe(\"with_gcs\"):\n            self.cpp_info.components[\"libarrow\"].requires.append(\"google-cloud-cpp::storage\")\n        if self.options.with_orc:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"orc::orc\")\n        if self.options.get_safe(\"with_opentelemetry\"):\n            self.cpp_info.components[\"libarrow\"].requires.append(\"opentelemetry-cpp::opentelemetry-cpp\")\n        if self.options.with_brotli:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"brotli::brotli\")\n        if self.options.with_bz2:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"bzip2::bzip2\")\n        if self.options.with_lz4:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"lz4::lz4\")\n        if self.options.with_snappy:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"snappy::snappy\")\n        if self.options.get_safe(\"simd_level\") != None or self.options.get_safe(\"runtime_simd_level\") != None:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"xsimd::xsimd\")\n        if self.options.with_zlib:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"zlib::zlib\")\n        if self.options.with_zstd:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"zstd::zstd\")\n        if self.options.with_boost:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"boost::boost\")\n        if self.options.with_grpc:\n            self.cpp_info.components[\"libarrow\"].requires.append(\"grpc::grpc\")\n        if self.options.with_flight_rpc:\n            self.cpp_info.components[\"libarrow_flight\"].requires.append(\"protobuf::protobuf\")\n\n        # TODO: to remove in conan v2\n        self.cpp_info.filenames[\"cmake_find_package\"] = \"Arrow\"\n        self.cpp_info.filenames[\"cmake_find_package_multi\"] = \"Arrow\"\n        self.cpp_info.components[\"libarrow\"].names[\"cmake_find_package\"] = \"arrow\"\n        self.cpp_info.components[\"libarrow\"].names[\"cmake_find_package_multi\"] = \"arrow\"\n        if self.options.parquet:\n            self.cpp_info.components[\"libparquet\"].names[\"cmake_find_package\"] = \"parquet\"\n            self.cpp_info.components[\"libparquet\"].names[\"cmake_find_package_multi\"] = \"parquet\"\n        if self.options.get_safe(\"substrait\"):\n            self.cpp_info.components[\"libarrow_substrait\"].names[\"cmake_find_package\"] = \"arrow_substrait\"\n            self.cpp_info.components[\"libarrow_substrait\"].names[\"cmake_find_package_multi\"] = \"arrow_substrait\"\n        if self.options.gandiva:\n            self.cpp_info.components[\"libgandiva\"].names[\"cmake_find_package\"] = \"gandiva\"\n            self.cpp_info.components[\"libgandiva\"].names[\"cmake_find_package_multi\"] = \"gandiva\"\n        if self.options.with_flight_rpc:\n            self.cpp_info.components[\"libarrow_flight\"].names[\"cmake_find_package\"] = \"flight_rpc\"\n            self.cpp_info.components[\"libarrow_flight\"].names[\"cmake_find_package_multi\"] = \"flight_rpc\"\n        if self.options.get_safe(\"with_flight_sql\"):\n            self.cpp_info.components[\"libarrow_flight_sql\"].names[\"cmake_find_package\"] = \"flight_sql\"\n            self.cpp_info.components[\"libarrow_flight_sql\"].names[\"cmake_find_package_multi\"] = \"flight_sql\"\n        if self.options.cli and (self.options.with_cuda or self.options.with_flight_rpc or self.options.parquet):\n            self.env_info.PATH.append(os.path.join(self.package_folder, \"bin\"))\n"}