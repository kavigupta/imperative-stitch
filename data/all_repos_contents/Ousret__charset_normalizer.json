{"setup.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nfrom re import search\n\nfrom setuptools import setup\n\n\ndef get_version():\n    with open('charset_normalizer/version.py') as version_file:\n        return search(r\"\"\"__version__\\s+=\\s+(['\"])(?P<version>.+?)\\1\"\"\",\n                      version_file.read()).group('version')\n\n\nUSE_MYPYC = False\n\nif len(sys.argv) > 1 and sys.argv[1] == \"--use-mypyc\":\n    sys.argv.pop(1)\n    USE_MYPYC = True\nif os.getenv(\"CHARSET_NORMALIZER_USE_MYPYC\", None) == \"1\":\n    USE_MYPYC = True\n\nif USE_MYPYC:\n    from mypyc.build import mypycify\n\n    MYPYC_MODULES = mypycify([\n        \"charset_normalizer/md.py\",\n    ], debug_level=\"0\")\nelse:\n    MYPYC_MODULES = None\n\nsetup(\n    name=\"charset-normalizer\",\n    version=get_version(),\n    ext_modules=MYPYC_MODULES\n)\n", "bin/coverage.py": "#!/bin/python\nfrom glob import glob\nfrom os.path import isdir\nfrom sys import argv\nfrom typing import List\nimport argparse\n\nfrom charset_normalizer import from_path, __version__\nfrom charset_normalizer.utils import iana_name\n\nfrom os import sep\n\n\ndef calc_equivalence(content: bytes, cp_a: str, cp_b: str):\n    str_a = content.decode(cp_a)\n    str_b = content.decode(cp_b)\n\n    character_count = len(str_a)\n    diff_character_count = sum(\n        chr_a != chr_b for chr_a, chr_b in zip(str_a, str_b)\n    )\n\n\n    return 1. - (diff_character_count / character_count)\n\n\ndef cli_coverage(arguments: List[str]):\n    parser = argparse.ArgumentParser(\n        description=\"Embedded detection success coverage script checker for Charset-Normalizer\"\n    )\n\n    parser.add_argument('-p', '--with-preemptive', action=\"store_true\", default=False, dest='preemptive',\n                        help='Enable the preemptive scan behaviour during coverage check')\n    parser.add_argument('-c', '--coverage', action=\"store\", default=90, type=int, dest='coverage',\n                        help=\"Define the minimum acceptable coverage to succeed\")\n\n    args = parser.parse_args(arguments)\n\n    if not isdir(\"./char-dataset\"):\n        print(\"This script require https://github.com/Ousret/char-dataset to be cloned on package root directory\")\n        exit(1)\n\n    print(f\"> using charset-normalizer {__version__}\")\n\n    success_count = 0\n    total_count = 0\n\n    for tbt_path in sorted(glob(\"./char-dataset/**/*.*\")):\n\n        expected_encoding = tbt_path.split(sep)[-2]\n        total_count += 1\n\n        results = from_path(\n            tbt_path,\n            preemptive_behaviour=args.preemptive\n        )\n\n        if expected_encoding == \"None\" and len(results) == 0:\n            print(\"\u2705\u2705 '{}'\".format(tbt_path))\n            success_count += 1\n            continue\n\n        if len(results) == 0:\n            print(\"\u26a1\u26a1 '{}' (nothing)\".format(tbt_path))\n            continue\n\n        result = results.best()\n\n        if expected_encoding in result.could_be_from_charset or iana_name(expected_encoding) in result.could_be_from_charset:\n            print(\"\u2705\u2705 '{}'\".format(tbt_path))\n            success_count += 1\n            continue\n\n        calc_eq = calc_equivalence(result.raw, expected_encoding, result.encoding)\n\n        if calc_eq >= 0.98:\n            success_count += 1\n            print(\"\ufe0f\u2705 \ufe0f'{}' (got '{}' but equivalence {} %)\".format(tbt_path, result.encoding, round(calc_eq * 100., 3)))\n            continue\n\n        print(\"\u26a1 '{}' (got '{}')\".format(tbt_path, result.encoding))\n\n    success_ratio = round(success_count / total_count, 2) * 100.\n\n    print(\"Total EST coverage = {} % ({} / {} files)\".format(success_ratio, success_count, total_count))\n\n    return 0 if success_ratio >= args.coverage else 1\n\n\nif __name__ == \"__main__\":\n    exit(\n        cli_coverage(\n            argv[1:]\n        )\n    )\n", "bin/serve.py": "from flask import Flask, jsonify, send_from_directory\nfrom glob import glob\n\napp = Flask(__name__)\n\n\n@app.route('/raw/<path:path>')\ndef read_file(path):\n    return send_from_directory('../char-dataset', path, as_attachment=True), 200, {\"Content-Type\": \"text/plain\"}\n\n\n@app.route(\"/\")\ndef read_targets():\n    return jsonify(\n        [\n            el.replace(\"./char-dataset\", \"/raw\").replace(\"\\\\\", \"/\") for el in sorted(glob(\"./char-dataset/**/*\"))\n        ]\n    )\n\n\n@app.route(\"/edge/empty/plain\")\ndef read_empty_response_plain():\n    return b\"\", 200, {\"Content-Type\": \"text/plain\"}\n\n\n@app.route(\"/edge/empty/json\")\ndef read_empty_response_json():\n    return b\"{}\", 200, {\"Content-Type\": \"application/json\"}\n\n\n@app.route(\"/edge/gb18030/json\")\ndef read_gb18030_response_json():\n    return '{\"abc\": \"\u6211\u6ca1\u6709\u57cb\u6028\uff0c\u78cb\u7823\u7684\u53ea\u662f\u4e00\u4e9b\u65f6\u95f4\u3002\u3002\u4eca\u89c0\u4fd7\u58eb\u4e4b\u8ad6\u4e5f\uff0c\u4ee5\u65cf\u8209\u5fb7\uff0c\u4ee5\u4f4d\u547d\u8ce2\uff0c\u8332\u53ef\u8b02\u5f97\u8ad6\u4e4b\u4e00\u9ad4\u77e3\uff0c\u800c\u672a\u7372\u81f3\u8ad6\u4e4b\u6dd1\u771f\u4e5f\u3002\"}'.encode(\"gb18030\"), 200, {\"Content-Type\": \"application/json\"}\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"127.0.0.1\", port=8080)\n", "bin/bc.py": "#!/bin/python\nfrom glob import glob\nfrom os.path import isdir\nfrom sys import argv\nfrom typing import List\nimport argparse\n\nfrom charset_normalizer import detect as tbt_detect\nfrom chardet import detect as chardet_detect\n\nfrom charset_normalizer.utils import iana_name\n\n\ndef calc_equivalence(content: bytes, cp_a: str, cp_b: str):\n    try:\n        str_a = content.decode(cp_a)\n        str_b = content.decode(cp_b)\n    except UnicodeDecodeError:\n        return 0.\n\n    character_count = len(str_a)\n    diff_character_count = sum(\n        chr_a != chr_b for chr_a, chr_b in zip(str_a, str_b)\n    )\n\n    return 1. - (diff_character_count / character_count)\n\n\ndef cli_bc(arguments: List[str]):\n    parser = argparse.ArgumentParser(\n        description=\"BC script checker for Charset-Normalizer with Chardet\"\n    )\n\n    parser.add_argument('-c', '--coverage', action=\"store\", default=85, type=int, dest='coverage',\n                        help=\"Define the minimum acceptable coverage to succeed\")\n\n    args = parser.parse_args(arguments)\n\n    if not isdir(\"./char-dataset\"):\n        print(\"This script require https://github.com/Ousret/char-dataset to be cloned on package root directory\")\n        exit(1)\n\n    success_count = 0\n    total_count = 0\n\n    for tbt_path in sorted(glob(\"./char-dataset/**/*.*\")):\n        total_count += 1\n\n        with open(tbt_path, \"rb\") as fp:\n            content = fp.read()\n\n        chardet_result = chardet_detect(content)\n        chardet_encoding = chardet_result['encoding']\n\n        charset_normalizer_result = tbt_detect(content)\n        charset_normalizer_encoding = charset_normalizer_result['encoding']\n\n        if [chardet_encoding, charset_normalizer_encoding].count(None) == 1:\n            print(\"\u26a1\u26a1 '{}' (BC-Break) New('{}') vs Legacy('{}')\".format(tbt_path, charset_normalizer_encoding, chardet_encoding))\n            continue\n\n        if charset_normalizer_encoding == chardet_encoding:\n            success_count += 1\n            print(\"\u2705\u2705 '{}' (BC)\".format(tbt_path))\n            continue\n\n        if (chardet_encoding is None and charset_normalizer_encoding is None) or (iana_name(chardet_encoding, False) == iana_name(charset_normalizer_encoding, False)):\n            success_count += 1\n            print(\"\u2705\u2705 '{}' (BC)\".format(tbt_path))\n            continue\n\n        calc_eq = calc_equivalence(content, chardet_encoding, charset_normalizer_encoding)\n\n        if calc_eq >= 0.98:\n            success_count += 1\n            print(\"\ufe0f\u2705 \ufe0f'{}' (got '{}' but eq {} WITH {} %)\".format(tbt_path, charset_normalizer_encoding, chardet_encoding, round(calc_eq * 100., 3)))\n            continue\n\n        print(\"\u26a1\u26a1 '{}' (BC-Break) New('{}') vs Legacy('{}')\".format(tbt_path, charset_normalizer_encoding, chardet_encoding))\n\n    success_ratio = round(success_count / total_count, 2) * 100.\n\n    print(\"Total EST BC = {} % ({} / {} files)\".format(success_ratio, success_count, total_count))\n\n    return 0 if success_ratio >= args.coverage else 1\n\n\nif __name__ == \"__main__\":\n    exit(\n        cli_bc(\n            argv[1:]\n        )\n    )\n", "bin/integration.py": "from requests import get, __version__\nfrom typing import List\nfrom charset_normalizer import detect, __version__ as __version_cn__\n\nif __name__ == \"__main__\":\n\n    print(f\"requests {__version__}\")\n    print(f\"charset_normalizer {__version_cn__}\")\n\n    files: List[str] = get(\"http://127.0.0.1:8080/\").json()\n\n    print(\"## Testing with actual files\")\n\n    for file in files:\n        r = get(\n            \"http://127.0.0.1:8080/\" + file\n        )\n\n        if r.ok is False:\n            print(f\"Unable to retrieve '{file}' | HTTP/{r.status_code}\")\n            exit(1)\n\n        expected_encoding = detect(r.content)[\"encoding\"]\n\n        if expected_encoding != r.apparent_encoding:\n            print(f\"Integration test failed | File '{file}' | Expected '{expected_encoding}' got '{r.apparent_encoding}'\")\n            exit(1)\n\n        print(f\"\u2705\u2705 '{file}' OK\")\n\n    print(\"## Testing with edge cases\")\n\n    # Should NOT crash\n    get(\"http://127.0.0.1:8080/edge/empty/json\").json()\n\n    print(\"\u2705\u2705 Empty JSON OK\")\n\n    if get(\"http://127.0.0.1:8080/edge/empty/plain\").apparent_encoding != \"utf-8\":\n        print(\"Empty payload SHOULD not return apparent_encoding != UTF-8\")\n        exit(1)\n\n    print(\"\u2705\u2705 Empty Plain Text OK\")\n\n    r = get(\"http://127.0.0.1:8080/edge/gb18030/json\")\n\n    if r.apparent_encoding != \"GB18030\":\n        print(\"JSON Basic Detection FAILURE (/edge/gb18030/json)\")\n        exit(1)\n\n    r.json()\n\n    print(\"\u2705\u2705 GB18030 JSON Encoded OK\")\n\n    print(\"Integration tests passed!\")\n", "bin/performance.py": "#!/bin/python\nfrom glob import glob\nfrom time import perf_counter_ns\nimport argparse\nfrom sys import argv\nfrom os.path import isdir\n\nfrom charset_normalizer import detect\nfrom chardet import detect as chardet_detect\n\nfrom statistics import mean, stdev\nfrom math import ceil\n\n\ndef calc_percentile(data, percentile):\n    n = len(data)\n    p = n * percentile / 100\n    sorted_data = sorted(data)\n\n    return sorted_data[int(p)] if p.is_integer() else sorted_data[int(ceil(p)) - 1]\n\n\ndef performance_compare(arguments):\n    parser = argparse.ArgumentParser(\n        description=\"Performance CI/CD check for Charset-Normalizer\"\n    )\n\n    parser.add_argument(\n        \"-s\",\n        \"--size-increase\",\n        action=\"store\",\n        default=1,\n        type=int,\n        dest=\"size_coeff\",\n        help=\"Apply artificial size increase to challenge the detection mechanism further\",\n    )\n\n    args = parser.parse_args(arguments)\n\n    if not isdir(\"./char-dataset\"):\n        print(\n            \"This script require https://github.com/Ousret/char-dataset to be cloned on package root directory\"\n        )\n        exit(1)\n\n    chardet_results = []\n    charset_normalizer_results = []\n\n    file_list = sorted(glob(\"./char-dataset/**/*.*\"))\n    total_files = len(file_list)\n\n    for idx, tbt_path in enumerate(file_list):\n        with open(tbt_path, \"rb\") as fp:\n            content = fp.read() * args.size_coeff\n\n        before = perf_counter_ns()\n        chardet_detect(content)\n        chardet_time = round((perf_counter_ns() - before) / 1000000000, 5)\n        chardet_results.append(chardet_time)\n\n        before = perf_counter_ns()\n        detect(content)\n        charset_normalizer_time = round((perf_counter_ns() - before) / 1000000000, 5)\n        charset_normalizer_results.append(charset_normalizer_time)\n\n        charset_normalizer_time = charset_normalizer_time or 0.000005\n        cn_faster = (chardet_time / charset_normalizer_time) * 100 - 100\n        print(\n            f\"{idx+1:>3}/{total_files} {tbt_path:<82} C:{chardet_time:.5f}  CN:{charset_normalizer_time:.5f}  {cn_faster:.1f} %\"\n        )\n\n    # Print the top 10 rows with the slowest execution time\n    print(\n        f\"\\n{'-' * 102}\\nTop 10 rows with the slowest execution time of charset_normalizer:\\n\"\n    )\n    sorted_results = sorted(\n        enumerate(charset_normalizer_results), key=lambda x: x[1], reverse=True\n    )\n    for idx, time in sorted_results[:10]:\n        tbt_path = file_list[idx]\n        print(f\"{idx+1:>3}/{total_files} {tbt_path:<82}  CN:{time:.5f}\")\n\n    # Print charset normalizer statistics\n    min_time = min(charset_normalizer_results)\n    max_time = max(charset_normalizer_results)\n    stdev_time = stdev(charset_normalizer_results)\n    mean_time = mean(charset_normalizer_results)\n    cv = (stdev_time / mean_time) * 100  # Coefficient of variation\n    print(f\"\\n{'-' * 102}\\nCharset Normalizer statistics:\\n\")\n    print(f\"Minimum Execution Time: {min_time:.5f} seconds\")\n    print(f\"Maximum Execution Time: {max_time:.5f} seconds\")\n    print(f\"Mean Execution Time: {mean_time:.5f} seconds\")\n    print(f\"Standard Deviation: {stdev_time:.5f} seconds\")\n    print(f\"Coefficient of Variation (CV): {cv:.1f} %\")\n\n    # Print comparison statistics for chardet and charset normalizer\n    chardet_avg_delay = round(mean(chardet_results) * 1000)\n    chardet_99p = round(calc_percentile(chardet_results, 99) * 1000)\n    chardet_95p = round(calc_percentile(chardet_results, 95) * 1000)\n    chardet_50p = round(calc_percentile(chardet_results, 50) * 1000)\n\n    charset_normalizer_avg_delay = round(mean(charset_normalizer_results) * 1000)\n    charset_normalizer_99p = round(\n        calc_percentile(charset_normalizer_results, 99) * 1000\n    )\n    charset_normalizer_95p = round(\n        calc_percentile(charset_normalizer_results, 95) * 1000\n    )\n    charset_normalizer_50p = round(\n        calc_percentile(charset_normalizer_results, 50) * 1000\n    )\n\n    # mypyc can offer performance ~1ms in the 50p. When eq to 0 assume 1 due to imprecise nature of this test.\n    if charset_normalizer_50p == 0:\n        charset_normalizer_50p = 1\n\n    print(f\"\\n{'-' * 102}\\nCharset Normalizer vs Chardet statistics:\\n\")\n\n    print(\"------------------------------\")\n    print(\"--> Chardet Conclusions\")\n    print(\"   --> Avg: \" + str(chardet_avg_delay) + \"ms\")\n    print(\"   --> 99th: \" + str(chardet_99p) + \"ms\")\n    print(\"   --> 95th: \" + str(chardet_95p) + \"ms\")\n    print(\"   --> 50th: \" + str(chardet_50p) + \"ms\")\n\n    print(\"------------------------------\")\n    print(\"--> Charset-Normalizer Conclusions\")\n    print(\"   --> Avg: \" + str(charset_normalizer_avg_delay) + \"ms\")\n    print(\"   --> 99th: \" + str(charset_normalizer_99p) + \"ms\")\n    print(\"   --> 95th: \" + str(charset_normalizer_95p) + \"ms\")\n    print(\"   --> 50th: \" + str(charset_normalizer_50p) + \"ms\")\n\n    print(\"------------------------------\")\n    print(\"--> Charset-Normalizer / Chardet: Performance \u0421omparison\")\n    print(\n        \"   --> Avg: x\"\n        + str(round(chardet_avg_delay / charset_normalizer_avg_delay, 2))\n    )\n    print(\"   --> 99th: x\" + str(round(chardet_99p / charset_normalizer_99p, 2)))\n    print(\"   --> 95th: x\" + str(round(chardet_95p / charset_normalizer_95p, 2)))\n    print(\"   --> 50th: x\" + str(round(chardet_50p / charset_normalizer_50p, 2)))\n\n    return (\n        0\n        if chardet_avg_delay > charset_normalizer_avg_delay\n        and chardet_99p > charset_normalizer_99p\n        else 1\n    )\n\n\nif __name__ == \"__main__\":\n    exit(performance_compare(argv[1:]))\n", "charset_normalizer/legacy.py": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, Optional\nfrom warnings import warn\n\nfrom .api import from_bytes\nfrom .constant import CHARDET_CORRESPONDENCE\n\n# TODO: remove this check when dropping Python 3.7 support\nif TYPE_CHECKING:\n    from typing_extensions import TypedDict\n\n    class ResultDict(TypedDict):\n        encoding: Optional[str]\n        language: str\n        confidence: Optional[float]\n\n\ndef detect(\n    byte_str: bytes, should_rename_legacy: bool = False, **kwargs: Any\n) -> ResultDict:\n    \"\"\"\n    chardet legacy method\n    Detect the encoding of the given byte string. It should be mostly backward-compatible.\n    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)\n    This function is deprecated and should be used to migrate your project easily, consult the documentation for\n    further information. Not planned for removal.\n\n    :param byte_str:     The byte sequence to examine.\n    :param should_rename_legacy:  Should we rename legacy encodings\n                                  to their more modern equivalents?\n    \"\"\"\n    if len(kwargs):\n        warn(\n            f\"charset-normalizer disregard arguments '{','.join(list(kwargs.keys()))}' in legacy function detect()\"\n        )\n\n    if not isinstance(byte_str, (bytearray, bytes)):\n        raise TypeError(  # pragma: nocover\n            \"Expected object of type bytes or bytearray, got: \"\n            \"{0}\".format(type(byte_str))\n        )\n\n    if isinstance(byte_str, bytearray):\n        byte_str = bytes(byte_str)\n\n    r = from_bytes(byte_str).best()\n\n    encoding = r.encoding if r is not None else None\n    language = r.language if r is not None and r.language != \"Unknown\" else \"\"\n    confidence = 1.0 - r.chaos if r is not None else None\n\n    # Note: CharsetNormalizer does not return 'UTF-8-SIG' as the sig get stripped in the detection/normalization process\n    # but chardet does return 'utf-8-sig' and it is a valid codec name.\n    if r is not None and encoding == \"utf_8\" and r.bom:\n        encoding += \"_sig\"\n\n    if should_rename_legacy is False and encoding in CHARDET_CORRESPONDENCE:\n        encoding = CHARDET_CORRESPONDENCE[encoding]\n\n    return {\n        \"encoding\": encoding,\n        \"language\": language,\n        \"confidence\": confidence,\n    }\n", "charset_normalizer/api.py": "import logging\nfrom os import PathLike\nfrom typing import BinaryIO, List, Optional, Set, Union\n\nfrom .cd import (\n    coherence_ratio,\n    encoding_languages,\n    mb_encoding_languages,\n    merge_coherence_ratios,\n)\nfrom .constant import IANA_SUPPORTED, TOO_BIG_SEQUENCE, TOO_SMALL_SEQUENCE, TRACE\nfrom .md import mess_ratio\nfrom .models import CharsetMatch, CharsetMatches\nfrom .utils import (\n    any_specified_encoding,\n    cut_sequence_chunks,\n    iana_name,\n    identify_sig_or_bom,\n    is_cp_similar,\n    is_multi_byte_encoding,\n    should_strip_sig_or_bom,\n)\n\n# Will most likely be controversial\n# logging.addLevelName(TRACE, \"TRACE\")\nlogger = logging.getLogger(\"charset_normalizer\")\nexplain_handler = logging.StreamHandler()\nexplain_handler.setFormatter(\n    logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n)\n\n\ndef from_bytes(\n    sequences: Union[bytes, bytearray],\n    steps: int = 5,\n    chunk_size: int = 512,\n    threshold: float = 0.2,\n    cp_isolation: Optional[List[str]] = None,\n    cp_exclusion: Optional[List[str]] = None,\n    preemptive_behaviour: bool = True,\n    explain: bool = False,\n    language_threshold: float = 0.1,\n    enable_fallback: bool = True,\n) -> CharsetMatches:\n    \"\"\"\n    Given a raw bytes sequence, return the best possibles charset usable to render str objects.\n    If there is no results, it is a strong indicator that the source is binary/not text.\n    By default, the process will extract 5 blocks of 512o each to assess the mess and coherence of a given sequence.\n    And will give up a particular code page after 20% of measured mess. Those criteria are customizable at will.\n\n    The preemptive behavior DOES NOT replace the traditional detection workflow, it prioritize a particular code page\n    but never take it for granted. Can improve the performance.\n\n    You may want to focus your attention to some code page or/and not others, use cp_isolation and cp_exclusion for that\n    purpose.\n\n    This function will strip the SIG in the payload/sequence every time except on UTF-16, UTF-32.\n    By default the library does not setup any handler other than the NullHandler, if you choose to set the 'explain'\n    toggle to True it will alter the logger configuration to add a StreamHandler that is suitable for debugging.\n    Custom logging format and handler can be set manually.\n    \"\"\"\n\n    if not isinstance(sequences, (bytearray, bytes)):\n        raise TypeError(\n            \"Expected object of type bytes or bytearray, got: {0}\".format(\n                type(sequences)\n            )\n        )\n\n    if explain:\n        previous_logger_level: int = logger.level\n        logger.addHandler(explain_handler)\n        logger.setLevel(TRACE)\n\n    length: int = len(sequences)\n\n    if length == 0:\n        logger.debug(\"Encoding detection on empty bytes, assuming utf_8 intention.\")\n        if explain:\n            logger.removeHandler(explain_handler)\n            logger.setLevel(previous_logger_level or logging.WARNING)\n        return CharsetMatches([CharsetMatch(sequences, \"utf_8\", 0.0, False, [], \"\")])\n\n    if cp_isolation is not None:\n        logger.log(\n            TRACE,\n            \"cp_isolation is set. use this flag for debugging purpose. \"\n            \"limited list of encoding allowed : %s.\",\n            \", \".join(cp_isolation),\n        )\n        cp_isolation = [iana_name(cp, False) for cp in cp_isolation]\n    else:\n        cp_isolation = []\n\n    if cp_exclusion is not None:\n        logger.log(\n            TRACE,\n            \"cp_exclusion is set. use this flag for debugging purpose. \"\n            \"limited list of encoding excluded : %s.\",\n            \", \".join(cp_exclusion),\n        )\n        cp_exclusion = [iana_name(cp, False) for cp in cp_exclusion]\n    else:\n        cp_exclusion = []\n\n    if length <= (chunk_size * steps):\n        logger.log(\n            TRACE,\n            \"override steps (%i) and chunk_size (%i) as content does not fit (%i byte(s) given) parameters.\",\n            steps,\n            chunk_size,\n            length,\n        )\n        steps = 1\n        chunk_size = length\n\n    if steps > 1 and length / steps < chunk_size:\n        chunk_size = int(length / steps)\n\n    is_too_small_sequence: bool = len(sequences) < TOO_SMALL_SEQUENCE\n    is_too_large_sequence: bool = len(sequences) >= TOO_BIG_SEQUENCE\n\n    if is_too_small_sequence:\n        logger.log(\n            TRACE,\n            \"Trying to detect encoding from a tiny portion of ({}) byte(s).\".format(\n                length\n            ),\n        )\n    elif is_too_large_sequence:\n        logger.log(\n            TRACE,\n            \"Using lazy str decoding because the payload is quite large, ({}) byte(s).\".format(\n                length\n            ),\n        )\n\n    prioritized_encodings: List[str] = []\n\n    specified_encoding: Optional[str] = (\n        any_specified_encoding(sequences) if preemptive_behaviour else None\n    )\n\n    if specified_encoding is not None:\n        prioritized_encodings.append(specified_encoding)\n        logger.log(\n            TRACE,\n            \"Detected declarative mark in sequence. Priority +1 given for %s.\",\n            specified_encoding,\n        )\n\n    tested: Set[str] = set()\n    tested_but_hard_failure: List[str] = []\n    tested_but_soft_failure: List[str] = []\n\n    fallback_ascii: Optional[CharsetMatch] = None\n    fallback_u8: Optional[CharsetMatch] = None\n    fallback_specified: Optional[CharsetMatch] = None\n\n    results: CharsetMatches = CharsetMatches()\n\n    sig_encoding, sig_payload = identify_sig_or_bom(sequences)\n\n    if sig_encoding is not None:\n        prioritized_encodings.append(sig_encoding)\n        logger.log(\n            TRACE,\n            \"Detected a SIG or BOM mark on first %i byte(s). Priority +1 given for %s.\",\n            len(sig_payload),\n            sig_encoding,\n        )\n\n    prioritized_encodings.append(\"ascii\")\n\n    if \"utf_8\" not in prioritized_encodings:\n        prioritized_encodings.append(\"utf_8\")\n\n    for encoding_iana in prioritized_encodings + IANA_SUPPORTED:\n        if cp_isolation and encoding_iana not in cp_isolation:\n            continue\n\n        if cp_exclusion and encoding_iana in cp_exclusion:\n            continue\n\n        if encoding_iana in tested:\n            continue\n\n        tested.add(encoding_iana)\n\n        decoded_payload: Optional[str] = None\n        bom_or_sig_available: bool = sig_encoding == encoding_iana\n        strip_sig_or_bom: bool = bom_or_sig_available and should_strip_sig_or_bom(\n            encoding_iana\n        )\n\n        if encoding_iana in {\"utf_16\", \"utf_32\"} and not bom_or_sig_available:\n            logger.log(\n                TRACE,\n                \"Encoding %s won't be tested as-is because it require a BOM. Will try some sub-encoder LE/BE.\",\n                encoding_iana,\n            )\n            continue\n        if encoding_iana in {\"utf_7\"} and not bom_or_sig_available:\n            logger.log(\n                TRACE,\n                \"Encoding %s won't be tested as-is because detection is unreliable without BOM/SIG.\",\n                encoding_iana,\n            )\n            continue\n\n        try:\n            is_multi_byte_decoder: bool = is_multi_byte_encoding(encoding_iana)\n        except (ModuleNotFoundError, ImportError):\n            logger.log(\n                TRACE,\n                \"Encoding %s does not provide an IncrementalDecoder\",\n                encoding_iana,\n            )\n            continue\n\n        try:\n            if is_too_large_sequence and is_multi_byte_decoder is False:\n                str(\n                    sequences[: int(50e4)]\n                    if strip_sig_or_bom is False\n                    else sequences[len(sig_payload) : int(50e4)],\n                    encoding=encoding_iana,\n                )\n            else:\n                decoded_payload = str(\n                    sequences\n                    if strip_sig_or_bom is False\n                    else sequences[len(sig_payload) :],\n                    encoding=encoding_iana,\n                )\n        except (UnicodeDecodeError, LookupError) as e:\n            if not isinstance(e, LookupError):\n                logger.log(\n                    TRACE,\n                    \"Code page %s does not fit given bytes sequence at ALL. %s\",\n                    encoding_iana,\n                    str(e),\n                )\n            tested_but_hard_failure.append(encoding_iana)\n            continue\n\n        similar_soft_failure_test: bool = False\n\n        for encoding_soft_failed in tested_but_soft_failure:\n            if is_cp_similar(encoding_iana, encoding_soft_failed):\n                similar_soft_failure_test = True\n                break\n\n        if similar_soft_failure_test:\n            logger.log(\n                TRACE,\n                \"%s is deemed too similar to code page %s and was consider unsuited already. Continuing!\",\n                encoding_iana,\n                encoding_soft_failed,\n            )\n            continue\n\n        r_ = range(\n            0 if not bom_or_sig_available else len(sig_payload),\n            length,\n            int(length / steps),\n        )\n\n        multi_byte_bonus: bool = (\n            is_multi_byte_decoder\n            and decoded_payload is not None\n            and len(decoded_payload) < length\n        )\n\n        if multi_byte_bonus:\n            logger.log(\n                TRACE,\n                \"Code page %s is a multi byte encoding table and it appear that at least one character \"\n                \"was encoded using n-bytes.\",\n                encoding_iana,\n            )\n\n        max_chunk_gave_up: int = int(len(r_) / 4)\n\n        max_chunk_gave_up = max(max_chunk_gave_up, 2)\n        early_stop_count: int = 0\n        lazy_str_hard_failure = False\n\n        md_chunks: List[str] = []\n        md_ratios = []\n\n        try:\n            for chunk in cut_sequence_chunks(\n                sequences,\n                encoding_iana,\n                r_,\n                chunk_size,\n                bom_or_sig_available,\n                strip_sig_or_bom,\n                sig_payload,\n                is_multi_byte_decoder,\n                decoded_payload,\n            ):\n                md_chunks.append(chunk)\n\n                md_ratios.append(\n                    mess_ratio(\n                        chunk,\n                        threshold,\n                        explain is True and 1 <= len(cp_isolation) <= 2,\n                    )\n                )\n\n                if md_ratios[-1] >= threshold:\n                    early_stop_count += 1\n\n                if (early_stop_count >= max_chunk_gave_up) or (\n                    bom_or_sig_available and strip_sig_or_bom is False\n                ):\n                    break\n        except (\n            UnicodeDecodeError\n        ) as e:  # Lazy str loading may have missed something there\n            logger.log(\n                TRACE,\n                \"LazyStr Loading: After MD chunk decode, code page %s does not fit given bytes sequence at ALL. %s\",\n                encoding_iana,\n                str(e),\n            )\n            early_stop_count = max_chunk_gave_up\n            lazy_str_hard_failure = True\n\n        # We might want to check the sequence again with the whole content\n        # Only if initial MD tests passes\n        if (\n            not lazy_str_hard_failure\n            and is_too_large_sequence\n            and not is_multi_byte_decoder\n        ):\n            try:\n                sequences[int(50e3) :].decode(encoding_iana, errors=\"strict\")\n            except UnicodeDecodeError as e:\n                logger.log(\n                    TRACE,\n                    \"LazyStr Loading: After final lookup, code page %s does not fit given bytes sequence at ALL. %s\",\n                    encoding_iana,\n                    str(e),\n                )\n                tested_but_hard_failure.append(encoding_iana)\n                continue\n\n        mean_mess_ratio: float = sum(md_ratios) / len(md_ratios) if md_ratios else 0.0\n        if mean_mess_ratio >= threshold or early_stop_count >= max_chunk_gave_up:\n            tested_but_soft_failure.append(encoding_iana)\n            logger.log(\n                TRACE,\n                \"%s was excluded because of initial chaos probing. Gave up %i time(s). \"\n                \"Computed mean chaos is %f %%.\",\n                encoding_iana,\n                early_stop_count,\n                round(mean_mess_ratio * 100, ndigits=3),\n            )\n            # Preparing those fallbacks in case we got nothing.\n            if (\n                enable_fallback\n                and encoding_iana in [\"ascii\", \"utf_8\", specified_encoding]\n                and not lazy_str_hard_failure\n            ):\n                fallback_entry = CharsetMatch(\n                    sequences, encoding_iana, threshold, False, [], decoded_payload\n                )\n                if encoding_iana == specified_encoding:\n                    fallback_specified = fallback_entry\n                elif encoding_iana == \"ascii\":\n                    fallback_ascii = fallback_entry\n                else:\n                    fallback_u8 = fallback_entry\n            continue\n\n        logger.log(\n            TRACE,\n            \"%s passed initial chaos probing. Mean measured chaos is %f %%\",\n            encoding_iana,\n            round(mean_mess_ratio * 100, ndigits=3),\n        )\n\n        if not is_multi_byte_decoder:\n            target_languages: List[str] = encoding_languages(encoding_iana)\n        else:\n            target_languages = mb_encoding_languages(encoding_iana)\n\n        if target_languages:\n            logger.log(\n                TRACE,\n                \"{} should target any language(s) of {}\".format(\n                    encoding_iana, str(target_languages)\n                ),\n            )\n\n        cd_ratios = []\n\n        # We shall skip the CD when its about ASCII\n        # Most of the time its not relevant to run \"language-detection\" on it.\n        if encoding_iana != \"ascii\":\n            for chunk in md_chunks:\n                chunk_languages = coherence_ratio(\n                    chunk,\n                    language_threshold,\n                    \",\".join(target_languages) if target_languages else None,\n                )\n\n                cd_ratios.append(chunk_languages)\n\n        cd_ratios_merged = merge_coherence_ratios(cd_ratios)\n\n        if cd_ratios_merged:\n            logger.log(\n                TRACE,\n                \"We detected language {} using {}\".format(\n                    cd_ratios_merged, encoding_iana\n                ),\n            )\n\n        results.append(\n            CharsetMatch(\n                sequences,\n                encoding_iana,\n                mean_mess_ratio,\n                bom_or_sig_available,\n                cd_ratios_merged,\n                decoded_payload,\n            )\n        )\n\n        if (\n            encoding_iana in [specified_encoding, \"ascii\", \"utf_8\"]\n            and mean_mess_ratio < 0.1\n        ):\n            logger.debug(\n                \"Encoding detection: %s is most likely the one.\", encoding_iana\n            )\n            if explain:\n                logger.removeHandler(explain_handler)\n                logger.setLevel(previous_logger_level)\n            return CharsetMatches([results[encoding_iana]])\n\n        if encoding_iana == sig_encoding:\n            logger.debug(\n                \"Encoding detection: %s is most likely the one as we detected a BOM or SIG within \"\n                \"the beginning of the sequence.\",\n                encoding_iana,\n            )\n            if explain:\n                logger.removeHandler(explain_handler)\n                logger.setLevel(previous_logger_level)\n            return CharsetMatches([results[encoding_iana]])\n\n    if len(results) == 0:\n        if fallback_u8 or fallback_ascii or fallback_specified:\n            logger.log(\n                TRACE,\n                \"Nothing got out of the detection process. Using ASCII/UTF-8/Specified fallback.\",\n            )\n\n        if fallback_specified:\n            logger.debug(\n                \"Encoding detection: %s will be used as a fallback match\",\n                fallback_specified.encoding,\n            )\n            results.append(fallback_specified)\n        elif (\n            (fallback_u8 and fallback_ascii is None)\n            or (\n                fallback_u8\n                and fallback_ascii\n                and fallback_u8.fingerprint != fallback_ascii.fingerprint\n            )\n            or (fallback_u8 is not None)\n        ):\n            logger.debug(\"Encoding detection: utf_8 will be used as a fallback match\")\n            results.append(fallback_u8)\n        elif fallback_ascii:\n            logger.debug(\"Encoding detection: ascii will be used as a fallback match\")\n            results.append(fallback_ascii)\n\n    if results:\n        logger.debug(\n            \"Encoding detection: Found %s as plausible (best-candidate) for content. With %i alternatives.\",\n            results.best().encoding,  # type: ignore\n            len(results) - 1,\n        )\n    else:\n        logger.debug(\"Encoding detection: Unable to determine any suitable charset.\")\n\n    if explain:\n        logger.removeHandler(explain_handler)\n        logger.setLevel(previous_logger_level)\n\n    return results\n\n\ndef from_fp(\n    fp: BinaryIO,\n    steps: int = 5,\n    chunk_size: int = 512,\n    threshold: float = 0.20,\n    cp_isolation: Optional[List[str]] = None,\n    cp_exclusion: Optional[List[str]] = None,\n    preemptive_behaviour: bool = True,\n    explain: bool = False,\n    language_threshold: float = 0.1,\n    enable_fallback: bool = True,\n) -> CharsetMatches:\n    \"\"\"\n    Same thing than the function from_bytes but using a file pointer that is already ready.\n    Will not close the file pointer.\n    \"\"\"\n    return from_bytes(\n        fp.read(),\n        steps,\n        chunk_size,\n        threshold,\n        cp_isolation,\n        cp_exclusion,\n        preemptive_behaviour,\n        explain,\n        language_threshold,\n        enable_fallback,\n    )\n\n\ndef from_path(\n    path: Union[str, bytes, PathLike],  # type: ignore[type-arg]\n    steps: int = 5,\n    chunk_size: int = 512,\n    threshold: float = 0.20,\n    cp_isolation: Optional[List[str]] = None,\n    cp_exclusion: Optional[List[str]] = None,\n    preemptive_behaviour: bool = True,\n    explain: bool = False,\n    language_threshold: float = 0.1,\n    enable_fallback: bool = True,\n) -> CharsetMatches:\n    \"\"\"\n    Same thing than the function from_bytes but with one extra step. Opening and reading given file path in binary mode.\n    Can raise IOError.\n    \"\"\"\n    with open(path, \"rb\") as fp:\n        return from_fp(\n            fp,\n            steps,\n            chunk_size,\n            threshold,\n            cp_isolation,\n            cp_exclusion,\n            preemptive_behaviour,\n            explain,\n            language_threshold,\n            enable_fallback,\n        )\n\n\ndef is_binary(\n    fp_or_path_or_payload: Union[PathLike, str, BinaryIO, bytes],  # type: ignore[type-arg]\n    steps: int = 5,\n    chunk_size: int = 512,\n    threshold: float = 0.20,\n    cp_isolation: Optional[List[str]] = None,\n    cp_exclusion: Optional[List[str]] = None,\n    preemptive_behaviour: bool = True,\n    explain: bool = False,\n    language_threshold: float = 0.1,\n    enable_fallback: bool = False,\n) -> bool:\n    \"\"\"\n    Detect if the given input (file, bytes, or path) points to a binary file. aka. not a string.\n    Based on the same main heuristic algorithms and default kwargs at the sole exception that fallbacks match\n    are disabled to be stricter around ASCII-compatible but unlikely to be a string.\n    \"\"\"\n    if isinstance(fp_or_path_or_payload, (str, PathLike)):\n        guesses = from_path(\n            fp_or_path_or_payload,\n            steps=steps,\n            chunk_size=chunk_size,\n            threshold=threshold,\n            cp_isolation=cp_isolation,\n            cp_exclusion=cp_exclusion,\n            preemptive_behaviour=preemptive_behaviour,\n            explain=explain,\n            language_threshold=language_threshold,\n            enable_fallback=enable_fallback,\n        )\n    elif isinstance(\n        fp_or_path_or_payload,\n        (\n            bytes,\n            bytearray,\n        ),\n    ):\n        guesses = from_bytes(\n            fp_or_path_or_payload,\n            steps=steps,\n            chunk_size=chunk_size,\n            threshold=threshold,\n            cp_isolation=cp_isolation,\n            cp_exclusion=cp_exclusion,\n            preemptive_behaviour=preemptive_behaviour,\n            explain=explain,\n            language_threshold=language_threshold,\n            enable_fallback=enable_fallback,\n        )\n    else:\n        guesses = from_fp(\n            fp_or_path_or_payload,\n            steps=steps,\n            chunk_size=chunk_size,\n            threshold=threshold,\n            cp_isolation=cp_isolation,\n            cp_exclusion=cp_exclusion,\n            preemptive_behaviour=preemptive_behaviour,\n            explain=explain,\n            language_threshold=language_threshold,\n            enable_fallback=enable_fallback,\n        )\n\n    return not guesses\n", "charset_normalizer/utils.py": "import importlib\nimport logging\nimport unicodedata\nfrom codecs import IncrementalDecoder\nfrom encodings.aliases import aliases\nfrom functools import lru_cache\nfrom re import findall\nfrom typing import Generator, List, Optional, Set, Tuple, Union\n\nfrom _multibytecodec import MultibyteIncrementalDecoder\n\nfrom .constant import (\n    ENCODING_MARKS,\n    IANA_SUPPORTED_SIMILAR,\n    RE_POSSIBLE_ENCODING_INDICATION,\n    UNICODE_RANGES_COMBINED,\n    UNICODE_SECONDARY_RANGE_KEYWORD,\n    UTF8_MAXIMAL_ALLOCATION,\n)\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_accentuated(character: str) -> bool:\n    try:\n        description: str = unicodedata.name(character)\n    except ValueError:\n        return False\n    return (\n        \"WITH GRAVE\" in description\n        or \"WITH ACUTE\" in description\n        or \"WITH CEDILLA\" in description\n        or \"WITH DIAERESIS\" in description\n        or \"WITH CIRCUMFLEX\" in description\n        or \"WITH TILDE\" in description\n        or \"WITH MACRON\" in description\n        or \"WITH RING ABOVE\" in description\n    )\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef remove_accent(character: str) -> str:\n    decomposed: str = unicodedata.decomposition(character)\n    if not decomposed:\n        return character\n\n    codes: List[str] = decomposed.split(\" \")\n\n    return chr(int(codes[0], 16))\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef unicode_range(character: str) -> Optional[str]:\n    \"\"\"\n    Retrieve the Unicode range official name from a single character.\n    \"\"\"\n    character_ord: int = ord(character)\n\n    for range_name, ord_range in UNICODE_RANGES_COMBINED.items():\n        if character_ord in ord_range:\n            return range_name\n\n    return None\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_latin(character: str) -> bool:\n    try:\n        description: str = unicodedata.name(character)\n    except ValueError:\n        return False\n    return \"LATIN\" in description\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_punctuation(character: str) -> bool:\n    character_category: str = unicodedata.category(character)\n\n    if \"P\" in character_category:\n        return True\n\n    character_range: Optional[str] = unicode_range(character)\n\n    if character_range is None:\n        return False\n\n    return \"Punctuation\" in character_range\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_symbol(character: str) -> bool:\n    character_category: str = unicodedata.category(character)\n\n    if \"S\" in character_category or \"N\" in character_category:\n        return True\n\n    character_range: Optional[str] = unicode_range(character)\n\n    if character_range is None:\n        return False\n\n    return \"Forms\" in character_range and character_category != \"Lo\"\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_emoticon(character: str) -> bool:\n    character_range: Optional[str] = unicode_range(character)\n\n    if character_range is None:\n        return False\n\n    return \"Emoticons\" in character_range or \"Pictographs\" in character_range\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_separator(character: str) -> bool:\n    if character.isspace() or character in {\"\uff5c\", \"+\", \"<\", \">\"}:\n        return True\n\n    character_category: str = unicodedata.category(character)\n\n    return \"Z\" in character_category or character_category in {\"Po\", \"Pd\", \"Pc\"}\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_case_variable(character: str) -> bool:\n    return character.islower() != character.isupper()\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_cjk(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return \"CJK\" in character_name\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_hiragana(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return \"HIRAGANA\" in character_name\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_katakana(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return \"KATAKANA\" in character_name\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_hangul(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return \"HANGUL\" in character_name\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_thai(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return \"THAI\" in character_name\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_arabic(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return \"ARABIC\" in character_name\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_arabic_isolated_form(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return \"ARABIC\" in character_name and \"ISOLATED FORM\" in character_name\n\n\n@lru_cache(maxsize=len(UNICODE_RANGES_COMBINED))\ndef is_unicode_range_secondary(range_name: str) -> bool:\n    return any(keyword in range_name for keyword in UNICODE_SECONDARY_RANGE_KEYWORD)\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_unprintable(character: str) -> bool:\n    return (\n        character.isspace() is False  # includes \\n \\t \\r \\v\n        and character.isprintable() is False\n        and character != \"\\x1A\"  # Why? Its the ASCII substitute character.\n        and character != \"\\ufeff\"  # bug discovered in Python,\n        # Zero Width No-Break Space located in \tArabic Presentation Forms-B, Unicode 1.1 not acknowledged as space.\n    )\n\n\ndef any_specified_encoding(sequence: bytes, search_zone: int = 8192) -> Optional[str]:\n    \"\"\"\n    Extract using ASCII-only decoder any specified encoding in the first n-bytes.\n    \"\"\"\n    if not isinstance(sequence, bytes):\n        raise TypeError\n\n    seq_len: int = len(sequence)\n\n    results: List[str] = findall(\n        RE_POSSIBLE_ENCODING_INDICATION,\n        sequence[: min(seq_len, search_zone)].decode(\"ascii\", errors=\"ignore\"),\n    )\n\n    if len(results) == 0:\n        return None\n\n    for specified_encoding in results:\n        specified_encoding = specified_encoding.lower().replace(\"-\", \"_\")\n\n        encoding_alias: str\n        encoding_iana: str\n\n        for encoding_alias, encoding_iana in aliases.items():\n            if encoding_alias == specified_encoding:\n                return encoding_iana\n            if encoding_iana == specified_encoding:\n                return encoding_iana\n\n    return None\n\n\n@lru_cache(maxsize=128)\ndef is_multi_byte_encoding(name: str) -> bool:\n    \"\"\"\n    Verify is a specific encoding is a multi byte one based on it IANA name\n    \"\"\"\n    return name in {\n        \"utf_8\",\n        \"utf_8_sig\",\n        \"utf_16\",\n        \"utf_16_be\",\n        \"utf_16_le\",\n        \"utf_32\",\n        \"utf_32_le\",\n        \"utf_32_be\",\n        \"utf_7\",\n    } or issubclass(\n        importlib.import_module(\"encodings.{}\".format(name)).IncrementalDecoder,\n        MultibyteIncrementalDecoder,\n    )\n\n\ndef identify_sig_or_bom(sequence: bytes) -> Tuple[Optional[str], bytes]:\n    \"\"\"\n    Identify and extract SIG/BOM in given sequence.\n    \"\"\"\n\n    for iana_encoding in ENCODING_MARKS:\n        marks: Union[bytes, List[bytes]] = ENCODING_MARKS[iana_encoding]\n\n        if isinstance(marks, bytes):\n            marks = [marks]\n\n        for mark in marks:\n            if sequence.startswith(mark):\n                return iana_encoding, mark\n\n    return None, b\"\"\n\n\ndef should_strip_sig_or_bom(iana_encoding: str) -> bool:\n    return iana_encoding not in {\"utf_16\", \"utf_32\"}\n\n\ndef iana_name(cp_name: str, strict: bool = True) -> str:\n    cp_name = cp_name.lower().replace(\"-\", \"_\")\n\n    encoding_alias: str\n    encoding_iana: str\n\n    for encoding_alias, encoding_iana in aliases.items():\n        if cp_name in [encoding_alias, encoding_iana]:\n            return encoding_iana\n\n    if strict:\n        raise ValueError(\"Unable to retrieve IANA for '{}'\".format(cp_name))\n\n    return cp_name\n\n\ndef range_scan(decoded_sequence: str) -> List[str]:\n    ranges: Set[str] = set()\n\n    for character in decoded_sequence:\n        character_range: Optional[str] = unicode_range(character)\n\n        if character_range is None:\n            continue\n\n        ranges.add(character_range)\n\n    return list(ranges)\n\n\ndef cp_similarity(iana_name_a: str, iana_name_b: str) -> float:\n    if is_multi_byte_encoding(iana_name_a) or is_multi_byte_encoding(iana_name_b):\n        return 0.0\n\n    decoder_a = importlib.import_module(\n        \"encodings.{}\".format(iana_name_a)\n    ).IncrementalDecoder\n    decoder_b = importlib.import_module(\n        \"encodings.{}\".format(iana_name_b)\n    ).IncrementalDecoder\n\n    id_a: IncrementalDecoder = decoder_a(errors=\"ignore\")\n    id_b: IncrementalDecoder = decoder_b(errors=\"ignore\")\n\n    character_match_count: int = 0\n\n    for i in range(255):\n        to_be_decoded: bytes = bytes([i])\n        if id_a.decode(to_be_decoded) == id_b.decode(to_be_decoded):\n            character_match_count += 1\n\n    return character_match_count / 254\n\n\ndef is_cp_similar(iana_name_a: str, iana_name_b: str) -> bool:\n    \"\"\"\n    Determine if two code page are at least 80% similar. IANA_SUPPORTED_SIMILAR dict was generated using\n    the function cp_similarity.\n    \"\"\"\n    return (\n        iana_name_a in IANA_SUPPORTED_SIMILAR\n        and iana_name_b in IANA_SUPPORTED_SIMILAR[iana_name_a]\n    )\n\n\ndef set_logging_handler(\n    name: str = \"charset_normalizer\",\n    level: int = logging.INFO,\n    format_string: str = \"%(asctime)s | %(levelname)s | %(message)s\",\n) -> None:\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(format_string))\n    logger.addHandler(handler)\n\n\ndef cut_sequence_chunks(\n    sequences: bytes,\n    encoding_iana: str,\n    offsets: range,\n    chunk_size: int,\n    bom_or_sig_available: bool,\n    strip_sig_or_bom: bool,\n    sig_payload: bytes,\n    is_multi_byte_decoder: bool,\n    decoded_payload: Optional[str] = None,\n) -> Generator[str, None, None]:\n    if decoded_payload and is_multi_byte_decoder is False:\n        for i in offsets:\n            chunk = decoded_payload[i : i + chunk_size]\n            if not chunk:\n                break\n            yield chunk\n    else:\n        for i in offsets:\n            chunk_end = i + chunk_size\n            if chunk_end > len(sequences) + 8:\n                continue\n\n            cut_sequence = sequences[i : i + chunk_size]\n\n            if bom_or_sig_available and strip_sig_or_bom is False:\n                cut_sequence = sig_payload + cut_sequence\n\n            chunk = cut_sequence.decode(\n                encoding_iana,\n                errors=\"ignore\" if is_multi_byte_decoder else \"strict\",\n            )\n\n            # multi-byte bad cutting detector and adjustment\n            # not the cleanest way to perform that fix but clever enough for now.\n            if is_multi_byte_decoder and i > 0:\n                chunk_partial_size_chk: int = min(chunk_size, 16)\n\n                if (\n                    decoded_payload\n                    and chunk[:chunk_partial_size_chk] not in decoded_payload\n                ):\n                    for j in range(i, i - 4, -1):\n                        cut_sequence = sequences[j:chunk_end]\n\n                        if bom_or_sig_available and strip_sig_or_bom is False:\n                            cut_sequence = sig_payload + cut_sequence\n\n                        chunk = cut_sequence.decode(encoding_iana, errors=\"ignore\")\n\n                        if chunk[:chunk_partial_size_chk] in decoded_payload:\n                            break\n\n            yield chunk\n", "charset_normalizer/cd.py": "import importlib\nfrom codecs import IncrementalDecoder\nfrom collections import Counter\nfrom functools import lru_cache\nfrom typing import Counter as TypeCounter, Dict, List, Optional, Tuple\n\nfrom .constant import (\n    FREQUENCIES,\n    KO_NAMES,\n    LANGUAGE_SUPPORTED_COUNT,\n    TOO_SMALL_SEQUENCE,\n    ZH_NAMES,\n)\nfrom .md import is_suspiciously_successive_range\nfrom .models import CoherenceMatches\nfrom .utils import (\n    is_accentuated,\n    is_latin,\n    is_multi_byte_encoding,\n    is_unicode_range_secondary,\n    unicode_range,\n)\n\n\ndef encoding_unicode_range(iana_name: str) -> List[str]:\n    \"\"\"\n    Return associated unicode ranges in a single byte code page.\n    \"\"\"\n    if is_multi_byte_encoding(iana_name):\n        raise IOError(\"Function not supported on multi-byte code page\")\n\n    decoder = importlib.import_module(\n        \"encodings.{}\".format(iana_name)\n    ).IncrementalDecoder\n\n    p: IncrementalDecoder = decoder(errors=\"ignore\")\n    seen_ranges: Dict[str, int] = {}\n    character_count: int = 0\n\n    for i in range(0x40, 0xFF):\n        chunk: str = p.decode(bytes([i]))\n\n        if chunk:\n            character_range: Optional[str] = unicode_range(chunk)\n\n            if character_range is None:\n                continue\n\n            if is_unicode_range_secondary(character_range) is False:\n                if character_range not in seen_ranges:\n                    seen_ranges[character_range] = 0\n                seen_ranges[character_range] += 1\n                character_count += 1\n\n    return sorted(\n        [\n            character_range\n            for character_range in seen_ranges\n            if seen_ranges[character_range] / character_count >= 0.15\n        ]\n    )\n\n\ndef unicode_range_languages(primary_range: str) -> List[str]:\n    \"\"\"\n    Return inferred languages used with a unicode range.\n    \"\"\"\n    languages: List[str] = []\n\n    for language, characters in FREQUENCIES.items():\n        for character in characters:\n            if unicode_range(character) == primary_range:\n                languages.append(language)\n                break\n\n    return languages\n\n\n@lru_cache()\ndef encoding_languages(iana_name: str) -> List[str]:\n    \"\"\"\n    Single-byte encoding language association. Some code page are heavily linked to particular language(s).\n    This function does the correspondence.\n    \"\"\"\n    unicode_ranges: List[str] = encoding_unicode_range(iana_name)\n    primary_range: Optional[str] = None\n\n    for specified_range in unicode_ranges:\n        if \"Latin\" not in specified_range:\n            primary_range = specified_range\n            break\n\n    if primary_range is None:\n        return [\"Latin Based\"]\n\n    return unicode_range_languages(primary_range)\n\n\n@lru_cache()\ndef mb_encoding_languages(iana_name: str) -> List[str]:\n    \"\"\"\n    Multi-byte encoding language association. Some code page are heavily linked to particular language(s).\n    This function does the correspondence.\n    \"\"\"\n    if (\n        iana_name.startswith(\"shift_\")\n        or iana_name.startswith(\"iso2022_jp\")\n        or iana_name.startswith(\"euc_j\")\n        or iana_name == \"cp932\"\n    ):\n        return [\"Japanese\"]\n    if iana_name.startswith(\"gb\") or iana_name in ZH_NAMES:\n        return [\"Chinese\"]\n    if iana_name.startswith(\"iso2022_kr\") or iana_name in KO_NAMES:\n        return [\"Korean\"]\n\n    return []\n\n\n@lru_cache(maxsize=LANGUAGE_SUPPORTED_COUNT)\ndef get_target_features(language: str) -> Tuple[bool, bool]:\n    \"\"\"\n    Determine main aspects from a supported language if it contains accents and if is pure Latin.\n    \"\"\"\n    target_have_accents: bool = False\n    target_pure_latin: bool = True\n\n    for character in FREQUENCIES[language]:\n        if not target_have_accents and is_accentuated(character):\n            target_have_accents = True\n        if target_pure_latin and is_latin(character) is False:\n            target_pure_latin = False\n\n    return target_have_accents, target_pure_latin\n\n\ndef alphabet_languages(\n    characters: List[str], ignore_non_latin: bool = False\n) -> List[str]:\n    \"\"\"\n    Return associated languages associated to given characters.\n    \"\"\"\n    languages: List[Tuple[str, float]] = []\n\n    source_have_accents = any(is_accentuated(character) for character in characters)\n\n    for language, language_characters in FREQUENCIES.items():\n        target_have_accents, target_pure_latin = get_target_features(language)\n\n        if ignore_non_latin and target_pure_latin is False:\n            continue\n\n        if target_have_accents is False and source_have_accents:\n            continue\n\n        character_count: int = len(language_characters)\n\n        character_match_count: int = len(\n            [c for c in language_characters if c in characters]\n        )\n\n        ratio: float = character_match_count / character_count\n\n        if ratio >= 0.2:\n            languages.append((language, ratio))\n\n    languages = sorted(languages, key=lambda x: x[1], reverse=True)\n\n    return [compatible_language[0] for compatible_language in languages]\n\n\ndef characters_popularity_compare(\n    language: str, ordered_characters: List[str]\n) -> float:\n    \"\"\"\n    Determine if a ordered characters list (by occurrence from most appearance to rarest) match a particular language.\n    The result is a ratio between 0. (absolutely no correspondence) and 1. (near perfect fit).\n    Beware that is function is not strict on the match in order to ease the detection. (Meaning close match is 1.)\n    \"\"\"\n    if language not in FREQUENCIES:\n        raise ValueError(\"{} not available\".format(language))\n\n    character_approved_count: int = 0\n    FREQUENCIES_language_set = set(FREQUENCIES[language])\n\n    ordered_characters_count: int = len(ordered_characters)\n    target_language_characters_count: int = len(FREQUENCIES[language])\n\n    large_alphabet: bool = target_language_characters_count > 26\n\n    for character, character_rank in zip(\n        ordered_characters, range(0, ordered_characters_count)\n    ):\n        if character not in FREQUENCIES_language_set:\n            continue\n\n        character_rank_in_language: int = FREQUENCIES[language].index(character)\n        expected_projection_ratio: float = (\n            target_language_characters_count / ordered_characters_count\n        )\n        character_rank_projection: int = int(character_rank * expected_projection_ratio)\n\n        if (\n            large_alphabet is False\n            and abs(character_rank_projection - character_rank_in_language) > 4\n        ):\n            continue\n\n        if (\n            large_alphabet is True\n            and abs(character_rank_projection - character_rank_in_language)\n            < target_language_characters_count / 3\n        ):\n            character_approved_count += 1\n            continue\n\n        characters_before_source: List[str] = FREQUENCIES[language][\n            0:character_rank_in_language\n        ]\n        characters_after_source: List[str] = FREQUENCIES[language][\n            character_rank_in_language:\n        ]\n        characters_before: List[str] = ordered_characters[0:character_rank]\n        characters_after: List[str] = ordered_characters[character_rank:]\n\n        before_match_count: int = len(\n            set(characters_before) & set(characters_before_source)\n        )\n\n        after_match_count: int = len(\n            set(characters_after) & set(characters_after_source)\n        )\n\n        if len(characters_before_source) == 0 and before_match_count <= 4:\n            character_approved_count += 1\n            continue\n\n        if len(characters_after_source) == 0 and after_match_count <= 4:\n            character_approved_count += 1\n            continue\n\n        if (\n            before_match_count / len(characters_before_source) >= 0.4\n            or after_match_count / len(characters_after_source) >= 0.4\n        ):\n            character_approved_count += 1\n            continue\n\n    return character_approved_count / len(ordered_characters)\n\n\ndef alpha_unicode_split(decoded_sequence: str) -> List[str]:\n    \"\"\"\n    Given a decoded text sequence, return a list of str. Unicode range / alphabet separation.\n    Ex. a text containing English/Latin with a bit a Hebrew will return two items in the resulting list;\n    One containing the latin letters and the other hebrew.\n    \"\"\"\n    layers: Dict[str, str] = {}\n\n    for character in decoded_sequence:\n        if character.isalpha() is False:\n            continue\n\n        character_range: Optional[str] = unicode_range(character)\n\n        if character_range is None:\n            continue\n\n        layer_target_range: Optional[str] = None\n\n        for discovered_range in layers:\n            if (\n                is_suspiciously_successive_range(discovered_range, character_range)\n                is False\n            ):\n                layer_target_range = discovered_range\n                break\n\n        if layer_target_range is None:\n            layer_target_range = character_range\n\n        if layer_target_range not in layers:\n            layers[layer_target_range] = character.lower()\n            continue\n\n        layers[layer_target_range] += character.lower()\n\n    return list(layers.values())\n\n\ndef merge_coherence_ratios(results: List[CoherenceMatches]) -> CoherenceMatches:\n    \"\"\"\n    This function merge results previously given by the function coherence_ratio.\n    The return type is the same as coherence_ratio.\n    \"\"\"\n    per_language_ratios: Dict[str, List[float]] = {}\n    for result in results:\n        for sub_result in result:\n            language, ratio = sub_result\n            if language not in per_language_ratios:\n                per_language_ratios[language] = [ratio]\n                continue\n            per_language_ratios[language].append(ratio)\n\n    merge = [\n        (\n            language,\n            round(\n                sum(per_language_ratios[language]) / len(per_language_ratios[language]),\n                4,\n            ),\n        )\n        for language in per_language_ratios\n    ]\n\n    return sorted(merge, key=lambda x: x[1], reverse=True)\n\n\ndef filter_alt_coherence_matches(results: CoherenceMatches) -> CoherenceMatches:\n    \"\"\"\n    We shall NOT return \"English\u2014\" in CoherenceMatches because it is an alternative\n    of \"English\". This function only keeps the best match and remove the em-dash in it.\n    \"\"\"\n    index_results: Dict[str, List[float]] = dict()\n\n    for result in results:\n        language, ratio = result\n        no_em_name: str = language.replace(\"\u2014\", \"\")\n\n        if no_em_name not in index_results:\n            index_results[no_em_name] = []\n\n        index_results[no_em_name].append(ratio)\n\n    if any(len(index_results[e]) > 1 for e in index_results):\n        filtered_results: CoherenceMatches = []\n\n        for language in index_results:\n            filtered_results.append((language, max(index_results[language])))\n\n        return filtered_results\n\n    return results\n\n\n@lru_cache(maxsize=2048)\ndef coherence_ratio(\n    decoded_sequence: str, threshold: float = 0.1, lg_inclusion: Optional[str] = None\n) -> CoherenceMatches:\n    \"\"\"\n    Detect ANY language that can be identified in given sequence. The sequence will be analysed by layers.\n    A layer = Character extraction by alphabets/ranges.\n    \"\"\"\n\n    results: List[Tuple[str, float]] = []\n    ignore_non_latin: bool = False\n\n    sufficient_match_count: int = 0\n\n    lg_inclusion_list = lg_inclusion.split(\",\") if lg_inclusion is not None else []\n    if \"Latin Based\" in lg_inclusion_list:\n        ignore_non_latin = True\n        lg_inclusion_list.remove(\"Latin Based\")\n\n    for layer in alpha_unicode_split(decoded_sequence):\n        sequence_frequencies: TypeCounter[str] = Counter(layer)\n        most_common = sequence_frequencies.most_common()\n\n        character_count: int = sum(o for c, o in most_common)\n\n        if character_count <= TOO_SMALL_SEQUENCE:\n            continue\n\n        popular_character_ordered: List[str] = [c for c, o in most_common]\n\n        for language in lg_inclusion_list or alphabet_languages(\n            popular_character_ordered, ignore_non_latin\n        ):\n            ratio: float = characters_popularity_compare(\n                language, popular_character_ordered\n            )\n\n            if ratio < threshold:\n                continue\n            elif ratio >= 0.8:\n                sufficient_match_count += 1\n\n            results.append((language, round(ratio, 4)))\n\n            if sufficient_match_count >= 3:\n                break\n\n    return sorted(\n        filter_alt_coherence_matches(results), key=lambda x: x[1], reverse=True\n    )\n", "charset_normalizer/version.py": "\"\"\"\nExpose version\n\"\"\"\n\n__version__ = \"3.3.2\"\nVERSION = __version__.split(\".\")\n", "charset_normalizer/models.py": "from encodings.aliases import aliases\nfrom hashlib import sha256\nfrom json import dumps\nfrom typing import Any, Dict, Iterator, List, Optional, Tuple, Union\n\nfrom .constant import TOO_BIG_SEQUENCE\nfrom .utils import iana_name, is_multi_byte_encoding, unicode_range\n\n\nclass CharsetMatch:\n    def __init__(\n        self,\n        payload: bytes,\n        guessed_encoding: str,\n        mean_mess_ratio: float,\n        has_sig_or_bom: bool,\n        languages: \"CoherenceMatches\",\n        decoded_payload: Optional[str] = None,\n    ):\n        self._payload: bytes = payload\n\n        self._encoding: str = guessed_encoding\n        self._mean_mess_ratio: float = mean_mess_ratio\n        self._languages: CoherenceMatches = languages\n        self._has_sig_or_bom: bool = has_sig_or_bom\n        self._unicode_ranges: Optional[List[str]] = None\n\n        self._leaves: List[CharsetMatch] = []\n        self._mean_coherence_ratio: float = 0.0\n\n        self._output_payload: Optional[bytes] = None\n        self._output_encoding: Optional[str] = None\n\n        self._string: Optional[str] = decoded_payload\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, CharsetMatch):\n            if isinstance(other, str):\n                return iana_name(other) == self.encoding\n            return False\n        return self.encoding == other.encoding and self.fingerprint == other.fingerprint\n\n    def __lt__(self, other: object) -> bool:\n        \"\"\"\n        Implemented to make sorted available upon CharsetMatches items.\n        \"\"\"\n        if not isinstance(other, CharsetMatch):\n            raise ValueError\n\n        chaos_difference: float = abs(self.chaos - other.chaos)\n        coherence_difference: float = abs(self.coherence - other.coherence)\n\n        # Below 1% difference --> Use Coherence\n        if chaos_difference < 0.01 and coherence_difference > 0.02:\n            return self.coherence > other.coherence\n        elif chaos_difference < 0.01 and coherence_difference <= 0.02:\n            # When having a difficult decision, use the result that decoded as many multi-byte as possible.\n            # preserve RAM usage!\n            if len(self._payload) >= TOO_BIG_SEQUENCE:\n                return self.chaos < other.chaos\n            return self.multi_byte_usage > other.multi_byte_usage\n\n        return self.chaos < other.chaos\n\n    @property\n    def multi_byte_usage(self) -> float:\n        return 1.0 - (len(str(self)) / len(self.raw))\n\n    def __str__(self) -> str:\n        # Lazy Str Loading\n        if self._string is None:\n            self._string = str(self._payload, self._encoding, \"strict\")\n        return self._string\n\n    def __repr__(self) -> str:\n        return \"<CharsetMatch '{}' bytes({})>\".format(self.encoding, self.fingerprint)\n\n    def add_submatch(self, other: \"CharsetMatch\") -> None:\n        if not isinstance(other, CharsetMatch) or other == self:\n            raise ValueError(\n                \"Unable to add instance <{}> as a submatch of a CharsetMatch\".format(\n                    other.__class__\n                )\n            )\n\n        other._string = None  # Unload RAM usage; dirty trick.\n        self._leaves.append(other)\n\n    @property\n    def encoding(self) -> str:\n        return self._encoding\n\n    @property\n    def encoding_aliases(self) -> List[str]:\n        \"\"\"\n        Encoding name are known by many name, using this could help when searching for IBM855 when it's listed as CP855.\n        \"\"\"\n        also_known_as: List[str] = []\n        for u, p in aliases.items():\n            if self.encoding == u:\n                also_known_as.append(p)\n            elif self.encoding == p:\n                also_known_as.append(u)\n        return also_known_as\n\n    @property\n    def bom(self) -> bool:\n        return self._has_sig_or_bom\n\n    @property\n    def byte_order_mark(self) -> bool:\n        return self._has_sig_or_bom\n\n    @property\n    def languages(self) -> List[str]:\n        \"\"\"\n        Return the complete list of possible languages found in decoded sequence.\n        Usually not really useful. Returned list may be empty even if 'language' property return something != 'Unknown'.\n        \"\"\"\n        return [e[0] for e in self._languages]\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        Most probable language found in decoded sequence. If none were detected or inferred, the property will return\n        \"Unknown\".\n        \"\"\"\n        if not self._languages:\n            # Trying to infer the language based on the given encoding\n            # Its either English or we should not pronounce ourselves in certain cases.\n            if \"ascii\" in self.could_be_from_charset:\n                return \"English\"\n\n            # doing it there to avoid circular import\n            from charset_normalizer.cd import encoding_languages, mb_encoding_languages\n\n            languages = (\n                mb_encoding_languages(self.encoding)\n                if is_multi_byte_encoding(self.encoding)\n                else encoding_languages(self.encoding)\n            )\n\n            if len(languages) == 0 or \"Latin Based\" in languages:\n                return \"Unknown\"\n\n            return languages[0]\n\n        return self._languages[0][0]\n\n    @property\n    def chaos(self) -> float:\n        return self._mean_mess_ratio\n\n    @property\n    def coherence(self) -> float:\n        if not self._languages:\n            return 0.0\n        return self._languages[0][1]\n\n    @property\n    def percent_chaos(self) -> float:\n        return round(self.chaos * 100, ndigits=3)\n\n    @property\n    def percent_coherence(self) -> float:\n        return round(self.coherence * 100, ndigits=3)\n\n    @property\n    def raw(self) -> bytes:\n        \"\"\"\n        Original untouched bytes.\n        \"\"\"\n        return self._payload\n\n    @property\n    def submatch(self) -> List[\"CharsetMatch\"]:\n        return self._leaves\n\n    @property\n    def has_submatch(self) -> bool:\n        return len(self._leaves) > 0\n\n    @property\n    def alphabets(self) -> List[str]:\n        if self._unicode_ranges is not None:\n            return self._unicode_ranges\n        # list detected ranges\n        detected_ranges: List[Optional[str]] = [\n            unicode_range(char) for char in str(self)\n        ]\n        # filter and sort\n        self._unicode_ranges = sorted(list({r for r in detected_ranges if r}))\n        return self._unicode_ranges\n\n    @property\n    def could_be_from_charset(self) -> List[str]:\n        \"\"\"\n        The complete list of encoding that output the exact SAME str result and therefore could be the originating\n        encoding.\n        This list does include the encoding available in property 'encoding'.\n        \"\"\"\n        return [self._encoding] + [m.encoding for m in self._leaves]\n\n    def output(self, encoding: str = \"utf_8\") -> bytes:\n        \"\"\"\n        Method to get re-encoded bytes payload using given target encoding. Default to UTF-8.\n        Any errors will be simply ignored by the encoder NOT replaced.\n        \"\"\"\n        if self._output_encoding is None or self._output_encoding != encoding:\n            self._output_encoding = encoding\n            self._output_payload = str(self).encode(encoding, \"replace\")\n\n        return self._output_payload  # type: ignore\n\n    @property\n    def fingerprint(self) -> str:\n        \"\"\"\n        Retrieve the unique SHA256 computed using the transformed (re-encoded) payload. Not the original one.\n        \"\"\"\n        return sha256(self.output()).hexdigest()\n\n\nclass CharsetMatches:\n    \"\"\"\n    Container with every CharsetMatch items ordered by default from most probable to the less one.\n    Act like a list(iterable) but does not implements all related methods.\n    \"\"\"\n\n    def __init__(self, results: Optional[List[CharsetMatch]] = None):\n        self._results: List[CharsetMatch] = sorted(results) if results else []\n\n    def __iter__(self) -> Iterator[CharsetMatch]:\n        yield from self._results\n\n    def __getitem__(self, item: Union[int, str]) -> CharsetMatch:\n        \"\"\"\n        Retrieve a single item either by its position or encoding name (alias may be used here).\n        Raise KeyError upon invalid index or encoding not present in results.\n        \"\"\"\n        if isinstance(item, int):\n            return self._results[item]\n        if isinstance(item, str):\n            item = iana_name(item, False)\n            for result in self._results:\n                if item in result.could_be_from_charset:\n                    return result\n        raise KeyError\n\n    def __len__(self) -> int:\n        return len(self._results)\n\n    def __bool__(self) -> bool:\n        return len(self._results) > 0\n\n    def append(self, item: CharsetMatch) -> None:\n        \"\"\"\n        Insert a single match. Will be inserted accordingly to preserve sort.\n        Can be inserted as a submatch.\n        \"\"\"\n        if not isinstance(item, CharsetMatch):\n            raise ValueError(\n                \"Cannot append instance '{}' to CharsetMatches\".format(\n                    str(item.__class__)\n                )\n            )\n        # We should disable the submatch factoring when the input file is too heavy (conserve RAM usage)\n        if len(item.raw) <= TOO_BIG_SEQUENCE:\n            for match in self._results:\n                if match.fingerprint == item.fingerprint and match.chaos == item.chaos:\n                    match.add_submatch(item)\n                    return\n        self._results.append(item)\n        self._results = sorted(self._results)\n\n    def best(self) -> Optional[\"CharsetMatch\"]:\n        \"\"\"\n        Simply return the first match. Strict equivalent to matches[0].\n        \"\"\"\n        if not self._results:\n            return None\n        return self._results[0]\n\n    def first(self) -> Optional[\"CharsetMatch\"]:\n        \"\"\"\n        Redundant method, call the method best(). Kept for BC reasons.\n        \"\"\"\n        return self.best()\n\n\nCoherenceMatch = Tuple[str, float]\nCoherenceMatches = List[CoherenceMatch]\n\n\nclass CliDetectionResult:\n    def __init__(\n        self,\n        path: str,\n        encoding: Optional[str],\n        encoding_aliases: List[str],\n        alternative_encodings: List[str],\n        language: str,\n        alphabets: List[str],\n        has_sig_or_bom: bool,\n        chaos: float,\n        coherence: float,\n        unicode_path: Optional[str],\n        is_preferred: bool,\n    ):\n        self.path: str = path\n        self.unicode_path: Optional[str] = unicode_path\n        self.encoding: Optional[str] = encoding\n        self.encoding_aliases: List[str] = encoding_aliases\n        self.alternative_encodings: List[str] = alternative_encodings\n        self.language: str = language\n        self.alphabets: List[str] = alphabets\n        self.has_sig_or_bom: bool = has_sig_or_bom\n        self.chaos: float = chaos\n        self.coherence: float = coherence\n        self.is_preferred: bool = is_preferred\n\n    @property\n    def __dict__(self) -> Dict[str, Any]:  # type: ignore\n        return {\n            \"path\": self.path,\n            \"encoding\": self.encoding,\n            \"encoding_aliases\": self.encoding_aliases,\n            \"alternative_encodings\": self.alternative_encodings,\n            \"language\": self.language,\n            \"alphabets\": self.alphabets,\n            \"has_sig_or_bom\": self.has_sig_or_bom,\n            \"chaos\": self.chaos,\n            \"coherence\": self.coherence,\n            \"unicode_path\": self.unicode_path,\n            \"is_preferred\": self.is_preferred,\n        }\n\n    def to_json(self) -> str:\n        return dumps(self.__dict__, ensure_ascii=True, indent=4)\n", "charset_normalizer/constant.py": "# -*- coding: utf-8 -*-\nfrom codecs import BOM_UTF8, BOM_UTF16_BE, BOM_UTF16_LE, BOM_UTF32_BE, BOM_UTF32_LE\nfrom encodings.aliases import aliases\nfrom re import IGNORECASE, compile as re_compile\nfrom typing import Dict, List, Set, Union\n\n# Contain for each eligible encoding a list of/item bytes SIG/BOM\nENCODING_MARKS: Dict[str, Union[bytes, List[bytes]]] = {\n    \"utf_8\": BOM_UTF8,\n    \"utf_7\": [\n        b\"\\x2b\\x2f\\x76\\x38\",\n        b\"\\x2b\\x2f\\x76\\x39\",\n        b\"\\x2b\\x2f\\x76\\x2b\",\n        b\"\\x2b\\x2f\\x76\\x2f\",\n        b\"\\x2b\\x2f\\x76\\x38\\x2d\",\n    ],\n    \"gb18030\": b\"\\x84\\x31\\x95\\x33\",\n    \"utf_32\": [BOM_UTF32_BE, BOM_UTF32_LE],\n    \"utf_16\": [BOM_UTF16_BE, BOM_UTF16_LE],\n}\n\nTOO_SMALL_SEQUENCE: int = 32\nTOO_BIG_SEQUENCE: int = int(10e6)\n\nUTF8_MAXIMAL_ALLOCATION: int = 1_112_064\n\n# Up-to-date Unicode ucd/15.0.0\nUNICODE_RANGES_COMBINED: Dict[str, range] = {\n    \"Control character\": range(32),\n    \"Basic Latin\": range(32, 128),\n    \"Latin-1 Supplement\": range(128, 256),\n    \"Latin Extended-A\": range(256, 384),\n    \"Latin Extended-B\": range(384, 592),\n    \"IPA Extensions\": range(592, 688),\n    \"Spacing Modifier Letters\": range(688, 768),\n    \"Combining Diacritical Marks\": range(768, 880),\n    \"Greek and Coptic\": range(880, 1024),\n    \"Cyrillic\": range(1024, 1280),\n    \"Cyrillic Supplement\": range(1280, 1328),\n    \"Armenian\": range(1328, 1424),\n    \"Hebrew\": range(1424, 1536),\n    \"Arabic\": range(1536, 1792),\n    \"Syriac\": range(1792, 1872),\n    \"Arabic Supplement\": range(1872, 1920),\n    \"Thaana\": range(1920, 1984),\n    \"NKo\": range(1984, 2048),\n    \"Samaritan\": range(2048, 2112),\n    \"Mandaic\": range(2112, 2144),\n    \"Syriac Supplement\": range(2144, 2160),\n    \"Arabic Extended-B\": range(2160, 2208),\n    \"Arabic Extended-A\": range(2208, 2304),\n    \"Devanagari\": range(2304, 2432),\n    \"Bengali\": range(2432, 2560),\n    \"Gurmukhi\": range(2560, 2688),\n    \"Gujarati\": range(2688, 2816),\n    \"Oriya\": range(2816, 2944),\n    \"Tamil\": range(2944, 3072),\n    \"Telugu\": range(3072, 3200),\n    \"Kannada\": range(3200, 3328),\n    \"Malayalam\": range(3328, 3456),\n    \"Sinhala\": range(3456, 3584),\n    \"Thai\": range(3584, 3712),\n    \"Lao\": range(3712, 3840),\n    \"Tibetan\": range(3840, 4096),\n    \"Myanmar\": range(4096, 4256),\n    \"Georgian\": range(4256, 4352),\n    \"Hangul Jamo\": range(4352, 4608),\n    \"Ethiopic\": range(4608, 4992),\n    \"Ethiopic Supplement\": range(4992, 5024),\n    \"Cherokee\": range(5024, 5120),\n    \"Unified Canadian Aboriginal Syllabics\": range(5120, 5760),\n    \"Ogham\": range(5760, 5792),\n    \"Runic\": range(5792, 5888),\n    \"Tagalog\": range(5888, 5920),\n    \"Hanunoo\": range(5920, 5952),\n    \"Buhid\": range(5952, 5984),\n    \"Tagbanwa\": range(5984, 6016),\n    \"Khmer\": range(6016, 6144),\n    \"Mongolian\": range(6144, 6320),\n    \"Unified Canadian Aboriginal Syllabics Extended\": range(6320, 6400),\n    \"Limbu\": range(6400, 6480),\n    \"Tai Le\": range(6480, 6528),\n    \"New Tai Lue\": range(6528, 6624),\n    \"Khmer Symbols\": range(6624, 6656),\n    \"Buginese\": range(6656, 6688),\n    \"Tai Tham\": range(6688, 6832),\n    \"Combining Diacritical Marks Extended\": range(6832, 6912),\n    \"Balinese\": range(6912, 7040),\n    \"Sundanese\": range(7040, 7104),\n    \"Batak\": range(7104, 7168),\n    \"Lepcha\": range(7168, 7248),\n    \"Ol Chiki\": range(7248, 7296),\n    \"Cyrillic Extended-C\": range(7296, 7312),\n    \"Georgian Extended\": range(7312, 7360),\n    \"Sundanese Supplement\": range(7360, 7376),\n    \"Vedic Extensions\": range(7376, 7424),\n    \"Phonetic Extensions\": range(7424, 7552),\n    \"Phonetic Extensions Supplement\": range(7552, 7616),\n    \"Combining Diacritical Marks Supplement\": range(7616, 7680),\n    \"Latin Extended Additional\": range(7680, 7936),\n    \"Greek Extended\": range(7936, 8192),\n    \"General Punctuation\": range(8192, 8304),\n    \"Superscripts and Subscripts\": range(8304, 8352),\n    \"Currency Symbols\": range(8352, 8400),\n    \"Combining Diacritical Marks for Symbols\": range(8400, 8448),\n    \"Letterlike Symbols\": range(8448, 8528),\n    \"Number Forms\": range(8528, 8592),\n    \"Arrows\": range(8592, 8704),\n    \"Mathematical Operators\": range(8704, 8960),\n    \"Miscellaneous Technical\": range(8960, 9216),\n    \"Control Pictures\": range(9216, 9280),\n    \"Optical Character Recognition\": range(9280, 9312),\n    \"Enclosed Alphanumerics\": range(9312, 9472),\n    \"Box Drawing\": range(9472, 9600),\n    \"Block Elements\": range(9600, 9632),\n    \"Geometric Shapes\": range(9632, 9728),\n    \"Miscellaneous Symbols\": range(9728, 9984),\n    \"Dingbats\": range(9984, 10176),\n    \"Miscellaneous Mathematical Symbols-A\": range(10176, 10224),\n    \"Supplemental Arrows-A\": range(10224, 10240),\n    \"Braille Patterns\": range(10240, 10496),\n    \"Supplemental Arrows-B\": range(10496, 10624),\n    \"Miscellaneous Mathematical Symbols-B\": range(10624, 10752),\n    \"Supplemental Mathematical Operators\": range(10752, 11008),\n    \"Miscellaneous Symbols and Arrows\": range(11008, 11264),\n    \"Glagolitic\": range(11264, 11360),\n    \"Latin Extended-C\": range(11360, 11392),\n    \"Coptic\": range(11392, 11520),\n    \"Georgian Supplement\": range(11520, 11568),\n    \"Tifinagh\": range(11568, 11648),\n    \"Ethiopic Extended\": range(11648, 11744),\n    \"Cyrillic Extended-A\": range(11744, 11776),\n    \"Supplemental Punctuation\": range(11776, 11904),\n    \"CJK Radicals Supplement\": range(11904, 12032),\n    \"Kangxi Radicals\": range(12032, 12256),\n    \"Ideographic Description Characters\": range(12272, 12288),\n    \"CJK Symbols and Punctuation\": range(12288, 12352),\n    \"Hiragana\": range(12352, 12448),\n    \"Katakana\": range(12448, 12544),\n    \"Bopomofo\": range(12544, 12592),\n    \"Hangul Compatibility Jamo\": range(12592, 12688),\n    \"Kanbun\": range(12688, 12704),\n    \"Bopomofo Extended\": range(12704, 12736),\n    \"CJK Strokes\": range(12736, 12784),\n    \"Katakana Phonetic Extensions\": range(12784, 12800),\n    \"Enclosed CJK Letters and Months\": range(12800, 13056),\n    \"CJK Compatibility\": range(13056, 13312),\n    \"CJK Unified Ideographs Extension A\": range(13312, 19904),\n    \"Yijing Hexagram Symbols\": range(19904, 19968),\n    \"CJK Unified Ideographs\": range(19968, 40960),\n    \"Yi Syllables\": range(40960, 42128),\n    \"Yi Radicals\": range(42128, 42192),\n    \"Lisu\": range(42192, 42240),\n    \"Vai\": range(42240, 42560),\n    \"Cyrillic Extended-B\": range(42560, 42656),\n    \"Bamum\": range(42656, 42752),\n    \"Modifier Tone Letters\": range(42752, 42784),\n    \"Latin Extended-D\": range(42784, 43008),\n    \"Syloti Nagri\": range(43008, 43056),\n    \"Common Indic Number Forms\": range(43056, 43072),\n    \"Phags-pa\": range(43072, 43136),\n    \"Saurashtra\": range(43136, 43232),\n    \"Devanagari Extended\": range(43232, 43264),\n    \"Kayah Li\": range(43264, 43312),\n    \"Rejang\": range(43312, 43360),\n    \"Hangul Jamo Extended-A\": range(43360, 43392),\n    \"Javanese\": range(43392, 43488),\n    \"Myanmar Extended-B\": range(43488, 43520),\n    \"Cham\": range(43520, 43616),\n    \"Myanmar Extended-A\": range(43616, 43648),\n    \"Tai Viet\": range(43648, 43744),\n    \"Meetei Mayek Extensions\": range(43744, 43776),\n    \"Ethiopic Extended-A\": range(43776, 43824),\n    \"Latin Extended-E\": range(43824, 43888),\n    \"Cherokee Supplement\": range(43888, 43968),\n    \"Meetei Mayek\": range(43968, 44032),\n    \"Hangul Syllables\": range(44032, 55216),\n    \"Hangul Jamo Extended-B\": range(55216, 55296),\n    \"High Surrogates\": range(55296, 56192),\n    \"High Private Use Surrogates\": range(56192, 56320),\n    \"Low Surrogates\": range(56320, 57344),\n    \"Private Use Area\": range(57344, 63744),\n    \"CJK Compatibility Ideographs\": range(63744, 64256),\n    \"Alphabetic Presentation Forms\": range(64256, 64336),\n    \"Arabic Presentation Forms-A\": range(64336, 65024),\n    \"Variation Selectors\": range(65024, 65040),\n    \"Vertical Forms\": range(65040, 65056),\n    \"Combining Half Marks\": range(65056, 65072),\n    \"CJK Compatibility Forms\": range(65072, 65104),\n    \"Small Form Variants\": range(65104, 65136),\n    \"Arabic Presentation Forms-B\": range(65136, 65280),\n    \"Halfwidth and Fullwidth Forms\": range(65280, 65520),\n    \"Specials\": range(65520, 65536),\n    \"Linear B Syllabary\": range(65536, 65664),\n    \"Linear B Ideograms\": range(65664, 65792),\n    \"Aegean Numbers\": range(65792, 65856),\n    \"Ancient Greek Numbers\": range(65856, 65936),\n    \"Ancient Symbols\": range(65936, 66000),\n    \"Phaistos Disc\": range(66000, 66048),\n    \"Lycian\": range(66176, 66208),\n    \"Carian\": range(66208, 66272),\n    \"Coptic Epact Numbers\": range(66272, 66304),\n    \"Old Italic\": range(66304, 66352),\n    \"Gothic\": range(66352, 66384),\n    \"Old Permic\": range(66384, 66432),\n    \"Ugaritic\": range(66432, 66464),\n    \"Old Persian\": range(66464, 66528),\n    \"Deseret\": range(66560, 66640),\n    \"Shavian\": range(66640, 66688),\n    \"Osmanya\": range(66688, 66736),\n    \"Osage\": range(66736, 66816),\n    \"Elbasan\": range(66816, 66864),\n    \"Caucasian Albanian\": range(66864, 66928),\n    \"Vithkuqi\": range(66928, 67008),\n    \"Linear A\": range(67072, 67456),\n    \"Latin Extended-F\": range(67456, 67520),\n    \"Cypriot Syllabary\": range(67584, 67648),\n    \"Imperial Aramaic\": range(67648, 67680),\n    \"Palmyrene\": range(67680, 67712),\n    \"Nabataean\": range(67712, 67760),\n    \"Hatran\": range(67808, 67840),\n    \"Phoenician\": range(67840, 67872),\n    \"Lydian\": range(67872, 67904),\n    \"Meroitic Hieroglyphs\": range(67968, 68000),\n    \"Meroitic Cursive\": range(68000, 68096),\n    \"Kharoshthi\": range(68096, 68192),\n    \"Old South Arabian\": range(68192, 68224),\n    \"Old North Arabian\": range(68224, 68256),\n    \"Manichaean\": range(68288, 68352),\n    \"Avestan\": range(68352, 68416),\n    \"Inscriptional Parthian\": range(68416, 68448),\n    \"Inscriptional Pahlavi\": range(68448, 68480),\n    \"Psalter Pahlavi\": range(68480, 68528),\n    \"Old Turkic\": range(68608, 68688),\n    \"Old Hungarian\": range(68736, 68864),\n    \"Hanifi Rohingya\": range(68864, 68928),\n    \"Rumi Numeral Symbols\": range(69216, 69248),\n    \"Yezidi\": range(69248, 69312),\n    \"Arabic Extended-C\": range(69312, 69376),\n    \"Old Sogdian\": range(69376, 69424),\n    \"Sogdian\": range(69424, 69488),\n    \"Old Uyghur\": range(69488, 69552),\n    \"Chorasmian\": range(69552, 69600),\n    \"Elymaic\": range(69600, 69632),\n    \"Brahmi\": range(69632, 69760),\n    \"Kaithi\": range(69760, 69840),\n    \"Sora Sompeng\": range(69840, 69888),\n    \"Chakma\": range(69888, 69968),\n    \"Mahajani\": range(69968, 70016),\n    \"Sharada\": range(70016, 70112),\n    \"Sinhala Archaic Numbers\": range(70112, 70144),\n    \"Khojki\": range(70144, 70224),\n    \"Multani\": range(70272, 70320),\n    \"Khudawadi\": range(70320, 70400),\n    \"Grantha\": range(70400, 70528),\n    \"Newa\": range(70656, 70784),\n    \"Tirhuta\": range(70784, 70880),\n    \"Siddham\": range(71040, 71168),\n    \"Modi\": range(71168, 71264),\n    \"Mongolian Supplement\": range(71264, 71296),\n    \"Takri\": range(71296, 71376),\n    \"Ahom\": range(71424, 71504),\n    \"Dogra\": range(71680, 71760),\n    \"Warang Citi\": range(71840, 71936),\n    \"Dives Akuru\": range(71936, 72032),\n    \"Nandinagari\": range(72096, 72192),\n    \"Zanabazar Square\": range(72192, 72272),\n    \"Soyombo\": range(72272, 72368),\n    \"Unified Canadian Aboriginal Syllabics Extended-A\": range(72368, 72384),\n    \"Pau Cin Hau\": range(72384, 72448),\n    \"Devanagari Extended-A\": range(72448, 72544),\n    \"Bhaiksuki\": range(72704, 72816),\n    \"Marchen\": range(72816, 72896),\n    \"Masaram Gondi\": range(72960, 73056),\n    \"Gunjala Gondi\": range(73056, 73136),\n    \"Makasar\": range(73440, 73472),\n    \"Kawi\": range(73472, 73568),\n    \"Lisu Supplement\": range(73648, 73664),\n    \"Tamil Supplement\": range(73664, 73728),\n    \"Cuneiform\": range(73728, 74752),\n    \"Cuneiform Numbers and Punctuation\": range(74752, 74880),\n    \"Early Dynastic Cuneiform\": range(74880, 75088),\n    \"Cypro-Minoan\": range(77712, 77824),\n    \"Egyptian Hieroglyphs\": range(77824, 78896),\n    \"Egyptian Hieroglyph Format Controls\": range(78896, 78944),\n    \"Anatolian Hieroglyphs\": range(82944, 83584),\n    \"Bamum Supplement\": range(92160, 92736),\n    \"Mro\": range(92736, 92784),\n    \"Tangsa\": range(92784, 92880),\n    \"Bassa Vah\": range(92880, 92928),\n    \"Pahawh Hmong\": range(92928, 93072),\n    \"Medefaidrin\": range(93760, 93856),\n    \"Miao\": range(93952, 94112),\n    \"Ideographic Symbols and Punctuation\": range(94176, 94208),\n    \"Tangut\": range(94208, 100352),\n    \"Tangut Components\": range(100352, 101120),\n    \"Khitan Small Script\": range(101120, 101632),\n    \"Tangut Supplement\": range(101632, 101760),\n    \"Kana Extended-B\": range(110576, 110592),\n    \"Kana Supplement\": range(110592, 110848),\n    \"Kana Extended-A\": range(110848, 110896),\n    \"Small Kana Extension\": range(110896, 110960),\n    \"Nushu\": range(110960, 111360),\n    \"Duployan\": range(113664, 113824),\n    \"Shorthand Format Controls\": range(113824, 113840),\n    \"Znamenny Musical Notation\": range(118528, 118736),\n    \"Byzantine Musical Symbols\": range(118784, 119040),\n    \"Musical Symbols\": range(119040, 119296),\n    \"Ancient Greek Musical Notation\": range(119296, 119376),\n    \"Kaktovik Numerals\": range(119488, 119520),\n    \"Mayan Numerals\": range(119520, 119552),\n    \"Tai Xuan Jing Symbols\": range(119552, 119648),\n    \"Counting Rod Numerals\": range(119648, 119680),\n    \"Mathematical Alphanumeric Symbols\": range(119808, 120832),\n    \"Sutton SignWriting\": range(120832, 121520),\n    \"Latin Extended-G\": range(122624, 122880),\n    \"Glagolitic Supplement\": range(122880, 122928),\n    \"Cyrillic Extended-D\": range(122928, 123024),\n    \"Nyiakeng Puachue Hmong\": range(123136, 123216),\n    \"Toto\": range(123536, 123584),\n    \"Wancho\": range(123584, 123648),\n    \"Nag Mundari\": range(124112, 124160),\n    \"Ethiopic Extended-B\": range(124896, 124928),\n    \"Mende Kikakui\": range(124928, 125152),\n    \"Adlam\": range(125184, 125280),\n    \"Indic Siyaq Numbers\": range(126064, 126144),\n    \"Ottoman Siyaq Numbers\": range(126208, 126288),\n    \"Arabic Mathematical Alphabetic Symbols\": range(126464, 126720),\n    \"Mahjong Tiles\": range(126976, 127024),\n    \"Domino Tiles\": range(127024, 127136),\n    \"Playing Cards\": range(127136, 127232),\n    \"Enclosed Alphanumeric Supplement\": range(127232, 127488),\n    \"Enclosed Ideographic Supplement\": range(127488, 127744),\n    \"Miscellaneous Symbols and Pictographs\": range(127744, 128512),\n    \"Emoticons range(Emoji)\": range(128512, 128592),\n    \"Ornamental Dingbats\": range(128592, 128640),\n    \"Transport and Map Symbols\": range(128640, 128768),\n    \"Alchemical Symbols\": range(128768, 128896),\n    \"Geometric Shapes Extended\": range(128896, 129024),\n    \"Supplemental Arrows-C\": range(129024, 129280),\n    \"Supplemental Symbols and Pictographs\": range(129280, 129536),\n    \"Chess Symbols\": range(129536, 129648),\n    \"Symbols and Pictographs Extended-A\": range(129648, 129792),\n    \"Symbols for Legacy Computing\": range(129792, 130048),\n    \"CJK Unified Ideographs Extension B\": range(131072, 173792),\n    \"CJK Unified Ideographs Extension C\": range(173824, 177984),\n    \"CJK Unified Ideographs Extension D\": range(177984, 178208),\n    \"CJK Unified Ideographs Extension E\": range(178208, 183984),\n    \"CJK Unified Ideographs Extension F\": range(183984, 191472),\n    \"CJK Compatibility Ideographs Supplement\": range(194560, 195104),\n    \"CJK Unified Ideographs Extension G\": range(196608, 201552),\n    \"CJK Unified Ideographs Extension H\": range(201552, 205744),\n    \"Tags\": range(917504, 917632),\n    \"Variation Selectors Supplement\": range(917760, 918000),\n    \"Supplementary Private Use Area-A\": range(983040, 1048576),\n    \"Supplementary Private Use Area-B\": range(1048576, 1114112),\n}\n\n\nUNICODE_SECONDARY_RANGE_KEYWORD: List[str] = [\n    \"Supplement\",\n    \"Extended\",\n    \"Extensions\",\n    \"Modifier\",\n    \"Marks\",\n    \"Punctuation\",\n    \"Symbols\",\n    \"Forms\",\n    \"Operators\",\n    \"Miscellaneous\",\n    \"Drawing\",\n    \"Block\",\n    \"Shapes\",\n    \"Supplemental\",\n    \"Tags\",\n]\n\nRE_POSSIBLE_ENCODING_INDICATION = re_compile(\n    r\"(?:(?:encoding)|(?:charset)|(?:coding))(?:[\\:= ]{1,10})(?:[\\\"\\']?)([a-zA-Z0-9\\-_]+)(?:[\\\"\\']?)\",\n    IGNORECASE,\n)\n\nIANA_NO_ALIASES = [\n    \"cp720\",\n    \"cp737\",\n    \"cp856\",\n    \"cp874\",\n    \"cp875\",\n    \"cp1006\",\n    \"koi8_r\",\n    \"koi8_t\",\n    \"koi8_u\",\n]\n\nIANA_SUPPORTED: List[str] = sorted(\n    filter(\n        lambda x: x.endswith(\"_codec\") is False\n        and x not in {\"rot_13\", \"tactis\", \"mbcs\"},\n        list(set(aliases.values())) + IANA_NO_ALIASES,\n    )\n)\n\nIANA_SUPPORTED_COUNT: int = len(IANA_SUPPORTED)\n\n# pre-computed code page that are similar using the function cp_similarity.\nIANA_SUPPORTED_SIMILAR: Dict[str, List[str]] = {\n    \"cp037\": [\"cp1026\", \"cp1140\", \"cp273\", \"cp500\"],\n    \"cp1026\": [\"cp037\", \"cp1140\", \"cp273\", \"cp500\"],\n    \"cp1125\": [\"cp866\"],\n    \"cp1140\": [\"cp037\", \"cp1026\", \"cp273\", \"cp500\"],\n    \"cp1250\": [\"iso8859_2\"],\n    \"cp1251\": [\"kz1048\", \"ptcp154\"],\n    \"cp1252\": [\"iso8859_15\", \"iso8859_9\", \"latin_1\"],\n    \"cp1253\": [\"iso8859_7\"],\n    \"cp1254\": [\"iso8859_15\", \"iso8859_9\", \"latin_1\"],\n    \"cp1257\": [\"iso8859_13\"],\n    \"cp273\": [\"cp037\", \"cp1026\", \"cp1140\", \"cp500\"],\n    \"cp437\": [\"cp850\", \"cp858\", \"cp860\", \"cp861\", \"cp862\", \"cp863\", \"cp865\"],\n    \"cp500\": [\"cp037\", \"cp1026\", \"cp1140\", \"cp273\"],\n    \"cp850\": [\"cp437\", \"cp857\", \"cp858\", \"cp865\"],\n    \"cp857\": [\"cp850\", \"cp858\", \"cp865\"],\n    \"cp858\": [\"cp437\", \"cp850\", \"cp857\", \"cp865\"],\n    \"cp860\": [\"cp437\", \"cp861\", \"cp862\", \"cp863\", \"cp865\"],\n    \"cp861\": [\"cp437\", \"cp860\", \"cp862\", \"cp863\", \"cp865\"],\n    \"cp862\": [\"cp437\", \"cp860\", \"cp861\", \"cp863\", \"cp865\"],\n    \"cp863\": [\"cp437\", \"cp860\", \"cp861\", \"cp862\", \"cp865\"],\n    \"cp865\": [\"cp437\", \"cp850\", \"cp857\", \"cp858\", \"cp860\", \"cp861\", \"cp862\", \"cp863\"],\n    \"cp866\": [\"cp1125\"],\n    \"iso8859_10\": [\"iso8859_14\", \"iso8859_15\", \"iso8859_4\", \"iso8859_9\", \"latin_1\"],\n    \"iso8859_11\": [\"tis_620\"],\n    \"iso8859_13\": [\"cp1257\"],\n    \"iso8859_14\": [\n        \"iso8859_10\",\n        \"iso8859_15\",\n        \"iso8859_16\",\n        \"iso8859_3\",\n        \"iso8859_9\",\n        \"latin_1\",\n    ],\n    \"iso8859_15\": [\n        \"cp1252\",\n        \"cp1254\",\n        \"iso8859_10\",\n        \"iso8859_14\",\n        \"iso8859_16\",\n        \"iso8859_3\",\n        \"iso8859_9\",\n        \"latin_1\",\n    ],\n    \"iso8859_16\": [\n        \"iso8859_14\",\n        \"iso8859_15\",\n        \"iso8859_2\",\n        \"iso8859_3\",\n        \"iso8859_9\",\n        \"latin_1\",\n    ],\n    \"iso8859_2\": [\"cp1250\", \"iso8859_16\", \"iso8859_4\"],\n    \"iso8859_3\": [\"iso8859_14\", \"iso8859_15\", \"iso8859_16\", \"iso8859_9\", \"latin_1\"],\n    \"iso8859_4\": [\"iso8859_10\", \"iso8859_2\", \"iso8859_9\", \"latin_1\"],\n    \"iso8859_7\": [\"cp1253\"],\n    \"iso8859_9\": [\n        \"cp1252\",\n        \"cp1254\",\n        \"cp1258\",\n        \"iso8859_10\",\n        \"iso8859_14\",\n        \"iso8859_15\",\n        \"iso8859_16\",\n        \"iso8859_3\",\n        \"iso8859_4\",\n        \"latin_1\",\n    ],\n    \"kz1048\": [\"cp1251\", \"ptcp154\"],\n    \"latin_1\": [\n        \"cp1252\",\n        \"cp1254\",\n        \"cp1258\",\n        \"iso8859_10\",\n        \"iso8859_14\",\n        \"iso8859_15\",\n        \"iso8859_16\",\n        \"iso8859_3\",\n        \"iso8859_4\",\n        \"iso8859_9\",\n    ],\n    \"mac_iceland\": [\"mac_roman\", \"mac_turkish\"],\n    \"mac_roman\": [\"mac_iceland\", \"mac_turkish\"],\n    \"mac_turkish\": [\"mac_iceland\", \"mac_roman\"],\n    \"ptcp154\": [\"cp1251\", \"kz1048\"],\n    \"tis_620\": [\"iso8859_11\"],\n}\n\n\nCHARDET_CORRESPONDENCE: Dict[str, str] = {\n    \"iso2022_kr\": \"ISO-2022-KR\",\n    \"iso2022_jp\": \"ISO-2022-JP\",\n    \"euc_kr\": \"EUC-KR\",\n    \"tis_620\": \"TIS-620\",\n    \"utf_32\": \"UTF-32\",\n    \"euc_jp\": \"EUC-JP\",\n    \"koi8_r\": \"KOI8-R\",\n    \"iso8859_1\": \"ISO-8859-1\",\n    \"iso8859_2\": \"ISO-8859-2\",\n    \"iso8859_5\": \"ISO-8859-5\",\n    \"iso8859_6\": \"ISO-8859-6\",\n    \"iso8859_7\": \"ISO-8859-7\",\n    \"iso8859_8\": \"ISO-8859-8\",\n    \"utf_16\": \"UTF-16\",\n    \"cp855\": \"IBM855\",\n    \"mac_cyrillic\": \"MacCyrillic\",\n    \"gb2312\": \"GB2312\",\n    \"gb18030\": \"GB18030\",\n    \"cp932\": \"CP932\",\n    \"cp866\": \"IBM866\",\n    \"utf_8\": \"utf-8\",\n    \"utf_8_sig\": \"UTF-8-SIG\",\n    \"shift_jis\": \"SHIFT_JIS\",\n    \"big5\": \"Big5\",\n    \"cp1250\": \"windows-1250\",\n    \"cp1251\": \"windows-1251\",\n    \"cp1252\": \"Windows-1252\",\n    \"cp1253\": \"windows-1253\",\n    \"cp1255\": \"windows-1255\",\n    \"cp1256\": \"windows-1256\",\n    \"cp1254\": \"Windows-1254\",\n    \"cp949\": \"CP949\",\n}\n\n\nCOMMON_SAFE_ASCII_CHARACTERS: Set[str] = {\n    \"<\",\n    \">\",\n    \"=\",\n    \":\",\n    \"/\",\n    \"&\",\n    \";\",\n    \"{\",\n    \"}\",\n    \"[\",\n    \"]\",\n    \",\",\n    \"|\",\n    '\"',\n    \"-\",\n}\n\n\nKO_NAMES: Set[str] = {\"johab\", \"cp949\", \"euc_kr\"}\nZH_NAMES: Set[str] = {\"big5\", \"cp950\", \"big5hkscs\", \"hz\"}\n\n# Logging LEVEL below DEBUG\nTRACE: int = 5\n\n\n# Language label that contain the em dash \"\u2014\"\n# character are to be considered alternative seq to origin\nFREQUENCIES: Dict[str, List[str]] = {\n    \"English\": [\n        \"e\",\n        \"a\",\n        \"t\",\n        \"i\",\n        \"o\",\n        \"n\",\n        \"s\",\n        \"r\",\n        \"h\",\n        \"l\",\n        \"d\",\n        \"c\",\n        \"u\",\n        \"m\",\n        \"f\",\n        \"p\",\n        \"g\",\n        \"w\",\n        \"y\",\n        \"b\",\n        \"v\",\n        \"k\",\n        \"x\",\n        \"j\",\n        \"z\",\n        \"q\",\n    ],\n    \"English\u2014\": [\n        \"e\",\n        \"a\",\n        \"t\",\n        \"i\",\n        \"o\",\n        \"n\",\n        \"s\",\n        \"r\",\n        \"h\",\n        \"l\",\n        \"d\",\n        \"c\",\n        \"m\",\n        \"u\",\n        \"f\",\n        \"p\",\n        \"g\",\n        \"w\",\n        \"b\",\n        \"y\",\n        \"v\",\n        \"k\",\n        \"j\",\n        \"x\",\n        \"z\",\n        \"q\",\n    ],\n    \"German\": [\n        \"e\",\n        \"n\",\n        \"i\",\n        \"r\",\n        \"s\",\n        \"t\",\n        \"a\",\n        \"d\",\n        \"h\",\n        \"u\",\n        \"l\",\n        \"g\",\n        \"o\",\n        \"c\",\n        \"m\",\n        \"b\",\n        \"f\",\n        \"k\",\n        \"w\",\n        \"z\",\n        \"p\",\n        \"v\",\n        \"\u00fc\",\n        \"\u00e4\",\n        \"\u00f6\",\n        \"j\",\n    ],\n    \"French\": [\n        \"e\",\n        \"a\",\n        \"s\",\n        \"n\",\n        \"i\",\n        \"t\",\n        \"r\",\n        \"l\",\n        \"u\",\n        \"o\",\n        \"d\",\n        \"c\",\n        \"p\",\n        \"m\",\n        \"\u00e9\",\n        \"v\",\n        \"g\",\n        \"f\",\n        \"b\",\n        \"h\",\n        \"q\",\n        \"\u00e0\",\n        \"x\",\n        \"\u00e8\",\n        \"y\",\n        \"j\",\n    ],\n    \"Dutch\": [\n        \"e\",\n        \"n\",\n        \"a\",\n        \"i\",\n        \"r\",\n        \"t\",\n        \"o\",\n        \"d\",\n        \"s\",\n        \"l\",\n        \"g\",\n        \"h\",\n        \"v\",\n        \"m\",\n        \"u\",\n        \"k\",\n        \"c\",\n        \"p\",\n        \"b\",\n        \"w\",\n        \"j\",\n        \"z\",\n        \"f\",\n        \"y\",\n        \"x\",\n        \"\u00eb\",\n    ],\n    \"Italian\": [\n        \"e\",\n        \"i\",\n        \"a\",\n        \"o\",\n        \"n\",\n        \"l\",\n        \"t\",\n        \"r\",\n        \"s\",\n        \"c\",\n        \"d\",\n        \"u\",\n        \"p\",\n        \"m\",\n        \"g\",\n        \"v\",\n        \"f\",\n        \"b\",\n        \"z\",\n        \"h\",\n        \"q\",\n        \"\u00e8\",\n        \"\u00e0\",\n        \"k\",\n        \"y\",\n        \"\u00f2\",\n    ],\n    \"Polish\": [\n        \"a\",\n        \"i\",\n        \"o\",\n        \"e\",\n        \"n\",\n        \"r\",\n        \"z\",\n        \"w\",\n        \"s\",\n        \"c\",\n        \"t\",\n        \"k\",\n        \"y\",\n        \"d\",\n        \"p\",\n        \"m\",\n        \"u\",\n        \"l\",\n        \"j\",\n        \"\u0142\",\n        \"g\",\n        \"b\",\n        \"h\",\n        \"\u0105\",\n        \"\u0119\",\n        \"\u00f3\",\n    ],\n    \"Spanish\": [\n        \"e\",\n        \"a\",\n        \"o\",\n        \"n\",\n        \"s\",\n        \"r\",\n        \"i\",\n        \"l\",\n        \"d\",\n        \"t\",\n        \"c\",\n        \"u\",\n        \"m\",\n        \"p\",\n        \"b\",\n        \"g\",\n        \"v\",\n        \"f\",\n        \"y\",\n        \"\u00f3\",\n        \"h\",\n        \"q\",\n        \"\u00ed\",\n        \"j\",\n        \"z\",\n        \"\u00e1\",\n    ],\n    \"Russian\": [\n        \"\u043e\",\n        \"\u0430\",\n        \"\u0435\",\n        \"\u0438\",\n        \"\u043d\",\n        \"\u0441\",\n        \"\u0442\",\n        \"\u0440\",\n        \"\u0432\",\n        \"\u043b\",\n        \"\u043a\",\n        \"\u043c\",\n        \"\u0434\",\n        \"\u043f\",\n        \"\u0443\",\n        \"\u0433\",\n        \"\u044f\",\n        \"\u044b\",\n        \"\u0437\",\n        \"\u0431\",\n        \"\u0439\",\n        \"\u044c\",\n        \"\u0447\",\n        \"\u0445\",\n        \"\u0436\",\n        \"\u0446\",\n    ],\n    # Jap-Kanji\n    \"Japanese\": [\n        \"\u4eba\",\n        \"\u4e00\",\n        \"\u5927\",\n        \"\u4e85\",\n        \"\u4e01\",\n        \"\u4e28\",\n        \"\u7af9\",\n        \"\u7b11\",\n        \"\u53e3\",\n        \"\u65e5\",\n        \"\u4eca\",\n        \"\u4e8c\",\n        \"\u5f73\",\n        \"\u884c\",\n        \"\u5341\",\n        \"\u571f\",\n        \"\u4e36\",\n        \"\u5bf8\",\n        \"\u5bfa\",\n        \"\u6642\",\n        \"\u4e59\",\n        \"\u4e3f\",\n        \"\u4e42\",\n        \"\u6c14\",\n        \"\u6c17\",\n        \"\u5182\",\n        \"\u5dfe\",\n        \"\u4ea0\",\n        \"\u5e02\",\n        \"\u76ee\",\n        \"\u513f\",\n        \"\u898b\",\n        \"\u516b\",\n        \"\u5c0f\",\n        \"\u51f5\",\n        \"\u770c\",\n        \"\u6708\",\n        \"\u5f50\",\n        \"\u9580\",\n        \"\u9593\",\n        \"\u6728\",\n        \"\u6771\",\n        \"\u5c71\",\n        \"\u51fa\",\n        \"\u672c\",\n        \"\u4e2d\",\n        \"\u5200\",\n        \"\u5206\",\n        \"\u8033\",\n        \"\u53c8\",\n        \"\u53d6\",\n        \"\u6700\",\n        \"\u8a00\",\n        \"\u7530\",\n        \"\u5fc3\",\n        \"\u601d\",\n        \"\u5202\",\n        \"\u524d\",\n        \"\u4eac\",\n        \"\u5c39\",\n        \"\u4e8b\",\n        \"\u751f\",\n        \"\u53b6\",\n        \"\u4e91\",\n        \"\u4f1a\",\n        \"\u672a\",\n        \"\u6765\",\n        \"\u767d\",\n        \"\u51ab\",\n        \"\u697d\",\n        \"\u706c\",\n        \"\u99ac\",\n        \"\u5c38\",\n        \"\u5c3a\",\n        \"\u99c5\",\n        \"\u660e\",\n        \"\u8002\",\n        \"\u8005\",\n        \"\u4e86\",\n        \"\u961d\",\n        \"\u90fd\",\n        \"\u9ad8\",\n        \"\u535c\",\n        \"\u5360\",\n        \"\u5382\",\n        \"\u5e7f\",\n        \"\u5e97\",\n        \"\u5b50\",\n        \"\u7533\",\n        \"\u5944\",\n        \"\u4ebb\",\n        \"\u4ffa\",\n        \"\u4e0a\",\n        \"\u65b9\",\n        \"\u5196\",\n        \"\u5b66\",\n        \"\u8863\",\n        \"\u826e\",\n        \"\u98df\",\n        \"\u81ea\",\n    ],\n    # Jap-Katakana\n    \"Japanese\u2014\": [\n        \"\u30fc\",\n        \"\u30f3\",\n        \"\u30b9\",\n        \"\u30fb\",\n        \"\u30eb\",\n        \"\u30c8\",\n        \"\u30ea\",\n        \"\u30a4\",\n        \"\u30a2\",\n        \"\u30e9\",\n        \"\u30c3\",\n        \"\u30af\",\n        \"\u30c9\",\n        \"\u30b7\",\n        \"\u30ec\",\n        \"\u30b8\",\n        \"\u30bf\",\n        \"\u30d5\",\n        \"\u30ed\",\n        \"\u30ab\",\n        \"\u30c6\",\n        \"\u30de\",\n        \"\u30a3\",\n        \"\u30b0\",\n        \"\u30d0\",\n        \"\u30e0\",\n        \"\u30d7\",\n        \"\u30aa\",\n        \"\u30b3\",\n        \"\u30c7\",\n        \"\u30cb\",\n        \"\u30a6\",\n        \"\u30e1\",\n        \"\u30b5\",\n        \"\u30d3\",\n        \"\u30ca\",\n        \"\u30d6\",\n        \"\u30e3\",\n        \"\u30a8\",\n        \"\u30e5\",\n        \"\u30c1\",\n        \"\u30ad\",\n        \"\u30ba\",\n        \"\u30c0\",\n        \"\u30d1\",\n        \"\u30df\",\n        \"\u30a7\",\n        \"\u30e7\",\n        \"\u30cf\",\n        \"\u30bb\",\n        \"\u30d9\",\n        \"\u30ac\",\n        \"\u30e2\",\n        \"\u30c4\",\n        \"\u30cd\",\n        \"\u30dc\",\n        \"\u30bd\",\n        \"\u30ce\",\n        \"\u30a1\",\n        \"\u30f4\",\n        \"\u30ef\",\n        \"\u30dd\",\n        \"\u30da\",\n        \"\u30d4\",\n        \"\u30b1\",\n        \"\u30b4\",\n        \"\u30ae\",\n        \"\u30b6\",\n        \"\u30db\",\n        \"\u30b2\",\n        \"\u30a9\",\n        \"\u30e4\",\n        \"\u30d2\",\n        \"\u30e6\",\n        \"\u30e8\",\n        \"\u30d8\",\n        \"\u30bc\",\n        \"\u30cc\",\n        \"\u30a5\",\n        \"\u30be\",\n        \"\u30f6\",\n        \"\u30c2\",\n        \"\u30f2\",\n        \"\u30c5\",\n        \"\u30f5\",\n        \"\u30f1\",\n        \"\u30f0\",\n        \"\u30ee\",\n        \"\u30fd\",\n        \"\u30a0\",\n        \"\u30fe\",\n        \"\u30f7\",\n        \"\u30ff\",\n        \"\u30f8\",\n        \"\u30f9\",\n        \"\u30fa\",\n    ],\n    # Jap-Hiragana\n    \"Japanese\u2014\u2014\": [\n        \"\u306e\",\n        \"\u306b\",\n        \"\u308b\",\n        \"\u305f\",\n        \"\u3068\",\n        \"\u306f\",\n        \"\u3057\",\n        \"\u3044\",\n        \"\u3092\",\n        \"\u3067\",\n        \"\u3066\",\n        \"\u304c\",\n        \"\u306a\",\n        \"\u308c\",\n        \"\u304b\",\n        \"\u3089\",\n        \"\u3055\",\n        \"\u3063\",\n        \"\u308a\",\n        \"\u3059\",\n        \"\u3042\",\n        \"\u3082\",\n        \"\u3053\",\n        \"\u307e\",\n        \"\u3046\",\n        \"\u304f\",\n        \"\u3088\",\n        \"\u304d\",\n        \"\u3093\",\n        \"\u3081\",\n        \"\u304a\",\n        \"\u3051\",\n        \"\u305d\",\n        \"\u3064\",\n        \"\u3060\",\n        \"\u3084\",\n        \"\u3048\",\n        \"\u3069\",\n        \"\u308f\",\n        \"\u3061\",\n        \"\u307f\",\n        \"\u305b\",\n        \"\u3058\",\n        \"\u3070\",\n        \"\u3078\",\n        \"\u3073\",\n        \"\u305a\",\n        \"\u308d\",\n        \"\u307b\",\n        \"\u3052\",\n        \"\u3080\",\n        \"\u3079\",\n        \"\u3072\",\n        \"\u3087\",\n        \"\u3086\",\n        \"\u3076\",\n        \"\u3054\",\n        \"\u3083\",\n        \"\u306d\",\n        \"\u3075\",\n        \"\u3050\",\n        \"\u304e\",\n        \"\u307c\",\n        \"\u3085\",\n        \"\u3065\",\n        \"\u3056\",\n        \"\u305e\",\n        \"\u306c\",\n        \"\u305c\",\n        \"\u3071\",\n        \"\u307d\",\n        \"\u3077\",\n        \"\u3074\",\n        \"\u3043\",\n        \"\u3041\",\n        \"\u3047\",\n        \"\u307a\",\n        \"\u309e\",\n        \"\u3062\",\n        \"\u3049\",\n        \"\u3045\",\n        \"\u3090\",\n        \"\u309d\",\n        \"\u3091\",\n        \"\u309b\",\n        \"\u309c\",\n        \"\u308e\",\n        \"\u3094\",\n        \"\u309a\",\n        \"\u309f\",\n        \"\u3099\",\n        \"\u3095\",\n        \"\u3096\",\n    ],\n    \"Portuguese\": [\n        \"a\",\n        \"e\",\n        \"o\",\n        \"s\",\n        \"i\",\n        \"r\",\n        \"d\",\n        \"n\",\n        \"t\",\n        \"m\",\n        \"u\",\n        \"c\",\n        \"l\",\n        \"p\",\n        \"g\",\n        \"v\",\n        \"b\",\n        \"f\",\n        \"h\",\n        \"\u00e3\",\n        \"q\",\n        \"\u00e9\",\n        \"\u00e7\",\n        \"\u00e1\",\n        \"z\",\n        \"\u00ed\",\n    ],\n    \"Swedish\": [\n        \"e\",\n        \"a\",\n        \"n\",\n        \"r\",\n        \"t\",\n        \"s\",\n        \"i\",\n        \"l\",\n        \"d\",\n        \"o\",\n        \"m\",\n        \"k\",\n        \"g\",\n        \"v\",\n        \"h\",\n        \"f\",\n        \"u\",\n        \"p\",\n        \"\u00e4\",\n        \"c\",\n        \"b\",\n        \"\u00f6\",\n        \"\u00e5\",\n        \"y\",\n        \"j\",\n        \"x\",\n    ],\n    \"Chinese\": [\n        \"\u7684\",\n        \"\u4e00\",\n        \"\u662f\",\n        \"\u4e0d\",\n        \"\u4e86\",\n        \"\u5728\",\n        \"\u4eba\",\n        \"\u6709\",\n        \"\u6211\",\n        \"\u4ed6\",\n        \"\u8fd9\",\n        \"\u4e2a\",\n        \"\u4eec\",\n        \"\u4e2d\",\n        \"\u6765\",\n        \"\u4e0a\",\n        \"\u5927\",\n        \"\u4e3a\",\n        \"\u548c\",\n        \"\u56fd\",\n        \"\u5730\",\n        \"\u5230\",\n        \"\u4ee5\",\n        \"\u8bf4\",\n        \"\u65f6\",\n        \"\u8981\",\n        \"\u5c31\",\n        \"\u51fa\",\n        \"\u4f1a\",\n        \"\u53ef\",\n        \"\u4e5f\",\n        \"\u4f60\",\n        \"\u5bf9\",\n        \"\u751f\",\n        \"\u80fd\",\n        \"\u800c\",\n        \"\u5b50\",\n        \"\u90a3\",\n        \"\u5f97\",\n        \"\u4e8e\",\n        \"\u7740\",\n        \"\u4e0b\",\n        \"\u81ea\",\n        \"\u4e4b\",\n        \"\u5e74\",\n        \"\u8fc7\",\n        \"\u53d1\",\n        \"\u540e\",\n        \"\u4f5c\",\n        \"\u91cc\",\n        \"\u7528\",\n        \"\u9053\",\n        \"\u884c\",\n        \"\u6240\",\n        \"\u7136\",\n        \"\u5bb6\",\n        \"\u79cd\",\n        \"\u4e8b\",\n        \"\u6210\",\n        \"\u65b9\",\n        \"\u591a\",\n        \"\u7ecf\",\n        \"\u4e48\",\n        \"\u53bb\",\n        \"\u6cd5\",\n        \"\u5b66\",\n        \"\u5982\",\n        \"\u90fd\",\n        \"\u540c\",\n        \"\u73b0\",\n        \"\u5f53\",\n        \"\u6ca1\",\n        \"\u52a8\",\n        \"\u9762\",\n        \"\u8d77\",\n        \"\u770b\",\n        \"\u5b9a\",\n        \"\u5929\",\n        \"\u5206\",\n        \"\u8fd8\",\n        \"\u8fdb\",\n        \"\u597d\",\n        \"\u5c0f\",\n        \"\u90e8\",\n        \"\u5176\",\n        \"\u4e9b\",\n        \"\u4e3b\",\n        \"\u6837\",\n        \"\u7406\",\n        \"\u5fc3\",\n        \"\u5979\",\n        \"\u672c\",\n        \"\u524d\",\n        \"\u5f00\",\n        \"\u4f46\",\n        \"\u56e0\",\n        \"\u53ea\",\n        \"\u4ece\",\n        \"\u60f3\",\n        \"\u5b9e\",\n    ],\n    \"Ukrainian\": [\n        \"\u043e\",\n        \"\u0430\",\n        \"\u043d\",\n        \"\u0456\",\n        \"\u0438\",\n        \"\u0440\",\n        \"\u0432\",\n        \"\u0442\",\n        \"\u0435\",\n        \"\u0441\",\n        \"\u043a\",\n        \"\u043b\",\n        \"\u0443\",\n        \"\u0434\",\n        \"\u043c\",\n        \"\u043f\",\n        \"\u0437\",\n        \"\u044f\",\n        \"\u044c\",\n        \"\u0431\",\n        \"\u0433\",\n        \"\u0439\",\n        \"\u0447\",\n        \"\u0445\",\n        \"\u0446\",\n        \"\u0457\",\n    ],\n    \"Norwegian\": [\n        \"e\",\n        \"r\",\n        \"n\",\n        \"t\",\n        \"a\",\n        \"s\",\n        \"i\",\n        \"o\",\n        \"l\",\n        \"d\",\n        \"g\",\n        \"k\",\n        \"m\",\n        \"v\",\n        \"f\",\n        \"p\",\n        \"u\",\n        \"b\",\n        \"h\",\n        \"\u00e5\",\n        \"y\",\n        \"j\",\n        \"\u00f8\",\n        \"c\",\n        \"\u00e6\",\n        \"w\",\n    ],\n    \"Finnish\": [\n        \"a\",\n        \"i\",\n        \"n\",\n        \"t\",\n        \"e\",\n        \"s\",\n        \"l\",\n        \"o\",\n        \"u\",\n        \"k\",\n        \"\u00e4\",\n        \"m\",\n        \"r\",\n        \"v\",\n        \"j\",\n        \"h\",\n        \"p\",\n        \"y\",\n        \"d\",\n        \"\u00f6\",\n        \"g\",\n        \"c\",\n        \"b\",\n        \"f\",\n        \"w\",\n        \"z\",\n    ],\n    \"Vietnamese\": [\n        \"n\",\n        \"h\",\n        \"t\",\n        \"i\",\n        \"c\",\n        \"g\",\n        \"a\",\n        \"o\",\n        \"u\",\n        \"m\",\n        \"l\",\n        \"r\",\n        \"\u00e0\",\n        \"\u0111\",\n        \"s\",\n        \"e\",\n        \"v\",\n        \"p\",\n        \"b\",\n        \"y\",\n        \"\u01b0\",\n        \"d\",\n        \"\u00e1\",\n        \"k\",\n        \"\u1ed9\",\n        \"\u1ebf\",\n    ],\n    \"Czech\": [\n        \"o\",\n        \"e\",\n        \"a\",\n        \"n\",\n        \"t\",\n        \"s\",\n        \"i\",\n        \"l\",\n        \"v\",\n        \"r\",\n        \"k\",\n        \"d\",\n        \"u\",\n        \"m\",\n        \"p\",\n        \"\u00ed\",\n        \"c\",\n        \"h\",\n        \"z\",\n        \"\u00e1\",\n        \"y\",\n        \"j\",\n        \"b\",\n        \"\u011b\",\n        \"\u00e9\",\n        \"\u0159\",\n    ],\n    \"Hungarian\": [\n        \"e\",\n        \"a\",\n        \"t\",\n        \"l\",\n        \"s\",\n        \"n\",\n        \"k\",\n        \"r\",\n        \"i\",\n        \"o\",\n        \"z\",\n        \"\u00e1\",\n        \"\u00e9\",\n        \"g\",\n        \"m\",\n        \"b\",\n        \"y\",\n        \"v\",\n        \"d\",\n        \"h\",\n        \"u\",\n        \"p\",\n        \"j\",\n        \"\u00f6\",\n        \"f\",\n        \"c\",\n    ],\n    \"Korean\": [\n        \"\uc774\",\n        \"\ub2e4\",\n        \"\uc5d0\",\n        \"\uc758\",\n        \"\ub294\",\n        \"\ub85c\",\n        \"\ud558\",\n        \"\uc744\",\n        \"\uac00\",\n        \"\uace0\",\n        \"\uc9c0\",\n        \"\uc11c\",\n        \"\ud55c\",\n        \"\uc740\",\n        \"\uae30\",\n        \"\uc73c\",\n        \"\ub144\",\n        \"\ub300\",\n        \"\uc0ac\",\n        \"\uc2dc\",\n        \"\ub97c\",\n        \"\ub9ac\",\n        \"\ub3c4\",\n        \"\uc778\",\n        \"\uc2a4\",\n        \"\uc77c\",\n    ],\n    \"Indonesian\": [\n        \"a\",\n        \"n\",\n        \"e\",\n        \"i\",\n        \"r\",\n        \"t\",\n        \"u\",\n        \"s\",\n        \"d\",\n        \"k\",\n        \"m\",\n        \"l\",\n        \"g\",\n        \"p\",\n        \"b\",\n        \"o\",\n        \"h\",\n        \"y\",\n        \"j\",\n        \"c\",\n        \"w\",\n        \"f\",\n        \"v\",\n        \"z\",\n        \"x\",\n        \"q\",\n    ],\n    \"Turkish\": [\n        \"a\",\n        \"e\",\n        \"i\",\n        \"n\",\n        \"r\",\n        \"l\",\n        \"\u0131\",\n        \"k\",\n        \"d\",\n        \"t\",\n        \"s\",\n        \"m\",\n        \"y\",\n        \"u\",\n        \"o\",\n        \"b\",\n        \"\u00fc\",\n        \"\u015f\",\n        \"v\",\n        \"g\",\n        \"z\",\n        \"h\",\n        \"c\",\n        \"p\",\n        \"\u00e7\",\n        \"\u011f\",\n    ],\n    \"Romanian\": [\n        \"e\",\n        \"i\",\n        \"a\",\n        \"r\",\n        \"n\",\n        \"t\",\n        \"u\",\n        \"l\",\n        \"o\",\n        \"c\",\n        \"s\",\n        \"d\",\n        \"p\",\n        \"m\",\n        \"\u0103\",\n        \"f\",\n        \"v\",\n        \"\u00ee\",\n        \"g\",\n        \"b\",\n        \"\u0219\",\n        \"\u021b\",\n        \"z\",\n        \"h\",\n        \"\u00e2\",\n        \"j\",\n    ],\n    \"Farsi\": [\n        \"\u0627\",\n        \"\u06cc\",\n        \"\u0631\",\n        \"\u062f\",\n        \"\u0646\",\n        \"\u0647\",\n        \"\u0648\",\n        \"\u0645\",\n        \"\u062a\",\n        \"\u0628\",\n        \"\u0633\",\n        \"\u0644\",\n        \"\u06a9\",\n        \"\u0634\",\n        \"\u0632\",\n        \"\u0641\",\n        \"\u06af\",\n        \"\u0639\",\n        \"\u062e\",\n        \"\u0642\",\n        \"\u062c\",\n        \"\u0622\",\n        \"\u067e\",\n        \"\u062d\",\n        \"\u0637\",\n        \"\u0635\",\n    ],\n    \"Arabic\": [\n        \"\u0627\",\n        \"\u0644\",\n        \"\u064a\",\n        \"\u0645\",\n        \"\u0648\",\n        \"\u0646\",\n        \"\u0631\",\n        \"\u062a\",\n        \"\u0628\",\n        \"\u0629\",\n        \"\u0639\",\n        \"\u062f\",\n        \"\u0633\",\n        \"\u0641\",\n        \"\u0647\",\n        \"\u0643\",\n        \"\u0642\",\n        \"\u0623\",\n        \"\u062d\",\n        \"\u062c\",\n        \"\u0634\",\n        \"\u0637\",\n        \"\u0635\",\n        \"\u0649\",\n        \"\u062e\",\n        \"\u0625\",\n    ],\n    \"Danish\": [\n        \"e\",\n        \"r\",\n        \"n\",\n        \"t\",\n        \"a\",\n        \"i\",\n        \"s\",\n        \"d\",\n        \"l\",\n        \"o\",\n        \"g\",\n        \"m\",\n        \"k\",\n        \"f\",\n        \"v\",\n        \"u\",\n        \"b\",\n        \"h\",\n        \"p\",\n        \"\u00e5\",\n        \"y\",\n        \"\u00f8\",\n        \"\u00e6\",\n        \"c\",\n        \"j\",\n        \"w\",\n    ],\n    \"Serbian\": [\n        \"\u0430\",\n        \"\u0438\",\n        \"\u043e\",\n        \"\u0435\",\n        \"\u043d\",\n        \"\u0440\",\n        \"\u0441\",\n        \"\u0443\",\n        \"\u0442\",\n        \"\u043a\",\n        \"\u0458\",\n        \"\u0432\",\n        \"\u0434\",\n        \"\u043c\",\n        \"\u043f\",\n        \"\u043b\",\n        \"\u0433\",\n        \"\u0437\",\n        \"\u0431\",\n        \"a\",\n        \"i\",\n        \"e\",\n        \"o\",\n        \"n\",\n        \"\u0446\",\n        \"\u0448\",\n    ],\n    \"Lithuanian\": [\n        \"i\",\n        \"a\",\n        \"s\",\n        \"o\",\n        \"r\",\n        \"e\",\n        \"t\",\n        \"n\",\n        \"u\",\n        \"k\",\n        \"m\",\n        \"l\",\n        \"p\",\n        \"v\",\n        \"d\",\n        \"j\",\n        \"g\",\n        \"\u0117\",\n        \"b\",\n        \"y\",\n        \"\u0173\",\n        \"\u0161\",\n        \"\u017e\",\n        \"c\",\n        \"\u0105\",\n        \"\u012f\",\n    ],\n    \"Slovene\": [\n        \"e\",\n        \"a\",\n        \"i\",\n        \"o\",\n        \"n\",\n        \"r\",\n        \"s\",\n        \"l\",\n        \"t\",\n        \"j\",\n        \"v\",\n        \"k\",\n        \"d\",\n        \"p\",\n        \"m\",\n        \"u\",\n        \"z\",\n        \"b\",\n        \"g\",\n        \"h\",\n        \"\u010d\",\n        \"c\",\n        \"\u0161\",\n        \"\u017e\",\n        \"f\",\n        \"y\",\n    ],\n    \"Slovak\": [\n        \"o\",\n        \"a\",\n        \"e\",\n        \"n\",\n        \"i\",\n        \"r\",\n        \"v\",\n        \"t\",\n        \"s\",\n        \"l\",\n        \"k\",\n        \"d\",\n        \"m\",\n        \"p\",\n        \"u\",\n        \"c\",\n        \"h\",\n        \"j\",\n        \"b\",\n        \"z\",\n        \"\u00e1\",\n        \"y\",\n        \"\u00fd\",\n        \"\u00ed\",\n        \"\u010d\",\n        \"\u00e9\",\n    ],\n    \"Hebrew\": [\n        \"\u05d9\",\n        \"\u05d5\",\n        \"\u05d4\",\n        \"\u05dc\",\n        \"\u05e8\",\n        \"\u05d1\",\n        \"\u05ea\",\n        \"\u05de\",\n        \"\u05d0\",\n        \"\u05e9\",\n        \"\u05e0\",\n        \"\u05e2\",\n        \"\u05dd\",\n        \"\u05d3\",\n        \"\u05e7\",\n        \"\u05d7\",\n        \"\u05e4\",\n        \"\u05e1\",\n        \"\u05db\",\n        \"\u05d2\",\n        \"\u05d8\",\n        \"\u05e6\",\n        \"\u05df\",\n        \"\u05d6\",\n        \"\u05da\",\n    ],\n    \"Bulgarian\": [\n        \"\u0430\",\n        \"\u0438\",\n        \"\u043e\",\n        \"\u0435\",\n        \"\u043d\",\n        \"\u0442\",\n        \"\u0440\",\n        \"\u0441\",\n        \"\u0432\",\n        \"\u043b\",\n        \"\u043a\",\n        \"\u0434\",\n        \"\u043f\",\n        \"\u043c\",\n        \"\u0437\",\n        \"\u0433\",\n        \"\u044f\",\n        \"\u044a\",\n        \"\u0443\",\n        \"\u0431\",\n        \"\u0447\",\n        \"\u0446\",\n        \"\u0439\",\n        \"\u0436\",\n        \"\u0449\",\n        \"\u0445\",\n    ],\n    \"Croatian\": [\n        \"a\",\n        \"i\",\n        \"o\",\n        \"e\",\n        \"n\",\n        \"r\",\n        \"j\",\n        \"s\",\n        \"t\",\n        \"u\",\n        \"k\",\n        \"l\",\n        \"v\",\n        \"d\",\n        \"m\",\n        \"p\",\n        \"g\",\n        \"z\",\n        \"b\",\n        \"c\",\n        \"\u010d\",\n        \"h\",\n        \"\u0161\",\n        \"\u017e\",\n        \"\u0107\",\n        \"f\",\n    ],\n    \"Hindi\": [\n        \"\u0915\",\n        \"\u0930\",\n        \"\u0938\",\n        \"\u0928\",\n        \"\u0924\",\n        \"\u092e\",\n        \"\u0939\",\n        \"\u092a\",\n        \"\u092f\",\n        \"\u0932\",\n        \"\u0935\",\n        \"\u091c\",\n        \"\u0926\",\n        \"\u0917\",\n        \"\u092c\",\n        \"\u0936\",\n        \"\u091f\",\n        \"\u0905\",\n        \"\u090f\",\n        \"\u0925\",\n        \"\u092d\",\n        \"\u0921\",\n        \"\u091a\",\n        \"\u0927\",\n        \"\u0937\",\n        \"\u0907\",\n    ],\n    \"Estonian\": [\n        \"a\",\n        \"i\",\n        \"e\",\n        \"s\",\n        \"t\",\n        \"l\",\n        \"u\",\n        \"n\",\n        \"o\",\n        \"k\",\n        \"r\",\n        \"d\",\n        \"m\",\n        \"v\",\n        \"g\",\n        \"p\",\n        \"j\",\n        \"h\",\n        \"\u00e4\",\n        \"b\",\n        \"\u00f5\",\n        \"\u00fc\",\n        \"f\",\n        \"c\",\n        \"\u00f6\",\n        \"y\",\n    ],\n    \"Thai\": [\n        \"\u0e32\",\n        \"\u0e19\",\n        \"\u0e23\",\n        \"\u0e2d\",\n        \"\u0e01\",\n        \"\u0e40\",\n        \"\u0e07\",\n        \"\u0e21\",\n        \"\u0e22\",\n        \"\u0e25\",\n        \"\u0e27\",\n        \"\u0e14\",\n        \"\u0e17\",\n        \"\u0e2a\",\n        \"\u0e15\",\n        \"\u0e30\",\n        \"\u0e1b\",\n        \"\u0e1a\",\n        \"\u0e04\",\n        \"\u0e2b\",\n        \"\u0e41\",\n        \"\u0e08\",\n        \"\u0e1e\",\n        \"\u0e0a\",\n        \"\u0e02\",\n        \"\u0e43\",\n    ],\n    \"Greek\": [\n        \"\u03b1\",\n        \"\u03c4\",\n        \"\u03bf\",\n        \"\u03b9\",\n        \"\u03b5\",\n        \"\u03bd\",\n        \"\u03c1\",\n        \"\u03c3\",\n        \"\u03ba\",\n        \"\u03b7\",\n        \"\u03c0\",\n        \"\u03c2\",\n        \"\u03c5\",\n        \"\u03bc\",\n        \"\u03bb\",\n        \"\u03af\",\n        \"\u03cc\",\n        \"\u03ac\",\n        \"\u03b3\",\n        \"\u03ad\",\n        \"\u03b4\",\n        \"\u03ae\",\n        \"\u03c9\",\n        \"\u03c7\",\n        \"\u03b8\",\n        \"\u03cd\",\n    ],\n    \"Tamil\": [\n        \"\u0b95\",\n        \"\u0ba4\",\n        \"\u0baa\",\n        \"\u0b9f\",\n        \"\u0bb0\",\n        \"\u0bae\",\n        \"\u0bb2\",\n        \"\u0ba9\",\n        \"\u0bb5\",\n        \"\u0bb1\",\n        \"\u0baf\",\n        \"\u0bb3\",\n        \"\u0b9a\",\n        \"\u0ba8\",\n        \"\u0b87\",\n        \"\u0ba3\",\n        \"\u0b85\",\n        \"\u0b86\",\n        \"\u0bb4\",\n        \"\u0b99\",\n        \"\u0b8e\",\n        \"\u0b89\",\n        \"\u0b92\",\n        \"\u0bb8\",\n    ],\n    \"Kazakh\": [\n        \"\u0430\",\n        \"\u044b\",\n        \"\u0435\",\n        \"\u043d\",\n        \"\u0442\",\n        \"\u0440\",\n        \"\u043b\",\n        \"\u0456\",\n        \"\u0434\",\n        \"\u0441\",\n        \"\u043c\",\n        \"\u049b\",\n        \"\u043a\",\n        \"\u043e\",\n        \"\u0431\",\n        \"\u0438\",\n        \"\u0443\",\n        \"\u0493\",\n        \"\u0436\",\n        \"\u04a3\",\n        \"\u0437\",\n        \"\u0448\",\n        \"\u0439\",\n        \"\u043f\",\n        \"\u0433\",\n        \"\u04e9\",\n    ],\n}\n\nLANGUAGE_SUPPORTED_COUNT: int = len(FREQUENCIES)\n", "charset_normalizer/__main__.py": "from .cli import cli_detect\n\nif __name__ == \"__main__\":\n    cli_detect()\n", "charset_normalizer/__init__.py": "# -*- coding: utf-8 -*-\n\"\"\"\nCharset-Normalizer\n~~~~~~~~~~~~~~\nThe Real First Universal Charset Detector.\nA library that helps you read text from an unknown charset encoding.\nMotivated by chardet, This package is trying to resolve the issue by taking a new approach.\nAll IANA character set names for which the Python core library provides codecs are supported.\n\nBasic usage:\n   >>> from charset_normalizer import from_bytes\n   >>> results = from_bytes('B\u0441\u0435\u043a\u0438 \u0447\u043e\u0432\u0435\u043a \u0438\u043c\u0430 \u043f\u0440\u0430\u0432\u043e \u043d\u0430 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435. O\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u0442\u043e!'.encode('utf_8'))\n   >>> best_guess = results.best()\n   >>> str(best_guess)\n   'B\u0441\u0435\u043a\u0438 \u0447\u043e\u0432\u0435\u043a \u0438\u043c\u0430 \u043f\u0440\u0430\u0432\u043e \u043d\u0430 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435. O\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u0442\u043e!'\n\nOthers methods and usages are available - see the full documentation\nat <https://github.com/Ousret/charset_normalizer>.\n:copyright: (c) 2021 by Ahmed TAHRI\n:license: MIT, see LICENSE for more details.\n\"\"\"\nimport logging\n\nfrom .api import from_bytes, from_fp, from_path, is_binary\nfrom .legacy import detect\nfrom .models import CharsetMatch, CharsetMatches\nfrom .utils import set_logging_handler\nfrom .version import VERSION, __version__\n\n__all__ = (\n    \"from_fp\",\n    \"from_path\",\n    \"from_bytes\",\n    \"is_binary\",\n    \"detect\",\n    \"CharsetMatch\",\n    \"CharsetMatches\",\n    \"__version__\",\n    \"VERSION\",\n    \"set_logging_handler\",\n)\n\n# Attach a NullHandler to the top level logger by default\n# https://docs.python.org/3.3/howto/logging.html#configuring-logging-for-a-library\n\nlogging.getLogger(\"charset_normalizer\").addHandler(logging.NullHandler())\n", "charset_normalizer/md.py": "from functools import lru_cache\nfrom logging import getLogger\nfrom typing import List, Optional\n\nfrom .constant import (\n    COMMON_SAFE_ASCII_CHARACTERS,\n    TRACE,\n    UNICODE_SECONDARY_RANGE_KEYWORD,\n)\nfrom .utils import (\n    is_accentuated,\n    is_arabic,\n    is_arabic_isolated_form,\n    is_case_variable,\n    is_cjk,\n    is_emoticon,\n    is_hangul,\n    is_hiragana,\n    is_katakana,\n    is_latin,\n    is_punctuation,\n    is_separator,\n    is_symbol,\n    is_thai,\n    is_unprintable,\n    remove_accent,\n    unicode_range,\n)\n\n\nclass MessDetectorPlugin:\n    \"\"\"\n    Base abstract class used for mess detection plugins.\n    All detectors MUST extend and implement given methods.\n    \"\"\"\n\n    def eligible(self, character: str) -> bool:\n        \"\"\"\n        Determine if given character should be fed in.\n        \"\"\"\n        raise NotImplementedError  # pragma: nocover\n\n    def feed(self, character: str) -> None:\n        \"\"\"\n        The main routine to be executed upon character.\n        Insert the logic in witch the text would be considered chaotic.\n        \"\"\"\n        raise NotImplementedError  # pragma: nocover\n\n    def reset(self) -> None:  # pragma: no cover\n        \"\"\"\n        Permit to reset the plugin to the initial state.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def ratio(self) -> float:\n        \"\"\"\n        Compute the chaos ratio based on what your feed() has seen.\n        Must NOT be lower than 0.; No restriction gt 0.\n        \"\"\"\n        raise NotImplementedError  # pragma: nocover\n\n\nclass TooManySymbolOrPunctuationPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._punctuation_count: int = 0\n        self._symbol_count: int = 0\n        self._character_count: int = 0\n\n        self._last_printable_char: Optional[str] = None\n        self._frenzy_symbol_in_word: bool = False\n\n    def eligible(self, character: str) -> bool:\n        return character.isprintable()\n\n    def feed(self, character: str) -> None:\n        self._character_count += 1\n\n        if (\n            character != self._last_printable_char\n            and character not in COMMON_SAFE_ASCII_CHARACTERS\n        ):\n            if is_punctuation(character):\n                self._punctuation_count += 1\n            elif (\n                character.isdigit() is False\n                and is_symbol(character)\n                and is_emoticon(character) is False\n            ):\n                self._symbol_count += 2\n\n        self._last_printable_char = character\n\n    def reset(self) -> None:  # pragma: no cover\n        self._punctuation_count = 0\n        self._character_count = 0\n        self._symbol_count = 0\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count == 0:\n            return 0.0\n\n        ratio_of_punctuation: float = (\n            self._punctuation_count + self._symbol_count\n        ) / self._character_count\n\n        return ratio_of_punctuation if ratio_of_punctuation >= 0.3 else 0.0\n\n\nclass TooManyAccentuatedPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._character_count: int = 0\n        self._accentuated_count: int = 0\n\n    def eligible(self, character: str) -> bool:\n        return character.isalpha()\n\n    def feed(self, character: str) -> None:\n        self._character_count += 1\n\n        if is_accentuated(character):\n            self._accentuated_count += 1\n\n    def reset(self) -> None:  # pragma: no cover\n        self._character_count = 0\n        self._accentuated_count = 0\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count < 8:\n            return 0.0\n\n        ratio_of_accentuation: float = self._accentuated_count / self._character_count\n        return ratio_of_accentuation if ratio_of_accentuation >= 0.35 else 0.0\n\n\nclass UnprintablePlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._unprintable_count: int = 0\n        self._character_count: int = 0\n\n    def eligible(self, character: str) -> bool:\n        return True\n\n    def feed(self, character: str) -> None:\n        if is_unprintable(character):\n            self._unprintable_count += 1\n        self._character_count += 1\n\n    def reset(self) -> None:  # pragma: no cover\n        self._unprintable_count = 0\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count == 0:\n            return 0.0\n\n        return (self._unprintable_count * 8) / self._character_count\n\n\nclass SuspiciousDuplicateAccentPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._successive_count: int = 0\n        self._character_count: int = 0\n\n        self._last_latin_character: Optional[str] = None\n\n    def eligible(self, character: str) -> bool:\n        return character.isalpha() and is_latin(character)\n\n    def feed(self, character: str) -> None:\n        self._character_count += 1\n        if (\n            self._last_latin_character is not None\n            and is_accentuated(character)\n            and is_accentuated(self._last_latin_character)\n        ):\n            if character.isupper() and self._last_latin_character.isupper():\n                self._successive_count += 1\n            # Worse if its the same char duplicated with different accent.\n            if remove_accent(character) == remove_accent(self._last_latin_character):\n                self._successive_count += 1\n        self._last_latin_character = character\n\n    def reset(self) -> None:  # pragma: no cover\n        self._successive_count = 0\n        self._character_count = 0\n        self._last_latin_character = None\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count == 0:\n            return 0.0\n\n        return (self._successive_count * 2) / self._character_count\n\n\nclass SuspiciousRange(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._suspicious_successive_range_count: int = 0\n        self._character_count: int = 0\n        self._last_printable_seen: Optional[str] = None\n\n    def eligible(self, character: str) -> bool:\n        return character.isprintable()\n\n    def feed(self, character: str) -> None:\n        self._character_count += 1\n\n        if (\n            character.isspace()\n            or is_punctuation(character)\n            or character in COMMON_SAFE_ASCII_CHARACTERS\n        ):\n            self._last_printable_seen = None\n            return\n\n        if self._last_printable_seen is None:\n            self._last_printable_seen = character\n            return\n\n        unicode_range_a: Optional[str] = unicode_range(self._last_printable_seen)\n        unicode_range_b: Optional[str] = unicode_range(character)\n\n        if is_suspiciously_successive_range(unicode_range_a, unicode_range_b):\n            self._suspicious_successive_range_count += 1\n\n        self._last_printable_seen = character\n\n    def reset(self) -> None:  # pragma: no cover\n        self._character_count = 0\n        self._suspicious_successive_range_count = 0\n        self._last_printable_seen = None\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count <= 24:\n            return 0.0\n\n        ratio_of_suspicious_range_usage: float = (\n            self._suspicious_successive_range_count * 2\n        ) / self._character_count\n\n        return ratio_of_suspicious_range_usage\n\n\nclass SuperWeirdWordPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._word_count: int = 0\n        self._bad_word_count: int = 0\n        self._foreign_long_count: int = 0\n\n        self._is_current_word_bad: bool = False\n        self._foreign_long_watch: bool = False\n\n        self._character_count: int = 0\n        self._bad_character_count: int = 0\n\n        self._buffer: str = \"\"\n        self._buffer_accent_count: int = 0\n\n    def eligible(self, character: str) -> bool:\n        return True\n\n    def feed(self, character: str) -> None:\n        if character.isalpha():\n            self._buffer += character\n            if is_accentuated(character):\n                self._buffer_accent_count += 1\n            if (\n                self._foreign_long_watch is False\n                and (is_latin(character) is False or is_accentuated(character))\n                and is_cjk(character) is False\n                and is_hangul(character) is False\n                and is_katakana(character) is False\n                and is_hiragana(character) is False\n                and is_thai(character) is False\n            ):\n                self._foreign_long_watch = True\n            return\n        if not self._buffer:\n            return\n        if (\n            character.isspace() or is_punctuation(character) or is_separator(character)\n        ) and self._buffer:\n            self._word_count += 1\n            buffer_length: int = len(self._buffer)\n\n            self._character_count += buffer_length\n\n            if buffer_length >= 4:\n                if self._buffer_accent_count / buffer_length > 0.34:\n                    self._is_current_word_bad = True\n                # Word/Buffer ending with an upper case accentuated letter are so rare,\n                # that we will consider them all as suspicious. Same weight as foreign_long suspicious.\n                if (\n                    is_accentuated(self._buffer[-1])\n                    and self._buffer[-1].isupper()\n                    and all(_.isupper() for _ in self._buffer) is False\n                ):\n                    self._foreign_long_count += 1\n                    self._is_current_word_bad = True\n            if buffer_length >= 24 and self._foreign_long_watch:\n                camel_case_dst = [\n                    i\n                    for c, i in zip(self._buffer, range(0, buffer_length))\n                    if c.isupper()\n                ]\n                probable_camel_cased: bool = False\n\n                if camel_case_dst and (len(camel_case_dst) / buffer_length <= 0.3):\n                    probable_camel_cased = True\n\n                if not probable_camel_cased:\n                    self._foreign_long_count += 1\n                    self._is_current_word_bad = True\n\n            if self._is_current_word_bad:\n                self._bad_word_count += 1\n                self._bad_character_count += len(self._buffer)\n                self._is_current_word_bad = False\n\n            self._foreign_long_watch = False\n            self._buffer = \"\"\n            self._buffer_accent_count = 0\n        elif (\n            character not in {\"<\", \">\", \"-\", \"=\", \"~\", \"|\", \"_\"}\n            and character.isdigit() is False\n            and is_symbol(character)\n        ):\n            self._is_current_word_bad = True\n            self._buffer += character\n\n    def reset(self) -> None:  # pragma: no cover\n        self._buffer = \"\"\n        self._is_current_word_bad = False\n        self._foreign_long_watch = False\n        self._bad_word_count = 0\n        self._word_count = 0\n        self._character_count = 0\n        self._bad_character_count = 0\n        self._foreign_long_count = 0\n\n    @property\n    def ratio(self) -> float:\n        if self._word_count <= 10 and self._foreign_long_count == 0:\n            return 0.0\n\n        return self._bad_character_count / self._character_count\n\n\nclass CjkInvalidStopPlugin(MessDetectorPlugin):\n    \"\"\"\n    GB(Chinese) based encoding often render the stop incorrectly when the content does not fit and\n    can be easily detected. Searching for the overuse of '\u4e05' and '\u4e04'.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._wrong_stop_count: int = 0\n        self._cjk_character_count: int = 0\n\n    def eligible(self, character: str) -> bool:\n        return True\n\n    def feed(self, character: str) -> None:\n        if character in {\"\u4e05\", \"\u4e04\"}:\n            self._wrong_stop_count += 1\n            return\n        if is_cjk(character):\n            self._cjk_character_count += 1\n\n    def reset(self) -> None:  # pragma: no cover\n        self._wrong_stop_count = 0\n        self._cjk_character_count = 0\n\n    @property\n    def ratio(self) -> float:\n        if self._cjk_character_count < 16:\n            return 0.0\n        return self._wrong_stop_count / self._cjk_character_count\n\n\nclass ArchaicUpperLowerPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._buf: bool = False\n\n        self._character_count_since_last_sep: int = 0\n\n        self._successive_upper_lower_count: int = 0\n        self._successive_upper_lower_count_final: int = 0\n\n        self._character_count: int = 0\n\n        self._last_alpha_seen: Optional[str] = None\n        self._current_ascii_only: bool = True\n\n    def eligible(self, character: str) -> bool:\n        return True\n\n    def feed(self, character: str) -> None:\n        is_concerned = character.isalpha() and is_case_variable(character)\n        chunk_sep = is_concerned is False\n\n        if chunk_sep and self._character_count_since_last_sep > 0:\n            if (\n                self._character_count_since_last_sep <= 64\n                and character.isdigit() is False\n                and self._current_ascii_only is False\n            ):\n                self._successive_upper_lower_count_final += (\n                    self._successive_upper_lower_count\n                )\n\n            self._successive_upper_lower_count = 0\n            self._character_count_since_last_sep = 0\n            self._last_alpha_seen = None\n            self._buf = False\n            self._character_count += 1\n            self._current_ascii_only = True\n\n            return\n\n        if self._current_ascii_only is True and character.isascii() is False:\n            self._current_ascii_only = False\n\n        if self._last_alpha_seen is not None:\n            if (character.isupper() and self._last_alpha_seen.islower()) or (\n                character.islower() and self._last_alpha_seen.isupper()\n            ):\n                if self._buf is True:\n                    self._successive_upper_lower_count += 2\n                    self._buf = False\n                else:\n                    self._buf = True\n            else:\n                self._buf = False\n\n        self._character_count += 1\n        self._character_count_since_last_sep += 1\n        self._last_alpha_seen = character\n\n    def reset(self) -> None:  # pragma: no cover\n        self._character_count = 0\n        self._character_count_since_last_sep = 0\n        self._successive_upper_lower_count = 0\n        self._successive_upper_lower_count_final = 0\n        self._last_alpha_seen = None\n        self._buf = False\n        self._current_ascii_only = True\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count == 0:\n            return 0.0\n\n        return self._successive_upper_lower_count_final / self._character_count\n\n\nclass ArabicIsolatedFormPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._character_count: int = 0\n        self._isolated_form_count: int = 0\n\n    def reset(self) -> None:  # pragma: no cover\n        self._character_count = 0\n        self._isolated_form_count = 0\n\n    def eligible(self, character: str) -> bool:\n        return is_arabic(character)\n\n    def feed(self, character: str) -> None:\n        self._character_count += 1\n\n        if is_arabic_isolated_form(character):\n            self._isolated_form_count += 1\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count < 8:\n            return 0.0\n\n        isolated_form_usage: float = self._isolated_form_count / self._character_count\n\n        return isolated_form_usage\n\n\n@lru_cache(maxsize=1024)\ndef is_suspiciously_successive_range(\n    unicode_range_a: Optional[str], unicode_range_b: Optional[str]\n) -> bool:\n    \"\"\"\n    Determine if two Unicode range seen next to each other can be considered as suspicious.\n    \"\"\"\n    if unicode_range_a is None or unicode_range_b is None:\n        return True\n\n    if unicode_range_a == unicode_range_b:\n        return False\n\n    if \"Latin\" in unicode_range_a and \"Latin\" in unicode_range_b:\n        return False\n\n    if \"Emoticons\" in unicode_range_a or \"Emoticons\" in unicode_range_b:\n        return False\n\n    # Latin characters can be accompanied with a combining diacritical mark\n    # eg. Vietnamese.\n    if (\"Latin\" in unicode_range_a or \"Latin\" in unicode_range_b) and (\n        \"Combining\" in unicode_range_a or \"Combining\" in unicode_range_b\n    ):\n        return False\n\n    keywords_range_a, keywords_range_b = unicode_range_a.split(\n        \" \"\n    ), unicode_range_b.split(\" \")\n\n    for el in keywords_range_a:\n        if el in UNICODE_SECONDARY_RANGE_KEYWORD:\n            continue\n        if el in keywords_range_b:\n            return False\n\n    # Japanese Exception\n    range_a_jp_chars, range_b_jp_chars = (\n        unicode_range_a\n        in (\n            \"Hiragana\",\n            \"Katakana\",\n        ),\n        unicode_range_b in (\"Hiragana\", \"Katakana\"),\n    )\n    if (range_a_jp_chars or range_b_jp_chars) and (\n        \"CJK\" in unicode_range_a or \"CJK\" in unicode_range_b\n    ):\n        return False\n    if range_a_jp_chars and range_b_jp_chars:\n        return False\n\n    if \"Hangul\" in unicode_range_a or \"Hangul\" in unicode_range_b:\n        if \"CJK\" in unicode_range_a or \"CJK\" in unicode_range_b:\n            return False\n        if unicode_range_a == \"Basic Latin\" or unicode_range_b == \"Basic Latin\":\n            return False\n\n    # Chinese/Japanese use dedicated range for punctuation and/or separators.\n    if (\"CJK\" in unicode_range_a or \"CJK\" in unicode_range_b) or (\n        unicode_range_a in [\"Katakana\", \"Hiragana\"]\n        and unicode_range_b in [\"Katakana\", \"Hiragana\"]\n    ):\n        if \"Punctuation\" in unicode_range_a or \"Punctuation\" in unicode_range_b:\n            return False\n        if \"Forms\" in unicode_range_a or \"Forms\" in unicode_range_b:\n            return False\n        if unicode_range_a == \"Basic Latin\" or unicode_range_b == \"Basic Latin\":\n            return False\n\n    return True\n\n\n@lru_cache(maxsize=2048)\ndef mess_ratio(\n    decoded_sequence: str, maximum_threshold: float = 0.2, debug: bool = False\n) -> float:\n    \"\"\"\n    Compute a mess ratio given a decoded bytes sequence. The maximum threshold does stop the computation earlier.\n    \"\"\"\n\n    detectors: List[MessDetectorPlugin] = [\n        md_class() for md_class in MessDetectorPlugin.__subclasses__()\n    ]\n\n    length: int = len(decoded_sequence) + 1\n\n    mean_mess_ratio: float = 0.0\n\n    if length < 512:\n        intermediary_mean_mess_ratio_calc: int = 32\n    elif length <= 1024:\n        intermediary_mean_mess_ratio_calc = 64\n    else:\n        intermediary_mean_mess_ratio_calc = 128\n\n    for character, index in zip(decoded_sequence + \"\\n\", range(length)):\n        for detector in detectors:\n            if detector.eligible(character):\n                detector.feed(character)\n\n        if (\n            index > 0 and index % intermediary_mean_mess_ratio_calc == 0\n        ) or index == length - 1:\n            mean_mess_ratio = sum(dt.ratio for dt in detectors)\n\n            if mean_mess_ratio >= maximum_threshold:\n                break\n\n    if debug:\n        logger = getLogger(\"charset_normalizer\")\n\n        logger.log(\n            TRACE,\n            \"Mess-detector extended-analysis start. \"\n            f\"intermediary_mean_mess_ratio_calc={intermediary_mean_mess_ratio_calc} mean_mess_ratio={mean_mess_ratio} \"\n            f\"maximum_threshold={maximum_threshold}\",\n        )\n\n        if len(decoded_sequence) > 16:\n            logger.log(TRACE, f\"Starting with: {decoded_sequence[:16]}\")\n            logger.log(TRACE, f\"Ending with: {decoded_sequence[-16::]}\")\n\n        for dt in detectors:  # pragma: nocover\n            logger.log(TRACE, f\"{dt.__class__}: {dt.ratio}\")\n\n    return round(mean_mess_ratio, 3)\n", "charset_normalizer/cli/__main__.py": "import argparse\nimport sys\nfrom json import dumps\nfrom os.path import abspath, basename, dirname, join, realpath\nfrom platform import python_version\nfrom typing import List, Optional\nfrom unicodedata import unidata_version\n\nimport charset_normalizer.md as md_module\nfrom charset_normalizer import from_fp\nfrom charset_normalizer.models import CliDetectionResult\nfrom charset_normalizer.version import __version__\n\n\ndef query_yes_no(question: str, default: str = \"yes\") -> bool:\n    \"\"\"Ask a yes/no question via input() and return their answer.\n\n    \"question\" is a string that is presented to the user.\n    \"default\" is the presumed answer if the user just hits <Enter>.\n        It must be \"yes\" (the default), \"no\" or None (meaning\n        an answer is required of the user).\n\n    The \"answer\" return value is True for \"yes\" or False for \"no\".\n\n    Credit goes to (c) https://stackoverflow.com/questions/3041986/apt-command-line-interface-like-yes-no-input\n    \"\"\"\n    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n    if default is None:\n        prompt = \" [y/n] \"\n    elif default == \"yes\":\n        prompt = \" [Y/n] \"\n    elif default == \"no\":\n        prompt = \" [y/N] \"\n    else:\n        raise ValueError(\"invalid default answer: '%s'\" % default)\n\n    while True:\n        sys.stdout.write(question + prompt)\n        choice = input().lower()\n        if default is not None and choice == \"\":\n            return valid[default]\n        elif choice in valid:\n            return valid[choice]\n        else:\n            sys.stdout.write(\"Please respond with 'yes' or 'no' \" \"(or 'y' or 'n').\\n\")\n\n\ndef cli_detect(argv: Optional[List[str]] = None) -> int:\n    \"\"\"\n    CLI assistant using ARGV and ArgumentParser\n    :param argv:\n    :return: 0 if everything is fine, anything else equal trouble\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"The Real First Universal Charset Detector. \"\n        \"Discover originating encoding used on text file. \"\n        \"Normalize text to unicode.\"\n    )\n\n    parser.add_argument(\n        \"files\", type=argparse.FileType(\"rb\"), nargs=\"+\", help=\"File(s) to be analysed\"\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        default=False,\n        dest=\"verbose\",\n        help=\"Display complementary information about file if any. \"\n        \"Stdout will contain logs about the detection process.\",\n    )\n    parser.add_argument(\n        \"-a\",\n        \"--with-alternative\",\n        action=\"store_true\",\n        default=False,\n        dest=\"alternatives\",\n        help=\"Output complementary possibilities if any. Top-level JSON WILL be a list.\",\n    )\n    parser.add_argument(\n        \"-n\",\n        \"--normalize\",\n        action=\"store_true\",\n        default=False,\n        dest=\"normalize\",\n        help=\"Permit to normalize input file. If not set, program does not write anything.\",\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--minimal\",\n        action=\"store_true\",\n        default=False,\n        dest=\"minimal\",\n        help=\"Only output the charset detected to STDOUT. Disabling JSON output.\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--replace\",\n        action=\"store_true\",\n        default=False,\n        dest=\"replace\",\n        help=\"Replace file when trying to normalize it instead of creating a new one.\",\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--force\",\n        action=\"store_true\",\n        default=False,\n        dest=\"force\",\n        help=\"Replace file without asking if you are sure, use this flag with caution.\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--threshold\",\n        action=\"store\",\n        default=0.2,\n        type=float,\n        dest=\"threshold\",\n        help=\"Define a custom maximum amount of chaos allowed in decoded content. 0. <= chaos <= 1.\",\n    )\n    parser.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=\"Charset-Normalizer {} - Python {} - Unicode {} - SpeedUp {}\".format(\n            __version__,\n            python_version(),\n            unidata_version,\n            \"OFF\" if md_module.__file__.lower().endswith(\".py\") else \"ON\",\n        ),\n        help=\"Show version information and exit.\",\n    )\n\n    args = parser.parse_args(argv)\n\n    if args.replace is True and args.normalize is False:\n        print(\"Use --replace in addition of --normalize only.\", file=sys.stderr)\n        return 1\n\n    if args.force is True and args.replace is False:\n        print(\"Use --force in addition of --replace only.\", file=sys.stderr)\n        return 1\n\n    if args.threshold < 0.0 or args.threshold > 1.0:\n        print(\"--threshold VALUE should be between 0. AND 1.\", file=sys.stderr)\n        return 1\n\n    x_ = []\n\n    for my_file in args.files:\n        matches = from_fp(my_file, threshold=args.threshold, explain=args.verbose)\n\n        best_guess = matches.best()\n\n        if best_guess is None:\n            print(\n                'Unable to identify originating encoding for \"{}\". {}'.format(\n                    my_file.name,\n                    \"Maybe try increasing maximum amount of chaos.\"\n                    if args.threshold < 1.0\n                    else \"\",\n                ),\n                file=sys.stderr,\n            )\n            x_.append(\n                CliDetectionResult(\n                    abspath(my_file.name),\n                    None,\n                    [],\n                    [],\n                    \"Unknown\",\n                    [],\n                    False,\n                    1.0,\n                    0.0,\n                    None,\n                    True,\n                )\n            )\n        else:\n            x_.append(\n                CliDetectionResult(\n                    abspath(my_file.name),\n                    best_guess.encoding,\n                    best_guess.encoding_aliases,\n                    [\n                        cp\n                        for cp in best_guess.could_be_from_charset\n                        if cp != best_guess.encoding\n                    ],\n                    best_guess.language,\n                    best_guess.alphabets,\n                    best_guess.bom,\n                    best_guess.percent_chaos,\n                    best_guess.percent_coherence,\n                    None,\n                    True,\n                )\n            )\n\n            if len(matches) > 1 and args.alternatives:\n                for el in matches:\n                    if el != best_guess:\n                        x_.append(\n                            CliDetectionResult(\n                                abspath(my_file.name),\n                                el.encoding,\n                                el.encoding_aliases,\n                                [\n                                    cp\n                                    for cp in el.could_be_from_charset\n                                    if cp != el.encoding\n                                ],\n                                el.language,\n                                el.alphabets,\n                                el.bom,\n                                el.percent_chaos,\n                                el.percent_coherence,\n                                None,\n                                False,\n                            )\n                        )\n\n            if args.normalize is True:\n                if best_guess.encoding.startswith(\"utf\") is True:\n                    print(\n                        '\"{}\" file does not need to be normalized, as it already came from unicode.'.format(\n                            my_file.name\n                        ),\n                        file=sys.stderr,\n                    )\n                    if my_file.closed is False:\n                        my_file.close()\n                    continue\n\n                dir_path = dirname(realpath(my_file.name))\n                file_name = basename(realpath(my_file.name))\n\n                o_: List[str] = file_name.split(\".\")\n\n                if args.replace is False:\n                    o_.insert(-1, best_guess.encoding)\n                    if my_file.closed is False:\n                        my_file.close()\n                elif (\n                    args.force is False\n                    and query_yes_no(\n                        'Are you sure to normalize \"{}\" by replacing it ?'.format(\n                            my_file.name\n                        ),\n                        \"no\",\n                    )\n                    is False\n                ):\n                    if my_file.closed is False:\n                        my_file.close()\n                    continue\n\n                try:\n                    x_[0].unicode_path = join(dir_path, \".\".join(o_))\n\n                    with open(x_[0].unicode_path, \"w\", encoding=\"utf-8\") as fp:\n                        fp.write(str(best_guess))\n                except IOError as e:\n                    print(str(e), file=sys.stderr)\n                    if my_file.closed is False:\n                        my_file.close()\n                    return 2\n\n        if my_file.closed is False:\n            my_file.close()\n\n    if args.minimal is False:\n        print(\n            dumps(\n                [el.__dict__ for el in x_] if len(x_) > 1 else x_[0].__dict__,\n                ensure_ascii=True,\n                indent=4,\n            )\n        )\n    else:\n        for my_file in args.files:\n            print(\n                \", \".join(\n                    [\n                        el.encoding or \"undefined\"\n                        for el in x_\n                        if el.path == abspath(my_file.name)\n                    ]\n                )\n            )\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    cli_detect()\n", "charset_normalizer/cli/__init__.py": "from .__main__ import cli_detect, query_yes_no\n\n__all__ = (\n    \"cli_detect\",\n    \"query_yes_no\",\n)\n"}