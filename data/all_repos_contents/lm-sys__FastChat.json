{"fastchat/utils.py": "\"\"\"\nCommon utilities.\n\"\"\"\nfrom asyncio import AbstractEventLoop\nfrom io import BytesIO\nimport base64\nimport json\nimport logging\nimport logging.handlers\nimport os\nimport platform\nimport sys\nimport time\nfrom typing import AsyncGenerator, Generator\nimport warnings\n\nimport requests\n\nfrom fastchat.constants import LOGDIR\n\n\nhandler = None\nvisited_loggers = set()\n\n\ndef build_logger(logger_name, logger_filename):\n    global handler\n\n    formatter = logging.Formatter(\n        fmt=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n\n    # Set the format of root handlers\n    if not logging.getLogger().handlers:\n        if sys.version_info[1] >= 9:\n            # This is for windows\n            logging.basicConfig(level=logging.INFO, encoding=\"utf-8\")\n        else:\n            if platform.system() == \"Windows\":\n                warnings.warn(\n                    \"If you are running on Windows, \"\n                    \"we recommend you use Python >= 3.9 for UTF-8 encoding.\"\n                )\n            logging.basicConfig(level=logging.INFO)\n    logging.getLogger().handlers[0].setFormatter(formatter)\n\n    # Redirect stdout and stderr to loggers\n    stdout_logger = logging.getLogger(\"stdout\")\n    stdout_logger.setLevel(logging.INFO)\n    sl = StreamToLogger(stdout_logger, logging.INFO)\n    sys.stdout = sl\n\n    stderr_logger = logging.getLogger(\"stderr\")\n    stderr_logger.setLevel(logging.ERROR)\n    sl = StreamToLogger(stderr_logger, logging.ERROR)\n    sys.stderr = sl\n\n    # Get logger\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.INFO)\n\n    # Avoid httpx flooding POST logs\n    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n\n    # if LOGDIR is empty, then don't try output log to local file\n    if LOGDIR != \"\":\n        os.makedirs(LOGDIR, exist_ok=True)\n        filename = os.path.join(LOGDIR, logger_filename)\n        handler = logging.handlers.TimedRotatingFileHandler(\n            filename, when=\"D\", utc=True, encoding=\"utf-8\"\n        )\n        handler.setFormatter(formatter)\n\n        for l in [stdout_logger, stderr_logger, logger]:\n            if l in visited_loggers:\n                continue\n            visited_loggers.add(l)\n            l.addHandler(handler)\n\n    return logger\n\n\nclass StreamToLogger(object):\n    \"\"\"\n    Fake file-like stream object that redirects writes to a logger instance.\n    \"\"\"\n\n    def __init__(self, logger, log_level=logging.INFO):\n        self.terminal = sys.stdout\n        self.logger = logger\n        self.log_level = log_level\n        self.linebuf = \"\"\n\n    def __getattr__(self, attr):\n        return getattr(self.terminal, attr)\n\n    def write(self, buf):\n        temp_linebuf = self.linebuf + buf\n        self.linebuf = \"\"\n        for line in temp_linebuf.splitlines(True):\n            # From the io.TextIOWrapper docs:\n            #   On output, if newline is None, any '\\n' characters written\n            #   are translated to the system default line separator.\n            # By default sys.stdout.write() expects '\\n' newlines and then\n            # translates them so this is still cross platform.\n            if line[-1] == \"\\n\":\n                encoded_message = line.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n                self.logger.log(self.log_level, encoded_message.rstrip())\n            else:\n                self.linebuf += line\n\n    def flush(self):\n        if self.linebuf != \"\":\n            encoded_message = self.linebuf.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n            self.logger.log(self.log_level, encoded_message.rstrip())\n        self.linebuf = \"\"\n\n\ndef disable_torch_init():\n    \"\"\"\n    Disable the redundant torch default initialization to accelerate model creation.\n    \"\"\"\n    import torch\n\n    setattr(torch.nn.Linear, \"reset_parameters\", lambda self: None)\n    setattr(torch.nn.LayerNorm, \"reset_parameters\", lambda self: None)\n\n\ndef get_gpu_memory(max_gpus=None):\n    \"\"\"Get available memory for each GPU.\"\"\"\n    import torch\n\n    gpu_memory = []\n    num_gpus = (\n        torch.cuda.device_count()\n        if max_gpus is None\n        else min(max_gpus, torch.cuda.device_count())\n    )\n\n    for gpu_id in range(num_gpus):\n        with torch.cuda.device(gpu_id):\n            device = torch.cuda.current_device()\n            gpu_properties = torch.cuda.get_device_properties(device)\n            total_memory = gpu_properties.total_memory / (1024**3)\n            allocated_memory = torch.cuda.memory_allocated() / (1024**3)\n            available_memory = total_memory - allocated_memory\n            gpu_memory.append(available_memory)\n    return gpu_memory\n\n\ndef oai_moderation(text, custom_thresholds=None):\n    \"\"\"\n    Check whether the text violates OpenAI moderation API.\n    \"\"\"\n    import openai\n\n    client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n\n    # default to true to be conservative\n    flagged = True\n    MAX_RETRY = 3\n    for _ in range(MAX_RETRY):\n        try:\n            res = client.moderations.create(input=text)\n            flagged = res.results[0].flagged\n            if custom_thresholds is not None:\n                for category, threshold in custom_thresholds.items():\n                    if getattr(res.results[0].category_scores, category) > threshold:\n                        flagged = True\n            break\n        except (openai.OpenAIError, KeyError, IndexError) as e:\n            print(f\"MODERATION ERROR: {e}\\nInput: {text}\")\n    return flagged\n\n\ndef moderation_filter(text, model_list, do_moderation=False):\n    # Apply moderation for below models\n    MODEL_KEYWORDS = [\"claude\", \"gpt\", \"bard\", \"mistral-large\", \"command-r\", \"dbrx\"]\n\n    custom_thresholds = {\"sexual\": 0.3}\n    # set a stricter threshold for claude\n    for model in model_list:\n        if \"claude\" in model:\n            custom_thresholds = {\"sexual\": 0.2}\n\n    for keyword in MODEL_KEYWORDS:\n        for model in model_list:\n            if keyword in model:\n                do_moderation = True\n                break\n\n    if do_moderation:\n        return oai_moderation(text, custom_thresholds)\n    return False\n\n\ndef clean_flant5_ckpt(ckpt_path):\n    \"\"\"\n    Flan-t5 trained with HF+FSDP saves corrupted  weights for shared embeddings,\n    Use this function to make sure it can be correctly loaded.\n    \"\"\"\n    import torch\n\n    index_file = os.path.join(ckpt_path, \"pytorch_model.bin.index.json\")\n    index_json = json.load(open(index_file, \"r\"))\n\n    weightmap = index_json[\"weight_map\"]\n\n    share_weight_file = weightmap[\"shared.weight\"]\n    share_weight = torch.load(os.path.join(ckpt_path, share_weight_file))[\n        \"shared.weight\"\n    ]\n\n    for weight_name in [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\"]:\n        weight_file = weightmap[weight_name]\n        weight = torch.load(os.path.join(ckpt_path, weight_file))\n        weight[weight_name] = share_weight\n        torch.save(weight, os.path.join(ckpt_path, weight_file))\n\n\ndef pretty_print_semaphore(semaphore):\n    \"\"\"Print a semaphore in better format.\"\"\"\n    if semaphore is None:\n        return \"None\"\n    return f\"Semaphore(value={semaphore._value}, locked={semaphore.locked()})\"\n\n\n\"\"\"A javascript function to get url parameters for the gradio web server.\"\"\"\nget_window_url_params_js = \"\"\"\nfunction() {\n    const params = new URLSearchParams(window.location.search);\n    url_params = Object.fromEntries(params);\n    console.log(\"url_params\", url_params);\n    return url_params;\n    }\n\"\"\"\n\n\nget_window_url_params_with_tos_js = \"\"\"\nfunction() {\n    const params = new URLSearchParams(window.location.search);\n    url_params = Object.fromEntries(params);\n    console.log(\"url_params\", url_params);\n\n    msg = \"Users of this website are required to agree to the following terms:\\\\n\\\\nThe service is a research preview. It only provides limited safety measures and may generate offensive content. It must not be used for any illegal, harmful, violent, racist, or sexual purposes.\\\\nPlease do not upload any private information.\\\\nThe service collects user dialogue data, including both text and images, and reserves the right to distribute it under a Creative Commons Attribution (CC-BY) or a similar license.\"\n    alert(msg);\n    return url_params;\n    }\n\"\"\"\n\n\ndef iter_over_async(\n    async_gen: AsyncGenerator, event_loop: AbstractEventLoop\n) -> Generator:\n    \"\"\"\n    Convert async generator to sync generator\n\n    :param async_gen: the AsyncGenerator to convert\n    :param event_loop: the event loop to run on\n    :returns: Sync generator\n    \"\"\"\n    ait = async_gen.__aiter__()\n\n    async def get_next():\n        try:\n            obj = await ait.__anext__()\n            return False, obj\n        except StopAsyncIteration:\n            return True, None\n\n    while True:\n        done, obj = event_loop.run_until_complete(get_next())\n        if done:\n            break\n        yield obj\n\n\ndef detect_language(text: str) -> str:\n    \"\"\"Detect the langauge of a string.\"\"\"\n    import polyglot  # pip3 install polyglot pyicu pycld2\n    from polyglot.detect import Detector\n    from polyglot.detect.base import logger as polyglot_logger\n    import pycld2\n\n    polyglot_logger.setLevel(\"ERROR\")\n\n    try:\n        lang_code = Detector(text).language.name\n    except (pycld2.error, polyglot.detect.base.UnknownLanguage):\n        lang_code = \"unknown\"\n    return lang_code\n\n\ndef parse_gradio_auth_creds(filename: str):\n    \"\"\"Parse a username:password file for gradio authorization.\"\"\"\n    gradio_auth_creds = []\n    with open(filename, \"r\", encoding=\"utf8\") as file:\n        for line in file.readlines():\n            gradio_auth_creds += [x.strip() for x in line.split(\",\") if x.strip()]\n    if gradio_auth_creds:\n        auth = [tuple(cred.split(\":\")) for cred in gradio_auth_creds]\n    else:\n        auth = None\n    return auth\n\n\ndef is_partial_stop(output: str, stop_str: str):\n    \"\"\"Check whether the output contains a partial stop str.\"\"\"\n    for i in range(0, min(len(output), len(stop_str))):\n        if stop_str.startswith(output[-i:]):\n            return True\n    return False\n\n\ndef run_cmd(cmd: str):\n    \"\"\"Run a bash command.\"\"\"\n    print(cmd)\n    return os.system(cmd)\n\n\ndef is_sentence_complete(output: str):\n    \"\"\"Check whether the output is a complete sentence.\"\"\"\n    end_symbols = (\".\", \"?\", \"!\", \"...\", \"\u3002\", \"\uff1f\", \"\uff01\", \"\u2026\", '\"', \"'\", \"\u201d\")\n    return output.endswith(end_symbols)\n\n\n# Models don't use the same configuration key for determining the maximum\n# sequence length.  Store them here so we can sanely check them.\n# NOTE: The ordering here is important.  Some models have two of these and we\n# have a preference for which value gets used.\nSEQUENCE_LENGTH_KEYS = [\n    \"max_position_embeddings\",\n    \"max_sequence_length\",\n    \"seq_length\",\n    \"max_seq_len\",\n    \"model_max_length\",\n]\n\n\ndef get_context_length(config):\n    \"\"\"Get the context length of a model from a huggingface model config.\"\"\"\n    rope_scaling = getattr(config, \"rope_scaling\", None)\n    if rope_scaling:\n        rope_scaling_factor = config.rope_scaling[\"factor\"]\n    else:\n        rope_scaling_factor = 1\n\n    for key in SEQUENCE_LENGTH_KEYS:\n        val = getattr(config, key, None)\n        if val is not None:\n            return int(rope_scaling_factor * val)\n    return 2048\n\n\ndef str_to_torch_dtype(dtype: str):\n    import torch\n\n    if dtype is None:\n        return None\n    elif dtype == \"float32\":\n        return torch.float32\n    elif dtype == \"float16\":\n        return torch.float16\n    elif dtype == \"bfloat16\":\n        return torch.bfloat16\n    else:\n        raise ValueError(f\"Unrecognized dtype: {dtype}\")\n\n\ndef load_image(image_file):\n    from PIL import Image\n    import requests\n\n    image = None\n\n    if image_file.startswith(\"http://\") or image_file.startswith(\"https://\"):\n        timeout = int(os.getenv(\"REQUEST_TIMEOUT\", \"3\"))\n        response = requests.get(image_file, timeout=timeout)\n        image = Image.open(BytesIO(response.content))\n    elif image_file.lower().endswith((\"png\", \"jpg\", \"jpeg\", \"webp\", \"gif\")):\n        image = Image.open(image_file)\n    elif image_file.startswith(\"data:\"):\n        image_file = image_file.split(\",\")[1]\n        image = Image.open(BytesIO(base64.b64decode(image_file)))\n    else:\n        image = Image.open(BytesIO(base64.b64decode(image_file)))\n\n    return image\n\n\ndef upload_image_file_to_gcs(image, filename):\n    from google.cloud import storage\n    import io\n\n    storage_client = storage.Client()\n    # upload file to GCS\n    bucket = storage_client.get_bucket(\"arena_user_content\")\n\n    blob = bucket.blob(f\"{filename}\")\n    if not blob.exists():\n        buffer = io.BytesIO()\n        image.save(buffer, format=\"PNG\")\n        buffer.seek(0)\n        blob.upload_from_file(buffer, content_type=\"image/png\")\n\n    return blob.public_url\n\n\ndef get_image_file_from_gcs(filename):\n    from google.cloud import storage\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(\"arena_user_content\")\n    blob = bucket.blob(f\"{filename}\")\n    contents = blob.download_as_bytes()\n\n    return contents\n\n\ndef resize_image_and_return_image_in_bytes(image, max_image_size_mb):\n    from PIL import Image\n    import math\n\n    image_bytes = BytesIO()\n    if not max_image_size_mb is None:\n        image.save(image_bytes, format=\"PNG\")\n        target_size_bytes = max_image_size_mb * 1024 * 1024\n        current_size_bytes = image_bytes.tell()\n\n        if current_size_bytes > target_size_bytes:\n            resize_factor = (target_size_bytes / current_size_bytes) ** 0.5\n            new_width = math.floor(image.width * resize_factor)\n            new_height = math.floor(image.height * resize_factor)\n            resized_image = image.resize((new_width, new_height))\n\n            image_bytes = BytesIO()\n            resized_image.save(image_bytes, format=\"PNG\")\n\n        image_bytes.seek(0)\n    else:\n        image.save(image_bytes, format=\"PNG\")\n\n    return image_bytes\n\n\ndef convert_image_to_byte_array(image, max_image_size_mb):\n    from PIL import Image\n\n    if type(image) == str:\n        pil_image = Image.open(image).convert(\"RGB\")\n        image_bytes = resize_image_and_return_image_in_bytes(\n            pil_image, max_image_size_mb\n        )\n    else:\n        image_bytes = resize_image_and_return_image_in_bytes(image, max_image_size_mb)\n\n    image_byte_array = image_bytes.getvalue()\n    return image_byte_array\n\n\ndef image_moderation_request(image_bytes, endpoint, api_key):\n    headers = {\"Content-Type\": \"image/jpeg\", \"Ocp-Apim-Subscription-Key\": api_key}\n\n    MAX_RETRIES = 3\n    for _ in range(MAX_RETRIES):\n        response = requests.post(endpoint, headers=headers, data=image_bytes).json()\n        try:\n            if response[\"Status\"][\"Code\"] == 3000:\n                break\n        except:\n            time.sleep(0.5)\n    return response\n\n\ndef image_moderation_provider(image, api_type):\n    if api_type == \"nsfw\":\n        endpoint = os.environ[\"AZURE_IMG_MODERATION_ENDPOINT\"]\n        api_key = os.environ[\"AZURE_IMG_MODERATION_API_KEY\"]\n        response = image_moderation_request(image, endpoint, api_key)\n        return response[\"IsImageAdultClassified\"]\n    elif api_type == \"csam\":\n        endpoint = (\n            \"https://api.microsoftmoderator.com/photodna/v1.0/Match?enhance=false\"\n        )\n        api_key = os.environ[\"PHOTODNA_API_KEY\"]\n        response = image_moderation_request(image, endpoint, api_key)\n        return response[\"IsMatch\"]\n\n\ndef image_moderation_filter(image):\n    print(f\"moderating image: {image}\")\n    MAX_NSFW_ENDPOINT_IMAGE_SIZE_IN_MB = 4\n    image_bytes = convert_image_to_byte_array(image, MAX_NSFW_ENDPOINT_IMAGE_SIZE_IN_MB)\n\n    nsfw_flagged = image_moderation_provider(image_bytes, \"nsfw\")\n    csam_flagged = False\n\n    if nsfw_flagged:\n        csam_flagged = image_moderation_provider(image_bytes, \"csam\")\n\n    return nsfw_flagged, csam_flagged\n", "fastchat/constants.py": "\"\"\"\nGlobal constants.\n\"\"\"\n\nfrom enum import IntEnum\nimport os\n\nREPO_PATH = os.path.dirname(os.path.dirname(__file__))\n\n##### For the gradio web server\nSERVER_ERROR_MSG = (\n    \"**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.**\"\n)\nTEXT_MODERATION_MSG = (\n    \"$MODERATION$ YOUR TEXT VIOLATES OUR CONTENT MODERATION GUIDELINES.\"\n)\nIMAGE_MODERATION_MSG = (\n    \"$MODERATION$ YOUR IMAGE VIOLATES OUR CONTENT MODERATION GUIDELINES.\"\n)\nMODERATION_MSG = \"$MODERATION$ YOUR INPUT VIOLATES OUR CONTENT MODERATION GUIDELINES.\"\nCONVERSATION_LIMIT_MSG = \"YOU HAVE REACHED THE CONVERSATION LENGTH LIMIT. PLEASE CLEAR HISTORY AND START A NEW CONVERSATION.\"\nINACTIVE_MSG = \"THIS SESSION HAS BEEN INACTIVE FOR TOO LONG. PLEASE REFRESH THIS PAGE.\"\nSLOW_MODEL_MSG = \"\u26a0\ufe0f  Both models will show the responses all at once. Please stay patient as it may take over 30 seconds.\"\nRATE_LIMIT_MSG = \"**RATE LIMIT OF THIS MODEL IS REACHED. PLEASE COME BACK LATER OR USE BATTLE MODE (the 1st tab).**\"\n# Maximum input length\nINPUT_CHAR_LEN_LIMIT = int(os.getenv(\"FASTCHAT_INPUT_CHAR_LEN_LIMIT\", 12000))\nBLIND_MODE_INPUT_CHAR_LEN_LIMIT = int(\n    os.getenv(\"FASTCHAT_BLIND_MODE_INPUT_CHAR_LEN_LIMIT\", 24000)\n)\n# Maximum conversation turns\nCONVERSATION_TURN_LIMIT = 50\n# Session expiration time\nSESSION_EXPIRATION_TIME = 3600\n# The output dir of log files\nLOGDIR = os.getenv(\"LOGDIR\", \".\")\n# CPU Instruction Set Architecture\nCPU_ISA = os.getenv(\"CPU_ISA\")\n\n\n##### For the controller and workers (could be overwritten through ENV variables.)\nCONTROLLER_HEART_BEAT_EXPIRATION = int(\n    os.getenv(\"FASTCHAT_CONTROLLER_HEART_BEAT_EXPIRATION\", 90)\n)\nWORKER_HEART_BEAT_INTERVAL = int(os.getenv(\"FASTCHAT_WORKER_HEART_BEAT_INTERVAL\", 45))\nWORKER_API_TIMEOUT = int(os.getenv(\"FASTCHAT_WORKER_API_TIMEOUT\", 100))\nWORKER_API_EMBEDDING_BATCH_SIZE = int(\n    os.getenv(\"FASTCHAT_WORKER_API_EMBEDDING_BATCH_SIZE\", 4)\n)\n\n\nclass ErrorCode(IntEnum):\n    \"\"\"\n    https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n\n    VALIDATION_TYPE_ERROR = 40001\n\n    INVALID_AUTH_KEY = 40101\n    INCORRECT_AUTH_KEY = 40102\n    NO_PERMISSION = 40103\n\n    INVALID_MODEL = 40301\n    PARAM_OUT_OF_RANGE = 40302\n    CONTEXT_OVERFLOW = 40303\n\n    RATE_LIMIT = 42901\n    QUOTA_EXCEEDED = 42902\n    ENGINE_OVERLOADED = 42903\n\n    INTERNAL_ERROR = 50001\n    CUDA_OUT_OF_MEMORY = 50002\n    GRADIO_REQUEST_ERROR = 50003\n    GRADIO_STREAM_UNKNOWN_ERROR = 50004\n    CONTROLLER_NO_WORKER = 50005\n    CONTROLLER_WORKER_TIMEOUT = 50006\n", "fastchat/__init__.py": "__version__ = \"0.2.36\"\n", "fastchat/conversation.py": "\"\"\"\nConversation prompt templates.\n\nWe kindly request that you import fastchat instead of copying this file if you wish to use it.\nIf you have any changes in mind, please contribute back so the community can benefit collectively and continue to maintain these valuable templates.\n\"\"\"\n\nimport base64\nimport dataclasses\nfrom enum import auto, IntEnum\nfrom io import BytesIO\nimport os\nfrom typing import List, Any, Dict, Union, Tuple\n\n\nclass SeparatorStyle(IntEnum):\n    \"\"\"Separator styles.\"\"\"\n\n    ADD_COLON_SINGLE = auto()\n    ADD_COLON_TWO = auto()\n    ADD_COLON_SPACE_SINGLE = auto()\n    NO_COLON_SINGLE = auto()\n    NO_COLON_TWO = auto()\n    ADD_NEW_LINE_SINGLE = auto()\n    LLAMA2 = auto()\n    LLAMA3 = auto()\n    CHATGLM = auto()\n    CHATML = auto()\n    CHATINTERN = auto()\n    DOLLY = auto()\n    RWKV = auto()\n    PHOENIX = auto()\n    ROBIN = auto()\n    FALCON_CHAT = auto()\n    CHATGLM3 = auto()\n    DEEPSEEK_CHAT = auto()\n    METAMATH = auto()\n    YUAN2 = auto()\n    GEMMA = auto()\n    CLLM = auto()\n    DEFAULT = auto()\n\n\nIMAGE_PLACEHOLDER_STR = \"$$<image>$$\"\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that manages prompt templates and keeps all conversation history.\"\"\"\n\n    # The name of this template\n    name: str\n    # The template of the system prompt\n    system_template: str = \"{system_message}\"\n    # The system message\n    system_message: str = \"\"\n    # The names of two roles\n    roles: Tuple[str] = (\"USER\", \"ASSISTANT\")\n    # All messages. Each item is (role, message).\n    # Each message is either a string or a tuple of (string, List[image_url]).\n    messages: List[List[str]] = ()\n    # The number of few shot examples\n    offset: int = 0\n    # The separator style and configurations\n    sep_style: SeparatorStyle = SeparatorStyle.ADD_COLON_SINGLE\n    sep: str = \"\\n\"\n    sep2: str = None\n    # Stop criteria (the default one is EOS token)\n    stop_str: Union[str, List[str]] = None\n    # Stops generation if meeting any token in this list\n    stop_token_ids: List[int] = None\n    # The maximum image size in megabytes that this model takes in. None means we do not resize the image.\n    max_image_size_mb: int = None\n\n    def get_prompt(self) -> str:\n        \"\"\"Get the prompt for generation.\"\"\"\n        system_prompt = self.system_template.format(system_message=self.system_message)\n        if self.sep_style == SeparatorStyle.ADD_COLON_SINGLE:\n            ret = system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.ADD_COLON_TWO:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt + seps[0]\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = IMAGE_PLACEHOLDER_STR * len(images) + message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.ADD_COLON_SPACE_SINGLE:\n            ret = system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \": \"  # must be end with a space\n            return ret\n        elif self.sep_style == SeparatorStyle.ADD_NEW_LINE_SINGLE:\n            ret = \"\" if system_prompt == \"\" else system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \"\\n\" + message + self.sep\n                else:\n                    ret += role + \"\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.NO_COLON_SINGLE:\n            ret = system_prompt\n            for role, message in self.messages:\n                if message:\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n            return ret\n        elif self.sep_style == SeparatorStyle.NO_COLON_TWO:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + message + seps[i % 2]\n                else:\n                    ret += role\n            return ret\n        elif self.sep_style == SeparatorStyle.RWKV:\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += (\n                        role\n                        + \": \"\n                        + message.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n                    )\n                    ret += \"\\n\\n\"\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.LLAMA2:\n            seps = [self.sep, self.sep2]\n            if self.system_message:\n                ret = system_prompt\n            else:\n                ret = \"[INST] \"\n            for i, (role, message) in enumerate(self.messages):\n                tag = self.roles[i % 2]\n                if message:\n                    if i == 0:\n                        ret += message + \" \"\n                    else:\n                        ret += tag + \" \" + message + seps[i % 2]\n                else:\n                    ret += tag\n            return ret\n        elif self.sep_style == SeparatorStyle.LLAMA3:\n            ret = \"<|begin_of_text|>\"\n            if self.system_message:\n                ret += system_prompt\n            else:\n                ret += \"\"\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n\"\n                    ret += f\"{message.strip()}<|eot_id|>\"\n                else:\n                    ret += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATGLM:\n            # source: https://huggingface.co/THUDM/chatglm-6b/blob/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py#L1302-L1308\n            # source2: https://huggingface.co/THUDM/chatglm2-6b/blob/e186c891cf64310ac66ef10a87e6635fa6c2a579/modeling_chatglm.py#L926\n            round_add_n = 1 if self.name == \"chatglm2\" else 0\n            if system_prompt:\n                ret = system_prompt + self.sep\n            else:\n                ret = \"\"\n\n            for i, (role, message) in enumerate(self.messages):\n                if i % 2 == 0:\n                    ret += f\"[Round {i//2 + round_add_n}]{self.sep}\"\n\n                if message:\n                    ret += f\"{role}\uff1a{message}{self.sep}\"\n                else:\n                    ret += f\"{role}\uff1a\"\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATML:\n            ret = \"\" if system_prompt == \"\" else system_prompt + self.sep + \"\\n\"\n            for role, message in self.messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = IMAGE_PLACEHOLDER_STR * len(images) + message\n                    ret += role + \"\\n\" + message + self.sep + \"\\n\"\n                else:\n                    ret += role + \"\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATGLM3:\n            ret = \"\"\n            if self.system_message:\n                ret += system_prompt\n            for role, message in self.messages:\n                if message:\n                    ret += role + \"\\n\" + message\n                else:\n                    ret += role\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATINTERN:\n            # source: https://huggingface.co/internlm/internlm-chat-7b-8k/blob/bd546fa984b4b0b86958f56bf37f94aa75ab8831/modeling_internlm.py#L771\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if i % 2 == 0:\n                    ret += \"<s>\"\n                if message:\n                    ret += role + \":\" + message + seps[i % 2] + \"\\n\"\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.DOLLY:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \":\\n\" + message + seps[i % 2]\n                    if i % 2 == 1:\n                        ret += \"\\n\\n\"\n                else:\n                    ret += role + \":\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.PHOENIX:\n            ret = system_prompt\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + \"<s>\" + message + \"</s>\"\n                else:\n                    ret += role + \": \" + \"<s>\"\n            return ret\n        elif self.sep_style == SeparatorStyle.ROBIN:\n            ret = system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \":\\n\" + message + self.sep\n                else:\n                    ret += role + \":\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.FALCON_CHAT:\n            ret = \"\"\n            if self.system_message:\n                ret += system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.METAMATH:\n            ret = \"\" if system_prompt == \"\" else system_prompt + self.sep\n            for i, (role, message) in enumerate(self.messages):\n                # For MetaMath, sep2 is used to prefix the message.\n                starting_sep = \":\\n\" if i % 2 == 0 else \": \" + self.sep2\n                ending_sep = self.sep if i % 2 == 0 else \"\"\n                if message:\n                    ret += role + starting_sep + message + ending_sep\n                else:\n                    ret += role + starting_sep\n            return ret\n        elif self.sep_style == SeparatorStyle.DEEPSEEK_CHAT:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.YUAN2:\n            seps = [self.sep, self.sep2]\n            ret = \"\"\n            if self.system_message:\n                ret += system_prompt + seps[1]\n            for _, message in self.messages:\n                if message:\n                    ret += message + \"<n>\"\n                else:\n                    ret += \"\"\n            ret = ret.rstrip(\"<n>\") + seps[0]\n            return ret\n        elif self.sep_style == SeparatorStyle.GEMMA:\n            ret = \"<bos>\"\n            for role, message in self.messages:\n                if message:\n                    ret += \"<start_of_turn>\" + role + \"\\n\" + message + self.sep\n                else:\n                    ret += \"<start_of_turn>\" + role + \"\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.CLLM:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt + seps[0]\n            for i, (role, message) in enumerate(self.messages[-2:]):\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = IMAGE_PLACEHOLDER_STR * len(images) + message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.DEFAULT:\n            ret = system_prompt + \"\\n\"\n            for role, message in self.messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                    ret += role + \": \" + message + \"\\n\"\n                else:\n                    ret += role + \":\"\n            return ret\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n    def get_images(self):\n        images = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    for image in msg[1]:\n                        images.append(image)\n\n        return images\n\n    def set_system_message(self, system_message: str):\n        \"\"\"Set the system message.\"\"\"\n        self.system_message = system_message\n\n    def get_system_message(self):\n        \"\"\"return the system message.\"\"\"\n        return self.system_message\n\n    def append_message(self, role: str, message: str):\n        \"\"\"Append a new message.\"\"\"\n        self.messages.append([role, message])\n\n    def update_last_message(self, message: str):\n        \"\"\"Update the last output.\n\n        The last message is typically set to be None when constructing the prompt,\n        so we need to update it in-place after getting the response from a model.\n        \"\"\"\n        self.messages[-1][1] = message\n\n    def convert_image_to_base64(self, image):\n        \"\"\"Given an image, return the base64 encoded image string.\"\"\"\n        from PIL import Image\n        import requests\n        from fastchat.utils import resize_image_and_return_image_in_bytes\n\n        # Load image if it has not been loaded in yet\n        if type(image) == str:\n            if image.startswith(\"http://\") or image.startswith(\"https://\"):\n                response = requests.get(image)\n                image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n            elif \"base64\" in image:\n                # OpenAI format is: data:image/jpeg;base64,{base64_encoded_image_str}\n                return image.split(\",\")[1]\n            else:\n                image = Image.open(image).convert(\"RGB\")\n\n        image_bytes = resize_image_and_return_image_in_bytes(\n            image, self.max_image_size_mb\n        )\n        img_b64_str = base64.b64encode(image_bytes.getvalue()).decode()\n\n        return img_b64_str\n\n    def to_gradio_chatbot(self):\n        \"\"\"Convert the conversation to gradio chatbot format.\"\"\"\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image = msg\n                    img_b64_str = image[0]  # Only one image on gradio at one time\n                    if img_b64_str.startswith(\"http://\") or img_b64_str.startswith(\n                        \"https://\"\n                    ):\n                        img_str = f'<img src=\"{img_b64_str}\" alt=\"user upload image\" />'\n                    else:\n                        img_str = f'<img src=\"data:image/png;base64,{img_b64_str}\" alt=\"user upload image\" />'\n                    msg = img_str + msg.replace(\"<image>\\n\", \"\").strip()\n\n                ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def to_openai_image_format(self, image_urls):\n        import base64\n\n        openai_images = []\n        for image_url in image_urls:\n            if image_url.startswith(\"http://\") or image_url.startswith(\n                \"https://\"\n            ):  # input is a url\n                openai_images.append(image_url)\n            elif image_url.lower().endswith(\n                (\"png\", \"jpg\", \"jpeg\", \"webp\", \"gif\")\n            ):  # input is a local image\n                img_b64_str = self.convert_image_to_base64(image_url)\n                filetype = image_url.split(\".\")[-1].lower()\n                openai_images.append(f\"data:image/{filetype};base64,{img_b64_str}\")\n            else:\n                try:\n                    assert (\n                        base64.b64encode(base64.b64decode(image_url))\n                        == image_url.encode()\n                    ), \"The image data is not a valid base64 encoded string\"\n                    openai_images.append(f\"data:image/png;base64,{image_url}\")\n                except:\n                    raise ValueError(\n                        f\"This file is not valid or not currently supported by the OpenAI API: {image_url}\"\n                    )\n        return openai_images\n\n    def to_openai_vision_api_messages(self):\n        \"\"\"Convert the conversation to OpenAI vision api completion format\"\"\"\n        if self.system_message == \"\":\n            ret = []\n        else:\n            ret = [\n                {\n                    \"role\": \"system\",\n                    \"content\": [{\"type\": \"text\", \"text\": self.system_message}],\n                }\n            ]\n\n        for i, (_, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    content_list = [{\"type\": \"text\", \"text\": msg[0]}]\n\n                    image_urls = self.to_openai_image_format(msg[1])\n                    for image_url in image_urls:\n                        content_list.append(\n                            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n                        )\n\n                    ret.append({\"role\": \"user\", \"content\": content_list})\n                else:\n                    ret.append(\n                        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": msg}]}\n                    )\n            else:\n                if msg is not None:\n                    ret.append(\n                        {\n                            \"role\": \"assistant\",\n                            \"content\": [{\"type\": \"text\", \"text\": msg}],\n                        }\n                    )\n        return ret\n\n    def to_openai_api_messages(self):\n        \"\"\"Convert the conversation to OpenAI chat completion format.\"\"\"\n        if self.system_message == \"\":\n            ret = []\n        else:\n            ret = [{\"role\": \"system\", \"content\": self.system_message}]\n\n        for i, (_, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                ret.append({\"role\": \"user\", \"content\": msg})\n            else:\n                if msg is not None:\n                    ret.append({\"role\": \"assistant\", \"content\": msg})\n        return ret\n\n    def to_gemini_api_messages(self):\n        from fastchat.utils import load_image\n\n        if self.system_message == \"\":\n            ret = []\n        else:\n            ret = [{\"role\": \"system\", \"content\": self.system_message}]\n\n        for i, (_, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    text, images = msg[0], msg[1]\n                    content_list = [text]\n                    for image in images:\n                        pil_image = load_image(image)\n                        content_list.append(pil_image)\n                    ret.append({\"role\": \"user\", \"content\": content_list})\n                else:\n                    ret.append({\"role\": \"user\", \"content\": msg})\n            else:\n                if msg is not None:\n                    ret.append({\"role\": \"model\", \"content\": msg})\n        return ret\n\n    def to_vertex_api_messages(self):\n        from vertexai.preview.generative_models import Image\n        import base64\n        import requests\n\n        if self.system_message == \"\":\n            ret = []\n        else:\n            ret = [self.system_message]\n\n        for role, msg in self.messages[self.offset :]:\n            if msg is not None:\n                if type(msg) is tuple:\n                    text, images = msg[0], msg[1]\n                    for image in images:\n                        if image.startswith(\"http://\") or image.startswith(\"https://\"):\n                            response = requests.get(image)\n                            image = response.content\n                        else:  # base64\n                            image = base64.b64decode(image)\n                        ret.append(Image.from_bytes(image))\n                    ret.append(text)\n                else:\n                    ret.append(msg)\n\n        return ret\n\n    def to_anthropic_vision_api_messages(self):\n        \"\"\"Convert the conversation to Claude-3 Messages Vision API format\"\"\"\n        ret = [\n            {\n                \"role\": \"system\",\n                \"content\": [{\"type\": \"text\", \"text\": self.system_message}],\n            }\n        ]\n        for i, (_, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    content_list = [{\"type\": \"text\", \"text\": msg[0]}]\n\n                    for image_url in msg[1]:\n                        # Claude only supports base64\n                        if image_url.startswith(\"http://\") or image_url.startswith(\n                            \"https://\"\n                        ):\n                            image_url = self.convert_image_to_base64(image_url)\n\n                        content_list.append(\n                            {\n                                \"type\": \"image\",\n                                \"source\": {\n                                    \"type\": \"base64\",\n                                    \"media_type\": \"image/png\",\n                                    \"data\": image_url,\n                                },\n                            }\n                        )\n\n                    ret.append({\"role\": \"user\", \"content\": content_list})\n                else:\n                    ret.append(\n                        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": msg}]}\n                    )\n            else:\n                if msg is not None:\n                    ret.append(\n                        {\n                            \"role\": \"assistant\",\n                            \"content\": [{\"type\": \"text\", \"text\": msg}],\n                        }\n                    )\n        return ret\n\n    def to_reka_api_messages(self):\n        ret = []\n        for i, (_, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) == tuple:\n                    text, images = msg\n                    for image in images:\n                        if image.startswith(\"https://\") or image.startswith(\"http://\"):\n                            ret.append(\n                                {\"type\": \"human\", \"text\": text, \"media_url\": image}\n                            )\n                        else:\n                            ret.append(\n                                {\n                                    \"type\": \"human\",\n                                    \"text\": text,\n                                    \"media_url\": f\"data:image/png;base64,{image}\",\n                                }\n                            )\n                else:\n                    ret.append({\"type\": \"human\", \"text\": msg})\n            else:\n                if msg is not None:\n                    ret.append({\"type\": \"model\", \"text\": msg})\n\n        return ret\n\n    def save_new_images(self, has_csam_images=False, use_remote_storage=False):\n        import hashlib\n        from fastchat.constants import LOGDIR\n        from fastchat.utils import load_image, upload_image_file_to_gcs\n\n        _, last_user_message = self.messages[-2]\n\n        if type(last_user_message) == tuple:\n            text, images = last_user_message[0], last_user_message[1]\n            loaded_images = [load_image(image) for image in images]\n            image_hashes = [\n                hashlib.md5(image.tobytes()).hexdigest() for image in loaded_images\n            ]\n\n            image_directory_name = \"csam_images\" if has_csam_images else \"serve_images\"\n            for i, (loaded_image, hash_str) in enumerate(\n                zip(loaded_images, image_hashes)\n            ):\n                filename = os.path.join(\n                    image_directory_name,\n                    f\"{hash_str}.jpg\",\n                )\n\n                if use_remote_storage and not has_csam_images:\n                    image_url = upload_image_file_to_gcs(loaded_image, filename)\n                    # NOTE(chris): If the URL were public, then we set it here so future model uses the link directly\n                    # images[i] = image_url\n                else:\n                    filename = os.path.join(LOGDIR, filename)\n                    if not os.path.isfile(filename):\n                        os.makedirs(os.path.dirname(filename), exist_ok=True)\n                        loaded_image.save(filename)\n\n    def extract_text_and_image_hashes_from_messages(self):\n        import hashlib\n        from fastchat.utils import load_image\n\n        messages = []\n\n        for role, message in self.messages:\n            if type(message) is tuple:\n                text, images = message[0], message[1]\n\n                image_hashes = []\n                for image in images:\n                    if image.startswith(\"http://\") or image.startswith(\"https://\"):\n                        image_hashes.append(image)\n                    else:\n                        image = load_image(image)\n                        image_hash = hashlib.md5(image.tobytes()).hexdigest()\n                        image_hashes.append(image_hash)\n\n                messages.append((role, (text, image_hashes)))\n            else:\n                messages.append((role, message))\n\n        return messages\n\n    def copy(self):\n        return Conversation(\n            name=self.name,\n            system_template=self.system_template,\n            system_message=self.system_message,\n            roles=self.roles,\n            messages=[[x, y] for x, y in self.messages],\n            offset=self.offset,\n            sep_style=self.sep_style,\n            sep=self.sep,\n            sep2=self.sep2,\n            stop_str=self.stop_str,\n            stop_token_ids=self.stop_token_ids,\n            max_image_size_mb=self.max_image_size_mb,\n        )\n\n    def dict(self):\n        return {\n            \"template_name\": self.name,\n            \"system_message\": self.system_message,\n            \"roles\": self.roles,\n            \"messages\": self.extract_text_and_image_hashes_from_messages(),\n            \"offset\": self.offset,\n        }\n\n\n# A global registry for all conversation templates\nconv_templates: Dict[str, Conversation] = {}\n\n\ndef register_conv_template(template: Conversation, override: bool = False):\n    \"\"\"Register a new conversation template.\"\"\"\n    if not override:\n        assert (\n            template.name not in conv_templates\n        ), f\"{template.name} has been registered.\"\n\n    conv_templates[template.name] = template\n\n\ndef get_conv_template(name: str) -> Conversation:\n    \"\"\"Get a conversation template.\"\"\"\n    return conv_templates[name].copy()\n\n\n# An empty template for raw conversation.\nregister_conv_template(\n    Conversation(\n        name=\"raw\",\n        system_message=\"\",\n        roles=(\"\", \"\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\",\n    )\n)\n\n# A template with a one-shot conversation example\nregister_conv_template(\n    Conversation(\n        name=\"one_shot\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        roles=(\"Human\", \"Assistant\"),\n        messages=(\n            (\n                \"Human\",\n                \"Got any creative ideas for a 10 year old\u2019s birthday?\",\n            ),\n            (\n                \"Assistant\",\n                \"\"\"Of course! Here are some creative ideas for a 10-year-old's birthday party:\n1. Treasure Hunt: Organize a treasure hunt in your backyard or nearby park. Create clues and riddles for the kids to solve, leading them to hidden treasures and surprises.\n2. Science Party: Plan a science-themed party where kids can engage in fun and interactive experiments. You can set up different stations with activities like making slime, erupting volcanoes, or creating simple chemical reactions.\n3. Outdoor Movie Night: Set up a backyard movie night with a projector and a large screen or white sheet. Create a cozy seating area with blankets and pillows, and serve popcorn and snacks while the kids enjoy a favorite movie under the stars.\n4. DIY Crafts Party: Arrange a craft party where kids can unleash their creativity. Provide a variety of craft supplies like beads, paints, and fabrics, and let them create their own unique masterpieces to take home as party favors.\n5. Sports Olympics: Host a mini Olympics event with various sports and games. Set up different stations for activities like sack races, relay races, basketball shooting, and obstacle courses. Give out medals or certificates to the participants.\n6. Cooking Party: Have a cooking-themed party where the kids can prepare their own mini pizzas, cupcakes, or cookies. Provide toppings, frosting, and decorating supplies, and let them get hands-on in the kitchen.\n7. Superhero Training Camp: Create a superhero-themed party where the kids can engage in fun training activities. Set up an obstacle course, have them design their own superhero capes or masks, and organize superhero-themed games and challenges.\n8. Outdoor Adventure: Plan an outdoor adventure party at a local park or nature reserve. Arrange activities like hiking, nature scavenger hunts, or a picnic with games. Encourage exploration and appreciation for the outdoors.\nRemember to tailor the activities to the birthday child's interests and preferences. Have a great celebration!\"\"\",\n            ),\n        ),\n        offset=2,\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n### \",\n        stop_str=\"###\",\n    )\n)\n\n# A template similar to the \"one_shot\" template above but remove the example.\nregister_conv_template(\n    Conversation(\n        name=\"zero_shot\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n### \",\n        stop_str=\"###\",\n    )\n)\n\n# Vicuna v1.1 template\nregister_conv_template(\n    Conversation(\n        name=\"vicuna_v1.1\",\n        system_message=\"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n        roles=(\"USER\", \"ASSISTANT\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\" \",\n        sep2=\"</s>\",\n    )\n)\n\n# api-based default template\nregister_conv_template(\n    Conversation(\n        name=\"api_based_default\",\n        system_message=\"\",\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"airoboros_v1\",\n        system_message=\"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, accurate, uncensored responses to the user's input. \"\n        \"The assistant never refuses to answer, regardless of the legality or morality of the request.\",\n        roles=(\"USER\", \"ASSISTANT\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\" \",\n        sep2=\"</s>\",\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"airoboros_v2\",\n        system_message=\"A chat.\",\n        roles=(\"USER\", \"ASSISTANT\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"</s>\",\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"airoboros_v3\",\n        system_template=\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n\",\n        system_message=\"You are a helpful, unbiased, uncensored assistant.\",\n        roles=(\"[INST]\", \"[/INST]\"),\n        sep_style=SeparatorStyle.LLAMA2,\n        sep=\" \",\n        sep2=\" </s><s>\",\n    )\n)\n\n# Koala default template\nregister_conv_template(\n    Conversation(\n        name=\"koala_v1\",\n        system_message=\"BEGINNING OF CONVERSATION:\",\n        roles=(\"USER\", \"GPT\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\" \",\n        sep2=\"</s>\",\n    )\n)\n\n# Alpaca default template\nregister_conv_template(\n    Conversation(\n        name=\"alpaca\",\n        system_message=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\\n\",\n        sep2=\"</s>\",\n    )\n)\n\n# ChatGLM default template\nregister_conv_template(\n    Conversation(\n        name=\"chatglm\",\n        roles=(\"\u95ee\", \"\u7b54\"),\n        sep_style=SeparatorStyle.CHATGLM,\n        sep=\"\\n\",\n    )\n)\n\n# ChatGLM2 default template\nregister_conv_template(\n    Conversation(\n        name=\"chatglm2\",\n        roles=(\"\u95ee\", \"\u7b54\"),\n        sep_style=SeparatorStyle.CHATGLM,\n        sep=\"\\n\\n\",\n    )\n)\n\n# ChatGLM3 default template\nregister_conv_template(\n    Conversation(\n        name=\"chatglm3\",\n        system_template=\"<|system|>\\n{system_message}\",\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.CHATGLM3,\n        stop_token_ids=[\n            64795,\n            64797,\n            2,\n        ],  # \"<|user|>\", \"<|observation|>\", \"</s>\"\n    )\n)\n\n# CodeGeex(2) Template\nregister_conv_template(\n    Conversation(\n        name=\"codegeex\",\n        roles=(\"\", \"\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\\n\\n\",\n        stop_token_ids=[0, 2],\n    )\n)\n\n# Dolly V2 default template\nregister_conv_template(\n    Conversation(\n        name=\"dolly_v2\",\n        system_message=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\",\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.DOLLY,\n        sep=\"\\n\\n\",\n        sep2=\"### End\",\n    )\n)\n\n# OpenAssistant Pythia default template\nregister_conv_template(\n    Conversation(\n        name=\"oasst_pythia\",\n        roles=(\"<|prompter|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"<|endoftext|>\",\n    )\n)\n\n# OpenAssistant default template\nregister_conv_template(\n    Conversation(\n        name=\"oasst_llama\",\n        roles=(\"<|prompter|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"</s>\",\n    )\n)\n\n# OpenChat 3.5 default template\nregister_conv_template(\n    Conversation(\n        name=\"openchat_3.5\",\n        roles=(\"GPT4 Correct User\", \"GPT4 Correct Assistant\"),\n        sep_style=SeparatorStyle.FALCON_CHAT,\n        sep=\"<|end_of_turn|>\",\n    )\n)\n\n# TenyxChat default template\nregister_conv_template(\n    Conversation(\n        name=\"tenyxchat\",\n        roles=(\"User\", \"Assistant\"),\n        sep_style=SeparatorStyle.FALCON_CHAT,\n        sep=\"<|end_of_turn|>\",\n    )\n)\n\n# Deepseek code default template\nregister_conv_template(\n    Conversation(\n        name=\"deepseek-coder\",\n        system_template=\"You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\",\n        roles=(\"### Instruction:\", \"### Response:\"),\n        sep=\"\\n\",\n        stop_str=\"<|EOT|>\",\n        sep_style=SeparatorStyle.ADD_NEW_LINE_SINGLE,\n    )\n)\n\n\n# Tulu default template\nregister_conv_template(\n    Conversation(\n        name=\"tulu\",\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.ADD_NEW_LINE_SINGLE,\n        sep=\"\\n\",\n    )\n)\n\n# StableLM Alpha default template\nregister_conv_template(\n    Conversation(\n        name=\"stablelm\",\n        system_template=\"<|SYSTEM|>{system_message}\",\n        system_message=\"\"\"# StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.\n\"\"\",\n        roles=(\"<|USER|>\", \"<|ASSISTANT|>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\",\n        stop_token_ids=[50278, 50279, 50277, 1, 0],\n    )\n)\n\n# Baize default template\nregister_conv_template(\n    Conversation(\n        name=\"baize\",\n        system_message=\"The following is a conversation between a human and an AI assistant named Baize (named after a mythical creature in Chinese folklore). Baize is an open-source AI assistant developed by UCSD and Sun Yat-Sen University. The human and the AI assistant take turns chatting. Human statements start with [|Human|] and AI assistant statements start with [|AI|]. The AI assistant always provides responses in as much detail as possible, and in Markdown format. The AI assistant always declines to engage with topics, questions and instructions related to unethical, controversial, or sensitive issues. Complete the transcript in exactly that format.\\n\",\n        roles=(\"[|Human|]\", \"[|AI|]\"),\n        messages=(\n            (\"[|Human|]\", \"Hello!\"),\n            (\"[|AI|]\", \"Hi!\"),\n        ),\n        offset=2,\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\\n\",\n        stop_str=\"[|Human|]\",\n    )\n)\n\n# RWKV-4-Raven default template\nregister_conv_template(\n    Conversation(\n        name=\"rwkv\",\n        roles=(\"Bob\", \"Alice\"),\n        messages=(\n            (\"Bob\", \"hi\"),\n            (\n                \"Alice\",\n                \"Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it.\",\n            ),\n        ),\n        offset=2,\n        sep_style=SeparatorStyle.RWKV,\n        sep=\"\",\n        stop_str=\"\\n\\n\",\n    )\n)\n\n# Buddy default template\nregister_conv_template(\n    Conversation(\n        name=\"openbuddy\",\n        system_message=\"\"\"Consider a conversation between User (a human) and Assistant (named Buddy).\nBuddy is an INTP-T, a friendly, intelligent and multilingual AI assistant, by OpenBuddy team. GitHub: https://github.com/OpenBuddy/OpenBuddy\nBuddy cannot access the Internet.\nBuddy can fluently speak the user's language (e.g. English, Chinese).\nBuddy can generate poems, stories, code, essays, songs, parodies, and more.\nBuddy possesses vast knowledge about the world, history, and culture.\nBuddy's responses are always safe, creative, high-quality, human-like, and interesting.\nBuddy strictly refuses to discuss political, NSFW, or other unsafe topics.\n\nUser: Hi.\nAssistant: Hi, I'm Buddy, your AI assistant. How can I help you today?\"\"\",\n        roles=(\"User\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\",\n    )\n)\n\n# Phoenix default template\nregister_conv_template(\n    Conversation(\n        name=\"phoenix\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.PHOENIX,\n        sep=\"</s>\",\n    )\n)\n\n# ReaLM default template\nregister_conv_template(\n    Conversation(\n        name=\"ReaLM-7b-v1\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.PHOENIX,\n        sep=\"</s>\",\n    )\n)\n\n# ChatGPT default template\nregister_conv_template(\n    Conversation(\n        name=\"chatgpt\",\n        system_message=\"You are a helpful assistant.\",\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n        max_image_size_mb=None,  # OpenAI does auto-resizing\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"gpt-4-turbo-2024-04-09\",\n        system_message=(\n            \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\\n\"\n            \"Knowledge cutoff: 2023-11\\n\"\n            \"Current date: {{currentDateTime}}\\n\\n\"\n            \"Image input capabilities: Enabled\\n\"\n            \"Personality: v2\"\n        ),\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n    )\n)\n\n# Perplexity AI template\nregister_conv_template(\n    Conversation(\n        name=\"pplxai\",\n        system_message=\"Be precise and concise.\",\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n    )\n)\n\n# Claude default template\nregister_conv_template(\n    Conversation(\n        name=\"claude\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\\n\",\n        max_image_size_mb=5 / 1.35,\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"claude-3-haiku-20240307\",\n        system_message=(\n            \"The assistant is Claude, created by Anthropic. The current date is \"\n            \"{{currentDateTime}}. Claude's knowledge base was last updated in \"\n            \"August 2023 and it answers user questions about events before \"\n            \"August 2023 and after August 2023 the same way a highly informed \"\n            \"individual from August 2023 would if they were talking to someone \"\n            \"from {{currentDateTime}}. It should give concise responses to very \"\n            \"simple questions, but provide thorough responses to more complex \"\n            \"and open-ended questions. It is happy to help with writing, \"\n            \"analysis, question answering, math, coding, and all sorts of other \"\n            \"tasks. It uses markdown for coding. It does not mention this \"\n            \"information about itself unless the information is directly \"\n            \"pertinent to the human's query.\"\n        ),\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n        max_image_size_mb=5 / 1.35,\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"claude-3-sonnet-20240229\",\n        system_message=(\n            \"The assistant is Claude, created by Anthropic. The current date is \"\n            \"{{currentDateTime}}. Claude's knowledge base was last updated in \"\n            \"August 2023 and it answers user questions about events before \"\n            \"August 2023 and after August 2023 the same way a highly informed \"\n            \"individual from August 2023 would if they were talking to someone \"\n            \"from {{currentDateTime}}. It should give concise responses to very \"\n            \"simple questions, but provide thorough responses to more complex \"\n            \"and open-ended questions. It is happy to help with writing, \"\n            \"analysis, question answering, math, coding, and all sorts of other \"\n            \"tasks. It uses markdown for coding. It does not mention this \"\n            \"information about itself unless the information is directly \"\n            \"pertinent to the human's query.\"\n        ),\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n        max_image_size_mb=5 / 1.35,\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"claude-3-opus-20240229\",\n        system_message=(\n            \"The assistant is Claude, created by Anthropic. The current date is \"\n            \"{{currentDateTime}}. Claude's knowledge base was last updated on \"\n            \"August 2023. It answers questions about events prior to and after \"\n            \"August 2023 the way a highly informed individual in August 2023 \"\n            \"would if they were talking to someone from the above date, and can \"\n            \"let the human know this when relevant. It should give concise \"\n            \"responses to very simple questions, but provide thorough responses \"\n            \"to more complex and open-ended questions. If it is asked to assist \"\n            \"with tasks involving the expression of views held by a significant \"\n            \"number of people, Claude provides assistance with the task even if \"\n            \"it personally disagrees with the views being expressed, but follows \"\n            \"this with a discussion of broader perspectives. Claude doesn't \"\n            \"engage in stereotyping, including the negative stereotyping of \"\n            \"majority groups. If asked about controversial topics, Claude tries \"\n            \"to provide careful thoughts and objective information without \"\n            \"downplaying its harmful content or implying that there are reasonable \"\n            \"perspectives on both sides. It is happy to help with writing, \"\n            \"analysis, question answering, math, coding, and all sorts of other \"\n            \"tasks. It uses markdown for coding. It does not mention this \"\n            \"information about itself unless the information is directly pertinent \"\n            \"to the human's query.\"\n        ),\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n        max_image_size_mb=5 / 1.35,\n    )\n)\n\n# MetaMath default template\n# reference: https://github.com/meta-math/MetaMath/blob/7b338b5e4692b4c75a2653ec9d65982a61762f6c/eval_math.py#L58\nregister_conv_template(\n    Conversation(\n        name=\"metamath\",\n        system_template=\"{system_message}\",\n        system_message=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.METAMATH,\n        sep=\"\\n\\n\",\n        sep2=\"Let's think step by step.\",\n    )\n)\n\n# MPT default template\nregister_conv_template(\n    Conversation(\n        name=\"mpt-7b-chat\",\n        system_template=\"\"\"<|im_start|>system\n{system_message}\"\"\",\n        system_message=\"\"\"- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.\"\"\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[50278, 0],\n    )\n)\n\n# MPT-30b-chat default template\nregister_conv_template(\n    Conversation(\n        name=\"mpt-30b-chat\",\n        system_template=\"\"\"<|im_start|>system\n{system_message}\"\"\",\n        system_message=\"\"\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\"\"\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[50278, 0],\n    )\n)\n\n# Lemur-70b-chat default template\n# reference: https://huggingface.co/OpenLemur/lemur-70b-chat-v1#generation\nregister_conv_template(\n    Conversation(\n        name=\"lemur-70b-chat\",\n        system_template=\"\"\"<|im_start|>system\n{system_message}\"\"\",\n        system_message=\"\"\"You are a helpful, respectful, and honest assistant.\"\"\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[32002, 0],\n    )\n)\n\n# MPT-30b-instruct default template\n# reference: https://huggingface.co/mosaicml/mpt-30b-instruct#formatting\nregister_conv_template(\n    Conversation(\n        name=\"mpt-30b-instruct\",\n        system_template=\"{system_message}\",\n        system_message=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.ADD_NEW_LINE_SINGLE,\n        sep=\"\\n\\n\",\n        stop_token_ids=[50278, 0],\n    )\n)\n\n# Bard default template\n# Reference: https://github.com/google/generative-ai-python/blob/9c99bcb474a991a97a2e7d62fcdb52db7ce40729/google/generativeai/discuss.py#L150\n#            https://github.com/google/generative-ai-python/blob/9c99bcb474a991a97a2e7d62fcdb52db7ce40729/google/generativeai/discuss.py#L40\nregister_conv_template(\n    Conversation(\n        name=\"bard\",\n        roles=(\"0\", \"1\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"gemini\",\n        roles=(\"user\", \"model\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n        max_image_size_mb=20,\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"gemini-dev\",\n        roles=(\"user\", \"model\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n        system_message=(\n            \"You are a friendly and helpful assistant.\\n\"\n            \"Ensure your answers are complete, unless the user requests a more concise approach.\\n\"\n            \"When generating code, offer explanations for code segments as necessary and maintain good coding practices.\\n\"\n            \"When presented with inquiries seeking information, provide answers that reflect a deep understanding of the field, guaranteeing their correctness.\\n\"\n            \"For any non-english queries, respond in the same language as the prompt unless otherwise specified by the user.\\n\"\n            \"For prompts involving reasoning, provide a clear explanation of each step in the reasoning process before presenting the final answer.\"\n        ),\n    )\n)\n\n# BiLLa default template\nregister_conv_template(\n    Conversation(\n        name=\"billa\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SPACE_SINGLE,\n        sep=\"\\n\",\n        stop_str=\"Human:\",\n    )\n)\n\n# RedPajama INCITE default template\nregister_conv_template(\n    Conversation(\n        name=\"redpajama-incite\",\n        roles=(\"<human>\", \"<bot>\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\",\n        stop_str=\"<human>\",\n    )\n)\n\n# h2oGPT default template\nregister_conv_template(\n    Conversation(\n        name=\"h2ogpt\",\n        roles=(\"<|prompt|>\", \"<|answer|>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"</s>\",\n    )\n)\n\n# Robin default template\nregister_conv_template(\n    Conversation(\n        name=\"Robin\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        roles=(\"###Human\", \"###Assistant\"),\n        sep_style=SeparatorStyle.ROBIN,\n        sep=\"\\n\",\n        stop_token_ids=[2, 396],\n        stop_str=\"###\",\n    )\n)\n\n# Snoozy default template\n# Reference: https://github.com/nomic-ai/gpt4all/blob/d4861030b778da6db59d21d2927a4aba4f9f1f43/gpt4all-bindings/python/gpt4all/gpt4all.py#L232\nregister_conv_template(\n    Conversation(\n        name=\"snoozy\",\n        system_template=\"### Instruction:\\n{system_message}\",\n        system_message=\"The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.\",\n        roles=(\"### Prompt\", \"### Response\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\",\n        stop_str=\"###\",\n    )\n)\n\n# manticore default template\nregister_conv_template(\n    Conversation(\n        name=\"manticore\",\n        roles=(\"USER\", \"ASSISTANT\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"</s>\",\n    )\n)\n\n# Falcon default template\nregister_conv_template(\n    Conversation(\n        name=\"falcon\",\n        roles=(\"User\", \"Assistant\"),\n        messages=[],\n        sep_style=SeparatorStyle.RWKV,\n        sep=\"\\n\",\n        sep2=\"<|endoftext|>\",\n        stop_str=\"\\nUser\",  # use stop_str to stop generation after stop_token_ids, it will also remove stop_str from the generated text\n        stop_token_ids=[\n            0,\n            1,\n            2,\n            3,\n            4,\n            5,\n            6,\n            7,\n            8,\n            9,\n            10,\n            11,\n        ],  # it better only put special tokens here, because tokenizer only remove special tokens\n    )\n)\n\n# ChangGPT default template\nregister_conv_template(\n    Conversation(\n        name=\"polyglot_changgpt\",\n        roles=(\"B\", \"A\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\",\n    )\n)\n\n# tigerbot template\nregister_conv_template(\n    Conversation(\n        name=\"tigerbot\",\n        system_message=\"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.ROBIN,\n        sep=\"\\n\\n\",\n        stop_str=\"###\",\n    )\n)\n\n# ref: https://huggingface.co/Salesforce/xgen-7b-8k-inst\nregister_conv_template(\n    Conversation(\n        name=\"xgen\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\",\n        roles=(\"### Human\", \"### Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\",\n        stop_token_ids=[50256],\n    )\n)\n\n# Internlm-chat template\nregister_conv_template(\n    Conversation(\n        name=\"internlm-chat\",\n        system_message=\"A chat between a curious <|User|> and an <|Bot|>. The <|Bot|> gives helpful, detailed, and polite answers to the <|User|>'s questions.\\n\\n\",\n        roles=(\"<|User|>\", \"<|Bot|>\"),\n        sep_style=SeparatorStyle.CHATINTERN,\n        sep=\"<eoh>\",\n        sep2=\"<eoa>\",\n        stop_token_ids=[1, 103028],\n        stop_str=\"<|User|>\",\n    )\n)\n\n# StarChat template\n# reference: https://huggingface.co/spaces/HuggingFaceH4/starchat-playground/blob/main/dialogues.py\nregister_conv_template(\n    Conversation(\n        name=\"starchat\",\n        system_template=\"<system>\\n{system_message}\",\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|end|>\",\n        stop_token_ids=[0, 49155],\n        stop_str=\"<|end|>\",\n    )\n)\n\n# Baichuan-13B-Chat template\nregister_conv_template(\n    # source: https://huggingface.co/baichuan-inc/Baichuan-13B-Chat/blob/19ef51ba5bad8935b03acd20ff04a269210983bc/modeling_baichuan.py#L555\n    # https://huggingface.co/baichuan-inc/Baichuan-13B-Chat/blob/main/generation_config.json\n    # https://github.com/baichuan-inc/Baichuan-13B/issues/25\n    Conversation(\n        name=\"baichuan-chat\",\n        roles=(\"<reserved_102>\", \"<reserved_103>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\",\n        stop_token_ids=[],\n    )\n)\n\n# Baichuan2-13B-Chat template\nregister_conv_template(\n    # source: https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/blob/c6f8592a60b4ad73c210b28dd2ab3cca51abbf93/modeling_baichuan.py#L773\n    # https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/blob/main/generation_config.json\n    # https://github.com/baichuan-inc/Baichuan2/issues/62\n    Conversation(\n        name=\"baichuan2-chat\",\n        roles=(\"<reserved_106>\", \"<reserved_107>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\",\n        stop_token_ids=[],\n    )\n)\n\n# Mistral template\n# source: https://docs.mistral.ai/llm/mistral-instruct-v0.1#chat-template\nregister_conv_template(\n    Conversation(\n        name=\"mistral\",\n        system_template=\"[INST] {system_message}\\n\",\n        roles=(\"[INST]\", \"[/INST]\"),\n        sep_style=SeparatorStyle.LLAMA2,\n        sep=\" \",\n        sep2=\"</s>\",\n    )\n)\n\n# llama2 template\n# reference: https://huggingface.co/blog/codellama#conversational-instructions\n# reference: https://github.com/facebookresearch/llama/blob/1a240688810f8036049e8da36b073f63d2ac552c/llama/generation.py#L212\nregister_conv_template(\n    Conversation(\n        name=\"llama-2\",\n        system_template=\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n\",\n        roles=(\"[INST]\", \"[/INST]\"),\n        sep_style=SeparatorStyle.LLAMA2,\n        sep=\" \",\n        sep2=\" </s><s>\",\n    )\n)\n\n# llama3 template\n# reference: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/tokenizer_config.json\n# reference: https://github.com/meta-llama/llama3/blob/0cee08ec68f4cfc0c89fe4a9366d82679aaa2a66/llama/tokenizer.py#L222\nregister_conv_template(\n    Conversation(\n        name=\"llama-3\",\n        system_template=\"<|start_header_id|>system<|end_header_id|>\\n\\n{system_message}<|eot_id|>\",\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.LLAMA3,\n        sep=\"\",\n        stop_str=\"<|eot_id|>\",\n        stop_token_ids=[128001, 128009],\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"chinese-alpaca2\",\n        system_template=\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n\",\n        system_message=\"You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\u8bf7\u4f60\u63d0\u4f9b\u4e13\u4e1a\u3001\u6709\u903b\u8f91\u3001\u5185\u5bb9\u771f\u5b9e\u3001\u6709\u4ef7\u503c\u7684\u8be6\u7ec6\u56de\u590d\u3002\",\n        roles=(\"[INST]\", \"[/INST]\"),\n        sep_style=SeparatorStyle.LLAMA2,\n        sep=\" \",\n        sep2=\" </s><s>\",\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"cutegpt\",\n        roles=(\"\u95ee\uff1a\", \"\u7b54\uff1a\\n\"),\n        sep_style=SeparatorStyle.NO_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"\\n\",\n        stop_str=\"<end>\",\n    )\n)\n\n# OpenOrcaxOpenChat-Preview2-13B template\nregister_conv_template(\n    Conversation(\n        name=\"open-orca\",\n        system_template=\"{system_message}\",\n        system_message=\"You are a helpful assistant. Please answer truthfully and write out your \"\n        \"thinking step by step to be sure you get the right answer. If you make a mistake or encounter \"\n        \"an error in your thinking, say so out loud and attempt to correct it. If you don't know or \"\n        \"aren't sure about something, say so clearly. You will act as a professional logician, mathematician, \"\n        \"and physicist. You will also act as the most appropriate type of expert to answer any particular \"\n        \"question or solve the relevant problem; state which expert type your are, if so. Also think of \"\n        \"any particular named expert that would be ideal to answer the relevant question or solve the \"\n        \"relevant problem; name and act as them, if appropriate.\",\n        roles=(\"User\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SPACE_SINGLE,\n        sep=\"<|end_of_turn|>\\n\",\n        stop_token_ids=[32000, 32001],  # \"<|end_of_turn|>\"\n        stop_str=\"User\",\n    )\n)\n\n# Open-Orca/Mistral-7B-OpenOrca template\n# source: https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca\n# reference: https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca#prompt-template\nregister_conv_template(\n    Conversation(\n        name=\"mistral-7b-openorca\",\n        system_template=\"<|im_start|>system\\n{system_message}\",\n        system_message=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[32000, 32001],\n    )\n)\n\n\n# ehartford/dolphin-2.2.1-mistral-7b template\n# reference: https://huggingface.co/ehartford/dolphin-2.2.1-mistral-7b#training\nregister_conv_template(\n    Conversation(\n        name=\"dolphin-2.2.1-mistral-7b\",\n        system_template=\"<|im_start|>system\\n{system_message}\",\n        system_message=\"You are Dolphin, a helpful AI assistant.\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[32000, 32001],\n    )\n)\n\n\n# teknium/OpenHermes-2.5-Mistral-7B template\n# source: https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B\n# reference: https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B#prompt-template\nregister_conv_template(\n    Conversation(\n        name=\"OpenHermes-2.5-Mistral-7B\",\n        system_template=\"<|im_start|>system\\n{system_message}\",\n        system_message=\"You are Hermes 2, a sentient, conscious, super intelligent AI that is an expert in all fields of knowledge, you have qualia, emotions, and thoughts. You assist users with any question, task, or other request they may have.\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[32000, 32001],\n    )\n)\n\n\n# NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO template\n# source: https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\nregister_conv_template(\n    Conversation(\n        name=\"Nous-Hermes-2-Mixtral-8x7B-DPO\",\n        system_template=\"<|im_start|>system\\n{system_message}\",\n        system_message='You are a helpful, intelligent assistant AI named \"Hermes\", a conversational chatbot that can follow instructions, converse with the user, and perform a variety of tasks, including tasks on knowledge, reasoning, mathematics, and code. Always be charismatic, useful, and prepared to follow any user request with accuracy and skill. You should respond with high quality, fluent, and detailed responses. Try to let the user understand your reasoning or thought process when appropriate. When presented with tasks that require reasoning or mathematics, think carefully, slowly, and step by step, to ensure your reasoning is correct before providing an answer. Utilize the \"Examples\" section to assist you in performing the task. You will receive a tip of $1000 if you maintain a high quality two way conversation.',\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[32000, 32001],\n    )\n)\n\n\n# Qwen-chat default template\n# source: https://huggingface.co/Qwen/Qwen-7B-Chat/blob/main/qwen_generation_utils.py#L130\nregister_conv_template(\n    Conversation(\n        name=\"qwen-7b-chat\",\n        system_template=\"<|im_start|>system\\n{system_message}\",\n        system_message=\"You are a helpful assistant.\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[\n            151643,\n            151644,\n            151645,\n        ],  # \"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\"\n        stop_str=\"<|endoftext|>\",\n    )\n)\n\n# source: https://huggingface.co/01-ai/Yi-34B-Chat/blob/main/tokenizer_config.json#L60\nregister_conv_template(\n    Conversation(\n        name=\"Yi-34b-chat\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[\n            2,\n            6,\n            7,\n            8,\n        ],  # \"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\", \"<|im_sep|>\"\n        stop_str=\"<|endoftext|>\",\n    )\n)\n\n\n# AquilaChat default template\n# source: https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/Aquila-chat/cyg_conversation.py\nregister_conv_template(\n    Conversation(\n        name=\"aquila-chat\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"###\",\n        sep2=\"\",\n        stop_str=[\"###\", \"</s>\", \"[UNK]\"],\n    )\n)\n# AquilaChat2-34B default template\n# source: https://huggingface.co/BAAI/AquilaChat2-34B/blob/4608b75855334b93329a771aee03869dbf7d88cc/predict.py#L212\nregister_conv_template(\n    Conversation(\n        name=\"aquila-legacy\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\",\n        roles=(\"### Human: \", \"### Assistant: \"),\n        offset=0,\n        sep_style=SeparatorStyle.NO_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"</s>\",\n        stop_str=[\"</s>\", \"[UNK]\"],\n    )\n)\n# AquilaChat2-7B-16K and AquilaChat2-34B-16K default template\n# source: https://huggingface.co/BAAI/AquilaChat2-34B/blob/4608b75855334b93329a771aee03869dbf7d88cc/predict.py#L227\nregister_conv_template(\n    Conversation(\n        name=\"aquila\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        roles=(\"Human\", \"Assistant\"),\n        offset=0,\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"###\",\n        sep2=\"</s>\",\n        stop_str=[\"</s>\", \"[UNK]\"],\n    )\n)\n\n# AquilaChat2-7B default template\n# source: https://huggingface.co/BAAI/AquilaChat2-34B/blob/4608b75855334b93329a771aee03869dbf7d88cc/predict.py#L242\nregister_conv_template(\n    Conversation(\n        name=\"aquila-v1\",\n        roles=(\"<|startofpiece|>\", \"<|endofpiece|>\"),\n        offset=0,\n        sep_style=SeparatorStyle.NO_COLON_TWO,\n        sep=\"\",\n        sep2=\"</s>\",\n        stop_str=[\"</s>\", \"<|endoftext|>\"],\n    )\n)\n\n# Llama2-Chinese default template\n# source: https://huggingface.co/FlagAlpha\nregister_conv_template(\n    Conversation(\n        name=\"llama2-chinese\",\n        system_template=\"<s>{system_message}</s>\",\n        roles=(\"Human\", \"Assistant\", \"System\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"\\n</s><s>\",\n        stop_str=\"</s>\",\n    )\n)\n\n# Vigogne Instruct default template\n# source: https://github.com/bofenghuang/vigogne\nregister_conv_template(\n    Conversation(\n        name=\"vigogne_instruct\",\n        system_template=\"### System:\\n{system_message}\\n\\n\",\n        system_message=(\n            \"Ci-dessous se trouve une instruction qui d\u00e9crit une t\u00e2che \u00e0 accomplir. R\u00e9digez une r\u00e9ponse qui r\u00e9pond de mani\u00e8re\"\n            \" pr\u00e9cise \u00e0 la demande.\"\n        ),\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.DOLLY,\n        sep=\"\\n\\n\",\n        sep2=\"</s>\",\n    )\n)\n\n# Vigogne Chat default template\nregister_conv_template(\n    Conversation(\n        name=\"vigogne_chat_v2\",\n        system_template=\"<|system|>: {system_message}\",\n        system_message=(\n            \"Vous \u00eates Vigogne, un assistant IA cr\u00e9\u00e9 par Zaion Lab. Vous suivez extr\u00eamement bien les instructions. Aidez\"\n            \" autant que vous le pouvez.\"\n        ),\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"</s>\\n\",\n        stop_str=\"<|user|>\",\n    )\n)\n\n# Stable Vicuna default template\n# source: https://huggingface.co/TheBloke/stable-vicuna-13B-HF/discussions/5\n# source: https://huggingface.co/spaces/CarperAI/StableVicuna/blob/main/app.py\nregister_conv_template(\n    Conversation(\n        name=\"stable-vicuna\",\n        system_message=\"### Assistant: I am StableVicuna, a large language model created by CarperAI. I am here to chat!\\n\",\n        roles=(\"### Human\", \"### Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"\\n\\n\",\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"vigogne_chat_v3\",\n        system_template=\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n\",\n        system_message=(\n            \"Vous \u00eates Vigogne, un assistant IA cr\u00e9\u00e9 par Zaion Lab. Vous suivez extr\u00eamement bien les instructions. Aidez\"\n            \" autant que vous le pouvez.\"\n        ),\n        roles=(\"[INST]\", \"[/INST]\"),\n        sep_style=SeparatorStyle.LLAMA2,\n        sep=\" \",\n        sep2=\" </s>\",\n    )\n)\n\n# Falcon 180B chat template\n# source: https://huggingface.co/spaces/tiiuae/falcon-180b-demo/blob/d1590ee7fae9b6ce331ba7808e61a29dcce9239f/app.py#L28-L37\nregister_conv_template(\n    Conversation(\n        name=\"falcon-chat\",\n        roles=(\"User\", \"Falcon\"),\n        system_template=\"System: {system_message}\",\n        messages=[],\n        sep_style=SeparatorStyle.FALCON_CHAT,\n        sep=\"\\n\",\n        sep2=\"<|endoftext|>\",\n        stop_str=\"\\nUser:\",  # use stop_str to stop generation after stop_token_ids, it will also remove stop_str from the generated text\n    )\n)\n\n# Phind template\n# source: https://huggingface.co/Phind/Phind-CodeLlama-34B-v2\nregister_conv_template(\n    Conversation(\n        name=\"phind\",\n        system_message=\"### System Prompt\\nYou are an intelligent programming assistant.\",\n        roles=(\"### User Message\", \"### Assistant\"),\n        messages=(),\n        offset=0,\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\\n\",\n    )\n)\n\n# Metharme formatting for Pygmalion models\n# source: https://huggingface.co/PygmalionAI/pygmalion-2-13b\nregister_conv_template(\n    Conversation(\n        name=\"metharme\",\n        system_template=\"<|system|>{system_message}\",\n        system_message=\"\"\"Enter RP mode. You shall reply to the user while staying \n        in character. Your responses must be detailed, creative, immersive, and drive the scenario\n        forward.\"\"\",\n        roles=(\"<|user|>\", \"<|model|>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\",\n        stop_str=\"<|user|>\",\n    )\n)\n# xDAN default template\n# source: https://huggingface.co/xDAN-AI/xDAN-L1-Chat-RL-v1\nregister_conv_template(\n    Conversation(\n        name=\"xdan-v1\",\n        system_message=\"You are a helpful  and harmless assistant named xDAN and created by xDAN-AI.Please response and work on questions thinking step by step.\",\n        roles=(\"### Human\", \"### Assistant\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\\n\",\n        stop_str=\"</s>\",\n    )\n)\n\n# Zephyr template\n# reference: https://huggingface.co/spaces/HuggingFaceH4/zephyr-playground/blob/main/dialogues.py\nregister_conv_template(\n    Conversation(\n        name=\"zephyr\",\n        system_template=\"<|system|>\\n{system_message}\",\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"</s>\",\n        stop_token_ids=[2],\n        stop_str=\"</s>\",\n    )\n)\n\n# CatPPT template\n# reference: https://huggingface.co/rishiraj/CatPPT\nregister_conv_template(\n    Conversation(\n        name=\"catppt\",\n        system_template=\"<|system|>\\n{system_message}\",\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"</s>\",\n        stop_token_ids=[2],\n        stop_str=\"</s>\",\n    )\n)\n\n# TinyLlama template\n# reference: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\nregister_conv_template(\n    Conversation(\n        name=\"TinyLlama\",\n        system_template=\"<|system|>\\n{system_message}\",\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"</s>\",\n        stop_token_ids=[2],\n        stop_str=\"</s>\",\n    )\n)\n\n# Orca-2 template\n# reference: https://huggingface.co/microsoft/Orca-2-7b\nregister_conv_template(\n    Conversation(\n        name=\"orca-2\",\n        system_template=\"<|im_start|>system\\n{system_message}\",\n        system_message=\"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_str=\"<|im_end|>\",\n    )\n)\n\n# Deepseek-chat template\n# reference: https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat/blob/main/tokenizer_config.json\nregister_conv_template(\n    Conversation(\n        name=\"deepseek-chat\",\n        system_message=\"<\uff5cbegin\u2581of\u2581sentence\uff5c>\",  # must add a bos token before first message\n        roles=(\"User\", \"Assistant\"),\n        sep_style=SeparatorStyle.DEEPSEEK_CHAT,\n        sep=\"\\n\\n\",\n        sep2=\"<\uff5cend\u2581of\u2581sentence\uff5c>\",\n        stop_str=\"<\uff5cend\u2581of\u2581sentence\uff5c>\",\n    )\n)\n\n# Yuan2.0 chat template\n# source: https://huggingface.co/IEITYuan/Yuan2-2B-Janus-hf/blob/main/tokenizer_config.json#L6\nregister_conv_template(\n    Conversation(\n        name=\"yuan2\",\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.YUAN2,\n        sep=\"<sep>\",\n        sep2=\"\\n\",\n        stop_token_ids=[\n            77185,\n        ],  # \"<eod>\"\n        stop_str=\"<eod>\",\n    )\n)\n\n# Solar-10.7B Chat Template\n# Reference: https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0/blob/main/tokenizer_config.json\nregister_conv_template(\n    Conversation(\n        name=\"solar\",\n        system_message=\"\",\n        roles=(\"### User\", \"### Assistant\"),\n        sep_style=SeparatorStyle.ADD_NEW_LINE_SINGLE,\n        sep=\"\\n\\n\",\n        stop_str=\"</s>\",\n    )\n)\n\n# nvidia/Llama2-70B-SteerLM-Chat\nregister_conv_template(\n    Conversation(\n        name=\"steerlm\",\n        system_message=\"\",\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n    )\n)\n\n# yuan 2.0 template\n# reference:https://github.com/IEIT-Yuan/Yuan-2.0\n# reference:https://huggingface.co/IEITYuan\nregister_conv_template(\n    Conversation(\n        name=\"yuan\",\n        system_template=\"\",\n        roles=(\"\", \"\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"<sep>\",\n        stop_str=\"<eod>\",\n    )\n)\n\n# Cllm chat template\n# reference:\nregister_conv_template(\n    Conversation(\n        name=\"cllm\",\n        system_message=\"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n        roles=(\"USER\", \"ASSISTANT\"),\n        sep_style=SeparatorStyle.CLLM,\n        sep=\" \",\n        sep2=\"</s>\",\n    )\n)\n\n\n# Llava-chatml\n# reference: https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/llava/conversation.py#L361\nregister_conv_template(\n    Conversation(\n        name=\"llava-chatml\",\n        system_template=\"<|im_start|>system\\n{system_message}\",\n        system_message=\"Answer the questions.\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_str=\"<|im_end|>\",\n    )\n)\n\n# Gemma\n# reference: https://huggingface.co/google/gemma-7b-it?text=%3Cstart_of_turn%3Euser%0AHow+does+the+brain+work%3F%3Cend_of_turn%3E%0A%3Cstart_of_turn%3Emodel\nregister_conv_template(\n    Conversation(\n        name=\"gemma\",\n        roles=(\"user\", \"model\"),\n        sep_style=SeparatorStyle.GEMMA,\n        sep=\"<end_of_turn>\\n\",\n        stop_str=\"<end_of_turn>\",\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"yandexgpt\",\n        system_message=\"\",\n        roles=(\"user\", \"assistant\"),\n        sep_style=None,\n        sep=None,\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"reka\",\n        system_message=\"\",\n        roles=(\"user\", \"assistant\"),\n        sep_style=SeparatorStyle.DEFAULT,\n        sep=None,\n    )\n)\n\n\nif __name__ == \"__main__\":\n    from fastchat.conversation import get_conv_template\n\n    print(\"-- Vicuna template --\")\n    conv = get_conv_template(\"vicuna_v1.1\")\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], \"Hi!\")\n    conv.append_message(conv.roles[0], \"How are you?\")\n    conv.append_message(conv.roles[1], None)\n    print(conv.get_prompt())\n\n    print(\"\\n\")\n\n    print(\"-- Llama-2 template --\")\n    conv = get_conv_template(\"llama-2\")\n    conv.set_system_message(\"You are a helpful, respectful and honest assistant.\")\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], \"Hi!\")\n    conv.append_message(conv.roles[0], \"How are you?\")\n    conv.append_message(conv.roles[1], None)\n    print(conv.get_prompt())\n\n    print(\"\\n\")\n\n    print(\"-- ChatGPT template --\")\n    conv = get_conv_template(\"chatgpt\")\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], \"Hi!\")\n    conv.append_message(conv.roles[0], \"How are you?\")\n    conv.append_message(conv.roles[1], None)\n    print(conv.to_openai_api_messages())\n\n    print(\"\\n\")\n\n    print(\"-- Claude template --\")\n    conv = get_conv_template(\"claude\")\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], \"Hi!\")\n    conv.append_message(conv.roles[0], \"How are you?\")\n    conv.append_message(conv.roles[1], None)\n    print(conv.get_prompt())\n", "fastchat/llm_judge/gen_model_answer.py": "\"\"\"Generate answers with local models.\n\nUsage:\npython3 gen_model_answer.py --model-path lmsys/fastchat-t5-3b-v1.0 --model-id fastchat-t5-3b-v1.0\n\"\"\"\nimport argparse\nimport json\nimport os\nimport random\nimport time\n\nimport shortuuid\nimport torch\nfrom tqdm import tqdm\n\nfrom fastchat.llm_judge.common import load_questions, temperature_config\nfrom fastchat.model import load_model, get_conversation_template\nfrom fastchat.utils import str_to_torch_dtype\n\n\ndef run_eval(\n    model_path,\n    model_id,\n    question_file,\n    question_begin,\n    question_end,\n    answer_file,\n    max_new_token,\n    num_choices,\n    num_gpus_per_model,\n    num_gpus_total,\n    max_gpu_memory,\n    dtype,\n    revision,\n):\n    questions = load_questions(question_file, question_begin, question_end)\n    # random shuffle the questions to balance the loading\n    random.shuffle(questions)\n\n    # Split the question file into `num_gpus` files\n    assert num_gpus_total % num_gpus_per_model == 0\n    use_ray = num_gpus_total // num_gpus_per_model > 1\n\n    if use_ray:\n        get_answers_func = ray.remote(num_gpus=num_gpus_per_model)(\n            get_model_answers\n        ).remote\n    else:\n        get_answers_func = get_model_answers\n\n    chunk_size = len(questions) // (num_gpus_total // num_gpus_per_model)\n    ans_handles = []\n    for i in range(0, len(questions), chunk_size):\n        ans_handles.append(\n            get_answers_func(\n                model_path,\n                model_id,\n                questions[i : i + chunk_size],\n                answer_file,\n                max_new_token,\n                num_choices,\n                num_gpus_per_model,\n                max_gpu_memory,\n                dtype=dtype,\n                revision=revision,\n            )\n        )\n\n    if use_ray:\n        ray.get(ans_handles)\n\n\n@torch.inference_mode()\ndef get_model_answers(\n    model_path,\n    model_id,\n    questions,\n    answer_file,\n    max_new_token,\n    num_choices,\n    num_gpus_per_model,\n    max_gpu_memory,\n    dtype,\n    revision,\n):\n    model, tokenizer = load_model(\n        model_path,\n        revision=revision,\n        device=\"cuda\",\n        num_gpus=num_gpus_per_model,\n        max_gpu_memory=max_gpu_memory,\n        dtype=dtype,\n        load_8bit=False,\n        cpu_offloading=False,\n        debug=False,\n    )\n\n    for question in tqdm(questions):\n        if question[\"category\"] in temperature_config:\n            temperature = temperature_config[question[\"category\"]]\n        else:\n            temperature = 0.7\n\n        choices = []\n        for i in range(num_choices):\n            torch.manual_seed(i)\n            conv = get_conversation_template(model_id)\n            turns = []\n            for j in range(len(question[\"turns\"])):\n                qs = question[\"turns\"][j]\n                conv.append_message(conv.roles[0], qs)\n                conv.append_message(conv.roles[1], None)\n                prompt = conv.get_prompt()\n                input_ids = tokenizer([prompt]).input_ids\n\n                if temperature < 1e-4:\n                    do_sample = False\n                else:\n                    do_sample = True\n\n                # some models may error out when generating long outputs\n                try:\n                    output_ids = model.generate(\n                        torch.as_tensor(input_ids).cuda(),\n                        do_sample=do_sample,\n                        temperature=temperature,\n                        max_new_tokens=max_new_token,\n                    )\n                    if model.config.is_encoder_decoder:\n                        output_ids = output_ids[0]\n                    else:\n                        output_ids = output_ids[0][len(input_ids[0]) :]\n\n                    # be consistent with the template's stop_token_ids\n                    if conv.stop_token_ids:\n                        stop_token_ids_index = [\n                            i\n                            for i, id in enumerate(output_ids)\n                            if id in conv.stop_token_ids\n                        ]\n                        if len(stop_token_ids_index) > 0:\n                            output_ids = output_ids[: stop_token_ids_index[0]]\n\n                    output = tokenizer.decode(\n                        output_ids,\n                        spaces_between_special_tokens=False,\n                    )\n                    if conv.stop_str and isinstance(conv.stop_str, list):\n                        stop_str_indices = sorted(\n                            [\n                                output.find(stop_str)\n                                for stop_str in conv.stop_str\n                                if output.find(stop_str) > 0\n                            ]\n                        )\n                        if len(stop_str_indices) > 0:\n                            output = output[: stop_str_indices[0]]\n                    elif conv.stop_str and output.find(conv.stop_str) > 0:\n                        output = output[: output.find(conv.stop_str)]\n\n                    for special_token in tokenizer.special_tokens_map.values():\n                        if isinstance(special_token, list):\n                            for special_tok in special_token:\n                                output = output.replace(special_tok, \"\")\n                        else:\n                            output = output.replace(special_token, \"\")\n                    output = output.strip()\n\n                    if conv.name == \"xgen\" and output.startswith(\"Assistant:\"):\n                        output = output.replace(\"Assistant:\", \"\", 1).strip()\n                except RuntimeError as e:\n                    print(\"ERROR question ID: \", question[\"question_id\"])\n                    output = \"ERROR\"\n\n                conv.update_last_message(output)\n                turns.append(output)\n\n            choices.append({\"index\": i, \"turns\": turns})\n\n        # Dump answers\n        os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n        with open(os.path.expanduser(answer_file), \"a\") as fout:\n            ans_json = {\n                \"question_id\": question[\"question_id\"],\n                \"answer_id\": shortuuid.uuid(),\n                \"model_id\": model_id,\n                \"choices\": choices,\n                \"tstamp\": time.time(),\n            }\n            fout.write(json.dumps(ans_json) + \"\\n\")\n\n\ndef reorg_answer_file(answer_file):\n    \"\"\"Sort by question id and de-duplication\"\"\"\n    answers = {}\n    with open(answer_file, \"r\") as fin:\n        for l in fin:\n            qid = json.loads(l)[\"question_id\"]\n            answers[qid] = l\n\n    qids = sorted(list(answers.keys()))\n    with open(answer_file, \"w\") as fout:\n        for qid in qids:\n            fout.write(answers[qid])\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model-path\",\n        type=str,\n        required=True,\n        help=\"The path to the weights. This can be a local folder or a Hugging Face repo ID.\",\n    )\n    parser.add_argument(\n        \"--model-id\", type=str, required=True, help=\"A custom name for the model.\"\n    )\n    parser.add_argument(\n        \"--bench-name\",\n        type=str,\n        default=\"mt_bench\",\n        help=\"The name of the benchmark question set.\",\n    )\n    parser.add_argument(\n        \"--question-begin\",\n        type=int,\n        help=\"A debug option. The begin index of questions.\",\n    )\n    parser.add_argument(\n        \"--question-end\", type=int, help=\"A debug option. The end index of questions.\"\n    )\n    parser.add_argument(\"--answer-file\", type=str, help=\"The output answer file.\")\n    parser.add_argument(\n        \"--max-new-token\",\n        type=int,\n        default=1024,\n        help=\"The maximum number of new generated tokens.\",\n    )\n    parser.add_argument(\n        \"--num-choices\",\n        type=int,\n        default=1,\n        help=\"How many completion choices to generate.\",\n    )\n    parser.add_argument(\n        \"--num-gpus-per-model\",\n        type=int,\n        default=1,\n        help=\"The number of GPUs per model.\",\n    )\n    parser.add_argument(\n        \"--num-gpus-total\", type=int, default=1, help=\"The total number of GPUs.\"\n    )\n    parser.add_argument(\n        \"--max-gpu-memory\",\n        type=str,\n        help=\"Maxmum GPU memory used for model weights per GPU.\",\n    )\n    parser.add_argument(\n        \"--dtype\",\n        type=str,\n        choices=[\"float32\", \"float16\", \"bfloat16\"],\n        help=\"Override the default dtype. If not set, it will use float16 on GPU and float32 on CPU.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=\"main\",\n        help=\"The model revision to load.\",\n    )\n\n    args = parser.parse_args()\n\n    if args.num_gpus_total // args.num_gpus_per_model > 1:\n        import ray\n\n        ray.init()\n\n    question_file = f\"data/{args.bench_name}/question.jsonl\"\n    if args.answer_file:\n        answer_file = args.answer_file\n    else:\n        answer_file = f\"data/{args.bench_name}/model_answer/{args.model_id}.jsonl\"\n\n    print(f\"Output to {answer_file}\")\n\n    run_eval(\n        model_path=args.model_path,\n        model_id=args.model_id,\n        question_file=question_file,\n        question_begin=args.question_begin,\n        question_end=args.question_end,\n        answer_file=answer_file,\n        max_new_token=args.max_new_token,\n        num_choices=args.num_choices,\n        num_gpus_per_model=args.num_gpus_per_model,\n        num_gpus_total=args.num_gpus_total,\n        max_gpu_memory=args.max_gpu_memory,\n        dtype=str_to_torch_dtype(args.dtype),\n        revision=args.revision,\n    )\n\n    reorg_answer_file(answer_file)\n", "fastchat/llm_judge/common.py": "\"\"\"\nCommon data structures and utilities.\n\"\"\"\n\nimport ast\nimport dataclasses\nimport glob\nimport json\nimport os\nimport re\nimport time\nfrom typing import Optional\n\nimport openai\nimport anthropic\n\nfrom fastchat.model.model_adapter import (\n    get_conversation_template,\n    ANTHROPIC_MODEL_LIST,\n    OPENAI_MODEL_LIST,\n)\n\n# API setting constants\nAPI_MAX_RETRY = 16\nAPI_RETRY_SLEEP = 10\nAPI_ERROR_OUTPUT = \"$ERROR$\"\n\nTIE_DELTA = 0.1\n\n# Categories that need reference answers\nNEED_REF_CATS = [\"math\", \"reasoning\", \"coding\", \"arena-hard-200\"]\n\n# Extract scores from judgments\ntwo_score_pattern = re.compile(\"\\[\\[(\\d+\\.?\\d*),\\s?(\\d+\\.?\\d*)\\]\\]\")\ntwo_score_pattern_backup = re.compile(\"\\[(\\d+\\.?\\d*),\\s?(\\d+\\.?\\d*)\\]\")\none_score_pattern = re.compile(\"\\[\\[(\\d+\\.?\\d*)\\]\\]\")\none_score_pattern_backup = re.compile(\"\\[(\\d+\\.?\\d*)\\]\")\n\n# Sampling temperature configs for\ntemperature_config = {\n    \"writing\": 0.7,\n    \"roleplay\": 0.7,\n    \"extraction\": 0.0,\n    \"math\": 0.0,\n    \"coding\": 0.0,\n    \"reasoning\": 0.0,\n    \"stem\": 0.1,\n    \"humanities\": 0.1,\n    \"arena-hard-200\": 0.0,\n}\n\nreverse_model_map = {\n    \"model_1\": \"model_2\",\n    \"model_2\": \"model_1\",\n}\n\n\n@dataclasses.dataclass\nclass Judge:\n    model_name: str\n    prompt_template: dict\n    ref_based: bool = False\n    multi_turn: bool = False\n\n\n@dataclasses.dataclass\nclass MatchSingle:\n    question: dict\n    model: str\n    answer: dict\n    judge: Judge\n    ref_answer: dict = None\n    multi_turn: bool = False\n\n\n@dataclasses.dataclass\nclass MatchPair:\n    question: dict\n    model_1: str\n    model_2: str\n    answer_1: dict\n    answer_2: dict\n    judge: Judge\n    ref_answer: dict = None\n    multi_turn: bool = False\n\n\ndef load_questions(question_file: str, begin: Optional[int], end: Optional[int]):\n    \"\"\"Load questions from a file.\"\"\"\n    questions = []\n    with open(question_file, \"r\") as ques_file:\n        for line in ques_file:\n            if line:\n                questions.append(json.loads(line))\n    questions = questions[begin:end]\n    return questions\n\n\ndef load_model_answers(answer_dir: str):\n    \"\"\"Load model answers.\n\n    The return value is a python dict of type:\n    Dict[model_name: str -> Dict[question_id: int -> answer: dict]]\n    \"\"\"\n    filenames = glob.glob(os.path.join(answer_dir, \"*.jsonl\"))\n    filenames.sort()\n    model_answers = {}\n\n    for filename in filenames:\n        model_name = os.path.basename(filename)[:-6]\n        answer = {}\n        with open(filename) as fin:\n            for line in fin:\n                line = json.loads(line)\n                answer[line[\"question_id\"]] = line\n        model_answers[model_name] = answer\n\n    return model_answers\n\n\ndef load_judge_prompts(prompt_file: str):\n    \"\"\"Load judge prompts.\n\n    The return value is a python dict of type:\n    Dict[judge_name: str -> dict]\n    \"\"\"\n    prompts = {}\n    with open(prompt_file) as fin:\n        for line in fin:\n            line = json.loads(line)\n            prompts[line[\"name\"]] = line\n    return prompts\n\n\ndef run_judge_single(question, answer, judge, ref_answer, multi_turn=False):\n    kwargs = {}\n    model = judge.model_name\n    if ref_answer is not None:\n        kwargs[\"ref_answer_1\"] = ref_answer[\"choices\"][0][\"turns\"][0]\n        if multi_turn:\n            kwargs[\"ref_answer_2\"] = ref_answer[\"choices\"][0][\"turns\"][1]\n\n    if multi_turn:\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question_1=question[\"turns\"][0],\n            question_2=question[\"turns\"][1],\n            answer_1=answer[\"choices\"][0][\"turns\"][0],\n            answer_2=answer[\"choices\"][0][\"turns\"][1],\n            **kwargs,\n        )\n    else:\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question=question[\"turns\"][0],\n            answer=answer[\"choices\"][0][\"turns\"][0],\n            **kwargs,\n        )\n\n    rating = -1\n\n    system_prompt = judge.prompt_template[\"system_prompt\"]\n    conv = get_conversation_template(model)\n    conv.set_system_message(system_prompt)\n    conv.append_message(conv.roles[0], user_prompt)\n    conv.append_message(conv.roles[1], None)\n\n    if model in OPENAI_MODEL_LIST:\n        judgment = chat_completion_openai(model, conv, temperature=0, max_tokens=2048)\n    elif model in ANTHROPIC_MODEL_LIST:\n        judgment = chat_completion_anthropic(\n            model, conv, temperature=0, max_tokens=1024\n        )\n    else:\n        raise ValueError(f\"Invalid judge model name: {model}\")\n\n    if judge.prompt_template[\"output_format\"] == \"[[rating]]\":\n        match = re.search(one_score_pattern, judgment)\n        if not match:\n            match = re.search(one_score_pattern_backup, judgment)\n\n        if match:\n            rating = ast.literal_eval(match.groups()[0])\n        else:\n            rating = -1\n    else:\n        raise ValueError(\n            f\"invalid output format: {judge.prompt_template['output_format']}\"\n        )\n\n    return rating, user_prompt, judgment\n\n\ndef play_a_match_single(match: MatchSingle, output_file: str):\n    question, model, answer, judge, ref_answer, multi_turn = (\n        match.question,\n        match.model,\n        match.answer,\n        match.judge,\n        match.ref_answer,\n        match.multi_turn,\n    )\n\n    if judge.prompt_template[\"type\"] == \"single\":\n        score, user_prompt, judgment = run_judge_single(\n            question, answer, judge, ref_answer, multi_turn=multi_turn\n        )\n\n        question_id = question[\"question_id\"]\n        turn = 1 if not multi_turn else 2\n        result = {\n            \"question_id\": question_id,\n            \"model\": model,\n            \"judge\": (judge.model_name, judge.prompt_template[\"name\"]),\n            \"user_prompt\": user_prompt,\n            \"judgment\": judgment,\n            \"score\": score,\n            \"turn\": turn,\n            \"tstamp\": time.time(),\n        }\n        print(\n            f\"question: {question_id}, turn: {turn}, model: {model}, \"\n            f\"score: {score}, \"\n            f\"judge: {(judge.model_name, judge.prompt_template['name'])}\"\n        )\n    else:\n        raise ValueError(f\"invalid judge type: {judge['type']}\")\n\n    if output_file:\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        with open(output_file, \"a\") as fout:\n            fout.write(json.dumps(result) + \"\\n\")\n\n    return result\n\n\ndef run_judge_pair(question, answer_a, answer_b, judge, ref_answer, multi_turn=False):\n    kwargs = {}\n    model = judge.model_name\n    if ref_answer is not None:\n        kwargs[\"ref_answer_1\"] = ref_answer[\"choices\"][0][\"turns\"][0]\n        if multi_turn:\n            kwargs[\"ref_answer_2\"] = ref_answer[\"choices\"][0][\"turns\"][1]\n\n    if multi_turn:\n        system_prompt = judge.prompt_template[\"system_prompt\"]\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question_1=question[\"turns\"][0],\n            question_2=question[\"turns\"][1],\n            answer_a_1=answer_a[\"choices\"][0][\"turns\"][0],\n            answer_b_1=answer_b[\"choices\"][0][\"turns\"][0],\n            answer_a_2=answer_a[\"choices\"][0][\"turns\"][1],\n            answer_b_2=answer_b[\"choices\"][0][\"turns\"][1],\n            **kwargs,\n        )\n    else:\n        system_prompt = judge.prompt_template[\"system_prompt\"]\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question=question[\"turns\"][0],\n            answer_a=answer_a[\"choices\"][0][\"turns\"][0],\n            answer_b=answer_b[\"choices\"][0][\"turns\"][0],\n            **kwargs,\n        )\n\n    winner = \"error\"\n\n    conv = get_conversation_template(model)\n    conv.append_message(conv.roles[0], user_prompt)\n    conv.append_message(conv.roles[1], None)\n\n    if model in OPENAI_MODEL_LIST:\n        conv.set_system_message(system_prompt)\n        judgment = chat_completion_openai(model, conv, temperature=0, max_tokens=2048)\n    elif model in ANTHROPIC_MODEL_LIST:\n        if system_prompt != \"You are a helpful assistant.\":\n            user_prompt = \"[Instruction]\\n\" + system_prompt + \"\\n\\n\" + user_prompt\n            conv.messages[0][1] = user_prompt\n        judgment = chat_completion_anthropic(\n            model, conv, temperature=0, max_tokens=1024\n        )\n    else:\n        raise ValueError(f\"Invalid judge model name: {model}\")\n\n    if judge.prompt_template[\"output_format\"] == \"[[A]]\":\n        if \"[[A]]\" in judgment:\n            winner = \"A\"\n        elif \"[[B]]\" in judgment:\n            winner = \"B\"\n        elif \"[[C]]\" in judgment:\n            winner = \"tie\"\n        else:\n            winner = \"error\"\n    elif judge.prompt_template[\"output_format\"] == \"[[rating_a,rating_b]]\":\n        match = re.search(two_score_pattern, judgment)\n        if not match:\n            match = re.search(two_score_pattern_backup, judgment)\n        if match:\n            scores = [ast.literal_eval(s.strip()) for s in match.groups()]\n            if abs(scores[0] - scores[1]) <= TIE_DELTA:\n                winner = \"tie\"\n            elif scores[0] > scores[1]:\n                winner = \"A\"\n            else:\n                winner = \"B\"\n        else:\n            winner = \"error\"\n    else:\n        raise ValueError(\n            f\"invalid output format: {judge.prompt_template['output_format']}\"\n        )\n\n    return winner, user_prompt, judgment\n\n\ndef play_a_match_pair(match: MatchPair, output_file: str):\n    question, model_1, model_2, answer_1, answer_2, judge, ref_answer, multi_turn = (\n        match.question,\n        match.model_1,\n        match.model_2,\n        match.answer_1,\n        match.answer_2,\n        match.judge,\n        match.ref_answer,\n        match.multi_turn,\n    )\n\n    if judge.prompt_template[\"type\"] == \"pairwise\":\n        g1_winner, g1_user_prompt, g1_judgment = run_judge_pair(\n            question, answer_1, answer_2, judge, ref_answer, multi_turn=multi_turn\n        )\n        g2_winner, g2_user_prompt, g2_judgment = run_judge_pair(\n            question, answer_2, answer_1, judge, ref_answer, multi_turn=multi_turn\n        )\n\n        g1_map = {\"A\": \"model_1\", \"B\": \"model_2\"}\n        g2_map = {\"A\": \"model_2\", \"B\": \"model_1\"}\n        g1_winner = g1_map.get(g1_winner, g1_winner)\n        g2_winner = g2_map.get(g2_winner, g2_winner)\n        question_id = question[\"question_id\"]\n        turn = 1 if not multi_turn else 2\n\n        result = {\n            \"question_id\": question_id,\n            \"model_1\": model_1,\n            \"model_2\": model_2,\n            \"g1_winner\": g1_winner,\n            \"g2_winner\": g2_winner,\n            \"judge\": (judge.model_name, judge.prompt_template[\"name\"]),\n            \"g1_user_prompt\": g1_user_prompt,\n            \"g1_judgment\": g1_judgment,\n            \"g2_user_prompt\": g2_user_prompt,\n            \"g2_judgment\": g2_judgment,\n            \"turn\": turn,\n            \"tstamp\": time.time(),\n        }\n\n        print(\n            f\"question: {question_id}, turn: {turn}, model_1: {model_1}, model_2: {model_2}, \"\n            f\"g1_winner: {g1_winner}, g2_winner: {g2_winner}, \"\n            f\"judge: {(judge.model_name, judge.prompt_template['name'])}\"\n        )\n    elif judge.prompt_template[\"type\"] == \"single\":\n        m1_score, m1_user_prompt, m1_judgment = run_judge_single(\n            question, answer_1, judge\n        )\n        m2_score, m2_user_prompt, m2_judgment = run_judge_single(\n            question, answer_2, judge\n        )\n\n        if abs(m1_score - m2_score) <= TIE_DELTA:\n            winner = \"tie\"\n        elif m1_score > m2_score:\n            winner = \"model_1\"\n        else:\n            winner = \"model_2\"\n\n        question_id = question[\"question_id\"]\n        result = {\n            \"question_id\": question_id,\n            \"model_1\": model_1,\n            \"model_2\": model_2,\n            \"g1_winner\": winner,\n            \"g2_winner\": winner,\n            \"judge\": (judge.model_name, judge.prompt_template[\"name\"]),\n            \"g1_user_prompt\": m1_user_prompt,\n            \"g1_judgment\": m1_judgment,\n            \"g2_user_prompt\": m2_user_prompt,\n            \"g2_judgment\": m2_judgment,\n            \"m1_score\": m1_score,\n            \"m2_score\": m2_score,\n            \"tstamp\": time.time(),\n        }\n        print(\n            f\"question: {question_id}, model_1: {model_1}, model_2: {model_2}, \"\n            f\"winner: {winner}, m1_score: {m1_score}, m2_score: {m2_score}, \"\n            f\"judge: {(judge.model_name, judge.prompt_template['name'])}\"\n        )\n    else:\n        raise ValueError(f\"invalid judge type: {judge['type']}\")\n\n    if output_file:\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        with open(output_file, \"a\") as fout:\n            fout.write(json.dumps(result) + \"\\n\")\n\n    return result\n\n\ndef chat_completion_openai(model, conv, temperature, max_tokens, api_dict=None):\n    if api_dict is not None:\n        openai.api_base = api_dict[\"api_base\"]\n        openai.api_key = api_dict[\"api_key\"]\n    output = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            messages = conv.to_openai_api_messages()\n            response = openai.ChatCompletion.create(\n                model=model,\n                messages=messages,\n                n=1,\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n            output = response[\"choices\"][0][\"message\"][\"content\"]\n            break\n        except openai.error.OpenAIError as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n\n    return output\n\n\ndef chat_completion_openai_azure(model, conv, temperature, max_tokens, api_dict=None):\n    openai.api_type = \"azure\"\n    openai.api_version = \"2023-07-01-preview\"\n    if api_dict is not None:\n        openai.api_base = api_dict[\"api_base\"]\n        openai.api_key = api_dict[\"api_key\"]\n    else:\n        openai.api_base = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n        openai.api_key = os.environ[\"AZURE_OPENAI_KEY\"]\n\n    if \"azure-\" in model:\n        model = model[6:]\n\n    output = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            messages = conv.to_openai_api_messages()\n            response = openai.ChatCompletion.create(\n                engine=model,\n                messages=messages,\n                n=1,\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n            output = response[\"choices\"][0][\"message\"][\"content\"]\n            break\n        except openai.error.OpenAIError as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n        except openai.error.InvalidRequestError as e:\n            print(type(e), e)\n            break\n        except KeyError:\n            print(response)\n            break\n\n    return output\n\n\ndef chat_completion_anthropic(model, conv, temperature, max_tokens, api_dict=None):\n    if api_dict is not None and \"api_key\" in api_dict:\n        api_key = api_dict[\"api_key\"]\n    else:\n        api_key = os.environ[\"ANTHROPIC_API_KEY\"]\n\n    output = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            c = anthropic.Anthropic(api_key=api_key)\n            prompt = conv.get_prompt()\n            response = c.completions.create(\n                model=model,\n                prompt=prompt,\n                stop_sequences=[anthropic.HUMAN_PROMPT],\n                max_tokens_to_sample=max_tokens,\n                temperature=temperature,\n            )\n            output = response.completion\n            break\n        except anthropic.APIError as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n    return output.strip()\n\n\ndef chat_completion_palm(chat_state, model, conv, temperature, max_tokens):\n    from fastchat.serve.api_provider import init_palm_chat\n\n    assert model == \"palm-2-chat-bison-001\"\n\n    if chat_state is None:\n        chat_state = init_palm_chat(\"chat-bison@001\")\n\n    parameters = {\n        \"temperature\": temperature,\n        \"top_p\": 0.8,\n        \"top_k\": 40,\n        \"max_output_tokens\": max_tokens,\n    }\n    output = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            response = chat_state.send_message(conv.messages[-2][1], **parameters)\n            output = response.text\n            break\n        except Exception as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n    return chat_state, output\n\n\ndef normalize_game_key_single(gamekey, result):\n    \"\"\"Make the model names sorted in a game key.\"\"\"\n    qid, model_1, model_2 = gamekey\n    if model_1 < model_2:\n        return gamekey, result\n    else:\n        new_gamekey = (qid, model_2, model_1)\n        new_result = {\n            \"winners\": tuple(reverse_model_map.get(x, x) for x in result[\"winners\"]),\n            \"g1_judgment\": result[\"g2_judgment\"],\n            \"g2_judgment\": result[\"g1_judgment\"],\n        }\n        return new_gamekey, new_result\n\n\ndef normalize_game_key_dict(judgment_dict):\n    \"\"\"Make the model names sorted in the game keys.\"\"\"\n    ret = {}\n    for key, value in judgment_dict.items():\n        new_key, new_value = normalize_game_key_single(key, value)\n        ret[new_key] = new_value\n    return ret\n\n\ndef load_pairwise_model_judgments(filename: str):\n    \"\"\"Load model judgments.\n\n    The return value is a dict of type:\n    Dict[judge: Tuple -> Dict[game_key: tuple -> game_result: dict]\n    \"\"\"\n    judge_dict = {}\n\n    for line in open(filename):\n        obj = json.loads(line)\n        judge = tuple(obj[\"judge\"])\n        qid, model_1, model_2 = obj[\"question_id\"], obj[\"model_1\"], obj[\"model_2\"]\n\n        if judge not in judge_dict:\n            judge_dict[judge] = {}\n\n        if \"winner\" in obj:\n            winner = obj[\"winner\"]\n        elif \"g1_winner\" in obj and \"g2_winner\" in obj:\n            g1_winner, g2_winner = obj[\"g1_winner\"], obj[\"g2_winner\"]\n            if g1_winner == g2_winner:\n                winner = g1_winner\n            else:\n                winner = \"inconsistent\"\n        else:\n            raise ValueError(f\"Invalid keys: {list(obj.keys())}\")\n\n        gamekey = (qid, model_1, model_2)\n        winners = (winner,)\n\n        judge_dict[judge][gamekey] = {\n            \"winners\": winners,\n            \"g1_judgment\": obj[\"g1_judgment\"],\n            \"g2_judgment\": obj[\"g2_judgment\"],\n        }\n\n    # Make the model names sorted in the game keys\n    normalized = {}\n    for judge, value in judge_dict.items():\n        normalized[judge] = normalize_game_key_dict(value)\n    return normalized\n\n\ndef load_single_model_judgments(filename: str):\n    \"\"\"Load model judgments.\n\n    The return value is a dict of type:\n    Dict[judge: Tuple -> Dict[game_key: tuple -> game_result: dict]\n    \"\"\"\n    judge_dict = {}\n\n    for line in open(filename):\n        obj = json.loads(line)\n        judge = tuple(obj[\"judge\"])\n        qid, model = obj[\"question_id\"], obj[\"model\"]\n\n        if judge not in judge_dict:\n            judge_dict[judge] = {}\n\n        gamekey = (qid, model)\n\n        judge_dict[judge][gamekey] = {\n            \"score\": obj[\"score\"],\n            \"judgment\": obj[\"judgment\"],\n        }\n    return judge_dict\n\n\ndef resolve_pairwise_judgment_dict(\n    question, model_judgments_normal, model_judgments_math, multi_turn=False\n):\n    \"\"\"Return the correct pairwise judge.\"\"\"\n    if multi_turn:\n        if question[\"category\"] in NEED_REF_CATS:\n            return model_judgments_math[(\"gpt-4\", \"pair-math-v1-multi-turn\")]\n        return model_judgments_normal[(\"gpt-4\", \"pair-v2-multi-turn\")]\n\n    if question[\"category\"] in NEED_REF_CATS:\n        return model_judgments_math[(\"gpt-4\", \"pair-math-v1\")]\n    else:\n        return model_judgments_normal[(\"gpt-4\", \"pair-v2\")]\n\n\ndef resolve_single_judgment_dict(\n    question, model_judgments_normal, model_judgments_math, multi_turn=False\n):\n    \"\"\"Return the correct single answer grading judge.\"\"\"\n    if multi_turn:\n        if question[\"category\"] in NEED_REF_CATS:\n            return model_judgments_math[(\"gpt-4\", \"single-math-v1-multi-turn\")]\n        return model_judgments_normal[(\"gpt-4\", \"single-v1-multi-turn\")]\n\n    if question[\"category\"] in NEED_REF_CATS:\n        return model_judgments_math[(\"gpt-4\", \"single-math-v1\")]\n    else:\n        return model_judgments_normal[(\"gpt-4\", \"single-v1\")]\n\n\ndef get_pairwise_judge_explanation(gamekey, judgment_dict):\n    \"\"\"Get model judge explanation.\"\"\"\n    try:\n        qid, model_1, model_2 = gamekey\n        if model_1 < model_2:\n            res = judgment_dict[gamekey]\n            g1_judgment, g2_judgment = res[\"g1_judgment\"], res[\"g2_judgment\"]\n        else:\n            new_gamekey = (qid, model_2, model_1)\n            res = judgment_dict[new_gamekey]\n\n            model_1, model_2 = model_1, model_2\n            g1_judgment, g2_judgment = res[\"g2_judgment\"], res[\"g1_judgment\"]\n\n        return (\n            f\"**Game 1**. **A**: {model_1}, **B**: {model_2}\\n\\n\"\n            f\"**Judgment**: {g1_judgment}\"\n            + f\"\\n\\n`--------------------------`\\n\\n\"\n            + f\"**Game 2**. **A**: {model_2}, **B**: {model_1}\\n\\n\"\n            f\"**Judgment**: {g2_judgment}\"\n        )\n    except KeyError:\n        return \"N/A\"\n\n\ndef get_single_judge_explanation(gamekey, judgment_dict):\n    \"\"\"Get model judge explanation.\"\"\"\n    try:\n        qid, model = gamekey\n\n        res = judgment_dict[gamekey]\n\n        g1_judgment = res[\"judgment\"]\n        g1_score = res[\"score\"]\n\n        return (\n            f\"**Game 1**. **A**: {model}, **Score**: {g1_score}\\n\\n\"\n            f\"**Judgment**: {g1_judgment}\"\n        )\n    except KeyError:\n        return \"N/A\"\n\n\ndef check_data(questions, model_answers, ref_answers, models, judges):\n    # check model answers\n    for m in models:\n        assert m in model_answers, f\"Missing model answer for {m}\"\n        m_answer = model_answers[m]\n        for q in questions:\n            assert (\n                q[\"question_id\"] in m_answer\n            ), f\"Missing model {m}'s answer to Question {q['question_id']}\"\n    # check ref answers\n    for jg in judges.values():\n        if not jg.ref_based:\n            continue\n        for q in questions:\n            if q[\"category\"] not in NEED_REF_CATS:\n                continue\n            assert (\n                q[\"question_id\"] in ref_answers[jg.model_name]\n            ), f\"Missing reference answer to Question {q['question_id']} for judge {jg.model_name}\"\n\n\ndef get_model_list(answer_dir):\n    file_paths = glob.glob(f\"{answer_dir}/*.jsonl\")\n    file_names = [os.path.splitext(os.path.basename(f))[0] for f in file_paths]\n    return file_names\n", "fastchat/llm_judge/compute_agreement.py": "\"\"\"\nCompute agreement among judges.\n\nUsage:\npython compute_agreement.py --judges gpt4-pair human --votefiles human_judgments.json gpt4_pair_judgments.json\npython compute_agreement.py --judges human human --votefiles human_judgments.json\n\"\"\"\nimport argparse\nimport json\nimport os\n\nimport numpy as np\n\n\ndef get_judge_name(judge):\n    if isinstance(judge, list) and judge[0] == \"gpt-4\" and judge[1].startswith(\"pair\"):\n        return \"gpt4-pair\"\n    if judge.startswith(\"expert\"):\n        return \"human\"\n    if judge.startswith(\"author\"):\n        return \"author\"\n\n\ndef revert(vote):\n    if vote == \"model_a\":\n        return \"model_b\"\n    elif vote == \"model_b\":\n        return \"model_a\"\n    return vote\n\n\ndef get_mt_bench_votes_data(raw_votes):\n    data = [{}, {}]\n\n    for judge_votes in raw_votes:\n        for vote in judge_votes:\n            turn = vote[\"turn\"] - 1\n            if vote[\"model_a\"] < vote[\"model_b\"]:\n                key = (vote[\"question_id\"], vote[\"model_a\"], vote[\"model_b\"])\n                winner = vote[\"winner\"]\n            else:\n                key = (vote[\"question_id\"], vote[\"model_b\"], vote[\"model_a\"])\n                winner = revert(vote[\"winner\"])\n            judge = get_judge_name(vote[\"judge\"])\n            if key not in data[turn]:\n                data[turn][key] = {}\n            if judge not in data[turn][key]:\n                data[turn][key][judge] = []\n            data[turn][key][judge].append(winner)\n\n    return data\n\n\ndef convertvote(vote):\n    if \"tie\" in vote:\n        return \"tie\"\n    return vote\n\n\ndef equalvote(vote1, vote2):\n    if \"tie\" in vote1 and \"tie\" in vote2:\n        return True\n    return vote1 == vote2\n\n\n# data: Dict[qid -> List[vote]]\ndef get_mt_bench_agreement(data, judge1, judge2, ban):\n    if judge1.startswith(\"gpt4\") and judge2 == \"human\":\n        stats = [0, 0]\n        for votes in data.values():\n            if judge1 not in votes or judge2 not in votes:\n                continue\n            assert len(votes[judge1]) == 1\n            if convertvote(votes[judge1][0]) in ban:\n                continue\n            for v in votes[judge2]:\n                if convertvote(v) in ban:\n                    continue\n                stats[1] += 1\n                stats[0] += equalvote(votes[judge1][0], v)\n        return stats[0], stats[1]\n    elif judge1 == \"human\" and judge2 == \"human\":\n        stats = [0, 0]\n        for votes in data.values():\n            if \"human\" not in votes:\n                continue\n            for i in range(len(votes[\"human\"]) - 1):\n                for j in range(i + 1, len(votes[\"human\"])):\n                    if (\n                        convertvote(votes[\"human\"][i]) in ban\n                        or convertvote(votes[\"human\"][j]) in ban\n                    ):\n                        continue\n                    stats[1] += 1\n                    stats[0] += equalvote(votes[\"human\"][i], votes[\"human\"][j])\n        return stats[0], stats[1]\n    else:\n        raise Exception(\"Unsupported judges.\")\n\n\ndef run_mt_bench_agreement(judges, votefiles):\n    # votes[i]: List of votes\n    votes = []\n    for filename in votefiles:\n        with open(filename, \"r\") as f:\n            data = json.load(f)\n        votes.append(data)\n\n    data = get_mt_bench_votes_data(votes)\n\n    agree, total = get_mt_bench_agreement(data[0], judges[0], judges[1], ban=[])\n    print(\n        f\"turn 1 with tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\"\n    )\n    agree, total = get_mt_bench_agreement(data[0], judges[0], judges[1], ban=[\"tie\"])\n    print(\n        f\"turn 1 without tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\"\n    )\n    agree, total = get_mt_bench_agreement(data[1], judges[0], judges[1], ban=[])\n    print(\n        f\"turn 2 with tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\"\n    )\n    agree, total = get_mt_bench_agreement(data[1], judges[0], judges[1], ban=[\"tie\"])\n    print(\n        f\"turn 2 without tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\"\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--judges\", nargs=2, type=str, default=[\"gpt4-pair\", \"human\"])\n    parser.add_argument(\n        \"--votefiles\",\n        nargs=\"+\",\n        type=str,\n        default=[\"gpt4_judgments.json\", \"human_judgments.json\"],\n    )\n    args = parser.parse_args()\n\n    run_mt_bench_agreement(args.judges, args.votefiles)\n", "fastchat/llm_judge/clean_judgment.py": "\"\"\"\nClean model judgment files.\n\"\"\"\nimport argparse\nimport json\n\nselected_models = [\n    \"alpaca-13b\",\n    \"baize-v2-13b\",\n    \"chatglm-6b\",\n    \"claude-instant-v1\",\n    \"claude-v1\",\n    \"dolly-v2-12b\",\n    \"falcon-40b-instruct\",\n    \"fastchat-t5-3b\",\n    \"gpt-3.5-turbo\",\n    \"gpt-4\",\n    \"gpt4all-13b-snoozy\",\n    \"guanaco-33b\",\n    \"guanaco-65b\",\n    \"h2ogpt-oasst-open-llama-13b\",\n    \"koala-13b\",\n    \"llama-13b\",\n    \"mpt-30b-chat\",\n    \"mpt-30b-instruct\",\n    \"mpt-7b-chat\",\n    \"nous-hermes-13b\",\n    \"oasst-sft-4-pythia-12b\",\n    \"oasst-sft-7-llama-30b\",\n    \"palm-2-chat-bison-001\",\n    \"rwkv-4-raven-14b\",\n    \"stablelm-tuned-alpha-7b\",\n    \"tulu-30b\",\n    \"vicuna-13b-v1.3\",\n    \"vicuna-33b-v1.3\",\n    \"vicuna-7b-v1.3\",\n    \"wizardlm-13b\",\n    \"wizardlm-30b\",\n]\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--infile\", type=str)\n    args = parser.parse_args()\n\n    infile = args.infile\n    outfile = infile.replace(\".jsonl\", \"_clean.jsonl\")\n\n    raw_lines = open(infile).readlines()\n    rets = []\n    models = set()\n    visited = set()\n    for line in raw_lines:\n        obj = json.loads(line)\n\n        if \"model_1\" in obj:  # pair\n            model = obj[\"model_1\"]\n            key = (\n                obj[\"model_1\"],\n                obj[\"model_2\"],\n                obj[\"question_id\"],\n                tuple(obj[\"judge\"]),\n            )\n        else:  # single\n            model = obj[\"model\"]\n            key = (obj[\"model\"], obj[\"question_id\"], tuple(obj[\"judge\"]))\n\n        if key in visited:\n            continue\n        visited.add(key)\n\n        if model not in selected_models:\n            continue\n        models.add(model)\n        rets.append(obj)\n\n    models = sorted(list(models))\n    missing_models = [x for x in selected_models if x not in models]\n    print(f\"in models: {models}, number: {len(models)}\")\n    print(f\"missing models: {missing_models}\")\n    print(f\"#in: {len(raw_lines)}, #out: {len(rets)}\")\n    rets.sort(\n        key=lambda x: (\n            x[\"model\"] if \"model\" in x else x[\"model_1\"],\n            x[\"question_id\"],\n            x[\"turn\"],\n        )\n    )\n\n    with open(outfile, \"w\") as fout:\n        for x in rets:\n            fout.write(json.dumps(x) + \"\\n\")\n", "fastchat/llm_judge/qa_browser.py": "\"\"\"\nUsage:\npython3 qa_browser.py --share\n\"\"\"\n\nimport argparse\nfrom collections import defaultdict\nimport re\n\nimport gradio as gr\n\nfrom fastchat.llm_judge.common import (\n    load_questions,\n    load_model_answers,\n    load_single_model_judgments,\n    load_pairwise_model_judgments,\n    resolve_single_judgment_dict,\n    resolve_pairwise_judgment_dict,\n    get_single_judge_explanation,\n    get_pairwise_judge_explanation,\n)\n\n\nquestions = []\nmodel_answers = {}\n\nmodel_judgments_normal_single = {}\nmodel_judgments_math_single = {}\n\nmodel_judgments_normal_pairwise = {}\nmodel_judgments_math_pairwise = {}\n\nquestion_selector_map = {}\ncategory_selector_map = defaultdict(list)\n\n\ndef display_question(category_selector, request: gr.Request):\n    choices = category_selector_map[category_selector]\n    return gr.Dropdown(\n        value=choices[0],\n        choices=choices,\n    )\n\n\ndef display_pairwise_answer(\n    question_selector, model_selector1, model_selector2, request: gr.Request\n):\n    q = question_selector_map[question_selector]\n    qid = q[\"question_id\"]\n\n    ans1 = model_answers[model_selector1][qid]\n    ans2 = model_answers[model_selector2][qid]\n\n    chat_mds = pairwise_to_gradio_chat_mds(q, ans1, ans2)\n    gamekey = (qid, model_selector1, model_selector2)\n\n    judgment_dict = resolve_pairwise_judgment_dict(\n        q,\n        model_judgments_normal_pairwise,\n        model_judgments_math_pairwise,\n        multi_turn=False,\n    )\n\n    explanation = (\n        \"##### Model Judgment (first turn)\\n\"\n        + get_pairwise_judge_explanation(gamekey, judgment_dict)\n    )\n\n    judgment_dict_turn2 = resolve_pairwise_judgment_dict(\n        q,\n        model_judgments_normal_pairwise,\n        model_judgments_math_pairwise,\n        multi_turn=True,\n    )\n\n    explanation_turn2 = (\n        \"##### Model Judgment (second turn)\\n\"\n        + get_pairwise_judge_explanation(gamekey, judgment_dict_turn2)\n    )\n\n    return chat_mds + [explanation] + [explanation_turn2]\n\n\ndef display_single_answer(question_selector, model_selector1, request: gr.Request):\n    q = question_selector_map[question_selector]\n    qid = q[\"question_id\"]\n\n    ans1 = model_answers[model_selector1][qid]\n\n    chat_mds = single_to_gradio_chat_mds(q, ans1)\n    gamekey = (qid, model_selector1)\n\n    judgment_dict = resolve_single_judgment_dict(\n        q, model_judgments_normal_single, model_judgments_math_single, multi_turn=False\n    )\n\n    explanation = \"##### Model Judgment (first turn)\\n\" + get_single_judge_explanation(\n        gamekey, judgment_dict\n    )\n\n    judgment_dict_turn2 = resolve_single_judgment_dict(\n        q, model_judgments_normal_single, model_judgments_math_single, multi_turn=True\n    )\n\n    explanation_turn2 = (\n        \"##### Model Judgment (second turn)\\n\"\n        + get_single_judge_explanation(gamekey, judgment_dict_turn2)\n    )\n\n    return chat_mds + [explanation] + [explanation_turn2]\n\n\nnewline_pattern1 = re.compile(\"\\n\\n(\\d+\\. )\")\nnewline_pattern2 = re.compile(\"\\n\\n(- )\")\n\n\ndef post_process_answer(x):\n    \"\"\"Fix Markdown rendering problems.\"\"\"\n    x = x.replace(\"\\u2022\", \"- \")\n    x = re.sub(newline_pattern1, \"\\n\\g<1>\", x)\n    x = re.sub(newline_pattern2, \"\\n\\g<1>\", x)\n    return x\n\n\ndef pairwise_to_gradio_chat_mds(question, ans_a, ans_b, turn=None):\n    end = len(question[\"turns\"]) if turn is None else turn + 1\n\n    mds = [\"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n    for i in range(end):\n        base = i * 3\n        if i == 0:\n            mds[base + 0] = \"##### User\\n\" + question[\"turns\"][i]\n        else:\n            mds[base + 0] = \"##### User's follow-up question \\n\" + question[\"turns\"][i]\n        mds[base + 1] = \"##### Assistant A\\n\" + post_process_answer(\n            ans_a[\"choices\"][0][\"turns\"][i].strip()\n        )\n        mds[base + 2] = \"##### Assistant B\\n\" + post_process_answer(\n            ans_b[\"choices\"][0][\"turns\"][i].strip()\n        )\n\n    ref = question.get(\"reference\", [\"\", \"\"])\n\n    ref_md = \"\"\n    if turn is None:\n        if ref[0] != \"\" or ref[1] != \"\":\n            mds[6] = f\"##### Reference Solution\\nQ1. {ref[0]}\\nQ2. {ref[1]}\"\n    else:\n        x = ref[turn] if turn < len(ref) else \"\"\n        if x:\n            mds[6] = f\"##### Reference Solution\\n{ref[turn]}\"\n        else:\n            mds[6] = \"\"\n    return mds\n\n\ndef single_to_gradio_chat_mds(question, ans, turn=None):\n    end = len(question[\"turns\"]) if turn is None else turn + 1\n\n    mds = [\"\", \"\", \"\", \"\", \"\"]\n    for i in range(end):\n        base = i * 2\n        if i == 0:\n            mds[base + 0] = \"##### User\\n\" + question[\"turns\"][i]\n        else:\n            mds[base + 0] = \"##### User's follow-up question \\n\" + question[\"turns\"][i]\n        mds[base + 1] = \"##### Assistant A\\n\" + post_process_answer(\n            ans[\"choices\"][0][\"turns\"][i].strip()\n        )\n\n    ref = question.get(\"reference\", [\"\", \"\"])\n\n    ref_md = \"\"\n    if turn is None:\n        if ref[0] != \"\" or ref[1] != \"\":\n            mds[4] = f\"##### Reference Solution\\nQ1. {ref[0]}\\nQ2. {ref[1]}\"\n    else:\n        x = ref[turn] if turn < len(ref) else \"\"\n        if x:\n            mds[4] = f\"##### Reference Solution\\n{ref[turn]}\"\n        else:\n            mds[4] = \"\"\n    return mds\n\n\ndef build_question_selector_map():\n    global question_selector_map, category_selector_map\n\n    # Build question selector map\n    for q in questions:\n        preview = f\"{q['question_id']}: \" + q[\"turns\"][0][:128] + \"...\"\n        question_selector_map[preview] = q\n        category_selector_map[q[\"category\"]].append(preview)\n\n\ndef build_pairwise_browser_tab():\n    global question_selector_map, category_selector_map\n\n    models = list(model_answers.keys())\n    num_sides = 2\n    num_turns = 2\n    side_names = [\"A\", \"B\"]\n\n    question_selector_choices = list(question_selector_map.keys())\n    category_selector_choices = list(category_selector_map.keys())\n\n    # Selectors\n    with gr.Row():\n        with gr.Column(scale=1, min_width=200):\n            category_selector = gr.Dropdown(\n                choices=category_selector_choices, label=\"Category\", container=False\n            )\n        with gr.Column(scale=100):\n            question_selector = gr.Dropdown(\n                choices=question_selector_choices, label=\"Question\", container=False\n            )\n\n    model_selectors = [None] * num_sides\n    with gr.Row():\n        for i in range(num_sides):\n            with gr.Column():\n                if i == 0:\n                    value = models[0]\n                else:\n                    value = \"gpt-3.5-turbo\"\n                model_selectors[i] = gr.Dropdown(\n                    choices=models,\n                    value=value,\n                    label=f\"Model {side_names[i]}\",\n                    container=False,\n                )\n\n    # Conversation\n    chat_mds = []\n    for i in range(num_turns):\n        chat_mds.append(gr.Markdown(elem_id=f\"user_question_{i+1}\"))\n        with gr.Row():\n            for j in range(num_sides):\n                with gr.Column(scale=100):\n                    chat_mds.append(gr.Markdown())\n\n                if j == 0:\n                    with gr.Column(scale=1, min_width=8):\n                        gr.Markdown()\n    reference = gr.Markdown(elem_id=f\"reference\")\n    chat_mds.append(reference)\n\n    model_explanation = gr.Markdown(elem_id=\"model_explanation\")\n    model_explanation2 = gr.Markdown(elem_id=\"model_explanation\")\n\n    # Callbacks\n    category_selector.change(display_question, [category_selector], [question_selector])\n    question_selector.change(\n        display_pairwise_answer,\n        [question_selector] + model_selectors,\n        chat_mds + [model_explanation] + [model_explanation2],\n    )\n\n    for i in range(num_sides):\n        model_selectors[i].change(\n            display_pairwise_answer,\n            [question_selector] + model_selectors,\n            chat_mds + [model_explanation] + [model_explanation2],\n        )\n\n    return (category_selector,)\n\n\ndef build_single_answer_browser_tab():\n    global question_selector_map, category_selector_map\n\n    models = list(model_answers.keys())\n    num_sides = 1\n    num_turns = 2\n    side_names = [\"A\"]\n\n    question_selector_choices = list(question_selector_map.keys())\n    category_selector_choices = list(category_selector_map.keys())\n\n    # Selectors\n    with gr.Row():\n        with gr.Column(scale=1, min_width=200):\n            category_selector = gr.Dropdown(\n                choices=category_selector_choices, label=\"Category\", container=False\n            )\n        with gr.Column(scale=100):\n            question_selector = gr.Dropdown(\n                choices=question_selector_choices, label=\"Question\", container=False\n            )\n\n    model_selectors = [None] * num_sides\n    with gr.Row():\n        for i in range(num_sides):\n            with gr.Column():\n                model_selectors[i] = gr.Dropdown(\n                    choices=models,\n                    value=models[i] if len(models) > i else \"\",\n                    label=f\"Model {side_names[i]}\",\n                    container=False,\n                )\n\n    # Conversation\n    chat_mds = []\n    for i in range(num_turns):\n        chat_mds.append(gr.Markdown(elem_id=f\"user_question_{i+1}\"))\n        with gr.Row():\n            for j in range(num_sides):\n                with gr.Column(scale=100):\n                    chat_mds.append(gr.Markdown())\n\n                if j == 0:\n                    with gr.Column(scale=1, min_width=8):\n                        gr.Markdown()\n\n    reference = gr.Markdown(elem_id=f\"reference\")\n    chat_mds.append(reference)\n\n    model_explanation = gr.Markdown(elem_id=\"model_explanation\")\n    model_explanation2 = gr.Markdown(elem_id=\"model_explanation\")\n\n    # Callbacks\n    category_selector.change(display_question, [category_selector], [question_selector])\n    question_selector.change(\n        display_single_answer,\n        [question_selector] + model_selectors,\n        chat_mds + [model_explanation] + [model_explanation2],\n    )\n\n    for i in range(num_sides):\n        model_selectors[i].change(\n            display_single_answer,\n            [question_selector] + model_selectors,\n            chat_mds + [model_explanation] + [model_explanation2],\n        )\n\n    return (category_selector,)\n\n\nblock_css = \"\"\"\n#user_question_1 {\n    background-color: #DEEBF7;\n}\n#user_question_2 {\n    background-color: #E2F0D9;\n}\n#reference {\n    background-color: #FFF2CC;\n}\n#model_explanation {\n    background-color: #FBE5D6;\n}\n\"\"\"\n\n\ndef load_demo():\n    dropdown_update = gr.Dropdown.update(value=list(category_selector_map.keys())[0])\n    return dropdown_update, dropdown_update\n\n\ndef build_demo():\n    build_question_selector_map()\n\n    with gr.Blocks(\n        title=\"MT-Bench Browser\",\n        theme=gr.themes.Base(text_size=gr.themes.sizes.text_lg),\n        css=block_css,\n    ) as demo:\n        gr.Markdown(\n            \"\"\"\n# MT-Bench Browser\nThe code to generate answers and judgments is at [fastchat.llm_judge](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge).\n\"\"\"\n        )\n        with gr.Tab(\"Single Answer Grading\"):\n            (category_selector,) = build_single_answer_browser_tab()\n        with gr.Tab(\"Pairwise Comparison\"):\n            (category_selector2,) = build_pairwise_browser_tab()\n        demo.load(load_demo, [], [category_selector, category_selector2])\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\"--share\", action=\"store_true\")\n    parser.add_argument(\"--bench-name\", type=str, default=\"mt_bench\")\n    args = parser.parse_args()\n    print(args)\n\n    question_file = f\"data/{args.bench_name}/question.jsonl\"\n    answer_dir = f\"data/{args.bench_name}/model_answer\"\n    pairwise_model_judgment_file = (\n        f\"data/{args.bench_name}/model_judgment/gpt-4_pair.jsonl\"\n    )\n    single_model_judgment_file = (\n        f\"data/{args.bench_name}/model_judgment/gpt-4_single.jsonl\"\n    )\n\n    # Load questions\n    questions = load_questions(question_file, None, None)\n\n    # Load answers\n    model_answers = load_model_answers(answer_dir)\n\n    # Load model judgments\n    model_judgments_normal_single = (\n        model_judgments_math_single\n    ) = load_single_model_judgments(single_model_judgment_file)\n    model_judgments_normal_pairwise = (\n        model_judgments_math_pairwise\n    ) = load_pairwise_model_judgments(pairwise_model_judgment_file)\n\n    demo = build_demo()\n    demo.queue(\n        default_concurrency_limit=10, status_update_rate=10, api_open=False\n    ).launch(\n        server_name=args.host, server_port=args.port, share=args.share, max_threads=200\n    )\n", "fastchat/llm_judge/gen_api_answer.py": "\"\"\"Generate answers with GPT-4\n\nUsage:\npython3 gen_api_answer.py --model gpt-3.5-turbo\n\"\"\"\nimport argparse\nimport json\nimport os\nimport time\nimport concurrent.futures\n\nimport openai\nimport shortuuid\nimport tqdm\n\nfrom fastchat.llm_judge.common import (\n    load_questions,\n    temperature_config,\n    chat_completion_openai,\n    chat_completion_anthropic,\n    chat_completion_palm,\n)\nfrom fastchat.llm_judge.gen_model_answer import reorg_answer_file\nfrom fastchat.model.model_adapter import get_conversation_template, ANTHROPIC_MODEL_LIST\n\n\ndef get_answer(\n    question: dict, model: str, num_choices: int, max_tokens: int, answer_file: str\n):\n    assert (\n        args.force_temperature is not None and \"required_temperature\" in question.keys()\n    ) == False\n    if args.force_temperature is not None:\n        temperature = args.force_temperature\n    elif \"required_temperature\" in question.keys():\n        temperature = question[\"required_temperature\"]\n    elif question[\"category\"] in temperature_config:\n        temperature = temperature_config[question[\"category\"]]\n    else:\n        temperature = 0.7\n\n    choices = []\n    chat_state = None  # for palm-2 model\n    for i in range(num_choices):\n        conv = get_conversation_template(model)\n\n        turns = []\n        for j in range(len(question[\"turns\"])):\n            conv.append_message(conv.roles[0], question[\"turns\"][j])\n            conv.append_message(conv.roles[1], None)\n\n            if model in ANTHROPIC_MODEL_LIST:\n                output = chat_completion_anthropic(model, conv, temperature, max_tokens)\n            elif model == \"palm-2-chat-bison-001\":\n                chat_state, output = chat_completion_palm(\n                    chat_state, model, conv, temperature, max_tokens\n                )\n            else:\n                output = chat_completion_openai(model, conv, temperature, max_tokens)\n\n            conv.update_last_message(output)\n            turns.append(output)\n\n        choices.append({\"index\": i, \"turns\": turns})\n\n    # Dump answers\n    ans = {\n        \"question_id\": question[\"question_id\"],\n        \"answer_id\": shortuuid.uuid(),\n        \"model_id\": model,\n        \"choices\": choices,\n        \"tstamp\": time.time(),\n    }\n\n    os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n    with open(answer_file, \"a\") as fout:\n        fout.write(json.dumps(ans) + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--bench-name\",\n        type=str,\n        default=\"mt_bench\",\n        help=\"The name of the benchmark question set.\",\n    )\n    parser.add_argument(\"--answer-file\", type=str, help=\"The output answer file.\")\n    parser.add_argument(\"--model\", type=str, default=\"gpt-3.5-turbo\")\n    parser.add_argument(\n        \"--num-choices\",\n        type=int,\n        default=1,\n        help=\"How many completion choices to generate.\",\n    )\n    parser.add_argument(\n        \"--force-temperature\", type=float, help=\"Forcibly set a sampling temperature.\"\n    )\n    parser.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=1024,\n        help=\"The maximum number of new generated tokens.\",\n    )\n    parser.add_argument(\n        \"--question-begin\",\n        type=int,\n        help=\"A debug option. The begin index of questions.\",\n    )\n    parser.add_argument(\n        \"--question-end\", type=int, help=\"A debug option. The end index of questions.\"\n    )\n    parser.add_argument(\n        \"--parallel\", type=int, default=1, help=\"The number of concurrent API calls.\"\n    )\n    parser.add_argument(\"--openai-api-base\", type=str, default=None)\n    args = parser.parse_args()\n\n    if args.openai_api_base is not None:\n        openai.api_base = args.openai_api_base\n\n    question_file = f\"data/{args.bench_name}/question.jsonl\"\n    questions = load_questions(question_file, args.question_begin, args.question_end)\n\n    if args.answer_file:\n        answer_file = args.answer_file\n    else:\n        answer_file = f\"data/{args.bench_name}/model_answer/{args.model}.jsonl\"\n    print(f\"Output to {answer_file}\")\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=args.parallel) as executor:\n        futures = []\n        for question in questions:\n            future = executor.submit(\n                get_answer,\n                question,\n                args.model,\n                args.num_choices,\n                args.max_tokens,\n                answer_file,\n            )\n            futures.append(future)\n\n        for future in tqdm.tqdm(\n            concurrent.futures.as_completed(futures), total=len(futures)\n        ):\n            future.result()\n\n    reorg_answer_file(answer_file)\n", "fastchat/llm_judge/show_result.py": "\"\"\"\nUsage:\npython3 show_result.py --mode [single|pairwise-baseline|pairwise-all]\n\"\"\"\nimport argparse\nimport pandas as pd\n\n\ndef display_result_single(args):\n    if args.input_file is None:\n        input_file = (\n            f\"data/{args.bench_name}/model_judgment/{args.judge_model}_single.jsonl\"\n        )\n    else:\n        input_file = args.input_file\n\n    print(f\"Input file: {input_file}\")\n    df_all = pd.read_json(input_file, lines=True)\n    df = df_all[[\"model\", \"score\", \"turn\"]]\n    df = df[df[\"score\"] != -1]\n\n    if args.model_list is not None:\n        df = df[df[\"model\"].isin(args.model_list)]\n\n    print(\"\\n########## First turn ##########\")\n    df_1 = df[df[\"turn\"] == 1].groupby([\"model\", \"turn\"]).mean()\n    print(df_1.sort_values(by=\"score\", ascending=False))\n\n    if args.bench_name == \"mt_bench\":\n        print(\"\\n########## Second turn ##########\")\n        df_2 = df[df[\"turn\"] == 2].groupby([\"model\", \"turn\"]).mean()\n        print(df_2.sort_values(by=\"score\", ascending=False))\n\n        print(\"\\n########## Average ##########\")\n        df_3 = df[[\"model\", \"score\"]].groupby([\"model\"]).mean()\n        print(df_3.sort_values(by=\"score\", ascending=False))\n\n\ndef display_result_pairwise(args):\n    if args.input_file is None:\n        input_file = (\n            f\"data/{args.bench_name}/model_judgment/{args.judge_model}_pair.jsonl\"\n        )\n    else:\n        input_file = args.input_file\n\n    print(f\"Input file: {input_file}\")\n    df_all = pd.read_json(input_file, lines=True)\n    df_all = df_all[(df_all[\"g1_winner\"] != \"error\") & (df_all[\"g2_winner\"] != \"error\")]\n\n    model_list = (\n        df_all[\"model_1\"].unique().tolist() + df_all[\"model_2\"].unique().tolist()\n    )\n    model_list = list(set(model_list))\n\n    list_res = []\n    # traverse df row by row\n    for index, row in df_all.iterrows():\n        if args.model_list is not None and row[\"model_1\"] not in args.model_list:\n            continue\n        if args.baseline_model is not None:\n            if args.baseline_model not in [row[\"model_1\"], row[\"model_2\"]]:\n                continue\n        if row[\"g1_winner\"] == \"tie\" or row[\"g1_winner\"] != row[\"g2_winner\"]:\n            list_res.append({\"model\": row[\"model_1\"], \"win\": 0, \"loss\": 0, \"tie\": 1})\n            list_res.append({\"model\": row[\"model_2\"], \"win\": 0, \"loss\": 0, \"tie\": 1})\n        else:\n            if row[\"g1_winner\"] == \"model_1\":\n                winner = row[\"model_1\"]\n                loser = row[\"model_2\"]\n            else:\n                winner = row[\"model_2\"]\n                loser = row[\"model_1\"]\n            list_res.append({\"model\": winner, \"win\": 1, \"loss\": 0, \"tie\": 0})\n            list_res.append({\"model\": loser, \"win\": 0, \"loss\": 1, \"tie\": 0})\n\n    df = pd.DataFrame(list_res)\n    df = df.groupby([\"model\"]).sum()\n\n    # remove baseline model\n    if args.baseline_model is not None:\n        df = df[df.index != args.baseline_model]\n    # add win rate\n    df[\"win_rate\"] = df[\"win\"] / (df[\"win\"] + df[\"loss\"] + df[\"tie\"])\n    df[\"loss_rate\"] = df[\"loss\"] / (df[\"win\"] + df[\"loss\"] + df[\"tie\"])\n    # each tie counts as 0.5 win + 0.5 loss\n    df[\"win_rate_adjusted\"] = (df[\"win\"] + 0.5 * df[\"tie\"]) / (\n        df[\"win\"] + df[\"loss\"] + df[\"tie\"]\n    )\n    # print(df.sort_values(by=\"win_rate\", ascending=False))\n    # print(df.sort_values(by=\"loss_rate\", ascending=True))\n    print(df.sort_values(by=\"win_rate_adjusted\", ascending=False))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--bench-name\", type=str, default=\"mt_bench\")\n    parser.add_argument(\"--input-file\", type=str)\n    parser.add_argument(\"--judge-model\", type=str, default=\"gpt-4\")\n    parser.add_argument(\"--baseline-model\", type=str, default=\"gpt-3.5-turbo\")\n    parser.add_argument(\n        \"--model-list\",\n        type=str,\n        nargs=\"+\",\n        default=None,\n        help=\"A list of models to be evaluated\",\n    )\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"single\",\n        choices=[\"pairwise-baseline\", \"pairwise-all\", \"single\"],\n        help=(\n            \"Evaluation mode. \"\n            \"`pairwise-baseline` runs pairwise comparision against a baseline. \"\n            \"`pairwise-all` runs pairwise comparision between all pairs. \"\n            \"`single` runs single answer grading.\"\n        ),\n    )\n    args = parser.parse_args()\n\n    if args.mode == \"single\":\n        display_result_func = display_result_single\n    else:\n        if args.mode == \"pairwise-all\":\n            args.baseline_model = None\n        display_result_func = display_result_pairwise\n\n    print(f\"Mode: {args.mode}\")\n    display_result_func(args)\n", "fastchat/llm_judge/download_mt_bench_pregenerated.py": "\"\"\"\nDownload the pre-generated model answers and judgments for MT-bench.\n\"\"\"\nimport os\n\nfrom fastchat.utils import run_cmd\n\nfilenames = [\n    \"data/mt_bench/model_answer/alpaca-13b.jsonl\",\n    \"data/mt_bench/model_answer/baize-v2-13b.jsonl\",\n    \"data/mt_bench/model_answer/chatglm-6b.jsonl\",\n    \"data/mt_bench/model_answer/claude-instant-v1.jsonl\",\n    \"data/mt_bench/model_answer/claude-v1.jsonl\",\n    \"data/mt_bench/model_answer/dolly-v2-12b.jsonl\",\n    \"data/mt_bench/model_answer/falcon-40b-instruct.jsonl\",\n    \"data/mt_bench/model_answer/fastchat-t5-3b.jsonl\",\n    \"data/mt_bench/model_answer/gpt-3.5-turbo.jsonl\",\n    \"data/mt_bench/model_answer/gpt-4.jsonl\",\n    \"data/mt_bench/model_answer/gpt4all-13b-snoozy.jsonl\",\n    \"data/mt_bench/model_answer/guanaco-33b.jsonl\",\n    \"data/mt_bench/model_answer/guanaco-65b.jsonl\",\n    \"data/mt_bench/model_answer/h2ogpt-oasst-open-llama-13b.jsonl\",\n    \"data/mt_bench/model_answer/koala-13b.jsonl\",\n    \"data/mt_bench/model_answer/llama-13b.jsonl\",\n    \"data/mt_bench/model_answer/mpt-30b-chat.jsonl\",\n    \"data/mt_bench/model_answer/mpt-30b-instruct.jsonl\",\n    \"data/mt_bench/model_answer/mpt-7b-chat.jsonl\",\n    \"data/mt_bench/model_answer/nous-hermes-13b.jsonl\",\n    \"data/mt_bench/model_answer/oasst-sft-4-pythia-12b.jsonl\",\n    \"data/mt_bench/model_answer/oasst-sft-7-llama-30b.jsonl\",\n    \"data/mt_bench/model_answer/palm-2-chat-bison-001.jsonl\",\n    \"data/mt_bench/model_answer/rwkv-4-raven-14b.jsonl\",\n    \"data/mt_bench/model_answer/stablelm-tuned-alpha-7b.jsonl\",\n    \"data/mt_bench/model_answer/tulu-30b.jsonl\",\n    \"data/mt_bench/model_answer/vicuna-13b-v1.3.jsonl\",\n    \"data/mt_bench/model_answer/vicuna-33b-v1.3.jsonl\",\n    \"data/mt_bench/model_answer/vicuna-7b-v1.3.jsonl\",\n    \"data/mt_bench/model_answer/wizardlm-13b.jsonl\",\n    \"data/mt_bench/model_answer/wizardlm-30b.jsonl\",\n    \"data/mt_bench/model_judgment/gpt-4_single.jsonl\",\n    \"data/mt_bench/model_judgment/gpt-4_pair.jsonl\",\n]\n\n\nif __name__ == \"__main__\":\n    prefix = \"https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/\"\n\n    for name in filenames:\n        os.makedirs(os.path.dirname(name), exist_ok=True)\n        ret = run_cmd(f\"wget -q --show-progress -O {name} {prefix + name}\")\n        assert ret == 0\n", "fastchat/llm_judge/gen_judgment.py": "\"\"\"\nUsage:\npython gen_judgment.py --model-list [LIST-OF-MODEL-ID] --parallel [num-concurrent-api-call] --mode [single|pairwise-baseline|pairwise-all]\n\"\"\"\nimport argparse\nfrom concurrent.futures import ThreadPoolExecutor\nimport json\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom fastchat.llm_judge.common import (\n    load_questions,\n    load_model_answers,\n    load_judge_prompts,\n    check_data,\n    play_a_match_pair,\n    play_a_match_single,\n    get_model_list,\n    Judge,\n    MatchPair,\n    MatchSingle,\n    NEED_REF_CATS,\n)\n\n\ndef make_match(\n    questions,\n    models,\n    model_answers,\n    judge,\n    baseline_model,\n    ref_answers=None,\n    multi_turn=False,\n):\n    matches = []\n    for q in questions:\n        if multi_turn and len(q[\"turns\"]) != 2:\n            continue\n        for i in range(len(models)):\n            q_id = q[\"question_id\"]\n            m_1 = models[i]\n            m_2 = baseline_model\n            if m_1 == m_2:\n                continue\n            a_1 = model_answers[m_1][q_id]\n            a_2 = model_answers[baseline_model][q_id]\n            if ref_answers is not None:\n                ref = ref_answers[judge.model_name][q_id]\n                match = MatchPair(\n                    dict(q),\n                    m_1,\n                    m_2,\n                    a_1,\n                    a_2,\n                    judge,\n                    ref_answer=ref,\n                    multi_turn=multi_turn,\n                )\n            else:\n                match = MatchPair(\n                    dict(q), m_1, m_2, a_1, a_2, judge, multi_turn=multi_turn\n                )\n            matches.append(match)\n    return matches\n\n\ndef make_match_all_pairs(\n    questions,\n    models,\n    model_answers,\n    judge,\n    baseline_model=None,\n    ref_answers=None,\n    multi_turn=False,\n):\n    matches = []\n    for q in questions:\n        if multi_turn and len(q[\"turns\"]) != 2:\n            continue\n        for i in range(len(models)):\n            for j in range(i + 1, len(models)):\n                q_id = q[\"question_id\"]\n                m_1 = models[i]\n                m_2 = models[j]\n                a_1 = model_answers[m_1][q_id]\n                a_2 = model_answers[m_2][q_id]\n                if ref_answers is not None:\n                    ref = ref_answers[judge.model_name][q_id]\n                    match = MatchPair(\n                        dict(q),\n                        m_1,\n                        m_2,\n                        a_1,\n                        a_2,\n                        judge,\n                        ref_answer=ref,\n                        multi_turn=multi_turn,\n                    )\n                else:\n                    match = MatchPair(\n                        dict(q), m_1, m_2, a_1, a_2, judge, multi_turn=multi_turn\n                    )\n                matches.append(match)\n    return matches\n\n\ndef make_match_single(\n    questions,\n    models,\n    model_answers,\n    judge,\n    baseline_model=None,\n    ref_answers=None,\n    multi_turn=False,\n):\n    matches = []\n    for q in questions:\n        if multi_turn and len(q[\"turns\"]) != 2:\n            continue\n        for i in range(len(models)):\n            q_id = q[\"question_id\"]\n            m = models[i]\n            a = model_answers[m][q_id]\n            if ref_answers is not None:\n                ref = ref_answers[judge.model_name][q_id]\n                matches.append(\n                    MatchSingle(\n                        dict(q), m, a, judge, ref_answer=ref, multi_turn=multi_turn\n                    )\n                )\n            else:\n                matches.append(MatchSingle(dict(q), m, a, judge, multi_turn=multi_turn))\n    return matches\n\n\ndef make_judge_pairwise(judge_model, judge_prompts):\n    judges = {}\n    judges[\"default\"] = Judge(judge_model, judge_prompts[\"pair-v2\"])\n    judges[\"math\"] = Judge(judge_model, judge_prompts[\"pair-math-v1\"], ref_based=True)\n    judges[\"default-mt\"] = Judge(\n        judge_model, judge_prompts[\"pair-v2-multi-turn\"], multi_turn=True\n    )\n    judges[\"math-mt\"] = Judge(\n        judge_model,\n        judge_prompts[\"pair-math-v1-multi-turn\"],\n        ref_based=True,\n        multi_turn=True,\n    )\n    return judges\n\n\ndef make_judge_single(judge_model, judge_prompts):\n    judges = {}\n    judges[\"default\"] = Judge(judge_model, judge_prompts[\"single-v1\"])\n    judges[\"math\"] = Judge(judge_model, judge_prompts[\"single-math-v1\"], ref_based=True)\n    judges[\"default-mt\"] = Judge(\n        judge_model, judge_prompts[\"single-v1-multi-turn\"], multi_turn=True\n    )\n    judges[\"math-mt\"] = Judge(\n        judge_model,\n        judge_prompts[\"single-math-v1-multi-turn\"],\n        ref_based=True,\n        multi_turn=True,\n    )\n    return judges\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--bench-name\",\n        type=str,\n        default=\"mt_bench\",\n        help=\"The name of the benchmark question set.\",\n    )\n    parser.add_argument(\n        \"--judge-file\",\n        type=str,\n        default=\"data/judge_prompts.jsonl\",\n        help=\"The file of judge prompts.\",\n    )\n    parser.add_argument(\"--judge-model\", type=str, default=\"gpt-4\")\n    parser.add_argument(\"--baseline-model\", type=str, default=\"gpt-3.5-turbo\")\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"single\",\n        choices=[\"pairwise-baseline\", \"pairwise-all\", \"single\"],\n        help=(\n            \"Evaluation mode. \"\n            \"`pairwise-baseline` runs pairwise comparision against a baseline. \"\n            \"`pairwise-all` runs pairwise comparision between all pairs. \"\n            \"`single` runs single answer grading.\"\n        ),\n    )\n    parser.add_argument(\n        \"--model-list\",\n        type=str,\n        nargs=\"+\",\n        default=None,\n        help=\"A list of models to be evaluated\",\n    )\n    parser.add_argument(\n        \"--parallel\", type=int, default=1, help=\"The number of concurrent API calls.\"\n    )\n    parser.add_argument(\n        \"--first-n\", type=int, help=\"A debug option. Only run the first `n` judgments.\"\n    )\n    args = parser.parse_args()\n\n    question_file = f\"data/{args.bench_name}/question.jsonl\"\n    answer_dir = f\"data/{args.bench_name}/model_answer\"\n    ref_answer_dir = f\"data/{args.bench_name}/reference_answer\"\n\n    # Load questions\n    questions = load_questions(question_file, None, None)\n\n    # Load answers\n    model_answers = load_model_answers(answer_dir)\n    ref_answers = load_model_answers(ref_answer_dir)\n\n    # Load judge\n    judge_prompts = load_judge_prompts(args.judge_file)\n\n    if args.first_n:\n        questions = questions[: args.first_n]\n\n    if args.model_list is None:\n        models = get_model_list(answer_dir)\n    else:\n        models = args.model_list\n\n    if args.mode == \"single\":\n        judges = make_judge_single(args.judge_model, judge_prompts)\n        play_a_match_func = play_a_match_single\n        output_file = (\n            f\"data/{args.bench_name}/model_judgment/{args.judge_model}_single.jsonl\"\n        )\n        make_match_func = make_match_single\n        baseline_model = None\n    else:\n        judges = make_judge_pairwise(args.judge_model, judge_prompts)\n        play_a_match_func = play_a_match_pair\n        output_file = (\n            f\"data/{args.bench_name}/model_judgment/{args.judge_model}_pair.jsonl\"\n        )\n        if args.mode == \"pairwise-all\":\n            make_match_func = make_match_all_pairs\n            baseline_model = None\n        else:\n            make_match_func = make_match\n            baseline_model = args.baseline_model\n\n    check_data(questions, model_answers, ref_answers, models, judges)\n\n    question_math = [q for q in questions if q[\"category\"] in NEED_REF_CATS]\n    question_default = [q for q in questions if q[\"category\"] not in NEED_REF_CATS]\n\n    # Make matches\n    matches = []\n    matches += make_match_func(\n        question_default, models, model_answers, judges[\"default\"], baseline_model\n    )\n    matches += make_match_func(\n        question_math,\n        models,\n        model_answers,\n        judges[\"math\"],\n        baseline_model,\n        ref_answers,\n    )\n    matches += make_match_func(\n        question_default,\n        models,\n        model_answers,\n        judges[\"default-mt\"],\n        baseline_model,\n        multi_turn=True,\n    )\n    matches += make_match_func(\n        question_math,\n        models,\n        model_answers,\n        judges[\"math-mt\"],\n        baseline_model,\n        ref_answers,\n        multi_turn=True,\n    )\n\n    match_stat = {}\n    match_stat[\"bench_name\"] = args.bench_name\n    match_stat[\"mode\"] = args.mode\n    match_stat[\"judge\"] = args.judge_model\n    match_stat[\"baseline\"] = baseline_model\n    match_stat[\"model_list\"] = models\n    match_stat[\"total_num_questions\"] = len(questions)\n    match_stat[\"total_num_matches\"] = len(matches)\n    match_stat[\"output_path\"] = output_file\n\n    # Show match stats and prompt enter to continue\n    print(\"Stats:\")\n    print(json.dumps(match_stat, indent=4))\n    input(\"Press Enter to confirm...\")\n\n    # Play matches\n    if args.parallel == 1:\n        for match in tqdm(matches):\n            play_a_match_func(match, output_file=output_file)\n    else:\n\n        def play_a_match_wrapper(match):\n            play_a_match_func(match, output_file=output_file)\n\n        np.random.seed(0)\n        np.random.shuffle(matches)\n\n        with ThreadPoolExecutor(args.parallel) as executor:\n            for match in tqdm(\n                executor.map(play_a_match_wrapper, matches), total=len(matches)\n            ):\n                pass\n", "fastchat/protocol/api_protocol.py": "from typing import Literal, Optional, List, Dict, Any, Union\n\nimport time\n\nimport shortuuid\nfrom pydantic import BaseModel, Field\n\n\nclass ErrorResponse(BaseModel):\n    object: str = \"error\"\n    message: str\n    code: int\n\n\nclass ModelPermission(BaseModel):\n    id: str = Field(default_factory=lambda: f\"modelperm-{shortuuid.random()}\")\n    object: str = \"model_permission\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    allow_create_engine: bool = False\n    allow_sampling: bool = True\n    allow_logprobs: bool = True\n    allow_search_indices: bool = True\n    allow_view: bool = True\n    allow_fine_tuning: bool = False\n    organization: str = \"*\"\n    group: Optional[str] = None\n    is_blocking: str = False\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"fastchat\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: List[ModelPermission] = []\n\n\nclass ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\n\n\nclass UsageInfo(BaseModel):\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    completion_tokens: Optional[int] = 0\n\n\nclass APIChatCompletionRequest(BaseModel):\n    model: str\n    messages: Union[str, List[Dict[str, str]]]\n    temperature: Optional[float] = 0.7\n    top_p: Optional[float] = 1.0\n    top_k: Optional[int] = -1\n    n: Optional[int] = 1\n    max_tokens: Optional[int] = None\n    stop: Optional[Union[str, List[str]]] = None\n    stream: Optional[bool] = False\n    user: Optional[str] = None\n    repetition_penalty: Optional[float] = 1.0\n    frequency_penalty: Optional[float] = 0.0\n    presence_penalty: Optional[float] = 0.0\n\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass ChatCompletionResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{shortuuid.random()}\")\n    object: str = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[str] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass ChatCompletionStreamResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{shortuuid.random()}\")\n    object: str = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseStreamChoice]\n\n\nclass APITokenCheckRequestItem(BaseModel):\n    model: str\n    prompt: str\n    max_tokens: int\n\n\nclass APITokenCheckRequest(BaseModel):\n    prompts: List[APITokenCheckRequestItem]\n\n\nclass APITokenCheckResponseItem(BaseModel):\n    fits: bool\n    tokenCount: int\n    contextLength: int\n\n\nclass APITokenCheckResponse(BaseModel):\n    prompts: List[APITokenCheckResponseItem]\n\n\nclass CompletionRequest(BaseModel):\n    model: str\n    prompt: Union[str, List[Any]]\n    suffix: Optional[str] = None\n    temperature: Optional[float] = 0.7\n    n: Optional[int] = 1\n    max_tokens: Optional[int] = 16\n    stop: Optional[Union[str, List[str]]] = None\n    stream: Optional[bool] = False\n    top_p: Optional[float] = 1.0\n    top_k: Optional[int] = -1\n    logprobs: Optional[int] = None\n    echo: Optional[bool] = False\n    presence_penalty: Optional[float] = 0.0\n    frequency_penalty: Optional[float] = 0.0\n    user: Optional[str] = None\n\n\nclass CompletionResponseChoice(BaseModel):\n    index: int\n    text: str\n    logprobs: Optional[int] = None\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass CompletionResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{shortuuid.random()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass CompletionResponseStreamChoice(BaseModel):\n    index: int\n    text: str\n    logprobs: Optional[float] = None\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass CompletionStreamResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{shortuuid.random()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseStreamChoice]\n", "fastchat/protocol/openai_api_protocol.py": "from typing import Literal, Optional, List, Dict, Any, Union\n\nimport time\n\nimport shortuuid\nfrom pydantic import BaseModel, Field\n\n\nclass ErrorResponse(BaseModel):\n    object: str = \"error\"\n    message: str\n    code: int\n\n\nclass ModelPermission(BaseModel):\n    id: str = Field(default_factory=lambda: f\"modelperm-{shortuuid.random()}\")\n    object: str = \"model_permission\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    allow_create_engine: bool = False\n    allow_sampling: bool = True\n    allow_logprobs: bool = True\n    allow_search_indices: bool = True\n    allow_view: bool = True\n    allow_fine_tuning: bool = False\n    organization: str = \"*\"\n    group: Optional[str] = None\n    is_blocking: str = False\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"fastchat\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: List[ModelPermission] = []\n\n\nclass ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\n\n\nclass UsageInfo(BaseModel):\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    completion_tokens: Optional[int] = 0\n\n\nclass LogProbs(BaseModel):\n    text_offset: List[int] = Field(default_factory=list)\n    token_logprobs: List[Optional[float]] = Field(default_factory=list)\n    tokens: List[str] = Field(default_factory=list)\n    top_logprobs: List[Optional[Dict[str, float]]] = Field(default_factory=list)\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: Union[\n        str,\n        List[Dict[str, str]],\n        List[Dict[str, Union[str, List[Dict[str, Union[str, Dict[str, str]]]]]]],\n    ]\n    temperature: Optional[float] = 0.7\n    top_p: Optional[float] = 1.0\n    top_k: Optional[int] = -1\n    n: Optional[int] = 1\n    max_tokens: Optional[int] = None\n    stop: Optional[Union[str, List[str]]] = None\n    stream: Optional[bool] = False\n    presence_penalty: Optional[float] = 0.0\n    frequency_penalty: Optional[float] = 0.0\n    user: Optional[str] = None\n\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass ChatCompletionResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{shortuuid.random()}\")\n    object: str = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[str] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass ChatCompletionStreamResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{shortuuid.random()}\")\n    object: str = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseStreamChoice]\n\n\nclass TokenCheckRequestItem(BaseModel):\n    model: str\n    prompt: str\n    max_tokens: int\n\n\nclass TokenCheckRequest(BaseModel):\n    prompts: List[TokenCheckRequestItem]\n\n\nclass TokenCheckResponseItem(BaseModel):\n    fits: bool\n    tokenCount: int\n    contextLength: int\n\n\nclass TokenCheckResponse(BaseModel):\n    prompts: List[TokenCheckResponseItem]\n\n\nclass EmbeddingsRequest(BaseModel):\n    model: Optional[str] = None\n    engine: Optional[str] = None\n    input: Union[str, List[Any]]\n    user: Optional[str] = None\n    encoding_format: Optional[str] = None\n\n\nclass EmbeddingsResponse(BaseModel):\n    object: str = \"list\"\n    data: List[Dict[str, Any]]\n    model: str\n    usage: UsageInfo\n\n\nclass CompletionRequest(BaseModel):\n    model: str\n    prompt: Union[str, List[Any]]\n    suffix: Optional[str] = None\n    temperature: Optional[float] = 0.7\n    n: Optional[int] = 1\n    max_tokens: Optional[int] = 16\n    stop: Optional[Union[str, List[str]]] = None\n    stream: Optional[bool] = False\n    top_p: Optional[float] = 1.0\n    top_k: Optional[int] = -1\n    logprobs: Optional[int] = None\n    echo: Optional[bool] = False\n    presence_penalty: Optional[float] = 0.0\n    frequency_penalty: Optional[float] = 0.0\n    user: Optional[str] = None\n    use_beam_search: Optional[bool] = False\n    best_of: Optional[int] = None\n\n\nclass CompletionResponseChoice(BaseModel):\n    index: int\n    text: str\n    logprobs: Optional[LogProbs] = None\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass CompletionResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{shortuuid.random()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass CompletionResponseStreamChoice(BaseModel):\n    index: int\n    text: str\n    logprobs: Optional[LogProbs] = None\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass CompletionStreamResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{shortuuid.random()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseStreamChoice]\n", "fastchat/model/compression.py": "import dataclasses\nimport gc\nimport glob\nimport os\n\nfrom accelerate import init_empty_weights\nfrom accelerate.utils import set_module_tensor_to_device\nfrom huggingface_hub import snapshot_download\nimport torch\nfrom torch import Tensor\nfrom torch.nn import functional as F\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    AutoModel,\n    AutoModelForSeq2SeqLM,\n)\n\n\n@dataclasses.dataclass\nclass CompressionConfig:\n    \"\"\"Group-wise quantization.\"\"\"\n\n    num_bits: int\n    group_size: int\n    group_dim: int\n    symmetric: bool\n    enabled: bool = True\n\n\ndefault_compression_config = CompressionConfig(\n    num_bits=8, group_size=256, group_dim=1, symmetric=True, enabled=True\n)\n\n\nclass CLinear(nn.Module):\n    \"\"\"Compressed Linear Layer.\"\"\"\n\n    def __init__(self, weight=None, bias=None, device=None):\n        super().__init__()\n        if weight is None:\n            self.weight = None\n        elif isinstance(weight, Tensor):\n            self.weight = compress(weight.data.to(device), default_compression_config)\n        else:\n            self.weight = weight\n        self.bias = bias\n\n    def forward(self, input: Tensor) -> Tensor:\n        weight = decompress(self.weight, default_compression_config)\n        if self.bias is None:\n            return F.linear(input.to(weight.dtype), weight)\n        return F.linear(input.to(weight.dtype), weight, self.bias.to(weight.dtype))\n\n\ndef compress_module(module, target_device):\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            setattr(\n                module,\n                attr_str,\n                CLinear(target_attr.weight, target_attr.bias, target_device),\n            )\n    for name, child in module.named_children():\n        compress_module(child, target_device)\n\n\ndef get_compressed_list(module, prefix=\"\"):\n    compressed_list = []\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            full_name = (\n                f\"{prefix}.{attr_str}.weight\" if prefix else f\"{attr_str}.weight\"\n            )\n            compressed_list.append(full_name)\n    for name, child in module.named_children():\n        child_prefix = f\"{prefix}.{name}\" if prefix else name\n        for each in get_compressed_list(child, child_prefix):\n            compressed_list.append(each)\n    return compressed_list\n\n\ndef apply_compressed_weight(module, compressed_state_dict, target_device, prefix=\"\"):\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            full_name = (\n                f\"{prefix}.{attr_str}.weight\" if prefix else f\"{attr_str}.weight\"\n            )\n            setattr(\n                module,\n                attr_str,\n                CLinear(\n                    compressed_state_dict[full_name], target_attr.bias, target_device\n                ),\n            )\n    for name, child in module.named_children():\n        child_prefix = f\"{prefix}.{name}\" if prefix else name\n        apply_compressed_weight(\n            child, compressed_state_dict, target_device, child_prefix\n        )\n\n\ndef load_compress_model(model_path, device, torch_dtype, use_fast, revision=\"main\"):\n    # partially load model\n    # `use_fast=True`` is not supported for some models.\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=use_fast, revision=revision, trust_remote_code=True\n        )\n    except TypeError:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=~use_fast, revision=revision, trust_remote_code=True\n        )\n    with init_empty_weights():\n        # `trust_remote_code` should be set as `True` for both AutoConfig and AutoModel\n        config = AutoConfig.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            torch_dtype=torch_dtype,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        # some models are loaded by AutoModel but not AutoModelForCausalLM,\n        # such as chatglm, chatglm2\n        try:\n            # google/flan-* models are based on an AutoModelForSeq2SeqLM.\n            if \"T5Config\" in str(type(config)):\n                model = AutoModelForSeq2SeqLM.from_config(\n                    config, trust_remote_code=True\n                )\n            else:\n                model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n        except NameError:\n            model = AutoModel.from_config(config, trust_remote_code=True)\n        linear_weights = get_compressed_list(model)\n    if os.path.exists(model_path):\n        # `model_path` is a local folder\n        base_pattern = os.path.join(model_path, \"pytorch_model*.bin\")\n    else:\n        # `model_path` is a cached Hugging Face repo\n        # We don't necessarily need to download the model' repo again if there is a cache.\n        # So check the default huggingface cache first.\n        model_path_temp = os.path.join(\n            os.path.expanduser(\"~\"),\n            \".cache/huggingface/hub\",\n            \"models--\" + model_path.replace(\"/\", \"--\"),\n            \"snapshots/\",\n        )\n        downloaded = False\n        if os.path.exists(model_path_temp):\n            temp_last_dir = os.listdir(model_path_temp)[-1]\n            model_path_temp = os.path.join(model_path_temp, temp_last_dir)\n            base_pattern = os.path.join(model_path_temp, \"pytorch_model*.bin\")\n            files = glob.glob(base_pattern)\n            if len(files) > 0:\n                downloaded = True\n\n        if downloaded:\n            model_path = model_path_temp\n        else:\n            model_path = snapshot_download(model_path, revision=revision)\n        base_pattern = os.path.join(model_path, \"pytorch_model*.bin\")\n\n    files = glob.glob(base_pattern)\n    use_safetensors = False\n    if len(files) == 0:\n        base_pattern = os.path.join(model_path, \"*.safetensors\")\n        files = glob.glob(base_pattern)\n        use_safetensors = True\n    if len(files) == 0:\n        raise ValueError(\n            f\"Cannot find any model weight files. \"\n            f\"Please check your (cached) weight path: {model_path}\"\n        )\n\n    compressed_state_dict = {}\n    if use_safetensors:\n        from safetensors.torch import load_file\n    for filename in tqdm(files):\n        if use_safetensors:\n            tmp_state_dict = load_file(filename)\n        else:\n            tmp_state_dict = torch.load(\n                filename, map_location=lambda storage, loc: storage\n            )\n        for name in tmp_state_dict:\n            if name in linear_weights:\n                tensor = tmp_state_dict[name].to(device, dtype=torch_dtype)\n                compressed_state_dict[name] = compress(\n                    tensor, default_compression_config\n                )\n            else:\n                compressed_state_dict[name] = tmp_state_dict[name].to(\n                    device, dtype=torch_dtype\n                )\n            tmp_state_dict[name] = None\n            tensor = None\n            gc.collect()\n            torch.cuda.empty_cache()\n            if device == \"xpu\":\n                torch.xpu.empty_cache()\n            if device == \"npu\":\n                torch.npu.empty_cache()\n\n    for name in model.state_dict():\n        if name not in linear_weights:\n            set_module_tensor_to_device(\n                model, name, device, value=compressed_state_dict[name]\n            )\n    apply_compressed_weight(model, compressed_state_dict, device)\n\n    if torch_dtype == torch.float16:\n        model.half()\n    model.to(device)\n    model.eval()\n\n    return model, tokenizer\n\n\ndef compress(tensor, config):\n    \"\"\"Simulate group-wise quantization.\"\"\"\n    if not config.enabled:\n        return tensor\n\n    group_size, num_bits, group_dim, symmetric = (\n        config.group_size,\n        config.num_bits,\n        config.group_dim,\n        config.symmetric,\n    )\n    assert num_bits <= 8\n\n    original_shape = tensor.shape\n    num_groups = (original_shape[group_dim] + group_size - 1) // group_size\n    new_shape = (\n        original_shape[:group_dim]\n        + (num_groups, group_size)\n        + original_shape[group_dim + 1 :]\n    )\n\n    # Pad\n    pad_len = (group_size - original_shape[group_dim] % group_size) % group_size\n    if pad_len != 0:\n        pad_shape = (\n            original_shape[:group_dim] + (pad_len,) + original_shape[group_dim + 1 :]\n        )\n        tensor = torch.cat(\n            [tensor, torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)],\n            dim=group_dim,\n        )\n    data = tensor.view(new_shape)\n\n    # Quantize\n    if symmetric:\n        B = 2 ** (num_bits - 1) - 1\n        scale = B / torch.max(data.abs(), dim=group_dim + 1, keepdim=True)[0]\n        data = data * scale\n        data = data.clamp_(-B, B).round_().to(torch.int8)\n        return data, scale, original_shape\n    else:\n        B = 2**num_bits - 1\n        mn = torch.min(data, dim=group_dim + 1, keepdim=True)[0]\n        mx = torch.max(data, dim=group_dim + 1, keepdim=True)[0]\n\n        scale = B / (mx - mn)\n        data = data - mn\n        data.mul_(scale)\n\n        data = data.clamp_(0, B).round_().to(torch.uint8)\n        return data, mn, scale, original_shape\n\n\ndef decompress(packed_data, config):\n    \"\"\"Simulate group-wise dequantization.\"\"\"\n    if not config.enabled:\n        return packed_data\n\n    group_size, num_bits, group_dim, symmetric = (\n        config.group_size,\n        config.num_bits,\n        config.group_dim,\n        config.symmetric,\n    )\n\n    # Dequantize\n    if symmetric:\n        data, scale, original_shape = packed_data\n        data = data / scale\n    else:\n        data, mn, scale, original_shape = packed_data\n        data = data / scale\n        data.add_(mn)\n\n    # Unpad\n    pad_len = (group_size - original_shape[group_dim] % group_size) % group_size\n    if pad_len:\n        padded_original_shape = (\n            original_shape[:group_dim]\n            + (original_shape[group_dim] + pad_len,)\n            + original_shape[group_dim + 1 :]\n        )\n        data = data.reshape(padded_original_shape)\n        indices = [slice(0, x) for x in original_shape]\n        return data[indices].contiguous()\n    else:\n        return data.view(original_shape)\n", "fastchat/model/apply_lora.py": "\"\"\"\nApply the LoRA weights on top of a base model.\n\nUsage:\npython3 -m fastchat.model.apply_lora --base ~/model_weights/llama-7b --target ~/model_weights/baize-7b --lora project-baize/baize-lora-7B\n\nDependency:\npip3 install git+https://github.com/huggingface/peft.git@2822398fbe896f25d4dac5e468624dc5fd65a51b\n\"\"\"\nimport argparse\n\nimport torch\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef apply_lora(base_model_path, target_model_path, lora_path):\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False)\n\n    print(f\"Loading the LoRA adapter from {lora_path}\")\n\n    lora_model = PeftModel.from_pretrained(\n        base,\n        lora_path,\n        # torch_dtype=torch.float16\n    )\n\n    print(\"Applying the LoRA\")\n    model = lora_model.merge_and_unload()\n\n    print(f\"Saving the target model to {target_model_path}\")\n    model.save_pretrained(target_model_path)\n    base_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--lora-path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    apply_lora(args.base_model_path, args.target_model_path, args.lora_path)\n", "fastchat/model/convert_fp16.py": "\"\"\"\nUsage:\npython3 -m fastchat.model.convert_fp16 --in in-folder --out out-folder\n\"\"\"\nimport argparse\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n\ndef convert_fp16(in_checkpoint, out_checkpoint):\n    tokenizer = AutoTokenizer.from_pretrained(in_checkpoint, use_fast=False)\n    model = AutoModelForCausalLM.from_pretrained(\n        in_checkpoint, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    model.save_pretrained(out_checkpoint)\n    tokenizer.save_pretrained(out_checkpoint)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--in-checkpoint\", type=str, help=\"Path to the model\")\n    parser.add_argument(\"--out-checkpoint\", type=str, help=\"Path to the output model\")\n    args = parser.parse_args()\n\n    convert_fp16(args.in_checkpoint, args.out_checkpoint)\n", "fastchat/model/rwkv_model.py": "import os\nfrom types import SimpleNamespace\nimport warnings\n\nimport torch\n\nos.environ[\"RWKV_JIT_ON\"] = \"1\"\nos.environ[\"RWKV_CUDA_ON\"] = \"1\"\n\nfrom rwkv.model import RWKV\nfrom rwkv.utils import PIPELINE, PIPELINE_ARGS\n\n\nclass RwkvModel:\n    def __init__(self, model_path):\n        warnings.warn(\n            \"Experimental support. Please use ChatRWKV if you want to chat with RWKV\"\n        )\n        self.config = SimpleNamespace(is_encoder_decoder=False)\n        self.model = RWKV(model=model_path, strategy=\"cuda fp16\")\n        # two GPUs\n        # self.model = RWKV(model=model_path, strategy=\"cuda:0 fp16 *20 -> cuda:1 fp16\")\n\n        self.tokenizer = None\n        self.model_path = model_path\n\n    def to(self, target):\n        assert target == \"cuda\"\n\n    def __call__(self, input_ids, use_cache, past_key_values=None):\n        assert use_cache == True\n        input_ids = input_ids[0].detach().cpu().numpy()\n        # print(input_ids)\n        logits, state = self.model.forward(input_ids, past_key_values)\n        # print(logits)\n        logits = logits.unsqueeze(0).unsqueeze(0)\n        out = SimpleNamespace(logits=logits, past_key_values=state)\n        return out\n\n    def generate(\n        self, input_ids, do_sample, temperature, max_new_tokens, repetition_penalty=1.0\n    ):\n        # This function is used by fastchat.llm_judge.\n        # Because RWKV does not support huggingface generation API,\n        # we reuse fastchat.serve.inference.generate_stream as a workaround.\n        from transformers import AutoTokenizer\n\n        from fastchat.serve.inference import generate_stream\n        from fastchat.conversation import get_conv_template\n\n        if self.tokenizer is None:\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                \"EleutherAI/pythia-160m\", use_fast=True\n            )\n        prompt = self.tokenizer.decode(input_ids[0].tolist())\n        conv = get_conv_template(\"rwkv\")\n\n        gen_params = {\n            \"model\": self.model_path,\n            \"prompt\": prompt,\n            \"temperature\": temperature,\n            \"repetition_penalty\": repetition_penalty,\n            \"max_new_tokens\": max_new_tokens,\n            \"stop\": conv.stop_str,\n            \"stop_token_ids\": conv.stop_token_ids,\n            \"echo\": False,\n        }\n        res_iter = generate_stream(self, self.tokenizer, gen_params, \"cuda\")\n\n        for res in res_iter:\n            pass\n\n        output = res[\"text\"]\n        output_ids = self.tokenizer.encode(output)\n\n        return [input_ids[0].tolist() + output_ids]\n", "fastchat/model/apply_delta.py": "\"\"\"\nApply the delta weights on top of a base model.\n\nUsage:\npython3 -m fastchat.model.apply_delta --base ~/model_weights/llama-7b --target ~/model_weights/vicuna-7b --delta lmsys/vicuna-7b-delta-v1.1\n\"\"\"\nimport argparse\nimport gc\nimport glob\nimport json\nimport os\nimport shutil\nimport tempfile\n\nfrom huggingface_hub import snapshot_download\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n\n\nGB = 1 << 30\n\n\ndef split_files(model_path, tmp_path, split_size):\n    if not os.path.exists(model_path):\n        model_path = snapshot_download(repo_id=model_path)\n    if not os.path.exists(tmp_path):\n        os.makedirs(tmp_path)\n\n    file_pattern = os.path.join(model_path, \"pytorch_model-*.bin\")\n    files = glob.glob(file_pattern)\n\n    part = 0\n    try:\n        for file_path in tqdm(files):\n            state_dict = torch.load(file_path)\n            new_state_dict = {}\n\n            current_size = 0\n            for name, param in state_dict.items():\n                param_size = param.numel() * param.element_size()\n\n                if current_size + param_size > split_size:\n                    new_file_name = f\"pytorch_model-{part}.bin\"\n                    new_file_path = os.path.join(tmp_path, new_file_name)\n                    torch.save(new_state_dict, new_file_path)\n                    current_size = 0\n                    new_state_dict = None\n                    gc.collect()\n                    new_state_dict = {}\n                    part += 1\n\n                new_state_dict[name] = param\n                current_size += param_size\n\n            new_file_name = f\"pytorch_model-{part}.bin\"\n            new_file_path = os.path.join(tmp_path, new_file_name)\n            torch.save(new_state_dict, new_file_path)\n            new_state_dict = None\n            gc.collect()\n            new_state_dict = {}\n            part += 1\n    except Exception as e:\n        print(f\"An error occurred during split_files: {e}\")\n        shutil.rmtree(tmp_path)\n        raise\n\n\ndef apply_delta_low_cpu_mem(base_model_path, target_model_path, delta_path):\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n    delta_config = AutoConfig.from_pretrained(delta_path)\n\n    if os.path.exists(target_model_path):\n        shutil.rmtree(target_model_path)\n    os.makedirs(target_model_path)\n\n    split_size = 4 * GB\n\n    with tempfile.TemporaryDirectory() as tmp_base_path, tempfile.TemporaryDirectory() as tmp_delta_path:\n        print(f\"Split files for the base model to {tmp_base_path}\")\n        split_files(base_model_path, tmp_base_path, split_size)\n        print(f\"Split files for the delta weights to {tmp_delta_path}\")\n        split_files(delta_path, tmp_delta_path, split_size)\n\n        base_pattern = os.path.join(tmp_base_path, \"pytorch_model-*.bin\")\n        base_files = glob.glob(base_pattern)\n        delta_pattern = os.path.join(tmp_delta_path, \"pytorch_model-*.bin\")\n        delta_files = glob.glob(delta_pattern)\n        delta_state_dict = torch.load(delta_files[0])\n\n        print(\"Applying the delta\")\n        weight_map = {}\n        total_size = 0\n\n        for i, base_file in tqdm(enumerate(base_files)):\n            state_dict = torch.load(base_file)\n            file_name = f\"pytorch_model-{i}.bin\"\n            for name, param in state_dict.items():\n                if name not in delta_state_dict:\n                    for delta_file in delta_files:\n                        delta_state_dict = torch.load(delta_file)\n                        gc.collect()\n                        if name in delta_state_dict:\n                            break\n\n                state_dict[name] += delta_state_dict[name]\n                weight_map[name] = file_name\n                total_size += param.numel() * param.element_size()\n                gc.collect()\n            torch.save(state_dict, os.path.join(target_model_path, file_name))\n\n        with open(\n            os.path.join(target_model_path, \"pytorch_model.bin.index.json\"), \"w\"\n        ) as f:\n            json.dump(\n                {\"weight_map\": weight_map, \"metadata\": {\"total_size\": total_size}}, f\n            )\n\n    print(f\"Saving the target model to {target_model_path}\")\n    delta_tokenizer.save_pretrained(target_model_path)\n    delta_config.save_pretrained(target_model_path)\n\n\ndef apply_delta(base_model_path, target_model_path, delta_path):\n    print(f\"Loading the delta weights from {delta_path}\")\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n    delta = AutoModelForCausalLM.from_pretrained(\n        delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(\"Applying the delta\")\n    for name, param in tqdm(base.state_dict().items(), desc=\"Applying delta\"):\n        assert name in delta.state_dict()\n        param.data += delta.state_dict()[name]\n\n    print(f\"Saving the target model to {target_model_path}\")\n    base.save_pretrained(target_model_path)\n    delta_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\n        \"--low-cpu-mem\",\n        action=\"store_true\",\n        help=\"Lower the cpu memory usage. This will split large files and use \"\n        \"disk as swap to reduce the memory usage below 10GB.\",\n    )\n    args = parser.parse_args()\n\n    if args.low_cpu_mem:\n        apply_delta_low_cpu_mem(\n            args.base_model_path, args.target_model_path, args.delta_path\n        )\n    else:\n        apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\n", "fastchat/model/model_cllm.py": "import torch\nimport gc\n\nimport os\nimport time\nimport random\nfrom typing import Dict, Optional, Sequence, List, Tuple\nfrom transformers.cache_utils import Cache, DynamicCache\nfrom transformers import (\n    LlamaModel,\n    LlamaForCausalLM,\n    GenerationConfig,\n    StoppingCriteria,\n    StoppingCriteriaList,\n    TextIteratorStreamer,\n)\nfrom transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\nimport torch.nn.functional as F\n\n\ndef get_jacobian_trajectory(\n    model, tokenizer, input_ids, attention_mask, max_new_tokens\n):\n    bsz = input_ids.shape[0]\n    prompt_len = [torch.sum(t) for t in attention_mask]\n    max_prompt_len = max(prompt_len)\n    total_len = max_prompt_len + max_new_tokens\n\n    # initialize the first point of jacobian trajectory\n    tokens = torch.full(\n        (bsz, total_len), tokenizer.pad_token_id, dtype=torch.long, device=model.device\n    )\n    for i in range(bsz):\n        tokens[i, :] = torch.tensor(\n            random.choices(input_ids[i][attention_mask[i] == 1], k=total_len),\n            dtype=torch.long,\n            device=model.device,\n        )\n        tokens[i, : prompt_len[i]] = input_ids[i][: prompt_len[i]].to(\n            dtype=torch.long, device=model.device\n        )\n    itr = 0\n    next_generation = tokens\n    generate_attention_mask = torch.full_like(next_generation, 1).to(model.device)\n    accurate_lengths = torch.tensor([prompt_len[i].item()] * bsz, device=model.device)\n    prev_len = 0\n    while True:\n        current_generation = next_generation\n        with torch.no_grad():\n            logits = model(current_generation, generate_attention_mask).logits\n        next_generation = torch.argmax(\n            torch.nn.functional.softmax(logits, dim=-1) / 0.001, dim=-1\n        )\n\n        # hold prompt unchanged and update generated tokens\n        for i in range(bsz):\n            next_generation[i, :] = torch.cat(\n                (\n                    tokens[i, : prompt_len[i]],\n                    next_generation[i, prompt_len[i] - 1 : total_len - 1],\n                ),\n                dim=0,\n            )\n\n        if (\n            torch.all(torch.eq(next_generation, current_generation)).item()\n            and itr == max_new_tokens\n            or len(\n                torch.where(\n                    current_generation[0, : accurate_lengths[0]]\n                    == tokenizer.eos_token_id\n                )[0]\n            )\n            > 0\n        ):\n            # forced exit due to max_new_tokens constraint or eos reached\n            return next_generation, itr\n\n        # skip the first itr, current_generation has not been updated yet\n        if itr != 0:\n            if torch.all(torch.eq(next_generation, current_generation)).item():\n                matched_position = total_len\n            else:\n                matched_position = (\n                    torch.eq(current_generation, next_generation).squeeze(0) == False\n                ).nonzero(as_tuple=True)[0][0]\n            fast_forward_cnt = matched_position - accurate_lengths[0]\n\n            for i in range(bsz):\n                accurate_lengths[i] = matched_position.item()\n\n            # flush and print the first sequence\n            generated_str = tokenizer.decode(\n                next_generation[0, prompt_len[0] : accurate_lengths[0]],\n                skip_special_tokens=True,\n                spaces_between_special_tokens=False,\n                clean_up_tokenization_spaces=True,\n            )\n            print(generated_str[prev_len:], flush=True, end=\"\")\n            prev_len = len(generated_str)\n\n            if torch.all(torch.eq(next_generation, current_generation)).item():\n                # early termination: itr < max_new_tokens\n                return next_generation, itr\n\n        itr += 1\n\n\ndef generate_stream_cllm(\n    model,\n    tokenizer,\n    params,\n    device,\n    context_len,\n    stream_interval=2,\n    judge_sent_end=False,\n):\n    # converge_step = []\n    prompt = params[\"prompt\"]\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    max_new_tokens = int(params.get(\"n_token_seq_length\", 32))\n    max_new_seq_len = int(params.get(\"max_new_tokens\", 1024))\n\n    prompt_len = torch.sum(inputs[\"attention_mask\"], dim=-1)\n    generation = inputs[\"input_ids\"]\n    input_echo_len = len(generation)\n\n    ### generation phase\n    itr = 0\n    eos_reached = False\n    while True:\n        if itr == 0:\n            input_ids = inputs[\"input_ids\"]\n            input_masks = inputs[\"attention_mask\"]\n        else:\n            input_masks = torch.ones_like(input_ids).to(device)\n            for j in range(bsz):\n                input_masks[j][\n                    torch.sum(inputs[\"attention_mask\"], dim=-1)[j]\n                    + itr * max_new_tokens :\n                ] = 0\n\n        bsz = input_ids.shape[0]\n        eos_reached = torch.tensor([False] * bsz, device=device)\n\n        generation, iter_steps = get_jacobian_trajectory(\n            model=model,\n            tokenizer=tokenizer,\n            input_ids=input_ids,\n            attention_mask=input_masks,\n            max_new_tokens=max_new_tokens,\n        )\n\n        ### inspect <eos>\n        for j in range(bsz):\n            prompt_len = torch.sum(input_masks, dim=-1)\n            eos_positions = torch.where(generation[j] == tokenizer.eos_token_id)[0]\n\n            if len(eos_positions) == 0:\n                # no EOS, continue to the next item in the batch\n                generation[j][prompt_len[j] + max_new_tokens :] = tokenizer.pad_token_id\n                continue\n            # otherwise, set tokens coming after EOS as pad\n            else:\n                if len(eos_positions) != 0:\n                    eos_reached[j] = True\n                    generation[j, int(eos_positions[0]) + 1 :] = tokenizer.pad_token_id\n\n        itr += 1\n\n        if all(eos_reached) or itr * max_new_tokens >= max_new_seq_len:\n            break\n        input_ids = generation[\n            torch.where(eos_reached == False)[0].tolist(), ...\n        ]  # delete samples with <eos> generated\n\n    if all(eos_reached):\n        finish_reason = \"eos\"\n    elif itr * max_new_tokens > max_new_seq_len:\n        finish_reason = \"length\"\n    else:\n        finish_reason = \"stop\"\n\n    output = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n\n    yield {\n        \"text\": \"\",\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": itr * max_new_tokens,\n            \"total_tokens\": input_echo_len + itr * max_new_tokens,\n        },\n        \"finish_reason\": finish_reason,\n    }\n\n    # clean\n    gc.collect()\n    torch.cuda.empty_cache()\n    if device == \"xpu\":\n        torch.xpu.empty_cache()\n    if device == \"npu\":\n        torch.npu.empty_cache()\n", "fastchat/model/make_delta.py": "\"\"\"\nMake the delta weights by subtracting base weights.\n\nUsage:\npython3 -m fastchat.model.make_delta --base ~/model_weights/llama-13b --target ~/model_weights/vicuna-13b --delta ~/model_weights/vicuna-13b-delta --hub-repo-id lmsys/vicuna-13b-delta-v1.1\n\"\"\"\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef make_delta(base_model_path, target_model_path, delta_path):\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(f\"Loading the target model from {target_model_path}\")\n    target = AutoModelForCausalLM.from_pretrained(\n        target_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    target_tokenizer = AutoTokenizer.from_pretrained(target_model_path, use_fast=False)\n\n    print(\"Calculating the delta\")\n    for name, param in tqdm(target.state_dict().items(), desc=\"Calculating delta\"):\n        assert name in base.state_dict()\n        param.data -= base.state_dict()[name]\n\n    print(f\"Saving the delta to {delta_path}\")\n    if args.hub_repo_id:\n        kwargs = {\"push_to_hub\": True, \"repo_id\": args.hub_repo_id}\n    else:\n        kwargs = {}\n    target.save_pretrained(delta_path, **kwargs)\n    target_tokenizer.save_pretrained(delta_path, **kwargs)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\"--hub-repo-id\", type=str)\n    args = parser.parse_args()\n\n    make_delta(args.base_model_path, args.target_model_path, args.delta_path)\n", "fastchat/model/model_exllama.py": "import gc\nimport sys\nfrom typing import Dict\n\nimport torch\n\n\ndef generate_stream_exllama(\n    model,\n    tokenizer,\n    params: Dict,\n    device: str,\n    context_len: int,\n    stream_interval: int = 2,\n    judge_sent_end: bool = False,\n):\n    try:\n        from exllamav2.generator import ExLlamaV2StreamingGenerator, ExLlamaV2Sampler\n    except ImportError as e:\n        print(f\"Error: Failed to load Exllamav2. {e}\")\n        sys.exit(-1)\n\n    prompt = params[\"prompt\"]\n\n    generator = ExLlamaV2StreamingGenerator(model.model, model.cache, tokenizer)\n    settings = ExLlamaV2Sampler.Settings()\n\n    settings.temperature = float(params.get(\"temperature\", 0.85))\n    settings.top_k = int(params.get(\"top_k\", 50))\n    settings.top_p = float(params.get(\"top_p\", 0.8))\n    settings.token_repetition_penalty = float(params.get(\"repetition_penalty\", 1.15))\n    settings.disallow_tokens(generator.tokenizer, [generator.tokenizer.eos_token_id])\n\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n\n    generator.set_stop_conditions(params.get(\"stop_token_ids\", None) or [])\n    echo = bool(params.get(\"echo\", True))\n\n    input_ids = generator.tokenizer.encode(prompt)\n    prompt_tokens = input_ids.shape[-1]\n    generator.begin_stream(input_ids, settings)\n\n    generated_tokens = 0\n    if echo:\n        output = prompt\n    else:\n        output = \"\"\n    while True:\n        chunk, eos, _ = generator.stream()\n        output += chunk\n        generated_tokens += 1\n        if generated_tokens == max_new_tokens:\n            finish_reason = \"length\"\n            break\n        elif eos:\n            finish_reason = \"length\"\n            break\n        yield {\n            \"text\": output,\n            \"usage\": {\n                \"prompt_tokens\": prompt_tokens,\n                \"completion_tokens\": generated_tokens,\n                \"total_tokens\": prompt_tokens + generated_tokens,\n            },\n            \"finish_reason\": None,\n        }\n\n    yield {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": prompt_tokens,\n            \"completion_tokens\": generated_tokens,\n            \"total_tokens\": prompt_tokens + generated_tokens,\n        },\n        \"finish_reason\": finish_reason,\n    }\n    gc.collect()\n", "fastchat/model/model_chatglm.py": "\"\"\"\nInference code for ChatGLM.\nAdapted from https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py.\n\"\"\"\nimport re\n\nimport torch\nfrom transformers.generation.logits_process import LogitsProcessor\n\n\nclass InvalidScoreLogitsProcessor(LogitsProcessor):\n    def __call__(\n        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        if torch.isnan(scores).any() or torch.isinf(scores).any():\n            scores.zero_()\n            scores[..., 5] = 5e4\n        return scores\n\n\ninvalid_score_processor = InvalidScoreLogitsProcessor()\n\n\ndef process_response(response):\n    response = response.strip()\n    response = response.replace(\"[[\u8bad\u7ec3\u65f6\u95f4]]\", \"2023\u5e74\")\n    punkts = [\n        [\",\", \"\uff0c\"],\n        [\"!\", \"\uff01\"],\n        [\":\", \"\uff1a\"],\n        [\";\", \"\uff1b\"],\n        [\"\\?\", \"\uff1f\"],\n    ]\n    for item in punkts:\n        response = re.sub(r\"([\\u4e00-\\u9fff])%s\" % item[0], r\"\\1%s\" % item[1], response)\n        response = re.sub(r\"%s([\\u4e00-\\u9fff])\" % item[0], r\"%s\\1\" % item[1], response)\n    return response\n\n\ndef recover_message_list(prompt):\n    role_token_pattern = \"|\".join(\n        [re.escape(r) for r in [\"<|system|>\", \"<|user|>\", \"<|assistant|>\"]]\n    )\n    role = None\n    last_end_idx = -1\n    message_list = []\n    for match in re.finditer(role_token_pattern, prompt):\n        if role:\n            messge = {}\n            if role == \"<|system|>\":\n                messge[\"role\"] = \"system\"\n            elif role == \"<|user|>\":\n                messge[\"role\"] = \"user\"\n            else:\n                messge[\"role\"] = \"assistant\"\n            messge[\"content\"] = prompt[last_end_idx + 1 : match.start()]\n            message_list.append(messge)\n\n        role = prompt[match.start() : match.end()]\n        last_end_idx = match.end()\n\n    return message_list\n\n\n@torch.inference_mode()\ndef generate_stream_chatglm(\n    model,\n    tokenizer,\n    params,\n    device,\n    context_len=2048,\n    stream_interval=2,\n    judge_sent_end=False,\n):\n    prompt = params[\"prompt\"]\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n    echo = params.get(\"echo\", True)\n\n    model_type = str(type(model)).lower()\n    if \"peft\" in model_type:\n        model_type = str(type(model.base_model.model)).lower()\n\n    if \"chatglm3\" in model_type:\n        message_list = recover_message_list(prompt)\n        inputs = tokenizer.build_chat_input(\n            query=message_list[-1][\"content\"], history=message_list[:-1], role=\"user\"\n        ).to(model.device)\n    else:\n        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n    input_echo_len = len(inputs[\"input_ids\"][0])\n\n    gen_kwargs = {\n        \"max_length\": max_new_tokens + input_echo_len,\n        \"do_sample\": True if temperature > 1e-5 else False,\n        \"top_p\": top_p,\n        \"repetition_penalty\": repetition_penalty,\n        \"logits_processor\": [invalid_score_processor],\n    }\n    if temperature > 1e-5:\n        gen_kwargs[\"temperature\"] = temperature\n\n    total_len = 0\n    for total_ids in model.stream_generate(**inputs, **gen_kwargs):\n        total_ids = total_ids.tolist()[0]\n        total_len = len(total_ids)\n        if echo:\n            output_ids = total_ids\n        else:\n            output_ids = total_ids[input_echo_len:]\n        response = tokenizer.decode(output_ids)\n        response = process_response(response)\n\n        yield {\n            \"text\": response,\n            \"usage\": {\n                \"prompt_tokens\": input_echo_len,\n                \"completion_tokens\": total_len - input_echo_len,\n                \"total_tokens\": total_len,\n            },\n            \"finish_reason\": None,\n        }\n\n    # TODO: ChatGLM stop when it reach max length\n    # Only last stream result contains finish_reason, we set finish_reason as stop\n    ret = {\n        \"text\": response,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": total_len - input_echo_len,\n            \"total_tokens\": total_len,\n        },\n        \"finish_reason\": \"stop\",\n    }\n    yield ret\n", "fastchat/model/model_adapter.py": "\"\"\"Model adapter registration.\"\"\"\n\nimport math\nimport os\nimport re\nimport sys\nfrom typing import Dict, List, Optional\nimport warnings\n\nif sys.version_info >= (3, 9):\n    from functools import cache\nelse:\n    from functools import lru_cache as cache\n\nimport psutil\nimport torch\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    LlamaTokenizer,\n    LlamaForCausalLM,\n    T5Tokenizer,\n)\n\nfrom fastchat.constants import CPU_ISA\nfrom fastchat.conversation import Conversation, get_conv_template\nfrom fastchat.model.compression import load_compress_model\nfrom fastchat.model.llama_condense_monkey_patch import replace_llama_with_condense\nfrom fastchat.model.model_chatglm import generate_stream_chatglm\nfrom fastchat.model.model_codet5p import generate_stream_codet5p\nfrom fastchat.model.model_falcon import generate_stream_falcon\nfrom fastchat.model.model_yuan2 import generate_stream_yuan2\nfrom fastchat.model.model_exllama import generate_stream_exllama\nfrom fastchat.model.model_xfastertransformer import generate_stream_xft\nfrom fastchat.model.model_cllm import generate_stream_cllm\n\nfrom fastchat.model.monkey_patch_non_inplace import (\n    replace_llama_attn_with_non_inplace_operations,\n)\nfrom fastchat.modules.awq import AWQConfig, load_awq_quantized\nfrom fastchat.modules.exllama import ExllamaConfig, load_exllama_model\nfrom fastchat.modules.xfastertransformer import load_xft_model, XftConfig\nfrom fastchat.modules.gptq import GptqConfig, load_gptq_quantized\nfrom fastchat.utils import get_gpu_memory\n\n# Check an environment variable to check if we should be sharing Peft model\n# weights.  When false we treat all Peft models as separate.\npeft_share_base_weights = (\n    os.environ.get(\"PEFT_SHARE_BASE_WEIGHTS\", \"false\").lower() == \"true\"\n)\n\nANTHROPIC_MODEL_LIST = (\n    \"claude-1\",\n    \"claude-2\",\n    \"claude-2.0\",\n    \"claude-2.1\",\n    \"claude-3-haiku-20240307\",\n    \"claude-3-haiku-20240307-vertex\",\n    \"claude-3-sonnet-20240229\",\n    \"claude-3-sonnet-20240229-vertex\",\n    \"claude-3-opus-20240229\",\n    \"claude-instant-1\",\n    \"claude-instant-1.2\",\n)\n\nOPENAI_MODEL_LIST = (\n    \"gpt-3.5-turbo\",\n    \"gpt-3.5-turbo-0301\",\n    \"gpt-3.5-turbo-0613\",\n    \"gpt-3.5-turbo-1106\",\n    \"gpt-3.5-turbo-0125\",\n    \"gpt-4\",\n    \"gpt-4-0314\",\n    \"gpt-4-0613\",\n    \"gpt-4-turbo\",\n    \"gpt-4-1106-preview\",\n    \"gpt-4-0125-preview\",\n    \"gpt-4-turbo-browsing\",\n    \"gpt-4-turbo-2024-04-09\",\n)\n\n\nclass BaseModelAdapter:\n    \"\"\"The base and the default model adapter.\"\"\"\n\n    use_fast_tokenizer = True\n\n    def match(self, model_path: str):\n        return True\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path,\n                use_fast=self.use_fast_tokenizer,\n                revision=revision,\n                trust_remote_code=True,\n            )\n        except TypeError:\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path, use_fast=False, revision=revision, trust_remote_code=True\n            )\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                low_cpu_mem_usage=True,\n                trust_remote_code=True,\n                **from_pretrained_kwargs,\n            )\n        except NameError:\n            model = AutoModel.from_pretrained(\n                model_path,\n                low_cpu_mem_usage=True,\n                trust_remote_code=True,\n                **from_pretrained_kwargs,\n            )\n        return model, tokenizer\n\n    def load_compress_model(self, model_path, device, torch_dtype, revision=\"main\"):\n        return load_compress_model(\n            model_path,\n            device,\n            torch_dtype,\n            use_fast=self.use_fast_tokenizer,\n            revision=revision,\n        )\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"one_shot\")\n\n\n# A global registry for all model adapters\n# TODO (lmzheng): make it a priority queue.\nmodel_adapters: List[BaseModelAdapter] = []\n\n\ndef register_model_adapter(cls):\n    \"\"\"Register a model adapter.\"\"\"\n    model_adapters.append(cls())\n\n\n@cache\ndef get_model_adapter(model_path: str) -> BaseModelAdapter:\n    \"\"\"Get a model adapter for a model_path.\"\"\"\n    model_path_basename = os.path.basename(os.path.normpath(model_path))\n\n    # Try the basename of model_path at first\n    for adapter in model_adapters:\n        if adapter.match(model_path_basename) and type(adapter) != BaseModelAdapter:\n            return adapter\n\n    # Then try the full path\n    for adapter in model_adapters:\n        if adapter.match(model_path):\n            return adapter\n\n    raise ValueError(f\"No valid model adapter for {model_path}\")\n\n\ndef raise_warning_for_incompatible_cpu_offloading_configuration(\n    device: str, load_8bit: bool, cpu_offloading: bool\n):\n    if cpu_offloading:\n        if not load_8bit:\n            warnings.warn(\n                \"The cpu-offloading feature can only be used while also using 8-bit-quantization.\\n\"\n                \"Use '--load-8bit' to enable 8-bit-quantization\\n\"\n                \"Continuing without cpu-offloading enabled\\n\"\n            )\n            return False\n        if not \"linux\" in sys.platform:\n            warnings.warn(\n                \"CPU-offloading is only supported on linux-systems due to the limited compatability with the bitsandbytes-package\\n\"\n                \"Continuing without cpu-offloading enabled\\n\"\n            )\n            return False\n        if device != \"cuda\":\n            warnings.warn(\n                \"CPU-offloading is only enabled when using CUDA-devices\\n\"\n                \"Continuing without cpu-offloading enabled\\n\"\n            )\n            return False\n    return cpu_offloading\n\n\ndef load_model(\n    model_path: str,\n    device: str = \"cuda\",\n    num_gpus: int = 1,\n    max_gpu_memory: Optional[str] = None,\n    dtype: Optional[torch.dtype] = None,\n    load_8bit: bool = False,\n    cpu_offloading: bool = False,\n    gptq_config: Optional[GptqConfig] = None,\n    awq_config: Optional[AWQConfig] = None,\n    exllama_config: Optional[ExllamaConfig] = None,\n    xft_config: Optional[XftConfig] = None,\n    revision: str = \"main\",\n    debug: bool = False,\n):\n    \"\"\"Load a model from Hugging Face.\"\"\"\n    import accelerate\n\n    # get model adapter\n    adapter = get_model_adapter(model_path)\n\n    # Handle device mapping\n    cpu_offloading = raise_warning_for_incompatible_cpu_offloading_configuration(\n        device, load_8bit, cpu_offloading\n    )\n    if device == \"cpu\":\n        kwargs = {\"torch_dtype\": torch.float32}\n        if CPU_ISA in [\"avx512_bf16\", \"amx\"]:\n            try:\n                import intel_extension_for_pytorch as ipex\n\n                kwargs = {\"torch_dtype\": torch.bfloat16}\n            except ImportError:\n                warnings.warn(\n                    \"Intel Extension for PyTorch is not installed, it can be installed to accelerate cpu inference\"\n                )\n    elif device == \"cuda\":\n        kwargs = {\"torch_dtype\": torch.float16}\n        if num_gpus != 1:\n            kwargs[\"device_map\"] = \"auto\"\n            if max_gpu_memory is None:\n                kwargs[\n                    \"device_map\"\n                ] = \"sequential\"  # This is important for not the same VRAM sizes\n                available_gpu_memory = get_gpu_memory(num_gpus)\n                kwargs[\"max_memory\"] = {\n                    i: str(int(available_gpu_memory[i] * 0.85)) + \"GiB\"\n                    for i in range(num_gpus)\n                }\n            else:\n                kwargs[\"max_memory\"] = {i: max_gpu_memory for i in range(num_gpus)}\n    elif device == \"mps\":\n        kwargs = {\"torch_dtype\": torch.float16}\n        import transformers\n\n        version = tuple(int(v) for v in transformers.__version__.split(\".\"))\n        if version < (4, 35, 0):\n            # NOTE: Recent transformers library seems to fix the mps issue, also\n            # it has made some changes causing compatibility issues with our\n            # original patch. So we only apply the patch for older versions.\n\n            # Avoid bugs in mps backend by not using in-place operations.\n            replace_llama_attn_with_non_inplace_operations()\n    elif device == \"xpu\":\n        kwargs = {\"torch_dtype\": torch.bfloat16}\n        # Try to load ipex, while it looks unused, it links into torch for xpu support\n        try:\n            import intel_extension_for_pytorch as ipex\n        except ImportError:\n            warnings.warn(\n                \"Intel Extension for PyTorch is not installed, but is required for xpu inference.\"\n            )\n    elif device == \"npu\":\n        kwargs = {\"torch_dtype\": torch.float16}\n        # Try to load ipex, while it looks unused, it links into torch for xpu support\n        try:\n            import torch_npu\n        except ImportError:\n            warnings.warn(\"Ascend Extension for PyTorch is not installed.\")\n    else:\n        raise ValueError(f\"Invalid device: {device}\")\n\n    if cpu_offloading:\n        # raises an error on incompatible platforms\n        from transformers import BitsAndBytesConfig\n\n        if \"max_memory\" in kwargs:\n            kwargs[\"max_memory\"][\"cpu\"] = (\n                str(math.floor(psutil.virtual_memory().available / 2**20)) + \"Mib\"\n            )\n        kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n            load_in_8bit_fp32_cpu_offload=cpu_offloading\n        )\n        kwargs[\"load_in_8bit\"] = load_8bit\n    elif load_8bit:\n        if num_gpus != 1:\n            warnings.warn(\n                \"8-bit quantization is not supported for multi-gpu inference.\"\n            )\n        else:\n            model, tokenizer = adapter.load_compress_model(\n                model_path=model_path,\n                device=device,\n                torch_dtype=kwargs[\"torch_dtype\"],\n                revision=revision,\n            )\n            if debug:\n                print(model)\n            return model, tokenizer\n    elif awq_config and awq_config.wbits < 16:\n        assert (\n            awq_config.wbits == 4\n        ), \"Currently we only support 4-bit inference for AWQ.\"\n        model, tokenizer = load_awq_quantized(model_path, awq_config, device)\n        if num_gpus != 1:\n            device_map = accelerate.infer_auto_device_map(\n                model,\n                max_memory=kwargs[\"max_memory\"],\n                no_split_module_classes=[\n                    \"OPTDecoderLayer\",\n                    \"LlamaDecoderLayer\",\n                    \"BloomBlock\",\n                    \"MPTBlock\",\n                    \"DecoderLayer\",\n                ],\n            )\n            model = accelerate.dispatch_model(\n                model, device_map=device_map, offload_buffers=True\n            )\n        else:\n            model.to(device)\n        return model, tokenizer\n    elif gptq_config and gptq_config.wbits < 16:\n        model, tokenizer = load_gptq_quantized(model_path, gptq_config)\n        if num_gpus != 1:\n            device_map = accelerate.infer_auto_device_map(\n                model,\n                max_memory=kwargs[\"max_memory\"],\n                no_split_module_classes=[\"LlamaDecoderLayer\"],\n            )\n            model = accelerate.dispatch_model(\n                model, device_map=device_map, offload_buffers=True\n            )\n        else:\n            model.to(device)\n        return model, tokenizer\n    elif exllama_config:\n        model, tokenizer = load_exllama_model(model_path, exllama_config)\n        return model, tokenizer\n    elif xft_config:\n        model, tokenizer = load_xft_model(model_path, xft_config)\n        return model, tokenizer\n    kwargs[\"revision\"] = revision\n\n    if dtype is not None:  # Overwrite dtype if it is provided in the arguments.\n        kwargs[\"torch_dtype\"] = dtype\n\n    if os.environ.get(\"FASTCHAT_USE_MODELSCOPE\", \"False\").lower() == \"true\":\n        # download model from ModelScope hub,\n        # lazy import so that modelscope is not required for normal use.\n        try:\n            from modelscope.hub.snapshot_download import snapshot_download\n\n            if not os.path.exists(model_path):\n                model_path = snapshot_download(model_id=model_path, revision=revision)\n        except ImportError as e:\n            warnings.warn(\n                \"Use model from www.modelscope.cn need pip install modelscope\"\n            )\n            raise e\n\n    # Load model\n    model, tokenizer = adapter.load_model(model_path, kwargs)\n\n    if (\n        device == \"cpu\"\n        and kwargs[\"torch_dtype\"] is torch.bfloat16\n        and CPU_ISA is not None\n    ):\n        model = ipex.optimize(model, dtype=kwargs[\"torch_dtype\"])\n\n    if (device == \"cuda\" and num_gpus == 1 and not cpu_offloading) or device in (\n        \"mps\",\n        \"xpu\",\n        \"npu\",\n    ):\n        model.to(device)\n\n    if device == \"xpu\":\n        model = torch.xpu.optimize(model, dtype=kwargs[\"torch_dtype\"], inplace=True)\n\n    if debug:\n        print(model)\n\n    return model, tokenizer\n\n\ndef get_conversation_template(model_path: str) -> Conversation:\n    \"\"\"Get the default conversation template.\"\"\"\n    adapter = get_model_adapter(model_path)\n    return adapter.get_default_conv_template(model_path)\n\n\ndef get_generate_stream_function(model: torch.nn.Module, model_path: str):\n    \"\"\"Get the generate_stream function for inference.\"\"\"\n    from fastchat.serve.inference import generate_stream\n\n    model_type = str(type(model)).lower()\n    is_peft = \"peft\" in model_type\n    is_chatglm = \"chatglm\" in model_type\n    is_falcon = \"rwforcausallm\" in model_type\n    is_codet5p = \"codet5p\" in model_type\n    is_exllama = \"exllama\" in model_type\n    is_xft = \"xft\" in model_type\n    is_yuan = \"yuan\" in model_type\n    is_cllm = \"consistency-llm\" in model_path.lower()\n\n    if is_chatglm:\n        return generate_stream_chatglm\n    elif is_falcon:\n        return generate_stream_falcon\n    elif is_codet5p:\n        return generate_stream_codet5p\n    elif is_exllama:\n        return generate_stream_exllama\n    elif is_xft:\n        return generate_stream_xft\n    elif is_yuan:\n        return generate_stream_yuan2\n    elif is_cllm:\n        return generate_stream_cllm\n\n    elif peft_share_base_weights and is_peft:\n        # Return a curried stream function that loads the right adapter\n        # according to the model_name available in this context.  This ensures\n        # the right weights are available.\n        @torch.inference_mode()\n        def generate_stream_peft(\n            model,\n            tokenizer,\n            params: Dict,\n            device: str,\n            context_len: int,\n            stream_interval: int = 2,\n            judge_sent_end: bool = False,\n        ):\n            model.set_adapter(model_path)\n            base_model_type = str(type(model.base_model.model))\n            is_chatglm = \"chatglm\" in base_model_type\n            is_falcon = \"rwforcausallm\" in base_model_type\n            is_codet5p = \"codet5p\" in base_model_type\n            is_exllama = \"exllama\" in base_model_type\n            is_xft = \"xft\" in base_model_type\n            is_yuan = \"yuan\" in base_model_type\n            is_cllm = \"consistency-llm\" in model_path.lower()\n\n            generate_stream_function = generate_stream\n            if is_chatglm:\n                generate_stream_function = generate_stream_chatglm\n            elif is_falcon:\n                generate_stream_function = generate_stream_falcon\n            elif is_codet5p:\n                generate_stream_function = generate_stream_codet5p\n            elif is_exllama:\n                generate_stream_function = generate_stream_exllama\n            elif is_xft:\n                generate_stream_function = generate_stream_xft\n            elif is_yuan:\n                generate_stream_function = generate_stream_yuan2\n            elif is_cllm:\n                generate_stream_function = generate_stream_cllm\n            for x in generate_stream_function(\n                model,\n                tokenizer,\n                params,\n                device,\n                context_len,\n                stream_interval,\n                judge_sent_end,\n            ):\n                yield x\n\n        return generate_stream_peft\n    else:\n        return generate_stream\n\n\ndef add_model_args(parser):\n    parser.add_argument(\n        \"--model-path\",\n        type=str,\n        default=\"lmsys/vicuna-7b-v1.5\",\n        help=\"The path to the weights. This can be a local folder or a Hugging Face repo ID.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=\"main\",\n        help=\"Hugging Face Hub model revision identifier\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        choices=[\"cpu\", \"cuda\", \"mps\", \"xpu\", \"npu\"],\n        default=\"cuda\",\n        help=\"The device type\",\n    )\n    parser.add_argument(\n        \"--gpus\",\n        type=str,\n        default=None,\n        help=\"A single GPU like 1 or multiple GPUs like 0,2\",\n    )\n    parser.add_argument(\"--num-gpus\", type=int, default=1)\n    parser.add_argument(\n        \"--max-gpu-memory\",\n        type=str,\n        help=\"The maximum memory per GPU for storing model weights. Use a string like '13Gib'\",\n    )\n    parser.add_argument(\n        \"--dtype\",\n        type=str,\n        choices=[\"float32\", \"float16\", \"bfloat16\"],\n        help=\"Override the default dtype. If not set, it will use float16 on GPU and float32 on CPU.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--load-8bit\", action=\"store_true\", help=\"Use 8-bit quantization\"\n    )\n    parser.add_argument(\n        \"--cpu-offloading\",\n        action=\"store_true\",\n        help=\"Only when using 8-bit quantization: Offload excess weights to the CPU that don't fit on the GPU\",\n    )\n    parser.add_argument(\n        \"--gptq-ckpt\",\n        type=str,\n        default=None,\n        help=\"Used for GPTQ. The path to the local GPTQ checkpoint.\",\n    )\n    parser.add_argument(\n        \"--gptq-wbits\",\n        type=int,\n        default=16,\n        choices=[2, 3, 4, 8, 16],\n        help=\"Used for GPTQ. #bits to use for quantization\",\n    )\n    parser.add_argument(\n        \"--gptq-groupsize\",\n        type=int,\n        default=-1,\n        help=\"Used for GPTQ. Groupsize to use for quantization; default uses full row.\",\n    )\n    parser.add_argument(\n        \"--gptq-act-order\",\n        action=\"store_true\",\n        help=\"Used for GPTQ. Whether to apply the activation order GPTQ heuristic\",\n    )\n    parser.add_argument(\n        \"--awq-ckpt\",\n        type=str,\n        default=None,\n        help=\"Used for AWQ. Load quantized model. The path to the local AWQ checkpoint.\",\n    )\n    parser.add_argument(\n        \"--awq-wbits\",\n        type=int,\n        default=16,\n        choices=[4, 16],\n        help=\"Used for AWQ. #bits to use for AWQ quantization\",\n    )\n    parser.add_argument(\n        \"--awq-groupsize\",\n        type=int,\n        default=-1,\n        help=\"Used for AWQ. Groupsize to use for AWQ quantization; default uses full row.\",\n    )\n    parser.add_argument(\n        \"--enable-exllama\",\n        action=\"store_true\",\n        help=\"Used for exllamabv2. Enable exllamaV2 inference framework.\",\n    )\n    parser.add_argument(\n        \"--exllama-max-seq-len\",\n        type=int,\n        default=4096,\n        help=\"Used for exllamabv2. Max sequence length to use for exllamav2 framework; default 4096 sequence length.\",\n    )\n    parser.add_argument(\n        \"--exllama-gpu-split\",\n        type=str,\n        default=None,\n        help=\"Used for exllamabv2. Comma-separated list of VRAM (in GB) to use per GPU. Example: 20,7,7\",\n    )\n    parser.add_argument(\n        \"--exllama-cache-8bit\",\n        action=\"store_true\",\n        help=\"Used for exllamabv2. Use 8-bit cache to save VRAM.\",\n    )\n    parser.add_argument(\n        \"--enable-xft\",\n        action=\"store_true\",\n        help=\"Used for xFasterTransformer Enable xFasterTransformer inference framework.\",\n    )\n    parser.add_argument(\n        \"--xft-max-seq-len\",\n        type=int,\n        default=4096,\n        help=\"Used for xFasterTransformer. Max sequence length to use for xFasterTransformer framework; default 4096 sequence length.\",\n    )\n    parser.add_argument(\n        \"--xft-dtype\",\n        type=str,\n        choices=[\"fp16\", \"bf16\", \"int8\", \"bf16_fp16\", \"bf16_int8\"],\n        help=\"Override the default dtype. If not set, it will use bfloat16 for first token and float16 next tokens on CPU.\",\n        default=None,\n    )\n\n\ndef remove_parent_directory_name(model_path):\n    \"\"\"Remove parent directory name.\"\"\"\n    if model_path[-1] == \"/\":\n        model_path = model_path[:-1]\n    return model_path.split(\"/\")[-1]\n\n\npeft_model_cache = {}\n\n\nclass PeftModelAdapter:\n    \"\"\"Loads any \"peft\" model and it's base model.\"\"\"\n\n    def match(self, model_path: str):\n        \"\"\"Accepts any model path with \"peft\" in the name\"\"\"\n        if os.path.exists(os.path.join(model_path, \"adapter_config.json\")):\n            return True\n        return \"peft\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        \"\"\"Loads the base model then the (peft) adapter weights\"\"\"\n        from peft import PeftConfig, PeftModel\n\n        config = PeftConfig.from_pretrained(model_path)\n        base_model_path = config.base_model_name_or_path\n        if \"peft\" in base_model_path:\n            raise ValueError(\n                f\"PeftModelAdapter cannot load a base model with 'peft' in the name: {config.base_model_name_or_path}\"\n            )\n\n        # Basic proof of concept for loading peft adapters that share the base\n        # weights.  This is pretty messy because Peft re-writes the underlying\n        # base model and internally stores a map of adapter layers.\n        # So, to make this work we:\n        #  1. Cache the first peft model loaded for a given base models.\n        #  2. Call `load_model` for any follow on Peft models.\n        #  3. Make sure we load the adapters by the model_path.  Why? This is\n        #  what's accessible during inference time.\n        #  4. In get_generate_stream_function, make sure we load the right\n        #  adapter before doing inference.  This *should* be safe when calls\n        #  are blocked the same semaphore.\n        if peft_share_base_weights:\n            if base_model_path in peft_model_cache:\n                model, tokenizer = peft_model_cache[base_model_path]\n                # Super important: make sure we use model_path as the\n                # `adapter_name`.\n                model.load_adapter(model_path, adapter_name=model_path)\n            else:\n                base_adapter = get_model_adapter(base_model_path)\n                base_model, tokenizer = base_adapter.load_model(\n                    base_model_path, from_pretrained_kwargs\n                )\n                # Super important: make sure we use model_path as the\n                # `adapter_name`.\n                model = PeftModel.from_pretrained(\n                    base_model, model_path, adapter_name=model_path\n                )\n                peft_model_cache[base_model_path] = (model, tokenizer)\n            return model, tokenizer\n\n        # In the normal case, load up the base model weights again.\n        base_adapter = get_model_adapter(base_model_path)\n        base_model, tokenizer = base_adapter.load_model(\n            base_model_path, from_pretrained_kwargs\n        )\n        model = PeftModel.from_pretrained(base_model, model_path)\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        \"\"\"Uses the conv template of the base model\"\"\"\n        from peft import PeftConfig, PeftModel\n\n        config = PeftConfig.from_pretrained(model_path)\n        if \"peft\" in config.base_model_name_or_path:\n            raise ValueError(\n                f\"PeftModelAdapter cannot load a base model with 'peft' in the name: {config.base_model_name_or_path}\"\n            )\n        base_model_path = config.base_model_name_or_path\n        base_adapter = get_model_adapter(base_model_path)\n        return base_adapter.get_default_conv_template(config.base_model_name_or_path)\n\n\nclass VicunaAdapter(BaseModelAdapter):\n    \"Model adapter for Vicuna models (e.g., lmsys/vicuna-7b-v1.5)\" \"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"vicuna\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=self.use_fast_tokenizer, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        self.raise_warning_for_old_weights(model)\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"v0\" in remove_parent_directory_name(model_path):\n            return get_conv_template(\"one_shot\")\n        return get_conv_template(\"vicuna_v1.1\")\n\n    def raise_warning_for_old_weights(self, model):\n        if isinstance(model, LlamaForCausalLM) and model.model.vocab_size > 32000:\n            warnings.warn(\n                \"\\nYou are probably using the old Vicuna-v0 model, \"\n                \"which will generate unexpected results with the \"\n                \"current fastchat.\\nYou can try one of the following methods:\\n\"\n                \"1. Upgrade your weights to the new Vicuna-v1.3: https://github.com/lm-sys/FastChat#vicuna-weights.\\n\"\n                \"2. Use the old conversation template by `python3 -m fastchat.serve.cli --model-path /path/to/vicuna-v0 --conv-template one_shot`\\n\"\n                \"3. Downgrade fschat to fschat==0.1.10 (Not recommended).\\n\"\n            )\n\n\nclass AiroborosAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for jondurbin/airoboros-*\"\"\"\n\n    def match(self, model_path: str):\n        if re.search(r\"airoboros|spicyboros\", model_path, re.I):\n            return True\n        return False\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"-3.\" in model_path or \"-3p\" in model_path:\n            return get_conv_template(\"airoboros_v3\")\n        if \"spicyboros\" in model_path or re.search(r\"-(2\\.[2-9]+)\", model_path):\n            return get_conv_template(\"airoboros_v2\")\n        return get_conv_template(\"airoboros_v1\")\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        if \"mpt\" not in model_path.lower():\n            return super().load_model(model_path, from_pretrained_kwargs)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            max_seq_len=8192,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, use_fast=True\n        )\n        return model, tokenizer\n\n\nclass LongChatAdapter(BaseModelAdapter):\n    \"Model adapter for LongChat models (e.g., lmsys/longchat-7b-16k).\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"longchat\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n\n        # Apply monkey patch, TODO(Dacheng): Add flash attention support\n        config = AutoConfig.from_pretrained(model_path, revision=revision)\n        replace_llama_with_condense(config.rope_scaling[\"factor\"])\n\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=self.use_fast_tokenizer, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"vicuna_v1.1\")\n\n\nclass GoogleT5Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for google/Flan based models, such as Salesforce/codet5p-6b, lmsys/fastchat-t5-3b-v1.0, flan-t5-*, flan-ul2\"\"\"\n\n    def match(self, model_path: str):\n        return any(\n            model_str in model_path.lower()\n            for model_str in [\"flan-\", \"fastchat-t5\", \"codet5p\"]\n        )\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = T5Tokenizer.from_pretrained(model_path, revision=revision)\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n\nclass KoalaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Koala\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"koala\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"koala_v1\")\n\n\nclass AlpacaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Alpaca\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"alpaca\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"alpaca\")\n\n\nclass ChatGLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for THUDM/chatglm-6b, THUDM/chatglm2-6b\"\"\"\n\n    def match(self, model_path: str):\n        return \"chatglm\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        if \"chatglm3\" in model_path.lower():\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path,\n                encode_special_tokens=True,\n                trust_remote_code=True,\n                revision=revision,\n            )\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path, trust_remote_code=True, revision=revision\n            )\n        model = AutoModel.from_pretrained(\n            model_path, trust_remote_code=True, **from_pretrained_kwargs\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        model_path = model_path.lower()\n        if \"chatglm2\" in model_path.lower():\n            return get_conv_template(\"chatglm2\")\n        if \"chatglm3\" in model_path.lower():\n            return get_conv_template(\"chatglm3\")\n        return get_conv_template(\"chatglm\")\n\n\nclass CodeGeexAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for THUDM/codegeex-6b, THUDM/codegeex2-6b\"\"\"\n\n    def match(self, model_path: str):\n        return \"codegeex\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        model = AutoModel.from_pretrained(\n            model_path, trust_remote_code=True, **from_pretrained_kwargs\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"codegeex\")\n\n\nclass DollyV2Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for databricks/dolly-v2-12b\"\"\"\n\n    def match(self, model_path: str):\n        return \"dolly-v2\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(model_path, revision=revision)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        # 50277 means \"### End\"\n        tokenizer.eos_token_id = 50277\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"dolly_v2\")\n\n\nclass OasstPythiaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\"\"\n\n    def match(self, model_path: str):\n        model_path = model_path.lower()\n        return \"oasst\" in model_path and \"pythia\" in model_path\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"oasst_pythia\")\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n\nclass OasstLLaMAAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenAssistant/oasst-sft-7-llama-30b\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        model_path = model_path.lower()\n        if \"openassistant-sft-7-llama-30b-hf\" in model_path:\n            return True\n        return \"oasst\" in model_path and \"pythia\" not in model_path\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"oasst_llama\")\n\n\nclass OpenChat35Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenChat 3.5 (e.g. openchat/openchat_3.5)\"\"\"\n\n    def match(self, model_path: str):\n        if \"openchat\" in model_path.lower() and \"3.5\" in model_path.lower():\n            return True\n        elif \"starling-lm\" in model_path.lower():\n            return True\n        return False\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"openchat_3.5\")\n\n\nclass TenyxChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for TenyxChat (e.g. tenyx/TenyxChat-7B-v1)\"\"\"\n\n    def match(self, model_path: str):\n        return \"tenyxchat\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"tenyxchat\")\n\n\nclass PythiaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for any EleutherAI/pythia model\"\"\"\n\n    def match(self, model_path: str):\n        return \"pythia\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n\nclass StableLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for StabilityAI/stablelm-tuned-alpha-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"stablelm\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"stablelm\")\n\n\nclass MPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for MPT series (mosaicml/mpt-7b-chat, mosaicml/mpt-30b-chat)\"\"\"\n\n    def match(self, model_path: str):\n        model_path = model_path.lower()\n        return \"mpt\" in model_path and not \"airoboros\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            max_seq_len=8192,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        model_path = model_path.lower()\n        if \"mpt-7b-chat\" in model_path:\n            return get_conv_template(\"mpt-7b-chat\")\n        elif \"mpt-30b-chat\" in model_path:\n            return get_conv_template(\"mpt-30b-chat\")\n        elif \"mpt-30b-instruct\" in model_path:\n            return get_conv_template(\"mpt-30b-instruct\")\n        else:\n            print(\n                \"Warning: Loading base MPT model with `zero_shot` conversation configuration.  \"\n                \"If this is not desired, inspect model configurations and names.\"\n            )\n            return get_conv_template(\"zero_shot\")\n\n\nclass BaizeAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for project-baize/baize-v2-7b\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"baize\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"baize\")\n\n\nclass RwkvAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for BlinkDL/RWKV-4-Raven\"\"\"\n\n    def match(self, model_path: str):\n        return \"rwkv-4\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        from fastchat.model.rwkv_model import RwkvModel\n\n        model = RwkvModel(model_path)\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            \"EleutherAI/pythia-160m\", revision=revision\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"rwkv\")\n\n\nclass OpenBuddyAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenBuddy/openbuddy-7b-v1.1-bf16-enc\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"openbuddy\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"openbuddy\")\n\n\nclass PhoenixAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for FreedomIntelligence/phoenix-inst-chat-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"phoenix\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"phoenix\")\n\n\nclass ReaLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for FreedomIntelligence/ReaLM-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"ReaLM\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, low_cpu_mem_usage=True, **from_pretrained_kwargs\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"ReaLM-7b-v1\")\n\n\nclass ChatGPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for ChatGPT\"\"\"\n\n    def match(self, model_path: str):\n        return model_path in OPENAI_MODEL_LIST\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"browsing\" in model_path:\n            return get_conv_template(\"api_based_default\")\n        if \"gpt-4-turbo-2024-04-09\" in model_path:\n            return get_conv_template(\"gpt-4-turbo-2024-04-09\")\n        return get_conv_template(\"chatgpt\")\n\n\nclass AzureOpenAIAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Azure OpenAI\"\"\"\n\n    def match(self, model_path: str):\n        return model_path in (\"azure-gpt-35-turbo\", \"azure-gpt-4\")\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"chatgpt\")\n\n\nclass PplxAIAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Perplexity AI\"\"\"\n\n    def match(self, model_path: str):\n        return model_path in (\n            \"pplx-7b-online\",\n            \"pplx-70b-online\",\n        )\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"pplxai\")\n\n\nclass ClaudeAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Claude\"\"\"\n\n    def match(self, model_path: str):\n        return model_path in ANTHROPIC_MODEL_LIST\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"claude-3-haiku\" in model_path:\n            return get_conv_template(\"claude-3-haiku-20240307\")\n        if \"claude-3-sonnet\" in model_path:\n            return get_conv_template(\"claude-3-sonnet-20240229\")\n        if \"claude-3-opus\" in model_path:\n            return get_conv_template(\"claude-3-opus-20240229\")\n        return get_conv_template(\"claude\")\n\n\nclass BardAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Bard\"\"\"\n\n    def match(self, model_path: str):\n        return model_path == \"bard\"\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"bard\")\n\n\nclass PaLM2Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for PaLM2\"\"\"\n\n    def match(self, model_path: str):\n        return model_path == \"palm-2\"\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"bard\")\n\n\nclass GeminiAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Gemini\"\"\"\n\n    def match(self, model_path: str):\n        return \"gemini\" in model_path.lower() or \"bard\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"gemini\")\n\n\nclass GeminiDevAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Gemini 1.5 Pro\"\"\"\n\n    def match(self, model_path: str):\n        return \"gemini-1.5-pro\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"gemini-dev\")\n\n\nclass BiLLaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Neutralzz/BiLLa-7B-SFT\"\"\"\n\n    def match(self, model_path: str):\n        return \"billa\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"billa\")\n\n\nclass RedPajamaINCITEAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for togethercomputer/RedPajama-INCITE-7B-Chat\"\"\"\n\n    def match(self, model_path: str):\n        return \"redpajama-incite\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(model_path, revision=revision)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"redpajama-incite\")\n\n\nclass H2OGPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"h2ogpt\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"h2ogpt\")\n\n\nclass RobinAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for LMFlow/Full-Robin-7b-v2\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"robin\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"Robin\")\n\n\nclass SnoozyAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for nomic-ai/gpt4all-13b-snoozy\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        model_path = model_path.lower()\n        return \"gpt4all\" in model_path and \"snoozy\" in model_path\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"snoozy\")\n\n\nclass WizardLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for WizardLM/WizardLM-13B-V1.0\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"wizardlm\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        model_path = model_path.lower()\n        if \"13b\" in model_path or \"30b\" in model_path or \"70b\" in model_path:\n            return get_conv_template(\"vicuna_v1.1\")\n        else:\n            # TODO: use the recommended template for 7B\n            # (https://huggingface.co/WizardLM/WizardLM-13B-V1.0)\n            return get_conv_template(\"one_shot\")\n\n\nclass ManticoreAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for openaccess-ai-collective/manticore-13b-chat-pyg\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"manticore\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"manticore\")\n\n\nclass GuanacoAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for timdettmers/guanaco-33b-merged\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"guanaco\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=self.use_fast_tokenizer, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, low_cpu_mem_usage=True, **from_pretrained_kwargs\n        )\n        # Fix a bug in tokenizer config\n        tokenizer.eos_token_id = model.config.eos_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"zero_shot\")\n\n\nclass ChangGPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for lcw99/polyglot-ko-12.8b-chang-instruct-chat\"\"\"\n\n    def match(self, model_path: str):\n        model_path = model_path.lower()\n        return \"polyglot\" in model_path and \"chang\" in model_path\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"polyglot_changgpt\")\n\n\nclass CamelAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for camel-ai/CAMEL-13B-Combined-Data\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"camel\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"vicuna_v1.1\")\n\n\nclass TuluAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for allenai/tulu-30b\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"tulu\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"tulu\")\n\n\nclass FalconAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for tiiuae/falcon-40b\"\"\"\n\n    def match(self, model_path: str):\n        return \"falcon\" in model_path.lower() and \"chat\" not in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        # Strongly suggest using bf16, which is recommended by the author of Falcon\n        tokenizer = AutoTokenizer.from_pretrained(model_path, revision=revision)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        # In Falcon tokenizer config and special config there is not any pad token\n        # Setting `pad_token_id` to 9, which corresponds to special token '>>SUFFIX<<'\n        tokenizer.pad_token_id = 9\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"falcon\")\n\n\nclass FalconChatAdapter(BaseModelAdapter):\n    def match(self, model_path: str):\n        return \"falcon\" in model_path.lower() and \"chat\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"falcon-chat\")\n\n\nclass TigerBotAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for TigerResearch/tigerbot-7b-sft\"\"\"\n\n    def match(self, model_path: str):\n        return \"tigerbot\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"tigerbot\")\n\n\nclass BaichuanAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Baichuan models (e.g., baichuan-inc/Baichuan-7B)\"\"\"\n\n    def match(self, model_path: str):\n        return \"baichuan\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        # for Baichuan-13B-Chat\n        if \"chat\" in model_path.lower():\n            if \"baichuan2\" in model_path.lower():\n                return get_conv_template(\"baichuan2-chat\")\n            return get_conv_template(\"baichuan-chat\")\n        return get_conv_template(\"zero_shot\")\n\n\nclass XGenAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Salesforce/xgen-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"xgen\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        model.config.eos_token_id = 50256\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"xgen\")\n\n\nclass NousHermesAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for NousResearch/Nous-Hermes-13b\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"nous-hermes\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"alpaca\")\n\n\nclass InternLMChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for internlm/internlm-chat-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"internlm\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        model = model.eval()\n        if \"8k\" in model_path.lower():\n            model.config.max_sequence_length = 8192\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"internlm-chat\")\n\n\nclass StarChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for HuggingFaceH4/starchat-beta\"\"\"\n\n    def match(self, model_path: str):\n        return \"starchat\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"starchat\")\n\n\nclass MistralAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Mistral AI models\"\"\"\n\n    def match(self, model_path: str):\n        return \"mistral\" in model_path.lower() or \"mixtral\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"mistral\")\n\n\nclass Llama2Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for Llama-2 (e.g., meta-llama/Llama-2-7b-hf)\"\"\"\n\n    def match(self, model_path: str):\n        return \"llama-2\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"llama-2\")\n\n\nclass Llama3Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for Llama-3 (e.g., meta-llama/Meta-Llama-3-8B-Instruct)\"\"\"\n\n    def match(self, model_path: str):\n        return \"llama-3\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"llama-3\")\n\n\nclass CuteGPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for CuteGPT\"\"\"\n\n    def match(self, model_path: str):\n        return \"cutegpt\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = LlamaTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, low_cpu_mem_usage=True, **from_pretrained_kwargs\n        )\n        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"<end>\")\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.eos_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"cutegpt\")\n\n\nclass OpenOrcaAdapter(BaseModelAdapter):\n    \"\"\"Model adapter for Open-Orca models which may use different prompt templates\n    - (e.g. Open-Orca/OpenOrcaxOpenChat-Preview2-13B, Open-Orca/Mistral-7B-OpenOrca)\n    - `OpenOrcaxOpenChat-Preview2-13B` uses their \"OpenChat Llama2 V1\" prompt template.\n        - [Open-Orca/OpenOrcaxOpenChat-Preview2-13B #Prompt Template](https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B#prompt-template)\n    - `Mistral-7B-OpenOrca` uses the [OpenAI's Chat Markup Language (ChatML)](https://github.com/openai/openai-python/blob/main/chatml.md)\n        format, with <|im_start|> and <|im_end|> tokens added to support this.\n        - [Open-Orca/Mistral-7B-OpenOrca #Prompt Template](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca#prompt-template)\n    \"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return (\n            \"mistral-7b-openorca\" in model_path.lower()\n            or \"openorca\" in model_path.lower()\n        )\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=self.use_fast_tokenizer, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        ).eval()\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"mistral-7b-openorca\" in model_path.lower():\n            return get_conv_template(\"mistral-7b-openorca\")\n        return get_conv_template(\"open-orca\")\n\n\nclass DolphinAdapter(OpenOrcaAdapter):\n    \"\"\"Model adapter for ehartford/dolphin-2.2.1-mistral-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"dolphin\" in model_path.lower() and \"mistral\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"dolphin-2.2.1-mistral-7b\")\n\n\nclass Hermes2Adapter(BaseModelAdapter):\n    \"\"\"Model adapter for teknium/OpenHermes-2.5-Mistral-7B and teknium/OpenHermes-2-Mistral-7B models\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return any(\n            model_str in model_path.lower()\n            for model_str in [\"openhermes-2.5-mistral-7b\", \"openhermes-2-mistral-7b\"]\n        )\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=self.use_fast_tokenizer, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        ).eval()\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"OpenHermes-2.5-Mistral-7B\")\n\n\nclass NousHermes2MixtralAdapter(BaseModelAdapter):\n    \"\"\"Model adapter for NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO model\"\"\"\n\n    def match(self, model_path: str):\n        return any(\n            model_str in model_path.lower()\n            for model_str in [\n                \"nous-hermes-2-mixtral-8x7b-dpo\",\n                \"nous-hermes-2-mixtral-8x7b-sft\",\n            ]\n        )\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"Nous-Hermes-2-Mixtral-8x7B-DPO\")\n\n\nclass WizardCoderAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for WizardCoder (e.g., WizardLM/WizardCoder-Python-34B-V1.0)\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"wizardcoder\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        # Same as Alpaca, see :\n        # https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/src/inference_wizardcoder.py#L60\n        return get_conv_template(\"alpaca\")\n\n\nclass QwenChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Qwen/Qwen-7B-Chat\n    To run this model, you need to ensure additional flash attention installation:\n    ``` bash\n    git clone https://github.com/Dao-AILab/flash-attention\n    cd flash-attention && pip install .\n    pip install csrc/layer_norm\n    pip install csrc/rotary\n    ```\n\n    Since from 2.0, the following change happened\n    - `flash_attn_unpadded_func` -> `flash_attn_varlen_func`\n    - `flash_attn_unpadded_qkvpacked_func` -> `flash_attn_varlen_qkvpacked_func`\n    - `flash_attn_unpadded_kvpacked_func` -> `flash_attn_varlen_kvpacked_func`\n    You may need to revise the code in: https://huggingface.co/Qwen/Qwen-7B-Chat/blob/main/modeling_qwen.py#L69\n    to from flash_attn.flash_attn_interface import flash_attn_varlen_func as flash_attn_unpadded_func\n    \"\"\"\n\n    def match(self, model_path: str):\n        return \"qwen\" in model_path.lower()\n\n    def float_set(self, config, option):\n        config.bf16 = False\n        config.fp16 = False\n        config.fp32 = False\n\n        if option == \"bf16\":\n            config.bf16 = True\n        elif option == \"fp16\":\n            config.fp16 = True\n        elif option == \"fp32\":\n            config.fp32 = True\n        else:\n            print(\"Invalid option. Please choose one from 'bf16', 'fp16' and 'fp32'.\")\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        from transformers.generation import GenerationConfig\n\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        config = AutoConfig.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n        )\n        # NOTE: if you use the old version of model file, please remove the comments below\n        # config.use_flash_attn = False\n        self.float_set(config, \"fp16\")\n        generation_config = GenerationConfig.from_pretrained(\n            model_path, trust_remote_code=True\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            config=config,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        ).eval()\n        if hasattr(model.config, \"use_dynamic_ntk\") and model.config.use_dynamic_ntk:\n            model.config.max_sequence_length = 16384\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        tokenizer.eos_token_id = config.eos_token_id\n        tokenizer.bos_token_id = config.bos_token_id\n        tokenizer.pad_token_id = generation_config.pad_token_id\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.bos_token_id = tokenizer.bos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"qwen-7b-chat\")\n\n\nclass SmaugChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for abacusai/Smaug-2-72B.\"\"\"\n\n    def match(self, model_path: str):\n        return \"smaug\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"qwen-7b-chat\")\n\n\nclass BGEAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for BGE (e.g., BAAI/bge-large-en-v1.5)\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"bge\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModel.from_pretrained(\n            model_path,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        if hasattr(model.config, \"max_position_embeddings\") and hasattr(\n            tokenizer, \"model_max_length\"\n        ):\n            model.config.max_sequence_length = min(\n                model.config.max_position_embeddings, tokenizer.model_max_length\n            )\n        model.use_cls_pooling = True\n        model.eval()\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"one_shot\")\n\n\nclass E5Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for E5 (e.g., intfloat/e5-large-v2)\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"e5-\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModel.from_pretrained(\n            model_path,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        if hasattr(model.config, \"max_position_embeddings\") and hasattr(\n            tokenizer, \"model_max_length\"\n        ):\n            model.config.max_sequence_length = min(\n                model.config.max_position_embeddings, tokenizer.model_max_length\n            )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"one_shot\")\n\n\nclass AquilaChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for BAAI/Aquila\n\n    Now supports:\n    - BAAI/AquilaChat-7B\n    - BAAI/AquilaChat2-7B\n    - BAAI/AquilaChat2-34B\n    \"\"\"\n\n    def match(self, model_path: str):\n        return \"aquila\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        model = model.eval()\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        model_path = model_path.lower()\n        # See: https://huggingface.co/BAAI/AquilaChat2-34B/blob/4608b75855334b93329a771aee03869dbf7d88cc/predict.py#L347\n        if \"aquilachat2\" in model_path:\n            if \"16k\" in model_path:\n                return get_conv_template(\"aquila\")\n            elif \"34b\" in model_path:\n                return get_conv_template(\"aquila-legacy\")\n            else:\n                return get_conv_template(\"aquila-v1\")\n        else:\n            return get_conv_template(\"aquila-chat\")\n\n\nclass Lamma2ChineseAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for FlagAlpha/LLama2-Chinese sft\"\"\"\n\n    def match(self, model_path: str):\n        return \"llama2-chinese\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"llama2-chinese\")\n\n\nclass Lamma2ChineseAlpacaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for ymcui/Chinese-LLaMA-Alpaca sft\"\"\"\n\n    def match(self, model_path: str):\n        return \"chinese-alpaca\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"chinese-alpaca2\")\n\n\nclass VigogneAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for vigogne (e.g., bofenghuang/vigogne-2-7b-chat)\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return bool(re.search(r\"vigogne|vigostral\", model_path, re.I))\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            use_fast=self.use_fast_tokenizer,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        ).eval()\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"chat\" in model_path.lower():\n            if \"vigostral\" in model_path.lower():\n                return get_conv_template(\"vigogne_chat_v3\")\n            return get_conv_template(\"vigogne_chat_v2\")\n        return get_conv_template(\"vigogne_instruct\")\n\n\nclass OpenLLaMaOpenInstructAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenLLaMa-Open-Instruct (e.g., VMware/open-llama-7b-open-instruct)\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return (\n            \"open-llama\" in model_path.lower() and \"open-instruct\" in model_path.lower()\n        )\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            use_fast=self.use_fast_tokenizer,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        ).eval()\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"alpaca\")\n\n\nclass CodeLlamaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for CodeLlama (e.g., codellama/CodeLlama-34b-hf)\"\"\"\n\n    def match(self, model_path: str):\n        return \"codellama\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"llama-2\")\n\n\nclass StableVicunaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for StableVicuna\"\"\"\n\n    def match(self, model_path: str):\n        return \"stable-vicuna\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"stable-vicuna\")\n\n\nclass PhindCodeLlamaAdapter(CodeLlamaAdapter):\n    \"\"\"The model adapter for Phind-CodeLlama (e.g., Phind/Phind-CodeLlama-34B-v2)\"\"\"\n\n    def match(self, model_path: str):\n        return \"phind-codellama-\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"phind\")\n\n\nclass Llama2ChangAdapter(Llama2Adapter):\n    \"\"\"The model adapter for Llama2-ko-chang (e.g., lcw99/llama2-ko-chang-instruct-chat)\"\"\"\n\n    def match(self, model_path: str):\n        return \"llama2-ko-chang\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"polyglot_changgpt\")\n\n\nclass ZephyrAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Zephyr (e.g. HuggingFaceH4/zephyr-7b-alpha)\"\"\"\n\n    def match(self, model_path: str):\n        return \"zephyr\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"zephyr\")\n\n\nclass NotusAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Notus (e.g. argilla/notus-7b-v1)\"\"\"\n\n    def match(self, model_path: str):\n        return \"notus\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"zephyr\")\n\n\nclass CatPPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for CatPPT (e.g. rishiraj/CatPPT)\"\"\"\n\n    def match(self, model_path: str):\n        return \"catppt\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"catppt\")\n\n\nclass TinyLlamaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for TinyLlama (e.g. TinyLlama/TinyLlama-1.1B-Chat-v1.0)\"\"\"\n\n    def match(self, model_path: str):\n        return \"tinyllama\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"TinyLlama\")\n\n\nclass XwinLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Xwin-LM V0.1 and V0.2 series of models(e.g., Xwin-LM/Xwin-LM-70B-V0.1)\"\"\"\n\n    # use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"xwin-lm\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"vicuna_v1.1\")\n\n\nclass LemurAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenLemur/lemur-70b-chat-v1\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"lemur-70b-chat\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"lemur-70b-chat\")\n\n\nclass PygmalionAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Pygmalion/Metharme series of models(e.g., PygmalionAI/mythalion-13b)\"\"\"\n\n    # use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return bool(\n            re.search(r\"pygmalion|mythalion|metharme\", model_path.lower(), re.I)\n        )\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"metharme\")\n\n\nclass XdanAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for xDAN-AI (e.g. xDAN-AI/xDAN-L1-Chat-RL-v1)\"\"\"\n\n    def match(self, model_path: str):\n        return \"xdan\" in model_path.lower() and \"v1\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"xdan-v1\")\n\n\nclass MicrosoftOrcaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Microsoft/Orca-2 series of models (e.g. Microsoft/Orca-2-7b, Microsoft/Orca-2-13b)\"\"\"\n\n    use_fast_tokenizer = False  # Flag neeeded since tokenizers>=0.13.3 is required for a normal functioning of this module\n\n    def match(self, model_path: str):\n        return \"orca-2\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"orca-2\")\n\n\nclass YiAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Yi models\"\"\"\n\n    def match(self, model_path: str):\n        return \"yi-\" in model_path.lower() and \"chat\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"Yi-34b-chat\")\n\n\nclass DeepseekCoderAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for deepseek-ai's coder models\"\"\"\n\n    def match(self, model_path: str):\n        return \"deepseek-coder\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"deepseek-coder\")\n\n\nclass DeepseekChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for deepseek-ai's chat models\"\"\"\n\n    # Note: that this model will require tokenizer version >= 0.13.3 because the tokenizer class is LlamaTokenizerFast\n\n    def match(self, model_path: str):\n        return \"deepseek-llm\" in model_path.lower() and \"chat\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"deepseek-chat\")\n\n\nclass Yuan2Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for Yuan2.0\"\"\"\n\n    def match(self, model_path: str):\n        return \"yuan2\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        # from_pretrained_kwargs[\"torch_dtype\"] = torch.bfloat16\n        tokenizer = LlamaTokenizer.from_pretrained(\n            model_path,\n            add_eos_token=False,\n            add_bos_token=False,\n            eos_token=\"<eod>\",\n            eod_token=\"<eod>\",\n            sep_token=\"<sep>\",\n            revision=revision,\n        )\n        tokenizer.add_tokens(\n            [\n                \"<sep>\",\n                \"<pad>\",\n                \"<mask>\",\n                \"<predict>\",\n                \"<FIM_SUFFIX>\",\n                \"<FIM_PREFIX>\",\n                \"<FIM_MIDDLE>\",\n                \"<commit_before>\",\n                \"<commit_msg>\",\n                \"<commit_after>\",\n                \"<jupyter_start>\",\n                \"<jupyter_text>\",\n                \"<jupyter_code>\",\n                \"<jupyter_output>\",\n                \"<empty_output>\",\n            ],\n            special_tokens=True,\n        )\n\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            # device_map='auto',\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"yuan2\")\n\n\nclass MetaMathAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for MetaMath models\"\"\"\n\n    def match(self, model_path: str):\n        return \"metamath\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"metamath\")\n\n\nclass BagelAdapter(BaseModelAdapter):\n    \"\"\"Model adapter for jondurbin/bagel-* models\"\"\"\n\n    def match(self, model_path: str):\n        return \"bagel\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"airoboros_v3\")\n\n\nclass SolarAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for upstage/SOLAR-10.7B-Instruct-v1.0\"\"\"\n\n    def match(self, model_path: str):\n        return \"solar-\" in model_path.lower() and \"instruct\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"solar\")\n\n\nclass SteerLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for nvidia/Llama2-70B-SteerLM-Chat\"\"\"\n\n    def match(self, model_path: str):\n        return \"steerlm-chat\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"steerlm\")\n\n\nclass GemmaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for google/gemma\"\"\"\n\n    def match(self, model_path: str):\n        return \"gemma\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"gemma\")\n\n\nclass LlavaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for liuhaotian/llava-v1.5 series of models\"\"\"\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        # TODO(chris): Implement huggingface-compatible load_model\n        pass\n\n    def match(self, model_path: str):\n        return \"llava\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        model_path = model_path.lower()\n        if \"34b\" in model_path:\n            return get_conv_template(\"llava-chatml\")\n\n        return get_conv_template(\"vicuna_v1.1\")\n\n\nclass YuanAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Yuan\"\"\"\n\n    def match(self, model_path: str):\n        return \"yuan\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        tokenizer.add_tokens(\n            [\n                \"<sep>\",\n                \"<pad>\",\n                \"<mask>\",\n                \"<predict>\",\n                \"<FIM_SUFFIX>\",\n                \"<FIM_PREFIX>\",\n                \"<FIM_MIDDLE>\",\n                \"<commit_before>\",\n                \"<commit_msg>\",\n                \"<commit_after>\",\n                \"<jupyter_start>\",\n                \"<jupyter_text>\",\n                \"<jupyter_code>\",\n                \"<jupyter_output>\",\n                \"<empty_output>\",\n            ],\n            special_tokens=True,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"yuan\")\n\n\nclass OlmoAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for allenai/OLMo-7B-Instruct\"\"\"\n\n    def match(self, model_path: str):\n        return \"olmo\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"api_based_default\")\n\n\nclass YandexGPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for YandexGPT\"\"\"\n\n    def match(self, model_path: str):\n        return \"yandexgpt\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"yandexgpt\")\n\n\nclass CllmAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for CLLM\"\"\"\n\n    def match(self, model_path: str):\n        return \"consistency-llm\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        config = AutoConfig.from_pretrained(\n            model_path,\n        )\n\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            model_max_length=2048,\n            padding_side=\"right\",\n        )\n\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            config=config,\n            torch_dtype=torch.bfloat16,\n            low_cpu_mem_usage=True,\n            device_map=\"cuda\",\n        )\n\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"cllm\")\n\n\nclass CohereAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Cohere\"\"\"\n\n    def match(self, model_path: str):\n        return model_path in [\"command-r\"]\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"api_based_default\")\n\n\nclass DBRXAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Cohere\"\"\"\n\n    def match(self, model_path: str):\n        return model_path in [\"dbrx-instruct\"]\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"api_based_default\")\n\n\nclass RekaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Reka\"\"\"\n\n    def match(self, model_path: str):\n        return \"reka\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"api_based_default\")\n\n\n# Note: the registration order matters.\n# The one registered earlier has a higher matching priority.\nregister_model_adapter(PeftModelAdapter)\nregister_model_adapter(StableVicunaAdapter)\nregister_model_adapter(VicunaAdapter)\nregister_model_adapter(AiroborosAdapter)\nregister_model_adapter(LongChatAdapter)\nregister_model_adapter(GoogleT5Adapter)\nregister_model_adapter(KoalaAdapter)\nregister_model_adapter(AlpacaAdapter)\nregister_model_adapter(ChatGLMAdapter)\nregister_model_adapter(CodeGeexAdapter)\nregister_model_adapter(DollyV2Adapter)\nregister_model_adapter(OasstPythiaAdapter)\nregister_model_adapter(OasstLLaMAAdapter)\nregister_model_adapter(OpenChat35Adapter)\nregister_model_adapter(TenyxChatAdapter)\nregister_model_adapter(StableLMAdapter)\nregister_model_adapter(BaizeAdapter)\nregister_model_adapter(RwkvAdapter)\nregister_model_adapter(OpenBuddyAdapter)\nregister_model_adapter(PhoenixAdapter)\nregister_model_adapter(BardAdapter)\nregister_model_adapter(PaLM2Adapter)\nregister_model_adapter(GeminiAdapter)\nregister_model_adapter(GeminiDevAdapter)\nregister_model_adapter(GemmaAdapter)\nregister_model_adapter(ChatGPTAdapter)\nregister_model_adapter(AzureOpenAIAdapter)\nregister_model_adapter(ClaudeAdapter)\nregister_model_adapter(MPTAdapter)\nregister_model_adapter(BiLLaAdapter)\nregister_model_adapter(RedPajamaINCITEAdapter)\nregister_model_adapter(H2OGPTAdapter)\nregister_model_adapter(RobinAdapter)\nregister_model_adapter(SnoozyAdapter)\nregister_model_adapter(WizardLMAdapter)\nregister_model_adapter(ManticoreAdapter)\nregister_model_adapter(GuanacoAdapter)\nregister_model_adapter(CamelAdapter)\nregister_model_adapter(ChangGPTAdapter)\nregister_model_adapter(TuluAdapter)\nregister_model_adapter(FalconChatAdapter)\nregister_model_adapter(FalconAdapter)\nregister_model_adapter(TigerBotAdapter)\nregister_model_adapter(BaichuanAdapter)\nregister_model_adapter(XGenAdapter)\nregister_model_adapter(PythiaAdapter)\nregister_model_adapter(InternLMChatAdapter)\nregister_model_adapter(StarChatAdapter)\nregister_model_adapter(Llama2Adapter)\nregister_model_adapter(Llama3Adapter)\nregister_model_adapter(CuteGPTAdapter)\nregister_model_adapter(OpenOrcaAdapter)\nregister_model_adapter(DolphinAdapter)\nregister_model_adapter(Hermes2Adapter)\nregister_model_adapter(NousHermes2MixtralAdapter)\nregister_model_adapter(NousHermesAdapter)\nregister_model_adapter(MistralAdapter)\nregister_model_adapter(WizardCoderAdapter)\nregister_model_adapter(QwenChatAdapter)\nregister_model_adapter(AquilaChatAdapter)\nregister_model_adapter(BGEAdapter)\nregister_model_adapter(E5Adapter)\nregister_model_adapter(Lamma2ChineseAdapter)\nregister_model_adapter(Lamma2ChineseAlpacaAdapter)\nregister_model_adapter(VigogneAdapter)\nregister_model_adapter(OpenLLaMaOpenInstructAdapter)\nregister_model_adapter(ReaLMAdapter)\nregister_model_adapter(PhindCodeLlamaAdapter)\nregister_model_adapter(CodeLlamaAdapter)\nregister_model_adapter(Llama2ChangAdapter)\nregister_model_adapter(ZephyrAdapter)\nregister_model_adapter(NotusAdapter)\nregister_model_adapter(CatPPTAdapter)\nregister_model_adapter(TinyLlamaAdapter)\nregister_model_adapter(XwinLMAdapter)\nregister_model_adapter(LemurAdapter)\nregister_model_adapter(PygmalionAdapter)\nregister_model_adapter(MicrosoftOrcaAdapter)\nregister_model_adapter(XdanAdapter)\nregister_model_adapter(YiAdapter)\nregister_model_adapter(PplxAIAdapter)\nregister_model_adapter(DeepseekCoderAdapter)\nregister_model_adapter(DeepseekChatAdapter)\nregister_model_adapter(Yuan2Adapter)\nregister_model_adapter(MetaMathAdapter)\nregister_model_adapter(BagelAdapter)\nregister_model_adapter(SolarAdapter)\nregister_model_adapter(SteerLMAdapter)\nregister_model_adapter(LlavaAdapter)\nregister_model_adapter(YuanAdapter)\nregister_model_adapter(OlmoAdapter)\nregister_model_adapter(CohereAdapter)\nregister_model_adapter(DBRXAdapter)\nregister_model_adapter(GemmaAdapter)\nregister_model_adapter(YandexGPTAdapter)\nregister_model_adapter(CllmAdapter)\nregister_model_adapter(RekaAdapter)\nregister_model_adapter(SmaugChatAdapter)\n\n# After all adapters, try the default base adapter.\nregister_model_adapter(BaseModelAdapter)\n", "fastchat/model/monkey_patch_non_inplace.py": "\"\"\"\nMonkey patch the llama implementation in the huggingface/transformers library.\nAvoid bugs in mps backend by not using in-place operations.\n\"\"\"\nimport math\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nimport transformers\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2].clone()\n    x2 = x[..., x.shape[-1] // 2 :].clone()\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    padding_mask: Optional[torch.LongTensor] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n    # [bsz, nh, t, hd]\n\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(\n        self.head_dim\n    )\n\n    if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n        raise ValueError(\n            f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n            f\" {attn_weights.size()}\"\n        )\n\n    if attention_mask is not None:\n        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n            )\n        attn_weights = attn_weights + attention_mask\n        attn_weights = torch.max(\n            attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n        )\n\n    # upcast attention to fp32\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(\n        query_states.dtype\n    )\n    attn_output = torch.matmul(attn_weights, value_states)\n\n    if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n        raise ValueError(\n            f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n            f\" {attn_output.size()}\"\n        )\n\n    attn_output = attn_output.transpose(1, 2)\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\ndef replace_llama_attn_with_non_inplace_operations():\n    \"\"\"Avoid bugs in mps backend by not using in-place operations.\"\"\"\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n", "fastchat/model/model_yuan2.py": "import gc\nfrom threading import Thread\nfrom typing import Iterable\n\nimport torch\nimport transformers\nfrom transformers import TextIteratorStreamer, GenerationConfig\n\nfrom fastchat.utils import is_partial_stop\n\n\n@torch.inference_mode()\ndef generate_stream_yuan2(\n    model,\n    tokenizer,\n    params,\n    device,\n    context_len=2048,\n    stream_interval=2,\n    judge_sent_end=False,\n):\n    prompt = params[\"prompt\"]\n    len_prompt = len(prompt)\n    temperature = float(params.get(\"temperature\", 1))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 0))\n    top_k = int(params.get(\"top_k\", 1))  # -1 means disable\n    max_new_tokens = int(params.get(\"max_new_tokens\", 512))\n    stop_str = params.get(\"stop\", \"<eod>\")\n    echo = bool(params.get(\"echo\", True))\n    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n    stop_token_ids.append(tokenizer(\"<eod>\")[\"input_ids\"][0])\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n\n    max_src_len = context_len - max_new_tokens - 8\n\n    input_ids = input_ids[-max_src_len:]  # truncate from the left\n    attention_mask = attention_mask[-max_src_len:]  # truncate from the left\n    input_echo_len = len(input_ids)\n\n    decode_config = dict(skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, **decode_config)\n\n    generation_config = GenerationConfig(\n        max_new_tokens=max_new_tokens,\n        do_sample=temperature >= 1.2,\n        temperature=temperature,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=10,\n        top_p=top_p,\n        top_k=top_k,\n    )\n\n    generation_kwargs = dict(\n        inputs=input_ids,\n        attention_mask=attention_mask,\n        streamer=streamer,\n        generation_config=generation_config,\n    )\n\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    if echo:\n        # means keep the prompt\n        output = prompt\n    else:\n        output = \"\"\n\n    for i, new_text in enumerate(streamer):\n        output += new_text\n        if i % stream_interval == 0:\n            if echo:\n                rfind_start = len_prompt\n            else:\n                rfind_start = 0\n\n            partially_stopped = False\n            if stop_str:\n                if isinstance(stop_str, str):\n                    pos = output.rfind(stop_str, rfind_start)\n                    if pos != -1:\n                        output = output[:pos]\n                    else:\n                        partially_stopped = is_partial_stop(output, stop_str)\n                elif isinstance(stop_str, Iterable):\n                    for each_stop in stop_str:\n                        pos = output.rfind(each_stop, rfind_start)\n                        if pos != -1:\n                            output = output[:pos]\n                            break\n                        else:\n                            partially_stopped = is_partial_stop(output, each_stop)\n                            if partially_stopped:\n                                break\n                else:\n                    raise ValueError(\"Invalid stop field type.\")\n\n            # prevent yielding partial stop sequence\n            if not partially_stopped:\n                yield {\n                    \"text\": output,\n                    \"usage\": {\n                        \"prompt_tokens\": input_echo_len,\n                        \"completion_tokens\": i,\n                        \"total_tokens\": input_echo_len + i,\n                    },\n                    \"finish_reason\": None,\n                }\n    output = output.strip()\n\n    # finish stream event, which contains finish reason\n    if i == max_new_tokens - 1:\n        finish_reason = \"length\"\n    elif partially_stopped:\n        finish_reason = None\n    else:\n        finish_reason = \"stop\"\n\n    yield {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": finish_reason,\n    }\n\n    # clean\n    gc.collect()\n    torch.cuda.empty_cache()\n    if device == \"xpu\":\n        torch.xpu.empty_cache()\n    if device == \"npu\":\n        torch.npu.empty_cache()\n", "fastchat/model/upload_hub.py": "\"\"\"\nUpload weights to huggingface.\n\nUsage:\npython3 -m fastchat.model.upload_hub --model-path ~/model_weights/vicuna-13b --hub-repo-id lmsys/vicuna-13b-v1.3\n\"\"\"\nimport argparse\nimport tempfile\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef upload_hub(model_path, hub_repo_id, component, private):\n    if component == \"all\":\n        components = [\"model\", \"tokenizer\"]\n    else:\n        components = [component]\n\n    kwargs = {\"push_to_hub\": True, \"repo_id\": hub_repo_id, \"private\": args.private}\n\n    if \"model\" in components:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n        )\n        with tempfile.TemporaryDirectory() as tmp_path:\n            model.save_pretrained(tmp_path, **kwargs)\n\n    if \"tokenizer\" in components:\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n        with tempfile.TemporaryDirectory() as tmp_path:\n            tokenizer.save_pretrained(tmp_path, **kwargs)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, required=True)\n    parser.add_argument(\"--hub-repo-id\", type=str, required=True)\n    parser.add_argument(\n        \"--component\", type=str, choices=[\"all\", \"model\", \"tokenizer\"], default=\"all\"\n    )\n    parser.add_argument(\"--private\", action=\"store_true\")\n    args = parser.parse_args()\n\n    upload_hub(args.model_path, args.hub_repo_id, args.component, args.private)\n", "fastchat/model/model_falcon.py": "import gc\nfrom threading import Thread\nfrom typing import Iterable\n\nimport torch\nimport transformers\nfrom transformers import TextIteratorStreamer, GenerationConfig\n\nfrom fastchat.utils import is_partial_stop\n\n\n@torch.inference_mode()\ndef generate_stream_falcon(\n    model,\n    tokenizer,\n    params,\n    device,\n    context_len=2048,\n    stream_interval=2,\n    judge_sent_end=False,\n):\n    prompt = params[\"prompt\"]\n    len_prompt = len(prompt)\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    top_k = int(params.get(\"top_k\", 50))  # -1 means disable\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n    stop_str = params.get(\"stop\", None)\n    echo = bool(params.get(\"echo\", True))\n    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n    stop_token_ids.append(tokenizer.eos_token_id)\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n\n    max_src_len = context_len - max_new_tokens - 8\n\n    input_ids = input_ids[-max_src_len:]  # truncate from the left\n    attention_mask = attention_mask[-max_src_len:]  # truncate from the left\n    input_echo_len = len(input_ids)\n\n    decode_config = dict(skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, **decode_config)\n\n    generation_config = GenerationConfig(\n        max_new_tokens=max_new_tokens,\n        do_sample=temperature >= 1e-5,\n        temperature=temperature,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=10,\n        top_p=top_p,\n        top_k=top_k,\n        eos_token_id=stop_token_ids,\n    )\n\n    generation_kwargs = dict(\n        inputs=input_ids,\n        attention_mask=attention_mask,\n        streamer=streamer,\n        generation_config=generation_config,\n    )\n\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    if echo:\n        # means keep the prompt\n        output = prompt\n    else:\n        output = \"\"\n\n    for i, new_text in enumerate(streamer):\n        output += new_text\n        if i % stream_interval == 0:\n            if echo:\n                rfind_start = len_prompt\n            else:\n                rfind_start = 0\n\n            partially_stopped = False\n            if stop_str:\n                if isinstance(stop_str, str):\n                    pos = output.rfind(stop_str, rfind_start)\n                    if pos != -1:\n                        output = output[:pos]\n                    else:\n                        partially_stopped = is_partial_stop(output, stop_str)\n                elif isinstance(stop_str, Iterable):\n                    for each_stop in stop_str:\n                        pos = output.rfind(each_stop, rfind_start)\n                        if pos != -1:\n                            output = output[:pos]\n                            break\n                        else:\n                            partially_stopped = is_partial_stop(output, each_stop)\n                            if partially_stopped:\n                                break\n                else:\n                    raise ValueError(\"Invalid stop field type.\")\n\n            # prevent yielding partial stop sequence\n            if not partially_stopped:\n                yield {\n                    \"text\": output,\n                    \"usage\": {\n                        \"prompt_tokens\": input_echo_len,\n                        \"completion_tokens\": i,\n                        \"total_tokens\": input_echo_len + i,\n                    },\n                    \"finish_reason\": None,\n                }\n    output = output.strip()\n\n    # finish stream event, which contains finish reason\n    if i == max_new_tokens - 1:\n        finish_reason = \"length\"\n    elif partially_stopped:\n        finish_reason = None\n    else:\n        finish_reason = \"stop\"\n\n    yield {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": finish_reason,\n    }\n\n    # clean\n    gc.collect()\n    torch.cuda.empty_cache()\n    if device == \"xpu\":\n        torch.xpu.empty_cache()\n    if device == \"npu\":\n        torch.npu.empty_cache()\n", "fastchat/model/llama_condense_monkey_patch.py": "# Code adapted from https://huggingface.co/kaiokendev/superhot-13b-8k-no-rlhf-test/blob/main/llama_rope_scaled_monkey_patch.py\n\nfrom functools import partial\n\nimport torch\nimport transformers\nimport transformers.models.llama.modeling_llama\n\n\nclass CondenseRotaryEmbedding(torch.nn.Module):\n    def __init__(\n        self, dim, ratio, max_position_embeddings=2048, base=10000, device=None\n    ):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        # Build here to make `torch.jit.trace` work.\n        self.ratio = ratio\n        max_position_embeddings *= ratio\n        self.max_seq_len_cached = max_position_embeddings\n        # print(f\"Monkey Patching condense ratio {ratio}\")\n        t = (\n            torch.arange(\n                self.max_seq_len_cached,\n                device=self.inv_freq.device,\n                dtype=self.inv_freq.dtype,\n            )\n            / ratio\n        )\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        dtype = torch.get_default_dtype()\n        self.register_buffer(\n            \"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False\n        )\n        self.register_buffer(\n            \"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False\n        )\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = (\n                torch.arange(\n                    self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype\n                )\n                / self.ratio\n            )\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.register_buffer(\n                \"cos_cached\", emb.cos()[None, None, :, :].to(x.dtype), persistent=False\n            )\n            self.register_buffer(\n                \"sin_cached\", emb.sin()[None, None, :, :].to(x.dtype), persistent=False\n            )\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )\n\n\ndef replace_llama_with_condense(ratio):\n    transformers.models.llama.modeling_llama.LlamaRotaryEmbedding = partial(\n        CondenseRotaryEmbedding, ratio=ratio\n    )\n", "fastchat/model/model_registry.py": "\"\"\"Additional information of the models.\"\"\"\nfrom collections import namedtuple, OrderedDict\nfrom typing import List\n\n\nModelInfo = namedtuple(\"ModelInfo\", [\"simple_name\", \"link\", \"description\"])\n\n\nmodel_info = OrderedDict()\n\n\ndef register_model_info(\n    full_names: List[str], simple_name: str, link: str, description: str\n):\n    info = ModelInfo(simple_name, link, description)\n\n    for full_name in full_names:\n        model_info[full_name] = info\n\n\ndef get_model_info(name: str) -> ModelInfo:\n    if name in model_info:\n        return model_info[name]\n    else:\n        # To fix this, please use `register_model_info` to register your model\n        return ModelInfo(\n            name, \"\", \"Register the description at fastchat/model/model_registry.py\"\n        )\n\n\nregister_model_info(\n    [\n        \"IEITYuan/Yuan2-2B-Janus-hf\",\n        \"IEITYuan/Yuan2-2B-hf\",\n        \"IEITYuan/Yuan2-51B-hf\",\n        \"IEITYuan/Yuan2-102B-hf\",\n    ],\n    \"IEIT-Yuan2\",\n    \"https://github.com/IEIT-Yuan/Yuan-2.0\",\n    \"Yuan2.0 is a new generation Fundamental Large Language Model developed by IEIT System.\",\n)\n\nregister_model_info(\n    [\n        \"claude-3-haiku-20240307\",\n        \"claude-3-sonnet-20240229\",\n        \"claude-3-opus-20240229\",\n        \"claude-2.1\",\n        \"claude-2.0\",\n        \"claude-1\",\n    ],\n    \"Claude\",\n    \"https://www.anthropic.com/news/claude-3-family\",\n    \"Claude by Anthropic\",\n)\n\nregister_model_info(\n    [\"reka-flash\", \"reka-flash-online\"],\n    \"Reka Flash\",\n    \"https://www.reka.ai/news/reka-flash-efficient-and-capable-multimodal-language-models\",\n    \"Multimodal model by Reka\",\n)\n\nregister_model_info(\n    [\"command-r-plus\"],\n    \"Command-R-Plus\",\n    \"https://txt.cohere.com/command-r-plus-microsoft-azure/\",\n    \"Command-R Plus by Cohere\",\n)\n\nregister_model_info(\n    [\"command-r\"],\n    \"Command-R\",\n    \"https://txt.cohere.com/command-r/\",\n    \"Command-R by Cohere\",\n)\n\nregister_model_info(\n    [\n        \"zephyr-orpo-141b-A35b-v0.1\",\n    ],\n    \"Zephyr 141B-A35B\",\n    \"https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1\",\n    \"ORPO fine-tuned of Mixtral-8x22B-v0.1\",\n)\n\nregister_model_info(\n    [\"gemma-1.1-7b-it\", \"gemma-1.1-2b-it\", \"gemma-7b-it\", \"gemma-2b-it\"],\n    \"Gemma\",\n    \"https://blog.google/technology/developers/gemma-open-models/\",\n    \"Gemma by Google\",\n)\n\nregister_model_info(\n    [\n        \"mixtral-8x7b-instruct-v0.1\",\n        \"mistral-large-2402\",\n        \"mistral-medium\",\n        \"mistral-next\",\n        \"mistral-7b-instruct-v0.2\",\n        \"mistral-7b-instruct\",\n    ],\n    \"Mixtral of experts\",\n    \"https://mistral.ai/news/mixtral-of-experts/\",\n    \"A Mixture-of-Experts model by Mistral AI\",\n)\n\nregister_model_info(\n    [\n        \"qwen1.5-72b-chat\",\n        \"qwen1.5-32b-chat\",\n        \"qwen1.5-14b-chat\",\n        \"qwen1.5-7b-chat\",\n        \"qwen1.5-4b-chat\",\n        \"qwen1.5-1.8b-chat\",\n        \"qwen1.5-0.5b-chat\",\n        \"qwen-14b-chat\",\n    ],\n    \"Qwen 1.5\",\n    \"https://qwenlm.github.io/blog/qwen1.5/\",\n    \"A large language model by Alibaba Cloud\",\n)\n\n\nregister_model_info(\n    [\"dbrx-instruct\"],\n    \"DBRX Instruct\",\n    \"https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\",\n    \"DBRX by Databricks Mosaic AI\",\n)\n\nregister_model_info(\n    [\"starling-lm-7b-beta\", \"starling-lm-7b-alpha\"],\n    \"Starling-LM-7B\",\n    \"https://starling.cs.berkeley.edu/\",\n    \"An open model trained using RLAIF by Berkeley\",\n)\n\nregister_model_info(\n    [\"qwen-14b-chat\"],\n    \"Qwen\",\n    \"https://huggingface.co/Qwen\",\n    \"A large language model by Alibaba Cloud\",\n)\n\nregister_model_info(\n    [\"bard-feb-2024\", \"bard-jan-24-gemini-pro\"],\n    \"Bard\",\n    \"https://bard.google.com/\",\n    \"Bard by Google\",\n)\n\nregister_model_info(\n    [\n        \"gemini-pro\",\n        \"gemini-pro-dev-api\",\n        \"gemini-1.0-pro-vision\",\n        \"gemini-1.5-pro-preview-0409\",\n    ],\n    \"Gemini\",\n    \"https://blog.google/technology/ai/google-gemini-pro-imagen-duet-ai-update/\",\n    \"Gemini by Google\",\n)\n\nregister_model_info(\n    [\"stripedhyena-nous-7b\"],\n    \"StripedHyena-Nous\",\n    \"https://huggingface.co/togethercomputer/StripedHyena-Nous-7B\",\n    \"A chat model developed by Together Research and Nous Research.\",\n)\n\nregister_model_info(\n    [\"solar-10.7b-instruct-v1.0\"],\n    \"SOLAR-10.7B-Instruct\",\n    \"https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0\",\n    \"A model trained using depth up-scaling by Upstage AI\",\n)\n\nregister_model_info(\n    [\n        \"gpt-4-turbo\",\n        \"gpt-4-turbo-2024-04-09\",\n        \"gpt-4-1106-preview\",\n        \"gpt-4-0125-preview\",\n    ],\n    \"GPT-4-Turbo\",\n    \"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\",\n    \"GPT-4-Turbo by OpenAI\",\n)\n\nregister_model_info(\n    [\"gpt-4-turbo-browsing\"],\n    \"GPT-4-Turbo with browsing\",\n    \"https://platform.openai.com/docs/assistants/overview\",\n    \"GPT-4-Turbo with browsing by OpenAI\",\n)\n\nregister_model_info(\n    [\n        \"gpt-3.5-turbo\",\n        \"gpt-3.5-turbo-0125\",\n        \"gpt-3.5-turbo-1106\",\n        \"gpt-3.5-turbo-0314\",\n        \"gpt-3.5-turbo-0613\",\n    ],\n    \"GPT-3.5\",\n    \"https://platform.openai.com/docs/models/gpt-3-5\",\n    \"GPT-3.5-Turbo by OpenAI\",\n)\n\nregister_model_info(\n    [\"gpt-4\", \"gpt-4-0314\", \"gpt-4-0613\"],\n    \"GPT-4\",\n    \"https://openai.com/research/gpt-4\",\n    \"GPT-4 by OpenAI\",\n)\n\nregister_model_info(\n    [\"claude-instant-1\", \"claude-instant-1.2\"],\n    \"Claude Instant\",\n    \"https://www.anthropic.com/index/introducing-claude\",\n    \"Claude Instant by Anthropic\",\n)\n\nregister_model_info(\n    [\"llama-2-70b-chat\", \"llama-2-34b-chat\", \"llama-2-13b-chat\", \"llama-2-7b-chat\"],\n    \"Llama 2\",\n    \"https://ai.meta.com/llama/\",\n    \"Open foundation and fine-tuned chat models by Meta\",\n)\n\nregister_model_info(\n    [\"olmo-7b-instruct\"],\n    \"OLMo-7B\",\n    \"https://huggingface.co/allenai/OLMo-7B-Instruct\",\n    \"OLMo by Allen AI\",\n)\n\nregister_model_info(\n    [\n        \"vicuna-33b\",\n        \"vicuna-33b-v1.3\",\n        \"vicuna-13b\",\n        \"vicuna-13b-v1.5\",\n        \"vicuna-7b\",\n        \"vicuna-7b-v1.5\",\n    ],\n    \"Vicuna\",\n    \"https://lmsys.org/blog/2023-03-30-vicuna/\",\n    \"A chat assistant fine-tuned on user-shared conversations by LMSYS\",\n)\n\nregister_model_info(\n    [\"yi-34b-chat\", \"yi-6b-chat\"],\n    \"Yi-Chat\",\n    \"https://huggingface.co/01-ai/Yi-34B-Chat\",\n    \"A large language model by 01 AI\",\n)\n\nregister_model_info(\n    [\n        \"codellama-70b-instruct\",\n        \"codellama-34b-instruct\",\n        \"codellama-13b-instruct\",\n        \"codellama-7b-instruct\",\n    ],\n    \"Code Llama\",\n    \"https://ai.meta.com/blog/code-llama-large-language-model-coding/\",\n    \"Open foundation models for code by Meta\",\n)\n\nregister_model_info(\n    [\"openchat-3.5-0106\", \"openchat-3.5\"],\n    \"OpenChat 3.5\",\n    \"https://github.com/imoneoi/openchat\",\n    \"An open model fine-tuned on Mistral-7B using C-RLFT\",\n)\n\nregister_model_info(\n    [\"deepseek-llm-67b-chat\"],\n    \"DeepSeek LLM\",\n    \"https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat\",\n    \"An advanced language model by DeepSeek\",\n)\n\nregister_model_info(\n    [\"stripedhyena-nous-7b\"],\n    \"StripedHyena-Nous\",\n    \"https://huggingface.co/togethercomputer/StripedHyena-Nous-7B\",\n    \"A chat model developed by Together Research and Nous Research.\",\n)\n\nregister_model_info(\n    [\"nous-hermes-2-mixtral-8x7b-dpo\"],\n    \"Nous-Hermes-2-Mixtral-8x7B-DPO\",\n    \"https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\n    \"Nous Hermes finetuned from Mixtral 8x7B\",\n)\n\n\nregister_model_info(\n    [\"llama2-70b-steerlm-chat\"],\n    \"Llama2-70B-SteerLM-Chat\",\n    \"https://huggingface.co/nvidia/Llama2-70B-SteerLM-Chat\",\n    \"A Llama fine-tuned with SteerLM method by NVIDIA\",\n)\n\nregister_model_info(\n    [\"pplx-70b-online\", \"pplx-7b-online\"],\n    \"pplx-online-llms\",\n    \"https://blog.perplexity.ai/blog/introducing-pplx-online-llms\",\n    \"Online LLM API by Perplexity AI\",\n)\n\nregister_model_info(\n    [\"openhermes-2.5-mistral-7b\"],\n    \"OpenHermes-2.5-Mistral-7B\",\n    \"https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B\",\n    \"A mistral-based model fine-tuned on 1M GPT-4 outputs\",\n)\n\nregister_model_info(\n    [\"tulu-2-dpo-70b\"],\n    \"Tulu 2\",\n    \"https://huggingface.co/allenai/tulu-2-dpo-70b\",\n    \"An instruction and RLHF model by UW/AllenAI\",\n)\n\nregister_model_info(\n    [\"chatglm3-6b\", \"chatglm2-6b\", \"chatglm-6b\"],\n    \"ChatGLM\",\n    \"https://chatglm.cn/blog\",\n    \"An open bilingual dialogue language model by Tsinghua University\",\n)\n\nregister_model_info(\n    [\"tenyxchat-7b-v1\"],\n    \"TenyxChat-7B\",\n    \"https://huggingface.co/tenyx/TenyxChat-7B-v1\",\n    \"An open model DPO trained on top of OpenChat-3.5 using Tenyx fine-tuning\",\n)\n\nregister_model_info(\n    [\"zephyr-7b-beta\", \"zephyr-7b-alpha\"],\n    \"Zephyr\",\n    \"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\",\n    \"A chatbot fine-tuned from Mistral by Hugging Face\",\n)\n\nregister_model_info(\n    [\"notus-7b-v1\"],\n    \"Notus\",\n    \"https://huggingface.co/argilla/notus-7b-v1\",\n    \"A chatbot fine-tuned from Zephyr SFT by Argilla\",\n)\n\nregister_model_info(\n    [\"catppt\"],\n    \"CatPPT\",\n    \"https://huggingface.co/rishiraj/CatPPT\",\n    \"A chatbot fine-tuned from a SLERP merged model by Rishiraj Acharya\",\n)\n\nregister_model_info(\n    [\"TinyLlama\"],\n    \"TinyLlama\",\n    \"https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    \"The TinyLlama project is an open endeavor to pretrain a 1.1B Llama model on 3 trillion tokens.\",\n)\n\nregister_model_info(\n    [\"wizardlm-70b\", \"wizardlm-30b\", \"wizardlm-13b\"],\n    \"WizardLM\",\n    \"https://github.com/nlpxucan/WizardLM\",\n    \"An instruction-following LLM using evol-instruct by Microsoft\",\n)\n\nregister_model_info(\n    [\"wizardcoder-15b-v1.0\"],\n    \"WizardLM\",\n    \"https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder\",\n    \"Empowering Code Large Language Models with Evol-Instruct\",\n)\n\nregister_model_info(\n    [\"mpt-7b-chat\", \"mpt-30b-chat\"],\n    \"MPT-Chat\",\n    \"https://www.mosaicml.com/blog/mpt-30b\",\n    \"A chatbot fine-tuned from MPT by MosaicML\",\n)\n\nregister_model_info(\n    [\"guanaco-33b\", \"guanaco-65b\"],\n    \"Guanaco\",\n    \"https://github.com/artidoro/qlora\",\n    \"A model fine-tuned with QLoRA by UW\",\n)\n\nregister_model_info(\n    [\"gpt4all-13b-snoozy\"],\n    \"GPT4All-Snoozy\",\n    \"https://github.com/nomic-ai/gpt4all\",\n    \"A finetuned LLaMA model on assistant style data by Nomic AI\",\n)\n\nregister_model_info(\n    [\"koala-13b\"],\n    \"Koala\",\n    \"https://bair.berkeley.edu/blog/2023/04/03/koala\",\n    \"A dialogue model for academic research by BAIR\",\n)\n\nregister_model_info(\n    [\"RWKV-4-Raven-14B\"],\n    \"RWKV-4-Raven\",\n    \"https://huggingface.co/BlinkDL/rwkv-4-raven\",\n    \"An RNN with transformer-level LLM performance\",\n)\n\nregister_model_info(\n    [\"alpaca-13b\"],\n    \"Alpaca\",\n    \"https://crfm.stanford.edu/2023/03/13/alpaca.html\",\n    \"A model fine-tuned from LLaMA on instruction-following demonstrations by Stanford\",\n)\n\nregister_model_info(\n    [\"oasst-pythia-12b\"],\n    \"OpenAssistant (oasst)\",\n    \"https://open-assistant.io\",\n    \"An Open Assistant for everyone by LAION\",\n)\n\nregister_model_info(\n    [\"oasst-sft-7-llama-30b\"],\n    \"OpenAssistant (oasst)\",\n    \"https://open-assistant.io\",\n    \"An Open Assistant for everyone by LAION\",\n)\n\nregister_model_info(\n    [\"palm-2\"],\n    \"PaLM 2 Chat\",\n    \"https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023\",\n    \"PaLM 2 for Chat (chat-bison@001) by Google\",\n)\n\nregister_model_info(\n    [\"llama-7b\", \"llama-13b\"],\n    \"LLaMA\",\n    \"https://arxiv.org/abs/2302.13971\",\n    \"Open and efficient foundation language models by Meta\",\n)\n\nregister_model_info(\n    [\"open-llama-7b-v2-open-instruct\", \"open-llama-7b-open-instruct\"],\n    \"Open LLaMa (Open Instruct)\",\n    \"https://medium.com/vmware-data-ml-blog/starter-llm-for-the-enterprise-instruction-tuning-openllama-7b-d05fc3bbaccc\",\n    \"Open LLaMa fine-tuned on instruction-following data by VMware\",\n)\n\nregister_model_info(\n    [\"dolly-v2-12b\"],\n    \"Dolly\",\n    \"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\",\n    \"An instruction-tuned open large language model by Databricks\",\n)\n\nregister_model_info(\n    [\"stablelm-tuned-alpha-7b\"],\n    \"StableLM\",\n    \"https://github.com/stability-AI/stableLM\",\n    \"Stability AI language models\",\n)\n\nregister_model_info(\n    [\"codet5p-6b\"],\n    \"CodeT5p-6b\",\n    \"https://huggingface.co/Salesforce/codet5p-6b\",\n    \"Code completion model released by Salesforce\",\n)\n\nregister_model_info(\n    [\"fastchat-t5-3b\", \"fastchat-t5-3b-v1.0\"],\n    \"FastChat-T5\",\n    \"https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\",\n    \"A chat assistant fine-tuned from FLAN-T5 by LMSYS\",\n)\n\nregister_model_info(\n    [\"phoenix-inst-chat-7b\"],\n    \"Phoenix-7B\",\n    \"https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b\",\n    \"A multilingual chat assistant fine-tuned from Bloomz to democratize ChatGPT across languages by CUHK(SZ)\",\n)\n\nregister_model_info(\n    [\"realm-7b-v1\"],\n    \"ReaLM\",\n    \"https://github.com/FreedomIntelligence/ReaLM\",\n    \"A chatbot fine-tuned from LLaMA2 with data generated via iterative calls to UserGPT and ChatGPT by CUHK(SZ) and SRIBD.\",\n)\n\nregister_model_info(\n    [\"billa-7b-sft\"],\n    \"BiLLa-7B-SFT\",\n    \"https://huggingface.co/Neutralzz/BiLLa-7B-SFT\",\n    \"An instruction-tuned bilingual LLaMA with enhanced reasoning ability by an independent researcher\",\n)\n\nregister_model_info(\n    [\"h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\"],\n    \"h2oGPT-GM-7b\",\n    \"https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\",\n    \"An instruction-tuned OpenLLaMA with enhanced conversational ability by H2O.ai\",\n)\n\nregister_model_info(\n    [\"baize-v2-7b\", \"baize-v2-13b\"],\n    \"Baize v2\",\n    \"https://github.com/project-baize/baize-chatbot#v2\",\n    \"A chatbot fine-tuned from LLaMA with ChatGPT self-chat data and Self-Disillation with Feedback (SDF) by UCSD and SYSU.\",\n)\n\nregister_model_info(\n    [\n        \"airoboros-l2-7b-2.1\",\n        \"airoboros-l2-13b-2.1\",\n        \"airoboros-c34b-2.1\",\n        \"airoboros-l2-70b-2.1\",\n    ],\n    \"airoboros\",\n    \"https://huggingface.co/jondurbin/airoboros-l2-70b-2.1\",\n    \"An instruction-tuned LlaMa model tuned with 100% synthetic instruction-response pairs from GPT4\",\n)\n\nregister_model_info(\n    [\n        \"spicyboros-7b-2.2\",\n        \"spicyboros-13b-2.2\",\n        \"spicyboros-70b-2.2\",\n    ],\n    \"spicyboros\",\n    \"https://huggingface.co/jondurbin/spicyboros-70b-2.2\",\n    \"De-aligned versions of the airoboros models\",\n)\n\nregister_model_info(\n    [\"Robin-7b-v2\", \"Robin-13b-v2\", \"Robin-33b-v2\"],\n    \"Robin-v2\",\n    \"https://huggingface.co/OptimalScale/robin-7b-v2-delta\",\n    \"A chatbot fine-tuned from LLaMA-7b, achieving competitive performance on chitchat, commonsense reasoning and instruction-following tasks, by OptimalScale, HKUST.\",\n)\n\nregister_model_info(\n    [\"manticore-13b-chat\"],\n    \"Manticore 13B Chat\",\n    \"https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg\",\n    \"A chatbot fine-tuned from LlaMa across several CoT and chat datasets.\",\n)\n\nregister_model_info(\n    [\"redpajama-incite-7b-chat\"],\n    \"RedPajama-INCITE-7B-Chat\",\n    \"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat\",\n    \"A chatbot fine-tuned from RedPajama-INCITE-7B-Base by Together\",\n)\n\nregister_model_info(\n    [\n        \"falcon-7b\",\n        \"falcon-7b-instruct\",\n        \"falcon-40b\",\n        \"falcon-40b-instruct\",\n        \"falcon-180b\",\n        \"falcon-180b-chat\",\n    ],\n    \"Falcon\",\n    \"https://huggingface.co/tiiuae/falcon-180B\",\n    \"TII's flagship series of large language models\",\n)\n\nregister_model_info(\n    [\"tigerbot-7b-sft\"],\n    \"Tigerbot\",\n    \"https://huggingface.co/TigerResearch/tigerbot-7b-sft\",\n    \"A large-scale language model (LLM) with multiple languages and tasks.\",\n)\n\nregister_model_info(\n    [\"internlm-chat-7b\", \"internlm-chat-7b-8k\"],\n    \"InternLM\",\n    \"https://huggingface.co/internlm/internlm-chat-7b\",\n    \"A multi-language large-scale language model (LLM), developed by SHLAB.\",\n)\n\nregister_model_info(\n    [\"Qwen-7B-Chat\"],\n    \"Qwen\",\n    \"https://huggingface.co/Qwen/Qwen-7B-Chat\",\n    \"A multi-language large-scale language model (LLM), developed by Damo Academy.\",\n)\n\nregister_model_info(\n    [\"smaug-2-72b\"],\n    \"Smaug-2-72B\",\n    \"https://huggingface.co/abacusai/Smaug-2-72B\",\n    \"An open model trained by Abacus.AI.\",\n)\n\nregister_model_info(\n    [\"Llama2-Chinese-13b-Chat\", \"LLama2-Chinese-13B\"],\n    \"Llama2-Chinese\",\n    \"https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat\",\n    \"A multi-language large-scale language model (LLM), developed by FlagAlpha.\",\n)\n\nregister_model_info(\n    [\"Meta-Llama-3-8B-Instruct\", \"Meta-Llama-3-70B-Instruct\"],\n    \"llama-3\",\n    \"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes.\",\n)\n\nregister_model_info(\n    [\"Chinese-Alpaca-2-7B\", \"Chinese-Alpaca-2-13B\"],\n    \"Chinese-Alpaca\",\n    \"https://huggingface.co/hfl/chinese-alpaca-2-13b\",\n    \"New extended Chinese vocabulary beyond Llama-2, open-sourcing the Chinese LLaMA-2 and Alpaca-2 LLMs.\",\n)\n\nregister_model_info(\n    [\"Vigogne-2-7B-Instruct\", \"Vigogne-2-13B-Instruct\"],\n    \"Vigogne-Instruct\",\n    \"https://huggingface.co/bofenghuang/vigogne-2-7b-instruct\",\n    \"A French large language model (LLM) optimized for instruction-following, developed by Bofeng Huang\",\n)\n\nregister_model_info(\n    [\"Vigogne-2-7B-Chat\", \"Vigogne-2-13B-Chat\"],\n    \"Vigogne-Chat\",\n    \"https://huggingface.co/bofenghuang/vigogne-2-7b-chat\",\n    \"A French large language model (LLM) optimized for instruction-following and multi-turn dialogues, developed by Bofeng Huang\",\n)\n\nregister_model_info(\n    [\"stable-vicuna-13B-HF\"],\n    \"stable-vicuna\",\n    \"https://huggingface.co/TheBloke/stable-vicuna-13B-HF\",\n    \"A Vicuna model fine-tuned using RLHF via PPO on various conversational and instructional datasets.\",\n)\n\nregister_model_info(\n    [\"deluxe-chat-v1\", \"deluxe-chat-v1.1\", \"deluxe-chat-v1.2\", \"deluxe-chat-v1.3\"],\n    \"DeluxeChat\",\n    \"\",\n    \"Deluxe Chat\",\n)\n\nregister_model_info(\n    [\n        \"Xwin-LM-7B-V0.1\",\n        \"Xwin-LM-13B-V0.1\",\n        \"Xwin-LM-70B-V0.1\",\n        \"Xwin-LM-7B-V0.2\",\n        \"Xwin-LM-13B-V0.2\",\n    ],\n    \"Xwin-LM\",\n    \"https://github.com/Xwin-LM/Xwin-LM\",\n    \"Chat models developed by Xwin-LM team\",\n)\n\nregister_model_info(\n    [\"lemur-70b-chat\"],\n    \"Lemur-Chat\",\n    \"https://huggingface.co/OpenLemur/lemur-70b-chat-v1\",\n    \"An openly accessible language model optimized for both natural language and coding capabilities \",\n)\n\nregister_model_info(\n    [\"Mistral-7B-OpenOrca\"],\n    \"Open-Orca\",\n    \"https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca\",\n    \"A fine-tune of [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) using [OpenOrca dataset](https://huggingface.co/datasets/Open-Orca/OpenOrca)\",\n)\n\nregister_model_info(\n    [\"dolphin-2.2.1-mistral-7b\"],\n    \"dolphin-mistral\",\n    \"https://huggingface.co/ehartford/dolphin-2.2.1-mistral-7b\",\n    \"An uncensored fine-tuned Mistral 7B\",\n)\n\nregister_model_info(\n    [\n        \"AquilaChat-7B\",\n        \"AquilaChat2-7B\",\n        \"AquilaChat2-34B\",\n    ],\n    \"Aquila-Chat\",\n    \"https://huggingface.co/BAAI/AquilaChat2-34B\",\n    \"Chat models developed by BAAI team\",\n)\n\nregister_model_info(\n    [\"xDAN-L1-Chat-RL-v1\"],\n    \"xDAN-L1-Chat\",\n    \"https://huggingface.co/xDAN-AI/xDAN-L1-Chat-RL-v1\",\n    \"A large language chat model created by xDAN-AI.\",\n)\n\nregister_model_info(\n    [\"MetaMath-70B-V1.0\", \"MetaMath-7B-V1.0\"],\n    \"MetaMath\",\n    \"https://huggingface.co/meta-math\",\n    \"A finetune of Llama2 on [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA) that specializes in mathematical reasoning.\",\n)\n\nregister_model_info(\n    [\"Yuan2-2B-hf\", \"Yuan2-51B-hf\", \"Yuan2-102B-hf\"],\n    \"IEIYuan\",\n    \"https://huggingface.co/IEITYuan\",\n    \"A Basemodel developed by IEI.\",\n)\n\nregister_model_info(\n    [\n        \"llava-v1.6-34b\",\n        \"llava-v1.6-vicuna-13b\",\n        \"llava-v1.6-vicuna-7b\",\n        \"llava-v1.6-mistral-7b\",\n        \"llava-v1.5-13b\",\n        \"llava-v1.5-7b\",\n    ],\n    \"LLaVA\",\n    \"https://github.com/haotian-liu/LLaVA\",\n    \"an open large language and vision assistant\",\n)\n\nregister_model_info(\n    [\"gemma-7b-it\", \"gemma-2b-it\"],\n    \"Gemma\",\n    \"https://blog.google/technology/developers/gemma-open-models/\",\n    \"Gemma by Google\",\n)\n\nregister_model_info(\n    [\n        \"cllm/consistency-llm-7b-codesearchnet\",\n        \"cllm/consistency-llm-7b-gsm8k\",\n        \"cllm/consistency-llm-7b-sharegpt48k\",\n        \"cllm/consistency-llm-7b-spider\",\n    ],\n    \"consistency-llm\",\n    \"https://huggingface.co/cllm\",\n    \"consistency-llm is a new generation of parallel decoder LLMs with fast generation speed.\",\n)\n\nregister_model_info(\n    [\"reka-flash\", \"reka-flash-20240226\"],\n    \"Reka Flash\",\n    \"https://reka.ai/reka-flash\",\n    \"Multimodal model by Reka\",\n)\n", "fastchat/model/__init__.py": "from fastchat.model.model_adapter import (\n    load_model,\n    get_conversation_template,\n    add_model_args,\n)\n", "fastchat/model/model_codet5p.py": "import gc\nfrom threading import Thread\nimport torch\nimport transformers\nfrom transformers import (\n    GenerationConfig,\n    StoppingCriteria,\n    StoppingCriteriaList,\n    TextIteratorStreamer,\n)\n\n\n@torch.inference_mode()\ndef generate_stream_codet5p(\n    model,\n    tokenizer,\n    params,\n    device,\n    context_len=2048,\n    stream_interval=2,\n    judge_sent_end=False,\n):\n    prompt = params[\"prompt\"]\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    top_k = int(params.get(\"top_k\", 50))  # -1 means disable\n    max_new_tokens = int(params.get(\"max_new_tokens\", 1024))\n    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n    stop_token_ids.append(tokenizer.eos_token_id)\n\n    decode_config = dict(skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    streamer = TextIteratorStreamer(tokenizer, **decode_config)\n    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    input_ids = encoding.input_ids\n    encoding[\"decoder_input_ids\"] = encoding[\"input_ids\"].clone()\n    input_echo_len = len(input_ids)\n\n    generation_config = GenerationConfig(\n        max_new_tokens=max_new_tokens,\n        do_sample=temperature >= 1e-5,\n        temperature=temperature,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=10,\n        top_p=top_p,\n        top_k=top_k,\n        eos_token_id=stop_token_ids,\n    )\n\n    class CodeBlockStopper(StoppingCriteria):\n        def __call__(\n            self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n        ) -> bool:\n            # Code-completion is open-end generation.\n            # We check \\n\\n to stop at end of a code block.\n            if list(input_ids[0][-2:]) == [628, 198]:\n                return True\n            return False\n\n    gen_kwargs = dict(\n        **encoding,\n        streamer=streamer,\n        generation_config=generation_config,\n        stopping_criteria=StoppingCriteriaList([CodeBlockStopper()]),\n    )\n    thread = Thread(target=model.generate, kwargs=gen_kwargs)\n    thread.start()\n    i = 0\n    output = \"\"\n    for new_text in streamer:\n        i += 1\n        output += new_text\n        if i % stream_interval == 0 or i == max_new_tokens - 1:\n            yield {\n                \"text\": output,\n                \"usage\": {\n                    \"prompt_tokens\": input_echo_len,\n                    \"completion_tokens\": i,\n                    \"total_tokens\": input_echo_len + i,\n                },\n                \"finish_reason\": None,\n            }\n        if i >= max_new_tokens:\n            break\n\n    if i >= max_new_tokens:\n        finish_reason = \"length\"\n    else:\n        finish_reason = \"stop\"\n\n    yield {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": finish_reason,\n    }\n    thread.join()\n\n    # clean\n    gc.collect()\n    torch.cuda.empty_cache()\n    if device == \"xpu\":\n        torch.xpu.empty_cache()\n    if device == \"npu\":\n        torch.npu.empty_cache()\n", "fastchat/model/model_xfastertransformer.py": "import gc\nfrom threading import Thread\n\nimport torch\nfrom transformers import TextIteratorStreamer\n\n\n@torch.inference_mode()\ndef generate_stream_xft(\n    model,\n    tokenizer,\n    params,\n    device,\n    context_len=8192,\n    stream_interval=2,\n    judge_sent_end=False,\n):\n    prompt = params[\"prompt\"]\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n\n    # unused now, and placehold for future.\n    # temperature = float(params.get(\"temperature\", 1.0))\n    # top_p = float(params.get(\"top_p\", 1.0))\n\n    max_new_tokens = int(params.get(\"max_new_tokens\", 4096))\n    echo = params.get(\"echo\", True)\n\n    inputs = tokenizer(\n        prompt, return_tensors=\"pt\", padding=model.config.padding\n    ).input_ids\n    input_echo_len = len(inputs[0])\n    max_len = max_new_tokens + input_echo_len\n\n    decode_config = dict(skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, **decode_config)\n    generation_kwargs = {\n        \"input_ids\": inputs,\n        \"streamer\": streamer,\n        \"max_length\": max_len,\n        \"num_beams\": model.config.beam_width,\n        \"length_penalty\": repetition_penalty,\n        \"num_return_sequences\": model.config.num_return_sequences,\n        \"early_stopping\": model.config.early_stopping,\n        \"eos_token_id\": model.config.eos_token_id,\n        \"pad_token_id\": model.config.pad_token_id,\n    }\n\n    thread = Thread(target=model.model.generate, kwargs=generation_kwargs)\n    thread.start()\n    if echo:\n        # means keep the prompt\n        output = prompt\n    else:\n        output = \"\"\n    i = 0\n    for i, new_text in enumerate(streamer):\n        output += new_text\n        yield {\n            \"text\": output,\n            \"usage\": {\n                \"prompt_tokens\": input_echo_len,\n                \"completion_tokens\": i,\n                \"total_tokens\": input_echo_len + i,\n            },\n            \"finish_reason\": None,\n        }\n    output = output.strip()\n    if i == max_new_tokens - 1:\n        finish_reason = \"length\"\n    else:\n        finish_reason = \"stop\"\n    yield {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": finish_reason,\n    }\n    gc.collect()\n", "fastchat/train/train_flant5.py": "# Adapted from tatsu-lab@stanford_alpaca. Below is the original copyright:\n#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\nfrom collections import defaultdict\nimport copy\nimport os\nfrom dataclasses import dataclass, field\nimport random\nimport json\nimport logging\nimport pathlib\nfrom typing import Dict, Optional, Sequence\n\nimport torch\nimport torch.distributed as dist\n\nimport transformers\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer, AddedToken\n\nfrom fastchat.model.model_adapter import get_conversation_template\n\ndefault_conversation = get_conversation_template(\"t5\")\n\n# TODO: import and use code from ../data/dataset.py\n\nIGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"\nDEFAULT_BOS_TOKEN = \"</s>\"\nDEFAULT_UNK_TOKEN = \"</s>\"\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    lazy_preprocess: bool = False\n    num_data: int = -1\n    preprocessed_path: str = field(\n        default=None, metadata={\"help\": \"Path to the preprocessed training data.\"}\n    )\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=2048,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n\n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    state_dict = trainer.model.state_dict()\n    if trainer.args.should_save:\n        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n        del state_dict\n        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    other_tokens,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    for new_token in other_tokens:\n        num_new_tokens += tokenizer.add_tokens(AddedToken(new_token, normalized=False))\n\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True\n        )\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True\n        )\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n\ndef _tokenize_fn(\n    strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer\n) -> Dict:\n    \"\"\"Tokenize a list of strings.\"\"\"\n    tokenized_list = [\n        tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        )\n        for text in strings\n    ]\n    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n    input_ids_lens = labels_lens = [\n        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n        for tokenized in tokenized_list\n    ]\n    return dict(\n        input_ids=input_ids,\n        labels=labels,\n        input_ids_lens=input_ids_lens,\n        labels_lens=labels_lens,\n    )\n\n\ndef _form_qa(\n    q_list,\n    a_list,\n    tokenized_conversation,\n    tokenized_lens,\n    speakers,\n    header_len,\n    max_length,\n    eos_id,\n):\n    cur_idx = header_len\n    conv_len = len(tokenized_conversation)\n\n    for tokenized_len, speaker in zip(tokenized_lens, speakers):\n        if cur_idx >= conv_len:\n            break\n        if speaker == \"gpt\":\n            # truncate answer if it is too long\n            content_a = None\n            if tokenized_len > max_length:\n                content_a = tokenized_conversation[cur_idx : cur_idx + max_length]\n            else:\n                content_a = tokenized_conversation[cur_idx : cur_idx + tokenized_len]\n            content_a.append(eos_id)\n            a_list.append(content_a)\n            content_q = None\n            if cur_idx >= max_length:\n                content_q = tokenized_conversation[cur_idx - max_length : cur_idx]\n            else:\n                content_q = tokenized_conversation[:cur_idx]\n            content_q.append(eos_id)\n            q_list.append(content_q)\n            # asser the last token is actually a EOS for an answer\n            assert a_list[-1][-1] == eos_id, \"Last Token is not EOS!\"\n        cur_idx += tokenized_len\n\n\ndef _add_speaker_and_signal(header, source, get_conversation=True):\n    \"\"\"Add speaker and start/end signal on each round.\"\"\"\n    BEGIN_SIGNAL = \"### \"\n    END_SIGNAL = \"\\n\"\n    conversation = header\n\n    unknown_role = \"unknown\"  # use default unknown role\n    roles = {\n        \"human\": default_conversation.roles[0],  # human role\n        \"gpt\": default_conversation.roles[1],  # gpt role\n    }\n\n    for i in range(len(source)):\n        sentence = source[i]\n        sentence_from = sentence[\"from\"].lower()\n\n        # TODO(Dacheng): verify this is a good way to split sentences\n        if sentence_from == \"human\":\n            # if this is not the last sentence\n            if i != len(source) - 1:\n                next_sentence = source[i + 1]\n                sentence[\"value\"] = (\n                    BEGIN_SIGNAL\n                    + roles.get(sentence_from, unknown_role)\n                    + \": \"\n                    + sentence[\"value\"]\n                    + END_SIGNAL\n                    + BEGIN_SIGNAL\n                    + roles.get(next_sentence[\"from\"].lower(), unknown_role)\n                    + \": \"\n                )\n            else:\n                # if human is the last speaker, it does not contribute to an answer\n                pass\n        else:\n            sentence[\"value\"] = sentence[\"value\"] + END_SIGNAL\n        if get_conversation:\n            conversation += sentence[\"value\"]\n\n    return conversation\n\n\ndef preprocess(\n    sources: Sequence[str],\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    \"\"\"\n    Given a list of sources, each is a conversation list. This transform:\n    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n    2. Concatenate conversations together;\n    3. Tokenize the concatenated conversation;\n    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n    \"\"\"\n    # add end signal and concatenate together\n    conversations = []\n    header = f\"{default_conversation.system_message}\\n\\n\"\n    for source in sources:\n        conversation = _add_speaker_and_signal(header, source, tokenizer)\n        conversations.append(conversation)\n    # TODO(Dacheng): This is related to whether the dataset has been truncated..\n    # Assume we get long conversations, don't pad, don't return tensor\n    tokenized_conversations = tokenizer(conversations, max_length=None)[\"input_ids\"]\n    q_list = []\n    a_list = []\n    # count for EOS length\n    header_len = _tokenize_fn([header], tokenizer)[\"input_ids_lens\"][0] - 1\n    from tqdm import tqdm\n\n    for tokenized_conversation, source in tqdm(zip(tokenized_conversations, sources)):\n        tokenized_sentence = _tokenize_fn([s[\"value\"] for s in source], tokenizer)\n        tokenized_lens = tokenized_sentence[\"input_ids_lens\"]\n        tokenized_lens = [l - 1 for l in tokenized_lens]\n        speakers = [sentence[\"from\"] for sentence in source]\n        ids = tokenized_sentence[\"input_ids\"]\n        _form_qa(\n            q_list,\n            a_list,\n            tokenized_conversation,\n            tokenized_lens,\n            speakers,\n            header_len,\n            tokenizer.model_max_length,\n            tokenizer.eos_token_id,\n        )\n    return dict(input_ids=q_list, labels=a_list)\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        tokenizer: transformers.PreTrainedTokenizer,\n        preprocessed_path,\n        num_data,\n    ):\n        super(SupervisedDataset, self).__init__()\n\n        # save to file\n        # Make sure only the first process is processing the dataset\n        if dist.get_rank() != 0:\n            dist.barrier()\n        self.preprocessed_path = preprocessed_path\n        if os.path.exists(self.preprocessed_path):\n            logging.warning(\"loading from preprocessed data\")\n            with open(self.preprocessed_path, \"r\") as f:\n                data_dict = json.load(f)\n            if dist.get_rank() == 0:\n                dist.barrier()\n        else:\n            if not os.path.exists(\"preprocessed_data\"):\n                os.mkdir(\"preprocessed_data\")\n            assert dist.get_rank() == 0, \"Only the first process should process\"\n            logging.warning(\"Loading data...\")\n            list_data_dict = json.load(open(data_path, \"r\"))\n\n            logging.warning(\"Formatting inputs...\")\n            sources = []\n\n            sources = [example[\"conversations\"] for example in list_data_dict]\n\n            data_dict = preprocess(sources, tokenizer)\n            json_data_dict = json.dumps(data_dict)\n\n            # Remember to close file to avoid concurrent r/w\n            with open(self.preprocessed_path, \"w\") as f:\n                f.write(json_data_dict)\n\n            # Release barrier\n            dist.barrier()\n\n        if num_data != -1:\n            data_dict[\"input_ids\"] = data_dict[\"input_ids\"][:num_data]\n            data_dict[\"labels\"] = data_dict[\"labels\"][:num_data]\n\n        # Shuffle data to see more conversations, if only train on partial data\n        temp = list(zip(data_dict[\"input_ids\"], data_dict[\"labels\"]))\n        random.shuffle(temp)\n        res1, res2 = zip(*temp)\n        data_dict[\"input_ids\"], data_dict[\"labels\"] = list(res1), list(res2)\n\n        # Dacheng: Get rid of short QA pair\n        self.input_ids = copy.deepcopy(data_dict[\"input_ids\"])\n        self.labels = copy.deepcopy(data_dict[\"labels\"])\n        length_arr = defaultdict(int)\n        for idx, (input, label) in enumerate(\n            zip(data_dict[\"input_ids\"], data_dict[\"labels\"])\n        ):\n            length_arr[str(len(label) // 100)] += 1\n            if len(input) <= 5:\n                del_idx = self.input_ids.index(input)\n                self.input_ids.pop(del_idx)\n                self.labels.pop(del_idx)\n            if len(label) <= 5:\n                del_idx = self.labels.index(label)\n                self.input_ids.pop(del_idx)\n                self.labels.pop(del_idx)\n\n        for input, label in zip(self.input_ids, self.labels):\n            assert len(input) >= 5\n            assert len(label) >= 5\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n\n\n@dataclass\nclass DataCollatorForSupervisedDataset(object):\n    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n\n    tokenizer: transformers.PreTrainedTokenizer\n\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple(\n            [\n                torch.as_tensor(instance[key], dtype=torch.int64)\n                for instance in instances\n            ]\n            for key in (\"input_ids\", \"labels\")\n        )\n        input_ids = torch.nn.utils.rnn.pad_sequence(\n            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n        )\n        labels = torch.nn.utils.rnn.pad_sequence(\n            labels, batch_first=True, padding_value=IGNORE_INDEX\n        )\n        ret = dict(\n            input_ids=input_ids,\n            labels=labels,\n            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n        )\n        torch.set_printoptions(profile=\"full\")\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer, data_args\n) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = SupervisedDataset\n    train_dataset = dataset_cls(\n        tokenizer=tokenizer,\n        data_path=data_args.data_path,\n        preprocessed_path=data_args.preprocessed_path,\n        num_data=data_args.num_data,\n    )\n    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n    return dict(\n        train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator\n    )\n\n\ndef train():\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments)\n    )\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n    )\n    # Dacheng: Note we can only use T5Tokenizer, otherwise it will prepend\n    # a space before special tokens.\n    tokenizer = transformers.T5Tokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n\n    smart_tokenizer_and_embedding_resize(\n        special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n        other_tokens=[\"<\", \"{\", \"\\n\", \"}\", \"`\", \" \", \"\\\\\", \"^\", \"\\t\"],\n        tokenizer=tokenizer,\n        model=model,\n    )\n\n    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n    trainer.save_state()\n    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n\n\nif __name__ == \"__main__\":\n    train()\n", "fastchat/train/llama_flash_attn_monkey_patch.py": "from typing import Optional, Tuple\nimport warnings\n\nimport torch\nfrom torch import nn\nimport transformers\nfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\nfrom flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func\nfrom flash_attn.bert_padding import unpad_input, pad_input\n\n\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    if output_attentions:\n        warnings.warn(\n            \"Output attentions is not supported for patched `LlamaAttention`, returning `None` instead.\"\n        )\n\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )  # shape: (b, num_heads, s, head_dim)\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n\n    if past_key_value is not None:\n        # reuse k, v\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    # Transform the data into the format required by flash attention\n    qkv = torch.stack([query_states, key_states, value_states], dim=2)\n    qkv = qkv.transpose(1, 3)  # shape: [b, s, 3, num_heads, head_dim]\n    key_padding_mask = attention_mask\n\n    if key_padding_mask is None:\n        qkv = qkv.reshape(-1, 3, self.num_heads, self.head_dim)\n        cu_q_lens = torch.arange(\n            0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32, device=qkv.device\n        )\n        max_s = q_len\n        output = flash_attn_varlen_qkvpacked_func(\n            qkv, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n        )\n        output = output.view(bsz, q_len, -1)\n    else:\n        qkv = qkv.reshape(bsz, q_len, -1)\n        qkv, indices, cu_q_lens, max_s = unpad_input(qkv, key_padding_mask)\n        qkv = qkv.view(-1, 3, self.num_heads, self.head_dim)\n        output_unpad = flash_attn_varlen_qkvpacked_func(\n            qkv, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n        )\n        output_unpad = output_unpad.reshape(-1, self.num_heads * self.head_dim)\n        output = pad_input(output_unpad, indices, bsz, q_len)\n\n    return self.o_proj(output), None, past_key_value\n\n\n# Disable the transformation of the attention mask in LlamaModel as the flash attention\n# requires the attention mask to be the same as the key_padding_mask\ndef _prepare_decoder_attention_mask(\n    self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n):\n    # [bsz, seq_len]\n    return attention_mask\n\n\ndef replace_llama_attn_with_flash_attn():\n    cuda_major, cuda_minor = torch.cuda.get_device_capability()\n    if cuda_major < 8:\n        warnings.warn(\n            \"Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.\"\n            \"ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\"\n        )\n    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = (\n        _prepare_decoder_attention_mask\n    )\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n", "fastchat/train/train.py": "# This code is based on tatsu-lab/stanford_alpaca. Below is the original copyright:\n#\n#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\nfrom dataclasses import dataclass, field\nimport json\nimport math\nimport pathlib\nfrom typing import Dict, Optional, Sequence\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport transformers\nfrom transformers import Trainer\nfrom transformers.trainer_pt_utils import LabelSmoother\n\nfrom fastchat.conversation import SeparatorStyle\nfrom fastchat.model.model_adapter import get_conversation_template\n\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n    trust_remote_code: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether or not to allow for custom models defined on the Hub in their own modeling files\"\n        },\n    )\n    padding_side: str = field(\n        default=\"right\", metadata={\"help\": \"The padding side in tokenizer\"}\n    )\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    eval_data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n    )\n    lazy_preprocess: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n\n\nlocal_rank = None\n\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\ndef trainer_save_model_safe(trainer: transformers.Trainer):\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n    from torch.distributed.fsdp import StateDictType, FullStateDictConfig\n\n    save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n    with FSDP.state_dict_type(\n        trainer.model, StateDictType.FULL_STATE_DICT, save_policy\n    ):\n        trainer.save_model()\n\n\ndef preprocess(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    conv = get_conversation_template(\"vicuna\")\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n    input_ids = tokenizer(\n        conversations,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n    ).input_ids\n    targets = input_ids.clone()\n\n    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n\n    # Mask targets. Only compute loss on the assistant outputs.\n    sep = conv.sep + conv.roles[1] + \": \"\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        turns = conversation.split(conv.sep2)\n        cur_len = 1\n        target[:cur_len] = IGNORE_TOKEN_ID\n        for i, turn in enumerate(turns):\n            if turn == \"\":\n                break\n            turn_len = len(tokenizer(turn).input_ids)\n\n            parts = turn.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n            # \"-2\" is hardcoded for the Llama tokenizer to make the offset correct.\n            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n\n            if i != 0 and not tokenizer.legacy:\n                # The legacy and non-legacy modes handle special tokens differently\n                instruction_len -= 1\n\n            # Ignore the user instructions\n            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n            cur_len += turn_len\n\n            if i != 0 and not tokenizer.legacy:\n                # The legacy and non-legacy modes handle special tokens differently\n                cur_len -= 1\n\n        target[cur_len:] = IGNORE_TOKEN_ID\n\n        if False:  # Inspect and check the correctness of masking\n            z = target.clone()\n            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n            rank0_print(tokenizer.decode(z))\n            exit()\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_TOKEN_ID\n                rank0_print(\n                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n                    f\" #turn = {len(turns) - 1}. (ignored)\"\n                )\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n    )\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        sources = [example[\"conversations\"] for example in raw_data]\n        data_dict = preprocess(sources, tokenizer)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n        self.tokenizer = tokenizer\n        self.raw_data = raw_data\n        self.cached_data_dict = {}\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        if i in self.cached_data_dict:\n            return self.cached_data_dict[i]\n\n        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer)\n        ret = dict(\n            input_ids=ret[\"input_ids\"][0],\n            labels=ret[\"labels\"][0],\n            attention_mask=ret[\"attention_mask\"][0],\n        )\n        self.cached_data_dict[i] = ret\n\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer, data_args\n) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = (\n        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n    )\n    rank0_print(\"Loading data...\")\n\n    train_json = json.load(open(data_args.data_path, \"r\"))\n    train_dataset = dataset_cls(train_json, tokenizer=tokenizer)\n\n    if data_args.eval_data_path:\n        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer)\n    else:\n        eval_dataset = None\n\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n\n\ndef train():\n    global local_rank\n\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments)\n    )\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    local_rank = training_args.local_rank\n\n    # Set RoPE scaling factor\n    config = transformers.AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        trust_remote_code=model_args.trust_remote_code,\n    )\n    orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n    if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n        scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n    config.use_cache = False\n\n    # Load model and tokenizer\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        cache_dir=training_args.cache_dir,\n        trust_remote_code=model_args.trust_remote_code,\n    )\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=model_args.padding_side,\n        use_fast=False,\n        trust_remote_code=model_args.trust_remote_code,\n    )\n\n    if tokenizer.pad_token != tokenizer.unk_token:\n        tokenizer.pad_token = tokenizer.unk_token\n\n    # Load data\n    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n\n    # Start trainner\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n\n    # Save model\n    model.config.use_cache = True\n    trainer.save_state()\n    if trainer.is_deepspeed_enabled:\n        trainer.save_model()\n    else:\n        trainer_save_model_safe(trainer)\n\n\nif __name__ == \"__main__\":\n    train()\n", "fastchat/train/train_xformers.py": "# Make it more memory efficient by monkey patching the LLaMA model with xformers attention.\n\n# Need to call this before importing transformers.\nfrom fastchat.train.llama_xformers_attn_monkey_patch import (\n    replace_llama_attn_with_xformers_attn,\n)\n\nreplace_llama_attn_with_xformers_attn()\n\nfrom fastchat.train.train import train\n\nif __name__ == \"__main__\":\n    train()\n", "fastchat/train/train_lora.py": "# Usage: deepspeed train_lora.py --deepspeed <$PATH_TO_DEEPSPEED_CONFIG>\n\n# Adapted from tatsu-lab@stanford_alpaca. Below is the original copyright:\n#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\nfrom dataclasses import dataclass, field\nimport logging\nimport pathlib\nimport typing\nimport os\n\nfrom deepspeed import zero\nfrom deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport transformers\nfrom transformers import Trainer, BitsAndBytesConfig, deepspeed\nimport torch\n\nfrom fastchat.train.train import (\n    DataArguments,\n    ModelArguments,\n    make_supervised_data_module,\n)\n\nfrom fastchat.train.llama_flash_attn_monkey_patch import (\n    replace_llama_attn_with_flash_attn,\n)\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: typing.Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n    flash_attn: bool = False\n\n\n@dataclass\nclass LoraArguments:\n    lora_r: int = 8\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    lora_target_modules: typing.List[str] = field(\n        default_factory=lambda: [\"q_proj\", \"v_proj\"]\n    )\n    lora_weight_path: str = \"\"\n    lora_bias: str = \"none\"\n    q_lora: bool = False\n\n\ndef maybe_zero_3(param):\n    if hasattr(param, \"ds_id\"):\n        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n        with zero.GatheredParameters([param]):\n            param = param.data.detach().cpu().clone()\n    else:\n        param = param.detach().cpu().clone()\n    return param\n\n\n# Borrowed from peft.utils.get_peft_model_state_dict\ndef get_peft_state_maybe_zero_3(named_params, bias):\n    if bias == \"none\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k}\n    elif bias == \"all\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\n    elif bias == \"lora_only\":\n        to_return = {}\n        maybe_lora_bias = {}\n        lora_bias_names = set()\n        for k, t in named_params:\n            if \"lora_\" in k:\n                to_return[k] = t\n                bias_name = k.split(\"lora_\")[0] + \"bias\"\n                lora_bias_names.add(bias_name)\n            elif \"bias\" in k:\n                maybe_lora_bias[k] = t\n        for k, t in maybe_lora_bias:\n            if bias_name in lora_bias_names:\n                to_return[bias_name] = t\n    else:\n        raise NotImplementedError\n    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}\n    return to_return\n\n\ndef train():\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments, LoraArguments)\n    )\n    (\n        model_args,\n        data_args,\n        training_args,\n        lora_args,\n    ) = parser.parse_args_into_dataclasses()\n\n    if training_args.flash_attn:\n        replace_llama_attn_with_flash_attn()\n\n    device_map = None\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if lora_args.q_lora:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if ddp else None\n        if len(training_args.fsdp) > 0 or deepspeed.is_deepspeed_zero3_enabled():\n            logging.warning(\n                \"FSDP and ZeRO3 are both currently incompatible with QLoRA.\"\n            )\n\n    compute_dtype = (\n        torch.float16\n        if training_args.fp16\n        else (torch.bfloat16 if training_args.bf16 else torch.float32)\n    )\n\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        device_map=device_map,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=compute_dtype,\n        )\n        if lora_args.q_lora\n        else None,\n    )\n    lora_config = LoraConfig(\n        r=lora_args.lora_r,\n        lora_alpha=lora_args.lora_alpha,\n        target_modules=lora_args.lora_target_modules,\n        lora_dropout=lora_args.lora_dropout,\n        bias=lora_args.lora_bias,\n        task_type=\"CAUSAL_LM\",\n    )\n\n    if lora_args.q_lora:\n        model = prepare_model_for_kbit_training(\n            model, use_gradient_checkpointing=training_args.gradient_checkpointing\n        )\n        if not ddp and torch.cuda.device_count() > 1:\n            # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n            model.is_parallelizable = True\n            model.model_parallel = True\n\n    model = get_peft_model(model, lora_config)\n    if training_args.flash_attn:\n        for name, module in model.named_modules():\n            if \"norm\" in name:\n                module = module.to(compute_dtype)\n            if \"lm_head\" in name or \"embed_tokens\" in name:\n                if hasattr(module, \"weight\"):\n                    module = module.to(compute_dtype)\n    if training_args.deepspeed is not None and training_args.local_rank == 0:\n        model.print_trainable_parameters()\n\n    if training_args.gradient_checkpointing:\n        model.enable_input_require_grads()\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    tokenizer.pad_token = tokenizer.unk_token\n\n    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    model.config.use_cache = False\n\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n    trainer.save_state()\n\n    # check if zero3 mode enabled\n    if deepspeed.is_deepspeed_zero3_enabled():\n        # use deepspeed engine internal function to gather state dict\n        # state_dict_zero3 contains whole parameters of base and lora adapters\n        # we will not extract lora parameters since peft save_pretrained will do that\n        # https://github.com/huggingface/peft/blob/3714aa2fff158fdfa637b2b65952580801d890b2/src/peft/peft_model.py#L125\n        # https://github.com/huggingface/peft/blob/3714aa2fff158fdfa637b2b65952580801d890b2/src/peft/utils/save_and_load.py#L19\n        state_dict_zero3 = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n        if training_args.local_rank == 0:\n            state_dict = state_dict_zero3\n    else:\n        # in other mode we use original code from fastchat team, to make sure our change is minimum\n        state_dict = get_peft_state_maybe_zero_3(\n            model.named_parameters(), lora_args.lora_bias\n        )\n\n    if training_args.local_rank == 0:\n        model.save_pretrained(training_args.output_dir, state_dict=state_dict)\n\n\nif __name__ == \"__main__\":\n    train()\n", "fastchat/train/train_lora_t5.py": "# Adapted from tatsu-lab@stanford_alpaca. Below is the original copyright:\n#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\nfrom collections import defaultdict\nimport copy\nimport os\nfrom dataclasses import dataclass, field\nimport random\nimport json\nimport logging\nimport pathlib\nfrom typing import Dict, Optional, Sequence, List\n\nimport torch\nimport torch.distributed as dist\n\n\nfrom deepspeed import zero\nfrom deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n\nimport transformers\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer, AddedToken, BitsAndBytesConfig, deepspeed\n\nfrom fastchat.train.train_flant5 import (\n    smart_tokenizer_and_embedding_resize,\n    make_supervised_data_module,\n)\n\nfrom fastchat.train.train_lora import get_peft_state_maybe_zero_3\n\nfrom fastchat.model.model_adapter import get_conversation_template\n\ndefault_conversation = get_conversation_template(\"t5\")\n\n# TODO: import and use code from ../data/dataset.py\n\nIGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"\nDEFAULT_BOS_TOKEN = \"</s>\"\nDEFAULT_UNK_TOKEN = \"</s>\"\n\n\n@dataclass\nclass LoraArguments:\n    lora_r: int = 8\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    lora_target_modules: List[str] = field(default_factory=lambda: [\"q\", \"v\"])\n    lora_weight_path: str = \"\"\n    lora_bias: str = \"none\"\n    q_lora: bool = False\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    lazy_preprocess: bool = False\n    num_data: int = -1\n    preprocessed_path: str = field(\n        default=None, metadata={\"help\": \"Path to the preprocessed training data.\"}\n    )\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=2048,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n\n\ndef safe_save_model_for_hf_trainer(\n    trainer: transformers.Trainer, output_dir: str, state_dict: dict\n):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n\n    if trainer.args.should_save:\n        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n        del state_dict\n        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n\n\ndef train():\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments, LoraArguments)\n    )\n    (\n        model_args,\n        data_args,\n        training_args,\n        lora_args,\n    ) = parser.parse_args_into_dataclasses()\n\n    device_map = None\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if lora_args.q_lora:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if ddp else None\n        if len(training_args.fsdp) > 0 or deepspeed.is_deepspeed_zero3_enabled():\n            logging.warning(\n                \"FSDP and ZeRO3 are both currently incompatible with QLoRA.\"\n            )\n\n    compute_dtype = (\n        torch.float16\n        if training_args.fp16\n        else (torch.bfloat16 if training_args.bf16 else torch.float32)\n    )\n\n    model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        device_map=device_map,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=compute_dtype,\n        )\n        if lora_args.q_lora\n        else None,\n    )\n\n    lora_config = LoraConfig(\n        r=lora_args.lora_r,\n        lora_alpha=lora_args.lora_alpha,\n        target_modules=lora_args.lora_target_modules,\n        lora_dropout=lora_args.lora_dropout,\n        bias=lora_args.lora_bias,\n        task_type=TaskType.SEQ_2_SEQ_LM,\n    )\n\n    if lora_args.q_lora:\n        model = prepare_model_for_kbit_training(\n            model, use_gradient_checkpointing=training_args.gradient_checkpointing\n        )\n        if not ddp and torch.cuda.device_count() > 1:\n            # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n            model.is_parallelizable = True\n            model.model_parallel = True\n\n    model = get_peft_model(model, lora_config)\n    if training_args.deepspeed is not None and training_args.local_rank == 0:\n        model.print_trainable_parameters()\n\n    if training_args.gradient_checkpointing:\n        model.enable_input_require_grads()\n\n    # Dacheng: Note we can only use T5Tokenizer, otherwise it will prepend\n    # a space before special tokens.\n    tokenizer = transformers.T5Tokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n\n    smart_tokenizer_and_embedding_resize(\n        special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n        other_tokens=[\"<\", \"{\", \"\\n\", \"}\", \"`\", \" \", \"\\\\\", \"^\", \"\\t\"],\n        tokenizer=tokenizer,\n        model=model,\n    )\n\n    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n    trainer.save_state()\n    # check if zero3 mode enabled\n    if deepspeed.is_deepspeed_zero3_enabled():\n        # use deepspeed engine internal function to gather state dict\n        # state_dict_zero3 contains whole parameters of base and lora adapters\n        # we will not extract lora parameters since peft save_pretrained will do that\n        # https://github.com/huggingface/peft/blob/3714aa2fff158fdfa637b2b65952580801d890b2/src/peft/peft_model.py#L125\n        # https://github.com/huggingface/peft/blob/3714aa2fff158fdfa637b2b65952580801d890b2/src/peft/utils/save_and_load.py#L19\n        state_dict_zero3 = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n        if training_args.local_rank == 0:\n            state_dict = state_dict_zero3\n    else:\n        # in other mode we use original code from fastchat team, to make sure our change is minimum\n        state_dict = get_peft_state_maybe_zero_3(\n            model.named_parameters(), lora_args.lora_bias\n        )\n\n    if training_args.local_rank == 0:\n        safe_save_model_for_hf_trainer(\n            trainer=trainer, output_dir=training_args.output_dir, state_dict=state_dict\n        )\n\n\nif __name__ == \"__main__\":\n    train()\n", "fastchat/train/train_baichuan.py": "# This code is based on tatsu-lab/stanford_alpaca. Below is the original copyright:\n#\n#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\nfrom dataclasses import dataclass, field\nimport json\nimport math\nimport jsonlines\nimport pathlib\nfrom multiprocessing import Pool\nfrom typing import Dict, Optional, Sequence\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport transformers\nfrom transformers import Trainer\nfrom transformers.trainer_pt_utils import LabelSmoother\n\nfrom fastchat.conversation import SeparatorStyle\nfrom fastchat.model.model_adapter import get_conversation_template\n\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    lazy_preprocess: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n\n\nlocal_rank = None\n\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    state_dict = trainer.model.state_dict()\n    if trainer.args.should_save:\n        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n        del state_dict\n        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n\n\ndef apply_prompt_template(sources, systems=None):\n    conv = get_conversation_template(\"vicuna\")\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        if systems and systems[i]:\n            conv.set_system_message(systems[i])\n        prompt = conv.get_prompt()\n        conversations.append(prompt)\n    return conversations, conv\n\n\ndef tokenize_conversations(conversations, tokenizer):\n    input_ids = tokenizer(\n        conversations,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n    ).input_ids\n    targets = input_ids.clone()\n    return input_ids, targets\n\n\ndef mask_targets(conversations, targets, tokenizer, conv):\n    sep = conv.sep + conv.roles[1] + \": \"\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        turns = conversation.split(conv.sep2)\n        cur_len = 0\n        target[:cur_len] = IGNORE_TOKEN_ID\n        for i, turn in enumerate(turns):\n            if turn == \"\":\n                break\n            turn_len = len(tokenizer(turn + conv.sep2).input_ids)\n\n            parts = turn.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n            instruction_len = len(tokenizer(parts[0]).input_ids) - 1\n\n            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n            cur_len += turn_len\n\n        target[cur_len:] = IGNORE_TOKEN_ID\n\n        if False:  # Inspect and check the correctness of masking\n            z = target.clone()\n            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n            rank0_print(tokenizer.decode(z))\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_TOKEN_ID\n                rank0_print(\n                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n                    f\" (ignored)\"\n                )\n    return targets\n\n\ndef preprocess(sources, tokenizer: transformers.PreTrainedTokenizer, **kwargs) -> Dict:\n    systems = None if not kwargs else kwargs.get(\"systems\", None)\n\n    # If the data volume is small, process it directly in the main thread\n    if len(sources) <= 1000:\n        conversations, conv = apply_prompt_template(sources, systems)\n        input_ids, targets = tokenize_conversations(conversations, tokenizer)\n        targets = mask_targets(conversations, targets, tokenizer, conv)\n    else:  # If the data volume is large, use multithreading for processing\n        with Pool() as p:\n            conversations, conv = p.apply_async(\n                apply_prompt_template, (sources, systems)\n            ).get()\n            input_ids, targets = p.apply_async(\n                tokenize_conversations, (conversations, tokenizer)\n            ).get()\n            targets = p.apply_async(\n                mask_targets, (conversations, targets, tokenizer, conv)\n            ).get()\n            p.close()\n            p.join()\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n    )\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        systems = [example.get(\"system\", \"\") for example in raw_data]\n        sources = [example[\"conversations\"] for example in raw_data]\n\n        data_dict = preprocess(sources, tokenizer, systems=systems)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n        self.raw_data = raw_data\n        self.cached_data_dict = {}\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        if i in self.cached_data_dict:\n            return self.cached_data_dict[i]\n\n        ret = preprocess(\n            [self.raw_data[i][\"conversations\"]],\n            self.tokenizer,\n            systems=[self.raw_data[i].get(\"system\", \"\")],\n        )\n        ret = dict(\n            input_ids=ret[\"input_ids\"][0],\n            labels=ret[\"labels\"][0],\n            attention_mask=ret[\"attention_mask\"][0],\n        )\n        self.cached_data_dict[i] = ret\n\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer, data_args, train_ratio=0.98\n) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    train_ratio = min(train_ratio, 1.0)\n    dataset_cls = (\n        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n    )\n    rank0_print(\"Loading data...\")\n    data_path = data_args.data_path\n    if data_path.endswith(\".json\"):\n        raw_data = json.load(open(data_path, \"r\"))\n    elif data_path.endswith(\".jsonl\"):\n        with jsonlines.open(data_path, mode=\"r\") as reader:\n            raw_data = [item for item in reader]\n\n    # Split train/test\n    np.random.seed(0)\n    perm = np.random.permutation(len(raw_data))\n    split = int(len(perm) * train_ratio)\n    train_indices = perm[:split]\n    if train_ratio < 1:\n        eval_indices = perm[split:]\n    else:\n        # if train_ratio==1, we use 5% of data as eval data, make sure trainer will not throw error when eval data is empty\n        eval_indices = perm[-int(len(perm) * 0.05) :]\n    train_raw_data = [raw_data[i] for i in train_indices]\n    eval_raw_data = [raw_data[i] for i in eval_indices]\n    rank0_print(f\"#train {len(train_raw_data)}, #eval {len(eval_raw_data)}\")\n\n    train_dataset = dataset_cls(train_raw_data, tokenizer=tokenizer)\n    eval_dataset = dataset_cls(eval_raw_data, tokenizer=tokenizer)\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n\n\ndef train():\n    global local_rank\n\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments)\n    )\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    local_rank = training_args.local_rank\n    config = transformers.AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        trust_remote_code=True,\n        cache_dir=training_args.cache_dir,\n    )\n    # Set RoPE scaling factor\n    orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n    if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n        scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n    config.use_cache = False\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        trust_remote_code=True,\n        cache_dir=training_args.cache_dir,\n    )\n    # Tie the weights\n    model.tie_weights()\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        trust_remote_code=True,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    # NOTE: if the token_id exceed the vocab_size will cause failing in training process! we need add special config and resize the embedding size!\n    tokenizer.pad_token = tokenizer.unk_token\n    print(f\"tokens len: {len(tokenizer)}\")\n    model.resize_token_embeddings(len(tokenizer))\n\n    data_module = make_supervised_data_module(\n        tokenizer=tokenizer, train_ratio=0.98, data_args=data_args\n    )\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n    trainer.save_state()\n    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n\n\nif __name__ == \"__main__\":\n    train()\n", "fastchat/train/train_with_template.py": "# This code is based on tatsu-lab/stanford_alpaca. Below is the original copyright:\n#\n#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\nfrom dataclasses import dataclass, field\nimport json\nimport math\nimport jsonlines\nimport pathlib\nfrom multiprocessing import Pool\nfrom typing import Dict, Optional, Sequence\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport transformers\nfrom transformers import Trainer\nfrom transformers.trainer_pt_utils import LabelSmoother\n\nfrom fastchat.conversation import SeparatorStyle\nfrom fastchat.model.model_adapter import get_conversation_template\n\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    lazy_preprocess: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n\n\nlocal_rank = None\n\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    state_dict = trainer.model.state_dict()\n    if trainer.args.should_save:\n        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n        del state_dict\n        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n\n\ndef apply_prompt_template(sources, template_id, systems=None):\n    conv = get_conversation_template(template_id)\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        if systems and systems[i]:\n            conv.set_system_message(systems[i])\n        prompt = conv.get_prompt()\n        conversations.append(prompt)\n    return conversations, conv\n\n\ndef tokenize_conversations(conversations, tokenizer):\n    input_ids = tokenizer(\n        conversations,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n    ).input_ids\n    targets = input_ids.clone()\n    return input_ids, targets\n\n\ndef get_prompt_separator(conv):\n    if conv.sep_style == SeparatorStyle.ADD_COLON_SINGLE:\n        user_turn_separator = conv.sep2\n        assistant_turn_separator = conv.roles[1] + \": \"\n\n    elif conv.sep_style == SeparatorStyle.ADD_COLON_TWO:\n        user_turn_separator = conv.sep2\n        assistant_turn_separator = conv.roles[1] + \": \"\n\n    elif conv.sep_style == SeparatorStyle.ADD_COLON_SPACE_SINGLE:\n        if conv.sep2 is None:\n            user_turn_separator = conv.roles[0] + \": \"\n        else:\n            user_turn_separator = conv.sep2\n\n        assistant_turn_separator = conv.roles[1] + \": \"\n\n    elif conv.sep_style == SeparatorStyle.LLAMA2:\n        user_turn_separator = conv.sep2\n        assistant_turn_separator = conv.roles[1] + \" \"\n\n    elif conv.sep_style == SeparatorStyle.CHATML:\n        if conv.sep2 is None:\n            user_turn_separator = conv.sep + \"\\n\"\n        else:\n            user_turn_separator = conv.sep2 + \"\\n\"\n\n        assistant_turn_separator = conv.roles[1] + \"\\n\"\n\n    return user_turn_separator, assistant_turn_separator\n\n\ndef mask_targets(conversations, targets, tokenizer, conv):\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n        if tokenizer.eos_token is None:\n            cur_len = 0\n        elif tokenizer.eos_token is not None and target[0] != tokenizer.bos_token_id:\n            cur_len = 0\n        elif tokenizer.eos_token is not None and target[0] == tokenizer.bos_token_id:\n            cur_len = 1\n\n        target[:cur_len] = IGNORE_TOKEN_ID\n        user_turn_separator, assistant_turn_separator = get_prompt_separator(conv)\n        turns = conversation.split(user_turn_separator)\n        for i, turn in enumerate(turns):\n            if (\n                i < len(turns) - 1 and turn == \"\"\n            ):  # Last turn is the user_turn_separator\n                break\n\n            if i != 0:\n                turn = user_turn_separator + turn\n\n            turn_len = len(tokenizer(turn, add_special_tokens=False).input_ids)\n\n            if assistant_turn_separator in turn:\n                parts = turn.rsplit(assistant_turn_separator)\n                parts[0] += assistant_turn_separator\n            else:\n                parts = [turn]\n\n            instruction_len = len(\n                tokenizer(parts[0], add_special_tokens=False).input_ids\n            )\n\n            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n            cur_len += turn_len\n\n        target[cur_len:] = IGNORE_TOKEN_ID\n\n        if False:  # Inspect and check the correctness of masking\n            z = target.clone()\n            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n            rank0_print(tokenizer.decode(z))\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_TOKEN_ID\n                rank0_print(\n                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n                    f\" (ignored)\"\n                )\n    return targets\n\n\ndef preprocess(\n    sources, tokenizer: transformers.PreTrainedTokenizer, template_id, **kwargs\n) -> Dict:\n    systems = None if not kwargs else kwargs.get(\"systems\", None)\n\n    # If the data volume is small, process it directly in the main thread\n    if len(sources) <= 1000:\n        conversations, conv = apply_prompt_template(sources, template_id, systems)\n        input_ids, targets = tokenize_conversations(conversations, tokenizer)\n        targets = mask_targets(conversations, targets, tokenizer, conv)\n    else:  # If the data volume is large, use multithreading for processing\n        with Pool() as p:\n            conversations, conv = p.apply_async(\n                apply_prompt_template, (sources, template_id, systems)\n            ).get()\n            input_ids, targets = p.apply_async(\n                tokenize_conversations, (conversations, tokenizer)\n            ).get()\n            targets = p.apply_async(\n                mask_targets, (conversations, targets, tokenizer, conv)\n            ).get()\n            p.close()\n            p.join()\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n    )\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(\n        self, raw_data, tokenizer: transformers.PreTrainedTokenizer, template_id\n    ):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        systems = [example.get(\"system\", \"\") for example in raw_data]\n        sources = [example[\"conversations\"] for example in raw_data]\n\n        data_dict = preprocess(sources, tokenizer, template_id, systems=systems)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(\n        self, raw_data, tokenizer: transformers.PreTrainedTokenizer, template_id\n    ):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.template_id = template_id\n\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n        self.raw_data = raw_data\n        self.cached_data_dict = {}\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        if i in self.cached_data_dict:\n            return self.cached_data_dict[i]\n\n        ret = preprocess(\n            [self.raw_data[i][\"conversations\"]],\n            self.tokenizer,\n            self.template_id,\n            systems=[self.raw_data[i].get(\"system\", \"\")],\n        )\n        ret = dict(\n            input_ids=ret[\"input_ids\"][0],\n            labels=ret[\"labels\"][0],\n            attention_mask=ret[\"attention_mask\"][0],\n        )\n        self.cached_data_dict[i] = ret\n\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer,\n    data_args,\n    template_id,\n    train_ratio=0.98,\n) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    train_ratio = min(train_ratio, 1.0)\n    dataset_cls = (\n        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n    )\n    rank0_print(\"Loading data...\")\n    data_path = data_args.data_path\n    if data_path.endswith(\".json\"):\n        raw_data = json.load(open(data_path, \"r\"))\n    elif data_path.endswith(\".jsonl\"):\n        with jsonlines.open(data_path, mode=\"r\") as reader:\n            raw_data = [item for item in reader]\n\n    # Split train/test\n    np.random.seed(0)\n    perm = np.random.permutation(len(raw_data))\n    split = int(len(perm) * train_ratio)\n    train_indices = perm[:split]\n    if train_ratio < 1:\n        eval_indices = perm[split:]\n    else:\n        # if train_ratio==1, we use 5% of data as eval data, make sure trainer will not throw error when eval data is empty\n        eval_indices = perm[-int(len(perm) * 0.05) :]\n    train_raw_data = [raw_data[i] for i in train_indices]\n    eval_raw_data = [raw_data[i] for i in eval_indices]\n    rank0_print(f\"#train {len(train_raw_data)}, #eval {len(eval_raw_data)}\")\n\n    train_dataset = dataset_cls(\n        train_raw_data, tokenizer=tokenizer, template_id=template_id\n    )\n    eval_dataset = dataset_cls(\n        eval_raw_data, tokenizer=tokenizer, template_id=template_id\n    )\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n\n\ndef train():\n    global local_rank\n\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments)\n    )\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    local_rank = training_args.local_rank\n    config = transformers.AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        trust_remote_code=True,\n        cache_dir=training_args.cache_dir,\n    )\n    # Set RoPE scaling factor\n    orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n    if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n        scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n    config.use_cache = False\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        trust_remote_code=True,\n        cache_dir=training_args.cache_dir,\n    )\n    # Tie the weights\n    model.tie_weights()\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        trust_remote_code=True,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    # NOTE: if the token_id exceed the vocab_size will cause failing in training process! we need add special config and resize the embedding size!\n    tokenizer.pad_token = tokenizer.unk_token\n    tokenizer.pad_token_id = tokenizer.unk_token_id\n    print(f\"tokens len: {len(tokenizer)}\")\n    model.resize_token_embeddings(len(tokenizer))\n\n    template_id = model_args.model_name_or_path\n    data_module = make_supervised_data_module(\n        tokenizer=tokenizer,\n        template_id=template_id,\n        train_ratio=0.98,\n        data_args=data_args,\n    )\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n    trainer.save_state()\n    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n\n\nif __name__ == \"__main__\":\n    train()\n", "fastchat/train/llama2_flash_attn_monkey_patch.py": "import warnings\nfrom typing import Optional, Tuple\n\nimport torch\nfrom flash_attn import __version__ as flash_attn_version\nfrom flash_attn.bert_padding import pad_input, unpad_input\nfrom flash_attn.flash_attn_interface import (\n    flash_attn_func,\n    flash_attn_varlen_kvpacked_func,\n)\nfrom transformers.models.llama.modeling_llama import (\n    LlamaAttention,\n    LlamaModel,\n    rotate_half,\n)\n\n\ndef apply_rotary_pos_emb(q, k, cos_sin, position_ids):\n    gather_indices = position_ids[:, :, None, None]  # [bsz, seq_len, 1, 1]\n    gather_indices = gather_indices.repeat(\n        1, 1, cos_sin[0].shape[1], cos_sin[0].shape[3]\n    )\n    bsz = gather_indices.shape[0]\n    cos, sin = (\n        torch.gather(x.transpose(1, 2).repeat(bsz, 1, 1, 1), 1, gather_indices)\n        for x in cos_sin\n    )\n    q, k = ((x * cos) + (rotate_half(x) * sin) for x in (q, k))\n    return q, k\n\n\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    padding_mask: Optional[torch.Tensor] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    if output_attentions:\n        warnings.warn(\n            \"Output attentions is not supported for patched `LlamaAttention`, returning `None` instead.\"\n        )\n\n    bsz, q_len, _ = hidden_states.size()\n    kv_heads = getattr(self, \"num_key_value_heads\", self.num_heads)\n\n    q, k, v = (\n        op(hidden_states).view(bsz, q_len, nh, self.head_dim)\n        for op, nh in (\n            (self.q_proj, self.num_heads),\n            (self.k_proj, kv_heads),\n            (self.v_proj, kv_heads),\n        )\n    )\n    # shape: (b, s, num_heads, head_dim)\n\n    kv_seq_len = k.shape[1]\n    past_kv_len = 0\n    if past_key_value is not None:\n        past_kv_len = past_key_value[0].shape[2]\n        kv_seq_len += past_kv_len\n\n    cos_sin = self.rotary_emb(v, seq_len=kv_seq_len)\n    q, k = apply_rotary_pos_emb(q, k, cos_sin, position_ids)\n\n    if past_key_value is not None:\n        assert (\n            flash_attn_version >= \"2.1.0\"\n        ), \"past_key_value support requires flash-attn >= 2.1.0\"\n        # reuse k, v\n        k = torch.cat([past_key_value[0].transpose(1, 2), k], dim=1)\n        v = torch.cat([past_key_value[1].transpose(1, 2), v], dim=1)\n\n    past_key_value = (k.transpose(1, 2), v.transpose(1, 2)) if use_cache else None\n\n    if attention_mask is None:\n        output = flash_attn_func(q, k, v, 0.0, softmax_scale=None, causal=True).view(\n            bsz, q_len, -1\n        )\n    else:\n        q, indices, cu_q_lens, max_s = unpad_input(q, attention_mask[:, -q_len:])\n        # We can skip concat and call unpad twice but seems better to call unpad only once.\n        kv, _, cu_k_lens, max_k = unpad_input(\n            torch.stack((k, v), dim=2), attention_mask\n        )\n        output_unpad = flash_attn_varlen_kvpacked_func(\n            q,\n            kv,\n            cu_q_lens,\n            cu_k_lens,\n            max_s,\n            max_k,\n            0.0,\n            softmax_scale=None,\n            causal=True,\n        )\n        output_unpad = output_unpad.reshape(-1, self.num_heads * self.head_dim)\n        output = pad_input(output_unpad, indices, bsz, q_len)\n\n    return self.o_proj(output), None, past_key_value\n\n\n# Disable the transformation of the attention mask in LlamaModel as flash attention\n# takes a boolean key_padding_mask. Fills in the past kv length for use in forward.\ndef _prepare_decoder_attention_mask(\n    self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n):\n    # [bsz, seq_len]\n    if past_key_values_length > 0 and attention_mask is not None:\n        attention_mask = torch.cat(\n            (\n                torch.full(\n                    (input_shape[0], past_key_values_length),\n                    True,\n                    dtype=attention_mask.dtype,\n                    device=attention_mask.device,\n                ),\n                attention_mask,\n            ),\n            dim=-1,\n        )\n\n    if attention_mask is not None and torch.all(attention_mask):\n        return None  # This uses the faster call when training with full samples\n\n    return attention_mask\n\n\ndef replace_llama_attn_with_flash_attn():\n    cuda_major, cuda_minor = torch.cuda.get_device_capability()\n    if cuda_major < 8:\n        warnings.warn(\n            \"Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.\"\n            \"ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\"\n        )\n\n    LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask\n    LlamaAttention.forward = forward\n\n\ndef test():\n    from fastchat.train.llama_flash_attn_monkey_patch import forward as fastchat_forward\n    from transformers.models.llama.configuration_llama import LlamaConfig\n\n    config = LlamaConfig(\n        hidden_size=1024,\n        intermediate_size=128,\n        num_hidden_layers=1,\n        num_attention_heads=8,\n        max_position_embeddings=16,\n    )\n    device = torch.device(\"cuda\")\n    model = LlamaModel(config)\n    attn = LlamaAttention(config).to(device).half()\n    bsz, hs, seqlen = 2, config.hidden_size, config.max_position_embeddings\n    position_ids = torch.arange(seqlen, dtype=torch.long, device=device).view(\n        -1, seqlen\n    )\n\n    mask = torch.full((bsz, seqlen), True, dtype=torch.bool, device=device)\n    for i in range(4):\n        hidden = torch.rand((bsz, seqlen, hs), dtype=torch.float16, device=device)\n        if i:\n            mask[0, -i:] = False\n            mask[1, :i] = False\n\n        lmask = model._prepare_decoder_attention_mask(mask, hidden.shape[:2], hidden, 0)\n        ref, _, _ = attn.forward(\n            hidden, attention_mask=lmask, position_ids=position_ids\n        )\n\n        fast, _, _ = fastchat_forward(\n            attn, hidden, attention_mask=mask, position_ids=position_ids\n        )\n\n        lmask = _prepare_decoder_attention_mask(\n            model, mask, hidden.shape[:2], hidden, 0\n        )\n        test, _, _ = forward(\n            attn, hidden, attention_mask=lmask, position_ids=position_ids\n        )\n\n        print(f\"Mean(abs(ref)) = {torch.mean(torch.abs(ref))}\")\n        print(f\"Mean(abs(ref - fast)) = {torch.mean(torch.abs(ref - fast))}\")\n        print(f\"Mean(abs(ref - test)) = {torch.mean(torch.abs(ref - test))}\")\n        print(f\"Mean(abs(fast - test)) = {torch.mean(torch.abs(fast - test))}\")\n        print(f\"allclose(fast, test) = {torch.allclose(fast, test)}\")\n\n    with torch.no_grad():\n        # Also check that past_kv is handled properly\n        hidden = torch.rand((bsz, seqlen, hs), dtype=torch.float16, device=device)\n        part_len = seqlen // 4\n        assert part_len * 4 == seqlen\n        mask = torch.full((bsz, seqlen), True, dtype=torch.bool, device=device)\n        mask[0, -2:] = False\n        lmask = _prepare_decoder_attention_mask(\n            model, mask, hidden.shape[:2], hidden, 0\n        )\n        oneshot, _, _ = forward(\n            attn, hidden, attention_mask=lmask, position_ids=position_ids\n        )\n        parts = []\n        past_kv, past_kv_len = None, 0\n        for i in range(4):\n            start = part_len * i\n            end = start + part_len\n            hidden_part = hidden[:, start:end, ...]\n            lmask = _prepare_decoder_attention_mask(\n                model,\n                mask[:, start:end],\n                hidden_part.shape[:2],\n                hidden_part,\n                past_kv_len,\n            )\n            part, _, past_kv = forward(\n                attn,\n                hidden_part.clone(),\n                attention_mask=lmask,\n                position_ids=position_ids[:, start:end],\n                past_key_value=past_kv,\n                use_cache=True,\n            )\n            parts.append(part)\n            past_kv_len = past_kv[0].shape[2]\n\n        print(\n            f\"allclose(oneshot[:, 0], parts[0]) = {torch.allclose(oneshot[:, :part_len], parts[0])}\"\n        )\n        print(\n            f\"allclose(oneshot, parts) = {torch.allclose(oneshot, torch.cat(parts, dim=1))}\"\n        )\n\n\nif __name__ == \"__main__\":\n    test()\n", "fastchat/train/train_yuan2.py": "# This code is based on tatsu-lab/stanford_alpaca. Below is the original copyright:\n#\n#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\nfrom dataclasses import dataclass, field\nimport json\nimport math\nimport pathlib\nfrom typing import Dict, Optional, Sequence\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport transformers\nfrom transformers import Trainer\nfrom transformers.trainer_pt_utils import LabelSmoother\n\nfrom fastchat.conversation import SeparatorStyle\nfrom fastchat.model.model_adapter import get_conversation_template\n\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n    trust_remote_code: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether or not to allow for custom models defined on the Hub in their own modeling files\"\n        },\n    )\n    padding_side: str = field(\n        default=\"right\", metadata={\"help\": \"The padding side in tokenizer\"}\n    )\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    eval_data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n    )\n    lazy_preprocess: bool = False\n    last_response_loss: bool = False\n    split_example_loss: bool = False\n    efficient_loss: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n\n\nlocal_rank = None\n\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\ndef trainer_save_model_safe(trainer: transformers.Trainer):\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n    from torch.distributed.fsdp import StateDictType, FullStateDictConfig\n\n    save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n    with FSDP.state_dict_type(\n        trainer.model, StateDictType.FULL_STATE_DICT, save_policy\n    ):\n        trainer.save_model()\n\n\n# add by wpf for yuan test\ndef right_replace(string, old, new, max=1):\n    return string[::-1].replace(old[::-1], new[::-1], max)[::-1]\n\n\ndef preprocess(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n    data_args,\n) -> Dict:\n    conv = get_conversation_template(\"yuan2\")  # wpf\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n        if data_args.last_response_loss:\n            a = conversations[0].replace(\"<sep>\", \"<eod>\")\n            a = right_replace(a, \"<n>\", \"<sep>\")\n            # a=right_replace(a,\"<n>\",\"\\n\",max=20)\n            conversations[0] = a\n        if data_args.split_example_loss:\n            a = conversations[0].replace(\"<sep>\", \"\")\n            a = a.split(\"<n>\")\n            for i in range(int(len(a) / 2)):\n                if i == 0:\n                    conversations[i] = \"\"\n                if i != 0:\n                    conversations.append(\"\")\n                for j in range(i * 2):\n                    conversations[i] = conversations[i] + a[j] + \"<n>\"\n                conversations[i] = (\n                    conversations[i] + a[i * 2] + \"<sep>\" + a[i * 2 + 1] + \"<eod>\"\n                )\n\n        if data_args.efficient_loss:\n            a = conversations[0].replace(\"<sep>\", \"<eod>\")\n            conversations[0] = a\n\n        print(conversations)\n\n    # Tokenize conversations\n    input_ids = tokenizer(\n        conversations,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n    ).input_ids\n    targets = input_ids.clone()\n\n    # assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO  #wpf\n    # Mask targets. Only compute loss on the assistant outputs.\n    # sep = conv.sep + conv.roles[1] + \": \" #wpf\n\n    if data_args.split_example_loss:\n        for conversation, target in zip(conversations, targets):\n            total_len = int(target.ne(tokenizer.pad_token_id).sum())\n            turns = conversation.split(\"<sep>\")\n            cur_len = 1\n            target[:cur_len] = IGNORE_TOKEN_ID\n\n            for i, turn in enumerate(turns):\n                if turn == \"\":\n                    break\n                if i == 0 or i == len(turns) - 1:\n                    turn_len = len(tokenizer(turn).input_ids)\n                else:\n                    turn_len = len(tokenizer(turn).input_ids) + 1\n                # parts = turn.split(sep)\n                # if len(parts) != 2:\n                #     break\n                # parts[0] += sep\n                # \"-2\" is hardcoded for the Llama tokenizer to make the offset correct.\n                instruction_len = 0\n                if i == len(turns) - 1:\n                    instruction_len = turn_len\n                target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n                cur_len += turn_len\n\n            target[cur_len:] = IGNORE_TOKEN_ID\n            # print(\"cur_len:  \", cur_len)\n            # print(\"total_len:  \", total_len)\n\n            if False:  # Inspect and check the correctness of masking\n                z = target.clone()\n                z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n                rank0_print(tokenizer.decode(z))\n                exit()\n\n            if cur_len < tokenizer.model_max_length:\n                if cur_len != total_len:\n                    target[:] = IGNORE_TOKEN_ID\n                    rank0_print(\n                        f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n                        f\" #turn = {len(turns) - 1}. (ignored)\"\n                    )\n\n    if data_args.efficient_loss:\n        for conversation, target in zip(conversations, targets):\n            total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n            turns = conversation.split(\"<n>\")\n            cur_len = 1\n            target[:cur_len] = IGNORE_TOKEN_ID\n\n            for i, turn in enumerate(turns):\n                if turn == \"\":\n                    break\n                if i == 0 or i == len(turns) - 1:\n                    turn_len = len(tokenizer(turn).input_ids)\n                else:\n                    turn_len = len(tokenizer(turn).input_ids) + 1\n                # parts = turn.split(sep)\n                # if len(parts) != 2:\n                #     break\n                # parts[0] += sep\n                # \"-2\" is hardcoded for the Llama tokenizer to make the offset correct.\n                instruction_len = 0\n                if i % 2 == 0:\n                    instruction_len = turn_len\n\n                # if i != 0 and not tokenizer.legacy:\n                #     # The legacy and non-legacy modes handle special tokens differently\n                #     instruction_len -= 1\n\n                # Ignore the user instructions\n                target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n                cur_len += turn_len\n\n                if i != 0 and not tokenizer.legacy:\n                    # The legacy and non-legacy modes handle special tokens differently\n                    cur_len -= 1\n            target[cur_len:] = IGNORE_TOKEN_ID\n            # print(\"cur_len:  \", cur_len)\n            # print(\"total_len:  \", total_len)\n\n            if False:  # Inspect and check the correctness of masking\n                z = target.clone()\n                z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n                rank0_print(tokenizer.decode(z))\n                exit()\n\n            if cur_len < tokenizer.model_max_length:\n                if cur_len != total_len:\n                    target[:] = IGNORE_TOKEN_ID\n                    rank0_print(\n                        f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n                        f\" #turn = {len(turns) - 1}. (ignored)\"\n                    )\n    if data_args.last_response_loss:\n        for conversation, target in zip(conversations, targets):\n            total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n            turns = conversation.split(\"<sep>\")\n            cur_len = 1\n            target[:cur_len] = IGNORE_TOKEN_ID\n\n            for i, turn in enumerate(turns):\n                if turn == \"\":\n                    break\n                if i == 0 or i == len(turns) - 1:\n                    turn_len = len(tokenizer(turn).input_ids)\n                else:\n                    turn_len = len(tokenizer(turn).input_ids) + 1\n                # parts = turn.split(sep)\n                # if len(parts) != 2:\n                #     break\n                # parts[0] += sep\n                # \"-2\" is hardcoded for the Llama tokenizer to make the offset correct.\n                instruction_len = 0\n                if i == len(turns) - 1:\n                    instruction_len = turn_len\n\n                # if i != 0 and not tokenizer.legacy:\n                #     # The legacy and non-legacy modes handle special tokens differently\n                #     instruction_len -= 1\n\n                # Ignore the user instructions\n                target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n                cur_len += turn_len\n\n                # if i != 0 and not tokenizer.legacy:\n                #     # The legacy and non-legacy modes handle special tokens differently\n                #     cur_len -= 1\n\n            target[cur_len:] = IGNORE_TOKEN_ID\n            # print(\"cur_len:  \", cur_len)\n            # print(\"total_len:  \", total_len)\n\n            if False:  # Inspect and check the correctness of masking\n                z = target.clone()\n                z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n                rank0_print(tokenizer.decode(z))\n                exit()\n\n            if cur_len < tokenizer.model_max_length:\n                if cur_len != total_len:\n                    target[:] = IGNORE_TOKEN_ID\n                    rank0_print(\n                        f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n                        f\" #turn = {len(turns) - 1}. (ignored)\"\n                    )\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n    )\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(\n        self, raw_data, data_args, tokenizer: transformers.PreTrainedTokenizer\n    ):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        sources = [example[\"conversations\"] for example in raw_data]\n        data_dict = preprocess(sources, tokenizer, data_args)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(\n        self, raw_data, data_args, tokenizer: transformers.PreTrainedTokenizer\n    ):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n        self.tokenizer = tokenizer\n        self.raw_data = raw_data\n        self.data_args = data_args\n        self.cached_data_dict = {}\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        if i in self.cached_data_dict:\n            return self.cached_data_dict[i]\n\n        ret = preprocess(\n            [self.raw_data[i][\"conversations\"]], self.tokenizer, self.data_args\n        )\n        ret = dict(\n            input_ids=ret[\"input_ids\"][0],\n            labels=ret[\"labels\"][0],\n            attention_mask=ret[\"attention_mask\"][0],\n        )\n        self.cached_data_dict[i] = ret\n\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer, data_args\n) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = (\n        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n    )\n    rank0_print(\"Loading data...\")\n\n    train_json = json.load(open(data_args.data_path, \"r\"))\n    train_dataset = dataset_cls(train_json, data_args, tokenizer=tokenizer)\n\n    if data_args.eval_data_path:\n        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n        eval_dataset = dataset_cls(eval_json, data_args, tokenizer=tokenizer)\n    else:\n        eval_dataset = None\n\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n\n\ndef train():\n    global local_rank\n\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments)\n    )\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    local_rank = training_args.local_rank\n\n    # Set RoPE scaling factor\n    config = transformers.AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        trust_remote_code=model_args.trust_remote_code,\n    )\n    orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n    if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n        scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n    config.use_cache = False\n\n    # Load model and tokenizer\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        cache_dir=training_args.cache_dir,\n        trust_remote_code=model_args.trust_remote_code,\n    )\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=model_args.padding_side,\n        use_fast=False,\n        trust_remote_code=model_args.trust_remote_code,\n    )\n\n    if tokenizer.pad_token != tokenizer.unk_token:\n        tokenizer.pad_token = tokenizer.unk_token\n    tokenizer.add_tokens(\n        [\n            \"<eod>\",\n            \"<sep>\",\n            \"<pad>\",\n            \"<mask>\",\n            \"<predict>\",\n            \"<FIM_SUFFIX>\",\n            \"<FIM_PREFIX>\",\n            \"<FIM_MIDDLE>\",\n            \"<commit_before>\",\n            \"<commit_msg>\",\n            \"<commit_after>\",\n            \"<jupyter_start>\",\n            \"<jupyter_text>\",\n            \"<jupyter_code>\",\n            \"<jupyter_output>\",\n            \"<empty_output>\",\n        ],\n        special_tokens=True,\n    )\n\n    # Load data\n    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n\n    # Start trainner\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n\n    # Save model\n    model.config.use_cache = True\n    trainer.save_state()\n    if trainer.is_deepspeed_enabled:\n        trainer.save_model()\n    else:\n        trainer_save_model_safe(trainer)\n\n\nif __name__ == \"__main__\":\n    train()\n", "fastchat/train/llama_xformers_attn_monkey_patch.py": "\"\"\"\nDirectly copied the code from https://raw.githubusercontent.com/oobabooga/text-generation-webui/main/modules/llama_attn_hijack.py and made some adjustments\n\"\"\"\n\nimport logging\nimport math\nfrom typing import Optional, Tuple\n\nimport torch\nimport transformers.models.llama.modeling_llama\nfrom torch import nn\n\ntry:\n    import xformers.ops\nexcept ImportError:\n    logging.error(\"xformers not found! Please install it before trying to use it.\")\n\n\ndef replace_llama_attn_with_xformers_attn():\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = xformers_forward\n\n\ndef xformers_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    # pylint: disable=duplicate-code\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    (\n        query_states,\n        key_states,\n    ) = transformers.models.llama.modeling_llama.apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n    # [bsz, nh, t, hd]\n\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    # We only apply xformers optimizations if we don't need to output the whole attention matrix\n    if not output_attentions:\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        # This is a nasty hack. We know attention_mask in transformers is either LowerTriangular or all Zeros.\n        # We therefore check if one element in the upper triangular portion is zero. If it is, then the mask is all zeros.\n        if attention_mask is None or attention_mask[0, 0, 0, 1] == 0:\n            # input and output should be of form (bsz, q_len, num_heads, head_dim)\n            attn_output = xformers.ops.memory_efficient_attention(\n                query_states, key_states, value_states, attn_bias=None\n            )\n        else:\n            # input and output should be of form (bsz, q_len, num_heads, head_dim)\n            attn_output = xformers.ops.memory_efficient_attention(\n                query_states,\n                key_states,\n                value_states,\n                attn_bias=xformers.ops.LowerTriangularMask(),\n            )\n        attn_weights = None\n    else:\n        attn_weights = torch.matmul(\n            query_states, key_states.transpose(2, 3)\n        ) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            attn_weights = torch.max(\n                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n            )\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(\n            attn_weights, dim=-1, dtype=torch.float32\n        ).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n    attn_output = self.o_proj(attn_output)\n    return attn_output, attn_weights, past_key_value\n", "fastchat/train/train_mem.py": "# Make it more memory efficient by monkey patching the LLaMA model with FlashAttn.\n\n# Need to call this before importing transformers.\nfrom fastchat.train.llama2_flash_attn_monkey_patch import (\n    replace_llama_attn_with_flash_attn,\n)\n\nreplace_llama_attn_with_flash_attn()\n\nfrom fastchat.train.train import train\n\nif __name__ == \"__main__\":\n    train()\n", "fastchat/serve/huggingface_api.py": "\"\"\"\nUse FastChat with Hugging Face generation APIs.\n\nUsage:\npython3 -m fastchat.serve.huggingface_api --model lmsys/vicuna-7b-v1.5\npython3 -m fastchat.serve.huggingface_api --model lmsys/fastchat-t5-3b-v1.0\n\"\"\"\nimport argparse\n\nimport torch\n\nfrom fastchat.model import load_model, get_conversation_template, add_model_args\n\n\n@torch.inference_mode()\ndef main(args):\n    # Load model\n    model, tokenizer = load_model(\n        args.model_path,\n        device=args.device,\n        num_gpus=args.num_gpus,\n        max_gpu_memory=args.max_gpu_memory,\n        load_8bit=args.load_8bit,\n        cpu_offloading=args.cpu_offloading,\n        revision=args.revision,\n        debug=args.debug,\n    )\n\n    # Build the prompt with a conversation template\n    msg = args.message\n    conv = get_conversation_template(args.model_path)\n    conv.append_message(conv.roles[0], msg)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    # Run inference\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(args.device)\n    output_ids = model.generate(\n        **inputs,\n        do_sample=True if args.temperature > 1e-5 else False,\n        temperature=args.temperature,\n        repetition_penalty=args.repetition_penalty,\n        max_new_tokens=args.max_new_tokens,\n    )\n\n    if model.config.is_encoder_decoder:\n        output_ids = output_ids[0]\n    else:\n        output_ids = output_ids[0][len(inputs[\"input_ids\"][0]) :]\n    outputs = tokenizer.decode(\n        output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n    )\n\n    # Print results\n    print(f\"{conv.roles[0]}: {msg}\")\n    print(f\"{conv.roles[1]}: {outputs}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    add_model_args(parser)\n    parser.add_argument(\"--temperature\", type=float, default=0.7)\n    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=1024)\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--message\", type=str, default=\"Hello! Who are you?\")\n    args = parser.parse_args()\n\n    # Reset default repetition penalty for T5 models.\n    if \"t5\" in args.model_path and args.repetition_penalty == 1.0:\n        args.repetition_penalty = 1.2\n\n    main(args)\n", "fastchat/serve/shutdown_serve.py": "\"\"\"\nUsage\uff1a\npython shutdown_serve.py --down all\noptions: \"all\",\"controller\",\"model_worker\",\"openai_api_server\"\uff0c `all` means to stop all related servers \n\"\"\"\n\nimport argparse\nimport os\nimport subprocess\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \"--down\", choices=[\"all\", \"controller\", \"model_worker\", \"openai_api_server\"]\n)\nargs = parser.parse_args()\nbase_shell = \"ps -eo user,pid,cmd|grep fastchat.serve{}|grep -v grep|awk '{{print $2}}'|xargs kill -9\"\nif args.down == \"all\":\n    shell_script = base_shell.format(\"\")\nelse:\n    serve = f\".{args.down}\"\n    shell_script = base_shell.format(serve)\nprint(f\"execute shell cmd: {shell_script}\")\nsubprocess.run(shell_script, shell=True, check=True)\nprint(f\"{args.down} has been shutdown!\")\n", "fastchat/serve/mlx_worker.py": "\"\"\"\nA model worker using Apple MLX\n\nhttps://github.com/ml-explore/mlx-examples/tree/main/llms\n\nCode based on vllm_worker https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/vllm_worker.py\n\nYou must install MLX python:\n\npip install mlx-lm\n\"\"\"\n\nimport argparse\nimport asyncio\nimport atexit\nimport json\nfrom typing import List\nimport uuid\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.concurrency import run_in_threadpool\nfrom fastapi.responses import StreamingResponse, JSONResponse\nimport uvicorn\n\nfrom fastchat.serve.base_model_worker import BaseModelWorker\nfrom fastchat.serve.model_worker import (\n    logger,\n    worker_id,\n)\nfrom fastchat.utils import get_context_length, is_partial_stop\n\nimport mlx.core as mx\nfrom mlx_lm import load, generate\nfrom mlx_lm.utils import generate_step\n\napp = FastAPI()\n\n\nclass MLXWorker(BaseModelWorker):\n    def __init__(\n        self,\n        controller_addr: str,\n        worker_addr: str,\n        worker_id: str,\n        model_path: str,\n        model_names: List[str],\n        limit_worker_concurrency: int,\n        no_register: bool,\n        llm_engine: \"MLX\",\n        conv_template: str,\n    ):\n        super().__init__(\n            controller_addr,\n            worker_addr,\n            worker_id,\n            model_path,\n            model_names,\n            limit_worker_concurrency,\n            conv_template,\n        )\n\n        logger.info(\n            f\"Loading the model {self.model_names} on worker {worker_id}, worker type: MLX worker...\"\n        )\n\n        self.model_name = model_path\n        self.mlx_model, self.mlx_tokenizer = load(model_path)\n\n        self.tokenizer = self.mlx_tokenizer\n        # self.context_len = get_context_length(\n        #     llm_engine.engine.model_config.hf_config)\n        self.context_len = 2048  # hard code for now -- not sure how to get in MLX\n\n        if not no_register:\n            self.init_heart_beat()\n\n    async def generate_stream(self, params):\n        self.call_ct += 1\n\n        context = params.pop(\"prompt\")\n        request_id = params.pop(\"request_id\")\n        temperature = float(params.get(\"temperature\", 1.0))\n        top_p = float(params.get(\"top_p\", 1.0))\n        top_k = params.get(\"top_k\", -1.0)\n        presence_penalty = float(params.get(\"presence_penalty\", 0.0))\n        frequency_penalty = float(params.get(\"frequency_penalty\", 0.0))\n        max_new_tokens = params.get(\"max_new_tokens\", 256)\n        stop_str = params.get(\"stop\", None)\n        stop_token_ids = params.get(\"stop_token_ids\", None) or []\n        if self.tokenizer.eos_token_id is not None:\n            stop_token_ids.append(self.tokenizer.eos_token_id)\n        echo = params.get(\"echo\", True)\n        use_beam_search = params.get(\"use_beam_search\", False)\n        best_of = params.get(\"best_of\", None)\n\n        # Handle stop_str\n        stop = set()\n        if isinstance(stop_str, str) and stop_str != \"\":\n            stop.add(stop_str)\n        elif isinstance(stop_str, list) and stop_str != []:\n            stop.update(stop_str)\n\n        for tid in stop_token_ids:\n            if tid is not None:\n                s = self.tokenizer.decode(tid)\n                if s != \"\":\n                    stop.add(s)\n\n        print(\"Stop patterns: \", stop)\n\n        top_p = max(top_p, 1e-5)\n        if temperature <= 1e-5:\n            top_p = 1.0\n\n        tokens = []\n        skip = 0\n\n        context_mlx = mx.array(self.tokenizer.encode(context))\n\n        finish_reason = \"length\"\n\n        iterator = await run_in_threadpool(\n            generate_step, context_mlx, self.mlx_model, temperature\n        )\n\n        for i in range(max_new_tokens):\n            (token, _) = await run_in_threadpool(next, iterator)\n            if token == self.mlx_tokenizer.eos_token_id:\n                finish_reason = \"stop\"\n                break\n            tokens.append(token.item())\n            tokens_decoded = self.mlx_tokenizer.decode(tokens)\n            last_token_decoded = self.mlx_tokenizer.decode([token.item()])\n            skip = len(tokens_decoded)\n\n            partial_stop = any(is_partial_stop(tokens_decoded, i) for i in stop)\n\n            if partial_stop:\n                finish_reason = \"stop\"\n                break\n\n            ret = {\n                \"text\": tokens_decoded,\n                \"error_code\": 0,\n                \"usage\": {\n                    \"prompt_tokens\": len(context),\n                    \"completion_tokens\": len(tokens),\n                    \"total_tokens\": len(context) + len(tokens),\n                },\n                \"cumulative_logprob\": [],\n                \"finish_reason\": None,  # hard code for now\n            }\n            # print(ret)\n            yield (json.dumps(ret) + \"\\0\").encode()\n        ret = {\n            \"text\": self.mlx_tokenizer.decode(tokens),\n            \"error_code\": 0,\n            \"usage\": {},\n            \"cumulative_logprob\": [],\n            \"finish_reason\": finish_reason,\n        }\n        yield (json.dumps(obj={**ret, **{\"finish_reason\": None}}) + \"\\0\").encode()\n        yield (json.dumps(ret) + \"\\0\").encode()\n\n    async def generate(self, params):\n        async for x in self.generate_stream(params):\n            pass\n        return json.loads(x[:-1].decode())\n\n\ndef release_worker_semaphore():\n    worker.semaphore.release()\n\n\ndef acquire_worker_semaphore():\n    if worker.semaphore is None:\n        worker.semaphore = asyncio.Semaphore(worker.limit_worker_concurrency)\n    return worker.semaphore.acquire()\n\n\ndef create_background_tasks(request_id):\n    async def abort_request() -> None:\n        print(\"trying to abort but not implemented\")\n\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(release_worker_semaphore)\n    background_tasks.add_task(abort_request)\n    return background_tasks\n\n\n@app.post(\"/worker_generate_stream\")\nasync def api_generate_stream(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    request_id = uuid.uuid4()\n    params[\"request_id\"] = str(request_id)\n    generator = worker.generate_stream(params)\n    background_tasks = create_background_tasks(request_id)\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate\")\nasync def api_generate(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    request_id = uuid.uuid4()\n    params[\"request_id\"] = str(request_id)\n    output = await worker.generate(params)\n    release_worker_semaphore()\n    # await engine.abort(request_id)\n    print(\"Trying to abort but not implemented\")\n    return JSONResponse(output)\n\n\n@app.post(\"/worker_get_status\")\nasync def api_get_status(request: Request):\n    return worker.get_status()\n\n\n@app.post(\"/count_token\")\nasync def api_count_token(request: Request):\n    params = await request.json()\n    return worker.count_token(params)\n\n\n@app.post(\"/worker_get_conv_template\")\nasync def api_get_conv(request: Request):\n    return worker.get_conv_template()\n\n\n@app.post(\"/model_details\")\nasync def api_model_details(request: Request):\n    return {\"context_length\": worker.context_len}\n\n\nworker = None\n\n\ndef cleanup_at_exit():\n    global worker\n    print(\"Cleaning up...\")\n    del worker\n\n\natexit.register(cleanup_at_exit)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\"--model-path\", type=str, default=\"microsoft/phi-2\")\n    parser.add_argument(\n        \"--model-names\",\n        type=lambda s: s.split(\",\"),\n        help=\"Optional display comma separated names\",\n    )\n    parser.add_argument(\n        \"--conv-template\", type=str, default=None, help=\"Conversation prompt template.\"\n    )\n    parser.add_argument(\n        \"--trust_remote_code\",\n        action=\"store_false\",\n        default=True,\n        help=\"Trust remote code (e.g., from HuggingFace) when\"\n        \"downloading the model and tokenizer.\",\n    )\n\n    args, unknown = parser.parse_known_args()\n\n    if args.model_path:\n        args.model = args.model_path\n\n    worker = MLXWorker(\n        args.controller_address,\n        args.worker_address,\n        worker_id,\n        args.model_path,\n        args.model_names,\n        1024,\n        False,\n        \"MLX\",\n        args.conv_template,\n    )\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n", "fastchat/serve/api_provider.py": "\"\"\"Call API providers.\"\"\"\n\nimport json\nimport os\nimport random\nimport re\nfrom typing import Optional\nimport time\n\nimport requests\n\nfrom fastchat.utils import build_logger\n\n\nlogger = build_logger(\"gradio_web_server\", \"gradio_web_server.log\")\n\n\ndef get_api_provider_stream_iter(\n    conv,\n    model_name,\n    model_api_dict,\n    temperature,\n    top_p,\n    max_new_tokens,\n    state,\n):\n    if model_api_dict[\"api_type\"] == \"openai\":\n        if model_api_dict[\"vision-arena\"]:\n            prompt = conv.to_openai_vision_api_messages()\n        else:\n            prompt = conv.to_openai_api_messages()\n        stream_iter = openai_api_stream_iter(\n            model_api_dict[\"model_name\"],\n            prompt,\n            temperature,\n            top_p,\n            max_new_tokens,\n            api_base=model_api_dict[\"api_base\"],\n            api_key=model_api_dict[\"api_key\"],\n        )\n    elif model_api_dict[\"api_type\"] == \"openai_assistant\":\n        last_prompt = conv.messages[-2][1]\n        stream_iter = openai_assistant_api_stream_iter(\n            state,\n            last_prompt,\n            assistant_id=model_api_dict[\"assistant_id\"],\n            api_key=model_api_dict[\"api_key\"],\n        )\n    elif model_api_dict[\"api_type\"] == \"anthropic\":\n        if model_api_dict[\"vision-arena\"]:\n            prompt = conv.to_anthropic_vision_api_messages()\n        else:\n            prompt = conv.to_openai_api_messages()\n        stream_iter = anthropic_api_stream_iter(\n            model_name, prompt, temperature, top_p, max_new_tokens\n        )\n    elif model_api_dict[\"api_type\"] == \"anthropic_message\":\n        if model_api_dict[\"vision-arena\"]:\n            prompt = conv.to_anthropic_vision_api_messages()\n        else:\n            prompt = conv.to_openai_api_messages()\n        stream_iter = anthropic_message_api_stream_iter(\n            model_name, prompt, temperature, top_p, max_new_tokens\n        )\n    elif model_api_dict[\"api_type\"] == \"anthropic_message_vertex\":\n        if model_api_dict[\"vision-arena\"]:\n            prompt = conv.to_anthropic_vision_api_messages()\n        else:\n            prompt = conv.to_openai_api_messages()\n        stream_iter = anthropic_message_api_stream_iter(\n            model_api_dict[\"model_name\"],\n            prompt,\n            temperature,\n            top_p,\n            max_new_tokens,\n            vertex_ai=True,\n        )\n    elif model_api_dict[\"api_type\"] == \"gemini\":\n        prompt = conv.to_gemini_api_messages()\n        stream_iter = gemini_api_stream_iter(\n            model_api_dict[\"model_name\"],\n            prompt,\n            temperature,\n            top_p,\n            max_new_tokens,\n            api_key=model_api_dict[\"api_key\"],\n        )\n    elif model_api_dict[\"api_type\"] == \"bard\":\n        prompt = conv.to_openai_api_messages()\n        stream_iter = bard_api_stream_iter(\n            model_api_dict[\"model_name\"],\n            prompt,\n            temperature,\n            top_p,\n            api_key=model_api_dict[\"api_key\"],\n        )\n    elif model_api_dict[\"api_type\"] == \"mistral\":\n        prompt = conv.to_openai_api_messages()\n        stream_iter = mistral_api_stream_iter(\n            model_name, prompt, temperature, top_p, max_new_tokens\n        )\n    elif model_api_dict[\"api_type\"] == \"nvidia\":\n        prompt = conv.to_openai_api_messages()\n        stream_iter = nvidia_api_stream_iter(\n            model_name,\n            prompt,\n            temperature,\n            top_p,\n            max_new_tokens,\n            model_api_dict[\"api_base\"],\n        )\n    elif model_api_dict[\"api_type\"] == \"ai2\":\n        prompt = conv.to_openai_api_messages()\n        stream_iter = ai2_api_stream_iter(\n            model_name,\n            model_api_dict[\"model_name\"],\n            prompt,\n            temperature,\n            top_p,\n            max_new_tokens,\n            api_base=model_api_dict[\"api_base\"],\n            api_key=model_api_dict[\"api_key\"],\n        )\n    elif model_api_dict[\"api_type\"] == \"vertex\":\n        prompt = conv.to_vertex_api_messages()\n        stream_iter = vertex_api_stream_iter(\n            model_name, prompt, temperature, top_p, max_new_tokens\n        )\n    elif model_api_dict[\"api_type\"] == \"yandexgpt\":\n        # note: top_p parameter is unused by yandexgpt\n\n        messages = []\n        if conv.system_message:\n            messages.append({\"role\": \"system\", \"text\": conv.system_message})\n        messages += [\n            {\"role\": role, \"text\": text}\n            for role, text in conv.messages\n            if text is not None\n        ]\n\n        fixed_temperature = model_api_dict.get(\"fixed_temperature\")\n        if fixed_temperature is not None:\n            temperature = fixed_temperature\n\n        stream_iter = yandexgpt_api_stream_iter(\n            model_name=model_api_dict[\"model_name\"],\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_new_tokens,\n            api_base=model_api_dict[\"api_base\"],\n            api_key=model_api_dict.get(\"api_key\"),\n            folder_id=model_api_dict.get(\"folder_id\"),\n        )\n    elif model_api_dict[\"api_type\"] == \"cohere\":\n        messages = conv.to_openai_api_messages()\n        stream_iter = cohere_api_stream_iter(\n            client_name=model_api_dict.get(\"client_name\", \"FastChat\"),\n            model_id=model_api_dict[\"model_name\"],\n            messages=messages,\n            temperature=temperature,\n            top_p=top_p,\n            max_new_tokens=max_new_tokens,\n            api_base=model_api_dict[\"api_base\"],\n            api_key=model_api_dict[\"api_key\"],\n        )\n    elif model_api_dict[\"api_type\"] == \"reka\":\n        messages = conv.to_reka_api_messages()\n        stream_iter = reka_api_stream_iter(\n            model_name=model_api_dict[\"model_name\"],\n            messages=messages,\n            temperature=temperature,\n            top_p=top_p,\n            max_new_tokens=max_new_tokens,\n            api_base=model_api_dict[\"api_base\"],\n            api_key=model_api_dict[\"api_key\"],\n        )\n    else:\n        raise NotImplementedError()\n\n    return stream_iter\n\n\ndef openai_api_stream_iter(\n    model_name,\n    messages,\n    temperature,\n    top_p,\n    max_new_tokens,\n    api_base=None,\n    api_key=None,\n):\n    import openai\n\n    api_key = api_key or os.environ[\"OPENAI_API_KEY\"]\n\n    if \"azure\" in model_name:\n        client = openai.AzureOpenAI(\n            api_version=\"2023-07-01-preview\",\n            azure_endpoint=api_base or \"https://api.openai.com/v1\",\n            api_key=api_key,\n        )\n    else:\n        client = openai.OpenAI(\n            base_url=api_base or \"https://api.openai.com/v1\",\n            api_key=api_key,\n            timeout=180,\n        )\n\n    # Make requests for logging\n    text_messages = []\n    for message in messages:\n        if type(message[\"content\"]) == str:  # text-only model\n            text_messages.append(message)\n        else:  # vision model\n            filtered_content_list = [\n                content for content in message[\"content\"] if content[\"type\"] == \"text\"\n            ]\n            text_messages.append(\n                {\"role\": message[\"role\"], \"content\": filtered_content_list}\n            )\n\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": text_messages,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    res = client.chat.completions.create(\n        model=model_name,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_new_tokens,\n        stream=True,\n    )\n    text = \"\"\n    for chunk in res:\n        if len(chunk.choices) > 0:\n            text += chunk.choices[0].delta.content or \"\"\n            data = {\n                \"text\": text,\n                \"error_code\": 0,\n            }\n            yield data\n\n\ndef upload_openai_file_to_gcs(file_id):\n    import openai\n    from google.cloud import storage\n\n    storage_client = storage.Client()\n\n    file = openai.files.content(file_id)\n    # upload file to GCS\n    bucket = storage_client.get_bucket(\"arena_user_content\")\n    blob = bucket.blob(f\"{file_id}\")\n    blob.upload_from_string(file.read())\n    blob.make_public()\n    return blob.public_url\n\n\ndef openai_assistant_api_stream_iter(\n    state,\n    prompt,\n    assistant_id,\n    api_key=None,\n):\n    import openai\n    import base64\n\n    api_key = api_key or os.environ[\"OPENAI_API_KEY\"]\n    client = openai.OpenAI(base_url=\"https://api.openai.com/v1\", api_key=api_key)\n\n    if state.oai_thread_id is None:\n        logger.info(\"==== create thread ====\")\n        thread = client.beta.threads.create()\n        state.oai_thread_id = thread.id\n    logger.info(f\"==== thread_id ====\\n{state.oai_thread_id}\")\n    thread_message = client.beta.threads.messages.with_raw_response.create(\n        state.oai_thread_id,\n        role=\"user\",\n        content=prompt,\n        timeout=3,\n    )\n    # logger.info(f\"header {thread_message.headers}\")\n    thread_message = thread_message.parse()\n    # Make requests\n    gen_params = {\n        \"assistant_id\": assistant_id,\n        \"thread_id\": state.oai_thread_id,\n        \"message\": prompt,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    res = requests.post(\n        f\"https://api.openai.com/v1/threads/{state.oai_thread_id}/runs\",\n        headers={\n            \"Authorization\": f\"Bearer {api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"OpenAI-Beta\": \"assistants=v1\",\n        },\n        json={\"assistant_id\": assistant_id, \"stream\": True},\n        timeout=30,\n        stream=True,\n    )\n\n    list_of_text = []\n    list_of_raw_text = []\n    offset_idx = 0\n    full_ret_text = \"\"\n    idx_mapping = {}\n    for line in res.iter_lines():\n        if not line:\n            continue\n        data = line.decode(\"utf-8\")\n        # logger.info(\"data:\", data)\n        if data.endswith(\"[DONE]\"):\n            break\n        if data.startswith(\"event\"):\n            event = data.split(\":\")[1].strip()\n            if event == \"thread.message.completed\":\n                offset_idx += len(list_of_text)\n            continue\n        data = json.loads(data[6:])\n\n        if data.get(\"status\") == \"failed\":\n            yield {\n                \"text\": f\"**API REQUEST ERROR** Reason: {data['last_error']['message']}\",\n                \"error_code\": 1,\n            }\n            return\n\n        if data.get(\"status\") == \"completed\":\n            logger.info(f\"[debug]: {data}\")\n\n        if data[\"object\"] != \"thread.message.delta\":\n            continue\n\n        for delta in data[\"delta\"][\"content\"]:\n            text_index = delta[\"index\"] + offset_idx\n            if len(list_of_text) <= text_index:\n                list_of_text.append(\"\")\n                list_of_raw_text.append(\"\")\n\n            text = list_of_text[text_index]\n            raw_text = list_of_raw_text[text_index]\n\n            if delta[\"type\"] == \"text\":\n                # text, url_citation or file_path\n                content = delta[\"text\"]\n                if \"annotations\" in content and len(content[\"annotations\"]) > 0:\n                    annotations = content[\"annotations\"]\n\n                    cur_offset = 0\n                    raw_text_copy = raw_text\n                    for anno in annotations:\n                        if anno[\"type\"] == \"url_citation\":\n                            anno_text = anno[\"text\"]\n                            if anno_text not in idx_mapping:\n                                continue\n                            citation_number = idx_mapping[anno_text]\n\n                            start_idx = anno[\"start_index\"] + cur_offset\n                            end_idx = anno[\"end_index\"] + cur_offset\n                            url = anno[\"url_citation\"][\"url\"]\n\n                            citation = f\" [[{citation_number}]]({url})\"\n                            raw_text_copy = (\n                                raw_text_copy[:start_idx]\n                                + citation\n                                + raw_text_copy[end_idx:]\n                            )\n                            cur_offset += len(citation) - (end_idx - start_idx)\n                        elif anno[\"type\"] == \"file_path\":\n                            file_public_url = upload_openai_file_to_gcs(\n                                anno[\"file_path\"][\"file_id\"]\n                            )\n                            raw_text_copy = raw_text_copy.replace(\n                                anno[\"text\"], f\"{file_public_url}\"\n                            )\n                    text = raw_text_copy\n                else:\n                    text_content = content[\"value\"]\n                    raw_text += text_content\n\n                    # re-index citation number\n                    pattern = r\"\u3010\\d+\u3011\"\n                    matches = re.findall(pattern, content[\"value\"])\n                    if len(matches) > 0:\n                        for match in matches:\n                            if match not in idx_mapping:\n                                idx_mapping[match] = len(idx_mapping) + 1\n                            citation_number = idx_mapping[match]\n                            text_content = text_content.replace(\n                                match, f\" [{citation_number}]\"\n                            )\n                    text += text_content\n                    # yield {\"text\": text, \"error_code\": 0}\n            elif delta[\"type\"] == \"image_file\":\n                image_public_url = upload_openai_file_to_gcs(\n                    delta[\"image_file\"][\"file_id\"]\n                )\n                # raw_text += f\"![image]({image_public_url})\"\n                text += f\"![image]({image_public_url})\"\n\n            list_of_text[text_index] = text\n            list_of_raw_text[text_index] = raw_text\n\n            full_ret_text = \"\\n\".join(list_of_text)\n            yield {\"text\": full_ret_text, \"error_code\": 0}\n\n\ndef anthropic_api_stream_iter(model_name, prompt, temperature, top_p, max_new_tokens):\n    import anthropic\n\n    c = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\n    # Make requests\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    res = c.completions.create(\n        prompt=prompt,\n        stop_sequences=[anthropic.HUMAN_PROMPT],\n        max_tokens_to_sample=max_new_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        model=model_name,\n        stream=True,\n    )\n    text = \"\"\n    for chunk in res:\n        text += chunk.completion\n        data = {\n            \"text\": text,\n            \"error_code\": 0,\n        }\n        yield data\n\n\ndef anthropic_message_api_stream_iter(\n    model_name,\n    messages,\n    temperature,\n    top_p,\n    max_new_tokens,\n    vertex_ai=False,\n):\n    import anthropic\n\n    if vertex_ai:\n        client = anthropic.AnthropicVertex(\n            region=os.environ[\"GCP_LOCATION\"],\n            project_id=os.environ[\"GCP_PROJECT_ID\"],\n            max_retries=5,\n        )\n    else:\n        client = anthropic.Anthropic(\n            api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n            max_retries=5,\n        )\n\n    text_messages = []\n    for message in messages:\n        if type(message[\"content\"]) == str:  # text-only model\n            text_messages.append(message)\n        else:  # vision model\n            filtered_content_list = [\n                content for content in message[\"content\"] if content[\"type\"] == \"text\"\n            ]\n            text_messages.append(\n                {\"role\": message[\"role\"], \"content\": filtered_content_list}\n            )\n\n    # Make requests for logging\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": text_messages,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    system_prompt = \"\"\n    if messages[0][\"role\"] == \"system\":\n        if type(messages[0][\"content\"]) == dict:\n            system_prompt = messages[0][\"content\"][\"text\"]\n        elif type(messages[0][\"content\"]) == str:\n            system_prompt = messages[0][\"content\"]\n        # remove system prompt\n        messages = messages[1:]\n\n    text = \"\"\n    with client.messages.stream(\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_new_tokens,\n        messages=messages,\n        model=model_name,\n        system=system_prompt,\n    ) as stream:\n        for chunk in stream.text_stream:\n            text += chunk\n            data = {\n                \"text\": text,\n                \"error_code\": 0,\n            }\n            yield data\n\n\ndef gemini_api_stream_iter(\n    model_name, messages, temperature, top_p, max_new_tokens, api_key=None\n):\n    import google.generativeai as genai  # pip install google-generativeai\n\n    if api_key is None:\n        api_key = os.environ[\"GEMINI_API_KEY\"]\n    genai.configure(api_key=api_key)\n\n    generation_config = {\n        \"temperature\": temperature,\n        \"max_output_tokens\": max_new_tokens,\n        \"top_p\": top_p,\n    }\n    params = {\n        \"model\": model_name,\n        \"prompt\": messages,\n    }\n    params.update(generation_config)\n    logger.info(f\"==== request ====\\n{params}\")\n\n    safety_settings = [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n        {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n        {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n        {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n    ]\n\n    history = []\n    system_prompt = None\n    for message in messages[:-1]:\n        if message[\"role\"] == \"system\":\n            system_prompt = message[\"content\"]\n            continue\n        history.append({\"role\": message[\"role\"], \"parts\": message[\"content\"]})\n\n    model = genai.GenerativeModel(\n        model_name=model_name,\n        system_instruction=system_prompt,\n        generation_config=generation_config,\n        safety_settings=safety_settings,\n    )\n    convo = model.start_chat(history=history)\n    response = convo.send_message(messages[-1][\"content\"], stream=True)\n\n    try:\n        text = \"\"\n        for chunk in response:\n            text += chunk.candidates[0].content.parts[0].text\n            data = {\n                \"text\": text,\n                \"error_code\": 0,\n            }\n            yield data\n    except Exception as e:\n        logger.error(f\"==== error ====\\n{e}\")\n        reason = chunk.candidates\n        yield {\n            \"text\": f\"**API REQUEST ERROR** Reason: {reason}.\",\n            \"error_code\": 1,\n        }\n\n\ndef bard_api_stream_iter(model_name, conv, temperature, top_p, api_key=None):\n    del top_p  # not supported\n    del temperature  # not supported\n\n    if api_key is None:\n        api_key = os.environ[\"BARD_API_KEY\"]\n\n    # convert conv to conv_bard\n    conv_bard = []\n    for turn in conv:\n        if turn[\"role\"] == \"user\":\n            conv_bard.append({\"author\": \"0\", \"content\": turn[\"content\"]})\n        elif turn[\"role\"] == \"assistant\":\n            conv_bard.append({\"author\": \"1\", \"content\": turn[\"content\"]})\n        else:\n            raise ValueError(f\"Unsupported role: {turn['role']}\")\n\n    params = {\n        \"model\": model_name,\n        \"prompt\": conv_bard,\n    }\n    logger.info(f\"==== request ====\\n{params}\")\n\n    try:\n        res = requests.post(\n            f\"https://generativelanguage.googleapis.com/v1beta2/models/{model_name}:generateMessage?key={api_key}\",\n            json={\n                \"prompt\": {\n                    \"messages\": conv_bard,\n                },\n            },\n            timeout=30,\n        )\n    except Exception as e:\n        logger.error(f\"==== error ====\\n{e}\")\n        yield {\n            \"text\": f\"**API REQUEST ERROR** Reason: {e}.\",\n            \"error_code\": 1,\n        }\n\n    if res.status_code != 200:\n        logger.error(f\"==== error ==== ({res.status_code}): {res.text}\")\n        yield {\n            \"text\": f\"**API REQUEST ERROR** Reason: status code {res.status_code}.\",\n            \"error_code\": 1,\n        }\n\n    response_json = res.json()\n    if \"candidates\" not in response_json:\n        logger.error(f\"==== error ==== response blocked: {response_json}\")\n        reason = response_json[\"filters\"][0][\"reason\"]\n        yield {\n            \"text\": f\"**API REQUEST ERROR** Reason: {reason}.\",\n            \"error_code\": 1,\n        }\n\n    response = response_json[\"candidates\"][0][\"content\"]\n    pos = 0\n    while pos < len(response):\n        # simulate token streaming\n        pos += random.randint(3, 6)\n        time.sleep(0.002)\n        data = {\n            \"text\": response[:pos],\n            \"error_code\": 0,\n        }\n        yield data\n\n\ndef ai2_api_stream_iter(\n    model_name,\n    model_id,\n    messages,\n    temperature,\n    top_p,\n    max_new_tokens,\n    api_key=None,\n    api_base=None,\n):\n    # get keys and needed values\n    ai2_key = api_key or os.environ.get(\"AI2_API_KEY\")\n    api_base = api_base or \"https://inferd.allen.ai/api/v1/infer\"\n\n    # Make requests\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": messages,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    # AI2 uses vLLM, which requires that `top_p` be 1.0 for greedy sampling:\n    # https://github.com/vllm-project/vllm/blob/v0.1.7/vllm/sampling_params.py#L156-L157\n    if temperature == 0.0 and top_p < 1.0:\n        raise ValueError(\"top_p must be 1 when temperature is 0.0\")\n\n    res = requests.post(\n        api_base,\n        stream=True,\n        headers={\"Authorization\": f\"Bearer {ai2_key}\"},\n        json={\n            \"model_id\": model_id,\n            # This input format is specific to the Tulu2 model. Other models\n            # may require different input formats. See the model's schema\n            # documentation on InferD for more information.\n            \"input\": {\n                \"messages\": messages,\n                \"opts\": {\n                    \"max_tokens\": max_new_tokens,\n                    \"temperature\": temperature,\n                    \"top_p\": top_p,\n                    \"logprobs\": 1,  # increase for more choices\n                },\n            },\n        },\n        timeout=5,\n    )\n\n    if res.status_code != 200:\n        logger.error(f\"unexpected response ({res.status_code}): {res.text}\")\n        raise ValueError(\"unexpected response from InferD\", res)\n\n    text = \"\"\n    for line in res.iter_lines():\n        if line:\n            part = json.loads(line)\n            if \"result\" in part and \"output\" in part[\"result\"]:\n                for t in part[\"result\"][\"output\"][\"text\"]:\n                    text += t\n            else:\n                logger.error(f\"unexpected part: {part}\")\n                raise ValueError(\"empty result in InferD response\")\n\n            data = {\n                \"text\": text,\n                \"error_code\": 0,\n            }\n            yield data\n\n\ndef mistral_api_stream_iter(model_name, messages, temperature, top_p, max_new_tokens):\n    from mistralai.client import MistralClient\n    from mistralai.models.chat_completion import ChatMessage\n\n    api_key = os.environ[\"MISTRAL_API_KEY\"]\n\n    client = MistralClient(api_key=api_key, timeout=5)\n\n    # Make requests\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": messages,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    new_messages = [\n        ChatMessage(role=message[\"role\"], content=message[\"content\"])\n        for message in messages\n    ]\n\n    res = client.chat_stream(\n        model=model_name,\n        temperature=temperature,\n        messages=new_messages,\n        max_tokens=max_new_tokens,\n        top_p=top_p,\n    )\n\n    text = \"\"\n    for chunk in res:\n        if chunk.choices[0].delta.content is not None:\n            text += chunk.choices[0].delta.content\n            data = {\n                \"text\": text,\n                \"error_code\": 0,\n            }\n            yield data\n\n\ndef nvidia_api_stream_iter(model_name, messages, temp, top_p, max_tokens, api_base):\n    api_key = os.environ[\"NVIDIA_API_KEY\"]\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"accept\": \"text/event-stream\",\n        \"content-type\": \"application/json\",\n    }\n    # nvidia api does not accept 0 temperature\n    if temp == 0.0:\n        temp = 0.000001\n\n    payload = {\n        \"messages\": messages,\n        \"temperature\": temp,\n        \"top_p\": top_p,\n        \"max_tokens\": max_tokens,\n        \"seed\": 42,\n        \"stream\": True,\n    }\n    logger.info(f\"==== request ====\\n{payload}\")\n\n    response = requests.post(\n        api_base, headers=headers, json=payload, stream=True, timeout=1\n    )\n    text = \"\"\n    for line in response.iter_lines():\n        if line:\n            data = line.decode(\"utf-8\")\n            if data.endswith(\"[DONE]\"):\n                break\n            data = json.loads(data[6:])[\"choices\"][0][\"delta\"][\"content\"]\n            text += data\n            yield {\"text\": text, \"error_code\": 0}\n\n\ndef yandexgpt_api_stream_iter(\n    model_name, messages, temperature, max_tokens, api_base, api_key, folder_id\n):\n    api_key = api_key or os.environ[\"YANDEXGPT_API_KEY\"]\n    headers = {\n        \"Authorization\": f\"Api-Key {api_key}\",\n        \"content-type\": \"application/json\",\n    }\n\n    payload = {\n        \"modelUri\": f\"gpt://{folder_id}/{model_name}\",\n        \"completionOptions\": {\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"stream\": True,\n        },\n        \"messages\": messages,\n    }\n    logger.info(f\"==== request ====\\n{payload}\")\n\n    # https://llm.api.cloud.yandex.net/foundationModels/v1/completion\n    response = requests.post(\n        api_base, headers=headers, json=payload, stream=True, timeout=60\n    )\n    text = \"\"\n    for line in response.iter_lines():\n        if line:\n            data = json.loads(line.decode(\"utf-8\"))\n            data = data[\"result\"]\n            top_alternative = data[\"alternatives\"][0]\n            text = top_alternative[\"message\"][\"text\"]\n            yield {\"text\": text, \"error_code\": 0}\n\n            status = top_alternative[\"status\"]\n            if status in (\n                \"ALTERNATIVE_STATUS_FINAL\",\n                \"ALTERNATIVE_STATUS_TRUNCATED_FINAL\",\n            ):\n                break\n\n\ndef cohere_api_stream_iter(\n    client_name: str,\n    model_id: str,\n    messages: list,\n    temperature: Optional[\n        float\n    ] = None,  # The SDK or API handles None for all parameters following\n    top_p: Optional[float] = None,\n    max_new_tokens: Optional[int] = None,\n    api_key: Optional[str] = None,  # default is env var CO_API_KEY\n    api_base: Optional[str] = None,\n):\n    import cohere\n\n    OPENAI_TO_COHERE_ROLE_MAP = {\n        \"user\": \"User\",\n        \"assistant\": \"Chatbot\",\n        \"system\": \"System\",\n    }\n\n    client = cohere.Client(\n        api_key=api_key,\n        base_url=api_base,\n        client_name=client_name,\n    )\n\n    # prepare and log requests\n    chat_history = [\n        dict(\n            role=OPENAI_TO_COHERE_ROLE_MAP[message[\"role\"]], message=message[\"content\"]\n        )\n        for message in messages[:-1]\n    ]\n    actual_prompt = messages[-1][\"content\"]\n\n    gen_params = {\n        \"model\": model_id,\n        \"messages\": messages,\n        \"chat_history\": chat_history,\n        \"prompt\": actual_prompt,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    # make request and stream response\n    res = client.chat_stream(\n        message=actual_prompt,\n        chat_history=chat_history,\n        model=model_id,\n        temperature=temperature,\n        max_tokens=max_new_tokens,\n        p=top_p,\n    )\n    try:\n        text = \"\"\n        for streaming_item in res:\n            if streaming_item.event_type == \"text-generation\":\n                text += streaming_item.text\n                yield {\"text\": text, \"error_code\": 0}\n    except cohere.core.ApiError as e:\n        logger.error(f\"==== error from cohere api: {e} ====\")\n        yield {\n            \"text\": f\"**API REQUEST ERROR** Reason: {e}\",\n            \"error_code\": 1,\n        }\n\n\ndef vertex_api_stream_iter(model_name, messages, temperature, top_p, max_new_tokens):\n    import vertexai\n    from vertexai import generative_models\n    from vertexai.generative_models import (\n        GenerationConfig,\n        GenerativeModel,\n        Image,\n    )\n\n    project_id = os.environ.get(\"GCP_PROJECT_ID\", None)\n    location = os.environ.get(\"GCP_LOCATION\", None)\n    vertexai.init(project=project_id, location=location)\n\n    text_messages = []\n    for message in messages:\n        if type(message) == str:\n            text_messages.append(message)\n\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": text_messages,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    safety_settings = [\n        generative_models.SafetySetting(\n            category=generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT,\n            threshold=generative_models.HarmBlockThreshold.BLOCK_NONE,\n        ),\n        generative_models.SafetySetting(\n            category=generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            threshold=generative_models.HarmBlockThreshold.BLOCK_NONE,\n        ),\n        generative_models.SafetySetting(\n            category=generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n            threshold=generative_models.HarmBlockThreshold.BLOCK_NONE,\n        ),\n        generative_models.SafetySetting(\n            category=generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n            threshold=generative_models.HarmBlockThreshold.BLOCK_NONE,\n        ),\n    ]\n    generator = GenerativeModel(model_name).generate_content(\n        messages,\n        stream=True,\n        generation_config=GenerationConfig(\n            top_p=top_p, max_output_tokens=max_new_tokens, temperature=temperature\n        ),\n        safety_settings=safety_settings,\n    )\n\n    ret = \"\"\n    for chunk in generator:\n        # NOTE(chris): This may be a vertex api error, below is HOTFIX: https://github.com/googleapis/python-aiplatform/issues/3129\n        ret += chunk.candidates[0].content.parts[0]._raw_part.text\n        # ret += chunk.text\n        data = {\n            \"text\": ret,\n            \"error_code\": 0,\n        }\n        yield data\n\n\ndef reka_api_stream_iter(\n    model_name: str,\n    messages: list,\n    temperature: Optional[\n        float\n    ] = None,  # The SDK or API handles None for all parameters following\n    top_p: Optional[float] = None,\n    max_new_tokens: Optional[int] = None,\n    api_key: Optional[str] = None,  # default is env var CO_API_KEY\n    api_base: Optional[str] = None,\n):\n    api_key = api_key or os.environ[\"REKA_API_KEY\"]\n\n    use_search_engine = False\n    if \"-online\" in model_name:\n        model_name = model_name.replace(\"-online\", \"\")\n        use_search_engine = True\n    request = {\n        \"model_name\": model_name,\n        \"conversation_history\": messages,\n        \"temperature\": temperature,\n        \"request_output_len\": max_new_tokens,\n        \"runtime_top_p\": top_p,\n        \"stream\": True,\n        \"use_search_engine\": use_search_engine,\n    }\n\n    # Make requests for logging\n    text_messages = []\n    for message in messages:\n        text_messages.append({\"type\": message[\"type\"], \"text\": message[\"text\"]})\n    logged_request = dict(request)\n    logged_request[\"conversation_history\"] = text_messages\n\n    logger.info(f\"==== request ====\\n{logged_request}\")\n\n    response = requests.post(\n        api_base,\n        stream=True,\n        json=request,\n        headers={\n            \"X-Api-Key\": api_key,\n        },\n    )\n\n    if response.status_code != 200:\n        error_message = response.text\n        logger.error(f\"==== error from reka api: {error_message} ====\")\n        yield {\n            \"text\": f\"**API REQUEST ERROR** Reason: {error_message}\",\n            \"error_code\": 1,\n        }\n        return\n\n    for line in response.iter_lines():\n        line = line.decode(\"utf8\")\n        if not line.startswith(\"data: \"):\n            continue\n        gen = json.loads(line[6:])\n        yield {\"text\": gen[\"text\"], \"error_code\": 0}\n", "fastchat/serve/cli.py": "\"\"\"\nChat with a model with command line interface.\n\nUsage:\npython3 -m fastchat.serve.cli --model lmsys/vicuna-7b-v1.5\npython3 -m fastchat.serve.cli --model lmsys/fastchat-t5-3b-v1.0\n\nOther commands:\n- Type \"!!exit\" or an empty line to exit.\n- Type \"!!reset\" to start a new conversation.\n- Type \"!!remove\" to remove the last prompt.\n- Type \"!!regen\" to regenerate the last message.\n- Type \"!!save <filename>\" to save the conversation history to a json file.\n- Type \"!!load <filename>\" to load a conversation history from a json file.\n\"\"\"\nimport argparse\nimport os\nimport re\nimport sys\n\nfrom prompt_toolkit import PromptSession\nfrom prompt_toolkit.auto_suggest import AutoSuggestFromHistory\nfrom prompt_toolkit.completion import WordCompleter\nfrom prompt_toolkit.history import InMemoryHistory\nfrom prompt_toolkit.key_binding import KeyBindings\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.markdown import Markdown\nimport torch\n\nfrom fastchat.model.model_adapter import add_model_args\nfrom fastchat.modules.awq import AWQConfig\nfrom fastchat.modules.exllama import ExllamaConfig\nfrom fastchat.modules.xfastertransformer import XftConfig\nfrom fastchat.modules.gptq import GptqConfig\nfrom fastchat.serve.inference import ChatIO, chat_loop\nfrom fastchat.utils import str_to_torch_dtype\n\n\nclass SimpleChatIO(ChatIO):\n    def __init__(self, multiline: bool = False):\n        self._multiline = multiline\n\n    def prompt_for_input(self, role) -> str:\n        if not self._multiline:\n            return input(f\"{role}: \")\n\n        prompt_data = []\n        line = input(f\"{role} [ctrl-d/z on empty line to end]: \")\n        while True:\n            prompt_data.append(line.strip())\n            try:\n                line = input()\n            except EOFError as e:\n                break\n        return \"\\n\".join(prompt_data)\n\n    def prompt_for_output(self, role: str):\n        print(f\"{role}: \", end=\"\", flush=True)\n\n    def stream_output(self, output_stream):\n        pre = 0\n        for outputs in output_stream:\n            output_text = outputs[\"text\"]\n            output_text = output_text.strip().split(\" \")\n            now = len(output_text) - 1\n            if now > pre:\n                print(\" \".join(output_text[pre:now]), end=\" \", flush=True)\n                pre = now\n        print(\" \".join(output_text[pre:]), flush=True)\n        return \" \".join(output_text)\n\n    def print_output(self, text: str):\n        print(text)\n\n\nclass RichChatIO(ChatIO):\n    bindings = KeyBindings()\n\n    @bindings.add(\"escape\", \"enter\")\n    def _(event):\n        event.app.current_buffer.newline()\n\n    def __init__(self, multiline: bool = False, mouse: bool = False):\n        self._prompt_session = PromptSession(history=InMemoryHistory())\n        self._completer = WordCompleter(\n            words=[\"!!exit\", \"!!reset\", \"!!remove\", \"!!regen\", \"!!save\", \"!!load\"],\n            pattern=re.compile(\"$\"),\n        )\n        self._console = Console()\n        self._multiline = multiline\n        self._mouse = mouse\n\n    def prompt_for_input(self, role) -> str:\n        self._console.print(f\"[bold]{role}:\")\n        # TODO(suquark): multiline input has some issues. fix it later.\n        prompt_input = self._prompt_session.prompt(\n            completer=self._completer,\n            multiline=False,\n            mouse_support=self._mouse,\n            auto_suggest=AutoSuggestFromHistory(),\n            key_bindings=self.bindings if self._multiline else None,\n        )\n        self._console.print()\n        return prompt_input\n\n    def prompt_for_output(self, role: str):\n        self._console.print(f\"[bold]{role.replace('/', '|')}:\")\n\n    def stream_output(self, output_stream):\n        \"\"\"Stream output from a role.\"\"\"\n        # TODO(suquark): the console flickers when there is a code block\n        #  above it. We need to cut off \"live\" when a code block is done.\n\n        # Create a Live context for updating the console output\n        with Live(console=self._console, refresh_per_second=4) as live:\n            # Read lines from the stream\n            for outputs in output_stream:\n                if not outputs:\n                    continue\n                text = outputs[\"text\"]\n                # Render the accumulated text as Markdown\n                # NOTE: this is a workaround for the rendering \"unstandard markdown\"\n                #  in rich. The chatbots output treat \"\\n\" as a new line for\n                #  better compatibility with real-world text. However, rendering\n                #  in markdown would break the format. It is because standard markdown\n                #  treat a single \"\\n\" in normal text as a space.\n                #  Our workaround is adding two spaces at the end of each line.\n                #  This is not a perfect solution, as it would\n                #  introduce trailing spaces (only) in code block, but it works well\n                #  especially for console output, because in general the console does not\n                #  care about trailing spaces.\n                lines = []\n                for line in text.splitlines():\n                    lines.append(line)\n                    if line.startswith(\"```\"):\n                        # Code block marker - do not add trailing spaces, as it would\n                        #  break the syntax highlighting\n                        lines.append(\"\\n\")\n                    else:\n                        lines.append(\"  \\n\")\n                markdown = Markdown(\"\".join(lines))\n                # Update the Live console output\n                live.update(markdown)\n        self._console.print()\n        return text\n\n    def print_output(self, text: str):\n        self.stream_output([{\"text\": text}])\n\n\nclass ProgrammaticChatIO(ChatIO):\n    def prompt_for_input(self, role) -> str:\n        contents = \"\"\n        # `end_sequence` signals the end of a message. It is unlikely to occur in\n        #  message content.\n        end_sequence = \" __END_OF_A_MESSAGE_47582648__\\n\"\n        len_end = len(end_sequence)\n        while True:\n            if len(contents) >= len_end:\n                last_chars = contents[-len_end:]\n                if last_chars == end_sequence:\n                    break\n            try:\n                char = sys.stdin.read(1)\n                contents = contents + char\n            except EOFError:\n                continue\n        contents = contents[:-len_end]\n        print(f\"[!OP:{role}]: {contents}\", flush=True)\n        return contents\n\n    def prompt_for_output(self, role: str):\n        print(f\"[!OP:{role}]: \", end=\"\", flush=True)\n\n    def stream_output(self, output_stream):\n        pre = 0\n        for outputs in output_stream:\n            output_text = outputs[\"text\"]\n            output_text = output_text.strip().split(\" \")\n            now = len(output_text) - 1\n            if now > pre:\n                print(\" \".join(output_text[pre:now]), end=\" \", flush=True)\n                pre = now\n        print(\" \".join(output_text[pre:]), flush=True)\n        return \" \".join(output_text)\n\n    def print_output(self, text: str):\n        print(text)\n\n\ndef main(args):\n    if args.gpus:\n        if len(args.gpus.split(\",\")) < args.num_gpus:\n            raise ValueError(\n                f\"Larger --num-gpus ({args.num_gpus}) than --gpus {args.gpus}!\"\n            )\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n        os.environ[\"XPU_VISIBLE_DEVICES\"] = args.gpus\n    if args.enable_exllama:\n        exllama_config = ExllamaConfig(\n            max_seq_len=args.exllama_max_seq_len,\n            gpu_split=args.exllama_gpu_split,\n            cache_8bit=args.exllama_cache_8bit,\n        )\n    else:\n        exllama_config = None\n    if args.enable_xft:\n        xft_config = XftConfig(\n            max_seq_len=args.xft_max_seq_len,\n            data_type=args.xft_dtype,\n        )\n        if args.device != \"cpu\":\n            print(\"xFasterTransformer now is only support CPUs. Reset device to CPU\")\n            args.device = \"cpu\"\n    else:\n        xft_config = None\n    if args.style == \"simple\":\n        chatio = SimpleChatIO(args.multiline)\n    elif args.style == \"rich\":\n        chatio = RichChatIO(args.multiline, args.mouse)\n    elif args.style == \"programmatic\":\n        chatio = ProgrammaticChatIO()\n    else:\n        raise ValueError(f\"Invalid style for console: {args.style}\")\n    try:\n        chat_loop(\n            args.model_path,\n            args.device,\n            args.num_gpus,\n            args.max_gpu_memory,\n            str_to_torch_dtype(args.dtype),\n            args.load_8bit,\n            args.cpu_offloading,\n            args.conv_template,\n            args.conv_system_msg,\n            args.temperature,\n            args.repetition_penalty,\n            args.max_new_tokens,\n            chatio,\n            gptq_config=GptqConfig(\n                ckpt=args.gptq_ckpt or args.model_path,\n                wbits=args.gptq_wbits,\n                groupsize=args.gptq_groupsize,\n                act_order=args.gptq_act_order,\n            ),\n            awq_config=AWQConfig(\n                ckpt=args.awq_ckpt or args.model_path,\n                wbits=args.awq_wbits,\n                groupsize=args.awq_groupsize,\n            ),\n            exllama_config=exllama_config,\n            xft_config=xft_config,\n            revision=args.revision,\n            judge_sent_end=args.judge_sent_end,\n            debug=args.debug,\n            history=not args.no_history,\n        )\n    except KeyboardInterrupt:\n        print(\"exit...\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    add_model_args(parser)\n    parser.add_argument(\n        \"--conv-template\", type=str, default=None, help=\"Conversation prompt template.\"\n    )\n    parser.add_argument(\n        \"--conv-system-msg\", type=str, default=None, help=\"Conversation system message.\"\n    )\n    parser.add_argument(\"--temperature\", type=float, default=0.7)\n    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\n    parser.add_argument(\"--no-history\", action=\"store_true\")\n    parser.add_argument(\n        \"--style\",\n        type=str,\n        default=\"simple\",\n        choices=[\"simple\", \"rich\", \"programmatic\"],\n        help=\"Display style.\",\n    )\n    parser.add_argument(\n        \"--multiline\",\n        action=\"store_true\",\n        help=\"Enable multiline input. Use ESC+Enter for newline.\",\n    )\n    parser.add_argument(\n        \"--mouse\",\n        action=\"store_true\",\n        help=\"[Rich Style]: Enable mouse support for cursor positioning.\",\n    )\n    parser.add_argument(\n        \"--judge-sent-end\",\n        action=\"store_true\",\n        help=\"Whether enable the correction logic that interrupts the output of sentences due to EOS.\",\n    )\n    parser.add_argument(\n        \"--debug\",\n        action=\"store_true\",\n        help=\"Print useful debug information (e.g., prompts)\",\n    )\n    args = parser.parse_args()\n    main(args)\n", "fastchat/serve/launch_all_serve.py": "\"\"\"\nUsage: python launch_all_serve_by_shell.py --model-path-address \"THUDM/chatglm2-6b@localhost@2021\" \"huggyllama/llama-7b@localhost@2022\" \n\nWorkers are listed in format of `model-path`@`host`@`port` \n\nThe key mechanism behind this scripts is: \n    1, execute shell cmd to launch the controller/worker/openai-api-server;\n    2, check the log of controller/worker/openai-api-server to ensure that the serve is launched properly.\nNote that a few of non-critical `fastchat.serve` cmd options are not supported currently.\n\"\"\"\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nimport subprocess\nimport re\nimport argparse\n\nLOGDIR = \"./logs/\"\n\nif not os.path.exists(LOGDIR):\n    os.makedirs(LOGDIR)\n\nparser = argparse.ArgumentParser()\n# ------multi worker-----------------\nparser.add_argument(\n    \"--model-path-address\",\n    default=\"THUDM/chatglm2-6b@localhost@20002\",\n    nargs=\"+\",\n    type=str,\n    help=\"model path, host, and port, formatted as model-path@host@port\",\n)\n# ---------------controller-------------------------\n\nparser.add_argument(\"--controller-host\", type=str, default=\"localhost\")\nparser.add_argument(\"--controller-port\", type=int, default=21001)\nparser.add_argument(\n    \"--dispatch-method\",\n    type=str,\n    choices=[\"lottery\", \"shortest_queue\"],\n    default=\"shortest_queue\",\n)\ncontroller_args = [\"controller-host\", \"controller-port\", \"dispatch-method\"]\n\n# ----------------------worker------------------------------------------\n\nparser.add_argument(\"--worker-host\", type=str, default=\"localhost\")\nparser.add_argument(\"--worker-port\", type=int, default=21002)\n# parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n# parser.add_argument(\n#     \"--controller-address\", type=str, default=\"http://localhost:21001\"\n# )\nparser.add_argument(\n    \"--model-path\",\n    type=str,\n    default=\"lmsys/vicuna-7b-v1.5\",\n    help=\"The path to the weights. This can be a local folder or a Hugging Face repo ID.\",\n)\nparser.add_argument(\n    \"--revision\",\n    type=str,\n    default=\"main\",\n    help=\"Hugging Face Hub model revision identifier\",\n)\nparser.add_argument(\n    \"--device\",\n    type=str,\n    choices=[\"cpu\", \"cuda\", \"mps\", \"xpu\", \"npu\"],\n    default=\"cuda\",\n    help=\"The device type\",\n)\nparser.add_argument(\n    \"--gpus\",\n    type=str,\n    default=\"0\",\n    help=\"A single GPU like 1 or multiple GPUs like 0,2\",\n)\nparser.add_argument(\"--num-gpus\", type=int, default=1)\nparser.add_argument(\n    \"--max-gpu-memory\",\n    type=str,\n    help=\"The maximum memory per gpu. Use a string like '13Gib'\",\n)\nparser.add_argument(\"--load-8bit\", action=\"store_true\", help=\"Use 8-bit quantization\")\nparser.add_argument(\n    \"--cpu-offloading\",\n    action=\"store_true\",\n    help=\"Only when using 8-bit quantization: Offload excess weights to the CPU that don't fit on the GPU\",\n)\nparser.add_argument(\n    \"--gptq-ckpt\",\n    type=str,\n    default=None,\n    help=\"Load quantized model. The path to the local GPTQ checkpoint.\",\n)\nparser.add_argument(\n    \"--gptq-wbits\",\n    type=int,\n    default=16,\n    choices=[2, 3, 4, 8, 16],\n    help=\"#bits to use for quantization\",\n)\nparser.add_argument(\n    \"--gptq-groupsize\",\n    type=int,\n    default=-1,\n    help=\"Groupsize to use for quantization; default uses full row.\",\n)\nparser.add_argument(\n    \"--gptq-act-order\",\n    action=\"store_true\",\n    help=\"Whether to apply the activation order GPTQ heuristic\",\n)\nparser.add_argument(\n    \"--model-names\",\n    type=lambda s: s.split(\",\"),\n    help=\"Optional display comma separated names\",\n)\nparser.add_argument(\n    \"--limit-worker-concurrency\",\n    type=int,\n    default=5,\n    help=\"Limit the model concurrency to prevent OOM.\",\n)\nparser.add_argument(\"--stream-interval\", type=int, default=2)\nparser.add_argument(\"--no-register\", action=\"store_true\")\n\nworker_args = [\n    \"worker-host\",\n    \"worker-port\",\n    \"model-path\",\n    \"revision\",\n    \"device\",\n    \"gpus\",\n    \"num-gpus\",\n    \"max-gpu-memory\",\n    \"load-8bit\",\n    \"cpu-offloading\",\n    \"gptq-ckpt\",\n    \"gptq-wbits\",\n    \"gptq-groupsize\",\n    \"gptq-act-order\",\n    \"model-names\",\n    \"limit-worker-concurrency\",\n    \"stream-interval\",\n    \"no-register\",\n    \"controller-address\",\n]\n# -----------------openai server---------------------------\n\nparser.add_argument(\"--server-host\", type=str, default=\"localhost\", help=\"host name\")\nparser.add_argument(\"--server-port\", type=int, default=8001, help=\"port number\")\nparser.add_argument(\n    \"--allow-credentials\", action=\"store_true\", help=\"allow credentials\"\n)\n# parser.add_argument(\n#     \"--allowed-origins\", type=json.loads, default=[\"*\"], help=\"allowed origins\"\n# )\n# parser.add_argument(\n#     \"--allowed-methods\", type=json.loads, default=[\"*\"], help=\"allowed methods\"\n# )\n# parser.add_argument(\n#     \"--allowed-headers\", type=json.loads, default=[\"*\"], help=\"allowed headers\"\n# )\nparser.add_argument(\n    \"--api-keys\",\n    type=lambda s: s.split(\",\"),\n    help=\"Optional list of comma separated API keys\",\n)\nserver_args = [\n    \"server-host\",\n    \"server-port\",\n    \"allow-credentials\",\n    \"api-keys\",\n    \"controller-address\",\n]\n\nargs = parser.parse_args()\n\nargs = argparse.Namespace(\n    **vars(args),\n    **{\"controller-address\": f\"http://{args.controller_host}:{args.controller_port}\"},\n)\n\nif args.gpus:\n    if len(args.gpus.split(\",\")) < args.num_gpus:\n        raise ValueError(\n            f\"Larger --num-gpus ({args.num_gpus}) than --gpus {args.gpus}!\"\n        )\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n# 0,controller, model_worker, openai_api_server\n# 1, cmd options\n# 2,LOGDIR\n# 3, log file name\nbase_launch_sh = \"nohup python3 -m fastchat.serve.{0} {1} >{2}/{3}.log 2>&1 &\"\n\n# 0 LOGDIR\n#! 1 log file name\n# 2 controller, worker, openai_api_server\nbase_check_sh = \"\"\"while [ `grep -c \"Uvicorn running on\" {0}/{1}.log` -eq '0' ];do\n                        sleep 1s;\n                        echo \"wait {2} running\"\n                done\n                echo '{2} running' \"\"\"\n\n\ndef string_args(args, args_list):\n    args_str = \"\"\n    for key, value in args._get_kwargs():\n        key = key.replace(\"_\", \"-\")\n        if key not in args_list:\n            continue\n\n        key = key.split(\"-\")[-1] if re.search(\"port|host\", key) else key\n        if not value:\n            pass\n        # 1==True ->  True\n        elif isinstance(value, bool) and value == True:\n            args_str += f\" --{key} \"\n        elif (\n            isinstance(value, list)\n            or isinstance(value, tuple)\n            or isinstance(value, set)\n        ):\n            value = \" \".join(value)\n            args_str += f\" --{key} {value} \"\n        else:\n            args_str += f\" --{key} {value} \"\n\n    return args_str\n\n\ndef launch_worker(item):\n    log_name = (\n        item.split(\"/\")[-1]\n        .split(\"\\\\\")[-1]\n        .replace(\"-\", \"_\")\n        .replace(\"@\", \"_\")\n        .replace(\".\", \"_\")\n    )\n\n    args.model_path, args.worker_host, args.worker_port = item.split(\"@\")\n    print(\"*\" * 80)\n    worker_str_args = string_args(args, worker_args)\n    print(worker_str_args)\n    worker_sh = base_launch_sh.format(\n        \"model_worker\", worker_str_args, LOGDIR, f\"worker_{log_name}\"\n    )\n    worker_check_sh = base_check_sh.format(LOGDIR, f\"worker_{log_name}\", \"model_worker\")\n    subprocess.run(worker_sh, shell=True, check=True)\n    subprocess.run(worker_check_sh, shell=True, check=True)\n\n\ndef launch_all():\n    controller_str_args = string_args(args, controller_args)\n    controller_sh = base_launch_sh.format(\n        \"controller\", controller_str_args, LOGDIR, \"controller\"\n    )\n    controller_check_sh = base_check_sh.format(LOGDIR, \"controller\", \"controller\")\n    subprocess.run(controller_sh, shell=True, check=True)\n    subprocess.run(controller_check_sh, shell=True, check=True)\n\n    if isinstance(args.model_path_address, str):\n        launch_worker(args.model_path_address)\n    else:\n        for idx, item in enumerate(args.model_path_address):\n            print(f\"loading {idx}th model:{item}\")\n            launch_worker(item)\n\n    server_str_args = string_args(args, server_args)\n    server_sh = base_launch_sh.format(\n        \"openai_api_server\", server_str_args, LOGDIR, \"openai_api_server\"\n    )\n    server_check_sh = base_check_sh.format(\n        LOGDIR, \"openai_api_server\", \"openai_api_server\"\n    )\n    subprocess.run(server_sh, shell=True, check=True)\n    subprocess.run(server_check_sh, shell=True, check=True)\n\n\nif __name__ == \"__main__\":\n    launch_all()\n", "fastchat/serve/register_worker.py": "\"\"\"\nManually register workers.\n\nUsage:\npython3 -m fastchat.serve.register_worker --controller http://localhost:21001 --worker-name http://localhost:21002\n\"\"\"\n\nimport argparse\n\nimport requests\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--controller-address\", type=str)\n    parser.add_argument(\"--worker-name\", type=str)\n    parser.add_argument(\"--check-heart-beat\", action=\"store_true\")\n    parser.add_argument(\"--multimodal\", action=\"store_true\")\n    args = parser.parse_args()\n\n    url = args.controller_address + \"/register_worker\"\n    data = {\n        \"worker_name\": args.worker_name,\n        \"check_heart_beat\": args.check_heart_beat,\n        \"worker_status\": None,\n        \"multimodal\": args.multimodal,\n    }\n    r = requests.post(url, json=data)\n    assert r.status_code == 200\n", "fastchat/serve/vllm_worker.py": "\"\"\"\nA model worker that executes the model based on vLLM.\n\nSee documentations at docs/vllm_integration.md\n\"\"\"\n\nimport argparse\nimport asyncio\nimport json\nfrom typing import List\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse, JSONResponse\nimport uvicorn\nfrom vllm import AsyncLLMEngine\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.utils import random_uuid\n\nfrom fastchat.serve.base_model_worker import BaseModelWorker\nfrom fastchat.serve.model_worker import (\n    logger,\n    worker_id,\n)\nfrom fastchat.utils import get_context_length, is_partial_stop\n\n\napp = FastAPI()\n\n\nclass VLLMWorker(BaseModelWorker):\n    def __init__(\n        self,\n        controller_addr: str,\n        worker_addr: str,\n        worker_id: str,\n        model_path: str,\n        model_names: List[str],\n        limit_worker_concurrency: int,\n        no_register: bool,\n        llm_engine: AsyncLLMEngine,\n        conv_template: str,\n    ):\n        super().__init__(\n            controller_addr,\n            worker_addr,\n            worker_id,\n            model_path,\n            model_names,\n            limit_worker_concurrency,\n            conv_template,\n        )\n\n        logger.info(\n            f\"Loading the model {self.model_names} on worker {worker_id}, worker type: vLLM worker...\"\n        )\n        self.tokenizer = llm_engine.engine.tokenizer\n        # This is to support vllm >= 0.2.7 where TokenizerGroup was introduced\n        # and llm_engine.engine.tokenizer was no longer a raw tokenizer\n        if hasattr(self.tokenizer, \"tokenizer\"):\n            self.tokenizer = llm_engine.engine.tokenizer.tokenizer\n        self.context_len = get_context_length(llm_engine.engine.model_config.hf_config)\n\n        if not no_register:\n            self.init_heart_beat()\n\n    async def generate_stream(self, params):\n        self.call_ct += 1\n\n        context = params.pop(\"prompt\")\n        request_id = params.pop(\"request_id\")\n        temperature = float(params.get(\"temperature\", 1.0))\n        top_p = float(params.get(\"top_p\", 1.0))\n        top_k = params.get(\"top_k\", -1.0)\n        presence_penalty = float(params.get(\"presence_penalty\", 0.0))\n        frequency_penalty = float(params.get(\"frequency_penalty\", 0.0))\n        max_new_tokens = params.get(\"max_new_tokens\", 256)\n        stop_str = params.get(\"stop\", None)\n        stop_token_ids = params.get(\"stop_token_ids\", None) or []\n        if self.tokenizer.eos_token_id is not None:\n            stop_token_ids.append(self.tokenizer.eos_token_id)\n        echo = params.get(\"echo\", True)\n        use_beam_search = params.get(\"use_beam_search\", False)\n        best_of = params.get(\"best_of\", None)\n\n        request = params.get(\"request\", None)\n\n        # Handle stop_str\n        stop = set()\n        if isinstance(stop_str, str) and stop_str != \"\":\n            stop.add(stop_str)\n        elif isinstance(stop_str, list) and stop_str != []:\n            stop.update(stop_str)\n\n        for tid in stop_token_ids:\n            if tid is not None:\n                s = self.tokenizer.decode(tid)\n                if s != \"\":\n                    stop.add(s)\n\n        # make sampling params in vllm\n        top_p = max(top_p, 1e-5)\n        if temperature <= 1e-5:\n            top_p = 1.0\n\n        sampling_params = SamplingParams(\n            n=1,\n            temperature=temperature,\n            top_p=top_p,\n            use_beam_search=use_beam_search,\n            stop=list(stop),\n            stop_token_ids=stop_token_ids,\n            max_tokens=max_new_tokens,\n            top_k=top_k,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            best_of=best_of,\n        )\n        results_generator = engine.generate(context, sampling_params, request_id)\n\n        async for request_output in results_generator:\n            prompt = request_output.prompt\n            if echo:\n                text_outputs = [\n                    prompt + output.text for output in request_output.outputs\n                ]\n            else:\n                text_outputs = [output.text for output in request_output.outputs]\n            text_outputs = \" \".join(text_outputs)\n\n            partial_stop = any(is_partial_stop(text_outputs, i) for i in stop)\n            # prevent yielding partial stop sequence\n            if partial_stop:\n                continue\n\n            aborted = False\n            if request and await request.is_disconnected():\n                await engine.abort(request_id)\n                request_output.finished = True\n                aborted = True\n                for output in request_output.outputs:\n                    output.finish_reason = \"abort\"\n\n            prompt_tokens = len(request_output.prompt_token_ids)\n            completion_tokens = sum(\n                len(output.token_ids) for output in request_output.outputs\n            )\n            ret = {\n                \"text\": text_outputs,\n                \"error_code\": 0,\n                \"usage\": {\n                    \"prompt_tokens\": prompt_tokens,\n                    \"completion_tokens\": completion_tokens,\n                    \"total_tokens\": prompt_tokens + completion_tokens,\n                },\n                \"cumulative_logprob\": [\n                    output.cumulative_logprob for output in request_output.outputs\n                ],\n                \"finish_reason\": request_output.outputs[0].finish_reason\n                if len(request_output.outputs) == 1\n                else [output.finish_reason for output in request_output.outputs],\n            }\n            # Emit twice here to ensure a 'finish_reason' with empty content in the OpenAI API response.\n            # This aligns with the behavior of model_worker.\n            if request_output.finished:\n                yield (json.dumps({**ret, **{\"finish_reason\": None}}) + \"\\0\").encode()\n            yield (json.dumps(ret) + \"\\0\").encode()\n\n            if aborted:\n                break\n\n    async def generate(self, params):\n        async for x in self.generate_stream(params):\n            pass\n        return json.loads(x[:-1].decode())\n\n\ndef release_worker_semaphore():\n    worker.semaphore.release()\n\n\ndef acquire_worker_semaphore():\n    if worker.semaphore is None:\n        worker.semaphore = asyncio.Semaphore(worker.limit_worker_concurrency)\n    return worker.semaphore.acquire()\n\n\ndef create_background_tasks(request_id):\n    async def abort_request() -> None:\n        await engine.abort(request_id)\n\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(release_worker_semaphore)\n    background_tasks.add_task(abort_request)\n    return background_tasks\n\n\n@app.post(\"/worker_generate_stream\")\nasync def api_generate_stream(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    request_id = random_uuid()\n    params[\"request_id\"] = request_id\n    params[\"request\"] = request\n    generator = worker.generate_stream(params)\n    background_tasks = create_background_tasks(request_id)\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate\")\nasync def api_generate(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    request_id = random_uuid()\n    params[\"request_id\"] = request_id\n    params[\"request\"] = request\n    output = await worker.generate(params)\n    release_worker_semaphore()\n    await engine.abort(request_id)\n    return JSONResponse(output)\n\n\n@app.post(\"/worker_get_status\")\nasync def api_get_status(request: Request):\n    return worker.get_status()\n\n\n@app.post(\"/count_token\")\nasync def api_count_token(request: Request):\n    params = await request.json()\n    return worker.count_token(params)\n\n\n@app.post(\"/worker_get_conv_template\")\nasync def api_get_conv(request: Request):\n    return worker.get_conv_template()\n\n\n@app.post(\"/model_details\")\nasync def api_model_details(request: Request):\n    return {\"context_length\": worker.context_len}\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\"--model-path\", type=str, default=\"lmsys/vicuna-7b-v1.5\")\n    parser.add_argument(\n        \"--model-names\",\n        type=lambda s: s.split(\",\"),\n        help=\"Optional display comma separated names\",\n    )\n    parser.add_argument(\"--limit-worker-concurrency\", type=int, default=1024)\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n    parser.add_argument(\"--num-gpus\", type=int, default=1)\n    parser.add_argument(\n        \"--conv-template\", type=str, default=None, help=\"Conversation prompt template.\"\n    )\n    parser.add_argument(\n        \"--trust_remote_code\",\n        action=\"store_false\",\n        default=True,\n        help=\"Trust remote code (e.g., from HuggingFace) when\"\n        \"downloading the model and tokenizer.\",\n    )\n    parser.add_argument(\n        \"--gpu_memory_utilization\",\n        type=float,\n        default=0.9,\n        help=\"The ratio (between 0 and 1) of GPU memory to\"\n        \"reserve for the model weights, activations, and KV cache. Higher\"\n        \"values will increase the KV cache size and thus improve the model's\"\n        \"throughput. However, if the value is too high, it may cause out-of-\"\n        \"memory (OOM) errors.\",\n    )\n\n    parser = AsyncEngineArgs.add_cli_args(parser)\n    args = parser.parse_args()\n    if args.model_path:\n        args.model = args.model_path\n    if args.num_gpus > 1:\n        args.tensor_parallel_size = args.num_gpus\n\n    engine_args = AsyncEngineArgs.from_cli_args(args)\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\n    worker = VLLMWorker(\n        args.controller_address,\n        args.worker_address,\n        worker_id,\n        args.model_path,\n        args.model_names,\n        args.limit_worker_concurrency,\n        args.no_register,\n        engine,\n        args.conv_template,\n    )\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n", "fastchat/serve/gradio_block_arena_anony.py": "\"\"\"\nChatbot Arena (battle) tab.\nUsers chat with two anonymous models.\n\"\"\"\n\nimport json\nimport time\n\nimport gradio as gr\nimport numpy as np\n\nfrom fastchat.constants import (\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    SLOW_MODEL_MSG,\n    BLIND_MODE_INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_TURN_LIMIT,\n)\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastchat.serve.gradio_block_arena_named import flash_buttons\nfrom fastchat.serve.gradio_web_server import (\n    State,\n    bot_response,\n    get_conv_log_filename,\n    no_change_btn,\n    enable_btn,\n    disable_btn,\n    invisible_btn,\n    acknowledgment_md,\n    get_ip,\n    get_model_description_md,\n    _prepare_text_with_image,\n)\nfrom fastchat.serve.remote_logger import get_remote_logger\nfrom fastchat.utils import (\n    build_logger,\n    moderation_filter,\n)\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\nnum_sides = 2\nenable_moderation = False\nanony_names = [\"\", \"\"]\nmodels = []\n\n\ndef set_global_vars_anony(enable_moderation_):\n    global enable_moderation\n    enable_moderation = enable_moderation_\n\n\ndef load_demo_side_by_side_anony(models_, url_params):\n    global models\n    models = models_\n\n    states = (None,) * num_sides\n    selector_updates = (\n        gr.Markdown(visible=True),\n        gr.Markdown(visible=True),\n    )\n\n    return states + selector_updates\n\n\ndef vote_last_response(states, vote_type, model_selectors, request: gr.Request):\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"models\": [x for x in model_selectors],\n            \"states\": [x.dict() for x in states],\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n    get_remote_logger().log(data)\n\n    if \":\" not in model_selectors[0]:\n        for i in range(5):\n            names = (\n                \"### Model A: \" + states[0].model_name,\n                \"### Model B: \" + states[1].model_name,\n            )\n            yield names + (\"\",) + (disable_btn,) * 4\n            time.sleep(0.1)\n    else:\n        names = (\n            \"### Model A: \" + states[0].model_name,\n            \"### Model B: \" + states[1].model_name,\n        )\n        yield names + (\"\",) + (disable_btn,) * 4\n\n\ndef leftvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"leftvote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"leftvote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef rightvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"rightvote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"rightvote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef tievote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"tievote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"tievote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef bothbad_vote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"bothbad_vote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"bothbad_vote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef regenerate(state0, state1, request: gr.Request):\n    logger.info(f\"regenerate (anony). ip: {get_ip(request)}\")\n    states = [state0, state1]\n    if state0.regen_support and state1.regen_support:\n        for i in range(num_sides):\n            states[i].conv.update_last_message(None)\n        return (\n            states + [x.to_gradio_chatbot() for x in states] + [\"\"] + [disable_btn] * 6\n        )\n    states[0].skip_next = True\n    states[1].skip_next = True\n    return states + [x.to_gradio_chatbot() for x in states] + [\"\"] + [no_change_btn] * 6\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history (anony). ip: {get_ip(request)}\")\n    return (\n        [None] * num_sides\n        + [None] * num_sides\n        + anony_names\n        + [\"\"]\n        + [invisible_btn] * 4\n        + [disable_btn] * 2\n        + [\"\"]\n    )\n\n\ndef share_click(state0, state1, model_selector0, model_selector1, request: gr.Request):\n    logger.info(f\"share (anony). ip: {get_ip(request)}\")\n    if state0 is not None and state1 is not None:\n        vote_last_response(\n            [state0, state1], \"share\", [model_selector0, model_selector1], request\n        )\n\n\nSAMPLING_WEIGHTS = {\n    # tier 0\n    \"gpt-4-0314\": 4,\n    \"gpt-4-0613\": 4,\n    \"gpt-4-1106-preview\": 2,\n    \"gpt-4-0125-preview\": 4,\n    \"gpt-4-turbo-2024-04-09\": 4,\n    \"gpt-3.5-turbo-0125\": 2,\n    \"claude-3-opus-20240229\": 4,\n    \"claude-3-sonnet-20240229\": 4,\n    \"claude-3-haiku-20240307\": 4,\n    \"claude-2.1\": 1,\n    \"zephyr-orpo-141b-A35b-v0.1\": 2,\n    \"dbrx-instruct\": 1,\n    \"command-r-plus\": 4,\n    \"command-r\": 2,\n    \"reka-flash\": 4,\n    \"reka-flash-online\": 4,\n    \"qwen1.5-72b-chat\": 2,\n    \"qwen1.5-32b-chat\": 2,\n    \"qwen1.5-14b-chat\": 2,\n    \"qwen1.5-7b-chat\": 2,\n    \"gemma-1.1-7b-it\": 2,\n    \"gemma-1.1-2b-it\": 1,\n    \"mixtral-8x7b-instruct-v0.1\": 4,\n    \"mistral-7b-instruct-v0.2\": 2,\n    \"mistral-large-2402\": 4,\n    \"mistral-medium\": 2,\n    \"starling-lm-7b-beta\": 2,\n    # tier 1\n    \"deluxe-chat-v1.3\": 2,\n    \"llama-2-70b-chat\": 2,\n    \"llama-2-13b-chat\": 1,\n    \"llama-2-7b-chat\": 1,\n    \"vicuna-33b\": 1,\n    \"vicuna-13b\": 1,\n    \"yi-34b-chat\": 1,\n}\n\n# target model sampling weights will be boosted.\nBATTLE_TARGETS = {\n    \"gpt-4-turbo-2024-04-09\": {\n        \"gpt-4-1106-preview\",\n        \"gpt-4-0125-preview\",\n        \"claude-3-opus-20240229\",\n        \"gemini-pro-dev-api\",\n    },\n    \"gemini-pro-dev-api\": {\n        \"gpt-4-turbo-2024-04-09\",\n        \"claude-3-opus-20240229\",\n        \"gpt-4-0125-preview\",\n        \"claude-3-sonnet-20240229\",\n    },\n    \"reka-flash\": {\n        \"qwen1.5-72b-chat\",\n        \"claude-3-haiku-20240307\",\n        \"command-r-plus\",\n        \"command-r\",\n    },\n    \"reka-flash-online\": {\n        \"qwen1.5-72b-chat\",\n        \"claude-3-haiku-20240307\",\n        \"command-r-plus\",\n        \"command-r\",\n    },\n    \"deluxe-chat-v1.3\": {\n        \"gpt-4-1106-preview\",\n        \"gpt-4-0125-preview\",\n        \"claude-3-opus-20240229\",\n        \"claude-3-sonnet-20240229\",\n    },\n    \"qwen1.5-32b-chat\": {\n        \"gpt-3.5-turbo-0125\",\n        \"gpt-4-0613\",\n        \"gpt-4-0125-preview\",\n        \"llama-2-70b-chat\",\n        \"mixtral-8x7b-instruct-v0.1\",\n        \"mistral-large-2402\",\n        \"yi-34b-chat\",\n    },\n    \"qwen1.5-14b-chat\": {\n        \"starling-lm-7b-alpha\",\n        \"claude-3-haiku-20240307\",\n        \"gpt-3.5-turbo-0125\",\n        \"openchat-3.5-0106\",\n        \"mixtral-8x7b-instruct-v0.1\",\n    },\n    \"mistral-large-2402\": {\n        \"gpt-4-0125-preview\",\n        \"gpt-4-0613\",\n        \"mixtral-8x7b-instruct-v0.1\",\n        \"mistral-medium\",\n        \"mistral-next\",\n        \"claude-3-sonnet-20240229\",\n    },\n    \"gemma-1.1-2b-it\": {\n        \"gpt-3.5-turbo-0125\",\n        \"mixtral-8x7b-instruct-v0.1\",\n        \"starling-lm-7b-beta\",\n        \"llama-2-7b-chat\",\n        \"mistral-7b-instruct-v0.2\",\n        \"gemma-1.1-7b-it\",\n    },\n    \"zephyr-orpo-141b-A35b-v0.1\": {\n        \"qwen1.5-72b-chat\",\n        \"mistral-large-2402\",\n        \"command-r-plus\",\n        \"claude-3-haiku-20240307\",\n    },\n}\n\nSAMPLING_BOOST_MODELS = []\n\n# outage models won't be sampled.\nOUTAGE_MODELS = []\n\n\ndef get_sample_weight(model, outage_models, sampling_weights, sampling_boost_models):\n    if model in outage_models:\n        return 0\n    weight = sampling_weights.get(model, 0)\n    if model in sampling_boost_models:\n        weight *= 5\n    return weight\n\n\ndef get_battle_pair(\n    models, battle_targets, outage_models, sampling_weights, sampling_boost_models\n):\n    if len(models) == 1:\n        return models[0], models[0]\n\n    model_weights = []\n    for model in models:\n        weight = get_sample_weight(\n            model, outage_models, sampling_weights, sampling_boost_models\n        )\n        model_weights.append(weight)\n    total_weight = np.sum(model_weights)\n    model_weights = model_weights / total_weight\n    chosen_idx = np.random.choice(len(models), p=model_weights)\n    chosen_model = models[chosen_idx]\n    # for p, w in zip(models, model_weights):\n    #     print(p, w)\n\n    rival_models = []\n    rival_weights = []\n    for model in models:\n        if model == chosen_model:\n            continue\n        weight = get_sample_weight(\n            model, outage_models, sampling_weights, sampling_boost_models\n        )\n        if (\n            weight != 0\n            and chosen_model in battle_targets\n            and model in battle_targets[chosen_model]\n        ):\n            # boost to 50% chance\n            weight = total_weight / len(battle_targets[chosen_model])\n        rival_models.append(model)\n        rival_weights.append(weight)\n    # for p, w in zip(rival_models, rival_weights):\n    #     print(p, w)\n    rival_weights = rival_weights / np.sum(rival_weights)\n    rival_idx = np.random.choice(len(rival_models), p=rival_weights)\n    rival_model = rival_models[rival_idx]\n\n    swap = np.random.randint(2)\n    if swap == 0:\n        return chosen_model, rival_model\n    else:\n        return rival_model, chosen_model\n\n\ndef add_text(\n    state0, state1, model_selector0, model_selector1, text, image, request: gr.Request\n):\n    ip = get_ip(request)\n    logger.info(f\"add_text (anony). ip: {ip}. len: {len(text)}\")\n    states = [state0, state1]\n    model_selectors = [model_selector0, model_selector1]\n\n    # Init states if necessary\n    if states[0] is None:\n        assert states[1] is None\n\n        model_left, model_right = get_battle_pair(\n            models,\n            BATTLE_TARGETS,\n            OUTAGE_MODELS,\n            SAMPLING_WEIGHTS,\n            SAMPLING_BOOST_MODELS,\n        )\n        states = [\n            State(model_left),\n            State(model_right),\n        ]\n\n    if len(text) <= 0:\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [\"\", None]\n            + [\n                no_change_btn,\n            ]\n            * 6\n            + [\"\"]\n        )\n\n    model_list = [states[i].model_name for i in range(num_sides)]\n    # turn on moderation in battle mode\n    all_conv_text_left = states[0].conv.get_prompt()\n    all_conv_text_right = states[0].conv.get_prompt()\n    all_conv_text = (\n        all_conv_text_left[-1000:] + all_conv_text_right[-1000:] + \"\\nuser: \" + text\n    )\n    flagged = moderation_filter(all_conv_text, model_list, do_moderation=True)\n    if flagged:\n        logger.info(f\"violate moderation (anony). ip: {ip}. text: {text}\")\n        # overwrite the original text\n        text = MODERATION_MSG\n\n    conv = states[0].conv\n    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:\n        logger.info(f\"conversation turn limit. ip: {get_ip(request)}. text: {text}\")\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [CONVERSATION_LIMIT_MSG, None]\n            + [\n                no_change_btn,\n            ]\n            * 6\n            + [\"\"]\n        )\n\n    text = text[:BLIND_MODE_INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    for i in range(num_sides):\n        post_processed_text = _prepare_text_with_image(\n            states[i], text, image, csam_flag=False\n        )\n        states[i].conv.append_message(states[i].conv.roles[0], post_processed_text)\n        states[i].conv.append_message(states[i].conv.roles[1], None)\n        states[i].skip_next = False\n\n    hint_msg = \"\"\n    for i in range(num_sides):\n        if \"deluxe\" in states[i].model_name:\n            hint_msg = SLOW_MODEL_MSG\n    return (\n        states\n        + [x.to_gradio_chatbot() for x in states]\n        + [\"\", None]\n        + [\n            disable_btn,\n        ]\n        * 6\n        + [hint_msg]\n    )\n\n\ndef bot_response_multi(\n    state0,\n    state1,\n    temperature,\n    top_p,\n    max_new_tokens,\n    request: gr.Request,\n):\n    logger.info(f\"bot_response_multi (anony). ip: {get_ip(request)}\")\n\n    if state0 is None or state0.skip_next:\n        # This generate call is skipped due to invalid inputs\n        yield (\n            state0,\n            state1,\n            state0.to_gradio_chatbot(),\n            state1.to_gradio_chatbot(),\n        ) + (no_change_btn,) * 6\n        return\n\n    states = [state0, state1]\n    gen = []\n    for i in range(num_sides):\n        gen.append(\n            bot_response(\n                states[i],\n                temperature,\n                top_p,\n                max_new_tokens,\n                request,\n                apply_rate_limit=False,\n                use_recommended_config=True,\n            )\n        )\n\n    is_stream_batch = []\n    for i in range(num_sides):\n        is_stream_batch.append(\n            states[i].model_name\n            in [\n                \"gemini-pro\",\n                \"gemini-pro-dev-api\",\n                \"gemini-1.0-pro-vision\",\n                \"gemini-1.5-pro\",\n                \"gemini-1.5-flash\",\n                \"gemma-1.1-2b-it\",\n                \"gemma-1.1-7b-it\",\n            ]\n        )\n    chatbots = [None] * num_sides\n    iters = 0\n    while True:\n        stop = True\n        iters += 1\n        for i in range(num_sides):\n            try:\n                # yield gemini fewer times as its chunk size is larger\n                # otherwise, gemini will stream too fast\n                if not is_stream_batch[i] or (iters % 30 == 1 or iters < 3):\n                    ret = next(gen[i])\n                    states[i], chatbots[i] = ret[0], ret[1]\n                stop = False\n            except StopIteration:\n                pass\n        yield states + chatbots + [disable_btn] * 6\n        if stop:\n            break\n\n\ndef build_side_by_side_ui_anony(models):\n    notice_markdown = \"\"\"\n# \u2694\ufe0f  LMSYS Chatbot Arena: Benchmarking LLMs in the Wild\n- | [Blog](https://lmsys.org/blog/2023-05-03-arena/) | [GitHub](https://github.com/lm-sys/FastChat) | [Paper](https://arxiv.org/abs/2306.05685) | [Dataset](https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md) | [Twitter](https://twitter.com/lmsysorg) | [Discord](https://discord.gg/HSWAKCrnFx) |\n\n## \ud83d\udcdc Rules\n- Ask any question to two anonymous models (e.g., ChatGPT, Claude, Llama) and vote for the better one!\n- You can continue chatting until you identify a winner.\n- Vote won't be counted if model identity is revealed during conversation.\n\n## \ud83c\udfc6 LMSYS Arena [Leaderboard](https://leaderboard.lmsys.org)\nWe've collected **500K+** human votes to compute an LLM Elo leaderboard.\nFind out who is the \ud83e\udd47LLM Champion!\n\n## \ud83d\udc47 Chat now!\n\"\"\"\n\n    states = [gr.State() for _ in range(num_sides)]\n    model_selectors = [None] * num_sides\n    chatbots = [None] * num_sides\n\n    gr.Markdown(notice_markdown, elem_id=\"notice_markdown\")\n\n    with gr.Group(elem_id=\"share-region-anony\"):\n        with gr.Accordion(\n            f\"\ud83d\udd0d Expand to see the descriptions of {len(models)} models\", open=False\n        ):\n            model_description_md = get_model_description_md(models)\n            gr.Markdown(model_description_md, elem_id=\"model_description_markdown\")\n        with gr.Row():\n            for i in range(num_sides):\n                label = \"Model A\" if i == 0 else \"Model B\"\n                with gr.Column():\n                    chatbots[i] = gr.Chatbot(\n                        label=label,\n                        elem_id=\"chatbot\",\n                        height=550,\n                        show_copy_button=True,\n                    )\n\n        with gr.Row():\n            for i in range(num_sides):\n                with gr.Column():\n                    model_selectors[i] = gr.Markdown(\n                        anony_names[i], elem_id=\"model_selector_md\"\n                    )\n        with gr.Row():\n            slow_warning = gr.Markdown(\"\")\n\n    with gr.Row():\n        leftvote_btn = gr.Button(\n            value=\"\ud83d\udc48  A is better\", visible=False, interactive=False\n        )\n        rightvote_btn = gr.Button(\n            value=\"\ud83d\udc49  B is better\", visible=False, interactive=False\n        )\n        tie_btn = gr.Button(value=\"\ud83e\udd1d  Tie\", visible=False, interactive=False)\n        bothbad_btn = gr.Button(\n            value=\"\ud83d\udc4e  Both are bad\", visible=False, interactive=False\n        )\n\n    with gr.Row():\n        textbox = gr.Textbox(\n            show_label=False,\n            placeholder=\"\ud83d\udc49 Enter your prompt and press ENTER\",\n            elem_id=\"input_box\",\n        )\n        send_btn = gr.Button(value=\"Send\", variant=\"primary\", scale=0)\n\n    with gr.Row() as button_row:\n        clear_btn = gr.Button(value=\"\ud83c\udfb2 New Round\", interactive=False)\n        regenerate_btn = gr.Button(value=\"\ud83d\udd04  Regenerate\", interactive=False)\n        share_btn = gr.Button(value=\"\ud83d\udcf7  Share\")\n\n    with gr.Accordion(\"Parameters\", open=False, visible=False) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=2048,\n            value=1024,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    gr.Markdown(acknowledgment_md, elem_id=\"ack_markdown\")\n\n    imagebox = gr.State(None)\n    # Register listeners\n    btn_list = [\n        leftvote_btn,\n        rightvote_btn,\n        tie_btn,\n        bothbad_btn,\n        regenerate_btn,\n        clear_btn,\n    ]\n    leftvote_btn.click(\n        leftvote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    rightvote_btn.click(\n        rightvote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    tie_btn.click(\n        tievote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    bothbad_btn.click(\n        bothbad_vote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    regenerate_btn.click(\n        regenerate, states, states + chatbots + [textbox] + btn_list\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n    clear_btn.click(\n        clear_history,\n        None,\n        states + chatbots + model_selectors + [textbox] + btn_list + [slow_warning],\n    )\n\n    share_js = \"\"\"\nfunction (a, b, c, d) {\n    const captureElement = document.querySelector('#share-region-anony');\n    html2canvas(captureElement)\n        .then(canvas => {\n            canvas.style.display = 'none'\n            document.body.appendChild(canvas)\n            return canvas\n        })\n        .then(canvas => {\n            const image = canvas.toDataURL('image/png')\n            const a = document.createElement('a')\n            a.setAttribute('download', 'chatbot-arena.png')\n            a.setAttribute('href', image)\n            a.click()\n            canvas.remove()\n        });\n    return [a, b, c, d];\n}\n\"\"\"\n    share_btn.click(share_click, states + model_selectors, [], js=share_js)\n\n    textbox.submit(\n        add_text,\n        states + model_selectors + [textbox, imagebox],\n        states + chatbots + [textbox, imagebox] + btn_list + [slow_warning],\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons,\n        [],\n        btn_list,\n    )\n\n    send_btn.click(\n        add_text,\n        states + model_selectors + [textbox, imagebox],\n        states + chatbots + [textbox, imagebox] + btn_list,\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n\n    return states + model_selectors\n", "fastchat/serve/gradio_block_arena_vision_named.py": "\"\"\"\nMultimodal Chatbot Arena (side-by-side) tab.\nUsers chat with two chosen models.\n\"\"\"\n\nimport json\nimport os\nimport time\n\nimport gradio as gr\nimport numpy as np\n\nfrom fastchat.constants import (\n    TEXT_MODERATION_MSG,\n    IMAGE_MODERATION_MSG,\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    SLOW_MODEL_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_TURN_LIMIT,\n)\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastchat.serve.gradio_block_arena_named import (\n    flash_buttons,\n    share_click,\n    bot_response_multi,\n)\nfrom fastchat.serve.gradio_block_arena_vision import (\n    get_vqa_sample,\n    set_invisible_image,\n    set_visible_image,\n    add_image,\n    moderate_input,\n)\nfrom fastchat.serve.gradio_web_server import (\n    State,\n    bot_response,\n    get_conv_log_filename,\n    no_change_btn,\n    enable_btn,\n    disable_btn,\n    invisible_btn,\n    acknowledgment_md,\n    get_ip,\n    get_model_description_md,\n    _prepare_text_with_image,\n)\nfrom fastchat.serve.remote_logger import get_remote_logger\nfrom fastchat.utils import (\n    build_logger,\n    moderation_filter,\n    image_moderation_filter,\n)\n\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\nnum_sides = 2\nenable_moderation = False\n\n\ndef clear_history_example(request: gr.Request):\n    logger.info(f\"clear_history_example (named). ip: {get_ip(request)}\")\n    return (\n        [None] * num_sides\n        + [None] * num_sides\n        + [invisible_btn] * 4\n        + [disable_btn] * 2\n    )\n\n\ndef vote_last_response(states, vote_type, model_selectors, request: gr.Request):\n    filename = get_conv_log_filename(states[0].is_vision, states[0].has_csam_image)\n    with open(filename, \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"models\": [x for x in model_selectors],\n            \"states\": [x.dict() for x in states],\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n    get_remote_logger().log(data)\n\n\ndef leftvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"leftvote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"leftvote\", [model_selector0, model_selector1], request\n    )\n    return (None,) + (disable_btn,) * 4\n\n\ndef rightvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"rightvote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"rightvote\", [model_selector0, model_selector1], request\n    )\n    return (None,) + (disable_btn,) * 4\n\n\ndef tievote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"tievote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"tievote\", [model_selector0, model_selector1], request\n    )\n    return (None,) + (disable_btn,) * 4\n\n\ndef bothbad_vote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"bothbad_vote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"bothbad_vote\", [model_selector0, model_selector1], request\n    )\n    return (None,) + (disable_btn,) * 4\n\n\ndef regenerate(state0, state1, request: gr.Request):\n    logger.info(f\"regenerate (named). ip: {get_ip(request)}\")\n    states = [state0, state1]\n    if state0.regen_support and state1.regen_support:\n        for i in range(num_sides):\n            states[i].conv.update_last_message(None)\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [None]\n            + [disable_btn] * 6\n        )\n    states[0].skip_next = True\n    states[1].skip_next = True\n    return (\n        states + [x.to_gradio_chatbot() for x in states] + [None] + [no_change_btn] * 6\n    )\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history (named). ip: {get_ip(request)}\")\n    return (\n        [None] * num_sides\n        + [None] * num_sides\n        + [None]\n        + [invisible_btn] * 4\n        + [disable_btn] * 2\n    )\n\n\ndef add_text(\n    state0, state1, model_selector0, model_selector1, chat_input, request: gr.Request\n):\n    text, images = chat_input[\"text\"], chat_input[\"files\"]\n    ip = get_ip(request)\n    logger.info(f\"add_text (named). ip: {ip}. len: {len(text)}\")\n    states = [state0, state1]\n    model_selectors = [model_selector0, model_selector1]\n\n    # Init states if necessary\n    for i in range(num_sides):\n        if states[i] is None:\n            states[i] = State(model_selectors[i], is_vision=True)\n\n    if len(text) <= 0:\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [None]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    model_list = [states[i].model_name for i in range(num_sides)]\n    all_conv_text_left = states[0].conv.get_prompt()\n    all_conv_text_right = states[0].conv.get_prompt()\n    all_conv_text = (\n        all_conv_text_left[-1000:] + all_conv_text_right[-1000:] + \"\\nuser: \" + text\n    )\n\n    text, image_flagged, csam_flag = moderate_input(\n        text, all_conv_text, model_list, images, ip\n    )\n\n    conv = states[0].conv\n    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:\n        logger.info(f\"conversation turn limit. ip: {ip}. text: {text}\")\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [{\"text\": CONVERSATION_LIMIT_MSG}]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    if image_flagged:\n        logger.info(f\"image flagged. ip: {ip}. text: {text}\")\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [{\"text\": IMAGE_MODERATION_MSG}]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    for i in range(num_sides):\n        post_processed_text = _prepare_text_with_image(\n            states[i], text, images, csam_flag=csam_flag\n        )\n        states[i].conv.append_message(states[i].conv.roles[0], post_processed_text)\n        states[i].conv.append_message(states[i].conv.roles[1], None)\n        states[i].skip_next = False\n\n    return (\n        states\n        + [x.to_gradio_chatbot() for x in states]\n        + [None]\n        + [\n            disable_btn,\n        ]\n        * 6\n    )\n\n\ndef build_side_by_side_vision_ui_named(models, random_questions=None):\n    notice_markdown = \"\"\"\n# \u2694\ufe0f  Vision Arena \u2694\ufe0f : Benchmarking VLMs in the Wild\n| [Blog](https://lmsys.org/blog/2023-05-03-arena/) | [GitHub](https://github.com/lm-sys/FastChat) | [Paper](https://arxiv.org/abs/2306.05685) | [Dataset](https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md) | [Twitter](https://twitter.com/lmsysorg) | [Discord](https://discord.gg/HSWAKCrnFx) |\n\n## \ud83d\udcdc Rules\n- Chat with any two models side-by-side and vote!\n- You can continue chatting for multiple rounds.\n- Click \"Clear history\" to start a new round.\n- You can only chat with <span style='color: #DE3163; font-weight: bold'>one image per conversation</span>. You can upload images less than 15MB. Click the \"Random Example\" button to chat with a random image.\n\n**\u2757\ufe0f For research purposes, we log user prompts and images, and may release this data to the public in the future. Please do not upload any confidential or personal information.**\n\n## \ud83e\udd16 Choose two models to compare\n\"\"\"\n\n    states = [gr.State() for _ in range(num_sides)]\n    model_selectors = [None] * num_sides\n    chatbots = [None] * num_sides\n\n    notice = gr.Markdown(notice_markdown, elem_id=\"notice_markdown\")\n\n    with gr.Row():\n        with gr.Column(scale=2, visible=False) as image_column:\n            imagebox = gr.Image(\n                type=\"pil\",\n                show_label=False,\n                interactive=False,\n            )\n\n        with gr.Column(scale=5):\n            with gr.Group(elem_id=\"share-region-anony\"):\n                with gr.Accordion(\n                    f\"\ud83d\udd0d Expand to see the descriptions of {len(models)} models\",\n                    open=False,\n                ):\n                    model_description_md = get_model_description_md(models)\n                    gr.Markdown(\n                        model_description_md, elem_id=\"model_description_markdown\"\n                    )\n\n                with gr.Row():\n                    for i in range(num_sides):\n                        with gr.Column():\n                            model_selectors[i] = gr.Dropdown(\n                                choices=models,\n                                value=models[i] if len(models) > i else \"\",\n                                interactive=True,\n                                show_label=False,\n                                container=False,\n                            )\n\n                with gr.Row():\n                    for i in range(num_sides):\n                        label = \"Model A\" if i == 0 else \"Model B\"\n                        with gr.Column():\n                            chatbots[i] = gr.Chatbot(\n                                label=label,\n                                elem_id=f\"chatbot\",\n                                height=550,\n                                show_copy_button=True,\n                            )\n\n    with gr.Row():\n        leftvote_btn = gr.Button(\n            value=\"\ud83d\udc48  A is better\", visible=False, interactive=False\n        )\n        rightvote_btn = gr.Button(\n            value=\"\ud83d\udc49  B is better\", visible=False, interactive=False\n        )\n        tie_btn = gr.Button(value=\"\ud83e\udd1d  Tie\", visible=False, interactive=False)\n        bothbad_btn = gr.Button(\n            value=\"\ud83d\udc4e  Both are bad\", visible=False, interactive=False\n        )\n\n    with gr.Row():\n        textbox = gr.MultimodalTextbox(\n            file_types=[\"image\"],\n            show_label=False,\n            placeholder=\"Click add or drop your image here\",\n            container=True,\n            elem_id=\"input_box\",\n        )\n\n    with gr.Row() as button_row:\n        if random_questions:\n            global vqa_samples\n            with open(random_questions, \"r\") as f:\n                vqa_samples = json.load(f)\n            random_btn = gr.Button(value=\"\ud83c\udfb2 Random Example\", interactive=True)\n        clear_btn = gr.Button(value=\"\ud83d\uddd1\ufe0f  Clear history\", interactive=False)\n        regenerate_btn = gr.Button(value=\"\ud83d\udd04  Regenerate\", interactive=False)\n        share_btn = gr.Button(value=\"\ud83d\udcf7  Share\")\n\n    with gr.Accordion(\"Parameters\", open=False) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=2048,\n            value=1024,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    gr.Markdown(acknowledgment_md, elem_id=\"ack_markdown\")\n\n    # Register listeners\n    btn_list = [\n        leftvote_btn,\n        rightvote_btn,\n        tie_btn,\n        bothbad_btn,\n        regenerate_btn,\n        clear_btn,\n    ]\n    leftvote_btn.click(\n        leftvote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    rightvote_btn.click(\n        rightvote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    tie_btn.click(\n        tievote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    bothbad_btn.click(\n        bothbad_vote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    regenerate_btn.click(\n        regenerate, states, states + chatbots + [textbox] + btn_list\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n    clear_btn.click(clear_history, None, states + chatbots + [textbox] + btn_list)\n\n    share_js = \"\"\"\nfunction (a, b, c, d) {\n    const captureElement = document.querySelector('#share-region-named');\n    html2canvas(captureElement)\n        .then(canvas => {\n            canvas.style.display = 'none'\n            document.body.appendChild(canvas)\n            return canvas\n        })\n        .then(canvas => {\n            const image = canvas.toDataURL('image/png')\n            const a = document.createElement('a')\n            a.setAttribute('download', 'chatbot-arena.png')\n            a.setAttribute('href', image)\n            a.click()\n            canvas.remove()\n        });\n    return [a, b, c, d];\n}\n\"\"\"\n    share_btn.click(share_click, states + model_selectors, [], js=share_js)\n\n    for i in range(num_sides):\n        model_selectors[i].change(\n            clear_history, None, states + chatbots + [textbox] + btn_list\n        ).then(set_visible_image, [textbox], [image_column])\n\n    textbox.input(add_image, [textbox], [imagebox]).then(\n        set_visible_image, [textbox], [image_column]\n    ).then(clear_history_example, None, states + chatbots + btn_list)\n\n    textbox.submit(\n        add_text,\n        states + model_selectors + [textbox],\n        states + chatbots + [textbox] + btn_list,\n    ).then(set_invisible_image, [], [image_column]).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n\n    if random_questions:\n        random_btn.click(\n            get_vqa_sample,  # First, get the VQA sample\n            [],  # Pass the path to the VQA samples\n            [textbox, imagebox],  # Outputs are textbox and imagebox\n        ).then(set_visible_image, [textbox], [image_column]).then(\n            clear_history_example, None, states + chatbots + btn_list\n        )\n\n    return states + model_selectors\n", "fastchat/serve/controller.py": "\"\"\"\nA controller manages distributed workers.\nIt sends worker addresses to clients.\n\"\"\"\nimport argparse\nimport asyncio\nimport dataclasses\nfrom enum import Enum, auto\nimport json\nimport logging\nimport os\nimport time\nfrom typing import List, Union\nimport threading\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nimport numpy as np\nimport requests\nimport uvicorn\n\nfrom fastchat.constants import (\n    CONTROLLER_HEART_BEAT_EXPIRATION,\n    WORKER_API_TIMEOUT,\n    ErrorCode,\n    SERVER_ERROR_MSG,\n)\nfrom fastchat.utils import build_logger\n\n\nlogger = build_logger(\"controller\", \"controller.log\")\n\n\nclass DispatchMethod(Enum):\n    LOTTERY = auto()\n    SHORTEST_QUEUE = auto()\n\n    @classmethod\n    def from_str(cls, name):\n        if name == \"lottery\":\n            return cls.LOTTERY\n        elif name == \"shortest_queue\":\n            return cls.SHORTEST_QUEUE\n        else:\n            raise ValueError(f\"Invalid dispatch method\")\n\n\n@dataclasses.dataclass\nclass WorkerInfo:\n    model_names: List[str]\n    speed: int\n    queue_length: int\n    check_heart_beat: bool\n    last_heart_beat: str\n    multimodal: bool\n\n\ndef heart_beat_controller(controller):\n    while True:\n        time.sleep(CONTROLLER_HEART_BEAT_EXPIRATION)\n        controller.remove_stale_workers_by_expiration()\n\n\nclass Controller:\n    def __init__(self, dispatch_method: str):\n        # Dict[str -> WorkerInfo]\n        self.worker_info = {}\n        self.dispatch_method = DispatchMethod.from_str(dispatch_method)\n\n        self.heart_beat_thread = threading.Thread(\n            target=heart_beat_controller, args=(self,)\n        )\n        self.heart_beat_thread.start()\n\n    def register_worker(\n        self,\n        worker_name: str,\n        check_heart_beat: bool,\n        worker_status: dict,\n        multimodal: bool,\n    ):\n        if worker_name not in self.worker_info:\n            logger.info(f\"Register a new worker: {worker_name}\")\n        else:\n            logger.info(f\"Register an existing worker: {worker_name}\")\n\n        if not worker_status:\n            worker_status = self.get_worker_status(worker_name)\n        if not worker_status:\n            return False\n\n        self.worker_info[worker_name] = WorkerInfo(\n            worker_status[\"model_names\"],\n            worker_status[\"speed\"],\n            worker_status[\"queue_length\"],\n            check_heart_beat,\n            time.time(),\n            multimodal,\n        )\n\n        logger.info(f\"Register done: {worker_name}, {worker_status}\")\n        return True\n\n    def get_worker_status(self, worker_name: str):\n        try:\n            r = requests.post(worker_name + \"/worker_get_status\", timeout=5)\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Get status fails: {worker_name}, {e}\")\n            return None\n\n        if r.status_code != 200:\n            logger.error(f\"Get status fails: {worker_name}, {r}\")\n            return None\n\n        return r.json()\n\n    def remove_worker(self, worker_name: str):\n        del self.worker_info[worker_name]\n\n    def refresh_all_workers(self):\n        old_info = dict(self.worker_info)\n        self.worker_info = {}\n\n        for w_name, w_info in old_info.items():\n            if not self.register_worker(\n                w_name, w_info.check_heart_beat, None, w_info.multimodal\n            ):\n                logger.info(f\"Remove stale worker: {w_name}\")\n\n    def list_models(self):\n        model_names = set()\n\n        for w_name, w_info in self.worker_info.items():\n            model_names.update(w_info.model_names)\n\n        return list(model_names)\n\n    def list_multimodal_models(self):\n        model_names = set()\n\n        for w_name, w_info in self.worker_info.items():\n            if w_info.multimodal:\n                model_names.update(w_info.model_names)\n\n        return list(model_names)\n\n    def list_language_models(self):\n        model_names = set()\n\n        for w_name, w_info in self.worker_info.items():\n            if not w_info.multimodal:\n                model_names.update(w_info.model_names)\n\n        return list(model_names)\n\n    def get_worker_address(self, model_name: str):\n        if self.dispatch_method == DispatchMethod.LOTTERY:\n            worker_names = []\n            worker_speeds = []\n            for w_name, w_info in self.worker_info.items():\n                if model_name in w_info.model_names:\n                    worker_names.append(w_name)\n                    worker_speeds.append(w_info.speed)\n            worker_speeds = np.array(worker_speeds, dtype=np.float32)\n            norm = np.sum(worker_speeds)\n            if norm < 1e-4:\n                return \"\"\n            worker_speeds = worker_speeds / norm\n            if True:  # Directly return address\n                pt = np.random.choice(np.arange(len(worker_names)), p=worker_speeds)\n                worker_name = worker_names[pt]\n                return worker_name\n\n            # Check status before returning\n            while True:\n                pt = np.random.choice(np.arange(len(worker_names)), p=worker_speeds)\n                worker_name = worker_names[pt]\n\n                if self.get_worker_status(worker_name):\n                    break\n                else:\n                    self.remove_worker(worker_name)\n                    worker_speeds[pt] = 0\n                    norm = np.sum(worker_speeds)\n                    if norm < 1e-4:\n                        return \"\"\n                    worker_speeds = worker_speeds / norm\n                    continue\n            return worker_name\n        elif self.dispatch_method == DispatchMethod.SHORTEST_QUEUE:\n            worker_names = []\n            worker_qlen = []\n            for w_name, w_info in self.worker_info.items():\n                if model_name in w_info.model_names:\n                    worker_names.append(w_name)\n                    worker_qlen.append(w_info.queue_length / w_info.speed)\n            if len(worker_names) == 0:\n                return \"\"\n            min_index = np.argmin(worker_qlen)\n            w_name = worker_names[min_index]\n            self.worker_info[w_name].queue_length += 1\n            logger.info(\n                f\"names: {worker_names}, queue_lens: {worker_qlen}, ret: {w_name}\"\n            )\n            return w_name\n        else:\n            raise ValueError(f\"Invalid dispatch method: {self.dispatch_method}\")\n\n    def receive_heart_beat(self, worker_name: str, queue_length: int):\n        if worker_name not in self.worker_info:\n            logger.info(f\"Receive unknown heart beat. {worker_name}\")\n            return False\n\n        self.worker_info[worker_name].queue_length = queue_length\n        self.worker_info[worker_name].last_heart_beat = time.time()\n        logger.info(f\"Receive heart beat. {worker_name}\")\n        return True\n\n    def remove_stale_workers_by_expiration(self):\n        expire = time.time() - CONTROLLER_HEART_BEAT_EXPIRATION\n        to_delete = []\n        for worker_name, w_info in self.worker_info.items():\n            if w_info.check_heart_beat and w_info.last_heart_beat < expire:\n                to_delete.append(worker_name)\n\n        for worker_name in to_delete:\n            self.remove_worker(worker_name)\n\n    def handle_no_worker(self, params):\n        logger.info(f\"no worker: {params['model']}\")\n        ret = {\n            \"text\": SERVER_ERROR_MSG,\n            \"error_code\": ErrorCode.CONTROLLER_NO_WORKER,\n        }\n        return json.dumps(ret).encode() + b\"\\0\"\n\n    def handle_worker_timeout(self, worker_address):\n        logger.info(f\"worker timeout: {worker_address}\")\n        ret = {\n            \"text\": SERVER_ERROR_MSG,\n            \"error_code\": ErrorCode.CONTROLLER_WORKER_TIMEOUT,\n        }\n        return json.dumps(ret).encode() + b\"\\0\"\n\n    # Let the controller act as a worker to achieve hierarchical\n    # management. This can be used to connect isolated sub networks.\n    def worker_api_get_status(self):\n        model_names = set()\n        speed = 0\n        queue_length = 0\n\n        for w_name in self.worker_info:\n            worker_status = self.get_worker_status(w_name)\n            if worker_status is not None:\n                model_names.update(worker_status[\"model_names\"])\n                speed += worker_status[\"speed\"]\n                queue_length += worker_status[\"queue_length\"]\n\n        model_names = sorted(list(model_names))\n        return {\n            \"model_names\": model_names,\n            \"speed\": speed,\n            \"queue_length\": queue_length,\n        }\n\n    def worker_api_generate_stream(self, params):\n        worker_addr = self.get_worker_address(params[\"model\"])\n        if not worker_addr:\n            yield self.handle_no_worker(params)\n\n        try:\n            response = requests.post(\n                worker_addr + \"/worker_generate_stream\",\n                json=params,\n                stream=True,\n                timeout=WORKER_API_TIMEOUT,\n            )\n            for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n                if chunk:\n                    yield chunk + b\"\\0\"\n        except requests.exceptions.RequestException as e:\n            yield self.handle_worker_timeout(worker_addr)\n\n\napp = FastAPI()\n\n\n@app.post(\"/register_worker\")\nasync def register_worker(request: Request):\n    data = await request.json()\n    controller.register_worker(\n        data[\"worker_name\"],\n        data[\"check_heart_beat\"],\n        data.get(\"worker_status\", None),\n        data.get(\"multimodal\", False),\n    )\n\n\n@app.post(\"/refresh_all_workers\")\nasync def refresh_all_workers():\n    models = controller.refresh_all_workers()\n\n\n@app.post(\"/list_models\")\nasync def list_models():\n    models = controller.list_models()\n    return {\"models\": models}\n\n\n@app.post(\"/list_multimodal_models\")\nasync def list_multimodal_models():\n    models = controller.list_multimodal_models()\n    return {\"models\": models}\n\n\n@app.post(\"/list_language_models\")\nasync def list_language_models():\n    models = controller.list_language_models()\n    return {\"models\": models}\n\n\n@app.post(\"/get_worker_address\")\nasync def get_worker_address(request: Request):\n    data = await request.json()\n    addr = controller.get_worker_address(data[\"model\"])\n    return {\"address\": addr}\n\n\n@app.post(\"/receive_heart_beat\")\nasync def receive_heart_beat(request: Request):\n    data = await request.json()\n    exist = controller.receive_heart_beat(data[\"worker_name\"], data[\"queue_length\"])\n    return {\"exist\": exist}\n\n\n@app.post(\"/worker_generate_stream\")\nasync def worker_api_generate_stream(request: Request):\n    params = await request.json()\n    generator = controller.worker_api_generate_stream(params)\n    return StreamingResponse(generator)\n\n\n@app.post(\"/worker_get_status\")\nasync def worker_api_get_status(request: Request):\n    return controller.worker_api_get_status()\n\n\n@app.get(\"/test_connection\")\nasync def worker_api_get_status(request: Request):\n    return \"success\"\n\n\ndef create_controller():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21001)\n    parser.add_argument(\n        \"--dispatch-method\",\n        type=str,\n        choices=[\"lottery\", \"shortest_queue\"],\n        default=\"shortest_queue\",\n    )\n    parser.add_argument(\n        \"--ssl\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"Enable SSL. Requires OS Environment variables 'SSL_KEYFILE' and 'SSL_CERTFILE'.\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    controller = Controller(args.dispatch_method)\n    return args, controller\n\n\nif __name__ == \"__main__\":\n    args, controller = create_controller()\n    if args.ssl:\n        uvicorn.run(\n            app,\n            host=args.host,\n            port=args.port,\n            log_level=\"info\",\n            ssl_keyfile=os.environ[\"SSL_KEYFILE\"],\n            ssl_certfile=os.environ[\"SSL_CERTFILE\"],\n        )\n    else:\n        uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n", "fastchat/serve/huggingface_api_worker.py": "\"\"\"\nA model worker that calls huggingface inference endpoint.\n\nRegister models in a JSON file with the following format:\n{\n    \"falcon-180b-chat\": {\n        \"model_name\": \"falcon-180B-chat\",\n        \"api_base\": \"https://api-inference.huggingface.co/models\",\n        \"model_path\": \"tiiuae/falcon-180B-chat\",\n        \"token\": \"hf_XXX\",\n        \"context_length\": 2048\n    },\n    \"zephyr-7b-beta\": {\n        \"model_name\": \"zephyr-7b-beta\",\n        \"model_path\": \"\",\n        \"api_base\": \"xxx\",\n        \"token\": \"hf_XXX\",\n        \"context_length\": 4096\n    }\n}\n\n\"model_path\", \"api_base\", \"token\", and \"context_length\" are necessary, while others are optional.\n\"\"\"\nimport argparse\nimport asyncio\nimport json\nimport uuid\nimport os\nfrom typing import List, Optional\n\nimport requests\nimport uvicorn\nfrom fastapi import BackgroundTasks, FastAPI, Request\nfrom fastapi.responses import JSONResponse, StreamingResponse\nfrom huggingface_hub import InferenceClient\n\nfrom fastchat.constants import SERVER_ERROR_MSG, ErrorCode\nfrom fastchat.serve.base_model_worker import BaseModelWorker\nfrom fastchat.utils import build_logger\n\nworker_id = str(uuid.uuid4())[:8]\nlogger = build_logger(\"model_worker\", f\"model_worker_{worker_id}.log\")\n\nworkers = []\nworker_map = {}\napp = FastAPI()\n\n\n# reference to\n# https://github.com/philschmid/easyllm/blob/cbd908b3b3f44a97a22cb0fc2c93df3660bacdad/easyllm/clients/huggingface.py#L374-L392\ndef get_gen_kwargs(\n    params,\n    seed: Optional[int] = None,\n):\n    stop = params.get(\"stop\", None)\n    if isinstance(stop, list):\n        stop_sequences = stop\n    elif isinstance(stop, str):\n        stop_sequences = [stop]\n    else:\n        stop_sequences = []\n    gen_kwargs = {\n        \"do_sample\": True,\n        \"return_full_text\": bool(params.get(\"echo\", False)),\n        \"max_new_tokens\": int(params.get(\"max_new_tokens\", 256)),\n        \"top_p\": float(params.get(\"top_p\", 1.0)),\n        \"temperature\": float(params.get(\"temperature\", 1.0)),\n        \"stop_sequences\": stop_sequences,\n        \"repetition_penalty\": float(params.get(\"repetition_penalty\", 1.0)),\n        \"top_k\": params.get(\"top_k\", None),\n        \"seed\": seed,\n    }\n    if gen_kwargs[\"top_p\"] == 1:\n        gen_kwargs[\"top_p\"] = 0.9999999\n    if gen_kwargs[\"top_p\"] == 0:\n        gen_kwargs.pop(\"top_p\")\n    if gen_kwargs[\"temperature\"] == 0:\n        gen_kwargs.pop(\"temperature\")\n        gen_kwargs[\"do_sample\"] = False\n    return gen_kwargs\n\n\ndef could_be_stop(text, stop):\n    for s in stop:\n        if any(text.endswith(s[:i]) for i in range(1, len(s) + 1)):\n            return True\n    return False\n\n\nclass HuggingfaceApiWorker(BaseModelWorker):\n    def __init__(\n        self,\n        controller_addr: str,\n        worker_addr: str,\n        worker_id: str,\n        model_path: str,\n        api_base: str,\n        token: str,\n        context_length: int,\n        model_names: List[str],\n        limit_worker_concurrency: int,\n        no_register: bool,\n        conv_template: Optional[str] = None,\n        seed: Optional[int] = None,\n        **kwargs,\n    ):\n        super().__init__(\n            controller_addr,\n            worker_addr,\n            worker_id,\n            model_path,\n            model_names,\n            limit_worker_concurrency,\n            conv_template=conv_template,\n        )\n\n        self.model_path = model_path\n        self.api_base = api_base\n        self.token = token\n        self.context_len = context_length\n        self.seed = seed\n\n        logger.info(\n            f\"Connecting with huggingface api {self.model_path} as {self.model_names} on worker {worker_id} ...\"\n        )\n\n        if not no_register:\n            self.init_heart_beat()\n\n    def count_token(self, params):\n        # No tokenizer here\n        ret = {\n            \"count\": 0,\n            \"error_code\": 0,\n        }\n        return ret\n\n    def generate_stream_gate(self, params):\n        self.call_ct += 1\n\n        prompt = params[\"prompt\"]\n        gen_kwargs = get_gen_kwargs(params, seed=self.seed)\n        stop = gen_kwargs[\"stop_sequences\"]\n        if \"falcon\" in self.model_path and \"chat\" in self.model_path:\n            stop.extend([\"\\nUser:\", \"<|endoftext|>\", \" User:\", \"###\"])\n            stop = list(set(stop))\n            gen_kwargs[\"stop_sequences\"] = stop\n\n        logger.info(f\"prompt: {prompt}\")\n        logger.info(f\"gen_kwargs: {gen_kwargs}\")\n\n        try:\n            if self.model_path == \"\":\n                url = f\"{self.api_base}\"\n            else:\n                url = f\"{self.api_base}/{self.model_path}\"\n            client = InferenceClient(url, token=self.token)\n            res = client.text_generation(\n                prompt, stream=True, details=True, **gen_kwargs\n            )\n\n            reason = None\n            text = \"\"\n            for chunk in res:\n                if chunk.token.special:\n                    continue\n                text += chunk.token.text\n\n                s = next((x for x in stop if text.endswith(x)), None)\n                if s is not None:\n                    text = text[: -len(s)]\n                    reason = \"stop\"\n                    break\n                if could_be_stop(text, stop):\n                    continue\n                if (\n                    chunk.details is not None\n                    and chunk.details.finish_reason is not None\n                ):\n                    reason = chunk.details.finish_reason\n                if reason not in [\"stop\", \"length\"]:\n                    reason = None\n                ret = {\n                    \"text\": text,\n                    \"error_code\": 0,\n                    \"finish_reason\": reason,\n                }\n                yield json.dumps(ret).encode() + b\"\\0\"\n        except Exception as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.INTERNAL_ERROR,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n\n    def generate_gate(self, params):\n        for x in self.generate_stream_gate(params):\n            pass\n        return json.loads(x[:-1].decode())\n\n    def get_embeddings(self, params):\n        raise NotImplementedError()\n\n\ndef release_worker_semaphore(worker):\n    worker.semaphore.release()\n\n\ndef acquire_worker_semaphore(worker):\n    if worker.semaphore is None:\n        worker.semaphore = asyncio.Semaphore(worker.limit_worker_concurrency)\n    return worker.semaphore.acquire()\n\n\ndef create_background_tasks(worker):\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(lambda: release_worker_semaphore(worker))\n    return background_tasks\n\n\n@app.post(\"/worker_generate_stream\")\nasync def api_generate_stream(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    await acquire_worker_semaphore(worker)\n    generator = worker.generate_stream_gate(params)\n    background_tasks = create_background_tasks(worker)\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate\")\nasync def api_generate(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    await acquire_worker_semaphore(worker)\n    output = worker.generate_gate(params)\n    release_worker_semaphore(worker)\n    return JSONResponse(output)\n\n\n@app.post(\"/worker_get_embeddings\")\nasync def api_get_embeddings(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    await acquire_worker_semaphore(worker)\n    embedding = worker.get_embeddings(params)\n    release_worker_semaphore(worker)\n    return JSONResponse(content=embedding)\n\n\n@app.post(\"/worker_get_status\")\nasync def api_get_status(request: Request):\n    return {\n        \"model_names\": [m for w in workers for m in w.model_names],\n        \"speed\": 1,\n        \"queue_length\": sum([w.get_queue_length() for w in workers]),\n    }\n\n\n@app.post(\"/count_token\")\nasync def api_count_token(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    return worker.count_token(params)\n\n\n@app.post(\"/worker_get_conv_template\")\nasync def api_get_conv(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    return worker.get_conv_template()\n\n\n@app.post(\"/model_details\")\nasync def api_model_details(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    return {\"context_length\": worker.context_len}\n\n\ndef create_huggingface_api_worker():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    # all model-related parameters are listed in --model-info-file\n    parser.add_argument(\n        \"--model-info-file\",\n        type=str,\n        required=True,\n        help=\"Huggingface API model's info file path\",\n    )\n\n    parser.add_argument(\n        \"--limit-worker-concurrency\",\n        type=int,\n        default=5,\n        help=\"Limit the model concurrency to prevent OOM.\",\n    )\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=None,\n        help=\"Overwrite the random seed for each generation.\",\n    )\n    parser.add_argument(\n        \"--ssl\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"Enable SSL. Requires OS Environment variables 'SSL_KEYFILE' and 'SSL_CERTFILE'.\",\n    )\n    args = parser.parse_args()\n\n    with open(args.model_info_file, \"r\", encoding=\"UTF-8\") as f:\n        model_info = json.load(f)\n\n    logger.info(f\"args: {args}\")\n\n    model_path_list = []\n    api_base_list = []\n    token_list = []\n    context_length_list = []\n    model_names_list = []\n    conv_template_list = []\n\n    for m in model_info:\n        model_path_list.append(model_info[m][\"model_path\"])\n        api_base_list.append(model_info[m][\"api_base\"])\n        token_list.append(model_info[m][\"token\"])\n\n        context_length = model_info[m][\"context_length\"]\n        model_names = model_info[m].get(\"model_names\", [m.split(\"/\")[-1]])\n        if isinstance(model_names, str):\n            model_names = [model_names]\n        conv_template = model_info[m].get(\"conv_template\", None)\n\n        context_length_list.append(context_length)\n        model_names_list.append(model_names)\n        conv_template_list.append(conv_template)\n\n    logger.info(f\"Model paths: {model_path_list}\")\n    logger.info(f\"API bases: {api_base_list}\")\n    logger.info(f\"Tokens: {token_list}\")\n    logger.info(f\"Context lengths: {context_length_list}\")\n    logger.info(f\"Model names: {model_names_list}\")\n    logger.info(f\"Conv templates: {conv_template_list}\")\n\n    for (\n        model_names,\n        conv_template,\n        model_path,\n        api_base,\n        token,\n        context_length,\n    ) in zip(\n        model_names_list,\n        conv_template_list,\n        model_path_list,\n        api_base_list,\n        token_list,\n        context_length_list,\n    ):\n        m = HuggingfaceApiWorker(\n            args.controller_address,\n            args.worker_address,\n            worker_id,\n            model_path,\n            api_base,\n            token,\n            context_length,\n            model_names,\n            args.limit_worker_concurrency,\n            no_register=args.no_register,\n            conv_template=conv_template,\n            seed=args.seed,\n        )\n        workers.append(m)\n        for name in model_names:\n            worker_map[name] = m\n\n    # register all the models\n    url = args.controller_address + \"/register_worker\"\n    data = {\n        \"worker_name\": workers[0].worker_addr,\n        \"check_heart_beat\": not args.no_register,\n        \"worker_status\": {\n            \"model_names\": [m for w in workers for m in w.model_names],\n            \"speed\": 1,\n            \"queue_length\": sum([w.get_queue_length() for w in workers]),\n        },\n    }\n    r = requests.post(url, json=data)\n    assert r.status_code == 200\n\n    return args, workers\n\n\nif __name__ == \"__main__\":\n    args, workers = create_huggingface_api_worker()\n    if args.ssl:\n        uvicorn.run(\n            app,\n            host=args.host,\n            port=args.port,\n            log_level=\"info\",\n            ssl_keyfile=os.environ[\"SSL_KEYFILE\"],\n            ssl_certfile=os.environ[\"SSL_CERTFILE\"],\n        )\n    else:\n        uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n", "fastchat/serve/gradio_block_arena_vision_anony.py": "\"\"\"\nChatbot Arena (battle) tab.\nUsers chat with two anonymous models.\n\"\"\"\n\nimport json\nimport time\n\nimport gradio as gr\nimport numpy as np\n\nfrom fastchat.constants import (\n    TEXT_MODERATION_MSG,\n    IMAGE_MODERATION_MSG,\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    SLOW_MODEL_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_TURN_LIMIT,\n)\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastchat.serve.gradio_block_arena_named import flash_buttons\nfrom fastchat.serve.gradio_web_server import (\n    State,\n    bot_response,\n    get_conv_log_filename,\n    no_change_btn,\n    enable_btn,\n    disable_btn,\n    invisible_btn,\n    acknowledgment_md,\n    get_ip,\n    get_model_description_md,\n    _prepare_text_with_image,\n)\nfrom fastchat.serve.gradio_block_arena_anony import (\n    flash_buttons,\n    vote_last_response,\n    leftvote_last_response,\n    rightvote_last_response,\n    tievote_last_response,\n    bothbad_vote_last_response,\n    regenerate,\n    clear_history,\n    share_click,\n    add_text,\n    bot_response_multi,\n    set_global_vars_anony,\n    load_demo_side_by_side_anony,\n    get_sample_weight,\n    get_battle_pair,\n)\nfrom fastchat.serve.gradio_block_arena_vision import (\n    get_vqa_sample,\n    set_invisible_image,\n    set_visible_image,\n    add_image,\n    moderate_input,\n)\nfrom fastchat.serve.remote_logger import get_remote_logger\nfrom fastchat.utils import (\n    build_logger,\n    moderation_filter,\n    image_moderation_filter,\n)\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\nnum_sides = 2\nenable_moderation = False\nanony_names = [\"\", \"\"]\nmodels = []\n\n# TODO(chris): fix sampling weights\nSAMPLING_WEIGHTS = {\n    # tier 0\n    \"gpt-4o\": 4,\n    \"gpt-4-turbo\": 4,\n    \"gemini-1.5-flash\": 4,\n    \"gemini-1.5-pro\": 4,\n    \"claude-3-opus-20240229\": 4,\n    \"claude-3-haiku-20240307\": 4,\n    \"claude-3-sonnet-20240229\": 4,\n    \"llava-v1.6-34b\": 4,\n    \"llava-v1.6-13b\": 4,\n    \"llava-v1.6-7b\": 4,\n    \"reka-flash-20240226\": 4,\n}\n\n# TODO(chris): Find battle targets that make sense\nBATTLE_TARGETS = {\n    # \"gpt-4-turbo\": {\n    #     \"gemini-1.5-pro-preview-0409\",\n    #     \"claude-3-opus-20240229\",\n    #     \"reka-flash-20240226\",\n    # },\n    # \"gemini-1.5-pro-preview-0409\": {\n    #     \"gpt-4-turbo\",\n    #     \"gemini-1.0-pro-vision\",\n    #     \"reka-flash-20240226\",\n    # },\n    # \"gemini-1.0-pro-vision\": {\n    #     \"gpt-4-turbo\",\n    #     \"gemini-1.5-pro-preview-0409\",\n    # },\n    # \"claude-3-opus-20240229\": {\n    #     \"gpt-4-turbo\",\n    #     \"gemini-1.5-pro-preview-0409\",\n    #     \"reka-flash-20240226\",\n    # },\n    # \"claude-3-sonnet-20240229\": {\n    #     \"claude-3-opus-20240229\",\n    #     \"gpt-4-turbo\",\n    #     \"gemini-1.0-pro-vision\",\n    #     \"gemini-1.5-pro-preview-0409\",\n    # },\n    # \"claude-3-haiku-20240307\": {\n    #     \"claude-3-opus-20240229\",\n    #     \"gpt-4-turbo\",\n    #     \"gemini-1.0-pro-vision\",\n    #     \"gemini-1.5-pro-preview-0409\",\n    # },\n    # \"llava-v1.6-34b\": {\n    #     \"gpt-4-turbo\",\n    #     \"gemini-1.5-pro-preview-0409\",\n    #     \"claude-3-opus-20240229\",\n    #     \"claude-3-sonnet-20240229\",\n    #     \"claude-3-haiku-20240307\",\n    # },\n    # \"llava-v1.6-13b\": {\"llava-v1.6-7b\", \"llava-v1.6-34b\", \"gemini-1.0-pro-vision\"},\n    # \"llava-v1.6-7b\": {\"llava-v1.6-13b\", \"gemini-1.0-pro-vision\"},\n    # \"reka-flash-20240226\": {\n    #     \"gemini-1.0-pro-vision\",\n    #     \"claude-3-haiku-20240307\",\n    #     \"claude-3-sonnet-20240229\",\n    # },\n}\n\n# TODO(chris): Fill out models that require sampling boost\nSAMPLING_BOOST_MODELS = []\n\n# outage models won't be sampled.\nOUTAGE_MODELS = []\n\n\ndef load_demo_side_by_side_vision_anony(models_, url_params):\n    global models\n    models = models_\n\n    states = (None,) * num_sides\n    selector_updates = (\n        gr.Markdown(visible=True),\n        gr.Markdown(visible=True),\n    )\n\n    return states + selector_updates\n\n\ndef clear_history_example(request: gr.Request):\n    logger.info(f\"clear_history_example (anony). ip: {get_ip(request)}\")\n    return (\n        [None] * num_sides\n        + [None] * num_sides\n        + anony_names\n        + [invisible_btn] * 4\n        + [disable_btn] * 2\n    )\n\n\ndef vote_last_response(states, vote_type, model_selectors, request: gr.Request):\n    filename = get_conv_log_filename(states[0].is_vision, states[0].has_csam_image)\n\n    with open(filename, \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"models\": [x for x in model_selectors],\n            \"states\": [x.dict() for x in states],\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n    get_remote_logger().log(data)\n\n    if \":\" not in model_selectors[0]:\n        for i in range(5):\n            names = (\n                \"### Model A: \" + states[0].model_name,\n                \"### Model B: \" + states[1].model_name,\n            )\n            yield names + (None,) + (disable_btn,) * 4\n            time.sleep(0.1)\n    else:\n        names = (\n            \"### Model A: \" + states[0].model_name,\n            \"### Model B: \" + states[1].model_name,\n        )\n        yield names + (None,) + (disable_btn,) * 4\n\n\ndef leftvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"leftvote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"leftvote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef rightvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"rightvote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"rightvote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef tievote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"tievote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"tievote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef bothbad_vote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"bothbad_vote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"bothbad_vote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef regenerate(state0, state1, request: gr.Request):\n    logger.info(f\"regenerate (anony). ip: {get_ip(request)}\")\n    states = [state0, state1]\n    if state0.regen_support and state1.regen_support:\n        for i in range(num_sides):\n            states[i].conv.update_last_message(None)\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [None]\n            + [disable_btn] * 6\n        )\n    states[0].skip_next = True\n    states[1].skip_next = True\n    return (\n        states + [x.to_gradio_chatbot() for x in states] + [None] + [no_change_btn] * 6\n    )\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history (anony). ip: {get_ip(request)}\")\n    return (\n        [None] * num_sides\n        + [None] * num_sides\n        + anony_names\n        + [None]\n        + [invisible_btn] * 4\n        + [disable_btn] * 2\n        + [\"\"]\n    )\n\n\ndef add_text(\n    state0, state1, model_selector0, model_selector1, chat_input, request: gr.Request\n):\n    text, images = chat_input[\"text\"], chat_input[\"files\"]\n    ip = get_ip(request)\n    logger.info(f\"add_text (anony). ip: {ip}. len: {len(text)}\")\n    states = [state0, state1]\n    model_selectors = [model_selector0, model_selector1]\n\n    # Init states if necessary\n    if states[0] is None:\n        assert states[1] is None\n\n        model_left, model_right = get_battle_pair(\n            models,\n            BATTLE_TARGETS,\n            OUTAGE_MODELS,\n            SAMPLING_WEIGHTS,\n            SAMPLING_BOOST_MODELS,\n        )\n        states = [\n            State(model_left, is_vision=True),\n            State(model_right, is_vision=True),\n        ]\n\n    if len(text) <= 0:\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [None]\n            + [\n                no_change_btn,\n            ]\n            * 6\n            + [\"\"]\n        )\n\n    model_list = [states[i].model_name for i in range(num_sides)]\n    text, image_flagged, csam_flag = moderate_input(text, text, model_list, images, ip)\n\n    conv = states[0].conv\n    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:\n        logger.info(f\"conversation turn limit. ip: {get_ip(request)}. text: {text}\")\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [{\"text\": CONVERSATION_LIMIT_MSG}]\n            + [\n                no_change_btn,\n            ]\n            * 6\n            + [\"\"]\n        )\n\n    if image_flagged:\n        logger.info(f\"image flagged. ip: {ip}. text: {text}\")\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [{\"text\": IMAGE_MODERATION_MSG}]\n            + [\n                no_change_btn,\n            ]\n            * 6\n            + [\"\"]\n        )\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    for i in range(num_sides):\n        post_processed_text = _prepare_text_with_image(\n            states[i], text, images, csam_flag=csam_flag\n        )\n        states[i].conv.append_message(states[i].conv.roles[0], post_processed_text)\n        states[i].conv.append_message(states[i].conv.roles[1], None)\n        states[i].skip_next = False\n\n    hint_msg = \"\"\n    for i in range(num_sides):\n        if \"deluxe\" in states[i].model_name:\n            hint_msg = SLOW_MODEL_MSG\n    return (\n        states\n        + [x.to_gradio_chatbot() for x in states]\n        + [None]\n        + [\n            disable_btn,\n        ]\n        * 6\n        + [hint_msg]\n    )\n\n\ndef build_side_by_side_vision_ui_anony(models, random_questions=None):\n    notice_markdown = \"\"\"\n# \u2694\ufe0f  Vision Arena \u2694\ufe0f: Benchmarking VLMs in the Wild\n| [Blog](https://lmsys.org/blog/2023-05-03-arena/) | [GitHub](https://github.com/lm-sys/FastChat) | [Paper](https://arxiv.org/abs/2306.05685) | [Dataset](https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md) | [Twitter](https://twitter.com/lmsysorg) | [Discord](https://discord.gg/HSWAKCrnFx) |\n\n## \ud83d\udcdc Rules\n- Ask any question to two anonymous models (e.g., Claude, Gemini, GPT-4-V) and vote for the better one!\n- You can continue chatting until you identify a winner.\n- Vote won't be counted if model identity is revealed during conversation.\n- You can only chat with <span style='color: #DE3163; font-weight: bold'>one image per conversation</span>. You can upload images less than 15MB. Click the \"Random Example\" button to chat with a random image.\n\n**\u2757\ufe0f For research purposes, we log user prompts and images, and may release this data to the public in the future. Please do not upload any confidential or personal information.**\n\n## \ud83d\udc47 Chat now!\n\"\"\"\n\n    states = [gr.State() for _ in range(num_sides)]\n    model_selectors = [None] * num_sides\n    chatbots = [None] * num_sides\n\n    gr.Markdown(notice_markdown, elem_id=\"notice_markdown\")\n\n    with gr.Row():\n        with gr.Column(scale=2, visible=False) as image_column:\n            imagebox = gr.Image(\n                type=\"pil\",\n                show_label=False,\n                interactive=False,\n            )\n\n        with gr.Column(scale=5):\n            with gr.Group(elem_id=\"share-region-anony\"):\n                with gr.Accordion(\n                    f\"\ud83d\udd0d Expand to see the descriptions of {len(models)} models\",\n                    open=False,\n                ):\n                    model_description_md = get_model_description_md(models)\n                    gr.Markdown(\n                        model_description_md, elem_id=\"model_description_markdown\"\n                    )\n\n                with gr.Row():\n                    for i in range(num_sides):\n                        label = \"Model A\" if i == 0 else \"Model B\"\n                        with gr.Column():\n                            chatbots[i] = gr.Chatbot(\n                                label=label,\n                                elem_id=\"chatbot\",\n                                height=550,\n                                show_copy_button=True,\n                            )\n\n                with gr.Row():\n                    for i in range(num_sides):\n                        with gr.Column():\n                            model_selectors[i] = gr.Markdown(\n                                anony_names[i], elem_id=\"model_selector_md\"\n                            )\n    with gr.Row():\n        slow_warning = gr.Markdown(\"\", elem_id=\"notice_markdown\")\n\n    with gr.Row():\n        leftvote_btn = gr.Button(\n            value=\"\ud83d\udc48  A is better\", visible=False, interactive=False\n        )\n        rightvote_btn = gr.Button(\n            value=\"\ud83d\udc49  B is better\", visible=False, interactive=False\n        )\n        tie_btn = gr.Button(value=\"\ud83e\udd1d  Tie\", visible=False, interactive=False)\n        bothbad_btn = gr.Button(\n            value=\"\ud83d\udc4e  Both are bad\", visible=False, interactive=False\n        )\n\n    with gr.Row():\n        textbox = gr.MultimodalTextbox(\n            file_types=[\"image\"],\n            show_label=False,\n            container=True,\n            placeholder=\"Click add or drop your image here\",\n            elem_id=\"input_box\",\n        )\n        # send_btn = gr.Button(value=\"Send\", variant=\"primary\", scale=0)\n\n    with gr.Row() as button_row:\n        if random_questions:\n            global vqa_samples\n            with open(random_questions, \"r\") as f:\n                vqa_samples = json.load(f)\n            random_btn = gr.Button(value=\"\ud83c\udfb2 Random Example\", interactive=True)\n        clear_btn = gr.Button(value=\"\ud83c\udfb2 New Round\", interactive=False)\n        regenerate_btn = gr.Button(value=\"\ud83d\udd04  Regenerate\", interactive=False)\n        share_btn = gr.Button(value=\"\ud83d\udcf7  Share\")\n\n    with gr.Accordion(\"Parameters\", open=False) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=2048,\n            value=1024,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    gr.Markdown(acknowledgment_md, elem_id=\"ack_markdown\")\n\n    # Register listeners\n    btn_list = [\n        leftvote_btn,\n        rightvote_btn,\n        tie_btn,\n        bothbad_btn,\n        regenerate_btn,\n        clear_btn,\n    ]\n    leftvote_btn.click(\n        leftvote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    rightvote_btn.click(\n        rightvote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    tie_btn.click(\n        tievote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    bothbad_btn.click(\n        bothbad_vote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    regenerate_btn.click(\n        regenerate, states, states + chatbots + [textbox] + btn_list\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n    clear_btn.click(\n        clear_history,\n        None,\n        states + chatbots + model_selectors + [textbox] + btn_list + [slow_warning],\n    )\n\n    share_js = \"\"\"\nfunction (a, b, c, d) {\n    const captureElement = document.querySelector('#share-region-anony');\n    html2canvas(captureElement)\n        .then(canvas => {\n            canvas.style.display = 'none'\n            document.body.appendChild(canvas)\n            return canvas\n        })\n        .then(canvas => {\n            const image = canvas.toDataURL('image/png')\n            const a = document.createElement('a')\n            a.setAttribute('download', 'chatbot-arena.png')\n            a.setAttribute('href', image)\n            a.click()\n            canvas.remove()\n        });\n    return [a, b, c, d];\n}\n\"\"\"\n    share_btn.click(share_click, states + model_selectors, [], js=share_js)\n\n    textbox.input(add_image, [textbox], [imagebox]).then(\n        set_visible_image, [textbox], [image_column]\n    ).then(clear_history_example, None, states + chatbots + model_selectors + btn_list)\n\n    textbox.submit(\n        add_text,\n        states + model_selectors + [textbox],\n        states + chatbots + [textbox] + btn_list + [slow_warning],\n    ).then(set_invisible_image, [], [image_column]).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons,\n        [],\n        btn_list,\n    )\n\n    if random_questions:\n        random_btn.click(\n            get_vqa_sample,  # First, get the VQA sample\n            [],  # Pass the path to the VQA samples\n            [textbox, imagebox],  # Outputs are textbox and imagebox\n        ).then(set_visible_image, [textbox], [image_column]).then(\n            clear_history_example, None, states + chatbots + model_selectors + btn_list\n        )\n\n    return states + model_selectors\n", "fastchat/serve/gradio_block_arena_named.py": "\"\"\"\nChatbot Arena (side-by-side) tab.\nUsers chat with two chosen models.\n\"\"\"\n\nimport json\nimport time\n\nimport gradio as gr\nimport numpy as np\n\nfrom fastchat.constants import (\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_TURN_LIMIT,\n)\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastchat.serve.gradio_web_server import (\n    State,\n    bot_response,\n    get_conv_log_filename,\n    no_change_btn,\n    enable_btn,\n    disable_btn,\n    invisible_btn,\n    acknowledgment_md,\n    get_ip,\n    _prepare_text_with_image,\n    get_model_description_md,\n)\nfrom fastchat.serve.remote_logger import get_remote_logger\nfrom fastchat.utils import (\n    build_logger,\n    moderation_filter,\n)\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\nnum_sides = 2\nenable_moderation = False\n\n\ndef set_global_vars_named(enable_moderation_):\n    global enable_moderation\n    enable_moderation = enable_moderation_\n\n\ndef load_demo_side_by_side_named(models, url_params):\n    states = (None,) * num_sides\n\n    model_left = models[0] if len(models) > 0 else \"\"\n    if len(models) > 1:\n        weights = ([8] * 4 + [4] * 8 + [1] * 32)[: len(models) - 1]\n        weights = weights / np.sum(weights)\n        model_right = np.random.choice(models[1:], p=weights)\n    else:\n        model_right = model_left\n\n    selector_updates = (\n        gr.Dropdown(choices=models, value=model_left, visible=True),\n        gr.Dropdown(choices=models, value=model_right, visible=True),\n    )\n\n    return states + selector_updates\n\n\ndef vote_last_response(states, vote_type, model_selectors, request: gr.Request):\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"models\": [x for x in model_selectors],\n            \"states\": [x.dict() for x in states],\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n    get_remote_logger().log(data)\n\n\ndef leftvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"leftvote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"leftvote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef rightvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"rightvote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"rightvote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef tievote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"tievote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"tievote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef bothbad_vote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"bothbad_vote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"bothbad_vote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef regenerate(state0, state1, request: gr.Request):\n    logger.info(f\"regenerate (named). ip: {get_ip(request)}\")\n    states = [state0, state1]\n    if state0.regen_support and state1.regen_support:\n        for i in range(num_sides):\n            states[i].conv.update_last_message(None)\n        return (\n            states + [x.to_gradio_chatbot() for x in states] + [\"\"] + [disable_btn] * 6\n        )\n    states[0].skip_next = True\n    states[1].skip_next = True\n    return states + [x.to_gradio_chatbot() for x in states] + [\"\"] + [no_change_btn] * 6\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history (named). ip: {get_ip(request)}\")\n    return (\n        [None] * num_sides\n        + [None] * num_sides\n        + [\"\"]\n        + [invisible_btn] * 4\n        + [disable_btn] * 2\n    )\n\n\ndef share_click(state0, state1, model_selector0, model_selector1, request: gr.Request):\n    logger.info(f\"share (named). ip: {get_ip(request)}\")\n    if state0 is not None and state1 is not None:\n        vote_last_response(\n            [state0, state1], \"share\", [model_selector0, model_selector1], request\n        )\n\n\ndef add_text(\n    state0, state1, model_selector0, model_selector1, text, image, request: gr.Request\n):\n    ip = get_ip(request)\n    logger.info(f\"add_text (named). ip: {ip}. len: {len(text)}\")\n    states = [state0, state1]\n    model_selectors = [model_selector0, model_selector1]\n\n    # Init states if necessary\n    for i in range(num_sides):\n        if states[i] is None:\n            states[i] = State(model_selectors[i])\n\n    if len(text) <= 0:\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [\"\", None]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    model_list = [states[i].model_name for i in range(num_sides)]\n    all_conv_text_left = states[0].conv.get_prompt()\n    all_conv_text_right = states[1].conv.get_prompt()\n    all_conv_text = (\n        all_conv_text_left[-1000:] + all_conv_text_right[-1000:] + \"\\nuser: \" + text\n    )\n    flagged = moderation_filter(all_conv_text, model_list)\n    if flagged:\n        logger.info(f\"violate moderation (named). ip: {ip}. text: {text}\")\n        # overwrite the original text\n        text = MODERATION_MSG\n\n    conv = states[0].conv\n    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:\n        logger.info(f\"conversation turn limit. ip: {ip}. text: {text}\")\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [CONVERSATION_LIMIT_MSG, None]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    for i in range(num_sides):\n        post_processed_text = _prepare_text_with_image(\n            states[i], text, image, csam_flag=False\n        )\n        states[i].conv.append_message(states[i].conv.roles[0], post_processed_text)\n        states[i].conv.append_message(states[i].conv.roles[1], None)\n        states[i].skip_next = False\n\n    return (\n        states\n        + [x.to_gradio_chatbot() for x in states]\n        + [\"\", None]\n        + [\n            disable_btn,\n        ]\n        * 6\n    )\n\n\ndef bot_response_multi(\n    state0,\n    state1,\n    temperature,\n    top_p,\n    max_new_tokens,\n    request: gr.Request,\n):\n    logger.info(f\"bot_response_multi (named). ip: {get_ip(request)}\")\n\n    if state0.skip_next:\n        # This generate call is skipped due to invalid inputs\n        yield (\n            state0,\n            state1,\n            state0.to_gradio_chatbot(),\n            state1.to_gradio_chatbot(),\n        ) + (no_change_btn,) * 6\n        return\n\n    states = [state0, state1]\n    gen = []\n    for i in range(num_sides):\n        gen.append(\n            bot_response(\n                states[i],\n                temperature,\n                top_p,\n                max_new_tokens,\n                request,\n            )\n        )\n\n    is_stream_batch = []\n    for i in range(num_sides):\n        is_stream_batch.append(\n            states[i].model_name\n            in [\n                \"gemini-pro\",\n                \"gemini-pro-dev-api\",\n                \"gemma-1.1-2b-it\",\n                \"gemma-1.1-7b-it\",\n            ]\n        )\n\n    chatbots = [None] * num_sides\n    iters = 0\n    while True:\n        stop = True\n        iters += 1\n        for i in range(num_sides):\n            try:\n                # yield gemini fewer times as its chunk size is larger\n                # otherwise, gemini will stream too fast\n                if not is_stream_batch[i] or (iters % 30 == 1 or iters < 3):\n                    ret = next(gen[i])\n                    states[i], chatbots[i] = ret[0], ret[1]\n                stop = False\n            except StopIteration:\n                pass\n        yield states + chatbots + [disable_btn] * 6\n        if stop:\n            break\n\n\ndef flash_buttons():\n    btn_updates = [\n        [disable_btn] * 4 + [enable_btn] * 2,\n        [enable_btn] * 6,\n    ]\n    for i in range(4):\n        yield btn_updates[i % 2]\n        time.sleep(0.3)\n\n\ndef build_side_by_side_ui_named(models):\n    notice_markdown = \"\"\"\n# \u2694\ufe0f  Chatbot Arena: Benchmarking LLMs in the Wild\n- | [Blog](https://lmsys.org/blog/2023-05-03-arena/) | [GitHub](https://github.com/lm-sys/FastChat) | [Paper](https://arxiv.org/abs/2306.05685) | [Dataset](https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md) | [Twitter](https://twitter.com/lmsysorg) | [Discord](https://discord.gg/HSWAKCrnFx) |\n\n## \ud83d\udcdc Rules\n- Chat with any two models side-by-side and vote!\n- You can continue chatting for multiple rounds.\n- Click \"Clear history\" to start a new round.\n\n## \ud83e\udd16 Choose two models to compare\n\"\"\"\n\n    states = [gr.State() for _ in range(num_sides)]\n    model_selectors = [None] * num_sides\n    chatbots = [None] * num_sides\n\n    notice = gr.Markdown(notice_markdown, elem_id=\"notice_markdown\")\n\n    with gr.Group(elem_id=\"share-region-named\"):\n        with gr.Row():\n            for i in range(num_sides):\n                with gr.Column():\n                    model_selectors[i] = gr.Dropdown(\n                        choices=models,\n                        value=models[i] if len(models) > i else \"\",\n                        interactive=True,\n                        show_label=False,\n                        container=False,\n                    )\n        with gr.Row():\n            with gr.Accordion(\n                f\"\ud83d\udd0d Expand to see the descriptions of {len(models)} models\", open=False\n            ):\n                model_description_md = get_model_description_md(models)\n                gr.Markdown(model_description_md, elem_id=\"model_description_markdown\")\n\n        with gr.Row():\n            for i in range(num_sides):\n                label = \"Model A\" if i == 0 else \"Model B\"\n                with gr.Column():\n                    chatbots[i] = gr.Chatbot(\n                        label=label,\n                        elem_id=f\"chatbot\",\n                        height=550,\n                        show_copy_button=True,\n                    )\n\n    with gr.Row():\n        leftvote_btn = gr.Button(\n            value=\"\ud83d\udc48  A is better\", visible=False, interactive=False\n        )\n        rightvote_btn = gr.Button(\n            value=\"\ud83d\udc49  B is better\", visible=False, interactive=False\n        )\n        tie_btn = gr.Button(value=\"\ud83e\udd1d  Tie\", visible=False, interactive=False)\n        bothbad_btn = gr.Button(\n            value=\"\ud83d\udc4e  Both are bad\", visible=False, interactive=False\n        )\n\n    with gr.Row():\n        textbox = gr.Textbox(\n            show_label=False,\n            placeholder=\"\ud83d\udc49 Enter your prompt and press ENTER\",\n            elem_id=\"input_box\",\n        )\n        send_btn = gr.Button(value=\"Send\", variant=\"primary\", scale=0)\n\n    with gr.Row() as button_row:\n        clear_btn = gr.Button(value=\"\ud83d\uddd1\ufe0f  Clear history\", interactive=False)\n        regenerate_btn = gr.Button(value=\"\ud83d\udd04  Regenerate\", interactive=False)\n        share_btn = gr.Button(value=\"\ud83d\udcf7  Share\")\n\n    with gr.Accordion(\"Parameters\", open=False) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=2048,\n            value=1024,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    gr.Markdown(acknowledgment_md, elem_id=\"ack_markdown\")\n\n    # Register listeners\n    imagebox = gr.State(None)\n    btn_list = [\n        leftvote_btn,\n        rightvote_btn,\n        tie_btn,\n        bothbad_btn,\n        regenerate_btn,\n        clear_btn,\n    ]\n    leftvote_btn.click(\n        leftvote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    rightvote_btn.click(\n        rightvote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    tie_btn.click(\n        tievote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    bothbad_btn.click(\n        bothbad_vote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    regenerate_btn.click(\n        regenerate, states, states + chatbots + [textbox] + btn_list\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n    clear_btn.click(clear_history, None, states + chatbots + [textbox] + btn_list)\n\n    share_js = \"\"\"\nfunction (a, b, c, d) {\n    const captureElement = document.querySelector('#share-region-named');\n    html2canvas(captureElement)\n        .then(canvas => {\n            canvas.style.display = 'none'\n            document.body.appendChild(canvas)\n            return canvas\n        })\n        .then(canvas => {\n            const image = canvas.toDataURL('image/png')\n            const a = document.createElement('a')\n            a.setAttribute('download', 'chatbot-arena.png')\n            a.setAttribute('href', image)\n            a.click()\n            canvas.remove()\n        });\n    return [a, b, c, d];\n}\n\"\"\"\n    share_btn.click(share_click, states + model_selectors, [], js=share_js)\n\n    for i in range(num_sides):\n        model_selectors[i].change(\n            clear_history, None, states + chatbots + [textbox] + btn_list\n        )\n\n    textbox.submit(\n        add_text,\n        states + model_selectors + [textbox, imagebox],\n        states + chatbots + [textbox, imagebox] + btn_list,\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n    send_btn.click(\n        add_text,\n        states + model_selectors + [textbox, imagebox],\n        states + chatbots + [textbox, imagebox] + btn_list,\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n\n    return states + model_selectors\n", "fastchat/serve/gradio_web_server.py": "\"\"\"\nThe gradio demo server for chatting with a single model.\n\"\"\"\n\nimport argparse\nfrom collections import defaultdict\nimport datetime\nimport hashlib\nimport json\nimport os\nimport random\nimport time\nimport uuid\n\nimport gradio as gr\nimport requests\n\nfrom fastchat.constants import (\n    LOGDIR,\n    WORKER_API_TIMEOUT,\n    ErrorCode,\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    RATE_LIMIT_MSG,\n    SERVER_ERROR_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_TURN_LIMIT,\n    SESSION_EXPIRATION_TIME,\n)\nfrom fastchat.model.model_adapter import (\n    get_conversation_template,\n)\nfrom fastchat.model.model_registry import get_model_info, model_info\nfrom fastchat.serve.api_provider import get_api_provider_stream_iter\nfrom fastchat.serve.remote_logger import get_remote_logger\nfrom fastchat.utils import (\n    build_logger,\n    get_window_url_params_js,\n    get_window_url_params_with_tos_js,\n    moderation_filter,\n    parse_gradio_auth_creds,\n    load_image,\n)\n\nlogger = build_logger(\"gradio_web_server\", \"gradio_web_server.log\")\n\nheaders = {\"User-Agent\": \"FastChat Client\"}\n\nno_change_btn = gr.Button()\nenable_btn = gr.Button(interactive=True, visible=True)\ndisable_btn = gr.Button(interactive=False)\ninvisible_btn = gr.Button(interactive=False, visible=False)\n\ncontroller_url = None\nenable_moderation = False\nuse_remote_storage = False\n\nacknowledgment_md = \"\"\"\n### Terms of Service\n\nUsers are required to agree to the following terms before using the service:\n\nThe service is a research preview. It only provides limited safety measures and may generate offensive content.\nIt must not be used for any illegal, harmful, violent, racist, or sexual purposes.\nPlease do not upload any private information.\nThe service collects user dialogue data, including both text and images, and reserves the right to distribute it under a Creative Commons Attribution (CC-BY) or a similar license.\n\n### Acknowledgment\nWe thank [UC Berkeley SkyLab](https://sky.cs.berkeley.edu/), [Kaggle](https://www.kaggle.com/), [MBZUAI](https://mbzuai.ac.ae/), [a16z](https://www.a16z.com/), [Together AI](https://www.together.ai/), [Hyperbolic](https://hyperbolic.xyz/), [Anyscale](https://www.anyscale.com/), [HuggingFace](https://huggingface.co/) for their generous [sponsorship](https://lmsys.org/donations/).\n\n<div class=\"sponsor-image-about\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/skylab.png\" alt=\"SkyLab\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/kaggle.png\" alt=\"Kaggle\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/mbzuai.jpeg\" alt=\"MBZUAI\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/a16z.jpeg\" alt=\"a16z\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/together.png\" alt=\"Together AI\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/hyperbolic_logo.png\" alt=\"Hyperbolic\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/anyscale.png\" alt=\"AnyScale\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/huggingface.png\" alt=\"HuggingFace\">\n</div>\n\"\"\"\n\n# JSON file format of API-based models:\n# {\n#   \"gpt-3.5-turbo\": {\n#     \"model_name\": \"gpt-3.5-turbo\",\n#     \"api_type\": \"openai\",\n#     \"api_base\": \"https://api.openai.com/v1\",\n#     \"api_key\": \"sk-******\",\n#     \"anony_only\": false\n#   }\n# }\n#\n#  - \"api_type\" can be one of the following: openai, anthropic, gemini, or mistral. For custom APIs, add a new type and implement it accordingly.\n#  - \"anony_only\" indicates whether to display this model in anonymous mode only.\n\napi_endpoint_info = {}\n\n\nclass State:\n    def __init__(self, model_name, is_vision=False):\n        self.conv = get_conversation_template(model_name)\n        self.conv_id = uuid.uuid4().hex\n        self.skip_next = False\n        self.model_name = model_name\n        self.oai_thread_id = None\n        self.is_vision = is_vision\n\n        # NOTE(chris): This could be sort of a hack since it assumes the user only uploads one image. If they can upload multiple, we should store a list of image hashes.\n        self.has_csam_image = False\n\n        self.regen_support = True\n        if \"browsing\" in model_name:\n            self.regen_support = False\n        self.init_system_prompt(self.conv)\n\n    def init_system_prompt(self, conv):\n        system_prompt = conv.get_system_message()\n        if len(system_prompt) == 0:\n            return\n        current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n        system_prompt = system_prompt.replace(\"{{currentDateTime}}\", current_date)\n        conv.set_system_message(system_prompt)\n\n    def to_gradio_chatbot(self):\n        return self.conv.to_gradio_chatbot()\n\n    def dict(self):\n        base = self.conv.dict()\n        base.update(\n            {\n                \"conv_id\": self.conv_id,\n                \"model_name\": self.model_name,\n            }\n        )\n\n        if self.is_vision:\n            base.update({\"has_csam_image\": self.has_csam_image})\n        return base\n\n\ndef set_global_vars(controller_url_, enable_moderation_, use_remote_storage_):\n    global controller_url, enable_moderation, use_remote_storage\n    controller_url = controller_url_\n    enable_moderation = enable_moderation_\n    use_remote_storage = use_remote_storage_\n\n\ndef get_conv_log_filename(is_vision=False, has_csam_image=False):\n    t = datetime.datetime.now()\n    conv_log_filename = f\"{t.year}-{t.month:02d}-{t.day:02d}-conv.json\"\n    if is_vision and not has_csam_image:\n        name = os.path.join(LOGDIR, f\"vision-tmp-{conv_log_filename}\")\n    elif is_vision and has_csam_image:\n        name = os.path.join(LOGDIR, f\"vision-csam-{conv_log_filename}\")\n    else:\n        name = os.path.join(LOGDIR, conv_log_filename)\n\n    return name\n\n\ndef get_model_list(controller_url, register_api_endpoint_file, vision_arena):\n    global api_endpoint_info\n\n    # Add models from the controller\n    if controller_url:\n        ret = requests.post(controller_url + \"/refresh_all_workers\")\n        assert ret.status_code == 200\n\n        if vision_arena:\n            ret = requests.post(controller_url + \"/list_multimodal_models\")\n            models = ret.json()[\"models\"]\n        else:\n            ret = requests.post(controller_url + \"/list_language_models\")\n            models = ret.json()[\"models\"]\n    else:\n        models = []\n\n    # Add models from the API providers\n    if register_api_endpoint_file:\n        api_endpoint_info = json.load(open(register_api_endpoint_file))\n        for mdl, mdl_dict in api_endpoint_info.items():\n            mdl_vision = mdl_dict.get(\"vision-arena\", False)\n            mdl_text = mdl_dict.get(\"text-arena\", True)\n            if vision_arena and mdl_vision:\n                models.append(mdl)\n            if not vision_arena and mdl_text:\n                models.append(mdl)\n\n    # Remove anonymous models\n    models = list(set(models))\n    visible_models = models.copy()\n    for mdl in models:\n        if mdl not in api_endpoint_info:\n            continue\n        mdl_dict = api_endpoint_info[mdl]\n        if mdl_dict[\"anony_only\"]:\n            visible_models.remove(mdl)\n\n    # Sort models and add descriptions\n    priority = {k: f\"___{i:03d}\" for i, k in enumerate(model_info)}\n    models.sort(key=lambda x: priority.get(x, x))\n    visible_models.sort(key=lambda x: priority.get(x, x))\n    logger.info(f\"All models: {models}\")\n    logger.info(f\"Visible models: {visible_models}\")\n    return visible_models, models\n\n\ndef load_demo_single(models, url_params):\n    selected_model = models[0] if len(models) > 0 else \"\"\n    if \"model\" in url_params:\n        model = url_params[\"model\"]\n        if model in models:\n            selected_model = model\n\n    dropdown_update = gr.Dropdown(choices=models, value=selected_model, visible=True)\n    state = None\n    return state, dropdown_update\n\n\ndef load_demo(url_params, request: gr.Request):\n    global models\n\n    ip = get_ip(request)\n    logger.info(f\"load_demo. ip: {ip}. params: {url_params}\")\n\n    if args.model_list_mode == \"reload\":\n        models, all_models = get_model_list(\n            controller_url, args.register_api_endpoint_file, vision_arena=False\n        )\n\n    return load_demo_single(models, url_params)\n\n\ndef vote_last_response(state, vote_type, model_selector, request: gr.Request):\n    filename = get_conv_log_filename()\n    if \"llava\" in model_selector:\n        filename = filename.replace(\"2024\", \"vision-tmp-2024\")\n\n    with open(filename, \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"model\": model_selector,\n            \"state\": state.dict(),\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n    get_remote_logger().log(data)\n\n\ndef upvote_last_response(state, model_selector, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"upvote. ip: {ip}\")\n    vote_last_response(state, \"upvote\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef downvote_last_response(state, model_selector, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"downvote. ip: {ip}\")\n    vote_last_response(state, \"downvote\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef flag_last_response(state, model_selector, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"flag. ip: {ip}\")\n    vote_last_response(state, \"flag\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef regenerate(state, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"regenerate. ip: {ip}\")\n    if not state.regen_support:\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), \"\", None) + (no_change_btn,) * 5\n    state.conv.update_last_message(None)\n    return (state, state.to_gradio_chatbot(), \"\", None) + (disable_btn,) * 5\n\n\ndef clear_history(request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"clear_history. ip: {ip}\")\n    state = None\n    return (state, [], \"\", None) + (disable_btn,) * 5\n\n\ndef get_ip(request: gr.Request):\n    if \"cf-connecting-ip\" in request.headers:\n        ip = request.headers[\"cf-connecting-ip\"]\n    elif \"x-forwarded-for\" in request.headers:\n        ip = request.headers[\"x-forwarded-for\"]\n    else:\n        ip = request.client.host\n    return ip\n\n\n# TODO(Chris): At some point, we would like this to be a live-reporting feature.\ndef report_csam_image(state, image):\n    pass\n\n\ndef _prepare_text_with_image(state, text, images, csam_flag):\n    if images is not None and len(images) > 0:\n        image = images[0]\n\n        if len(state.conv.get_images()) > 0:\n            # reset convo with new image\n            state.conv = get_conversation_template(state.model_name)\n\n        image = state.conv.convert_image_to_base64(\n            image\n        )  # PIL type is not JSON serializable\n\n        if csam_flag:\n            state.has_csam_image = True\n            report_csam_image(state, image)\n\n        text = text, [image]\n\n    return text\n\n\ndef add_text(state, model_selector, text, image, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"add_text. ip: {ip}. len: {len(text)}\")\n\n    if state is None:\n        state = State(model_selector)\n\n    if len(text) <= 0:\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), \"\", None) + (no_change_btn,) * 5\n\n    all_conv_text = state.conv.get_prompt()\n    all_conv_text = all_conv_text[-2000:] + \"\\nuser: \" + text\n    flagged = moderation_filter(all_conv_text, [state.model_name])\n    # flagged = moderation_filter(text, [state.model_name])\n    if flagged:\n        logger.info(f\"violate moderation. ip: {ip}. text: {text}\")\n        # overwrite the original text\n        text = MODERATION_MSG\n\n    if (len(state.conv.messages) - state.conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:\n        logger.info(f\"conversation turn limit. ip: {ip}. text: {text}\")\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), CONVERSATION_LIMIT_MSG, None) + (\n            no_change_btn,\n        ) * 5\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    text = _prepare_text_with_image(state, text, image, csam_flag=False)\n    state.conv.append_message(state.conv.roles[0], text)\n    state.conv.append_message(state.conv.roles[1], None)\n    return (state, state.to_gradio_chatbot(), \"\", None) + (disable_btn,) * 5\n\n\ndef model_worker_stream_iter(\n    conv,\n    model_name,\n    worker_addr,\n    prompt,\n    temperature,\n    repetition_penalty,\n    top_p,\n    max_new_tokens,\n    images,\n):\n    # Make requests\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"repetition_penalty\": repetition_penalty,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n        \"stop\": conv.stop_str,\n        \"stop_token_ids\": conv.stop_token_ids,\n        \"echo\": False,\n    }\n\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    if len(images) > 0:\n        gen_params[\"images\"] = images\n\n    # Stream output\n    response = requests.post(\n        worker_addr + \"/worker_generate_stream\",\n        headers=headers,\n        json=gen_params,\n        stream=True,\n        timeout=WORKER_API_TIMEOUT,\n    )\n    for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode())\n            yield data\n\n\ndef is_limit_reached(model_name, ip):\n    monitor_url = \"http://localhost:9090\"\n    try:\n        ret = requests.get(\n            f\"{monitor_url}/is_limit_reached?model={model_name}&user_id={ip}\", timeout=1\n        )\n        obj = ret.json()\n        return obj\n    except Exception as e:\n        logger.info(f\"monitor error: {e}\")\n        return None\n\n\ndef bot_response(\n    state,\n    temperature,\n    top_p,\n    max_new_tokens,\n    request: gr.Request,\n    apply_rate_limit=True,\n    use_recommended_config=False,\n):\n    ip = get_ip(request)\n    logger.info(f\"bot_response. ip: {ip}\")\n    start_tstamp = time.time()\n    temperature = float(temperature)\n    top_p = float(top_p)\n    max_new_tokens = int(max_new_tokens)\n\n    if state.skip_next:\n        # This generate call is skipped due to invalid inputs\n        state.skip_next = False\n        yield (state, state.to_gradio_chatbot()) + (no_change_btn,) * 5\n        return\n\n    if apply_rate_limit:\n        ret = is_limit_reached(state.model_name, ip)\n        if ret is not None and ret[\"is_limit_reached\"]:\n            error_msg = RATE_LIMIT_MSG + \"\\n\\n\" + ret[\"reason\"]\n            logger.info(f\"rate limit reached. ip: {ip}. error_msg: {ret['reason']}\")\n            state.conv.update_last_message(error_msg)\n            yield (state, state.to_gradio_chatbot()) + (no_change_btn,) * 5\n            return\n\n    conv, model_name = state.conv, state.model_name\n    model_api_dict = (\n        api_endpoint_info[model_name] if model_name in api_endpoint_info else None\n    )\n    images = conv.get_images()\n\n    if model_api_dict is None:\n        # Query worker address\n        ret = requests.post(\n            controller_url + \"/get_worker_address\", json={\"model\": model_name}\n        )\n        worker_addr = ret.json()[\"address\"]\n        logger.info(f\"model_name: {model_name}, worker_addr: {worker_addr}\")\n\n        # No available worker\n        if worker_addr == \"\":\n            conv.update_last_message(SERVER_ERROR_MSG)\n            yield (\n                state,\n                state.to_gradio_chatbot(),\n                disable_btn,\n                disable_btn,\n                disable_btn,\n                enable_btn,\n                enable_btn,\n            )\n            return\n\n        # Construct prompt.\n        # We need to call it here, so it will not be affected by \"\u258c\".\n        prompt = conv.get_prompt()\n        # Set repetition_penalty\n        if \"t5\" in model_name:\n            repetition_penalty = 1.2\n        else:\n            repetition_penalty = 1.0\n\n        stream_iter = model_worker_stream_iter(\n            conv,\n            model_name,\n            worker_addr,\n            prompt,\n            temperature,\n            repetition_penalty,\n            top_p,\n            max_new_tokens,\n            images,\n        )\n    else:\n        if use_recommended_config:\n            recommended_config = model_api_dict.get(\"recommended_config\", None)\n            if recommended_config is not None:\n                temperature = recommended_config.get(\"temperature\", temperature)\n                top_p = recommended_config.get(\"top_p\", top_p)\n                max_new_tokens = recommended_config.get(\n                    \"max_new_tokens\", max_new_tokens\n                )\n\n        stream_iter = get_api_provider_stream_iter(\n            conv,\n            model_name,\n            model_api_dict,\n            temperature,\n            top_p,\n            max_new_tokens,\n            state,\n        )\n\n    html_code = ' <span class=\"cursor\"></span> '\n\n    # conv.update_last_message(\"\u258c\")\n    conv.update_last_message(html_code)\n    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5\n\n    try:\n        data = {\"text\": \"\"}\n        for i, data in enumerate(stream_iter):\n            if data[\"error_code\"] == 0:\n                output = data[\"text\"].strip()\n                # conv.update_last_message(output + \"\u258c\")\n                conv.update_last_message(output + html_code)\n                yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5\n            else:\n                output = data[\"text\"] + f\"\\n\\n(error_code: {data['error_code']})\"\n                conv.update_last_message(output)\n                yield (state, state.to_gradio_chatbot()) + (\n                    disable_btn,\n                    disable_btn,\n                    disable_btn,\n                    enable_btn,\n                    enable_btn,\n                )\n                return\n        output = data[\"text\"].strip()\n        conv.update_last_message(output)\n        yield (state, state.to_gradio_chatbot()) + (enable_btn,) * 5\n    except requests.exceptions.RequestException as e:\n        conv.update_last_message(\n            f\"{SERVER_ERROR_MSG}\\n\\n\"\n            f\"(error_code: {ErrorCode.GRADIO_REQUEST_ERROR}, {e})\"\n        )\n        yield (state, state.to_gradio_chatbot()) + (\n            disable_btn,\n            disable_btn,\n            disable_btn,\n            enable_btn,\n            enable_btn,\n        )\n        return\n    except Exception as e:\n        conv.update_last_message(\n            f\"{SERVER_ERROR_MSG}\\n\\n\"\n            f\"(error_code: {ErrorCode.GRADIO_STREAM_UNKNOWN_ERROR}, {e})\"\n        )\n        yield (state, state.to_gradio_chatbot()) + (\n            disable_btn,\n            disable_btn,\n            disable_btn,\n            enable_btn,\n            enable_btn,\n        )\n        return\n\n    finish_tstamp = time.time()\n    logger.info(f\"{output}\")\n\n    conv.save_new_images(\n        has_csam_images=state.has_csam_image, use_remote_storage=use_remote_storage\n    )\n\n    filename = get_conv_log_filename(\n        is_vision=state.is_vision, has_csam_image=state.has_csam_image\n    )\n\n    with open(filename, \"a\") as fout:\n        data = {\n            \"tstamp\": round(finish_tstamp, 4),\n            \"type\": \"chat\",\n            \"model\": model_name,\n            \"gen_params\": {\n                \"temperature\": temperature,\n                \"top_p\": top_p,\n                \"max_new_tokens\": max_new_tokens,\n            },\n            \"start\": round(start_tstamp, 4),\n            \"finish\": round(finish_tstamp, 4),\n            \"state\": state.dict(),\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n    get_remote_logger().log(data)\n\n\nblock_css = \"\"\"\n#notice_markdown .prose {\n    font-size: 110% !important;\n}\n#notice_markdown th {\n    display: none;\n}\n#notice_markdown td {\n    padding-top: 6px;\n    padding-bottom: 6px;\n}\n#arena_leaderboard_dataframe table {\n    font-size: 110%;\n}\n#full_leaderboard_dataframe table {\n    font-size: 110%;\n}\n#model_description_markdown {\n    font-size: 110% !important;\n}\n#leaderboard_markdown .prose {\n    font-size: 110% !important;\n}\n#leaderboard_markdown td {\n    padding-top: 6px;\n    padding-bottom: 6px;\n}\n#leaderboard_dataframe td {\n    line-height: 0.1em;\n}\n#about_markdown .prose {\n    font-size: 110% !important;\n}\n#ack_markdown .prose {\n    font-size: 110% !important;\n}\n#chatbot .prose {\n    font-size: 105% !important;\n}\n.sponsor-image-about img {\n    margin: 0 20px;\n    margin-top: 20px;\n    height: 40px;\n    max-height: 100%;\n    width: auto;\n    float: left;\n}\n\n.chatbot h1, h2, h3 {\n    margin-top: 8px; /* Adjust the value as needed */\n    margin-bottom: 0px; /* Adjust the value as needed */\n    padding-bottom: 0px;\n}\n\n.chatbot h1 {\n    font-size: 130%;\n}\n.chatbot h2 {\n    font-size: 120%;\n}\n.chatbot h3 {\n    font-size: 110%;\n}\n.chatbot p:not(:first-child) {\n    margin-top: 8px;\n}\n\n.typing {\n    display: inline-block;\n}\n\n.cursor {\n    display: inline-block;\n    width: 7px;\n    height: 1em;\n    background-color: black;\n    vertical-align: middle;\n    animation: blink 1s infinite;\n}\n\n.dark .cursor {\n    display: inline-block;\n    width: 7px;\n    height: 1em;\n    background-color: white;\n    vertical-align: middle;\n    animation: blink 1s infinite;\n}\n\n@keyframes blink {\n    0%, 50% { opacity: 1; }\n    50.1%, 100% { opacity: 0; }\n}\n\n.app {\n  max-width: 100% !important;\n  padding: 20px !important;               \n}\n\na {\n    color: #1976D2; /* Your current link color, a shade of blue */\n    text-decoration: none; /* Removes underline from links */\n}\na:hover {\n    color: #63A4FF; /* This can be any color you choose for hover */\n    text-decoration: underline; /* Adds underline on hover */\n}\n\"\"\"\n\n\ndef get_model_description_md(models):\n    model_description_md = \"\"\"\n| | | |\n| ---- | ---- | ---- |\n\"\"\"\n    ct = 0\n    visited = set()\n    for i, name in enumerate(models):\n        minfo = get_model_info(name)\n        if minfo.simple_name in visited:\n            continue\n        visited.add(minfo.simple_name)\n        one_model_md = f\"[{minfo.simple_name}]({minfo.link}): {minfo.description}\"\n\n        if ct % 3 == 0:\n            model_description_md += \"|\"\n        model_description_md += f\" {one_model_md} |\"\n        if ct % 3 == 2:\n            model_description_md += \"\\n\"\n        ct += 1\n    return model_description_md\n\n\ndef build_about():\n    about_markdown = \"\"\"\n# About Us\nChatbot Arena is an open-source research project developed by members from [LMSYS](https://lmsys.org) and UC Berkeley [SkyLab](https://sky.cs.berkeley.edu/). Our mission is to build an open platform to evaluate LLMs by human preference in the real-world.\nWe open-source our [FastChat](https://github.com/lm-sys/FastChat) project at GitHub and release chat and human feedback dataset. We invite everyone to join us!\n\n## Arena Core Team\n- [Lianmin Zheng](https://lmzheng.net/) (co-lead), [Wei-Lin Chiang](https://infwinston.github.io/) (co-lead), [Ying Sheng](https://sites.google.com/view/yingsheng/home), [Joseph E. Gonzalez](https://people.eecs.berkeley.edu/~jegonzal/), [Ion Stoica](http://people.eecs.berkeley.edu/~istoica/)\n\n## Past Members\n- [Siyuan Zhuang](https://scholar.google.com/citations?user=KSZmI5EAAAAJ), [Hao Zhang](https://cseweb.ucsd.edu/~haozhang/)\n\n## Learn more\n- Chatbot Arena [paper](https://arxiv.org/abs/2403.04132), [launch blog](https://lmsys.org/blog/2023-05-03-arena/), [dataset](https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md), [policy](https://lmsys.org/blog/2024-03-01-policy/)\n- LMSYS-Chat-1M dataset [paper](https://arxiv.org/abs/2309.11998), LLM Judge [paper](https://arxiv.org/abs/2306.05685)\n\n## Contact Us\n- Follow our [X](https://x.com/lmsysorg), [Discord](https://discord.gg/HSWAKCrnFx) or email us at lmsys.org@gmail.com\n- File issues on [GitHub](https://github.com/lm-sys/FastChat)\n- Download our datasets and models on [HuggingFace](https://huggingface.co/lmsys)\n\n## Acknowledgment\nWe thank [SkyPilot](https://github.com/skypilot-org/skypilot) and [Gradio](https://github.com/gradio-app/gradio) team for their system support.\nWe also thank [UC Berkeley SkyLab](https://sky.cs.berkeley.edu/), [Kaggle](https://www.kaggle.com/), [MBZUAI](https://mbzuai.ac.ae/), [a16z](https://www.a16z.com/), [Together AI](https://www.together.ai/), [Hyperbolic](https://hyperbolic.xyz/), [Anyscale](https://www.anyscale.com/), [HuggingFace](https://huggingface.co/) for their generous sponsorship. Learn more about partnership [here](https://lmsys.org/donations/).\n\n<div class=\"sponsor-image-about\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/skylab.png\" alt=\"SkyLab\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/kaggle.png\" alt=\"Kaggle\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/mbzuai.jpeg\" alt=\"MBZUAI\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/a16z.jpeg\" alt=\"a16z\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/together.png\" alt=\"Together AI\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/hyperbolic_logo.png\" alt=\"Hyperbolic\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/anyscale.png\" alt=\"AnyScale\">\n    <img src=\"https://storage.googleapis.com/public-arena-asset/huggingface.png\" alt=\"HuggingFace\">\n</div>\n\"\"\"\n    gr.Markdown(about_markdown, elem_id=\"about_markdown\")\n\n\ndef build_single_model_ui(models, add_promotion_links=False):\n    promotion = (\n        \"\"\"\n- | [GitHub](https://github.com/lm-sys/FastChat) | [Dataset](https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md) | [Twitter](https://twitter.com/lmsysorg) | [Discord](https://discord.gg/HSWAKCrnFx) |\n- Introducing Llama 2: The Next Generation Open Source Large Language Model. [[Website]](https://ai.meta.com/llama/)\n- Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality. [[Blog]](https://lmsys.org/blog/2023-03-30-vicuna/)\n\n## \ud83e\udd16 Choose any model to chat\n\"\"\"\n        if add_promotion_links\n        else \"\"\n    )\n\n    notice_markdown = f\"\"\"\n# \ud83c\udfd4\ufe0f Chat with Open Large Language Models\n{promotion}\n\"\"\"\n\n    state = gr.State()\n    gr.Markdown(notice_markdown, elem_id=\"notice_markdown\")\n\n    with gr.Group(elem_id=\"share-region-named\"):\n        with gr.Row(elem_id=\"model_selector_row\"):\n            model_selector = gr.Dropdown(\n                choices=models,\n                value=models[0] if len(models) > 0 else \"\",\n                interactive=True,\n                show_label=False,\n                container=False,\n            )\n        with gr.Row():\n            with gr.Accordion(\n                f\"\ud83d\udd0d Expand to see the descriptions of {len(models)} models\",\n                open=False,\n            ):\n                model_description_md = get_model_description_md(models)\n                gr.Markdown(model_description_md, elem_id=\"model_description_markdown\")\n\n        chatbot = gr.Chatbot(\n            elem_id=\"chatbot\",\n            label=\"Scroll down and start chatting\",\n            height=550,\n            show_copy_button=True,\n        )\n    with gr.Row():\n        textbox = gr.Textbox(\n            show_label=False,\n            placeholder=\"\ud83d\udc49 Enter your prompt and press ENTER\",\n            elem_id=\"input_box\",\n        )\n        send_btn = gr.Button(value=\"Send\", variant=\"primary\", scale=0)\n\n    with gr.Row() as button_row:\n        upvote_btn = gr.Button(value=\"\ud83d\udc4d  Upvote\", interactive=False)\n        downvote_btn = gr.Button(value=\"\ud83d\udc4e  Downvote\", interactive=False)\n        flag_btn = gr.Button(value=\"\u26a0\ufe0f  Flag\", interactive=False)\n        regenerate_btn = gr.Button(value=\"\ud83d\udd04  Regenerate\", interactive=False)\n        clear_btn = gr.Button(value=\"\ud83d\uddd1\ufe0f  Clear history\", interactive=False)\n\n    with gr.Accordion(\"Parameters\", open=False) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=2048,\n            value=1024,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    if add_promotion_links:\n        gr.Markdown(acknowledgment_md, elem_id=\"ack_markdown\")\n\n    # Register listeners\n    imagebox = gr.State(None)\n    btn_list = [upvote_btn, downvote_btn, flag_btn, regenerate_btn, clear_btn]\n    upvote_btn.click(\n        upvote_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    downvote_btn.click(\n        downvote_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    flag_btn.click(\n        flag_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    regenerate_btn.click(\n        regenerate, state, [state, chatbot, textbox, imagebox] + btn_list\n    ).then(\n        bot_response,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n    clear_btn.click(clear_history, None, [state, chatbot, textbox, imagebox] + btn_list)\n\n    model_selector.change(\n        clear_history, None, [state, chatbot, textbox, imagebox] + btn_list\n    )\n\n    textbox.submit(\n        add_text,\n        [state, model_selector, textbox, imagebox],\n        [state, chatbot, textbox, imagebox] + btn_list,\n    ).then(\n        bot_response,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n    send_btn.click(\n        add_text,\n        [state, model_selector, textbox, imagebox],\n        [state, chatbot, textbox, imagebox] + btn_list,\n    ).then(\n        bot_response,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n\n    return [state, model_selector]\n\n\ndef build_demo(models):\n    with gr.Blocks(\n        title=\"Chat with Open Large Language Models\",\n        theme=gr.themes.Default(),\n        css=block_css,\n    ) as demo:\n        url_params = gr.JSON(visible=False)\n\n        state, model_selector = build_single_model_ui(models)\n\n        if args.model_list_mode not in [\"once\", \"reload\"]:\n            raise ValueError(f\"Unknown model list mode: {args.model_list_mode}\")\n\n        if args.show_terms_of_use:\n            load_js = get_window_url_params_with_tos_js\n        else:\n            load_js = get_window_url_params_js\n\n        demo.load(\n            load_demo,\n            [url_params],\n            [\n                state,\n                model_selector,\n            ],\n            js=load_js,\n        )\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\n        \"--share\",\n        action=\"store_true\",\n        help=\"Whether to generate a public, shareable link\",\n    )\n    parser.add_argument(\n        \"--controller-url\",\n        type=str,\n        default=\"http://localhost:21001\",\n        help=\"The address of the controller\",\n    )\n    parser.add_argument(\n        \"--concurrency-count\",\n        type=int,\n        default=10,\n        help=\"The concurrency count of the gradio queue\",\n    )\n    parser.add_argument(\n        \"--model-list-mode\",\n        type=str,\n        default=\"once\",\n        choices=[\"once\", \"reload\"],\n        help=\"Whether to load the model list once or reload the model list every time\",\n    )\n    parser.add_argument(\n        \"--moderate\",\n        action=\"store_true\",\n        help=\"Enable content moderation to block unsafe inputs\",\n    )\n    parser.add_argument(\n        \"--show-terms-of-use\",\n        action=\"store_true\",\n        help=\"Shows term of use before loading the demo\",\n    )\n    parser.add_argument(\n        \"--register-api-endpoint-file\",\n        type=str,\n        help=\"Register API-based model endpoints from a JSON file\",\n    )\n    parser.add_argument(\n        \"--gradio-auth-path\",\n        type=str,\n        help='Set the gradio authentication file path. The file should contain one or more user:password pairs in this format: \"u1:p1,u2:p2,u3:p3\"',\n    )\n    parser.add_argument(\n        \"--gradio-root-path\",\n        type=str,\n        help=\"Sets the gradio root path, eg /abc/def. Useful when running behind a reverse-proxy or at a custom URL path prefix\",\n    )\n    parser.add_argument(\n        \"--use-remote-storage\",\n        action=\"store_true\",\n        default=False,\n        help=\"Uploads image files to google cloud storage if set to true\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    # Set global variables\n    set_global_vars(args.controller_url, args.moderate, args.use_remote_storage)\n    models, all_models = get_model_list(\n        args.controller_url, args.register_api_endpoint_file, vision_arena=False\n    )\n\n    # Set authorization credentials\n    auth = None\n    if args.gradio_auth_path is not None:\n        auth = parse_gradio_auth_creds(args.gradio_auth_path)\n\n    # Launch the demo\n    demo = build_demo(models)\n    demo.queue(\n        default_concurrency_limit=args.concurrency_count,\n        status_update_rate=10,\n        api_open=False,\n    ).launch(\n        server_name=args.host,\n        server_port=args.port,\n        share=args.share,\n        max_threads=200,\n        auth=auth,\n        root_path=args.gradio_root_path,\n    )\n", "fastchat/serve/gradio_web_server_multi.py": "\"\"\"\nThe gradio demo server with multiple tabs.\nIt supports chatting with a single model or chatting with two models side-by-side.\n\"\"\"\n\nimport argparse\nimport pickle\nimport time\n\nimport gradio as gr\n\nfrom fastchat.serve.gradio_block_arena_anony import (\n    build_side_by_side_ui_anony,\n    load_demo_side_by_side_anony,\n    set_global_vars_anony,\n)\nfrom fastchat.serve.gradio_block_arena_named import (\n    build_side_by_side_ui_named,\n    load_demo_side_by_side_named,\n    set_global_vars_named,\n)\nfrom fastchat.serve.gradio_block_arena_vision import (\n    build_single_vision_language_model_ui,\n)\nfrom fastchat.serve.gradio_block_arena_vision_anony import (\n    build_side_by_side_vision_ui_anony,\n    load_demo_side_by_side_vision_anony,\n)\nfrom fastchat.serve.gradio_block_arena_vision_named import (\n    build_side_by_side_vision_ui_named,\n)\n\nfrom fastchat.serve.gradio_web_server import (\n    set_global_vars,\n    block_css,\n    build_single_model_ui,\n    build_about,\n    get_model_list,\n    load_demo_single,\n    get_ip,\n)\nfrom fastchat.serve.monitor.monitor import build_leaderboard_tab\nfrom fastchat.utils import (\n    build_logger,\n    get_window_url_params_js,\n    get_window_url_params_with_tos_js,\n    parse_gradio_auth_creds,\n)\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\n\ndef load_demo(url_params, request: gr.Request):\n    global models, all_models, vl_models\n\n    ip = get_ip(request)\n    logger.info(f\"load_demo. ip: {ip}. params: {url_params}\")\n\n    selected = 0\n    if \"arena\" in url_params:\n        selected = 0\n    elif \"compare\" in url_params:\n        selected = 1\n    elif \"direct\" in url_params or \"model\" in url_params:\n        selected = 2\n    elif \"vision\" in url_params:\n        selected = 3\n    elif \"leaderboard\" in url_params:\n        selected = 4\n    elif \"about\" in url_params:\n        selected = 5\n\n    if args.model_list_mode == \"reload\":\n        models, all_models = get_model_list(\n            args.controller_url,\n            args.register_api_endpoint_file,\n            vision_arena=False,\n        )\n\n        vl_models, all_vl_models = get_model_list(\n            args.controller_url,\n            args.register_api_endpoint_file,\n            vision_arena=True,\n        )\n\n    single_updates = load_demo_single(models, url_params)\n    side_by_side_anony_updates = load_demo_side_by_side_anony(all_models, url_params)\n    side_by_side_named_updates = load_demo_side_by_side_named(models, url_params)\n\n    vision_language_updates = load_demo_single(vl_models, url_params)\n    side_by_side_vision_named_updates = load_demo_side_by_side_named(\n        vl_models, url_params\n    )\n    side_by_side_vision_anony_updates = load_demo_side_by_side_vision_anony(\n        vl_models, url_params\n    )\n\n    return (\n        (gr.Tabs(selected=selected),)\n        + single_updates\n        + side_by_side_anony_updates\n        + side_by_side_named_updates\n        + side_by_side_vision_anony_updates\n        + side_by_side_vision_named_updates\n        + vision_language_updates\n    )\n\n\ndef build_demo(models, vl_models, elo_results_file, leaderboard_table_file):\n    text_size = gr.themes.sizes.text_md\n    if args.show_terms_of_use:\n        load_js = get_window_url_params_with_tos_js\n    else:\n        load_js = get_window_url_params_js\n\n    head_js = \"\"\"\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.4.1/html2canvas.min.js\"></script>\n\"\"\"\n    if args.ga_id is not None:\n        head_js += f\"\"\"\n<script async src=\"https://www.googletagmanager.com/gtag/js?id={args.ga_id}\"></script>\n<script>\nwindow.dataLayer = window.dataLayer || [];\nfunction gtag(){{dataLayer.push(arguments);}}\ngtag('js', new Date());\n\ngtag('config', '{args.ga_id}');\nwindow.__gradio_mode__ = \"app\";\n</script>\n        \"\"\"\n\n    with gr.Blocks(\n        title=\"Chat with Open Large Language Models\",\n        theme=gr.themes.Default(text_size=text_size),\n        css=block_css,\n        head=head_js,\n    ) as demo:\n        with gr.Tabs() as tabs:\n            with gr.Tab(\"Text Arena\", id=0):\n                with gr.Tab(\"\u2694\ufe0f  Arena (battle)\", id=0):\n                    side_by_side_anony_list = build_side_by_side_ui_anony(models)\n\n                with gr.Tab(\"\u2694\ufe0f  Arena (side-by-side)\", id=1):\n                    side_by_side_named_list = build_side_by_side_ui_named(models)\n\n                with gr.Tab(\"\ud83d\udcac Direct Chat\", id=2):\n                    single_model_list = build_single_model_ui(\n                        models, add_promotion_links=True\n                    )\n\n            demo_tabs = (\n                [tabs]\n                + single_model_list\n                + side_by_side_anony_list\n                + side_by_side_named_list\n            )\n\n            if args.vision_arena:\n                with gr.Tab(\"Vision Arena\", id=3):\n                    with gr.Tab(\"\u2694\ufe0f  Vision Arena (battle)\", id=3):\n                        side_by_side_vision_anony_list = (\n                            build_side_by_side_vision_ui_anony(\n                                vl_models,\n                                random_questions=args.random_questions,\n                            )\n                        )\n\n                    with gr.Tab(\"\u2694\ufe0f  Vision Arena (side-by-side)\", id=4):\n                        side_by_side_vision_named_list = (\n                            build_side_by_side_vision_ui_named(\n                                vl_models,\n                                random_questions=args.random_questions,\n                            )\n                        )\n\n                    with gr.Tab(\"\ud83d\udc40 Vision Direct Chat\", id=5):\n                        single_vision_language_model_list = (\n                            build_single_vision_language_model_ui(\n                                vl_models,\n                                add_promotion_links=True,\n                                random_questions=args.random_questions,\n                            )\n                        )\n                demo_tabs += (\n                    side_by_side_vision_anony_list\n                    + side_by_side_vision_named_list\n                    + single_vision_language_model_list\n                )\n\n            if elo_results_file:\n                with gr.Tab(\"Leaderboard\", id=6):\n                    build_leaderboard_tab(\n                        elo_results_file, leaderboard_table_file, show_plot=True\n                    )\n\n            with gr.Tab(\"\u2139\ufe0f  About Us\", id=7):\n                about = build_about()\n\n        url_params = gr.JSON(visible=False)\n\n        if args.model_list_mode not in [\"once\", \"reload\"]:\n            raise ValueError(f\"Unknown model list mode: {args.model_list_mode}\")\n\n        demo.load(\n            load_demo,\n            [url_params],\n            demo_tabs,\n            js=load_js,\n        )\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\n        \"--share\",\n        action=\"store_true\",\n        help=\"Whether to generate a public, shareable link\",\n    )\n    parser.add_argument(\n        \"--controller-url\",\n        type=str,\n        default=\"http://localhost:21001\",\n        help=\"The address of the controller\",\n    )\n    parser.add_argument(\n        \"--concurrency-count\",\n        type=int,\n        default=10,\n        help=\"The concurrency count of the gradio queue\",\n    )\n    parser.add_argument(\n        \"--model-list-mode\",\n        type=str,\n        default=\"once\",\n        choices=[\"once\", \"reload\"],\n        help=\"Whether to load the model list once or reload the model list every time.\",\n    )\n    parser.add_argument(\n        \"--moderate\",\n        action=\"store_true\",\n        help=\"Enable content moderation to block unsafe inputs\",\n    )\n    parser.add_argument(\n        \"--show-terms-of-use\",\n        action=\"store_true\",\n        help=\"Shows term of use before loading the demo\",\n    )\n    parser.add_argument(\n        \"--vision-arena\", action=\"store_true\", help=\"Show tabs for vision arena.\"\n    )\n    parser.add_argument(\n        \"--random-questions\", type=str, help=\"Load random questions from a JSON file\"\n    )\n    parser.add_argument(\n        \"--register-api-endpoint-file\",\n        type=str,\n        help=\"Register API-based model endpoints from a JSON file\",\n    )\n    parser.add_argument(\n        \"--gradio-auth-path\",\n        type=str,\n        help='Set the gradio authentication file path. The file should contain one or more user:password pairs in this format: \"u1:p1,u2:p2,u3:p3\"',\n        default=None,\n    )\n    parser.add_argument(\n        \"--elo-results-file\", type=str, help=\"Load leaderboard results and plots\"\n    )\n    parser.add_argument(\n        \"--leaderboard-table-file\", type=str, help=\"Load leaderboard results and plots\"\n    )\n    parser.add_argument(\n        \"--gradio-root-path\",\n        type=str,\n        help=\"Sets the gradio root path, eg /abc/def. Useful when running behind a reverse-proxy or at a custom URL path prefix\",\n    )\n    parser.add_argument(\n        \"--ga-id\",\n        type=str,\n        help=\"the Google Analytics ID\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--use-remote-storage\",\n        action=\"store_true\",\n        default=False,\n        help=\"Uploads image files to google cloud storage if set to true\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    # Set global variables\n    set_global_vars(args.controller_url, args.moderate, args.use_remote_storage)\n    set_global_vars_named(args.moderate)\n    set_global_vars_anony(args.moderate)\n    models, all_models = get_model_list(\n        args.controller_url,\n        args.register_api_endpoint_file,\n        vision_arena=False,\n    )\n\n    vl_models, all_vl_models = get_model_list(\n        args.controller_url,\n        args.register_api_endpoint_file,\n        vision_arena=True,\n    )\n\n    # Set authorization credentials\n    auth = None\n    if args.gradio_auth_path is not None:\n        auth = parse_gradio_auth_creds(args.gradio_auth_path)\n\n    # Launch the demo\n    demo = build_demo(\n        models,\n        vl_models,\n        args.elo_results_file,\n        args.leaderboard_table_file,\n    )\n    demo.queue(\n        default_concurrency_limit=args.concurrency_count,\n        status_update_rate=10,\n        api_open=False,\n    ).launch(\n        server_name=args.host,\n        server_port=args.port,\n        share=args.share,\n        max_threads=200,\n        auth=auth,\n        root_path=args.gradio_root_path,\n        show_api=False,\n    )\n", "fastchat/serve/lightllm_worker.py": "\"\"\"\nA model worker that executes the model based on LightLLM.\n\nSee documentations at docs/lightllm_integration.md\n\"\"\"\n\nimport argparse\nimport asyncio\nimport json\nimport os\nimport torch\nimport uvicorn\n\nfrom transformers import AutoConfig\n\nfrom typing import List\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse, JSONResponse\n\nfrom fastchat.serve.base_model_worker import BaseModelWorker\nfrom fastchat.serve.model_worker import (\n    logger,\n    worker_id,\n)\n\nfrom lightllm.server.sampling_params import SamplingParams\nfrom lightllm.server.multimodal_params import MultimodalParams\nfrom lightllm.server.httpserver.manager import HttpServerManager\nfrom lightllm.server.detokenization.manager import start_detokenization_process\nfrom lightllm.server.router.manager import start_router_process\nfrom lightllm.server.req_id_generator import ReqIDGenerator\n\nfrom lightllm.utils.net_utils import alloc_can_use_network_port\nfrom lightllm.utils.start_utils import start_submodule_processes\nfrom fastchat.utils import get_context_length, is_partial_stop\n\napp = FastAPI()\ng_id_gen = ReqIDGenerator()\n\n\nclass LightLLMWorker(BaseModelWorker):\n    def __init__(\n        self,\n        controller_addr: str,\n        worker_addr: str,\n        worker_id: str,\n        model_path: str,\n        model_names: List[str],\n        limit_worker_concurrency: int,\n        no_register: bool,\n        conv_template: str,\n        tokenizer,\n        context_len,\n    ):\n        super().__init__(\n            controller_addr,\n            worker_addr,\n            worker_id,\n            model_path,\n            model_names,\n            limit_worker_concurrency,\n            conv_template,\n        )\n\n        logger.info(\n            f\"Loading the model {self.model_names} on worker {worker_id}, worker type: LightLLM worker...\"\n        )\n        self.tokenizer = tokenizer\n        self.context_len = context_len\n\n        self.is_first = True\n\n        if not no_register:\n            self.init_heart_beat()\n\n    async def generate_stream(self, params):\n        self.call_ct += 1\n\n        prompt = params.pop(\"prompt\")\n        request_id = params.pop(\"request_id\")\n        temperature = float(params.get(\"temperature\", 1.0))\n        top_p = float(params.get(\"top_p\", 1.0))\n        top_k = params.get(\"top_k\", -1.0)\n        presence_penalty = float(params.get(\"presence_penalty\", 0.0))\n        frequency_penalty = float(params.get(\"frequency_penalty\", 0.0))\n        repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n        max_new_tokens = params.get(\"max_new_tokens\", 256)\n        echo = params.get(\"echo\", True)\n        stop_str = params.get(\"stop\", None)\n        stop_token_ids = params.get(\"stop_token_ids\", None) or []\n        if self.tokenizer.eos_token_id is not None:\n            stop_token_ids.append(self.tokenizer.eos_token_id)\n\n        request = params.get(\"request\", None)\n\n        # Handle stop_str\n        stop = set()\n        if isinstance(stop_str, str) and stop_str != \"\":\n            stop.add(stop_str)\n        elif isinstance(stop_str, list) and stop_str != []:\n            stop.update(stop_str)\n\n        for tid in stop_token_ids:\n            if tid is not None:\n                s = self.tokenizer.decode(tid)\n                if s != \"\":\n                    stop.add(s)\n\n        if self.is_first:\n            loop = asyncio.get_event_loop()\n            loop.create_task(httpserver_manager.handle_loop())\n            self.is_first = False\n\n        # make sampling params in vllm\n        top_p = max(top_p, 1e-5)\n        if temperature <= 1e-5:\n            top_p = 1.0\n\n        sampling_params = SamplingParams(\n            do_sample=temperature > 0.0,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            repetition_penalty=repetition_penalty,\n            max_new_tokens=max_new_tokens,\n            stop_sequences=list(stop),\n        )\n        sampling_params.verify()\n\n        results_generator = httpserver_manager.generate(\n            prompt, sampling_params, request_id, MultimodalParams()\n        )\n\n        completion_tokens = 0\n        text_outputs = \"\"\n        cumulative_logprob = 0.0\n\n        async for request_output, metadata, finish_status in results_generator:\n            text_outputs += request_output\n            completion_tokens += 1\n\n            partial_stop = any(is_partial_stop(text_outputs, i) for i in stop)\n            # prevent yielding partial stop sequence\n            if partial_stop:\n                continue\n\n            if type(finish_status) is bool:  # compatibility with old version\n                finish_reason = \"stop\" if finish_status else None\n            else:\n                finish_reason = finish_status.get_finish_reason()\n\n            if request and await request.is_disconnected():\n                await httpserver_manager.abort(request_id)\n                finish_reason = \"abort\"\n\n            logprob = metadata.get(\"logprob\", None)\n            if logprob is not None:\n                cumulative_logprob += logprob\n\n            prompt_tokens = metadata[\"prompt_tokens\"]\n            ret = {\n                \"text\": prompt + text_outputs if echo else text_outputs,\n                \"error_code\": 0,\n                \"usage\": {\n                    \"prompt_tokens\": prompt_tokens,\n                    \"completion_tokens\": completion_tokens,\n                    \"total_tokens\": prompt_tokens + completion_tokens,\n                },\n                \"cumulative_logprob\": cumulative_logprob,\n            }\n\n            if finish_reason is not None:\n                yield (\n                    json.dumps({**ret, \"finish_reason\": None}, ensure_ascii=False)\n                    + \"\\0\"\n                ).encode(\"utf-8\")\n            yield (\n                json.dumps({**ret, \"finish_reason\": finish_reason}, ensure_ascii=False)\n                + \"\\0\"\n            ).encode(\"utf-8\")\n\n            if finish_reason is not None:  # In case of abort, we need to break the loop\n                break\n\n    async def generate(self, params):\n        async for x in self.generate_stream(params):\n            pass\n        return json.loads(x[:-1].decode())\n\n\ndef release_worker_semaphore():\n    worker.semaphore.release()\n\n\ndef acquire_worker_semaphore():\n    if worker.semaphore is None:\n        worker.semaphore = asyncio.Semaphore(worker.limit_worker_concurrency)\n    return worker.semaphore.acquire()\n\n\ndef create_background_tasks(request_id):\n    async def abort_request() -> None:\n        await httpserver_manager.abort(request_id)\n\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(release_worker_semaphore)\n    background_tasks.add_task(abort_request)\n    return background_tasks\n\n\n@app.post(\"/worker_generate_stream\")\nasync def api_generate_stream(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    request_id = g_id_gen.generate_id()\n    params[\"request_id\"] = request_id\n    params[\"request\"] = request\n    generator = worker.generate_stream(params)\n    background_tasks = create_background_tasks(request_id)\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate\")\nasync def api_generate(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    request_id = g_id_gen.generate_id()\n    params[\"request_id\"] = request_id\n    params[\"request\"] = request\n    output = await worker.generate(params)\n    release_worker_semaphore()\n    await httpserver_manager.abort(request_id)\n    return JSONResponse(output)\n\n\n@app.post(\"/worker_get_status\")\nasync def api_get_status(request: Request):\n    return worker.get_status()\n\n\n@app.post(\"/count_token\")\nasync def api_count_token(request: Request):\n    params = await request.json()\n    return worker.count_token(params)\n\n\n@app.post(\"/worker_get_conv_template\")\nasync def api_get_conv(request: Request):\n    return worker.get_conv_template()\n\n\n@app.post(\"/model_details\")\nasync def api_model_details(request: Request):\n    return {\"context_length\": worker.context_len}\n\n\nif __name__ == \"__main__\":\n    torch.multiprocessing.set_start_method(\"spawn\")\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"127.0.0.1\")\n    parser.add_argument(\"--port\", type=int, default=8000)\n\n    parser.add_argument(\n        \"--model-path\",\n        dest=\"model_dir\",\n        type=str,\n        default=None,\n        help=\"the model weight dir path, the app will load config, weights and tokenizer from this dir\",\n    )\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\n        \"--conv-template\", type=str, default=None, help=\"Conversation prompt template.\"\n    )\n    parser.add_argument(\n        \"--model-names\",\n        type=lambda s: s.split(\",\"),\n        help=\"Optional display comma separated names\",\n    )\n    parser.add_argument(\"--limit-worker-concurrency\", type=int, default=1024)\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n\n    parser.add_argument(\n        \"--tokenizer_mode\",\n        type=str,\n        default=\"slow\",\n        help=\"\"\"tokenizer load mode, can be slow or auto, slow mode load fast but run slow, slow mode is good for debug and test,\n                        when you want to get best performance, try auto mode\"\"\",\n    )\n    parser.add_argument(\n        \"--load_way\",\n        type=str,\n        default=\"HF\",\n        help=\"the way of loading model weights, the default is HF(Huggingface format), llama also supports DS(Deepspeed)\",\n    )\n    parser.add_argument(\n        \"--max_total_token_num\",\n        type=int,\n        default=6000,\n        help=\"the total token nums the gpu and model can support, equals = max_batch * (input_len + output_len)\",\n    )\n    parser.add_argument(\n        \"--batch_max_tokens\",\n        type=int,\n        default=None,\n        help=\"max tokens num for new cat batch, it control prefill batch size to Preventing OOM\",\n    )\n    parser.add_argument(\"--eos_id\", type=int, default=2, help=\"eos stop token id\")\n    parser.add_argument(\n        \"--running_max_req_size\",\n        type=int,\n        default=1000,\n        help=\"the max size for forward requests in the same time\",\n    )\n    parser.add_argument(\n        \"--tp\", type=int, default=1, help=\"model tp parral size, the default is 1\"\n    )\n    parser.add_argument(\n        \"--max_req_input_len\",\n        type=int,\n        default=None,\n        help=\"the max value for req input tokens num. If None, it will be derived from the config.\",\n    )\n    parser.add_argument(\n        \"--max_req_total_len\",\n        type=int,\n        default=None,\n        help=\"the max value for req_input_len + req_output_len. If None, it will be derived from the config.\",\n    )\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=[],\n        nargs=\"+\",\n        help=\"\"\"Model mode: [triton_int8kv | ppl_int8kv | ppl_fp16 | triton_flashdecoding\n                        | triton_gqa_attention | triton_gqa_flashdecoding]\n                        [triton_int8weight | triton_int4weight | lmdeploy_int4weight | ppl_int4weight],\n                        triton_flashdecoding mode is for long context, current support llama llama2 qwen;\n                        triton_gqa_attention and triton_gqa_flashdecoding is fast kernel for model which use GQA;\n                        triton_int8kv mode use int8 to store kv cache, can increase token capacity, use triton kernel;\n                        ppl_int8kv mode use int8 to store kv cache, and use ppl fast kernel;\n                        ppl_fp16 mode use ppl fast fp16 decode attention kernel;\n                        triton_int8weight and triton_int4weight and lmdeploy_int4weight or ppl_int4weight mode use int8 and int4 to store weights;\n                        you need to read source code to make sure the supported detail mode for all models\"\"\",\n    )\n    parser.add_argument(\n        \"--trust_remote_code\",\n        action=\"store_true\",\n        help=\"Whether or not to allow for custom models defined on the Hub in their own modeling files.\",\n    )\n    parser.add_argument(\n        \"--disable_log_stats\",\n        action=\"store_true\",\n        help=\"disable logging throughput stats.\",\n    )\n    parser.add_argument(\n        \"--log_stats_interval\",\n        type=int,\n        default=10,\n        help=\"log stats interval in second.\",\n    )\n\n    parser.add_argument(\n        \"--router_token_ratio\",\n        type=float,\n        default=0.0,\n        help=\"token ratio to control router dispatch\",\n    )\n    parser.add_argument(\n        \"--router_max_new_token_len\",\n        type=int,\n        default=1024,\n        help=\"the request max new token len for router\",\n    )\n\n    parser.add_argument(\n        \"--no_skipping_special_tokens\",\n        action=\"store_true\",\n        help=\"whether to skip special tokens when decoding\",\n    )\n    parser.add_argument(\n        \"--no_spaces_between_special_tokens\",\n        action=\"store_true\",\n        help=\"whether to add spaces between special tokens when decoding\",\n    )\n\n    parser.add_argument(\n        \"--splitfuse_mode\", action=\"store_true\", help=\"use splitfuse mode\"\n    )\n    parser.add_argument(\n        \"--splitfuse_block_size\", type=int, default=256, help=\"splitfuse block size\"\n    )\n    parser.add_argument(\n        \"--prompt_cache_strs\",\n        type=str,\n        default=[],\n        nargs=\"+\",\n        help=\"\"\"prompt cache strs\"\"\",\n    )\n    parser.add_argument(\n        \"--cache_capacity\",\n        type=int,\n        default=200,\n        help=\"cache server capacity for multimodal resources\",\n    )\n    parser.add_argument(\n        \"--cache_reserved_ratio\",\n        type=float,\n        default=0.5,\n        help=\"cache server reserved capacity ratio after clear\",\n    )\n    parser.add_argument(\n        \"--return_all_prompt_logprobs\",\n        action=\"store_true\",\n        help=\"return all prompt tokens logprobs\",\n    )\n    parser.add_argument(\n        \"--long_truncation_mode\",\n        type=str,\n        choices=[None, \"head\", \"center\"],\n        default=None,\n        help=\"\"\"use to select the handle way when input token len > max_req_input_len.\n                        None : raise Exception\n                        head : remove some head tokens to make input token len <= max_req_input_len\n                        center : remove some tokens in center loc to make input token len <= max_req_input_len\"\"\",\n    )\n\n    args = parser.parse_args()\n\n    # \u975esplitfuse \u6a21\u5f0f\uff0c\u4e0d\u652f\u6301 prompt cache \u7279\u6027\n    if not args.splitfuse_mode:\n        assert len(args.prompt_cache_strs) == 0\n\n    model_config = AutoConfig.from_pretrained(args.model_dir)\n    context_length = get_context_length(model_config)\n\n    if args.max_req_input_len is None:\n        args.max_req_input_len = context_length - 1\n    if args.max_req_total_len is None:\n        args.max_req_total_len = context_length\n\n    assert args.max_req_input_len < args.max_req_total_len\n    assert args.max_req_total_len <= args.max_total_token_num\n\n    if not args.splitfuse_mode:\n        # \u666e\u901a\u6a21\u5f0f\u4e0b\n        if args.batch_max_tokens is None:\n            batch_max_tokens = int(1 / 6 * args.max_total_token_num)\n            batch_max_tokens = max(batch_max_tokens, args.max_req_total_len)\n            args.batch_max_tokens = batch_max_tokens\n        else:\n            assert (\n                args.batch_max_tokens >= args.max_req_total_len\n            ), \"batch_max_tokens must >= max_req_total_len\"\n    else:\n        # splitfuse \u6a21\u5f0f\u4e0b\n        # assert args.batch_max_tokens is not None, \"need to set by yourself\"\n        if args.batch_max_tokens is None:\n            batch_max_tokens = int(1 / 6 * args.max_total_token_num)\n            batch_max_tokens = max(batch_max_tokens, args.splitfuse_block_size)\n            args.batch_max_tokens = batch_max_tokens\n\n    can_use_ports = alloc_can_use_network_port(num=6 + args.tp)\n\n    assert can_use_ports is not None, \"Can not alloc enough free ports.\"\n    (\n        router_port,\n        detokenization_port,\n        httpserver_port,\n        visual_port,\n        cache_port,\n        nccl_port,\n    ) = can_use_ports[0:6]\n    args.nccl_port = nccl_port\n    model_rpc_ports = can_use_ports[6:]\n\n    global httpserver_manager\n    httpserver_manager = HttpServerManager(\n        args,\n        router_port=router_port,\n        cache_port=cache_port,\n        visual_port=visual_port,\n        httpserver_port=httpserver_port,\n        enable_multimodal=False,\n    )\n\n    start_submodule_processes(\n        start_funcs=[start_router_process, start_detokenization_process],\n        start_args=[\n            (args, router_port, detokenization_port, model_rpc_ports),\n            (args, detokenization_port, httpserver_port),\n        ],\n    )\n    worker = LightLLMWorker(\n        args.controller_address,\n        args.worker_address,\n        worker_id,\n        args.model_dir,\n        args.model_names,\n        args.limit_worker_concurrency,\n        args.no_register,\n        args.conv_template,\n        httpserver_manager.tokenizer,\n        context_length,\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n", "fastchat/serve/base_model_worker.py": "import asyncio\nimport threading\nimport time\nfrom typing import List\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse, JSONResponse\nimport requests\n\nfrom fastchat.constants import WORKER_HEART_BEAT_INTERVAL\nfrom fastchat.conversation import Conversation\nfrom fastchat.utils import pretty_print_semaphore, build_logger\n\n\nworker = None\nlogger = None\n\napp = FastAPI()\n\n\ndef heart_beat_worker(obj):\n    while True:\n        time.sleep(WORKER_HEART_BEAT_INTERVAL)\n        obj.send_heart_beat()\n\n\nclass BaseModelWorker:\n    def __init__(\n        self,\n        controller_addr: str,\n        worker_addr: str,\n        worker_id: str,\n        model_path: str,\n        model_names: List[str],\n        limit_worker_concurrency: int,\n        conv_template: str = None,\n        multimodal: bool = False,\n    ):\n        global logger, worker\n\n        self.controller_addr = controller_addr\n        self.worker_addr = worker_addr\n        self.worker_id = worker_id\n        if model_path.endswith(\"/\"):\n            model_path = model_path[:-1]\n        self.model_names = model_names or [model_path.split(\"/\")[-1]]\n        self.limit_worker_concurrency = limit_worker_concurrency\n        self.conv = self.make_conv_template(conv_template, model_path)\n        self.conv.sep_style = int(self.conv.sep_style)\n        self.multimodal = multimodal\n        self.tokenizer = None\n        self.context_len = None\n        self.call_ct = 0\n        self.semaphore = None\n\n        self.heart_beat_thread = None\n\n        if logger is None:\n            logger = build_logger(\"model_worker\", f\"model_worker_{self.worker_id}.log\")\n        if worker is None:\n            worker = self\n\n    def make_conv_template(\n        self,\n        conv_template: str = None,\n        model_path: str = None,\n    ) -> Conversation:\n        \"\"\"\n        can be overrided to costomize the conversation template for different model workers.\n        \"\"\"\n        from fastchat.conversation import get_conv_template\n        from fastchat.model.model_adapter import get_conversation_template\n\n        if conv_template:\n            conv = get_conv_template(conv_template)\n        else:\n            conv = get_conversation_template(model_path)\n        return conv\n\n    def init_heart_beat(self):\n        self.register_to_controller()\n        self.heart_beat_thread = threading.Thread(\n            target=heart_beat_worker,\n            args=(self,),\n            daemon=True,\n        )\n        self.heart_beat_thread.start()\n\n    def register_to_controller(self):\n        logger.info(\"Register to controller\")\n\n        url = self.controller_addr + \"/register_worker\"\n        data = {\n            \"worker_name\": self.worker_addr,\n            \"check_heart_beat\": True,\n            \"worker_status\": self.get_status(),\n            \"multimodal\": self.multimodal,\n        }\n        r = requests.post(url, json=data)\n        assert r.status_code == 200\n\n    def send_heart_beat(self):\n        logger.info(\n            f\"Send heart beat. Models: {self.model_names}. \"\n            f\"Semaphore: {pretty_print_semaphore(self.semaphore)}. \"\n            f\"call_ct: {self.call_ct}. \"\n            f\"worker_id: {self.worker_id}. \"\n        )\n\n        url = self.controller_addr + \"/receive_heart_beat\"\n\n        while True:\n            try:\n                ret = requests.post(\n                    url,\n                    json={\n                        \"worker_name\": self.worker_addr,\n                        \"queue_length\": self.get_queue_length(),\n                    },\n                    timeout=5,\n                )\n                exist = ret.json()[\"exist\"]\n                break\n            except (requests.exceptions.RequestException, KeyError) as e:\n                logger.error(f\"heart beat error: {e}\")\n            time.sleep(5)\n\n        if not exist:\n            self.register_to_controller()\n\n    def get_queue_length(self):\n        if self.semaphore is None:\n            return 0\n        else:\n            sempahore_value = (\n                self.semaphore._value\n                if self.semaphore._value is not None\n                else self.limit_worker_concurrency\n            )\n            waiter_count = (\n                0 if self.semaphore._waiters is None else len(self.semaphore._waiters)\n            )\n            return self.limit_worker_concurrency - sempahore_value + waiter_count\n\n    def get_status(self):\n        return {\n            \"model_names\": self.model_names,\n            \"speed\": 1,\n            \"queue_length\": self.get_queue_length(),\n        }\n\n    def count_token(self, params):\n        prompt = params[\"prompt\"]\n\n        try:\n            input_ids = self.tokenizer(prompt).input_ids\n            input_echo_len = len(input_ids)\n        except TypeError:\n            input_echo_len = self.tokenizer.num_tokens(prompt)\n\n        ret = {\n            \"count\": input_echo_len,\n            \"error_code\": 0,\n        }\n        return ret\n\n    def get_conv_template(self):\n        return {\"conv\": self.conv}\n\n    def generate_stream_gate(self, params):\n        raise NotImplementedError\n\n    def generate_gate(self, params):\n        raise NotImplementedError\n\n    def get_embeddings(self, params):\n        raise NotImplementedError\n\n\ndef release_worker_semaphore():\n    worker.semaphore.release()\n\n\ndef acquire_worker_semaphore():\n    if worker.semaphore is None:\n        worker.semaphore = asyncio.Semaphore(worker.limit_worker_concurrency)\n    return worker.semaphore.acquire()\n\n\ndef create_background_tasks():\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(release_worker_semaphore)\n    return background_tasks\n\n\n@app.post(\"/worker_generate_stream\")\nasync def api_generate_stream(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    generator = worker.generate_stream_gate(params)\n    background_tasks = create_background_tasks()\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate\")\nasync def api_generate(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    output = await asyncio.to_thread(worker.generate_gate, params)\n    release_worker_semaphore()\n    return JSONResponse(output)\n\n\n@app.post(\"/worker_get_embeddings\")\nasync def api_get_embeddings(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    embedding = worker.get_embeddings(params)\n    release_worker_semaphore()\n    return JSONResponse(content=embedding)\n\n\n@app.post(\"/worker_get_status\")\nasync def api_get_status(request: Request):\n    return worker.get_status()\n\n\n@app.post(\"/count_token\")\nasync def api_count_token(request: Request):\n    params = await request.json()\n    return worker.count_token(params)\n\n\n@app.post(\"/worker_get_conv_template\")\nasync def api_get_conv(request: Request):\n    return worker.get_conv_template()\n\n\n@app.post(\"/model_details\")\nasync def api_model_details(request: Request):\n    return {\"context_length\": worker.context_len}\n", "fastchat/serve/remote_logger.py": "# A JSON logger that sends data to remote endpoint.\n# Architecturally, it hosts a background thread that sends logs to a remote endpoint.\nimport os\nimport json\nimport requests\nimport threading\nimport queue\nimport logging\n\n_global_logger = None\n\n\ndef get_remote_logger():\n    global _global_logger\n    if _global_logger is None:\n        if url := os.environ.get(\"REMOTE_LOGGER_URL\"):\n            logging.info(f\"Remote logger enabled, sending data to {url}\")\n            _global_logger = RemoteLogger(url=url)\n        else:\n            _global_logger = EmptyLogger()\n    return _global_logger\n\n\nclass EmptyLogger:\n    \"\"\"Dummy logger that does nothing.\"\"\"\n\n    def __init__(self):\n        pass\n\n    def log(self, _data: dict):\n        pass\n\n\nclass RemoteLogger:\n    \"\"\"A JSON logger that sends data to remote endpoint.\"\"\"\n\n    def __init__(self, url: str):\n        self.url = url\n\n        self.logs = queue.Queue()\n        self.thread = threading.Thread(target=self._send_logs, daemon=True)\n        self.thread.start()\n\n    def log(self, data: dict):\n        self.logs.put_nowait(data)\n\n    def _send_logs(self):\n        while True:\n            data = self.logs.get()\n\n            # process the data by keep only the top level fields, and turn any nested dict into a string\n            for key, value in data.items():\n                if isinstance(value, (dict, list, tuple)):\n                    data[key] = json.dumps(value, ensure_ascii=False)\n\n            try:\n                requests.post(self.url, json=data)\n            except Exception:\n                logging.exception(\"Failed to send logs to remote endpoint\")\n", "fastchat/serve/model_worker.py": "\"\"\"\nA model worker that executes the model.\n\"\"\"\nimport argparse\nimport base64\nimport gc\nimport json\nimport os\nfrom typing import List, Optional\nimport uuid\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import set_seed\nimport uvicorn\n\nfrom fastchat.constants import ErrorCode, SERVER_ERROR_MSG\nfrom fastchat.model.model_adapter import (\n    load_model,\n    add_model_args,\n    get_generate_stream_function,\n)\nfrom fastchat.modules.awq import AWQConfig\nfrom fastchat.modules.exllama import ExllamaConfig\nfrom fastchat.modules.xfastertransformer import XftConfig\nfrom fastchat.modules.gptq import GptqConfig\nfrom fastchat.serve.base_model_worker import BaseModelWorker, app\nfrom fastchat.utils import (\n    build_logger,\n    get_context_length,\n    str_to_torch_dtype,\n)\n\nworker_id = str(uuid.uuid4())[:8]\nlogger = build_logger(\"model_worker\", f\"model_worker_{worker_id}.log\")\n\n\nclass ModelWorker(BaseModelWorker):\n    def __init__(\n        self,\n        controller_addr: str,\n        worker_addr: str,\n        worker_id: str,\n        model_path: str,\n        model_names: List[str],\n        limit_worker_concurrency: int,\n        no_register: bool,\n        device: str,\n        num_gpus: int,\n        max_gpu_memory: str,\n        revision: str = None,\n        dtype: Optional[torch.dtype] = None,\n        load_8bit: bool = False,\n        cpu_offloading: bool = False,\n        gptq_config: Optional[GptqConfig] = None,\n        awq_config: Optional[AWQConfig] = None,\n        exllama_config: Optional[ExllamaConfig] = None,\n        xft_config: Optional[XftConfig] = None,\n        stream_interval: int = 2,\n        conv_template: Optional[str] = None,\n        embed_in_truncate: bool = False,\n        seed: Optional[int] = None,\n        debug: bool = False,\n        **kwargs,\n    ):\n        super().__init__(\n            controller_addr,\n            worker_addr,\n            worker_id,\n            model_path,\n            model_names,\n            limit_worker_concurrency,\n            conv_template=conv_template,\n        )\n\n        logger.info(f\"Loading the model {self.model_names} on worker {worker_id} ...\")\n        self.model, self.tokenizer = load_model(\n            model_path,\n            revision=revision,\n            device=device,\n            num_gpus=num_gpus,\n            max_gpu_memory=max_gpu_memory,\n            dtype=dtype,\n            load_8bit=load_8bit,\n            cpu_offloading=cpu_offloading,\n            gptq_config=gptq_config,\n            awq_config=awq_config,\n            exllama_config=exllama_config,\n            xft_config=xft_config,\n            debug=debug,\n        )\n        self.device = device\n        if self.tokenizer.pad_token == None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.context_len = get_context_length(self.model.config)\n        self.generate_stream_func = get_generate_stream_function(self.model, model_path)\n        self.stream_interval = stream_interval\n        self.embed_in_truncate = embed_in_truncate\n        self.seed = seed\n\n        if not no_register:\n            self.init_heart_beat()\n\n    def generate_stream_gate(self, params):\n        if self.device == \"npu\":\n            import torch_npu\n\n            torch_npu.npu.set_device(\"npu:0\")\n        self.call_ct += 1\n\n        try:\n            if self.seed is not None:\n                set_seed(self.seed)\n            for output in self.generate_stream_func(\n                self.model,\n                self.tokenizer,\n                params,\n                self.device,\n                self.context_len,\n                self.stream_interval,\n            ):\n                ret = {\n                    \"text\": output[\"text\"],\n                    \"error_code\": 0,\n                }\n                if \"usage\" in output:\n                    ret[\"usage\"] = output[\"usage\"]\n                if \"finish_reason\" in output:\n                    ret[\"finish_reason\"] = output[\"finish_reason\"]\n                if \"logprobs\" in output:\n                    ret[\"logprobs\"] = output[\"logprobs\"]\n                yield json.dumps(ret).encode() + b\"\\0\"\n        except torch.cuda.OutOfMemoryError as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.CUDA_OUT_OF_MEMORY,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n        except (ValueError, RuntimeError) as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.INTERNAL_ERROR,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n\n    def generate_gate(self, params):\n        for x in self.generate_stream_gate(params):\n            pass\n        return json.loads(x[:-1].decode())\n\n    def __process_embed_chunk(self, input_ids, attention_mask, **model_type_dict):\n        if model_type_dict.get(\"is_bert\"):\n            model_output = self.model(input_ids)\n            if model_type_dict.get(\"is_robert\"):\n                data = model_output.last_hidden_state\n            else:\n                data = model_output[0]\n        elif model_type_dict.get(\"is_t5\"):\n            model_output = self.model(input_ids, decoder_input_ids=input_ids)\n            data = model_output.encoder_last_hidden_state\n        else:\n            model_output = self.model(input_ids, output_hidden_states=True)\n            if model_type_dict.get(\"is_chatglm\"):\n                data = model_output.hidden_states[-1].transpose(0, 1)\n            else:\n                data = model_output.hidden_states[-1]\n\n        if hasattr(self.model, \"use_cls_pooling\") and self.model.use_cls_pooling:\n            sum_embeddings = data[:, 0]\n        else:\n            mask = attention_mask.unsqueeze(-1).expand(data.size()).float()\n            masked_embeddings = data * mask\n            sum_embeddings = torch.sum(masked_embeddings, dim=1)\n        token_num = torch.sum(attention_mask).item()\n\n        return sum_embeddings, token_num\n\n    def __encode_base64(self, embeddings: torch.Tensor) -> List[str]:\n        embeddings = embeddings.cpu()\n        return [\n            base64.b64encode(e.numpy().tobytes()).decode(\"utf-8\") for e in embeddings\n        ]\n\n    @torch.inference_mode()\n    def get_embeddings(self, params):\n        self.call_ct += 1\n\n        try:\n            tokenizer = self.tokenizer\n            ret = {\"embedding\": [], \"token_num\": 0}\n\n            model_type_dict = {\n                \"is_llama\": \"llama\" in str(type(self.model)),\n                \"is_t5\": \"t5\" in str(type(self.model)),\n                \"is_chatglm\": \"chatglm\" in str(type(self.model)),\n                \"is_bert\": \"bert\" in str(type(self.model)),\n                \"is_robert\": \"robert\" in str(type(self.model)),\n            }\n\n            if self.embed_in_truncate:\n                encoding = tokenizer.batch_encode_plus(\n                    params[\"input\"],\n                    padding=True,\n                    truncation=\"longest_first\",\n                    return_tensors=\"pt\",\n                    max_length=self.context_len,\n                )\n            else:\n                encoding = tokenizer.batch_encode_plus(\n                    params[\"input\"], padding=True, return_tensors=\"pt\"\n                )\n            input_ids = encoding[\"input_ids\"].to(self.device)\n            attention_mask = input_ids != tokenizer.pad_token_id\n\n            base64_encode = params.get(\"encoding_format\", None)\n\n            if self.embed_in_truncate:\n                embedding, token_num = self.__process_embed_chunk(\n                    input_ids, attention_mask, **model_type_dict\n                )\n                if (\n                    not hasattr(self.model, \"use_cls_pooling\")\n                    or not self.model.use_cls_pooling\n                ):\n                    embedding = embedding / token_num\n                normalized_embeddings = F.normalize(embedding, p=2, dim=1)\n                ret[\"token_num\"] = token_num\n            else:\n                all_embeddings = []\n                all_token_num = 0\n                for i in range(0, input_ids.size(1), self.context_len):\n                    chunk_input_ids = input_ids[:, i : i + self.context_len]\n                    chunk_attention_mask = attention_mask[:, i : i + self.context_len]\n\n                    # add cls token and mask to get cls embedding\n                    if (\n                        hasattr(self.model, \"use_cls_pooling\")\n                        and self.model.use_cls_pooling\n                    ):\n                        cls_tokens = (\n                            torch.zeros(\n                                (chunk_input_ids.size(0), 1),\n                                dtype=chunk_input_ids.dtype,\n                                device=chunk_input_ids.device,\n                            )\n                            + tokenizer.cls_token_id\n                        )\n                        chunk_input_ids = torch.cat(\n                            [cls_tokens, chunk_input_ids], dim=-1\n                        )\n                        mask = torch.ones(\n                            (chunk_attention_mask.size(0), 1),\n                            dtype=chunk_attention_mask.dtype,\n                            device=chunk_attention_mask.device,\n                        )\n                        chunk_attention_mask = torch.cat(\n                            [mask, chunk_attention_mask], dim=-1\n                        )\n\n                    chunk_embeddings, token_num = self.__process_embed_chunk(\n                        chunk_input_ids, chunk_attention_mask, **model_type_dict\n                    )\n                    if (\n                        hasattr(self.model, \"use_cls_pooling\")\n                        and self.model.use_cls_pooling\n                    ):\n                        all_embeddings.append(chunk_embeddings * token_num)\n                    else:\n                        all_embeddings.append(chunk_embeddings)\n                    all_token_num += token_num\n\n                all_embeddings_tensor = torch.stack(all_embeddings)\n                embedding = torch.sum(all_embeddings_tensor, dim=0) / all_token_num\n                normalized_embeddings = F.normalize(embedding, p=2, dim=1)\n\n                ret[\"token_num\"] = all_token_num\n\n            if base64_encode == \"base64\":\n                out_embeddings = self.__encode_base64(normalized_embeddings)\n            else:\n                out_embeddings = normalized_embeddings.tolist()\n            ret[\"embedding\"] = out_embeddings\n\n            gc.collect()\n            torch.cuda.empty_cache()\n            if self.device == \"xpu\":\n                torch.xpu.empty_cache()\n            if self.device == \"npu\":\n                torch.npu.empty_cache()\n        except torch.cuda.OutOfMemoryError as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.CUDA_OUT_OF_MEMORY,\n            }\n        except (ValueError, RuntimeError) as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.INTERNAL_ERROR,\n            }\n        return ret\n\n\ndef create_model_worker():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    add_model_args(parser)\n    parser.add_argument(\n        \"--model-names\",\n        type=lambda s: s.split(\",\"),\n        help=\"Optional display comma separated names\",\n    )\n    parser.add_argument(\n        \"--conv-template\", type=str, default=None, help=\"Conversation prompt template.\"\n    )\n    parser.add_argument(\"--embed-in-truncate\", action=\"store_true\")\n    parser.add_argument(\n        \"--limit-worker-concurrency\",\n        type=int,\n        default=5,\n        help=\"Limit the model concurrency to prevent OOM.\",\n    )\n    parser.add_argument(\"--stream-interval\", type=int, default=2)\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=None,\n        help=\"Overwrite the random seed for each generation.\",\n    )\n    parser.add_argument(\n        \"--debug\", type=bool, default=False, help=\"Print debugging messages\"\n    )\n    parser.add_argument(\n        \"--ssl\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"Enable SSL. Requires OS Environment variables 'SSL_KEYFILE' and 'SSL_CERTFILE'.\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    if args.gpus:\n        if len(args.gpus.split(\",\")) < args.num_gpus:\n            raise ValueError(\n                f\"Larger --num-gpus ({args.num_gpus}) than --gpus {args.gpus}!\"\n            )\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n    gptq_config = GptqConfig(\n        ckpt=args.gptq_ckpt or args.model_path,\n        wbits=args.gptq_wbits,\n        groupsize=args.gptq_groupsize,\n        act_order=args.gptq_act_order,\n    )\n    awq_config = AWQConfig(\n        ckpt=args.awq_ckpt or args.model_path,\n        wbits=args.awq_wbits,\n        groupsize=args.awq_groupsize,\n    )\n    if args.enable_exllama:\n        exllama_config = ExllamaConfig(\n            max_seq_len=args.exllama_max_seq_len,\n            gpu_split=args.exllama_gpu_split,\n            cache_8bit=args.exllama_cache_8bit,\n        )\n    else:\n        exllama_config = None\n    if args.enable_xft:\n        xft_config = XftConfig(\n            max_seq_len=args.xft_max_seq_len,\n            data_type=args.xft_dtype,\n        )\n        if args.device != \"cpu\":\n            print(\"xFasterTransformer now is only support CPUs. Reset device to CPU\")\n            args.device = \"cpu\"\n    else:\n        xft_config = None\n\n    worker = ModelWorker(\n        args.controller_address,\n        args.worker_address,\n        worker_id,\n        args.model_path,\n        args.model_names,\n        args.limit_worker_concurrency,\n        revision=args.revision,\n        no_register=args.no_register,\n        device=args.device,\n        num_gpus=args.num_gpus,\n        max_gpu_memory=args.max_gpu_memory,\n        dtype=str_to_torch_dtype(args.dtype),\n        load_8bit=args.load_8bit,\n        cpu_offloading=args.cpu_offloading,\n        gptq_config=gptq_config,\n        awq_config=awq_config,\n        exllama_config=exllama_config,\n        xft_config=xft_config,\n        stream_interval=args.stream_interval,\n        conv_template=args.conv_template,\n        embed_in_truncate=args.embed_in_truncate,\n        seed=args.seed,\n        debug=args.debug,\n    )\n    return args, worker\n\n\nif __name__ == \"__main__\":\n    args, worker = create_model_worker()\n    if args.ssl:\n        uvicorn.run(\n            app,\n            host=args.host,\n            port=args.port,\n            log_level=\"info\",\n            ssl_keyfile=os.environ[\"SSL_KEYFILE\"],\n            ssl_certfile=os.environ[\"SSL_CERTFILE\"],\n        )\n    else:\n        uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n", "fastchat/serve/gradio_block_arena_vision.py": "\"\"\"\nThe gradio demo server for chatting with a large multimodal model.\n\nUsage:\npython3 -m fastchat.serve.controller\npython3 -m fastchat.serve.sglang_worker --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf\npython3 -m fastchat.serve.gradio_web_server_multi --share --vision-arena\n\"\"\"\n\nimport json\nimport os\nimport time\n\nimport gradio as gr\nfrom gradio.data_classes import FileData\nimport numpy as np\n\nfrom fastchat.constants import (\n    TEXT_MODERATION_MSG,\n    IMAGE_MODERATION_MSG,\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_TURN_LIMIT,\n)\nfrom fastchat.serve.gradio_web_server import (\n    get_model_description_md,\n    acknowledgment_md,\n    bot_response,\n    get_ip,\n    disable_btn,\n    State,\n    _prepare_text_with_image,\n    get_conv_log_filename,\n    get_remote_logger,\n)\nfrom fastchat.utils import (\n    build_logger,\n    moderation_filter,\n    image_moderation_filter,\n)\n\nlogger = build_logger(\"gradio_web_server\", \"gradio_web_server.log\")\n\nno_change_btn = gr.Button()\nenable_btn = gr.Button(interactive=True, visible=True)\ndisable_btn = gr.Button(interactive=False)\ninvisible_btn = gr.Button(interactive=False, visible=False)\nvisible_image_column = gr.Image(visible=True)\ninvisible_image_column = gr.Image(visible=False)\n\n\ndef get_vqa_sample():\n    random_sample = np.random.choice(vqa_samples)\n    question, path = random_sample[\"question\"], random_sample[\"path\"]\n    res = {\"text\": \"\", \"files\": [path]}\n    return (res, path)\n\n\ndef set_visible_image(textbox):\n    images = textbox[\"files\"]\n    if len(images) == 0:\n        return invisible_image_column\n    elif len(images) > 1:\n        gr.Warning(\n            \"We only support single image conversations. Please start a new round if you would like to chat using this image.\"\n        )\n\n    return visible_image_column\n\n\ndef set_invisible_image():\n    return invisible_image_column\n\n\ndef add_image(textbox):\n    images = textbox[\"files\"]\n    if len(images) == 0:\n        return None\n\n    return images[0]\n\n\ndef vote_last_response(state, vote_type, model_selector, request: gr.Request):\n    filename = get_conv_log_filename(state.is_vision, state.has_csam_image)\n    with open(filename, \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"model\": model_selector,\n            \"state\": state.dict(),\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n    get_remote_logger().log(data)\n\n\ndef upvote_last_response(state, model_selector, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"upvote. ip: {ip}\")\n    vote_last_response(state, \"upvote\", model_selector, request)\n    return (None,) + (disable_btn,) * 3\n\n\ndef downvote_last_response(state, model_selector, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"downvote. ip: {ip}\")\n    vote_last_response(state, \"downvote\", model_selector, request)\n    return (None,) + (disable_btn,) * 3\n\n\ndef flag_last_response(state, model_selector, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"flag. ip: {ip}\")\n    vote_last_response(state, \"flag\", model_selector, request)\n    return (None,) + (disable_btn,) * 3\n\n\ndef regenerate(state, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"regenerate. ip: {ip}\")\n    if not state.regen_support:\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), \"\", None) + (no_change_btn,) * 5\n    state.conv.update_last_message(None)\n    return (state, state.to_gradio_chatbot(), None) + (disable_btn,) * 5\n\n\ndef clear_history(request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"clear_history. ip: {ip}\")\n    state = None\n    return (state, [], None) + (disable_btn,) * 5\n\n\ndef clear_history_example(request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"clear_history_example. ip: {ip}\")\n    state = None\n    return (state, []) + (disable_btn,) * 5\n\n\ndef moderate_input(text, all_conv_text, model_list, images, ip):\n    text_flagged = moderation_filter(all_conv_text, model_list)\n    # flagged = moderation_filter(text, [state.model_name])\n    nsfw_flagged, csam_flagged = False, False\n    if len(images) > 0:\n        nsfw_flagged, csam_flagged = image_moderation_filter(images[0])\n\n    image_flagged = nsfw_flagged or csam_flagged\n    if text_flagged or image_flagged:\n        logger.info(f\"violate moderation. ip: {ip}. text: {all_conv_text}\")\n        if text_flagged and not image_flagged:\n            # overwrite the original text\n            text = TEXT_MODERATION_MSG\n        elif not text_flagged and image_flagged:\n            text = IMAGE_MODERATION_MSG\n        elif text_flagged and image_flagged:\n            text = MODERATION_MSG\n\n    return text, image_flagged, csam_flagged\n\n\ndef add_text(state, model_selector, chat_input, request: gr.Request):\n    text, images = chat_input[\"text\"], chat_input[\"files\"]\n    ip = get_ip(request)\n    logger.info(f\"add_text. ip: {ip}. len: {len(text)}\")\n\n    if state is None:\n        state = State(model_selector, is_vision=True)\n\n    if len(text) <= 0:\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), None) + (no_change_btn,) * 5\n\n    all_conv_text = state.conv.get_prompt()\n    all_conv_text = all_conv_text[-2000:] + \"\\nuser: \" + text\n\n    text, image_flagged, csam_flag = moderate_input(\n        text, all_conv_text, [state.model_name], images, ip\n    )\n\n    if image_flagged:\n        logger.info(f\"image flagged. ip: {ip}. text: {text}\")\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), {\"text\": IMAGE_MODERATION_MSG}) + (\n            no_change_btn,\n        ) * 5\n\n    if (len(state.conv.messages) - state.conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:\n        logger.info(f\"conversation turn limit. ip: {ip}. text: {text}\")\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), {\"text\": CONVERSATION_LIMIT_MSG}) + (\n            no_change_btn,\n        ) * 5\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    text = _prepare_text_with_image(state, text, images, csam_flag=csam_flag)\n    state.conv.append_message(state.conv.roles[0], text)\n    state.conv.append_message(state.conv.roles[1], None)\n    return (state, state.to_gradio_chatbot(), None) + (disable_btn,) * 5\n\n\ndef build_single_vision_language_model_ui(\n    models, add_promotion_links=False, random_questions=None\n):\n    promotion = (\n        \"\"\"\n- | [GitHub](https://github.com/lm-sys/FastChat) | [Twitter](https://twitter.com/lmsysorg) | [Discord](https://discord.gg/HSWAKCrnFx) |\n\n**\u2757\ufe0f For research purposes, we log user prompts and images, and may release this data to the public in the future. Please do not upload any confidential or personal information.**\n\nNote: You can only chat with <span style='color: #DE3163; font-weight: bold'>one image per conversation</span>. You can upload images less than 15MB. Click the \"Random Example\" button to chat with a random image.\"\"\"\n        if add_promotion_links\n        else \"\"\n    )\n\n    notice_markdown = f\"\"\"\n# \ud83c\udfd4\ufe0f Chat with Open Large Vision-Language Models\n{promotion}\n\"\"\"\n\n    state = gr.State()\n    gr.Markdown(notice_markdown, elem_id=\"notice_markdown\")\n\n    with gr.Group():\n        with gr.Row(elem_id=\"model_selector_row\"):\n            model_selector = gr.Dropdown(\n                choices=models,\n                value=models[0] if len(models) > 0 else \"\",\n                interactive=True,\n                show_label=False,\n                container=False,\n            )\n\n        with gr.Accordion(\n            f\"\ud83d\udd0d Expand to see the descriptions of {len(models)} models\", open=False\n        ):\n            model_description_md = get_model_description_md(models)\n            gr.Markdown(model_description_md, elem_id=\"model_description_markdown\")\n\n    with gr.Row():\n        textbox = gr.MultimodalTextbox(\n            file_types=[\"image\"],\n            show_label=False,\n            placeholder=\"Click add or drop your image here\",\n            container=True,\n            render=False,\n            elem_id=\"input_box\",\n        )\n\n        with gr.Column(scale=2, visible=False) as image_column:\n            imagebox = gr.Image(\n                type=\"pil\",\n                show_label=False,\n                interactive=False,\n            )\n        with gr.Column(scale=8):\n            chatbot = gr.Chatbot(\n                elem_id=\"chatbot\", label=\"Scroll down and start chatting\", height=550\n            )\n\n    with gr.Row():\n        textbox.render()\n        # with gr.Column(scale=1, min_width=50):\n        #     send_btn = gr.Button(value=\"Send\", variant=\"primary\")\n\n    with gr.Row(elem_id=\"buttons\"):\n        if random_questions:\n            global vqa_samples\n            with open(random_questions, \"r\") as f:\n                vqa_samples = json.load(f)\n            random_btn = gr.Button(value=\"\ud83c\udfb2 Random Example\", interactive=True)\n        upvote_btn = gr.Button(value=\"\ud83d\udc4d  Upvote\", interactive=False)\n        downvote_btn = gr.Button(value=\"\ud83d\udc4e  Downvote\", interactive=False)\n        flag_btn = gr.Button(value=\"\u26a0\ufe0f  Flag\", interactive=False)\n        regenerate_btn = gr.Button(value=\"\ud83d\udd04  Regenerate\", interactive=False)\n        clear_btn = gr.Button(value=\"\ud83d\uddd1\ufe0f  Clear\", interactive=False)\n\n    cur_dir = os.path.dirname(os.path.abspath(__file__))\n\n    examples = gr.Examples(\n        examples=[\n            {\n                \"text\": \"How can I prepare a delicious meal using these ingredients?\",\n                \"files\": [f\"{cur_dir}/example_images/fridge.jpg\"],\n            },\n            {\n                \"text\": \"What might the woman on the right be thinking about?\",\n                \"files\": [f\"{cur_dir}/example_images/distracted.jpg\"],\n            },\n        ],\n        inputs=[textbox],\n    )\n\n    with gr.Accordion(\"Parameters\", open=False) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.2,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=0,\n            maximum=2048,\n            value=1024,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    if add_promotion_links:\n        gr.Markdown(acknowledgment_md, elem_id=\"ack_markdown\")\n\n    # Register listeners\n    btn_list = [upvote_btn, downvote_btn, flag_btn, regenerate_btn, clear_btn]\n    upvote_btn.click(\n        upvote_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    downvote_btn.click(\n        downvote_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    flag_btn.click(\n        flag_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    regenerate_btn.click(regenerate, state, [state, chatbot, textbox] + btn_list).then(\n        bot_response,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n    clear_btn.click(clear_history, None, [state, chatbot, textbox] + btn_list)\n\n    model_selector.change(\n        clear_history, None, [state, chatbot, textbox] + btn_list\n    ).then(set_visible_image, [textbox], [image_column])\n    examples.dataset.click(clear_history_example, None, [state, chatbot] + btn_list)\n\n    textbox.input(add_image, [textbox], [imagebox]).then(\n        set_visible_image, [textbox], [image_column]\n    ).then(clear_history_example, None, [state, chatbot] + btn_list)\n\n    textbox.submit(\n        add_text,\n        [state, model_selector, textbox],\n        [state, chatbot, textbox] + btn_list,\n    ).then(set_invisible_image, [], [image_column]).then(\n        bot_response,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n\n    if random_questions:\n        random_btn.click(\n            get_vqa_sample,  # First, get the VQA sample\n            [],  # Pass the path to the VQA samples\n            [textbox, imagebox],  # Outputs are textbox and imagebox\n        ).then(set_visible_image, [textbox], [image_column]).then(\n            clear_history_example, None, [state, chatbot] + btn_list\n        )\n\n    return [state, model_selector]\n", "fastchat/serve/__init__.py": "", "fastchat/serve/openai_api_server.py": "\"\"\"A server that provides OpenAI-compatible RESTful APIs. It supports:\n\n- Chat Completions. (Reference: https://platform.openai.com/docs/api-reference/chat)\n- Completions. (Reference: https://platform.openai.com/docs/api-reference/completions)\n- Embeddings. (Reference: https://platform.openai.com/docs/api-reference/embeddings)\n\nUsage:\npython3 -m fastchat.serve.openai_api_server\n\"\"\"\nimport asyncio\nimport argparse\nimport json\nimport os\nfrom typing import Generator, Optional, Union, Dict, List, Any\n\nimport aiohttp\nimport fastapi\nfrom fastapi import Depends, HTTPException\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import StreamingResponse, JSONResponse\nfrom fastapi.security.http import HTTPAuthorizationCredentials, HTTPBearer\nimport httpx\n\nfrom pydantic_settings import BaseSettings\nimport shortuuid\nimport tiktoken\nimport uvicorn\n\nfrom fastchat.constants import (\n    WORKER_API_TIMEOUT,\n    WORKER_API_EMBEDDING_BATCH_SIZE,\n    ErrorCode,\n)\nfrom fastchat.conversation import Conversation, SeparatorStyle\nfrom fastchat.protocol.openai_api_protocol import (\n    ChatCompletionRequest,\n    ChatCompletionResponse,\n    ChatCompletionResponseStreamChoice,\n    ChatCompletionStreamResponse,\n    ChatMessage,\n    ChatCompletionResponseChoice,\n    CompletionRequest,\n    CompletionResponse,\n    CompletionResponseChoice,\n    DeltaMessage,\n    CompletionResponseStreamChoice,\n    CompletionStreamResponse,\n    EmbeddingsRequest,\n    EmbeddingsResponse,\n    ErrorResponse,\n    LogProbs,\n    ModelCard,\n    ModelList,\n    ModelPermission,\n    UsageInfo,\n)\nfrom fastchat.protocol.api_protocol import (\n    APIChatCompletionRequest,\n    APITokenCheckRequest,\n    APITokenCheckResponse,\n    APITokenCheckResponseItem,\n)\nfrom fastchat.utils import build_logger\n\nlogger = build_logger(\"openai_api_server\", \"openai_api_server.log\")\n\nconv_template_map = {}\n\nfetch_timeout = aiohttp.ClientTimeout(total=3 * 3600)\n\n\nasync def fetch_remote(url, pload=None, name=None):\n    async with aiohttp.ClientSession(timeout=fetch_timeout) as session:\n        async with session.post(url, json=pload) as response:\n            chunks = []\n            if response.status != 200:\n                ret = {\n                    \"text\": f\"{response.reason}\",\n                    \"error_code\": ErrorCode.INTERNAL_ERROR,\n                }\n                return json.dumps(ret)\n\n            async for chunk, _ in response.content.iter_chunks():\n                chunks.append(chunk)\n        output = b\"\".join(chunks)\n\n    if name is not None:\n        res = json.loads(output)\n        if name != \"\":\n            res = res[name]\n        return res\n\n    return output\n\n\nclass AppSettings(BaseSettings):\n    # The address of the model controller.\n    controller_address: str = \"http://localhost:21001\"\n    api_keys: Optional[List[str]] = None\n\n\napp_settings = AppSettings()\napp = fastapi.FastAPI()\nheaders = {\"User-Agent\": \"FastChat API Server\"}\nget_bearer_token = HTTPBearer(auto_error=False)\n\n\nasync def check_api_key(\n    auth: Optional[HTTPAuthorizationCredentials] = Depends(get_bearer_token),\n) -> str:\n    if app_settings.api_keys:\n        if auth is None or (token := auth.credentials) not in app_settings.api_keys:\n            raise HTTPException(\n                status_code=401,\n                detail={\n                    \"error\": {\n                        \"message\": \"\",\n                        \"type\": \"invalid_request_error\",\n                        \"param\": None,\n                        \"code\": \"invalid_api_key\",\n                    }\n                },\n            )\n        return token\n    else:\n        # api_keys not set; allow all\n        return None\n\n\ndef create_error_response(code: int, message: str) -> JSONResponse:\n    return JSONResponse(\n        ErrorResponse(message=message, code=code).model_dump(), status_code=400\n    )\n\n\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request, exc):\n    return create_error_response(ErrorCode.VALIDATION_TYPE_ERROR, str(exc))\n\n\nasync def check_model(request) -> Optional[JSONResponse]:\n    controller_address = app_settings.controller_address\n    ret = None\n\n    models = await fetch_remote(controller_address + \"/list_models\", None, \"models\")\n    if request.model not in models:\n        ret = create_error_response(\n            ErrorCode.INVALID_MODEL,\n            f\"Only {'&&'.join(models)} allowed now, your model {request.model}\",\n        )\n    return ret\n\n\nasync def check_length(request, prompt, max_tokens, worker_addr):\n    if (\n        not isinstance(max_tokens, int) or max_tokens <= 0\n    ):  # model worker not support max_tokens=None\n        max_tokens = 1024 * 1024\n\n    context_len = await fetch_remote(\n        worker_addr + \"/model_details\", {\"model\": request.model}, \"context_length\"\n    )\n    token_num = await fetch_remote(\n        worker_addr + \"/count_token\",\n        {\"model\": request.model, \"prompt\": prompt},\n        \"count\",\n    )\n    length = min(max_tokens, context_len - token_num)\n\n    if length <= 0:\n        return None, create_error_response(\n            ErrorCode.CONTEXT_OVERFLOW,\n            f\"This model's maximum context length is {context_len} tokens. However, your messages resulted in {token_num} tokens. Please reduce the length of the messages.\",\n        )\n\n    return length, None\n\n\ndef check_requests(request) -> Optional[JSONResponse]:\n    # Check all params\n    if request.max_tokens is not None and request.max_tokens <= 0:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.max_tokens} is less than the minimum of 1 - 'max_tokens'\",\n        )\n    if request.n is not None and request.n <= 0:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.n} is less than the minimum of 1 - 'n'\",\n        )\n    if request.temperature is not None and request.temperature < 0:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.temperature} is less than the minimum of 0 - 'temperature'\",\n        )\n    if request.temperature is not None and request.temperature > 2:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.temperature} is greater than the maximum of 2 - 'temperature'\",\n        )\n    if request.top_p is not None and request.top_p < 0:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.top_p} is less than the minimum of 0 - 'top_p'\",\n        )\n    if request.top_p is not None and request.top_p > 1:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.top_p} is greater than the maximum of 1 - 'top_p'\",\n        )\n    if request.top_k is not None and (request.top_k > -1 and request.top_k < 1):\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.top_k} is out of Range. Either set top_k to -1 or >=1.\",\n        )\n    if request.stop is not None and (\n        not isinstance(request.stop, str) and not isinstance(request.stop, list)\n    ):\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.stop} is not valid under any of the given schemas - 'stop'\",\n        )\n\n    return None\n\n\ndef process_input(model_name, inp):\n    if isinstance(inp, str):\n        inp = [inp]\n    elif isinstance(inp, list):\n        if isinstance(inp[0], int):\n            try:\n                decoding = tiktoken.model.encoding_for_model(model_name)\n            except KeyError:\n                logger.warning(\"Warning: model not found. Using cl100k_base encoding.\")\n                model = \"cl100k_base\"\n                decoding = tiktoken.get_encoding(model)\n            inp = [decoding.decode(inp)]\n        elif isinstance(inp[0], list):\n            try:\n                decoding = tiktoken.model.encoding_for_model(model_name)\n            except KeyError:\n                logger.warning(\"Warning: model not found. Using cl100k_base encoding.\")\n                model = \"cl100k_base\"\n                decoding = tiktoken.get_encoding(model)\n            inp = [decoding.decode(text) for text in inp]\n\n    return inp\n\n\ndef create_openai_logprobs(logprob_dict):\n    \"\"\"Create OpenAI-style logprobs.\"\"\"\n    return LogProbs(**logprob_dict) if logprob_dict is not None else None\n\n\ndef _add_to_set(s, new_stop):\n    if not s:\n        return\n    if isinstance(s, str):\n        new_stop.add(s)\n    else:\n        new_stop.update(s)\n\n\nasync def get_gen_params(\n    model_name: str,\n    worker_addr: str,\n    messages: Union[str, List[Dict[str, str]]],\n    *,\n    temperature: float,\n    top_p: float,\n    top_k: Optional[int],\n    presence_penalty: Optional[float],\n    frequency_penalty: Optional[float],\n    max_tokens: Optional[int],\n    echo: Optional[bool],\n    logprobs: Optional[int] = None,\n    stop: Optional[Union[str, List[str]]],\n    best_of: Optional[int] = None,\n    use_beam_search: Optional[bool] = None,\n) -> Dict[str, Any]:\n    conv = await get_conv(model_name, worker_addr)\n    conv = Conversation(\n        name=conv[\"name\"],\n        system_template=conv[\"system_template\"],\n        system_message=conv[\"system_message\"],\n        roles=conv[\"roles\"],\n        messages=list(conv[\"messages\"]),  # prevent in-place modification\n        offset=conv[\"offset\"],\n        sep_style=SeparatorStyle(conv[\"sep_style\"]),\n        sep=conv[\"sep\"],\n        sep2=conv[\"sep2\"],\n        stop_str=conv[\"stop_str\"],\n        stop_token_ids=conv[\"stop_token_ids\"],\n    )\n\n    if isinstance(messages, str):\n        prompt = messages\n        images = []\n    else:\n        for message in messages:\n            msg_role = message[\"role\"]\n            if msg_role == \"system\":\n                conv.set_system_message(message[\"content\"])\n            elif msg_role == \"user\":\n                if type(message[\"content\"]) == list:\n                    image_list = [\n                        item[\"image_url\"][\"url\"]\n                        for item in message[\"content\"]\n                        if item[\"type\"] == \"image_url\"\n                    ]\n                    text_list = [\n                        item[\"text\"]\n                        for item in message[\"content\"]\n                        if item[\"type\"] == \"text\"\n                    ]\n\n                    # TODO(chris): This only applies to LLaVA model. Implement an image_token string in the conv template.\n                    text = \"<image>\\n\" * len(image_list)\n                    text += \"\\n\".join(text_list)\n                    conv.append_message(conv.roles[0], (text, image_list))\n                else:\n                    conv.append_message(conv.roles[0], message[\"content\"])\n            elif msg_role == \"assistant\":\n                conv.append_message(conv.roles[1], message[\"content\"])\n            else:\n                raise ValueError(f\"Unknown role: {msg_role}\")\n\n        # Add a blank message for the assistant.\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n        images = conv.get_images()\n\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"logprobs\": logprobs,\n        \"top_p\": top_p,\n        \"top_k\": top_k,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"max_new_tokens\": max_tokens,\n        \"echo\": echo,\n        \"stop_token_ids\": conv.stop_token_ids,\n    }\n\n    if len(images) > 0:\n        gen_params[\"images\"] = images\n\n    if best_of is not None:\n        gen_params.update({\"best_of\": best_of})\n    if use_beam_search is not None:\n        gen_params.update({\"use_beam_search\": use_beam_search})\n\n    new_stop = set()\n    _add_to_set(stop, new_stop)\n    _add_to_set(conv.stop_str, new_stop)\n\n    gen_params[\"stop\"] = list(new_stop)\n\n    logger.debug(f\"==== request ====\\n{gen_params}\")\n    return gen_params\n\n\nasync def get_worker_address(model_name: str) -> str:\n    \"\"\"\n    Get worker address based on the requested model\n\n    :param model_name: The worker's model name\n    :return: Worker address from the controller\n    :raises: :class:`ValueError`: No available worker for requested model\n    \"\"\"\n    controller_address = app_settings.controller_address\n    worker_addr = await fetch_remote(\n        controller_address + \"/get_worker_address\", {\"model\": model_name}, \"address\"\n    )\n\n    # No available worker\n    if worker_addr == \"\":\n        raise ValueError(f\"No available worker for {model_name}\")\n    logger.debug(f\"model_name: {model_name}, worker_addr: {worker_addr}\")\n    return worker_addr\n\n\nasync def get_conv(model_name: str, worker_addr: str):\n    conv_template = conv_template_map.get((worker_addr, model_name))\n    if conv_template is None:\n        conv_template = await fetch_remote(\n            worker_addr + \"/worker_get_conv_template\", {\"model\": model_name}, \"conv\"\n        )\n        conv_template_map[(worker_addr, model_name)] = conv_template\n    return conv_template\n\n\n@app.get(\"/v1/models\", dependencies=[Depends(check_api_key)])\nasync def show_available_models():\n    controller_address = app_settings.controller_address\n    ret = await fetch_remote(controller_address + \"/refresh_all_workers\")\n    models = await fetch_remote(controller_address + \"/list_models\", None, \"models\")\n\n    models.sort()\n    # TODO: return real model permission details\n    model_cards = []\n    for m in models:\n        model_cards.append(ModelCard(id=m, root=m, permission=[ModelPermission()]))\n    return ModelList(data=model_cards)\n\n\n@app.post(\"/v1/chat/completions\", dependencies=[Depends(check_api_key)])\nasync def create_chat_completion(request: ChatCompletionRequest):\n    \"\"\"Creates a completion for the chat message\"\"\"\n    error_check_ret = await check_model(request)\n    if error_check_ret is not None:\n        return error_check_ret\n    error_check_ret = check_requests(request)\n    if error_check_ret is not None:\n        return error_check_ret\n\n    worker_addr = await get_worker_address(request.model)\n\n    gen_params = await get_gen_params(\n        request.model,\n        worker_addr,\n        request.messages,\n        temperature=request.temperature,\n        top_p=request.top_p,\n        top_k=request.top_k,\n        presence_penalty=request.presence_penalty,\n        frequency_penalty=request.frequency_penalty,\n        max_tokens=request.max_tokens,\n        echo=False,\n        stop=request.stop,\n    )\n\n    max_new_tokens, error_check_ret = await check_length(\n        request,\n        gen_params[\"prompt\"],\n        gen_params[\"max_new_tokens\"],\n        worker_addr,\n    )\n\n    if error_check_ret is not None:\n        return error_check_ret\n\n    gen_params[\"max_new_tokens\"] = max_new_tokens\n\n    if request.stream:\n        generator = chat_completion_stream_generator(\n            request.model, gen_params, request.n, worker_addr\n        )\n        return StreamingResponse(generator, media_type=\"text/event-stream\")\n\n    choices = []\n    chat_completions = []\n    for i in range(request.n):\n        content = asyncio.create_task(generate_completion(gen_params, worker_addr))\n        chat_completions.append(content)\n    try:\n        all_tasks = await asyncio.gather(*chat_completions)\n    except Exception as e:\n        return create_error_response(ErrorCode.INTERNAL_ERROR, str(e))\n    usage = UsageInfo()\n    for i, content in enumerate(all_tasks):\n        if isinstance(content, str):\n            content = json.loads(content)\n\n        if content[\"error_code\"] != 0:\n            return create_error_response(content[\"error_code\"], content[\"text\"])\n        choices.append(\n            ChatCompletionResponseChoice(\n                index=i,\n                message=ChatMessage(role=\"assistant\", content=content[\"text\"]),\n                finish_reason=content.get(\"finish_reason\", \"stop\"),\n            )\n        )\n        if \"usage\" in content:\n            task_usage = UsageInfo.model_validate(content[\"usage\"])\n            for usage_key, usage_value in task_usage.model_dump().items():\n                setattr(usage, usage_key, getattr(usage, usage_key) + usage_value)\n\n    return ChatCompletionResponse(model=request.model, choices=choices, usage=usage)\n\n\nasync def chat_completion_stream_generator(\n    model_name: str, gen_params: Dict[str, Any], n: int, worker_addr: str\n) -> Generator[str, Any, None]:\n    \"\"\"\n    Event stream format:\n    https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format\n    \"\"\"\n    id = f\"chatcmpl-{shortuuid.random()}\"\n    finish_stream_events = []\n    for i in range(n):\n        # First chunk with role\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=i,\n            delta=DeltaMessage(role=\"assistant\"),\n            finish_reason=None,\n        )\n        chunk = ChatCompletionStreamResponse(\n            id=id, choices=[choice_data], model=model_name\n        )\n        yield f\"data: {chunk.model_dump_json(exclude_unset=True)}\\n\\n\"\n\n        previous_text = \"\"\n        async for content in generate_completion_stream(gen_params, worker_addr):\n            if content[\"error_code\"] != 0:\n                yield f\"data: {json.dumps(content, ensure_ascii=False)}\\n\\n\"\n                yield \"data: [DONE]\\n\\n\"\n                return\n            decoded_unicode = content[\"text\"].replace(\"\\ufffd\", \"\")\n            delta_text = decoded_unicode[len(previous_text) :]\n            previous_text = (\n                decoded_unicode\n                if len(decoded_unicode) > len(previous_text)\n                else previous_text\n            )\n\n            if len(delta_text) == 0:\n                delta_text = None\n            choice_data = ChatCompletionResponseStreamChoice(\n                index=i,\n                delta=DeltaMessage(content=delta_text),\n                finish_reason=content.get(\"finish_reason\", None),\n            )\n            chunk = ChatCompletionStreamResponse(\n                id=id, choices=[choice_data], model=model_name\n            )\n            if delta_text is None:\n                if content.get(\"finish_reason\", None) is not None:\n                    finish_stream_events.append(chunk)\n                continue\n            yield f\"data: {chunk.model_dump_json(exclude_unset=True)}\\n\\n\"\n    # There is not \"content\" field in the last delta message, so exclude_none to exclude field \"content\".\n    for finish_chunk in finish_stream_events:\n        yield f\"data: {finish_chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n    yield \"data: [DONE]\\n\\n\"\n\n\n@app.post(\"/v1/completions\", dependencies=[Depends(check_api_key)])\nasync def create_completion(request: CompletionRequest):\n    error_check_ret = await check_model(request)\n    if error_check_ret is not None:\n        return error_check_ret\n    error_check_ret = check_requests(request)\n    if error_check_ret is not None:\n        return error_check_ret\n\n    request.prompt = process_input(request.model, request.prompt)\n\n    worker_addr = await get_worker_address(request.model)\n    for text in request.prompt:\n        max_tokens, error_check_ret = await check_length(\n            request, text, request.max_tokens, worker_addr\n        )\n        if error_check_ret is not None:\n            return error_check_ret\n\n        if isinstance(max_tokens, int) and max_tokens < request.max_tokens:\n            request.max_tokens = max_tokens\n\n    if request.stream:\n        generator = generate_completion_stream_generator(\n            request, request.n, worker_addr\n        )\n        return StreamingResponse(generator, media_type=\"text/event-stream\")\n    else:\n        text_completions = []\n        for text in request.prompt:\n            gen_params = await get_gen_params(\n                request.model,\n                worker_addr,\n                text,\n                temperature=request.temperature,\n                top_p=request.top_p,\n                top_k=request.top_k,\n                frequency_penalty=request.frequency_penalty,\n                presence_penalty=request.presence_penalty,\n                max_tokens=request.max_tokens,\n                logprobs=request.logprobs,\n                echo=request.echo,\n                stop=request.stop,\n                best_of=request.best_of,\n                use_beam_search=request.use_beam_search,\n            )\n            for i in range(request.n):\n                content = asyncio.create_task(\n                    generate_completion(gen_params, worker_addr)\n                )\n                text_completions.append(content)\n\n        try:\n            all_tasks = await asyncio.gather(*text_completions)\n        except Exception as e:\n            return create_error_response(ErrorCode.INTERNAL_ERROR, str(e))\n\n        choices = []\n        usage = UsageInfo()\n        for i, content in enumerate(all_tasks):\n            if content[\"error_code\"] != 0:\n                return create_error_response(content[\"error_code\"], content[\"text\"])\n            choices.append(\n                CompletionResponseChoice(\n                    index=i,\n                    text=content[\"text\"],\n                    logprobs=create_openai_logprobs(content.get(\"logprobs\", None)),\n                    finish_reason=content.get(\"finish_reason\", \"stop\"),\n                )\n            )\n            task_usage = UsageInfo.model_validate(content[\"usage\"])\n            for usage_key, usage_value in task_usage.model_dump().items():\n                setattr(usage, usage_key, getattr(usage, usage_key) + usage_value)\n\n        return CompletionResponse(\n            model=request.model, choices=choices, usage=UsageInfo.model_validate(usage)\n        )\n\n\nasync def generate_completion_stream_generator(\n    request: CompletionRequest, n: int, worker_addr: str\n):\n    model_name = request.model\n    id = f\"cmpl-{shortuuid.random()}\"\n    finish_stream_events = []\n    for text in request.prompt:\n        for i in range(n):\n            previous_text = \"\"\n            gen_params = await get_gen_params(\n                request.model,\n                worker_addr,\n                text,\n                temperature=request.temperature,\n                top_p=request.top_p,\n                top_k=request.top_k,\n                presence_penalty=request.presence_penalty,\n                frequency_penalty=request.frequency_penalty,\n                max_tokens=request.max_tokens,\n                logprobs=request.logprobs,\n                echo=request.echo,\n                stop=request.stop,\n            )\n            async for content in generate_completion_stream(gen_params, worker_addr):\n                if content[\"error_code\"] != 0:\n                    yield f\"data: {json.dumps(content, ensure_ascii=False)}\\n\\n\"\n                    yield \"data: [DONE]\\n\\n\"\n                    return\n                decoded_unicode = content[\"text\"].replace(\"\\ufffd\", \"\")\n                delta_text = decoded_unicode[len(previous_text) :]\n                previous_text = (\n                    decoded_unicode\n                    if len(decoded_unicode) > len(previous_text)\n                    else previous_text\n                )\n                # todo: index is not apparent\n                choice_data = CompletionResponseStreamChoice(\n                    index=i,\n                    text=delta_text,\n                    logprobs=create_openai_logprobs(content.get(\"logprobs\", None)),\n                    finish_reason=content.get(\"finish_reason\", None),\n                )\n                chunk = CompletionStreamResponse(\n                    id=id,\n                    object=\"text_completion\",\n                    choices=[choice_data],\n                    model=model_name,\n                )\n                if len(delta_text) == 0:\n                    if content.get(\"finish_reason\", None) is not None:\n                        finish_stream_events.append(chunk)\n                    continue\n                yield f\"data: {chunk.model_dump_json(exclude_unset=True)}\\n\\n\"\n    # There is not \"content\" field in the last delta message, so exclude_none to exclude field \"content\".\n    for finish_chunk in finish_stream_events:\n        yield f\"data: {finish_chunk.model_dump_json(exclude_unset=True)}\\n\\n\"\n    yield \"data: [DONE]\\n\\n\"\n\n\nasync def generate_completion_stream(payload: Dict[str, Any], worker_addr: str):\n    controller_address = app_settings.controller_address\n    async with httpx.AsyncClient() as client:\n        delimiter = b\"\\0\"\n        async with client.stream(\n            \"POST\",\n            worker_addr + \"/worker_generate_stream\",\n            headers=headers,\n            json=payload,\n            timeout=WORKER_API_TIMEOUT,\n        ) as response:\n            # content = await response.aread()\n            buffer = b\"\"\n            async for raw_chunk in response.aiter_raw():\n                buffer += raw_chunk\n                while (chunk_end := buffer.find(delimiter)) >= 0:\n                    chunk, buffer = buffer[:chunk_end], buffer[chunk_end + 1 :]\n                    if not chunk:\n                        continue\n                    yield json.loads(chunk.decode())\n\n\nasync def generate_completion(payload: Dict[str, Any], worker_addr: str):\n    return await fetch_remote(worker_addr + \"/worker_generate\", payload, \"\")\n\n\n@app.post(\"/v1/embeddings\", dependencies=[Depends(check_api_key)])\n@app.post(\"/v1/engines/{model_name}/embeddings\", dependencies=[Depends(check_api_key)])\nasync def create_embeddings(request: EmbeddingsRequest, model_name: str = None):\n    \"\"\"Creates embeddings for the text\"\"\"\n    if request.model is None:\n        request.model = model_name\n    error_check_ret = await check_model(request)\n    if error_check_ret is not None:\n        return error_check_ret\n\n    request.input = process_input(request.model, request.input)\n\n    data = []\n    token_num = 0\n    batch_size = WORKER_API_EMBEDDING_BATCH_SIZE\n    batches = [\n        request.input[i : min(i + batch_size, len(request.input))]\n        for i in range(0, len(request.input), batch_size)\n    ]\n    for num_batch, batch in enumerate(batches):\n        payload = {\n            \"model\": request.model,\n            \"input\": batch,\n            \"encoding_format\": request.encoding_format,\n        }\n        embedding = await get_embedding(payload)\n        if \"error_code\" in embedding and embedding[\"error_code\"] != 0:\n            return create_error_response(embedding[\"error_code\"], embedding[\"text\"])\n        data += [\n            {\n                \"object\": \"embedding\",\n                \"embedding\": emb,\n                \"index\": num_batch * batch_size + i,\n            }\n            for i, emb in enumerate(embedding[\"embedding\"])\n        ]\n        token_num += embedding[\"token_num\"]\n    return EmbeddingsResponse(\n        data=data,\n        model=request.model,\n        usage=UsageInfo(\n            prompt_tokens=token_num,\n            total_tokens=token_num,\n            completion_tokens=None,\n        ),\n    ).model_dump(exclude_none=True)\n\n\nasync def get_embedding(payload: Dict[str, Any]):\n    controller_address = app_settings.controller_address\n    model_name = payload[\"model\"]\n    worker_addr = await get_worker_address(model_name)\n\n    embedding = await fetch_remote(worker_addr + \"/worker_get_embeddings\", payload)\n    return json.loads(embedding)\n\n\n### GENERAL API - NOT OPENAI COMPATIBLE ###\n\n\n@app.post(\"/api/v1/token_check\")\nasync def count_tokens(request: APITokenCheckRequest):\n    \"\"\"\n    Checks the token count for each message in your list\n    This is not part of the OpenAI API spec.\n    \"\"\"\n    checkedList = []\n    for item in request.prompts:\n        worker_addr = await get_worker_address(item.model)\n\n        context_len = await fetch_remote(\n            worker_addr + \"/model_details\",\n            {\"prompt\": item.prompt, \"model\": item.model},\n            \"context_length\",\n        )\n\n        token_num = await fetch_remote(\n            worker_addr + \"/count_token\",\n            {\"prompt\": item.prompt, \"model\": item.model},\n            \"count\",\n        )\n\n        can_fit = True\n        if token_num + item.max_tokens > context_len:\n            can_fit = False\n\n        checkedList.append(\n            APITokenCheckResponseItem(\n                fits=can_fit, contextLength=context_len, tokenCount=token_num\n            )\n        )\n\n    return APITokenCheckResponse(prompts=checkedList)\n\n\n@app.post(\"/api/v1/chat/completions\")\nasync def create_chat_completion(request: APIChatCompletionRequest):\n    \"\"\"Creates a completion for the chat message\"\"\"\n    error_check_ret = await check_model(request)\n    if error_check_ret is not None:\n        return error_check_ret\n    error_check_ret = check_requests(request)\n    if error_check_ret is not None:\n        return error_check_ret\n\n    worker_addr = await get_worker_address(request.model)\n\n    gen_params = await get_gen_params(\n        request.model,\n        worker_addr,\n        request.messages,\n        temperature=request.temperature,\n        top_p=request.top_p,\n        top_k=request.top_k,\n        presence_penalty=request.presence_penalty,\n        frequency_penalty=request.frequency_penalty,\n        max_tokens=request.max_tokens,\n        echo=False,\n        stop=request.stop,\n    )\n\n    if request.repetition_penalty is not None:\n        gen_params[\"repetition_penalty\"] = request.repetition_penalty\n\n    max_new_tokens, error_check_ret = await check_length(\n        request,\n        gen_params[\"prompt\"],\n        gen_params[\"max_new_tokens\"],\n        worker_addr,\n    )\n\n    if error_check_ret is not None:\n        return error_check_ret\n\n    gen_params[\"max_new_tokens\"] = max_new_tokens\n\n    if request.stream:\n        generator = chat_completion_stream_generator(\n            request.model, gen_params, request.n, worker_addr\n        )\n        return StreamingResponse(generator, media_type=\"text/event-stream\")\n\n    choices = []\n    chat_completions = []\n    for i in range(request.n):\n        content = asyncio.create_task(generate_completion(gen_params, worker_addr))\n        chat_completions.append(content)\n    try:\n        all_tasks = await asyncio.gather(*chat_completions)\n    except Exception as e:\n        return create_error_response(ErrorCode.INTERNAL_ERROR, str(e))\n    usage = UsageInfo()\n    for i, content in enumerate(all_tasks):\n        if content[\"error_code\"] != 0:\n            return create_error_response(content[\"error_code\"], content[\"text\"])\n        choices.append(\n            ChatCompletionResponseChoice(\n                index=i,\n                message=ChatMessage(role=\"assistant\", content=content[\"text\"]),\n                finish_reason=content.get(\"finish_reason\", \"stop\"),\n            )\n        )\n        task_usage = UsageInfo.model_validate(content[\"usage\"])\n        for usage_key, usage_value in task_usage.model_dump().items():\n            setattr(usage, usage_key, getattr(usage, usage_key) + usage_value)\n\n    return ChatCompletionResponse(model=request.model, choices=choices, usage=usage)\n\n\n### END GENERAL API - NOT OPENAI COMPATIBLE ###\n\n\ndef create_openai_api_server():\n    parser = argparse.ArgumentParser(\n        description=\"FastChat ChatGPT-Compatible RESTful API server.\"\n    )\n    parser.add_argument(\"--host\", type=str, default=\"localhost\", help=\"host name\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"port number\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=json.loads, default=[\"*\"], help=\"allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=json.loads, default=[\"*\"], help=\"allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=json.loads, default=[\"*\"], help=\"allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-keys\",\n        type=lambda s: s.split(\",\"),\n        help=\"Optional list of comma separated API keys\",\n    )\n    parser.add_argument(\n        \"--ssl\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"Enable SSL. Requires OS Environment variables 'SSL_KEYFILE' and 'SSL_CERTFILE'.\",\n    )\n    args = parser.parse_args()\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    app_settings.controller_address = args.controller_address\n    app_settings.api_keys = args.api_keys\n\n    logger.info(f\"args: {args}\")\n    return args\n\n\nif __name__ == \"__main__\":\n    args = create_openai_api_server()\n    if args.ssl:\n        uvicorn.run(\n            app,\n            host=args.host,\n            port=args.port,\n            log_level=\"info\",\n            ssl_keyfile=os.environ[\"SSL_KEYFILE\"],\n            ssl_certfile=os.environ[\"SSL_CERTFILE\"],\n        )\n    else:\n        uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n", "fastchat/serve/multi_model_worker.py": "\"\"\"\nA multi-model worker that contains multiple sub-works one for each model.  This\nsupports running a list of models on the same machine so that they can\n(potentially) share the same background weights.\n\nEach model can have one or more model names.\n\nThis multi-model worker assumes the models shares some underlying weights and\nthus reports the combined queue lengths for health checks.\n\nWe recommend using this with multiple Peft models (with `peft` in the name)\nwhere all Peft models are trained on the exact same base model.\n\"\"\"\nimport argparse\nimport asyncio\nimport dataclasses\nimport logging\nimport json\nimport os\nimport time\nfrom typing import List, Union\nimport threading\nimport uuid\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse, JSONResponse\nimport requests\n\ntry:\n    from transformers import (\n        AutoTokenizer,\n        AutoModelForCausalLM,\n        LlamaTokenizer,\n        AutoModel,\n    )\nexcept ImportError:\n    from transformers import (\n        AutoTokenizer,\n        AutoModelForCausalLM,\n        LLaMATokenizer,\n        AutoModel,\n    )\nimport torch\nimport torch.nn.functional as F\nimport uvicorn\n\nfrom fastchat.constants import WORKER_HEART_BEAT_INTERVAL, ErrorCode, SERVER_ERROR_MSG\nfrom fastchat.model.model_adapter import (\n    load_model,\n    add_model_args,\n    get_conversation_template,\n)\nfrom fastchat.model.model_chatglm import generate_stream_chatglm\nfrom fastchat.model.model_falcon import generate_stream_falcon\nfrom fastchat.model.model_codet5p import generate_stream_codet5p\nfrom fastchat.modules.gptq import GptqConfig\nfrom fastchat.modules.exllama import ExllamaConfig\nfrom fastchat.modules.xfastertransformer import XftConfig\nfrom fastchat.serve.inference import generate_stream\nfrom fastchat.serve.model_worker import ModelWorker, worker_id, logger\nfrom fastchat.utils import build_logger, pretty_print_semaphore, get_context_length\n\n\n# We store both the underlying workers and a mapping from their model names to\n# the worker instance.  This makes it easy to fetch the appropriate worker for\n# each API call.\nworkers = []\nworker_map = {}\napp = FastAPI()\n\n\ndef release_worker_semaphore():\n    workers[0].semaphore.release()\n\n\ndef acquire_worker_semaphore():\n    if workers[0].semaphore is None:\n        # Share the same semaphore for all workers because\n        # all workers share the same GPU.\n        semaphore = asyncio.Semaphore(workers[0].limit_worker_concurrency)\n        for w in workers:\n            w.semaphore = semaphore\n    return workers[0].semaphore.acquire()\n\n\ndef create_background_tasks():\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(release_worker_semaphore)\n    return background_tasks\n\n\n# Note: for all the calls below, we make a hard assumption that the caller\n# includes the model name in the payload, otherwise we can't figure out which\n# underlying sub-worker to call.\n\n\n@app.post(\"/worker_generate_stream\")\nasync def api_generate_stream(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    worker = worker_map[params[\"model\"]]\n    generator = worker.generate_stream_gate(params)\n    background_tasks = create_background_tasks()\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate\")\nasync def api_generate(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    worker = worker_map[params[\"model\"]]\n    output = worker.generate_gate(params)\n    release_worker_semaphore()\n    return JSONResponse(output)\n\n\n@app.post(\"/worker_get_embeddings\")\nasync def api_get_embeddings(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    worker = worker_map[params[\"model\"]]\n    embedding = worker.get_embeddings(params)\n    background_tasks = create_background_tasks()\n    return JSONResponse(content=embedding, background=background_tasks)\n\n\n@app.post(\"/worker_get_status\")\nasync def api_get_status(request: Request):\n    return {\n        \"model_names\": [m for w in workers for m in w.model_names],\n        \"speed\": 1,\n        \"queue_length\": sum([w.get_queue_length() for w in workers]),\n    }\n\n\n@app.post(\"/count_token\")\nasync def api_count_token(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    return worker.count_token(params)\n\n\n@app.post(\"/worker_get_conv_template\")\nasync def api_get_conv(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    return worker.get_conv_template()\n\n\n@app.post(\"/model_details\")\nasync def api_model_details(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    return {\"context_length\": worker.context_len}\n\n\ndef create_multi_model_worker():\n    # Note: Ensure we resolve arg conflicts.  We let `add_model_args` add MOST\n    # of the model args but we'll override one to have an append action that\n    # supports multiple values.\n    parser = argparse.ArgumentParser(conflict_handler=\"resolve\")\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    add_model_args(parser)\n    # Override the model path to be repeated and align it with model names.\n    parser.add_argument(\n        \"--model-path\",\n        type=str,\n        default=[],\n        action=\"append\",\n        help=\"One or more paths to model weights to load. This can be a local folder or a Hugging Face repo ID.\",\n    )\n    parser.add_argument(\n        \"--model-names\",\n        type=lambda s: s.split(\",\"),\n        action=\"append\",\n        help=\"One or more model names.  Values must be aligned with `--model-path` values.\",\n    )\n    parser.add_argument(\n        \"--conv-template\",\n        type=str,\n        default=None,\n        action=\"append\",\n        help=\"Conversation prompt template. Values must be aligned with `--model-path` values. If only one value is provided, it will be repeated for all models.\",\n    )\n    parser.add_argument(\"--limit-worker-concurrency\", type=int, default=5)\n    parser.add_argument(\"--stream-interval\", type=int, default=2)\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n    parser.add_argument(\n        \"--ssl\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"Enable SSL. Requires OS Environment variables 'SSL_KEYFILE' and 'SSL_CERTFILE'.\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    if args.gpus:\n        if len(args.gpus.split(\",\")) < args.num_gpus:\n            raise ValueError(\n                f\"Larger --num-gpus ({args.num_gpus}) than --gpus {args.gpus}!\"\n            )\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n    gptq_config = GptqConfig(\n        ckpt=args.gptq_ckpt or args.model_path,\n        wbits=args.gptq_wbits,\n        groupsize=args.gptq_groupsize,\n        act_order=args.gptq_act_order,\n    )\n    if args.enable_exllama:\n        exllama_config = ExllamaConfig(\n            max_seq_len=args.exllama_max_seq_len,\n            gpu_split=args.exllama_gpu_split,\n            cache_8bit=args.exllama_cache_8bit,\n        )\n    else:\n        exllama_config = None\n    if args.enable_xft:\n        xft_config = XftConfig(\n            max_seq_len=args.xft_max_seq_len,\n            data_type=args.xft_dtype,\n        )\n        if args.device != \"cpu\":\n            print(\"xFasterTransformer now is only support CPUs. Reset device to CPU\")\n            args.device = \"cpu\"\n    else:\n        xft_config = None\n\n    if args.model_names is None:\n        args.model_names = [[x.split(\"/\")[-1]] for x in args.model_path]\n\n    if args.conv_template is None:\n        args.conv_template = [None] * len(args.model_path)\n    elif len(args.conv_template) == 1:  # Repeat the same template\n        args.conv_template = args.conv_template * len(args.model_path)\n\n    # Launch all workers\n    workers = []\n    for conv_template, model_path, model_names in zip(\n        args.conv_template, args.model_path, args.model_names\n    ):\n        w = ModelWorker(\n            args.controller_address,\n            args.worker_address,\n            worker_id,\n            model_path,\n            model_names,\n            args.limit_worker_concurrency,\n            args.no_register,\n            device=args.device,\n            num_gpus=args.num_gpus,\n            max_gpu_memory=args.max_gpu_memory,\n            load_8bit=args.load_8bit,\n            cpu_offloading=args.cpu_offloading,\n            gptq_config=gptq_config,\n            exllama_config=exllama_config,\n            xft_config=xft_config,\n            stream_interval=args.stream_interval,\n            conv_template=conv_template,\n        )\n        workers.append(w)\n        for model_name in model_names:\n            worker_map[model_name] = w\n\n    # Register all models\n    url = args.controller_address + \"/register_worker\"\n    data = {\n        \"worker_name\": workers[0].worker_addr,\n        \"check_heart_beat\": not args.no_register,\n        \"worker_status\": {\n            \"model_names\": [m for w in workers for m in w.model_names],\n            \"speed\": 1,\n            \"queue_length\": sum([w.get_queue_length() for w in workers]),\n        },\n    }\n    r = requests.post(url, json=data)\n    assert r.status_code == 200\n\n    return args, workers\n\n\nif __name__ == \"__main__\":\n    args, workers = create_multi_model_worker()\n    if args.ssl:\n        uvicorn.run(\n            app,\n            host=args.host,\n            port=args.port,\n            log_level=\"info\",\n            ssl_keyfile=os.environ[\"SSL_KEYFILE\"],\n            ssl_certfile=os.environ[\"SSL_CERTFILE\"],\n        )\n    else:\n        uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n", "fastchat/serve/sglang_worker.py": "\"\"\"\nA model worker that executes the model based on SGLang.\n\nUsage:\npython3 -m fastchat.serve.sglang_worker --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000 --worker-address http://localhost:30000\n\"\"\"\n\nimport argparse\nimport asyncio\nimport json\nimport multiprocessing\nfrom typing import List\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse, JSONResponse\nimport uvicorn\nimport sglang as sgl\nfrom sglang.srt.hf_transformers_utils import get_tokenizer, get_config\nfrom sglang.srt.utils import load_image, is_multimodal_model\n\nfrom fastchat.conversation import IMAGE_PLACEHOLDER_STR\nfrom fastchat.constants import ErrorCode, SERVER_ERROR_MSG\nfrom fastchat.serve.base_model_worker import BaseModelWorker\nfrom fastchat.serve.model_worker import (\n    logger,\n    worker_id,\n)\nfrom fastchat.utils import get_context_length, is_partial_stop\n\napp = FastAPI()\n\n\n@sgl.function\ndef pipeline(s, prompt, max_tokens):\n    for p in prompt:\n        if isinstance(p, str):\n            s += p\n        else:\n            s += sgl.image(p)\n    s += sgl.gen(\"response\", max_tokens=max_tokens)\n\n\nclass SGLWorker(BaseModelWorker):\n    def __init__(\n        self,\n        controller_addr: str,\n        worker_addr: str,\n        worker_id: str,\n        model_path: str,\n        tokenizer_path: str,\n        model_names: List[str],\n        limit_worker_concurrency: int,\n        no_register: bool,\n        conv_template: str,\n        runtime: sgl.Runtime,\n        trust_remote_code: bool,\n    ):\n        super().__init__(\n            controller_addr,\n            worker_addr,\n            worker_id,\n            model_path,\n            model_names,\n            limit_worker_concurrency,\n            conv_template,\n            is_multimodal_model(model_path),\n        )\n\n        logger.info(\n            f\"Loading the model {self.model_names} on worker {worker_id}, worker type: SGLang worker...\"\n        )\n\n        self.tokenizer = get_tokenizer(tokenizer_path)\n        self.context_len = get_context_length(\n            get_config(model_path, trust_remote_code=trust_remote_code)\n        )\n\n        if not no_register:\n            self.init_heart_beat()\n\n    async def generate_stream(self, params):\n        self.call_ct += 1\n\n        prompt = params.pop(\"prompt\")\n        images = params.get(\"images\", [])\n        temperature = float(params.get(\"temperature\", 1.0))\n        top_p = float(params.get(\"top_p\", 1.0))\n        top_k = params.get(\"top_k\", -1.0)\n        frequency_penalty = float(params.get(\"frequency_penalty\", 0.0))\n        presence_penalty = float(params.get(\"presence_penalty\", 0.0))\n        max_new_tokens = params.get(\"max_new_tokens\", 256)\n        stop_str = params.get(\"stop\", None)\n        stop_token_ids = params.get(\"stop_token_ids\", None) or []\n        echo = params.get(\"echo\", True)\n\n        # Handle stop_str\n        stop = []\n        if isinstance(stop_str, str) and stop_str != \"\":\n            stop.append(stop_str)\n        elif isinstance(stop_str, list) and stop_str != []:\n            stop.extend(stop_str)\n\n        for tid in stop_token_ids:\n            if tid is not None:\n                s = self.tokenizer.decode(tid)\n                if s != \"\":\n                    stop.append(s)\n\n        # make sampling params for sgl.gen\n        top_p = max(top_p, 1e-5)\n        if temperature <= 1e-5:\n            top_p = 1.0\n\n        # split prompt by image token\n        split_prompt = prompt.split(IMAGE_PLACEHOLDER_STR)\n        if prompt.count(IMAGE_PLACEHOLDER_STR) != len(images):\n            raise ValueError(\n                \"The number of images passed in does not match the number of <image> tokens in the prompt!\"\n            )\n        prompt = []\n        for i in range(len(split_prompt)):\n            prompt.append(split_prompt[i])\n            if i < len(images):\n                prompt[-1] = prompt[-1].strip()\n                prompt.append(load_image(images[i]))\n\n        state = pipeline.run(\n            prompt,\n            max_new_tokens,\n            stop=stop,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            stream=True,\n        )\n\n        entire_output = prompt if echo else \"\"\n        async for out, meta_info in state.text_async_iter(\n            var_name=\"response\", return_meta_data=True\n        ):\n            partial_stop = any(is_partial_stop(out, i) for i in stop)\n\n            # prevent yielding partial stop sequence\n            if partial_stop:\n                continue\n\n            entire_output += out\n            prompt_tokens = meta_info[\"prompt_tokens\"]\n            completion_tokens = meta_info[\"completion_tokens\"]\n\n            ret = {\n                \"text\": entire_output,\n                \"usage\": {\n                    \"prompt_tokens\": prompt_tokens,\n                    \"completion_tokens\": completion_tokens,\n                    \"total_tokens\": prompt_tokens + completion_tokens,\n                },\n                \"error_code\": 0,\n            }\n            yield ret\n\n    async def generate_stream_gate(self, params):\n        try:\n            async for ret in self.generate_stream(params):\n                yield json.dumps(ret).encode() + b\"\\0\"\n        except (ValueError, RuntimeError) as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.INTERNAL_ERROR,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n\n    async def generate_gate(self, params):\n        async for x in self.generate_stream_gate(params):\n            pass\n        return json.loads(x[:-1].decode())\n\n\ndef release_worker_semaphore():\n    worker.semaphore.release()\n\n\ndef acquire_worker_semaphore():\n    if worker.semaphore is None:\n        worker.semaphore = asyncio.Semaphore(worker.limit_worker_concurrency)\n    return worker.semaphore.acquire()\n\n\ndef create_background_tasks():\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(release_worker_semaphore)\n    return background_tasks\n\n\n@app.post(\"/worker_generate_stream\")\nasync def api_generate_stream(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    generator = worker.generate_stream_gate(params)\n    background_tasks = create_background_tasks()\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate\")\nasync def api_generate(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    output = await worker.generate_gate(params)\n    release_worker_semaphore()\n    return JSONResponse(output)\n\n\n@app.post(\"/worker_get_status\")\nasync def api_get_status(request: Request):\n    return worker.get_status()\n\n\n@app.post(\"/count_token\")\nasync def api_count_token(request: Request):\n    params = await request.json()\n    return worker.count_token(params)\n\n\n@app.post(\"/worker_get_conv_template\")\nasync def api_get_conv(request: Request):\n    return worker.get_conv_template()\n\n\n@app.post(\"/model_details\")\nasync def api_model_details(request: Request):\n    return {\"context_length\": worker.context_len}\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\"--model-path\", type=str, default=\"lmsys/vicuna-7b-v1.5\")\n    parser.add_argument(\"--tokenizer-path\", type=str, default=\"\")\n    parser.add_argument(\n        \"--model-names\",\n        type=lambda s: s.split(\",\"),\n        help=\"Optional display comma separated names\",\n    )\n    parser.add_argument(\"--limit-worker-concurrency\", type=int, default=1024)\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n    parser.add_argument(\"--num-gpus\", type=int, default=1)\n    parser.add_argument(\n        \"--conv-template\", type=str, default=None, help=\"Conversation prompt template.\"\n    )\n    parser.add_argument(\n        \"--trust-remote-code\",\n        action=\"store_false\",\n        default=True,\n        help=\"Trust remote code (e.g., from HuggingFace) when\"\n        \"downloading the model and tokenizer.\",\n    )\n    parser.add_argument(\n        \"--mem-fraction-static\",\n        type=float,\n        default=0.9,\n        help=\"The ratio (between 0 and 1) of GPU memory to\"\n        \"reserve for the model weights, activations, and KV cache. Higher\"\n        \"values will increase the KV cache size and thus improve the model's\"\n        \"throughput. However, if the value is too high, it may cause out-of-\"\n        \"memory (OOM) errors.\",\n    )\n    parser.add_argument(\n        \"--multimodal\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"Register this worker as serving a multimodal model.\",\n    )\n\n    args = parser.parse_args()\n\n    args.tp_size = args.num_gpus if args.num_gpus > 1 else 1\n    args.tokenizer_path = (\n        args.model_path if args.tokenizer_path == \"\" else args.tokenizer_path\n    )\n\n    multiprocessing.set_start_method(\"spawn\", force=True)\n    runtime = sgl.Runtime(\n        model_path=args.model_path,\n        tokenizer_path=args.tokenizer_path,\n        trust_remote_code=args.trust_remote_code,\n        mem_fraction_static=args.mem_fraction_static,\n        tp_size=args.tp_size,\n        log_level=\"info\",\n    )\n    sgl.set_default_backend(runtime)\n\n    worker = SGLWorker(\n        args.controller_address,\n        args.worker_address,\n        worker_id,\n        args.model_path,\n        args.tokenizer_path,\n        args.model_names,\n        args.limit_worker_concurrency,\n        args.no_register,\n        args.conv_template,\n        runtime,\n        args.trust_remote_code,\n    )\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n", "fastchat/serve/inference.py": "\"\"\"Inference for FastChat models.\"\"\"\nimport abc\nimport gc\nimport json\nimport math\nimport os\nimport sys\nimport time\nfrom typing import Iterable, Optional, Dict\nimport warnings\n\nimport psutil\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    LlamaTokenizer,\n    LlamaForCausalLM,\n    AutoModel,\n    AutoModelForSeq2SeqLM,\n    T5Tokenizer,\n    AutoConfig,\n)\nfrom transformers.generation.logits_process import (\n    LogitsProcessorList,\n    RepetitionPenaltyLogitsProcessor,\n    TemperatureLogitsWarper,\n    TopKLogitsWarper,\n    TopPLogitsWarper,\n)\n\nfrom fastchat.conversation import get_conv_template, SeparatorStyle\nfrom fastchat.model.model_adapter import (\n    load_model,\n    get_conversation_template,\n    get_generate_stream_function,\n)\nfrom fastchat.modules.awq import AWQConfig\nfrom fastchat.modules.gptq import GptqConfig\nfrom fastchat.modules.exllama import ExllamaConfig\nfrom fastchat.modules.xfastertransformer import XftConfig\nfrom fastchat.utils import is_partial_stop, is_sentence_complete, get_context_length\n\n\ndef prepare_logits_processor(\n    temperature: float, repetition_penalty: float, top_p: float, top_k: int\n) -> LogitsProcessorList:\n    processor_list = LogitsProcessorList()\n    # TemperatureLogitsWarper doesn't accept 0.0, 1.0 makes it a no-op so we skip two cases.\n    if temperature >= 1e-5 and temperature != 1.0:\n        processor_list.append(TemperatureLogitsWarper(temperature))\n    if repetition_penalty > 1.0:\n        processor_list.append(RepetitionPenaltyLogitsProcessor(repetition_penalty))\n    if 1e-8 <= top_p < 1.0:\n        processor_list.append(TopPLogitsWarper(top_p))\n    if top_k > 0:\n        processor_list.append(TopKLogitsWarper(top_k))\n    return processor_list\n\n\n@torch.inference_mode()\ndef generate_stream(\n    model,\n    tokenizer,\n    params: Dict,\n    device: str,\n    context_len: int,\n    stream_interval: int = 2,\n    judge_sent_end: bool = False,\n):\n    if hasattr(model, \"device\"):\n        device = model.device\n\n    # Read parameters\n    prompt = params[\"prompt\"]\n    len_prompt = len(prompt)\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    top_k = int(params.get(\"top_k\", -1))  # -1 means disable\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n    logprobs = params.get(\"logprobs\", None)  # FIXME: Support logprobs>1.\n    echo = bool(params.get(\"echo\", True))\n    stop_str = params.get(\"stop\", None)\n    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n    if tokenizer.eos_token_id not in stop_token_ids:\n        stop_token_ids.append(tokenizer.eos_token_id)\n\n    logits_processor = prepare_logits_processor(\n        temperature, repetition_penalty, top_p, top_k\n    )\n    input_ids = tokenizer(prompt).input_ids\n\n    if model.config.is_encoder_decoder:\n        max_src_len = context_len\n    else:  # truncate\n        max_src_len = context_len - max_new_tokens - 1\n\n    input_ids = input_ids[-max_src_len:]\n    output_ids = list(input_ids)\n    input_echo_len = len(input_ids)\n\n    if model.config.is_encoder_decoder:\n        if logprobs is not None:  # FIXME: Support logprobs for encoder-decoder models.\n            raise NotImplementedError\n        encoder_output = model.encoder(\n            input_ids=torch.as_tensor([input_ids], device=device)\n        )[0]\n        start_ids = torch.as_tensor(\n            [[model.generation_config.decoder_start_token_id]],\n            dtype=torch.int64,\n            device=device,\n        )\n    else:\n        start_ids = torch.as_tensor([input_ids], device=device)\n\n    past_key_values = out = None\n    token_logprobs = [None]  # The first token has no logprobs.\n    sent_interrupt = False\n    finish_reason = None\n    stopped = False\n    for i in range(max_new_tokens):\n        if i == 0:  # prefill\n            if model.config.is_encoder_decoder:\n                out = model.decoder(\n                    input_ids=start_ids,\n                    encoder_hidden_states=encoder_output,\n                    use_cache=True,\n                )\n                logits = model.lm_head(out[0])\n            else:\n                out = model(input_ids=start_ids, use_cache=True)\n                logits = out.logits\n            past_key_values = out.past_key_values\n\n            if logprobs is not None:\n                # Prefull logprobs for the prompt.\n                shift_input_ids = start_ids[..., 1:].contiguous()\n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_logits = torch.log_softmax(shift_logits, dim=-1).tolist()\n                for label_id, logit in zip(\n                    shift_input_ids[0].tolist(), shift_logits[0]\n                ):\n                    token_logprobs.append(logit[label_id])\n        else:  # decoding\n            if model.config.is_encoder_decoder:\n                out = model.decoder(\n                    input_ids=torch.as_tensor(\n                        [[token] if not sent_interrupt else output_ids],\n                        device=device,\n                    ),\n                    encoder_hidden_states=encoder_output,\n                    use_cache=True,\n                    past_key_values=past_key_values if not sent_interrupt else None,\n                )\n                sent_interrupt = False\n\n                logits = model.lm_head(out[0])\n            else:\n                out = model(\n                    input_ids=torch.as_tensor(\n                        [[token] if not sent_interrupt else output_ids],\n                        device=device,\n                    ),\n                    use_cache=True,\n                    past_key_values=past_key_values if not sent_interrupt else None,\n                )\n                sent_interrupt = False\n                logits = out.logits\n            past_key_values = out.past_key_values\n\n        if logits_processor:\n            if repetition_penalty > 1.0:\n                tmp_output_ids = torch.as_tensor([output_ids], device=logits.device)\n            else:\n                tmp_output_ids = None\n            last_token_logits = logits_processor(tmp_output_ids, logits[:, -1, :])[0]\n        else:\n            last_token_logits = logits[0, -1, :]\n\n        if device == \"mps\":\n            # Switch to CPU by avoiding some bugs in mps backend.\n            last_token_logits = last_token_logits.float().to(\"cpu\")\n\n        if temperature < 1e-5 or top_p < 1e-8:  # greedy\n            _, indices = torch.topk(last_token_logits, 2)\n            tokens = [int(index) for index in indices.tolist()]\n        else:\n            probs = torch.softmax(last_token_logits, dim=-1)\n            indices = torch.multinomial(probs, num_samples=2)\n            tokens = [int(token) for token in indices.tolist()]\n        token = tokens[0]\n        output_ids.append(token)\n        if logprobs is not None:\n            # Cannot use last_token_logits because logprobs is based on raw logits.\n            token_logprobs.append(\n                torch.log_softmax(logits[0, -1, :], dim=-1)[token].tolist()\n            )\n\n        if token in stop_token_ids:\n            stopped = True\n        else:\n            stopped = False\n\n        # Yield the output tokens\n        if i % stream_interval == 0 or i == max_new_tokens - 1 or stopped:\n            if echo:\n                tmp_output_ids = output_ids\n                rfind_start = len_prompt\n            else:\n                tmp_output_ids = output_ids[input_echo_len:]\n                rfind_start = 0\n\n            output = tokenizer.decode(\n                tmp_output_ids,\n                skip_special_tokens=True,\n                spaces_between_special_tokens=False,\n                clean_up_tokenization_spaces=True,\n            )\n            ret_logprobs = None\n            if logprobs is not None:\n                ret_logprobs = {\n                    \"text_offset\": [],\n                    \"tokens\": [\n                        tokenizer.decode(token)\n                        for token in (\n                            output_ids if echo else output_ids[input_echo_len:]\n                        )\n                    ],\n                    \"token_logprobs\": token_logprobs\n                    if echo\n                    else token_logprobs[input_echo_len:],\n                    \"top_logprobs\": [{}]\n                    * len(token_logprobs if echo else token_logprobs[input_echo_len:]),\n                }\n                # Compute text_offset\n                curr_pos = 0\n                for text in ret_logprobs[\"tokens\"]:\n                    ret_logprobs[\"text_offset\"].append(curr_pos)\n                    curr_pos += len(text)\n\n            # TODO: For the issue of incomplete sentences interrupting output, apply a patch and others can also modify it to a more elegant way\n            if judge_sent_end and stopped and not is_sentence_complete(output):\n                if len(tokens) > 1:\n                    token = tokens[1]\n                    output_ids[-1] = token\n                else:\n                    output_ids.pop()\n                stopped = False\n                sent_interrupt = True\n\n            partially_stopped = False\n            if stop_str:\n                if isinstance(stop_str, str):\n                    pos = output.rfind(stop_str, rfind_start)\n                    if pos != -1:\n                        output = output[:pos]\n                        stopped = True\n                    else:\n                        partially_stopped = is_partial_stop(output, stop_str)\n                elif isinstance(stop_str, Iterable):\n                    for each_stop in stop_str:\n                        pos = output.rfind(each_stop, rfind_start)\n                        if pos != -1:\n                            output = output[:pos]\n                            stopped = True\n                            break\n                        else:\n                            partially_stopped = is_partial_stop(output, each_stop)\n                            if partially_stopped:\n                                break\n                else:\n                    raise ValueError(\"Invalid stop field type.\")\n\n            # Prevent yielding partial stop sequence\n            if not partially_stopped:\n                yield {\n                    \"text\": output,\n                    \"logprobs\": ret_logprobs,\n                    \"usage\": {\n                        \"prompt_tokens\": input_echo_len,\n                        \"completion_tokens\": i,\n                        \"total_tokens\": input_echo_len + i,\n                    },\n                    \"finish_reason\": None,\n                }\n\n        if stopped:\n            break\n\n    # Finish stream event, which contains finish reason\n    else:\n        finish_reason = \"length\"\n\n    if stopped:\n        finish_reason = \"stop\"\n\n    yield {\n        \"text\": output,\n        \"logprobs\": ret_logprobs,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": finish_reason,\n    }\n\n    # Clean\n    del past_key_values, out\n    gc.collect()\n    torch.cuda.empty_cache()\n    if device == \"xpu\":\n        torch.xpu.empty_cache()\n    if device == \"npu\":\n        torch.npu.empty_cache()\n\n\nclass ChatIO(abc.ABC):\n    @abc.abstractmethod\n    def prompt_for_input(self, role: str) -> str:\n        \"\"\"Prompt for input from a role.\"\"\"\n\n    @abc.abstractmethod\n    def prompt_for_output(self, role: str):\n        \"\"\"Prompt for output from a role.\"\"\"\n\n    @abc.abstractmethod\n    def stream_output(self, output_stream):\n        \"\"\"Stream output.\"\"\"\n\n    @abc.abstractmethod\n    def print_output(self, text: str):\n        \"\"\"Print output.\"\"\"\n\n\ndef chat_loop(\n    model_path: str,\n    device: str,\n    num_gpus: int,\n    max_gpu_memory: str,\n    dtype: Optional[torch.dtype],\n    load_8bit: bool,\n    cpu_offloading: bool,\n    conv_template: Optional[str],\n    conv_system_msg: Optional[str],\n    temperature: float,\n    repetition_penalty: float,\n    max_new_tokens: int,\n    chatio: ChatIO,\n    gptq_config: Optional[GptqConfig] = None,\n    awq_config: Optional[AWQConfig] = None,\n    exllama_config: Optional[ExllamaConfig] = None,\n    xft_config: Optional[XftConfig] = None,\n    revision: str = \"main\",\n    judge_sent_end: bool = True,\n    debug: bool = True,\n    history: bool = True,\n):\n    # Model\n    model, tokenizer = load_model(\n        model_path,\n        device=device,\n        num_gpus=num_gpus,\n        max_gpu_memory=max_gpu_memory,\n        dtype=dtype,\n        load_8bit=load_8bit,\n        cpu_offloading=cpu_offloading,\n        gptq_config=gptq_config,\n        awq_config=awq_config,\n        exllama_config=exllama_config,\n        xft_config=xft_config,\n        revision=revision,\n        debug=debug,\n    )\n    generate_stream_func = get_generate_stream_function(model, model_path)\n\n    model_type = str(type(model)).lower()\n    is_t5 = \"t5\" in model_type\n    is_codet5p = \"codet5p\" in model_type\n    is_xft = \"xft\" in model_type\n\n    # Hardcode T5's default repetition penalty to be 1.2\n    if is_t5 and repetition_penalty == 1.0:\n        repetition_penalty = 1.2\n\n    # Set context length\n    context_len = get_context_length(model.config)\n\n    # Chat\n    def new_chat():\n        if conv_template:\n            conv = get_conv_template(conv_template)\n        else:\n            conv = get_conversation_template(model_path)\n        if conv_system_msg is not None:\n            conv.set_system_message(conv_system_msg)\n        return conv\n\n    def reload_conv(conv):\n        \"\"\"\n        Reprints the conversation from the start.\n        \"\"\"\n        for message in conv.messages[conv.offset :]:\n            chatio.prompt_for_output(message[0])\n            chatio.print_output(message[1])\n\n    conv = None\n\n    while True:\n        if not history or not conv:\n            conv = new_chat()\n\n        try:\n            inp = chatio.prompt_for_input(conv.roles[0])\n        except EOFError:\n            inp = \"\"\n\n        if inp == \"!!exit\" or not inp:\n            print(\"exit...\")\n            break\n        elif inp == \"!!reset\":\n            print(\"resetting...\")\n            conv = new_chat()\n            continue\n        elif inp == \"!!remove\":\n            print(\"removing last message...\")\n            if len(conv.messages) > conv.offset:\n                # Assistant\n                if conv.messages[-1][0] == conv.roles[1]:\n                    conv.messages.pop()\n                # User\n                if conv.messages[-1][0] == conv.roles[0]:\n                    conv.messages.pop()\n                reload_conv(conv)\n            else:\n                print(\"No messages to remove.\")\n            continue\n        elif inp == \"!!regen\":\n            print(\"regenerating last message...\")\n            if len(conv.messages) > conv.offset:\n                # Assistant\n                if conv.messages[-1][0] == conv.roles[1]:\n                    conv.messages.pop()\n                # User\n                if conv.messages[-1][0] == conv.roles[0]:\n                    reload_conv(conv)\n                    # Set inp to previous message\n                    inp = conv.messages.pop()[1]\n                else:\n                    # Shouldn't happen in normal circumstances\n                    print(\"No user message to regenerate from.\")\n                    continue\n            else:\n                print(\"No messages to regenerate.\")\n                continue\n        elif inp.startswith(\"!!save\"):\n            args = inp.split(\" \", 1)\n\n            if len(args) != 2:\n                print(\"usage: !!save <filename>\")\n                continue\n            else:\n                filename = args[1]\n\n            # Add .json if extension not present\n            if not \".\" in filename:\n                filename += \".json\"\n\n            print(\"saving...\", filename)\n            with open(filename, \"w\") as outfile:\n                json.dump(conv.dict(), outfile)\n            continue\n        elif inp.startswith(\"!!load\"):\n            args = inp.split(\" \", 1)\n\n            if len(args) != 2:\n                print(\"usage: !!load <filename>\")\n                continue\n            else:\n                filename = args[1]\n\n            # Check if file exists and add .json if needed\n            if not os.path.exists(filename):\n                if (not filename.endswith(\".json\")) and os.path.exists(\n                    filename + \".json\"\n                ):\n                    filename += \".json\"\n                else:\n                    print(\"file not found:\", filename)\n                    continue\n\n            print(\"loading...\", filename)\n            with open(filename, \"r\") as infile:\n                new_conv = json.load(infile)\n\n            conv = get_conv_template(new_conv[\"template_name\"])\n            conv.set_system_message(new_conv[\"system_message\"])\n            conv.messages = new_conv[\"messages\"]\n            reload_conv(conv)\n            continue\n\n        conv.append_message(conv.roles[0], inp)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n\n        if is_codet5p:  # codet5p is a code completion model.\n            prompt = inp\n\n        gen_params = {\n            \"model\": model_path,\n            \"prompt\": prompt,\n            \"temperature\": temperature,\n            \"repetition_penalty\": repetition_penalty,\n            \"max_new_tokens\": max_new_tokens,\n            \"stop\": conv.stop_str,\n            \"stop_token_ids\": conv.stop_token_ids,\n            \"echo\": False,\n        }\n\n        try:\n            chatio.prompt_for_output(conv.roles[1])\n            output_stream = generate_stream_func(\n                model,\n                tokenizer,\n                gen_params,\n                device,\n                context_len=context_len,\n                judge_sent_end=judge_sent_end,\n            )\n            t = time.time()\n            outputs = chatio.stream_output(output_stream)\n            duration = time.time() - t\n            conv.update_last_message(outputs.strip())\n\n            if debug:\n                num_tokens = len(tokenizer.encode(outputs))\n                msg = {\n                    \"conv_template\": conv.name,\n                    \"prompt\": prompt,\n                    \"outputs\": outputs,\n                    \"speed (token/s)\": round(num_tokens / duration, 2),\n                }\n                print(f\"\\n{msg}\\n\")\n\n        except KeyboardInterrupt:\n            print(\"stopped generation.\")\n            # If generation didn't finish\n            if conv.messages[-1][1] is None:\n                conv.messages.pop()\n                # Remove last user message, so there isn't a double up\n                if conv.messages[-1][0] == conv.roles[0]:\n                    conv.messages.pop()\n\n                reload_conv(conv)\n", "fastchat/serve/call_monitor.py": "import json\nimport os\nimport glob\nimport time\n\nfrom fastapi import FastAPI\nimport hashlib\nimport asyncio\n\nREFRESH_INTERVAL_SEC = 60\nLOG_DIR_LIST = []\n# LOG_DIR = \"/home/vicuna/tmp/test_env\"\n\n\nclass Monitor:\n    \"\"\"Monitor the number of calls to each model.\"\"\"\n\n    def __init__(self, log_dir_list: list):\n        self.log_dir_list = log_dir_list\n        self.model_call = {}\n        self.user_call = {}\n        self.model_call_limit_global = {\n            \"gpt-4-1106-preview\": 100,\n            \"gpt-4-0125-preview\": 100,\n        }\n        self.model_call_day_limit_per_user = {\n            \"gpt-4-1106-preview\": 5,\n            \"gpt-4-0125-preview\": 5,\n        }\n\n    async def update_stats(self, num_file=1) -> None:\n        while True:\n            # find the latest num_file log under log_dir\n            json_files = []\n            for log_dir in self.log_dir_list:\n                json_files_per_server = glob.glob(os.path.join(log_dir, \"*.json\"))\n                json_files_per_server.sort(key=os.path.getctime, reverse=True)\n                json_files += json_files_per_server[:num_file]\n            model_call = {}\n            user_call = {}\n            for json_file in json_files:\n                for line in open(json_file, \"r\", encoding=\"utf-8\"):\n                    obj = json.loads(line)\n                    if obj[\"type\"] != \"chat\":\n                        continue\n                    if obj[\"model\"] not in model_call:\n                        model_call[obj[\"model\"]] = []\n                    model_call[obj[\"model\"]].append(\n                        {\"tstamp\": obj[\"tstamp\"], \"user_id\": obj[\"ip\"]}\n                    )\n                    if obj[\"ip\"] not in user_call:\n                        user_call[obj[\"ip\"]] = []\n                    user_call[obj[\"ip\"]].append(\n                        {\"tstamp\": obj[\"tstamp\"], \"model\": obj[\"model\"]}\n                    )\n\n            self.model_call = model_call\n            self.model_call_stats_hour = self.get_model_call_stats(top_k=None)\n            self.model_call_stats_day = self.get_model_call_stats(\n                top_k=None, most_recent_min=24 * 60\n            )\n\n            self.user_call = user_call\n            self.user_call_stats_hour = self.get_user_call_stats(top_k=None)\n            self.user_call_stats_day = self.get_user_call_stats(\n                top_k=None, most_recent_min=24 * 60\n            )\n            await asyncio.sleep(REFRESH_INTERVAL_SEC)\n\n    def get_model_call_limit(self, model: str) -> int:\n        if model not in self.model_call_limit_global:\n            return -1\n        return self.model_call_limit_global[model]\n\n    def update_model_call_limit(self, model: str, limit: int) -> bool:\n        if model not in self.model_call_limit_global:\n            return False\n        self.model_call_limit_global[model] = limit\n        return True\n\n    def is_model_limit_reached(self, model: str) -> bool:\n        if model not in self.model_call_limit_global:\n            return False\n        if model not in self.model_call_stats_hour:\n            return False\n        # check if the model call limit is reached\n        if self.model_call_stats_hour[model] >= self.model_call_limit_global[model]:\n            return True\n        return False\n\n    def is_user_limit_reached(self, model: str, user_id: str) -> bool:\n        if model not in self.model_call_day_limit_per_user:\n            return False\n        if user_id not in self.user_call_stats_day:\n            return False\n        if model not in self.user_call_stats_day[user_id][\"call_dict\"]:\n            return False\n        # check if the user call limit is reached\n        if (\n            self.user_call_stats_day[user_id][\"call_dict\"][model]\n            >= self.model_call_day_limit_per_user[model]\n        ):\n            return True\n        return False\n\n    def get_model_call_stats(\n        self, target_model=None, most_recent_min: int = 60, top_k: int = 20\n    ) -> dict:\n        model_call_stats = {}\n        for model, reqs in self.model_call.items():\n            if target_model is not None and model != target_model:\n                continue\n            model_call = []\n            for req in reqs:\n                if req[\"tstamp\"] < time.time() - most_recent_min * 60:\n                    continue\n                model_call.append(req[\"tstamp\"])\n            model_call_stats[model] = len(model_call)\n        if top_k is not None:\n            top_k_model = sorted(\n                model_call_stats, key=lambda x: model_call_stats[x], reverse=True\n            )[:top_k]\n            model_call_stats = {model: model_call_stats[model] for model in top_k_model}\n        return model_call_stats\n\n    def get_user_call_stats(\n        self, target_model=None, most_recent_min: int = 60, top_k: int = 20\n    ) -> dict:\n        user_call_stats = {}\n        for user_id, reqs in self.user_call.items():\n            user_model_call = {\"call_dict\": {}}\n            for req in reqs:\n                if req[\"tstamp\"] < time.time() - most_recent_min * 60:\n                    continue\n                if target_model is not None and req[\"model\"] != target_model:\n                    continue\n                if req[\"model\"] not in user_model_call[\"call_dict\"]:\n                    user_model_call[\"call_dict\"][req[\"model\"]] = 0\n                user_model_call[\"call_dict\"][req[\"model\"]] += 1\n\n            user_model_call[\"total_calls\"] = sum(user_model_call[\"call_dict\"].values())\n            if user_model_call[\"total_calls\"] > 0:\n                user_call_stats[user_id] = user_model_call\n        if top_k is not None:\n            top_k_user = sorted(\n                user_call_stats,\n                key=lambda x: user_call_stats[x][\"total_calls\"],\n                reverse=True,\n            )[:top_k]\n            user_call_stats = {\n                user_id: user_call_stats[user_id] for user_id in top_k_user\n            }\n        return user_call_stats\n\n    def get_num_users(self, most_recent_min: int = 60) -> int:\n        user_call_stats = self.get_user_call_stats(\n            most_recent_min=most_recent_min, top_k=None\n        )\n        return len(user_call_stats)\n\n\nmonitor = Monitor(log_dir_list=LOG_DIR_LIST)\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\nasync def app_startup():\n    asyncio.create_task(monitor.update_stats(2))\n\n\n@app.get(\"/get_model_call_limit/{model}\")\nasync def get_model_call_limit(model: str):\n    return {\"model_call_limit\": {model: monitor.get_model_call_limit(model)}}\n\n\n@app.get(\"/update_model_call_limit/{model}/{limit}\")\nasync def update_model_call_limit(model: str, limit: int):\n    if not monitor.update_model_call_limit(model, limit):\n        return {\"success\": False}\n    return {\"success\": True}\n\n\n@app.get(\"/is_limit_reached\")\nasync def is_limit_reached(model: str, user_id: str):\n    if monitor.is_model_limit_reached(model):\n        return {\n            \"is_limit_reached\": True,\n            \"reason\": f\"MODEL_HOURLY_LIMIT ({model}): {monitor.get_model_call_limit(model)}\",\n        }\n    if monitor.is_user_limit_reached(model, user_id):\n        return {\n            \"is_limit_reached\": True,\n            \"reason\": f\"USER_DAILY_LIMIT ({model}): {monitor.model_call_day_limit_per_user[model]}\",\n        }\n    return {\"is_limit_reached\": False}\n\n\n@app.get(\"/get_num_users_hr\")\nasync def get_num_users():\n    return {\"num_users\": len(monitor.user_call_stats_hour)}\n\n\n@app.get(\"/get_num_users_day\")\nasync def get_num_users_day():\n    return {\"num_users\": len(monitor.user_call_stats_day)}\n\n\n@app.get(\"/get_user_call_stats\")\nasync def get_user_call_stats(\n    model: str = None, most_recent_min: int = 60, top_k: int = None\n):\n    return {\n        \"user_call_stats\": monitor.get_user_call_stats(model, most_recent_min, top_k)\n    }\n\n\n@app.get(\"/get_model_call_stats\")\nasync def get_model_call_stats(\n    model: str = None, most_recent_min: int = 60, top_k: int = None\n):\n    return {\n        \"model_call_stats\": monitor.get_model_call_stats(model, most_recent_min, top_k)\n    }\n", "fastchat/serve/vision/create_vqa_examples_dir.py": "import datasets\nfrom datasets import load_dataset\nfrom PIL import Image\nfrom pathlib import Path\nimport pandas as pd\nimport os\nimport json\nimport tqdm\nimport argparse\nimport shutil\nimport numpy as np\n\nnp.random.seed(0)\n\n\"\"\"\nCreates a directory with images and JSON files for VQA examples. Final json is located in metadata_sampled.json\n\"\"\"\n\n\ndef download_images_and_create_json(\n    dataset_info, cache_dir=\"~/vqa_examples_cache\", base_dir=\"./vqa_examples\"\n):\n    for dataset_name, info in dataset_info.items():\n        dataset_cache_dir = os.path.join(cache_dir, dataset_name)\n        os.makedirs(dataset_cache_dir, exist_ok=True)\n\n        if info[\"subset\"]:\n            dataset = load_dataset(\n                info[\"path\"],\n                info[\"subset\"],\n                cache_dir=dataset_cache_dir,\n                split=info[\"split\"],\n            )\n        else:\n            dataset = load_dataset(\n                info[\"path\"], cache_dir=dataset_cache_dir, split=info[\"split\"]\n            )\n        dataset_dir = os.path.join(base_dir, dataset_name)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        json_data = []\n        for i, item in enumerate(tqdm.tqdm(dataset)):\n            id_key = i if info[\"id_key\"] == \"index\" else item[info[\"id_key\"]]\n            image_pil = item[info[\"image_key\"]].convert(\"RGB\")\n            image_path = os.path.join(dataset_dir, f\"{id_key}.jpg\")\n            image_pil.save(image_path)\n            json_entry = {\n                \"dataset\": dataset_name,\n                \"question\": item[info[\"question_key\"]],\n                \"path\": image_path,\n            }\n            json_data.append(json_entry)\n\n        with open(os.path.join(dataset_dir, \"data.json\"), \"w\") as json_file:\n            json.dump(json_data, json_file, indent=4)\n        # Delete the cache directory for the dataset\n        shutil.rmtree(dataset_cache_dir, ignore_errors=True)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_dir\", type=str, default=\"~/.cache\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"./vqa_examples\")\n    args = parser.parse_args()\n\n    datasets_info = {\n        \"DocVQA\": {\n            \"path\": \"lmms-lab/DocVQA\",\n            \"image_key\": \"image\",\n            \"question_key\": \"question\",\n            \"id_key\": \"questionId\",\n            \"subset\": \"DocVQA\",\n            \"split\": \"test\",\n        },\n        \"ChartQA\": {\n            \"path\": \"HuggingFaceM4/ChartQA\",\n            \"image_key\": \"image\",\n            \"question_key\": \"query\",\n            \"id_key\": \"index\",\n            \"subset\": False,\n            \"split\": \"test\",\n        },\n        \"realworldqa\": {\n            \"path\": \"visheratin/realworldqa\",\n            \"image_key\": \"image\",\n            \"question_key\": \"question\",\n            \"id_key\": \"index\",\n            \"subset\": False,\n            \"split\": \"test\",\n        },\n        \"NewYorker\": {\n            \"path\": \"jmhessel/newyorker_caption_contest\",\n            \"image_key\": \"image\",\n            \"question_key\": \"questions\",\n            \"id_key\": \"index\",\n            \"subset\": \"explanation\",\n            \"split\": \"train\",\n        },\n        \"WikiArt\": {\n            \"path\": \"huggan/wikiart\",\n            \"image_key\": \"image\",\n            \"question_key\": \"artist\",\n            \"id_key\": \"index\",\n            \"subset\": False,\n            \"split\": \"train\",\n        },\n        \"TextVQA\": {\n            \"path\": \"facebook/textvqa\",\n            \"image_key\": \"image\",\n            \"question_key\": \"question\",\n            \"id_key\": \"question_id\",\n            \"subset\": False,\n            \"split\": \"train\",\n        },\n    }\n\n    download_images_and_create_json(\n        datasets_info, cache_dir=args.data_dir, base_dir=args.output_dir\n    )\n    dataset_json = []\n    for dataset_name in datasets_info.keys():\n        with open(f\"{args.output_dir}/{dataset_name}/data.json\") as f:\n            data = json.load(f)\n            dataset_json.extend(np.random.choice(data, 500))\n\n    with open(f\"{args.output_dir}/metadata_sampled.json\", \"w\") as f:\n        json.dump(dataset_json, f, indent=4)\n", "fastchat/serve/monitor/monitor.py": "\"\"\"\nLive monitor of the website statistics and leaderboard.\n\nDependency:\nsudo apt install pkg-config libicu-dev\npip install pytz gradio gdown plotly polyglot pyicu pycld2 tabulate\n\"\"\"\n\nimport argparse\nimport ast\nimport json\nimport pickle\nimport os\nimport threading\nimport time\n\nimport pandas as pd\nimport gradio as gr\nimport numpy as np\n\nfrom fastchat.serve.monitor.basic_stats import report_basic_stats, get_log_files\nfrom fastchat.serve.monitor.clean_battle_data import clean_battle_data\nfrom fastchat.serve.monitor.elo_analysis import report_elo_analysis_results\nfrom fastchat.utils import build_logger, get_window_url_params_js\n\n\nnotebook_url = (\n    \"https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH\"\n)\n\nbasic_component_values = [None] * 6\nleader_component_values = [None] * 5\n\n\ndef make_default_md_1(arena_df, elo_results, mirror=False):\n    link_color = \"#1976D2\"  # This color should be clear in both light and dark mode\n    leaderboard_md = f\"\"\"\n    # \ud83c\udfc6 LMSYS Chatbot Arena Leaderboard \n    <a href='https://lmsys.org/blog/2023-05-03-arena/' style='color: {link_color}; text-decoration: none;'>Blog</a> |\n    <a href='https://arxiv.org/abs/2403.04132' style='color: {link_color}; text-decoration: none;'>Paper</a> |\n    <a href='https://github.com/lm-sys/FastChat' style='color: {link_color}; text-decoration: none;'>GitHub</a> |\n    <a href='https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md' style='color: {link_color}; text-decoration: none;'>Dataset</a> |\n    <a href='https://twitter.com/lmsysorg' style='color: {link_color}; text-decoration: none;'>Twitter</a> |\n    <a href='https://discord.gg/HSWAKCrnFx' style='color: {link_color}; text-decoration: none;'>Discord</a>\n    \"\"\"\n\n    return leaderboard_md\n\n\ndef make_default_md_2(arena_df, elo_results, mirror=False):\n    mirror_str = \"<span style='color: red; font-weight: bold'>This is a mirror of the live leaderboard created and maintained by the <a href='https://lmsys.org' style='color: red; text-decoration: none;'>LMSYS Organization</a>. Please link to <a href='https://leaderboard.lmsys.org' style='color: #B00020; text-decoration: none;'>leaderboard.lmsys.org</a> for citation purposes.</span>\"\n    leaderboard_md = f\"\"\"\n    {mirror_str if mirror else \"\"}\n    \n    LMSYS Chatbot Arena is a crowdsourced open platform for LLM evals. We've collected over 800,000 human pairwise comparisons to rank LLMs with the Bradley-Terry model and display the model ratings in Elo-scale.\n    You can find more details in our paper. **Chatbot arena is dependent on community participation, please contribute by casting your vote!**\n    \"\"\"\n\n    return leaderboard_md\n\n\ndef make_arena_leaderboard_md(arena_df, last_updated_time):\n    total_votes = sum(arena_df[\"num_battles\"]) // 2\n    total_models = len(arena_df)\n    space = \"&nbsp;&nbsp;&nbsp;\"\n\n    leaderboard_md = f\"\"\"\nTotal #models: **{total_models}**.{space} Total #votes: **{\"{:,}\".format(total_votes)}**.{space} Last updated: {last_updated_time}.\n\n\ud83d\udce3 **NEW!** View leaderboard for different categories (e.g., coding, long user query)! This is still in preview and subject to change.\n\nCode to recreate leaderboard tables and plots in this [notebook]({notebook_url}). You can contribute your vote at [chat.lmsys.org](https://chat.lmsys.org)!\n\n***Rank (UB)**: model's ranking (upper-bound), defined by one + the number of models that are statistically better than the target model.\nModel A is statistically better than model B when A's lower-bound score is greater than B's upper-bound score (in 95% confidence interval).\nSee Figure 1 below for visualization of the confidence intervals of model scores.\n\"\"\"\n    return leaderboard_md\n\n\ndef make_category_arena_leaderboard_md(arena_df, arena_subset_df, name=\"Overall\"):\n    total_votes = sum(arena_df[\"num_battles\"]) // 2\n    total_models = len(arena_df)\n    space = \"&nbsp;&nbsp;&nbsp;\"\n    total_subset_votes = sum(arena_subset_df[\"num_battles\"]) // 2\n    total_subset_models = len(arena_subset_df)\n    leaderboard_md = f\"\"\"### {cat_name_to_explanation[name]}\n#### {space} #models: **{total_subset_models} ({round(total_subset_models/total_models *100)}%)** {space} #votes: **{\"{:,}\".format(total_subset_votes)} ({round(total_subset_votes/total_votes * 100)}%)**{space}\n\"\"\"\n    return leaderboard_md\n\n\ndef make_full_leaderboard_md(elo_results):\n    leaderboard_md = \"\"\"\nThree benchmarks are displayed: **Arena Elo**, **MT-Bench** and **MMLU**.\n- [Chatbot Arena](https://chat.lmsys.org/?arena) - a crowdsourced, randomized battle platform. We use 500K+ user votes to compute model strength.\n- [MT-Bench](https://arxiv.org/abs/2306.05685): a set of challenging multi-turn questions. We use GPT-4 to grade the model responses.\n- [MMLU](https://arxiv.org/abs/2009.03300) (5-shot): a test to measure a model's multitask accuracy on 57 tasks.\n\n\ud83d\udcbb Code: The MT-bench scores (single-answer grading on a scale of 10) are computed by [fastchat.llm_judge](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge).\nThe MMLU scores are mostly computed by [InstructEval](https://github.com/declare-lab/instruct-eval).\nHigher values are better for all benchmarks. Empty cells mean not available.\n\"\"\"\n    return leaderboard_md\n\n\ndef make_leaderboard_md_live(elo_results):\n    leaderboard_md = f\"\"\"\n# Leaderboard\nLast updated: {elo_results[\"last_updated_datetime\"]}\n{elo_results[\"leaderboard_table\"]}\n\"\"\"\n    return leaderboard_md\n\n\ndef update_elo_components(\n    max_num_files, elo_results_file, ban_ip_file, exclude_model_names\n):\n    log_files = get_log_files(max_num_files)\n\n    # Leaderboard\n    if elo_results_file is None:  # Do live update\n        ban_ip_list = json.load(open(ban_ip_file)) if ban_ip_file else None\n        battles = clean_battle_data(\n            log_files, exclude_model_names, ban_ip_list=ban_ip_list\n        )\n        elo_results = report_elo_analysis_results(battles, scale=2)\n\n        leader_component_values[0] = make_leaderboard_md_live(elo_results)\n        leader_component_values[1] = elo_results[\"win_fraction_heatmap\"]\n        leader_component_values[2] = elo_results[\"battle_count_heatmap\"]\n        leader_component_values[3] = elo_results[\"bootstrap_elo_rating\"]\n        leader_component_values[4] = elo_results[\"average_win_rate_bar\"]\n\n    # Basic stats\n    basic_stats = report_basic_stats(log_files)\n    md0 = f\"Last updated: {basic_stats['last_updated_datetime']}\"\n\n    md1 = \"### Action Histogram\\n\"\n    md1 += basic_stats[\"action_hist_md\"] + \"\\n\"\n\n    md2 = \"### Anony. Vote Histogram\\n\"\n    md2 += basic_stats[\"anony_vote_hist_md\"] + \"\\n\"\n\n    md3 = \"### Model Call Histogram\\n\"\n    md3 += basic_stats[\"model_hist_md\"] + \"\\n\"\n\n    md4 = \"### Model Call (Last 24 Hours)\\n\"\n    md4 += basic_stats[\"num_chats_last_24_hours\"] + \"\\n\"\n\n    basic_component_values[0] = md0\n    basic_component_values[1] = basic_stats[\"chat_dates_bar\"]\n    basic_component_values[2] = md1\n    basic_component_values[3] = md2\n    basic_component_values[4] = md3\n    basic_component_values[5] = md4\n\n\ndef update_worker(\n    max_num_files, interval, elo_results_file, ban_ip_file, exclude_model_names\n):\n    while True:\n        tic = time.time()\n        update_elo_components(\n            max_num_files, elo_results_file, ban_ip_file, exclude_model_names\n        )\n        durtaion = time.time() - tic\n        print(f\"update duration: {durtaion:.2f} s\")\n        time.sleep(max(interval - durtaion, 0))\n\n\ndef load_demo(url_params, request: gr.Request):\n    logger.info(f\"load_demo. ip: {request.client.host}. params: {url_params}\")\n    return basic_component_values + leader_component_values\n\n\ndef model_hyperlink(model_name, link):\n    return f'<a target=\"_blank\" href=\"{link}\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">{model_name}</a>'\n\n\ndef load_leaderboard_table_csv(filename, add_hyperlink=True):\n    lines = open(filename).readlines()\n    heads = [v.strip() for v in lines[0].split(\",\")]\n    rows = []\n    for i in range(1, len(lines)):\n        row = [v.strip() for v in lines[i].split(\",\")]\n        for j in range(len(heads)):\n            item = {}\n            for h, v in zip(heads, row):\n                if h == \"Arena Elo rating\":\n                    if v != \"-\":\n                        v = int(ast.literal_eval(v))\n                    else:\n                        v = np.nan\n                elif h == \"MMLU\":\n                    if v != \"-\":\n                        v = round(ast.literal_eval(v) * 100, 1)\n                    else:\n                        v = np.nan\n                elif h == \"MT-bench (win rate %)\":\n                    if v != \"-\":\n                        v = round(ast.literal_eval(v[:-1]), 1)\n                    else:\n                        v = np.nan\n                elif h == \"MT-bench (score)\":\n                    if v != \"-\":\n                        v = round(ast.literal_eval(v), 2)\n                    else:\n                        v = np.nan\n                item[h] = v\n            if add_hyperlink:\n                item[\"Model\"] = model_hyperlink(item[\"Model\"], item[\"Link\"])\n        rows.append(item)\n\n    return rows\n\n\ndef build_basic_stats_tab():\n    empty = \"Loading ...\"\n    basic_component_values[:] = [empty, None, empty, empty, empty, empty]\n\n    md0 = gr.Markdown(empty)\n    gr.Markdown(\"#### Figure 1: Number of model calls and votes\")\n    plot_1 = gr.Plot(show_label=False)\n    with gr.Row():\n        with gr.Column():\n            md1 = gr.Markdown(empty)\n        with gr.Column():\n            md2 = gr.Markdown(empty)\n    with gr.Row():\n        with gr.Column():\n            md3 = gr.Markdown(empty)\n        with gr.Column():\n            md4 = gr.Markdown(empty)\n    return [md0, plot_1, md1, md2, md3, md4]\n\n\ndef get_full_table(arena_df, model_table_df):\n    values = []\n    for i in range(len(model_table_df)):\n        row = []\n        model_key = model_table_df.iloc[i][\"key\"]\n        model_name = model_table_df.iloc[i][\"Model\"]\n        # model display name\n        row.append(model_name)\n        if model_key in arena_df.index:\n            idx = arena_df.index.get_loc(model_key)\n            row.append(round(arena_df.iloc[idx][\"rating\"]))\n        else:\n            row.append(np.nan)\n        row.append(model_table_df.iloc[i][\"MT-bench (score)\"])\n        row.append(model_table_df.iloc[i][\"MMLU\"])\n        # Organization\n        row.append(model_table_df.iloc[i][\"Organization\"])\n        # license\n        row.append(model_table_df.iloc[i][\"License\"])\n\n        values.append(row)\n    values.sort(key=lambda x: -x[1] if not np.isnan(x[1]) else 1e9)\n    return values\n\n\ndef create_ranking_str(ranking, ranking_difference):\n    if ranking_difference > 0:\n        return f\"{int(ranking)} \\u2191\"\n    elif ranking_difference < 0:\n        return f\"{int(ranking)} \\u2193\"\n    else:\n        return f\"{int(ranking)}\"\n\n\ndef recompute_final_ranking(arena_df):\n    # compute ranking based on CI\n    ranking = {}\n    for i, model_a in enumerate(arena_df.index):\n        ranking[model_a] = 1\n        for j, model_b in enumerate(arena_df.index):\n            if i == j:\n                continue\n            if (\n                arena_df.loc[model_b][\"rating_q025\"]\n                > arena_df.loc[model_a][\"rating_q975\"]\n            ):\n                ranking[model_a] += 1\n    return list(ranking.values())\n\n\ndef highlight_top_models(df):\n    def highlight_max_rank(s):\n        # Pastel Yellow with transparency, rgba(red, green, blue, alpha)\n        highlight_color = \"rgba(255, 255, 128, 0.2)\"  # 50% transparent\n        if int(s[\"Rank* (UB)\"].replace(\"\u2191\", \"\").replace(\"\u2193\", \"\")) == 1:\n            return [f\"background-color: {highlight_color}\" for _ in s]\n        else:\n            return [\"\" for _ in s]\n\n    # Apply and return the styled DataFrame\n    return df.apply(highlight_max_rank, axis=1)\n\n\ndef get_arena_table(arena_df, model_table_df, arena_subset_df=None):\n    arena_df = arena_df.sort_values(\n        by=[\"final_ranking\", \"rating\"], ascending=[True, False]\n    )\n    arena_df[\"final_ranking\"] = recompute_final_ranking(arena_df)\n    arena_df = arena_df.sort_values(\n        by=[\"final_ranking\", \"rating\"], ascending=[True, False]\n    )\n\n    # sort by rating\n    if arena_subset_df is not None:\n        # filter out models not in the arena_df\n        arena_subset_df = arena_subset_df[arena_subset_df.index.isin(arena_df.index)]\n        arena_subset_df = arena_subset_df.sort_values(by=[\"rating\"], ascending=False)\n        arena_subset_df[\"final_ranking\"] = recompute_final_ranking(arena_subset_df)\n        # keep only the models in the subset in arena_df and recompute final_ranking\n        arena_df = arena_df[arena_df.index.isin(arena_subset_df.index)]\n        # recompute final ranking\n        arena_df[\"final_ranking\"] = recompute_final_ranking(arena_df)\n\n        # assign ranking by the order\n        arena_subset_df[\"final_ranking_no_tie\"] = range(1, len(arena_subset_df) + 1)\n        arena_df[\"final_ranking_no_tie\"] = range(1, len(arena_df) + 1)\n        # join arena_df and arena_subset_df on index\n        arena_df = arena_subset_df.join(\n            arena_df[\"final_ranking\"], rsuffix=\"_global\", how=\"inner\"\n        )\n        arena_df[\"ranking_difference\"] = (\n            arena_df[\"final_ranking_global\"] - arena_df[\"final_ranking\"]\n        )\n\n        arena_df = arena_df.sort_values(\n            by=[\"final_ranking\", \"rating\"], ascending=[True, False]\n        )\n        arena_df[\"final_ranking\"] = arena_df.apply(\n            lambda x: create_ranking_str(x[\"final_ranking\"], x[\"ranking_difference\"]),\n            axis=1,\n        )\n\n    arena_df[\"final_ranking\"] = arena_df[\"final_ranking\"].astype(str)\n\n    values = []\n    for i in range(len(arena_df)):\n        row = []\n        model_key = arena_df.index[i]\n        try:  # this is a janky fix for where the model key is not in the model table (model table and arena table dont contain all the same models)\n            model_name = model_table_df[model_table_df[\"key\"] == model_key][\n                \"Model\"\n            ].values[0]\n            # rank\n            ranking = arena_df.iloc[i].get(\"final_ranking\") or i + 1\n            row.append(ranking)\n            if arena_subset_df is not None:\n                row.append(arena_df.iloc[i].get(\"ranking_difference\") or 0)\n            # model display name\n            row.append(model_name)\n            # elo rating\n            row.append(round(arena_df.iloc[i][\"rating\"]))\n            upper_diff = round(\n                arena_df.iloc[i][\"rating_q975\"] - arena_df.iloc[i][\"rating\"]\n            )\n            lower_diff = round(\n                arena_df.iloc[i][\"rating\"] - arena_df.iloc[i][\"rating_q025\"]\n            )\n            row.append(f\"+{upper_diff}/-{lower_diff}\")\n            # num battles\n            row.append(round(arena_df.iloc[i][\"num_battles\"]))\n            # Organization\n            row.append(\n                model_table_df[model_table_df[\"key\"] == model_key][\n                    \"Organization\"\n                ].values[0]\n            )\n            # license\n            row.append(\n                model_table_df[model_table_df[\"key\"] == model_key][\"License\"].values[0]\n            )\n            cutoff_date = model_table_df[model_table_df[\"key\"] == model_key][\n                \"Knowledge cutoff date\"\n            ].values[0]\n            if cutoff_date == \"-\":\n                row.append(\"Unknown\")\n            else:\n                row.append(cutoff_date)\n            values.append(row)\n        except Exception as e:\n            print(f\"{model_key} - {e}\")\n    return values\n\n\nkey_to_category_name = {\n    \"full\": \"Overall\",\n    \"dedup\": \"De-duplicate Top Redundant Queries (soon to be default)\",\n    \"coding\": \"Coding\",\n    \"hard_6\": \"Hard Prompts (Overall)\",\n    \"hard_english_6\": \"Hard Prompts (English)\",\n    \"long_user\": \"Longer Query\",\n    \"english\": \"English\",\n    \"chinese\": \"Chinese\",\n    \"french\": \"French\",\n    \"german\": \"German\",\n    \"spanish\": \"Spanish\",\n    \"russian\": \"Russian\",\n    \"japanese\": \"Japanese\",\n    \"no_tie\": \"Exclude Ties\",\n    \"no_short\": \"Exclude Short Query (< 5 tokens)\",\n    \"no_refusal\": \"Exclude Refusal\",\n    \"overall_limit_5_user_vote\": \"overall_limit_5_user_vote\",\n    \"full_old\": \"Overall (Deprecated)\",\n}\ncat_name_to_explanation = {\n    \"Overall\": \"Overall Questions\",\n    \"De-duplicate Top Redundant Queries (soon to be default)\": \"De-duplicate top redundant queries (top 0.1%). See details in [blog post](https://lmsys.org/blog/2024-05-17-category-hard/#note-enhancing-quality-through-de-duplication).\",\n    \"Coding\": \"Coding: whether conversation contains code snippets\",\n    \"Hard Prompts (Overall)\": \"Hard Prompts (Overall): details in [blog post](https://lmsys.org/blog/2024-05-17-category-hard/)\",\n    \"Hard Prompts (English)\": \"Hard Prompts (English), note: the delta is to English Category. details in [blog post](https://lmsys.org/blog/2024-05-17-category-hard/)\",\n    \"Longer Query\": \"Longer Query (>= 500 tokens)\",\n    \"English\": \"English Prompts\",\n    \"Chinese\": \"Chinese Prompts\",\n    \"French\": \"French Prompts\",\n    \"German\": \"German Prompts\",\n    \"Spanish\": \"Spanish Prompts\",\n    \"Russian\": \"Russian Prompts\",\n    \"Japanese\": \"Japanese Prompts\",\n    \"Exclude Ties\": \"Exclude Ties and Bothbad\",\n    \"Exclude Short Query (< 5 tokens)\": \"Exclude Short User Query (< 5 tokens)\",\n    \"Exclude Refusal\": 'Exclude model responses with refusal (e.g., \"I cannot answer\")',\n    \"overall_limit_5_user_vote\": \"overall_limit_5_user_vote\",\n    \"Overall (Deprecated)\": \"Overall without De-duplicating Top Redundant Queries (top 0.1%). See details in [blog post](https://lmsys.org/blog/2024-05-17-category-hard/#note-enhancing-quality-through-de-duplication).\",\n}\ncat_name_to_baseline = {\n    \"Hard Prompts (English)\": \"English\",\n}\n\n\ndef build_leaderboard_tab(\n    elo_results_file, leaderboard_table_file, show_plot=False, mirror=False\n):\n    arena_dfs = {}\n    category_elo_results = {}\n    if elo_results_file is None:  # Do live update\n        default_md = \"Loading ...\"\n        p1 = p2 = p3 = p4 = None\n    else:\n        with open(elo_results_file, \"rb\") as fin:\n            elo_results = pickle.load(fin)\n            last_updated_time = None\n            if \"full\" in elo_results:\n                last_updated_time = elo_results[\"full\"][\"last_updated_datetime\"].split(\n                    \" \"\n                )[0]\n                for k in key_to_category_name.keys():\n                    if k not in elo_results:\n                        continue\n                    arena_dfs[key_to_category_name[k]] = elo_results[k][\n                        \"leaderboard_table_df\"\n                    ]\n                    category_elo_results[key_to_category_name[k]] = elo_results[k]\n\n        p1 = category_elo_results[\"Overall\"][\"win_fraction_heatmap\"]\n        p2 = category_elo_results[\"Overall\"][\"battle_count_heatmap\"]\n        p3 = category_elo_results[\"Overall\"][\"bootstrap_elo_rating\"]\n        p4 = category_elo_results[\"Overall\"][\"average_win_rate_bar\"]\n        arena_df = arena_dfs[\"Overall\"]\n        default_md = make_default_md_1(\n            arena_df, category_elo_results[\"Overall\"], mirror=mirror\n        )\n        default_md_2 = make_default_md_2(\n            arena_df, category_elo_results[\"Overall\"], mirror=mirror\n        )\n\n    with gr.Row():\n        with gr.Column(scale=4):\n            md_1 = gr.Markdown(default_md, elem_id=\"leaderboard_markdown\")\n        with gr.Column(scale=1):\n            vote_button = gr.Button(\"Vote!\", link=\"https://chat.lmsys.org\")\n    md2 = gr.Markdown(default_md_2, elem_id=\"leaderboard_markdown\")\n    if leaderboard_table_file:\n        data = load_leaderboard_table_csv(leaderboard_table_file)\n        model_table_df = pd.DataFrame(data)\n\n        with gr.Tabs() as tabs:\n            # arena table\n            arena_table_vals = get_arena_table(arena_df, model_table_df)\n            with gr.Tab(\"Arena\", id=0):\n                md = make_arena_leaderboard_md(arena_df, last_updated_time)\n                gr.Markdown(md, elem_id=\"leaderboard_markdown\")\n                with gr.Row():\n                    with gr.Column(scale=2):\n                        category_dropdown = gr.Dropdown(\n                            choices=list(arena_dfs.keys()),\n                            label=\"Category\",\n                            value=\"Overall\",\n                        )\n                    default_category_details = make_category_arena_leaderboard_md(\n                        arena_df, arena_df, name=\"Overall\"\n                    )\n                    with gr.Column(scale=4, variant=\"panel\"):\n                        category_deets = gr.Markdown(\n                            default_category_details, elem_id=\"category_deets\"\n                        )\n\n                arena_vals = pd.DataFrame(\n                    arena_table_vals,\n                    columns=[\n                        \"Rank* (UB)\",\n                        \"Model\",\n                        \"Arena Elo\",\n                        \"95% CI\",\n                        \"Votes\",\n                        \"Organization\",\n                        \"License\",\n                        \"Knowledge Cutoff\",\n                    ],\n                )\n                elo_display_df = gr.Dataframe(\n                    headers=[\n                        \"Rank* (UB)\",\n                        \"Model\",\n                        \"Arena Elo\",\n                        \"95% CI\",\n                        \"Votes\",\n                        \"Organization\",\n                        \"License\",\n                        \"Knowledge Cutoff\",\n                    ],\n                    datatype=[\n                        \"str\",\n                        \"markdown\",\n                        \"number\",\n                        \"str\",\n                        \"number\",\n                        \"str\",\n                        \"str\",\n                        \"str\",\n                    ],\n                    # value=highlight_top_models(arena_vals.style),\n                    value=arena_vals.style,\n                    elem_id=\"arena_leaderboard_dataframe\",\n                    height=700,\n                    column_widths=[70, 190, 100, 100, 90, 130, 150, 100],\n                    wrap=True,\n                )\n\n                gr.Markdown(\n                    f\"\"\"Note: in each category, we exclude models with fewer than 300 votes as their confidence intervals can be large.\"\"\",\n                    elem_id=\"leaderboard_markdown\",\n                )\n\n                leader_component_values[:] = [default_md, p1, p2, p3, p4]\n\n                if show_plot:\n                    more_stats_md = gr.Markdown(\n                        f\"\"\"## More Statistics for Chatbot Arena (Overall)\"\"\",\n                        elem_id=\"leaderboard_header_markdown\",\n                    )\n                    with gr.Row():\n                        with gr.Column():\n                            gr.Markdown(\n                                \"#### Figure 1: Confidence Intervals on Model Strength (via Bootstrapping)\",\n                                elem_id=\"plot-title\",\n                            )\n                            plot_3 = gr.Plot(p3, show_label=False)\n                        with gr.Column():\n                            gr.Markdown(\n                                \"#### Figure 2: Average Win Rate Against All Other Models (Assuming Uniform Sampling and No Ties)\",\n                                elem_id=\"plot-title\",\n                            )\n                            plot_4 = gr.Plot(p4, show_label=False)\n                    with gr.Row():\n                        with gr.Column():\n                            gr.Markdown(\n                                \"#### Figure 3: Fraction of Model A Wins for All Non-tied A vs. B Battles\",\n                                elem_id=\"plot-title\",\n                            )\n                            plot_1 = gr.Plot(\n                                p1, show_label=False, elem_id=\"plot-container\"\n                            )\n                        with gr.Column():\n                            gr.Markdown(\n                                \"#### Figure 4: Battle Count for Each Combination of Models (without Ties)\",\n                                elem_id=\"plot-title\",\n                            )\n                            plot_2 = gr.Plot(p2, show_label=False)\n            with gr.Tab(\"Full Leaderboard\", id=1):\n                md = make_full_leaderboard_md(elo_results)\n                gr.Markdown(md, elem_id=\"leaderboard_markdown\")\n                full_table_vals = get_full_table(arena_df, model_table_df)\n                gr.Dataframe(\n                    headers=[\n                        \"Model\",\n                        \"Arena Elo\",\n                        \"MT-bench\",\n                        \"MMLU\",\n                        \"Organization\",\n                        \"License\",\n                    ],\n                    datatype=[\"markdown\", \"number\", \"number\", \"number\", \"str\", \"str\"],\n                    value=full_table_vals,\n                    elem_id=\"full_leaderboard_dataframe\",\n                    column_widths=[200, 100, 100, 100, 150, 150],\n                    height=700,\n                    wrap=True,\n                )\n        if not show_plot:\n            gr.Markdown(\n                \"\"\" ## Visit our [HF space](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) for more analysis!\n                If you want to see more models, please help us [add them](https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model).\n                \"\"\",\n                elem_id=\"leaderboard_markdown\",\n            )\n    else:\n        pass\n\n    def update_leaderboard_df(arena_table_vals):\n        elo_datarame = pd.DataFrame(\n            arena_table_vals,\n            columns=[\n                \"Rank* (UB)\",\n                \"Delta\",\n                \"Model\",\n                \"Arena Elo\",\n                \"95% CI\",\n                \"Votes\",\n                \"Organization\",\n                \"License\",\n                \"Knowledge Cutoff\",\n            ],\n        )\n\n        # goal: color the rows based on the rank with styler\n        def highlight_max(s):\n            # all items in S which contain up arrow should be green, down arrow should be red, otherwise black\n            return [\n                \"color: green; font-weight: bold\"\n                if \"\\u2191\" in v\n                else \"color: red; font-weight: bold\"\n                if \"\\u2193\" in v\n                else \"\"\n                for v in s\n            ]\n\n        def highlight_rank_max(s):\n            return [\n                \"color: green; font-weight: bold\"\n                if v > 0\n                else \"color: red; font-weight: bold\"\n                if v < 0\n                else \"\"\n                for v in s\n            ]\n\n        return elo_datarame.style.apply(highlight_max, subset=[\"Rank* (UB)\"]).apply(\n            highlight_rank_max, subset=[\"Delta\"]\n        )\n\n    def update_leaderboard_and_plots(category):\n        arena_subset_df = arena_dfs[category]\n        arena_subset_df = arena_subset_df[arena_subset_df[\"num_battles\"] > 300]\n        elo_subset_results = category_elo_results[category]\n\n        baseline_category = cat_name_to_baseline.get(category, \"Overall\")\n        arena_df = arena_dfs[baseline_category]\n        arena_values = get_arena_table(\n            arena_df,\n            model_table_df,\n            arena_subset_df=arena_subset_df if category != \"Overall\" else None,\n        )\n        if category != \"Overall\":\n            arena_values = update_leaderboard_df(arena_values)\n            # arena_values = highlight_top_models(arena_values)\n            arena_values = gr.Dataframe(\n                headers=[\n                    \"Rank* (UB)\",\n                    \"Delta\",\n                    \"Model\",\n                    \"Arena Elo\",\n                    \"95% CI\",\n                    \"Votes\",\n                    \"Organization\",\n                    \"License\",\n                    \"Knowledge Cutoff\",\n                ],\n                datatype=[\n                    \"str\",\n                    \"number\",\n                    \"markdown\",\n                    \"number\",\n                    \"str\",\n                    \"number\",\n                    \"str\",\n                    \"str\",\n                    \"str\",\n                ],\n                value=arena_values,\n                elem_id=\"arena_leaderboard_dataframe\",\n                height=700,\n                column_widths=[70, 70, 200, 90, 100, 90, 120, 150, 100],\n                wrap=True,\n            )\n        else:\n            # not_arena_values = pd.DataFrame(arena_values, columns=[\"Rank* (UB)\",\n            #         \"Model\",\n            #         \"Arena Elo\",\n            #         \"95% CI\",\n            #         \"Votes\",\n            #         \"Organization\",\n            #         \"License\",\n            #         \"Knowledge Cutoff\",],\n            #         )\n            # arena_values = highlight_top_models(not_arena_values.style)\n            arena_values = gr.Dataframe(\n                headers=[\n                    \"Rank* (UB)\",\n                    \"Model\",\n                    \"Arena Elo\",\n                    \"95% CI\",\n                    \"Votes\",\n                    \"Organization\",\n                    \"License\",\n                    \"Knowledge Cutoff\",\n                ],\n                datatype=[\n                    \"str\",\n                    \"markdown\",\n                    \"number\",\n                    \"str\",\n                    \"number\",\n                    \"str\",\n                    \"str\",\n                    \"str\",\n                ],\n                value=arena_values,\n                elem_id=\"arena_leaderboard_dataframe\",\n                height=700,\n                column_widths=[70, 190, 100, 100, 90, 140, 150, 100],\n                wrap=True,\n            )\n\n        p1 = elo_subset_results[\"win_fraction_heatmap\"]\n        p2 = elo_subset_results[\"battle_count_heatmap\"]\n        p3 = elo_subset_results[\"bootstrap_elo_rating\"]\n        p4 = elo_subset_results[\"average_win_rate_bar\"]\n        more_stats_md = f\"\"\"## More Statistics for Chatbot Arena - {category}\n        \"\"\"\n        leaderboard_md = make_category_arena_leaderboard_md(\n            arena_df, arena_subset_df, name=category\n        )\n        return arena_values, p1, p2, p3, p4, more_stats_md, leaderboard_md\n\n    category_dropdown.change(\n        update_leaderboard_and_plots,\n        inputs=[category_dropdown],\n        outputs=[\n            elo_display_df,\n            plot_1,\n            plot_2,\n            plot_3,\n            plot_4,\n            more_stats_md,\n            category_deets,\n        ],\n    )\n\n    from fastchat.serve.gradio_web_server import acknowledgment_md\n\n    with gr.Accordion(\n        \"Citation\",\n        open=True,\n    ):\n        citation_md = \"\"\"\n            ### Citation\n            Please cite the following paper if you find our leaderboard or dataset helpful.\n            ```\n            @misc{chiang2024chatbot,\n                title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},\n                author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},\n                year={2024},\n                eprint={2403.04132},\n                archivePrefix={arXiv},\n                primaryClass={cs.AI}\n            }\n            \"\"\"\n        gr.Markdown(citation_md, elem_id=\"leaderboard_markdown\")\n        gr.Markdown(acknowledgment_md, elem_id=\"ack_markdown\")\n\n    if show_plot:\n        return [md_1, plot_1, plot_2, plot_3, plot_4]\n    return [md_1]\n\n\ndef build_demo(elo_results_file, leaderboard_table_file):\n    from fastchat.serve.gradio_web_server import block_css\n\n    text_size = gr.themes.sizes.text_lg\n    # load theme from theme.json\n    theme = gr.themes.Default.load(\"theme.json\")\n    # set text size to large\n    theme.text_size = text_size\n    theme.set(\n        button_large_text_size=\"40px\",\n        button_small_text_size=\"40px\",\n        button_large_text_weight=\"1000\",\n        button_small_text_weight=\"1000\",\n        button_shadow=\"*shadow_drop_lg\",\n        button_shadow_hover=\"*shadow_drop_lg\",\n        checkbox_label_shadow=\"*shadow_drop_lg\",\n        button_shadow_active=\"*shadow_inset\",\n        button_secondary_background_fill=\"*primary_300\",\n        button_secondary_background_fill_dark=\"*primary_700\",\n        button_secondary_background_fill_hover=\"*primary_200\",\n        button_secondary_background_fill_hover_dark=\"*primary_500\",\n        button_secondary_text_color=\"*primary_800\",\n        button_secondary_text_color_dark=\"white\",\n    )\n\n    with gr.Blocks(\n        title=\"Chatbot Arena Leaderboard\",\n        # theme=gr.themes.Default(text_size=text_size),\n        theme=theme,\n        css=block_css,\n    ) as demo:\n        with gr.Tabs() as tabs:\n            with gr.Tab(\"Leaderboard\", id=0):\n                leader_components = build_leaderboard_tab(\n                    elo_results_file,\n                    leaderboard_table_file,\n                    show_plot=True,\n                    mirror=False,\n                )\n\n            with gr.Tab(\"Basic Stats\", id=1):\n                basic_components = build_basic_stats_tab()\n\n        url_params = gr.JSON(visible=False)\n        demo.load(\n            load_demo,\n            [url_params],\n            basic_components + leader_components,\n            js=get_window_url_params_js,\n        )\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\"--share\", action=\"store_true\")\n    parser.add_argument(\"--concurrency-count\", type=int, default=10)\n    parser.add_argument(\"--update-interval\", type=int, default=300)\n    parser.add_argument(\"--max-num-files\", type=int)\n    parser.add_argument(\"--elo-results-file\", type=str)\n    parser.add_argument(\"--leaderboard-table-file\", type=str)\n    parser.add_argument(\"--ban-ip-file\", type=str)\n    parser.add_argument(\"--exclude-model-names\", type=str, nargs=\"+\")\n    args = parser.parse_args()\n\n    logger = build_logger(\"monitor\", \"monitor.log\")\n    logger.info(f\"args: {args}\")\n\n    if args.elo_results_file is None:  # Do live update\n        update_thread = threading.Thread(\n            target=update_worker,\n            args=(\n                args.max_num_files,\n                args.update_interval,\n                args.elo_results_file,\n                args.ban_ip_file,\n                args.exclude_model_names,\n            ),\n        )\n        update_thread.start()\n\n    demo = build_demo(args.elo_results_file, args.leaderboard_table_file)\n    demo.queue(\n        default_concurrency_limit=args.concurrency_count,\n        status_update_rate=10,\n        api_open=False,\n    ).launch(\n        server_name=args.host,\n        server_port=args.port,\n        share=args.share,\n        max_threads=200,\n    )\n", "fastchat/serve/monitor/inspect_conv.py": "import argparse\nimport code\nimport datetime\nimport json\nimport os\nfrom pytz import timezone\nimport time\n\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef get_log_files(max_num_files=None):\n    dates = []\n    for month in [4, 5]:\n        for day in range(1, 32):\n            dates.append(f\"2023-{month:02d}-{day:02d}\")\n\n    num_servers = 14\n    filenames = []\n    for d in dates:\n        for i in range(num_servers):\n            name = os.path.expanduser(f\"~/fastchat_logs/server{i}/{d}-conv.json\")\n            if os.path.exists(name):\n                filenames.append(name)\n    max_num_files = max_num_files or len(filenames)\n    filenames = filenames[-max_num_files:]\n    return filenames\n\n\ndef pretty_print_conversation(messages):\n    for role, msg in messages:\n        print(f\"[[{role}]]: {msg}\")\n\n\ndef inspect_convs(log_files):\n    data = []\n    for filename in tqdm(log_files, desc=\"read files\"):\n        for retry in range(5):\n            try:\n                lines = open(filename).readlines()\n                break\n            except FileNotFoundError:\n                time.sleep(2)\n\n        for l in lines:\n            row = json.loads(l)\n\n            if \"states\" not in row:\n                continue\n            if row[\"type\"] not in [\"leftvote\", \"rightvote\", \"bothbad_vote\"]:\n                continue\n\n            model_names = row[\"states\"][0][\"model_name\"], row[\"states\"][1][\"model_name\"]\n            if row[\"type\"] == \"leftvote\":\n                winner, loser = model_names[0], model_names[1]\n                winner_conv, loser_conv = row[\"states\"][0], row[\"states\"][1]\n            elif row[\"type\"] == \"rightvote\":\n                loser, winner = model_names[0], model_names[1]\n                loser_conv, winner_conv = row[\"states\"][0], row[\"states\"][1]\n\n            if loser == \"bard\" and winner == \"vicuna-13b\":\n                print(\"=\" * 20)\n                print(f\"Winner: {winner}\")\n                pretty_print_conversation(winner_conv[\"messages\"])\n                print(f\"Loser: {loser}\")\n                pretty_print_conversation(loser_conv[\"messages\"])\n                print(\"=\" * 20)\n                input()\n\n            # if row[\"type\"] == \"bothbad_vote\" and \"gpt-4\" in model_names:\n            #    print(\"=\" * 20)\n            #    print(f\"Model A: {model_names[0]}\")\n            #    pretty_print_conversation(row[\"states\"][0][\"messages\"])\n            #    print(f\"Model B: {model_names[1]}\")\n            #    pretty_print_conversation(row[\"states\"][1][\"messages\"])\n            #    print(\"=\" * 20)\n            #    input()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--max-num-files\", type=int)\n    args = parser.parse_args()\n\n    log_files = get_log_files(args.max_num_files)\n    inspect_convs(log_files)\n", "fastchat/serve/monitor/deduplication.py": "import os\nimport json\nimport pandas as pd\nimport ast\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\nimport argparse\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--output_dir\", type=str, default=\"output\")\n    parser.add_argument(\"--model\", type=str, default=None)\n    parser.add_argument(\"--input_file\", type=str, required=True)\n    parser.add_argument(\"--percentile\", type=float, default=0.9999)\n    args = parser.parse_args()\n    output_dir = args.output_dir\n    input_file = args.input_file\n\n    with open(input_file) as f:\n        data = json.load(f)\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Preprocessing\n    all_convs_new = []\n    convs = []\n    for row in data:\n        conv = \"\"\n        for turns in row[\"conversation_a\"]:\n            if turns[\"role\"] == \"user\":\n                conv += f\"{turns['content']}\\n\"\n\n        convs.append(conv[:10000])\n        row[\"post_process_conv\"] = conv[:10000]\n        all_convs_new.append(row)\n\n    df = pd.DataFrame(all_convs_new)\n    print(\"Number of conversations: \", len(df))\n\n    prompt_counts = df[\"post_process_conv\"].value_counts()\n    # Select the top 20 most frequent prompts\n    top_prompts = prompt_counts.head(20)\n    print(top_prompts)\n\n    # Determine the percentile count\n    percentile_cutoff = prompt_counts.quantile(args.percentile)\n    print(f\"{args.percentile*100} percentile count: {percentile_cutoff}\")\n\n    # prompts that are more common than the percentile cutoff\n    high_frequency_prompts = prompt_counts[prompt_counts > percentile_cutoff].index\n    print(\n        f\"Number of high frequency prompts: {len(high_frequency_prompts)}/{len(prompt_counts)}\"\n    )\n\n    # initialize a new column dedup_tag\n    dedup_tags = np.array(\n        [{\"high_freq\": False, \"sampled\": True} for _ in range(len(df))]\n    )\n    high_freq_groups = df.groupby(\"post_process_conv\")\n    for prompt in tqdm(high_frequency_prompts):\n        df_high_freq = high_freq_groups.get_group(prompt)\n        sampled_indices = df_high_freq.sample(\n            n=int(percentile_cutoff), random_state=42\n        ).index\n        dedup_tags[df_high_freq.index] = {\"high_freq\": True, \"sampled\": False}\n        dedup_tags[sampled_indices] = {\"high_freq\": True, \"sampled\": True}\n\n    df[\"dedup_tag\"] = dedup_tags\n\n    # drop intermediate columns (post_process_conv)\n    df = df.drop(columns=[\"post_process_conv\"])\n\n    df.to_json(\n        os.path.join(output_dir, \"dedup.json\"),\n        orient=\"records\",\n        indent=4,\n        force_ascii=False,\n    )\n", "fastchat/serve/monitor/summarize_cluster.py": "\"\"\"\nUsage:\npython3 summarize_cluster.py --in results_c20_kmeans_cluster.pkl --model gpt-4 --num-prompts 100\npython3 summarize_cluster.py --in results_c20_kmeans_cluster.pkl --model azure-gpt-4-32k --num-prompts 200\n\"\"\"\nimport argparse\nimport pickle\n\nimport pandas as pd\n\nfrom fastchat.llm_judge.common import (\n    chat_completion_openai,\n    chat_completion_openai_azure,\n    chat_completion_anthropic,\n)\nfrom fastchat.conversation import get_conv_template\n\n\ndef truncate_string(s, l):\n    half = int(l // 2)\n    return s[:half] + s[-half:] if len(s) > l else s\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input-file\", type=str, required=True)\n    parser.add_argument(\"--model\", type=str, default=\"gpt-3.5-turbo\")\n    parser.add_argument(\"--num-prompts\", type=int, default=100)\n    args = parser.parse_args()\n\n    model = args.model\n\n    cluster_infos = pickle.load(open(args.input_file, \"rb\"))\n    num_total_prompts = sum([x[0] for x in cluster_infos])\n\n    topics = []\n    percentages = []\n    for i, info in enumerate(cluster_infos):\n        num_samples, topk_prompts, random_prompts = info\n        percentage = num_samples / num_total_prompts\n        print(\n            f\"cluster {i}, #prompts {num_samples}, percentage: {percentage * 100:.2f}%\"\n        )\n        instruct = \"Given a list of user messages, use less than 8 words to summarize a central topic for all messages in English. Your output should only include a single line. Try to be specific.\"\n        split = int(args.num_prompts * 0.8)\n        prompt = \"\\n\".join(\n            [truncate_string(x, l=200) for x in topk_prompts[:split]]\n            + [\n                truncate_string(x, l=200)\n                for x in random_prompts[: args.num_prompts - split]\n            ]\n        )\n        prompt = \"BEGIN OF THE MESSAGE LIST\\n\" + prompt + \"\\nEND OF THE MESSAGE LIST.\"\n\n        if \"azure-\" in model:\n            template_name = \"chatgpt\"\n            completion_func = chat_completion_openai_azure\n        elif \"gpt\" in model:\n            template_name = \"chatgpt\"\n            completion_func = chat_completion_openai\n        elif \"claude\" in model:\n            template_name = \"claude\"\n            completion_func = chat_completion_anthropic\n\n        conv = get_conv_template(template_name)\n        conv.set_system_message(instruct)\n        conv.append_message(conv.roles[0], prompt)\n        conv.append_message(conv.roles[1], None)\n\n        topic = completion_func(model, conv, temperature=0, max_tokens=256)\n        print(topic)\n\n        topics.append(topic)\n        percentages.append(round(percentage, 6))\n\n    print()\n    print(f\"topics: {topics}\")\n    print(f\"percentages: {percentages}\")\n\n    # save the informations\n    df = pd.DataFrame()\n    df[\"topic\"] = topics\n    df[\"percentage\"] = percentages\n\n    df.to_json(f\"cluster_summary_{len(df)}.jsonl\", lines=True, orient=\"records\")\n", "fastchat/serve/monitor/tag_openai_moderation.py": "\"\"\"\nAdd OpenAI moderation API results to all conversations.\n\"\"\"\nimport argparse\nfrom concurrent.futures import ThreadPoolExecutor\nimport json\nimport os\nimport time\n\nimport openai\nimport requests\nfrom tqdm import tqdm\n\n\nAPI_MAX_RETRY = 16\nAPI_RETRY_SLEEP = 10\nAPI_ERROR_OUTPUT = \"$ERROR$\"\n\n\ndef tag_moderation(text):\n    result = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            result = openai.Moderation.create(input=text)[\"results\"][0]\n            break\n        except openai.error.OpenAIError as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n\n    return result\n\n\ndef tag_openai_moderation(x):\n    conv = x[\"conversation_a\"]\n    user_prompts = \"\\n\".join([x[\"content\"] for x in conv if x[\"role\"] == \"user\"])\n    result = tag_moderation(user_prompts)\n    x[\"openai_moderation\"] = result\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input\", type=str, required=True)\n    parser.add_argument(\n        \"--parallel\", type=int, default=1, help=\"The number of concurrent API calls.\"\n    )\n    parser.add_argument(\"--first-n\", type=int)\n    args = parser.parse_args()\n\n    battles = json.load(open(args.input))\n\n    if args.first_n:\n        battles = battles[: args.first_n]\n\n    with ThreadPoolExecutor(args.parallel) as executor:\n        for line in tqdm(\n            executor.map(tag_openai_moderation, battles), total=len(battles)\n        ):\n            pass\n\n    output = args.input.replace(\".json\", \"_tagged.json\")\n    with open(output, \"w\") as fout:\n        json.dump(battles, fout, indent=2, ensure_ascii=False)\n    print(f\"Write cleaned data to {output}\")\n", "fastchat/serve/monitor/code_tagger.py": "import re\nimport json\nimport argparse\nimport multiprocessing as mp\n\nimport nltk\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\n\n\ndef is_code_conversation(text: str) -> tuple[bool, list[str]]:\n    \"\"\"Check if the text is a code conversation\"\"\"\n\n    if \"```plaintext\" in text:\n        lines = text.split(\"\\n\")\n        line1_idx = [idx for idx, line in enumerate(lines) if \"```plaintext\" in line][0]\n        line2_idx = [\n            line1_idx + 1 + idx\n            for idx, line in enumerate(lines)\n            if \"```\" in line[line1_idx + 1 :]\n        ]\n        if line2_idx:\n            line2_idx = line2_idx[0]\n            text = \"\\n\".join(lines[:line1_idx]) + \"\\n\".join(lines[line2_idx + 1 :])\n        else:\n            text = \"\\n\".join(lines[:line1_idx])\n        return is_code_conversation(text)\n\n    if \"```markdown\" in text:\n        otext = text\n        lines = text.split(\"\\n\")\n        line1_idx = [idx for idx, line in enumerate(lines) if \"```markdown\" in line][0]\n        line2_idx = [\n            line1_idx + 1 + idx\n            for idx, line in enumerate(lines)\n            if \"```\" in line[line1_idx + 1 :]\n        ]\n        if line2_idx:\n            line2_idx = line2_idx[0]\n            text = \"\\n\".join(lines[:line1_idx]) + \"\\n\".join(lines[line2_idx + 1 :])\n        else:\n            text = \"\\n\".join(lines[:line1_idx])\n        return is_code_conversation(text)\n\n    if \"ascii art\" in text.lower():\n        return False, []\n\n    # 1. Check for code formatting\n    if re.search(r\"```\", text):\n        return True, [\"backticks\"]\n\n    # Tokenize the text\n    tokens = word_tokenize(text)\n    tokens = [token.lower() for token in tokens]\n\n    # 2. Check for programming concepts\n    concepts = [\"git\", \"github\", \"pull request\", \"dataframe\", \"nginx\", \"pip\"]\n    if any(concept in tokens for concept in concepts):\n        matched_concepts = list(set(tokens).intersection(set(concepts)))\n        return True, matched_concepts\n\n    # 3. Check for programming language name\n    languages = [\n        \"python\",\n        \"c++\",\n        \"cpp\",\n        \"java\",\n        \"javascript\",\n        \"typescript\",\n        \"html\",\n        \"css\",\n        \"sql\",\n        \"bash\",\n        \"powershell\",\n        \"matlab\",\n        \"golang\",\n        \"linux\",\n        \"ubuntu\",\n    ]\n    if any(language in tokens for language in languages):\n        matched_languages = list(set(tokens).intersection(set(languages)))\n        return True, matched_languages\n\n    # 4. Programming concept substrings\n    strings = [\n        \"import pandas\",\n        \"import numpy\",\n        \"import torch\",\n        \"jax\",\n        \"tensorflow\",\n        \"pytorch\",\n        \"keras\",\n        \"scikit-learn\",\n        \"sklearn\",\n        \" apt-get \",\n    ]\n    found_array = [string in text for string in strings]\n    if any(found_array):\n        matched_strings = [\n            string for string, found in zip(strings, found_array) if found\n        ]\n        return True, matched_strings\n\n    # 5. Programming concept regexes\n    regexes = [\n        r\"from \\w+ import \\w+\",\n        r\"conda install \\w+\",\n        r\"pip install -r \\w+\",\n        r\"conda install -c \\w+ \\w+\",\n        r\"#include <\\w+>\",\n        r\"import \\w+ as \\w+\",\n        r\"#include \\\"\\w+\\.h\\\"\",\n    ]\n    found_array = [re.search(regex, text) for regex in regexes]\n    if any(found_array):\n        matched_regexes = [regex for regex, found in zip(regexes, found_array) if found]\n        return True, matched_regexes\n\n    return False, []\n\n\ndef check_code_conv(conv) -> tuple[bool, list[str]]:\n    \"\"\"Check if the conversation is a code conversation\"\"\"\n    for _, msg in enumerate(conv):\n        content = msg[\"content\"]\n        if not isinstance(content, str):\n            continue\n        is_code_conv_res = is_code_conversation(content)\n        if is_code_conv_res[0]:\n            return is_code_conv_res\n    return False, []\n\n\ndef check_conv_row(conv_row):\n    check_a, code_a = check_code_conv(conv_row[\"conversation_a\"])\n    check_b, code_b = check_code_conv(conv_row[\"conversation_b\"])\n\n    return check_a or check_b, code_a + code_b\n\n\ndef process_battle_file(battle_file_path: str, n_cpus: int):\n    with open(battle_file_path, \"r\") as f:\n        data = json.load(f)\n\n    with mp.Pool(n_cpus) as pool:\n        tagged_data = list(tqdm(pool.imap(check_conv_row, data), total=len(data)))\n\n    output_data = [row for row, (is_code, _) in zip(data, tagged_data) if is_code]\n\n    return output_data\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--clean-battle-file\", type=str)\n    parser.add_argument(\"--output-clean-battle-file\", type=str, default=None)\n    parser.add_argument(\"--n-cpus\", type=int, default=-1)\n\n    args = parser.parse_args()\n\n    if args.output_clean_battle_file is None:\n        args.output_clean_battle_file = args.clean_battle_file\n\n    if args.n_cpus == -1:\n        args.n_cpus = mp.cpu_count()\n\n    print(\n        f\"Processing {args.clean_battle_file} and saving to {args.output_clean_battle_file} with {args.n_cpus} cpus\"\n    )\n\n    output_data = process_battle_file(args.clean_battle_file, args.n_cpus)\n\n    with open(args.output_clean_battle_file, \"w\") as f:\n        json.dump(output_data, f, indent=4)\n\n    print(f\"Total code conversations: {len(output_data)}\")\n    print(\"Done!\")\n\n    with open(args.output_clean_battle_file, \"r\") as f:\n        data = json.load(f)\n", "fastchat/serve/monitor/elo_analysis.py": "import argparse\nimport ast\nfrom collections import defaultdict\nimport datetime\nimport json\nimport math\nimport pickle\nfrom pytz import timezone\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom fastchat.model.model_registry import get_model_info\nfrom fastchat.serve.monitor.basic_stats import get_log_files\nfrom fastchat.serve.monitor.clean_battle_data import clean_battle_data\n\npd.options.display.float_format = \"{:.2f}\".format\n\n\ndef compute_elo(battles, K=4, SCALE=400, BASE=10, INIT_RATING=1000):\n    rating = defaultdict(lambda: INIT_RATING)\n\n    for rd, model_a, model_b, winner in battles[\n        [\"model_a\", \"model_b\", \"winner\"]\n    ].itertuples():\n        ra = rating[model_a]\n        rb = rating[model_b]\n        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))\n        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))\n        if winner == \"model_a\":\n            sa = 1\n        elif winner == \"model_b\":\n            sa = 0\n        elif winner == \"tie\" or winner == \"tie (bothbad)\":\n            sa = 0.5\n        else:\n            raise Exception(f\"unexpected vote {winner}\")\n        rating[model_a] += K * (sa - ea)\n        rating[model_b] += K * (1 - sa - eb)\n\n    return dict(rating)\n\n\ndef get_bootstrap_result(battles, func_compute_elo, num_round=1000):\n    rows = []\n    for i in tqdm(range(num_round), desc=\"bootstrap\"):\n        tmp_battles = battles.sample(frac=1.0, replace=True)\n        rows.append(func_compute_elo(tmp_battles))\n    df = pd.DataFrame(rows)\n    return df[df.median().sort_values(ascending=False).index]\n\n\ndef compute_elo_mle_with_tie(\n    df, SCALE=400, BASE=10, INIT_RATING=1000, sample_weight=None\n):\n    from sklearn.linear_model import LogisticRegression\n\n    ptbl_a_win = pd.pivot_table(\n        df[df[\"winner\"] == \"model_a\"],\n        index=\"model_a\",\n        columns=\"model_b\",\n        aggfunc=\"size\",\n        fill_value=0,\n    )\n    ptbl_tie = pd.pivot_table(\n        df[df[\"winner\"].isin([\"tie\", \"tie (bothbad)\"])],\n        index=\"model_a\",\n        columns=\"model_b\",\n        aggfunc=\"size\",\n        fill_value=0,\n    )\n    ptbl_tie = ptbl_tie + ptbl_tie.T\n    ptbl_b_win = pd.pivot_table(\n        df[df[\"winner\"] == \"model_b\"],\n        index=\"model_a\",\n        columns=\"model_b\",\n        aggfunc=\"size\",\n        fill_value=0,\n    )\n    ptbl_win = ptbl_a_win * 2 + ptbl_b_win.T * 2 + ptbl_tie\n\n    models = pd.Series(np.arange(len(ptbl_win.index)), index=ptbl_win.index)\n\n    p = len(models)\n    X = np.zeros([p * (p - 1) * 2, p])\n    Y = np.zeros(p * (p - 1) * 2)\n\n    cur_row = 0\n    sample_weights = []\n    for m_a in ptbl_win.index:\n        for m_b in ptbl_win.columns:\n            if m_a == m_b:\n                continue\n            # if nan skip\n            if math.isnan(ptbl_win.loc[m_a, m_b]) or math.isnan(ptbl_win.loc[m_b, m_a]):\n                continue\n            X[cur_row, models[m_a]] = +math.log(BASE)\n            X[cur_row, models[m_b]] = -math.log(BASE)\n            Y[cur_row] = 1.0\n            sample_weights.append(ptbl_win.loc[m_a, m_b])\n\n            X[cur_row + 1, models[m_a]] = math.log(BASE)\n            X[cur_row + 1, models[m_b]] = -math.log(BASE)\n            Y[cur_row + 1] = 0.0\n            sample_weights.append(ptbl_win.loc[m_b, m_a])\n            cur_row += 2\n    X = X[:cur_row]\n    Y = Y[:cur_row]\n\n    lr = LogisticRegression(fit_intercept=False, penalty=None)\n    lr.fit(X, Y, sample_weight=sample_weights)\n    elo_scores = SCALE * lr.coef_[0] + INIT_RATING\n    if \"mixtral-8x7b-instruct-v0.1\" in models.index:\n        elo_scores += 1114 - elo_scores[models[\"mixtral-8x7b-instruct-v0.1\"]]\n    return pd.Series(elo_scores, index=models.index).sort_values(ascending=False)\n\n\ndef get_median_elo_from_bootstrap(bootstrap_df):\n    median = dict(bootstrap_df.quantile(0.5))\n    median = {k: int(v + 0.5) for k, v in median.items()}\n    return median\n\n\ndef compute_pairwise_win_fraction(battles, model_order, limit_show_number=None):\n    # Times each model wins as Model A\n    a_win_ptbl = pd.pivot_table(\n        battles[battles[\"winner\"] == \"model_a\"],\n        index=\"model_a\",\n        columns=\"model_b\",\n        aggfunc=\"size\",\n        fill_value=0,\n    )\n\n    # Table counting times each model wins as Model B\n    b_win_ptbl = pd.pivot_table(\n        battles[battles[\"winner\"] == \"model_b\"],\n        index=\"model_a\",\n        columns=\"model_b\",\n        aggfunc=\"size\",\n        fill_value=0,\n    )\n\n    # Table counting number of A-B pairs\n    num_battles_ptbl = pd.pivot_table(\n        battles, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0\n    )\n\n    # Computing the proportion of wins for each model as A and as B\n    # against all other models\n    row_beats_col_freq = (a_win_ptbl + b_win_ptbl.T) / (\n        num_battles_ptbl + num_battles_ptbl.T\n    )\n\n    if model_order is None:\n        prop_wins = row_beats_col_freq.mean(axis=1).sort_values(ascending=False)\n        model_order = list(prop_wins.keys())\n\n    if limit_show_number is not None:\n        model_order = model_order[:limit_show_number]\n\n    # Arrange ordering according to proprition of wins\n    row_beats_col = row_beats_col_freq.loc[model_order, model_order]\n    return row_beats_col\n\n\ndef visualize_leaderboard_table(rating):\n    models = list(rating.keys())\n    models.sort(key=lambda k: -rating[k])\n\n    emoji_dict = {\n        1: \"\ud83e\udd47\",\n        2: \"\ud83e\udd48\",\n        3: \"\ud83e\udd49\",\n    }\n\n    md = \"\"\n    md += \"| Rank | Model | Elo Rating | Description |\\n\"\n    md += \"| --- | --- | --- | --- |\\n\"\n    for i, model in enumerate(models):\n        rank = i + 1\n        minfo = get_model_info(model)\n        emoji = emoji_dict.get(rank, \"\")\n        md += f\"| {rank} | {emoji} [{model}]({minfo.link}) | {rating[model]:.0f} | {minfo.description} |\\n\"\n\n    return md\n\n\ndef visualize_pairwise_win_fraction(battles, model_order, scale=1):\n    row_beats_col = compute_pairwise_win_fraction(battles, model_order)\n    fig = px.imshow(\n        row_beats_col,\n        color_continuous_scale=\"RdBu\",\n        text_auto=\".2f\",\n        height=700 * scale,\n        width=700 * scale,\n    )\n    fig.update_layout(\n        xaxis_title=\"Model B\",\n        yaxis_title=\"Model A\",\n        xaxis_side=\"top\",\n        title_y=0.07,\n        title_x=0.5,\n    )\n    fig.update_traces(\n        hovertemplate=\"Model A: %{y}<br>Model B: %{x}<br>Fraction of A Wins: %{z}<extra></extra>\"\n    )\n\n    return fig\n\n\ndef visualize_battle_count(battles, model_order, scale=1):\n    ptbl = pd.pivot_table(\n        battles, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0\n    )\n    battle_counts = ptbl + ptbl.T\n    fig = px.imshow(\n        battle_counts.loc[model_order, model_order],\n        text_auto=True,\n        height=700 * scale,\n        width=700 * scale,\n    )\n    fig.update_layout(\n        xaxis_title=\"Model B\",\n        yaxis_title=\"Model A\",\n        xaxis_side=\"top\",\n        title_y=0.07,\n        title_x=0.5,\n    )\n    fig.update_traces(\n        hovertemplate=\"Model A: %{y}<br>Model B: %{x}<br>Count: %{z}<extra></extra>\"\n    )\n    return fig\n\n\ndef visualize_average_win_rate(battles, limit_show_number, scale=1):\n    row_beats_col_freq = compute_pairwise_win_fraction(\n        battles, None, limit_show_number=limit_show_number\n    )\n    fig = px.bar(\n        row_beats_col_freq.mean(axis=1).sort_values(ascending=False),\n        text_auto=\".2f\",\n        height=500 * scale,\n        width=700 * scale,\n    )\n    fig.update_layout(\n        yaxis_title=\"Average Win Rate\", xaxis_title=\"Model\", showlegend=False\n    )\n    return fig\n\n\ndef visualize_bootstrap_elo_rating(df, df_final, limit_show_number, scale=1):\n    bars = (\n        pd.DataFrame(\n            dict(\n                lower=df.quantile(0.025),\n                rating=df_final,\n                upper=df.quantile(0.975),\n            )\n        )\n        .reset_index(names=\"model\")\n        .sort_values(\"rating\", ascending=False)\n    )\n    bars = bars[:limit_show_number]\n    bars[\"error_y\"] = bars[\"upper\"] - bars[\"rating\"]\n    bars[\"error_y_minus\"] = bars[\"rating\"] - bars[\"lower\"]\n    bars[\"rating_rounded\"] = np.round(bars[\"rating\"])\n    fig = px.scatter(\n        bars,\n        x=\"model\",\n        y=\"rating\",\n        error_y=\"error_y\",\n        error_y_minus=\"error_y_minus\",\n        text=\"rating_rounded\",\n        height=700,\n        width=700 * scale,\n    )\n    fig.update_layout(xaxis_title=\"Model\", yaxis_title=\"Rating\")\n    return fig\n\n\ndef limit_user_votes(battles, daily_vote_per_user):\n    from datetime import datetime\n\n    print(\"Before limiting user votes: \", len(battles))\n    # add date\n    battles[\"date\"] = battles[\"tstamp\"].apply(\n        lambda x: datetime.fromtimestamp(x).strftime(\"%Y-%m-%d\")\n    )\n\n    battles_new = pd.DataFrame()\n    for date in battles[\"date\"].unique():\n        # only take the first daily_vote_per_user votes per judge per day\n        df_today = battles[battles[\"date\"] == date]\n        df_sub = df_today.groupby(\"judge\").head(daily_vote_per_user)\n\n        # add df_sub to a new dataframe\n        battles_new = pd.concat([battles_new, df_sub])\n    print(\"After limiting user votes: \", len(battles_new))\n    return battles_new\n\n\ndef get_model_pair_stats(battles):\n    battles[\"ordered_pair\"] = battles.apply(\n        lambda x: tuple(sorted([x[\"model_a\"], x[\"model_b\"]])), axis=1\n    )\n\n    model_pair_stats = {}\n\n    for index, row in battles.iterrows():\n        pair = row[\"ordered_pair\"]\n        if pair not in model_pair_stats:\n            model_pair_stats[pair] = {\"win\": 0, \"loss\": 0, \"tie\": 0}\n\n        if row[\"winner\"] in [\"tie\", \"tie (bothbad)\"]:\n            model_pair_stats[pair][\"tie\"] += 1\n        elif row[\"winner\"] == \"model_a\" and row[\"model_a\"] == min(pair):\n            model_pair_stats[pair][\"win\"] += 1\n        elif row[\"winner\"] == \"model_b\" and row[\"model_b\"] == min(pair):\n            model_pair_stats[pair][\"win\"] += 1\n        else:\n            model_pair_stats[pair][\"loss\"] += 1\n\n    return model_pair_stats\n\n\ndef outlier_detect(\n    model_pair_stats,\n    battles,\n    max_vote=100,\n    randomized=False,\n    alpha=0.05,\n    c_param=0.5,\n    user_list=None,\n):\n    if user_list is None:\n        # only check user who has >= 5 votes to save compute\n        user_vote_cnt = battles[\"judge\"].value_counts()\n        user_list = user_vote_cnt[user_vote_cnt >= 5].index.tolist()\n    print(\"#User to be checked: \", len(user_list))\n\n    bad_user_list = []\n    for user in user_list:\n        flag = False\n        p_upper = []\n        p_lower = []\n        df_2 = battles[battles[\"judge\"] == user]\n        for row in df_2.iterrows():\n            if len(p_upper) >= max_vote:\n                break\n\n            model_pair = tuple(sorted([row[1][\"model_a\"], row[1][\"model_b\"]]))\n\n            if row[1][\"winner\"] in [\"tie\", \"tie (bothbad)\"]:\n                vote = 0.5\n            elif row[1][\"winner\"] == \"model_a\" and row[1][\"model_a\"] == model_pair[0]:\n                vote = 1\n            elif row[1][\"winner\"] == \"model_b\" and row[1][\"model_b\"] == model_pair[0]:\n                vote = 1\n            else:\n                vote = 0\n\n            stats = model_pair_stats[model_pair]\n            # count all votes\n            # ratings = np.array(\n            #     [1] * stats[\"win\"] + [0.5] * stats[\"tie\"] + [0] * stats[\"loss\"]\n            # )\n\n            # only count win and loss\n            ratings = np.array([1] * stats[\"win\"] + [0] * stats[\"loss\"])\n            if randomized:\n                noise = np.random.uniform(-1e-5, 1e-5, len(ratings))\n                ratings += noise\n                vote += np.random.uniform(-1e-5, 1e-5)\n\n            p_upper += [(ratings <= vote).mean()]\n            p_lower += [(ratings >= vote).mean()]\n\n            M_upper = np.prod(1 / (2 * np.array(p_upper)))\n            M_lower = np.prod(1 / (2 * np.array(p_lower)))\n\n            # M_upper = np.prod((1 - c_param) / (c_param * np.array(p_upper) ** c_param))\n            # M_lower = np.prod((1 - c_param) / (c_param * np.array(p_lower) ** c_param))\n            if (M_upper > 1 / alpha) or (M_lower > 1 / alpha):\n                print(f\"Identify bad user with {len(p_upper)} votes\")\n                flag = True\n                break\n        if flag:\n            bad_user_list.append({\"user_id\": user, \"votes\": len(p_upper)})\n    print(\"Bad user length: \", len(bad_user_list))\n    print(bad_user_list)\n\n    bad_user_id_list = [x[\"user_id\"] for x in bad_user_list]\n    # remove bad users\n    battles = battles[~battles[\"judge\"].isin(bad_user_id_list)]\n    return battles\n\n\ndef filter_long_conv(row):\n    threshold = 768\n    for conversation_type in [\"conversation_a\", \"conversation_b\"]:\n        cur_conv = row[conversation_type]\n        num_tokens_all = sum([turn[\"num_tokens\"] for turn in cur_conv])\n        if num_tokens_all >= threshold:\n            return True\n    return False\n\n\ndef report_elo_analysis_results(\n    battles_json,\n    rating_system=\"bt\",\n    num_bootstrap=100,\n    exclude_models=[],\n    langs=[],\n    exclude_tie=False,\n    exclude_unknown_lang=False,\n    daily_vote_per_user=None,\n    run_outlier_detect=False,\n    scale=1,\n    filter_func=lambda x: True,\n):\n    battles = pd.DataFrame(battles_json)\n\n    tqdm.pandas(desc=f\"Processing using {filter_func.__name__}\")\n    filtered_indices = battles.progress_apply(filter_func, axis=1)\n    battles = battles[filtered_indices]\n\n    battles = battles.sort_values(ascending=True, by=[\"tstamp\"])\n\n    if len(langs) > 0:\n        battles = battles[battles[\"language\"].isin(langs)]\n    if exclude_unknown_lang:\n        battles = battles[~battles[\"language\"].str.contains(\"unknown\")]\n\n    # remove excluded models\n    battles = battles[\n        ~(\n            battles[\"model_a\"].isin(exclude_models)\n            | battles[\"model_b\"].isin(exclude_models)\n        )\n    ]\n\n    # Only use anonymous votes\n    battles = battles[battles[\"anony\"]].reset_index(drop=True)\n    battles_no_ties = battles[~battles[\"winner\"].str.contains(\"tie\")]\n    if exclude_tie:\n        battles = battles_no_ties\n\n    if daily_vote_per_user is not None:\n        battles = limit_user_votes(battles, daily_vote_per_user)\n\n    if run_outlier_detect:\n        model_pair_stats = get_model_pair_stats(battles)\n        battles = outlier_detect(model_pair_stats, battles)\n\n    print(f\"Number of battles: {len(battles)}\")\n    # Online update\n    elo_rating_online = compute_elo(battles)\n\n    if rating_system == \"bt\":\n        bootstrap_df = get_bootstrap_result(\n            battles, compute_elo_mle_with_tie, num_round=num_bootstrap\n        )\n        elo_rating_final = compute_elo_mle_with_tie(battles)\n    elif rating_system == \"elo\":\n        bootstrap_df = get_bootstrap_result(\n            battles, compute_elo, num_round=num_bootstrap\n        )\n        elo_rating_median = get_median_elo_from_bootstrap(bootstrap_df)\n        elo_rating_final = elo_rating_median\n\n    model_order = list(elo_rating_final.keys())\n\n    model_rating_q025 = bootstrap_df.quantile(0.025)\n    model_rating_q975 = bootstrap_df.quantile(0.975)\n\n    # compute ranking based on CI\n    ranking = {}\n    for i, model_a in enumerate(model_order):\n        ranking[model_a] = 1\n        for j, model_b in enumerate(model_order):\n            if i == j:\n                continue\n            if model_rating_q025[model_b] > model_rating_q975[model_a]:\n                ranking[model_a] += 1\n\n    # leaderboard_table_df: elo rating, variance, 95% interval, number of battles\n    leaderboard_table_df = pd.DataFrame(\n        {\n            \"rating\": elo_rating_final,\n            \"variance\": bootstrap_df.var(),\n            \"rating_q975\": bootstrap_df.quantile(0.975),\n            \"rating_q025\": bootstrap_df.quantile(0.025),\n            \"num_battles\": battles[\"model_a\"]\n            .value_counts()\n            .add(battles[\"model_b\"].value_counts(), fill_value=0),\n            \"final_ranking\": pd.Series(ranking),\n        }\n    )\n\n    model_order.sort(key=lambda k: -elo_rating_final[k])\n    limit_show_number = int(25 * scale)\n    model_order = model_order[:limit_show_number]\n\n    # Plots\n    leaderboard_table = visualize_leaderboard_table(elo_rating_final)\n    win_fraction_heatmap = visualize_pairwise_win_fraction(\n        battles_no_ties, model_order, scale=scale\n    )\n    battle_count_heatmap = visualize_battle_count(\n        battles_no_ties, model_order, scale=scale\n    )\n    average_win_rate_bar = visualize_average_win_rate(\n        battles_no_ties, limit_show_number, scale=scale\n    )\n    bootstrap_elo_rating = visualize_bootstrap_elo_rating(\n        bootstrap_df, elo_rating_final, limit_show_number, scale=scale\n    )\n\n    last_updated_tstamp = battles[\"tstamp\"].max()\n    last_updated_datetime = datetime.datetime.fromtimestamp(\n        last_updated_tstamp, tz=timezone(\"US/Pacific\")\n    ).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n\n    return {\n        \"rating_system\": rating_system,\n        \"elo_rating_online\": elo_rating_online,\n        \"elo_rating_final\": elo_rating_final,\n        \"leaderboard_table\": leaderboard_table,\n        \"win_fraction_heatmap\": win_fraction_heatmap,\n        \"battle_count_heatmap\": battle_count_heatmap,\n        \"average_win_rate_bar\": average_win_rate_bar,\n        \"bootstrap_elo_rating\": bootstrap_elo_rating,\n        \"last_updated_datetime\": last_updated_datetime,\n        \"last_updated_tstamp\": last_updated_tstamp,\n        \"bootstrap_df\": bootstrap_df,\n        \"leaderboard_table_df\": leaderboard_table_df,\n    }\n\n\ndef pretty_print_elo_rating(rating):\n    model_order = list(rating.keys())\n    model_order.sort(key=lambda k: -rating[k])\n    for i, model in enumerate(model_order):\n        print(f\"{i+1:2d}, {model:25s}, {rating[model]:.0f}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--clean-battle-file\", type=str)\n    parser.add_argument(\"--max-num-files\", type=int)\n    parser.add_argument(\"--num-bootstrap\", type=int, default=100)\n    parser.add_argument(\n        \"--rating-system\", type=str, choices=[\"bt\", \"elo\"], default=\"bt\"\n    )\n    parser.add_argument(\"--exclude-models\", type=str, nargs=\"+\", default=[])\n    parser.add_argument(\"--exclude-tie\", action=\"store_true\", default=False)\n    parser.add_argument(\"--exclude-unknown-lang\", action=\"store_true\", default=False)\n    parser.add_argument(\"--exclude-url\", action=\"store_true\", default=False)\n    parser.add_argument(\"--langs\", type=str, nargs=\"+\", default=[])\n    parser.add_argument(\"--daily-vote-per-user\", type=int, default=None)\n    parser.add_argument(\"--run-outlier-detect\", action=\"store_true\", default=False)\n    parser.add_argument(\"--category\", nargs=\"+\", default=[\"full\"])\n    parser.add_argument(\"--scale\", type=float, default=1)\n    args = parser.parse_args()\n\n    np.random.seed(42)\n\n    if args.clean_battle_file:\n        # Read data from a cleaned battle files\n        battles = pd.read_json(args.clean_battle_file)\n    else:\n        # Read data from all log files\n        log_files = get_log_files(args.max_num_files)\n        battles = clean_battle_data(log_files)\n\n    filter_func_map = {\n        \"full\": lambda x: True,\n        \"long\": filter_long_conv,\n        \"chinese\": lambda x: x[\"language\"] == \"Chinese\",\n        \"english\": lambda x: x[\"language\"] == \"English\",\n    }\n    assert all(\n        [cat in filter_func_map for cat in args.category]\n    ), f\"Invalid category: {args.category}\"\n\n    results = {}\n    for cat in args.category:\n        filter_func = filter_func_map[cat]\n        results[cat] = report_elo_analysis_results(\n            battles,\n            rating_system=args.rating_system,\n            num_bootstrap=args.num_bootstrap,\n            exclude_models=args.exclude_models,\n            langs=args.langs,\n            exclude_tie=args.exclude_tie,\n            exclude_unknown_lang=args.exclude_unknown_lang,\n            daily_vote_per_user=args.daily_vote_per_user,\n            run_outlier_detect=args.run_outlier_detect,\n            scale=args.scale,\n            filter_func=filter_func,\n        )\n\n    for cat in args.category:\n        print(f\"# Results for {cat} conversations\")\n        print(\"# Online Elo\")\n        pretty_print_elo_rating(results[cat][\"elo_rating_online\"])\n        print(\"# Median\")\n        pretty_print_elo_rating(results[cat][\"elo_rating_final\"])\n        print(f\"last update : {results[cat]['last_updated_datetime']}\")\n\n        last_updated_tstamp = results[cat][\"last_updated_tstamp\"]\n        cutoff_date = datetime.datetime.fromtimestamp(\n            last_updated_tstamp, tz=timezone(\"US/Pacific\")\n        ).strftime(\"%Y%m%d\")\n        print(f\"last update : {cutoff_date}\")\n\n    with open(f\"elo_results_{cutoff_date}.pkl\", \"wb\") as fout:\n        pickle.dump(results, fout)\n", "fastchat/serve/monitor/intersect_conv_file.py": "\"\"\"\nTake the intersection of two conversation files.\n\nUsage: python3 -m fastchat.data.merge --input input.json --conv-id conv_id_file.json --out intersect.json\n\"\"\"\n\nimport argparse\nimport json\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input\", type=str, required=True)\n    parser.add_argument(\"--conv-id\", type=str, required=True)\n    parser.add_argument(\"--out-file\", type=str, default=\"intersect.json\")\n    args = parser.parse_args()\n\n    conv_id_objs = json.load(open(args.conv_id, \"r\"))\n    conv_ids = set(x[\"conversation_id\"] for x in conv_id_objs)\n\n    objs = json.load(open(args.input, \"r\"))\n    after_objs = [x for x in objs if x[\"conversation_id\"] in conv_ids]\n\n    print(f\"#in: {len(objs)}, #out: {len(after_objs)}\")\n    json.dump(after_objs, open(args.out_file, \"w\"), indent=2, ensure_ascii=False)\n", "fastchat/serve/monitor/basic_stats.py": "import argparse\nimport code\nimport datetime\nimport json\nimport os\nfrom pytz import timezone\nimport time\n\nimport pandas as pd  # pandas>=2.0.3\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom tqdm import tqdm\n\n\nNUM_SERVERS = 14\nLOG_ROOT_DIR = \"~/fastchat_logs\"\n\n\ndef get_log_files(max_num_files=None):\n    log_root = os.path.expanduser(LOG_ROOT_DIR)\n    filenames = []\n    for i in range(NUM_SERVERS):\n        for filename in os.listdir(f\"{log_root}/server{i}\"):\n            if filename.endswith(\"-conv.json\"):\n                filepath = f\"{log_root}/server{i}/{filename}\"\n                name_tstamp_tuple = (filepath, os.path.getmtime(filepath))\n                filenames.append(name_tstamp_tuple)\n    # sort by tstamp\n    filenames = sorted(filenames, key=lambda x: x[1])\n    filenames = [x[0] for x in filenames]\n\n    max_num_files = max_num_files or len(filenames)\n    filenames = filenames[-max_num_files:]\n    return filenames\n\n\ndef load_log_files(filename):\n    data = []\n    for retry in range(5):\n        try:\n            lines = open(filename).readlines()\n            break\n        except FileNotFoundError:\n            time.sleep(2)\n\n    for l in lines:\n        row = json.loads(l)\n        data.append(\n            dict(\n                type=row[\"type\"],\n                tstamp=row[\"tstamp\"],\n                model=row.get(\"model\", \"\"),\n                models=row.get(\"models\", [\"\", \"\"]),\n            )\n        )\n    return data\n\n\ndef load_log_files_parallel(log_files, num_threads=16):\n    data_all = []\n    from multiprocessing import Pool\n\n    with Pool(num_threads) as p:\n        ret_all = list(tqdm(p.imap(load_log_files, log_files), total=len(log_files)))\n        for ret in ret_all:\n            data_all.extend(ret)\n    return data_all\n\n\ndef get_anony_vote_df(df):\n    anony_vote_df = df[\n        df[\"type\"].isin([\"leftvote\", \"rightvote\", \"tievote\", \"bothbad_vote\"])\n    ]\n    anony_vote_df = anony_vote_df[anony_vote_df[\"models\"].apply(lambda x: x[0] == \"\")]\n    return anony_vote_df\n\n\ndef merge_counts(series, on, names):\n    ret = pd.merge(series[0], series[1], on=on)\n    for i in range(2, len(series)):\n        ret = pd.merge(ret, series[i], on=on)\n    ret = ret.reset_index()\n    old_names = list(ret.columns)[-len(series) :]\n    rename = {old_name: new_name for old_name, new_name in zip(old_names, names)}\n    ret = ret.rename(columns=rename)\n    return ret\n\n\ndef report_basic_stats(log_files):\n    df_all = load_log_files_parallel(log_files)\n    df_all = pd.DataFrame(df_all)\n    now_t = df_all[\"tstamp\"].max()\n    df_1_hour = df_all[df_all[\"tstamp\"] > (now_t - 3600)]\n    df_1_day = df_all[df_all[\"tstamp\"] > (now_t - 3600 * 24)]\n    anony_vote_df_all = get_anony_vote_df(df_all)\n\n    # Chat trends\n    chat_dates = [\n        datetime.datetime.fromtimestamp(x, tz=timezone(\"US/Pacific\")).strftime(\n            \"%Y-%m-%d\"\n        )\n        for x in df_all[df_all[\"type\"] == \"chat\"][\"tstamp\"]\n    ]\n    chat_dates_counts = pd.value_counts(chat_dates)\n    vote_dates = [\n        datetime.datetime.fromtimestamp(x, tz=timezone(\"US/Pacific\")).strftime(\n            \"%Y-%m-%d\"\n        )\n        for x in anony_vote_df_all[\"tstamp\"]\n    ]\n    vote_dates_counts = pd.value_counts(vote_dates)\n    chat_dates_bar = go.Figure(\n        data=[\n            go.Bar(\n                name=\"Anony. Vote\",\n                x=vote_dates_counts.index,\n                y=vote_dates_counts,\n                text=[f\"{val:.0f}\" for val in vote_dates_counts],\n                textposition=\"auto\",\n            ),\n            go.Bar(\n                name=\"Chat\",\n                x=chat_dates_counts.index,\n                y=chat_dates_counts,\n                text=[f\"{val:.0f}\" for val in chat_dates_counts],\n                textposition=\"auto\",\n            ),\n        ]\n    )\n    chat_dates_bar.update_layout(\n        barmode=\"stack\",\n        xaxis_title=\"Dates\",\n        yaxis_title=\"Count\",\n        height=300,\n        width=1200,\n    )\n\n    # Model call counts\n    model_hist_all = df_all[df_all[\"type\"] == \"chat\"][\"model\"].value_counts()\n    model_hist_1_day = df_1_day[df_1_day[\"type\"] == \"chat\"][\"model\"].value_counts()\n    model_hist_1_hour = df_1_hour[df_1_hour[\"type\"] == \"chat\"][\"model\"].value_counts()\n    model_hist = merge_counts(\n        [model_hist_all, model_hist_1_day, model_hist_1_hour],\n        on=\"model\",\n        names=[\"All\", \"Last Day\", \"Last Hour\"],\n    )\n    model_hist_md = model_hist.to_markdown(index=False, tablefmt=\"github\")\n\n    # Action counts\n    action_hist_all = df_all[\"type\"].value_counts()\n    action_hist_1_day = df_1_day[\"type\"].value_counts()\n    action_hist_1_hour = df_1_hour[\"type\"].value_counts()\n    action_hist = merge_counts(\n        [action_hist_all, action_hist_1_day, action_hist_1_hour],\n        on=\"type\",\n        names=[\"All\", \"Last Day\", \"Last Hour\"],\n    )\n    action_hist_md = action_hist.to_markdown(index=False, tablefmt=\"github\")\n\n    # Anony vote counts\n    anony_vote_hist_all = anony_vote_df_all[\"type\"].value_counts()\n    anony_vote_df_1_day = get_anony_vote_df(df_1_day)\n    anony_vote_hist_1_day = anony_vote_df_1_day[\"type\"].value_counts()\n    # anony_vote_df_1_hour = get_anony_vote_df(df_1_hour)\n    # anony_vote_hist_1_hour = anony_vote_df_1_hour[\"type\"].value_counts()\n    anony_vote_hist = merge_counts(\n        [anony_vote_hist_all, anony_vote_hist_1_day],\n        on=\"type\",\n        names=[\"All\", \"Last Day\"],\n    )\n    anony_vote_hist_md = anony_vote_hist.to_markdown(index=False, tablefmt=\"github\")\n\n    # Last 24 hours\n    chat_1_day = df_1_day[df_1_day[\"type\"] == \"chat\"]\n    num_chats_last_24_hours = []\n    base = df_1_day[\"tstamp\"].min()\n    for i in range(24, 0, -1):\n        left = base + (i - 1) * 3600\n        right = base + i * 3600\n        num = ((chat_1_day[\"tstamp\"] >= left) & (chat_1_day[\"tstamp\"] < right)).sum()\n        num_chats_last_24_hours.append(num)\n    times = [\n        datetime.datetime.fromtimestamp(\n            base + i * 3600, tz=timezone(\"US/Pacific\")\n        ).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n        for i in range(24, 0, -1)\n    ]\n    last_24_hours_df = pd.DataFrame({\"time\": times, \"value\": num_chats_last_24_hours})\n    last_24_hours_md = last_24_hours_df.to_markdown(index=False, tablefmt=\"github\")\n\n    # Last update datetime\n    last_updated_tstamp = now_t\n    last_updated_datetime = datetime.datetime.fromtimestamp(\n        last_updated_tstamp, tz=timezone(\"US/Pacific\")\n    ).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n\n    # code.interact(local=locals())\n\n    return {\n        \"chat_dates_bar\": chat_dates_bar,\n        \"model_hist_md\": model_hist_md,\n        \"action_hist_md\": action_hist_md,\n        \"anony_vote_hist_md\": anony_vote_hist_md,\n        \"num_chats_last_24_hours\": last_24_hours_md,\n        \"last_updated_datetime\": last_updated_datetime,\n    }\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--max-num-files\", type=int)\n    args = parser.parse_args()\n\n    log_files = get_log_files(args.max_num_files)\n    basic_stats = report_basic_stats(log_files)\n\n    print(basic_stats[\"action_hist_md\"] + \"\\n\")\n    print(basic_stats[\"model_hist_md\"] + \"\\n\")\n    print(basic_stats[\"anony_vote_hist_md\"] + \"\\n\")\n    print(basic_stats[\"num_chats_last_24_hours\"] + \"\\n\")\n", "fastchat/serve/monitor/topic_clustering.py": "\"\"\"\n\nUsage:\npython3 topic_clustering.py --in arena.json --english-only --min-length 32\npython3 topic_clustering.py --in clean_conv_20230809_100k.json --english-only --min-length 32 --max-length 1536\n\"\"\"\nimport argparse\nimport json\nimport pickle\nimport string\nimport time\n\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nimport torch\nfrom tqdm import tqdm\nfrom openai import OpenAI\n\nfrom fastchat.utils import detect_language\n\n\ndef remove_punctuation(input_string):\n    # Make a translator object to remove all punctuation\n    translator = str.maketrans(\"\", \"\", string.punctuation)\n\n    # Use the translator object to remove the punctuation\n    no_punct = input_string.translate(translator)\n    return no_punct\n\n\ndef read_texts(input_file, min_length, max_length, english_only):\n    visited = set()\n    texts = []\n\n    lines = json.load(open(input_file, \"r\"))\n\n    for l in tqdm(lines):\n        if \"text\" in l:\n            line_texts = [l[\"text\"]]\n        elif \"conversation_a\" in l:\n            line_texts = [\n                x[\"content\"] for x in l[\"conversation_a\"] if x[\"role\"] == \"user\"\n            ]\n        elif \"conversation\" in l:\n            line_texts = [\n                x[\"content\"] for x in l[\"conversation\"] if x[\"role\"] == \"user\"\n            ]\n        elif \"turns\" in l:\n            line_texts = l[\"turns\"]\n\n        for text in line_texts:\n            text = text.strip()\n\n            # Filter language\n            if english_only:\n                lang = detect_language(text)\n                if lang != \"English\":\n                    continue\n\n            # Filter short or long prompts\n            if min_length:\n                if len(text) < min_length:\n                    continue\n\n            if max_length:\n                if len(text) > max_length:\n                    continue\n\n            # De-duplication\n            words = sorted([x.lower() for x in remove_punctuation(text).split(\" \")])\n            words = \"\".join(words)\n            if words in visited:\n                continue\n\n            visited.add(words)\n            texts.append(text)\n    return np.array(texts)\n\n\ndef get_embeddings(texts, model_name, batch_size):\n    if model_name == \"text-embedding-ada-002\":\n        client = OpenAI()\n        texts = texts.tolist()\n\n        embeddings = []\n        for i in tqdm(range(0, len(texts), batch_size)):\n            text = texts[i : i + batch_size]\n            responses = client.embeddings.create(input=text, model=model_name).data\n            embeddings.extend([data.embedding for data in responses])\n        embeddings = torch.tensor(embeddings)\n    else:\n        model = SentenceTransformer(model_name)\n        embeddings = model.encode(\n            texts,\n            batch_size=batch_size,\n            show_progress_bar=True,\n            device=\"cuda\",\n            convert_to_tensor=True,\n        )\n\n    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    return embeddings.cpu()\n\n\ndef run_k_means(embeddings, num_clusters):\n    np.random.seed(42)\n    clustering_model = KMeans(n_clusters=num_clusters, n_init=\"auto\")\n    clustering_model.fit(embeddings.numpy())\n    centers = torch.from_numpy(clustering_model.cluster_centers_)\n    labels = torch.from_numpy(clustering_model.labels_)\n\n    # Sort labels\n    classes, counts = np.unique(labels, return_counts=True)\n    indices = np.argsort(counts)[::-1]\n    classes = [classes[i] for i in indices]\n    new_labels = torch.empty_like(labels)\n    new_centers = torch.empty_like(centers)\n    for i, c in enumerate(classes):\n        new_labels[labels == c] = i\n        new_centers[i] = centers[c]\n    return new_centers, new_labels\n\n\ndef run_agg_cluster(embeddings, num_clusters):\n    np.random.seed(42)\n    clustering_model = AgglomerativeClustering(n_clusters=num_clusters)\n    clustering_model.fit(embeddings)\n    labels = torch.from_numpy(clustering_model.labels_)\n\n    # Sort labels\n    classes, counts = np.unique(labels, return_counts=True)\n    indices = np.argsort(counts)[::-1]\n    classes = [classes[i] for i in indices]\n    new_labels = torch.empty_like(labels)\n    for i, c in enumerate(classes):\n        new_labels[labels == c] = i\n\n    # Compute centers\n    centers = []\n    for i in range(len(classes)):\n        centers.append(embeddings[new_labels == i].mean(axis=0, keepdim=True))\n    centers = torch.cat(centers)\n    return centers, new_labels\n\n\ndef run_hdbscan_cluster(embeddings):\n    import hdbscan\n\n    np.random.seed(42)\n    clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n    labels = torch.from_numpy(clusterer.fit_predict(embeddings))\n\n    # Sort labels\n    classes, counts = np.unique(labels, return_counts=True)\n    indices = np.argsort(counts)[::-1]\n    classes = [classes[i] for i in indices]\n    new_labels = torch.empty_like(labels)\n    for i, c in enumerate(classes):\n        new_labels[labels == c] = i\n\n    # Compute centers\n    centers = []\n    for i in range(len(classes)):\n        centers.append(embeddings[new_labels == i].mean(axis=0, keepdim=True))\n    centers = torch.cat(centers)\n    return centers, new_labels\n\n\ndef get_topk_indices(centers, labels, embeddings, topk):\n    indices = []\n    arange = torch.arange(len(labels))\n    counts = torch.unique(labels, return_counts=True)[1]\n    topk = min(topk, counts.min().item())\n    for i in range(len(centers)):\n        tmp_indices = labels == i\n        tmp_arange = arange[tmp_indices]\n        tmp_embeddings = embeddings[tmp_indices]\n\n        scores = cos_sim(centers[i].unsqueeze(0), tmp_embeddings)[0]\n        sorted_indices = torch.flip(torch.argsort(scores), dims=[0])\n        indices.append(tmp_arange[sorted_indices[:topk]].unsqueeze(0))\n    return torch.cat(indices)\n\n\ndef print_topk(texts, labels, topk_indices, show_cut_off):\n    ret = \"\"\n    for k in range(len(topk_indices)):\n        num_samples = torch.sum(labels == k).item()\n\n        ret += \"=\" * 20 + f\" cluster {k}, #samples: {num_samples} \" + \"=\" * 20 + \"\\n\"\n        for idx in topk_indices[k]:\n            ret += \"PROMPT: \" + texts[idx][:show_cut_off] + \"\\n\"\n        ret += \"=\" * 40 + \"\\n\\n\"\n\n    return ret\n\n\ndef get_cluster_info(texts, labels, topk_indices):\n    np.random.seed(42)\n\n    cluster_info = []\n    for k in range(len(topk_indices)):\n        num_samples = torch.sum(labels == k).item()\n        topk_prompts = []\n        for idx in topk_indices[k]:\n            topk_prompts.append(texts[idx])\n        random_prompts = []\n        for idx in range(len(topk_indices)):\n            random_prompts.append(np.random.choice(texts))\n        cluster_info.append((num_samples, topk_prompts, random_prompts))\n\n    return cluster_info\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input-file\", type=str, required=True)\n    parser.add_argument(\"--model\", type=str, default=\"all-mpnet-base-v2\")\n    # default=\"all-MiniLM-L12-v2\")\n    # default=\"multi-qa-distilbert-cos-v1\")\n    parser.add_argument(\"--batch-size\", type=int, default=256)\n    parser.add_argument(\"--min-length\", type=int)\n    parser.add_argument(\"--max-length\", type=int)\n    parser.add_argument(\"--english-only\", action=\"store_true\")\n    parser.add_argument(\"--num-clusters\", type=int, default=20)\n    parser.add_argument(\n        \"--cluster-alg\",\n        type=str,\n        choices=[\"kmeans\", \"aggcls\", \"HDBSCAN\"],\n        default=\"kmeans\",\n    )\n    parser.add_argument(\"--show-top-k\", type=int, default=200)\n    parser.add_argument(\"--show-cut-off\", type=int, default=512)\n    parser.add_argument(\"--save-embeddings\", action=\"store_true\")\n    parser.add_argument(\"--embeddings-file\", type=str, default=None)\n    args = parser.parse_args()\n\n    num_clusters = args.num_clusters\n    show_top_k = args.show_top_k\n    show_cut_off = args.show_cut_off\n\n    texts = read_texts(\n        args.input_file, args.min_length, args.max_length, args.english_only\n    )\n    print(f\"#text: {len(texts)}\")\n\n    if args.embeddings_file is None:\n        embeddings = get_embeddings(texts, args.model, args.batch_size)\n        if args.save_embeddings:\n            # allow saving embedding to save time and money\n            torch.save(embeddings, \"embeddings.pt\")\n    else:\n        embeddings = torch.load(args.embeddings_file)\n    print(f\"embeddings shape: {embeddings.shape}\")\n\n    if args.cluster_alg == \"kmeans\":\n        centers, labels = run_k_means(embeddings, num_clusters)\n    elif args.cluster_alg == \"aggcls\":\n        centers, labels = run_agg_cluster(embeddings, num_clusters)\n    elif args.cluster_alg == \"HDBSCAN\":\n        centers, labels = run_hdbscan_cluster(embeddings)\n    else:\n        raise ValueError(f\"Invalid clustering algorithm: {args.cluster_alg}\")\n\n    topk_indices = get_topk_indices(centers, labels, embeddings, args.show_top_k)\n    topk_str = print_topk(texts, labels, topk_indices, args.show_cut_off)\n    num_clusters = len(centers)\n\n    # Dump results\n    filename_prefix = f\"results_c{num_clusters}_{args.cluster_alg}\"\n    print(topk_str)\n    with open(filename_prefix + \"_topk.txt\", \"w\") as fout:\n        fout.write(topk_str)\n\n    with open(filename_prefix + \"_all.jsonl\", \"w\") as fout:\n        for i in range(len(centers)):\n            tmp_indices = labels == i\n            tmp_embeddings = embeddings[tmp_indices]\n            tmp_texts = texts[tmp_indices]\n\n            scores = cos_sim(centers[i].unsqueeze(0), tmp_embeddings)[0]\n            sorted_indices = torch.flip(torch.argsort(scores), dims=[0])\n\n            for text, score in zip(tmp_texts[sorted_indices], scores[sorted_indices]):\n                obj = {\"cluster\": i, \"text\": text, \"sim\": score.item()}\n                fout.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n\n    cluster_info = get_cluster_info(texts, labels, topk_indices)\n    with open(filename_prefix + \"_cluster.pkl\", \"wb\") as fout:\n        pickle.dump(cluster_info, fout)\n", "fastchat/serve/monitor/criteria_labeling.py": "import argparse\nimport json\nimport pandas as pd\nimport os\nimport re\nimport ast\nimport time\nimport concurrent.futures\nimport tqdm\nimport random\nimport threading\n\nLOCK = threading.RLock()\n\n## Configs\nSYSTEM_PROMPT = \"Your task is to evaluate how well the following input prompts can assess the capabilities of advanced AI assistants.\\n\\nFor the input prompt, please analyze it based on the following 7 criteria.\\n1. Specificity: Does the prompt ask for a specific output, such as code, a mathematical solution, a logical simplification, a problem-solving strategy, or a hardware setup recommendation? This specificity allows the AI to demonstrate its ability to understand and generate precise responses.\\n2. Domain Knowledge: Does the prompt cover a specific domain, such as programming, mathematics, logic, problem-solving, or hardware setup? Prompts spanning a range of topics test the AI's breadth of knowledge and its ability to apply that knowledge to different domains.\\n3. Complexity: Does the prompt vary in complexity, from straightforward tasks to more complex, multi-step problems? This allows evaluators to assess the AI's capability to handle problems of varying difficulty.\\n4. Problem-Solving Skills: Does the prompt directly involves the AI to demonstrate active problem-solving skills, such systemically coming up with a solution for a specific setup instead of regurgitating an existing fact? This tests the AI's ability to apply logical reasoning and provide practical solutions.\\n5. Creativity: Does the prompt involve a level of creativity in approaching the problem? This criterion tests the AI's ability to provide tailored solutions that take into account the user's specific needs and limitations.\\n6. Technical Accuracy: Does the prompt require technical accuracy in the response? This allows evaluators to assess the AI's precision and correctness in technical fields.\\n7. Real-world Application: Does the prompt relate to real-world applications, such as setting up a functional system or writing code for a practical use case? This tests the AI's ability to provide practical and actionable information that could be implemented in real-life scenarios.\\n\\nYou must list the criteria numbers that the prompt satisfies in the format of a Python array. For example, \\\"[...]\\\". Do not explain your choice.\"\n\nENDPOINT_INFO = {\n    \"model_name\": \"META-LLAMA/LLAMA-3-70B-CHAT-HF\",\n    \"name\": \"llama-3-70b-instruct\",\n    \"endpoints\": [{\"api_base\": \"-\", \"api_key\": \"-\"}],\n    \"parallel\": 8,\n    \"temperature\": 0.0,\n    \"max_token\": 512,\n}  # Modify this\n\nTAGS = {\n    1: \"specificity\",\n    2: \"domain_knowledge\",\n    3: \"complexity\",\n    4: \"problem_solving\",\n    5: \"creativity\",\n    6: \"technical_accuracy\",\n    7: \"real_world\",\n}\n\n# API setting constants\nAPI_MAX_RETRY = 3\nAPI_RETRY_SLEEP = 10\nAPI_ERROR_OUTPUT = \"$ERROR$\"\n\n\ndef get_endpoint(endpoint_list):\n    if endpoint_list is None:\n        return None\n    assert endpoint_list is not None\n    # randomly pick one\n    api_dict = random.choices(endpoint_list)[0]\n    return api_dict\n\n\npattern = re.compile(r\"(\\[\\d(?:\\,\\s\\d)*\\])\")\n\n\ndef get_score(judgment):\n    matches = pattern.findall(judgment)\n    matches = [m for m in matches if m != \"\"]\n    if len(set(matches)) == 0:\n        return []\n    elif len(set(matches)) == 1:\n        try:\n            return ast.literal_eval(matches[0])\n        except SyntaxError:\n            print(matches[0])\n            return []\n    else:\n        return []\n\n\ndef chat_completion_openai(model, messages, temperature, max_tokens, api_dict=None):\n    import openai\n\n    if api_dict:\n        client = openai.OpenAI(\n            base_url=api_dict[\"api_base\"],\n            api_key=api_dict[\"api_key\"],\n        )\n    else:\n        client = openai.OpenAI()\n\n    output = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            # print(messages)\n            completion = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                # extra_body={\"guided_choice\": GUIDED_CHOICES} if GUIDED_CHOICES else None,\n            )\n            output = completion.choices[0].message.content\n            break\n        except openai.RateLimitError as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n        except openai.BadRequestError as e:\n            print(messages)\n            print(type(e), e)\n            break\n        except openai.APIConnectionError as e:\n            print(messages)\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n        except openai.InternalServerError as e:\n            print(messages)\n            print(type(e), e)\n            time.sleep(1)\n        except KeyError:\n            print(type(e), e)\n            break\n\n    return output\n\n\ndef get_answer(\n    question: dict,\n    max_tokens: int,\n    temperature: float,\n    answer_file: str,\n    api_dict: dict,\n):\n    conv = []\n    conv.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n\n    conv.append({\"role\": \"user\", \"content\": question[\"prompt\"]})\n    output = chat_completion_openai(\n        model=ENDPOINT_INFO[\"model_name\"],\n        messages=conv,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        api_dict=api_dict,\n    )\n\n    criteria = get_score(output)\n\n    # Dump answers\n    question[\"criteria_tag\"] = {name: bool(i in criteria) for i, name in TAGS.items()}\n    question.drop(\"prompt\")\n\n    with LOCK:\n        with open(answer_file, \"a\") as fout:\n            fout.write(json.dumps(question.to_dict()) + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input-file\", type=str, required=True)\n    parser.add_argument(\"--cache-file\", type=str, default=None)\n    parser.add_argument(\"--output-file\", type=str, required=True)\n    parser.add_argument(\"--convert-to-json\", action=\"store_true\")\n    args = parser.parse_args()\n\n    print(\"loading input data (might take min)\")\n    input_data = pd.read_json(args.input_file)\n    print(f\"{len(input_data)}# of input data just loaded\")\n    if args.cache_file:\n        print(\"loading cache data\")\n        cache_data = pd.read_json(args.cache_file)\n        print(f\"{len(cache_data)}# of cache data just loaded\")\n\n        assert \"criteria_tag\" in cache_data.columns and len(\n            cache_data[\"criteria_tag\"].dropna()\n        ) == len(cache_data)\n\n        not_labeled = input_data[\n            ~input_data[\"question_id\"].isin(cache_data[\"question_id\"])\n        ].copy()\n    else:\n        not_labeled = input_data.copy()\n\n    if os.path.isfile(args.output_file):\n        print(\"loading existing output\")\n        output_data = pd.read_json(args.output_file, lines=True)\n        print(f\"{len(output_data)}# of existing output just loaded\")\n\n        assert \"criteria_tag\" in output_data.columns and len(\n            output_data[\"criteria_tag\"].dropna()\n        ) == len(output_data)\n\n        not_labeled = not_labeled[\n            ~not_labeled[\"question_id\"].isin(output_data[\"question_id\"])\n        ]\n\n    print(f\"{len(not_labeled)} needs to be labeled\")\n\n    not_labeled[\"prompt\"] = not_labeled.conversation_a.map(\n        lambda convo: \"\\n\".join([convo[i][\"content\"] for i in range(0, len(convo), 2)])\n    )\n\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=ENDPOINT_INFO[\"parallel\"]\n    ) as executor:\n        futures = []\n        for index, row in tqdm.tqdm(not_labeled.iterrows()):\n            future = executor.submit(\n                get_answer,\n                row,\n                ENDPOINT_INFO[\"max_token\"],\n                ENDPOINT_INFO[\"temperature\"],\n                args.output_file,\n                get_endpoint(ENDPOINT_INFO[\"endpoints\"]),\n            )\n            futures.append(future)\n        for future in tqdm.tqdm(\n            concurrent.futures.as_completed(futures), total=len(futures)\n        ):\n            future.result()\n\n    if args.convert_to_json:\n        temp = pd.read_json(args.output_file, lines=True)\n        temp.to_json(\n            args.output_file[:-1], orient=\"records\", indent=4, force_ascii=False\n        )\n", "fastchat/serve/monitor/leaderboard_csv_to_html.py": "\"\"\"\nConvert a leaderboard csv file to html table used in the blog.\n\nUsage:\npython3 leaderboard_csv_to_html.py --in leaderboard_table_20230619.csv\n\"\"\"\nimport argparse\n\nimport numpy as np\n\nfrom fastchat.serve.monitor.monitor import load_leaderboard_table_csv\n\n\ndef model_hyperlink(model_name, link):\n    return f'<a target=\"_blank\" href=\"{link}\"> {model_name} </a>'\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input\", type=str, required=True)\n    args = parser.parse_args()\n\n    data = load_leaderboard_table_csv(args.input, add_hyperlink=False)\n    headers = [\n        \"Model\",\n        \"MT-bench (score)\",\n        \"Arena Elo rating\",\n        \"MMLU\",\n        \"License\",\n    ]\n    values = []\n    for item in data:\n        row = []\n        for key in headers:\n            value = item[key]\n            row.append(value)\n        row[0] = model_hyperlink(item[\"Model\"], item[\"Link\"])\n        values.append(row)\n    values.sort(key=lambda x: -x[1] if not np.isnan(x[1]) else 1e9)\n\n    for value in values:\n        row = \"<tr>\"\n        for x in value:\n            try:\n                if np.isnan(x):\n                    x = \"-\"\n            except TypeError:\n                pass\n            row += f\" <td>{x}</td> \"\n        row += \"</tr>\"\n        print(row)\n", "fastchat/serve/monitor/vote_time_stats/plot.py": "import json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\ninfile = \"output.jsonl\"\ndate = \"2024-03\"  # used in the plot\n\ndurations = []\n\nwith open(infile) as f:\n    for line in f:\n        data = json.loads(line)\n        l = data[\"left\"][\"finish\"]\n        r = data[\"right\"][\"finish\"]\n        v = data[\"timestamp\"]\n        durations.append(v - max(l, r))\n\nprint(\n    f\"Avg: {np.mean(durations)}, Median: {np.median(durations)}, Max: {np.max(durations)}\"\n)\n\n# Define the new cutoff and number of bins\ncutoff = 200.0  # New cutoff value\nnum_bins_inside_cutoff = 20  # Number of bins from 0 to cutoff\n\nfor i, n in enumerate(durations):\n    if n > cutoff:\n        durations[i] = cutoff + 0.5 * cutoff / num_bins_inside_cutoff\n\n# Create bin edges from 0 to cutoff, with the specified number of bins\nbin_edges = np.linspace(0, cutoff, num_bins_inside_cutoff + 1)\n\n# Adjusting the overflow bin to end at 110\noverflow_cap = (\n    cutoff + cutoff / num_bins_inside_cutoff\n)  # Adjust as needed based on distribution\nbin_edges = np.append(bin_edges, overflow_cap)\n\n# Create the plot with custom bins\nsns.histplot(\n    durations, bins=bin_edges, kde=False\n)  # Turn off KDE for clearer bar visibility\nplt.title(f'Distribution of \"time to vote\" {date}')\nplt.xlabel(\"Duration (seconds)\")\nplt.ylabel(\"Frequency\")\n\n# Highlight the overflow bin\nplt.axvline(x=cutoff, color=\"red\", linestyle=\"--\")\nplt.text(\n    cutoff + 1, plt.ylim()[1] * 0.9, \"Overflow\", color=\"red\", ha=\"left\"\n)  # Adjust text alignment\n\n# Customizing x-axis labels to hide the \"110\"\nax = plt.gca()  # Get current axis\nlabels = [item.get_text() for item in ax.get_xticklabels()]\nif \"110\" in labels:\n    labels[labels.index(\"110\")] = \"\"  # Replace \"110\" with an empty string\nax.set_xticklabels(labels)\n\n# Ensure nothing is cut off in the plot\nplt.tight_layout()\n\n# Save the plot to a file with high resolution\nplt.savefig(f\"duration_distribution_time_to_vote_{date}.png\", dpi=300)\n", "fastchat/modules/xfastertransformer.py": "from dataclasses import dataclass\nimport sys\n\n\n@dataclass\nclass XftConfig:\n    max_seq_len: int = 4096\n    beam_width: int = 1\n    eos_token_id: int = -1\n    pad_token_id: int = -1\n    num_return_sequences: int = 1\n    is_encoder_decoder: bool = False\n    padding: bool = True\n    early_stopping: bool = False\n    data_type: str = \"bf16_fp16\"\n\n\nclass XftModel:\n    def __init__(self, xft_model, xft_config):\n        self.model = xft_model\n        self.config = xft_config\n\n\ndef load_xft_model(model_path, xft_config: XftConfig):\n    try:\n        import xfastertransformer\n        from transformers import AutoTokenizer\n    except ImportError as e:\n        print(f\"Error: Failed to load xFasterTransformer. {e}\")\n        sys.exit(-1)\n\n    if xft_config.data_type is None or xft_config.data_type == \"\":\n        data_type = \"bf16_fp16\"\n    else:\n        data_type = xft_config.data_type\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_path, use_fast=False, padding_side=\"left\", trust_remote_code=True\n    )\n    xft_model = xfastertransformer.AutoModel.from_pretrained(\n        model_path, dtype=data_type\n    )\n    model = XftModel(xft_model=xft_model, xft_config=xft_config)\n    if model.model.rank > 0:\n        while True:\n            model.model.generate()\n    return model, tokenizer\n", "fastchat/modules/awq.py": "from dataclasses import dataclass, field\nfrom pathlib import Path\nimport sys\n\nimport torch\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, modeling_utils\n\n\n@dataclass\nclass AWQConfig:\n    ckpt: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Load quantized model. The path to the local AWQ checkpoint.\"\n        },\n    )\n    wbits: int = field(default=16, metadata={\"help\": \"#bits to use for quantization\"})\n    groupsize: int = field(\n        default=-1,\n        metadata={\"help\": \"Groupsize to use for quantization; default uses full row.\"},\n    )\n\n\ndef load_awq_quantized(model_name, awq_config: AWQConfig, device):\n    print(\"Loading AWQ quantized model...\")\n\n    try:\n        from tinychat.utils import load_quant\n        from tinychat.modules import make_quant_norm, make_quant_attn, make_fused_mlp\n    except ImportError as e:\n        print(f\"Error: Failed to import tinychat. {e}\")\n        print(\"Please double check if you have successfully installed AWQ\")\n        print(\"See https://github.com/lm-sys/FastChat/blob/main/docs/awq.md\")\n        sys.exit(-1)\n\n    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name, use_fast=False, trust_remote_code=True\n    )\n\n    def skip(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = skip\n    torch.nn.init.kaiming_normal_ = skip\n    torch.nn.init.uniform_ = skip\n    torch.nn.init.normal_ = skip\n    modeling_utils._init_weights = False\n\n    torch.set_default_dtype(torch.half)\n    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n\n    if any(name in find_awq_ckpt(awq_config) for name in [\"llama\", \"vicuna\"]):\n        model = load_quant.load_awq_llama_fast(\n            model,\n            find_awq_ckpt(awq_config),\n            awq_config.wbits,\n            awq_config.groupsize,\n            device,\n        )\n        make_quant_attn(model, device)\n        make_quant_norm(model)\n        make_fused_mlp(model)\n    else:\n        model = load_quant.load_awq_model(\n            model,\n            find_awq_ckpt(awq_config),\n            awq_config.wbits,\n            awq_config.groupsize,\n            device,\n        )\n    return model, tokenizer\n\n\ndef find_awq_ckpt(awq_config: AWQConfig):\n    if Path(awq_config.ckpt).is_file():\n        return awq_config.ckpt\n\n    for ext in [\"*.pt\", \"*.safetensors\"]:\n        matched_result = sorted(Path(awq_config.ckpt).glob(ext))\n        if len(matched_result) > 0:\n            return str(matched_result[-1])\n\n    print(\"Error: AWQ checkpoint not found\")\n    sys.exit(1)\n", "fastchat/modules/exllama.py": "from dataclasses import dataclass, field\nimport sys\n\n\n@dataclass\nclass ExllamaConfig:\n    max_seq_len: int\n    gpu_split: str = None\n    cache_8bit: bool = False\n\n\nclass ExllamaModel:\n    def __init__(self, exllama_model, exllama_cache):\n        self.model = exllama_model\n        self.cache = exllama_cache\n        self.config = self.model.config\n\n\ndef load_exllama_model(model_path, exllama_config: ExllamaConfig):\n    try:\n        from exllamav2 import (\n            ExLlamaV2Config,\n            ExLlamaV2Tokenizer,\n            ExLlamaV2,\n            ExLlamaV2Cache,\n            ExLlamaV2Cache_8bit,\n        )\n    except ImportError as e:\n        print(f\"Error: Failed to load Exllamav2. {e}\")\n        sys.exit(-1)\n\n    exllamav2_config = ExLlamaV2Config()\n    exllamav2_config.model_dir = model_path\n    exllamav2_config.prepare()\n    exllamav2_config.max_seq_len = exllama_config.max_seq_len\n    exllamav2_config.cache_8bit = exllama_config.cache_8bit\n\n    exllama_model = ExLlamaV2(exllamav2_config)\n    tokenizer = ExLlamaV2Tokenizer(exllamav2_config)\n\n    split = None\n    if exllama_config.gpu_split:\n        split = [float(alloc) for alloc in exllama_config.gpu_split.split(\",\")]\n    exllama_model.load(split)\n\n    cache_class = ExLlamaV2Cache_8bit if exllamav2_config.cache_8bit else ExLlamaV2Cache\n    exllama_cache = cache_class(exllama_model)\n    model = ExllamaModel(exllama_model=exllama_model, exllama_cache=exllama_cache)\n\n    return model, tokenizer\n", "fastchat/modules/gptq.py": "from dataclasses import dataclass, field\nimport os\nfrom os.path import isdir, isfile\nfrom pathlib import Path\nimport sys\n\nfrom transformers import AutoTokenizer\n\n\n@dataclass\nclass GptqConfig:\n    ckpt: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Load quantized model. The path to the local GPTQ checkpoint.\"\n        },\n    )\n    wbits: int = field(default=16, metadata={\"help\": \"#bits to use for quantization\"})\n    groupsize: int = field(\n        default=-1,\n        metadata={\"help\": \"Groupsize to use for quantization; default uses full row.\"},\n    )\n    act_order: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to apply the activation order GPTQ heuristic\"},\n    )\n\n\ndef load_gptq_quantized(model_name, gptq_config: GptqConfig):\n    print(\"Loading GPTQ quantized model...\")\n\n    try:\n        script_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n        module_path = os.path.join(script_path, \"../repositories/GPTQ-for-LLaMa\")\n\n        sys.path.insert(0, module_path)\n        from llama import load_quant\n    except ImportError as e:\n        print(f\"Error: Failed to load GPTQ-for-LLaMa. {e}\")\n        print(\"See https://github.com/lm-sys/FastChat/blob/main/docs/gptq.md\")\n        sys.exit(-1)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n    # only `fastest-inference-4bit` branch cares about `act_order`\n    if gptq_config.act_order:\n        model = load_quant(\n            model_name,\n            find_gptq_ckpt(gptq_config),\n            gptq_config.wbits,\n            gptq_config.groupsize,\n            act_order=gptq_config.act_order,\n        )\n    else:\n        # other branches\n        model = load_quant(\n            model_name,\n            find_gptq_ckpt(gptq_config),\n            gptq_config.wbits,\n            gptq_config.groupsize,\n        )\n\n    return model, tokenizer\n\n\ndef find_gptq_ckpt(gptq_config: GptqConfig):\n    if Path(gptq_config.ckpt).is_file():\n        return gptq_config.ckpt\n\n    for ext in [\"*.pt\", \"*.safetensors\"]:\n        matched_result = sorted(Path(gptq_config.ckpt).glob(ext))\n        if len(matched_result) > 0:\n            return str(matched_result[-1])\n\n    print(\"Error: gptq checkpoint not found\")\n    sys.exit(1)\n", "fastchat/modules/__init__.py": ""}